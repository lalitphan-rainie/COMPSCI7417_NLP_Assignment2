Limit CPU / Stack for Java method call?,"<p>I am using an NLP library (Stanford NER) that throws <code>java.lang.OutOfMemoryError</code> errors for rare input documents.</p>
<p>I plan to eventually isolate these documents and figure out what about them causes the errors, but this is hard to do (I'm running in Hadoop, so I just know the error occurs 17% through split 379/500 or something like that). As an interim solution, I'd like to be able to apply a CPU and memory limit to this particular call. I'm not sure what the best way to do this would be. My first thought is to create a fixed thread pool of one thread and use the timed get() on Future. This would at least give me a wall clock limit which would likely help somewhat.</p>
<p>My question is whether there is any way to do better than this with a reasonable amount of effort.</p>
","java, nlp, stanford-nlp","<p>Just catch the OutOfMemoryError, log which document you were on, then move on to the next one. The garbage collector will make sure you have enough memory for the next document.</p>

<p>(This is one of the strategies I use with the Stanford dependency parser to move on to the next sentence if one sentence is too long or convoluted to parse.)</p>
",1,1,936,2009-07-04 04:09:10,https://stackoverflow.com/questions/1081466/limit-cpu-stack-for-java-method-call
Java Stanford NLP: Part of Speech labels?,"<p>The Stanford NLP, demo'd <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""noreferrer"">here</a>, gives an output like this:</p>

<pre><code>Colorless/JJ green/JJ ideas/NNS sleep/VBP furiously/RB ./.
</code></pre>

<p>What do the Part of Speech tags mean? I am unable to find an official list. Is it Stanford's own system, or are they using universal tags? (What is <code>JJ</code>, for instance?)</p>

<p>Also, when I am iterating through the sentences, looking for nouns, for instance, I end up doing something like checking to see if the tag <code>.contains('N')</code>. This feels pretty weak. Is there a better way to programmatically search for a certain part of speech?</p>
","java, nlp, stanford-nlp, part-of-speech","<p><a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&amp;rep=rep1&amp;type=pdf"" rel=""noreferrer"">The Penn Treebank Project</a>. Look at the <a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""noreferrer"">Part-of-speech tagging</a> ps.</p>

<p>JJ is adjective. NNS is noun, plural. VBP is verb present tense. RB is adverb.</p>

<p>That's for english. For chinese, it's the Penn Chinese Treebank. And for german it's the NEGRA corpus.</p>

<blockquote>
  <ol>
  <li>CC Coordinating conjunction </li>
  <li>CD Cardinal number </li>
  <li>DT Determiner </li>
  <li>EX Existential there </li>
  <li>FW Foreign word </li>
  <li>IN Preposition or subordinating conjunction </li>
  <li>JJ Adjective </li>
  <li>JJR Adjective, comparative </li>
  <li>JJS Adjective, superlative </li>
  <li>LS List item marker </li>
  <li>MD Modal </li>
  <li>NN Noun, singular or mass </li>
  <li>NNS Noun, plural </li>
  <li>NNP Proper noun, singular </li>
  <li>NNPS Proper noun, plural </li>
  <li>PDT Predeterminer </li>
  <li>POS Possessive ending </li>
  <li>PRP Personal pronoun </li>
  <li>PRP$ Possessive pronoun </li>
  <li>RB Adverb </li>
  <li>RBR Adverb, comparative </li>
  <li>RBS Adverb, superlative </li>
  <li>RP Particle </li>
  <li>SYM Symbol </li>
  <li>TO to </li>
  <li>UH Interjection </li>
  <li>VB Verb, base form </li>
  <li>VBD Verb, past tense </li>
  <li>VBG Verb, gerund or present participle </li>
  <li>VBN Verb, past participle </li>
  <li>VBP Verb, non­3rd person singular present </li>
  <li>VBZ Verb, 3rd person singular present </li>
  <li>WDT Wh­determiner </li>
  <li>WP Wh­pronoun </li>
  <li>WP$ Possessive wh­pronoun </li>
  <li>WRB Wh­adverb </li>
  </ol>
</blockquote>
",290,186,105417,2009-12-02 14:30:50,https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels
Java Stanford NLP: Spell checking,"<p>I'm trying to check spelling accuracy of text samples using the Stanford NLP. It's just a metric of the text, not a filter or anything, so if it's off by a bit it's fine, as long as the error is uniform.</p>

<p>My first idea was to check if the word is known by the lexicon:</p>

<pre><code>private static LexicalizedParser lp = new LexicalizedParser(""englishPCFG.ser.gz"");

@Analyze(weight=25, name=""Spelling"")
    public double spelling() {
        int result = 0;

        for (List&lt;? extends HasWord&gt; list : sentences) {
            for (HasWord w : list) {
                if (! lp.getLexicon().isKnown(w.word())) {
                    System.out.format(""misspelled: %s\n"", w.word());
                    result++;
                }
            }
        }

        return result / sentences.size();
    }
</code></pre>

<p>However, this produces quite a lot of false positives:</p>

<pre><code>misspelled: Sincerity
misspelled: Sisyphus
misspelled: Sisyphus
misspelled: fidelity
misspelled: negates
misspelled: gods
misspelled: henceforth
misspelled: atom
misspelled: flake
misspelled: Sisyphus
misspelled: Camus
misspelled: foandf
misspelled: foandf
misspelled: babby
misspelled: formd
misspelled: gurl
misspelled: pregnent
misspelled: babby
misspelled: formd
misspelled: gurl
misspelled: pregnent
misspelled: Camus
misspelled: Sincerity
misspelled: Sisyphus
misspelled: Sisyphus
misspelled: fidelity
misspelled: negates
misspelled: gods
misspelled: henceforth
misspelled: atom
misspelled: flake
misspelled: Sisyphus
</code></pre>

<p>Any ideas on how to do this better?</p>
","java, nlp, stanford-nlp, spell-checking","<p>Using the parser's lexicon's isKnown(String) method as a spellchecker isn't a viable use case of the parser.  The method is correct: ""false"" means that this word was not seen (with the given capitalization) in the approximately 1 million words of text the parser is trained from.  But 1 million words just isn't enough text to train a comprehensive spellchecker from in a data-driven manner. People would typically use at least two orders of magnitude of text more, and might well add some cleverness to handle capitalization.  The parser includes some of this cleverness to handle words that were unseen in the training data, but this isn't reflected in what the isKnown(String) method returns.</p>
",9,5,4569,2009-12-05 20:36:14,https://stackoverflow.com/questions/1853378/java-stanford-nlp-spell-checking
Stanford POS tagger in Java,"<p>I'm trying this:</p>

<pre><code>Sentence&lt;TaggedWord&gt; taggedString = MaxentTagger.tagStringTokenized(""here is a string to tag"");
</code></pre>

<p>which gives me:</p>

<blockquote>
  <p>Error:
  \u\nlp\data\pos-tagger\wsj3t0-18-left3words\left3words-wsj-0-18.tagger (The system cannot find the path
  specified)</p>
</blockquote>

<p>I'm using Stanford's <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow noreferrer"">POS tagger</a>.</p>

<p>What can I do to overcome this problem?</p>
","java, stanford-nlp, pos-tagger","<p>It seems you first have to instantiate a tagger passing the included file:</p>

<pre><code>new MaxentTagger(""models/left3words-wsj-0-18.tagger"");
</code></pre>

<p>Which is pretty nasty as the tagging method used later is static:</p>

<pre><code>MaxentTagger.tagStringTokenized(""here is a string to tag"");
</code></pre>

<p>I also had to pass <code>-Xmx256m</code> to make it run with that setup.</p>
",4,1,4955,2009-12-23 18:59:17,https://stackoverflow.com/questions/1954682/stanford-pos-tagger-in-java
"Using the Stanford postagger in java, getting java.lang.IncompatibleClassChangeError","<p>I am trying to initialize the Stanford NLP Part of Speech tagger and I keep getting a java.lang.IncompatibleClassChangeError.  When I print the cause of the error, I get null, when I print the message I get Implementing Class.</p>

<p>This is my code:</p>

<pre><code>    try {
        MaxentTagger tagger = new MaxentTagger(path+""left3words-wsj-0-18.tagger"");
        Reader reader = new BufferedReader(new InputStreamReader(System.in));
        List&lt;Sentence&lt;? extends HasWord&gt;&gt; sentences = MaxentTagger.tokenizeText(reader);
        for (Sentence&lt;? extends HasWord&gt; sentence : sentences) {
            Sentence&lt;TaggedWord&gt; tSentence = MaxentTagger.tagSentence(sentence);
            System.out.println(tSentence.toString(false));
        }

    } catch (IOException e) {
        System.err.println(""READ ERROR"");
        e.printStackTrace();
    } catch (Exception e) {
        System.err.println(""TAGGER ERROR"");
        e.getMessage();
    } catch(java.lang.IncompatibleClassChangeError e){
        e.getStackTrace();
    }

}
</code></pre>

<p>Does anyone know how to fix this?</p>
","java, stanford-nlp, pos-tagger","<p>An <code>IncompatibleClassChangeError</code> happens when some class <code>X</code> has been compiled against one version of some other class <code>Y</code>, and then an attempt is made to load it with a different (incompatible) version of the class <code>Y</code>.  If you are getting this error rather than a subtype, it probably means that <code>Y</code> has changed from an <code>interface</code> to a <code>class</code> or vice-versa, or that there has been a change in its inherited interfaces and implemented classes.</p>

<p>The reason that you are not seeing any <code>getMessage()</code> output is that <code>IncompatibleClassChangeError</code> is a subtype of <code>lava.lang.Error</code> not <code>java.lang.Exception</code>.  Therefore the <code>catch</code> for <code>Exception</code> is not catching it.  Simply copying the <code>println(e.getMessage())</code> line to the next catch block should fix this.</p>

<p>When you print the exception message, it should tell you more about what is causing the problem.  But the root cause is that you need to recompile and/or make sure that you are using the same library JAR files at compile time and runtime.</p>
",1,1,770,2010-01-11 06:06:47,https://stackoverflow.com/questions/2040206/using-the-stanford-postagger-in-java-getting-java-lang-incompatibleclasschangee
Natural Language Processing Package,"<p>I have started working on a project which requires Natural Language Processing. We have do the spell checking as well as mapping sentences to phrases and their synonyms. I first thought of using GATE but i am confused on what to use? I found an interesting post here which got me even more confused. </p>

<p><a href=""http://lordpimpington.com/codespeaks/drupal-5.1/?q=node/5"" rel=""noreferrer"">http://lordpimpington.com/codespeaks/drupal-5.1/?q=node/5</a></p>

<p>Please help me decide on what suits my purpose the best. I am working a web application which will us this NLP tool as a service.</p>
","nlp, stanford-nlp","<p>You didn't really give much info, but try this: <a href=""http://www.nltk.org/"" rel=""noreferrer"">http://www.nltk.org/</a></p>

<p>I don't think NLTK does spell checking (I could be wrong on this), but it can do parts of speech tagging for text input.</p>

<p>For finding/matching synonyms you could use something like WordNet <a href=""http://wordnet.princeton.edu/"" rel=""noreferrer"">http://wordnet.princeton.edu/</a></p>

<p>If you're doing something really domain specific: I would recommend coming up with your own ontology for domain specific terms.</p>
",7,5,1044,2010-03-05 12:29:41,https://stackoverflow.com/questions/2386652/natural-language-processing-package
How to get parent node in Stanford&#39;s JavaNLP?,"<p>Suppose I have such chunk of a sentence:</p>

<pre><code>(NP
      (NP (DT A) (JJ single) (NN page))
      (PP (IN in)
        (NP (DT a) (NN wiki) (NN website))))
</code></pre>

<p>At a certain moment of time I have a reference to <code>(JJ single)</code> and I want to get the <code>NP</code> node binding <code>A single page</code>. If I get it right, that <code>NP</code> is the parent of the node, <code>A</code> and <code>page</code> are its siblings and it has no children (?). When I try to use the <code>.parent()</code> method of a tree, I <em>always</em> get null. The API says that's because the implementation doesn't know how to determine the parent node. Another method of interest is <code>.ancestor(int height, Tree root)</code>, but I don't know how to get the root of the node. In both cases, since the parser knows how to indent and group trees, it must know the ""parent"" tree, right? How can I get it? Thanks</p>
","java, data-structures, tree, nlp, stanford-nlp","<p>It looks like the <code>Tree</code> itself will never return the parent of the node, since it's hardcoded in the <code>Tree</code> sources. Meanwhile <code>TreeGraphNode</code> overrides that method and works well. Changing a Tree to a TreeGraphNode is as easy as </p>

<pre><code>TreeGraphNode sentence = new TreeGraph(tree).root();
</code></pre>
",0,1,1502,2010-04-24 06:06:13,https://stackoverflow.com/questions/2703421/how-to-get-parent-node-in-stanfords-javanlp
Stanford Parser - Traversing the typed dependencies graph,"<p>Basically I want to find a path between two NP tokens in the dependencies graph. However, I can't seem to find a good way to do this in the Stanford Parser. Any help?</p>

<p>Thank You Very Much</p>
","java, graph, nlp, stanford-nlp","<p>The Stanford Parser just returns a list of dependencies between word tokens.  (We do this to avoid external library dependencies.)  But if you want to manipulate the dependencies, you'll almost certainly want to put them in a graph data structure.  We usually use jgrapht: <a href=""http://jgrapht.sourceforge.net/"" rel=""noreferrer"">http://jgrapht.sourceforge.net/</a></p>
",9,3,5082,2010-04-26 10:18:03,https://stackoverflow.com/questions/2712609/stanford-parser-traversing-the-typed-dependencies-graph
R interface for Stanford Parser,"<p>Is there a package that ports Stanford parser in R? </p>
","r, stanford-nlp","<p><a href=""http://cran.r-project.org/web/packages/helloJavaWorld/vignettes/helloJavaWorld.pdf"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/packages/helloJavaWorld/vignettes/helloJavaWorld.pdf</a> will show you how to leverage the rJava package and some minimal Java code to use external Java packages.</p>

<p>The Stanford NLP parsers have API documentation which you could use to create a wrapper in Java.</p>

<p>I am trying to use the NLP parsers from Stanford from R and once I have usable code, I'll b glad to share it with you here.</p>
",2,3,1431,2010-06-04 13:54:27,https://stackoverflow.com/questions/2974728/r-interface-for-stanford-parser
how do I create my own training corpus for stanford tagger?,"<p>I have to analyze informal english text with lots of short hands and local lingo. Hence I was thinking of creating the model for the stanford tagger.</p>

<p>How do i create my own set of labelled corpus for the stanford tagger to train on?</p>

<p>What is the syntax of the corpus and how long should my corpus be in order to achieve a desirable performance?</p>
","java, nlp, stanford-nlp","<p>To train the PoS tagger, see <a href=""https://mailman.stanford.edu/pipermail/parser-user/2009-July/000233.html"" rel=""noreferrer"">this mailing list post</a> which is also included in the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/tagger/maxent/MaxentTagger.html"" rel=""noreferrer"">JavaDocs</a> for the MaxentTagger class.</p>

<p>The javadocs for the <a href=""http://reason.cs.uiuc.edu/mtyoung/postagger/javadoc/edu/stanford/nlp/tagger/maxent/Train.html"" rel=""noreferrer"">edu.stanford.nlp.tagger.maxent.Train class</a> specifies the training format:</p>

<blockquote>
  <p>The training file should be in the
  following format: one word and one tag
  per line separated by a space or a
  tab. Each sentence should end in an
  EOS word-tag pair. (Actually, I'm not
  entirely sure that is still the case,
  but it probably won't hurt. -wmorgan)</p>
</blockquote>
",8,15,7646,2010-07-01 08:49:46,https://stackoverflow.com/questions/3156256/how-do-i-create-my-own-training-corpus-for-stanford-tagger
Identifying collocation in Stanford POS Tagger?,"<p>Is the Stanford POS tagger able to detect collocation? If so, how do I use it?</p>

<p>If I want to provide my own training file for the Stanford POS Tagger, do I have to tag the words according to the 
<a href=""http://images.freshmeat.net/editorials/python_linguistics/wsj_tagged/wsj_0099.pos"" rel=""nofollow noreferrer"">one like the WSJ</a></p>

<p>This means that I have to 'bracket"" the words into Entities and collocation right?</p>

<p>If so, how do I find collocations from the tagger? </p>

<p>I am avoiding the need of using a parser.</p>
","nlp, stanford-nlp","<p>No, the Stanford tagger neither needs nor provides collocations.  It just puts part of speech labels on individual words.  (If you are training a tagger, you don't have to use WSJ tags, but you do have to provide training data with a tag for each word.)</p>
",3,1,780,2010-07-06 06:45:24,https://stackoverflow.com/questions/3184066/identifying-collocation-in-stanford-pos-tagger
stanford tagger - tagging speed,"<p>regarding the stanford tagger, I've provided my own labelled corpus for training the model for the stanford tagger. However, I've realised that the tagging speed of my model for the tagger is much less slower than the default wsjleft3 tagger model. What might contribute to this? And how do I improve the speed of my model? (I've added 3 or 4 custom tags in addition to the Penn treebank tagsets)  </p>
","nlp, stanford-nlp","<p>While adding more features (in arch) makes it a bit slower in general (as feature extraction is one of the main runtime costs), the two big determinants of speed are:</p>

<ul>
<li>Number of context tags used in
features:  left3words uses the
previous and second previous tag (2)
and so is fairly fast, bidirectional
uses 4 (two on each side) and so is
very slow.  A tagger that uses just 1
or 0 context tags is much faster
again.</li>
<li>Size of the tag set in general, and in particular the size of the set of open class tags that can be applied to unknown words.  (But adding 3 or 4 should make almost no difference -- it's problematic when you have a tag set with hundreds of tags.)</li>
</ul>
",4,2,596,2010-07-14 04:17:30,https://stackoverflow.com/questions/3243357/stanford-tagger-tagging-speed
arch options in stanford tagger?,"<p>other than the standard arch options like <code>left3words</code>, <code>left5words</code>, <code>bidirectional</code>, <code>bi5words</code>, what do the rest of the options mean? And what arguments are needed for them?</p>
<p>I can't seem to find the documentation anywhere!</p>
","nlp, stanford-nlp","<p>I'm afraid that the arch options are at present only documented in the source code :-(.</p>

<p>See the ExtractorFrames and ExtractorFramesRare classes.</p>

<p>A first thing to do would be to look at the arch options that are used in the distributed taggers.  You can find them in the *.props files in the models subdirectory.</p>

<p>In brief:</p>

<ul>
<li>""generic"" gives you a decent basic
set of word and tag features
(current, previous, and next word
features, previous tag and previous
two tags, and conjunctions of
previous tag and current word and
current and previous word).  It's a
good place to start.</li>
<li>There are various options that turn on a whole bunch of extractors to give known good configurations for English and Chinese (bidirectional, sighan2005, naacl2003unknowns).</li>
<li>Other options, often with a parameter, turn on sets of features in sensible ways that can be mixed together.  You can see this in the definitions of the distributed Chinese and Arabic taggers.  E.g., suffix(6) includes as features all word-ending substrings of length up to 6.</li>
</ul>
",2,1,345,2010-07-22 07:18:34,https://stackoverflow.com/questions/3306588/arch-options-in-stanford-tagger
Serializing Stanford Parser objects,"<p>I've run into an issue that is requiring me to serialize <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">Stanford Parser</a> objects (all different sorts) to a file for later use.  As far as I know, none of the Stanford Parser objects implement a serialization interface and I'm wondering: is there a way to serialize a Java object when the object doesn't implement serialization or another process to do the same thing?</p>

<p>Thanks!</p>
","java, serialization, stanford-nlp","<p><code>GrammaticalStructure</code> extends <code>TreeGraph</code> that implements <code>Serializable</code>, so it is positively the intent of the developers that you should be able to persist objects of this class. <code>Tree</code> directly implements <code>Serializable</code> and is as such perhaps more suited for persistence (in the eyes of the standford developers).</p>

<p>There is no online API documentation, the information i relay above is taken from the source code of the 1.6.4 (dated 2010-08-20) release of the software. You can download is from <a href=""http://nlp.stanford.edu/software/lex-parser.shtml#Download"" rel=""nofollow noreferrer"">here</a>. It includes javadocs and source code.</p>
",1,0,708,2010-09-04 02:23:30,https://stackoverflow.com/questions/3640638/serializing-stanford-parser-objects
How do I manipulate parse trees?,"<p>I've been playing around with natural language parse trees and manipulating them in various ways. I've been using Stanford's Tregex and Tsurgeon tools but the code is a mess and doesn't fit in well with my mostly Python environment (those tools are Java and aren't ideal for tweaking). I'd like to have a toolset that would allow for easy hacking when I need more functionality. Are there any other tools that are well suited for doing pattern matching on trees and then manipulation of those matched branches?</p>

<p>For example, I'd like to take the following tree as input:</p>

<pre><code>(ROOT
  (S
    (NP
      (NP (NNP Bank))
      (PP (IN of)
        (NP (NNP America))))
    (VP (VBD used)
      (S
        (VP (TO to)
          (VP (VB be)
            (VP (VBN called)
              (NP
                (NP (NNP Bank))
                (PP (IN of)
                  (NP (NNP Italy)))))))))))
</code></pre>

<p>and (this is a simplified example):</p>

<ol>
<li>Find any node with the label NP that has a first child with the label NP and some descendent named ""Bank"", and a second child with the label PP.</li>
<li>If that matches, then take all of the children of the PP node and move them to end of the matched NP's children.</li>
</ol>

<p>For example, take this part of the tree:</p>

<pre><code>(NP
  (NP (NNP Bank))
  (PP (IN of)
    (NP (NNP America))))
</code></pre>

<p>and turn it into this:</p>

<pre><code>(NP
  (NP (NNP Bank) (IN of) (NP (NNP America))))
</code></pre>

<p>Since my input trees are S-expressions I've considered using Lisp (embedded into my Python program) but it's been so long that I've written anything significant in Lisp that I have no idea where to even start.</p>

<p>What would be a good way to describe the patterns? What would be a good way to describe the manipulations? What's a good way to think about this problem?</p>
","lisp, nlp, pattern-matching, stanford-nlp, s-expression","<p>This is a typical case of using Lisp. You would need a function that maps another function over the tree.</p>

<p>Here is a procedural matching example using Common Lisp. There are matchers in Lisp that work over list structures, which could be used instead. Using a list matcher would simplify the example (see my other answer for an example using a pattern matcher).</p>

<p>The code:</p>

<pre><code>(defun node-children (node)
  (rest node))

(defun node-name (node)
  (second node))

(defun node-type (node)
  (first node))


(defun treemap (tree matcher transformer)
  (cond ((null tree) nil)
        ((consp tree)
         (if (funcall matcher tree)
             (funcall transformer tree)
           (cons (node-type tree)
                 (mapcar (lambda (child)
                           (treemap child matcher transformer))
                         (node-children tree)))))
        (t tree))))
</code></pre>

<p>The example:</p>

<pre><code>(defvar *tree*
  '(ROOT
    (S
     (NP
      (NP (NNP Bank))
      (PP (IN of)
          (NP (NNP America))))
     (VP (VBD used)
         (S
          (VP (TO to)
              (VP (VB be)
                  (VP (VBN called)
                      (NP
                       (NP (NNP Bank))
                       (PP (IN of)
                           (NP (NNP Italy))))))))))))



(defun example ()
  (pprint
   (treemap *tree*
            (lambda (node)
              (and (= (length (node-children node)) 2)
                   (eq (node-type (first (node-children node))) 'np)
                   (some (lambda (node)
                           (eq (node-name node) 'bank))
                         (children (first (node-children node))))
                   (eq (first (second (node-children node))) 'pp)))
            (lambda (node)
              (list (node-type node)
                    (append (first (node-children node))
                            (node-children (second (node-children node)))))))))
</code></pre>

<p>Running the example:</p>

<pre><code>CL-USER 75 &gt; (example)

(ROOT
 (S
  (NP
   (NP (NNP BANK) (IN OF) (NP (NNP AMERICA))))
  (VP
   (VBD USED)
   (S
    (VP
     (TO TO)
     (VP
      (VB BE)
      (VP
       (VBN CALLED)
       (NP
        (NP
         (NNP BANK)
         (IN OF)
         (NP (NNP ITALY)))))))))))
</code></pre>
",8,15,3979,2010-09-12 01:03:10,https://stackoverflow.com/questions/3693323/how-do-i-manipulate-parse-trees
How to get POS tagging using Stanford Parser,"<p>I'm using Stanford Parser to parse the dependence relations between pair of words, but I also need the tagging of words. However, in the ParseDemo.java, the program only output the Tagging Tree. I need each word's tagging like this:</p>

<pre><code>My/PRP$ dog/NN also/RB likes/VBZ eating/VBG bananas/NNS ./.
</code></pre>

<p>not like this:</p>

<pre><code>(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (S
            (ADJP (NNS bananas))))))
    (. .)))
</code></pre>

<p>Who can help me? thanks a lot.</p>
","nlp, stanford-nlp","<p>If you're mainly interested in manipulating the tags in a program, and don't need the <code>TreePrint</code> functionality, you can just get the tagged words as a List:</p>

<pre><code>LexicalizedParser lp =
  LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
Tree parse = lp.apply(Arrays.asList(sent));
List taggedWords = parse.taggedYield();    
</code></pre>
",12,9,7825,2010-09-17 08:01:24,https://stackoverflow.com/questions/3733587/how-to-get-pos-tagging-using-stanford-parser
How to use Stanford NLP API to retrieve phrases or tokens from NL query?,"<p>I need phrases returned from Stanford parser to use to in my program. </p>
","java, tokenize, stanford-nlp","<p>Do you just want the tokens (words)?  If so, you want something like:</p>

<pre><code>Reader r; // initialized somehow by you
Tokenizer&lt;CoreLabel&gt; tokenizer = new PTBTokenizer&lt;CoreLabel&gt;(r, new CoreLabelTokenFactory(), """");
while (tokenizer.hasNext()) {
  CoreLabel token = tokenizer.next();
  System.out.println(token);
}
</code></pre>

<p>Or do you want the phrases in the parse tree?  If so, you should get the returned Tree as in ParserDemo in the distribution and use the phrases (subtrees) in it (you can iterate over them:</p>

<pre><code>Tree parse = lp.apply(sentence);
for (Tree subtree : tree) {
  System.out.println(subtree);
}
</code></pre>
",0,0,1775,2010-09-23 10:23:20,https://stackoverflow.com/questions/3777386/how-to-use-stanford-nlp-api-to-retrieve-phrases-or-tokens-from-nl-query
Stanford parser - typed dependencies,"<p>I need to handle with possessive dependencies in stanford parser.I read in a mailing list that to use the output format</p>

<pre><code>includePunctuationDependencies 
</code></pre>

<p>but I get this exception. Can anyone please give me link to the Stanford parser that supports this output format?</p>

<p>I'm currently using the Stanford parser version 9-07-2010.</p>

<pre><code>java.lang.RuntimeException: Error: output tree format includePunctuationDependencies not supported
        at edu.stanford.nlp.trees.TreePrint.&lt;init&gt;(TreePrint.java:134)
        at edu.stanford.nlp.trees.TreePrint.&lt;init&gt;(TreePrint.java:101)
        at edu.stanford.nlp.trees.TreePrint.&lt;init&gt;(TreePrint.java:89)
        at finalproj.logic.parser.SentenceSplitterTagger.getTypedDependenciesSentence(SentenceSplitterTagger.java:82)
        at finalproj.logic.parser.SentenceSplitterTagger.splitFile(SentenceSplitterTagger.java:36)
        at finalproj.ui.LoadReqFile.jBtnSentenceSplitActionPerformed(LoadReqFile.java:143)
        at finalproj.ui.LoadReqFile.access$200(LoadReqFile.java:23)
        at finalproj.ui.LoadReqFile$3.actionPerformed(LoadReqFile.java:74)
        at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:1995)
        at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2318)
        at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:387)
        at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:242)
        at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(BasicButtonListener.java:236)
        at java.awt.Component.processMouseEvent(Component.java:6267)
        at javax.swing.JComponent.processMouseEvent(JComponent.java:3267)
        at java.awt.Component.processEvent(Component.java:6032)
        at java.awt.Container.processEvent(Container.java:2041)
        at java.awt.Component.dispatchEventImpl(Component.java:4630)
        at java.awt.Container.dispatchEventImpl(Container.java:2099)
        at java.awt.Component.dispatchEvent(Component.java:4460)
        at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4577)
        at java.awt.LightweightDispatcher.processMouseEvent(Container.java:4238)
        at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4168)
        at java.awt.Container.dispatchEventImpl(Container.java:2085)
        at java.awt.Component.dispatchEvent(Component.java:4460)
        at java.awt.EventQueue.dispatchEvent(EventQueue.java:599)
        at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:269)
        at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:184)
        at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:174)
        at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:169)
        at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:161)
        at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)
</code></pre>
","java, parsing, stanford-nlp","<p>You may get a better/quicker response from their mail list: parser-user@lists.stanford.edu.</p>

<p>You can search their <a href=""https://mailman.stanford.edu/pipermail/parser-user/"" rel=""nofollow"">mail list archive</a> as well.</p>
",1,0,739,2010-09-24 04:56:47,https://stackoverflow.com/questions/3784288/stanford-parser-typed-dependencies
Typed dependencies for the Stanford parser,"<p>The typed dependencies given by <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">Stanford parser online</a> and the dependencies generated by the sourcecode given aren't same.</p>

<p>The versions of source codes available in Stanford website does not generate abbrev, possessive and poss tags.</p>

<p>Which version of Stanford parser to use for generating these tags?</p>
","java, parsing, stanford-nlp","<p>They are the same.  The answer was that you had to invoke the parser in a way that would cause tokenization of the input.  Some examples are:</p>

<pre><code>LexicalizedParser lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
String sent = ""This is one last test!"";
lp.apply(sent).pennPrint();
</code></pre>

<p>or in a more complex scenario:</p>

<pre><code>TreebankLanguagePack tlp = lp.getOp().langpack();
Tokenizer&lt;? extends HasWord&gt; toke = tlp.getTokenizerFactory().getTokenizer(new StringReader(sent));
List&lt;? extends HasWord&gt; sentence = toke.tokenize();
lp.apply(sentence);
</code></pre>
",4,1,2489,2010-09-26 09:26:54,https://stackoverflow.com/questions/3797203/typed-dependencies-for-the-stanford-parser
Illegal argument exception with Stanford Parser,"<p>I tried to parse a sentence using Stanford parser, but I get the exception. The input file, code and exception are specified below.</p>

<p>I think the problem is because the penn tree in input file doesn't handle punctuation. How do I generate a penn tree that handles punctuation too?</p>

<p>Input file</p>

<pre><code>(ROOT
  (S
    (NP (DT A) (NN doctor) (NN investigation) (NN system) (NN (DIS)))
    (VP (VBZ is)
      (NP
        (NP (DT a) (NN part))
        (PP (IN of)
          (NP (DT a) (NN hospital) (NN information) (NN system) (NN (HIS).)))))))
</code></pre>

<p>code</p>

<pre><code>            String str=""-collapsed -treeFile temp.txt"";
            String ar[]=str.split("" "");
            edu.stanford.nlp.trees.EnglishGrammaticalStructure.main(ar);

             try {
                FileOutputStream fw = new FileOutputStream(""k.txt"");
                PrintStream out = new PrintStream(fw);
                System.setOut(out);



            } catch (Exception e) {
                System.out.print(e);
            }
</code></pre>

<p>Exception raised :</p>

<pre><code>Head is null: NN-37
Exception in thread ""main"" java.lang.IllegalArgumentException: governor or dependent cannot be null
        at edu.stanford.nlp.trees.UnnamedDependency.&lt;init&gt;(UnnamedDependency.java:105)
        at edu.stanford.nlp.trees.TreeGraphNode.dependencies(TreeGraphNode.java:519)
        at edu.stanford.nlp.trees.Tree.dependencies(Tree.java:1090)
        at edu.stanford.nlp.trees.GrammaticalStructure.&lt;init&gt;(GrammaticalStructure.java:71)
        at edu.stanford.nlp.trees.EnglishGrammaticalStructure.&lt;init&gt;(EnglishGrammaticalStructure.java:115)
        at edu.stanford.nlp.trees.EnglishGrammaticalStructure.&lt;init&gt;(EnglishGrammaticalStructure.java:89)
        at edu.stanford.nlp.trees.EnglishGrammaticalStructure.&lt;init&gt;(EnglishGrammaticalStructure.java:61)
        at edu.stanford.nlp.trees.EnglishGrammaticalStructure.&lt;init&gt;(EnglishGrammaticalStructure.java:53)
</code></pre>
","java, stanford-nlp","<p>Unhelpful error message, but this is because the input tree is ill-formed: there is that stray period towards the end.  The tree should be a well-formed s-expression.</p>
",0,0,456,2010-09-28 07:59:54,https://stackoverflow.com/questions/3810764/illegal-argument-exception-with-stanford-parser
opennlp vs stanford nlptools vs berkeley,"<p>Hi the aim is to parse a sizeable corpus like wikipedia to generate the most probable parse tree,and named entity recognition. Which is the best library to achieve this in terms of performance and accuracy?  Has anyone used more than one of the above libraries?</p>
","parsing, nlp, stanford-nlp, opennlp","<p>I'm currently using Enju HPSG parser  which seems to be better than the others.</p>
",1,4,4907,2010-10-01 03:29:59,https://stackoverflow.com/questions/3836369/opennlp-vs-stanford-nlptools-vs-berkeley
finding noun and verb in stanford parser,"<p>I need to find whether a word is verb or noun or it is both</p>

<p>For example, the word is ""search"" it can be both noun and a verb but stanford parser gives NN tag to  it..</p>

<p>is there any way that stanford parser will give that ""search"" is both noun and verb?</p>

<p>code that i use now</p>

<pre><code>public static String Lemmatize(String word) {
    WordTag w = new WordTag(word);
    w.setTag(POSTagWord(word));
    Morphology m = new Morphology();
    WordLemmaTag wT = m.lemmatize(w);

    return wT.lemma();
}
</code></pre>

<p>or should i use any other software to do it? please suggest me 
thanks in advance</p>
","java, nlp, stanford-nlp","<p>The Stanford Parser guesses the part-of-speech tag of a word based on context statistics. You should really pass in a complete sentence to determine whether, in that sentence, ""search"" is a noun or a verb.</p>

<p>You don't need a full parser just to get part-of-speech tags. The <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""noreferrer"">Stanford POS Tagger</a> is enough; it also includes the <code>Morphology</code> class, but it too takes context into account.</p>

<p>If you want <em>all</em> part-of-speech tags that an English word can take on, without giving context, then <a href=""http://wordnet.princeton.edu/"" rel=""noreferrer"">WordNet</a> is probably a better choice. It has several Java interfaces, including <a href=""http://sourceforge.net/apps/mediawiki/jwordnet/index.php?title=Main_Page"" rel=""noreferrer"">JWNL</a> and <a href=""http://projects.csail.mit.edu/jwi/"" rel=""noreferrer"">JWI</a>.</p>
",8,6,10184,2010-10-04 11:44:33,https://stackoverflow.com/questions/3854900/finding-noun-and-verb-in-stanford-parser
What should i use to crawl many news articles?,"<p>I've a project of natural language processing but for that i need to crawl many web articles from some sources like Yahoo news, Google news or blogs...</p>

<p>I'm a java developper (so i'd rather use java tools). I guess i can parse each source website on my own and extract the articles with HttpClient / XPath but i'm a bit lazy :) is there a way so that i won't have to make a parser per source?</p>

<p>(I'm not only interested by new articles but articles from 2000 to now too)</p>
","java, nlp, web-crawler, stanford-nlp","<p>The hardest part of NLP is getting data you can use.  Everything else is just math.</p>

<p>It may be hard to find a large collection of news articles other than on each news source's website because of all the copyright issues involved.  If you don't need recent news, your best bet is probably to look at the Linguistic Data Consortium's <a href=""http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2009T13"" rel=""nofollow noreferrer"">English Gigaword corpus</a>; if you are at a university, there may already be an existing relationship for you to use the data for free.</p>

<p>If you need to actually crawl and parse websites, for now you'll probably find you have to write specific parsers for the various news websites to make sure you get the right text.  However, once more websites start using HTML5, it will be easier to pull out the relevant text through the use of the <a href=""http://www.quackit.com/html_5/tags/html_article_tag.cfm"" rel=""nofollow noreferrer"">article tag</a>.  </p>

<p>To do the actual crawling, <a href=""https://stackoverflow.com/questions/2495289/what-is-a-good-java-web-crawler-library"">this previous question</a> can point you in some useful directions.</p>
",4,2,2554,2010-11-29 00:28:11,https://stackoverflow.com/questions/4299987/what-should-i-use-to-crawl-many-news-articles
Is NER necessary for Coreference resolution?,"<p>... or is gender information enough?
More specifically, I'm interested in knowing if I can reduce the number of models loaded by the Stanford Core NLP to extract coreferences. I am not interested in actual named entity recognition.</p>

<p>Thank you</p>
","nlp, stanford-nlp, named-entity-recognition","<p>According to the EMNLP paper that describes the coref system packaged with Stanford CoreNLP, named entities tags are just used in the following coref annotation passes: <em>precise constructs</em>, <em>relaxed head matching</em>, and <em>pronouns</em> <a href=""http://cs.stanford.edu/people/nc/pubs/emnlp2010-sieve-coref.pdf"">(Raghunathan et al. 2010)</a>. </p>

<p>You can specify what passes to use with the <strong>dcoref.sievePasses</strong> configuration property. If you want coreference but you don't want to do NER, you should be able to just run the pipeline without NER and specify that the coref system should only use the annotation passes that don't require NER labels. </p>

<p>However, the resulting coref annotations will take a hit on <a href=""http://en.wikipedia.org/wiki/Recall_%28information_retrieval%29"">recall</a>. So, you might want to do some experiments to determine whether the degraded quality of the annotations is problem for whatever your are using them for downstream.</p>
",5,4,1511,2010-12-15 19:33:08,https://stackoverflow.com/questions/4454029/is-ner-necessary-for-coreference-resolution
Identifying Cataphora and Anaphora using the Stanford Parser,"<p>Can the Stanford Parser find instances of cataphora and anaphora in a given set of sentences?</p>
<p>Are there any alternative open-source (or proprietary) software packages that are capable of coreference resolution?</p>
","nlp, stanford-nlp, jnlp, coreference-resolution","<p>The Stanford Parser can't do this, but the <a href=""http://nlp.stanford.edu/software/dcoref.shtml"" rel=""nofollow"">coreference resolution system</a> packaged in Stanford's <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">CoreNLP</a> can.</p>
",2,0,710,2011-01-13 01:40:48,https://stackoverflow.com/questions/4675991/identifying-cataphora-and-anaphora-using-the-stanford-parser
out of heap space memory error,"<p>I am trying to run the coreNLP package with the following program</p>

<pre><code>package corenlp;
import edu.stanford.nlp.pipeline.*;
import java.io.IOException;
/**
 *
 * @author Karthi
 */
public class Main {

    /**
     * @param args the command line arguments
     */
    public static void main(String[] args) throws IOException, ClassNotFoundException {
        // TODO code application liogic here
        String str=""-cp stanford-corenlp-2010-11-12.jar:stanford-corenlp-models-2010-11-06.jar:xom-1.2.6.jar:jgrapht-0.7.3.jar -Xms3g edu.stanford.nlp.pipeline.StanfordCoreNLP [ -props &lt;Main&gt; ] -file &lt;input.txt&gt;"";
        args=str.split("" "");
        StanfordCoreNLP scn=new StanfordCoreNLP();
        scn.main(args);
    }

}
</code></pre>

<p>I am not sure if the code itself is correct, but am getting the following error</p>

<pre><code>    Searching for resource: StanfordCoreNLP.properties
Searching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties
Loading POS Model [edu/stanford/nlp/models/pos-tagger/wsj3t0-18-left3words/left3words-distsim-wsj-0-18.tagger] ... Loading default properties from trained tagger edu/stanford/nlp/models/pos-tagger/wsj3t0-18-left3words/left3words-distsim-wsj-0-18.tagger
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/wsj3t0-18-left3words/left3words-distsim-wsj-0-18.tagger ... Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:704)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:649)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:268)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:228)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:57)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:44)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:441)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:434)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:309)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:347)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:337)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:329)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:319)
        at corenlp.Main.main(Main.java:22)
Java Result: 1
</code></pre>

<p>I tried giving these values in VM options in netbeans, but for each value i am getting error</p>

<p>-Xms3g</p>

<pre><code>run:
Error occurred during initialization of VM
Incompatible initial and maximum heap sizes specified
Java Result: 1
BUILD SUCCESSFUL (total time: 0 seconds)
</code></pre>

<p>-Xmx3g</p>

<pre><code>run:
Error occurred during initialization of VM
Could not create the Java virtual machine.
Could not reserve enough space for object heap
Java Result: 1
BUILD SUCCESSFUL (total time: 0 seconds)
</code></pre>

<p>-Xms3g -Xmx4g</p>

<pre><code>run:
Could not create the Java virtual machine.
Invalid maximum heap size: -Xmx4g
The specified size exceeds the maximum representable size.
Java Result: 1
BUILD SUCCESSFUL (total time: 0 seconds)
</code></pre>
","java, netbeans, nlp, stanford-nlp","<p>Which OS are you running this on? Is it a 64 bit system? If not, then you are pretty much restricted when it comes to how much heap you can allocate to a single Java process. Try running with <code>-Xms1024M -Xmx1024M</code> and see if it solves your issue.</p>
",1,0,4725,2011-01-18 07:40:25,https://stackoverflow.com/questions/4721586/out-of-heap-space-memory-error
Stanford Parser questions,"<p>I am writing a project that works with NLP (natural language parser). I am using the stanford parser.</p>

<p>I create a thread pool that takes sentences and run the parser with them.
When I create one thread its all works fine, but when I create more, I get errors.
The ""test"" procedure is finding words that have some connections.
If I do an synchronized its supposed to work like one thread but still I get errors.
My problem is that I have errors on this code:</p>

<pre><code>public synchronized String test(String s,LexicalizedParser lp )
{

    if (s.isEmpty()) return """";
    if (s.length()&gt;80) return """";
    System.out.println(s);
    String[] sent = s.split("" "");
 Tree parse = (Tree) lp.apply(Arrays.asList(sent));
TreebankLanguagePack tlp = new PennTreebankLanguagePack();
GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
Collection tdl = gs.typedDependenciesCollapsed();
List list = new ArrayList(tdl);


//for (int i=0;i&lt;list.size();i++)
//System.out.println(list.get(1).toString());

//remove scops and numbers like sbj(screen-4,good-6)-&gt;screen good

 Pattern p = Pattern.compile("".*\\((.*?)\\-\\d+,(.*?)\\-\\d+\\).*"");

       if (list.size()&gt;2){
    // Split input with the pattern
        Matcher m = p.matcher(list.get(1).toString());
        //check if the result have more than  1 groups
       if (m.find()&amp;&amp; m.groupCount()&gt;1){
           if (m.groupCount()&gt;1)
           {
               System.out.println(list);
 return  m.group(1)+m.group(2);
    }}
}
        return """";

}
</code></pre>

<p>the errors that I have are:</p>

<blockquote>
  <p>at blogsOpinions.ParserText.(ParserText.java:47)
    at blogsOpinions.ThreadPoolTest$1.run(ThreadPoolTest.java:50)
    at blogsOpinions.ThreadPool$PooledThread.run(ThreadPoolTest.java:196)
  Recovering using fall through
  strategy: will construct an (X ...)
  tree. Exception in thread
  ""PooledThread-21""
  java.lang.ClassCastException:
  java.lang.String cannot be cast to
  edu.stanford.nlp.ling.HasWord</p>
  
  <p>at
  edu.stanford.nlp.parser.lexparser.LexicalizedParser.apply(LexicalizedParser.java:289)
      at blogsOpinions.ParserText.test(ParserText.java:174)
      at blogsOpinions.ParserText.insertDb(ParserText.java:76)
      at blogsOpinions.ParserText.(ParserText.java:47)
      at blogsOpinions.ThreadPoolTest$1.run(ThreadPoolTest.java:50)
      at blogsOpinions.ThreadPool$PooledThread.run(ThreadPoolTest.java:196)</p>
</blockquote>

<p>and how can i get the discription of the subject like the screen is very good, and I want to get screen good from the list the I get and not like <code>list.get(1)</code>.</p>
","java, nlp, stanford-nlp","<p>You can't call <code>LexicalizedParser.parse</code> on a <code>List</code> of <code>String</code>s; it expects a list of <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/HasWord.html"" rel=""nofollow""><code>HasWord</code></a> objects. It's much easier to call the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html#apply%28java.lang.Object%29"" rel=""nofollow""><code>apply</code></a> method on your input string. This will also run a proper tokenizer on your input (instead of your simple <code>split</code> on spaces).</p>

<p>To get relations such as subjectness out of the returned <code>Tree</code>, call its <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html#dependencies%28%29"" rel=""nofollow""><code>dependencies</code></a> member.</p>
",4,0,2617,2011-01-22 16:31:11,https://stackoverflow.com/questions/4769040/stanford-parser-questions
Getting corefrences with Standard corenlp package,"<p>I'm trying to get coreferences in a text. I'm new to the corenlp package. I tried the code below, which doesn't work, but I'm open to other methods as well.</p>

<pre><code>/*
 * To change this template, choose Tools | Templates
 * and open the template in the editor.
 */

package corenlp;
import edu.stanford.nlp.ling.CoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.CorefGraphAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.semgraph.SemanticGraph;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.IntTuple;
import edu.stanford.nlp.util.Pair;
import edu.stanford.nlp.util.Timing;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import java.util.Properties;
/**
 *
 * @author Karthi
 */
public class Main {


        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
    Properties props = new Properties();
    FileInputStream in = new FileInputStream(""Main.properties"");

    props.load(in);
    in.close();
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = ""The doctor can consult with other doctors about this patient. If that is the case, the name of the doctor and the names of the consultants have to be maintained. Otherwise, only the name of the doctor is kept. ""; // Add your text here!

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);
    System.out.println(document);
    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = (List&lt;CoreMap&gt;) document.get(SentencesAnnotation.class);
    System.out.println(sentences);
    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);
System.out.println(tree);
      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
      System.out.println(dependencies);
    }

    // this is the coreference link graph
    // each link stores an arc in the graph; the first element in the Pair is the source, the second is the target
    // each node is stored as &lt;sentence id, token id&gt;. Both offsets start at 1!
    List&lt;Pair&lt;IntTuple, IntTuple&gt;&gt; graph = document.get(CorefGraphAnnotation.class);
    System.out.println(graph);

    }

}
</code></pre>

<p>This is the error I get:</p>

<pre class=""lang-none prettyprint-override""><code>Loading POS Model [// For POS model] ... Loading default properties from trained tagger // For POS model
Error: No such trained tagger config file found.
java.io.FileNotFoundException: \\ For POS model (The specified path is invalid)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:106)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:66)
        at edu.stanford.nlp.tagger.maxent.TaggerConfig.getTaggerDataInputStream(TaggerConfig.java:741)
        at edu.stanford.nlp.tagger.maxent.TaggerConfig.&lt;init&gt;(TaggerConfig.java:178)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:228)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:57)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:44)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:441)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:434)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:309)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:347)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:337)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:329)
        at corenlp.Main.main(Main.java:66)
Exception in thread ""main"" java.lang.RuntimeException: java.io.FileNotFoundException: \\ For POS model (The specified path is invalid)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:443)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:434)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:309)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:347)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:337)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:329)
        at corenlp.Main.main(Main.java:66)
Caused by: java.io.FileNotFoundException: \\ For POS model (The specified path is invalid)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:106)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:66)
        at edu.stanford.nlp.tagger.maxent.TaggerConfig.getTaggerDataInputStream(TaggerConfig.java:741)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:643)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:268)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:228)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:57)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:44)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:441)
        ... 7 more
Java Result: 1
</code></pre>
","java, nlp, stanford-nlp","<p>This error simply means the program is not finding the data models it needs to run.  They need to be on your classpath.  If you're in the distribution directory, you can do this with a command like:</p>

<pre><code>java -cp stanford-corenlp-2010-11-12.jar:stanford-corenlp-models-2010-11-06.jar:xom.jar:jgrapht.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</code></pre>

<p>The second jar contains the models. If you're using Windows, replace the colons above with semicolons.</p>
",2,3,2948,2011-02-19 05:11:48,https://stackoverflow.com/questions/5049018/getting-corefrences-with-standard-corenlp-package
Cannot import edu.stanford.nlp  - Stanford parser with jython problem,"<p>Can someone help me out with the stanford parser from <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/lex-parser.shtml</a>?</p>

<p>I've only downloaded and unzipped the parser. 
I've also installed the jython fully but i cannot parse a sentence, it seems like i've installed some modules or something.
<a href=""http://wiki.python.org/jython/InstallationInstructions"" rel=""nofollow"">http://wiki.python.org/jython/InstallationInstructions</a></p>

<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.path.append('~/standford-parser-2010-11-30/stanford-parser-2011-11-30.jar')
&gt;&gt;&gt; from java.io import CharArrayReader
&gt;&gt;&gt; from edu.stanford.nlp import *
Traceback (innermost last):
  File ""&lt;console&gt;"", line 1, in ?
ImportError: no module named edu
</code></pre>

<p>Is there more installation procedures other than unzipping it and importing it in jython?</p>
","java, python, nlp, jython, stanford-nlp","<p>You have a typo in your <code>sys.append</code> statement. The filename says <code>2011</code> when it should be <code>2010</code>:</p>

<pre><code>import sys
sys.path.append('./stanford-parser-2010-11-30/stanford-parser-2010-11-30.jar')
from edu.stanford.nlp import *
print fsm
&lt;java package edu.stanford.nlp.fsm 1&gt;
</code></pre>
",1,1,1284,2011-03-09 03:47:44,https://stackoverflow.com/questions/5241079/cannot-import-edu-stanford-nlp-stanford-parser-with-jython-problem
Stanford POS tagger in Java usage,"<pre><code>Mar 9, 2011 1:22:06 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: � (U+FFFD, decimal: 65533)
Mar 9, 2011 1:22:06 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: � (U+FFFD, decimal: 65533)
Mar 9, 2011 1:22:06 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: � (U+FFFD, decimal: 65533)
Mar 9, 2011 1:22:06 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: � (U+FFFD, decimal: 65533)
Mar 9, 2011 1:22:06 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: � (U+FFFD, decimal: 65533)
Mar 9, 2011 1:22:06 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: � (U+FFFD, decimal: 65533)
Mar 9, 2011 1:22:06 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: � (U+FFFD, decimal: 65533)
</code></pre>

<p>These are the errors that I'm getting when I want to assign POS tags to sentences. I read  sentences from a file. Initially (for few sentences) I'm not getting this error (i.e untokenizable), but after reading some sentences this error arises. I use v2.0 (i.e. 2009) of POS tagger and model is <code>left3words</code>. </p>
","java, stanford-nlp, pos-tagger","<p>I agree with Yuval -- a character encoding problem, but the commonest case is actually when the file is in a single byte encoding such as ISO-8859-1 while the tagger is trying to read it in UTF-8.  See the discussion of U+FFFD on <a href=""http://en.wikipedia.org/wiki/Specials_(Unicode_block)"" rel=""noreferrer"">Wikipedia</a>.</p>
",8,11,6409,2011-03-09 08:02:21,https://stackoverflow.com/questions/5242890/stanford-pos-tagger-in-java-usage
stanford parse bash script error - linux bash,"<p>Can someone help me check my bash script? i'm trying to feed a directory of .txt files to the stanford parser (http://nlp.stanford.edu/software/pos-tagger-faq.shtml) but i can't get it to work. i'm working on ubuntu 10.10</p>

<p>the loop is working and reading the right files with:</p>

<pre><code>#!/bin/bash -x
cd $HOME/path/to
for file in 'dir -d *'
do
#       $HOME/chinesesegmenter-2006-05-11/segment.sh ctb $file UTF-8
        echo $file
done
</code></pre>

<p>but with</p>

<pre><code>#!/bin/bash -x
cd $HOME/yoursing/sentseg_zh
for file in 'dir -d *'
do
#       echo $file
        $HOME/chinesesegmenter-2006-05-11/segment.sh ctb $file UTF-8
done
</code></pre>

<p>i'm getting this error:</p>

<pre><code>alvas@ikoma:~/chinesesegmenter-2006-05-11$ bash segchi.sh
Standard: CTB
File: dir
Encoding: -d
-------------------------------
Exception in thread ""main"" java.lang.NoClassDefFoundError: edu/stanford/nlp/ie/crf/CRFClassifier
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.ie.crf.CRFClassifier
    at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
Could not find the main class: edu.stanford.nlp.ie.crf.CRFClassifier. Program will exit.
</code></pre>

<p>the following command works:</p>

<pre><code>~/chinesesegmenter-2006-05-11/segment.sh ctb ~/path/to/input.txt UTF-8
</code></pre>

<p>and output this</p>

<pre><code>alvas@ikoma:~/chinesesegmenter-2006-05-11$ ./segment.sh ctb ~/path/to/input.txt UTF-8
Standard: CTB
File: /home/alvas/path/to/input.txt
Encoding: UTF-8
-------------------------------
Loading classifier from data/ctb.gz...done [1.5 sec].
Using ChineseSegmenterFeatureFactory
Reading data using CTBSegDocumentReader
Sequence tagging 7 documents
如果 您 在 新加坡 只 能 前往 一 间 俱乐部 ， 祖卡 酒吧 必然 是 您 的 不二 选择 。
</code></pre>

<p>作为 或许 是 新加坡 唯一 一 家 国际 知名 的 夜店 ， 祖卡 既 是 一 个 公共 机构 ， 也 是 狮城 年轻人 选择 进行 成人 礼等 庆祝 的 不二场所 。</p>
","java, bash, nlp, stanford-nlp","<p>You could try: </p>

<pre><code>for file in *
do
    $HOME/segment.sh ctb ""$file"" UTF-8
done
</code></pre>

<p>So there were a couple of things to correct: </p>

<ul>
<li>Don't use <code>:</code> after the for statement, use <code>;</code> or a newline</li>
<li>Put quotation marks around the <code>""$file""</code> object to allow whitespaces in file name</li>
<li>If you want to use a command where you put <code>'dir -d *'</code> you should use <code>$(dir -d *)</code> or angle quation marks instead ``</li>
</ul>
",2,1,908,2011-03-11 08:26:10,https://stackoverflow.com/questions/5270571/stanford-parse-bash-script-error-linux-bash
reading # char in python,"<p>can someone help with me reading ""#"" char in python? i can't seem to get the file. because this is an output from the stanford postagger, is there any scripts available to convert the stanford postagger <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tagger.shtml</a> file to cwb. <a href=""http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html"" rel=""nofollow"">http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html</a></p>

<p>so this is the utf-8 txt file that i'm trying to read:</p>

<pre><code> 如果#CS 您#PN 在#P 新加坡#NR 只#AD 能#VV 前往#VV 一#CD 间#M 俱乐部#NN ，#PU 祖卡#NN 酒吧#NN 必然#AD 是#VC 您#PN 的#DEG 不二#JJ 选择#NN 。#PU
    作为#P 或许#AD 是#VC 新加坡#NR 唯一#JJ 一#CD 家#M 国际#NN 知名#VA 的#DEC 夜店#NN ，#PU 祖卡#NN 既#CC 是#VC 一#CD 个#M 公共#JJ 机构#NN ，#PU
</code></pre>

<p>So with this code i'm not readin the # char in the utf-8 txt files:</p>

<pre><code>#!/usr/bin/python # -*- coding: utf-8 -*-

'''
stanford POS tagger to CWB format
'''

import codecs
import nltk
import os, sys, re, glob

reload(sys)
sys.setdefaultencoding('utf-8')

cwd = './path/to/file.txt' #os.getcwd()

for infile in glob.glob(os.path.join(cwd, 'zouk.txt')):
        print infile
        (PATH, FILENAME) = os.path.split(infile)
        reader = codecs.open(infile, 'r', 'utf-8')
        for line in reader:
                for word in line:
                        if word == '\#':
                                print 'hex is here'
</code></pre>
","python, utf-8, nlp, stanford-nlp","<pre><code>if word == '\#':
</code></pre>

<p>This probably doesn't do what you think it does.  (Hint: <code>print ""\#""</code>)</p>
",1,1,340,2011-03-12 03:49:57,https://stackoverflow.com/questions/5280421/reading-char-in-python
CWB encoding Corpus,"<p>According to the Corpus Work Bench, to encode a corpus i need to use the cwb-encode perl script</p>

<p>""encode the corpus, i.e. convert the verticalized text to CWB binary format with the cwb-encode tool. Note that the command below has to be entered on a single line."" <a href=""http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html"" rel=""nofollow"">http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html</a></p>

<pre><code>$ cwb-encode -d /corpora/data/example -f example.vrt -R /usr/local/share/cwb/registry/example -P pos -S s
</code></pre>

<p>when i tried it, it says the file is missing but i'm sure the file is in $HOME/corpora/data/example, the error was</p>

<pre><code>$ cwb-encode -d /corpora/data/example -f example.vrt -R /usr/local/share/cwb/registry/example -P pos -S s
example.vrt: No such file or directory
Can't open input file example.vrt!
</code></pre>

<p>can anyone figure out why?</p>
","nlp, stanford-nlp, corpus","<p>How about giving full path to <code>example.vrt</code>:</p>

<pre><code>cwb-encode -d /corpora/data/example -f /corpora/data/example/example.vrt -R /usr/local/share/cwb/registry/example -P pos -S s
</code></pre>
",1,1,507,2011-04-08 02:11:06,https://stackoverflow.com/questions/5589684/cwb-encoding-corpus
Am I using the StanfordParser wrapper for ruby properly? It&#39;s returning incorrect results,"<p>I've installed the latest version of the stanfordparser and the ruby wrapper library for it. When trying to test it with a simple example from the website:</p>

<p>vi test.rb:</p>

<pre><code>require ""stanfordparser""

preproc =
StanfordParser::DocumentPreprocessor.new
puts
preproc.getSentencesFromString(""This
is a sentence.  So is this."")
</code></pre>

<p>ruby -rubygems test.rb </p>

<pre><code>This
is
a
sentence
.
So
is
this
.
</code></pre>

<p>This is a sanity check really - am I doing something wrong, or is this a bug in the parser or wrapper?</p>
","ruby, stanford-nlp","<p>You might be confused about how <code>puts</code> is formatting the output. Try this:</p>

<pre><code>x = preproc.getSentencesFromString(""This is a sentence. So is this."")
puts x.inspect
</code></pre>

<p>to make sure that you're getting what you're supposed to be getting.</p>
",1,1,354,2011-04-12 21:02:17,https://stackoverflow.com/questions/5641599/am-i-using-the-stanfordparser-wrapper-for-ruby-properly-its-returning-incorrec
Stanford NER - Extract Multi word entities,"<p>How can I tag collocations in Stanford NER? Currently it tags <code>Federal Reserve Bank of New York</code> as   </p>

<pre><code>&lt;wi num=""11"" entity=""ORGANIZATION""&gt;Federal&lt;/wi&gt; &lt;wi num=""12"" entity=""ORGANIZATION""&gt;Reserve&lt;/wi&gt; &lt;wi num=""13"" entity=""ORGANIZATION""&gt;Bank&lt;/wi&gt; &lt;wi num=""14"" entity=""ORGANIZATION""&gt;of&lt;/wi&gt; &lt;wi num=""15"" entity=""ORGANIZATION""&gt;New&lt;/wi&gt; &lt;wi num=""16"" entity=""ORGANIZATION""&gt;York&lt;/wi&gt;
</code></pre>

<p>I want it to be recognized as </p>

<pre><code>&lt;wi num=""11"" entity=""ORGANIZATION""&gt;Federal Reserve Bank of New York&lt;/wi&gt;
</code></pre>

<p>Is this possible?</p>
","java, stanford-nlp, named-entity-recognition","<p>Something similar is, yes.  If you give the flag </p>

<pre><code>-outputFormat inlineXML
</code></pre>

<p>then you'll get:</p>

<pre><code>&lt;ORGANIZATION&gt;Federal Reserve Bank of New York&lt;/ORGANIZATION&gt;
</code></pre>

<p>(Note that this isn't really changing how Stanford NER works but just the formatting of output.  If you don't like any of the provided output formats, it is fairly simple to write your own.)</p>
",2,1,1716,2011-04-18 07:54:27,https://stackoverflow.com/questions/5700002/stanford-ner-extract-multi-word-entities
IncompatibleClassChangeError while running Stanford NER and Stanford POS tagger,"<p>I am trying to use Stanford NER and Stanford POS tagger in one application. I am getting <code>IncompatibleClassChangeError</code> when I try to run POS tagger method. </p>

<p>I have jar files of both NER and POS tagger in the class path. If I remove jar of NER from my classpath then this error is not coming. I guess there are some classes common in NER jar and POS jar and java is not able to determine which class to be used at runtime.</p>

<p>Following is the stacktrace :</p>

<pre><code>java.lang.IncompatibleClassChangeError: Implementing class
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(Unknown Source)
    at java.security.SecureClassLoader.defineClass(Unknown Source)
    at java.net.URLClassLoader.defineClass(Unknown Source)
    at java.net.URLClassLoader.access$000(Unknown Source)
    at java.net.URLClassLoader$1.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClassInternal(Unknown Source)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.init(MaxentTagger.java:407)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:699)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:673)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:280)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:260)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1305)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1499)
    at com.tcs.srl.stanford.POSWrapper.executePOSTagger(POSWrapper.java:39)
    at com.tcs.srl.stanford.test.POSWrapperTester.ExecutePOSTagger(POSWrapperTester.java:19)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
</code></pre>

<p>I have no clue why this error is coming.</p>
","java, exception, stanford-nlp","<p>I think you are close but not quite right in your diagnosis. Java will always use the first instance of a class in your classpath. However, if you have a heirarchy of classloaders, and a child classloader tries to define a class that a parent has already defined you will get an error like this.</p>

<p>I'm not familiar with the Stanford NER or POS code. However, I can offer some advice.</p>

<p>If you are <em>convinced</em> the classes with the same names are identical in the two jars, then just combine the two jars into a single jar - this will remove any duplicates.</p>

<p>If you are worried the NER and POS code might be incompatible, I'd move the source code for the two into a single project, and recompile, carefully inspecting the duplicates.</p>

<p>The alternative is to take the source code from one project and move it into a different package so the class names stay the same, but the packages no longer clash. A good IDE should be able to do this fairly painlessly.</p>

<p>However, for issues like this mailing lists are often the best place to get advice: <a href=""http://www-nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">http://www-nlp.stanford.edu/software/CRF-NER.shtml</a>. I'm sure you won't be the only person to be using these pieces of software together. None of the solutions I mention above should be necessary, so getting advice from someone else who uses them is probably the best.</p>
",1,1,826,2011-05-13 08:09:48,https://stackoverflow.com/questions/5989055/incompatibleclasschangeerror-while-running-stanford-ner-and-stanford-pos-tagger
Link to list which contains frequnecy ranks of all English language words,"<p>Does any body knows link to the list which contains frequency rank of all english language words.
Some sixth months back, I found one list on 'wiki' which contains this list, but unfortunately I did not bookmark it and now I am unable to find the same link or any better link.
If anybody has this link or something better, please post it. </p>
","nlp, stanford-nlp, word-frequency","<p>Some links would include:</p>

<ul>
<li><a href=""http://en.wiktionary.org/wiki/Wiktionary%3aFrequency_lists"" rel=""nofollow"">Wiktionary</a></li>
<li><a href=""http://www.wordfrequency.info/"" rel=""nofollow"">Word frequency info</a></li>
<li><a href=""http://answers.google.com/answers/threadview?id=470042"" rel=""nofollow"">Google Answers discussion of English word frequencies pointing to British National Corpus</a></li>
<li><a href=""http://ucrel.lancs.ac.uk/bncfreq/"" rel=""nofollow"">A different pointer to British National Corpus and how to use it</a></li>
</ul>
",2,2,112,2011-05-16 11:11:22,https://stackoverflow.com/questions/6016513/link-to-list-which-contains-frequnecy-ranks-of-all-english-language-words
corenlp package of stanford&#39;s StaggerDemo,"<p>I wanna make a program automatically tagger the text in a directory. Here's my first step.
I made a little change to TaggerDemo.java. But it is not working properly as expected.</p>

<pre><code>import java.io.BufferedReader;
import java.io.FileReader;
import java.util.ArrayList;
import java.util.List;

import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.Sentence;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;

class auto{

  public static void main (String[] args) throws Exception{

    MaxentTagger tagger = new MaxentTagger(""models/left3words-wsj-0-18.tagger"");
    @SuppressWarnings(""unchecked"")
    List&lt;List&lt;HasWord&gt;&gt; sentences = tagger.tokenizeText(new BufferedReader(new FileReader(args[0])));
    for (List&lt;HasWord&gt; sentence : sentences) {
      ArrayList&lt;TaggedWord&gt; tSentence = tagger.tagSentence(sentence);
      System.out.println(Sentence.listToString(tSentence, false));
    }
  }

}
</code></pre>

<p>This is the error i got.</p>

<pre><code>Loading default properties from trained tagger models/left3words-wsj-0-18.tagger
Reading POS tagger model from models/left3words-wsj-0-18.tagger ... done [2.9 sec].
Exception in thread ""main"" java.io.FileNotFoundException: sample-input.txt (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:106)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:66)
    at java.io.FileReader.&lt;init&gt;(FileReader.java:41)
    at auto.main(auto.java:17)
</code></pre>

<p>Why it says file not found?</p>

<p>When i tried to compile it under terminal, it says edu.stanford.nlp.ling.* can not be imported...</p>

<p>Thanks a lot.</p>
","java, nlp, stanford-nlp","<p>It seems that you don't have the file <code>sample-input.txt</code> in the current directory.</p>

<p>To see your current directory, use:</p>

<pre><code>File f = new File(""."");
System.out.println(f.getAbsolutePath());
</code></pre>
",1,-2,470,2011-05-16 11:49:29,https://stackoverflow.com/questions/6016923/corenlp-package-of-stanfords-staggerdemo
using pipe with python,"<pre><code>import re
import subprocess

sub = subprocess.Popen(['/home/karthik/Downloads/stanford-parser-2011-06-   08/lexparser.csh'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr = subprocess.PIPE)

sub.stdin.write(""i am a fan of ac milan which is the best club in the world"")

relns = []

while(True):
    rel = sub.stdout.readline()
        m = re.search(""Sentence skipped"", rel)

    if m != None:
            print 'stop'
            sys.exit(0)

        if rel == '\n':
                break
        relns.append(rel) 

print relns

sub.terminate()
</code></pre>

<p>So i want to the stanford parser and using the lexparser.csh to parse this line of text . But when i run this piece of code i am a getting the output of the default of text. The actual text given in is not being parsed. So am i using pipes the right way ?And i've seen in a lot of examples - a '-' is used along with the command . Why is that being used ? Cos when i use that the script just stalls at sub.stdout.readline()</p>
","python, pipe, stanford-nlp","<p>You may need to call <code>flush()</code> on <code>sub.stdin</code> after writing.</p>
",1,0,714,2011-06-15 12:41:51,https://stackoverflow.com/questions/6357810/using-pipe-with-python
Extracting Arabic Proper Names from a text using Stanford-Parser,"<p>I am trying to extract Arabic proper names from a text using Stanford Parser.</p>

<p>for example if I have an input sentence:</p>

<pre><code>تكريم سعد الدين الشاذلى
</code></pre>

<p>using the Arabic Stanford parser, the tree diagram will be:</p>

<pre><code>(ROOT (NP (NN تكريم) (NP (NNP سعد) (DTNNP الدين) (NNP الشاذلى))))
</code></pre>

<p>I want to extract the proper name:</p>

<pre><code>سعد الدين الشاذلى
</code></pre>

<p>which have the sub-tree:</p>

<pre><code>(NP (NNP سعد) (DTNNP الدين) (NNP الشاذلى))
</code></pre>

<p>I have tried this: <a href=""https://stackoverflow.com/questions/6044354/extracting-all-nouns-adjectives-form-a-text-via-stanford-parser"">similar question</a></p>

<p>but there is some thing wrong in this line:</p>

<pre><code>List&lt;TaggedWord&gt; taggedWords = (Tree) lp.apply(str);
</code></pre>

<p>the error in putting a tree type in a list of taggedword
another thing that I didnot understand that where could i use the suggested  <code>taggedYield()</code>  function</p>

<p>Any Ideas, please?</p>
","text-parsing, arabic, stanford-nlp","<p>This is pretty basic Java with respect to the library, but what you want is:</p>

<pre><code>Tree tree = lp.apply(str);
List&lt;TaggedWord&gt; taggedWords = tree.taggedYield();
for (TaggedWord tw : taggedWords) {
  if (tw.tag().contains(""NNP"")) {
    System.err.println(tw.word());
  }
}    
</code></pre>
",2,1,636,2011-06-28 11:17:37,https://stackoverflow.com/questions/6505569/extracting-arabic-proper-names-from-a-text-using-stanford-parser
Stanford Core NLP - understanding coreference resolution,"<p>I'm having some trouble understanding the changes made to the coref resolver in the last version of the Stanford NLP tools.
As an example, below is a sentence and the corresponding CorefChainAnnotation:</p>

<pre><code>The atom is a basic unit of matter, it consists of a dense central nucleus surrounded by a cloud of negatively charged electrons.

{1=[1 1, 1 2], 5=[1 3], 7=[1 4], 9=[1 5]}
</code></pre>

<p>I am not sure I understand the meaning of these numbers. Looking at the source doesn't really help either.</p>

<p>Thank you</p>
","java, nlp, stanford-nlp","<p>The first number is a cluster id (representing tokens, which stand for the same entity), see source code of <code>SieveCoreferenceSystem#coref(Document)</code>. The pair numbers are outout of CorefChain#toString():</p>

<pre><code>public String toString(){
    return position.toString();
}
</code></pre>

<p>where position is a set of postion pairs of entity mentioning (to get them use <code>CorefChain.getCorefMentions()</code>). Here is an example of a complete code (in <a href=""http://groovy.codehaus.org/"" rel=""nofollow noreferrer"">groovy</a>), which shows how to get from positions to tokens: </p>

<pre><code>class Example {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        props.put(""dcoref.score"", true);
        pipeline = new StanfordCoreNLP(props);
        Annotation document = new Annotation(""The atom is a basic unit of matter, it   consists of a dense central nucleus surrounded by a cloud of negatively charged electrons."");

        pipeline.annotate(document);
        Map&lt;Integer, CorefChain&gt; graph = document.get(CorefChainAnnotation.class);

        println aText

        for(Map.Entry&lt;Integer, CorefChain&gt; entry : graph) {
          CorefChain c =   entry.getValue();                
          println ""ClusterId: "" + entry.getKey();
          CorefMention cm = c.getRepresentativeMention();
          println ""Representative Mention: "" + aText.subSequence(cm.startIndex, cm.endIndex);

          List&lt;CorefMention&gt; cms = c.getCorefMentions();
          println  ""Mentions:  "";
          cms.each { it -&gt; 
              print aText.subSequence(it.startIndex, it.endIndex) + ""|""; 
          }         
        }
    }
}
</code></pre>

<p>Output (I do not understand where 's' comes from):</p>

<pre><code>The atom is a basic unit of matter, it consists of a dense central nucleus surrounded by a cloud of negatively charged electrons.
ClusterId: 1
Representative Mention: he
Mentions: he|atom |s|
ClusterId: 6
Representative Mention:  basic unit 
Mentions:  basic unit |
ClusterId: 8
Representative Mention:  unit 
Mentions:  unit |
ClusterId: 10
Representative Mention: it 
Mentions: it |
</code></pre>
",9,16,10649,2011-07-04 13:32:20,https://stackoverflow.com/questions/6572207/stanford-core-nlp-understanding-coreference-resolution
trouble importing stanford pos tagger into nltk,"<p>This is probably a very trivial question. I am trying to use the stanford pos tagger through nltk given <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.tag.stanford.StanfordTagger-class.html"" rel=""nofollow"">here</a> The problem is that my nltk lib doesnt contain the stanford module. So I copied the same into the appropriate folder and compiled the same. Now when i try to run an example the module is getting detected but not the class inside the module. Can anyone tell me where I am going wrong?? Again this is probably very dumb.</p>

<pre><code>&gt;&gt;&gt; from nltk.tag import stanford 
&gt;&gt;&gt; st = StanfordTagger('bidirection-distsim-wsj-0-18.tagger')
</code></pre>

<p>I used py_compile to compile the stanford.py file. Am i missing something</p>
","python, nltk, stanford-nlp","<p>You are only importing <code>stanford</code>. In order to access <code>StanfordTagger</code> you need to use either:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import StanfordTagger
</code></pre>

<p>(assuming that `StanfordTagger is not further nested in a module) or access it by</p>

<pre><code>&gt;&gt;&gt; st = stanford.StanfordTagger('bidirection-distsim-wsj-0-18.tagger')
</code></pre>
",3,1,7259,2011-09-08 08:01:59,https://stackoverflow.com/questions/7344916/trouble-importing-stanford-pos-tagger-into-nltk
How do I split sentences?,"<p>So, I found and am currently using Stanford Parser and it works GREAT for splitting sentences. Most of our sentences are from AP so it works very well for that task.</p>

<p>Here's the problems:</p>

<ul>
<li>it eats a LOT of memory (600M a lot)</li>
<li>it really screws up the formatting of a body of text where I have to make a lot of edge cases for later on. (the document pre-processor API calls don't allow to specify ascii/utf8 quotes -- they immediately goto latex style, contractions get split into different words (obviously) and spurious spaces are put into different places)</li>
</ul>

<p>To this end, I've already written multiple patches to compensate for what I really shouldn't be having to do.</p>

<p>Basically it's at the point where it is just as much of a hindrance to use as the problem of splitting sentences to begin with.</p>

<p>What are my other options? Any other NLP type of frameworks out there that might help out?</p>

<p>My original problem is just being able to detection sentence edges with a high degree of probability.</p>
","java, parsing, stanford-nlp","<p>If you want to try sticking with the Stanford Tokenizer/Parser, look at the <a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""noreferrer"">documentation page for the tokenizer</a>. </p>

<p>If you just want to split sentences, you don't need to invoke the parser proper, and so you should be able to get away with a tiny amount of memory - a megabyte or two - by directly using DocumentPreprocessor.</p>

<p>While there is only limited customization of the tokenizer available, you <em>can</em> change the processing of quotes. You might want to try one of:</p>

<pre><code>unicodeQuotes=false,latexQuotes=false,asciiQuotes=false
unicodeQuotes=true
</code></pre>

<p>The first will mean no quote mapping of any kind, the second would change single or doubled ascii quotes (if any) into left and right quotes according to the best of its ability.</p>

<p>And while the tokenizer splits words in various ways to match Penn Treebank conventions, you should be able to construct precisely the original text from the tokens returned (see the various other fields in the CoreLabel). Otherwise it's a bug.</p>
",5,3,2667,2011-09-21 21:33:59,https://stackoverflow.com/questions/7506945/how-do-i-split-sentences
Add a language in the Stanford parser,"<p>I would like to use the Stanford parser in another language not already implemented.</p>

<p>I looked on the website but found nothing that could help me with that.</p>

<p>I guess what I have to do is ""just"" create a new languagePCFG.ser but to do that?</p>

<p>Also, if anyone knows if French and Spanish are supposed to be released?</p>
","parsing, nlp, stanford-nlp","<p>Several things are needed:</p>

<ul>
<li>You need a treebank (set of hand-parsed trees) from which the probabilities used in the parser are calculated</li>
<li>You need language-specific files (like xLanguagePack, xTreebankParserParams, which specify things about the language, treebank encoding, and parsing options</li>
<li>You then train the parser on the treebank to produce the grammar file (see makeSerialized.csh in the distribution)</li>
<li>You might need a language-specific tokenizer to divide text into tokens</li>
<li>If you want Stanford Dependencies output, then there is also a rule-based layer that defines the dependencies</li>
</ul>

<p>Starting in 2011, we did start distributing a French model with the Stanford Parser. And starting in 2015, we have begun distributing a Spanish model.</p>
",5,2,1588,2011-09-29 12:05:51,https://stackoverflow.com/questions/7597061/add-a-language-in-the-stanford-parser
Java query about finding a word in a sentence,"<p>I am using Stanford's NLP parser (http://nlp.stanford.edu/software/lex-parser.shtml) to split a block of text into sentences and then see which sentences contain a given word. </p>

<p>Here is my code so far:</p>

<pre><code>import java.io.FileReader;
import java.io.IOException;
import java.util.List;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.process.*;

public class TokenizerDemo {

    public static void main(String[] args) throws IOException {
        DocumentPreprocessor dp = new DocumentPreprocessor(args[0]);
        for (List sentence : dp) {
            for (Object word : sentence) {
                System.out.println(word);
                System.out.println(word.getClass().getName());
                if (word.equals(args[1])) {
                    System.out.println(""yes!\n"");
                }
            }
        }
    }
}
</code></pre>

<p>I run the code from the command line using ""java TokenizerDemo testfile.txt wall""</p>

<p>The contents of testfile.txt is:</p>

<pre><code>Humpty Dumpty sat on a wall. Humpty Dumpty had a great fall.
</code></pre>

<p>So I want the program to detect ""wall"" in the first sentence (""wall"" entered as the second argument on the command line). But the program doesn't detect ""wall"", because it never prints ""yes!"". The output of the program is:</p>

<pre><code>Humpty
edu.stanford.nlp.ling.Word
Dumpty
edu.stanford.nlp.ling.Word
sat
edu.stanford.nlp.ling.Word
on
edu.stanford.nlp.ling.Word
a
edu.stanford.nlp.ling.Word
wall
edu.stanford.nlp.ling.Word
.
edu.stanford.nlp.ling.Word
Humpty
edu.stanford.nlp.ling.Word
Dumpty
edu.stanford.nlp.ling.Word
had
edu.stanford.nlp.ling.Word
a
edu.stanford.nlp.ling.Word
great
edu.stanford.nlp.ling.Word
fall
edu.stanford.nlp.ling.Word
.
edu.stanford.nlp.ling.Word
</code></pre>

<p>DocumentPreprocessor from the Stanford parser correctly splits the text into two sentences. The problem appears to be with the use of the equals method. Each word has type ""edu.stanford.nlp.ling.Word"". I've tried accessing the underlying string of the word, so I can then check if the string equals ""wall"", but I can't figure out how to access it.  </p>

<p>If I write the second for loop as ""for (Word word : sentence) {"" then I get an incompatible types error message on complilation. </p>
","java, string, nlp, stanford-nlp, sentence","<p>The <code>String</code> content can be accessed by calling the method: <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/Word.html#word%28%29"" rel=""nofollow""><code>word()</code></a> on <code>edu.stanford.nlp.ling.Word</code>; e.g.</p>

<pre><code>import edu.stanford.nlp.ling.Word;

List&lt;Word&gt; words = ...
for (Word word : words) {
  if (word.word().equals(args(1))) {
    System.err.println(""Yes!"");
  }
}
</code></pre>

<p>Also note that it is better to use generics when defining the <code>List</code> as it means the compiler or IDE will typically warn you if you attempt to compare classes of incompatible types (e.g. <code>Word</code> versus <code>String</code>).</p>

<p><strong>EDIT</strong></p>

<p>Turns out I was looking at an older version of the NLP API.  Looking at the most recent <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/DocumentPreprocessor.html"" rel=""nofollow""><code>DocumentPreprocessor</code></a> documentation I see that it implements <code>Iterable&lt;List&lt;HasWord&gt;&gt;</code> whereby <code>HasWord</code> defines the <code>word()</code> method.  Hence your code should look something like this:</p>

<pre><code>DocumentPreprocessor dp = ...
for (HasWord hw : dp) {
  if (hw.word().equals(args[1])) {
    System.err.println(""Yes!"");
  }
}
</code></pre>
",2,2,1514,2011-10-13 13:36:21,https://stackoverflow.com/questions/7754950/java-query-about-finding-a-word-in-a-sentence
What do the abbreviations in POS tagging etc mean?,"<p>Say I have the following Penn Tree:</p>

<pre><code>(S (NP-SBJ the steel strike)
 (VP lasted
     (ADVP-TMP (ADVP much longer)
               (SBAR than
                     (S (NP-SBJ he)
                        (VP anticipated
                            (SBAR *?*))))))
 .)
</code></pre>

<p>What do abbrevations like <code>VP</code> and <code>SBAR</code> etc mean? Where can I find these definitions? What are these abbreviations called?</p>
","language-agnostic, nlp, stanford-nlp","<p>Those are the Penn Treebank tags, for example, VP means ""Verb Phrase"". The full list can be found <a href=""http://web.mit.edu/6.863/www/PennTreebankTags.html"" rel=""nofollow noreferrer"">here</a></p>
",10,9,4371,2011-10-21 17:52:43,https://stackoverflow.com/questions/7853295/what-do-the-abbreviations-in-pos-tagging-etc-mean
Extracting clause from a Penn Treebank-formatted text,"<p>Say I have a sentence:</p>

<pre><code>After he had eaten the cheese, Bill went to the grocery.
</code></pre>

<p>In my program, I get the following output:</p>

<pre><code>---PARSE TREE---
(ROOT
  (S
    (SBAR (IN After)
      (S
        (NP (PRP he))
        (VP (VBD had)
          (VP (VBN eaten)
            (NP (DT the) (NN cheese))))))
    (, ,)
    (NP (NNP Bill))
    (VP (VBD went)
      (PP (TO to)
        (NP (DT the) (NN grocery))))
    (. .)))
</code></pre>

<p>How would I merge the stuff not within a clause to become an independent clause? Like this:</p>

<pre><code>S Clause {
    SBAR Clause {
         After he had eaten the cheese,
    }

    S Clause {
        Bill went to the grocery.
    }
}
</code></pre>

<p>I'm pretty sure that I'm not clear, but basically I want to extract the independent and dependent clauses of the sentence, and the subclauses of those clauses.</p>
","nlp, stanford-nlp","<p>Here is a demonstration code from the NLTK guide (It doesn't explicitly show how to extract a clause):
<a href=""http://nltk.googlecode.com/svn/trunk/doc/howto/tree.html"" rel=""nofollow"">http://nltk.googlecode.com/svn/trunk/doc/howto/tree.html</a></p>
",1,4,1525,2011-10-24 01:27:38,https://stackoverflow.com/questions/7870554/extracting-clause-from-a-penn-treebank-formatted-text
using Dependency Parser in Stanford coreNLP,"<p>I am using the Stanford coreNLP ( <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a> ) in order to parse sentences and extract dependencies between the words.</p>

<p>I have managed to create the dependencies graph like in the example in the supplied link, but I don't know how to work with it. I can print the entire graph using the <code>toString()</code> method, but the problem I have is that the methods that search for certain words in the graph, such as <code>getChildList</code>, require an IndexedWord object as a parameter. Now, it is clear why they do because the nodes of the graph are of IndexedWord type, but it's not clear to me how I create such an object in order to search for a specific node.</p>

<p>For example: I want to find the children of the node that represents the word ""problem"" in my sentence. How I create an IndexWord object that represents the word ""problem"" so I can search for it in the graph?</p>
","nlp, stanford-nlp","<p>In general, you shouldn't be creating your own IndexedWord objects. (These are used to represent ""word tokens"", i.e., particular words in a text, not ""word types"", and so asking for the word ""problem"" -- a word type -- isn't really valid; in particular, a sentence could have multiple tokens of this word type.)</p>

<p>There are a couple of convenience methods that let you do what you want:</p>

<ul>
<li>sg.getNodeByWordPattern(String pattern)</li>
<li>sg.getAllNodesByWordPattern(String pattern)</li>
</ul>

<p>The first is a little dangerous, since it just returns the first IndexedWord matching the pattern, or null if there are none.  But it's most directly what you asked for.</p>

<p>Some other methods to start from are:</p>

<ul>
<li>sg.getFirstRoot() to find the (first, usually only) root of the graph and then to navigate down from there, such as by using the sg.getChildren(root) method.</li>
<li>sg.vertexSet() to get all of the IndexWord objects in the graph.</li>
<li>sg.getNodeByIndex(int) if you already know the input sentence, and therefore can ask for words by their integer index.</li>
</ul>

<p>Commonly these methods leave you iterating through nodes. Really, the first two get...Node... methods just do the iteration for you.  </p>
",12,5,4313,2011-11-17 15:33:47,https://stackoverflow.com/questions/8169827/using-dependency-parser-in-stanford-corenlp
getting started with Stanford Parser in jruby,"<p>I'm looking to add some text parsing in my rails app, and have been going in circles for the past few days looking for any tutorials or hints as to how to get this working. </p>

<p>I am completely new to Java, but nothing like jumping in with both feet. </p>

<p>i suspect the following code doesn't belong in my controller, and should likely be in a model, but I'm just seeing if I've got all the pieces in the right place at this point. </p>

<p>I borrowed this code from this SO question, <a href=""https://stackoverflow.com/questions/3161312/implementing-custom-java-class-in-jruby"">implementing custom java class in jruby</a>, because I was having trouble finding any sort of example code. </p>

<pre>
#my requires/imports/includes, included multiple versions to be safe
require 'java'
#include Java
require '/media/sf_Ruby192/java_progs/parser/stanford-parser.jar'
#require '/media/sf_Ruby192/java_progs/parser/'
require 'rubygems'
include_class 'edu.stanford.nlp.parser.lexparser.LexicalizedParser'

class ParseController &lt; ApplicationController

    def index
lp = LexicalizedParser.new
    #check if regular Java is working
list = java.util.ArrayList.new 
a = ""1""
b = ""2""
list.add(a)
list.add(b)
d = list[0]
    return render :text =&gt; list
    end
end
</pre>

<p>unfortunately for me, I get the error</p>

<pre>
java.lang.NullPointerException: null
</pre>

<p>when I include the </p>

<pre>
lp = LexicalizedParser.new
</pre>

<p>am i doing EVERYTHING wrong? when I comment out the lp = ..., I get the list output, so jruby is working, and I can write java in my rails app and get the output. </p>

<p>can somebody point me in the right direction, maybe tell me what is wrong with this bit of code, but hopefully actually set me straight on how I'm supposed to be working with jruby and rails. Hopefully some input on Stanford Parser too (I know, it's a lot to ask). There seems to be very little by the way of documentation or example code that i've found. </p>
","ruby-on-rails, jruby, stanford-nlp","<p>I don't think so. But I do think that you need to read up on how this parser works.</p>

<p>According to <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html</a>, the default constructor works as follows:</p>

<blockquote>
  <p>Construct a new LexicalizedParser object from a previously serialized
  grammar read from a property
  edu.stanford.nlp.SerializedLexicalizedParser, or a default file
  location.</p>
</blockquote>

<p>In other words, you are getting the NPE because the default constructor can't find enough information to create the parser.</p>

<p>If you grab the binary distribution from Stanford, appropriate grammars will be found in <code>grammar</code> directory. For example:</p>

<pre><code>$ jruby -S irb
irb(main):001:0&gt; require 'java'
=&gt; true
irb(main):002:0&gt; require 'stanford-parser.jar'
=&gt; true
irb(main):003:0&gt; java_import Java::edu.stanford.nlp.parser.lexparser.LexicalizedParser
=&gt; Java::EduStanfordNlpParserLexparser::LexicalizedParser
irb(main):004:0&gt; lp = LexicalizedParser.new(""grammar/englishPCFG.ser.gz"")
Loading parser from serialized file grammar/englishPCFG.ser.gz ... done [2.5 sec].
=&gt; #&lt;Java::EduStanfordNlpParserLexparser::LexicalizedParser:0x7d627b8b&gt;
</code></pre>
",1,1,512,2011-11-23 19:46:16,https://stackoverflow.com/questions/8248178/getting-started-with-stanford-parser-in-jruby
Multitask learning,"<p>Can anybody please explain multitask learning in simple and intuitive way? May be some real
world problem would be useful.Mostly, these days i am seeing many people are using it for natural language processing tasks.</p>
","nlp, machine-learning, stanford-nlp","<p>Let's say you've built a sentiment classifier for a few different domains.  Say, movies, music DVDs, and electronics.  These are easy to build high quality classifiers for, because there is tons of training data that you've scraped from Amazon.  Along with each classifier, you also build a similarity detector that will tell you for a given piece of text, how similar it was to the dataset each of the classifiers was trained on.</p>

<p>Now you want to find the sentiment of some text from an unknown domain or one in which there isn't such a great dataset to train on.  Well, how about we take a similarity weighted combination of the classifications from the three high quality classifiers we already have.  If we are trying to classify a dish washer review (there is no giant corpus of dish washer reviews, unfortunately), it's probably most similar to electronics, and so the electronics classifier will be given the most weight.  On the other hand, if we are trying to classify a review of a TV show, probably the movies classifier will do the best job.</p>
",5,5,371,2011-12-31 13:10:36,https://stackoverflow.com/questions/8688271/multitask-learning
Anaphora resolution using Stanford Coref,"<p>I have sentences <strong>(Text I)</strong>:</p>

<blockquote>
  <p><em>Tom is a smart boy. <strong>He</strong> know a lot of thing.</em></p>
</blockquote>

<p>I want to change <strong>He</strong> in the second sentence to <strong>Tom</strong>, so final sentences will become <strong>(Text II)</strong>:</p>

<blockquote>
  <p><em>Tom is a smart boy. <strong>Tom</strong> know a lot of thing.</em></p>
</blockquote>

<p>I've wrote some code, but my <em><strong>coref</strong></em> object always <strong><em>null</em></strong>.<br>
Besides I have no idea what to do next to get correct result.</p>

<pre><code>    String text = ""Tom is a smart boy. He know a lot of thing."";
    Annotation document = new Annotation(text);
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, parse, lemma, ner, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);

    List&lt;Pair&lt;IntTuple, IntTuple&gt;&gt; coref = document.get(CorefGraphAnnotation.class);
</code></pre>

<p>I want to know if I'm doing it wrong and what I should do next to get <strong>Text II</strong> from <strong>Text I</strong>.<br>
PS: I'm using Stanford CoreNLP 1.3.0.</p>

<p>Thanks.</p>
","nlp, stanford-nlp","<pre><code>List&lt;Pair&lt;IntTuple, IntTuple&gt;&gt; coref = document.get(CorefGraphAnnotation.class);
</code></pre>

<p>This is an old coref output format.</p>

<p>You can change this line to     </p>

<pre><code>Map&lt;Integer, CorefChain&gt; graph = document.get(CorefChainAnnotation.class);
</code></pre>

<p>or you can use the <code>oldCorefFormat</code> option:</p>

<pre><code>props.put(""oldCorefFormat"", ""true"");
</code></pre>
",2,2,3520,2012-01-07 09:47:16,https://stackoverflow.com/questions/8768760/anaphora-resolution-using-stanford-coref
Using Stanford CoreNLP,"<p>I am trying to get around using the Stanford CoreNLP. I used some code from the web to understand what is going on with the coreference tool. I tried running the project in Eclipse but keep encountering an out of memory exception. I tried increasing the heap size but there isnt any difference.</p>
<p>Why this keeps happening? Is this a code specific problem?</p>
<p>Here is my code:</p>
<pre><code>import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;


import java.util.Iterator;
import java.util.Map;
import java.util.Properties;


public class testmain {

    public static void main(String[] args) {

        String text = &quot;Viki is a smart boy. He knows a lot of things.&quot;;
        Annotation document = new Annotation(text);
        Properties props = new Properties();
        props.put(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, parse, dcoref&quot;);
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        pipeline.annotate(document);


        Map&lt;Integer, CorefChain&gt; graph = document.get(CorefCoreAnnotations.CorefChainAnnotation.class);



        Iterator&lt;Integer&gt; itr = graph.keySet().iterator();
    
        while (itr.hasNext()) {
        
             String key = itr.next().toString();
        
             String value = graph.get(key).toString();
        
             System.out.println(key + &quot; &quot; + value);      
        }

   }
}
</code></pre>
","java, eclipse, nlp, stanford-nlp","<p>I found similar problem when building small application using Stanford CoreNLP in Eclipse.<br>
Increasing Eclipse's heap size will not solve your problem.<br>
After doing search, it is <strong>ant build tool</strong> heap size that should be increased, but I have no idea how to do that.<br>
So I give up Eclipse and use Netbeans instead.</p>

<p><strong>PS:</strong> You will eventually get out of memory exception with default setting in Netbeans. But it can easily solved by adjust setting <strong>-Xms</strong> per application basis.</p>
",4,7,9437,2012-01-23 05:29:44,https://stackoverflow.com/questions/8967544/using-stanford-corenlp
Stanford Parser multithread usage,"<p>Stanford Parser is now 'thread-safe' as of <a href=""http://nlp.stanford.edu/software/lex-parser.shtml#History"" rel=""noreferrer"">version 2.0</a> (02.03.2012).  I am currently running the command line tools and cannot figure out how to make use of my multiple cores by threading  the program.</p>

<p>In the past, this question has been answered with ""Stanford Parser is not thread-safe"", as the FAQ still says.  I am hoping to find someone who has had success threading the latest version.</p>

<p>I have tried using -t flag (-t10 and -tLLP) since that was all I could find in my searches, but both throw errors.</p>

<p>An example of a command I issue is:</p>

<pre><code>java -cp stanford-parser.jar edu.stanford.nlp.parser.lexparser.LexicalizedParser \
-outputFormat ""oneline"" ./grammar/englishPCFG.ser.gz ./corpus &gt; corpus.lex
</code></pre>
","multithreading, nlp, multiprocessing, stanford-nlp","<p>Starting with version 2.0.5, you can now easily use multiple threads with the option <code>-nthreads k</code>. For example, your command can be like this:</p>

<pre><code>java -mx6g edu.stanford.nlp.parser.lexparser.LexicalizedParser -nthreads 4 edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz file.txt &gt; file.stp
</code></pre>

<p>(Releases of version 2 prior to 2013 had no way to enable multithreading from the command-line, but only when using the API.)</p>

<p>Internally, you can simultaneously run as many parsing threads inside one JVM process as you want. You can do this either by getting and using multiple LexicalizedParserQuery objects (via the <code>parserQuery()</code> method) or implicitly by calling <code>apply(...)</code> or <code>parseTree(...)</code> off one LexicalizedParser.  The <code>-nthreads k</code> option does this for you by sending successive sentences to different parsers using the <code>Executor</code> framework. You can also simultaneously create multiple LexicalizedParser's, e.g., for parsing different languages.</p>

<p>Multiple LexicalizedparserQuery objects share the same grammar (LexicalizedParser), but the memory space savings aren't huge, as most of the memory goes to the transient structures used in chart parsing. So, if you are running lots of parsing threads concurrently, you will need to give a lot of memory to the JVM, as in the example above.</p>

<p>p.s. Sorry, yes, some of the documentation still needs updating.  But -tLPP is one flag for specifying language-specific resources. The Stanford Parser has no -t flag.</p>
",16,8,3473,2012-02-15 01:10:40,https://stackoverflow.com/questions/9286597/stanford-parser-multithread-usage
Stanford-parser in Ruby does not create Preprocesser,"<p>I am trying to use Stanford-parser for Ruby and get a RuntimeError: Constructor not found</p>

<p>I had to install 'rbj' and 'treebank' gems to get it running.</p>

<p>Now I can </p>

<pre><code>require 'stanfordparser'
</code></pre>

<p>but can't get to  </p>

<pre><code>preproc = StanfordParser::DocumentPreprocessor.new
</code></pre>

<p>The funciton that returns the error is here (ruby-1.9.3-p0/gems/stanfordparser-2.2.0/lib/java_object.rb:40:in `new'):</p>

<pre><code>def initialize(obj, *args)
  @java_object = obj.class == String ?
  Rjb::import(obj).send(:new, *args) : obj
end
</code></pre>

<p>I saw a couple posts on some forums about this issue, but it seems no one has figured it out.
Any ideas are greatly appreciated!</p>
","java, ruby, constructor, runtime-error, stanford-nlp","<p>It seems like no one has updated either of the two Ruby interfaces to the Stanford Parser recently, and so there may well be interface rot, with the API changes we made in version 2.0 to accommodate multithreading.</p>

<p>Would it be a choice to run the parser within StanfordCoreNLP?  A gem for that was written very recently and is actively being developed: <a href=""http://rubygems.org/gems/stanford-core-nlp"" rel=""nofollow"">stanford-core-nlp</a>. </p>
",3,2,243,2012-02-26 19:24:33,https://stackoverflow.com/questions/9456356/stanford-parser-in-ruby-does-not-create-preprocesser
Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space,"<p>I'm using Eclipse to run java program class, while I run it i got this error</p>

<pre><code>Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
</code></pre>

<p>then i changed the VM from the Properties > Run > VM Options, and I run the program again i got a new error,</p>

<pre><code>Error occurred during initialization of VM
Incompatible initial and maximum heap sizes specified
</code></pre>

<p>I'm trying to apply stanford libraries in my program, any idea how to solve this error .</p>
","java, eclipse, stanford-nlp","<p>to change the VM for Eclipse you can change the amount of the MV from 
    Windows> Preferences> Java> Installed JREs 
from there select the JRE and click edit, then write in the Default VM Arguments: to -Xmx1024M or any other amount of memory ... </p>
",7,6,29097,2012-02-27 04:18:39,https://stackoverflow.com/questions/9460015/exception-in-thread-main-java-lang-outofmemoryerror-java-heap-space
How can I split a text into sentences using the Stanford parser?,"<p>How can I split a text or paragraph into sentences using <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""noreferrer"">Stanford parser</a>?</p>

<p>Is there any method that can extract sentences, such as <code>getSentencesFromString()</code> as it's provided for <a href=""http://stanfordparser.rubyforge.org/"" rel=""noreferrer"">Ruby</a>?</p>
","java, parsing, artificial-intelligence, nlp, stanford-nlp","<p>You can check the DocumentPreprocessor class. Below is a short snippet. I think there may be other ways to do what you want.</p>

<pre><code>String paragraph = ""My 1st sentence. “Does it work for questions?” My third sentence."";
Reader reader = new StringReader(paragraph);
DocumentPreprocessor dp = new DocumentPreprocessor(reader);
List&lt;String&gt; sentenceList = new ArrayList&lt;String&gt;();

for (List&lt;HasWord&gt; sentence : dp) {
   // SentenceUtils not Sentence
   String sentenceString = SentenceUtils.listToString(sentence);
   sentenceList.add(sentenceString);
}

for (String sentence : sentenceList) {
   System.out.println(sentence);
}
</code></pre>
",31,28,36839,2012-02-29 02:19:53,https://stackoverflow.com/questions/9492707/how-can-i-split-a-text-into-sentences-using-the-stanford-parser
Error in Stanford CoreNLP: java.lang.NoClassDefFoundError,"<p>I downloaded the Stanford CoreNLP and when I run the code which is given in their <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow noreferrer"">website</a>. I get an error in this line</p>
<pre><code>StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>
<p>The error is as follows</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: nu/xom/Node at sample1.main(sample1.java:35)
Caused by: java.lang.ClassNotFoundException: nu.xom.Node
at java.net.URLClassLoader$1.run(Unknown Source)
at java.net.URLClassLoader$1.run(Unknown Source)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
... 1 more
</code></pre>
<p>I use Eclipse IDE, should I do some configuration? Please help me out with it!</p>
","java, stanford-nlp","<p>I have downloaded the stanford-corenlp-2012-01-08.tgz from the link you provided. Using 7-zip I have uncompressed it and found another compressed file with name stanford-corenlp-2012-01-08 and again uncompressed it using 7-zip. The content is shown below:
<img src=""https://i.sstatic.net/d4DBZ.png"" alt=""enter image description here""></p>

<p>Then I created a new Java Project in eclipse and created a new folder lib inside that project and put</p>

<ul>
<li>joda-time.jar</li>
<li>stanford-corenlp-2011-12-27-models.jar</li>
<li>stanford-corenlp-2012-01-08.jar</li>
<li>xom.jar</li>
</ul>

<p>jars to the lib. Then set the project Java Build Path to these jars.</p>

<p><img src=""https://i.sstatic.net/56TNr.png"" alt=""enter image description here""></p>

<p>Next I created a test class with main method.</p>

<pre><code>import java.util.Properties;

import edu.stanford.nlp.pipeline.StanfordCoreNLP;


public class NLP {

    /**
     * @param args
     */
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP coreNLP = new StanfordCoreNLP(props);
    }

}
</code></pre>

<p>And lastly run the application. The output is depicted below:</p>

<p><img src=""https://i.sstatic.net/JuYqJ.png"" alt=""enter image description here""></p>

<p>It runs successfully. </p>

<p>Hope this will help you.</p>
",30,5,9006,2012-03-02 04:40:10,https://stackoverflow.com/questions/9528080/error-in-stanford-corenlp-java-lang-noclassdeffounderror
How can I differentiate between a person&#39;s name and other names that are derived from verbs,"<p>How can I extract person names from the text?</p>

<p>I have applied some NLP toolkit for this, specifically I used the Stanford NER toolkit to extract names from text. With that, I can extract person names from the text, but when I want the program to extract words like 'programmer', 'lecturer' or 'engineer', the libraries couldn't extract those. Is there any way to extract these from the text?</p>
","java, nlp, stanford-nlp","<p>Since ""Programmer, lecturer, and engineer"" are not named-entities,  you may have to maintain a list of those words. I think you can obtain them from word derivation relationships in Wordnet, like ""sing"" (verb) and ""singer"" or ""lecture"" (verb) and ""lecturer"" (noun).</p>

<p>A <a href=""http://medialab.di.unipi.it/wiki/SuperSense_Tagger"" rel=""nofollow"">SuperSense tagger</a> may also be used as NER, I think it can tag those words you mentioned as ""noun.person"" which is what you need. <a href=""http://www.ark.cs.cmu.edu/ARKref/"" rel=""nofollow"">ArkRef</a> (Java) is a coreference tool that uses it (through a Java port of supersense tagger, bundled), and there's an online demo there, so you can check if your target words are tagged in square brackets.</p>
",1,-1,752,2012-03-05 03:42:57,https://stackoverflow.com/questions/9561370/how-can-i-differentiate-between-a-persons-name-and-other-names-that-are-derived
Tools for text simplification (Java),"<p>What is the best tool that can do text simplification using Java?</p>

<p>Here is an example of text simplification:</p>

<pre><code>John, who was the CEO of a company, played golf.
                       ↓
John played golf. John was the CEO of a company.
</code></pre>
","java, nlp, stanford-nlp, gate","<p>I see your problem as a task of converting complex or compound sentence into simple sentences.
Based on literature <a href=""http://www.redwoods.edu/Eureka/ASC/Handouts/Sentence%20Types.pdf"" rel=""noreferrer"">Sentence Types</a>, a simple sentence is built from one independent clause. A compound and complex sentence is built from at least two clauses. Also, clause must have subject and verb.<br />
So your task is to split sentence into clauses that form your sentence.</p>
<p>Dependency parsing from Stanford CoreNLP is a perfect tools to split compound and complex sentence into simple sentence. You can try the <a href=""http://corenlp.run/#text=John,%20who%20was%20the%20CEO%20of%20a%20company,%20played%20golf"" rel=""noreferrer"">demo online</a>.<br />
From your sample sentence, we will get parse result in <strong>Stanford typed dependency (SD)</strong> notation  as shown below:</p>
<blockquote>
<p><code>nsubj(CEO-6, John-1)</code><br />
<code>nsubj(played-11, John-1)</code><br />
<code>cop(CEO-6, was-4)</code><br />
<code>det(CEO-6, the-5)</code><br />
<code>rcmod(John-1, CEO-6)</code><br />
<code>det(company-9, a-8)</code><br />
<code>prep_of(CEO-6, company-9)</code><br />
<code>root(ROOT-0, played-11)</code><br />
<code>dobj(played-11, golf-12)</code></p>
</blockquote>
<p>A clause can be identified from relation (in SD) which category is subject, e.g. <strong>nsubj</strong>, <strong>nsubjpass</strong>. See <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""noreferrer"">Stanford Dependency Manual</a><br />
Basic clause can be extracted from <strong>head</strong> as verb part and <strong>dependent</strong> as subject part. From SD above, there are two basic clause i.e.</p>
<ul>
<li>John CEO</li>
<li>John played</li>
</ul>
<p>After you get basic clause, you can add another part to make your clause a complete and meaningful sentence. To do so, please consult <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""noreferrer"">Stanford Dependency Manual</a>.</p>
<p>By the way, your question might be related with <a href=""https://stackoverflow.com/questions/8974090/finding-meaningful-sub-sentences-from-a-sentence/"">Finding meaningful sub-sentences from a sentence</a></p>
<hr />
<h3>Answer to 3rd comment:</h3>
<p>Once you got the pair of subject an verb, i.e. <strong><code>nsubj(CEO-6, John-1)</code></strong>, get all dependencies that have link to that dependency, except any dependency which category is subject, then extract unique word from these dependencies.</p>
<p>Based on example, <strong><code>nsubj(CEO-6, John-1)</code></strong>, if you start traversing from <strong><code>John-1</code></strong>, you'll get <strong><code>nsubj(played-11, John-1)</code></strong> but you should ignore it since its category is subject.</p>
<p>Next step is traversing from <strong><code>CEO-6</code></strong> part. You'll get</p>
<blockquote>
<p><code>cop(CEO-6, was-4)</code><br />
<code>det(CEO-6, the-5)</code><br />
<code>rcmod(John-1, CEO-6)</code><br />
<code>prep_of(CEO-6, company-9)</code></p>
</blockquote>
<p>From result above, you got new dependencies to traverse (i.e. find another dependencies that have <strong><code>was-4, the-5, company-9</code></strong> in either head or dependent).<br />
Now your dependencies are</p>
<blockquote>
<p><code>cop(CEO-6, was-4)</code><br />
<code>det(CEO-6, the-5)</code><br />
<code>rcmod(John-1, CEO-6)</code><br />
<code>prep_of(CEO-6, company-9)</code><br />
<strong><code>det(company-9, a-8)</code></strong></p>
</blockquote>
<p>In this step, you've finished traversing all dependecies linked to <strong><code>nsubj(CEO-6, John-1)</code></strong>. Next, extract words from all head and dependent, then arrange the word in ascending order based on number appended to these words. This number indicating word order in original sentence.</p>
<blockquote>
<p><code>John was the CEO a company</code></p>
</blockquote>
<p>Our new sentence is missing one part, i.e <strong><code>of</code></strong>. This part is hidden in <strong><code>prep_of(CEO-6, company-9)</code></strong>. If you read <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""noreferrer"">Stanford Dependency Manual</a>, there are two kinds of <strong>SD</strong>, collapsed and non-collapsed. Please read them to understand why this <strong><code>of</code></strong> is hidden and how to get the word order of this hidden part.</p>
<p>With same approach, you'll get second sentence</p>
<blockquote>
<p><code>John played golf</code></p>
</blockquote>
",33,18,11240,2012-03-07 04:47:31,https://stackoverflow.com/questions/9595983/tools-for-text-simplification-java
java lang Class Cast Exception,"<p>I have written a code which can reduce the grammatical boundaries for a text, but when I run the program this exception comes up     </p>

<pre><code>java.lang.ClassCastException
</code></pre>

<p>here is the class that i run,</p>

<pre><code>public class paerser {
public static void main (String [] arg){
    LexicalizedParser lp = new LexicalizedParser(""grammar/englishPCFG.ser.gz"");
        lp.setOptionFlags(""-maxLength"", ""500"", ""-retainTmpSubcategories"");
        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
       GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
       String text = ""John, who was the CEO of a company, played golf."";
       edu.stanford.nlp.trees.Tree parse = lp.apply(Arrays.asList(text));
       GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
       List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();
       System.out.println(tdl);

}
}
</code></pre>

<p>Updated,</p>

<p>here is the full stack trace ...</p>

<pre><code>Loading parser from serialized file grammar/englishPCFG.ser.gz ... done [1.5 sec].
Following exception caught during parsing:
java.lang.ClassCastException: java.lang.String cannot be cast to edu.stanford.nlp.ling.HasWord
    at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.parse(ExhaustivePCFGParser.java:346)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:386)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.apply(LexicalizedParser.java:304)
    at paerser.main(paerser.java:19)
Recovering using fall through strategy: will construct an (X ...) tree.
Exception in thread ""main"" java.lang.ClassCastException: java.lang.String cannot be  cast to edu.stanford.nlp.ling.HasWord
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.apply(LexicalizedParser.java:317)
    at paerser.main(paerser.java:19)
</code></pre>
","java, eclipse, exception, nlp, stanford-nlp","<p>Stacktrace shows that <a href=""http://cogcomp.cs.illinois.edu/trac/browser/Public/public/curator-src/curator-annotators/stanford-parser/src/edu/stanford/nlp/parser/lexparser/ExhaustivePCFGParser.java"" rel=""nofollow"">ExhaustivePCFGParser's parse method</a> is being used. It expects a List of HasWord objects. You are passing a list of String. Hence, the exception.</p>

<pre><code>public boolean parse(List&lt;? extends HasWord&gt; sentence) { // ExhaustivePCFGParser
</code></pre>
",3,-1,1131,2012-03-13 04:05:09,https://stackoverflow.com/questions/9678209/java-lang-class-cast-exception
Remove tags of POS tagger,"<p>Is it possible to remove the tags from the sentences? One can accomplish it by scanning through the file and finding tags and removing them, but since there are many tags( some models have 30+, some have around 48-50, they basically follow the <a href=""http://www.google.co.in/#hl=en&amp;output=search&amp;sclient=psy-ab&amp;q=penn+treebank+pos+tags&amp;oq=penn+treebank+pos+tags&amp;aq=f&amp;aqi=g1&amp;aql=&amp;gs_sm=3&amp;gs_upl=90l3139l0l3312l22l15l0l2l2l1l592l3727l0.4.1.3.0.3l11l0&amp;gs_l=hp.3..0.90l3139l0l3313l22l15l0l2l2l1l592l3727l0j4j1j3j0j3l11l0&amp;pbx=1&amp;bav=on.2,or.r_gc.r_pw.r_qf.,cf.osb&amp;fp=b1609e7f0f1800b7&amp;biw=1366&amp;bih=601"" rel=""nofollow"">penn treebank pos tags</a> ), is there a fast and sweet way to remove tags in a more efficient manner?
I did check the API, but there was no such method for removal of tags.</p>
","java, stanford-nlp, pos-tagger","<p>There's nothing special built in for this, but since the output includes both the word and its tag, I'm not sure why you need to scan the original document again.  Can't you just delete the tags by deleting from the last tagSeparator character ('/' or whatever) until white-space?  Or, it could be simpler to use</p>

<pre><code>-outputFormat tsv
</code></pre>

<p>Then you will get two column output with the words in the first column and the tag in the second column and you could just keep the first column when done.</p>
",1,0,642,2012-03-14 10:01:06,https://stackoverflow.com/questions/9699674/remove-tags-of-pos-tagger
Implementing a mini-summarizer in Java,"<p>I'm working on making a small summarization utility in Java. I'm using the <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford Log-linear Part-Of-Speech Tagger</a> to find the parts of speech in the sentences. Then, I'm scoring specific tags and awarding each sentence a score. Then, finally when I summarize, I only add those line which have a score of beyond a certain limit. That's the plan.</p>

<p>Here's a sample code that I have worked out for just scoring adjectives, and then generating a summary based on a score greater than,say 1.</p>

<pre><code>MaxentTagger tagger = new MaxentTagger(""taggers/bidirectional-distsim-wsj-0-18.tagger"");
BufferedReader reader = new BufferedReader( new FileReader (""C:\\Summarizer\\src\\summarizer\\testing\\testingtext.txt"")); 
String line  = null;
int score = 0;
StringBuilder stringBuilder = new StringBuilder();
File tempFile = new File(""C:\\Summarizer\\src\\summarizer\\testing\\tempFile.txt"");
Writer writerForTempFile = new BufferedWriter(new FileWriter(tempFile));

String ls = System.getProperty(""line.separator"");
while( ( line = reader.readLine() ) != null )
{
    stringBuilder.append( line );
    stringBuilder.append( ls );
    String tagged = tagger.tagString(line);
    Pattern tagFinder = Pattern.compile(""/JJ"");
    Matcher tagMatcher = tagFinder.matcher(tagged);
    while(tagMatcher.find())
    {
        score++;
    }
    if(score &gt; 1)
        writerForTempFile.write(stringBuilder.toString());
    score = 0;
}
reader.close();
writerForTempFile.close();
</code></pre>

<p>But apparently, I'm going wrong somewhere. It does write the required lines into the <code>tempFile</code> , but there are many extra lines as well. Kindly help!</p>
","java, file-io, stanford-nlp, pos-tagger","<p>You need to reset you StringBuilder for every new line you want to write to the file. Currently, for every line you write the currently appended line and all previous appended lines in the StringBuilder is written to your file when <code>score &gt; 1</code>.</p>
",1,0,286,2012-03-14 11:21:59,https://stackoverflow.com/questions/9700965/implementing-a-mini-summarizer-in-java
Score each sentence in a line based upon a tag and summarize the text. (Java),"<p>I'm trying to create a summarizer in Java. I'm using the <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford Log-linear Part-Of-Speech Tagger</a> to tag the words, and then, for certain tags, I'm scoring the sentence and finally in the summary, I'm printing sentences with a high score value.
Here's the code:</p>

<pre><code>    MaxentTagger tagger = new MaxentTagger(""taggers/bidirectional-distsim-wsj-0-18.tagger"");

    BufferedReader reader = new BufferedReader( new FileReader (""C:\\Summarizer\\src\\summarizer\\testing\\testingtext.txt""));
    String line  = null;
    int score = 0;
    StringBuilder stringBuilder = new StringBuilder();
    File tempFile = new File(""C:\\Summarizer\\src\\summarizer\\testing\\tempFile.txt"");
    Writer writerForTempFile = new BufferedWriter(new FileWriter(tempFile));


    String ls = System.getProperty(""line.separator"");
    while( ( line = reader.readLine() ) != null )
    {
        stringBuilder.append( line );
        stringBuilder.append( ls );
        String tagged = tagger.tagString(line);
        Pattern pattern = Pattern.compile(""[.?!]""); //Find new line
        Matcher matcher = pattern.matcher(tagged);
        while(matcher.find())
        {
            Pattern tagFinder = Pattern.compile(""/JJ""); // find adjective tag
            Matcher tagMatcher = tagFinder.matcher(matcher.group());
            while(tagMatcher.find())
            {
                score++; // increase score of sentence for every occurence of adjective tag
            }
            if(score &gt; 1)
                writerForTempFile.write(stringBuilder.toString());
            score = 0;
            stringBuilder.setLength(0);
        }

    }

    reader.close();
    writerForTempFile.close();
</code></pre>

<p>The above code isn't working. Although, if I cut my work and generate score for every line(not sentence),it works. But summaries aren't generated that way,are they?
Here's the code for that: (all the declarations being the same as above)</p>

<pre><code>while( ( line = reader.readLine() ) != null )
        {
            stringBuilder.append( line );
            stringBuilder.append( ls );
            String tagged = tagger.tagString(line);
            Pattern tagFinder = Pattern.compile(""/JJ""); // find adjective tag
            Matcher tagMatcher = tagFinder.matcher(tagged);
            while(tagMatcher.find())
            {
                score++;  //increase score of line for every occurence of adjective tag
            }
            if(score &gt; 1)
                writerForTempFile.write(stringBuilder.toString());
            score = 0;
            stringBuilder.setLength(0);
        }
</code></pre>

<p><strong>EDIT 1:</strong></p>

<p>Information regarding what the MaxentTagger does. A sample code to show it's functioning :</p>

<pre><code>import java.io.IOException;

import edu.stanford.nlp.tagger.maxent.MaxentTagger;

public class TagText {
    public static void main(String[] args) throws IOException,
            ClassNotFoundException {

        // Initialize the tagger
        MaxentTagger tagger = new MaxentTagger(
                ""taggers/bidirectional-distsim-wsj-0-18.tagger"");

        // The sample string
        String sample = ""This is a sample text"";

        // The tagged string
        String tagged = tagger.tagString(sample);

        // Output the result
        System.out.println(tagged);
    }
}
</code></pre>

<p>Output:</p>

<pre><code>This/DT is/VBZ a/DT sample/NN sentence/NN
</code></pre>

<p><strong>EDIT 2:</strong></p>

<p>Modified code using BreakIterator to find sentence breaks. Yet the problem is persisting.</p>

<pre><code>while( ( line = reader.readLine() ) != null )
        {
            stringBuilder.append( line );
            stringBuilder.append( ls );
            String tagged = tagger.tagString(line);
            BreakIterator bi = BreakIterator.getSentenceInstance();
            bi.setText(tagged);
            int end, start = bi.first();
            while ((end = bi.next()) != BreakIterator.DONE)
            {
                String sentence = tagged.substring(start, end);
                Pattern tagFinder = Pattern.compile(""/JJ"");
                Matcher tagMatcher = tagFinder.matcher(sentence);
                while(tagMatcher.find())
                {
                    score++;
                }
                scoreTracker.add(score);
                if(score &gt; 1)
                    writerForTempFile.write(stringBuilder.toString());
                score = 0;
                stringBuilder.setLength(0);
                start = end;
            }
</code></pre>
","java, stanford-nlp, pos-tagger","<p>Finding sentence breaks can be a bit more involved than just looking for [.?!], consider using <a href=""http://docs.oracle.com/javase/6/docs/api/java/text/BreakIterator.html"" rel=""nofollow"">BreakIterator</a>.getSentenceInstance()</p>

<p>Its performance is actually quite similar to LingPipe's (more complex) implementation, and better than the one in OpenNLP (from my own testing, at least).</p>

<h3>Sample Code</h3>

<pre><code>BreakIterator bi = BreakIterator.getSentenceInstance();
bi.setText(text);
int end, start = bi.first();
while ((end = bi.next()) != BreakIterator.DONE) {
    String sentence = text.substring(start, end);
    start = end;
}
</code></pre>

<h3>Edit</h3>

<p>I think this is what you're looking for:</p>

<pre><code>    Pattern tagFinder = Pattern.compile(""/JJ"");
    BufferedReader reader = getMyReader();
    String line = null;
    while ((line = reader.readLine()) != null) {
        BreakIterator bi = BreakIterator.getSentenceInstance();
        bi.setText(line);
        int end, start = bi.first();
        while ((end = bi.next()) != BreakIterator.DONE) {
            String sentence = line.substring(start, end);
            String tagged = tagger.tagString(sentence);
            int score = 0;
            Matcher tag = tagFinder.matcher(tagged);
            while (tag.find())
                score++;
            if (score &gt; 1)
                writerForTempFile.println(sentence);
            start = end;
        }
    }
</code></pre>
",3,2,1792,2012-03-14 13:15:04,https://stackoverflow.com/questions/9702739/score-each-sentence-in-a-line-based-upon-a-tag-and-summarize-the-text-java
Extracting information from context-free phrase structure output from Stanford Parser,"<p>The Stanford Parser (http://nlp.stanford.edu/software/lex-parser.shtml) gives context-free phrase structure trees as following. What is the best way to extract things like all the Noun Phrases(NP) and Verb Phrases(NP) in the tree? Is there any Python (or Java) library that can allow me to read structures like these? Thank you.</p>

<pre><code>(ROOT
  (S
    (S
      (NP
        (NP (DT The) (JJS strongest) (NN rain))
        (VP
          (ADVP (RB ever))
          (VBN recorded)
          (PP (IN in)
            (NP (NNP India)))))
      (VP
        (VP (VBD shut)
          (PRT (RP down))
          (NP
            (NP (DT the) (JJ financial) (NN hub))
            (PP (IN of)
              (NP (NNP Mumbai)))))
        (, ,)
        (VP (VBD snapped)
          (NP (NN communication) (NNS lines)))
        (, ,)
        (VP (VBD closed)
          (NP (NNS airports)))
        (CC and)
        (VP (VBD forced)
          (NP
            (NP (NNS thousands))
            (PP (IN of)
              (NP (NNS people))))
          (S
            (VP (TO to)
              (VP
                (VP (VB sleep)
                  (PP (IN in)
                    (NP (PRP$ their) (NNS offices))))
                (CC or)
                (VP (VB walk)
                  (NP (NN home))
                  (PP (IN during)
                    (NP (DT the) (NN night))))))))))
    (, ,)
    (NP (NNS officials))
    (VP (VBD said)
      (NP-TMP (NN today)))
    (. .)))
</code></pre>
","python, nlp, stanford-nlp","<p>Check out the Natural Language Toolkit (NLTK) at <a href=""http://www.nltk.org"" rel=""nofollow"">nltk.org</a>.</p>

<p>The toolkit is written in Python and provides code for reading precisely these kinds of trees (as well as lots of other stuff).</p>

<p>Alternatively, you could write your own recursive function for doing this.  It would be pretty straightforward.</p>

<hr>

<p>Just for fun: here's a super simple implementation of what you want:</p>

<pre><code>def parse():
  itr = iter(filter(lambda x: x, re.split(""\\s+"", s.replace('(', ' ( ').replace(')', ' ) '))))

  def _parse():
    stuff = []
    for x in itr:
      if x == ')':
        return stuff
      elif x == '(':
        stuff.append(_parse())
      else:
        stuff.append(x)
    return stuff

  return _parse()[0]

def find(parsed, tag):
  if parsed[0] == tag:
    yield parsed
  for x in parsed[1:]:
    for y in find(x, tag):
      yield y

p = parse()
np = find(p, 'NP')
for x in np:
  print x
</code></pre>

<p>yields:</p>

<pre><code>['NP', ['NP', ['DT', 'The'], ['JJS', 'strongest'], ['NN', 'rain']], ['VP', ['ADVP', ['RB', 'ever']], ['VBN', 'recorded'], ['PP', ['IN', 'in'], ['NP', ['NNP', 'India']]]]]
['NP', ['DT', 'The'], ['JJS', 'strongest'], ['NN', 'rain']]
['NP', ['NNP', 'India']]
['NP', ['NP', ['DT', 'the'], ['JJ', 'financial'], ['NN', 'hub']], ['PP', ['IN', 'of' ['NP', ['NNP', 'Mumbai']]]]
['NP', ['DT', 'the'], ['JJ', 'financial'], ['NN', 'hub']]
['NP', ['NNP', 'Mumbai']]
['NP', ['NN', 'communication'], ['NNS', 'lines']]
['NP', ['NNS', 'airports']]
['NP', ['NP', ['NNS', 'thousands']], ['PP', ['IN', 'of'], ['NP', ['NNS', 'people']]]]
['NP', ['NNS', 'thousands']]
['NP', ['NNS', 'people']]
['NP', ['PRP$', 'their'], ['NNS', 'offices']]
['NP', ['NN', 'home']]
['NP', ['DT', 'the'], ['NN', 'night']]
['NP', ['NNS', 'officials']]
</code></pre>
",2,3,1510,2012-03-21 03:50:48,https://stackoverflow.com/questions/9798366/extracting-information-from-context-free-phrase-structure-output-from-stanford-p
Detect a pronoun and its noun?,"<p>Wondering if there is any tool that can help me to detect a pronoun's name in a text.</p>

<p>Example</p>

<pre><code>Jone is Spanish. He can speak German.
</code></pre>

<p>How can I tag <code>He</code> to <code>Jone</code>?</p>
","java, nlp, weka, stanford-nlp","<p>The problem you are trying to solve is known as <a href=""http://en.wikipedia.org/wiki/Anaphora_%28linguistics%29#Anaphor_resolution"" rel=""noreferrer"">anaphora resolution</a>. A java tool to perform this task is available <a href=""http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html"" rel=""noreferrer"">here</a>. The source code is made available on that site. For a theoretical look at how it works, check out <a href=""http://acl.ldc.upenn.edu/J/J94/J94-4002.pdf"" rel=""noreferrer"">this paper</a> by Lappin and Leass from 1994.</p>
",12,7,2482,2012-04-01 07:39:09,https://stackoverflow.com/questions/9962721/detect-a-pronoun-and-its-noun
Stanford NER: extracting separate lists of entities?,"<p>I can get a string annotated with Named Entities with the following code.</p>

<pre><code>String NEString =  classifier.classifyWithInlineXML(fileContents)
</code></pre>

<p>I'm wondering if there is any method to call so that I can get separate entities (PERSON, ORGANIZATION, LOCATIOIN) lists in the file, that way I don't have to parse the retrieved string with the above method to get the entity lists?</p>
","java, stanford-nlp, named-entity-recognition","<p>In my opinion, the cleanes way to run the classification is:</p>

<pre><code>List&lt;Triple&lt;String,Integer,Integer&gt;&gt; out = classifier.classifyToCharacterOffsets(text);
triple.first(): entity type
triple.second(): start position
triple.third(): end position
</code></pre>

<p>It groups consequent entities and returns the start and end position of entities.</p>
",3,4,1229,2012-04-16 12:31:37,https://stackoverflow.com/questions/10174122/stanford-ner-extracting-separate-lists-of-entities
Is there an option to get per-sentence processing time from the Stanford Parser?,"<p>I'm currently parsing Arabic text using the following command:</p>

<pre><code>java -mx1500m edu.stanford.nlp.parser.lexparser.LexicalizedParser \
  -cp ""$scriptdir/*:"" -sentences newline -outputFormat ""penn,wordsAndTags"" \
  edu/stanford/nlp/models/lexparser/arabicFactored.ser.gz $FILE
</code></pre>

<p>This outputs tokens, tags, and parse trees, plus a global overview on the processing speed similar to this output:</p>

<pre><code>Parsed 280 words in 10 sentences (1.95 wds/sec; 0.07 sents/sec).
</code></pre>

<p>Is there an option or command line switch to trigger output of processing time on a <em>per-sentence</em> level? Or would I have to add that to the code myself?</p>

<p>Haven't found such an option neither in the FAQ nor on the Stanford NLP website, so I'd appreciate if someone could clarify on this issue.</p>
",stanford-nlp,"<p>No, at present there isn't.  You'd need to add the code yourself.</p>
",2,3,202,2012-04-19 08:13:02,https://stackoverflow.com/questions/10224085/is-there-an-option-to-get-per-sentence-processing-time-from-the-stanford-parser
Initialise a Java program and listen for queries,"<p>I'm writing an application that uses the Stanford CoreNLP library among many other library. The application takes a URL, extracts text from it, and then process the the text using CoreNLP. The problem is that the CoreNLP library takes a lot of time to load its models into memory before annotating the text. I'am looking for the best way to do the initialisation step only one time and the next time when I want to use the app and I enter a URL it doesn't load the models again.</p>
","java, stanford-nlp","<p>Maybe you should try to run your application inside a container - <a href=""http://onjava.com/onjava/2003/05/14/java_webserver.html"" rel=""nofollow noreferrer"">Servlet Container</a> or even <a href=""http://en.wikipedia.org/wiki/Web_container"" rel=""nofollow noreferrer"">Web Container</a> like <a href=""http://tomcat.apache.org/"" rel=""nofollow noreferrer"">Apache Tomcat</a>? You can package your application as a <a href=""https://stackoverflow.com/questions/1001714/how-to-create-war-files"">simple WAR</a> then the container will initialize all the java classes once and assuming that you expose a servlet that would trigger the NLP processing for a single URL you will save the initialization time for all the consequent NLP processing calls. </p>

<p><strong>Edit:</strong></p>

<p>You do not have to use the servlets.
Alternatively, you can do one of the following:</p>

<ol>
<li>Run in you WAR a loop that would read URLs from some configuration source (DB or file) and for each URL will do the NLP and parsing (storing the results for later analysis?)</li>
<li>More advanced solution could include your existing java program cobined with <a href=""http://quartz-scheduler.org/"" rel=""nofollow noreferrer"">Quartz</a> that will run one NLP+parser every X seconds/minutes/hours etc.  Even more advanced version would be using Quartz with <a href=""http://static.springsource.org/spring-batch/"" rel=""nofollow noreferrer"">Spring Batch</a>. </li>
</ol>

<p>Good Luck!</p>
",1,1,352,2012-04-24 20:40:38,https://stackoverflow.com/questions/10305677/initialise-a-java-program-and-listen-for-queries
Parse out phrasal verbs,"<p>Has anyone ever tried parsing out phrasal verbs with Stanford NLP?
The problem is with separable phrasal verbs, e.g.: climb up, do over: <em>We climbed that hill up. I have to do this job over.</em></p>

<p>The first phrase looks like this in the parse tree:</p>

<pre><code>(VP 
    (VBD climbed)
    (ADVP 
        (IN that) 
        (NP (NN hill)
        )
    ) 
    (ADVP 
        (RB up)
    )
) 
</code></pre>

<p>the second phrase:</p>

<pre><code>(VB do) 
   (NP 
     (DT this) 
     (NN job)
   ) 
(PP 
   (IN over)
) 
</code></pre>

<p>So it seems like reading the parse tree would be the right way, but how to know that verb is going to be phrasal?</p>
",stanford-nlp,"<p>Dependency parsing, dude. Look at the prt (phrasal verb particle) dependency in both sentences. See the <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"">Stanford typed dependencies manual</a> for more info.</p>

<pre><code>nsubj(climbed-2, We-1)
root(ROOT-0, climbed-2)
det(hill-4, that-3)
dobj(climbed-2, hill-4)
prt(climbed-2, up-5)

nsubj(have-2, I-1)
root(ROOT-0, have-2)
aux(do-4, to-3)
xcomp(have-2, do-4)
det(job-6, this-5)
dobj(do-4, job-6)
prt(do-4, over-7)
</code></pre>

<p>The stanford parser gives you very nice dependency parses. I have code for programmatically accessing these if you need it: <a href=""https://gist.github.com/2562754"">https://gist.github.com/2562754</a></p>
",7,5,2676,2012-05-01 11:59:59,https://stackoverflow.com/questions/10397342/parse-out-phrasal-verbs
How to obtain the &quot;Grammatical Relation&quot; using Stanford NLP Parser?,"<p>I am absolutely new to Java development.</p>

<p>Can someone please elaborate on how to obtain ""Grammatical Relations"" using the Stanfords's Natural Language Processing Lexical Parser- open source Java code?</p>

<p>Thanks!</p>
","java, stanford-nlp","<p>See line 88 of first file in <a href=""https://gist.github.com/2562754"">my code to run the Stanford Parser programmatically</a></p>

<pre><code>GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
Collection tdl = gs.typedDependenciesCollapsed();

System.out.println(""words: ""+words); 
System.out.println(""POStags: ""+tags); 
System.out.println(""stemmedWordsAndTags: ""+stems); 
System.out.println(""typedDependencies: ""+tdl); 
</code></pre>

<p>The collection tdl is a list of these typed dependencies. If you look on the <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/TypedDependency.html"">javadoc for TypedDependency</a> you'll see that using the .reln() method gets you the grammatical relation.</p>

<p>Lines 311-318 of the third file in my code show how to use that list of typed dependencies. I happen to get the name of the relation, but you could get the relation itself, which would be of the class GrammaticalRelation.</p>

<pre><code>for( Iterator&lt;TypedDependency&gt; iter = tdl.iterator(); iter.hasNext(); ) {
    TypedDependency var = iter.next();

    TreeGraphNode dep = var.dep();
    TreeGraphNode gov = var.gov();

    // All useful information for a node in the tree
    String reln = var.reln().getShortName();
</code></pre>

<p>Don't feel bad, I spent a miserable day or two trying to figure out how to use the parser. I don't know if the docs have improved, but when I used it they were pretty damn awful.</p>
",6,0,2510,2012-05-02 11:14:30,https://stackoverflow.com/questions/10412611/how-to-obtain-the-grammatical-relation-using-stanford-nlp-parser
Clean text coming from PDFs,"<p>this is more of an algorithmic question rather than a specific language question, so I am happy to receive an answer in any language - even pseudocode, even just an idea.</p>

<p>Here is my problem: I need to work on large dataset of papers that come from articles in PDF and that were brutally copied/pasted into .txt. I only have the result of this abomination, which is around 16k papers, for 3.5 GB or text (the corpus I am using is the ACL Antology Network, <a href=""http://clair.si.umich.edu/clair/aan/DatasetContents.html"">http://clair.si.umich.edu/clair/aan/DatasetContents.html</a> ).</p>

<p>The ""junk"" comes from things like formulae, images, tables, and so on. It just pops in the middle of the running text, so I can't use regular expressions to clean it, and I can't think of any way to use machine learning for it either. I already spent a week on it, and then I decided to move on with a quick&amp;dirty fix. I don't care about cleaning it completely anymore, I don't care about false negatives and positives as long as the majority of this areas of text is removed.</p>

<p>Some examples of the text: note that formulae contain junk characters, but tables and caption don't (but they still make my sentence very long, and thus unparsable). Junk in bold.</p>

<p>Easy one:</p>

<blockquote>
  <p>The experiments were repeated while inhibiting specialization of first the scheme with the most expansions, and then the two most expanded schemata.
  Measures of coverage and speedup are important 1 As long as we are interested in preserving the f-structure assigned to sentences, this notion of coverage is stricter than necessary.
  The same f-structure can in fact be assigned by more than one parse, so that in some cases a sentence is considered out of coverage even if the specialized grammar assigns to it the correct f-structure.
  <strong>2'VPv' and 'VPverb[main]' cover VPs headed by a main verb.
  'NPadj' covers NPs with adjectives attached.
  205 The original rule: l/Pperfp --+ ADVP* SE (t ADJUNCT) ($ ADV_TYPE) = t,padv ~/r { @M_Head_Perfp I@M_Head_Passp } @( Anaph_Ctrl $) { AD VP+ SE ('~ ADJUNCT) ($ ADV_TYPE) = vpadv is replaced by the following: ADVP,[.E (~ ADJUNCT) (.l.
  ADV_TYPE) = vpadv l/'Pperfp --+ @PPadjunct @PPcase_obl {@M.Head_Pevfp [@M..Head_Passp} @( Anaph_Ctrl ~ ) V { @M_Head_Perfp I@M_Head_Passp } @( Anaph_Ctrl ~) Figure 1: The pruning of a rule from the actual French grammar.</strong>
  The ""*"" and the ""+"" signs have the usual interpretation as in regular expressions.
  A sub-expression enclosed in parenthesis is optional.
  Alternative sub-expressions are enclosed in curly brackets and separated by the ""["" sign.
  An ""@"" followed by an identifier is a macro expansion operator, and is eventually replaced by further functional descriptions.
  <strong>Corpus --..
  ,, 0.1[ Disambiguated Treebank treebank Human expert Grammar specialization Specialized grammar Figure 2: The setting for our experiments on grammar specialization.
  indicators of what can be achieved with this form of grammar pruning.</strong>
  However, they could potentially be misleading, since failure times for uncovered sentences might be considerably lower than their sentences times, had they not been out of coverage.</p>
</blockquote>

<p>Hard one:</p>

<blockquote>
  <p>Table 4 summarizes the precision results for both English and Romanian coreference.
  The results indicate that the English coreference is more indicate than the Romanian coreference, but SNIZZLE improves coreference resolution in both languages.
  There were 64% cases when the English coreference was resolved by a heuristic with higher priority than the corresponding heuristic for the Romanian counterpart.
  This result explains why there is better precision enhancement for 
  <strong>English Romanian SWIZZLE on English SWIZZLE on Romanian Nominal Pronominal 73% 89% 66% 78% 76% 93% 71°/o 82% Table 4: Coreference precision Total 84% 72% 87% 76% English Romanian SWIZZLE on English SWIZZLE on Romanian Nominal 69% 63% 66% 61% Pronominal Total 89% 78% 83% 72% 87% 77% 80% 70% Table 5: Coreference recall</strong> the English coreference. Table 5 also illustrates the recall results.
  The advantage of the data-driven coreference resolution over other methods is based on its better recall performance.
  This is explained by the fact that this method captures a larger variety of coreference patterns.
  Even though other coreference resolution systems perform better for some specific forms of systems, their recall results are surpassed by the systems approach.
  Multilingual coreference in turn improves more the precision than the recall of the monolingual data-driven coreference systems.
  In addition, Table 5 shows that the English coref- erence results in better recall than Romanian coref- erence.
  However, the recall shows a decrease for both languages for SNIZZLE because imprecise coreference links are deleted.
  As is usually the case, deleting data lowers the recall.
  All results were obtained by using the automatic scorer program developed for the MUC evaluations.</p>
</blockquote>

<p>Note how the table does not contain strange characters and goes right in the middle of the sentence: ""This result explains why there is better precision enhancement for -TABLE HERE- the English coreference."" I can't know where the table will be in regard to the running text. It may occur before a sentence, after it or within it like in this case. Also note that the table shit does not end with a full stop (most captions in papers don't...) so I can't rely on punctuation to spot it. I am happy with non-accurate boundaries of course, but I still need to do something with these tables. Some of them contain words rather than numbers, and I don't have enough information in those cases: no junky characters, nothing. It is obvious to only humans :S</p>
","language-agnostic, nlp, stanford-nlp","<p>(I hate crappy copy&amp;pastes. )</p>

<p>Few ideas that you might find helpful (I used each and every one of them myself in that point or another)</p>

<ol>
<li><p>(Very brute force) : Using a tokenizer and a dictionary (real dictionary, not the data structure) - parse the words out and any word which is not a dictionary word - remove it. It might prove problematic if your text contains a lot of company/products names - but this too can be solved using the correct indexes (there are a few on the web - I'm using some propriety ones so I can't share them, sorry) </p></li>
<li><p>Given a set of clean documents (lets say a 2K), build an tf/idf index of them, and use this as a dictionary - every term from the other documents that doesn't appear in the index (or appears with a very low tf/idf) - remove it. This should give you a rather clean document.</p></li>
<li><p>Use Amazon's mechanical turk mechanism : set up a task where the person reading the document needs to mark the paragraph that doesn't make sense. Should be rather easy for the mechanical turk platform (16.5K is not that much) - this will probably cost you a couple of hundred $ , but you'll probably get a rather nice cleanup of the text (So if it's on corporate money, that can be your way out - they need to pay for their mistakes :) ).</p></li>
<li><p>Considering your documents are from the same domain (same topics, all in all), and the problems are quite the same (same table headers, roughly same formulas): Break all the documents to sentences, and try clustering the sentences using ML. If the table headers / formulas are relatively similar, they should cluster nicely away from the rest of the sentences, and then you can clean the documents sentence-by-sentence (Get a document, break it to sentences,  for each sentence, if it's part of the ""weird"" cluster, remove it)</p></li>
</ol>
",2,9,763,2012-05-02 14:42:31,https://stackoverflow.com/questions/10416077/clean-text-coming-from-pdfs
Get certain nodes out of a Parse Tree,"<p>I am working on a project involving anaphora resolution via Hobbs algorithm. I have parsed my text using the Stanford parser, and now I would like to manipulate the nodes in order to implement my algorithm.</p>

<p>At the moment, I don't understand how to:</p>

<ul>
<li><p>Access a node based on its POS tag (e.g. I need to start with a pronoun - how do I get all pronouns?).</p></li>
<li><p>Use visitors. I'm a bit of a noob of Java, but in C++ I needed to implement a Visitor functor and then work on its hooks. I could not find much for the Stanford Parser's Tree structure though. Is that jgrapht? If it is, could you provide me with some pointers at code snippets?</p></li>
</ul>
","java, nlp, stanford-nlp, jgrapht","<p>@dhg's answer works fine, but here are two other options that it might also be useful to know about:</p>

<ul>
<li><p>The <code>Tree</code> class implements <code>Iterable</code>.  You can iterate through all the nodes of a <code>Tree</code>, or, strictly, the subtrees headed by each node, in a pre-order traversal, with:</p>

<pre><code>for (Tree subtree : t) { 
    if (subtree.label().value().equals(""PRP"")) {
        pronouns.add(subtree);
    }
}
</code></pre></li>
<li><p>You can also get just nodes that satisfy some (potentially quite complex pattern) by using <code>tregex</code>, which behaves rather like <code>java.util.regex</code> by allowing pattern matches over trees.  You would have something like:</p>

<pre><code>TregexPattern tgrepPattern = TregexPattern.compile(""PRP"");
TregexMatcher m = tgrepPattern.matcher(t);
while (m.find()) {
    Tree subtree = m.getMatch();
    pronouns.add(subtree);
}
</code></pre></li>
</ul>
",10,3,6010,2012-05-06 22:28:56,https://stackoverflow.com/questions/10474827/get-certain-nodes-out-of-a-parse-tree
Stanford parser- tagging with financial instruments,"<p>I have set of financial documents (Fixed terms deposit documents, Credit card documents). I want to automatically identify and tag financial entities/instruments in those documents. </p>

<p>For example if the document contains this phrase “reserves the right to repay with interest without notice”. I want to identify financial term related to it, and tag with it, for this sentence it is “Callable”.
For this phrase “permit premature withdrawal” the related financial term is “Putable”, so if this phrase is in the documents I want to tag it with term “Putable”.</p>

<p>The financial terms will come from, <a href=""http://www.hypercube.co.uk/edmcouncil/"" rel=""nofollow"">Financial Industry Business Ontology</a>.
Is there any possibility of using Stanford parser for this purpose?  Can I use POS tagger for this purpose?
I may have to train the Stanford parser with financial instruments, If it is possible how can I train the Stanford parser to identify financial instruments?</p>
","java, nlp, machine-learning, finance, stanford-nlp","<p>A parser or part of speech tagger out of the box will not identify domain specific concepts such as these.  However, the natural language analysis they provide may be useful building blocks for a solution. Or if the phrases you need to identify are near enough to fixed phrases, they may be unnecessary and you should concentrate on finding the fixed phrases and classifying them. </p>

<p>While these are not ""named entities"", the problem is closer to named entity recognition, in that you are recognizing semantic phrase classes. You could either annotate examples of the phrases you wish to find and train a model with a named entity recognizer (e.g., Stanford NER) or write rules that match instances (using something like ANNIE in GATE or Stanford's TokensRegexPattern.</p>
",7,1,893,2012-05-09 16:25:35,https://stackoverflow.com/questions/10520196/stanford-parser-tagging-with-financial-instruments
Resolve coreference using Stanford CoreNLP - unable to load parser model,"<p>I want to do a very simple job: given a string containing pronouns, I want to resolve them.</p>

<p>for example, I want to turn the sentence ""Mary has a little lamb. She is cute."" in ""Mary has a little lamb. Mary is cute."".</p>

<p>I have tried to use Stanford CoreNLP. However, I seem unable to get the parser to start. I have imported all the included jars in my project using Eclipse, and I have allocated 3GB to the JVM (-Xmx3g).</p>

<p>The error is very awkward:</p>

<blockquote>
  <p>Exception in thread ""main"" java.lang.NoSuchMethodError:
  edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(Ljava/lang/String;[Ljava/lang/String;)Ledu/stanford/nlp/parser/lexparser/LexicalizedParser;</p>
</blockquote>

<p>I don't understand where that L comes from, I think it is the root of my problem... This is rather weird. I have tried to get inside the source files, but there is no wrong reference there.</p>

<p>Code:</p>

<pre><code>import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefGraphAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.IntTuple;
import edu.stanford.nlp.util.Pair;
import edu.stanford.nlp.util.Timing;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

import java.util.Properties;

public class Coref {

/**
 * @param args the command line arguments
 */
public static void main(String[] args) throws IOException, ClassNotFoundException {
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = ""Mary has a little lamb. She is very cute.""; // Add your text here!

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);       
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);
      System.out.println(tree);

      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map&lt;Integer, CorefChain&gt; graph = 
      document.get(CorefChainAnnotation.class);
    System.out.println(graph);
  }
}
</code></pre>

<p>Full stack trace:</p>

<blockquote>
  <p>Adding annotator tokenize
  Adding annotator ssplit
  Adding annotator pos
  Loading POS Model [edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger] ... Loading default properties from trained tagger edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger
  Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.1 sec].
  done [2.2 sec].
  Adding annotator lemma
  Adding annotator ner
  Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [4.0 sec].
  Loading classifier from edu/stanford/nlp/models/ner/english.muc.distsim.crf.ser.gz ... done [3.0 sec].
  Loading classifier from edu/stanford/nlp/models/ner/english.conll.distsim.crf.ser.gz ... done [3.3 sec].
  Adding annotator parse
  Exception in thread ""main"" java.lang.NoSuchMethodError: edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(Ljava/lang/String;[Ljava/lang/String;)Ledu/stanford/nlp/parser/lexparser/LexicalizedParser;
      at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:115)
      at edu.stanford.nlp.pipeline.ParserAnnotator.(ParserAnnotator.java:64)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP$12.create(StanfordCoreNLP.java:603)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP$12.create(StanfordCoreNLP.java:585)
      at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:329)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:196)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:186)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:178)
      at Coref.main(Coref.java:41)</p>
</blockquote>
","java, nlp, stanford-nlp","<p>Yes, the L is just a bizarre Sun thing from ever since Java 1.0.</p>

<p><code>LexicalizedParser.loadModel(String, String ...)</code> is a new method added to the parser, which is not being found. I suspect this means that you have another version of the parser in your classpath which is being used instead.</p>

<p>Try this: at the shell outside of any IDE, give these commands (giving the path to stanford-corenlp appropriately, and changing : to ; if on Windows:</p>

<pre><code>javac -cp "".:stanford-corenlp-2012-04-09/*"" Coref.java
java -mx3g -cp "".:stanford-corenlp-2012-04-09/*"" Coref
</code></pre>

<p>The parser loads and your code runs correctly for me - just need to add some print statements so you can see what it has done :-).</p>
",9,9,7785,2012-05-21 16:10:59,https://stackoverflow.com/questions/10688739/resolve-coreference-using-stanford-corenlp-unable-to-load-parser-model
"How to calculate probabilities from confusion matrices? need denominator, chars matrices","<p><a href=""http://acl.ldc.upenn.edu/C/C90/C90-2036.pdf"" rel=""noreferrer"">This paper</a> contains confusion matrices for spelling errors in a noisy channel. It describes how to correct the errors based on conditional properties.</p>

<p>The conditional probability computation is on page 2, left column. In footnote 4, page 2, left column, the authors say: ""The chars matrices  can  be   easily  replicated, and are therefore omitted from the appendix."" I cannot figure out how can they be replicated!</p>

<p><strong>How to replicate them? Do I need the original corpus? or, did the authors mean they could be recomputed from the material in the paper itself?</strong></p>
","nlp, machine-learning, stanford-nlp, opennlp, confusion-matrix","<p>Looking at the paper, you just need to calculate them using a corpus, either the same one or one relevant to your application.</p>

<p>In replicating the matrices, note that they implicitly define two different <code>chars</code> matrices: a vector and an n-by-n matrix. For each character <code>x</code>, the vector <code>chars</code> contains a count of the number of times the character <code>x</code> occurred in the corpus. For each character sequence <code>xy</code>, the matrix <code>chars</code> contains a count of the number of times that sequence occurred in the corpus.</p>

<p><code>chars[x]</code> represents a look-up of <code>x</code> in the vector; <code>chars[x,y]</code> represents a look-up of the sequence <code>xy</code> in the matrix. Note that <code>chars[x]</code> = the sum over <code>chars[x,y]</code> for each value of <code>y</code>.</p>

<p>Note that their counts are all based on the 1988 AP Newswire corpus (<a href=""http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC93T3A"" rel=""nofollow"">available from the LDC</a>). If you can't use their exact corpus, I don't think it would be unreasonable to use another text from the same genre (i.e. another newswire corpus) and scale your counts such that they fit the original data. That is, the frequency of a given character shouldn't vary too much from one text to another if they're similar enough, so if you've got a corpus of 22 million words of newswire, you could count characters in that text and then double them to approximate their original counts.</p>
",3,5,2014,2012-05-22 19:33:16,https://stackoverflow.com/questions/10708852/how-to-calculate-probabilities-from-confusion-matrices-need-denominator-chars
Use POS tags in typed dependencies with Stanford Parser,"<p>I have a set of sentences. For each of them I want to obtain a ""generalized"" typed dependencies graph in which every word is replaced by the corresponding POS tag (except verbs and some keywords like <em>LOCATION</em> and <em>TARGET</em>).</p>

<p>For example, from the sentence</p>

<pre><code>take a left turn till you come to a LOCATION 
</code></pre>

<p>I want to obtain</p>

<pre><code>amod(take,JJ)dobj(take,NN)mark(come,IN)nsubj(come,PP)prep_to(come,LOCATION)
</code></pre>

<p>What is the simpler way to do this?</p>
","java, stanford-nlp","<p>Ok. I solved this problem with a bit of tree manipulation.</p>

<pre><code>public static void exploreTree(Tree t) {
    List&lt;Tree&gt; child = t.getChildrenAsList();
    Tree terminal;
    for (Tree c : child) {
        if (c.isPreTerminal()) {
            terminal = c.getChild(0);
            String t_value = terminal.value();
            String c_value = c.value();
            if (!c_value.startsWith(""VB"")) {
                if (!t_value.equals(""LOCATION"") &amp;&amp; !t_value.equals(""TARGET"")) {
                    terminal.setValue(c.value());
                }
            }
            // test[1].setValue(test[0].value());
        } else
            exploreTree(c);
    }
}
</code></pre>

<p>It change internal data of Stanford Parser tree according my request. Maybe is not the most elegant and robust solution but works.</p>
",0,0,459,2012-05-28 10:55:39,https://stackoverflow.com/questions/10783461/use-pos-tags-in-typed-dependencies-with-stanford-parser
get contents from array and show in UILabel,"<p>I am going through the iOS development videos by Stanford and am doing the assignment 1.</p>

<p>I am having a problem with task 4:</p>

<blockquote>
  <p>Add a new text label (UILabel) to your user-interface which shows everything that
  has been sent to the brain (separated by spaces). For example, if the user has entered
  6.3 Enter 5 + 2 *, this new text label would show 6.3 5 + 2 *. A good place
  to put this label is to make it a thin strip above the display text label. Don’t forget to
  have the C button clear this too. All of the code for this task should be in your
  Controller (no changes to your Model are required for this one). You do not have to
  display an unlimited number of operations and operands, just a reasonable amount.</p>
</blockquote>

<p>I have the UILabel and have done all of them. I am finding it hard on how I can do this. Any help is greatly appreciated.</p>

<p>(P.S. Please don't post a full code solution as I want to learn how to do it and posting code would just render my learning pointless, any hints and pointers would be the best! Thanks).</p>
","ios, objective-c, nsmutablearray, stanford-nlp","<p>You should convert your array into a string, which you can then assign to the label.</p>

<pre><code>label.text = [array componentsJoinedByString:@"" ""];
</code></pre>
",5,1,2797,2012-05-29 21:05:00,https://stackoverflow.com/questions/10806696/get-contents-from-array-and-show-in-uilabel
When does setter method get called from UIView subclass,"<p>I am taking the free Stanford course on iTunesU(193P) and we created setting up a class that is a subclass of UIView and created a public property called scale. The idea was that when we pinch, the scale of the view is changed accordingly but I am confused about when the setter of the property scale gets called. here is the relevant code below:</p>

<pre><code>@interface FaceView : UIView

@property (nonatomic) CGFloat scale; //anyone who wants do publicly can set my scale

-(void)pinch:(UIPinchGestureRecognizer *)gesture;

@end
</code></pre>



<pre><code>@synthesize scale = _scale;

#define DEFAULT_SCALE 0.90

-(CGFloat)scale{
    if(!_scale){
        return DEFAULT_SCALE;
    }else {
        return _scale;
    }
}

-(void)setScale:(CGFloat)scale{
    NSLog(@""setting the scale"");
    if(scale != _scale){
        _scale = scale;
        [self setNeedsDisplay];
    }
}

-(void)pinch:(UIPinchGestureRecognizer *)gesture{

    if ( (gesture.state == UIGestureRecognizerStateChanged) || (gesture.state == UIGestureRecognizerStateEnded)){

        self.scale *= gesture.scale;
        gesture.scale = 1;
    }
}
</code></pre>

<p>When I am in ""pinch mode"" the setScale method continues to be called as I am pinching as my NSLog statement prints out until I stop the pinch. When or how does the setScale method continued to be called when there isn't any code programmatically calling it? Perhaps I missed something along the way here.</p>
","objective-c, uiview, stanford-nlp, cs193p","<p>@cspam, remember that to set the gesture recognizer is 2 steps:<br>
1) Adding a gesture recognizer to UIView - This kind of confused me in the lecture but eventhough he is saying add a gesture recognizer to UIView, he really means add a gesture recognizer FOR UIView, IN UIViewController. That is the code that you are missing that you have to add in the UIViewController subclass in this case (the Faceviewcontroller - your name might be different) and that is what will keep calling your pinch method in FaceView above:</p>

<pre><code> UIPinchGestureRecognizer *pinchGesture=[[UIPinchGestureRecognizer alloc]initWithTarget:self.faceView action:@selector(pinch)];  
[self.faceView addGestureRecognizer:pinchGesture];
</code></pre>

<p>You would add this code in your UIViewController (subclass) in the setter method of your UIView [in other words, create and connect an IBOutlet property in your UIViewController to your UIView in the storyboard] and override the setter method to include the code above.</p>

<p>2) The second part is what you have in your code. So pinch method will be called everytime the controller senses a pinch gesture.</p>

<p>Hope this helps.</p>
",0,1,253,2012-06-04 17:50:01,https://stackoverflow.com/questions/10885744/when-does-setter-method-get-called-from-uiview-subclass
Unconventional named-entity recognition,"<p>I'm trying to design a somewhat unconventional NER system that marks certain multiword strings as single units/tokens. </p>

<p>There are a lot of cool NER tools out there, but I have a few special needs that make it pretty much impossible to use something straight out of the box:</p>

<p>First, the entities can't just be extracted and printed out in a list--they need to be marked in some way and consolidated into tokens.</p>

<p>Second, categorization is not important--Person/Organization/Location doesn't matter (at least in the output).</p>

<p>Third, these aren't just your typical ENAMEX named entities we're looking for.  We want companies and organizations, but also concepts like 'climate change' and 'gay marriage.'  I've seen tags like these on some tools out there, but all of them were 'extraction-style'.</p>

<p>How would I got about getting this type of functionality?  Would training the Stanford tagger on my own, hand-annotated dataset do the job (where 'climate change'-esque phrases are labeled MISC or something)?  Or am I better off just making a shortlist of the 'weird' entities and checking the text against that after it's been run through a regular NER system?</p>

<p>Thanks so much!</p>
","python, nlp, nltk, stanford-nlp","<p>The underlying CRF model of a named entity tagger such as Stanford NER can actually be used to recognize anything, not just named entities. There are certainly people who have used them quite successfully to pick out various kinds of terminological phrases. The software can certainly give you marked up token sequences in context.</p>

<p>There is, however, a choice as to whether to approach this in a ""more unsupervised"" way, where something like NP chunking and collocation statistics are used, or the fully supervised way of a straightforward CRF, where you're providing lots of annotated data of the kind of phrases you'd like to get out.</p>
",3,2,1576,2012-06-20 19:48:52,https://stackoverflow.com/questions/11127088/unconventional-named-entity-recognition
Good Examples: English Parsing / Natural Language Processing,"<p>I would like to make a calendar application that accepts plain english input better than those that exist. I have found Stanford's NLP which seems cool, but I was wondering if it's helpful to this kind of task. I can't find examples of people using it for anything. Should an app actually understand the language?  It seems like the natural english calendars that exist are looking for keywords / patterns and trying to parse that way, but I think an app could do better than that.</p>

<p>My real question: Could someone tell me how to find examples of people using the NLP or a different (publicly available) english parser to make a really useful app?</p>
","parsing, nlp, stanford-nlp","<p>A couple years on there is significant emergent technology in NLP around NodeJS. See here for more of an overview of the situation: <a href=""http://www.quora.com/Are-there-any-JavaScript-natural-language-processing-projects"" rel=""nofollow"">http://www.quora.com/Are-there-any-JavaScript-natural-language-processing-projects</a></p>

<p>But, here is the example to the +1 question, because I too was looking for the same question... just a few years later.</p>

<p>Working Example of NLP... in JavaScript?</p>

<p>Here was my answer...</p>

<p>Steps 1 - Boilerplate Node Server:</p>

<pre><code>install npm

npm install nodebootstrap

nodebootstrap naturalNode

cd naturalNode &amp;&amp; npm install

node app
</code></pre>

<p>//This should give you a node bootstrap app running at localhost:3000</p>

<p>For full info on easy Node server setup go here: <a href=""https://github.com/stonebk/nodeboilerplate"" rel=""nofollow"">https://github.com/stonebk/nodeboilerplate</a></p>

<p>STEP 2 - Include Natural Library: </p>

<p>Head to GitHub Natural Library to check out what it can do...</p>

<p><a href=""https://github.com/NaturalNode/natural"" rel=""nofollow"">https://github.com/NaturalNode/natural</a></p>

<p>Run: </p>

<pre><code>npm install natural 
</code></pre>

<p>(within your bootstrap server named naturalNode)</p>

<p>STEP 3 - Run an Example:</p>

<p>Include the example code from the link above into the app.js bootstrap file. </p>

<pre><code>var natural = require('natural'),
  tokenizer = new natural.WordTokenizer();
console.log(tokenizer.tokenize(""your dog has fleas.""));
// [ 'your', 'dog', 'has', 'fleas' ]
</code></pre>

<p>Now when you run your server you have full access to the natural library, and the ability to extend it with front-end interface.</p>

<p>Let me know if any instruction is missing... </p>
",1,5,6413,2012-06-21 23:45:32,https://stackoverflow.com/questions/11148405/good-examples-english-parsing-natural-language-processing
How to print the parse tree of Stanford JavaNLP,"<p>I am trying to get all the noun phrases using the <code>edu.stanford.nlp.*</code> package. I got all the subtrees of label value ""NP"", but I am not able to get the normal original <code>String</code> format (not Penn Tree format). </p>

<p>E.g. for the <code>subtree.toString()</code> gives <code>(NP (ND all)(NSS times)))</code> but I want the string ""all times"". Can anyone please help me. Thanks in advance.</p>
","java, nlp, stanford-nlp","<p>I believe what you want is something like:</p>

<pre><code>final StringBuilder sb = new StringBuilder();

for ( final Tree t : tree.getLeaves() ) {
     sb.append(t.toString()).append("" "");
}
</code></pre>

<p>While I'm not 100% sure, I seem to recall this being the solution used for some software I worked on a few years back.</p>
",2,0,2000,2012-06-22 00:52:34,https://stackoverflow.com/questions/11148890/how-to-print-the-parse-tree-of-stanford-javanlp
Fast double-valued priority queue implementation in Java,"<p>I am looking for an implementation of a priority queue that uses constant <code>double</code> values as priorities for the key. I believe that this, if implemented properly, can be faster than the default <code>PriorityQueue</code> implementation with a flexible comparator. A decreaseKey operation (=decreasing the priority of an element already in the queue) is not necessary.</p>

<p>I have found an <a href=""http://www.jarvana.com/jarvana/view/edu/stanford/nlp/stanford-corenlp/1.3.0/stanford-corenlp-1.3.0-javadoc.jar!/javadoc/edu/stanford/nlp/util/PriorityQueue.html"" rel=""nofollow"">implementation from the NLP group in Stanford</a>, but they claim that it's twice as <em>slow</em> as the original implementation. Is there a PQ implementation out there that can outperform the default <code>PriorityQueue</code> for our use case?</p>
","java, performance, algorithm, priority-queue, stanford-nlp","<p>We ended up implementing our own heap structure. A d-ary heap with an optimized <code>remove()</code> operation turned out to perform reasonably well, especially on a loaded shared-memory machine.</p>
",0,1,805,2012-06-26 08:55:58,https://stackoverflow.com/questions/11203875/fast-double-valued-priority-queue-implementation-in-java
List of HashMaps Java,"<pre><code>Map&lt;String, List&lt;String&gt;&gt; words = new HashMap&lt;String, List&lt;String&gt;&gt;();
            List&lt;Map&gt; listOfHash = new ArrayList&lt;Map&gt;();

            for (int temp = 0; temp &lt; nList.getLength(); temp++) {
                Node nNode = nList.item(temp);
                if (nNode.getNodeType() == Node.ELEMENT_NODE) {
                    Element eElement = (Element) nNode;
                    String word = getTagValue(""word"", eElement);
                    List&lt;String&gt; add_word = new ArrayList&lt;String&gt;();
                    String pos = getTagValue(""POS"", eElement);
                    if(words.get(pos)!=null){
                        add_word.addAll(words.get(pos));
                        add_word.add(word);
                    }
                    else{
                        add_word.add(word);
                    }
                    words.put(pos, add_word);
                }
            }
</code></pre>

<p>This is a segment of code I have written (it uses Stanford CoreNLP). The problem I am facing is that presently this code is working only with one Map i.e. ""words"". Now, I want that as soon as the parser sees ""000000000"" which is my delimiter, then a new Map should be added to the List and then the keys and values to be inserted into it. If it does not see ""000000000"", then the Keys and Values are to be added in the same Map.
Please help me with that as I am not able to do it even after a lot of efforts.</p>
","java, list, hashmap, stanford-nlp","<p>I guess listOfHash is to contain all your Maps...</p>

<p>So rename <code>words</code> to <code>currentMap</code> for example and add to it. When you see ""000000000"" instantiate a new Map, assign it to <code>currentMap</code>, add it to the list and go on...</p>

<p>something like:</p>

<pre><code>if (""000000000"".equals(word)){
    currentMap = new HashMap&lt;String, List&lt;String&gt;&gt;();
    listOfHash.add(currentMap);
    continue; // if we wan't to skip the insertion of ""000000000""
}
</code></pre>

<p>And don't forget to add your initial Map to listOfHash.</p>

<p>I also see you have other problems with your code, here is modified version (I have not tried to compile it):</p>

<pre><code>Map&lt;String, List&lt;String&gt;&gt; currentMap = new HashMap&lt;String, List&lt;String&gt;&gt;();
List&lt;Map&gt; listOfHash = new ArrayList&lt;Map&gt;();
listOfHash.add(currentMap);


for (int temp = 0; temp &lt; nList.getLength(); temp++) {
    Node nNode = nList.item(temp);
    if (nNode.getNodeType() == Node.ELEMENT_NODE) {
        Element eElement = (Element) nNode;
        String word = getTagValue(""word"", eElement);    

        if (""000000000"".equals(word)){
            currentMap = new HashMap&lt;String, List&lt;String&gt;&gt;();
            listOfHash.add(currentMap);
            continue; // if we wan't to skip the insertion of ""000000000""
        }

        String pos = getTagValue(""POS"", eElement);

        List&lt;String&gt; add_word = currentMap.get(pos);
        if(add_word==null){
            add_word = new ArrayList&lt;String&gt;();
            currentMap.put(pos, add_word);
        }
        add_word.add(word);
    }

}
</code></pre>
",3,0,3892,2012-06-26 09:27:56,https://stackoverflow.com/questions/11204353/list-of-hashmaps-java
Stanford CoreNLP- Text to XML,"<p>Can somebody please provide me with the Java implementation of Stanford CoreNLP to convert a text file to XML file. The same thing that I can do with </p>

<pre><code>java -cp stanford-corenlp-2012-05-22.jar;stanford-corenlp-2012-05-22-models.jar;xom.jar;joda-time.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</code></pre>

<p>in command line.</p>
","java, stanford-nlp","<p>In java you might call it thus:</p>

<pre><code>import edu.stanford.nlp.pipeline.StanfordCoreNLP;
</code></pre>

<p>...</p>

<pre><code>StanfordCoreNLP.main(new String[] {
    ""-annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,dcoref"",
    ""-file"", ""input.txt"" });
</code></pre>

<p><em>(If this suffices)</em></p>
",2,1,2324,2012-07-03 08:02:16,https://stackoverflow.com/questions/11306772/stanford-corenlp-text-to-xml
How does instantiating an object in the getter work if the setter is used first?,"<p>I'm working through the Stanford lectures, calculator tutorial. <a href=""http://www.stanford.edu/class/cs193p/cgi-bin/drupal/downloads-2011-fall"" rel=""nofollow"">http://www.stanford.edu/class/cs193p/cgi-bin/drupal/downloads-2011-fall</a></p>

<p>In it he suggests a good technique for creating an instance of a model is to alloc/init in the getter with:</p>

<pre><code>- (NSMutableArray *)operandStack
{
    if(!_operandStack) {
        _operandStack = [[NSMutableArray alloc] init];
    }
    return _operandStack;
}
</code></pre>

<p>However, the first time <code>[operandStack]</code> is used is:</p>

<pre><code> [self.operandStack addObject:operandObject];
</code></pre>

<p>Which, I understand, is using the setter.</p>

<p>I can see that it obviously works (it runs) - but I'm at a loss understanding why if no-ones tried to get anything from <code>operandStack</code> yet. Could someone please enlighten me, I've not had any luck with any searches.</p>
","initialization, calculator, getter, stanford-nlp, lazy-initialization","<p>Your misunderstanding seems to stem from the idea that <code>[self.operandStack addObject:operandObject];</code> is a ""setter"" operation. To translate that expression to english:""Send the 'addObject' message to the NSMutableArray returned by the method operandStack, passing it the operandObject object."" </p>

<p>This seems to be a dot notation confusion. Dot notation simply resolves to basic function calls. So for example the above code excerpt could be written using the familiar square bracket notation. Like so:</p>

<pre><code>[[self operandStack] addObject:operandObject];
</code></pre>

<p>This form is still perfectly valid, and even preferred by some. In this form it makes it a bit clearer to see that you are actually calling the ""getter"" function.</p>
",0,0,127,2012-07-09 22:23:39,https://stackoverflow.com/questions/11404070/how-does-instantiating-an-object-in-the-getter-work-if-the-setter-is-used-first
"How to parse languages other than English with Stanford Parser？ in java, not command lines","<p>I have been trying to use Stanford Parser in my Java program to parse some sentences in Chinese. Since I am quite new at both Java and Stanford Parser, I used the 'ParseDemo.java' to practice. The code works fine with sentences in English and outputs the right result. However, when I changed the model to 'chinesePCFG.ser.gz' and tried to parse some segmented Chinese sentences, things went wrong.</p>

<p>Here's my code in Java</p>

<pre class=""lang-java prettyprint-override""><code>class ParserDemo {

  public static void main(String[] args) {
    LexicalizedParser lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz"");
    if (args.length &gt; 0) {
      demoDP(lp, args[0]);
    } else {
      demoAPI(lp);
    }
  }

  public static void demoDP(LexicalizedParser lp, String filename) {
    // This option shows loading and sentence-segment and tokenizing
    // a file using DocumentPreprocessor
    TreebankLanguagePack tlp = new PennTreebankLanguagePack();
    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
    // You could also create a tokenier here (as below) and pass it
    // to DocumentPreprocessor
    for (List&lt;HasWord&gt; sentence : new DocumentPreprocessor(filename)) {
      Tree parse = lp.apply(sentence);
      parse.pennPrint();
      System.out.println();

      GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
      Collection tdl = gs.typedDependenciesCCprocessed(true);
      System.out.println(tdl);
      System.out.println();
    }
  }

  public static void demoAPI(LexicalizedParser lp) {
    // This option shows parsing a list of correctly tokenized words
    String sent[] = { ""我"", ""是"", ""一名"", ""学生"" };
    List&lt;CoreLabel&gt; rawWords = Sentence.toCoreLabelList(sent);
    Tree parse = lp.apply(rawWords);
    parse.pennPrint();
    System.out.println();

    TreebankLanguagePack tlp = new PennTreebankLanguagePack();
    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
    GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
    List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();
    System.out.println(tdl);
    System.out.println();

    TreePrint tp = new TreePrint(""penn,typedDependenciesCollapsed"");
    tp.printTree(parse);
  }

  private ParserDemo() {} // static methods only
}
</code></pre>

<p>It's basically the same as ParserDemo.java, but when I run it I get the following result:</p>

<blockquote>
  <p>Loading parser from serialized file
  edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz ... done [2.2
  sec]. (ROOT   (IP
      (NP (PN 我))
      (VP (VC 是)
        (NP
          (QP (CD 一名))
          (NP (NN 学生))))))</p>
  
  <p>Exception in thread ""main"" java.lang.RuntimeException: Failed to
  invoke public
  edu.stanford.nlp.trees.EnglishGrammaticalStructure(edu.stanford.nlp.trees.Tree)
    at
  edu.stanford.nlp.trees.GrammaticalStructureFactory.newGrammaticalStructure(GrammaticalStructureFactory.java:104)
    at parserdemo.ParserDemo.demoAPI(ParserDemo.java:65)    at
  parserdemo.ParserDemo.main(ParserDemo.java:23) </p>
</blockquote>

<p>the code on line 65 is:</p>

<pre><code> GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
</code></pre>

<p>My guess is that chinesePCFG.ser.gz misses something relevant to 'edu.stanford.nlp.trees.EnglishGrammaticalStructure'. Since the parser parses Chinese correctly via commandlines, there must be something wrong with my own code. I have been searching, only to find few similar cases some of which mentioned about using the right model, but I don't really know how to modify the code to the 'right model'. Hope that someone could help me with it. I am a newbie on Java and Stanford Parser, so please be specific. Thank you! </p>
","java, nlp, stanford-nlp","<p>The problem is that the GrammaticalStructureFactory is constructed from a <code>PennTreebankLanguagePack</code>, which is for the English Penn Treebank.  You need to use (in two places)</p>

<pre><code>TreebankLanguagePack tlp = new ChineseTreebankLanguagePack();
</code></pre>

<p>and to import this appropriately</p>

<pre><code>import edu.stanford.nlp.trees.international.pennchinese.ChineseTreebankLanguagePack;
</code></pre>

<p>But we also generally recommend using the factored parser for Chinese (since it works considerably better, unlike for English, although at the cost of more memory and time usage)</p>

<pre><code>LexicalizedParser lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz"");
</code></pre>
",2,1,2563,2012-07-11 09:36:24,https://stackoverflow.com/questions/11429722/how-to-parse-languages-other-than-english-with-stanford-parser-in-java-not-com
Missing words in Stanford NLP dependency tree parser,"<p>I'm making an application using a dependency tree parser. Actually, the parser is this one: 
<a href=""http://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""nofollow"">Parser Stanford</a>, but it rarely change one or two letters of some words in a sentence that I want to parse. This is a big trouble for me, because I can't see any pattern in these changes and I need the dependency tree with the same words of my sentence. </p>

<p>All I can see is that just some words have these problems. I'm working with a tweets database. So, I have a lot of grammar mistakes in this data. For example the hashtag '#AllAmericanhumour  ' becomes AllAmericanhumor. It misses one letter(u).</p>

<p>Is there anything I can do to solve this problem? In my first view I thought using an edit distance algorithm, but I think that might be an easier way to do it. </p>

<p>Thanks everybody in advance</p>
","parsing, tree, stanford-nlp","<p>You can give options to the tokenizer with the -tokenize.options flag/property. For this particular normalization, you can turn it off with</p>

<pre><code>-tokenize.options americanize=false
</code></pre>

<p>There are also various other normalizations that you can turn off (see PTBTokenizer or <a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokenizer.shtml</a>.  You can turn off a lot with</p>

<pre><code>-tokenize.options ptb3Escaping=false
</code></pre>

<p>However, the parser is trained on data that looks like the output of <code>ptb3Escaping=true</code> and so will tend to degrade in performance if used with unnormalized tokens. So, you may want to consider alternative strategies.</p>

<p>If you're working at the Java level, you can look at the word tokens, which are actually Maps, and they have various keys.  OriginalTextAnnotation will give you the unnormalized token, even when it has been normalized. CharacterOffsetBeginAnnotation and CharacterOffsetEndAnnotation will map to character offsets into the text.</p>

<p>p.s. And you should accept some answers :-).</p>
",2,1,484,2012-07-27 02:52:10,https://stackoverflow.com/questions/11680825/missing-words-in-stanford-nlp-dependency-tree-parser
Is there a set of adjective word list for positive or negative polarity,"<p>I am working on sentiment analysis. I thought if there is any available set of adjectives indicating positive/negative(like for positive: good,awesome,amazing,) meaning? and the second thing is a set of data from which i can use as a test case.</p>
","nlp, stanford-nlp, sentiment-analysis","<p>Resources related to polarity:</p>

<p>SentiWordNet: <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""noreferrer"">http://sentiwordnet.isti.cnr.it/</a></p>

<p>Inquirer Dictionary: <a href=""http://www.wjh.harvard.edu/~inquirer/homecat.htm"" rel=""noreferrer"">http://www.wjh.harvard.edu/~inquirer/homecat.htm</a></p>

<p>Possible test data: <a href=""http://www.cs.pitt.edu/mpqa/"" rel=""noreferrer"">http://www.cs.pitt.edu/mpqa/</a></p>
",13,5,11215,2012-08-03 16:49:11,https://stackoverflow.com/questions/11799971/is-there-a-set-of-adjective-word-list-for-positive-or-negative-polarity
stanford core nlp java output,"<p>I'm a newbie with Java and Stanford NLP toolkit and trying to use them for a project. Specifically, I'm trying to use Stanford Corenlp toolkit to annotate a text (with Netbeans and not command line) and I tried to use the code provided on <a href=""http://nlp.stanford.edu/software/corenlp.shtml#Usage"" rel=""noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml#Usage</a> (Using the Stanford CoreNLP API).. question is: can anybody tell me how I can get the output in a file so that I can further process it?</p>

<p>I've tried printing the graphs and the sentence to the console, just to see the content. That works. Basically what I'd need is to return the annotated document, so that I can call it from my main class and output a text file (if that's possible). I'm trying to look in the API of stanford corenlp, but I don't really know what is the best way to return such kind of information, given my lack of experience.</p>

<p>Here is the code:</p>

<pre><code>Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = ""the quick fox jumps over the lazy dog"";

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);       
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);

      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map&lt;Integer, CorefChain&gt; graph = 
      document.get(CorefChainAnnotation.class);
</code></pre>
","java, nlp, stanford-nlp","<p>Once you have any or all of the natural language analyses shown in your code example, all you need to do is send them to a file in the normal Java fashion, e.g., with a FileWriter for text format output.  Concretely, here's a simple complete example that shows output sent to files (if you give it appropriate command-line arguments):</p>

<pre><code>import java.io.*;
import java.util.*;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

public class StanfordCoreNlpDemo {

  public static void main(String[] args) throws IOException {
    PrintWriter out;
    if (args.length &gt; 1) {
      out = new PrintWriter(args[1]);
    } else {
      out = new PrintWriter(System.out);
    }
    PrintWriter xmlOut = null;
    if (args.length &gt; 2) {
      xmlOut = new PrintWriter(args[2]);
    }

    StanfordCoreNLP pipeline = new StanfordCoreNLP();
    Annotation annotation;
    if (args.length &gt; 0) {
      annotation = new Annotation(IOUtils.slurpFileNoExceptions(args[0]));
    } else {
      annotation = new Annotation(""Kosgi Santosh sent an email to Stanford University. He didn't get a reply."");
    }

    pipeline.annotate(annotation);
    pipeline.prettyPrint(annotation, out);
    if (xmlOut != null) {
      pipeline.xmlPrint(annotation, xmlOut);
    }
    // An Annotation is a Map and you can get and use the various analyses individually.
    // For instance, this gets the parse tree of the first sentence in the text.
    List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
    if (sentences != null &amp;&amp; sentences.size() &gt; 0) {
      CoreMap sentence = sentences.get(0);
      Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
      out.println();
      out.println(""The first sentence parsed is:"");
      tree.pennPrint(out);
    }
  }

}
</code></pre>
",25,19,20707,2012-08-06 16:46:49,https://stackoverflow.com/questions/11832490/stanford-core-nlp-java-output
Stanford POS Tagger: How to preserve newlines in the output?,"<p>My input.txt file contains the following sample text:</p>

<p>you have to let's<br>
come and see me.</p>

<p>Now if I invoke the Stanford POS tagger with the default command:</p>

<pre><code>java -classpath stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/wsj-0-18-bidirectional-distsim.tagger -textFile input.txt &gt; output.txt
</code></pre>

<p>I get the following in my output.txt file:</p>

<pre><code>you_PRP have_VBP to_TO let_VB 's_POS come_VB and_CC see_VB me_PRP ._.
</code></pre>

<p>The problem with the above output is that I have lost my original newline delimiter used in the input file.</p>

<p>Now, if I use the following command to preserve my newline sentence delimiter in the output file I have to set -tokenize option to false:</p>

<pre><code>java -classpath stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/wsj-0-18-bidirectional-distsim.tagger -sentenceDelimiter newline -tokenize false -textFile input.txt &gt; output.txt 
</code></pre>

<p>The problem with this code is that it totally messed up the output:</p>

<pre><code>you_PRP have_VBP to_TO let's_NNS  
come_VB and_CC see_VB me._NN
</code></pre>

<p>Here let's and me. are tagged inappropriately.</p>

<p>My question is how can I preserve the newline delimiters in the output file without messing up the tokenization?</p>
","java, text, nlp, stanford-nlp, pos-tagger","<p>The answer should have been to use the command:</p>

<pre><code>java -classpath stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/wsj-0-18-bidirectional-distsim.tagger -sentenceDelimiter newline -textFile input.txt &gt; output.txt 
</code></pre>

<p>But there was a bug and it didn't work (ignored the newlines) in version 3.1.3 (and perhaps all earlier versions). It will work in version 3.1.4+.</p>

<p>In the meantime, if the amount of text is small, you might try using the Stanford Parser (where the corresponding flag is named differently so it's <code>-sentences newline</code>).</p>
",1,0,1606,2012-08-27 10:57:35,https://stackoverflow.com/questions/12140683/stanford-pos-tagger-how-to-preserve-newlines-in-the-output
Java Heap Space error on Stanford NER using Netbeans,"<p>I am using Stanford NER to parse a sentence for getting following tags : tokenize,ssplit,pos,lemma,ner. I also increased memory in netbeans by Project->Properties->Run->VM Options to <code>-Xms1600M -Xmx1600M</code>. Still I am getting Java out of memory exception.
I am running 32-bit java, on windows 7 JDK version 1.7.
Here is my code </p>

<pre><code>public ArrayList&lt;String&gt; NERTokensRet(String string) {
    ArrayList&lt;String&gt; myArr = new ArrayList&lt;String&gt;();


    props = new Properties();
    props.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    pipeline = new StanfordCoreNLP(props);


    //     String resultString = string.replaceAll(""[^\\p{L}\\p{N}]"", "" "");   
    Annotation annotation = new Annotation(string);
    pipeline.annotate(annotation);
    int j;
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
        List&lt;CoreLabel&gt; tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);

        for (j = 0; j &lt; tokens.size(); j++) {
            CoreLabel token = tokens.get(j);
            myArr.add(""["" + token.originalText() + "","" + token.tag() + "","" + token.beginPosition() + "","" + token.endPosition() + ""]"");
        }
        //System.out.println(myArr);
    }

    return myArr;
}
</code></pre>

<p>StackTrace : </p>

<pre><code>Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
at java.lang.AbstractStringBuilder.&lt;init&gt;(AbstractStringBuilder.java:45)
at java.lang.StringBuilder.&lt;init&gt;(StringBuilder.java:68)
at java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:2998)
at java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:2819)
at java.io.ObjectInputStream.readString(ObjectInputStream.java:1598)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1319)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
at java.util.HashMap.readObject(HashMap.java:1030)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1848)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
at edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2255)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1444)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1421)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1500)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1487)
at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2386)
at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:130)
at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:116)
at edu.stanford.nlp.ie.ClassifierCombiner.&lt;init&gt;(ClassifierCombiner.java:98)
at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:64)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$6.create(StanfordCoreNLP.java:500)
</code></pre>

<p>Can some one please help as soon as possible?</p>
","java, nlp, netbeans-7, stanford-nlp","<p>I uninstalled everything(java and netbeans) and reinstalled everything(java and netbeans). It is still not able to assign -Xmx1400m but assigns -Xmx1000m and works well. Thank you everyone for your effort.</p>
",0,0,1587,2012-08-31 12:26:00,https://stackoverflow.com/questions/12215372/java-heap-space-error-on-stanford-ner-using-netbeans
Force serialization of unserializable Java object?,"<p>I am experimenting with the Stanford CoreNLP library, and I want to serialize the main StanfordCoreNLP pipeline object, even though it throws a java.io.NotSerializableException.</p>

<p>Full story:
Whenever I run my implementation, it takes ~15 seconds to load the pipeline annotators and classifiers into memory. The end process is about 600MB in memory (easily small enough to be stored in my case). I'd like to save this pipeline after creating it the first time, so I can just read it into memory afterwards.</p>

<p>However it throws a NotSerializableException. I tried making a trivial subclass that implements Serializable, but StanfordCoreNLP has annotator and classifier properties that don't implement this interface, and I can't make subclasses for all of them.</p>

<p>Is there any Java library that will serialize an object that doesn't implement Serializable? I suppose it would have to recurse through it's properties and do the same for any similar object.</p>

<p>The serialization code I tried:</p>

<pre><code>static StanfordCoreNLP pipeline;
static String file = ""/Users/ME/Desktop/pipeline.sav"";
    static StanfordCoreNLP pipeline() {
    if (pipeline == null) {
        try {
            FileInputStream saveFile = new FileInputStream(file);
            ObjectInputStream read = new ObjectInputStream(saveFile);
            pipeline = (StanfordCoreNLP) read.readObject();
            System.out.println(""Pipeline loaded from file."");
            read.close();
        } catch (FileNotFoundException e) {
            System.out.println(""Cached pipeline not found. Creating new pipeline..."");
            Properties props = new Properties();
            props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            pipeline = new StanfordCoreNLP(props);
            savePipeline(pipeline);
        } catch (IOException e) {
            System.err.println(e.getLocalizedMessage());
        } catch (Exception e) {
            System.err.println(e.getLocalizedMessage());
        }
    }
    return pipeline;
}

static void savePipeline(StanfordCoreNLP pipeline) {
    try {
        FileOutputStream saveFile = new FileOutputStream(file);
        ObjectOutputStream save = new ObjectOutputStream(saveFile);
        save.writeObject(pipeline);
        System.out.println(""Pipeline saved to file."");
        save.close();
    } catch (FileNotFoundException e) {
        System.out.println(""Pipeline file not found during save."");
    } catch (IOException e) {
        System.err.println(e.getLocalizedMessage());
    }
}
</code></pre>
","java, serialization, stanford-nlp","<p>In general, the Stanford NLP classes that represent data objects (Tree, LexicalizedParser, etc.) are serializable, while classes that represent processors (StanfordCoreNLP, LexicalizedParserQuery, CRFClassifier) are not. To achieve what you ask for, you'd need to make a lot of classes serializable, which aren't, and to deal with any ramifications of that.</p>

<p>However, I think you are mistaken in your underlying thinking. The things that StanfordCoreNLP is loading during those 15 seconds are mainly standard java serialized objects. The NER classifiers and the parser grammars are standard serialized java objects.  (A couple of things aren't of this form but just binary data, including for the POS tagger, largely for historical reasons.)  The fact is that loading a lot of objects with standard Java serialization is not that fast ... you can find discussions on the web of the speed of Java serialization and of how the speed of alternatives compare.  Making a new even larger serialized object which contains all the current serialized objects couldn't make it much quicker.  (You could potentially gain a fraction by having things all in one continuous data stream, but unless you do extra work marking transient fields that don't need to be serialized, you would almost surely lose from the increased size of the serialized data structures.)</p>

<p>Rather, I would suggest that the key to dealing with this problem is to pay the cost of loading the system only once, and then to keep it in memory while processing many sentences. </p>
",2,5,3303,2012-09-04 03:11:19,https://stackoverflow.com/questions/12256302/force-serialization-of-unserializable-java-object
Collision detection in Java ufo game,"<p>I have a simple Java game where you fire a bullet up at a moving target. Both objects are GRects. I have collision detection that checks when the ufo and bullet intersect each other, but here is the weirdness:</p>

<p>This works:</p>

<pre><code>private void collideWithUFO() { 
    if (bullet != null) {
        GObject collObj = getElementAt(ufo.getX(), ufo.getY()); 
        if (collObj == bullet) {
            remove(ufo); 
            remove(bullet); 
            ufo = null; 
            bullet = null;
        }
     }
}
</code></pre>

<p>..but if I change the <code>getElementAt</code> to bullet like below, and check with respect to ufo, it fails to detect collisions:</p>

<pre><code>private void collideWithUFO() { 
    if (bullet != null) {
        GObject collObj = getElementAt(bullet.getX(), bullet.getY()); 
        if (collObj == ufo) {
            remove(ufo); 
            remove(bullet); 
            ufo = null; 
            bullet = null;
        }
     }
 }
</code></pre>

<p>It should be irrelevant whether I choose ufo or bullet first, but apparently it isn't. Now here is something that's even WEIRDER. If I change the bullet from GRect to GOval, all of a sudden the second form of collision detection works. I am a Java noob so please let me know if this behavior makes any sense.</p>
","java, collision-detection, collision, stanford-nlp","<pre><code>public GObject getElementAt(double x, double y)

Returns the topmost graphical object that contains the point (x, y), or null if no such object exists.
</code></pre>

<p>So <code>getElementAt(bullet.getX(), bullet.getY())</code> will only give you the ufo if the ufo is the topmost graphical element, otherwise you will get the bullet.</p>
",3,0,642,2012-09-08 23:08:47,https://stackoverflow.com/questions/12335196/collision-detection-in-java-ufo-game
stanford Core NLP: Splitting sentences from text,"<p>I am new to stanford Core NLP. I would like to use it for splitting sentences from text in English, German,French. Which class does this work?Thanks in advance.</p>
","java, nlp, stanford-nlp, sentence","<p>For the lower level classes that handle this, you can look at the <a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""noreferrer"">tokenizer documentation</a>. At the CoreNLP level, you can just use the Annotator's ""tokenize,ssplit"".</p>
",8,5,11731,2012-09-10 17:58:47,https://stackoverflow.com/questions/12357066/stanford-core-nlp-splitting-sentences-from-text
extracting the text from output parse Tree,"<p>I am new to nlp, I am trying to use stanford parser to extract the (NP ) sentence from a text,  I want to retrieve the parts of the text where it's tagged (NP )</p>

<p>if a part is tagged (NP ) and a smaller part inside it is also tagged (NP ) I want to take the smaller part.</p>

<p>till now I managed to do what I wanted in the following method:</p>

<pre><code>private static ArrayList&lt;Tree&gt; extract(Tree t) 
{
    ArrayList&lt;Tree&gt; wanted = new ArrayList&lt;Tree&gt;();
   if (t.label().value().equals(""NP"") )
    {
       wanted.add(t);
        for (Tree child : t.children())
        {
            ArrayList&lt;Tree&gt; temp = new ArrayList&lt;Tree&gt;();
            temp=extract(child);
            if(temp.size()&gt;0)
            {
                int o=-1;
                o=wanted.indexOf(t);
                if(o!=-1)
                    wanted.remove(o);
            }
            wanted.addAll(temp);
        }
    }

    else
        for (Tree child : t.children())
            wanted.addAll(extract(child));
    return wanted;
}
</code></pre>

<p>The return type of this method is a list of trees, When I do the following:</p>

<pre><code>     LexicalizedParser parser = LexicalizedParser.loadModel();
        x = parser.apply(""Who owns club barcelona?"");
     outs=extract(x);
    for(int i=0;i&lt;outs.size();i++){System.out.println(""tree #""+i+"": ""+outs.get(i));}
</code></pre>

<p>is :</p>

<pre><code>tree #0: (NP (NN club) (NN barcelona))
</code></pre>

<p>I want the output to be <code>""club barcelona""</code> right away, without the tags, I tried the <code>.labels();</code> property and <code>.label().value();</code> they return the tags instead</p>
","java, nlp, stanford-nlp","<p>You can get a list of the words under a subtree tr with</p>

<pre><code>tr.yield()
</code></pre>

<p>You can convert that to just the String form with convenience methods in Sentence:</p>

<pre><code>Sentence.listToString(tr.yield())
</code></pre>

<p>You can just walk a tree as you're doing, but if you're going to do this kind of thing much, you might want to look at tregex which makes it easier to find particular nodes in trees via declarative patterns, such as NPs with no NP below them.  A neat way to do what you are looking for is this:</p>

<pre><code>Tree x = lp.apply(""Christopher Manning owns club barcelona?"");
TregexPattern NPpattern = TregexPattern.compile(""@NP !&lt;&lt; @NP"");
TregexMatcher matcher = NPpattern.matcher(x);
while (matcher.findNextMatchingNode()) {
  Tree match = matcher.getMatch();
  System.out.println(Sentence.listToString(match.yield()));
}
</code></pre>
",10,5,3722,2012-09-20 14:18:10,https://stackoverflow.com/questions/12514621/extracting-the-text-from-output-parse-tree
Proxem Stanford Parser asp MVC,"<p>I'm using Proxem wrapper for Stanford Parser and I'm facing problem with Parsing in ASP.NET MVC 3 and 4 application. It throws </p>

<pre><code>System.Reflection.TargetInvocationException: Exception has been thrown by the target of an      invocation. ---&gt; System.NullReferenceException: Object reference not set to an instance of an object.
 at Proxem.Antelope.Parsing.Sentence.a(List`1 A_0)
 at Proxem.Antelope.Parsing.Sentence..ctor(SerializationInfo info, StreamingContext ctxt)
--- End of inner exception stack trace ---
</code></pre>

<p>In WPF and console application it works fine.</p>
","asp.net-mvc, parsing, stanford-nlp","<p>I fixed it. Constructor Parser(string path) is invoking constructor <code>Parser(string path, int poolsize)</code> with poolsize <code>value = 1</code>. Using constructor <code>Parser(string path, int poolsize)</code> with values -> Parser(yourPath, 0) make it work with MVC and WCF.</p>
",0,-1,161,2012-10-02 12:54:20,https://stackoverflow.com/questions/12690665/proxem-stanford-parser-asp-mvc
Stanford Parser tags,"<p>I just started using Stanford Parser but I do not understand the tags very well. This might be a stupid question to ask but can anyone tell me what does the SBARQ and SQ tags represent and where can I find a complete list for them? I know how the Penn Treebank looks like but these are slightly different. </p>

<pre><code>Sentence: What is the highest waterfall in the United States ?

(ROOT
  (SBARQ
    (WHNP (WP What))
    (SQ (VBZ is)
      (NP
        (NP (DT the) (JJS highest) (NN waterfall))
        (PP (IN in)
          (NP (DT the) (NNP United) (NNPS States)))))
    (. ?)))
</code></pre>

<p>I have looked at Stanford Parser website and read a few of the journals listed there but there are no explanation of the tags mentioned earlier. I found a manual describing all the dependencies used but it doesn't explain what I am looking for. Thanks!</p>
",stanford-nlp,"<p><a href=""https://gist.github.com/nlothian/9240750"">This reference</a> looks to have an extensive list - not sure if it is complete or not.</p>

<p>Specifically, it lists the ones you're asking about as:</p>

<pre><code>SBARQ - Direct question introduced by a wh-word or a wh-phrase. Indirect 
        questions and relative clauses should be bracketed as SBAR, not SBARQ.
SQ    - Inverted yes/no question, or main clause of a wh-question, 
        following the wh-phrase in SBARQ.
</code></pre>
",31,20,11237,2012-10-23 09:57:14,https://stackoverflow.com/questions/13027908/stanford-parser-tags
App working in IOS 5.0 but not on IOS 5.1,"<p>I am trying to test an app from that I created by following an tutorial of Paul Hegarty from stanford university. </p>

<p>You can find a download link over <a href=""http://www.stanford.edu/class/cs193p/cgi-bin/drupal/system/files/sample_code/Photomania_0.zip"" rel=""nofollow"">here</a>. </p>

<p>When I test it on the IOS5.0 simulator, it works fine. But when I test it on the IOS 5.1 simulator. It doesn't do anything. Can anybody test it in their simulators and tell me if it's working?</p>

<p>You only need to download it, and fill this API-key. In FlickrAPI-key.h</p>

<pre><code>95f1f522e7332485662d0b1dfbba6544
</code></pre>

<p>Kind regards.</p>
","ios, objective-c, core-data, ios-simulator, stanford-nlp","<p>This indeed doesn't work in the iOS 5.1 Simulator. </p>

<p>The (first) problem seems to be in PhotographersTableViewController.m on line 76, where <code>openWithCompletionHandler:</code> is being called on a <code>UIManagedDocument</code>, but for some reason, the completion handler is never called, even though it should be, <a href=""http://developer.apple.com/library/ios/#documentation/uikit/reference/UIDocument_Class/UIDocument/UIDocument.html#//apple_ref/doc/uid/TP40010879-CH2-SW5"" rel=""nofollow noreferrer"">according to the documentation</a>. </p>

<p>Others have had this problem, too: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/12439528/uimanageddocument-openwithcompletionhandler-never-returns"">UIManagedDocument OpenWithCompletionHandler never returns</a></li>
<li><a href=""https://stackoverflow.com/questions/12577672/uimanageddocument-completionhandler-not-executed-in-ios-5-1-simulator-but-works"">UIManagedDocument CompletionHandler not executed in iOS 5.1
simulator but works on device and iOS 6.0 simulator</a></li>
<li><a href=""https://stackoverflow.com/questions/12557442/bug-in-iphone-simulator-5-1-with-xcode-4-5-using-uimanageddocument"">Bug in iPhone Simulator 5.1 with Xcode 4.5 using
UIManagedDocument</a></li>
</ul>

<p>Looks like a bug in iOS 5.1 to me - but probably just in the Simulator, and maybe only when using the iOS 6.0 SDK. I don't have a device with iOS 5.1 with me, so I can't test that, but it seems to work fine on the iOS 6.0 Simulator.</p>
",2,0,160,2012-10-30 09:58:34,https://stackoverflow.com/questions/13136526/app-working-in-ios-5-0-but-not-on-ios-5-1
Finding collocation patterns in Java,"<p>I am working in a project which require the use of collocations. I have created the following code to extract them. The code takes a string and returns a list of the collocation patterns in this string. I have used Stanford POS to do the tagging. </p>

<p>I need your suggestion on the code, it seems very slow as I process huge amount of text.
Any suggestion to improve the code would be highly appreciated.</p>

<pre><code>/**
*
*  A COLLOCATION is an expression consisting of two or more words that
*  correspond to some conventional way of saying things.
* 
*  I used the seventh Part-of-speech-tag patterns for collocation filtering that 
*  were suggested by Justeson and Katz(1995).
*  These patterns are:
* 
*  -----------------------------------------
*  |Tag |     Pattern Example              |
*  -----------------------------------------
*  |AN  | linear function                  |
*  |NN  | regression coefficients          |
*  |AAN | Gaussian random variable         |
*  |ANN | cumulative distribution function |
*  |NAN | mean squared error               |
*  |NNN | class probability function       |
*  |NPN | degrees of freedom               |                     
*  -----------------------------------------
*  Where A=adjective, P=preposition, &amp; N=noun.
* 
*  Stanford POS have been used for the extraction process. 
*  see: http://nlp.stanford.edu/software/tagger.shtml#Download
* 
*  more on collocation:    http://nlp.stanford.edu/fsnlp/promo/colloc.pdf
*  more on POS:            http://acl.ldc.upenn.edu/J/J93/J93-2004.pdf
*  
*/

public class GetCollocations {
    public static ArrayList&lt;String&gt; GetCollocations(String text) throws IOException,                ClassNotFoundException{
       MaxentTagger tagger = new MaxentTagger(""taggers/wsj-0-18-left3words.tagger"");
       String[] tagged = tagger.tagString(text).split(""\\s+"");

       ArrayList&lt;String&gt; collocations = new ArrayList();
       for (int i = 0; i &lt; tagged.length; i++) {

           String pot = tagged[i].substring(tagged[i].indexOf(""_"") + 1);
           if (pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") ||    pot.equals(""NNPS"")) {

               pot = tagged[i + 1].substring(tagged[i + 1].indexOf(""_"") + 1);
               if (pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") || pot.equals(""NNPS"")) {

                collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]));

                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(""_"") + 1);
                if (pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") || pot.equals(""NNPS"")) {
                    collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]) + "" "" + GetWordWithoutTag(tagged[i + 2]));
                }

            } else if (pot.equals(""JJ"") || pot.equals(""JJR"") || pot.equals(""JJS"")) {
                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(""_"") + 1);

                if (pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") || pot.equals(""NNPS"")) {
                    collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]) + "" "" + GetWordWithoutTag(tagged[i + 2]));
                }

            } else if (pot.equals(""IN"")) {
                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(""_"") + 1);

                if (pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") || pot.equals(""NNPS"")) {
                    collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]) + "" "" + GetWordWithoutTag(tagged[i + 2]));
                }

            }


        } else if (pot.equals(""JJ"") || pot.equals(""JJR"") || pot.equals(""JJS"")) {
            pot = tagged[i + 1].substring(tagged[i + 1].indexOf(""_"") + 1);
            if (pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") || pot.equals(""NNPS"")) {
                collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]));
                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(""_"") + 1);
                if (pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") || pot.equals(""NNPS"")) {
                    collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]) + "" "" + GetWordWithoutTag(tagged[i + 2]));
                }

            } else if (pot.equals(""JJ"") || pot.equals(""JJR"") || pot.equals(""JJS"")) {
                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(""_"") + 1);
                if (pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") || pot.equals(""NNPS"")) {
                    collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]) + "" "" + GetWordWithoutTag(tagged[i + 2]));
                }
            }

        }

    }
    return collocations;

}
public static String GetWordWithoutTag(String wordWithTag){
    String wordWithoutTag = wordWithTag.substring(0,wordWithTag.indexOf(""_""));
    return wordWithoutTag;
}

}
</code></pre>
","java, nlp, stanford-nlp","<p>If you are processing anywhere near 15,000 words per second then you are maxing out with the POS tagger.  According to the Stanford <a href=""http://nlp.stanford.edu/software/pos-tagger-faq.shtml#h"" rel=""nofollow"">Stanford POS tagger FAQ</a>:</p>

<pre><code>on a 2008 nothing-special Intel server, it tags about 15000 words per second
</code></pre>

<p>The rest of your algorithm appears fine, though if you really want to squeeze some juice out of it you could pre-allocate an Array as a static class variable instead of the ArrayList.  Essentially sacrificing the upfront memory costs to not have to instantiate the ArrayList with each call or suffer the <a href=""http://docs.oracle.com/javase/6/docs/api/java/util/ArrayList.html"" rel=""nofollow"">amortized O(n) cost</a> of adding elements.</p>

<p>Also just a suggestion on improving the readability of the code, you may consider using some private methods for checking what part of speech the <code>pot</code> variable is,</p>

<pre><code>private static Boolean  _isNoun(String pot) {
    if(pot.equals(""NN"") || pot.equals(""NNS"") || pot.equals(""NNP"") || pot.equals(""NNPS"")) return true;
    else return false;
}

private static Boolean _isAdjective(String pot){
    if(pot.equals(""JJ"") || pot.equals(""JJR"") || pot.equals(""JJS"")) return true;
    else return false;
}
</code></pre>

<p>Also if I'm not mistaking you should be able to simplify what you are doing, combining some of the <code>if</code> statements.  This won't really speed up your code but it will make it nicer to work with.  Please go through this carefully, I have just tried to simplify your logic to demonstrate my point.  Keep in mind the code below is UNTESTED:</p>

<pre><code>public static ArrayList&lt;String&gt; GetCollocations(String text) throws IOException,                ClassNotFoundException{
    MaxentTagger tagger = new MaxentTagger(""taggers/wsj-0-18-left3words.tagger"");
    String[] tagged = tagger.tagString(text).split(""\\s+"");
    ArrayList&lt;String&gt; collocations = new ArrayList();

    for (int i = 0; i &lt; tagged.length; i++) {
        String pot = tagged[i].substring(tagged[i].indexOf(""_"") + 1);

        if (_isNoun(pot) || _isAdjective(pot)) {
            pot = tagged[i + 1].substring(tagged[i + 1].indexOf(""_"") + 1);

            if (_isNoun(pot) || _isAdjective(pot)) {
                collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]));
                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(""_"") + 1);

                if (_isNoun(pot)) {
                    collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]) + "" "" + GetWordWithoutTag(tagged[i + 2]));
                }

            } else if (pot.equals(""IN"")) {
                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(""_"") + 1);

                if (_isNoun(pot)) {
                    collocations.add(GetWordWithoutTag(tagged[i]) + "" "" + GetWordWithoutTag(tagged[i + 1]) + "" "" + GetWordWithoutTag(tagged[i + 2]));
                }

            }
        }
    }
    return collocations;

}
</code></pre>
",3,4,1874,2012-11-01 23:19:27,https://stackoverflow.com/questions/13186995/finding-collocation-patterns-in-java
NLP or Antlr to Manipulate Home-grown Spec Language,"<p>Looking for suggestion or ideas on best way to procede
Was trying to develop a way for Analyst to develop a Spec doc that describes the possible STATES that our
Process(BlackBoc) generates that can
then feed other process like Documentation generation(Flowcharts etc) , Testing, Code Generation etc</p>

<p>If the Spec was written in a Programming Coding type Language Syntax then manipulated with NLP or ANTLR
to create a List of POssible States that our Process(BlackBoc) generates</p>

<p>I gues my questions are
1)Has anyone done anything like this ?
2)Would ANTLR or NLP be best way to procede to maniulate a document written in Programming Coding type Language??</p>

<p>Thanks</p>

<p><strong>LIST of States from Spec Language - that could feed other processes</strong></p>

<pre><code>""CAR"" : CAR_FULLSIZE_CHASSIS  :  350cc : CAR = 1X23 (Altima SE)
""CAR"" : CAR = 1X23 (Altima S)
</code></pre>

<p>or maybe</p>

<pre><code>VEHICLE = ""CAR"" = True : CHASSIS = CAR_FULLSIZE_CHASSIS = True  : CAR_ENGIN_SIZE &gt; 350cc = True : CAR = 1X23 (Altima SE)
VEHICLE = ""CAR"" = True : CHASSIS = CAR_FULLSIZE_CHASSIS = False: CAR_ENGIN_SIZE &gt; 350cc = False : CAR = 1X23 (Altima S)
</code></pre>

<p><strong>Spec Language written by Analyst</strong></p>

<pre><code>if VEHICLE = ""CAR""
if CHASSIS IN LIST ( CAR_FULLSIZE_CHASSIS )
    if CAR_ENGIN_SIZE &gt; 350cc
        CAR = 1X23 (Altima SE)
    else
        CAR = 1X24 (Altima S)
else
    ....
</code></pre>
","nlp, antlr, stanford-nlp","<blockquote>
  <p>I gues my questions are </p>
  
  <p>1)Has anyone done anything like this ? </p>
  
  <p>2)Would ANTLR or NLP be best way to procede to maniulate a document written in Programming Coding type Language?</p>
</blockquote>

<ol>
<li>yes;</li>
<li>as mentioned in the comments already: this is not a job for an NLP parser. An NLP parser is suitable for parsing natural languages, as the name suggests. The example snippet of the language you posted might have ambiguities, but looks structured enough to describe through a grammar and let some parser generator (like ANTLR) generate a parser for you.</li>
</ol>
",0,0,201,2012-11-16 15:07:13,https://stackoverflow.com/questions/13419233/nlp-or-antlr-to-manipulate-home-grown-spec-language
Multi-term named entities in Stanford Named Entity Recognizer,"<p>I'm using the Stanford Named Entity Recognizer <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"">http://nlp.stanford.edu/software/CRF-NER.shtml</a> and it's working fine. This is</p>

<pre><code>    List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(text);
    for (List&lt;CoreLabel&gt; sentence : out) {
        for (CoreLabel word : sentence) {
            if (!StringUtils.equals(word.get(AnswerAnnotation.class), ""O"")) {
                namedEntities.add(word.word().trim());           
            }
        }
    }
</code></pre>

<p>However the problem I'm finding is identifying names and surnames. If the recognizer encounters ""Joe Smith"", it is returning ""Joe"" and ""Smith"" separately. I'd really like it to return ""Joe Smith"" as one term. </p>

<p>Could this be achieved through the recognizer maybe through a configuration? I didn't find anything in the javadoc till now. </p>

<p>Thanks!</p>
","nlp, stanford-nlp, named-entity-recognition","<p>This is because your inner for loop is iterating over individual tokens (words) and adding them separately. You need to change things to add whole names at once.</p>

<p>One way is to replace the inner for loop with a regular for loop with a while loop inside it which takes adjacent non-O things of the same class and adds them as a single entity.*</p>

<p>Another way would be to use the CRFClassifier method call:</p>

<pre><code>List&lt;Triple&lt;String,Integer,Integer&gt;&gt; classifyToCharacterOffsets(String sentences)
</code></pre>

<p>which will give you whole entities, which you can extract the String form of by using <code>substring</code> on the original input.</p>

<p>*The models that we distribute use a simple raw IO label scheme, where things are labeled PERSON or LOCATION, and the appropriate thing to do is simply to coalesce adjacent tokens with the same label. Many NER systems use more complex labels such as IOB labels, where codes like B-PERS indicates where a person entity starts. The CRFClassifier class and feature factories support such labels, but they're not used in the models we currently distribute (as of 2012).</p>
",20,19,10254,2012-12-07 14:45:42,https://stackoverflow.com/questions/13765349/multi-term-named-entities-in-stanford-named-entity-recognizer
Extracting the relationship between entities in Stanford CoreNLP,"<p>I want to extract the complete relationship between two entities using Stanford CoreNLP (or maybe other tools).</p>

<p>For example:</p>

<blockquote>
  <p>Windows is <em>more popular than</em> Linux.</p>
  
  <p>This tool <em>requires</em> Java.</p>
  
  <p>Football is <em>the most popular game in</em> the World.</p>
</blockquote>

<p>What is the fastest way? And what is the best practice for that?</p>

<p>Thanks in advance</p>
","nlp, stanford-nlp","<p>You are probably looking for dependency relations between nouns. Stanford Parser provides such output. Have a look <a href=""http://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""noreferrer"">here</a>. You can combine what Pete said (i.e. the POS graph) with the dependency graph to identify what relationship (for example, direct object or nominal subject, etc.) a pair of nouns (or noun phrases) share.</p>
",6,13,8488,2012-12-15 13:30:52,https://stackoverflow.com/questions/13892638/extracting-the-relationship-between-entities-in-stanford-corenlp
"Using the Stanford NLP libraries from within R, using the rJava package","<p>Does anybody have any experience with using <strong>StanfordCoreNLP</strong> ( <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml</a> through <strong>rJava</strong> in R?  I’ve been struggling to get it to work for two days now, and think I’ve exhausted Google and previous questions on StackOverflow.</p>

<p>Essentially I’m trying to use the StanfordNLP libraries from within R.  I have zero Java experience, but experience with other languages, so understand the basics about classes and objects etc.</p>

<p>From what I can see, the demo .java file that comes with the libraries seems to show that to use the classes from within Java, you’d import the libraries and then create a new object, along the lines of:</p>

<pre><code>import java.io.*;
import java.util.*;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

    public class demo {

        etc.
        etc.

        StanfordCoreNLP pipeline = new StanfordCoreNLP();

        etc.
</code></pre>

<p>From within R, I’ve tried calling some standard java functions; this works fine, which makes me think it’s the way I’m trying to access the Stanford libraries that’s causing the issue.</p>

<p>I extracted the Stanford ZIP to <strong>h:\stanfordcore</strong>, so the .jar files are all in the root of this directory.  As well as the various other files contained in the zip, it contains the main .jar files:</p>

<ul>
<li>joda-time.jar</li>
<li>stanford-corenlp-1.3.4.jar</li>
<li>stanford-corenlp-1.3.4-javadoc.jar</li>
<li>stanford-corenlp-1.3.4-models.jar</li>
<li>joda-time-2.1-sources.jar</li>
<li>jollyday-0.4.7-sources.jar</li>
<li>stanford-corenlp-1.3.4-sources.jar</li>
<li>xom.jar</li>
<li>jollyday.jar</li>
</ul>

<p>If I try to access the NLP tools from the command line, it works fine.</p>

<p>From within R, I initalized the JVM and set the classpath variable:</p>

<pre><code>.jinit(classpath = "" h:/stanfordcore"", parameters = getOption(""java.parameters""),silent = FALSE, force.init = TRUE)
</code></pre>

<p>After this, if I use the command </p>

<pre><code>.jclassPath() 
</code></pre>

<p>This shows that the directory containing the required .jar files has been added and gives this output in R:</p>

<p><strong>[1] ""H:\RProject-2.15.1\library\rJava\java"" ""h:\ stanfordcore""</strong></p>

<p>However, when I try create a new object (not sure if this is the right Java terminology) I get an error.  </p>

<p>I’ve tried creating the object in dozens of different ways (basically shooting in the dark though), but the most promising (simply because it seems to actually find the class is):</p>

<pre><code>pipeline &lt;- .jnew(class=""edu/stanford/nlp/pipeline/StanfordCoreNLP"",check=TRUE,silent=FALSE)
</code></pre>

<p>I know this finds the class, because if I change the class parameter to something not listed in the API, I get a cannot find class error.  </p>

<p>As it stands, however, I get the error:</p>

<p><strong>Error in .jnew(class = ""edu/stanford/nlp/pipeline/StanfordCoreNLP"", check = TRUE,  : 
  java.lang.NoClassDefFoundError: Could not initialize class edu.stanford.nlp.pipeline.StanfordCoreNLP</strong></p>

<p>My Googling indicates that this might be something to do with not finding a required .jar file, but I’m completely stuck.  Am I missing something obvious?</p>

<p>If anyone can point me even a little in the right direction, I’d be incredibly grateful.</p>

<p>Thanks in advance!</p>

<p>Peter</p>
","java, r, stanford-nlp, rjava","<p>Your classpath is wrong - you are using a directory but you have JAR files. You have to either unpack all JAR files in the directory you specify (unusual) or you have to add all the JAR files to the class path (more common). [And you'll have to fix your typos, obviously, but I assume those come form the fact that you were not using copy/paste]</p>

<p>PS: please use <a href=""http://mailman.rz.uni-augsburg.de/mailman/listinfo/stats-rosuda-devel"" rel=""nofollow"">stats-rosuda-devel</a> mailing list if you want more timely answers.</p>
",2,6,3671,2012-12-18 17:23:04,https://stackoverflow.com/questions/13938272/using-the-stanford-nlp-libraries-from-within-r-using-the-rjava-package
Parse arbitrary text to produce dependency graph,"<p>How to create dependency graph (parse tree) for random sentences. Is there any predined grammer to parse english sentences using nltk.</p>

<p>Example:</p>

<p>I want to make a parse tree for the sentence </p>

<p>“A large company needs a sustainable business model.”
&nbsp;&nbsp;which should look like this.</p>

<p><img src=""https://i.sstatic.net/BL7tS.jpg"" alt=""enter image description here""></p>

<p>Please suggest me how this can be done.</p>
","parsing, nlp, nltk, stanford-nlp, word-sense-disambiguation","<p>This question is a near-duplicate of <a href=""https://stackoverflow.com/questions/3125926/does-nltk-have-a-tool-for-dependency-parsing"">3125926</a>. But I'll elaborate just a little on the answer given there.</p>

<p>I don't have personal experience with dependency parsing under NLTK, but according to the accepted answer, the integration with <a href=""http://www.maltparser.org/"" rel=""nofollow noreferrer"">MaltParser</a> is documented at <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.parse.malt.MaltParser-class.html"" rel=""nofollow noreferrer"">http://nltk.googlecode.com/svn/trunk/doc/api/nltk.parse.malt.MaltParser-class.html</a></p>

<p>If for some reason MaltParser doesn't suit your needs, you might also take a look at <a href=""http://sourceforge.net/projects/mstparser/"" rel=""nofollow noreferrer"">MSTParser</a> and the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">Stanford Parser</a>. I think those three options are the best-known, and I expect one (or all) of them will work for you.</p>

<p>Note that the Stanford Parser includes routines to convert from constituency trees and between several of the standard dependency representations, so if you need a specific format, you might look at the format-conversion arguments to the <code>edu.stanford.nlp.trees.EnglishGrammaticalStructure</code> class.</p>

<p>e.g., to convert from constituency trees to basic dependencies:</p>

<p><code>java -cp stanford-parser.jar edu.stanford.nlp.trees.EnglishGrammaticalStructure -treeFile &lt;input trees&gt; -basic</code></p>
",3,1,2270,2012-12-18 18:03:53,https://stackoverflow.com/questions/13938839/parse-arbitrary-text-to-produce-dependency-graph
Get the K best parses of a sentence with Stanford Parser,"<p>I want to have the K best parses of a sentence, I figured that this can be done with  ExhaustivePCFGParser Class , the problem is that I don't know how to use this class , more precisely haw can I instantiate this class ?
 ( the constructor is : ExhaustivePCFGParser(BinaryGrammar bg, UnaryGrammar ug, Lexicon lex, Options op, Index stateIndex, Index wordIndex, Index tagIndex) ) but i don't know how to fit all this parameters  </p>

<p>Is there any more easy way to have the K best parses ?</p>
","java, parsing, stanford-nlp","<p>In general you do things via a <code>LexicalizedParser</code> object which is a ""grammar"" which provides all these things (the grammars, lexicon, indices, etc.).</p>

<p>From the command-line, the following will work:</p>

<pre><code>java -mx500m -cp ""*"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -printPCFGkBest 20 edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz data/testsent.txt
</code></pre>

<p>At the API level, you need to get a <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParserQuery.html"" rel=""nofollow""><code>LexicalizedParserQuery</code></a> object. Once you have a <code>LexicalizedParser lp</code> (as in <code>ParserDemo.java</code>) you can do the following:</p>

<pre><code>LexicalizedParser lp = ... // Load / train a model
LexicalizedParserQuery lpq = lp.parserQuery();
lpq.parse(sentence);
List&lt;ScoredObject&lt;Tree&gt;&gt; kBest = lpq.getKBestPCFGParses(20);
</code></pre>

<p>A <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParserQuery.html"" rel=""nofollow""><code>LexicalizedParserQuery</code></a> is sort of equivalent to a java regex <code>Matcher</code>.</p>

<p>Note: at present kBest parsing works well only for PCFG not factored grammars.</p>
",2,2,1635,2012-12-23 20:54:07,https://stackoverflow.com/questions/14014631/get-the-k-best-parses-of-a-sentence-with-stanford-parser
Stanford coreNLP - split words ignoring apostrophe,"<p>I'm trying to split a sentence into words using Stanford coreNLP .
I'm having problem with words that contains apostrophe.</p>

<p>For example, the sentence:
I'm 24 years old.</p>

<p>Splits like this:
[I] ['m] [24] [years] [old]</p>

<p>Is it possible to split it like this using Stanford coreNLP?:
[I'm] [24] [years] [old]</p>

<p>I've tried using tokenize.whitespace, but it doesn't split on other punctuation marks like: '?' and ','</p>
","nlp, stanford-nlp","<p>Currently, no. The subsequent Stanford CoreNLP processing tools all use <a href=""http://www.cis.upenn.edu/~treebank/tokenization.html"" rel=""nofollow"">Penn Treebank tokenization</a>, which splits contractions into two tokens (regarding ""I'm"" as a reduced form of ""I am"" by making it the two ""words"" [I] ['m]). It sounds like you want a different type of tokenization.</p>

<p>While there are some tokenization options, there isn't one to change this, and subsequent tools (like the POS tagger or parser) would work badly without contractions being split. You could add such an option to the tokenizer, changing (deleting) the treatment of REDAUX and SREDAUX trailing contexts.</p>

<p>You can also join contractions via post processing as @dhg suggests, but you'd want to do it a little more carefully in the ""if"" so it didn't join on quotes.</p>
",4,2,4937,2012-12-27 17:07:34,https://stackoverflow.com/questions/14058399/stanford-corenlp-split-words-ignoring-apostrophe
Writing this while-loop in for-loop,"<p>I'm working with StanfordNLP to extract data from a parsed Tree.</p>

<p>I'm using Scala for coding.</p>

<pre><code>val tp = TregexPattern.compile(""SOME_PATTERN"")
val res = tp.matcher(""SOME_TREE"")
</code></pre>

<p>to read the results of this I use</p>

<pre><code>while (res.find()) {
  println(res.getMatch.getLeaves.mkString("" ""))
}
</code></pre>

<p>I want to rewrite this while-loop in for-loop.</p>
","scala, stanford-nlp","<p>How about this:</p>

<pre><code>val tp = TregexPattern.compile(""SOME_PATTERN"")
val res = tp.matcher(""SOME_TREE"")
for(it &lt;- Iterator.continually(res.getMatch).takeWhile(_ =&gt; res.find)) {
  println(it.getLeaves.mkString("" ""))
}
</code></pre>
",1,0,148,2013-01-05 13:30:58,https://stackoverflow.com/questions/14172234/writing-this-while-loop-in-for-loop
Matcher gives different results on Ubuntu vs. Windows,"<p>I'm running the exact same eclipse project on Ubuntu and on Windows but getting different output.</p>

<p>The unevenly behavior occurs in the following code:</p>

<pre><code>String regex = ""&lt;token id=\""(.*)\""&gt;.*\n.*&lt;word&gt;(.*)&lt;/word&gt;.*\n.*&lt;lemma&gt;(.*)&lt;/lemma&gt;.*\n.*\n.*\n.*&lt;POS&gt;(.*)&lt;/POS&gt;"";
Pattern pattern = Pattern.compile(regex);
Matcher matcher = pattern.matcher(fileAsString);
while (matcher.find()) {
    ...
}
</code></pre>

<p>The (matcher.find()) check return false on Windows but true on Ubuntu (which is the expected behavior).</p>

<p>Eclipse Juno and jdk7 on both.</p>

<p>Maybe it's not related to the operating system, but that's the only different I found after debug parallelly and after check the project's properties in the two environments..</p>

<p>Any idea to the differences???</p>
","java, regex, eclipse, nlp, stanford-nlp","<p>You're matching <code>\n</code>, which is the line ending for Linux, but not Windows (you need <code>\r\n</code> for Windows). Something like <code>\r?\n</code> would fix your specific problem.</p>

<p>That said, you should never parse anything HTML-like (including XML) with regex. You're missing out on everything XML is about, not the least of which its flexibility with hand-written ""mistakes"" like different order of tags, spaces etc.</p>
",4,1,259,2013-01-10 20:48:06,https://stackoverflow.com/questions/14266998/matcher-gives-different-results-on-ubuntu-vs-windows
how to train the stanford LexicalizedParser to recognize new words as nouns?,"<p>I am trying to figure out how to train the stanford LexicalizedParser<br>
( edu.stanford.nlp.parser.lexparser.LexicalizedParser )  to incorporate new nouns into its lexicon.</p>

<p>At first my goal was to take take an existing model and tweak it slightly, rather than creating a brand new model 
from a vast set of training examples.</p>

<p>the answer to this question suggests that is not possible > 
    <a href=""https://stackoverflow.com/questions/5570765/how-can-i-add-more-tagged-words-to-the-stanford-pos-taggers-trained-models"">How can I add more tagged words to the Stanford POS-Tagger&#39;s trained models?</a></p>

<p>Hopefully someone out there can put me on the right track as to how to do this.</p>

<p>As a concrete example of what i want to do, say i have the word 'researchgate' which i want to be treated as a noun when i parse 
sentences.  Currently, 'researchgate' is getting treated as different parts of speech, depending on its 
position.. but i want it identified as an 'NN' (noun).</p>

<p>Examples... </p>

<p>instead of this:</p>

<pre><code>      (NP
        (NP (JJ recent) (NN activity))
        (PP (IN in)
          (NP (PRP$ your) (JJ researchgate) (NNS topics)))))
</code></pre>

<p>i want this: </p>

<pre><code>      (NP
        (NP (JJ recent) (NN activity))
        (PP (IN in)
          (NP (PRP$ your) (NN researchgate) (NNS topics)))))
</code></pre>

<p>and instead of this:</p>

<pre><code>    (ROOT
      (FRAG
        (NP (NN subscription))
        (S
          (VP (TO to)
            (VP (VB researchgate))))))
</code></pre>

<p>i want this: </p>

<pre><code>    (ROOT
      (NP
        (NP (NN subscription))
        (PP (TO to)
          (NP (NN researchgate)))))
</code></pre>

<p>I am currently using this model: models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz</p>

<p>I tried doing this > </p>

<pre><code>    java -cp  stanford-parser.jar        
            edu.stanford.nlp.parser.lexparser.LexicalizedParser   -train  /tmp/train.txt
</code></pre>

<p>with the contensts of /tmp/train.txt as follows > </p>

<pre><code>              (NP
                (NP (JJ recent) (NN activity))
                (PP (IN in)
                  (NP (PRP$ your) (JJ researchgate) (NNS topics)))))
</code></pre>

<p>I got a bunch of promising output, but then got this error > </p>

<pre><code>    Error. Can't parse test sentence: [This, is, just, a, test, .]
</code></pre>

<p>So clearly i need to supply more examples than just the one i have in /tmp/train.txt.</p>

<p>Looking at the documentation there seems to be one promising method on 
LexicalizedParser  that I am considering trying... > </p>

<pre><code>    public static LexicalizedParser getParserFromTreebank(Treebank trainTreebank,
                                                          Treebank secondaryTrainTreebank,
                                                          double weight,
                                                          GrammarCompactor compactor,
                                                          Options op,
                                                          Treebank tuneTreebank,
                                                          List&lt;List&lt;TaggedWord&gt;&gt; extraTaggedWords)
</code></pre>

<p>i am hesitant to jump in and try this because it seems tricky to get the Options right.
The doco says:<br>
        options to the parser which MUST be the SAME at both training and testing (parsing) time in 
        order for the parser to work properly</p>

<p>so i might need guidance on how to extract the options used for 
edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz   perhaps it is </p>

<pre><code>        edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams  ?
</code></pre>

<p>Also, maybe i want to add researchgate in as one of my extraTaggedWords  ?</p>

<p>I have the feeling i am on the right track but was hoping to get some advice before descending 
into a rat hole.</p>

<p>Thanks in advance !</p>

<pre><code>chris
</code></pre>
","parsing, nlp, stanford-nlp","<p>I posted to stanford parser mailing list and I received an answer from John Bauer (thanks, John !) </p>

<p><em>John Bauer 
2:09 PM (39 minutes ago)
to me, parser-user 
Unfortunately, you would need to start training from the beginning.  <strong>There is no way to extend a current parser model.</strong>
That feature is on ""the list"", but it's somewhere near the back, so don't hold your breath...
John</em></p>
",1,1,1199,2013-01-23 05:24:26,https://stackoverflow.com/questions/14473017/how-to-train-the-stanford-lexicalizedparser-to-recognize-new-words-as-nouns
How is stemming useful?,"<p>Simple question: When do we stem or lemmatize the words? Is stemming helpful for all nlp processes or are there applications where using full form of words might result in better accuracy or precision?</p>
","nlp, stanford-nlp","<p>In the context of machine learning based NLP, stemming makes your training data more dense. It reduces the size of the dictionary (number of words used in the corpus) two or three-fold (of even more for languages with many flections like French, where a single stem can generate dozens of words in case of verbs for instance).</p>

<p>Having the same corpus, but less input dimensions, ML will work better. Recall should really be better.</p>

<p>The downside is, if in some cases the actual word (as opposed to its stem) makes a difference, then your system won't be able to leverage it. So you might lose some precision.</p>
",11,8,6162,2013-01-24 20:57:37,https://stackoverflow.com/questions/14510618/how-is-stemming-useful
Perfomance issue while using stanford lexicalized parser in java,"<p>I am a newbie to Stanford NLP.I am using lexicalized parser for parsing the contents of the file and extracting the noun phrases.While parsing the line it is taking more time for generating a tree structure.</p>

<p>I am using a Tregex pattern to get noun phrases from a line.</p>

<p>I am using 1 MB file to parse,so it is taking, more than two hours for parsing as well as for extracting the noun phrases.</p>

<p>Here is my full code that i am using.</p>

<pre><code>        Tree x = parser.apply(line);
        System.out.println(""tree s==""+x);
        TregexPattern NPpattern = TregexPattern.compile(""@NP &lt;@/NN.?/"");
        TregexMatcher matcher = NPpattern.matcher(x);

        while (matcher.findNextMatchingNode()) {
            Tree match = matcher.getMatch();
            List&lt;TaggedWord&gt; tWord = match.taggedYield();
            Iterator&lt;TaggedWord&gt; it = tWord.iterator();
            String str="""";
            while(it.hasNext()){
                TaggedWord word = it.next();
                String taggedWord = word.tag();
                if(taggedWord.equals(""NN"")||taggedWord.equals(""NNS"")||taggedWord.equals(""NNP"")){
                    str = str+word.value()+"" "";
                }
            }
       }
</code></pre>

<p>So please help me how to increase the performance or is there another way to optimize this code.</p>

<p>Thanks in advance
Gouse.</p>
","java, nlp, stanford-nlp","<p>Full constituency parsing of text is just kind of slow.... If you stick with it, there may not be much that you can do. </p>

<p>But a couple of things to mention: (i) If you're not using the englishPCFG.ser.gz grammar, then you should, because it's faster than using englishFactored.seer.gz and (ii) Parsing very long sentences is especially slow, so if you can get by omitting or breaking very long sentences (say, over70 words), that can help a lot. In particular, if some of the text is from web scraping or whatever and has long lists of stuff that aren't really sentences, filtering or dividing them may help a lot.</p>

<p>The other direction you could go is that you appear to not really need a full parser but just an NP chunker (something that identifies minimal noun phrases in a text). These can be much faster as they don't build recursive structure. There isn't one at present among the Stanford NLP tools, but you can find some by searching for this term on the web.</p>
",1,0,639,2013-01-30 06:52:06,https://stackoverflow.com/questions/14598250/perfomance-issue-while-using-stanford-lexicalized-parser-in-java
Is there a C# utility for matching patterns in (syntactic parse) trees?,"<p>I'm working on a Natural Language Processing (NLP) project in which I use a syntactic parser to create a syntactic parse tree out of a given sentence.</p>

<p><strong><em>Example Input:</em></strong> I ran into Joe and Jill and then we went shopping<br>
<strong><em>Example Output:</em></strong> [TOP [S [S [NP [PRP I]] [VP [VBD ran] [PP [IN into] [NP [NNP Joe] [CC and] [NNP Jill]]]]] [CC and] [S [ADVP [RB then]] [NP [PRP we]] [VP [VBD went] [NP [NN shopping]]]]]]
<img src=""https://i.sstatic.net/EedCb.png"" alt=""enter image description here""></p>

<p>I'm looking for a C# utility that will let me do complex queries like:</p>

<ul>
<li>Get the first VBD related to 'Joe'</li>
<li>Get the NP closest to 'Shopping'</li>
</ul>

<p>Here's a <a href=""http://nlp.stanford.edu/software/tregex.shtml"" rel=""noreferrer"">Java utility</a> that does this, I'm looking for a C# equivalent.<br>
Any help would be much appreciated.</p>
","c#, tree, nlp, stanford-nlp, s-expression","<p>We already use </p>

<p>One option would be to <a href=""https://stackoverflow.com/questions/14675335/s-expressions-parsing"">parse the output into C# code</a> and then encoding it to XML making every node into <code>string.Format(""&lt;{0}&gt;"", this.Name);</code> and <code>string.Format(""&lt;/{0}&gt;"", this._name);</code> in the middle put all the child nodes recursively. </p>

<p>After you do this, I would use <a href=""https://github.com/jamietre/CsQuery"" rel=""nofollow noreferrer"">a tool for querying XML/HTML</a> to parse the tree. Thousands of people already use query selectors and jQuery to parse tree-like structure based on the relation between nodes. I think this is far superior to TRegex or other outdated and un-maintained java utilities. </p>

<p>For example, this is to answer your first example:</p>

<pre><code>var xml = CQ.Create(d.ToXml());
//this can be simpler with CSS selectors but I chose Linq since you'll probably find it easier
//Find joe, in our case the node that has the text 'Joe'
var joe = xml[""*""].First(x =&gt; x.InnerHTML.Equals(""Joe"")); 
//Find the last (deepest) element that answers the critiria that it has ""Joe"" in it, and has a VBD in it
//in our case the VP
var closestToVbd = xml[""*""].Last(x =&gt; x.Cq().Has(joe).Has(""VBD"").Any());
Console.WriteLine(""Closest node to VPD:\n "" +closestToVbd.OuterHTML);
//If we want the VBD itself we can just find the VBD in that element
Console.WriteLine(""\n\n VBD itself is "" + closestToVbd.Cq().Find(""VBD"")[0].OuterHTML);
</code></pre>

<p>Here is your second example</p>

<pre><code>//Now for NP closest to 'Shopping', find the element with the text 'shopping' and find it's closest NP
var closest = xml[""*""].First(x =&gt;     x.InnerHTML.Equals(""shopping"")).Cq()
                      .Closest(""NP"")[0].OuterHTML;
Console.WriteLine(""\n\n NP closest to shopping is: "" + closest);
</code></pre>
",2,11,1851,2013-02-03 15:11:05,https://stackoverflow.com/questions/14673902/is-there-a-c-utility-for-matching-patterns-in-syntactic-parse-trees
Is it possible to get a set of a specific named entity tokens that comprise a phrase,"<p>I'm using the Stanford CoreNLP parsers to run through some text and there are date phrases, such as 'the second Monday in October' and 'the past year'.  The library will appropriately tag each token as a DATE named entity, but is there a way to programmatically get this whole date phrase?  And it's not just dates, ORGANIZATION named entities will do the same (""The International Olympic Committee"", for example, could be one identified in a given text example).</p>

<pre><code>String content = ""Thanksgiving, or Thanksgiving Day (Canadian French: Jour de""
        + "" l'Action de grâce), occurring on the second Monday in October, is""
        + "" an annual Canadian holiday which celebrates the harvest and other""
        + "" blessings of the past year."";

Properties p = new Properties();
p.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(p);

Annotation document = new Annotation(content);
pipeline.annotate(document);

for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
    for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {

        String word = token.get(CoreAnnotations.TextAnnotation.class);
        String ne = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);

        if (ne.equals(""DATE"")) {
            System.out.println(""DATE: "" + word);
        }

    }
}
</code></pre>

<p>Which, after the Stanford annotator and classifier loading, will yield the output:</p>

<pre><code>DATE: Thanksgiving
DATE: Thanksgiving
DATE: the
DATE: second
DATE: Monday
DATE: in
DATE: October
DATE: the
DATE: past
DATE: year
</code></pre>

<p>I feel like the library has to be recognizing the phrases and using them for the named entity tagging, so the question would be is that data kept and available somehow through the api?</p>

<p>Thanks,
Kevin</p>
","stanford-nlp, named-entity-recognition","<p>After discussions on the mailing list I've found that the api does not support this.  My solution was to just keep the state of the last NE, and build a string if necessary.  John B. from the nlp mailing lists was helpful in answering my question.</p>
",1,1,1522,2013-02-04 15:16:47,https://stackoverflow.com/questions/14689717/is-it-possible-to-get-a-set-of-a-specific-named-entity-tokens-that-comprise-a-ph
How to extract the noun phrases using Open nlp&#39;s chunking parser,"<p>I am newbie to Natural Language processing.I need to extract the noun phrases from the text.So far i have used open nlp's chunking parser for parsing my text to get the Tree structure.But i am not able to extract the noun phrases from the tree structure, is there any regular expression pattern in open nlp so that i can use it to extract the noun phrases.</p>

<p>Below is the code that i am using </p>

<pre><code>    InputStream is = new FileInputStream(""en-parser-chunking.bin"");
    ParserModel model = new ParserModel(is);
    Parser parser = ParserFactory.create(model);
    Parse topParses[] = ParserTool.parseLine(line, parser, 1);
        for (Parse p : topParses){
                 p.show();}
</code></pre>

<p>Here I am getting the output as</p>

<p>(TOP (S (S (ADJP (JJ welcome) (PP (TO to) (NP (NNP Big) (NNP Data.))))) (S (NP (PRP We)) (VP (VP (VBP are) (VP (VBG working) (PP (IN on) (NP (NNP Natural) (NNP Language) (NNP Processing.can))))) (NP (DT some) (CD one) (NN help)) (NP (PRP us)) (PP (IN in) (S (VP (VBG extracting) (NP (DT the) (NN noun) (NNS phrases)) (PP (IN from) (NP (DT the) (NN tree) (WP stucture.))))))))))</p>

<p>Can some one please help me in getting the noun phrases like NP,NNP,NN etc.Can some one tell me do I need to use any other NP Chunker to get the noun phrases?Is there any regex pattern to achieve the same.</p>

<p>Please help me on this.</p>

<p>Thanks in advance</p>

<p>Gouse.</p>
","java, nlp, stanford-nlp, opennlp","<p>The <code>Parse</code> object is a tree; you can use <code>getParent()</code> and <code>getChildren()</code> and <code>getType()</code> to navigate the tree.</p>

<pre><code>List&lt;Parse&gt; nounPhrases;

public void getNounPhrases(Parse p) {
    if (p.getType().equals(""NP"")) {
         nounPhrases.add(p);
    }
    for (Parse child : p.getChildren()) {
         getNounPhrases(child);
    }
}
</code></pre>
",6,13,11251,2013-02-05 12:53:52,https://stackoverflow.com/questions/14708047/how-to-extract-the-noun-phrases-using-open-nlps-chunking-parser
"Stanford NLP ported to ikvm, FileLoad Exception","<p>I'm trying to use the Stanford NLP tools ported to IKVM, but it gives me unhandeled exception.</p>

<p>here's the code I'm using</p>

<pre><code>       `string StanfordModelsDirectory = ""englishPCFG.ser.gz"";
        try
        {
            LexicalizedParser LP = LexicalizedParser.loadModel(StanfordModelsDirectory);

            TreebankLanguagePack tlp = new PennTreebankLanguagePack();
            GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();


            Tree parse = LP.apply(""what's the largest city in canada?"");
            parse.pennPrint();
        }
        catch (Exception e)
        {
            Console.WriteLine(e.Message);
        }`
</code></pre>

<p>I've referenced IKVM.OpenJDK.Core and stanford-parser, but the message 
""Could not load file or assembly 'IKVM.OpenJDK.Core, Version=7.1.4532.2, Culture=neutral, PublicKeyToken=13235d27fcbfff58' or one of its dependencies. The located assembly's manifest definition does not match the assembly reference. (Exception from HRESULT: 0x80131040)"" appears.</p>

<p>I'm using windows 8 (visual studio 2012, .NET 4.5)</p>
","exception, stanford-nlp, ikvm","<p>IKVM is compiled for .NET 2.0. You need to add a config file to your application to map to a different .NET version like 4.5.</p>

<p>IKVM 7.2.x contains such config file for ikvm.exe, ikvmc.exe, .....</p>
",1,0,458,2013-02-11 00:54:58,https://stackoverflow.com/questions/14804493/stanford-nlp-ported-to-ikvm-fileload-exception
Converting stanfordNLP parse tree into DOT format,"<p>Is there a way/method to convert the parse tree generated by stanford parser into DOT format. I'm aware of the method toDotFormat() that converts stanford dependency output to DOT. But I want to convert edu.stanford.nlp.trees.Tree to DOT.
Thanks </p>
","nlp, stanford-nlp","<p>There is currently no code to do this. But you could write such a method by walking down through the children of a tree yourself.</p>
",0,1,499,2013-02-18 19:06:50,https://stackoverflow.com/questions/14943565/converting-stanfordnlp-parse-tree-into-dot-format
Syntactic Checking,"<p>We have a paragraph of text that we want to check for syntactic correctness. We are using NLTK and all was good till POS tagging but then as we begin doing parsing we realized nltk still does not have a statistical parse like the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">Stanford Parser</a>. NLTK allows us to make toy grammars which is inflexible.</p>

<p>So in a similar <a href=""https://stackoverflow.com/questions/6115677/english-grammar-for-parsing-in-nltk"">question</a> user larsmans mentions using Stanford parser and then checking chunks based on probability. But stanford does not <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#j"" rel=""nofollow noreferrer"">recommend</a> it.</p>

<p>Can you recommend us tools,algorithms,pointers,suggestions that can help to set us up on right path,to check syntax(of say average 20 words length sentences,preferably python).We found <a href=""https://github.com/wavii/pfp"" rel=""nofollow noreferrer"">this.</a>   </p>
","nlp, nltk, stanford-nlp","<p>You can consider <a href=""http://www.link.cs.cmu.edu/link/index.html"" rel=""nofollow"">Link Grammar Parser</a> which is used by AbiWord (word processor) for checking grammar. According to their <a href=""http://www.abisource.com/projects/link-grammar/"" rel=""nofollow"">page</a>:</p>

<blockquote>
  <p>The AbiWord team had a concrete need - to integrate a grammar checking feature into AbiWord. The best choice, they felt, was to build upon Temperley et. al.'s successful Link Grammar project.</p>
</blockquote>
",3,0,361,2013-03-02 13:26:53,https://stackoverflow.com/questions/15174997/syntactic-checking
NLP to find relationship between entities,"<p>My current understanding is that it's possible to extract entities from a text document using toolkits such as OpenNLP, Stanford NLP. </p>

<p>However, is there a way to find <em>relationships</em> between these entities? </p>

<p>For example consider the following text : </p>

<p><em>""As some of you may know, I spent last week at CERN, the European high-energy physics laboratory where the famous Higgs boson was discovered last July. Every time I go to CERN I feel a deep sense of reverence. Apart from quick visits over the years, I was there for three months in the late 1990s as a visiting scientist, doing work on early Universe physics, trying to figure out how to connect the Universe we see today with what may have happened in its infancy.""</em> </p>

<p>Entities: <strong>I</strong> (author), <strong>CERN</strong>, <strong>Higgs boson</strong></p>

<p>Relationships : 
- I ""<strong>visited</strong>"" CERN
- CERN ""<strong>discovered</strong>"" Higgs boson</p>

<p>Thanks. </p>
","text, nlp, stanford-nlp, opennlp, information-extraction","<p>You can extract verbs with their dependants using Stanford Parser, for example. E.g., you might get ""dependency chains"" like </p>

<pre><code>""I :: spent :: at :: CERN"". 
</code></pre>

<p>It is a much tougher task to recognise that ""I spent at CERN"" and ""I visited CERN"" and ""CERN hosted my visit"" (etc) denote the same kind of event. Going into how this can be done is beyond the scope of an SO question, but you can read up literature of paraphrases recognition (<a href=""http://www.jair.org/media/2985/live-2985-5001-jair.pdf"" rel=""nofollow noreferrer"">here</a> is one overview paper). There is also a <a href=""https://stackoverflow.com/questions/4633391/paraphrase-recognition-using-sentence-level-similarity"">related question</a> on SO. </p>

<p>Once you can cluster similar chains, you'd need to find a way to label them. You could simply choose the verb of the most common chain in a cluster.</p>

<p>If, however, you have a pre-defined set of relation types you want to extract and lots of texts manually annotated for these relations, then the approach could be very different, e.g., using machine learning to learn how to recognize a relation type based on annotated data.</p>
",3,9,10975,2013-03-06 23:27:10,https://stackoverflow.com/questions/15260212/nlp-to-find-relationship-between-entities
Load XML as an Annotation for Stanford CoreNLP?,"<p>There is lots of information on taking an Annotation and saving it to XML.  Is it possible to deserialize, i.e. load the XML into an Annotation?  For example, my use case is: I want to do cheap preprocessing like sentence splitting, then later decide to do more expensive parsing, and I'd like the original document text offsets to be correctly preserved.</p>
",stanford-nlp,"<pre>
---------- Forwarded message ----------
From: John Bauer
Date: Wed, Mar 13, 2013 at 1:52 AM
Subject: Re: [java-nlp-user] Can XML be deserialized into CoreMap etc. structures?
To: Brendan O'Connor
Cc: ""java-nlp-user@lists.stanford.edu"" 


No, there is no code to deserialize the XML output.
We're always interested in user extensions, though :)

John
</pre>
",0,0,238,2013-03-11 18:05:48,https://stackoverflow.com/questions/15345468/load-xml-as-an-annotation-for-stanford-corenlp
Minimal example for creating a warm StanfordNLP parser,"<p>I want a warm (already loaded) parser to parse inputs instead of creating a new instance each time I want to parse an input.</p>

<p>I want a parser that functions similarly to <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/</a> . I installed <code>stanford-corenlp</code> from Maven. I executed the <code>StanfordCoreNlpDemo</code> class. </p>

<p>But I am stuck at how I can embed the parser into my own program. Please provide a minimal example of creating a parser programatically.</p>
","java, nlp, stanford-nlp","<p>But bear in mind that:</p>

<ul>
<li><p>Stanford Core NLP != Stanford Parser; the former includes the parser along with other NLP tools.</p></li>
<li><p>Core NLP eats a great deal of your RAM!</p></li>
</ul>

<p>I've been trying to achieve the same. This is what I've got so far for a webservice, you could do something similar with a singleton.</p>

<pre><code>    public class NLPServlet extends HttpServlet {
    private StanfordCoreNLP pipeline;
    public void init(ServletConfig config) throws ServletException {
        super.init(config);
        try {
            Properties props = new Properties();
            props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            this.pipeline = new StanfordCoreNLP(props);
        } catch (Exception e) {
            System.err.println(""Error "" + e.getLocalizedMessage());
        }
    }
    public void doGet(HttpServletRequest req, HttpServletResponse resp)
            throws IOException {
        text=""blah, blah, blah."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }
}
</code></pre>
",1,1,718,2013-03-14 14:04:12,https://stackoverflow.com/questions/15411496/minimal-example-for-creating-a-warm-stanfordnlp-parser
java program to get parse score of a sentence using stanford parser,"<p>I am able to get the output of Tags and words for the sentence like ""My name is Rahul."" as </p>

<blockquote>
  <p>My/PRP$, name/NN, is/VBZ, Rahul/NNP, ./.]</p>
</blockquote>

<p>with the program:</p>

<pre><code>LexicalizedParser lp = LexicalizedParser.loadModel(
    ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz""
);
lp.setOptionFlags(new String[]{""-maxLength"", ""80"", ""-retainTmpSubcategories""});

String sent = ""My name is Rahul"";
Tree parse = (Tree) lp.apply(sent);

List taggedWords = parse.taggedYield();
System.out.println(taggedWords);
</code></pre>

<p>But, I also need to get the parse score of the sentence. Is there any kind of modification that I can do to my program to get the parse score? </p>

<p>Thanks.</p>
","java, parsing, nlp, stanford-nlp","<p>The <code>Tree</code> class has a <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html#score%28%29"" rel=""nofollow"">score</a> method you can call to get the score of the sentence.</p>

<pre><code>double score = parse.score();
</code></pre>
",4,4,2181,2013-03-15 11:23:11,https://stackoverflow.com/questions/15431139/java-program-to-get-parse-score-of-a-sentence-using-stanford-parser
How to get XML parse of a tree to a string?,"<pre><code>Tree tree = sentence.get(TreeAnnotation.class);
</code></pre>

<p>I want to get an XML representation of the parse tree into a string. How do I do it?</p>
",stanford-nlp,"<p>The string s contains the XML output of the parse tree""</p>

<pre><code>import java.io.StringWriter;
import java.io.PrintWriter;

StringWriter stringWriter = new StringWriter();
PrintWriter writer = new PrintWriter(stringWriter);
tree.indentedXMLPrint( writer, false);
String s = stringWriter.toString()
</code></pre>
",1,1,438,2013-03-18 07:06:45,https://stackoverflow.com/questions/15471379/how-to-get-xml-parse-of-a-tree-to-a-string
Training n-gram NER with Stanford NLP,"<p>Recently I have been trying to train n-gram entities with Stanford Core NLP. I have followed the following tutorials - <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#b"" rel=""noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#b</a></p>

<p>With this, I am able to specify only unigram tokens and the class it belongs to. Can any one guide me through so that I can extend it to n-grams. I am trying to extract known entities like movie names from chat data set.  </p>

<p>Please guide me through in case I have mis-interpretted the Stanford Tutorials and the same can be used for the n-gram training. </p>

<p>What I am stuck with is the following property</p>

<pre><code>#structure of your training file; this tells the classifier
#that the word is in column 0 and the correct answer is in
#column 1
map = word=0,answer=1
</code></pre>

<p>Here the first column is the word (unigram) and the second column is the entity, for example </p>

<pre><code>CHAPTER O
I   O
Emma    PERS
Woodhouse   PERS
</code></pre>

<p>Now that I need to train known entities (say movie names) like <strong>Hulk</strong>, <strong>Titanic</strong> etc as movies, it would be easy with this approach. But in case I need to train <strong>I know what you did last summer</strong> or <strong>Baby's day out</strong>, what is the best approach ? </p>
","nlp, stanford-nlp, opennlp, named-entity-recognition, named-entity-extraction","<p>It had been a long wait here for an answer. I have not been able to figure out the way to get it done using Stanford Core. However mission accomplished. I have used the LingPipe NLP libraries for the same. Just quoting the answer here because, I think someone else could benefit from it. </p>

<p>Please check out the <a href=""http://alias-i.com/lingpipe/web/licensing.html"" rel=""noreferrer"">Lingpipe licencing</a> before diving in for an implementation in case you are a developer or researcher or what ever.</p>

<p>Lingpipe provides various NER methods. </p>

<p>1) Dictionary Based NER</p>

<p>2) Statistical NER (HMM Based)</p>

<p>3) Rule Based NER etc.</p>

<p>I have used the Dictionary as well as the statistical approaches. </p>

<p>First one is a direct look up methodology and the second one being a training based. </p>

<p>An example for the dictionary based NER can be found <a href=""http://alias-i.com/lingpipe/demos/tutorial/ne/src/DictionaryChunker.java"" rel=""noreferrer"">here</a></p>

<p>The statstical approach requires a training file. I have used the file with the following format -</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;root&gt;
&lt;s&gt; data line with the &lt;ENAMEX TYPE=""myentity""&gt;entity1&lt;/ENAMEX&gt;  to be trained&lt;/s&gt;
...
&lt;s&gt; with the &lt;ENAMEX TYPE=""myentity""&gt;entity2&lt;/ENAMEX&gt;  annotated &lt;/s&gt;
&lt;/root&gt;
</code></pre>

<p>I then used the following code to train the entities. </p>

<pre class=""lang-java prettyprint-override""><code>import java.io.File;
import java.io.IOException;

import com.aliasi.chunk.CharLmHmmChunker;
import com.aliasi.corpus.parsers.Muc6ChunkParser;
import com.aliasi.hmm.HmmCharLmEstimator;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;

@SuppressWarnings(""deprecation"")
public class TrainEntities {

    static final int MAX_N_GRAM = 50;
    static final int NUM_CHARS = 300;
    static final double LM_INTERPOLATION = MAX_N_GRAM; // default behavior

    public static void main(String[] args) throws IOException {
        File corpusFile = new File(""inputfile.txt"");// my annotated file
        File modelFile = new File(""outputmodelfile.model""); 

        System.out.println(""Setting up Chunker Estimator"");
        TokenizerFactory factory
            = IndoEuropeanTokenizerFactory.INSTANCE;
        HmmCharLmEstimator hmmEstimator
            = new HmmCharLmEstimator(MAX_N_GRAM,NUM_CHARS,LM_INTERPOLATION);
        CharLmHmmChunker chunkerEstimator
            = new CharLmHmmChunker(factory,hmmEstimator);

        System.out.println(""Setting up Data Parser"");
        Muc6ChunkParser parser = new Muc6ChunkParser();  
        parser.setHandler( chunkerEstimator);

        System.out.println(""Training with Data from File="" + corpusFile);
        parser.parse(corpusFile);

        System.out.println(""Compiling and Writing Model to File="" + modelFile);
        AbstractExternalizable.compileTo(chunkerEstimator,modelFile);
    }

}
</code></pre>

<p>And to test the NER I used the following class</p>

<pre class=""lang-java prettyprint-override""><code>import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.util.ArrayList;
import java.util.Set;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.util.AbstractExternalizable;

public class Recognition {
    public static void main(String[] args) throws Exception {
        File modelFile = new File(""outputmodelfile.model"");
        Chunker chunker = (Chunker) AbstractExternalizable
                .readObject(modelFile);
        String testString=""my test string"";
            Chunking chunking = chunker.chunk(testString);
            Set&lt;Chunk&gt; test = chunking.chunkSet();
            for (Chunk c : test) {
                System.out.println(testString + "" : ""
                        + testString.substring(c.start(), c.end()) + "" &gt;&gt; ""
                        + c.type());

        }
    }
}
</code></pre>

<p>Code Courtesy : Google :)</p>
",26,23,16768,2013-03-25 06:59:22,https://stackoverflow.com/questions/15609324/training-n-gram-ner-with-stanford-nlp
Python 3.3: Process inlineXML,"<p>Whilst trying to tag named entities with the stanford NRE tool, I get this kind of output:</p>

<pre><code>A jury in &lt;ORGANIZATION&gt;Marion County Superior Court&lt;/ORGANIZATION&gt; was expected to begin deliberations in the case on &lt;DATE&gt;Wednesday&lt;/DATE&gt; or &lt;DATE&gt;Thursday&lt;/DATE&gt;.
</code></pre>

<p>Of course processing any XML without a root does not work, so I added this:</p>

<pre><code>&lt;root&gt;A jury in &lt;ORGANIZATION&gt;Marion County Superior Court&lt;/ORGANIZATION&gt; was expected to begin deliberations in the case on &lt;DATE&gt;Wednesday&lt;/DATE&gt; or &lt;DATE&gt;Thursday&lt;/DATE&gt;.&lt;/root&gt;
</code></pre>

<p>I tried building a tree with this method: <a href=""https://stackoverflow.com/questions/6476548/stripping-inline-tags-with-pythons-lxml"">stripping inline tags with python&#39;s lxml</a> but it does not work... It yields this error on the line <code>tree = etree.fromstring(text)</code>:</p>

<pre><code>lxml.etree.XMLSyntaxError: xmlParseEntityRef: no name, line 1, column 1793
</code></pre>

<p>Does anyone know a solution for this? Or perhaps another method which allows me to build a tree from any text with inlineXML tags, keeping only the tagged tokens and removing/ignoring the rest of the text.</p>
","python-3.x, xml-parsing, stanford-nlp, inline-xml","<p>In the end I did it without using a parser or a tree but just used regular expressions. This is the code that works nice and fast:</p>

<pre><code>import re
NER = ['TIME','LOCATION','ORGANISATION','PERSON','MONEY','PERCENT','DATA']
entities = {}
for cat in NER:
    regex_cat = re.compile('&lt;'+cat+'&gt;(.*?)&lt;/'+cat+'&gt;')
    entities[cat] = re.findall(regex_cat,data)
</code></pre>

<p>Here <code>data</code> is just a string of text. It uses regular expressions to find all entities of a category specified in <code>NER</code> and stores it as is list in a dictionary. This could be used for all inlineXML strings where <code>NER</code> is just a list of all possible tags in the string.</p>
",0,0,793,2013-03-29 10:59:17,https://stackoverflow.com/questions/15701906/python-3-3-process-inlinexml
How do I use python interface of Stanford NER(named entity recogniser)?,"<p>I want to use Stanford NER in python using pyner library. Here is one basic code snippet.</p>

<pre><code>import ner 
tagger = ner.HttpNER(host='localhost', port=80)
tagger.get_entities(""University of California is located in California, United States"")
</code></pre>

<p>When I run this on my local python console(IDLE). It should have given me an output like this</p>

<pre><code>  {'LOCATION': ['California', 'United States'],
 'ORGANIZATION': ['University of California']}
</code></pre>

<p>but when I execut this, it showed empty brackets. I am actually new to all this.</p>
","python-2.7, nlp, stanford-nlp, named-entity-recognition","<p>I am able to run the stanford-ner server in socket mode using: </p>

<pre><code>java -mx1000m -cp stanford-ner.jar edu.stanford.nlp.ie.NERServer \
    -loadClassifier classifiers/english.muc.7class.distsim.crf.ser.gz \
    -port 8080 -outputFormat inlineXML
</code></pre>

<p>and receive the following output from the command line:</p>

<pre><code>Loading classifier from 
/Users/roneill/stanford-ner-2012-11-11/classifiers/english.muc.7class.distsim.crf.ser.gz 
... done [1.7 sec].
</code></pre>

<p>Then in python repl:</p>

<pre><code>Python 2.7.2 (default, Jun 20 2012, 16:23:33) 
[GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import ner
&gt;&gt;&gt; tagger = ner.SocketNER(host='localhost', port=8080)
&gt;&gt;&gt; tagger.get_entities(""University of California is located in California, United States"")
{'ORGANIZATION': ['University of California'], 'LOCATION': ['California', 'United States']}
</code></pre>
",27,14,9039,2013-03-30 19:48:35,https://stackoverflow.com/questions/15722802/how-do-i-use-python-interface-of-stanford-nernamed-entity-recogniser
Increase performance of Stanford-tagger based program,"<p>I just implemented a program that uses the Stanford POS tagger in Java.</p>

<p>I used an input file of a few KB in size, consisting of a few hundred words. I even set the heap size to 600 MB.</p>

<p>But it is still slow and sometimes runs out of heap memory. How can I increase its execution speed and memory performance? I would like to be able to use a few MB as input.</p>

<pre><code>  public static void postag(String args) throws ClassNotFoundException

  {

     try

     {

     File filein=new File(""c://input.txt"");

     String content = FileUtils.readFileToString(filein);

     MaxentTagger tagger = new MaxentTagger(""postagging/wsj-0-18-bidirectional-distsim.tagger"");

     String tagged = tagger.tagString(content);

        try 
        {
            File file = new File(""c://output.txt"");
            if (!file.exists()) 
            {
                file.createNewFile();
            } 

            FileWriter fw = new FileWriter(file.getAbsoluteFile());
            BufferedWriter bw = new BufferedWriter(fw);
            bw.write(""\n""+tagged);
            bw.close();

            }
              catch (IOException e) 
              {
                    e.printStackTrace();
               }

     } catch (IOException e1)
     {
         e1.printStackTrace();
     }

 }
</code></pre>
","java, nlp, pos-tagger, stanford-nlp","<p>The main first piece of advice is to use the <code>wsj-0-18-left3words-distsim.tagger</code> (or probably better, the <code>english-left3words-distsim.tagger</code> in recent versions, for general text), rather than the <code>wsj-0-18-bidirectional-distsim.tagger</code>. While the tagging performance of the bidirectional tagger is <em>fractionally</em> better, it is about 6 times slower and uses about twice as much memory. A figure FWIW: on a 2012 MacBook Pro, when given enough text to ""warm up"" the <code>left3words</code> tagger will tag text at about 35000 words per second.</p>

<p>The other piece of advice on memory use is that if you have a large amount of text, make sure you pass it to <code>tagString()</code> in reasonable-sized chunks, not all as one huge String, since that whole String will be tokenized at once, adding to the memory requirements.</p>
",8,8,694,2013-03-31 06:53:41,https://stackoverflow.com/questions/15727144/increase-performance-of-stanford-tagger-based-program
Extract attributes and values from XML file in perl,"<p>This is part of the output XML file I get as output from Stanford CoreNLP:</p>

<pre><code>&lt;collapsed-ccprocessed-dependencies&gt;  
      &lt;dep type=""nn""&gt;
        &lt;governor idx=""25""&gt;Mullen&lt;/governor&gt;
        &lt;dependent idx=""24""&gt;Ms.&lt;/dependent&gt;
      &lt;/dep&gt;
      &lt;dep type=""nsubj""&gt;
        &lt;governor idx=""26""&gt;said&lt;/governor&gt;
        &lt;dependent idx=""25""&gt;Mullen&lt;/dependent&gt;
      &lt;/dep&gt;
    &lt;/collapsed-ccprocessed-dependencies&gt;
  &lt;/sentence&gt;
&lt;/sentences&gt;
&lt;coreference&gt;
  &lt;coreference&gt;
    &lt;mention representative=""true""&gt;
      &lt;sentence&gt;1&lt;/sentence&gt;
      &lt;start&gt;1&lt;/start&gt;
      &lt;end&gt;2&lt;/end&gt;
      &lt;head&gt;1&lt;/head&gt;
    &lt;/mention&gt;
    &lt;mention&gt;
      &lt;sentence&gt;1&lt;/sentence&gt;
      &lt;start&gt;33&lt;/start&gt;
      &lt;end&gt;34&lt;/end&gt;
      &lt;head&gt;33&lt;/head&gt;
    &lt;/mention&gt;
  &lt;/coreference&gt;
 &lt;/coreference&gt;
&lt;mention representative=""true""&gt;
      &lt;sentence&gt;1&lt;/sentence&gt;
      &lt;start&gt;6&lt;/start&gt;
      &lt;end&gt;9&lt;/end&gt;
      &lt;head&gt;8&lt;/head&gt;
    &lt;/mention&gt;
    &lt;mention&gt;
      &lt;sentence&gt;1&lt;/sentence&gt;
      &lt;start&gt;10&lt;/start&gt;
      &lt;end&gt;11&lt;/end&gt;
      &lt;head&gt;10&lt;/head&gt;
    &lt;/mention&gt;
  &lt;/coreference&gt;
  &lt;coreference&gt;   
</code></pre>

<p>How do I parse it using Perl so that I get something like this:</p>

<pre><code>1. sentence 1, head 1
   sentence 1, head 33
2. sentence 1, head 8
   sentence 1, head 10
</code></pre>

<p>I have tried with XML::Simple but the output is not easily understandable. Here is what I did: 
    use XML::Simple;
    use Data::Dumper;</p>

<pre><code>$outfile = $filename."".xml"";
$xml = new XML::Simple;

$data = $xml -&gt; XMLin($outfile);
print Dumper($data);
</code></pre>
","perl, xml-parsing, stanford-nlp","<p>Regrettably, <code>XML::Simple</code> was first to stake its claim for the <code>Simple</code> namespace. It is perhaps simple in implementation but not so simple in use except in the most trivial of cases. If you want something similar, then <a href=""https://metacpan.org/module/XML%3a%3aSmart"" rel=""nofollow""><code>XML::Smart</code></a> offers a nested data-structure API but does it a lot better.</p>

<p>Thankfully there is a lot of choice for excellent Perl XML modules. <a href=""https://metacpan.org/module/XML%3a%3aTwig"" rel=""nofollow""><code>XML::Twig</code></a> is one of these, and it allows you to specify callback subroutines that will be executed when specific elements within the XML data are encountered during parsing.</p>

<p>This program uses <code>XML::Twig</code>, and sets a callback on <code>coreference[mention]</code>, i.e. <code>coreference</code> elements that have at least one <code>mention</code> child.</p>

<p>The code in the handler subroutine makes no checks and assumes that there will always be at least two <code>mention</code> child elements, each with a <code>sentence</code> and a <code>header</code> element. The text values of these nodes are output in the format you have described.</p>

<pre><code>use strict;
use warnings;

use XML::Twig;

my $twig = XML::Twig-&gt;new(twig_handlers =&gt; {
  'coreference[mention]' =&gt; \&amp;handle_coreference
});
$twig-&gt;parsefile('myxml.xml');

my $n;
sub handle_coreference {

  my ($twig, $elt) = @_;

  my @mentions = $elt-&gt;children('mention');

  for my $i (0 .. $#mentions) {
    printf ""%s sentence %d, head %d\n"",
      $i == 0 ? sprintf '%3d.', ++$n : '    ',
      map $mentions[$i]-&gt;first_child_trimmed_text($_), qw/ sentence head /;
  }
}
</code></pre>

<p><strong>output</strong></p>

<pre><code>  1. sentence 1, head 1
     sentence 1, head 33
  2. sentence 1, head 8
     sentence 1, head 10
</code></pre>
",2,0,6874,2013-04-08 21:10:47,https://stackoverflow.com/questions/15889055/extract-attributes-and-values-from-xml-file-in-perl
Creating own POS Tagger,"<p>I have found the <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""noreferrer"">Stanford POS Tagger</a> pretty good, but somehow I found myself in need of creating my own POS tagger.</p>

<p>For the last two weeks, I am rambling here and there, on whether to start from parsing tree, or once we have a pos tagger than we can parse tree, using ugly CFGs and NFAs so that they can help me in creating a POS tagger and what not.</p>

<p>I am ending the question here, asking seniors, where to begin POS tagging.
(language of choice is Python, but C and JAVA won't hurt).</p>
","java, python, c, nlp, stanford-nlp","<p>It depends on what your ultimate goal is.</p>

<p><strong>If the goal is to perform syntax analysis,</strong> i.e. to determine the subject, the predicate, its arguments, its modifiers etc., and then to possibly even perform a semantic analysis, then you should not worry about the POS tagger. Instead you should first look at the various methods for syntactic analysis &ndash; in particular, phrase structure based methods, probabilistic methods, and finite-state methods &ndash; and determine the tool you want to use for that. The decision will depend on what your speed and accuracy requirements are, how much time you will have for long-term improvement and maintenance, and other factors. Once you've decided on the right tool (or implementation strategy) for that, you may end up not needing a tagger any more. The reason is that many syntax analysis strategies fundamentally don't need a tagger: They only perform a dictionary lookup, which returns, for each word, one or more possible POS tags; the disambiguation (i.e. deciding which of these tags is actually correct) is performed implicitly by the syntax analyser. <em>Some</em> syntax analysers may expect that you apply a POS tagger after the dictionary lookup, but they will also tell you which one to use, so the answer to your question will then follow quite naturally.</p>

<p><strong>If, on the other hand, your goal does not require a full-fledged syntax analysis,</strong> but only part-of-speech tagging, I'd suggest that you first look at existing alternatives before deciding to make your own one. Possible choices include, but are not limited to:</p>

<ul>
<li><a href=""http://nlp.stanford.edu/software/tagger.shtml"">Stanford Tagger</a></li>
<li><a href=""http://mallet.cs.umass.edu/sequences.php"">Mallet Simple Tagger</a> </li>
<li><a href=""http://gate.ac.uk/sale/tao/splitch6.html#chap%3aannie"">ANNIE tagger</a> (This is a Brill-style tagger embedded into a larger NLP framework)</li>
<li><a href=""http://www.ims.uni-stuttgart.de/forschung/ressourcen/werkzeuge/treetagger.html"">TreeTagger</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Brill_tagger"">Brill tagger</a></li>
<li><a href=""http://ml.nec-labs.com/senna/"">SENNA tagger</a></li>
</ul>

<p>Which one is right for your needs depends on a number of factors, not necessarily in this prioritized order:</p>

<ol>
<li><p><strong>Opaqueness:</strong> Do you intend to make corrections to improve the results, possibly by maintaining exception lists and post-correction rules, possibly over a long period of time? In this case, you may need a tagger that is not only open-source, but uses a methodology that enables manual modifications to the disambiguation strategy it uses. This is easier in a rule-based or TBL tagger (such as the Brill tagger), and to some extent taggers based on decision-tree learning (such as the TreeTagger); it is more difficult and possibilities are more limited in Hidden-Markov-Model based (HMM) taggers and taggers based on conditional random fields (CRF) (such as the Mallet Simple Tagger), and very difficult (except for pure post-correction exception lists) in taggers based on neural networks (such as SENNA).</p></li>
<li><p><strong>Target language:</strong> Do you need it just for English, or other languages as well? The TreeTagger has out-of-the-box support for many European languages, but the others in the list above don't. Adding support for a language will always require a dictionary, it will usually require an annotated training corpus (which may be expensive), and it will sometimes require that you write or modify a few hundred initial rules (e.g. if a Brill-tagger approach is used).</p></li>
<li><p><strong>Framework and programming language:</strong> Mallet and Stanford are for Java, the TreeTagger is in C (but not open-source; there are Python and Java wrappers, but they may cause significant slow-down and have other issues<sup>(&ddagger;)</sup>), SENNA is in C and open-source, ANNIE is in Java and made for the GATE framework, and so on. There are differences in the environment these taggers require, and moving them out of their natural habitat can be painful. NLTK (Python) has wrappers for some of them, but they typically don't involve an actual embedding of the source into Python; instead they simply perform a system call for each piece of text you want to tag. This has severe performance implications.</p></li>
<li><p><strong>Speed:</strong> If you only process a few sentences per second, any tagger will be able to handle that. But if you are dealing with terabytes of data or need to cope with extreme peaks in usage, you need to perform the right kind of stress tests as part of your evaluation and decision making. I know from personal experience that TreeTagger and SENNA are very fast, Stanford is quite a bit slower, and NLTK-wrappers are often several orders of magnitude slower. In any case, you need to test. Note that POS tagging can be parallized in a straight-forward way by dividing the input into partitions and running several tagging processes in parallel. Memory footprint is usually not an issue for the tagger itself (but it can be if the tagger is part of a general NLP framework that is loaded into memory completely).</p></li>
</ol>

<p>Finally, if none of the existing taggers meets your needs and you really decide to create your own tagger, you'll still need to make a decision similar to the above: The right approach depends on accuracy, speed, maintenance and multi-linguality related factors. The main approaches to POS tagging are quite well-represented by the list of examples above, i.e. Rule/TBL-style (Brill), HMM/CRF (Mallet), entropy-based (Stanford), decision-tree learning (TreeTagger), neural network (SENNA). Even if you decide to make your own, it's a good idea to study some of the existing ones to understand how they operate and where the problems are.</p>

<p>As a final remark on multi-linguality: Classic POS-taggers such as the above require that you tokenize the input before you apply the tagger (or they implicitly perform a simple tokenization). This won't work with languages that cannot be tokenized using punctuation and white-space as token boundaries, i.e. Chinese, Japanese, Thai, to some extent Korean, and a few other languages. For those, you'll need to use a specialised tokenizer, and those tokenizers usually perform both tokenization <em>and</em> POS-tagging in one step.</p>

<hr>

<p><sup>(&ddagger;)</sup> I don't know about the <a href=""https://code.google.com/p/tt4j/"">Java wrapper</a>, but the <a href=""http://perso.limsi.fr/pointal/dev%3atreetaggerwrapper"">Python wrapper</a> had several problems the last time I checked (approx. 1 year ago): It only worked with Python 2, it used system calls in a fairly complicated way, which was necessary to ensure that the Tree Tagger flushed its buffers after each input is processed. The latter has two consequences: The processing is slower than when using the Tree Tagger directly, and not the full pipeline of command-line tools can be used for some languages, because the buffer-flushing gets too complicated then.</p>
",18,7,5720,2013-04-10 09:01:50,https://stackoverflow.com/questions/15921417/creating-own-pos-tagger
Stanford parser output to SVG,"<p>I am using <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">Stanford Parser</a>. With provided GUI tool it outputs grafic tree from the sentence - <a href=""http://screencast.com/t/3xqeJIQSxA"" rel=""nofollow"">like this</a> but when saving output it just gives bracketed format like this <code>(ROOT (NP (NP (DT The) (NN capability))...</code>.
Is it possible to get the same output(SVG) using command line?
If not, maybe there is some other tools for that? Maybe getting DOT file first and use Graphviz to get SVG in the end!?</p>
","svg, stanford-nlp","<p>In the end I wrote my own script to get DOT format and used it further with Graphviz to get SVG output.</p>

<pre><code>public static void main(String[] args)
        {
            Properties props = new Properties();
            props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

            Annotation document = new Annotation(args[0]);

            pipeline.annotate(document);

            List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

            for (CoreMap sentence : sentences) {
                SemanticGraph dependencies = sentence.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
                System.out.println(dependencies.toDotFormat());
            }
        }
</code></pre>
",0,0,299,2013-04-13 11:52:57,https://stackoverflow.com/questions/15987497/stanford-parser-output-to-svg
Extract clause form sentence,"<p>I want to extract subordinate clause,main clause,relative clause,restrictive relative clause,non-restrictive relative clause from sentences but I don't know how doing this work. for example:</p>

<p>""I first saw her in Paris, where I lived in the early nineties.""<br>
[main clause][relative clause]</p>

<p>""She held out the hand that was hurt.""<br>
[main clause][restrictive relative clause]</p>

<p>please help me to do this work?</p>
","nlp, stanford-nlp, opennlp, sentence","<p>You should be able to extract this information (to a reasonable accuracy) with a constituency parse. There are a number of implementations listed <a href=""http://aclweb.org/aclwiki/index.php?title=Parsers_for_English"" rel=""nofollow"">here</a>. To that list, I'd add my own, available for download <a href=""https://code.google.com/p/bubs-parser/"" rel=""nofollow"">here</a>. t's faster than any other high-accuracy parser available, and documented reasonably well (for research code).</p>
",4,4,3215,2013-04-13 20:52:59,https://stackoverflow.com/questions/15992813/extract-clause-form-sentence
Stanford Dependency Parser - how to get spans?,"<p>I'm doing dependency parsing with the Stanford library in Java.
Is there any way to get back the indices within my original string of a dependency?
I have tried to call the getSpans() method, but it returns null for every token:</p>

<pre><code>LexicalizedParser lp = LexicalizedParser.loadModel(
        ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"",
        ""-maxLength"", ""80"", ""-retainTmpSubcategories"");
TreebankLanguagePack tlp = new PennTreebankLanguagePack();
GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
Tree parse = lp.apply(text);
GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
Collection&lt;TypedDependency&gt; tdl = gs.typedDependenciesCollapsedTree();
for(TypedDependency td:tdl)
{
      td.gov().getSpan()  // it's null!
      td.dep().getSpan()  // it's null!
}
</code></pre>

<p>Any idea?</p>
","java, parsing, nlp, stanford-nlp","<p>I've finally ended up writing my own helper function to get the spans out my original string:</p>

<pre><code>public HashMap&lt;Integer, TokenSpan&gt; getTokenSpans(String text, Tree parse)
{
    List&lt;String&gt; tokens = new ArrayList&lt;String&gt;();
    traverse(tokens, parse, parse.getChildrenAsList());
    return extractTokenSpans(text, tokens);
}

private void traverse(List&lt;String&gt; tokens, Tree parse, List&lt;Tree&gt; children)
{
    if(children == null)
        return;
    for(Tree child:children)
    {
        if(child.isLeaf())
        {
            tokens.add(child.value());
        }
        traverse(tokens, parse, child.getChildrenAsList());         
    }
}

private HashMap&lt;Integer, TokenSpan&gt; extractTokenSpans(String text, List&lt;String&gt; tokens)
{
    HashMap&lt;Integer, TokenSpan&gt; result = new HashMap&lt;Integer, TokenSpan&gt;();
    int spanStart, spanEnd;

    int actCharIndex = 0;
    int actTokenIndex = 0;
    char actChar;
    while(actCharIndex &lt; text.length())
    {
        actChar = text.charAt(actCharIndex);
        if(actChar == ' ')
        {
            actCharIndex++;
        }
        else
        {
            spanStart = actCharIndex;
            String actToken = tokens.get(actTokenIndex);
            int tokenCharIndex = 0;
            while(tokenCharIndex &lt; actToken.length() &amp;&amp; text.charAt(actCharIndex) == actToken.charAt(tokenCharIndex))
            {
                tokenCharIndex++;
                actCharIndex++;
            }

            if(tokenCharIndex != actToken.length())
            {
                //TODO: throw exception
            }
            actTokenIndex++;
            spanEnd = actCharIndex;
            result.put(actTokenIndex, new TokenSpan(spanStart, spanEnd));
        }
    }
    return result;
}
</code></pre>

<p>Then I will call </p>

<pre><code> getTokenSpans(originalString, parse)
</code></pre>

<p>So I get a map, which can map every token to its corresponding token span.
It's not an elegant solution, but at least it works.</p>
",2,4,1035,2013-04-16 00:36:40,https://stackoverflow.com/questions/16026881/stanford-dependency-parser-how-to-get-spans
How to resolve Stanford Charniak Parser dependency,"<p>I am trying to run the Charniak parser provided in <code>stanford-corenlp-1.3.5.jar</code>.  The package for the record is <code>edu.stanford.nlp.parser.charniak</code> and the class <code>CharniakParser</code>.</p>

<p>So I will give a code example of how I am trying to use it just for completeness:</p>

<pre><code>CharniakParser cp = new CharniakParser();
PTBTokenizer&lt;HasWord&gt; ptbt = new PTBTokenizer(new FileReader(""sample1.txt""), new WordTokenFactory(), """");
List&lt;Word&gt; tokens = new ArrayList&lt;Word&gt;();
for (Word token; ptbt.hasNext(); ) {
    token = (Word) ptbt.next();
    tokens.add(token);
}
Tree t = cp.getBarseParse(tokens);
</code></pre>

<p>On the last line there when running this code , I get </p>

<p><code>cannot run program ""/u/nlp/packages/bllip-parser/reranking-parser.sh"" ... The system cannot find the specified file</code></p>

<p>The problem is the human cannot find the specified file either.  I do not see that included in the distribution and I cannot find a dependency that I might need to download with that name.  A Google search reveals that the only place a 'reranking-parser.sh' exists is in the actual source code for Stanford's version of Charniak parser (I am trying hard to be clear because I know Stanford did not originally create the Charniak parser, it is from Brown).</p>

<p>So, does anyone have experience with this parser? What is missing?  I like my chances better on SO, so I ask here.</p>
","java, stanford-nlp, charniak-parser","<p>Ok, ok, @DMoses et al.  I subscribed to the mailing list and I got my answer as to what needs to be done.  So let me pass it along as answer for future visitors since it is not documented (until now):</p>

<p>The file <code>reranking-parser.sh</code> belongs to a <em>particular</em> version of the Charniak parser.  So you need to get <em>that version</em> of the parser which is on github and called ""bllip-parser"".  There is no official support in CoreNLP for the Charniak parser, nor is the functionality provided meant to be standalone, rather it is a student extension meant to work with the external parser from github.</p>

<p>A fairly simple procedure really to point to the C++ executable, but this does not work for me because at this point I would be using Python to call Java to call C++.</p>
",2,3,705,2013-04-17 15:29:31,https://stackoverflow.com/questions/16064157/how-to-resolve-stanford-charniak-parser-dependency
Stanford POS Tagger not tagging Chinese text,"<p>I'm using Stanford POS Tagger (for the first time) and while it tags English correctly, it does not seem to recognize (Simplified) Chinese even when changing the model parameter. Have I overlooked something?</p>
<p>I've downloaded and unpacked the latest full version from here:
<a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/tagger.shtml</a></p>
<p>Then I've inputed sample text into the &quot;sample-input.txt&quot;.</p>
<blockquote>
<p>这是一个测试的句子。这是另一个句子。</p>
</blockquote>
<p>Then I simply run</p>
<blockquote>
<p>./stanford-postagger.sh models/chinese-distsim.tagger sample-input.txt</p>
</blockquote>
<p>The expected output is to tag each of the words with a part of speech, but instead it recognizes the entire string of text as one word:</p>
<blockquote>
<p>Loading default properties from tagger models/chinese-distsim.tagger</p>
<p>Reading POS tagger model from models/chinese-distsim.tagger ... done [3.5 sec].</p>
<p>這是一個測試的句子。這是另一個句子。#NR</p>
<p>Tagged 1 words at 30.30 words per second.</p>
</blockquote>
<p>I appreciate any help.</p>
","linux, nlp, stanford-nlp, pos-tagger","<p>I finally realized that tokenization/segmentation is not included in this pos tagger. It appears the words must be space delimited before feeding them to the tagger. For those interested in maximum entropy word segmentation of Chinese, there is a separate package available here:</p>

<p><a href=""http://nlp.stanford.edu/software/segmenter.shtml"">http://nlp.stanford.edu/software/segmenter.shtml</a></p>

<p>Thanks everyone.</p>
",6,5,934,2013-04-18 04:00:16,https://stackoverflow.com/questions/16074238/stanford-pos-tagger-not-tagging-chinese-text
How to detect that two sentences are similar?,"<p>I want to compute how similar two arbitrary sentences are to each other.  For example:</p>

<blockquote>
  <ol>
  <li>A mathematician found a solution to the problem.</li>
  <li>The problem was solved by a young mathematician.</li>
  </ol>
</blockquote>

<p>I can use a tagger, a stemmer, and a parser, but I don’t know how detect that these sentences are similar.</p>
","nlp, similarity, stanford-nlp, opennlp","<p>These two sentences are not just similar, they are almost <a href=""http://en.wikipedia.org/wiki/Paraphrase"">paraphrases</a>, i.e., two alternative ways of expressing the same meaning. It is also a very simple case of paraphrase, in which both utterances use the same words with the only exception of one being in active form while the other is passive. (The two sentences are not exactly paraphrases because in the second sentence the mathematician is ""young"". This additional information makes the semantic relation between the two sentences non symmetric. In these cases, you would say that the second utterance <a href=""http://en.wikipedia.org/wiki/Textual_entailment"">""entails""</a> the first one, or in other words that the first can be inferred from the second).</p>

<p>From the example it is not possible to understand whether you are actually interested in paraphrase detection, textual entailment or in sentence similarity in general, which is an even broader and fuzzier problem. For example, is ""people eat food"" more similar to ""people eat bread"" or to ""men eat food""? </p>

<p>Both paraphrase detection and text similarity are complex, open research problems in Natural Language Processing, with a large and active community of researchers working on them. It is not clear what is the extent of your interest in this topic, but consider that even though many brilliant researchers have spent and spend their whole careers trying to crack it, we are still very far from finding sound solutions that just work in general.</p>

<p>Unless you are interested in a very superficial solution that would only work in specific cases and that would not capture syntactic alternation (as in this case), I would suggest that you look into the problem of text similarity in more depth. A good starting point would be  the book <a href=""http://nlp.stanford.edu/fsnlp/"">""Foundations of Statistical Natural Language Processing""</a>, which provides a very well organised presentation of most statistical natural language processing topics. Once you have clarified your requirements (e.g., under what conditions is your method supposed to work? what levels of precision/recall are you after? what kind of phenomena can you safely ignore, and which ones you need to account for?) you can start looking into specific approaches by diving into recent research work. Here, a good place to start would be the <a href=""http://aclweb.org/anthology/"">online archives of the Association for Computational Linguistics (ACL)</a>, which is the publisher of most research results in the field.</p>

<p>Just to give you something practical to work with, a very rough baseline for sentence similarity would be the <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"">cosine similarity</a> between two binary vectors representing the sentences as bags of words. A bag of word is a very simplified representation of text, commonly used for information retrieval, in which you completely disregard syntax and only represent a sentence as a vector whose size is the size of the vocabulary (i.e., the number of words in the language) and whose component ""i"" is valued ""1"" if the word at position ""i"" in the vocabulary appears in the sentence, and ""0"" otherwise.</p>
",33,28,18286,2013-04-21 16:04:46,https://stackoverflow.com/questions/16133184/how-to-detect-that-two-sentences-are-similar
German stanford parser,"<p>I am using the german version of the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">stanford parser</a> to parse short german sentences into a parse tree. An example:</p>

<pre><code>Hallo, mein Name ist Luke.

(ROOT
  (S (ITJ Hallo) ($, ,)
    (NP (PPOSAT mein) (NN Name))
    (VAFIN ist)
    (NP-SB (NE Luke))
    ($. .)))
</code></pre>

<p>I would like to take this output and use it as input for a Haskell program which would arrange it into a tree structure.</p>

<p>Trying to write a Haskell program that parses this output, or any such output from the german  parser would be hard. Does anyone have any advice for any shortcuts, libraries or the like that would make this easier?</p>

<p>I would appreciate not being told to 'just use Java'.</p>
","java, parsing, haskell, stanford-nlp","<p>The generally suggested library for these things is parsec. <a href=""http://www.haskell.org/haskellwiki/Parsec"" rel=""nofollow"">http://www.haskell.org/haskellwiki/Parsec</a></p>
",1,1,780,2013-04-23 18:29:14,https://stackoverflow.com/questions/16176596/german-stanford-parser
Using Stanford Core NLP in PHP?,"<p>In Ruby, there is this <a href=""https://github.com/louismullie/stanford-core-nlp"" rel=""nofollow"">this</a> nice Stanford Core NLP integration gem which simply acts as a bridge to the JVM version. In Python, there is <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""nofollow"">this</a> one, which while slightly more complicated, appears to do basically the same thing. </p>

<p>Does anyone know if there is a way to do this in PHP? </p>
","php, nlp, stanford-nlp, php-java-bridge, java-interop","<p>In a past project, we tried searching for a PHP library that would work well with Core NLP to no avail. We ended up writing our own wrapper by creating methods that would just run an exec command and get us some of the base functionality we needed. </p>
",1,1,800,2013-04-28 23:32:52,https://stackoverflow.com/questions/16269174/using-stanford-core-nlp-in-php
Stanford Core NLP missing ROOTs,"<p>From online demo <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"">Stanford CoreNLP</a> with example sentence ""A minimal software item that can be tested in isolation"" it gives Collapsed dependencies with CC processed as following:</p>

<pre><code>root ( ROOT-0 , item-4 )
det ( item-4 , A-1 )
amod ( item-4 , minimal-2 )
nn ( item-4 , software-3 )
nsubjpass ( tested-8 , that-5 )
aux ( tested-8 , can-6 )
auxpass ( tested-8 , be-7 )
rcmod ( item-4 , tested-8 )
prep_in ( tested-8 , isolation-10 )
</code></pre>

<p>From my Java class I get the same except root(...). The code I am running is as following:</p>

<pre><code>public static void main(String[] args)
    {
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        Annotation document = new Annotation(args[0]);

        pipeline.annotate(document);

        List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

        for (CoreMap sentence : sentences) {
            SemanticGraph dependencies = sentence.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
            System.out.println(dependencies.toList());
        }
    }
</code></pre>

<p>So the question is why my Java code doesnt output root`s!? Am I missing something?</p>
","dependencies, root, stanford-nlp","<p>This is a good question, in the sense that it exposes a badness in the current code. At present, a root node and an edge from it are not stored in the graph.* Instead, they have to be accessed separately as a root/list of roots of the graph, stored as a separate list.  Here are two things that will work: (1) Add this code above the <code>System.out.println</code>:</p>

<pre><code>IndexedWord root = dependencies.getFirstRoot();
System.out.printf(""ROOT(root-0, %s-%d)%n"", root.word(), root.index());
</code></pre>

<p>(2) Use instead of your current line:</p>

<pre><code>System.out.println(dependencies.toString(""readable""));
</code></pre>

<p>Unlike the other <code>toList()</code> or <code>toString()</code> methods, it does print the root(s).</p>

<p>*There are historical reasons for this: We used to not have any explicit root. But at this point the behavior is awkward and dysfunctional and should be changed. It'll probably happen in a future release.</p>
",3,1,1142,2013-04-30 12:35:13,https://stackoverflow.com/questions/16300056/stanford-core-nlp-missing-roots
gender identification in natural language processing,"<p>I have written below code using stanford nlp packages.</p>

<pre><code>GenderAnnotator myGenderAnnotation = new GenderAnnotator();
myGenderAnnotation.annotate(annotation);
</code></pre>

<p>But for the sentence ""Annie goes to school"", it is not able to identify the gender of Annie. </p>

<p>The output of application is:</p>

<pre><code>     [Text=Annie CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNP Lemma=Annie NamedEntityTag=PERSON] 
     [Text=goes CharacterOffsetBegin=6 CharacterOffsetEnd=10 PartOfSpeech=VBZ Lemma=go NamedEntityTag=O] 
     [Text=to CharacterOffsetBegin=11 CharacterOffsetEnd=13 PartOfSpeech=TO Lemma=to NamedEntityTag=O] 
     [Text=school CharacterOffsetBegin=14 CharacterOffsetEnd=20 PartOfSpeech=NN Lemma=school NamedEntityTag=O] 
     [Text=. CharacterOffsetBegin=20 CharacterOffsetEnd=21 PartOfSpeech=. Lemma=. NamedEntityTag=O]
</code></pre>

<p>What is the correct approach to get the gender?</p>
","nlp, stanford-nlp","<p>If your named entity recognizer outputs <code>PERSON</code> for a token, you might use (or build if you don't have one) a gender classifier based on first names. As an example, see the <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow"">Gender Identification</a> section from the NLTK library tutorial pages. They use the following features:</p>

<ul>
<li>Last letter of name.</li>
<li>First letter of name.</li>
<li>Length of name (number of characters).</li>
<li>Character unigram presence (boolean whether a character is in the name).</li>
</ul>

<p>Though, I have a hunch that using character n-gram frequency---possibly up to character trigrams---will give you pretty good results.</p>
",5,4,9079,2013-05-01 17:22:07,https://stackoverflow.com/questions/16323078/gender-identification-in-natural-language-processing
Stanford CoreNLP : Get CharacterOffset Annotation from Parse Tree,"<p>Using the parser output from another parser, I have created the parse tree for a sentence. Now, I need to find the character offset for each Noun Phrase that occurs in the parse.</p>

<p>How can I go about that? </p>
","java, stanford-nlp","<p>Take a subtree that corresponds to a Noun Phrase. Get the leaves of this tree:</p>

<pre><code>List&lt;Tree&gt; leaves = tree.getLeaves();
</code></pre>

<p>Then take the starting point of the first leaf (<em>CharacterOffsetBeginAnnotation</em> value) and the end point of the last leaf (<em>CharacterOffsetEndAnnotation</em>). The resulting interval is the offset of an NP.</p>

<p>To get the offset value, take the leaf's label and cast it to HasOffset:</p>

<pre><code>Label label = firstLeaf.label();
HasOffset ofs = (HasOffset) label;
int start = ofs.beginPosition();
</code></pre>

<p>This works for Stanford CoreNLP 3.2.0.</p>
",3,0,1143,2013-05-11 22:37:33,https://stackoverflow.com/questions/16502378/stanford-corenlp-get-characteroffset-annotation-from-parse-tree
How to use Stanford parser,"<p>I downloaded the Stanford parser 2.0.5 and use Demo2.java source code that is in the package, but After I compile and run the program it has many errors. 
A part of my program is:</p>

<pre><code>public class testStanfordParser {
/** Usage: ParserDemo2 [[grammar] textFile] */
  public static void main(String[] args) throws IOException {
    String grammar = args.length &gt; 0 ? args[0] : ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"";
    String[] options = { ""-maxLength"", ""80"", ""-retainTmpSubcategories"" };
    LexicalizedParser lp = LexicalizedParser.loadModel(grammar, options);
    TreebankLanguagePack tlp = new PennTreebankLanguagePack();
    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
 ...
</code></pre>

<p>the errors are:</p>

<pre><code>Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz java.io.IOException: Unable to resolve edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"" as either class path, filename or URL
at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:408)
at edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:356)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:594)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:389)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:157)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:143)
at testStanfordParser.main(testStanfordParser.java:19).                                             Loading parser from text file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz Exception in thread ""main"" java.lang.NoSuchMethodError: edu.stanford.nlp.io.IOUtils.readerFromString(Ljava/lang/String;)Ljava/io/BufferedReader;
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTextFile(LexicalizedParser.java:528)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:391)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:157)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:143)
at testStanfordParser.main(testStanfordParser.java:19)
</code></pre>

<p>please help me to solve it.
Thanks</p>
","java, eclipse, parsing, nlp, stanford-nlp","<p>I am using Stanford parser to extract entities like name ,location,organization.</p>

<p>Here is my code:</p>

<pre><code>public class stanfrdIntro {

    public static void main(String[] args) throws IOException, SAXException,
  {

        String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";


        AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier
                .getClassifierNoExceptions(serializedClassifier);

       String s1 = ""Good afternoon Rahul Kulhari, how are you today?"";

       s1 = s1.replaceAll(""\\s+"", "" "");
       String  t=classifier.classifyWithInlineXML(s1);
    System.out.println(Arrays.toString(getTagValues(t).toArray()));

    }
       private static final Pattern TAG_REGEX = Pattern.compile(""&lt;PERSON&gt;(.+?)&lt;/PERSON&gt;"");

private static Set&lt;String&gt; getTagValues(final String str) {
    final Set&lt;String&gt; tagValues = new HashSet&lt;String&gt;();
    //final Set&lt;String&gt; tagValues = new TreeSet();
    final Matcher matcher = TAG_REGEX.matcher(str);
    while (matcher.find()) {
        tagValues.add(matcher.group(1));
    }

    return tagValues;
}
</code></pre>

<p>This might help you but i am extracting only entities.</p>
",2,8,11512,2013-05-13 13:17:55,https://stackoverflow.com/questions/16523067/how-to-use-stanford-parser
Obtain multiple taggings with Stanford POS Tagger,"<p>I'm performing POS tagging with the <a href=""http://nlp.stanford.edu/software/tagger.shtml"">Stanford POS Tagger</a>. The tagger only returns one possible tagging for the input sentence. For instance, when provided with the input sentence ""The clown weeps."", the POS tagger produces the (erroneous) ""The_DT clown_NN weeps_NNS ._."".</p>

<p>However, my application will try to parse the result, and may reject a POS tagging because there is no way to parse it. Hence, in this example, it would reject ""The_DT clown_NN weeps_NNS ._."" but would accept ""The_DT clown_NN weeps_VBZ ._."" which I assume is a lower-confidence tagging for the parser.</p>

<p>I would therefore like the POS tagger to provide multiple hypotheses for the tagging of each word, annotated by some kind of confidence value. In this way, my application could choose the POS tagging with highest confidence that achieves a valid parsing for its purposes.</p>

<p>I have found no way to ask the Stanford POS Tagger to produce multiple (n-best) tagging hypotheses for each word (or even for the whole sentence). Is there a way to do this? (Alternatively, I am also OK with using another POS tagger with comparable performance that would have support for this.)</p>
","nlp, stanford-nlp, pos-tagger","<p><a href=""https://opennlp.apache.org/docs/1.8.0/manual/opennlp.html#tools.postagger.tagging.cmdline"" rel=""nofollow noreferrer"">OpenNLP</a> allows getting n best for POS tagging: </p>

<blockquote>
  <p>Some applications need to retrieve the n-best pos tag sequences and
  not only the best sequence. The topKSequences method is capable of
  returning the top sequences. It can be called in a similar way as tag.</p>

<pre><code>Sequence topSequences[] = tagger.topKSequences(sent);
</code></pre>
  
  <p>Each Sequence object contains one sequence. The sequence can be retrieved via Sequence.getOutcomes() which returns a tags array and
  Sequence.getProbs() returns the probability array for this sequence.</p>
</blockquote>

<p>Also, there is also a way to make spaCy do something like this:</p>

<pre><code>Doc.set_extension('tag_scores', default=None)
Token.set_extension('tag_scores', getter=lambda token: token.doc._.tag_scores[token.i])

class ProbabilityTagger(Tagger):
    def predict(self, docs):
        tokvecs = self.model.tok2vec(docs)
        scores = self.model.softmax(tokvecs)
        guesses = []
        for i, doc_scores in enumerate(scores):
            docs[i]._.tag_scores = doc_scores
            doc_guesses = doc_scores.argmax(axis=1)

            if not isinstance(doc_guesses, numpy.ndarray):
                doc_guesses = doc_guesses.get()
            guesses.append(doc_guesses)
        return guesses, tokvecs


Language.factories['tagger'] = lambda nlp, **cfg: ProbabilityTagger(nlp.vocab, **cfg)
</code></pre>

<p>Then each token will have tag_scores with the probabilities for each part of speech from spaCy's <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tag_map.py"" rel=""nofollow noreferrer"">tag map</a>.</p>

<p>Source: <a href=""https://github.com/explosion/spaCy/issues/2087"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/2087</a></p>
",4,8,992,2013-05-28 12:09:54,https://stackoverflow.com/questions/16791716/obtain-multiple-taggings-with-stanford-pos-tagger
"What&#39;s the difference between Stanford Tagger, Parser and CoreNLP?","<p>I'm currently using different tools from Stanford NLP Group and trying to understand the differences between them. It seems to me that somehow they intersect each other, since I can use same features in different tools (e.g. tokenize, and POS-Tag a sentence can be done by Stanford POS-Tagger, Parser and CoreNLP).</p>

<p>I'd like to know what's the actual difference between each tool and in which situations I should use each of them.</p>
","nlp, stanford-nlp","<p>All Java classes from the same release are the same, and, yes, they overlap. On a code basis, the parser and tagger are basically subsets of what is available in CoreNLP, except that they do have a couple of little add-ons of their own, such as the GUI for the parser. In terms of provided models, the parser and tagger come with models for a range of languages, whereas CoreNLP ships only with English out of the box. However, you can then download language-particular jars for CoreNLP which provide all the models we have for different languages. Anything that is available in any of the releases is present in the CoreNLP github site: <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP</a></p>
",13,7,1992,2013-06-02 14:54:54,https://stackoverflow.com/questions/16883919/whats-the-difference-between-stanford-tagger-parser-and-corenlp
Piglatin jodatime error with StanfordCoreNLP,"<p>I am trying to create a Pig UDF that extracts the locations mentioned in a tweet using the Stanford CoreNLP package interfaced through the sista Scala API. It works fine when run locally with 'sbt run', but throws a ""java.lang.NoSuchMethodError"" exception when called from Pig:</p>

<blockquote>
  <p>Loading default properties from tagger
  edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger
  Reading POS tagger model from
  edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger
  Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz 
  2013-06-14 10:47:54,952 [communication thread] INFO 
  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce done [7.5
  sec]. Loading classifier from
  edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ...
  2013-06-14 10:48:02,108 [Low Memory Detector] INFO 
  org.apache.pig.impl.util.SpillableMemoryManager - first memory handler
  call - Collection threshold init = 18546688(18112K) used =
  358671232(350264K) committed = 366542848(357952K) max =
  699072512(682688K) done [5.0 sec]. Loading classifier from
  edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz
  ... 2013-06-14 10:48:10,522 [Low Memory Detector] INFO 
  org.apache.pig.impl.util.SpillableMemoryManager - first memory handler
  call- Usage threshold init = 18546688(18112K) used =
  590012928(576184K) committed = 597786624(583776K) max =
  699072512(682688K) done [5.6 sec]. 2013-06-14 10:48:11,469 [Thread-11]
  WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local_0001
  java.lang.NoSuchMethodError:
  org.joda.time.Duration.compareTo(Lorg/joda/time/ReadableDuration;)I
    at edu.stanford.nlp.time.SUTime$Duration.compareTo(SUTime.java:3406)
    at edu.stanford.nlp.time.SUTime$Duration.max(SUTime.java:3488)  at
  edu.stanford.nlp.time.SUTime$Time.difference(SUTime.java:1308)    at
  edu.stanford.nlp.time.SUTime$Range.(SUTime.java:3793)   at
  edu.stanford.nlp.time.SUTime.(SUTime.java:570)</p>
</blockquote>

<p>Here is the relevant code: </p>

<pre><code>object CountryTokenizer {
  def tokenize(text: String): String = {
    val locations = TweetEntityExtractor.NERLocationFilter(text)
    println(locations)
    locations.map(x =&gt; Cities.country(x)).flatten.mkString("" "")
  }
}

class PigCountryTokenizer extends EvalFunc[String] {
  override def exec(tuple: Tuple): java.lang.String = {
    val text: java.lang.String = Util.cast[java.lang.String](tuple.get(0))
    CountryTokenizer.tokenize(text)
  }
}

object TweetEntityExtractor {
    val processor:Processor = new CoreNLPProcessor()


    def NERLocationFilter(text: String): List[String] =  {
        val doc = processor.mkDocument(text)

        processor.tagPartsOfSpeech(doc)
        processor.lemmatize(doc)
        processor.recognizeNamedEntities(doc)

        val locations = doc.sentences.map(sentence =&gt; {
            val entities = sentence.entities.map(List.fromArray(_)) match {
                case Some(l) =&gt; l
                case _ =&gt; List()
            }
            val words = List.fromArray(sentence.words)

            (words zip entities).filter(x =&gt; {
                x._1 != """" &amp;&amp; x._2 == ""LOCATION"" 
            }).map(_._1)
        })
        List.fromArray(locations).flatten
    }
}
</code></pre>

<p>I am using sbt-assembly to construct a fat-jar, and so the joda-time jar file should be accessible. What is going on?</p>
","apache-pig, stanford-nlp","<p>Pig ships with its own version of joda-time (1.6), which is incompatible with 2.x. </p>
",0,0,619,2013-06-14 10:54:10,https://stackoverflow.com/questions/17106853/piglatin-jodatime-error-with-stanfordcorenlp
Stanford Core NLP,"<p>can someone tell me the function of this output in eclipse when using Stanford Core NLP?
<a href=""http://s24.postimg.org/tx6ahxn45/Immagine.png"" rel=""nofollow"">IMAGE</a></p>

<p>Initializing JollyDayHoliday for sutime</p>
","java, stanford-nlp","<p>SUTime uses the <a href=""http://sourceforge.net/projects/jollyday/"" rel=""nofollow"">Jollyday</a> library for turing holiday names into dates. This message just says that it is using this library.</p>
",0,0,371,2013-06-17 09:50:58,https://stackoverflow.com/questions/17144689/stanford-core-nlp
Finding out adjectives describing a noun using Stanford NLP,"<p>I need to write a code that takes a few lines of comments about a product as input and rate the product based on adjectives that describe the product in the reviews. I have just used POS tagger to tag the parts of speech of every comment. Now, I have to pick out the adjectives that describe the nouns, And if a noun appears to be related to the product, I need to consider the corresponding adjective. This is the code I've used for POS tagging.. It just works fine.    </p>

<pre><code>import java.io.*;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
public class Tagg {
public static void main(String[] args) throws IOException,
ClassNotFoundException {

String tagged;

// Initialize the tagger
MaxentTagger tagger = new MaxentTagger(""edu/stanford/nlp/models/pos-tagger/wsj-        left3words/wsj-0-18-left3words-distsim.tagger"");
FileInputStream fstream = new FileInputStream(""src/input.txt"");
BufferedReader br = new BufferedReader(new InputStreamReader(fstream));
FileWriter q = new FileWriter(""src/output.txt"",true);
BufferedWriter out =new BufferedWriter(q);
String sample;
//we will now pick up sentences line by line from the file input.txt and store it in the string sample
while((sample = br.readLine())!=null)
{
//tag the string
tagged = tagger.tagString(sample);
System.out.print(tagged+""\n"");
//write it to the file output.txt
out.write(tagged);
out.newLine();
}
out.close();
}
}
</code></pre>

<p>I need a way to proceed. .</p>
","java, nlp, stanford-nlp, pos-tagger","<p>A simple solution that will get you a long way is to use the dependency parser, which is included with Stanford CoreNLP. The algorithm goes like this:</p>

<ol>
<li>PoS tag and Dependency parse your sentence</li>
<li>Decide which of the nouns you are interested in. If you are dealing with product reviews, an easy way of doing this is to match all nouns in the text against a list of known product names.</li>
<li>Look for <code>amod</code> relations in the output of the dependency parser that include the noun you are interested in.</li>
</ol>

<p>Example using the <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""noreferrer"">online Stanford demo</a>:</p>

<p>Input:</p>

<pre><code>I own a tall glass and just bought a big red car.
</code></pre>

<p><code>amod</code> dependencies:</p>

<pre><code>amod(glass-5, tall-4)
amod(car-12, big-10)
amod(car-12, red-11)
</code></pre>

<p>Suppose the reviews are about cars. The last two dependencies contain the target noun <code>car</code>, and the adjectives you are looking for are therefore <code>big</code> and <code>red</code>.</p>

<p>Warning: this is a <strong>high-precision</strong> search algorithm rather than high recall. Your list of keywords will never be exhaustive, so you are likely to miss some of the adjectives. Also, the parser is not perfect and will sometimes make mistakes. Moreover, the <code>amod</code> relation is one of many way an adjective can describe a noun. For example, <code>""The car is red""</code> parses as</p>

<pre><code>det(car-2, The-1)
nsubj(red-4, car-2)
nsubj(black-6, car-2)
cop(red-4, is-3)
root(ROOT-0, red-4)
conj_and(red-4, black-6)
</code></pre>

<p>As you can see, there isn't an <code>amod</code> relations here, just a copula and a conjunction. You could try and craft more complex rules trying to extract the fact that the <code>car is red</code> and <code>car is black</code>. Whether you want to do that is up to up. In its current form, when this algorithm returns an adjective, you can be reasonably confident it is indeed describing the noun. This, in my opinion, is a good characteristic, but it all depends on your use case.</p>

<hr>

<p>Edit after comment by OP:</p>

<p>Yes, <code>I bought a new car.</code> and <code>It is awesome.</code> are two separate sentences and will be parsed separately. This problem is known as <a href=""http://en.wikipedia.org/wiki/Coreference#Coreference_resolution"" rel=""noreferrer"">coreference (anaphora) resolution</a>. It turns out Stanford also supports this- see <a href=""http://nlp.stanford.edu/software/dcoref.shtml"" rel=""noreferrer"">their webpage</a>. There is also <a href=""http://www.ark.cs.cmu.edu/ARKref/"" rel=""noreferrer"">a system by CMU</a>, which is also in Java. I haven't used either of these systems, but the latter has a very helpful online demo. Putting the above two sentences in, I get</p>

<pre><code>[I] bought [a new car]2 .
[It]2 is awesome .
</code></pre>
",7,3,5164,2013-06-22 13:14:04,https://stackoverflow.com/questions/17251156/finding-out-adjectives-describing-a-noun-using-stanford-nlp
Create .conll file as output of Stanford Parser,"<p>I want to use Stanford Parser to create a .conll file for further processing. 
So far I managed to parse the test sentence with the command:</p>

<pre><code>stanford-parser-full-2013-06-20/lexparser.sh  stanford-parser-full-2013-06-20/data/testsent.txt &gt; output.txt
</code></pre>

<p>Instead of a txt file I would like to have a file in .conll. I'm pretty sure it is possible, at it is mentioned in the documentation (see <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/GrammaticalStructure.html"" rel=""nofollow"">here</a>). Can I somehow modify my command or will I have to write Javacode?</p>

<p>Thanks for help! </p>
","parsing, format, stanford-nlp","<p>If you're looking for dependencies printed out in CoNLL X (CoNLL 2006) format, try this from the command line:</p>

<pre><code>java -mx150m -cp ""stanford-parser-full-2013-06-20/*:"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat ""penn"" edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz stanford-parser-full-2013-06-20/data/testsent.txt &gt;testsent.tree

java -mx150m -cp ""stanford-parser-full-2013-06-20/*:"" edu.stanford.nlp.trees.EnglishGrammaticalStructure -treeFile testsent.tree -conllx
</code></pre>

<p>Here's the output for the first test sentence:</p>

<pre><code>1       Scores        _       NNS     NNS     _       4       nsubj        _       _
2       of            _       IN      IN      _       0       erased       _       _
3       properties    _       NNS     NNS     _       1       prep_of      _       _
4       are           _       VBP     VBP     _       0       root         _       _
5       under         _       IN      IN      _       0       erased       _       _
6       extreme       _       JJ      JJ      _       8       amod         _       _
7       fire          _       NN      NN      _       8       nn           _       _
8       threat        _       NN      NN      _       4       prep_under   _       _
9       as            _       IN      IN      _      13       mark         _       _
10      a             _       DT      DT      _      12       det          _       _
11      huge          _       JJ      JJ      _      12       amod         _       _
12      blaze         _       NN      NN      _      15       xsubj        _       _
13      continues     _       VBZ     VBZ     _       4       advcl        _       _
14      to            _       TO      TO      _      15       aux          _       _
15      advance       _       VB      VB      _      13       xcomp        _       _
16      through       _       IN      IN      _       0       erased       _       _
17      Sydney        _       NNP     NNP     _      20       poss         _       _
18      's            _       POS     POS     _       0       erased       _       _
19      north-western _       JJ      JJ      _      20       amod         _       _
20      suburbs       _       NNS     NNS     _      15       prep_through _       _
21      .             _       .       .       _       4       punct        _       _
</code></pre>
",9,3,5218,2013-07-03 14:24:25,https://stackoverflow.com/questions/17450652/create-conll-file-as-output-of-stanford-parser
finding dates and their position within a string using stanford nlp,"<p>I need to find the dates within a string and their positions. Consider the example string</p>

<p>""The interesting date is 4 days from today and it is 20th july of this year, another date is 18th Feb 1997""  </p>

<p>I need the output (Assuming today is 2013-07-14)<br>
2013-07-17, position 25<br>
2013-07-20, position 56<br>
1997-02-18, position 93  </p>

<p>I have managed to write the code to get the various parts of the string that is recognized as date. Need to enhance/change this to achieve the above output. Any hints or help is appreciated:</p>

<pre><code>    Properties props = new Properties();
    AnnotationPipeline pipeline = new AnnotationPipeline();
    pipeline.addAnnotator(new PTBTokenizerAnnotator(false));
    pipeline.addAnnotator(new WordsToSentencesAnnotator(false));
    pipeline.addAnnotator(new POSTaggerAnnotator(false));
    pipeline.addAnnotator(new TimeAnnotator(""sutime"", props));

    Annotation annotation = new Annotation(""The interesting date is 4 days from today and it is 20th july of this year, another date is 18th Feb 1997"");
    annotation.set(CoreAnnotations.DocDateAnnotation.class, ""2013-07-14"");
    pipeline.annotate(annotation);
    List&lt;CoreMap&gt; timexAnnsAll = annotation.get(TimeAnnotations.TimexAnnotations.class);
    timexAnnsAll.each(){
        println it
    }
</code></pre>

<p>With the above code I get the output as:<br>
4 days from today<br>
20th july of this year<br>
18th Feb 1997  </p>

<p>EDIT::<br>
Managed to get the date part, with the following change</p>

<pre><code>timexAnnsAll.each(){it -&gt;  
    Timex timex = it.get(TimeAnnotations.TimexAnnotation.class);  
    println timex.val + "" from : $it""  
}
</code></pre>

<p>Now the output is:<br>
2013-07-18 from : 4 days from today<br>
2013-07-20 from : 20th july of this year<br>
1997-02-18 from : 18th Feb 1997  </p>

<p>All I need to solve now is to find the position of the date within the original string.</p>
","groovy, nlp, stanford-nlp, date-parsing","<p>Each CoreMap returned in the list from <code>annotation.get(TimeAnnotations.TimexAnnotations.class)</code> is an <code>Annotation</code> and you can get other attributes of it, such as the list of tokens, each of which stores character offset information. So you can finish off your example like this:</p>

<pre><code>List&lt;CoreMap&gt; timexAnnsAll = annotation.get(TimeAnnotations.TimexAnnotations.class);
for (CoreMap cm : timexAnnsAll) {
  List&lt;CoreLabel&gt; tokens = cm.get(CoreAnnotations.TokensAnnotation.class);
  System.out.println(cm +
          "" [from char offset "" +
          tokens.get(0).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class) +
          "" to "" + tokens.get(tokens.size() -1)
          .get(CoreAnnotations.CharacterOffsetEndAnnotation.class) + ']');
  /* -- This shows printing out each token and its character offsets
  for (CoreLabel token : tokens) {
    System.out.println(token +
            "", start: "" + token.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class) +
            "", end: "" + token.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
  }
  */
}
</code></pre>

<p>Then the output is:</p>

<pre><code>4 days from today [from char offset 24 to 41]
20th july of this year [from char offset 52 to 74]
18th Feb 1997 [from char offset 92 to 105]
</code></pre>
",4,3,497,2013-07-13 22:12:29,https://stackoverflow.com/questions/17634675/finding-dates-and-their-position-within-a-string-using-stanford-nlp
Stanford NER prop file meaning of DistSim,"<p>In one of the example .prop files coming with the Stanford NER software there are two options I do not understand:</p>

<pre><code>useDistSim = true
distSimLexicon = /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters
</code></pre>

<p>Does anyone have a hint what DistSim stands for and where I can find any more documentation on how to use these options?</p>

<p>UPDATE: I just found out that DistSim means distributional similarity. I still wonder what that means in this context.</p>
","nlp, stanford-nlp, named-entity-recognition","<p>""DistSim"" refers to using features based on word classes/clusters, built using distributional similarity clustering methods (e.g., Brown clustering, exchange clustering). Word classes group words which are similar, semantically and/or syntactically, and allow an NER system to generalize better, including handling words not in the training data of the NER system better. Many of our distributed models use a distributional similarity clustering features as well as word identity features, and gain significantly from doing so.  In Stanford NER, there are a whole bunch of flags/properties that affect how distributional similarity is interpreted/used: <code>useDistSim</code>, <code>distSimLexicon</code>, <code>distSimFileFormat</code>, <code>distSimMaxBits</code>, <code>casedDistSim</code>, <code>numberEquivalenceDistSim</code>, <code>unknownWordDistSimClass</code>, and you need to look at the code in <code>NERFeatureFactory.java</code> to decode the details, but in the simple case, you just need the first two, and they need to be used while training the model, as well as at test time.  The default format of the lexicon is just a text file with a series of lines with two tab separated columns of <code>word clusterName</code>. The cluster names are arbitrary.</p>
",9,4,1689,2013-07-18 12:59:03,https://stackoverflow.com/questions/17724164/stanford-ner-prop-file-meaning-of-distsim
Stanford Dependencies Conversion Tool,"<p>The Stanford dependencies manual (<a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/dependencies_manual.pdf</a>) mentions: ""Or our conversion tool can convert the output of other constituency parsers to the Stanford Dependencies representation.""</p>

<p>Does anyone know where is that tool available or how to use it?</p>

<p>The Stanford Parser documentation (<a href=""http://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/stanford-dependencies.shtml</a>) mentions: ""the dependencies can be obtained using our software [...] on phrase-structure trees using the EnglishGrammaticalStructure class available in the parser package."" </p>

<p>I am interested in obtaining (ccprocessed) typed dependency lists to use in NLTK. I see there is a constructor EnglishGrammaticalStructure(Tree t) and I'd like some guidance on how to provide a NLTK tree to it.</p>

<p>First idea: Use nltk.tree.Tree.pprint to produce a string and then parse it using Tree.valueOf from Java. Any suggestion?</p>

<p>Related questions:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk/17846159#17846159"">How do I do dependency parsing in NLTK?</a></li>
<li><a href=""https://stackoverflow.com/questions/3125926/does-nltk-have-a-tool-for-dependency-parsing"">Does NLTK have a tool for dependency parsing?</a></li>
</ul>
","java, python, nltk, stanford-nlp","<p>I am not sure if you have looked at the Stanford Parser's FAQs:</p>
<h2><a href=""http://nlp.stanford.edu/software/parser-faq.shtml#s"" rel=""nofollow noreferrer"">Can I just get your typed dependencies (grammatical relations) output from the trees produced by another parser?</a></h2>
<blockquote>
<p>You can use the main method of EnglishGrammaticalStructure. You can give it options like -treeFile to read in trees, and, say, -collapsed to output typedDependenciesCollapsed. For example, this command (with appropriate paths) will convert a Penn Treebank file to uncollapsed typed dependencies:</p>
<p><code>java -cp stanford-parser.jar edu.stanford.nlp.trees.EnglishGrammaticalStructure -treeFile wsj/02/wsj_0201.mrg -basic</code></p>
<p>[...]</p>
</blockquote>
<p>The <code>mrg</code> file, here, is a 'merged' (i.e. POS tags and phrase structure) Penn Treebank representation, which you can get NLTK's Tree.pprint to emit, if you use an appropriate grammar definition. However, I cannot expand on this because the question description does not go into why these two tools must be pipelined.</p>
",2,2,2625,2013-07-24 23:09:15,https://stackoverflow.com/questions/17846355/stanford-dependencies-conversion-tool
Why there is a difference in parse tree output generated from api and GUI provided in stanfordNLP,"<p>I am using 'stanford-corenlp-full-2013-06-20' api to generate parse tree as given below</p>

<pre><code>private String text= ""Heart attack causes reduced lifespan average"";
Annotation annotation = new Annotation(text);
coreNLP.annotate(annotation);
List&lt;CoreMap&gt; sentences = annotation.get(SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
    Tree tree = sentence.get(TreeAnnotation.class);
    tree.pennPrint();
}
</code></pre>

<p>It is showing sub sentence 'S' as shown below </p>

<pre><code>(ROOT (**S** (NP (NNP Heart) (NN attack))
             (VP (VBZ causes)
                 (**S** (NP (VBN reduced) (NN lifespan) (NN average))))))
</code></pre>

<p>But When I try to parse the same sentence using the GUI provided by 'stanford-parser-full-2013-06-20' it is giving a different tree (It seems right one) as given below</p>

<pre><code>(ROOT (**S** (NP (NNP Heart) (NN attack))
             (VP (VBZ causes)
                 (VP (VBN reduced) (NP (NN lifespan) (NN average))))))
</code></pre>

<p>Can some one point out why they both are showing two different outputs though they both belong to same version.</p>
","nlp, stanford-nlp","<p>The Stanford parser will output different results depending on the number of annotation tasks you are asking it to do (<a href=""http://rants.tempura.org/2013/03/12/stanford-core-nlp-parsing.html"" rel=""nofollow"">Source</a>).  All that is required to get parser output is the sentence split, tokenization, and parse tasks.  However, if you run sentence spilt, tokenization, part-of-speech tag, and parse tasks all together you will get different results.</p>

<p>So the CoreNLP annotation is going to add the POS tagging as well by default, giving you different parse results than the parse only task.</p>

<p>In my experience working with parse trees and both forms of output neither method is strictly better.</p>
",2,0,330,2013-07-31 11:03:31,https://stackoverflow.com/questions/17968588/why-there-is-a-difference-in-parse-tree-output-generated-from-api-and-gui-provid
Method(s) to output confidence score from Stanford Classifier?,"<p>I'm currently working on Stanford Classifier (version 2.1.8 released on 04-04-2013) and wrote a java wrapper for an internal research project. Based on the ClassifierDemo.java (comes with the Classifier zip file), I was able to call the my serialized trained model and property file to process one string at a time. Note that Stanford classifier can process files only, once the input string is read and then saved in a temporary file, then Classifier starts to process it. Method trainedClassifier.classOf is able to output the class for the given string using trained model (myClassifier.ser.gz). However, I can't find the method to output confidence score along with it (cf: <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html</a>). </p>

<p>A desire output is
stringCategory: Dummy 
Confidence Score:0.85</p>

<p>The following is the Java class/method that I use in the wrapper:</p>

<pre><code>         //.....
         LinearClassifier&lt;String, String&gt; trainedClassifier = 
         IOUtils.readObjectFromFile(""myClassifier.ser.gz"");

         //Have to call *.prop every time
         ColumnDataClassifier myProp = 
           new ColumnDataClassifier(""myClassifierProp.prop"");

         //Specify the temporary one sentence file saved in class-tmp.txt
         for (String line : ObjectBank.getLineIterator(""class-tmp.txt"")) 
               { Datum&lt;String,String&gt; classType = myProp.makeDatumFromLine(line, 0); 
                 classOutput = trainedClassifier.classOf(classType);

                 System.out.println(""stringCategory: ""+ classOutput + ""/n"");
                  //end of for
                  //.....
</code></pre>
","java, methods, classification, stanford-nlp","<p>You can get the score using the method <code>Counter&lt;L&gt; scoresOf(Datum&lt;L,F&gt; example);</code> (Construct a counter with keys the labels of the classifier and values the score (unnormalized log probability) of each class.) which can be found in Classifier interface. You can use it like this: <code>trainedClassifier.scoresOf(classType)</code>. This will return a <code>Counter&lt;String&gt;</code> in this particular case. To find the score of the output class, you can use the following code: </p>

<pre><code>score = Double.toString(trainedClassifier.scoresOf(classType).getCount(classOutput));
System.out.println(""Confidence score: "" + score);
</code></pre>

<p>I did on a project like I did above. You can also try this method: </p>

<pre><code>score = trainedClassifier.scoreOf(classType, classOutput);
</code></pre>

<p>I think this will do the job too.</p>

<p>For more details:
<a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/LinearClassifier.html"" rel=""nofollow"">Stanford LinearClassifier Documentation</a>, 
<a href=""http://grepcode.com/file/repo1.maven.org/maven2/edu.stanford.nlp/stanford-corenlp/3.2.0/edu/stanford/nlp/classify/LinearClassifier.java?av=f"" rel=""nofollow"">Stanford LinearClassifier Implementation</a></p>
",1,1,1423,2013-08-03 23:04:50,https://stackoverflow.com/questions/18038153/methods-to-output-confidence-score-from-stanford-classifier
how to use Entity Recognition with Apache solr and LingPipe or similar tools,"<p>I would like to use NLP while indexing the data with Apache Solr.</p>

<ol>
<li><p>Identify the synonyms of the words and index that also.</p></li>
<li><p>Identify thenamed entity and label it while indexing. </p></li>
<li><p>when some one query the Solr Index, I should able to extract the
named entity and intention from the query and form the query string,
so that it can effectively search the indexed file.</p></li>
</ol>

<p>Is there any tools / plugins available to satisfy my requirements? I believe it is a common use cases for most of the content based websites. How people handling it?  </p>
","solr, nlp, named-entity-recognition, stanford-nlp","<p>Here's <a href=""http://www.searchbox.com/named-entity-recognition-ner-in-solr/"" rel=""nofollow"">a tutorial on using Stanford NER with SOLR</a>.</p>
",4,1,5904,2013-08-04 09:46:15,https://stackoverflow.com/questions/18041663/how-to-use-entity-recognition-with-apache-solr-and-lingpipe-or-similar-tools
Missing relations in new stanford-corenlp-3.2.0-models.jar,"<p>I used <strong>stanford-parser-2.0.4-models.jar</strong> earlier in my application . Now I want to port my application to <strong>stanford-corenlp-3.2.0-models.jar</strong>. I used <strong>edu.stanford.nlp.trees.EnglishGrammaticalRelations.PURPOSE_CLAUSE_MODIFIER</strong> and <strong>edu.stanford.nlp.trees.EnglishGrammaticalRelations.COMPLEMENTIZER</strong> in my application to identify <strong>purpose clause modifier</strong> and <strong>complementizer</strong> relations from semantic graph edges but unfortunately I could not see them in latest version of stanford-corenlp-3.2.0-models.jar. Could some one suggest how can I do it using new jar and explain me what could be the reason behind this avoiding these relations in new jar.</p>
",stanford-nlp,"<p>I could find the those details in stanford-corenlp-3.2.0-sources.jar. As part of this they removed these relations and treated them as special cases with the existing relations.
find the below comments I could see from source code</p>

<pre><code>The ""purpose clause modifier"" grammatical relation has been discontinued
It is now just seen as a special case of an advcl.  A purpose clause
modifier of a VP is a clause headed by ""(in order) to"" specifying a
purpose.  Note: at present we only recognize ones that have
""in order to"" or are fronted.  Otherwise we can't use our surface representations to
distinguish these from xcomp's. We can also recognize ""to"" clauses
introduced by ""be VBN"".
&lt;p/&gt;
Example: &lt;br/&gt;
""He talked to the president in order to secure the account"" &amp;rarr;
&lt;code&gt;purpcl&lt;/code&gt;(talked, secure)


The ""complementizer"" grammatical relation is a discontinued grammatical relation. A
A complementizer of a clausal complement was the word introducing it.
It only matched ""that"" or ""whether"". We've now merged this in with ""mark"" which plays a        similar
role with other clausal modifiers.
&lt;p/&gt;
&lt;p/&gt;
Example: &lt;br/&gt;
""He says that you like to swim"" &amp;rarr;
&lt;code&gt;complm&lt;/code&gt;(like, that)
</code></pre>
",0,0,241,2013-08-14 05:26:02,https://stackoverflow.com/questions/18223799/missing-relations-in-new-stanford-corenlp-3-2-0-models-jar
CoreNlp splitting the stanford-corenlp-3.2.0-models.jar,"<p>I'm trying to deploy <code>stanford-corenlp-3.2.0-models.jar</code> but my host says the jar is to big?</p>

<p>If I'm just want to use the POS, what jar, can I use instead.</p>

<p>Or how can I split the jar?</p>
","java, stanford-nlp","<p>You just need to read up on how to use the <code>jar</code> command. A jar file is just a variant on a zip file. You can expand its contents with <code>jar -xf stanford-corenlp-3.2.0-models.jar</code>, get what you need, and then put it into a new smaller jar file.</p>
",1,0,420,2013-08-18 01:08:26,https://stackoverflow.com/questions/18294791/corenlp-splitting-the-stanford-corenlp-3-2-0-models-jar
Named entity recognition promlem to identify the text &quot;next monday&quot; as date?,"<p>I'm new to text mining and NLP. I'm trying to use named entity recognition (NER) (Stanford Named Entity Tagger) to extract dates from the given text. I'm using the online demo provided in Stanford NLP <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/ner/process</a> and GATE ANNIE <a href=""http://services.gate.ac.uk/annie/"" rel=""nofollow"">http://services.gate.ac.uk/annie/</a></p>

<p>This demo is not able to recognize the texts like complete ""last Sunday"", ""next Monday"", ""this month end"" ""till this Sunday evening"" as date. Sunday or Monday alone will not be useful to determine the date.  Is there any option to extract the actual text mentioned by the given example texts? </p>

<p>Example text:</p>

<p>Treat yourself with Puma as it offers Flat 50% off. Hurry offer valid till this Sunday. Happy Shopping.</p>

<p>Extracted date: 25-08-2013 (Considering today is 19-08-2013. Date format can be anything)</p>

<p>Any library provides this kind of date recognition feature or is it possible to build custom model to recognize date as given in the example text? </p>
","nlp, stanford-nlp, opennlp, gate","<p><a href=""http://nlp.stanford.edu/software/sutime.shtml"" rel=""nofollow noreferrer"">SUTime</a> in Stanford CoreNLP can do temporal recognition. That page includes example code and it has an online demo available <a href=""http://nlp.stanford.edu:8080/sutime/process"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Related question: <a href=""https://stackoverflow.com/questions/18593443/is-it-good-to-use-stanford-temporal-tagger-for-big-data"">Is it good to use stanford temporal tagger for big data?</a></p>
",2,1,1241,2013-08-19 14:59:26,https://stackoverflow.com/questions/18316990/named-entity-recognition-promlem-to-identify-the-text-next-monday-as-date
What parsing strategy is used on Stanford Parser?,"<p>I'm writing a paper where I analyse different available tools for natural language parsing. I found out that two main strategies for parsing are top-down and bottom down.</p>

<p><strong>I wonder which strategy is used in Stanford Parser?</strong> </p>

<p>I know that they used probabilistic approach there, but is not based on any of bottom-up or top-down?</p>
","parsing, nlp, stanford-nlp","<p>As far as I know, it is a <a href=""http://en.wikipedia.org/wiki/CYK_algorithm"" rel=""nofollow"">CYK parser</a> (see <a href=""http://nlp.stanford.edu/~manning/papers/unlexicalized-parsing.pdf"" rel=""nofollow"">here</a>, Section 1), i.e. a bottom up parser.</p>
",1,0,163,2013-08-20 10:01:47,https://stackoverflow.com/questions/18332234/what-parsing-strategy-is-used-on-stanford-parser
Why does my superclass object call its subclass method?,"<p>I have seen so many helpful threads here, but this is my first time posting!</p>

<p>I was working on the infamous Stanford OpenCourse project: Matchismo. While I got everything going just fine, but I don't understand one part of the sample codes. </p>

<p>Basically the code below is used to get a Card object to compare with another card.</p>

<pre><code>- (void) flipCardAtIndex: (NSUInteger)index
{
    Card *card = [self cardAtIndex:index];
    if (card &amp;&amp; !card.isUnplayable)
    {
        if (!card.isFaceUp)
        {
            for (Card* otherCard in self.cards)//for-in loop
            {
                if (otherCard.isFaceUp &amp;&amp; !otherCard.isUnplayable)
                {
                    int matchScore = [card match:@[otherCard]];
......
</code></pre>

<p>And this is how cardAtIndex works:</p>

<pre><code>-(Card *) cardAtIndex:(NSUInteger)index
{
    if (index &lt; [self.cards count])
        //dot notation is used for property
        //[] is used for method
    {
        return self.cards[index];
    }
    return nil;
}
</code></pre>

<p>Here are the methods for Match(card*) and Match(playingCard)</p>

<p>Match(card*)</p>

<pre><code>-(int) match:(NSArray *)otherCards
{
    NSLog(@""here"");
    int score = 0;

    for (Card *card in otherCards)
    {
        if ([card.content isEqualToString:self.content])
            score = 1;
        {
            NSLog(@""Card Match"");
        }

    }
    return score;
}
</code></pre>

<p>Match(PlayingCard*)</p>

<pre><code>-(int) match: (NSArray *)otherCards;
{
    int score = 0;
    if ([otherCards count] == 1)
    {
        PlayingCard *otherCard = [otherCards lastObject];//the last object in the array
        if ([otherCard.suit isEqualToString:self.suit])
            score = 1;
        else if (otherCard.rank == self.rank)
            score = 4;
        NSLog(@""PlayingCard Match"");
    }
    return score; 
}
</code></pre>

<p>It worked just fine, but I don't get why when a Card* object calls a method, its subclass's PlayingCard's method is invoked.
Thanks so much for help me!</p>
","objective-c, stanford-nlp","<p>This concept is called <a href=""http://en.wikipedia.org/wiki/Polymorphism_in_object-oriented_programming"" rel=""nofollow"">Polymorphism</a>.</p>

<p>It allows you to have a base class which provides some interface, and a set of subclasses that implement these methods in some different ways. The classic example is a <code>Drawable</code> class method <code>draw</code>, and its subclasses <code>Circle</code> and <code>Rectangle</code>, that both override the <code>draw</code> method to render themselves in some specific manner.</p>

<p>So goes for your <code>Card</code> base class, it calls its own interface method <code>match</code>, but as an object is actually not an instance of <code>Card</code>, but of a <code>PlayingCard</code> subclass, subclass method gets called instead to provide specific implementation.</p>
",1,0,574,2013-08-22 09:26:14,https://stackoverflow.com/questions/18376508/why-does-my-superclass-object-call-its-subclass-method
POS tagging in Scala,"<p>I tried to POS tag a sentence in Scala using Stanford parser like below</p>

<pre><code>val lp:LexicalizedParser = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
lp.setOptionFlags(""-maxLength"", ""50"", ""-retainTmpSubcategories"")
val s = ""I love to play""
val parse :Tree =  lp.apply(s)
val taggedWords = parse.taggedYield()
println(taggedWords)
</code></pre>

<p>I got an error <strong>type mismatch; found : java.lang.String required: java.util.List[_ &lt;: edu.stanford.nlp.ling.HasWord]</strong> in the line <strong>val parse :Tree =  lp.apply(s)</strong></p>

<p>I don't know whether this is the right way of doing it or not. Are there any other easy ways of POS tagging a sentence in Scala?</p>
","scala, nlp, stanford-nlp, pos-tagger","<p>I found a very simple way to do POS tagging in Scala</p>

<p><strong>Step 1</strong></p>

<p>Download stanford tagger version 3.2.0 form the link below</p>

<p><a href=""http://nlp.stanford.edu/software/stanford-postagger-2013-06-20.zip"" rel=""nofollow"">http://nlp.stanford.edu/software/stanford-postagger-2013-06-20.zip</a></p>

<p><strong>Step 2</strong></p>

<p>Add <strong>stanford-postagger</strong> jar present in the folder to your project and also place the <strong>english-left3words-distsim.tagger</strong> file present in the models folder in your project</p>

<p>Then, with the code below you can pos tag a sentence in Scala</p>

<pre><code>              val tagger = new MaxentTagger(
                ""english-left3words-distsim.tagger"")
              val art_con = ""My name is Rahul""
              val tagged = tagger.tagString(art_con)
              println(tagged)
</code></pre>

<p><strong>Output:</strong> My_PRP$ name_NN is_VBZ Rahul_NNP</p>
",4,11,2906,2013-08-24 08:29:28,https://stackoverflow.com/questions/18416561/pos-tagging-in-scala
Maven fails to download CoreNLP models,"<p>When building the sample application from the Stanford CoreNLP website, I ran into a curious exception:</p>

<pre><code>Exception in thread ""main"" java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:493)
…
Caused by: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
…
</code></pre>

<p>This only happened when the property <code>pos</code> and the ones after it were included in the properties.</p>

<pre class=""lang-java prettyprint-override""><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>Here is the dependency from my pom.xml:</p>

<pre><code>&lt;dependencies&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.2.0&lt;/version&gt;
    &lt;scope&gt;compile&lt;/scope&gt;
&lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>
","maven, stanford-nlp","<p>I actually found the answer to that in the problem description of another question on Stackoverflow.</p>

<p><a href=""https://stackoverflow.com/questions/14250476/maven-dependencyget-does-not-download-stanford-nlp-model-files"">Quoting W.P. McNeill</a>:</p>

<blockquote>
  <p>Maven does not download the model files automatically, but only if you
  add models line to the .pom. Here is a .pom
  snippet that fetches both the code and the models.</p>
</blockquote>

<p>Here's what my dependencies look like now:</p>

<pre><code>&lt;dependencies&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.2.0&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>The important part to note is the entry <code>&lt;classifier&gt;models&lt;/classifier&gt;</code> at the bottom. In order for Eclipse to maintain both references, you'll need to configure a dependency for each <code>stanford-corenlp-3.2.0</code> and <code>stanford-corenlp-3.2.0-models</code>.</p>
",56,27,10383,2013-08-28 15:48:17,https://stackoverflow.com/questions/18492608/maven-fails-to-download-corenlp-models
Is it good to use stanford temporal tagger for big data?,"<p>I'm exploring Stanford Temporal Tagger for my project to extract date entity from the text. The demo from <a href=""http://nlp.stanford.edu:8080/sutime/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/sutime/process</a> seems to be promising. I would like to understand whether this library is matured. And also somebody help me understanding how this library performs with big data. It would be also helpful if you can guide me about other java based  temporal tagger libraries especially for big data requirements. Is there any apache project which does temporal tagging? </p>

<p>I found some of the libraries like </p>

<p><a href=""https://code.google.com/p/heideltime/"" rel=""nofollow"">https://code.google.com/p/heideltime/</a></p>

<p><a href=""https://code.google.com/p/stemptag/"" rel=""nofollow"">https://code.google.com/p/stemptag/</a></p>
","nlp, stanford-nlp, information-extraction, named-entity-recognition","<p>Yes, the SUTime library is mature and quite accurate, and has been run over tens of millions of words of text. (Just make sure you are not invoking the more expensive and much slower parts of Stanford CoreNLP - parsing and dcoref - which are not needed for temporal tagging.)</p>

<p>Heideltime is another very good Java library for temporal tagging. It has the advantage of supporting several languages, whereas SUTime at present only supports English. It has the disadvantage of coming configured to use TreeTagger as its part-of-speech tagger, which means you either need to deal with using this non-open source, non-Java component, or you need to write stuff to get it configured to use some other POS tagger. I'm not familiar with stemptag; I don't think there is any apache project for this.</p>
",4,2,1253,2013-09-03 13:20:58,https://stackoverflow.com/questions/18593443/is-it-good-to-use-stanford-temporal-tagger-for-big-data
Is StanfordCoreNLP deprecated?,"<p>In the <a href=""http://nlp.stanford.edu/software/corenlp.shtml#Usage"" rel=""nofollow"">documentation</a>, there is an example that utilizes the class: <code>StanfordCoreNLP</code>. But when you download the latest version, the class is nowhere to be found. It's found easily in earlier versions and the example works great. I've been looking all over but can't seem to figure out what happened to <code>StanfordCoreNLP</code>. </p>

<p>Has anyone used recent versions of the library who can tell me what happened to <code>StanfordCoreNLP</code>?</p>
","java, stanford-nlp","<p>The class is still in CoreNLP 3.2.0: </p>

<pre><code>edu.stanford.nlp.pipeline.StanfordCoreNLP
</code></pre>

<p>Just found it in the CoreNLP 3.2.0 Maven artifact using the Eclipse ""open type"" feature.</p>
",3,2,162,2013-09-08 18:37:05,https://stackoverflow.com/questions/18687260/is-stanfordcorenlp-deprecated
I want to ignore all other tags except noun and verb tags. is it possible to do it using Stanford corenlp word class?,"<p>I want to ignore all other tags except noun and verb tags. is it possible to do it using Stanford corenlp word lavel?</p>
",stanford-nlp,"<p>You can generate POS tags using Stanford corenlp and then prune the ones not having verb and noun tags. Try <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"" title=""this"">this</a> to get a feel of how tagging is done for different input sentences and do read the POS tagging manual to get a hold of what different POS tag mean.</p>
",0,0,111,2013-09-10 09:31:48,https://stackoverflow.com/questions/18715314/i-want-to-ignore-all-other-tags-except-noun-and-verb-tags-is-it-possible-to-do
"comparison of natural language processing tools (UIMA, LingPipe, Lucene, Gate, Stanford)","<p>I want to choose a natural language processing tool to do common tasks such as tokenization, sentence detection,various tagging (Name Entity Recognition, POS tagging, ... ). my question has two parts:</p>

<ol>
<li>What are the criteria for choosing a natural language processing
tool?</li>
<li>Among (UIMA, LingPipe, Lucene, Gate, Stanford), which one satisfy
these criteria better?</li>
</ol>

<p>and what is your suggestion ?</p>
","lucene, stanford-nlp, gate, uima","<p>Some <strong>general</strong> Criteria:</p>

<ol>
<li>how many tasks can I perform with the provided models (e.g. does the tool contains models for my tasks like spanish tokenisation or protein NER)?</li>
<li>how easy is it for me to add the missing tools.</li>
</ol>

<p>BTW, I would add <a href=""http://nltk.org/"" rel=""nofollow"">NLTK</a> to your list, and its <a href=""http://nltk.org/book/"" rel=""nofollow"">excellent, free accompanying book</a>. </p>
",2,1,2448,2013-09-18 09:13:03,https://stackoverflow.com/questions/18868180/comparison-of-natural-language-processing-tools-uima-lingpipe-lucene-gate-s
"Output results in conll format (POS-tagging, stanford pos tagger)","<p>I am trying to use Stanford POS-tagger, I want to ask if it is possible to parse (actually only pos tag would be enough) an english text and output the results in conll format. Is there such an option?</p>

<p>I am using the full 3.2.0 version of the Stanford pos tagger</p>

<p>Thanks a lot</p>
","nlp, stanford-nlp, pos-tagger, output-formatting, outputformat","<p>When it comes to the CONLL format, i presume you mean the CONLL2000 chunking task format as such:</p>

<pre><code>   He        PRP  B-NP
   reckons   VBZ  B-VP
   the       DT   B-NP
   current   JJ   I-NP
   account   NN   I-NP
   deficit   NN   I-NP
   will      MD   B-VP
   narrow    VB   I-VP
   to        TO   B-PP
   only      RB   B-NP
   #         #    I-NP
   1.8       CD   I-NP
   billion   CD   I-NP
   in        IN   B-PP
   September NNP  B-NP
   .         .    O
</code></pre>

<p>There are three columns in the CONLL chunking task format:</p>

<ol>
<li><code>token</code> (i.e. word)</li>
<li><code>POS</code> tag</li>
<li><code>BIO</code> (begin, inside, outside) of chunk/phrase tag</li>
</ol>

<p>Sadly, if you use the stanford MaxEnt tagger, it <strong>only give you the <code>token</code> and <code>POS</code> information but has no <code>BIO</code> chunk information</strong>. </p>

<pre><code>java -cp stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/left3words-wsj-0-18.tagger -textFile short.txt -outputFormat tsv 2&gt; /dev/null
</code></pre>

<p>Using the above command the Stanford POS tagger already give you the tab separated format, just that it's without the 3rd column (see <a href=""http://nlp.stanford.edu/software/pos-tagger-faq.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/pos-tagger-faq.shtml</a>):</p>

<pre><code>   He        PRP
   reckons   VBZ
   the       DT
   ...
</code></pre>

<p><strong>To get the <code>BIO</code> colum, you would require either</strong>: </p>

<ul>
<li>a <strong>statistical chunker</strong> or</li>
<li>a <strong>full parser</strong></li>
</ul>

<p>see <a href=""http://www-nlp.stanford.edu/links/statnlp.html"" rel=""nofollow"">http://www-nlp.stanford.edu/links/statnlp.html</a> for a list of chunker/parser, if you want to stick with stanford tools, i suggest the stanford parser but it gives you the bracketed parse format, which you have to do some post-processing to get it into CONLL2000 format, see <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/lex-parser.shtml</a></p>
",3,3,2134,2013-09-22 21:00:41,https://stackoverflow.com/questions/18948712/output-results-in-conll-format-pos-tagging-stanford-pos-tagger
Is there a reasonably accurate heuristic for detecting the subject and object of an English sentence?,"<p>I realize that flawlessly separating subject noun phrases and object noun phrases from a sentence is an open research problem and is not easily explained here, but is there a smart way of doing this (assuming I already have a POS-tagged sentence) which works for most sentences, or at least relatively simple ones? I know that simply assuming the first noun phrase is the subject is a pretty good approximation, but in sentences beginning with prepositional phrases (e.g., ""Across the clearing and through the stream ran the frightened deer.""), this fails. Ideally, I'd like something which also recognizes the subject in this case.</p>

<p>For reference, that example sentence gives the following parse tree with the Stanford Parser:</p>

<p><code>[ROOT [S [PP [IN Across] [NP [NP [DT the] [NN clearing] ] [CC and] [NP [IN through] ] ] ] [NP [DT the] [NN stream] ] [VP [VBD ran] [NP [DT the] [ADJP [JJ frightened] ] [NNS deer] ] ] [. .] ] ]</code></p>

<p>My current strategy is as follows:</p>

<ol>
<li><p>Subject: Do a BFS on the tree, looking for the first NP.</p></li>
<li><p>Verb: Do a BFS on the tree, looking for the first VP. On this subtree, do a BFS looking for a VB(D|G|N|P|Z).</p></li>
<li><p>Object: Do a BFS on the VP subtree found above, looking for a NP.</p></li>
</ol>

<p>This strategy results in the following for my example:
<code>SUBJECT: (NP (DT the) (NN stream) ) , VERB: (VBD ran) , OBJECT: (NP (DT the) (ADJP (JJ frightened) ) (NNS deer) )</code>.</p>

<p>If possible, I'd like to modify my strategy to not fail on these cases, and eventually even more complicated cases.</p>
","nlp, stanford-nlp","<p>Edit: Ok, you are using the Stanford parser. Then the parser gives you the result. For example if I run it on <code>In London, my friend ate a big apple and a pear.</code> I get:</p>

<pre><code>    (ROOT
      (S
        (PP (IN In)
          (NP (NNP London)))
        (, ,)
        (NP (PRP$ my) (NN friend))
        (VP (VBD ate)
          (NP
            (NP (DT a) (JJ big) (NN apple))
            (CC and)
            (NP (DT a) (NN pear))))
        (. .)))
</code></pre>

<p>Then subject is the NP under S (my friend) and object is the NP under the VP under S (a big apple and a pear). And actually the dependency parse:</p>

<pre><code>prep_in(ate-6, London-2)
poss(friend-5, my-4)
nsubj(ate-6, friend-5)
root(ROOT-0, ate-6)
det(apple-9, a-7)
amod(apple-9, big-8)
dobj(ate-6, apple-9)
det(pear-12, a-11)
dobj(ate-6, pear-12)
conj_and(apple-9, pear-12)
</code></pre>

<p>tells you what is the head of the subject (friend) and of the direct object (apple,pear).</p>

<p>Obviously, the parser is not without errors, and actually on your sentence with inversion (subj follows the verb) it gets confused:</p>

<pre><code>(ROOT
  (S
    (PP (IN Across)
      (NP
        (NP (DT the) (NN clearing))
        (CC and)
        (NP (IN through))))
    (NP (DT the) (NN stream))
    (VP (VBD ran)
      (NP (DT the)
        (ADJP (JJ frightened))
        (NNS deer)))
    (. .)))
</code></pre>

<p>the correct parse would be </p>

<pre><code>(ROOT
  (S
    (PP
        (PP (IN Across)
            (NP (DT the) (NN clearing)))
        (CC and)
        (PP (IN through)
            (NP (DT the) (NN stream))))
    (VP (VBD ran))
    (NP (DT the)
        (ADJP (JJ frightened))
        (NNS deer))
    (. .)))
</code></pre>

<p>Then you would correctly identify <code>the frightened deer</code> as the subject.</p>

<p>What to do about it? You can either try to improve the parser by retraining it on more sentences like this one (in addition to the ones they trained it on), but that is A LOT of work. Or you might try to identify the types of sentences it does not get right and focus on identify the errors. Not easy either. You can also try a different parser, like the one used in <a href=""http://lingo.stanford.edu/"" rel=""nofollow noreferrer"">LinGO project</a>, but it is much harder to use (I think it requires lisp or something similar)</p>

<p><hr>
(This is the old answer, before I knew we have a result of a parser)</p>

<p>In addition to the POS tagger, I  would get a <a href=""https://stackoverflow.com/questions/4757947/what-is-a-chunker-in-natural-language-processing"">chunker</a> and then:</p>

<ul>
<li>subject is the first (top level) NP (noun phrase). In your <em>Across the clearing</em> sentences, the NP would be part of a prepositional phrase (PP), so you would not pick it up; it would still fail as there is inversion.</li>
<li>object is the (top level) NP immediately following the finite verb, if any.</li>
</ul>

<p>Depending on the chunker, you might miss coordinated NPs, possibly strip NPs of their PPs (getting <code>my friend</code> instead of <code>my friend from New York</code>). </p>

<p>If you cannot afford to run a chunker, just look for the head of the subject/object:
 - N or subject pronoun (<em>we</em> but not <em>us</em>), preceding the finite verb (but keep in mind there are sentences like <code>John and me went to the store</code>.
 - N or object pronoun (<em>us</em> but not <em>we</em>), if any. You should add a check that NP, which the N is part of is immediately following the verb and it is not a part of a PP). Say you can check the verb is immediately followed by <code>(Det) ((Adv) Adj)* N)</code> </p>

<p>You should also consider questions (where the subject follows the finite verb and object follows the base verb: <code>Do YOU see THE APPLE?</code>). You could also deal with inversion by requiring a subject and looking for it after the verb if you do not find it before, but that would cause problems with imperative (<code>Eat the deer!</code>). Not sure it is worth it.</p>

<p>Obviously, slightly unusual sentences such as sentences with unbounded dependencies will throw you off (<code>Kim, Sandy knows Chris trusts</code> where <code>Kim</code> is the object of <code>trusts</code>). If you need reasonable answers in such cases you need to run a real parser.</p>
",6,3,2592,2013-09-23 20:42:12,https://stackoverflow.com/questions/18968457/is-there-a-reasonably-accurate-heuristic-for-detecting-the-subject-and-object-of
How to extract the primary subject and object phrases from a complex sentence?,"<p>In the documentation for the Stanford Parser, the following example sentence is given: </p>

<blockquote>
  <p>The strongest rain ever recorded in India shut down the financial hub
  of Mumbai, snapped communication lines, closed airports and forced
  thousands of people to sleep in their offices or walk home during the
  night, officials said today.</p>
</blockquote>

<p>This produces the parse tree:</p>

<blockquote>
  <p>[ROOT [S [S [NP [NP [DT The] [JJS strongest] [NN rain] ] [VP [ADVP [RB
  ever] ] [VBN recorded][PP [IN in] [NP [NNP India] ] ] ] ] [VP [VP [VBD
  shut] [PRT [RP down] ] [NP [NP [DT the] [JJ financial] [NN hub] ] [PP
  [IN of] [NP [NNP Mumbai] ] ] ] ] [, ,] [VP [VBD snapped] [NP [NN
  communication] [NNS lines] ] ] [, ,] [VP [VBD closed] [NP [NNS
  airports] ] ] [CC and] [VP [VBD forced] [NP [NP [NNS thousands] ] [PP
  [IN of] [NP [NNS people] ] ] ] [S [VP [TO to] [VP [VP [VB sleep] [PP
  [IN in] [NP [PRP$ their] [NNS offices] ] ] ] [CC or] [VP [VB walk] [NP
  [NN home] ] [PP [IN during] [NP [DT the] [NN night] ] ] ] ] ] ] ] ] ]
  [, ,] [NP [NNS officials] ] [VP [VBD said] [NP-TMP [NN today] ] ] [.
  .] ] ]</p>
</blockquote>

<p>(see <a href=""https://i.sstatic.net/yjnJT.png"" rel=""nofollow"">https://i.sstatic.net/yjnJT.png</a>).</p>

<p><strong>What sort of NLP tool would be able to output the sentential subject and object from the above complex sentence example?</strong> Desired output:</p>

<pre><code>sentence_subj_phrase = ""the strongest rain ever recorded in India""
sentence_obj_phrase = ""the financial hub of Mumbai""
</code></pre>

<p><strong>FROM ORIGINAL OP's POST</strong> (It's just details about what he's thinks doesn't work):</p>

<p>A naive way of extracting the subject and object in a sentence is to find the noun phrases immediately preceding and succeeding the verb. In complex sentences, however, there are multiple verbs, and thus multiple subjects and objects. It is possible to consider complex sentences like this as multiple sentences (using the first part of the independent clause as the ""root"", and replacing the second part with each of the dependent clauses), but usually the first clause is the most important and could be considered the main ""topic"" of the sentence.</p>

<p>Doing a simple BFS to find the first NP prior to a verb will result in ""officials"" being the subject, since it is at the lowest depth level. This doesn't capture the intuition of the first clause containing the subject. One approach I tried was searching for the NPs in the first ""base"" S node (i.e., lowest level subtree rooted at an S node), but in this case that would capture nodes rooted at S<sub>3</sub>.</p>
","tree, nlp, stanford-nlp","<p>Some points to take note, when you talk about grammatical subjects and objects, they are following <a href=""https://en.wikipedia.org/wiki/Structural_linguistics"" rel=""nofollow"">structuralist</a> theory of linguistics, which most NLP tasks adhere to.</p>

<p>Next when you talk about grammatical subjects and object, you should only refer to the entity (i.e. the thing/event) itself and that excludes the entity modifiers:
""the strongest rain ever recorded in India""</p>

<pre><code>entity = ""rain""
entity modifiers = [('Adjective/Preposition_Phrase', ""ever recorded in India""), (""Determiners"", ""the""), (Adjective_Phrase, ""strongest"")]
entity phrase = ""The strongest rain""
entity phrase with all posssible modifiers (EP_mod)= ""the strongest rain ever recorded in India""
</code></pre>

<p>Then we come down to the NLP task of how to detect <code>EP_mod</code>:</p>

<ol>
<li><p>First, you can try to <strong>figure out an algorithm that determines the primary predicate</strong> (i.e. verb in shallow computational grammar) in the complex sentence. (I suggest, find the verb in the top most hierarchy of the parse tree)</p></li>
<li><p>Then, you need to <strong>find the phrase that contains the SUBJ/OBJ entity</strong> of the primary predicate. (Any normal NLP parser should tell you this)</p></li>
<li><p>Lastly, you need to <strong>find the modifiers of the phrase</strong> that contains the SUBJ/OBJ entity of the primary predicate (Possibly you need to find a dependency parser (Stanford parser is a dependency parser) that gives you the annotation that says <code>SUBJ_phrase governs Modifier_phrase</code>)</p></li>
</ol>

<p>What you're asking for is a mish-mash of current existing tools, so the best solution is the <a href=""https://en.wikipedia.org/wiki/Eating_your_own_dog_food"" rel=""nofollow"">eat your own dog food</a> solution. Have fun with it =)</p>
",3,3,5999,2013-09-26 14:36:03,https://stackoverflow.com/questions/19031199/how-to-extract-the-primary-subject-and-object-phrases-from-a-complex-sentence
How to print a part of the dependencies graph,"<p>I would like to print subtrees of a dependency graph. Specifically for the sentence ""<code>I turn the red meat</code>"" and the starting word <code>meat-NN</code>, the output should be: ""<code>the red meat</code>"".</p>

<p>Right now I'm doing it like this:</p>

<pre><code>protected String printSubGraph(IndexedWord startingWord, SemanticGraph graph) {
    Iterable&lt;SemanticGraphEdge&gt; outiter = graph.outgoingEdgeIterable(startingWord);

    // set the default bounds to the startingWord 
    int start = startingWord.beginPosition();
    int end = startingWord.endPosition();

    // search the next level for larger bounds
    // assume that everything in between the bounds belongs to the sub-graph of the startingWord
    for (SemanticGraphEdge edge : outiter) {
        start = Math.min(start, edge.getGovernor().beginPosition());
        start = Math.min(start, edge.getDependent().beginPosition());
        end = Math.max(end, edge.getGovernor().endPosition());
        end = Math.max(end, edge.getDependent().endPosition());
    }

    return graph.toRecoveredSentenceString().substring(start, end);
}
</code></pre>

<p>This is bad for three reasons: </p>

<ol>
<li>I assume that everything in between the tokens belongs to starting word's subtree.</li>
<li>I do not search the whole subtree for larger bounds.</li>
<li>I assume that the graph is the whole text and the bounds are valid for the RecoveredSentenceString. (Which is not true, if the original text contains more than one sentence.)</li>
</ol>

<p>Is there a way to get this subtree (and only this subtree) from a SemanticGraph or a CoreMap without implementing DFS myself? I'm aware of <a href=""https://stackoverflow.com/questions/11148890/how-to-print-the-parse-tree-of-stanford-javanlp"">the other way</a>, but I don't know any means to locate an IndexedWord inside a Tree.</p>
","java, nlp, stanford-nlp","<p>Perhaps what you are looking for is not a dependency parse but a phrase structure parse.</p>

<p>Your sentence is :</p>

<blockquote>
  <p>I turn the red meat.</p>
</blockquote>

<p>The phrase structure parse of which is:</p>

<blockquote>
  <p>(ROOT
   (S
     (NP (PRP I))
     (VP (VBP turn)
       (NP (DT the) (JJ red) (NN meat)))
     (. .)))    </p>
</blockquote>

<p>You can write a <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/tregex/TregexPattern.html"" rel=""nofollow"">TregexPattern</a> of the form : </p>

<blockquote>
  <p>NP&lt; (NN &lt; meat) </p>
</blockquote>

<p>to get the desired subtree or simply</p>

<blockquote>
  <p>NP</p>
</blockquote>

<p>to get all noun phrases.</p>
",1,1,217,2013-09-30 12:35:33,https://stackoverflow.com/questions/19094445/how-to-print-a-part-of-the-dependencies-graph
Parse model ignored,"<p>I'm trying to get the Stanford parser to work for my pipeline for German text, but it refuses to take the German parser:</p>

<pre><code>Properties props = new Properties();

props.put(""annotators"", ""tokenize, ssplit, pos, parse"");
props.put(""ssplit.isOneSentence"", ""true"");
props.put(""pos.model"", ""pos-taggers/german-fast/german-fast.tagger"");
props.put(""pos.maxlen"", ""30"");
props.put(""parse.model"", ""edu/stanford/nlp/models/lexparser/germanPCFG.ser.gz"");
props.put(""encoding"", ""utf-8"");

pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>I still get the following output and nothing more because German tags are not recognized:</p>

<pre><code>Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ...
Initializing lexicon scores ... The 15 open class tags are: [ TRUNC NE NN XY VVIZU ADV VVINF VVFIN VVPP CARD NN-OA ADJA FM ADJD NN-SB ] 
</code></pre>

<p>The failure trace:</p>

<pre><code>java.lang.IllegalArgumentException: Unknown option: -retainTmpSubcategories
at edu.stanford.nlp.parser.lexparser.Options.setOption(Options.java:175)
at edu.stanford.nlp.parser.lexparser.Options.setOptions(Options.java:68)
at edu.stanford.nlp.parser.lexparser.Options.setOptions(Options.java:49)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.setOptionFlags(LexicalizedParser.java:841)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:159)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:143)
at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:176)
at edu.stanford.nlp.pipeline.ParserAnnotator.&lt;init&gt;(ParserAnnotator.java:106)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$12.create(StanfordCoreNLP.java:734)
at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:261)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:127)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:123)
at da.utils.nlp.SentimentExtractor.initPipeline(SentimentExtractor.java:111)
at da.utils.nlp.SentimentExtractor.coreAnnotate(SentimentExtractor.java:117)
at da.utils.nlp.SentimentExtractorTest.testCoreAnnotate(SentimentExtractorTest.java:29)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
</code></pre>

<p>Any idea what may be wrong in my implementation?</p>

<p>I checked the file location with no success.</p>
","java, stanford-nlp","<p>The simple (if confusing) answer should be that you just need to add this line in your Properties setup:</p>

<pre><code>props.put(""parse.flags"", """");
</code></pre>

<p>(This should be fixed, but the flags default to an option that is useful when getting out English dependencies, but not relevant or available in other languages, hence your getting the error message above.)</p>

<p>HOWEVER, if this were the only problem, you should first see it loading the German parser before giving the long error dump like this:</p>

<pre><code>Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/germanFactored.ser.gz ... done [5.2 sec].
Exception in thread ""main"" java.lang.IllegalArgumentException: Unknown option: -retainTmpSubcategories
</code></pre>

<p>But in the output you show, it is still loading an English parser.  So something else must be wrong. I'm not sure about this part, but two possibilities are:</p>

<ul>
<li>You're running an old version of Stanford CoreNLP.  A while back, the options were called ""parser.model"", ""parser.flags"", etc., but we renamed them for consistency.</li>
<li>You don't have a resource called <code>edu/stanford/nlp/models/lexparser/germanPCFG.ser.gz</code> on your CLASSPATH</li>
</ul>
",2,3,794,2013-10-17 02:34:55,https://stackoverflow.com/questions/19417302/parse-model-ignored
"Getting additional information (Active/Passive, Tenses ...) from a Tagger","<p>I'm using the Stanford Tagger for determining the Parts of Speech. However, I want to get more information out of the text. Is there a possibility to get further information like the tense of the sentence or if it is in active/passive?</p>

<p>So far, I'm using the very basic PoS-Tagging approach:</p>

<pre><code>List&lt;List&lt;TaggedWord&gt;&gt; taggedUnits = new ArrayList&lt;List&lt;TaggedWord&gt;&gt;();

String input = ""This sentence is going to be future. The door was opened."";
for (List&lt;HasWord&gt; sentence : MaxentTagger.tokenizeText(new StringReader(input)))
{
     taggedUnits.add(tagger.tagSentence(sentence));
}
</code></pre>
","nlp, stanford-nlp, pos-tagger","<p>You can get tense information from the various penn tags:</p>

<pre><code>27. VB  Verb, base form
28. VBD Verb, past tense
29. VBG Verb, gerund or present participle
30. VBN Verb, past participle
31. VBP Verb, non-3rd person singular present
32. VBZ Verb, 3rd person singular present
</code></pre>

<p>About the active/passive aspect, you can use typed dependencies included in Stanford Core NLP.</p>

<ol>
<li>If the sentence is in active voice, a 'nsubj' dependecy should exist.</li>
<li>If the sentence is in passive voice a 'nsubjpass' dependency should
exist</li>
</ol>

<p>Hope this helps.</p>
",21,6,4196,2013-10-21 13:31:57,https://stackoverflow.com/questions/19495967/getting-additional-information-active-passive-tenses-from-a-tagger
Utility to generate performance report of a NLP based text annotator,"<p>I am trying to build a quality test framework for my text annotator. I wrote my annotators using <a href=""http://gate.ac.uk/"" rel=""nofollow"">GATE</a></p>

<p>I do have gold-standard (human annotated) data for every input document.</p>

<p>Here is list of gate resource for quality assurance <a href=""http://gate.ac.uk/sale/tao/splitch10.html#x14-28100010.3.5"" rel=""nofollow"">GATE Embedded API for the measures</a> </p>

<p>So far, I am able to get performance matrix containing <code>FP,TP,FN, Precision, Recall and Fscores</code> using methods in
<a href=""http://gate.ac.uk/gate/src/gate/util/AnnotationDiffer.java"" rel=""nofollow"">AnnotationDiﬀer</a></p>

<p>Now, I want to dive deeper. I would like to look at individual FP,FN on per document basis.
i.e. I want to analyize  each FP and FN so that I can fix my annotator accordingly.</p>

<p><strong>I didn't see any function in any of GATE's classes such as AnnotationDiffer which returns <code>List&lt;Annotation&gt;</code> of FP or FN. They just return count of FP and FN</strong></p>

<pre><code>int fp=annotationDiffer.getFalsePositivesStrict()
int fn=annotationDiffer.getMissing()
</code></pre>

<p>Before I go ahead and create my own utility to get <code>List&lt;Annotation&gt;</code> of FP and FN and couple of surrounding sentences, to create an HTML report per input document for analysis. I wanted to check if there is something like that already exists.</p>
","nlp, stanford-nlp, gate","<p>I figured it out how to get FP and FN annotations</p>

<pre><code>List&lt;AnnotationDiffer.Pairing&gt; differ= annotationDiffer.calculateDiff(goldAnnotSet, systemAnnotSet);


    for(Annotation fnAnnotation:annotationDiffer.missingAnnotations)
    {
       System.out.println(""FN=&gt;""+fnAnnotation);
    }


    for(Annotation fpAnnotation:annotationDiffer.spuriousAnnotations)
    {
       System.out.println(""FP=&gt;""+fpAnnotation);
    }
</code></pre>

<p>Based on offsets of <code>fnAnnotation</code> or <code>fpAnnotations</code>, I can easily get the surrounding sentences and create a nice html report.</p>
",2,2,126,2013-10-25 19:55:02,https://stackoverflow.com/questions/19598415/utility-to-generate-performance-report-of-a-nlp-based-text-annotator
stanford nlp tool pos tagger property file arch parameter,"<p>I'm using the left3words model provided by stanford nlp tool. in the props file, the arch parameter has indicated some directories which i am unable to locate. can anyone help on this? thanks alot.</p>

<p>arch = left3words,naacl2003unknowns,wordshapes(-1,1),distsim(/u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters,-1,1),distsimconjunction(/u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters,-1,1)</p>
","java, nlp, stanford-nlp","<p>You can get those word clusters <a href=""http://nlp.stanford.edu/software/egw4-reut.512.clusters"" rel=""nofollow"">here</a>.</p>
",0,0,232,2013-11-03 07:59:15,https://stackoverflow.com/questions/19750962/stanford-nlp-tool-pos-tagger-property-file-arch-parameter
Cannot convert result of opennlp Parse,"<p>Im using opennlp's Parse model to parse a line of input, my code:</p>

<pre><code>public void parse(String input){
InputStream modelIn = null;
try {
    modelIn = new FileInputStream(""en-parser-chunking.bin"");
  ParserModel model = new ParserModel(modelIn);
    opennlp.tools.parser.Parser parser = ParserFactory.create(model);
    opennlp.tools.parser.Parse topParses[] = ParserTool.parseLine(input, parser, 1);
for (opennlp.tools.parser.Parse p : topParses){
            p.show();

                         }
}catch (IOException e) {
  e.printStackTrace();
}finally {
  if (modelIn != null) {
    try {
      modelIn.close();
    }
    catch (IOException e) {
    }
  }
}
    }
</code></pre>

<p>if my input is <code>this is a test line</code>
p.show displays <code>(TOP (S (NP (DT this)) (VP (VBZ is) (NP (DT a) (NN test) (NN line)))))</code>
but p.toString displays <code>this is a test line</code> </p>

<p>how can i make it the same as p.show?</p>
","java, parsing, stanford-nlp, opennlp","<p>this works for me... you have to use the overloaded show, which internally updates the StringBuffer reference passed in</p>

<pre><code>public void parse(String input){
InputStream modelIn = null;
try {
    modelIn = new FileInputStream(""en-parser-chunking.bin"");
  ParserModel model = new ParserModel(modelIn);
    opennlp.tools.parser.Parser parser = ParserFactory.create(model);
    opennlp.tools.parser.Parse topParses[] = ParserTool.parseLine(input, parser, 1);
    for (opennlp.tools.parser.Parse p : topParses){

      StringBuilder sb = new StringBuilder(input.length() * 4);
      p.show(sb);
      //sb now contains all the tags
      System.out.println(sb);

    }
}catch (IOException e) {
  e.printStackTrace();
}finally {
  if (modelIn != null) {
    try {
      modelIn.close();
    }
    catch (IOException e) {
    }
  }
}
} 
</code></pre>
",2,2,534,2013-11-05 21:33:02,https://stackoverflow.com/questions/19799428/cannot-convert-result-of-opennlp-parse
Crossvalidation in Stanford NER,"<p>I'm trying to use cross validation in <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">Stanford NER</a>. The <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow"">feature factory</a> lists 3 properties:</p>

<pre><code>numFolds    int 1   The number of folds to use for cross-validation.
startFold   int 1   The starting fold to run.
numFoldsToRun   int 1   The number of folds to run.
</code></pre>

<p>which I think should be used for cross validation. But I don't think they actually work. Setting numFolds to 1 or 10 doesn't change the running time for training at all. And strangely, using numFoldsToRun gives the following warning:</p>

<pre><code>Unknown property: |numFoldsToRun|
</code></pre>
","stanford-nlp, cross-validation","<p>You're right. These options haven't been implemented. If you want to run cross-validation experiments, you'll have to do it completely manually by preparing the data sets yourself. (Sorry!)</p>
",2,2,516,2013-11-09 19:49:32,https://stackoverflow.com/questions/19882294/crossvalidation-in-stanford-ner
Paragraph breaks using Stanford CoreNLP,"<p>Is there a way to extract paragraph information from Stanford CoreNLP?  I'm currently using it to extract sentences from a document, but am also interested in identifying the paragraph structure of the document, which I'd ideally like CoreNLP to do for me.  I have paragraph breaks as double line breaks in my source document.  I've looked through CoreNLP's javadoc, and it seems there is a <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/CoreAnnotations.ParagraphsAnnotation.html"" rel=""nofollow""><code>ParagraphAnnotation</code></a> class, but the documentation doesn't seem to specify what it contains, and I see no example anywhere of how to use it.  Can anyone point me in the right direction?</p>

<p>For reference, my current code does something like this:</p>

<pre><code>    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    List&lt;Sentence&gt; convertedSentences = new ArrayList&lt;&gt; ();
    for (CoreMap sentence : sentences)
    {
        convertedSentences.add (new Sentence (sentence));
    }
</code></pre>

<p>where Sentence's constructor extracts the words from the sentence.  How would I extend this so that I get an extra level of data, that is my currently document-wide 'convertedSentences' list is supplemented by a 'convertedParagraphs' list, each entry of which contains a 'convertedSentences' list?</p>

<p>I tried the approach that seemed most obvious to me:</p>

<pre><code>List&lt;CoreMap&gt; paragraphs = document.get(ParagraphsAnnotation.class);
for (CoreMap paragraph : paragraphs)
{
        List&lt;CoreMap&gt; sentences = paragraph.get(SentencesAnnotation.class);
        List&lt;Sentence&gt; convertedSentences = new ArrayList&lt;&gt; ();
        for (CoreMap sentence : sentences)
        {
            convertedSentences.add (new Sentence (sentence));
        }

        convertedParagraphs.add (new Paragraph (convertedSentences));
}
</code></pre>

<p>but this didn't work, so I guess I misunderstand something about how this is supposed to work.</p>
","java, nlp, stanford-nlp","<p>It appears that the existence of a <code>ParagraphsAnnotation</code> class in CoreNLP is a red herring - nothing actually uses this class (see <a href=""http://grepcode.com/search/usages?type=type&amp;id=repo1.maven.org%24maven2@edu.stanford.nlp%24stanford-corenlp@3.2.0@edu%24stanford%24nlp%24ling@CoreAnnotations.ParagraphsAnnotation&amp;k=u"">http://grepcode.com/search/usages?type=type&amp;id=repo1.maven.org%24maven2@edu.stanford.nlp%24stanford-corenlp@3.2.0@edu%24stanford%24nlp%24ling@CoreAnnotations.ParagraphsAnnotation&amp;k=u</a> - quite literally, there are no references to this class other than its definition).  Therefore, I have to break the paragraphs myself.</p>

<p>The key to this is to notice that each sentence contained within the <code>SentencesAnnotation</code> contains a <code>CharacterOffsetBeginAnnotation</code>.  My code then becomes something like this:</p>

<pre><code>    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    List&lt;Sentence&gt; convertedSentences = new ArrayList&lt;&gt; ();
    for (CoreMap sentence : sentences)
    {
        int sentenceOffsetStart = sentence.get (CharacterOffsetBeginAnnotation.class);
        if (sentenceOffsetStart &gt; 1 &amp;&amp; text.substring (sentenceOffsetStart - 2, sentenceOffsetStart).equals(""\n\n"") &amp;&amp; !convertedSentences.isEmpty ())
        {
            Paragraph current = new Paragraph (convertedSentences);
            paragraphs.add (current);
            convertedSentences = new ArrayList&lt;&gt; ();
        }           
        convertedSentences.add (new Sentence (sentence));
    }
    Paragraph current = new Paragraph (convertedSentences);
    paragraphs.add (current);
</code></pre>
",7,4,1730,2013-11-16 06:55:22,https://stackoverflow.com/questions/20015715/paragraph-breaks-using-stanford-corenlp
java.lang.OutOfMemoryError on running Hadoop job,"<p>I have an input file (~31GB in size) containing consumer reviews about some products which I'm trying to lemmatize and find the corresponding lemma counts of. The approach is somewhat similar to the WordCount example provided with Hadoop. I've 4 classes in all to carry out the processing: StanfordLemmatizer [contains goodies for lemmatizing from Stanford's coreNLP package v3.3.0], WordCount [the driver], WordCountMapper [the mapper], and WordCountReducer [the reducer].</p>

<p>I've tested the program on a subset (in MB's) of the original dataset and it runs fine. Unfortunately, when I run the job on the complete dataset of size ~31GB, the job fails out. I checked the <em>syslog</em> for the job and it contained this:</p>

<blockquote>
  <p>java.lang.OutOfMemoryError: Java heap space at
  edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:109)
  [...]</p>
</blockquote>

<p>Any suggestions on how to handle this?</p>

<p><strong>Note:</strong> I'm using the Yahoo's VM which comes pre-configured with hadoop-0.18.0. I've also tried the solution of assigning more heap as mentioned in this thread: <a href=""https://stackoverflow.com/questions/8464048/out-of-memory-error-in-hadoop"">out of Memory Error in Hadoop</a></p>

<p>WordCountMapper code:</p>

<pre><code>import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

public class WordCountMapper extends MapReduceBase
    implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {

  private final IntWritable one = new IntWritable(1);
  private final Text word = new Text();
  private final StanfordLemmatizer slem = new StanfordLemmatizer();

  public void map(LongWritable key, Text value,
      OutputCollector output, Reporter reporter) throws IOException {

    String line = value.toString();

    if(line.matches(""^review/(summary|text).*""))    //if the current line represents a summary/text of a review, process it! 
    {
        for(String lemma: slem.lemmatize(line.replaceAll(""^review/(summary|text):."", """").toLowerCase()))
        {
            word.set(lemma);
            output.collect(word, one);
        }
    }
  }
}
</code></pre>
","java, hadoop, stanford-nlp","<p>You need to make the size of the individual units that you are processing (i.e., each Map job in the map-reduce) reasonable. The first unit is the size of document that you are providing to the StanfordCoreNLP's annotate() call. The whole of the piece of text that you provide here will be tokenized and processed in memory. In tokenized and processed form, it is over an order of magnitude larger than its size on disk.  So, the document size needs to be reasonable.  E.g., you might pass in one consumer review at a time (and not a 31GB file of text!)</p>

<p>Secondly, one level down, the POS tagger (which precedes the lemmatization) is annotating a sentence at a time, and it uses large temporary dynamic programming data structures for tagging a sentence, which might be 3 orders of magnitude larger in size than the sentence. So, the length of individual sentences also needs to be reasonable. If there are long stretches of text or junk which doesn't divide into sentences, then you may also have problems at this level. One simple way to fix that is to use the <code>pos.maxlen</code> property to avoid POS tagging super long sentences.</p>

<p>p.s. And of course you shouldn't run annotators like parse, dcoref that you're not using, if you only need the lemmatizer.</p>
",2,0,869,2013-11-27 15:58:44,https://stackoverflow.com/questions/20247185/java-lang-outofmemoryerror-on-running-hadoop-job
What is Part of speech (POS) tag in natural language processing,"<p>I am trying to get introduce by Stanford NLP package. I tried to execute few examples on my system.</p>

<p>for sentense: <code>I like it</code> it gives following result:</p>

<p><img src=""https://i.sstatic.net/fucNu.png"" alt=""Result""></p>

<p>Can some one please tell me what is <code>PRP</code> , <code>VBP</code>?
for sentence <code>It was very fantastic experience</code> it gives:</p>

<p><img src=""https://i.sstatic.net/sR96q.png"" alt=""Example 2""></p>

<p><strong>Can someone please elaborate this result?</strong></p>

<p>It can be tested here: <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a></p>

<p>I want to get sentiment result of the sentence. Whether +ve or -ve? with its score value.</p>
","nlp, data-mining, stanford-nlp","<p><a href=""http://nlp.stanford.edu/downloads/tagger.shtml"" rel=""nofollow"">The Stanford NLP documentation</a> refers to <a href=""http://acl.ldc.upenn.edu/J/J93/J93-2004.pdf"" rel=""nofollow"">the Penn treebank documentation</a> for the precise definitions.</p>

<p>However, if <code>PRP</code> == personal pronoun and <code>VBP</code> == present-tense verb were hard to guess, perhaps you should start with some NLP fundamentals before attempting this.</p>

<p>A part-of-speech tagger as such does not perform any sort of sentiment analysis.</p>
",1,-2,221,2013-12-02 18:47:38,https://stackoverflow.com/questions/20335572/what-is-part-of-speech-pos-tag-in-natural-language-processing
Executing and testing stanford core nlp example,"<p>I downloaded stanford core nlp packages and tried to test it on my machine.</p>

<p>Using command: <code>java -cp ""*"" -mx1g edu.stanford.nlp.sentiment.SentimentPipeline -file input.txt</code></p>

<p>I got sentiment result in form of <code>positive</code> or <code>negative</code>. <code>input.txt</code> contains the sentence to be tested.</p>

<p>On more command: <code>java -cp stanford-corenlp-3.3.0.jar;stanford-corenlp-3.3.0-models.jar;xom.jar;joda-time.jar -Xmx600m edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse -file input.txt</code> when executed gives follwing lines :</p>

<pre><code>H:\Drive E\Stanford\stanfor-corenlp-full-2013~&gt;java -cp stanford-corenlp-3.3.0.j
ar;stanford-corenlp-3.3.0-models.jar;xom.jar;joda-time.jar -Xmx600m edu.stanford
.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse -file
input.txt
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3wo
rds/english-left3words-distsim.tagger ... done [36.6 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCF
G.ser.gz ... done [13.7 sec].

Ready to process: 1 files, skipped 0, total 1
Processing file H:\Drive E\Stanford\stanfor-corenlp-full-2013~\input.txt ... wri
ting to H:\Drive E\Stanford\stanfor-corenlp-full-2013~\input.txt.xml {
  Annotating file H:\Drive E\Stanford\stanfor-corenlp-full-2013~\input.txt [13.6
81 seconds]
} [20.280 seconds]
Processed 1 documents
Skipped 0 documents, error annotating 0 documents
Annotation pipeline timing information:
PTBTokenizerAnnotator: 0.4 sec.
WordsToSentencesAnnotator: 0.0 sec.
POSTaggerAnnotator: 1.8 sec.
MorphaAnnotator: 2.2 sec.
ParserAnnotator: 9.1 sec.
TOTAL: 13.6 sec. for 10 tokens at 0.7 tokens/sec.
Pipeline setup: 58.2 sec.
Total time for StanfordCoreNLP pipeline: 79.6 sec.

H:\Drive E\Stanford\stanfor-corenlp-full-2013~&gt;
</code></pre>

<p>Could understand. No informative result.</p>

<p>I got one example at : <a href=""https://stackoverflow.com/questions/11832490/stanford-core-nlp-java-output"">stanford core nlp java output</a></p>

<pre><code>import java.io.*;
import java.util.*;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

public class StanfordCoreNlpDemo {

  public static void main(String[] args) throws IOException {
    PrintWriter out;
    if (args.length &gt; 1) {
      out = new PrintWriter(args[1]);
    } else {
      out = new PrintWriter(System.out);
    }
    PrintWriter xmlOut = null;
    if (args.length &gt; 2) {
      xmlOut = new PrintWriter(args[2]);
    }

    StanfordCoreNLP pipeline = new StanfordCoreNLP();
    Annotation annotation;
    if (args.length &gt; 0) {
      annotation = new Annotation(IOUtils.slurpFileNoExceptions(args[0]));
    } else {
      annotation = new Annotation(""Kosgi Santosh sent an email to Stanford University. He didn't get a reply."");
    }

    pipeline.annotate(annotation);
    pipeline.prettyPrint(annotation, out);
    if (xmlOut != null) {
      pipeline.xmlPrint(annotation, xmlOut);
    }
    // An Annotation is a Map and you can get and use the various analyses individually.
    // For instance, this gets the parse tree of the first sentence in the text.
    List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
    if (sentences != null &amp;&amp; sentences.size() &gt; 0) {
      CoreMap sentence = sentences.get(0);
      Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
      out.println();
      out.println(""The first sentence parsed is:"");
      tree.pennPrint(out);
    }
  }

}
</code></pre>

<p>Tried to execute it in netbeans with including necessary library. But it always stuck in between or gives exception <code>Exception in thread “main” java.lang.OutOfMemoryError: Java heap space</code></p>

<p>Thou I set the memory to be allocated in <code>property/run/VM box</code></p>

<p>Any idea how can I run above java example using command line?</p>

<p><strong>I want to get sentiment score of the example</strong></p>

<p><strong>UPDATE</strong></p>

<p>output of :  <code>java -cp ""*"" -mx1g edu.stanford.nlp.sentiment.SentimentPipeline -file input.txt</code></p>

<p><img src=""https://i.sstatic.net/Xy8ms.png"" alt=""enter image description here""></p>

<p>out put of: <code>java -cp stanford-corenlp-3.3.0.j
ar;stanford-corenlp-3.3.0-models.jar;xom.jar;joda-time.jar -Xmx600m edu.stanford
.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse -file
input.txt</code></p>

<p><img src=""https://i.sstatic.net/HRbp4.png"" alt=""Out of of above command""></p>
","java, nlp, stanford-nlp","<p>You need to add the ""sentiment"" annotator to the list of annotators:</p>

<pre><code>-annotators tokenize,ssplit,pos,lemma,parse,sentiment
</code></pre>

<p>This will add a ""sentiment"" property to each sentence node in your XML.</p>
",15,11,40147,2013-12-03 18:55:43,https://stackoverflow.com/questions/20359346/executing-and-testing-stanford-core-nlp-example
Inconsistencies in tokenizing large English files using Stanford&#39;s PTBTokenizer?,"<p>I have the Stanford PTBTokenizer (included with POS tagger v3.2.0) from the Stanford JavaNLP API that I'm using to try to tokenize a largish (~12M) file (English language text).  Invoking from bash:</p>

<pre><code>java -cp ../david/Desktop/quest/lib/stanford-parser.jar \
  edu.stanford.nlp.process.PTBTokenizer -options normalizeAmpersandEntity=true \
  -preserveLines foo.txt &gt;tmp.out
</code></pre>

<p>I see instances of punctuation not tokenized properly in some places but not others.  E.g., output contains ""Center, Radius{4}"" and also contains elsewhere ""Center , Radius -LCB- 4 -RCB-"".  (The former is a bad tokenization; the latter is correct.)</p>

<p>If I isolate the lines that don't get tokenized properly in their own file and invoke the parser on the new file, the output is fine.</p>

<p>Has anybody else run into this?  Is there a way to work around that doesn't involve checking output for bad parses, separating them, and re-tokenizing?</p>
","tokenize, stanford-nlp","<p>Upgrading to the latest version (3.3.0) fixed the comma attachment problem.  There are spurious cases of brackets/braces not being tokenized correctly (mostly because they are [mis-]recognized as emoticons).</p>

<p>Thanks again to Professor Manning &amp; John Bauer for their prompt &amp; thorough help.</p>
",0,0,189,2013-12-07 03:47:59,https://stackoverflow.com/questions/20437326/inconsistencies-in-tokenizing-large-english-files-using-stanfords-ptbtokenizer
Stanford POS Tagger OutOfMemoryError reading tagger,"<p>I'm trying to use the Stanford POS tagger (<a href=""http://nlp.stanford.edu/downloads/tagger.shtml#Download"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/downloads/tagger.shtml#Download</a>), but when trying to initialize the tagger by</p>
<pre><code>MaxentTagger tagger = new MaxentTagger(&quot;english-left3words-distsim.tagger&quot;);
</code></pre>
<p>I always get this error:</p>
<pre><code>Reading POS tagger model from stanford-postagger-2013-11-12/models/english-left3words-distsim.tagger ... Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space
  at java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:3443)
  at java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3250)
  at java.io.ObjectInputStream.readString(ObjectInputStream.java:1628)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1320)
  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
  at java.util.HashMap.readObject(HashMap.java:1029)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1685)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323)
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
  at edu.stanford.nlp.tagger.maxent.MaxentTagger.readExtractors(MaxentTagger.java:582)
  at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:808)
  at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:755)
  at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:289)
  at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:253)
</code></pre>
<p>I'm running this in Eclipse, and I'm allocating 4gb of heap space to the jvm as per the linked instructions from <a href=""http://nlp.stanford.edu/downloads/pos-tagger-faq.shtml#oom"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/downloads/pos-tagger-faq.shtml#oom</a> (using <code>-vmargs -Xms4096M -Xmx4096M -mx4096m</code>)</p>
<p>I found this bug in a bit of searching: <a href=""https://bugs.java.com/bugdatabase/view_bug?bug_id=6525563"" rel=""nofollow noreferrer"">https://bugs.java.com/bugdatabase/view_bug?bug_id=6525563</a> which seems related, but I think the Stanford tagger is so widely used I doubt I would be the first one to turn up this OutOfMemoryError if it were due to that bug...</p>
<p><strong>Update</strong>: it seems that Eclipse is not actually getting the memory I'm trying to allocate.       Runtime.getRuntime().maxMemory() reports that it only has 123Mb available, while other projects in the same workspace have 1 gig available.</p>
","java, out-of-memory, stanford-nlp","<p>Forgot to add the vmargs to the project's run configuration.</p>

<p>Adding <code>-Xms512m -Xmx1024m -mx512m</code> to the VM arguments section of Run Configurations allows the tagger to be read properly.</p>
",0,0,815,2013-12-07 18:09:01,https://stackoverflow.com/questions/20444731/stanford-pos-tagger-outofmemoryerror-reading-tagger
Java Modifying key object inside map,"<p>I am having a problem with JAVA map. I enter an object as a key in the map. Then I modify the key and the map does not consider the object as a key of the map any more. Even though the key inside the object has been modified accordingly.</p>

<p>I am working with the object <code>CoreLabel</code> from StanfordNLP but it applies to a general case I guess.</p>

<pre><code>Map &lt;CoreLabel, String&gt; myMap = new HashMap...
CoreLabel key = someCreatedCoreLabel
myMap.put(key, someString)
myMap.get(key) != null ----&gt; TRUE
key.setValue(""someValue"");
myMap.get(key) != null ----&gt; FALSE
</code></pre>

<p>I hope I was clear enough. The question is why is the last statement false? I am not a very experienced programmer but I would expect it to be true. Maybe has something to do with the <code>CoreLabel</code> object?</p>

<p>I check if <code>.equals()</code> still holds, and it actually does</p>

<pre><code>for(CoreLabel token: myMap.keySet()) {
     if(key.equals(token))
        System.out.println(""OK"");
}
</code></pre>
","java, dictionary, pass-by-reference, stanford-nlp, pass-by-value","<p>The problem is that in modifying the value of the key, now the hash code of the key has changed as well.  A <code>HashMap</code> will first use the hash code of the key to determine if it exists.  The modified hash code didn't exist in the map, so it didn't even get to try the step of using the <code>equals</code> method.  That's why it's a bad idea to change your key objects while they're in a <code>HashMap</code>.</p>
",3,1,1156,2013-12-16 21:57:16,https://stackoverflow.com/questions/20621744/java-modifying-key-object-inside-map
Java reading past saved variables,"<p>I am taking questions from user input which is being scanned for information and i am saving the information and the original question. </p>

<p>What is the best way to save the question+information so that it can be recalled so that i am able to read X questions before the question just asked and get the information/variables on it?</p>

<pre><code>An example:

[User]""what is 1+1?""
[Reply]""two""
save information: ""what is 1+1"",""two""

[User]""how is the dog?""
[Reply]""fine""
save information:""how is the dog?"",subject = ""dog"", condition = ""fine""

getQuestion(1) -&gt; ""what is 1+1?""
getInformation(current) -&gt; subject = ""dog"" condition = ""fine""
</code></pre>

<p>Also what would be better to call the information for each question using, an int (question X), string (question) or other?</p>

<p>Code so far:</p>

<pre><code>Map&lt;String,String&gt; variable = new HashMap&lt;&gt;();
    public QuestionInfo(String name,String variable) {
        this.variable.put(name, variable);
    }
    public String get(String name){
    if(variable.containsKey(name)) {
        return variable.get(name);
    } else {
        return """";
    }
    }
</code></pre>

<p>I though about using:</p>

<pre><code>  Map&lt;String,QuestionInfo&gt; Questions = new HashMap&lt;&gt;();
</code></pre>

<p>first but then if i wanted to call i cant use an int
so instead:</p>

<pre><code>Map&lt;Integer,QuestionInfo&gt; Questions = new HashMap&lt;&gt;();
</code></pre>

<p>then i can add the question as the first piece of information.</p>
","java, file, io, save, stanford-nlp","<p>Since you want to preserve the order of responses, you probably want to use a <a href=""http://docs.oracle.com/javase/7/docs/api/java/util/List.html"" rel=""nofollow"">List</a>.</p>

<p>Then you just need to decide what kind of list to make. Since you have two items that are closely related - the subject (a String) and condition - I would probably create a custom class:</p>

<pre><code>import java.util.Map;
public class QuestionInfo {

    private String question;
    private Map&lt;String, String&gt; variables;

    public QuestionInfo(String question, Map&lt;String, String&gt; variables) {
        this.question = question;
        this.variables = variables;
    }

    public String question() {
        return question;
    }

    public Map&lt;String, String&gt; vars() {
        return variables;
    }
}
</code></pre>

<p>Then you would create your list to store these like this, for example:</p>

<pre><code>List&lt;QuestionInfo&gt; questionList = new ArrayList&lt;QuestionInfo&gt;();
</code></pre>

<p>There are other ways to store these strings (for example, you could concatenate them with a delimiter, or use a String[2]).. but this will probably be the easiest to read and understand later, in my opinion.</p>

<p>Here's an approximation of what the user would enter, and how you would use the <code>questionList</code> + <code>QuestionInfo</code> to store the data:</p>

<p>[User] ""how is the dog?""
[Reply] ""fine""</p>

<pre><code>Map&lt;String, String&gt; varMap = new HashMap&lt;String, String&gt;();
varMap.put(""subject"", ""dog"");
varMap.put(""condition"", ""fine"");
questionList.add(new QuestionInfo(""how is the dog?"", varMap));
</code></pre>

<p>Now you can access this info by using:</p>

<pre><code>questionList.get(0).question() // returns ""how is the dog?""
</code></pre>

<p>or</p>

<pre><code>questionList.get(0).vars().get(""condition"") // returns ""fine""
</code></pre>

<p>Note that this is glossing over the code to actually gather the user input.</p>

<p>There are other possibilities.
You could pass in a <code>String...</code> to the <code>QuestionInfo</code> constructor and parse the variables/create the map there, for example.
You also may want to store the entire response as a String separate from the variables.
You may also want to create an enum for the keys, if you have a consistent set of keys you'll be populating.</p>
",2,1,95,2013-12-19 16:23:47,https://stackoverflow.com/questions/20686686/java-reading-past-saved-variables
Parse sentence Stanford Parser by passing String not an array of strings,"<p>Is it possible to parse a sentence using the Stanford Parser by passing a string and not an array of strings. This is the example they gave in their short tutorial (<a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/package-summary.html"" rel=""nofollow"">See Docs</a>) :</p>

<p>Here's example:</p>

<pre><code>    import java.util.*;
    import edu.stanford.nlp.ling.*;
    import edu.stanford.nlp.trees.*;
    import edu.stanford.nlp.parser.lexparser.LexicalizedParser;

    class ParserDemo {
      public static void main(String[] args) {
        LexicalizedParser lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
        lp.setOptionFlags(new String[]{""-maxLength"", ""80"", ""-retainTmpSubcategories""});

        String[] sent = { ""This"", ""is"", ""an"", ""easy"", ""sentence"", ""."" }; // This is the sentence to be parsed
        List&lt;CoreLabel&gt; rawWords = Sentence.toCoreLabelList(sent);
        Tree parse = lp.apply(rawWords);
        parse.pennPrint();
        System.out.println();

        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
        GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
        List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();
        System.out.println(tdl);
        System.out.println();

      }

}
</code></pre>

<p>I am trying to see if I can do this because I need to get sentences from a MySQL database and parse them directly as strings. I could tokezine the sentences and add the words, commas, and period to a String Array, However, to tokenize these sentences, I would have to use the Stanford Tokenizer , PTBTokenizer. The constructor of this tokenizer as listed here</p>

<p>(<a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/PTBTokenizer.html#PTBTokenizer%28java.io.Reader,%20edu.stanford.nlp.process.LexedTokenFactory,%20java.lang.String%29"" rel=""nofollow"">See Docs</a>)</p>

<p>requires a ""java.io.FileReader"" Object, but I am not reading a file from directory. So I am wondering if there is a way to either Parse the sentence directly by passing a string, or if I can solve my problem by tokenizing the sentence without requiring a ""java.io.FileReader"" Object.</p>
","java, nlp, stanford-nlp","<p>For simple usage, with the default tokenizer and default tokenizer options for a grammar, there is an easy convenience method you can use:</p>

<pre><code>lp.parse(String)
</code></pre>

<p>But the <code>PTBTokenizer</code> methods that you point at don't take a <code>FileReader</code>, they just take a <code>Reader</code>, so you can also easily point a <code>PTBTokenizer</code> at a String by wrapping the String in a <code>StringReader</code>. This is the right approach if you need more control over how tokenization happens.</p>
",1,1,1481,2013-12-28 10:35:57,https://stackoverflow.com/questions/20813541/parse-sentence-stanford-parser-by-passing-string-not-an-array-of-strings
Stanford CoreNLP remove/stop red information print outs,"<p>I'm using Stanford's CoreNLP Java API and while running it prints out information in red.
It just fills up the command lines when i don't want to see it.
is there anyway of disabling this feature?</p>
<p>Example of the red info lines:</p>
<pre><code>Searching for resource: StanfordCoreNLP.properties
Searching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.2 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [3.0 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.7 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [2.0 sec].
Initializing JollyDayHoliday for sutime
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Jan 03, 2014 3:52:37 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Ignoring inactive rule: temporal-composite-8:ranges
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.8 sec].
Adding annotator dcoref
Searching for resource: StanfordCoreNLP.properties
Searching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Adding annotator lemma
Adding annotator ner
Adding annotator parse
Adding annotator dcoref
Searching for resource: StanfordCoreNLP.properties
Searching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Adding annotator lemma
Adding annotator ner
Adding annotator parse
Adding annotator dcoref
</code></pre>
","java, nlp, stanford-nlp","<p>Depending on your program context, you can just drop all text from the error stream's output during corenlp execution.</p>

<pre>
// this is your print stream, store the reference
PrintStream err = System.err;

// now make all writes to the System.err stream silent 
System.setErr(new PrintStream(new OutputStream() {
    public void write(int b) {
    }
}));

// YOUR CODE HERE

// set everything bck to its original state afterwards
System.setErr(err);         
</pre>
",5,6,2213,2014-01-03 16:09:50,https://stackoverflow.com/questions/20907909/stanford-corenlp-remove-stop-red-information-print-outs
Using the Stanford Dependency Parser on a previously tagged sentence,"<p>I'm currently using the Twitter POS tagger <a href=""http://www.ark.cs.cmu.edu/TweetNLP/"" rel=""nofollow"">available here</a> to tag out tweets into the Penn-Tree Bank tags.  </p>

<p>Here is that code:</p>

<pre><code>import java.util.List;

import cmu.arktweetnlp.Tagger;
import cmu.arktweetnlp.Tagger.TaggedToken;

/* Tags the tweet text */
List&lt;TaggedToken&gt; tagTweet(String text) throws IOException {

    // Loads Penn Treebank POS tags
    tagger.loadModel(""res/model.ritter_ptb_alldata_fixed.txt"");

    // Tags the tweet text
    taggedTokens = tagger.tokenizeAndTag(text);

    return taggedTokens;
}
</code></pre>

<p>Now I need to identify where the direct objects are in these tags.  After some searching, I've discovered that the Stanford Parser can do this, by way of the Stanford Typed Dependencies, (<a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">online example</a>).  By using the dobj() call, I should be able to get what I need.</p>

<p>However, I have not found any good documentation about how to feed already-tagged sentences into this tool.  From what I understand, before using the Dependency Parser I need to create a tree from the sentence's tokens/tags. How is this done? I have not been able to find any example code.</p>

<p>The Twitter POS Tagger contains an instance of the Stanford NLP Tools, so I'm not far off, however I am not familiar enough with the Stanford tools to feed my POS-tagged text into it in order to get the dependency parser to work properly.  <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#f"" rel=""nofollow"">The FAQ does mention this functionality</a>, but without any example code to go off of, I'm a bit stuck.</p>
","java, twitter, nlp, stanford-nlp, part-of-speech","<p>Here is how it is done with completely manual creation of the <code>List</code> discussed in <a href=""http://nlp.stanford.edu/downloads/parser-faq.shtml#f"" rel=""nofollow"">the FAQ</a>:</p>

<pre><code>String[] sent3 = { ""It"", ""can"", ""can"", ""it"", ""."" };
// Parser gets second ""can"" wrong without help (parsing it as modal MD)
String[] tag3 = { ""PRP"", ""MD"", ""VB"", ""PRP"", ""."" };                                                 
List&lt;TaggedWord&gt; sentence3 = new ArrayList&lt;TaggedWord&gt;();
for (int i = 0; i &lt; sent3.length; i++) {
  sentence3.add(new TaggedWord(sent3[i], tag3[i]));
}
Tree parse = lp.parse(sentence3);
parse.pennPrint();
</code></pre>
",2,2,2234,2014-01-08 01:54:42,https://stackoverflow.com/questions/20985604/using-the-stanford-dependency-parser-on-a-previously-tagged-sentence
Two JARs on buildpath with identical method names but different constructors. How can I specify which JAR&#39;s method to use?,"<p>I am building a tool from several different open source libraries.  My buildpath is in the following order:</p>

<p>My first JAR file, <code>stanford-corenlp-3.3.0.jar</code>, contains a package called <code>edu.stanford.nlp.process</code>, which has the <code>Morphology.class</code> class.</p>

<p><img src=""https://i.sstatic.net/IgHEA.png"" alt=""enter image description here""></p>

<p>My second JAR file, <code>ark-tweet-nlp-0.3.2.jar</code>, contains an identical package name (<code>edu.stanford.nlp.process</code>), and an identical class name <code>Morphology.class</code>.</p>

<p><img src=""https://i.sstatic.net/jjGxA.png"" alt=""enter image description here""></p>

<p>In both JARS, inside their respective <code>Morphology</code> classes there exists a method called <code>stem()</code>.  However, the constructors for these methods are different. I want to use the <code>stem(String, String)</code> method from my second JAR file, but since the import statement (<code>import edu.stanford.nlp.process.Morphology;</code>) does not specify which JAR to use, I get an error since it thinks the first JAR on the buildpath is the one I want to implement.  </p>

<p>I don't want to change the order of my buildpath since it would throw off my other method calls. </p>

<p>How can I specify which JAR's <code>Morphology</code> class to use?  Is there an import statement that specifies the JAR, along with the package.class?</p>

<p>EDIT:  What about a way to combine my two JARs so that the two <code>Morphology</code> classes merge, giving me two methods with different constructors?</p>
","java, eclipse, jar, stanford-nlp","<p>I think your problem can be solved simply by using the <code>lemma(String word, String tag)</code> method in the current CoreNLP's Morphology class:</p>

<pre><code>String word = ...;
String tag = ...;
String lemma = morphology.lemma(word, tag);
WordTag wt = new WordTag(lemma, tag);
</code></pre>

<p>When the class was revised a couple of years ago, the method you're looking for was deleted. The feeling was that with most of the Stanford NLP code moving to using <code>CoreLabel</code>s, methods that return <code>WordTag</code> are less useful (though deleting all such methods is still a work in progress).</p>
",1,2,996,2014-01-08 19:38:48,https://stackoverflow.com/questions/21004983/two-jars-on-buildpath-with-identical-method-names-but-different-constructors-ho
Sentiment analysis of entity using stanford nlp,"<p>I extracted all the entities present in a particular sentence. For example if my sentence is</p>

<pre><code>  infrastructure is good, Work-culture is pathetic,hikes are not good either
</code></pre>

<p>I have developed a code that gives me entity. now i need sentiment based upon entities. my output should be something like</p>

<pre><code>infrastructure--&gt; positive
work-culture--&gt; negative
hikes--&gt; negative
</code></pre>

<p>how am i supposed to do that?</p>
","java, stanford-nlp","<p>If you are done with the coding next thing which is the most challenging part is to train the system with proper content. I have worked in Google prediction API for same sentiment analysis. You need content for the matter, means if it is a movie review then the training content should contains lots of movie review. I can tell you I have trained a system for movie review analysis with 30 movie review contents(15 positive and 15 negative). Still the system does not give 80% perfect result.</p>
",1,0,1243,2014-01-10 11:52:02,https://stackoverflow.com/questions/21043665/sentiment-analysis-of-entity-using-stanford-nlp
java.lang.NoClassDefFoundError CRFClassifier in a Rails app,"<p>I'm trying to run the CRFClassifier on a string to extract entities from the string. I'm using the Ruby bindings for the Stanford NLP entity recognizer from here: <a href=""https://github.com/tiendung/ruby-nlp"" rel=""nofollow"">https://github.com/tiendung/ruby-nlp</a></p>

<p>It works perfectly fine on its own class say (nlp.rb). When I run <code>ruby nlp.rb</code> it works fine. However, I've tried to create an object of this class inside one of my controllers in my rails app and for some reason I'm getting the following error: </p>

<p><strong><code>java.lang.NoClassDefFoundError: edu/stanford/nlp/ie/crf/CRFClassifier</code></strong></p>

<p>Here is the code that works fine on its own but not inside a controller.</p>

<pre><code>    def initialize
        Rjb::load('stanford-postagger.jar:stanford-ner.jar', ['-Xmx200m'])
        crfclassifier = Rjb::import('edu.stanford.nlp.ie.crf.CRFClassifier')
        maxentTagger = Rjb::import('edu.stanford.nlp.tagger.maxent.MaxentTagger')
        maxentTagger.init(""left3words-wsj-0-18.tagger"")
        sentence = Rjb::import('edu.stanford.nlp.ling.Sentence')
        @classifier = crfclassifier.getClassifierNoExceptions(""ner-eng-ie.crf-4-conll.ser.gz"")


    end


    def get_entities(sentence)
        sent = sentence
        @classifier.testStringInlineXML( sent )

    end
</code></pre>

<p>It's the same exact code in both cases. Anyone has any idea of what's happening here!?</p>

<p>Thanks in advance!</p>
","ruby, nlp, stanford-nlp, named-entity-recognition","<p>I think you need this:</p>

<p>Rjb::load('/path/to/jar/stanford-postagger.jar:/path/to/jar/stanford-ner.jar', ['-Xmx200m'])</p>

<p>I just tried this and it works. Create a dir in lib called nlp. Put the jars there and then create a class which loads the jars using the full path:</p>

<p>So you end up with:</p>

<pre><code>├── lib
│   ├── nlp
│   │   ├── stanford-ner.jar
│   │   └── stanford-postagger.jar
│   └── nlp.rb



require 'rjb'

class NLP
  def initialize
    pos_tagger = File.expand_path('../nlp/stanford-postagger.jar', __FILE__)
    ner = File.expand_path('../nlp/stanford-ner.jar', __FILE__)
    Rjb::load(""#{pos_tagger}:#{ner}"", ['-Xmx200m'])
    crfclassifier = Rjb::import('edu.stanford.nlp.ie.crf.CRFClassifier')
    maxentTagger = Rjb::import('edu.stanford.nlp.tagger.maxent.MaxentTagger')
    maxentTagger.init(""left3words-wsj-0-18.tagger"")
    sentence = Rjb::import('edu.stanford.nlp.ling.Sentence')
    @classifier = crfclassifier.getClassifierNoExceptions(""ner-eng-ie.crf-4-conll.ser.gz"")
  end


  def get_entities(sentence)
    sent = sentence
    @classifier.testStringInlineXML( sent )
  end
end
</code></pre>

<p>Little test class:</p>

<pre><code>require_relative 'lib/nlp'

n = NLP.new
n.get_entities(""Good afternoon Rajat Raina, how are you today?"")
</code></pre>

<p>output:</p>

<pre><code>ruby t.rb
Loading classifier from /Users/brendan/code/ruby/ruby-nlp/ner-eng-ie.crf-4-conll.ser.gz ... done [1.2 sec].
Getting data from Good afternoon Rajat Raina, how are you today? (default encoding)
Good afternoon &lt;PERSON&gt;Rajat Raina&lt;/PERSON&gt;, how are you today?
</code></pre>
",1,1,608,2014-01-24 21:29:21,https://stackoverflow.com/questions/21342811/java-lang-noclassdeffounderror-crfclassifier-in-a-rails-app
Custom NER and POS tagging,"<p>I was checking out Stanford CoreNLP in order to understand NER and POS tagging. But what if I want to create custom tags for entities like<code>&lt;title&gt;Nights&lt;/title&gt;, &lt;genre&gt;Jazz&lt;/genre&gt;, &lt;year&gt;1992&lt;/year&gt;</code> How can I do it? is CoreNLP useful in this case?</p>
","nlp, stanford-nlp, named-entity-recognition, pos-tagger","<p>CoreNLP out-of-the-box will be restricted to types they mention : PERSON, LOCATION, ORGANIZATION, MISC, DATE, TIME, MONEY, NUMBER. No, you won't be able to recognize other entities just by assuming it could ""intuitively"" do it :)</p>

<p>In practice, you'll have to choose, either:</p>

<ol>
<li>Find another NER systems that tags those types</li>
<li>Address this tagging task using knowledge-based / unsupervised approaches.</li>
<li>Search for extra resources (corpora) that contain types you want recognize, and re-train a supervised NER system (CoreNLP or other)</li>
<li>Build (and possibly annotate) your own resources - then you'll have to define an annotation scheme, rules, etc. - quite an interesting part of the work!</li>
</ol>

<p>Indeed, unless you find an existing system that fulfills your needs, some effort will be required! Unsupervised approaches may help you bootstrapping a system, so as to see if you need to find / annotate a dedicated corpus. In the latter case, it would be better to separate data as train/dev/test parts, so as to be able to assess how much the resulting system performs on unseen data.</p>
",4,4,2496,2014-01-26 00:03:02,https://stackoverflow.com/questions/21357881/custom-ner-and-pos-tagging
Which Stanford NLP package to use for content categorization&gt;,"<p>I have about 5000 terms in a table and I want to group them into categories that make sense.</p>
<p>For example some terms are:</p>
<pre><code>Nissan
Ford
Arrested
Jeep
Court
</code></pre>
<p>The result should be that Nissan, Ford, Jeep get grouped into one category and that Arrested and Court are in another category. I looked at the Stanford Classifier NLP. Am I right to assume that this is the right one to choose to do this for me?</p>
","machine-learning, nlp, stanford-nlp, categorization","<p>I would suggest you to use <a href=""http://nltk.org/"" rel=""nofollow"">NLTK</a> if there weren't many proper nouns. You can use the semantic similarity from WordNet as features and try to cluster the words. Here's a <a href=""https://groups.google.com/forum/#!topic/nltk-users/kr-ezQ_rrOk"" rel=""nofollow"">discussion</a> about how to do that.</p>

<p>To use the Stanford Classifier, you need to know how many buckets (classes) of words you want. Besides I think that is for documents rather than words.</p>
",2,-1,240,2014-01-28 15:56:21,https://stackoverflow.com/questions/21410490/which-stanford-nlp-package-to-use-for-content-categorization
How to run multiple classifiers with Stanford NER?,"<p>I'd like to run one of the built-in classifiers on a file, then run my own classifier, merging the results.</p>

<p>How do I do so with Stanford NER, in particular, via the command line?</p>

<p>I am aware of <a href=""https://stackoverflow.com/questions/15746695/how-do-i-include-more-than-one-classifiers-when-using-stanford-named-entity-reco"">How do I include more than one classifiers when using Stanford named entity recogniser?</a> , but this is slightly different, as that questions asks about multiple classifiers with <strong>NERServer</strong>.</p>

<p>Looks like I need to use CoreNLP to run multiple NER models in sequence...can I do it without CoreNLP?</p>

<p>Say I had a file with contents ""the quick brown fox jumped over the lazy dog in America"". I run the one of the built-in classifiers, and it finds ""America"" as a location, then I run my own, and it finds ""fox"" and ""dog"", the result should be:</p>

<pre><code>the quick brown &lt;animal&gt;fox&lt;/animal&gt; jumped over the lazy &lt;animal&gt;dog&lt;/animal&gt; in &lt;location&gt;America&lt;/location
</code></pre>
","nlp, stanford-nlp, named-entity-recognition","<p>So, a place to get started if you're dead set on doing this in a single command from the command line: </p>

<pre><code>cat corpus.txt | tee `stanfordNER -options here &gt; out1.xml` | myNERTagger -options here &gt; out2.xml &amp;&amp; diff out1.xml out2.xml | awk to do whatever merging you want here...
</code></pre>

<p>But what you'll likely find is that this is not a solution. You're going to want to go sentence-by-sentence in a little script, calling <a href=""https://github.com/dat/pyner"" rel=""nofollow"">pyner</a> or similar to hook into the Stanford tagger and then whatever custom tagger you've built, merging the differences as you go along. The output formatting of your taggers will change how this looks pretty dramatically. </p>
",0,2,1164,2014-01-30 19:46:03,https://stackoverflow.com/questions/21466083/how-to-run-multiple-classifiers-with-stanford-ner
How do I use IOB tags with Stanford NER?,"<p>There seem to be a few different settings:</p>

<pre><code>iobtags
iobTags
entitySubclassification (IOB1 or IOB2?)
evaluateIOB
</code></pre>

<p>Which setting do I use, and how do I use it correctly?</p>

<p>I tried labelling like this:</p>

<pre><code>1997    B-DATE
volvo   B-BRAND
wia64t  B-MODEL
highway B-TYPE
tractor I-TYPE
</code></pre>

<p>But on the training output, it seemed to think that B-TYPE and I-TYPE were different classes. </p>

<p>I am using the 2013-11-12 release. </p>
","stanford-nlp, named-entity-recognition","<p>How this can be done is currently (2013 releases) a bit of a mess, since there are two different sets of flags for two different <code>DocumentReaderAndWriter</code> implementations. Sorry.</p>

<p>The most flexible support for different IOB styles is found in <code>CoNLLDocumentReaderAndWriter</code>. You can have it map any IOB/IOE/... annotation done by hyphenated prefixes like your examples (B-BRAND) to any other while it is reading files with the flag:</p>

<pre><code>-entitySubclassification IOB2
</code></pre>

<p>The resulting label set is then used for training and classification. The options are documented in the <code>entitySubclassify()</code> method of <code>CoNLLDocumentReaderAndWriter</code>: IOB1, IOB2, IOE1, IOE2, SBIEO, IO. You can find a discussion of IOB1 vs. IOB2 in <a href=""http://acl.ldc.upenn.edu/E/E99/E99-1023.pdf"">Tjong Kim Sang and Veenstra 1999</a>. By default the representation is mapped back to IOB1 on output, since that is the default used in the CoNLL <code>conlleval</code> program, but you can keep it as what you mapped it to with the flag: </p>

<pre><code>-retainEntitySubclassification
</code></pre>

<p>To use this <code>DocumentReaderAndWriter</code>, you can give a training command like:</p>

<pre><code>java8 -mx6g edu.stanford.nlp.ie.crf.CRFClassifier -prop conll.crf.chris2009.prop -readerAndWriter edu.stanford.nlp.sequences.CoNLLDocumentReaderAndWriter -entitySubclassification iob2
</code></pre>

<p>Alternatively, <code>ColumnDocumentReaderAndWriter</code> is the default <code>DocumentReaderAndWriter</code> which we use in the distributed models. The options you get with it are different and slightly more limited. You have these two flags:</p>

<ul>
<li><code>-mergeTags</code> will take either plain (""BRAND"") or CoNLL-like (""I-BRAND"") labels and map them down to a prefix-less IO label (""BRAND"") and use that for training and classifying.</li>
<li><code>-iobTags</code> can take either plain (""BRAND"") or CoNLL-like (""I-BRAND"") labels and maps them to IOB2.</li>
</ul>

<p>In a sequence model, for any of the labeling schemes like IOB2, the labels <strong>are</strong> different classes. That is how these labeling schemes work. The special interpretation of ""I-"", ""B-"", etc. is left to the human observer and entity-level evaluation software. The included evaluation software will work with IOB1, IOB2, or prefixless IO encoding only. </p>
",14,7,7411,2014-01-30 22:48:22,https://stackoverflow.com/questions/21469082/how-do-i-use-iob-tags-with-stanford-ner
Natural Language Processing using nltk,"<p>I need an algorithm to tokenize given sentence into words which are correctly tagged to its grammar meaning. </p>

<p>for example: ""People took to the streets and protested""
people-noun
took- adjective
and-conjunction
to- ...and so on</p>
","python, nlp, nltk, stanford-nlp","<p>You mean you want part of speech tagging?</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; tokens = nltk.word_tokenize(""People took to the streets and protested"")
&gt;&gt;&gt; nltk.pos_tag(tokens)
[('People', 'NNS'), ('took', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('streets', 'NNS'), ('and', 'CC'), ('protested', 'VBD')]
</code></pre>
",0,-1,197,2014-02-06 15:49:27,https://stackoverflow.com/questions/21607541/natural-language-processing-using-nltk
Command Line Process,"<p>So after scouring the web I found a few articles (some on stackoverflow) which described how to execute a command line prompt by starting a new process in c#. The second argument, which I've commented out, works just fine, but the one I actually need (the first one) doesn't. It returns the error ""Could not find or load main class edu.stanford.nlp.parser.lexparser.LexicalizedParser"" When I open up a command line (non-programatically) and then execute the same command (aside from the escaped quotations) it works great. Any idea's about what the problem could be? Thanks!</p>

<pre><code>var proc = new Process
{
    StartInfo = new ProcessStartInfo
    {
        FileName = ""CMD.exe"",
        Arguments = ""/c java -mx100m -cp \""*\"" edu.stanford.nlp.parser.lexparser.LexicalizedParser edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz libtest.txt"",
        // Arguments = ""/c echo Foo"", 
        UseShellExecute = false,
        RedirectStandardOutput = true,
        RedirectStandardError = true,
        CreateNoWindow = true
    }
};
proc.Start();

Console.WriteLine(proc.StandardOutput.ReadToEnd());
Console.WriteLine(proc.StandardError.ReadToEnd());
</code></pre>
","c#, command-line, process, stanford-nlp","<p>Ensure that the executing path where you start your process is correct!</p>

<p>You can use Process Monitor from SysInternals to figure out where that class is looked for.</p>
",1,2,340,2014-02-07 07:29:14,https://stackoverflow.com/questions/21621793/command-line-process
nltk interface to stanford parser,"<p>I am getting problems to access Stanford parser through python NLTK (they developed an interface for NLTK)</p>

<p>import nltk.tag.stanford</p>

<p>Traceback (most recent call last):</p>

<p>File """", line 1, in </p>

<p>ImportError: No module named stanford</p>
","python, nlp, nltk, stanford-nlp","<p>You can use stanford parser from NLTK.
Check this link on how to use it - <a href=""http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford"" rel=""nofollow"">http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford</a></p>

<p>I guess it isn't problem with the stanford module in NLTK, it works well for me.
Check your NLTK version. Older versions doesn't have stanford modules in it. Try the latest version of NLTK.</p>

<p>You can also use this python wrapper for stanford parser which is very efficient because of it varied approach.</p>

<pre><code>    https://bitbucket.org/torotoki/corenlp-python
</code></pre>
",4,2,5596,2014-02-08 21:53:24,https://stackoverflow.com/questions/21652251/nltk-interface-to-stanford-parser
Stanford CoreNLP gives NullPointerException,"<p>I'm trying to get my head around the Stanford CoreNLP API. I wish to get a simple sentence to be tokenized using following code:</p>

<pre><code>    Properties props = new Properties();
    props.put(""annotators"", ""tokenize"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = ""I wish this code would run."";

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    for(CoreMap sentence: sentences) {
        // traversing the words in the current sentence
        // a CoreLabel is a CoreMap with additional token-specific methods
        for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
            // this is the text of the token
            String word = token.get(TextAnnotation.class);
            // this is the POS tag of the token
            String pos = token.get(PartOfSpeechAnnotation.class);
            // this is the NER label of the token
            String ne = token.get(NamedEntityTagAnnotation.class);       
        }

        // this is the parse tree of the current sentence
        Tree tree = sentence.get(TreeAnnotation.class);

        // this is the Stanford dependency graph of the current sentence
        SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map&lt;Integer, CorefChain&gt; graph = document.get(CorefChainAnnotation.class);
</code></pre>

<p>This is picked off from the Stanford NLP website itself, so I hoped it worked out of the box. Sadly it doesn't since it gives me a NullPointerException at:</p>

<pre><code>for(CoreMap sentence: sentences) {...
</code></pre>
","java, stanford-nlp","<p>The code you have picked up from Stanford NLP website performs all the annotations on the text variable. In order to perform specific annotations you have to change the code accordingly.</p>

<p>To perform tokenization, this would be sufficient</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

Annotation document = new Annotation(text);
pipeline.annotate(document);
for (CoreLabel token: document.get(TokensAnnotation.class)) {
    String word = token.get(TextAnnotation.class);
}
</code></pre>

<p>This line of code would return Null if annotators doesn't include Sentence Splitter(""ssplit"")</p>

<pre><code>document.get(SentencesAnnotation.class);
</code></pre>

<p>And so you were encountering NullPointerException.</p>
",2,1,1856,2014-02-12 23:30:17,https://stackoverflow.com/questions/21742186/stanford-corenlp-gives-nullpointerexception
How to shutdown Stanford CoreNLP Redwood logging?,"<p>How can I shut down the Stanford CoreNLP messages (see end of post)?
I first tried setting <code>log4j.category.edu.stanford=OFF</code> in log4j.properties but that didn't help so I found out that apparently it uses a nonstandard logging framework called ""Redwood"". According to <code>http://nlp.stanford.edu/nlp/javadoc/javanlp/</code>there is a documentation but it is password protected. I tried  <code>RedwoodConfiguration.empty().apply();</code> but that doesn't help either.</p>

<p>The logging messages:</p>

<pre><code>Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Loading default properties from tagger edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1,2 sec].
</code></pre>

<p>P.S.: <code>Redwood.hideAllChannels();</code> also doesn't work. The following however suppresses my own logging statement (but not the ones from StanfordCoreNLP):</p>

<pre><code>RedwoodConfiguration.empty().apply();
Redwood.log(""test redwood"");
</code></pre>

<p><strong>Solution</strong> Ok, StevenC was right, it weren't logging statements after all but the default initialization messages are written to stderr which I was not expecting seeing that Stanford has it's own logging framework and then doesn't use it :-)</p>

<p>Anyways, his hints led me to discover this solution:</p>

<pre><code>// shut off the annoying intialization messages
RedwoodConfiguration.empty().captureStderr().apply();
nlp = new StanfordCoreNLP(myproperties);
// enable stderr again
RedwoodConfiguration.current().clear().apply();
</code></pre>
","java, logging, stanford-nlp","<p>You can also find the Redwood Tutorial PDF on GitHub in the Redwood project.</p>

<p>The url is in this page: <a href=""https://github.com/gangeli/redwood/blob/master/doc/tutorial.pdf"" rel=""nofollow"">https://github.com/gangeli/redwood/blob/master/doc/tutorial.pdf</a></p>

<p>(Obviously, I can't tell you if the documents are the same, 'cos I don't know the username / password either :-) )</p>

<hr>

<p>Taking this a bit further, the Tutorial PDF I linked to really just a slide show.  If you want documentation of the properties file, the best I could find was the javadocs for the <code>RedwoodConfiguration.parse</code> method.  And in fact, the rest of that classes javadoc is probably the best doc that you will find ... short of reading the source code.</p>

<p>Warning ... there are indications that the free-standing Redwood code on GitHub may be different to the version in the NLP codebase.</p>
",3,10,1702,2014-02-18 10:40:23,https://stackoverflow.com/questions/21851217/how-to-shutdown-stanford-corenlp-redwood-logging
Different Output for Stanford Parser Online Tool and Stanford Parser Code,"<p>I am working with stanford parser to extract grammetical dependency structures from review sentences. My problem is that for some reason the output generated by my code is not similar to the one generated my stanford online tool. Below is an example.</p>

<p><strong>Review Sentence</strong>: <em>The picture quality of the camera is not good.</em></p>

<p><strong>My Code output (It used <em>EnglishPCFG</em> model and <em>typedDependenciesCollapsed</em> structure)</strong></p>

<pre><code>root(ROOT-0, -LSB--1), 
det(quality-4, The-2), 
nn(quality-4, picture-3),
nsubj(-RSB--11, quality-4), 
det(camera-7, the-6), 
prep_of(quality-4, camera-7), 
cop(-RSB--11, is-8), 
neg(-RSB--11, not-9), 
amod(-RSB--11, good-10), 
ccomp(-LSB--1, -RSB--11)
</code></pre>

<p><strong>Stanford Online tool Output:</strong></p>

<pre><code>det(quality-3, The-1)
nn(quality-3, picture-2)
nsubj(good-9, quality-3)
det(camera-6, the-5)
prep_of(quality-3, camera-6)
cop(good-9, is-7)
neg(good-9, not-8)
root(ROOT-0, good-9)
</code></pre>

<p>I am looking for the reason for this difference. What kind of model and dependency structure does online parser use ? I apologies if I am missing something obvious. Any help would be highly appreciated.</p>

<p><em>I can add code snippet if required</em></p>

<p><strong>Update</strong>: </p>

<p>I changed my code to ignore the <code>LSB</code> and <code>RSB</code> generated by the SP tokenizer but still the grammatical structure generated is different from that of online tool. Here is an example:</p>

<p><em><strong>Review Sentence</em></strong>: <em>The size and picture quality of the camera is perfect.</em></p>

<p><strong>My Code Output</strong>:</p>

<pre><code>det(quality-5, The-1), 
nn(quality-5, size-2), 
conj_and(size-2, picture-4),
nsubj(perfect-10, quality-5), 
det(camera-8, the-7), 
prep_of(quality-5, camera-8), 
cop(perfect-10, is-9), 
root(ROOT-0, perfect-10)
</code></pre>

<p><strong>Stanford Online Tool Output:</strong> </p>

<pre><code>det(quality-5, The-1)
nn(quality-5, size-2)
conj_and(size-2, picture-4)
**nn(quality-5, picture-4)**
nsubj(perfect-10, quality-5)
det(camera-8, the-7)
prep_of(quality-5, camera-8)
cop(perfect-10, is-9)
root(ROOT-0, perfect-10)
</code></pre>

<p>Note the missing <code>nn</code> dependency in my code output. I am trying to get my head around why this is happening. Any help would be appreciated.</p>

<p><strong>Update (Relevant code snippet below)</strong>:</p>

<p><strong>rawWords2</strong> = <em>[-LSB-, The, size, and, picture, quality, of, the, camera, is, perfect, -RSB-]</em></p>

<p><strong>lp</strong> = <em>LexicalizedParser using EnglishPCFG model</em></p>

<pre><code>Tree parse = lp.apply(rawWords2.subList(1,rawWords2.size() - 1));

TreebankLanguagePack tlp = new PennTreebankLanguagePack();

GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();

GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);

tdl = (List&lt;TypedDependency&gt;) gs.typedDependenciesCollapsed();

System.out.println(tdl.toString());
</code></pre>

<p>Output to screen is as mentioned earlier in the post.</p>

<p><em>Another observation.</em></p>

<p>I worked around with Stanford library to show me the dependency relation between <code>quality</code> and <code>picture</code> which as shown in the Stanford online tool is <code>nn</code> but the dependency shown by the library is <code>dep</code> (i.e. can't find more suitable dependency). Now the question is why is Stanford online tool showing nn dependency between <code>quality</code> and <code>picture</code>where as Stanford library showing <code>dep</code> as dependency.</p>
","nlp, stanford-nlp","<p>The major issue for whether you get that extra <code>nn</code> dependency or not is whether there is propagation of dependencies across coordination (<code>size</code> is a <code>nn</code> of <code>quality</code> and it is coordinated with <code>picture</code>, therefore we make it an <code>nn</code> of <code>quality</code> too).  The online output is showing the collapsed output with propagation, whereas you are calling the API method that doesn't include propagation.  You can see either from the command-line using options as shown at the bottom of this post. In the API, to get coordination propagation, you should instead call</p>

<pre><code>gs.typedDependenciesCCprocessed()
</code></pre>

<p>(instead of <code>gs.typedDependenciesCollapsed()</code>).</p>

<p>Other comments:</p>

<ul>
<li>Where are the square brackets (<code>-LSB-</code>) coming from? They shouldn't be introduced by the tokenizer. If they are, it's a bug. Can you say what you do for them to be generated? I suspect they may be coming from your preprocessing? Unexpected things like that in a sentence will tend to cause the parse quality to degrade very badly.</li>
<li>The online parser isn't always up-to-date with the latest released version. I'm not sure if it is up-to-date right now. But I don't think that is the main issue here.</li>
<li>We are doing some work evolving the dependencies representation. This is deliberate, but will create problems if you have code that depends substantively on how the dependencies were defined in an older version. We would be interested to know (perhaps by email to the <code>parser-user</code> list) if your accuracy was coming down for reasons other than your code was written to expect the dependency names as they were in an earlier version.</li>
</ul>

<p>Example of difference using the command line:    </p>

<pre><code>[manning]$ cat &gt; camera.txt 
The size and picture quality of the camera is perfect.
[manning]$ java edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat typedDependencies -outputFormatOptions collapsedDependencies edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz camera.txt
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [2.4 sec].
Parsing file: camera.txt
Parsing [sent. 1 len. 11]: The size and picture quality of the camera is perfect .
det(quality-5, The-1)
nn(quality-5, size-2)
conj_and(size-2, picture-4)
nsubj(perfect-10, quality-5)
det(camera-8, the-7)
prep_of(quality-5, camera-8)
cop(perfect-10, is-9)
root(ROOT-0, perfect-10)

Parsed file: camera.txt [1 sentences].
Parsed 11 words in 1 sentences (6.94 wds/sec; 0.63 sents/sec).
[manning]$ java edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat typedDependencies -outputFormatOptions CCPropagatedDependencies edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz camera.txt
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [2.2 sec].
Parsing file: camera.txt
Parsing [sent. 1 len. 11]: The size and picture quality of the camera is perfect .
det(quality-5, The-1)
nn(quality-5, size-2)
conj_and(size-2, picture-4)
nn(quality-5, picture-4)
nsubj(perfect-10, quality-5)
det(camera-8, the-7)
prep_of(quality-5, camera-8)
cop(perfect-10, is-9)
root(ROOT-0, perfect-10)

Parsed file: camera.txt [1 sentences].
Parsed 11 words in 1 sentences (12.85 wds/sec; 1.17 sents/sec).
</code></pre>
",2,0,1234,2014-02-20 23:02:43,https://stackoverflow.com/questions/21921625/different-output-for-stanford-parser-online-tool-and-stanford-parser-code
cleartk dependency not found when calling StanfordCoreNLPAnnotator from UIMA RUTA,"<p>I am trying to call ClearTK's StanfordCoreNLPAnnotator from within UIMA RUTA, but cannot get it to work. I am using eclipse with a maven-enabled RUTA project in which I also have Java code for auxiliary tasks. I have imported cleartk-stanford-corenlp 0.8 using maven.</p>

<p>I tried using this line in my script:</p>

<pre><code>ENGINE utils.MyStanfordEngine;
</code></pre>

<p>... where utils/MyStanfordEngine.xml is an XML descriptor file created using this java code:</p>

<pre><code>MyStanfordAnnotator.getDescription().toXML(new FileOutputStream(""descriptor/utils/MyStanfordEngine.xml""));
</code></pre>

<p>No errors appear, but upon execution I get:</p>

<pre><code>Exception in thread ""main"" org.apache.uima.resource.ResourceInitializationException: Initialization of annotator class ... failed.  
(Descriptor: file:.../descriptor/mainScriptEngine.xml)
...
Caused by: org.apache.uima.resource.ResourceInitializationException: Annotator class 
""org.cleartk.stanford.StanfordCoreNLPAnnotator"" was not found. 
(Descriptor: file:.../descriptor/utils/MyStanfordEngine.xml)
...
</code></pre>

<p>I think I understand that the RUTA project does not find it in the Maven dependencies, but I need to stick to Maven as my dependency tool because of collaboration purposes.</p>

<p>Can someone help?</p>

<hr>

<p>UPDATE:</p>

<p>When I encountered the problem, I was using RUTA 2.1.0. I have updated to 2.2.0rc1 since then, but the problem persisted.</p>

<p>With Peter's suggestion below (Thanks!), in the Java build path, I referenced a blank Maven-enabled Java project that does nothing but imports cleartk-stanford-corenlp 0.8. I can now run the following RUTA code:</p>

<pre><code>TYPESYSTEM utils.CleartkRutaTypeSystem;
ENGINE utils.MyStanfordEngine;
Document{-&gt; CALL(MyStanfordEngine)};
</code></pre>

<p>... successfully does what looks like all intended annotations for all documents in the input folder, but eventually crashes with this Exception:</p>

<pre><code>[Stanford Tools Logging output ...]
22.02.2014 12:44:22 org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl        callAnalysisComponentProcess(406)
SCHWERWIEGEND: Exception occurred
org.apache.uima.analysis_engine.AnalysisEngineProcessException: Annotator processing failed.    
at org.apache.uima.ruta.engine.RutaEngine.process(RutaEngine.java:477)
at org.apache.uima.analysis_component.JCasAnnotator_ImplBase.process(JCasAnnotator_ImplBase.java:48)
at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.callAnalysisComponentProcess(PrimitiveAnalysisEngine_impl.java:374)
at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.processAndOutputNewCASes(PrimitiveAnalysisEngine_impl.java:298)
at org.apache.uima.analysis_engine.impl.AnalysisEngineImplBase.process(AnalysisEngineImplBase.java:267)
at org.apache.uima.ruta.ide.launching.RutaLauncher.processFile(RutaLauncher.java:168)
at org.apache.uima.ruta.ide.launching.RutaLauncher.main(RutaLauncher.java:129)
Caused by: java.lang.NullPointerException
at org.apache.uima.cas.impl.CASImpl.createFS(CASImpl.java:483)
at org.apache.uima.cas.impl.CASImpl.createAnnotation(CASImpl.java:3837)
at org.apache.uima.ruta.action.CallAction.callEngine(CallAction.java:192)
at org.apache.uima.ruta.action.CallAction.execute(CallAction.java:62)
at org.apache.uima.ruta.rule.AbstractRuleElement.apply(AbstractRuleElement.java:130)
at org.apache.uima.ruta.rule.RuleElementCaretaker.applyRuleElements(RuleElementCaretaker.java:111)
at org.apache.uima.ruta.rule.ComposedRuleElement.applyRuleElements(ComposedRuleElement.java:547)
at org.apache.uima.ruta.rule.AbstractRuleElement.doneMatching(AbstractRuleElement.java:84)
at org.apache.uima.ruta.rule.ComposedRuleElement.fallback(ComposedRuleElement.java:468)
at org.apache.uima.ruta.rule.ComposedRuleElement.fallbackContinue(ComposedRuleElement.java:377)
at org.apache.uima.ruta.rule.RutaRuleElement.startMatch(RutaRuleElement.java:100)
at org.apache.uima.ruta.rule.ComposedRuleElement.startMatch(ComposedRuleElement.java:73)
at org.apache.uima.ruta.rule.RutaRule.apply(RutaRule.java:47)
at org.apache.uima.ruta.rule.RutaRule.apply(RutaRule.java:40)
at org.apache.uima.ruta.rule.RutaRule.apply(RutaRule.java:29)
at org.apache.uima.ruta.RutaScriptBlock.apply(RutaScriptBlock.java:63)
at org.apache.uima.ruta.RutaModule.apply(RutaModule.java:48)
at org.apache.uima.ruta.engine.RutaEngine.process(RutaEngine.java:475)
... 6 more
Exception in thread ""main"" org.apache.uima.analysis_engine.AnalysisEngineProcessException: Annotator processing failed.    
at org.apache.uima.ruta.engine.RutaEngine.process(RutaEngine.java:477)
at org.apache.uima.analysis_component.JCasAnnotator_ImplBase.process(JCasAnnotator_ImplBase.java:48)
at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.callAnalysisComponentProcess(PrimitiveAnalysisEngine_impl.java:374)
at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.processAndOutputNewCASes(PrimitiveAnalysisEngine_impl.java:298)
at org.apache.uima.analysis_engine.impl.AnalysisEngineImplBase.process(AnalysisEngineImplBase.java:267)
at org.apache.uima.ruta.ide.launching.RutaLauncher.processFile(RutaLauncher.java:168)
at org.apache.uima.ruta.ide.launching.RutaLauncher.main(RutaLauncher.java:129)
Caused by: java.lang.NullPointerException
at org.apache.uima.cas.impl.CASImpl.createFS(CASImpl.java:483)
at org.apache.uima.cas.impl.CASImpl.createAnnotation(CASImpl.java:3837)
at org.apache.uima.ruta.action.CallAction.callEngine(CallAction.java:192)
at org.apache.uima.ruta.action.CallAction.execute(CallAction.java:62)
at org.apache.uima.ruta.rule.AbstractRuleElement.apply(AbstractRuleElement.java:130)
at org.apache.uima.ruta.rule.RuleElementCaretaker.applyRuleElements(RuleElementCaretaker.java:111)
at org.apache.uima.ruta.rule.ComposedRuleElement.applyRuleElements(ComposedRuleElement.java:547)
at org.apache.uima.ruta.rule.AbstractRuleElement.doneMatching(AbstractRuleElement.java:84)
at org.apache.uima.ruta.rule.ComposedRuleElement.fallback(ComposedRuleElement.java:468)
at org.apache.uima.ruta.rule.ComposedRuleElement.fallbackContinue(ComposedRuleElement.java:377)
at org.apache.uima.ruta.rule.RutaRuleElement.startMatch(RutaRuleElement.java:100)
at org.apache.uima.ruta.rule.ComposedRuleElement.startMatch(ComposedRuleElement.java:73)
at org.apache.uima.ruta.rule.RutaRule.apply(RutaRule.java:47)
at org.apache.uima.ruta.rule.RutaRule.apply(RutaRule.java:40)
at org.apache.uima.ruta.rule.RutaRule.apply(RutaRule.java:29)
at org.apache.uima.ruta.RutaScriptBlock.apply(RutaScriptBlock.java:63)
at org.apache.uima.ruta.RutaModule.apply(RutaModule.java:48)
at org.apache.uima.ruta.engine.RutaEngine.process(RutaEngine.java:475)
... 6 more
</code></pre>

<p>Sorry for the whole stack trace, but I thought if a RUTA developer is reading this they may want the whole thing.</p>

<p>Is there a way to solve this? What am I doing wrong?</p>
","stanford-nlp, uima, ruta, cleartk","<p>There are several limitations to consider:</p>

<ul>
<li>UIMA Ruta 2.1.0 does not support mixin projects: maven dependencies need to be specified in another project. The Ruta project then has to depend on the additional java project.</li>
<li>UIMA Ruta Workbench 2.1.0 has some problems validating imported type system that import again other type systems by name. Here, rather import by location should be used.</li>
<li>UIMA CAS Editor 2.5.0 has some problems resolving type system imports using the datapath, which causes problems visualizing the created annotations if the type system descriptor needs additional information such as the datapath. Here, the creation of a type system descriptor of a script should include (not only import) all types of imported type systems. This can be configured in the preferences (I have not used that for a while). This problem can again be prevented by using import by location.</li>
<li>UIMA Ruta 2.2.0 supports mixin projects. Here, only the problem with the CAS Editor remains.</li>
</ul>

<p>This described project can be created the following way (with UIMA Ruta 2.2.0):</p>

<ol>
<li>Create a new UIMA Ruta Project</li>
<li>Make it a maven project: popup->Configure->Convert to Maven Project</li>
<li><p>Add a dependency to cleartk-stanford-corenlp in the pom</p>

<pre><code>&lt;dependency&gt;
&lt;groupId&gt;org.cleartk&lt;/groupId&gt;
&lt;artifactId&gt;cleartk-stanford-corenlp&lt;/artifactId&gt;
&lt;version&gt;0.8.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre></li>
<li>Provide the type systems in the descriptor folder or in a dependent project, e.g., copy the <code>org</code> folder of <code>cleartk-type-system-1.2.0</code> to the descriptor folder. Mind that the CAS Editor will have problems resolving the imports, if the descriptors are not adapted.</li>
<li><p>Create a simple script that imports the type system, imports the analysis engine and excutes the analysis engine. Here, the uimaFIT component is directly imported instead of a descriptor. The EXEC action need to be extended with interesting types if later rules should be able to operate on the result of the imported analysis engine.</p>

<pre><code>TYPESYSTEM org.cleartk.TypeSystem;
UIMAFIT org.cleartk.stanford.StanfordCoreNLPAnnotator;
Document{-&gt;EXEC(StanfordCoreNLPAnnotator)};
</code></pre></li>
<li>If there is a text file in the import folder, then running this script should be able to annotate it.</li>
</ol>

<p>This example directly uses the <code>StanfordCoreNLPAnnotator</code> instead of an additional analysis engine, but switching to another implementation or analysis engine should be straightforward.</p>
",1,3,835,2014-02-21 23:13:51,https://stackoverflow.com/questions/21947084/cleartk-dependency-not-found-when-calling-stanfordcorenlpannotator-from-uima-rut
Maven class name collision in Stanford-CoreNLP and Stanford-Parser,"<p>My (maven)project is dependent on both stanford-CoreNLP and stanford-Parser and apparently the (lexicalized)parser of each dependency is producing different outputs, they are not alike.</p>

<p>My question is that how can I determine which package the parser should be loaded from ? the parser class has a same name in both packages:
    <code>edu.stanford.nlp.parser.lexparser.LexicalizedParser</code>
and maven automatically loads the class from stanford-coreNLP package while I want it to be loaded from stanford-Parser.</p>

<p>I'd appreciate if you please help me with your suggestions.</p>
","java, maven, dependency-management, stanford-nlp","<p>I would raise a bug asking them to move the lexical parser into a new maven artifact (or several of them), so you can distinguish them.</p>

<p>If that doesn't happen, you have two options:</p>

<ol>
<li>Use the Maven shade plugin (as suggested by ooxi)</li>
<li>Delete the offending classes</li>
</ol>

<p>Breakdown of the second approach:</p>

<ol>
<li>Use you favorite ZIP tool to open the JAR archive.</li>
<li>Delete the offending packages.</li>
<li>Copy the original POM</li>
<li>Change the version version to something like <code>1.1.MythBuster.1</code> or <code>1.1.no-lexer.1</code></li>
<li>Use <code>mvn file:install</code> to install the modified artifact in your local repo</li>
<li>Test it</li>
<li>Use <code>mvn deploy:deploy-file</code> to install the modified artifact in your company's repo</li>
</ol>

<p>I prefer the second approach since it makes sure the build has a clean classpath, people know that you messed with the original file and it's pretty obvious what is going on.</p>
",5,1,203,2014-02-24 14:09:40,https://stackoverflow.com/questions/21990160/maven-class-name-collision-in-stanford-corenlp-and-stanford-parser
How to get overall sentiment for multiple sentences,"<p>How do you find the aggregated sentiment of multiple sentences/a paragraph/large passage of text.</p>

<p>I have the following code below which I have based on the github Stanford CoreNLP tests and various examples, but <em>everything</em> I've found has completed sentiment analysis only computes the sentiment for individual sentences. But I want the overall tweet's sentiment regardless of how many sentences are in it. </p>

<p>The only other way I can think of doing this is creating a separate thread for a <code>SentimentPipeline.main(String[])</code> and feeding the text to <code>stdin</code> and collecting the overall sentiment in <code>sdout</code>.  I would prefer just being able to use my code to make it simpler/more efficient, but I haven't found anything.</p>

<p>Also, I don't want to do a system call to a jar like most people do as I will be doing millions of tweets per day. The overhead would be too great loading the resources each time.</p>

<pre><code>Annotation document = new Annotation(text);
pipeline.annotate(document);

List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
        String output;
        for (CoreMap sentence : sentences) {
            // traversing the words in the current sentence a CoreLabel is a CoreMap with additional token-specific methods
             output = """";
            for (CoreLabel token : sentence.get(TokensAnnotation.class)) {

                // this is the text of the token
                String word = token.get(TextAnnotation.class);

                // this is the Parts Of Speech tag of the token (noun, verb, adjective etc)
                // String pos = token.get(PartOfSpeechAnnotation.class);

                // this is the NER label of the token
                String ne = token.get(NamedEntityTagAnnotation.class);
                if (!ne.contentEquals(""O"")) {
                    output = output + (ne + "" "" + word + "" "");
                }
            }

            //**************Sentiment Analysis 
            Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
             String sentiment = RNNCoreAnnotations.getPredictedClass(tree);
</code></pre>
","java, jar, stanford-nlp, sentiment-analysis","<p>The sentiment analysis toolkit in stanford corenlp is trained on a sentence-level data set. If you need a document-level sentiment engine, I think training a new model on documents is a better choice. You can also try to process the sentences one by one, and use some tricky methods (such as average, max) as your baselines to test how it works.</p>
",3,3,3098,2014-02-24 21:10:16,https://stackoverflow.com/questions/21999067/how-to-get-overall-sentiment-for-multiple-sentences
Invalid Stream header with Stanford nlp library,"<p>I am working through this <a href=""http://www.galalaly.me/index.php/2011/05/tagging-text-with-stanford-pos-tagger-in-java-applications/"" rel=""nofollow"">Stanford POS tagger tutorial</a>. I am doing it in Scala but I do not think that this matters.  </p>

<p>The line that produces the error is</p>

<pre><code>val tagger=new MaxentTagger(""/Users/user1/Documents/taggers/left3words-wsj-0-18.tagger"")
</code></pre>

<p>and the error is</p>

<pre><code>edu.stanford.nlp.io.RuntimeIOException: java.io.StreamCorruptedException: invalid stream header: 0003CBE8
</code></pre>

<p>The filepath is correct.</p>
","java, scala, stanford-nlp","<p>By default the tagger treats the model file path as a classpath-relative resource path, but it also accepts a fully qualified URL:</p>

<pre><code>val tagger=new MaxentTagger(""file:/Users/user1/Documents/taggers/left3words-wsj-0-18.tagger"")
</code></pre>
",1,2,1244,2014-02-26 17:08:31,https://stackoverflow.com/questions/22048384/invalid-stream-header-with-stanford-nlp-library
Finding Tense of A sentence using stanford nlp,"<p>Q1.I am trying to get tense of a complete sentence,just don't know how to do it using nlp.
Any help appreciated.</p>

<p>Q2 .What all information can be extracted from a sentence using nlp?</p>

<p>Currently I can,
I get : 1.Voice of sentence
        2.subject object verb
        3.POS tags.</p>

<p>Any more info can be extracted please let me know.</p>
","nlp, stanford-nlp, linguistics","<p>The Penn treebank defines <code>VBD</code> and <code>VBN</code> as the past tense and the past participle of a verb, respectively. In many sentences, simply getting the POS tags and checking for the presence of these two tags will suffice. In others, however, there may be verbs in multiple tenses while the sentence as a whole is in the past tense. For these cases, you need to use constituency parsing. Stanford NLP also provides a parser. You can use this to detect the outermost <em>verb phrase</em> (tagged as <code>VP</code>). If a past tense/participle form of a verb is an ancestor of all other verbs in the verb phrase, the tense of your sentence should be marked as past tense.</p>

<p>The example given by Dror yields this:</p>

<pre><code>(ROOT
  (S
    (NP (PRP I))
    (VP (VBD did) (RB n't)
      (VP (VB want)
        (NP (DT the) (NN dog)
          (S
            (VP (TO to)
              (VP (VB eat)
                (NP (PRP$ my) (NN homework))))))))
    (. .)))
</code></pre>

<p>Even though <em>eat</em> is not past tense, the topmost verb in the verb phrase is correctly tagged <code>VBD</code> (i.e. <em>past tense</em>).</p>

<p><strong>edit (some additional information):</strong></p>

<p>Complex sentences have what is called the <em>primary tense</em> and a <em>secondary tense</em>. For sentences like ""By the time I will reach there, he'd have already left"", there is no such thing as 'the complete tense'. You can only distinguish between the primary and the secondary.</p>

<p>If you want information about <em>perfect</em>, <em>continuous</em>, etc., then you will have to derive rules based on the POS tags. E.g. an auxiliary verb in present tense followed by a verb in the past tense will express the present perfect tense (if there are obvious counterexamples, please add to the answer ... I can't think of any right now).</p>
",18,10,12135,2014-03-03 06:12:04,https://stackoverflow.com/questions/22139866/finding-tense-of-a-sentence-using-stanford-nlp
How to get a node level with Stanford dependency parser,"<p>I would like to know if there is any method that allows to give the node level of the parse given by Stanford dependency parser. I haven't found the method that gives the node level. Thanks for the help.</p>
","parsing, nlp, stanford-nlp","<p>The Stanford NLP pipeline contains the class <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/ExhaustiveDependencyParser.html"" rel=""nofollow"">ExhaustiveDependencyParser</a> as well as the interface <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/ViterbiParser.html"" rel=""nofollow"">ViterbiParser</a>, both of which have methods from which you can get a <code>Tree</code> object. This, in turn, has a <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html#depth%28edu.stanford.nlp.trees.Tree%29"" rel=""nofollow""><code>depth(Tree)</code></a> method. Using this method with the root node as argument should suffice.</p>
",1,0,230,2014-03-03 20:26:59,https://stackoverflow.com/questions/22156698/how-to-get-a-node-level-with-stanford-dependency-parser
Is it possible to train Stanford NER system to recognize more named entities types?,"<p>I'm using some NLP libraries now, (stanford and nltk) 
Stanford I saw the demo part but just want to ask if it possible to use it to identify more entity types.</p>

<p>So currently stanford NER system (as the demo shows) can recognize entities as person(name), organization or location. But the organizations recognized are limited to universities or some, big organizations. I'm wondering if I can use its API to write program for more entity types, like if my input is ""Apple"" or  ""Square"" it can recognize it as a company.</p>

<p>Do I have to make my own training dataset?</p>

<p>Further more, if I ever want to extract entities and their relationships between each other, I feel I should use the stanford dependency parser.
I mean, extract first the named entities and other parts tagged as ""noun"" and find relations between them.</p>

<p>Am I correct.</p>

<p>Thanks.</p>
","nlp, stanford-nlp, named-entity-recognition","<p>Yes, you need your own training set. The pre-trained Stanford models only recognise the word ""Stanford"" as a named entity because they have been trained on data that had that word (or very similar words according to the feature set they use, I don't know what that is) marked as a named entity. </p>

<p>Once you have more data, you need to put it in the right format described in <a href=""https://stackoverflow.com/questions/15609324/training-n-gram-ner-with-stanford-nlp"">this question</a> and the Stanford tutorial.</p>
",13,28,18508,2014-03-03 22:07:18,https://stackoverflow.com/questions/22158530/is-it-possible-to-train-stanford-ner-system-to-recognize-more-named-entities-typ
How can I give some POS information before Stanford NLP POS tagger execute?,"<p>If I already know some word's POS information.</p>

<p>eg:I know <strong>st316(my id)</strong> is a Proper nouns <strong>(NR)</strong>.In the sentence""I am st316.""
   How can I make tagger use the Information that st316 is a NR,then decide the POS information of other words(I am).</p>

<p>Just like,</p>

<p>Input:<strong>I am st316/NR .</strong></p>

<p>Output: <strong>I/PN am/VC st316/NR ./PU</strong></p>

<p>Help me.Really thanks!</p>
","nlp, stanford-nlp, pos-tagger","<p>I can think of 2 options:</p>

<ol>
<li>(easy) Let the tagger do its magic and then overwrite its output. If you know <code>st316</code> must be tagged as X and Stanford failed to tag it as such, change the tag of <code>st316</code> to X. The disadvantage of this approach is that the tagger is not able to use that information to better tag the rest of the sentence.</li>
<li>(harder) <a href=""http://nlp.stanford.edu/software/pos-tagger-faq.shtml#train"" rel=""nofollow"">Retrain</a> the PoS tagger, adding the extra information you have to its training data. This way it will actually learn from the information you provide and will be able to make use of it. The drawback is you will need to obtain some training data and (depending on how much data you get) it may take a while to train a new model.</li>
</ol>

<p>If you go with option 2, you need to format your data as follows:</p>

<pre>
An_DT avocet_NN is_VBZ a_DT small_JJ ,_, cute_JJ bird_NN ._.
I_PRP am_VBP st316_NNP ._.
I_PRP am_VBP st316_NNP ._.
I_PRP am_VBP st316_NNP ._.
I_PRP am_VBP st316_NNP ._.
I_PRP am_VBP st316_NNP ._.
</pre>

<p>The first line is taken from the Stanford FAQ. The rest is your extra knowledge. Note the one extra sentence is repeated. This is in order to add pseudo-counts to that observation. Informally, if you only included <code>st316_NNP</code> once in the training data chances are the tagger will think it is noise/error and ignore it. Repeating is is like saying ""Yes, I am sure, I know what I'm doing, learn from that data"". Depending on how much data you have, you will need anywhere between 5 and 50 repetitions to ensure the tagger learns properly.</p>
",0,0,113,2014-03-06 02:05:37,https://stackoverflow.com/questions/22213418/how-can-i-give-some-pos-information-before-stanford-nlp-pos-tagger-execute
Extracting multi word named entities using CoreNLP,"<p>I'm using CoreNLP for named entity extraction and have run into a bit of an issue.
The issue is that whenever a named entity is composed of more than one token, such as ""Han Solo"", the annotator does not return ""Han Solo"" as a single named entity, but as two separate entities, ""Han"" ""Solo"".</p>

<p>Is it possible to get the named entity as one token? I know I can make use of the CRFClassifier with classifyWithInlineXML to this extent, but my solution requires that I use CoreNLP, since I need to know the word number as well.</p>

<p>The following is the code that I have so far:</p>

<pre><code>    Properties props = new Properties();
    props.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    props.setProperty(""ner.model"", ""edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz"");
    pipeline = new StanfordCoreNLP(props);
    Annotation document = new Annotation(text);
    pipeline.annotate(document);
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    for (CoreMap sentence : sentences) {
        for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
                System.out.println(token.get(NamedEntityTagAnnotation.class));
        }
    }
</code></pre>

<blockquote>
  <p>Help me Obi-Wan Kenobi. You're my only hope.</p>
</blockquote>
","stanford-nlp, named-entity-recognition","<pre><code>PrintWriter writer = null;
 try {  
     String inputLine = ""Several possible plans emerged from the talks, held at the Federal Reserve Bank of New York"" + "" and led by Timothy R. Geithner, the president of the New York Fed, and Treasury Secretary Henry M. Paulson Jr."";

     String serializedClassifier = ""english.all.3class.distsim.crf.ser.gz"";
     AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifierNoExceptions(serializedClassifier);

     writer = new PrintWriter(new File(""output.xml""));
     writer.println(""&lt;Sentences&gt;"");
     writer.flush();
     String output =""&lt;Sentence&gt;""+classifier.classifyToString(inputLine, ""xml"", true)+""&lt;/Sentence&gt;""; 
     writer.println(output);
     writer.flush();
     writer.println(""&lt;/Sentences&gt;"");
     writer.flush(); 
 } catch (FileNotFoundException ex) {
     ex.printStackTrace();
 } finally {
     writer.close();
 }
</code></pre>

<p>I was able to come up with this solution. I am writing the output to an XML file ""output.xml"". From the obtained output, you can merge continuous nodes in xml with ""PERSON"" or ""ORGANIZATION"" or ""LOCATION"" attributes in to one entity. And this format produces the word count by default.</p>

<p>Here is a snapshot of xml output.</p>

<pre><code>&lt;wi num=""11"" entity=""ORGANIZATION""&gt;Federal&lt;/wi&gt;
&lt;wi num=""12"" entity=""ORGANIZATION""&gt;Reserve&lt;/wi&gt;
&lt;wi num=""13"" entity=""ORGANIZATION""&gt;Bank&lt;/wi&gt;
&lt;wi num=""14"" entity=""ORGANIZATION""&gt;of&lt;/wi&gt;
&lt;wi num=""15"" entity=""ORGANIZATION""&gt;New&lt;/wi&gt;
&lt;wi num=""16"" entity=""ORGANIZATION""&gt;Yorkand&lt;/wi&gt;
</code></pre>

<p>From the above output you can see that continuously words are recognized as ""ORGANIZATION"". So these words could be combined to one entity.</p>
",3,15,3826,2014-03-09 11:59:08,https://stackoverflow.com/questions/22281889/extracting-multi-word-named-entities-using-corenlp
lexicalized parser vs dependency parser,"<p>I am new to NLP concepts , i am little bit confused with the concepts of lexicalized parser and dependency parser. Are they the same thing or if not in what ways they differ.
kindly help to understand. I am using Stanford parser java package.
Thanks</p>
","nlp, stanford-nlp","<p>Lexicalized parsing aims at building a tree structure from a set of tokens. So a correctly parsed sentence could for example return two sub phrases, so the <strong>syntactical</strong> structure of the sentence is identified. </p>

<p>Dependency parsing however, aims at finding relations between the words. For example, consider the sentence ""I have never ever seen this before"". A dependency parser could find a (directed) auxiliary relation from ""have"" to ""seen"". So a dependency parsing indicates the <strong>(grammatical)</strong> relations between</p>

<p>This is a rather informal explanation but maybe you can make some use of it.</p>
",5,3,1863,2014-03-11 06:57:36,https://stackoverflow.com/questions/22318532/lexicalized-parser-vs-dependency-parser
Stanford CoreNLP model sentiment.ser.gz missing?,"<p>I am new stanford to corenlp and trying to use it. I was able to run sentimental analysis pipeline and corenlp software. While when I am trying to execute evaluate tool it is asking for model sentiment.ser.gz. </p>

<pre><code>java edu.stanford.nlp.sentiment.Evaluate edu/stanford/nlp/models/sentiment/sentiment.ser.gz test.txt
</code></pre>

<p>I could not find the model in the software that I downloaded from stanford site or anywhere on internet.</p>

<p>Can someone please guide if we can create our own model or if I can find anywhere on the internet.</p>

<p>Appreciate your help.</p>
","nlp, stanford-nlp","<p>The file stanford-corenlp-full-2014-01-04.zip contains another file called stanford-corenlp-3.3.1-models.jar. The latter file is a ZIP archive that contains the model file you are looking for.</p>

<p>CoreNLP is able to load the model file from the classpath if you add the stanford-corenlp-3.3.1-models.jar to your Java classpath, so you do not have to do anything.</p>

<p>It also appears the documentation on running the Evaluate tool is slightly outdated. </p>

<p>The correct call goes like this (tested with CoreNLP 3.3.1 and the test data downloaded from the sentiment homepage):</p>

<pre><code>java -cp ""*"" edu.stanford.nlp.sentiment.Evaluate -model edu/stanford/nlp/models/sentiment/sentiment.ser.gz -treebank test.txt
</code></pre>

<p>The '-cp ""*""' adds everything in the current directory to the classpath. Thus, the command above must be executed in the directory to which you extracted CoreNLP, otherwise it will not work.</p>

<p>If you do not add the ""-model"" and -treebank"" to the call, you'll an error message like this</p>

<pre><code>Unknown argument test.txt
</code></pre>

<p>If you do not supply a treebank and a model, you get another error message</p>

<pre><code>Exception in thread ""main"" java.lang.NullPointerException
    at java.io.File.&lt;init&gt;(File.java:277)
</code></pre>
",3,0,1748,2014-03-12 09:27:04,https://stackoverflow.com/questions/22347037/stanford-corenlp-model-sentiment-ser-gz-missing
Sentiments Scores Stanford Core NLP,"<p>How can we get sentiment score of complete sentence using Stanford core NLP?</p>

<p>It classifies complete sentence into positive and negative sentiment, but can we get total sentiment score out Stanford NLP tool?</p>
","java, stanford-nlp, sentiment-analysis","<p>What I did was to average out the score of each sentence based on sentence length. Logic behind it is that longer sentences should carry more weight than shorter ones. </p>

<p>The code looks like this:</p>

<pre><code>String line = ""Great item! HDMI and decent wifi required as with all streaming devices.\n"" +
            ""The flow on the homepage is very good and responsive. Watching a series is a doddle, flow is great, no action required.\n"" +
            ""The remote and controller app both work a treat.\n"" +
            ""I really like this device.\n"" +
            ""I'd like to see an Amazon-written mirroring app available for non-Amazon products but no-one likes talking to each other in this field!"";

    Long textLength = 0L;
    int sumOfValues = 0;

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    int mainSentiment = 0;
    if (line != null &amp;&amp; line.length() &gt; 0) {
        int longest = 0;
        Annotation annotation = pipeline.process(line);
        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
            Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
            int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
            String partText = sentence.toString();
            if (partText.length() &gt; longest) {
                textLength += partText.length();
                sumOfValues = sumOfValues + sentiment * partText.length();

                System.out.println(sentiment + "" "" + partText);
            }
        }
    }

    System.out.println(""Overall: "" + (double)sumOfValues/textLength);
</code></pre>

<p>Download the entire project <a href=""http://www.project2ist.com/2015/04/sentiment-analysis-using-stanford-core.html"" rel=""noreferrer"">here</a></p>
",5,2,3718,2014-03-16 07:14:25,https://stackoverflow.com/questions/22434081/sentiments-scores-stanford-core-nlp
Faster alternatives to Stanford CoreNLP for obtaining parse trees?,"<p>I love <a href=""http://www-nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford CoreNLP</a>, by now it is pretty accurate for my NLP needs. The problem is that analyzing massive amounts of text (let's say <em>millions</em> of sentences) takes <em>days</em>.</p>

<p>Are there alternative Java implementations that sacrifice some accuracy for efficiency while (ideally) providing the same API?</p>
","java, nlp, stanford-nlp","<p>If you are using the PCFG or factored models, you might consider switching to the new <a href=""http://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf"" rel=""nofollow"">RNN models</a> that are available since version 3.2 - they are much faster.</p>

<p>Alternatively, if you only require dependencies, there are other parsers you could try (e.g. <a href=""https://code.google.com/p/mate-tools/"" rel=""nofollow"">mate-tools parser</a> or the <a href=""http://clearnlp.wikispaces.com"" rel=""nofollow"">ClearNLP dependency parser</a>). If you need constituents, you could try the <a href=""https://code.google.com/p/berkeleyparser/"" rel=""nofollow"">Berkeley parser</a>.</p>

<p>Afaik, there are no other parser implementations that have the same API as the Stanford Parser. However, there are collections that offer a rather uniform API to different parsers, e.g. <a href=""https://code.google.com/p/dkpro-core-asl/"" rel=""nofollow"">DKPro Core</a> or <a href=""http://cleartk.github.io/cleartk/"" rel=""nofollow"">ClearTK</a>.</p>

<p><em>Disclosure: I am a developer on the DKPro Core project.</em></p>
",5,3,2529,2014-03-19 15:27:29,https://stackoverflow.com/questions/22510616/faster-alternatives-to-stanford-corenlp-for-obtaining-parse-trees
Stanford nlp java,"<p>How can I extract these labels from a Tree? <a href=""http://s9.postimg.org/uvbjudgi7/Immagine.png"" rel=""nofollow"">http://s9.postimg.org/uvbjudgi7/Immagine.png</a></p>

<p>Should I extract the Syntactic Categories for each token, could you help me?</p>

<p>I tried with:</p>

<pre><code>Tree tree = sentence.get(TreeAnnotation.class);
tree.pennPrint();

for(int i = 0; i &lt; tree.children().length; i++) {
   for(Tree r : tree.children()[i].localTrees()){
       System.out.println(r.nodeString());
    }
}
</code></pre>

<p>but I do not know how to extract the Syntactic Categories of a token!</p>
","java, nlp, stanford-nlp","<p>I think you need a recursive function.</p>

<pre><code>public void output(Tree tree) {
    System.out.println(tree.nodeString());
    for(int i = 0; i &lt; tree.numChildren(); i++) {
        output(tree.children()[i]);
    }
}
</code></pre>

<p>And you can extract the tag of a token by judge whether the child node is leaf, for exam:</p>

<pre><code>if(tree.numChildren() == 1 &amp;&amp; tree.children()[0].isLeaf()) {
    System.out.println(tree.nodeString()+"" ""+tree.children()[0].nodeString());
}
</code></pre>
",0,0,201,2014-03-19 16:59:58,https://stackoverflow.com/questions/22513006/stanford-nlp-java
Is there a native .NET implementation of the Stanford NLP libraries?,"<p>I am aware of the <a href=""http://www.nuget.org/packages/Stanford.NLP.CoreNLP/"" rel=""nofollow"">IKVM-based port of the Stanford NLP libraries to .NET.</a></p>

<p>However, I'd like to know if there is somewhere an implementation that's an actual <em>rewrite</em> of these libraries that utilizes the .NET-native types, features and idioms and does not have a dependency on IKVM or OpenJDK. Is there?</p>
",".net, stanford-nlp","<p><a href=""http://sharpnlp.codeplex.com/"" rel=""nofollow"">SharpNLP</a> is a C# <em>rewrite</em> of <a href=""https://opennlp.apache.org/"" rel=""nofollow"">Apache OpenNLP</a>. However, the last commit is Feb 11, 2007 and, per <a href=""http://sharpnlp.codeplex.com/discussions/440333"" rel=""nofollow"">this comment</a>, the project seems to be inactive and without native alternatives. In fact, per <a href=""http://sharpnlp.codeplex.com/releases/view/1200#ReviewsAnchor"" rel=""nofollow"">this review</a>, the author has passed away.</p>

<p><a href=""https://github.com/sergey-tihon/Stanford.NLP.NET/"" rel=""nofollow"">Stanford.NLP.NET</a> is a <em>port</em> of <a href=""http://nlp.stanford.edu/software/"" rel=""nofollow"">Stanford CoreNLP</a> (actually .jars recompiled to .dlls using IKVM) with assemblies available on <a href=""https://www.nuget.org/packages?q=Stanford.NLP"" rel=""nofollow"">NuGet</a>.</p>

<p>I'm currently working on an NLP solution in C# and think I will go with <a href=""http://www.ikvm.net/"" rel=""nofollow"">IKVM</a> builds of one of these Java libs, simply because they come from respectable groups and seem quite active:</p>

<ul>
<li><a href=""https://opennlp.apache.org/"" rel=""nofollow"">Apache OpenNLP</a></li>
<li><a href=""http://nlp.stanford.edu/software/"" rel=""nofollow"">Stanford CoreNLP</a></li>
</ul>

<p>If you absolutely must have native .NET, it's a perfect opportunity to start an OSS project! I'd contribute!</p>
",3,2,963,2014-03-26 13:32:39,https://stackoverflow.com/questions/22662588/is-there-a-native-net-implementation-of-the-stanford-nlp-libraries
Stanford CoreNLP ExhaustivePCFGParser Initialization Query,"<p>I am a NLP student and relatively new to Stanford CoreNLP. I wanted to try PCFG parser for my sentence because I wanted to get the probability score for the best parse tree the parser fetches. I was able to successfully use LexicalizedParser as </p>

<pre><code>    LexicalizedParser myParser = LexicalizedParser.loadModel();
    myParser.parse(tokenLabels); // where List&lt;CoreLabel&gt; tokenLabels
</code></pre>

<p>But I don't see any loadModel method for ExhaustivePCFGParser so am confused as to how to initialize and start using parse function for it. I can see the constructor defined for it but have no idea what the parameters means ? Could you please provide some guidance for this.</p>
","java, nlp, stanford-nlp","<p>You almost certainly still want to work by using a <code>LexicalizedParser</code>.  From that you can get a <code>ParserQuery</code> object (just as the <code>parse(List&lt;? extends HasWord&gt; lst)</code> method of <code>LexicalizedParser</code> does) and then operate with it:</p>

<pre><code>ParserQuery pq = myParser.parserQuery();
if (pq.parse(tokenLabels)) {
    System.out.println(""Viterbi probability  is "" + pq.getPCFGScore());
}
</code></pre>
",0,0,202,2014-04-02 17:39:36,https://stackoverflow.com/questions/22818956/stanford-corenlp-exhaustivepcfgparser-initialization-query
Solving Homophone Confusion,"<p>This may be a question that is not suitable for stackoverflow, but I had no other better place to ask it. I was wondering if there are any known tools (non-commercial) that can be used to solve the homophone confusions such as these in a sentence?</p>

<ul>
<li>it's vs its </li>
<li>you're vs your</li>
</ul>

<p>I am new to NLP and I haven't used any of the known tools. Tried to search for these in google but nothing useful shows up. Are there any parts in NLTK or CoreNLP that cover this?</p>
","nlp, nltk, text-processing, stanford-nlp","<p>I have no experience with this topic but I found a how to PDF that may be of some use to you.</p>

<p><a href=""http://csl.ira.uka.de/fileadmin/Vorlesungen/WS2010-11/ATSP/presentations/IvayloJanev_HomophonesInASR.pdf"" rel=""nofollow"">How to solve homophone problems</a></p>
",2,1,759,2014-04-03 07:07:24,https://stackoverflow.com/questions/22830268/solving-homophone-confusion
How do I reach the leaves of the tree generatod by a stanford parser in python?,"<p>I am using the stanford parser in python by doing the following:</p>

<pre><code>import os
sentence = ""Did Matt win the men slalom?""
os.popen(""echo '""+sentence+""' &gt; ~/stanfordtemp.txt"")
parser_out = os.popen(""~/stanford-parser-2012-11-12/lexparser.sh  
  ~/stanfordtemp.txt"").readlines()

for tree in parser_out:
    print tree
</code></pre>

<p>However, I dont know how I can access the leaves of the tree being returned by the parser.Can you help me with this? I also have to write a code which will be able to generate sql queries from english sentences. Any tips on this? Any help will be much appreciated. I am also using nltk for doing all the operations.</p>
","python, nltk, stanford-nlp","<p>Here's an example of building a tree and then recursively building a list of the leaves.  The sample text is take from  <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">the online standford parser</a>.</p>

<pre><code># class for tree nodes
class Node:
    def __init__(self,start):
        self.start = start
        self.children = []
        self.text = ''

# make a tree        
def make_tree(s):
    stack = []
    nodes = []
    cur = None
    root = None    

    for i, c in enumerate(s):
        if c == '(':
            cur = Node(i)
            if stack:
                stack[-1].children.append(cur)
            stack.append(cur)

            if root is None:
                root = cur

        elif c == ')' and stack:
            topnode = stack.pop()

            text = s[topnode.start + 1: i]
            topnode.text = text

    return root

# list of leaves
def list_of_leaves(node):
    result = []
    for child in node.children:
        result.extend(list_of_leaves(child))
    if not result:
        return [node]

    return result

s = """"""(ROOT
  (SQ (VBD Did)
    (NP (NNP Matt))
    (VP (VB win)
      (NP (DT the) (NNS men) (NN slalom)))
    (. ?)))""""""

root = make_tree(s)    

for node in list_of_leaves(root):
    print node.text
</code></pre>
",1,0,693,2014-04-05 06:44:38,https://stackoverflow.com/questions/22877567/how-do-i-reach-the-leaves-of-the-tree-generatod-by-a-stanford-parser-in-python
how does google gives summary of the page,"<p><a href=""http://oi60.tinypic.com/vi1wgj.jpg"" rel=""nofollow"">Here</a> is a snapshot of a query <code>what is benzene</code>.</p>

<p>Google generally provide summaries of documents or websites located in response to a query.
A user browses such summaries, and typically selects a link associated with a summary that best matches the search.</p>

<p>I want to know that how does Google gives most accurate summery of any webpage.
I have tried this by selecting the keywords <code>(snippets)</code> of any query and compute the distances between snippets and every single sentences in the webpage using <code>Cosine Similarity</code> and select the sentence having highest score but the result is not satisfying. I want to know is there any better algorithms or any alternative way of generating summaries of web documents?</p>
","java, nlp, google-search, stanford-nlp, opennlp","<p>What you can use to give a summary is using w meta description tag. Google is using it as well unless bot decides that he can generate more accurate description, where more accurate means better fitting to your search query. For example one of summaries from image you've posted is straight from description</p>

<pre><code>&lt;meta name=""description"" content=""Benzene is a colorless, flammable liquid with a sweet odor. Learn what we know about benzene and cancer risk."" /&gt;
</code></pre>

<p>Unless you're writing a search for some other kind of documents than webpages. What many documents browsers do, they just give you a sentence (or couple words before/after) a matched keyword.</p>
",2,3,717,2014-04-05 11:56:59,https://stackoverflow.com/questions/22880577/how-does-google-gives-summary-of-the-page
When manually tagging a corpus for NLP is it important to have untagged text as well?,"<p>I am doing manual tagging to train my own NER 
Do I have to include untagged text in sentences I am preparing for named entity recognition?</p>

<pre><code>&lt;START:person&gt; Olivier Grisel &lt;END&gt; is working on the &lt;START:software&gt; Stanbol &lt;END&gt; project .
</code></pre>

<p>Or can I omit untagged parts like this?</p>

<pre><code>&lt;START:person&gt; Olivier Grisel &lt;END&gt;
&lt;START:software&gt; Stanbol &lt;END&gt;
</code></pre>

<p><strong>PS:</strong>
Thanks for all the great answers. I tried omitting the untagged parts and in that case OpenNLP marked every line as an entity, so it didn't work. As the answers explain, untagged parts are necessary.</p>
","nlp, nltk, stanford-nlp, opennlp, corpus","<p>If you are doing manual tagging to train your own NER model (it's not 100% clear from your question), you should include the same kind of data you expect to tag later, most likely full sentences.  The default model features (see <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.namefind.training.featuregen"" rel=""nofollow noreferrer"">OpenNLP docs</a>) include a window of tokens to the left and right of the token that's currently being considered, so you want your labeled entities to appear in their normal context.  You also want your model to learn which words shouldn't be tagged as entities, so they also need to appear in context in your training data.</p>

<p>See the related question: <a href=""https://stackoverflow.com/questions/11335013/open-nlp-name-finder-training"">Open NLP Name Finder Training</a></p>
",3,1,605,2014-04-06 10:18:40,https://stackoverflow.com/questions/22892710/when-manually-tagging-a-corpus-for-nlp-is-it-important-to-have-untagged-text-as
merge two models in machine learning,"<p>I'm currently training a ner model using stanford ner. Is it possible to merge two models together? or is there a way to expend current model?</p>

<p>Suppose I originally have a large corpus called A, and I get a model MA. (Which works good)
But the problem is, it takes long time to train it and in the future I want to add entries into corpus A, say, B. I have to retrain (A + B) again which takes longer time.</p>

<p>Is there a way to just train B and merge it with A? Or I can.. expend the model MA to make it work for new entities.</p>

<p>Thanks!</p>
","machine-learning, nlp, stanford-nlp","<p>There is no ""general way"". It depends on the models you are using, some of them can be trained in <strong>online mode</strong> (where new instancs, new data do not require retraining) while some cannot. So on this level of generality the answer to your question is <strong>no, you cannot, you need to retrain it each time</strong>. On the other hand, some of the models can be trained this way - look for those which support <strong>online learning</strong>. </p>
",2,1,544,2014-04-07 18:02:00,https://stackoverflow.com/questions/22919790/merge-two-models-in-machine-learning
Error using Stanford POS Tagger in NLTK Python,"<p>I am trying to use Stanford POS Tagger in NLTK but I am not able to run the example code given here <a href=""http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford"" rel=""noreferrer"">http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford</a></p>

<pre><code>import nltk
from nltk.tag.stanford import POSTagger
st = POSTagger(r'english-bidirectional-distim.tagger',r'D:/stanford-postagger/stanford-postagger.jar')
st.tag('What is the airspeed of an unladen swallow?'.split())
</code></pre>

<p>I have already added environment variables as</p>

<pre><code>CLASSPATH = D:/stanford-postagger/stanford-postagger.jar
STANFORD_MODELS =  D:/stanford-postagger/models/
</code></pre>

<p>Here is the error I keep getting</p>

<p>Traceback (most recent call last):</p>

<pre><code>File ""D:\pos_stanford.py"", line 4, in &lt;module&gt;
    st = POSTagger(r'english-bidirectional-distim.tagger',
         r'D:/stanford-postagger/stanford-postagger.jar')  
... LookupError: NLTK was unable to find the english-bidirectional-distim.tagger file! Use software specific configuration paramaters or set the STANFORD_MODELS environment variable.
</code></pre>

<p>Some forums suggest that </p>

<pre><code>File ""C:\Python27\lib\site-packages\nltk\tag\stanford.py"", line 45, in __init__
env_vars=('STANFORD_MODELS'), verbose=verbose)
</code></pre>

<p>should be changed so that there is a comma in </p>

<pre><code>env_vars=('STANFORD_MODELS',), verbose=verbose)
</code></pre>

<p>but it doesn't solve the problem either. 
Please Help me in solving this issue.</p>

<p>Other Information:
I am using
Windows 7 64 bit
Python 2.7 32 bit
NLTK 2.0</p>
","python-2.7, nltk, stanford-nlp, pos-tagger","<p>Note : Just posting it as answer to help in case others face this issue in future </p>

<p>I finally found out what I did wrong.. it turned out to be a blunder.</p>

<p>Tagger file name  is not 'english-bidirectional-distim.tagger'
but 'english-bidirectional-distsim.tagger'.</p>
",11,11,9284,2014-04-08 07:27:05,https://stackoverflow.com/questions/22930328/error-using-stanford-pos-tagger-in-nltk-python
Stanford-NER customization to classify software programming keywords,"<p>I am new in NLP and I used Stanford NER tool to classify some random text to extract special keywords used in software programming. </p>

<p>The problem is, I don't no how to do changes to the classifiers and text annotators in Stanford NER to recognize software programming keywords. For example:</p>

<pre><code>today Java used in different operating systems (Windows, Linux, ..)
</code></pre>

<p>the classification results should such as:</p>

<pre><code>Java ""Programming_Language""
Windows ""Operating_System""
Linux ""Operating_system""
</code></pre>

<p>Would you please help on how to customize the StanfordNER classifiers to satisfied my needs?</p>
","java, nlp, classification, stanford-nlp","<p>I think it is quite well documented in Stanford NER faq section <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#a</a>. </p>

<p>Here are the steps:</p>

<ul>
<li>In your properties file change the map to specify how your training data is annotated (or
structured)</li>
</ul>

<p>map = word=0,myfeature=1,answer=2</p>

<ul>
<li><p>In <code>src\edu\stanford\nlp\sequences\SeqClassifierFlags.java</code></p>

<p>Add a flag stating that you want to use your new feature, let's call it useMyFeature
Below <code>public boolean useLabelSource = false</code>  , Add
public boolean useMyFeature= true;</p>

<p>In  same file in <code>setProperties(Properties props, boolean printProps)</code> method after 
<code>else if (key.equalsIgnoreCase(""useTrainLexicon"")) { ..}</code> tell tool, if this flag is on/off for you</p>

<pre><code>else if (key.equalsIgnoreCase(""useMyFeature"")) {
      useMyFeature= Boolean.parseBoolean(val);
}
</code></pre></li>
<li><p>In <code>src/edu/stanford/nlp/ling/CoreAnnotations.java</code>, add following
section</p>

<pre><code>public static class myfeature implements CoreAnnotation&lt;String&gt; {
  public Class&lt;String&gt; getType() {
    return String.class;
  }
}
</code></pre></li>
<li><p>In <code>src/edu/stanford/nlp/ling/AnnotationLookup.java</code> in 
<code>public enumKeyLookup{..}</code>  in bottom add</p>

<p>MY_TAG(CoreAnnotations.myfeature.class,""myfeature"")</p></li>
<li><p>In <code>src\edu\stanford\nlp\ie\NERFeatureFactory.java</code>, depending on the
""type"" of feature it is, add in </p>

<pre><code>protected Collection&lt;String&gt; featuresC(PaddedList&lt;IN&gt; cInfo, int loc)

if(flags.useRahulPOSTAGS){
    featuresC.add(c.get(CoreAnnotations.myfeature.class)+""-my_tag"");
}
</code></pre></li>
</ul>

<p>Debugging:
In addition to this, there are methods which dump the features on file, use them to see how things are getting done under hood. Also, I think you would have to spend some time with debugger too :P</p>
",6,1,1474,2014-04-09 01:00:47,https://stackoverflow.com/questions/22950995/stanford-ner-customization-to-classify-software-programming-keywords
Stanford NLP WordsToSentencesAnnotator: splitting on \n not working,"<p>I am not able to split sentences on \n or \r using the Stanford NLP WordsToSentencesAnnotator. I am just trying to use the code as described in here: <a href=""http://nlp.stanford.edu/software/sutime.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/sutime.shtml</a>, but I am using custom splitter</p>

<pre><code>public static void main(String[] args) {
Properties props = new Properties();
AnnotationPipeline pipeline = new AnnotationPipeline();
pipeline.addAnnotator(new PTBTokenizerAnnotator(false));
pipeline.addAnnotator(new WordsToSentencesAnnotator(false,""\n""));
pipeline.addAnnotator(new POSTaggerAnnotator(false));
pipeline.addAnnotator(new TimeAnnotator(""sutime"", props));
</code></pre>

<p>...</p>

<p>I am using version 1.3.5 of the corenlp jar. I also tried using \r, \r\n etc. in place of \n, but nothing seems to be working. Any help?</p>
","java, regex, stanford-nlp","<p>Well, that is not the way I would build a pipeline, but have you tried </p>

<p><code>WordsToSentencesAnnotator newlineSplitter(false, ""\n"");</code></p>

<p>So, I would try something more like:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>to interact with the pipeline.  ""SUTime annotations are provided automatically with the StanfordCoreNLP pipeline by including the ner annotator"" according to the Stanford NLP page and therefore you should able to accomplish the same thing. Your sentence splitting annotator is ssplit.  The following options are available for ssplit (once again taken from the Stanford NLP page):</p>

<ul>
<li>ssplit.eolonly: only split sentences on newlines. Works well in conjunction with ""-tokenize.whitespace true"", in which case StanfordCoreNLP will treat the input as one sentence per line, only separating words on whitespace.</li>
<li>ssplit.isOneSentence: each document is to be treated as one sentence, no sentence splitting at all.</li>
<li>ssplit.newlineIsSentenceBreak: Whether to treat newlines as sentence breaks. This property has 3 legal values: ""always"", ""never"", or ""two"". The default is ""two"". ""always"" means that a newline is always a sentence break (but there still may be multiple sentences per line). This is often appropriate for texts with soft line breaks. ""never"" means to ignore newlines for the purpose of sentence splitting. This is appropriate when just the non-whitespace characters should be used to determine sentence breaks. ""two"" means that two or more consecutive newlines will be treated as a sentence break. This option can be appropriate when dealing with text with hard line breaking, and a blank line between paragraphs.</li>
<li>ssplit.boundaryMultiTokenRegex: Value is a multi-token sentence boundary regex.</li>
<li>ssplit.boundaryTokenRegex:</li>
<li>ssplit.boundariesToDiscard:</li>
<li>ssplit.htmlBoundariesToDiscard</li>
<li>ssplit.tokenPatternsToDiscard:</li>
</ul>
",3,2,1732,2014-04-10 19:13:30,https://stackoverflow.com/questions/22997005/stanford-nlp-wordstosentencesannotator-splitting-on-n-not-working
StanfordCoreNLP does not work in my way,"<p>I use below code. However, the outcome is not what I expected. The outcome is <code>[machine, Learning]</code>
But I want to get <code>[machine, learn]</code>. How can I do this? Also, when my input is <code>""biggest bigger""</code>, I wanna get the result like <code>[big, big]</code>, but the outcome is just <code>[biggest bigger]</code></p>

<p>(PS: I just add these four jars in my eclipse:<code>joda-time.jar, stanford-corenlp-3.3.1-models.jar, stanford-corenlp-3.3.1.jar, xom.jar</code> Do I need add some more?)</p>

<pre><code>import java.util.LinkedList;
import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations.LemmaAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class StanfordLemmatizer {

    protected StanfordCoreNLP pipeline;

    public StanfordLemmatizer() {
        // Create StanfordCoreNLP object properties, with POS tagging
        // (required for lemmatization), and lemmatization
        Properties props;
        props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma"");


        this.pipeline = new StanfordCoreNLP(props);
    }

    public List&lt;String&gt; lemmatize(String documentText)
    {
        List&lt;String&gt; lemmas = new LinkedList&lt;String&gt;();
        // Create an empty Annotation just with the given text
        Annotation document = new Annotation(documentText);
        // run all Annotators on this text
        this.pipeline.annotate(document);
        // Iterate over all of the sentences found
        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
        for(CoreMap sentence: sentences) {
            // Iterate over all tokens in a sentence
            for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
                // Retrieve and add the lemma for each word into the
                // list of lemmas
                lemmas.add(token.get(LemmaAnnotation.class));
            }
        }
        return lemmas;
    }


    // Test
    public static void main(String[] args) {
        System.out.println(""Starting Stanford Lemmatizer"");
        String text = ""Machine Learning\n"";
        StanfordLemmatizer slem = new StanfordLemmatizer();
        System.out.println(slem.lemmatize(text));
    }

}
</code></pre>
","java, nlp, stanford-nlp, stemming, lemmatization","<p>Lemmatization should ideally return a <em>canonical form</em> (known as 'lemma' or 'headword') of a group of words. This canonical form, however, is not always what we intuitively expect. For example, you expect ""learning"" to be yield the lemma ""learn"". But the noun ""learning"" has the lemma ""learning"", while only the present continuous verb ""learning"" has the lemma ""learn"". In case of ambiguity, the lemmatizer should depend on information from the part-of-speech tag.</p>

<p>Well, that explains <em>machine learning</em>, but what about <em>big, bigger and biggest</em>?</p>

<p>Lemmatization depends on morphological analysis. The Stanford Morphology-class computes the base form of English words, by removing just inflections (not derivational morphology). That is, it only does noun plurals, pronoun case, and verb endings, and not things like comparative adjectives or derived nominals. It is based on a finite-state transducer implemented by John Carroll et al., written in flex. I couldn't find the original version, but a Java version seems to be <a href=""https://github.com/knowitall/morpha"" rel=""nofollow"">available here</a>.</p>

<p>That is why <em>biggest</em> will not yield <em>big</em>.</p>

<p>The WordNet lexical database resolves to the correct lemma, though. I have usually used WordNet for lemmatization tasks, and have found no major issues so far. Two other well known tools that handle your example correctly are</p>

<ol>
<li><a href=""http://cst.dk/online/lemmatiser/uk/"" rel=""nofollow"">CST Lemmatizer</a></li>
<li><a href=""http://morphadorner.northwestern.edu/morphadorner/lemmatizer/example/"" rel=""nofollow"">MorphAdorner</a></li>
</ol>
",4,0,1051,2014-04-15 14:39:54,https://stackoverflow.com/questions/23086961/stanfordcorenlp-does-not-work-in-my-way
Converting NLTK phrase structure trees to BRAT .ann standoff,"<p>I'm trying to annotate a corpus of plain text. I'm working with systemic functional grammar, which is fairly standard in terms of part-of-speech annotation, but differs in terms of phrases/chunks.</p>

<p>Accordingly, I've POS tagged my data with NLTK defaults, and made a regex chunker with <code>nltk.RegexpParser</code>. Basically, the output now is an NLTK-style phrase structure tree:</p>

<blockquote>
  <p>Tree('S', [Tree('Clause', [Tree('Process-dependencies',
  [Tree('Participant', [('This', 'DT')]), Tree('Verbal-group', [('is',
  'VBZ')]), Tree('Participant', [('a', 'DT'), ('representation',
  'NN')]), Tree('Circumstance', [('of', 'IN'), ('the', 'DT'),
  ('grammar', 'NN')])])]), ('.', '.')])</p>
</blockquote>

<p>There is some stuff I want to manually annotate on top of this, however: the systemic grammar breaks down participants and verbal groups into sub-types that probably can't be automatically annotated. So, I was hoping to convert the parse tree format into something an annotation tool (preferably BRAT) could handle, and then go through the text and specify the sub-types manually, as in (one possible solution):</p>

<p><img src=""https://i.sstatic.net/09EzH.png"" alt=""BRAT annotation""></p>

<p>Perhaps the solution would be sort of tricking BRAT into treating the phrase structure like dependencies? I could modify the chunking regex if need be. Are there any converters out there? (Brat provides ways of converting from CONLL2000 and Stanford Core NLP, so if I could get the phrase structure into either of those forms it would be acceptable too.)</p>

<p>Thanks!</p>
","python, nlp, nltk, stanford-nlp, corpus","<p>Representing a non-binary tree as arcs will be difficult, but it is possible to nest ""entity"" annotations and use this for a constituency parse structure. Note that I'm not creating nodes for the terminals (part of speech tags) of the tree, partially because Brat is not currently good at displaying unary rules that often apply to terminals. The description of the target format is found <a href=""http://brat.nlplab.org/standoff.html"" rel=""nofollow"">here</a>.</p>

<p>Firstly, we need a function to produce standoff annotations. While Brat seeks standoff in terms of characters, in the following we just use token offsets, and will convert to characters below.</p>

<p>(Note this uses NLTK 3.0b and Python 3)</p>

<pre><code>def _standoff(path, leaves, slices, offset, tree):
    width = 0
    for i, child in enumerate(tree):
        if isinstance(child, tuple):
            tok, tag = child
            leaves.append(tok)
            width += 1
        else:
            path.append(i)
            width += _standoff(path, leaves, slices, offset + width, child)
            path.pop()
    slices.append((tuple(path), tree.label(), offset, offset + width))
    return width


def standoff(tree):
    leaves = []
    slices = []
    _standoff([], leaves, slices, 0, tree)
    return leaves, slices
</code></pre>

<p>Applying this to your example:</p>

<pre><code>&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; tree = Tree('S', [Tree('Clause', [Tree('Process-dependencies', [Tree('Participant', [('This', 'DT')]), Tree('Verbal-group', [('is', 'VBZ')]), Tree('Participant', [('a', 'DT'), ('representation', 'NN')]), Tree('Circumstance', [('of', 'IN'), ('the', 'DT'), ('grammar', 'NN')])])]), ('.', '.')])
&gt;&gt;&gt; standoff(tree)
(['This', 'is', 'a', 'representation', 'of', 'the', 'grammar', '.'],
 [((0, 0, 0), 'Participant', 0, 1),
  ((0, 0, 1), 'Verbal-group', 1, 2),
  ((0, 0, 2), 'Participant', 2, 4),
  ((0, 0, 3), 'Circumstance', 4, 7),
  ((0, 0), 'Process-dependencies', 0, 7),
  ((0,), 'Clause', 0, 7),
  ((), 'S', 0, 8)])
</code></pre>

<p>This returns the leaf tokens, then a list of tuples corresponding subtrees with elements: (index into root, label, start leaf, stop leaf).</p>

<p>To convert this into character standoff:</p>

<pre><code>def char_standoff(tree):
    leaves, tok_standoff = standoff(tree)
    text = ' '.join(leaves)
    # Map leaf index to its start and end character
    starts = []
    offset = 0
    for leaf in leaves:
        starts.append(offset)
        offset += len(leaf) + 1
    starts.append(offset)
    return text, [(path, label, starts[start_tok], starts[end_tok] - 1)
                  for path, label, start_tok, end_tok in tok_standoff]
</code></pre>

<p>Then:</p>

<pre><code>&gt;&gt;&gt; char_standoff(tree)
('This is a representation of the grammar .',
 [((0, 0, 0), 'Participant', 0, 4),
  ((0, 0, 1), 'Verbal-group', 5, 7),
  ((0, 0, 2), 'Participant', 8, 24),
  ((0, 0, 3), 'Circumstance', 25, 39),
  ((0, 0), 'Process-dependencies', 0, 39),
  ((0,), 'Clause', 0, 39),
  ((), 'S', 0, 41)])
</code></pre>

<p>Finally, we can write a function that converts this to Brat's format:</p>

<pre><code>def write_brat(tree, filename_prefix):
    text, standoff = char_standoff(tree)
    with open(filename_prefix + '.txt', 'w') as f:
        print(text, file=f)
    with open(filename_prefix + '.ann', 'w') as f:
        for i, (path, label, start, stop) in enumerate(standoff):
            print('T{}'.format(i), '{} {} {}'.format(label, start, stop), text[start:stop], sep='\t', file=f)
</code></pre>

<p>This writes the following to <em>/path/to/something.txt</em>:</p>

<pre><code>This is a representation of the grammar .
</code></pre>

<p>and this to <em>/path/to/something.ann</em>:</p>

<pre><code>T0  Participant 0 4 This
T1  Verbal-group 5 7    is
T2  Participant 8 24    a representation
T3  Circumstance 25 39  of the grammar
T4  Process-dependencies 0 39   This is a representation of the grammar
T5  Clause 0 39 This is a representation of the grammar
T6  S 0 41  This is a representation of the grammar .
</code></pre>
",3,3,1819,2014-04-18 01:32:00,https://stackoverflow.com/questions/23146072/converting-nltk-phrase-structure-trees-to-brat-ann-standoff
TSurgeon - relabel node using old value,"<p>I am trying to implement TSurgeon on a standford parse tree (from the core-nlp api). What my intended action will do is add a prefix to the node that I find (e.g. the node found is NN and I would like to rename it to Skip-NN)</p>

<p>What I am trying is this: </p>

<pre><code>TsurgeonPattern surgery = Tsurgeon.parseOperation(""relabel target Skip-target"");
for (TregexPattern pat : patterns) {
    Tsurgeon.processPattern(pat, surgery, tree).pennPrint();
}
</code></pre>

<p>An example of one the TregexPattern's used would be <code>NP &lt;&lt; NP=target</code></p>

<p>Although as you might of guessed the result is similar to:</p>

<p><code>NP -&gt; ""Skip-target""</code> instead of <code>NP -&gt; ""Skip-NP""</code></p>

<p>I am quite new to using TSurgeon and am unsure as to where to look for information regarding an issue like this. </p>

<p>EDIT: Essentially what I'm asking; is there a way to use the <em>current label</em> of a node when relabeling it.</p>
","java, stanford-nlp","<p>You should be able to use regexes for this.  Something like</p>

<pre><code>relabel target /^(.*)$/Skip-$1/
</code></pre>

<p>Though you will have to be careful with your pattern, it will have to ignore nodes beginning with <code>Skip-</code>.</p>
",1,0,45,2014-04-22 22:17:06,https://stackoverflow.com/questions/23231248/tsurgeon-relabel-node-using-old-value
How to use StanfordCoreNLP jar to conduct semantic analysis in a Java Map/Reduce Job?,"<p>I am currently working on Capital Markets Modelling. I am storing the data from various data sources  i.e. Financial news using Impala. </p>

<p>I have stored this data using Impala in a table of the form <code>articleID,ArticleDATA</code>. </p>

<p>My next step is to conduct semantic analysis row-by-row, on all records. </p>

<p>I have been reading through these resources </p>

<p><a href=""https://stackoverflow.com/questions/20359346/executing-and-testing-stanford-core-nlp-example"">Executing and testing stanford core nlp example</a></p>

<p><a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>

<p><a href=""https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java"" rel=""nofollow noreferrer"">https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java</a></p>

<p><a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/sentiment/code.html</a></p>

<p>I am making a Map/Reduce job in Eclipse using Java and couldn't figure out how do I use the JAR to help me with the analysis.</p>

<p>P.S: I ran the command
 <code>java -cp ""*"" -mx1g edu.stanford.nlp.sentiment.SentimentPipeline -file input.txt</code>
on sample data and it worked but via command line. I want to know how to do it in eclipse, map/reduce.</p>

<p>The current output sentiment is in terms of Positive, Neutral, Negative but I need it as 1,0,-1. I guess that's easy to figure out, so not a priority here.!</p>

<p>I Am confused as how to go about it. </p>

<p>Thanks</p>
","java, eclipse, mapreduce, stanford-nlp, sentiment-analysis","<p>Well I figured it out.. So closing the question. Had to work with the Sentiment files in the JAR. Trial and Error. Then made a MApReduce job.</p>
",0,0,1326,2014-04-23 04:41:24,https://stackoverflow.com/questions/23234886/how-to-use-stanfordcorenlp-jar-to-conduct-semantic-analysis-in-a-java-map-reduce
How to improve speed with Stanford NLP Tagger and NLTK,"<p>Is there any way to use the Standford Tagger in a more performant fashion?</p>

<p>Each call to NLTK's wrapper starts a new java instance per analyzed string which is very very slow especially when a larger foreign language model is used...</p>

<p><a href=""http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford"">http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford</a></p>
","python, nltk, stanford-nlp","<p>Found the solution. It is possible to run the POS Tagger in servlet mode and then connect to it via HTTP. Perfect.</p>

<p><a href=""http://nlp.stanford.edu/software/pos-tagger-faq.shtml#d"">http://nlp.stanford.edu/software/pos-tagger-faq.shtml#d</a></p>

<p>example</p>

<p><strong>start server in background</strong></p>

<pre><code>nohup java -mx1000m -cp /var/stanford-postagger-full-2014-01-04/stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTaggerServer -model /var/stanford-postagger-full-2014-01-04/models/german-dewac.tagger -port 2020 &gt;&amp; /dev/null &amp;
</code></pre>

<p><strong>adjust firewall to limit access to port 2020 from localhost only</strong></p>

<pre><code>iptables -A INPUT -p tcp -s localhost --dport 2020 -j ACCEPT
iptables -A INPUT -p tcp --dport 2020 -j DROP
</code></pre>

<p><strong>test it with wget</strong></p>

<pre><code>wget http://localhost:2020/?die welt ist schön
</code></pre>

<p><strong>shutdown server</strong></p>

<pre><code>pkill -f stanford
</code></pre>

<p><strong>restore iptable settings</strong></p>

<pre><code>iptables -D INPUT -p tcp -s localhost --dport 2020 -j ACCEPT
iptables -D INPUT -p tcp --dport 2020 -j DROP
</code></pre>
",14,12,5810,2014-04-27 11:15:19,https://stackoverflow.com/questions/23322674/how-to-improve-speed-with-stanford-nlp-tagger-and-nltk
kernelized methods in natural language processing,"<p>I am new to NLP. I want to implement a matching approach for short sentences( e.g. questions in cQA). I want to use tree kernel function as a syntactic feature. I am wondering to know is there any implementation available in NLP tools, or elsewhere? </p>

<p>Specifically, I like a method like this paper Collins and Duffy, Convolution kernels for natural language processing.</p>

<p>Any suggestion is useful and appreciated.</p>
","nlp, stanford-nlp","<p>It is common to compare two tree structures to find similarity between sentences in natural language processing. A good tutorial has been written by Alessandro Moschitti at <a href=""http://disi.unitn.it/moschitti/Tree-Kernel.htm"" rel=""nofollow"">SVM-LIGHT-TK </a>. Kernel methods have been widely used in machine learning tasks ( e.g. SVM ), and thus there are different approach of kernelization adapted for various tasks.</p>
",1,1,139,2014-05-03 23:53:32,https://stackoverflow.com/questions/23451047/kernelized-methods-in-natural-language-processing
Adding child node to Stanford Tree,"<p>I am attempting to add a new node to a stanford parse <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html"" rel=""nofollow"">tree</a> although what I have currently tried is not working:</p>

<p>So I use TregexMatcher and get back a tree like:</p>

<pre><code>(VP 
    (VB run) 
    (PP 
        (To to) 
        (NP 
            (DT the) 
            (NN shop) 
        ) 
    )
)
</code></pre>

<p>I am trying to insert (VBD did) to be the first child of (VP) giving the tree:</p>

<pre><code>(VP 
    (VBD did)
    (VB run) 
    (PP 
        (To to) 
        (NP 
            (DT the) 
            (NN shop) 
        ) 
    )
)
</code></pre>

<p>Here is the code I have tried:</p>

<pre><code>private Tree addNode(Tree tree, Label posTag, Label value, int index) {

    SimpleTree posNode = new SimpleTree(posTag);
    posNode.setValue(posTag.value());

    SimpleTree valueNode = new SimpleTree(value);
    valueNode.setValue(value.value());

    posNode.addChild(valueNode);

    tree.insertDtr(posNode, index);
    return tree;
}
</code></pre>

<p>I have been using the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/overview-summary.html"" rel=""nofollow"">online documentation</a> although I am not sure exactly how to approach this issue.</p>

<p>Where am I going wrong? Is there an example online someone could link me to?</p>

<p>EDIT: The altered tree after the above code is:</p>

<pre><code>(VP
    ( ) 
    (VB run)
    (PP 
        (TO to)
        (NP 
            (DT the) 
            (NN shop)
        )
    )
)
</code></pre>
","java, tree, stanford-nlp","<p>The mistake is using <code>SimpleTree</code>.  A <code>SimpleTree</code> is just a tree structure with no node labels, and so isn't useful for much. I've added a note in the documentation saying that. With <code>LabeledScoredTreeNode</code> it works fine:</p>

<pre><code>private static Tree addNodeFixed(Tree tree, Label posTag, Label value, int index) {
  Tree posNode = new LabeledScoredTreeNode(posTag);
  posNode.setValue(posTag.value());
  Tree valueNode = new LabeledScoredTreeNode(value);
  valueNode.setValue(value.value());
  posNode.addChild(valueNode);

  tree.insertDtr(posNode, index);
  return tree;
}
</code></pre>

<p>However, if you're using Tregex to match trees anyway, you might find it easier to do things with Tsurgeon.  Watch for the infinite loops on insertions, though!</p>

<pre><code>private static Tree addNodeTsurgeon(Tree tree, String tag, String word) {
  TregexPattern pat = TregexPattern.compile(""__=root !&gt; __ !&lt; "" + tag);
  TsurgeonPattern surgery = Tsurgeon.parseOperation(
          ""insert ("" + tag + "" "" + word +"") &gt;1 root"");
  return Tsurgeon.processPattern(pat, surgery, tree);
}
</code></pre>
",2,1,180,2014-05-08 23:10:35,https://stackoverflow.com/questions/23554113/adding-child-node-to-stanford-tree
Stanford POS tagger error when trying to read the tagger file from URL,"<p>I am using POS tagger for a project and it works successfully when it reads the tagger file from my computer (project's folder). 
But I need to upload the tagger file first and read the tagger file from a URL.
To do so, I have uploaded the POS tagger file and I am trying to read the tagger file by giving the URL to the constructor of the MaxentTagger method: (my code is in C# and I have overridden the MaxentTagger class so it's constructor looks like this:</p>

<blockquote>
  <blockquote>
    <p>public Tagger () 
    {</p>
    
    <p>java.io.ByteArrayInputStream inputStream = new java.io.ByteArrayInputStream(System.IO.File.ReadAllBytes(@""C:\models\english-left3words-distsim.tagger""));</p>
    
    <p>base.readModelAndInit(null, new java.io.DataInputStream(inputStream), false);
    }</p>
  </blockquote>
</blockquote>

<p>However I get this error when I run my code:</p>

<p>""An unhandled exception of type 'java.lang.RuntimeException' occurred in stanford-postagger.dll</p>

<p>Additional information: java.io.FileNotFoundException: Could not find a part of the path 'C:\u\nlp\data\pos_tags_are_useless\egw4-reut.512.clusters'.""</p>

<p>Does anybody know why this happens and how I can resolve this? I appreciate any sort of help very much!</p>
","c#, java, filenotfoundexception, stanford-nlp, pos-tagger","<p>This error comes from the program trying to load a file which gives the distributional similarity mapping from words to clusters. It's trying to get it from the location that is specified in the training properties file (and you naturally don't have a file at that location). This happened because you don't have a properly initialized TaggerConfig object at the time readModelAndInit() is called. The way it gets initialized is unintuitive (was badly architected), but you're only encountering this because you're trying to use a non-public API.</p>

<p>Why can't you just use the public API as follows?</p>

<pre><code>MaxentTagger base = new MaxentTagger(""http://my.url.com/models/english-left3words-distsim.tagger"");
</code></pre>
",1,0,796,2014-05-10 01:37:15,https://stackoverflow.com/questions/23576279/stanford-pos-tagger-error-when-trying-to-read-the-tagger-file-from-url
Lexicon dictionary for synonym words,"<p>There are few dictionaries available for natural language processing. Like positive, negative words dictionaries etc. </p>

<p>Is there any dictionary available which contains list of synonym for all dictionary words? </p>

<p>Like for <code>nice</code> </p>

<pre><code>synonyms: enjoyable, pleasant, pleasurable, agreeable, delightful, satisfying, gratifying, acceptable, to one's liking, entertaining, amusing, diverting, marvellous, good; 
</code></pre>
","dictionary, nlp, stanford-nlp, data-processing, text-classification","<p>Although WordNet is a good resource to start for finding synonym, one must note its limitations, here's an example with python API in <code>NLTK</code> library:</p>

<p>Firstly, words have multiple meanings (i.e. senses):</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; wn.synsets('nice')
[Synset('nice.n.01'), Synset('nice.a.01'), Synset('decent.s.01'), Synset('nice.s.03'), Synset('dainty.s.04'), Synset('courteous.s.01')]
</code></pre>

<p>And to access the correct sense of a word, you will need to know the correct sense of a word given a context. </p>

<pre><code>&gt;&gt;&gt; wn.synset('nice.a.01').definition()
u'pleasant or pleasing or agreeable in nature or appearance'
</code></pre>

<p>You can try Word Sense Disambiguation software but they are not perfect (see <a href=""https://stackoverflow.com/questions/4613773/anyone-know-of-some-good-word-sense-disambiguation-software/8808962#8808962"">Anyone know of some good Word Sense Disambiguation software?</a>). Even if you know the sense of the word, the entries of wordnet are limited. You cannot expect much:</p>

<pre><code>&gt;&gt;&gt; wn.synset('nice.a.01').lemma_names()
[u'nice']
&gt;&gt;&gt; wn.synset('nice.a.01').similar_tos()
[Synset('good.s.06'), Synset('pleasant.s.02')]
&gt;&gt;&gt; [i.lemma_names() for i in wn.synset('nice.a.01').similar_tos()]
[[u'good'], [u'pleasant']]
</code></pre>
",10,5,4519,2014-05-17 10:27:34,https://stackoverflow.com/questions/23710214/lexicon-dictionary-for-synonym-words
Parsing messy texts with Stanford Parser,"<p>I am running Stanford Parser on a large chunk of texts. The parser terminates when it hits a sentence it cannot parse, and gives the following runtime error. Is there a way to make Stanford Parser ignore the error, and move on to parsing the next sentence?</p>

<p>One way is to break down the text into a myriad of one-sentence documents, and parse each document and record the output. However, this involves loading the Stanford Parser many many times (each time a document is parsed, the Stanford Parser has to be reloaded). Loading the parser takes a lot of time, but parsing takes much shorter time. It would be great to find a way to avoid having to reload the parser on every sentence.</p>

<p>Another solution might be to reload the parser once it hits an error, and picking up the texts where it stopped and continue parsing from there. Does anyone know of a good way to implements this solution?</p>

<p>Last but not least, does there exist any Java wrapper that ignores errors and keeps a Java program running until the program terminates naturally?</p>

<p>Thanks!</p>

<pre><code>Exception in thread ""main"" java.lang.RuntimeException: CANNOT EVEN CREATE ARRAYS OF ORIGINAL SIZE!!
at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.considerCreatingArrays(ExhaustivePCFGParser.java:2190)
at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.parse(ExhaustivePCFGParser.java:347)
at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.parseInternal(LexicalizedParserQuery.java:258)
at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.parse(LexicalizedParserQuery.java:536)
at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.parseAndReport(LexicalizedParserQuery.java:585)
at edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:213)
at edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:73)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1535)
</code></pre>
","java, error-handling, machine-learning, nlp, stanford-nlp","<p>This error is basically an out of memory error. It likely occurs because there are long stretches of text with no sentence terminating punctuation (periods, question marks), and so it has been and is trying to parse a huge list of words that it regards as a single sentence.</p>

<p>The parser in general tries to continue after a parse failure, but can't in this case because it both failed to create data structures for parsing a longer sentence and then failed to recreate the data structures it was using previously. So, you need to do something.</p>

<p>Choices are:</p>

<ul>
<li>Indicate sentence/short document boundaries yourself. This does <strong>not</strong> require loading the parser many times (and you should avoid that). From the command-line you can put each sentence in a file and give the parser many documents to parse and ask it to save them in different files (See the <code>-writeOutputFiles</code> option).</li>
<li>Alternatively (and perhaps better) you can do this keeping everything in one file by either making the sentences one per line, or using simple XML/SGML style tags surrounding each sentence and then to use the <code>-sentences newline</code> or <code>-parseInside ELEMENT</code>.</li>
<li>Or you can just avoid this problem by specifying a maximum sentence length. Longer things that are not sentence divided will be skipped. (This is great for runtime too!)  You can do this with <code>-maxLength 80</code>.</li>
<li>If you are writing your own program, you could catch this Exception and try to resume. But it will only be successful if sufficient memory is available, unless you take the steps in the earlier bullet points.</li>
</ul>
",4,0,953,2014-05-22 17:32:00,https://stackoverflow.com/questions/23813611/parsing-messy-texts-with-stanford-parser
StanfordCoreNLP compiler errors using javac,"<p>I have just started looking into the Stanford NLP libraries. I downloaded and uncompressed the core file that includes everything, then tried to run the demo file (StanfordCoreNlpDemo.java).</p>

<pre><code>import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;
</code></pre>

<p>All of the import lines trigger an ""error: package edu.stanford.nlp. does not exist"" message. </p>

<p>Can anyone tell me what I need to do to make the demo work?</p>
","java, compiler-errors, stanford-nlp","<p>Include the class path while compiling the core java file by<br>
<strong>javac -cp stanford-corenlp-3.5.0.jar filename.java</strong> </p>
",2,2,2604,2014-05-30 18:28:59,https://stackoverflow.com/questions/23961174/stanfordcorenlp-compiler-errors-using-javac
Best way to output Stanford NLP results,"<p>Hi folks:  I'm using the Stanford CoreNLP software to process hundreds of letters by different people (each about 10KB).  After I get the output, I need to further process it and add information at the level of tokens, sentences, and letters.  I'm quite new to NLP and was wondering what the typical or best way would be to output the pipeline results from Stanford CoreNLP to permit further processing?</p>

<p>I'm guessing the typical approach would be to output to XML.  If I do, I estimate that will take about a GB of disk space, and I wonder, then, how quick and easy it would be to load that much XML back into Java for further processing and adding of information?</p>

<p>An alternative might be to have CoreNLP serialize the annotation objects it produces and load those back for processing.  An advantage:  not having to figure out how to convert a sentence parse string back into a tree for further processing.  A disadvantage:  annotation objects contain a lot of different types of objects I'm still quite rough on manipulating and the documentation on these in Stanford CoreNLP seems slim to me.</p>
","java, nlp, output, processing, stanford-nlp","<p>This is really matter of what you want to do afterwards. Doing serialization is probably the most straightforward and fast approach, the con is that you need to understand the CoreNLP data structure. </p>

<p>What if you want to read it in another language or read into your own data structure, save as XML. </p>

<p>I would go the first way. </p>
",0,0,610,2014-06-04 03:26:49,https://stackoverflow.com/questions/24028492/best-way-to-output-stanford-nlp-results
What does java -cp &quot;*&quot; mean?,"<p>I was working on the Stanford sentiment classifier on windows. I wanted to retrain my own model, and here's how it was specified on the website:</p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz
</code></pre>

<p>But this gave me the error:</p>

<blockquote>
  <p>could not find or load main class</p>
</blockquote>

<p>But on changing it to <code>java -cp ""*""</code> it worked.</p>
","java, stanford-nlp","<blockquote>
  <p>Class path entries can contain the basename wildcard character <em>, 
  which is considered equivalent to specifying a list of all the files 
  in the directory with the extension .jar or .JAR. For example, the 
  class path entry foo/</em> specifies all JAR files in the directory  named
  foo. A classpath entry consisting simply of * expands to a  list of
  all the jar files in the current directory.</p>
</blockquote>

<p>From <a href=""http://docs.oracle.com/javase/6/docs/technotes/tools/windows/classpath.html"" rel=""nofollow"">Oracle Docs</a></p>
",1,0,2183,2014-06-05 08:40:10,https://stackoverflow.com/questions/24055468/what-does-java-cp-mean
cant remove element from java.util.list,"<p>I'm using java.util in my c# program. I want to remove a element from List, however don't know why is is not working.</p>

<pre><code>foreach (List sentence in new DocumentPreprocessor(clfile))
{
    //int i = sentence.size();
    string rm = ""ASPECT"";
    Object objstr = rm;

    sentence.remove(objstr);
    string temp2 = String.Join("","", sentence.toArray());
    ...
}
</code></pre>

<p>After execution of above coe the word ""ASPECT"" is still there in List sentence.</p>

<p>PS: Document Preprocessor belongs to, edu.stanford.nlp.process</p>
","java, c#, stanford-nlp","<p>According to the Javadoc of the Java version, there are no <code>String</code>s in that list:
<a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/DocumentPreprocessor.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/DocumentPreprocessor.html</a>
, only <code>HasWord</code>s. Probably you should go through and remove by its toString() or find out the concrete implementation.</p>
",0,-2,187,2014-06-08 08:45:24,https://stackoverflow.com/questions/24104443/cant-remove-element-from-java-util-list
JAVA: How use Gazettes with Stanford NLP?,"<p>I read this <a href=""http://www-nlp.stanford.edu/software/crf-faq.shtml#gazette"" rel=""nofollow"">faq</a> but i not understand. I try with this code:</p>

<pre><code>   Properties pp=new Properties();  
   pp.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
   pp.put(""ner.useSUTime"",""false"");

   pp.put(""useGazettes"",""true"");
   pp.put(""gazette"",""C:\\gaz.txt"");

   StanfordCoreNLP s=new StanfordCoreNLP(pp);
</code></pre>

<p>This is String: ""Dan became a member of the Music friends association in 2008""</p>

<p>the gazette file is:</p>

<pre><code>  CLASS Music friends association 
</code></pre>

<p>But ""Music friends association"" is not recognized by NER.</p>

<p>Where am I wrong?</p>
","java, nlp, stanford-nlp, named-entity-recognition","<p>The answer is given there:</p>

<blockquote>
  <p>If a gazette is used, this does not guarantee that words in the gazette are always used as a member of the intended class, and it does not guarantee that words outside the gazette will not be chosen. It simply provides another feature for the CRF to train against. If the CRF has higher weights for other features, the gazette features may be overwhelmed.</p>
</blockquote>

<p>So there is not guarantee that your phrase will be tagged in any way.  The alternative is </p>

<blockquote>
  <p>either the regexner or the tokensregex tools included in Stanford CoreNLP</p>
</blockquote>
",0,3,1063,2014-06-11 13:28:08,https://stackoverflow.com/questions/24164186/java-how-use-gazettes-with-stanford-nlp
Huge overhead to loading StanfordOpenNLP model in Java?,"<p>I'm trying to use StanfordNLP to do coreference resolution on chunks of text relating to a given topic, and while trying to load in the StanfordCoreNLP model, it at first completely ran out of memory while loading models, but now is still taking upwards of 15 minutes to load.</p>

<p>I have code like: </p>

<pre><code> public Map&lt;Integer, CorefChain&gt; getCoreferences(String text) {
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation document = new Annotation(text);

    pipeline.annotate(document);

    return document.get(CorefCoreAnnotations.CorefChainAnnotation.class);
}
</code></pre>

<p>Is this unavoidable by design? Will it even be possible to do coreference resolution like this in a production application where anything more than 10 seconds is unacceptable?</p>
","java, memory, nlp, stanford-nlp","<p>Yes, it's much faster if you don't instantiate <i>StanfordCoreNLP</i>
inside your method. Store it as a class variable.</p>

<p>More specifically, move the following to outside your method:</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>Hope it helps! ;)</p>
",2,3,167,2014-06-12 22:00:51,https://stackoverflow.com/questions/24194606/huge-overhead-to-loading-stanfordopennlp-model-in-java
document similarity with documents using synonyms,"<p>I have a bunch of documents where some of the documents are a copy of other documents with their text jumbled up and some of the words replaced by their synonyms. Mentioned below is one such example of a sentence:</p>

<blockquote>
  <p><strong><em>Article 1 (original)</em></strong> : I caught up with John Snow in town making purchases at Kingslanding Hardware store to repair a broken tractor.  Snow has farmed soybeans his entire life, as did his father and their fathers. I asked him about his life on the farm.</p>
  
  <p><strong><em>Article 2 (duplicate)</em></strong> : I obtained John Snow which in city in purchases make rise of the hardware at Kingslanding to repair a broken motor tractor.  Snow have soya broad beans complete life have been treated, such as its father and their fathers. I asked him concerning its life on the agriculture company.</p>
  
  <p><strong><em>Article 3 (duplicate)</em></strong> : I took for above with John Snow in the city that made purchases in the warehouse of the hardware of Kingslanding to repair an broken tractor.  Snow has cultivated the soybeans its whole life, like its father and his parents. I asked to him about its life in the farm.</p>
  
  <p><strong><em>Article 4 (duplicate)</em></strong> : I caught up with myself compared to John Snow downtown making of the purchases to the kingslanding store of material to repair a broken tractor.  Snow cultivated soya its life whole, just as his/her father and their fathers. I questioned it about his life with the farm.</p>
</blockquote>

<p>I want to do a document similarity which ends up tagging all these documents in the same group. Any suggestions along with examples or tutorials will be greatly appreciated. </p>
","machine-learning, nlp, scikit-learn, stanford-nlp, information-retrieval","<p>It seems like a textbook case of <a href=""http://en.wikipedia.org/wiki/Locality-sensitive_hashing"" rel=""nofollow noreferrer"">Locality Sensitive Hashing</a>.
Check out <a href=""https://stackoverflow.com/questions/12952729/how-to-understand-locality-sensitive-hashing"">this SO thread</a></p>
",0,0,198,2014-06-19 04:15:27,https://stackoverflow.com/questions/24298677/document-similarity-with-documents-using-synonyms
proposed nlp algorithm for text tagging,"<p>I was looking for opensource tool which can help to identify the <code>tags</code> for any user post on social media and identifying topic/off-topic or spam comment on that post. Even after looking for entire day, I could not find any suitable tool/library. </p>

<p>Here I have proposed my own algorithm for tagging user post belonging to 7 categories (jobs, discussion, events, articles, services, buy/sell, talents).</p>

<p>Initially when user makes post, he tags his post. Tags can be like <code>marketing, suggestion, entrepreneurship, MNC etc</code>. So consider for some posts I have tags and to which category they belongs.</p>

<p>Steps:</p>

<ol>
<li><p>Perform POS (part of speech) tagging on user post. 
Here two things can be done.</p>

<ul>
<li><p>considering only nouns. Nouns may represent the tag for post more
intuitively I guess</p></li>
<li><p>Considering Nouns and adjectives both. Here we can collect large
numbers of nouns and adjectives. Frequency of such words can be used
to identify tag for that post.</p></li>
</ul></li>
<li><p>For each user defined tag, we will collect POS for that post belonging to particular tag. Example. Consider user assigned tag <code>marketing</code> and post for this tag contains POS words <code>SEO</code> and <code>adwords</code>. Suppose 10 post of <code>marketing</code> tag contains <code>SEO and adwords</code> 5 and 7 times respectively. So next time when user post comes which does not have any tag but contains POS word <code>SEO</code>. <code>SEO</code> is occurring maximum times <code>7</code> in marketing tag, So we will predict <code>marketing</code> tag for this post </p></li>
<li><p>NExt steps is for identify spam or off-topic comment for POST.
Consider one user post for <code>Job</code> category. This post contains tag <code>marketing</code>. Now I will check in database for TOP  most frequent 10-15 Part of speech tags(i.e. nouns and adjective) for marketing. </p>

<p>Parallel I have POS tag for that comment. I will check whether POS(noun &amp; adj) of this post contains top most frequent tags(we can consider 15-20 such POS tags) belonging to <code>marketing</code>. </p></li>
</ol>

<p>If POS in comments does not match with any of the most frequent, top POS for marketing then that comment can be said off-topic/span</p>

<p><strong>DO YOU HAVE ANY SUGGESTION TO MAKE THIS ALGO MORE INTUITIVE??</strong></p>

<p><strong>I guess SVM can help for classification, any suggestion for this?</strong></p>

<p><strong>Apart from this WhIch machine learning technique can help here to learn system to predict tag and spam(off topic) comments</strong> </p>
","php, algorithm, nlp, data-mining, stanford-nlp","<p>The main problem as I see it is with your feature modeling. While picking out only nouns would help reduce the feature space, it is an extra step with a potentially significant error rate. And do you really care whether you are looking at <code>market/N</code> and not <code>market/V</code>?</p>

<p>Most mainline text classification implementations using naive bayesian classifiers just ignore the POS, and simply count each distinct word form as an independent feature. (You could also do brute-force stemming to reduce <code>market</code>, <code>markets</code>, and <code>marketing</code> to a single stem form and thus a single feature. This tends to work in English, but might not be very adequate if you are actually working in a different language.)</p>

<p>A compromise could be to do POS filtering when you train your classifier. Then word forms which do not have a noun reading end up with a zero score in the classifier, so you don't have to do anything to filter them out when you use the resulting classifier.</p>

<p>Empirically, SVM tends to achieve a high accuracy, but it comes at the cost of complexity, both in implementation and behavior. A naive bayesian classifier has the distinct advantage that you can understand precisely how it arrived at a particular conclusion. (Well, most of us mortals cannot claim to have the same grasp of the mathematics behind SVM.) Perhaps a good way to proceed would be to prototype with Bayes, and iron out any kinks while learning how the system as a whole behaves, then maybe later consider switching to SVM once the other parts are stable?</p>

<p>The ""spam"" category is going to be harder than any well-defined content category. It would be tempting to suggest that anything which doesn't fit any of your content categories is off-topic, but if you are going to use the verdict for automatic spam filtering, this is likely to cause some false positives at least in the early stages. A possible alternative could be to train classifiers for particular spam categories -- one for medications, another for running shoes, etc.</p>
",1,0,926,2014-06-19 19:29:16,https://stackoverflow.com/questions/24314817/proposed-nlp-algorithm-for-text-tagging
Stanford NLP Parser. How to splitt the Tree?,"<p>If I take the example from the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">homepage</a>:</p>

<pre><code>The strongest rain ever recorded in India shut down 
the financial hub of Mumbai, snapped communication 
lines, closed airports and forced thousands of people 
to sleep in their offices or walk home during the night, 
officials said today.
</code></pre>

<p>The Stanford parser:</p>

<pre><code>LexicalizedParser lexicalizedParser = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");

Tree parse = lexicalizedParser.parse(text);
TreePrint treePrint = new TreePrint(""penn, typedDependencies"");

treePrint.printTree(parse);
</code></pre>

<p>Delivers the follwing tree:</p>

<pre><code>(ROOT
(S
  (S
    (NP
      (NP (DT The) (JJS strongest) (NN rain))
      (VP
        (ADVP (RB ever))
        (VBN recorded)
        (PP (IN in)
          (NP (NNP India)))))
    (VP
      (VP (VBD shut)
        (PRT (RP down))
        (NP
          (NP (DT the) (JJ financial) (NN hub))
          (PP (IN of)
            (NP (NNP Mumbai)))))
      (, ,)
      (VP (VBD snapped)
        (NP (NN communication) (NNS lines)))
      (, ,)
      (VP (VBD closed)
        (NP (NNS airports)))
      (CC and)
      (VP (VBD forced)
        (NP
          (NP (NNS thousands))
          (PP (IN of)
            (NP (NNS people))))
        (S
          (VP (TO to)
            (VP
              (VP (VB sleep)
                (PP (IN in)
                  (NP (PRP$ their) (NNS offices))))
              (CC or)
              (VP (VB walk)
                (NP (NN home))
                (PP (IN during)
                  (NP (DT the) (NN night))))))))))
  (, ,)
  (NP (NNS officials))
  (VP (VBD said)
    (NP-TMP (NN today)))
  (. .)))
</code></pre>

<p>I now want to splitt the Tree dependent to its structure to get the clauses.
So in this example i want to splitt the tree to get the following parts:</p>

<ul>
<li>The strongest rain ever recorded in India</li>
<li>The strongest rain shut down the financial hub of Mumbai</li>
<li>The strongest rain snapped communication lines</li>
<li>The strongest rain closed airports</li>
<li>The strongest rain forced thousands of people to sleep in their offices</li>
<li>The strongest rain forced thousands of people to walk home during night</li>
</ul>

<h2>How can i do that?</h2>

<hr>

<p>So the first answer was to use an recursive algorithm to print all 
root to leaf pathes. </p>

<p>Here is the code i tried:</p>

<pre><code>public static void main(String[] args) throws IOException {
    LexicalizedParser lexicalizedParser = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");

    Tree tree = lexicalizedParser.parse(""In a ceremony that was conspicuously short on pomp and circumstance at a time of austerity, Felipe, 46, took over from his father, King Juan Carlos, 76."");

    printAllRootToLeafPaths(tree, new ArrayList&lt;String&gt;());
}

private static void printAllRootToLeafPaths(Tree tree, List&lt;String&gt; path) {
    if(tree != null) {
        if(tree.isLeaf()) {
            path.add(tree.nodeString());
        }

        if(tree.children().length == 0) {
            System.out.println(path);
        } else {
            for(Tree child : tree.children()) {
                printAllRootToLeafPaths(child, path);
            }
        }

        path.remove(tree.nodeString());
    }
}
</code></pre>

<p>Ofcourse this code is totally unlogical because if i just add the leafs
to the paths there will never be the recursive call cause leafs have 
no children.
The problem here is, all real words are leafs and so this algorithm will 
just print out single words which are leafs:</p>

<pre><code>[The]
[strongest]
[rain]
[ever]
[recorded]
[in]
[India]
[shut]
[down]
[the]
[financial]
[hub]
[of]
[Mumbai]
[,]
[snapped]
[communication]
[lines]
[,]
[closed]
[airports]
[and]
[forced]
[thousands]
[of]
[people]
[to]
[sleep]
[in]
[their]
[offices]
[or]
[walk]
[home]
[during]
[the]
[night]
[,]
[officials]
[said]
[today]
[.]
</code></pre>
","java, stanford-nlp","<p>Take a look at <a href=""https://stackoverflow.com/questions/14936861/print-all-root-to-leaf-paths-in-a-binary-tree"">print all root to leaf paths in a binary tree</a> or splitting a binary tree:</p>

<ul>
<li><a href=""http://math-puzzles-computing.blogspot.nl/2011/02/splitting-binary-search-tree-at-given.html"" rel=""nofollow noreferrer"">http://math-puzzles-computing.blogspot.nl/2011/02/splitting-binary-search-tree-at-given.html</a></li>
<li><a href=""http://www.cs.cmu.edu/afs/cs/academic/class/15210-f11/www/lectures/18/lecture18.pdf"" rel=""nofollow noreferrer"">http://www.cs.cmu.edu/afs/cs/academic/class/15210-f11/www/lectures/18/lecture18.pdf</a></li>
<li><a href=""http://digital.cs.usu.edu/~allan/DS/Notes/Ch19.pdf"" rel=""nofollow noreferrer"">http://digital.cs.usu.edu/~allan/DS/Notes/Ch19.pdf</a></li>
</ul>
",0,1,912,2014-06-24 09:08:06,https://stackoverflow.com/questions/24382581/stanford-nlp-parser-how-to-splitt-the-tree
Pass array of floats when training Stanford CRFClassifier,"<p>For every word in a document, I'm looking to add in a series of floating point numbers as features for Stanford NER's <code>CRFClassifier</code> to train on.  Unfortunately, the documentation on Stanford NER's <code>.prop</code> files hasn't made it clear how to pass in custom features. In general, how does one go about adding custom features to a Stanford NER training set?</p>
","java, nlp, stanford-nlp, named-entity-recognition","<p>You will find your answer inside the 'NERFeatureFactory' class. </p>
",1,1,185,2014-06-27 19:20:38,https://stackoverflow.com/questions/24459351/pass-array-of-floats-when-training-stanford-crfclassifier
Calling StanfordCoreNLP API with a MapReduce job,"<p>I am trying to get large number of documents processed using MapReduce, the idea is to split the files into documents in mapper and apply stanford coreNLP annotators in the reducer phase. </p>

<p>I have a rather simple (standard) pipeline of ""tokenize,ssplit,pos,lemma,ner"", and the reducer just calls a function that applies these annotators to the values passed by the reducer and returns the annotations (as List of Strings), however the output that is generated is garbage.</p>

<p>I have observed that the job returns expected output if I call the annotation function from within the mapper, but that beats the whole parallelism play. Also the job returns expected output when I ignore the values in obtained in reducer and just apply the annotators on a dummy string.</p>

<p>This probably indicated that there is some thread safety issue in the process, but I am not able to figure out where, my annotation function is synchronized and pipeline is private final.</p>

<p>Can someone please provide some pointers as to how this can be resolved?</p>

<p>-Angshu</p>

<p>EDIT:</p>

<p>This is what my reducer looks like, hope this adds in more clarity</p>

<pre><code>public static class Reduce extends MapReduceBase implements Reducer&lt;Text, Text, Text, Text&gt; {
    public void reduce(Text key, Iterator&lt;Text&gt; values, OutputCollector&lt;Text, Text&gt; output, Reporter reporter) throws IOException {
        while (values.hasNext()) {
            output.collect(key, new Text(se.getExtracts(values.next().toString()).toString()));             
        }
    }
}
</code></pre>

<p>And this is the code for get extracts:</p>

<pre><code>final StanfordCoreNLP pipeline; 
public instantiatePipeline(){
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");

}


synchronized List&lt;String&gt; getExtracts(String l){
    Annotation document = new Annotation(l);

    ArrayList&lt;String&gt; ret = new ArrayList&lt;String&gt;();

    pipeline.annotate(document);

    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    int sid = 0;
    for(CoreMap sentence:sentences){
        sid++;
        for(CoreLabel token: sentence.get(TokensAnnotation.class)){
            String word = token.get(TextAnnotation.class);
            String pos = token.get(PartOfSpeechAnnotation.class);
            String ner = token.get(NamedEntityTagAnnotation.class);
            String lemma = token.get(LemmaAnnotation.class);

            Timex timex = token.get(TimeAnnotations.TimexAnnotation.class);

            String ex = word+"",""+pos+"",""+ner+"",""+lemma;
            if(timex!=null){
                ex = ex+"",""+timex.tid();
            }
            else{
                ex = ex+"","";
            }
            ex = ex+"",""+sid;
            ret.add(ex);
        }
    }
</code></pre>
","java, mapreduce, nlp, stanford-nlp, hadoop2","<p>I resolved the issue, actually the problem was with the text encoding in the file I was reading from  (converting it to Text caused further corruptions I guess) which were causing problems in the tokenization and spilling garbage. I am cleaning the input string and applying strict UTF-8 encoding and things are working fine now.</p>
",0,1,406,2014-06-28 14:51:25,https://stackoverflow.com/questions/24467990/calling-stanfordcorenlp-api-with-a-mapreduce-job
stanford.py NotImplementedError,"<p>I get a <code>NotImplementedError</code> when i run below code. Why?</p>

<pre><code>import nltk
from nltk import *
from nltk.tag import stanford
from nltk.tag.stanford import StanfordTagger

st = StanfordTagger('C:\\Python27\\stanford-postagger\\models\\english-bidirectional-distsim.tagger',
                    'C:\\Python27\\stanford-postagger\\stanford-postagger.jar')
print st.tag('What is the airspeed of an unladen swallow ?'.split()) 
</code></pre>

<p>Error Message:</p>

<pre><code>  File ""C:\Python27\lib\site-packages\nltk\tag\stanford.py"", line 51, in _cmd
    raise NotImplementedError
NotImplementedError
</code></pre>
","python, stanford-nlp","<p>Looking at the source code <a href=""http://pydoc.net/Python/nltk/2.0.1rc3/nltk.tag.stanford/"" rel=""nofollow"">here</a>:</p>

<p>The <code>_cmd</code> method is not implemented in <code>StanfordTagger</code>
<em>- <code>_cmd</code> property: A property that returns the command that will be
      executed.</em></p>

<pre><code>@property
    def _cmd(self):
      raise NotImplementedError
</code></pre>

<p>So when it is called here causes your problem:</p>

<pre><code> # Run the tagger and get the output
    stanpos_output, _stderr = java(self._cmd,classpath=self._stanford_jar, \
                                                    stdout=PIPE, stderr=PIPE)
</code></pre>

<p>In <code>POSTagger</code> the method looks like:</p>

<pre><code>@property
def _cmd(self):
    return ['edu.stanford.nlp.tagger.maxent.MaxentTagger', \
            '-model', self._stanford_model, '-textFile', \
            self._input_file_path, '-tokenize', 'false']
</code></pre>

<p>You could edit the method in  <code>StanfordTagger</code> to match the <code>POSTTagger</code>.</p>
",1,0,226,2014-07-02 08:50:30,https://stackoverflow.com/questions/24526763/stanford-py-notimplementederror
parsing a sentence using Stanford parser with nltk in python,"<p>I am working with a project in python where i need to parse sentences for comparisons and finding similarity between sentences.</p>

<p>I have seen how to parse sentences using stanford parser in java, for the same to do in python,I have downloaded nltk for python.I am new to both python and natural language processing. </p>

<p>I would like to know what are the methods and libraries available with nltk. Any help with possible resources is appreciated. </p>

<p>Thanks in advance. </p>
","python, parsing, nlp, nltk, stanford-nlp","<p>Use python wrapper for CoreNLP: 
<a href=""https://github.com/kedz/corenlp"" rel=""nofollow"">https://github.com/kedz/corenlp</a></p>
",1,0,1178,2014-07-02 15:08:38,https://stackoverflow.com/questions/24534699/parsing-a-sentence-using-stanford-parser-with-nltk-in-python
Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space not fixed,"<p>this is not a duplicate question, i see <a href=""https://stackoverflow.com/questions/2381849/exception-in-thread-main-java-lang-outofmemoryerror-java-heap-space"">this</a>, i want to run a java prograrm and have this error:</p>

<pre><code>Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at edu.stanford.nlp.ie.crf.CRFLogConditionalObjectiveFunction.empty2D(CRFLogConditionalObjectiveFunction.java:892)
    at edu.stanford.nlp.ie.crf.CRFLogConditionalObjectiveFunction.&lt;init&gt;(CRFLogConditionalObjectiveFunction.java:134)
    at edu.stanford.nlp.ie.crf.CRFLogConditionalObjectiveFunction.&lt;init&gt;(CRFLogConditionalObjectiveFunction.java:117)
    at edu.stanford.nlp.ie.crf.CRFClassifier.getObjectiveFunction(CRFClassifier.java:1792)
    at edu.stanford.nlp.ie.crf.CRFClassifier.trainWeights(CRFClassifier.java:1798)
    at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1713)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:763)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:751)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2917)
</code></pre>

<p>according this i try this:</p>

<pre><code>java -Xms2000m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop fa.prop 
</code></pre>

<p>but the error not fix and i see error again! when i was set a value more than 2000m, my os crashed, or i get this output:</p>

<pre><code>...
...
//stanford log
...

Time to convert docs to data/labels: 8.8 seconds
Killed
</code></pre>

<p>how i can fix it</p>

<p>edit:</p>

<p>and for this</p>

<pre><code>[stanford-ner]$ java -Xms1G -Xmx50G -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop fa.prop
</code></pre>

<p>i have this error:</p>

<pre><code>[1000][2000][3000][4000][5000][6000]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f04c7c00000, 1225785344, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 1225785344 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /stanford-ner/hs_err_pid1536.log
</code></pre>
","java, memory, stanford-nlp","<p>Looking at the software's purpose it is likely that it is very memory-consuming, so it is reasonable to assume that 1GB of heap just isn't sufficient, so you'll have to further increase your heap-size.</p>

<p>The messages you get when you try imply that you are using</p>

<ul>
<li>a 32-bit-OS or</li>
<li>a 32-bit-VM</li>
</ul>

<p>which might both limit you to a maximum heap-size of about 1.5GB (at least on windows).</p>

<p>So make sure you use a 64bit-VM on a 64-bit-OS and then try again to increase the heap-size. </p>
",1,0,4301,2014-07-09 11:42:26,https://stackoverflow.com/questions/24652768/exception-in-thread-main-java-lang-outofmemoryerror-java-heap-space-not-fixed
How to name a node when insert it into the tree in Tsurgeon,"<p>When I use Tsurgeon in the Stanford Parser API, I wonder how to name a node when inserting it into the syntactic tree.</p>

<p>for example, I have two operations:</p>

<pre><code>Tsurgeon.parseOperation(""replace predphrase (MAINVP=newpred &gt; PLACEHOLDER)"");//Step 1
Tsurgeon.parseOperation(""insert predphrase &gt;-1 &gt; newpred"")//Step 2
</code></pre>

<p>After these operations, the tree (A (B=predphrase ...)) becomes (A (MAINVP (B=predphrase ...))).</p>

<p>However, I found that it fails to name the MAINVP node in the step 1, and the operation in step 2 is unable to find the node named newpred and throws an exception.</p>
","java, nlp, stanford-nlp","<p>You need to give multiple Tsurgeon patterns in a single command so that the second pattern has access to the named nodes created in the first. Use square brackets to do so:</p>

<pre><code>TsurgeonPattern t = Tsurgeon.parseOperation(
    ""[replace predphrase (MAINVP=newpred &gt; PLACEHOLDER)]"" +
    ""[insert predphrase &gt;-1 &gt; newpred]"");
</code></pre>

<p>A simpler way to make the modification I think you want would be like so:</p>

<pre><code>[adjoinF (MAINVP &gt; foot@) predphrase]
</code></pre>

<p>That will place the matched node <code>predphrase</code> inside a new <code>MAINVP</code> tree.</p>
",0,1,128,2014-07-10 02:29:28,https://stackoverflow.com/questions/24666572/how-to-name-a-node-when-insert-it-into-the-tree-in-tsurgeon
Getting full text out of acronyms in TweetNLP,"<p><a href=""http://www.ark.cs.cmu.edu/TweetNLP/"" rel=""nofollow"">TweetNLP</a> provides tokenizer and part-of-speech tagger for tweets, which is really cool. Now, I wonder if I can take it a step further and extract acronyms. For example, when I get a tweet ""ikr"", I would be able to look it up and get ""I know, right?"". I guess I can write my own dictionary, but it seems that there should already be one?</p>
","nlp, stanford-nlp, tweets","<p>So what I end up doing is to use StanfordNLP with GATE tweeter model. </p>

<p>Sample tweet:</p>

<blockquote>
  <p>ikr smh he asked fir yo last name so he can add u on fb lololol</p>
</blockquote>

<p>Results without gate-EN-twitter.model</p>

<pre><code>word: ikr :: pos: NN :: ne:O
word: smh :: pos: NN :: ne:O
word: he :: pos: PRP :: ne:O
word: asked :: pos: VBD :: ne:O
word: fir :: pos: NNP :: ne:O
word: yo :: pos: NNP :: ne:O
word: last :: pos: JJ :: ne:O
word: name :: pos: NN :: ne:O
word: so :: pos: IN :: ne:O
word: he :: pos: PRP :: ne:O
word: can :: pos: MD :: ne:O
word: add :: pos: VB :: ne:O
word: u :: pos: NN :: ne:O
word: on :: pos: IN :: ne:O
word: fb :: pos: NN :: ne:O
word: lololol :: pos: NN :: ne:O
</code></pre>

<p>Results with gate-EN-twitter.model</p>

<pre><code>word: ikr :: pos: UH :: ne:O
word: smh :: pos: UH :: ne:O
word: he :: pos: PRP :: ne:O
word: asked :: pos: VBD :: ne:O
word: fir :: pos: IN :: ne:O
word: yo :: pos: PRP$ :: ne:O
word: last :: pos: JJ :: ne:O
word: name :: pos: NN :: ne:O
word: so :: pos: IN :: ne:O
word: he :: pos: PRP :: ne:O
word: can :: pos: MD :: ne:O
word: add :: pos: VB :: ne:O
word: u :: pos: PRP :: ne:O
word: on :: pos: IN :: ne:O
word: fb :: pos: NNP :: ne:O
word: lololol :: pos: UH :: ne:O
</code></pre>

<p>Now, I am able to identify slang by looking at the tag of UH and go against my custom dictionary. </p>

<p>Still puzzled why it was not already available out there, but it solves my issue at the moment.</p>
",1,2,371,2014-07-16 21:16:41,https://stackoverflow.com/questions/24790919/getting-full-text-out-of-acronyms-in-tweetnlp
Processing arabic text for transliteration,"<p>I used <a href=""http://www.ar-php.org/en_index-php-arabic.html"" rel=""nofollow noreferrer"">http://www.ar-php.org/en_index-php-arabic.html</a> library for Arabic to english and English to arabic transliteration.</p>

<p>For simple English or Arabic text copied from web it work fine. </p>

<p>But for English text which is written using <code>robert_bold</code> , <code>robert_regular_0</code> fonts, which looks like:</p>

<p><img src=""https://i.sstatic.net/ZXXaf.png"" alt=""Words""></p>

<p>When I convert it, it gives me unsupported text like : </p>

<pre><code>ال ‘؟ س[
كير[ ’[ ت
شو ’\ ن
به ’; س
؟ م[ن
س ال@اناه
</code></pre>

<p>When I convert simple English text, it gives all supported Arabic characters.</p>

<p>I am not native Arabic country residence. </p>

<p>Any suggestion to improve my system will appreciable.</p>
","php, nlp, arabic, stanford-nlp","<p>I believe your problem lies in encoding of your text in this 'robert_bold' font.
It does seem to use some other characters then the standard, so you will need to add those characters to your transliteration library as well.</p>

<p>Look at one of the words you mentioned - Shu'un. The second 'u' letter in the picture has a line above it. So, its outside of normal range of characters, and as such - there is no transliteration for it in that library.</p>
",1,0,312,2014-07-17 05:08:32,https://stackoverflow.com/questions/24795295/processing-arabic-text-for-transliteration
Stanford Topic Modeling Toolbox: not being able to compile with maven,"<p>I tried to compile the TMT toolkit source using maven as per the README file by invoking th command</p>

<pre><code>mvn compile
</code></pre>

<p>However, since there is no pom.xml file in the project root directory, maven was unable to compile the project.</p>

<p>I also tried to use an empty pom.xml. Here is the error which I got:</p>

<pre><code>[INFO] Scanning for projects...
[ERROR] The build could not read 1 project -&gt; [Help 1]
[ERROR]   
[ERROR]   The project  (/mnt/sdb2/software/stanford_tmt/pom.xml) has 1 error
[ERROR]     Non-readable POM /mnt/sdb2/software/stanford_tmt/pom.xml: input contained no data
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
</code></pre>

<p>My question is: Have any of you encountered a similar problem? If so, then how did you resolve it? Is there a way to generate a pom.xml file given the scala source files?</p>
","java, scala, maven, stanford-nlp","<p>It's a Scala project so you will need something like the pom.xml file in the <a href=""http://nlp.stanford.edu/software/tmt/tmt-0.2/"" rel=""nofollow"">source distribution for v0.2.1</a>.</p>
",2,0,295,2014-07-30 18:51:49,https://stackoverflow.com/questions/25044673/stanford-topic-modeling-toolbox-not-being-able-to-compile-with-maven
Stanford Named Entity Tagger - Inconsistency ?,"<p>I have a strange problem.</p>

<p>I have a list of sentences (around 0.1 million) which is want to tag using the stanford named entity recognition(ner) tagging. I was tagging using the following line of code that is provided from the stanford ner demo website (Java Demo Code).</p>

<pre><code>for (String str : List&lt;sentences&gt;) {
   System.out.print(classifier.classifyToString(str, ""slashTags"", false));
}
</code></pre>

<p>I thought everything is going right until I manually checked for some of the sentences that were not tagged at all which are supposed to be tagged. But when these sentences which are not tagged are hand picked into some sample list and tested with the above code they are getting tagged then. So I am confused where I am going wrong. The sentences which are not tagged correctly are like in the range of 1000 - 1500 sentences. so when I ran these incorrectly tagged sentences in a separate list then they are getting tagged. Is the size of the dataset (0.1 million) having any impact on the classifier ?</p>

<p>For example:
consider the following sentence - ""IBM Corporation Introduction""
Sentences like above are present in considerable number in my 0.1 million dataset. So when I do the tagging using the above code on the 0.1 million dataset, many of sentences like these have got no tagging at all. But When I hand pick those and put in in a list and then do the tagging then they are getting tagged. </p>

<p>I have tried all the approaches and I end up in the same result of no tagging for the sentences like above when tagging on the entire dataset.</p>

<p>I tried the following 3 different ways
1. classifier.classifyToString(inputString, ""slashTags"", false)
2. classifier.classify(inputString)
3. classifier.classifyToCharacterOffsets(inputString)  </p>

<p>Any ideas or suggestions where I am going wrong ?</p>

<p>Thanks</p>
","stanford-nlp, named-entity-recognition, named-entity-extraction","<p>I think you got answer from the below link:</p>

<p><a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2014-July/006045.html"" rel=""nofollow"">https://mailman.stanford.edu/pipermail/java-nlp-user/2014-July/006045.html</a></p>
",0,0,357,2014-07-31 02:43:12,https://stackoverflow.com/questions/25050041/stanford-named-entity-tagger-inconsistency
What does k fold validation mean in the context of POS tagging?,"<p>I know that for k-cross validation, I'm supposed to divide the corpus into k equal parts. Of these k parts, I'm to use k-1 parts for training and the remaining 1 part for testing. This process is to be repeated k times, such that <em>each</em> part is used once for testing.</p>

<p>But I don't understand <em>what exactly does training mean</em> and <em>what exactly does testing mean</em> . </p>

<p>What I think is (please correct me if I'm wrong):<br>
<strong><em>1.</em></strong> <strong>Training sets (k-1 out of k):</strong> These sets are to be used build to the Tag transition probabilities and Emission probabilities tables.  And then, apply some algorithm for tagging using these probability tables (Eg. Viterbi Algorithm)<br>
<strong><em>2.</em></strong> <strong>Test set (1 set):</strong> Use the remaining 1 set to validate the implementation done in step 1. That is, this set from the corpus will have untagged words and I should use the step 1 implementation <em>on</em> this set.</p>

<p>Is my understanding correct? Please explain if not.</p>

<p>Thanks.</p>
","machine-learning, nlp, stanford-nlp, pos-tagger","<p>I hope this helps:</p>

<pre><code>from nltk.corpus import brown
from nltk import UnigramTagger as ut

# Let's just take the first 100 sentences.
sents = brown.tagged_sents()[:1000]
num_sents = len(sents)
k = 10
foldsize = num_sents/10

fold_accurracies = []

for i in range(10):
    # Locate the test set in the fold.
    test = sents[i*foldsize:i*foldsize+foldsize]
    # Use the rest of the sent not in test for training.
    train = sents[:i*foldsize] + sents[i*foldsize+foldsize:]
    # Trains a unigram tagger with the train data.
    tagger = ut(train)
    # Evaluate the accuracy using the test data.
    accuracy = tagger.evaluate(test)
    print ""Fold"", i 
    print 'from sent', i*foldsize, 'to', i*foldsize+foldsize
    print 'accuracy =', accuracy 
    print
    fold_accurracies.append(accuracy)

print 'average accuracy =', sum(fold_accurracies)/k
</code></pre>

<p>[out]:</p>

<pre><code>Fold 0
from sent 0 to 100
accuracy = 0.785714285714

Fold 1
from sent 100 to 200
accuracy = 0.745431364216

Fold 2
from sent 200 to 300
accuracy = 0.749628896586

Fold 3
from sent 300 to 400
accuracy = 0.743798291989

Fold 4
from sent 400 to 500
accuracy = 0.803448275862

Fold 5
from sent 500 to 600
accuracy = 0.779836277467

Fold 6
from sent 600 to 700
accuracy = 0.772676371781

Fold 7
from sent 700 to 800
accuracy = 0.755679184052

Fold 8
from sent 800 to 900
accuracy = 0.706402915148

Fold 9
from sent 900 to 1000
accuracy = 0.774622079707

average accuracy = 0.761723794252
</code></pre>
",2,1,586,2014-08-03 17:03:58,https://stackoverflow.com/questions/25106997/what-does-k-fold-validation-mean-in-the-context-of-pos-tagging
"F#, Import library, Visual Studio, offline. How?? Stanford NLP","<p>F#/ Visual studio pros, please help!</p>
<p>How to import libraries? Please just give me an example. How about java.io?</p>
<p>For Java I have tried to install <a href=""https://marketplace.visualstudio.com/items?itemName=SamHarwell.JavaLanguageSupport"" rel=""nofollow noreferrer"">this</a>but didn't work.</p>
<p><a href=""https://i.sstatic.net/0crTt.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0crTt.jpg"" alt=""Problemz"" /></a></p>

","f#, stanford-nlp","<p>It looks like you're trying to use the Stanford NLP libraries, these are available on NuGet and can be installed by right clicking on the project and selecting manage nuget packages. From there it's possible to add the references to the project. The .nuspec files which you've referenced in your project are for packaging up the libraries. So in your case, the nuspec filenames will be the thing that you search for in Nuget.</p>

<p>The Nuget website offers a guide for how to install packages <a href=""http://docs.nuget.org/docs/start-here/managing-nuget-packages-using-the-dialog"" rel=""nofollow"">here</a></p>
",1,-3,363,2014-08-04 05:46:37,https://stackoverflow.com/questions/25112412/f-import-library-visual-studio-offline-how-stanford-nlp
CoreNLP SemanticGraph - search for edges with specific lemmas,"<p>I'm using Stanford CoreNLP's dependency parser, and wondering how to make a generic search for SemanticEdge(s) with specific head lemma, dependent lemma, and lexical relationship. For example, if I have an actual dependency like this:</p>

<ul>
<li>dobj(discover-4, insights-6)</li>
</ul>

<p>How do I search for it using lemmas instead of the literal word and the index? Basically I want to be able to pattern match the parts of the dependency graph using generic rules...</p>
",stanford-nlp,"<p>You could do this with <code>semgrex</code>. See <a href=""http://nlp.stanford.edu/software/tregex.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tregex.shtml</a>. Though you could also do it manually. Make sure that the nodes are storing lemmas -- see the <code>lemma</code> annotator of CoreNLP (currently available for English, only).</p>
",2,2,635,2014-08-15 12:31:37,https://stackoverflow.com/questions/25326451/corenlp-semanticgraph-search-for-edges-with-specific-lemmas
How to extract text tagged by Stanford NER into csv?,"<p>I don't have a background in NLP or much programming, but have drifted in as I am doing research on the history of newspaper publishing. I'm wrestling with 10k+ pages of plain text that I'm having difficulty sculpting into structured data to perform more complex analyses.  </p>

<p>I've been able to run Stanford NER on a large text file to successfully tag many of the entites I'm looking to examine. So here is my naive question: how can I extract or parse the tagged text file into a csv file - or at least separate lists for each category into some kind of structure? </p>

<p>For example, I'm looking at something like this: </p>

<pre><code>The &lt;ORGANIZATION&gt;Committee on Education&lt;/ORGANIZATION&gt; and the &lt;ORGANIZATION&gt;Philadelphia Assocation of Teachers&lt;/ORGANIZATION&gt; offer a plan for the organization of the school in the town of &lt;LOCATION&gt;Erie&lt;/LOCATION&gt;, &lt;LOCATION&gt;Pennsylvania&lt;/LOCATION&gt; as it will be run by the honorable &lt;PERSON&gt;Williamson&lt;/PERSON&gt; and &lt;PERSON&gt;Thompson&lt;/PERSON&gt;
</code></pre>

<p>Based on looking through the vaugely similar answers to other questions on this site, I've looked at possibly using some kind of regular expression or even sed, like below, but without success.  </p>

<p>sed -e '/^location/,/^/location/p' nertagged.txt</p>

<p>I've considered other options like BeautifulSoup or an XML parser (since the Stanford NER implementation can output XML), but I wonder if that isn't overkill since I'm dealing with a very limited number of tags -- basically just Person, Location, Organization. Are those my best options? What, in my ignorance, am I missing? </p>

<p>Many thanks.</p>
",stanford-nlp,"<p>Agree. This isn't actually as easy to do as would be desirable, and I'll add an option to make it easier for the next version :).  But, if you use <code>-outputFormat inlineXML</code> as in your example, then the following Perl one-liner will do the trick, run on the output file, which I've called <code>inlineXML.out</code>.</p>

<pre><code>perl -ne 'while (s/&lt;([^&gt;]+)&gt;([^&lt;]*)&lt;[^&gt;]+&gt;//) { print ""$2\t$1\n""; }' inlineXML.out
</code></pre>

<p>This actually puts a tab between columns not a comma.  Most spreadsheets will read that fine. If you really want a comma, you can replace the <code>\t</code> above with <code>,</code> but you may well end up with problems if some of the entities include commas, such as perhaps <code>University of California , Davis</code>.</p>
",0,0,748,2014-08-29 16:13:13,https://stackoverflow.com/questions/25572156/how-to-extract-text-tagged-by-stanford-ner-into-csv
Embed datafile in java code,"<p>I have a dictionary file that is being used for word matching, the java code is to be submitted online and get executed.(for a online coding competition)
How would I be able to use the dictionary data file, while my program executes online.
could it be embedded in the source code as compressed byte stream?
please suggest </p>
","java, serialization, stanford-nlp, text-analysis, bytestream","<p>There are multiple ways to achieve this: </p>

<ul>
<li><p>either refer to the dictionairy file as a remote resource in your code.  This means that you ll most your dictionary file on a different online location which is well known by your online application code.  You can then download the dictionary file and cache the file in memory for usage</p></li>
<li><p>You can encode the dictionary file (for instance in Base64 encoding - to take care of special characters in the dictonary file) as a predefined datastructure / buffer in your code.  This means however that you need to convert your dictionary file &amp; rebuild your application each time you adapt the dictionary file.</p></li>
</ul>

<p>Pointing to a different ""online"" location would seem to more suitable solution. </p>
",1,0,73,2014-08-30 14:27:27,https://stackoverflow.com/questions/25583421/embed-datafile-in-java-code
Previously tagged NER corpuses for training NER classifier,"<p>I am working with Stanford NER models to recognise organisation names from unstructured text. I understand that the training data on which stanford ner classifiers were built is not publicly available. I need previously tagged NER corpuses which have organisation names tagged so that I could retrain a stanford ner model.</p>

<p>One source I am aware of : Getting access to reuters corpus and combining it with the annotations from CoNll2003 shared task data. </p>

<p>Could I get suggestions/pointers on more sources to get previously tagged NER corpuses?
(I need to request these datasets through my school )</p>
",stanford-nlp,"<p>Do you mean that you wish to retrain with similar NER data as the original classifier uses, or avoid the default corpora altogether?</p>

<p>I'll assume the first. The corpora we use to train the Stanford English NER classifier are:</p>

<ul>
<li><a href=""http://www.cnts.ua.ac.be/conll2003/ner/"" rel=""nofollow"">CoNLL 2003 shared task data</a></li>
<li><a href=""https://catalog.ldc.upenn.edu/LDC2003T13"" rel=""nofollow"">MUC-6</a></li>
<li><a href=""https://catalog.ldc.upenn.edu/LDC2001T02"" rel=""nofollow"">MUC-7</a></li>
<li><a href=""https://catalog.ldc.upenn.edu/LDC2006T06"" rel=""nofollow"">ACE</a></li>
</ul>

<p>In any case, there is a nice longer list of NER datasets available <a href=""http://www.cs.technion.ac.il/~gabr/resources/data/ne_datasets.html"" rel=""nofollow"">here</a>.</p>
",0,1,439,2014-09-01 02:19:18,https://stackoverflow.com/questions/25598082/previously-tagged-ner-corpuses-for-training-ner-classifier
"Can the Stanford NLP library determine the case (nominative, accusative ...) of words?","<p>I'm parsing some German text with the Stanford NLP library, and I want it to annotate every noun, pronoun (and article?) with the relevant case (nominative, accusative, ...).</p>

<p>Is there a class that does that?</p>

<p>I was excited for a moment to see a class called TrueCaseAnnotator, but it seemed like that actually had to do with uppercase/lowercase (although I never quite got it to work, so maybe I'm wrong about that?)</p>
","nlp, stanford-nlp","<p>No there isn't. Sorry about that!</p>
",3,0,124,2014-09-01 14:49:38,https://stackoverflow.com/questions/25608390/can-the-stanford-nlp-library-determine-the-case-nominative-accusative-of
Bias towards negative sentiments from Stanford CoreNLP,"<p>I'm experimenting with deriving sentiment from Twitter using Stanford's CoreNLP library, a la <a href=""https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java"" rel=""nofollow"">https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java</a> - so see here for the code that I'm implementing. </p>

<p>I am getting results, but I've noticed that there appears to be a bias towards 'negative' results, both in my target dataset and another dataset I use with ground truth - the Sanders Analytics Twitter Sentiment Corpus <a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""nofollow"">http://www.sananalytics.com/lab/twitter-sentiment/</a> - even though the ground truth data do not have this bias. </p>

<p>I'm posting this question on the off chance that someone else has experienced this and/or may know if this is the result of something I've done or some bug in the CoreNLP code.</p>

<p>(edit - sorry it took me so long to respond)
I am posting links to plots showing what I mean. I don't have enough reputation to post the images, and can only include two links in this post, so I'll add the links in the comments. </p>
","java, twitter, nlp, stanford-nlp, sentiment-analysis","<p>I'd like to suggest this is simply a domain mismatch. The Stanford RNTN is trained on movie review snippets and you are testing on twitter data. Other than the topics mismatch, tweets also tend to be ungrammatical and use abbreviated (""creative"") language.
If I had to suggest a more concrete reason, I would start with a lexical mismatch. Perhaps negative emotions are expressed in a domain-independent way, e.g. with common adjectives, and positive emotions are more domain-dependent or more subtle.</p>

<p>It's still interesting that you're getting a negative bias. The Polyanna hypothesis  suggests a positive bias, IMHO.</p>

<p>Going beyond your original question, there are several approaches to do sentiment analysis specifically on microblogging data. See e.g. <a href=""http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2857/3251"" rel=""noreferrer"">""The Good, The Bad and the OMG!"" by Kouloumpis et al.</a></p>
",4,5,1317,2014-09-08 16:49:41,https://stackoverflow.com/questions/25729204/bias-towards-negative-sentiments-from-stanford-corenlp
Stanford Core NLP Parse tree without a root,"<p>I'm using Stanford CoreNLP for getting dependency trees of sentences. 
The problem that I came accross is, for some of the sentences, the tree does not have a root node. Is this possible?</p>

<p><a href=""https://stackoverflow.com/questions/16300056/stanford-core-nlp-missing-roots"">Here</a>, there is a similar example in which the problem is detected to be with the print method (i.e. there is a root, but somehow it is not being printed).</p>

<p>However, in my case, the sentence does not have a root at all. Here is the test sentence that I have: ""(Wendigo is) why we go to the cinema : to be fed through the eye , the heart , the mind .""</p>

<p>I'm printing the dependencies using the following code:</p>

<pre><code>SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
out.println(dependencies.toString(""plain""));
</code></pre>

<p>and here is the output:</p>

<pre><code>nsubj(-RRB--4, Wendigo-2)
cop(-RRB--4, is-3)
advmod(go-7, why-5)
nsubj(go-7, we-6)
advcl(-RRB--4, go-7)
det(cinema-10, the-9)
prep_to(go-7, cinema-10)
aux(fed-14, to-12)
auxpass(fed-14, be-13)
parataxis(go-7, fed-14)
det(eye-17, the-16)
prep_through(fed-14, eye-17)
det(heart-20, the-19)
appos(eye-17, heart-20)
det(mind-23, the-22)
appos(heart-20, mind-23)
</code></pre>

<p>Once I try to print the root node manually, using the following code:</p>

<pre><code>IndexedWord root = dependencies.getFirstRoot();
out.printf(""ROOT(root-0, %s-%d)%n"", root.word(), root.index());
</code></pre>

<p>I get the following error message:</p>

<pre><code>Exception in thread ""main"" java.lang.RuntimeException: No roots in graph:
dep                 reln                gov                 
---                 ----                ---                 
Wendigo-2           nsubj               -RRB--4             
is-3                cop                 -RRB--4             
why-5               advmod              go-7                
we-6                nsubj               go-7                
go-7                advcl               -RRB--4             
the-9               det                 cinema-10           
cinema-10           prep_to             go-7                
to-12               aux                 fed-14              
be-13               auxpass             fed-14              
fed-14              parataxis           go-7                
the-16              det                 eye-17              
eye-17              prep_through        fed-14              
the-19              det                 heart-20            
heart-20            appos               eye-17              
the-22              det                 mind-23             
mind-23             appos               heart-20            

Find where this graph was created and make sure you're adding roots.
</code></pre>

<p>The questions are:</p>

<ol>
<li>Do every sentence has to have a root node in its dependency tree?</li>
<li>Can a sentence have more than one root node? If yes, how is it going to be a tree then?</li>
</ol>

<p>Thanks,</p>
","dependencies, root, stanford-nlp","<p>It looks like this was a bug caused by the parser wrongly generating an adjective phrase constituent which only contained the right parenthesis, which sent the dependencies haywire (the right parenthesis became a governor, but was then deleted as punctuation).</p>

<p>Someone seems to have fixed this already. The current version (in github) gives the dependencies below. So, the new version that we're releasing this week (Apr 2015) should hopefully fix things for you....</p>

<pre><code>root(ROOT-0, is-3)
nsubj(is-3, Wendigo-2)
advmod(go-7, why-5)
nsubj(go-7, we-6)
advcl(is-3, go-7)
case(cinema-10, to-8)
det(cinema-10, the-9)
nmod:to(go-7, cinema-10)
mark(fed-14, to-12)
auxpass(fed-14, be-13)
parataxis(go-7, fed-14)
case(eye-17, through-15)
det(eye-17, the-16)
nmod:through(fed-14, eye-17)
det(heart-20, the-19)
appos(eye-17, heart-20)
det(mind-23, the-22)
appos(heart-20, mind-23)
</code></pre>
",5,1,1022,2014-09-09 17:48:51,https://stackoverflow.com/questions/25750878/stanford-core-nlp-parse-tree-without-a-root
How to convert a verb to its (derived) noun form?,"<p>I am working on a project related to NLP, in which I would like to identify main verb (I can do that with a dependency parser) from a sentence and then convert the verb to its noun form (or we can say noun derived from verb), for example <code>define to definition</code> or <code>sensitive to sensitivity</code> whenever possible. Are there any resources similar to wordnet or verbnet that provides this? </p>
","nlp, nltk, stanford-nlp, opennlp","<p>You can start exploring the following resources:</p>

<ol>
<li><a href=""http://wordnet.princeton.edu/wordnet/download/standoff/"" rel=""nofollow"">Morphosemantic Database</a> from WordNet.</li>
</ol>

<p>The Excel file contains entries like:</p>

<pre><code>define%2:42:03::    202736778   result  definition%1:07:00::    104702957   show the form or outline of; ""The tree w... clarity of outline; ""exercise had given ...
</code></pre>

<ol start=""2"">
<li><a href=""http://clipdemos.umiacs.umd.edu/catvar/"" rel=""nofollow"">CatVar</a></li>
</ol>

<blockquote>
  <p>A Categorial-Variation Database (or Catvar) is a database of clusters of uninflected words (lexemes) and their categorial (i.e. part-of-speech) variants. For example, the words hunger(V), hunger(N), hungry(AJ) and hungriness(N) are different English variants of some underlying concept describing the state of being hungry. Another example is the developing cluster:(develop(V), developer(N), developed(AJ), developing(N), developing(AJ), development(N)).</p>
</blockquote>

<ol start=""3"">
<li>WordNet's word derivation relationship</li>
</ol>
",1,0,886,2014-09-18 05:45:54,https://stackoverflow.com/questions/25905022/how-to-convert-a-verb-to-its-derived-noun-form
stanford core NLP RNNCoreAnnotations node vector,"<p>What does the:</p>

<pre><code>public static SimpleMatrix getNodeVector(Tree tree)
</code></pre>

<p>of the RNNCoreAnnotations class return exactly?
It is a vector of 25 decimal values, but what do they represent?</p>
","java, stanford-nlp","<p>It returns the distributed representation of the node, which is a vector. This corresponds to the vectors <em>a</em>, <em>b</em>, <em>c</em>, <em>p<sub>1</sub></em>, and <em>p<sub>2</sub></em> in Section 4 of the paper about the work: <a href=""http://nlp.stanford.edu/pubs/SocherEtAl_EMNLP2013.pdf"" rel=""nofollow"">http://nlp.stanford.edu/pubs/SocherEtAl_EMNLP2013.pdf</a> . It is not easily human interpretable, but a function of it predicts the node's sentiment, as explained in the paper.</p>
",3,2,207,2014-09-19 14:10:15,https://stackoverflow.com/questions/25935952/stanford-core-nlp-rnncoreannotations-node-vector
sentence boundry detection in noisy or ASR data,"<p>There are many tools and paper available which perform this task using basic sentence seperators.</p>

<p>Such tools are</p>

<ol>
<li><p><a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokenizer.shtml</a>   </p></li>
<li><p>OpenNLP </p></li>
<li><p>NLTK</p></li>
</ol>

<p>and there might be other. They mainly focus on</p>

<pre><code>(a) If it's a period, it ends a sentence.
(b) If the preceding token is on my hand-compiled list of abbreviations, then it doesn't end a sentence.
(c) If the next token is capitalized, then it ends a sentence. 
</code></pre>

<p>There are few paper which suggest techniques for SBD in ASR text</p>

<p><a href=""http://pdf.aminer.org/000/041/703/experiments_on_sentence_boundary_detection.pdf"" rel=""nofollow"">http://pdf.aminer.org/000/041/703/experiments_on_sentence_boundary_detection.pdf</a></p>

<p><a href=""http://www.icsd.aegean.gr/lecturers/kavallieratou/publications_files/icpr_2000.pdf"" rel=""nofollow"">http://www.icsd.aegean.gr/lecturers/kavallieratou/publications_files/icpr_2000.pdf</a></p>

<p><a href=""http://www.icsd.aegean.gr/lecturers/kavallieratou/publications_files/icpr_2000.pdf"" rel=""nofollow"">http://www.icsd.aegean.gr/lecturers/kavallieratou/publications_files/icpr_2000.pdf</a></p>

<p><strong>Is there any tools which can perform sentence detection on ambiguous sentences like</strong></p>

<ol>
<li><p>John is actor and his father Mr Smith was top city doctor in NW (2 sentences)</p></li>
<li><p>Where is statue of liberty, what is it's height and what is the history behind?  (3 sentences)</p></li>
</ol>
","parsing, nlp, artificial-intelligence, semantics, stanford-nlp","<p>What you are seeking to do is to identify the independent clauses in a <a href=""http://grammar.about.com/od/c/g/compoundsent.htm"" rel=""nofollow noreferrer"">compound sentence</a>. A compound sentence is a sentence with at least two independent clauses joined by a <a href=""http://grammar.about.com/od/c/g/coordconjterm.htm"" rel=""nofollow noreferrer"">coordinating conjunction</a>. There is no readily available tool for this, but you can identify compound sentences with a high degree of precision by using <a href=""http://en.wikipedia.org/wiki/Parse_tree#Constituency-based_parse_trees"" rel=""nofollow noreferrer"">constituency parse trees</a>.</p>

<p>Be wary, though. Sligh grammatical mistakes can yield a very wrong parse tree! For example, if you use the Berkeley parser (demo page: <a href=""http://tomato.banatao.berkeley.edu:8080/parser/parser.html"" rel=""nofollow noreferrer"">http://tomato.banatao.berkeley.edu:8080/parser/parser.html</a>) on your first example, the parse tree is not what you would expect, but correct it to ""John is <em>an</em> actor and his father ... "", and you can see the parse tree neatly divided into the structure <code>S CC S</code>:</p>

<p><img src=""https://i.sstatic.net/2RS1l.png"" alt=""The Berkeley Parser&#39;s output on the first sentence""></p>

<p>Now, you simply take each sentence-label <code>S</code> as an independent clause!</p>

<p>Questions are not handled well, I am afraid, as you can check with your second example.</p>
",2,1,307,2014-09-22 11:46:20,https://stackoverflow.com/questions/25973351/sentence-boundry-detection-in-noisy-or-asr-data
Clause Extraction using Stanford parser,"<p>I have a complex sentence and I  need to separate it into main and dependent clause.
For example for the sentence<br>
ABC cites the fact that chemical additives are banned in many countries and feels they may be banned in this state too.<br>
The split required</p>

<pre><code>1)ABC cites the fact   
2)chemical additives are banned in many countries   
3)ABC feels they may be banned in this state too.    
</code></pre>

<p>I think I could use the Stanford Parser tree or dependencies, but I am not  sure how to proceed from here.  </p>

<p>The tree </p>

<pre>
(ROOT
  (S
    (NP (NNP ABC))
    (VP (VBZ cites)
      (NP (DT the) (NN fact))
      (SBAR (IN that)
        (S
          (NP (NN chemical) (NNS additives))
          (VP
            (VP (VBP are)
              (VP (VBN banned)
                (PP (IN in)
                  (NP (JJ many) (NNS countries)))))
            (CC and)
            (VP (VBZ feels)
              (SBAR
                (S
                  (NP (PRP they))
                  (VP (MD may)
                    (VP (VB be)
                      (VP (VBN banned)
                        (PP (IN in)
                          (NP (DT this) (NN state)))
                        (ADVP (RB too))))))))))))
    (. .)))
</pre>

<p>and the collapsed dependency parse  </p>

<pre>
nsubj(cites-2, ABC-1)  
root(ROOT-0, cites-2)  
det(fact-4, the-3)   
dobj(cites-2, fact-4)  
mark(banned-9, that-5)  
nn(additives-7, chemical-6)  
nsubjpass(banned-9, additives-7)   
nsubj(feels-14, additives-7)   
auxpass(banned-9, are-8)   
ccomp(cites-2, banned-9)   
amod(countries-12, many-11)  
prep_in(banned-9, countries-12)   
ccomp(cites-2, feels-14)    
conj_and(banned-9, feels-14)    
nsubjpass(banned-18, they-15)   
aux(banned-18, may-16)    
auxpass(banned-18, be-17)    
ccomp(feels-14, banned-18)   
det(state-21, this-20)    
prep_in(banned-18, state-21)    
advmod(banned-18, too-22)   
</pre>
","nlp, stanford-nlp","<p>It is probably better if you <em>primarily</em> use the constituenty-based parse tree, and not the dependencies. The dependencies will be helpful, but only after the main work is done! I am going to explain this towards the end of my answer.</p>

<p>This is because constituency-parse is based on phrase structure grammar, which is the most relevant if you are seeking to extract clauses from a sentence. It can be done using dependencies as well, but in that case, you will essentially be reconstructing the phrase structure -- starting from the root and looking at dependent nodes (e.g. <code>ABC</code> and <code>facts</code> are the nominal subject and direct object of the verb <code>cites</code>, and so on ... ).</p>

<p>It is helpful to visualize the parse tree, however. In your example, the clauses are indicated by the <strong>SBAR</strong> tag, which is a <em>clause introduced by a (possibly empty) subordinating conjunction</em>. All you need to do is the following:</p>

<ol>
<li>Identify the non-root clausal nodes in the parse tree</li>
<li>Remove (but retain separately) the subtrees rooted at these clausal nodes from the main tree.</li>
<li>In the main tree (after removal of subtrees in step 2), remove any <em>hanging</em> prepositions, subordinating conjunctions and adverbs.</li>
</ol>

<p>In step 3, what I mean by ""hanging"" is that any prepositions, etc. whose dependency has been removed in step 2. E.g., from ""ABC cites the fact that"", you need to remove the preposition/subordinating-conjunction ""that"" because its dependent node ""banned"" was removed in step 2. You will thus have three independent clauses:</p>

<ul>
<li>chemical additives are banned in many countries (SBAR removal in step 2)</li>
<li>they may be banned in this state too (SBAR removal in step 2)</li>
<li>ABC cites the fact (step 3)</li>
</ul>

<p>The only issue here is the connection <em>ABC</em>--<em>feels</em>. For this, note that both ""banned"" and ""feels"" are complements of the verb ""cites"", and hence, have the same subject, which is ""ABC""! And you're done. When this is done, you will get a fourth clause, ""ABC feels"", which is something you may or may not want to include in your final result.</p>

<p>For a list of all clausal tags (and, in fact, all Penn Treebank tags), see this list: <a href=""http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html"" rel=""noreferrer"">http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html</a></p>

<p>For an online parse-tree visualization, you may want to use the <a href=""http://tomato.banatao.berkeley.edu:8080/parser/parser.html"" rel=""noreferrer"">online Berkeley parser demo</a>. It helps a lot in forming a better intuition. Here's the image generated for your example sentence:
<img src=""https://i.sstatic.net/rfIRl.png"" alt=""Berkeley Parser Tree""></p>

<p><strong>Caveats</strong></p>

<ol>
<li>Even the best parsers will not always parse sentences correctly, so keep that in mind.</li>
<li>Additionally, many complex sentences involve <a href=""http://en.wikipedia.org/wiki/Right_node_raising"" rel=""noreferrer"">right node raising</a>, which is almost never parsed correctly by most parsers.</li>
<li>You may need to modify the algorithm a little if a clause is in passive voice.</li>
</ol>

<p>Apart from these three pitfalls, the above algorithm should work quite accurately.</p>
",28,16,11350,2014-09-27 00:54:11,https://stackoverflow.com/questions/26070245/clause-extraction-using-stanford-parser
Stanford CoreNLP: Use partial existing annotation,"<p>We are trying to use existing </p>

<ul>
<li>tokenzation </li>
<li>sentence splitting</li>
<li>and named entity tagging</li>
</ul>

<p>while we would like to use Stanford CoreNlp to additionally provide us with</p>

<ul>
<li>part-of-speech tagging</li>
<li>lemmatization</li>
<li>and parsing</li>
</ul>

<p>Currently, we are trying it the following way:</p>

<p>1) make an annotator for ""pos, lemma, parse""</p>

<pre><code>Properties pipelineProps = new Properties();
pipelineProps.put(""annotators"", ""pos, lemma, parse"");
pipelineProps.setProperty(""parse.maxlen"", ""80"");
pipelineProps.setProperty(""pos.maxlen"", ""80"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(pipelineProps);
</code></pre>

<p>2) read in the sentences, with a custom method:</p>

<pre><code>List&lt;CoreMap&gt; sentences = getSentencesForTaggedFile(idToDoc.get(docId));
</code></pre>

<p>within that method, the tokens are constructed the following way:</p>

<pre><code>CoreLabel clToken = new CoreLabel();
clToken.setValue(stringToken);
clToken.setWord(stringToken);
clToken.setOriginalText(stringToken);
clToken.set(CoreAnnotations.NamedEntityTagAnnotation.class, neTag);
sentenceTokens.add(clToken);
</code></pre>

<p>and they are combined into sentences like this:</p>

<pre><code>Annotation sentence = new Annotation(sb.toString());
sentence.set(CoreAnnotations.TokensAnnotation.class, sentenceTokens);
sentence.set(CoreAnnotations.TokenBeginAnnotation.class, tokenOffset);
tokenOffset += sentenceTokens.size();
sentence.set(CoreAnnotations.TokenEndAnnotation.class, tokenOffset);
sentence.set(CoreAnnotations.SentenceIndexAnnotation.class, sentences.size());
</code></pre>

<p>3) the list of sentences is passed to the pipeline:</p>

<pre><code>  Annotation document = new Annotation(sentences);
  pipeline.annotate(document);
</code></pre>

<hr>

<p>However, when running this, we get the following error:</p>

<pre><code>null: InvocationTargetException: annotator ""pos"" requires annotator ""tokenize""
</code></pre>

<hr>

<p>Any pointers how we can achieve what we want to do?</p>
","nlp, stanford-nlp","<p>The exception is thrown due to unsatisfied requirement expected by ""pos"" annotator (an instance of <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/POSTaggerAnnotator.java"" rel=""nofollow"">POSTaggerAnnotator</a> class)</p>

<p>Requirements for annotators which StanfordCoreNLP knows how to create are defined in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/Annotator.java"" rel=""nofollow"">Annotator</a> interface. For the case of ""pos"" annotator there are 2 requirements defined:</p>

<ul>
<li>tokenize </li>
<li>ssplit</li>
</ul>

<p>Both of this requirements needs to be satisfied, which means that both ""tokenize"" annotator and ""ssplit"" annotator must be specified in annotators list before ""pos"" annotator.</p>

<p>Now back to the question... If you like to skip ""tokenize"" and ""ssplit"" annotations in your pipeline you need to disable requirements check which is performed during initialization of the pipeline. I found two equivalent ways how this can be done:</p>

<ul>
<li><p>Disable requirements enforcement in properties object passed to StanfordCoreNLP constructor:</p>

<p><code>props.setProperty(""enforceRequirements"", ""false"");</code></p></li>
<li><p>Set enforceRequirements parameter of StanfordCoreNLP constructor to false</p>

<p><code>StanfordCoreNLP pipeline = new StanfordCoreNLP(props, false);</code></p></li>
</ul>
",4,2,658,2014-10-07 21:14:47,https://stackoverflow.com/questions/26245422/stanford-corenlp-use-partial-existing-annotation
Train and retrain Stanford tagger using the API,"<p>I want to train the Stanford tagger using a corpus which consists of multiple files and will be extended in the future.</p>

<p>Is it possible to update an existant model or do I have to train using the entire corpus every time?</p>

<p>Are there any examples of how to do the training using the API? The JavaDoc of MaxentTagger only covers training via command line.</p>

<p>Thank you!</p>
",stanford-nlp,"<p>At present, you have to train using the entire corpus every time.  (Updating a model with additional data is theoretically possible, but it's not something that currently exists and it isn't on our front burner.)</p>

<p>We do all our training of models from the command line.... Actually, looking at the code, it seems like the train method is private, so you'd need to make it more public to be able to do training from the API. We should fix that. Might try to do this.</p>

<p>If the access level was different, you could create a TaggerConfig and then call this method:</p>

<pre><code>  private static void trainAndSaveModel(TaggerConfig config) throws IOException { ... }
</code></pre>

<p>But, even then, it currently always saves its built tagger to disk. So, things could do with a bit of reworking to enable this smoothly.</p>
",1,0,176,2014-10-19 18:01:11,https://stackoverflow.com/questions/26453646/train-and-retrain-stanford-tagger-using-the-api
concepts extraction from stanford parsing tree nlp,"<p>I am nlp research beginner
I want to extract the concepts from text
for example ""The Thing Things albums"" the concept is ""The Thing Things""</p>

<p>I am using parsing tree for noun phrases <strong>but</strong> in this example the tree extracts ""The Thing Things"" &amp;""albums""
and another example 
""Who started the handset alliance?"" I expected that ""handset alliance"" but the noun phrase is""the handset alliance""</p>

<p>how can I solve them ?</p>
","nlp, stanford-nlp","<p>Your definition of ""concepts"" is not perfectly clear to me, but take Illinois Shallow Parser might be useful for this. See a demo here: <a href=""http://cogcomp.cs.illinois.edu/demo/shallowparse/?id=7"" rel=""nofollow"">http://cogcomp.cs.illinois.edu/demo/shallowparse/?id=7</a> </p>
",0,1,655,2014-10-19 21:58:20,https://stackoverflow.com/questions/26455860/concepts-extraction-from-stanford-parsing-tree-nlp
"Stanford POSTagger, Java heap spce memory","<p>I am using Python3 on Ubuntu 14.04, and am running Stanford POSTagger on a corpus of 67 raw text articles, thje redacted python script is as follows:</p>

<pre><code>from nltk.tag.stanford import POSTagger

with open('the_file.txt','r') as file:
    G=file.readlines()

stan=[]

english_postagger = POSTagger('models/english-bidirectional-distsim.tagger', 'stanford-postagger.jar')

for line in g:
    stan.append(english_postagger.tag(tokenize_fast(line)))
</code></pre>

<p>after several iterations of which I get the following error:</p>

<pre><code>Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space at edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:109)
at edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:31)
at edu.stanford.nlp.tagger.maxent.TestSentence.runTagInference(TestSentence.java:322)
at edu.stanford.nlp.tagger.maxent.TestSentence.testTagInference(TestSentence.java:312)
at edu.stanford.nlp.tagger.maxent.TestSentence.tagSentence(TestSentence.java:135)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.tagSentence(MaxentTagger.java:998)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.tagCoreLabelsOrHasWords(MaxentTagger.java:1788)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.tagAndOutputSentence(MaxentTagger.java:1798)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1709)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1770)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1543)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1499)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1842)
</code></pre>

<p>I have also run the stanford postagger from command line as:</p>

<pre><code>java -mx300m -classpath stanford-postagger.jar   edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/wsj-0-18-bidirectional-distsim.tagger -textFile sample-input.txt &gt; sample-tagged.txt
</code></pre>

<p>with a similar error. I even passed Java 2 GB of memory, and still no luck.</p>

<p>Any thoughts/ideas or hacky type solutions are greatly welcomed!</p>

<p>Well spotted @nsanglar, so I tried:</p>

<pre><code>java -Xmx2g -classpath stanford-postagger.jar   edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/wsj-0-18-bidirectional-distsim.tagger -textFile raw_text.txt &gt; sample-tagged.txt
</code></pre>

<p>I get an error log message, with the following header:</p>

<pre><code># There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 283639808 bytes for committing   reserved memory.
# Possible reasons:
#   The system is out of physical RAM or swap space
#   In 32 bit mode, the process size limit was hit
# Possible solutions:
#   Reduce memory load on the system
#   Increase physical memory or swap space
#   Check if swap backing store is full
#   Use 64 bit Java on a 64 bit OS
#     Decrease Java heap size (-Xmx/-Xms)
#   Decrease number of Java threads
#   Decrease Java thread stack sizes (-Xss)
#   Set larger code cache with -XX:ReservedCodeCacheSize=
# This output file may be truncated or incomplete.

#  Out of Memory Error (os_linux.cpp:2798), pid=25677, tid=140571167794944

# JRE version: OpenJDK Runtime Environment (7.0_65-b32) (build 1.7.0_65-b32)
# Java VM: OpenJDK 64-Bit Server VM (24.65-b04 mixed mode linux-amd64 compressed oops)
# Derivative: IcedTea 2.5.2
# Distribution: Ubuntu 14.04 LTS, package 7u65-2.5.2-3~14.04
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try  ""ulimit -c unlimited"" before starting Java again
</code></pre>
","java, python, stanford-nlp","<p>Well, it turns out it was a RAM issue, I simply did not have enough memory to execute the command. Running the tagger off a server did the trick.</p>
",2,5,1816,2014-10-20 10:48:08,https://stackoverflow.com/questions/26463783/stanford-postagger-java-heap-spce-memory
How to make system call in python and store the output in a given output directory?,"<p>I was working with Stanford CoreNLP, right now I'm running the coreNLP toolkit by using the following command from command-line:</p>

<pre><code>java -cp stanford-corenlp-2012-07-09.jar:stanford-corenlp-2012-07-06-models.jar:xom.jar:
joda-time.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,
pos,lemma,ner -filelist file_list.txt -outputDirectory &lt;OUTPUT DIRECTORY PATH&gt;
</code></pre>

<p>This generates xml files with the required annotation. Now I need to using this command inside a function in python such that it stores the output in the output_dir. The function is like:</p>

<pre><code>def preprocess(file_list.txt, ouptut_dir)
</code></pre>

<p>I read about system calls, and using subprocess, but I didn't quite understand how to use it such that it writes the output to the given output_dir.</p>

<p>Please help!!!</p>
","python, system-calls, stanford-nlp","<p>That really does not have much to do with <code>subprocess</code>, but rather on how Stanford CoreNLP is used from the CLI. Assuming that the <code>-outputDirectory</code> flag tells it where to store it's output, it's a simple matter of passing the correct CLI argument. Here is one proposition:</p>

<pre><code>import subprocess

def preprocess(fname, output_dir):
    subprocess.check_call([
        'java',
        '-cp',
        'stanford-corenlp-2012-07-09.jar:stanford-corenlp-2012-07-06-models.jar:xom.jar:joda-time.jar',
        '-Xmx3g', 'edu.stanford.nlp.pipeline.StanfordCoreNLP'
        '-annotators', 'tokenize,ssplit,pos,lemma,ner',
        '-filelist', fname,
        '-outputDirectory', output_dir
    ])
</code></pre>
",0,0,210,2014-10-20 13:38:09,https://stackoverflow.com/questions/26467030/how-to-make-system-call-in-python-and-store-the-output-in-a-given-output-directo
Activate makeCopulaHead in Stanford CoreNLP parser,"<p>I want to use Stanford CoreNLP Parser to parse a sentence with the flag ""makeCopulaHead"" activated.</p>

<p>In my file input.txt, I have the following sentence:</p>

<pre><code>I am tall.
</code></pre>

<p>The objective is to not have a copula relation (cop) in the output dependency tree.</p>

<p>I tried:</p>

<pre><code>java -cp ""*"" -mx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -makeCopulaHead -file input.txt 
</code></pre>

<p>The .xml file contains cop relation :(</p>

<p>I also tried (a bug with xml-output: <a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2013-January/002959.html"" rel=""nofollow"">https://mailman.stanford.edu/pipermail/java-nlp-user/2013-January/002959.html</a> ?): </p>

<pre><code>java -cp ""*"" -mx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -makeCopulaHead -file input.txt -outputFormat text 
</code></pre>

<p>But it's the same thing...</p>
","nlp, stanford-nlp","<p>I've finally found out the solution, the correct line is:</p>

<pre><code>java -cp ""*"" -mx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -parse.flags "" -makeCopulaHead"" -file input.txt 
</code></pre>

<p>Don't forget the space in "" -makeCopulaHead"".</p>
",3,2,307,2014-10-23 10:12:33,https://stackoverflow.com/questions/26525859/activate-makecopulahead-in-stanford-corenlp-parser
Display Stanford NER confidence score,"<p>I'm extracting named-entities from news articles with the use of Stanford NER CRFClassifier and in order to implement active learning, I would like to know what are the confidence scores of the classes for each labelled entity.</p>

<p>Exemple of display :</p>

<blockquote>
  <p>LOCATION(0.20) PERSON(0.10) ORGANIZATION(0.60) MISC(0.10)</p>
</blockquote>

<p>Here is my code for extracting named-entities from a text :</p>

<pre><code>AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifierNoExceptions(classifier_path);
String annnotatedText = classifier.classifyWithInlineXML(text);
</code></pre>

<p>Is there a workaround to get thoses values along with the annotations ?</p>
","java, stanford-nlp, named-entity-recognition","<p>I've found it out by myself, in CRFClassifier's doc it is written :</p>

<blockquote>
  <p>Probabilities assigned by the CRF can be interrogated using either the
  <code>printProbsDocument()</code> or <code>getCliqueTrees()</code> methods.</p>
</blockquote>

<p>The first method is not useful since it only prints what I want on the console, but I want to be able to access this data, so I have read how this method is coded and copied a bit its behaviour like this :</p>

<pre><code>List&lt;CoreLabel&gt; classifiedLabels = classifier.classify(sentences);
CRFCliqueTree&lt;String&gt; cliqueTree = classifier.getCliqueTree(classifiedLabels);

for (int i = 0; i &lt; cliqueTree.length(); i++) {
    CoreLabel wi = classifiedLabels.get(i);
    for (Iterator&lt;String&gt; iter = classifier.classIndex.iterator(); iter.hasNext();) {
        String label = iter.next();
        int index = classifier.classIndex.indexOf(label);
        double prob = cliqueTree.prob(i, index);
        System.out.println(""\t"" + label + ""("" + prob + "")"");
    }
    String tag = StringUtils.getNotNullString(wi.get(CoreAnnotations.AnswerAnnotation.class));
    System.out.println(""Class : "" + tag);
}   
</code></pre>
",6,1,2066,2014-10-28 16:03:50,https://stackoverflow.com/questions/26612999/display-stanford-ner-confidence-score
Stanford CoreNLP python interface installation errors,"<p>I'm trying to build the python interface of the stanford NLP on Ubuntu 12.04.5 LTS.
There are two steps required, the first of which is:</p>

<ol>
<li>compile Jpype by running ""rake setup"" in 3rdParty/jpype </li>
</ol>

<p>When doing so I get the following error:</p>

<pre><code>In file included from src/native/common/jp_monitor.cpp:17:0:
src/native/common/include/jpype.h:45:17: fatal error: jni.h: No such file or directory
compilation terminated.
error: command 'gcc' failed with exit status 1
rake aborted!
Command failed with status (1): [cd JPype-0.5.4.1 &amp;&amp; python setup.py build...]
</code></pre>

<p>The error messages says I'm missing <code>jni.h</code>, so as suggested <a href=""https://stackoverflow.com/questions/15250536/installing-stanford-parsers-python-interface-error-command-gcc-failed-with"">here</a> if I ran the command <code>dpkg-query -L openjdk-7-jdk | grep ""jni.h""</code> getting <code>/usr/lib/jvm/java-7-openjdk-amd64/include/jni.h</code>.</p>

<p>I believe that means I do have <code>jni.h</code> on my system, so I'm very confused right now. What is causing the error? Can you suggest any fix?</p>

<p>Thanks for your help!</p>

<hr>

<p><strong>A FEW MORE INSIGHTS</strong></p>

<p>Here is the instruction causing the error:</p>

<pre><code>gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/lib/jvm/java-1.5.0-sun-1.5.0.08/include -I/usr/lib/jvm/java-1.5.0-sun-1.5.0.08/include/linux -Isrc/native/common/include -Isrc/native/python/include -I/usr/include/python2.7 -c src/native/common/jp_class.cpp -o build/temp.linux-x86_64-2.7/src/native/common/jp_class.o
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for Ada/C/ObjC but not for C++ [enabled by default]
In file included from src/native/common/jp_class.cpp:17:0:src/native/common/include/jpype.h:45:17: fatal error: jni.h: No such file or directory
compilation terminated.
error: command 'gcc' failed with exit status 1
</code></pre>

<p>It's coming from the compilation of <code>JPype</code> needed for the python interface. I do not know why but it includes paths that I don't have in my filesystem (i.e. <code>-I/usr/lib/jvm/java-1.5.0-sun-1.5.0.08/include/linux</code>).</p>

<p>How can I configure these paths correctly?</p>
","python, java-native-interface, stanford-nlp, java","<p>This problem is a path problem (as said in the question and correctly answered by @vikramls).</p>

<p>Apparently when running the script for installing the python interface of the StanfordNLP if <code>JPype</code> is missing it will get installed with the following command:</p>

<pre><code>python setup.py install
</code></pre>

<p>Now if you open the file <code>setup.py</code> you can see the following part which sets the <code>java</code> paths for a linux machine (I'm running on ubuntu):</p>

<pre><code>def setupLinux(self):
    self.javaHome = os.getenv(""JAVA_HOME"")
    if self.javaHome is None :
        self.javaHome = '/usr/lib/jvm/java-1.5.0-sun-1.5.0.08' # Ubuntu linux
        # self.javaHome = '/usr/java/jdk1.5.0_05'    
    self.jdkInclude = ""linux""    
    self.libraries = [""dl""]
    self.libraryDir = [self.javaHome+""/lib""]
</code></pre>

<p>Clearly this path will not work on every machine, so there are 2 possible solutions:</p>

<ol>
<li><p>Before running the installation script export a variable called <code>JAVA_HOME</code> with the location of your java installation. I.e. <code>export JAVA_HOME=""/usr/lib/jvm/java-7-openjdk-amd64</code> in my case.</p></li>
<li><p>As <a href=""http://www.network-theory.co.uk/docs/gccintro/gccintro_23.html"" rel=""nofollow"">this page</a> says you can set an automatic include variable for <code>gcc</code> with the following command <code>export C_INCLUDE_PATH=some_path</code> and that path should be set to where you java libraries are on your machine</p></li>
</ol>
",0,5,820,2014-11-05 16:31:05,https://stackoverflow.com/questions/26762239/stanford-corenlp-python-interface-installation-errors
"Stanford type dependency, can not extract &quot;prepositional modfier&quot;","<p>I am trying to extract the prepositional modifier, like it is stated in the <strong>Dependency Manual</strong>:</p>

<p>I try to parse the sentence : 
<em>""I saw a cat with a telescope""</em> , using the code: </p>

<pre><code>List&lt;CoreMap&gt; sentences = stanfordDocument.get(SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
   Tree tree = sentence.get(TreeAnnotation.class);
   TreebankLanguagePack languagePack = new PennTreebankLanguagePack();
   GrammaticalStructureFactory grammaticalStructureFactory = languagePack.grammaticalStructureFactory();
   GrammaticalStructure structure = grammaticalStructureFactory.newGrammaticalStructure(tree);
   Collection&lt;TypedDependency&gt; typedDependencies = structure.typedDependenciesCollapsed();
   for (TypedDependency td : typedDependencies) {
      System.out.println(td.reln());
   }
}     
</code></pre>

<p>As stated in the Manual I was expecting to get : <em>prep(saw, with)</em>.</p>

<p>In the Collection of the TypedDependeny I get only
 ""<em>nsubj; root; det; dobj; det; prep_with</em>"" as relation type, and not the <em>""prep/prepc""</em>  as stated in the <a href=""http://robotics.usc.edu/~gkoch/DependencyManual.pdf"" rel=""nofollow"">http://robotics.usc.edu/~gkoch/DependencyManual.pdf</a> (page 8). </p>

<p>I have also tried to extract <em>pcomp : Prepositional compelement</em> (page 7 of the manual) and it doesnt find it.</p>

<p>Did somebody encountered the same problem? Am I doing anything wrong?</p>
","parsing, nlp, stanford-nlp","<p>CoreNLP outputs ""Collapsed dependencies preserving a tree structure"" (section 4.4 of the manual) from my experience. I think it's the same thing here (e.g. prep_with is a collapsed dependency of prep(saw, with))</p>
",1,0,330,2014-11-06 12:58:05,https://stackoverflow.com/questions/26780092/stanford-type-dependency-can-not-extract-prepositional-modfier
Can Stanford-NLP detect interrogative sentence,"<p>I have been doing some basic tweaking with Stanford-NLP for last few days. I have read about Link Grammar too. If anyone have used any of these libraries, is it possible to detect interrogative sentence using any of them? Currently I am stuck in Stanford-NLP. I might have to add a large sum of code for it to work. It looks like i might need to create a separate PCFG.</p>
","nlp, stanford-nlp, sharpnlp","<p>My advice: don't use a PCFG for that. Use Stanford NLP's excellent MaxEnt classifier to do binary classification interrogative / non interrogative. It will work really well. You have to prepare a corpus with a few dozen examples of each class at least, maybe more depending on your sentences.</p>
",5,3,1558,2014-11-07 09:34:48,https://stackoverflow.com/questions/26798165/can-stanford-nlp-detect-interrogative-sentence
stanford NER classification with additional classes,"<p>Current stanford NER gives mainly 6 classes <code>LOCATION,   TIME,  PERSON'  ORGANIZATION'  MONEY'  PERCENT'  DATE</code>
 Additionally it has been trained with English data so could not classify Indian entities.</p>

<p>Is it possible to train the classifier with additional classes so that it can also identify NE as <code>product, month, disease, device</code> etc.</p>

<p>Also it does not classify Indian entities, so support for such non-english classes too can also be added if this is possible.</p>

<p>Is it possible to retrain classifier, tagger for this additional support?</p>
","machine-learning, nlp, classification, stanford-nlp","<p>The major hassle for training the model over other classes is the training data. <br>
Models require highly accurate training data like <code>I brought a &lt;START:product&gt; Mac Book Pro &lt;END&gt; in September and synced it with my &lt;START:device&gt; IPhone &lt;END&gt;.</code> Observe that <code>Iphone</code> could be annotated with either device or product.<br>
If you can generate or annotate at least 15,000 sentences annotated with classes you wish to recognise [which is not easy]; you are good to go.<br>
Stanford NER models or OpenNLP NER models don't recognise Indian names because the models are trained on Wall Street journal articles and they are not representative of many names.  </p>
",1,0,679,2014-11-10 06:02:55,https://stackoverflow.com/questions/26837718/stanford-ner-classification-with-additional-classes
"Trouble running Java application implementing Stanford POS tagger as a .jar, runs fine in NetBeans IDE","<p>I am trying to implement Stanford's POS tagger in a Java application using the following lines of code:</p>

<pre><code>             MaxentTagger tagger = new MaxentTagger(""taggers/english-left3words-distsim.tagger"");
             String taggedString = tagger.tagString(string);
</code></pre>

<p>It works fine when running the project in NetBeans, but the executable .jar compiled in NetBeans will not run the tagger.  I am fairly new to Java and to programming in general, and I haven't figured out how to log whatever error/exception is occurring in the GUI console of the application, and no error/exception occurs when running the project in NetBeans.  </p>

<p>A couple things occurred to me that could be happening.  The Stanford NLP group says: </p>

<p>""The system requires Java 1.8+ to be installed. Depending on whether you're running 32 or 64 bit Java and the complexity of the tagger model, you'll need somewhere between 60 and 200 MB of memory to run a trained tagger (i.e., you may need to give java an option like java -mx200m). Plenty of memory is needed to train a tagger. It again depends on the complexity of the model but at least 1GB is usually needed, often more.""  <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tagger.shtml</a></p>

<p>In the shortcut that I use to run the application, I specified the path of a javaw.exe that is version 1.8+, and I also tried replacing the latest release of the tagger with the August 2014 release, pre-Java 8 upgrade, and neither solved the problem.     </p>

<p>As for the memory issue, I went to my Java control panel and entered ""-Xms1024m"" as a runtime parameter for every javaw.exe on my machine, and that also did not solve the problem.  </p>

<p>Can anybody suggest what might be making the .jar fail to run the tagger, when it works fine running the project from NetBeans?     </p>

<p>Thanks!</p>
","java, netbeans, stanford-nlp, pos-tagger","<p>peeskillet that worked, thanks (great name by the way)! Specifically, in the projects tab I right-clicked ""Source Packages"" > ""New"" > ""Java Package,"" called the new package ""taggers,"" and moved the .tagger files into the ""taggers"" directory under the ""src"" directory under the project root directory.</p>
",1,0,829,2014-11-10 19:24:20,https://stackoverflow.com/questions/26851372/trouble-running-java-application-implementing-stanford-pos-tagger-as-a-jar-run
Run custom trained stanford ner model as server,"<p>I trained my own model for NER using Stanford NER. I can interface to it from my python code using the following code and it annotates named entities in the input text correctly.</p>

<pre><code>st = NERTagger('stanford_classifiers/ner-model-custom.ser.gz',
               'stanford_classifiers/stanford-ner.jar', 'utf-8')
        entities = st.tag(input_text.split())
</code></pre>

<p>But since, it is too slow for each text input , I run the model as a server and access the result by querying from python using following code. But I get no output from my trained model. </p>

<p>I start the server like this :</p>

<pre><code>java -mx1000m -cp stanford-ner.jar edu.stanford.nlp.ie.NERServer -loadClassifier ner-m
odel-custom.ser.gz -port 8080 -outputFormat inlineXML
</code></pre>

<p>I get this message on server :</p>

<pre><code>Loading classifier from XX\XX\stanford_classifiers\ner-model-custom.ser.gz ... done [6.1 sec].
</code></pre>

<p>On the client side (python code) :</p>

<pre><code>tagger = ner.SocketNER(host='localhost', port=8080)
entities = tagger.get_entities(input_text)
</code></pre>

<p>The same code works when an existing model like english.all.3class.distsim.crf.ser.gz is started as a server. But not for the custom model in server mode.</p>

<p>Is there a specific way to start our custom trained ner models in server version ? </p>

<p>Could this be a problem with the training of the custom model ?</p>
","python, stanford-nlp","<p>Thank you @ChristopherManning for responding. You are right. the model is working fine. The custom model also annotated the input text in the inlineXML format correctly.</p>

<p>It just did not return the entities in the <code>tagger.get_entities(input_text)</code> function. I instead had to use another function in the python interface : <code>tagger.tag_text(input_text)</code> and then using an xml minidom parser to extract the annotated companies.</p>

<pre><code>Input text : ""She is working at facebook""

Received by tagger using tag_text function : ""She is working at &lt;B-company&gt;facebook&lt;/B-company&gt;""
</code></pre>

<p>Parsed this output to get the list of entities.</p>
",1,0,770,2014-11-13 22:12:17,https://stackoverflow.com/questions/26919427/run-custom-trained-stanford-ner-model-as-server
How to convert text file to CoNLL format for malt parser?,"<p>I'm trying to use malt parser with the pre made english model. However, I do not know how to convert a text corpus of English sentences into the CoNLL format that is necessary for Malt Parser to operate on. I could not find any documentation on the site. How should I go about this?</p>

<p>Update. I am referring to this post <a href=""https://stackoverflow.com/questions/17450652/create-conll-file-as-output-of-stanford-parser"">Create .conll file as output of Stanford Parser</a> to create a .conll. However, this is using Stanford Parser.</p>
","parsing, nlp, stanford-nlp, pos-tagger","<p>There is a CoNLL formatting option for CoreNLP output, but unfortunately it doesn't match what MaltParser expects. (Confusingly, there are several different common CoNLL data formats, for the different competition years..)</p>

<p>If you run CoreNLP from the command line with the option <code>-outputFormat conll</code>, you'll get output in the following TSV format (example output at end of answer):</p>

<pre><code>INDEX    WORD    LEMMA    POS    NER    DEPHEAD    DEPREL
</code></pre>

<p>MaltParser expects a bit different format, but you can customize the data input / output format. Try putting this content in <code>maltparser/appdata/dataformat/myconll.xml</code>:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;dataformat name=""myconll"" reader=""tab"" writer=""tab""&gt;
    &lt;column name=""ID"" category=""INPUT"" type=""INTEGER""/&gt;
    &lt;column name=""FORM"" category=""INPUT"" type=""STRING""/&gt;
    &lt;column name=""LEMMA"" category=""INPUT"" type=""STRING""/&gt;
    &lt;column name=""POSTAG"" category=""INPUT"" type=""STRING""/&gt;
    &lt;column name=""NER"" category=""IGNORE"" type=""STRING""/&gt;
    &lt;column name=""HEAD"" category=""HEAD"" type=""INTEGER""/&gt;
    &lt;column name=""DEPREL"" category=""DEPENDENCY_EDGE_LABEL"" type=""STRING""/&gt;
&lt;/dataformat&gt;
</code></pre>

<p>Then add to your MaltParser config file (find an example config in <code>maltparser/examples/optionexample.xml</code>):</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;experiment&gt;
    &lt;optioncontainer&gt;
...
        &lt;optiongroup groupname=""input""&gt;
            &lt;option name=""format"" value=""myconll""/&gt;
        &lt;/optiongroup&gt;
    &lt;/optioncontainer&gt;
...
&lt;/experiment&gt;
</code></pre>

<p>Then you should be able to provide CoreNLP CoNLL output as training data to MaltParser.</p>

<p>Untested, but if the MaltParser docs are honest, this should work. Sources:</p>

<ul>
<li><a href=""http://www.maltparser.org/userguide.html#inout"">MaltParser user guide: I/O</a></li>
<li><a href=""http://www.maltparser.org/optiondesc.html#input-format"">MaltParser option documentation</a></li>
</ul>

<hr>

<p>Example CoreNLP CoNLL output (I only used annotators <code>tokenize,ssplit,pos</code>):</p>

<pre><code>$ echo ""This is a test."" | java edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos -outputFormat conll 2&gt;/dev/null

1   This    this    DT  _   _   _
2   is  be  VBZ _   _   _
3   a   a   DT  _   _   _
4   test    test    NN  _   _   _
5   .   .   .   _   _   _
</code></pre>
",8,5,7583,2014-11-16 22:20:23,https://stackoverflow.com/questions/26962725/how-to-convert-text-file-to-conll-format-for-malt-parser
How could I convert messy Stanford Core NLP string object into JSON,"<p>The Ruby Stanford Core NLP gem returns messy objects.</p>

<p>Example 1:</p>

<pre><code> =&gt; ""[produced nsubjpass:[lbs amod:[many advmod:how] prep:[of pobj:CO2]] auxpass:is prep:[from pcomp:[burning dobj:[gallons num:12 prep:[of pobj:gas]]]]]""
</code></pre>

<p>Example 2:</p>

<pre><code> -&gt; produced-VBN (root)
   -&gt; lbs-NNS (nsubjpass)
     -&gt; many-JJ (amod)
       -&gt; how-WRB (advmod)
     -&gt; of-IN (prep)
       -&gt; CO2-NN (pobj)
   -&gt; is-VBZ (auxpass)
   -&gt; from-IN (prep)
     -&gt; burning-VBG (pcomp)
       -&gt; gallons-NNS (dobj)
         -&gt; 12-CD (num)
         -&gt; of-IN (prep)
           -&gt; gas-NN (pobj)
</code></pre>

<p>Example 3:</p>

<pre><code>  @sentence.to_s
=&gt; ""-&gt; produced-VBN (root)\n  -&gt; lbs-NNS (nsubjpass)\n    -&gt; many-JJ (amod)\n      -&gt; how-WRB (advmod)\n    -&gt; of-IN (prep)\n      -&gt; CO2-NN (pobj)\n  -&gt; is-VBZ (auxpass)\n  -&gt; from-IN (prep)\n    -&gt; burning-VBG (pcomp)\n      -&gt; gallons-NNS (dobj)\n        -&gt; 12-CD (num)\n        -&gt; of-IN (prep)\n          -&gt; gas-NN (pobj)\n""
</code></pre>

<p>How could I convert any of those objects into something like JSON using Ruby?</p>
","ruby, json, casting, stanford-nlp","<p>See the <a href=""https://github.com/louismullie/treat/blob/master/lib/treat/workers/processors/parsers/stanford.rb#L55"" rel=""nofollow"">following excerpt</a> for how to iterate the results and build the corresponding tree. I'll leave it as an exercise to convert the resulting tree into JSON format.</p>

<p>Note you could also skip ahead and use <a href=""https://github.com/louismullie/treat/"" rel=""nofollow"">Treat</a> directly to retrieve the parsed tree of the sentence.</p>
",0,1,429,2014-11-16 22:30:07,https://stackoverflow.com/questions/26962808/how-could-i-convert-messy-stanford-core-nlp-string-object-into-json
Online documentation explaining tags output by Stanford NLP parser?,"<p>is there any online documentation explaining tags output by Stanford NLP parser?</p>

<p>I'm quite new to NLP and and to me it seems that the tags like NN, VBZ, .. and relationships like poss, nsubj ... seem to follow a kind of standard since I've seen this output on other parsers.</p>

<p>thanks a lot!</p>
","parsing, nlp, stanford-nlp","<p>For grammatical dependencies (nsubj, poss...), you can read the official manual: <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"">http://nlp.stanford.edu/software/dependencies_manual.pdf</a></p>

<p>Tags like NN, VBZ... are part-of-speech tags. You can find info about them here: <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>, or by googling ""part-of-speech tags penn treebank""</p>
",5,0,276,2014-11-18 07:19:07,https://stackoverflow.com/questions/26988686/online-documentation-explaining-tags-output-by-stanford-nlp-parser
Meaning of Stanford Spanish POS Tagger tags,"<p>I am tagging Spanish text with the Stanford POS Tagger (via NLTK in Python).</p>

<p>Here is my code:</p>

<pre><code>import nltk
from nltk.tag.stanford import POSTagger
spanish_postagger = POSTagger('models/spanish.tagger', 'stanford-postagger.jar')
spanish_postagger.tag('esta es una oracion de prueba'.split())
</code></pre>

<p>The result is:</p>

<pre><code>[(u'esta', u'pd000000'),
(u'es', u'vsip000'),
(u'una', u'di0000'),
(u'oracion', u'nc0s000'),
(u'de', u'sp000'),
(u'prueba', u'nc0s000')]
</code></pre>

<p>I want to know where can I found what exactly means pd000000, vsip000, di0000, nc0s000, sp000?</p>
","python, stanford-nlp, text-mining","<p>This is a simplified version of the tagset used in the <strong><a href=""http://clic.ub.edu/corpus/en"" rel=""nofollow noreferrer"">AnCora treebank</a></strong>. You can find their tagset documentation here: <a href=""https://web.archive.org/web/20160325024315/http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html"" rel=""nofollow noreferrer"">https://web.archive.org/web/20160325024315/http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html</a></p>

<p>The ""simplification"" consists of nulling out many of the final fields which don't strictly belong in a part-of-speech tag. For example, our part-of-speech tagger will always give you null (<code>0</code>) values for the NER field of the original tagset (see <a href=""https://web.archive.org/web/20160325024315/http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html#nombres"" rel=""nofollow noreferrer"">EAGLES noun documentation</a>).</p>

<p>In short: <strong>the fields in the POS tags produced by our tagger correspond exactly to AnCora POS fields, but a lot of those fields will be null</strong>. For most practical purposes you'll only need to look at the first 2–4 characters of the tag. The first character always indicates the broad POS category, and the second character indicates some kind of subtype.</p>

<hr>

<p>We're in the process of writing some introductory documentation for using Spanish with CoreNLP (that means understanding these tags, and much else) right now. For the moment, you can find more information on the first page of our <a href=""https://docs.google.com/document/d/1lI-ie4-GGx2IA6RJNc0PMb3CHDoNQMUa0gj0eQEDYQ0/edit?usp=sharing"" rel=""nofollow noreferrer"">technical documentation</a>.</p>
",10,6,3795,2014-11-20 19:01:39,https://stackoverflow.com/questions/27047450/meaning-of-stanford-spanish-pos-tagger-tags
Stanford NER 3.4.1 questions,"<p>I downloaded the NER 3.4.1 (released on 08-27-14) to train specific domain of articles (highly technical).</p>

<p>Would like to know the following:</p>

<p>(1)   Possible to output offset on each extracted entity?</p>

<p>(2)    Possible to output the confidence score of each extracted entity?</p>

<p>(3)    I have trained more than one CRF models on NER3.4.1, it seems
 Stanford GUI is able to display a single CRF model only, is there any 
 way to display multiple CRF models instead of writing a wrapper?</p>
","java, stanford-nlp, text-extraction","<p>(1) Yes, absolutely. The tokens (class: CoreLabel) returned each store begin and end character offsets for each token. The easiest way to get at the offsets for whole entities is with the <code>classifyToCharacterOffsets()</code> method. See the example below.</p>

<p>(2) Yes, but there is some subtlety in interpreting these. That is, a lot of the uncertainty is often over not whether these three words should be a PERSON or an ORGANIZATION but whether the ORGANIZATION should be two words long or three words long, etc. Actually, the NER classifier is putting probabilities (really, unnormalized clique potentials) over the assignments of labels and label sequences at each point. There are various methods that you can use to interrogate these scores. I illustrate a couple of the simpler ones, where they are rendered as probabilities below.  If you want more and know how to interpret CRFs, you can get the CliqueTree for a sentence and do what you want with it. In practice, rather than doing any of this, often the easier thing to deal with is just a k-best list of labelings, each with a full sentence probability assigned. I also show that below.</p>

<p>(3) Sorry, not with the code that is there now. It's just a simple demo. If you wanted to extend it's functionality, you're welcome to. Happy to get code contributions back!</p>

<p>Below is an expanded version of <code>NERDemo.java</code> from the distribution, which illustrates some of these options.</p>

<pre><code>package edu.stanford.nlp.ie.demo;

import edu.stanford.nlp.ie.AbstractSequenceClassifier;
import edu.stanford.nlp.ie.crf.*;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.sequences.DocumentReaderAndWriter;
import edu.stanford.nlp.sequences.PlainTextDocumentReaderAndWriter;
import edu.stanford.nlp.util.Triple;

import java.util.List;


/** This is a demo of calling CRFClassifier programmatically.
 *  &lt;p&gt;
 *  Usage: {@code java -mx400m -cp ""stanford-ner.jar:."" NERDemo [serializedClassifier [fileName]] }
 *  &lt;p&gt;
 *  If arguments aren't specified, they default to
 *  classifiers/english.all.3class.distsim.crf.ser.gz and some hardcoded sample text.
 *  &lt;p&gt;
 *  To use CRFClassifier from the command line:
 *  &lt;/p&gt;&lt;blockquote&gt;
 *  {@code java -mx400m edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier [classifier] -textFile [file] }
 *  &lt;/blockquote&gt;&lt;p&gt;
 *  Or if the file is already tokenized and one word per line, perhaps in
 *  a tab-separated value format with extra columns for part-of-speech tag,
 *  etc., use the version below (note the 's' instead of the 'x'):
 *  &lt;/p&gt;&lt;blockquote&gt;
 *  {@code java -mx400m edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier [classifier] -testFile [file] }
 *  &lt;/blockquote&gt;
 *
 *  @author Jenny Finkel
 *  @author Christopher Manning
 */

public class NERDemo {

  public static void main(String[] args) throws Exception {

    String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";

    if (args.length &gt; 0) {
      serializedClassifier = args[0];
    }

    AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifier(serializedClassifier);

    /* For either a file to annotate or for the hardcoded text example, this
       demo file shows several ways to process the input, for teaching purposes.
    */

    if (args.length &gt; 1) {

      /* For the file, it shows (1) how to run NER on a String, (2) how
         to get the entities in the String with character offsets, and
         (3) how to run NER on a whole file (without loading it into a String).
      */

      String fileContents = IOUtils.slurpFile(args[1]);
      List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(fileContents);
      for (List&lt;CoreLabel&gt; sentence : out) {
        for (CoreLabel word : sentence) {
          System.out.print(word.word() + '/' + word.get(CoreAnnotations.AnswerAnnotation.class) + ' ');
        }
        System.out.println();
      }

      System.out.println(""---"");
      out = classifier.classifyFile(args[1]);
      for (List&lt;CoreLabel&gt; sentence : out) {
        for (CoreLabel word : sentence) {
          System.out.print(word.word() + '/' + word.get(CoreAnnotations.AnswerAnnotation.class) + ' ');
        }
        System.out.println();
      }

      System.out.println(""---"");
      List&lt;Triple&lt;String, Integer, Integer&gt;&gt; list = classifier.classifyToCharacterOffsets(fileContents);
      for (Triple&lt;String, Integer, Integer&gt; item : list) {
        System.out.println(item.first() + "": "" + fileContents.substring(item.second(), item.third()));
      }
      System.out.println(""---"");
      System.out.println(""Ten best"");
      DocumentReaderAndWriter&lt;CoreLabel&gt; readerAndWriter = classifier.makePlainTextReaderAndWriter();
      classifier.classifyAndWriteAnswersKBest(args[1], 10, readerAndWriter);

      System.out.println(""---"");
      System.out.println(""Probabilities"");
      classifier.printProbs(args[1], readerAndWriter);


      System.out.println(""---"");
      System.out.println(""First Order Clique Probabilities"");
      ((CRFClassifier) classifier).printFirstOrderProbs(args[1], readerAndWriter);

    } else {

      /* For the hard-coded String, it shows how to run it on a single
         sentence, and how to do this and produce several formats, including
         slash tags and an inline XML output format. It also shows the full
         contents of the {@code CoreLabel}s that are constructed by the
         classifier. And it shows getting out the probabilities of different
         assignments and an n-best list of classifications with probabilities.
      */

      String[] example = {""Good afternoon Rajat Raina, how are you today?"",
                          ""I go to school at Stanford University, which is located in California."" };
      for (String str : example) {
        System.out.println(classifier.classifyToString(str));
      }
      System.out.println(""---"");

      for (String str : example) {
        // This one puts in spaces and newlines between tokens, so just print not println.
        System.out.print(classifier.classifyToString(str, ""slashTags"", false));
      }
      System.out.println(""---"");

      for (String str : example) {
        System.out.println(classifier.classifyWithInlineXML(str));
      }
      System.out.println(""---"");

      for (String str : example) {
        System.out.println(classifier.classifyToString(str, ""xml"", true));
      }
      System.out.println(""---"");

      int i=0;
      for (String str : example) {
        for (List&lt;CoreLabel&gt; lcl : classifier.classify(str)) {
          for (CoreLabel cl : lcl) {
            System.out.print(i++ + "": "");
            System.out.println(cl.toShorterString());
          }
        }
      }

      System.out.println(""---"");

    }
  }

}
</code></pre>
",4,1,1043,2014-11-25 20:52:34,https://stackoverflow.com/questions/27136472/stanford-ner-3-4-1-questions
Retraining stanford ner - new entities and multi word entities,"<p>I am retraining the Stanford NER system to extract Technology names and Organization names from Text. </p>

<p>If I want to retrain the stanford ner model, we should give the training data in the format:</p>

<pre><code>She     O
works    O
on O
C#     TECHNOLOGY
at O
New   ORGANIZATION
York  ORGANIZATION
TImes   ORGANIZATION
and O
Microsoft ORGANIZATION
in    O
New     LOCATION
York     LOCATION
</code></pre>

<p>Is it sufficient to just specify the named entities in this manner ? Do we need to specify the part of speech information in some format when we retrain a model ? Also, if we have entities that are multi word, then is this the correct way to annotate them ? </p>

<p>This is the approach I followed : Is the approach right ?</p>

<ol>
<li>Generate training data</li>
<li>Add new model name and location in properties file</li>
<li><p>Used this command from the FAQs of stanford ner : </p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre></li>
</ol>
","java, stanford-nlp","<p>Yes, it is sufficient to just annotate entities as shown. Stanford NER also supports training classifiers that use POS information at classification time, but it adds very little in accuracy when other techniques like distributional similarity word clusters are used. IN the models we distribute, we don't use POS information, for simplicity (and so NER can be run alone without the POS tagger).</p>

<p>For annotating multiple word entities, there are several strategies. The above encoding is sometimes called ""IO"" (inside outside). It has the advantages of being simple to interpret, and fast, but the disadvantage of not allowing two adjacent entities of the same class to be distinguished -- you have to assume that a run of words of the same category comprise one big entity. We use it by default, because it is simple and fast, despite that disadvantage. (Adjacent entities of the same category occur very rarely for person/organization/location, but they can be much more common in some other domains.)</p>

<p>But you can also annotate data and train a model using other encoding schemes such as IOB, which has the opposite properties (more complicated, tagger runs slower, can represent adjacent entities of the same category). See <a href=""https://stackoverflow.com/questions/21469082/how-do-i-use-iob-tags-with-stanford-ner"">this SO question</a> for details.</p>
",1,2,1848,2014-11-26 06:03:49,https://stackoverflow.com/questions/27142362/retraining-stanford-ner-new-entities-and-multi-word-entities
"Stanford NLP Sentiment - how to check if a word is recognized, is in the vocabulary?","<p>Is there a function that returns true if a word is recognized by Stanford NLP Sentiment and false if not?</p>

<p>For example, if I want to find the sentiment of the sentence: 
""I like AAA because of BBB."" 
both the phrases (AAA) and (BBB) will not be recognized and therefore get the same phrase vector (which will effect the results).
I would like to avoid that.</p>
","dictionary, stanford-nlp, words, vocabulary","<p>If you have a <code>SentimentModel</code> instance, you can inspect its public member <code>wordVectors</code> (not sure why this is public, but I suppose that's a different story..).</p>

<pre><code>SentimentModel model = SentimentModel.loadSerialized(""edu/stanford/nlp/models/sentiment/sentiment.ser.gz"");
boolean knownWord = model.wordVectors.containsKey(""foo"");
</code></pre>
",2,0,616,2014-11-27 14:37:00,https://stackoverflow.com/questions/27173112/stanford-nlp-sentiment-how-to-check-if-a-word-is-recognized-is-in-the-vocabul
Stanford tagger not working,"<p>I have tried using stanford pos tagger in nltk but it gives me error as:</p>

<pre><code>from nltk.tag.stanford import POSTagger
st = POSTagger('/.../models/english-bidirectional-distsim.tagger', '/.../stanford-postagger-full-2014-10-26/stanford-postagger.jar')
st.tag(""dogs and cats"".split())
</code></pre>

<blockquote>
  <p>Exception in thread ""main"" java.lang.UnsupportedClassVersionError:
  edu/stanford/nlp/tagger/maxent/MaxentTagger : Unsupported major.minor
  version 52.0
          at java.lang.ClassLoader.defineClass1(Native Method)
          at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
          at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
          at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
          at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
          at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
          at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
          at java.security.AccessController.doPrivileged(Native Method)
          at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
          at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
          at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
          at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
          at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)</p>
  
  <p>--------------------------------------------------------------------------- OSError                                   Traceback (most recent call
  last)  in ()
  ----> 1 st.tag(""love myself"".split())</p>
  
  <p>/Users/bowang/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.pyc
  in tag(self, tokens)
       57 
       58     def tag(self, tokens):
  ---> 59         return self.tag_sents([tokens])[0]
       60 
       61     def tag_sents(self, sentences):</p>
  
  <p>/Users/bowang/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.pyc
  in tag_sents(self, sentences)
       79         # Run the tagger and get the output
       80         stanpos_output, _stderr = java(self._cmd,classpath=self._stanford_jar,
  ---> 81                                                        stdout=PIPE, stderr=PIPE)
       82         stanpos_output = stanpos_output.decode(encoding)
       83 </p>
  
  <p>/Users/bowang/anaconda/lib/python2.7/site-packages/nltk/<strong>init</strong>.pyc
  in java(cmd, classpath, stdin, stdout, stderr, blocking)
      158     if p.returncode != 0:
      159         print(stderr.decode(sys.stdout.encoding))
  --> 160         raise OSError('Java command failed!')
      161 
      162     return (stdout, stderr)</p>
  
  <p>OSError: Java command failed!</p>
</blockquote>

<p>I have tried with what <a href=""https://stackoverflow.com/questions/27171298/nltk-stanford-pos-tagger-error-java-command-failed"">link</a> answered (apart from uninstalling and reinstalling nltk) but still not working. Also I have tried running stanford-tagger.jar from the command line to process my data and it gives the same error, as:</p>

<blockquote>
  <p>Exception in thread ""main"" java.lang.UnsupportedClassVersionError:
  edu/stanford/nlp/tagger/maxent/MaxentTaggerGUI : Unsupported
  major.minor version 52.0</p>
</blockquote>

<p>Has anyone had and solved this problem successfully before? Any help is greatly appreciated! Thanks!</p>
","java, python, nltk, stanford-nlp","<p>The latest version of the Stanford Tagger requires Java 8. If you aren't able to upgrade, use an older version of the tagger.</p>
",11,1,4216,2014-11-30 17:27:02,https://stackoverflow.com/questions/27216038/stanford-tagger-not-working
"How to split two or more sentences connected using conjunctions like(and,or..etc) into two separate sentences?","<p>I am new to stanford.I tried to split two or more independent sentences which are connected with conjunctions like (and,or..etc) into separate single sentences using stanford tree parser.</p>

<p>Example sentence - Lion and tiger are chasing a deer and fox is chasing a rabbit. I want split this for two independent sentences shown below.</p>

<p>1) Lion and tiger are chasing a deer.</p>

<p>2) fox is chasing a rabbit.</p>

<p>sentences should only have to break with conjunctions which connected <strong>independent sentences</strong>. <strong>Not</strong> for <strong>two subjects</strong>(Lion and tiger) or <strong>two objects</strong> connected with ""and"". If anyone knows please help me.</p>

<p>Parser tree structure </p>

<pre><code>(ROOT
  (S
    (NP (NNP Lion)
      (CC and)
      (NNP tiger))
    (VP (VBP are)
      (VP (VBG chasing)
        (SBAR
          (S
            (NP (DT a) (NNS deer)
              (CC and)
              (NN fox))
            (VP (VBZ is)
              (VP (VBG chasing)
                (NP (DT a) (NN rabbit))))))))))
</code></pre>

<p>Thanks.</p>
","java, regex, stanford-nlp","<p>This parse looks incorrect — did this come from the Stanford Parser? When I input the same sentence on the <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">parser demo page</a>, I get the following:</p>

<pre><code>(ROOT
  (S
    (S
      (NP (NNP Lion)
        (CC and)
        (NNP tiger))
      (VP (VBP are)
        (VP (VBG chasing)
          (NP (DT a) (NNS deer)))))
    (CC and)
    (S
      (NP (NN fox))
      (VP (VBZ is)
        (VP (VBG chasing)
          (NP (DT a) (NN rabbit)))))
    (. .)))
</code></pre>

<p>With this parse, extracting the two independent clauses would be fairly easy. You can use <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/tregex/TregexPattern.html"" rel=""nofollow"">Tregex</a> (also part of CoreNLP) to search for sibling clauses (<code>S</code> constituents) with intervening conjunctions (<code>CC</code> nodes).</p>
",1,0,2242,2014-12-01 15:45:10,https://stackoverflow.com/questions/27231944/how-to-split-two-or-more-sentences-connected-using-conjunctions-likeand-or-etc
Stanford NER: Can I use two classifiers at once in my code?,"<p>In my code, I get the <strong>Person</strong> recognition from the first classifier, and for the second one which I made, I added some words to be recognized or annotated as <strong>Organization</strong> but it does not annotate <strong>Person</strong>.</p>

<p>I need to get the benefit from the two of them, how can I do that?</p>

<p>I'm using Netbeans, and this is the code:</p>

<pre><code>String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";
String serializedClassifier2 = ""/Users/ha/stanford-ner-2014-10-26/classifiers/dept-model.ser.gz"";

if (args.length &gt; 0) {
  serializedClassifier = args[0];
}

AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifier(serializedClassifier);
AbstractSequenceClassifier&lt;CoreLabel&gt; classifier2 = CRFClassifier.getClassifier(serializedClassifier2);

  String fileContents = IOUtils.slurpFile(""/Users/ha/NetBeansProjects/NERtry/src/nertry/input.txt"");
  List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(fileContents);
  List&lt;List&lt;CoreLabel&gt;&gt; out2 = classifier2.classify(fileContents);

  for (List&lt;CoreLabel&gt; sentence : out) {
      System.out.print(""\nenglish.all.3class.distsim.crf.ser.gz: "");
    for (CoreLabel word : sentence) {
      System.out.print(word.word() + '/' + word.get(CoreAnnotations.AnswerAnnotation.class) + ' ');
    }

  for (List&lt;CoreLabel&gt; sentence2 : out2) {
      System.out.print(""\ndept-model.ser.gz"");
    for (CoreLabel word2 : sentence2) {
      System.out.print(word2.word() + '/' + word2.get(CoreAnnotations.AnswerAnnotation.class) + ' ');
    }

    System.out.println();
  }
}
</code></pre>

<p>The problem comes from the result I get:</p>

<pre><code>english.all.3class.distsim.crf.ser.gz: What/O date/O did/O James/PERSON started/O his/O job/O in/O Human/O and/O Finance/O ?/O 
dept-model.ser.gzWhat/O date/O did/O James/ORGANIZATION started/O his/O job/O in/O Human/ORGANIZATION and/O Finance/ORGANIZATION ?/O 
</code></pre>

<p>where it recognize the names as organization from the second classifier, and I need it to be annotated as PERSON. 
Any help?</p>
","netbeans, classification, stanford-nlp","<p>The class you should use to make this easy is <code>NERClassifierCombiner</code>. Its semantics is that it runs the classifiers in order from left to right as you specify them (any number can be given to it in the constructor), and that later classifiers cannot annotate an entity that overlaps with an entity tagging of an earlier classifier, but are otherwise free to add annotations. So, earlier classifiers are preferred in a simple preference ranking. I give a complete code example below.</p>

<p>(If you are training all your own classifiers, it is generally best to train all the entities together, so they can influence each other in the categories assigned. But this simple preference ordering usually works pretty well, and we use it ourselves.)</p>

<pre><code>import edu.stanford.nlp.ie.NERClassifierCombiner;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreLabel;

import java.io.IOException;
import java.util.List;

public class MultipleNERs {

  public static void main(String[] args) throws IOException {
    String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";
    String serializedClassifier2 = ""classifiers/english.muc.7class.distsim.crf.ser.gz"";

    if (args.length &gt; 0) {
      serializedClassifier = args[0];
    }

    NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, 
            serializedClassifier, serializedClassifier2);

    String fileContents = IOUtils.slurpFile(""input.txt"");
    List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(fileContents);

    int i = 0;
    for (List&lt;CoreLabel&gt; lcl : out) {
      i++;
      int j = 0;
      for (CoreLabel cl : lcl) {
        j++;
        System.out.printf(""%d:%d: %s%n"", i, j,
                cl.toShorterString(""Text"", ""CharacterOffsetBegin"", ""CharacterOffsetEnd"", ""NamedEntityTag""));
      }
    }
  }

}
</code></pre>
",8,2,886,2014-12-02 19:38:49,https://stackoverflow.com/questions/27257554/stanford-ner-can-i-use-two-classifiers-at-once-in-my-code
How to train a Chinese segmenter model by Stanford NLP Tools,"<p>I am new to the Stanford CoreNLP Tools. Now I do not get an excellent segment result in Chinese, so I want to change the granulity of the Segmenter. I thought I could do this by training my own dict. </p>

<p>I downloaded the <a href=""http://nlp.stanford.edu/software/trainSegmenter-20080521.tar.gz"" rel=""nofollow"" title=""trainSegmenter-20080521.tar.gz""><code>trainSegmenter-20080521</code></a> file, and follow the <code>trainSegmenter-20080521/README.txt</code>.</p>

<p>This is the README.txt:</p>

<pre><code>Sat Jun 21 00:57:22 2008
Author: Pi-Chuan Chang

Here's a documentation of how to train and test the segmenter on specific split 
range of the CTB data.

The following steps assumes you have 3 files defining the ranges of train/dev/test.
They should be named as ""ctb6.train"", ""ctb6.dev"", ""ctb6.test"" respectively.
The format should be like:
      chtb_0003.fid
      chtb_0015.fid
      ...

[STEP 1] change the CTB6 path in the Makefile:
      CTB6=/afs/ir/data/linguistic-data/Chinese-Treebank/6/

[STEP 2] download and uncompress the lastest segmenter from:
      http://nlp.stanford.edu/software/stanford-chinese-segmenter-2008-05-21.tar.gz
and change this path in the Makefile to your local path:
      SEGMENTER=/tmp/stanford-chinese-segmenter-2008-05-21/

[STEP 3] simply type:
      make all
You can also split down into these sub-steps:
      make internaldict # make internal dictionaries for affixation feaetures
      make data         # make datasets
      make traintest    # train &amp; test the CRF segmenter
</code></pre>

<p>But I still have some problems:</p>

<ol>
<li><p>What is the format of the training file and what is <code>train/dev/test</code> each for?</p></li>
<li><p>What is the <code>chtb_0003.fid</code>, <code>chtb_0015.fid</code> and so on?</p></li>
<li><p>What is the <code>CTB6 path</code> in the Makefile, it seems that I shoud change the variable <code>CTB6</code> into <code>/afs/ir/data/linguistic-data/Chinese-Treebank/6/</code>. But it is aready there and it seems not a valid subpath.</p></li>
</ol>

<p>By the way, there are many properties should be set for special demands, e.g., <code>sighanPostProcessing</code> and <code>serDictionary</code>. </p>

<p>Is there somewhere I can get all of those properties and their explanation?</p>

<p>All I can do now is to read the source code, e.g., <code>edu.stanford.nlp.sequences.SeqClassifierFlags.java</code>, but I still get confused with these property flags.</p>

<p>So appreciated for anyone's help.</p>
","java, nlp, classification, stanford-nlp","<p>I would ignore that README.  The information in it is fairly out of date.</p>

<p>A more recent explanation is here:</p>

<p><a href=""http://nlp.stanford.edu/software/segmenter-faq.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/segmenter-faq.shtml</a></p>

<p>The expected input format is one sentence per line with already segmented text on each line.  If you get your segmented data from parse trees, there are tools which will convert from parse tree to segmented text.</p>

<p>If there are particular sentences which aren't segmented correctly, it may be because it is using the CTB segmentation standard and you would prefer something different.  It may also be because of words that the segmenter doesn't know about.  If you send example sentences which follow the CTB segmentation standard to java-nlp-user, those unknown words will eventually make their way into the segmenter's training data.</p>
",2,1,1133,2014-12-05 12:49:10,https://stackoverflow.com/questions/27316268/how-to-train-a-chinese-segmenter-model-by-stanford-nlp-tools
Stanford_CoreNLP plugin for gate,"<p>I'm searching a <code>Stanford_CoreNLP plugin</code> with <code>Stanford NER</code>(not <code>StanfordParser</code> or <code>StandfordPOSTagger</code>) for <code>GATE</code> (General Architecture for Text Engineering). I found some information about the plugin <a href=""https://gate.ac.uk/gate/doc/plugins.html#Stanford_CoreNLP"" rel=""nofollow"">here</a>. But I couldn't find it integrated with <code>GATE</code> (version 8) by default. I also tried to find a link to download the plugin, but couldn't find...</p>

<p>Does anyone has a clue about how to activate it or from where to download it?</p>

<p>Thank you in advance...</p>
","nlp, stanford-nlp, gate","<p>As Jon mentioned, somehow the Stanford_CoreNLP plugin was left out of the most recent release package of GATE. However, it is included in the daily snapshots built by their Jenkins server. You can download those here:</p>

<p><a href=""http://jenkins.gate.ac.uk/job/GATE-Nightly/lastSuccessfulBuild/"" rel=""nofollow"">http://jenkins.gate.ac.uk/job/GATE-Nightly/lastSuccessfulBuild/</a></p>

<p>Unfortunately, there is no pre-built .gapp file for Stanford NER included with the GATE plugin. This means it isn't as simple as loading an application file to run Stanford NER inside GATE -- there's quite a bit more configuration involved. You might be able to build a custom .gapp file of your own, but in the meantime, the NER.java file in the source code for the Stanford NER plugin will help you get started running it inside GATE:</p>

<p><a href=""http://sourceforge.net/p/gate/code/HEAD/tree/gate/trunk/plugins/Stanford_CoreNLP/src/gate/stanford/NER.java"" rel=""nofollow"">http://sourceforge.net/p/gate/code/HEAD/tree/gate/trunk/plugins/Stanford_CoreNLP/src/gate/stanford/NER.java</a></p>
",2,1,282,2014-12-11 13:15:02,https://stackoverflow.com/questions/27423778/stanford-corenlp-plugin-for-gate
Stanford NER Error: Loading distsim lexicon Failed,"<p>In my project. I need to use NER annotation so I used NERDemo.java
It works fine when I create a new project and have only this code, but when I add it to my project I keep getting errors. I have edited the path in my code to the specific location of the classifiers.</p>

<p>I added the Jar files:</p>

<p><img src=""https://i.sstatic.net/23CCB.png"" alt=""enter image description here""></p>

<p>This is the code: </p>

<pre><code>String serializedClassifier = ""/Users/ha/stanford-ner-2014-10-26/classifiers/english.all.3class.distsim.crf.ser.gz"";
    String serializedClassifier2 = ""/Users/ha/stanford-ner-2014-10-26/classifiers/english.muc.7class.distsim.crf.ser.gz"";

    if (args.length &gt; 0) {
      serializedClassifier = args[0];
    }

    NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, serializedClassifier, serializedClassifier2);

    String fileContents = IOUtils.slurpFile(""/Users/ha/NetBeansProjects/StanfordPOSCode/src/stanfordposcode/input.txt"");
    List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(fileContents);

    int i = 0;
    for (List&lt;CoreLabel&gt; lcl : out) {
      i++;
      int j = 0;
      for (CoreLabel cl : lcl) {
        j++;
        System.out.printf(""%d:%d: %s%n"", i, j,
                cl.toShorterString(""Text"", ""CharacterOffsetBegin"", ""CharacterOffsetEnd"", ""NamedEntityTag""));
      }
    }
</code></pre>

<p>But I got this error:</p>

<pre><code>run:
Loading classifier from /Users/ha/stanford-ner-2014-10-26/classifiers/english.all.3class.distsim.crf.ser.gz ... Loading distsim lexicon from /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters ... java.lang.RuntimeException: java.io.FileNotFoundException: /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters (No such file or directory)
    at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setNextObject(ReaderIteratorFactory.java:225)
    at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.&lt;init&gt;(ReaderIteratorFactory.java:161)
    at edu.stanford.nlp.objectbank.ReaderIteratorFactory.iterator(ReaderIteratorFactory.java:98)
    at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.&lt;init&gt;(ObjectBank.java:404)
    at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:242)
    at edu.stanford.nlp.ie.NERFeatureFactory.initLexicon(NERFeatureFactory.java:471)
    at edu.stanford.nlp.ie.NERFeatureFactory.init(NERFeatureFactory.java:379)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.reinit(AbstractSequenceClassifier.java:171)
    at edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2630)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1620)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1736)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1679)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1662)
    at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2851)
    at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:189)
    at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:173)
    at edu.stanford.nlp.ie.ClassifierCombiner.&lt;init&gt;(ClassifierCombiner.java:125)
    at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:52)
    at stanfordposcode.MultipleNERs.main(MultipleNERs.java:24)
Caused by: java.io.FileNotFoundException: /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:131)
    at edu.stanford.nlp.io.EncodingFileReader.&lt;init&gt;(EncodingFileReader.java:78)
    at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setNextObject(ReaderIteratorFactory.java:192)
    ... 18 more
Loading classifier from /Users/ha/stanford-ner-2014-10-26/classifiers/english.all.3class.distsim.crf.ser.gz ... Exception in thread ""main"" java.io.FileNotFoundException
    at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:199)
    at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:173)
    at edu.stanford.nlp.ie.ClassifierCombiner.&lt;init&gt;(ClassifierCombiner.java:125)
    at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:52)
    at stanfordposcode.MultipleNERs.main(MultipleNERs.java:24)
Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to edu.stanford.nlp.classify.LinearClassifier
    at edu.stanford.nlp.ie.ner.CMMClassifier.loadClassifier(CMMClassifier.java:1070)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1620)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1736)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1679)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1662)
    at edu.stanford.nlp.ie.ner.CMMClassifier.getClassifier(CMMClassifier.java:1116)
    at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:195)
    ... 4 more
Java Result: 1
BUILD SUCCESSFUL (total time: 1 second)
</code></pre>
","java, ide, stanford-nlp","<p>You are mixing and matching the code from version 3.4 and the models from version 3.5.  I suggest upgrading everything to the latest version.</p>
",1,0,651,2014-12-15 11:03:09,https://stackoverflow.com/questions/27482591/stanford-ner-error-loading-distsim-lexicon-failed
Can I choose a pos.model in Stanford parser?,"<p>I want to use gate-EN-twitter.model for pos tagging when in the process of parsing by Stanford parser. Is there an option on command line that does that? like <code>-pos.model gate-EN-twitter.model</code>? Or do I have to use Stanford pos tagger with gate model for tagging first then use its output as input for the parser?</p>

<p>Thanks!</p>
",stanford-nlp,"<p>If I understand you correctly, you want to force the Stanford Parser to use the tags generated by this Twitter-specific POS tagger. That's definitely possible, though this tweet from Stanford NLP about this exact model should serve as a warning:</p>

<blockquote>
  <p><strong>Tweet from Stanford NLP, 13 Apr 2014:</strong></p>
  
  <p>Using CoreNLP on social media? Try GATE Twitter model <strong>(iff not parsing…)</strong> <code>-pos.model gate-EN-twitter.model</code> <a href=""https://gate.ac.uk/wiki/twitter-postagger.html"" rel=""nofollow"">https://gate.ac.uk/wiki/twitter-postagger.html</a> #nlproc</p>
  
  <p>(<a href=""https://twitter.com/stanfordnlp/status/455409761492549632"" rel=""nofollow"">https://twitter.com/stanfordnlp/status/455409761492549632</a>)</p>
</blockquote>

<p>That being said, if you really want to try, we can't stop you :)</p>

<p>There is a <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#f"" rel=""nofollow"">parser FAQ entry</a> on forcing in your own tags. See <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#f"" rel=""nofollow"">http://nlp.stanford.edu/software/parser-faq.shtml#f</a></p>

<p>Basically, you have two options (see the FAQ for full details):</p>

<ul>
<li>If calling the parser from the command line, you can pre-tag your text file and then alert the parser to the fact that the text is pre-tagged using some command-line options.</li>
<li>If parsing programmatically, the <code>LexicalizedParser#parse</code> method will accept any <code>List&lt;? extends HasTag&gt;</code> and treat the tags in that list as golden. Just pre-tag your list (using the CoreNLP pipeline or <code>MaxentTagger</code>) and pass on that token list to the parser.</li>
</ul>
",1,1,471,2014-12-15 16:00:05,https://stackoverflow.com/questions/27488093/can-i-choose-a-pos-model-in-stanford-parser
Stanford NLP pos tag PRN,"<p>I am using Stanford's CoreNLP pipeline. 
I am getting following TreeAnnotation:</p>

<pre><code>(ROOT (S (NP (PRP I)) (VP (VP (VBP love) (NP (NN hepling))) (PRN (CC and) (ADJP (JJ willing) (S (VP (TO to) (VP (VB help)))))))))
</code></pre>

<p>Note: the sentence is grammatically incorrect, but this is intensional.</p>

<p><strong>What is ""PRN"" tag in TreeAnnotation?</strong></p>

<p>I did not find such tag in <a href=""http://www.ling.upenn.edu/courses/Fall_2007/ling001/penn_treebank_pos.html"" rel=""nofollow"">Penn tags</a> nor in <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow"">Stanford dependency tags</a>.</p>
","nlp, stanford-nlp","<p>The labels can be over individual words (part of speech tags) or phrases (phrasal categories). <code>PRN</code> is a phrasal category for parenthetical phrases.</p>

<p>See <a href=""https://stackoverflow.com/questions/7853295/what-do-the-abbreviations-in-pos-tagging-etc-mean/7853420#7853420"">What do the abbreviations in POS tagging etc mean?</a> which will point you to a <a href=""http://web.archive.org/web/20130517134339/http://bulba.sdsu.edu/jeanette/thesis/PennTags.html"" rel=""nofollow noreferrer"">full list</a> of phrasal and part-of-speech tags.</p>
",1,0,660,2014-12-15 17:32:54,https://stackoverflow.com/questions/27489715/stanford-nlp-pos-tag-prn
Java: Convert an English verb to a particular tense,"<p>Is there a way to convert an English verb to a particular tense with java?</p>

<p>for example:</p>

<pre><code>convertToPast(""go""); //will give went
convertToCtn(""go""); //will give going
</code></pre>

<p>I have used Stanford nlp and WordNet with JAWS.</p>
","java, nlp, stanford-nlp, wordnet, jaws-wordnet","<p>have a look at <a href=""https://code.google.com/p/simplenlg/"" rel=""nofollow noreferrer"">this library</a></p>

<p>for more details refer this <a href=""https://stackoverflow.com/questions/9520501/how-do-you-get-the-past-tense-of-a-verb"">question</a></p>
",1,1,3209,2014-12-17 04:33:23,https://stackoverflow.com/questions/27518386/java-convert-an-english-verb-to-a-particular-tense
Stanford Tokenizer NullPointerException,"<p>I was just struck with an odd exception from the entrails of StanfordNLP, when trying to tokenize:</p>

<blockquote>
  <p>java.lang.NullPointerException    at
  edu.stanford.nlp.process.PTBLexer.zzRefill(PTBLexer.java:24511)   at
  edu.stanford.nlp.process.PTBLexer.next(PTBLexer.java:24718)   at
  edu.stanford.nlp.process.PTBTokenizer.getNext(PTBTokenizer.java:276)
    at
  edu.stanford.nlp.process.PTBTokenizer.getNext(PTBTokenizer.java:163)
    at
  edu.stanford.nlp.process.AbstractTokenizer.hasNext(AbstractTokenizer.java:55)
    at
  edu.stanford.nlp.process.DocumentPreprocessor$PlainTextIterator.primeNext(DocumentPreprocessor.java:270)
    at
  edu.stanford.nlp.process.DocumentPreprocessor$PlainTextIterator.hasNext(DocumentPreprocessor.java:334)</p>
</blockquote>

<p>The code that cause it looks like this:</p>

<blockquote>
<pre><code>  DocumentPreprocessor dp = new DocumentPreprocessor(new StringReader(
            tweet));

    // unigrams
    for (List&lt;HasWord&gt; sentence : dp) {
        for (HasWord word : sentence) {
            // do stuff
        }
    }

    // bigrams
    for (List&lt;HasWord&gt; sentence : dp) { //&lt;&lt; exception is thrown here
        Iterator&lt;HasWord&gt; it = sentence.iterator();
        String st1 = it.next().word();
        while (it.hasNext()) {
            String st2 = it.next().word();
            String bigram = st1 + "" "" + st2;
            // do stuff
            st1 = st2;
        }
    }
</code></pre>
</blockquote>

<p>What is going on? Has this to do with me looping over the tokens twice?</p>
","java, stanford-nlp","<p>This is certainly an ugly stacktrace, which can and should be improved. (I'm about to check in a fix for that.) But the reason that this doesn't work is that a DocumentProcessor acts like a Reader: It only lets you make a single pass through the sentences of a document. So after the first for-loop, the document is exhausted, and the underlying Reader has been closed. Hence the second for-loop fails, and here crashes out deep in the lexer. I'm going to change it so that it just will give you nothing.  But to get what you want you either want to (most efficient) get both the unigrams and bigrams in one for-loop pass through the document or to create a second DocumentPreprocessor for the second pass.</p>
",1,0,170,2014-12-18 22:04:41,https://stackoverflow.com/questions/27556511/stanford-tokenizer-nullpointerexception
Stanford CoreNLP get all edges of a SemanticGraph,"<p>I am trying to get a list of all the edges in a given SemanticGraph.<br>
There is <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html#getAllEdges-edu.stanford.nlp.ling.IndexedWord-edu.stanford.nlp.ling.IndexedWord-"" rel=""nofollow"">""getAllEdges(IndexedWord gov, IndexedWord dep)"" method</a> but it seems to only work on a pair of 2 Indexed words, not the whole graph.<br>
Is there another way to get all the edges without traversing the whole graph manually?</p>
",stanford-nlp,"<p>You can try:</p>

<pre><code>SemanticGraph parse = ...
for (SemanticGraphEdge edge : parse.edgeIterable()) {
  ...
}
</code></pre>

<p>There are also parallels of this in <code>outgoingEdgeIterable</code> and <code>incomingEdgeIterable</code>.</p>
",2,1,646,2014-12-19 19:32:39,https://stackoverflow.com/questions/27572744/stanford-corenlp-get-all-edges-of-a-semanticgraph
How to get the stanford NER plugin working with GATE?,"<p>I am new to GATE (version 8.0) and trying to get the Stanford NER working as part of the processing pipeline. </p>

<p>If I just do a search for Stanford in the plugins I don't see it:</p>

<p><img src=""https://i.sstatic.net/6WlSj.png"" alt=""enter image description here""></p>

<p>I'm finding conflicting information online about how to add it in. </p>

<p>I know that there is <a href=""http://sourceforge.net/p/gate/code/HEAD/tree/gate/trunk/plugins/Stanford_CoreNLP/src/gate/stanford/NER.java"" rel=""nofollow noreferrer"">this</a> plugin file. I also see that there is a StanfordNER plugin <a href=""https://gate.ac.uk/sale/tao/splitch23.html#sec:misc:creole:stanford"" rel=""nofollow noreferrer"">listed</a> in the GATE docs. I saw this on a stanford <a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2012-March/001918.html"" rel=""nofollow noreferrer"">mailing list</a>. Plus I saw this old post from stackoverflow: <a href=""https://stackoverflow.com/questions/27423778/stanford-corenlp-plugin-for-gate"">Stanford_CoreNLP plugin for gate</a></p>

<p>How do I get Stanford NER installed and working with GATE? </p>
","nlp, stanford-nlp, gate","<blockquote>
  <p>I am new to GATE (version 8.0)</p>
</blockquote>

<p>The <code>Stanford_CoreNLP</code> plugin was introduced in June 2014, <em>after</em> the release of GATE version 8.0, so you need to download a more recent <a href=""http://jenkins.gate.ac.uk/job/GATE-Nightly/lastSuccessfulBuild/"" rel=""nofollow"">nightly snapshot</a> build and install that instead.  In version 8.0 the Stanford POS tagger and parser were available as separate plugins (as you show in your screenshot) but the NER tools were not included at that point.</p>

<p>As a general rule, if you want to be sure you're looking at the correct version of the GATE user guide then you should access it via the ""help"" menu in GATE Developer.  The <a href=""https://gate.ac.uk/userguide/"" rel=""nofollow"">user guide link</a> on the GATE website refers to the latest snapshot, not to a numbered release.</p>

<hr>

<p>Edit June 2015: GATE Developer version 8.1 has now been released, which includes the <code>Stanford_CoreNLP</code> plugin.</p>
",4,3,859,2014-12-19 20:00:02,https://stackoverflow.com/questions/27573095/how-to-get-the-stanford-ner-plugin-working-with-gate
Need a good relation extractor,"<p>I'm doing a NLP project.</p>

<p>The purpose of the project is to extract possible relationship between two things. For example, for a pair ""location"" and ""person"" the extracted results would be ""near"", ""lives in"", ""works in"", etc.</p>

<p>Is there any existing NLP tool capable of doing this?</p>
","nlp, nltk, stanford-nlp","<p>There are a few different tools you might want to look at:</p>

<h2>MITIE</h2>

<p>MIT's new <strong><a href=""https://github.com/mit-nlp/MITIE"">MITIE</a></strong> tool supports basic relationship extraction. Included in the distribution are 21 English binary relation extraction models trained on a combination of Wikipedia and Freebase data. You can also train your own custom relation detectors. Here's a listing of the MITIE/MITIE-models/english/binary_relations/ directory, which is downloaded when you run the <code>make MITIE-models</code> target during the build process (the names should be relatively self-explanatory):</p>

<ul>
<li>rel_classifier_book.written_work.author.svm</li>
<li>rel_classifier_film.film.directed_by.svm</li>
<li>rel_classifier_influence.influence_node.influenced_by.svm</li>
<li>rel_classifier_law.inventor.inventions.svm</li>
<li>rel_classifier_location.location.contains.svm</li>
<li>rel_classifier_location.location.nearby_airports.svm</li>
<li>rel_classifier_location.location.partially_contains.svm</li>
<li>rel_classifier_organization.organization.place_founded.svm</li>
<li>rel_classifier_organization.organization_founder.organizations_founded.svm</li>
<li>rel_classifier_organization.organization_scope.organizations_with_this_scope.svm</li>
<li>rel_classifier_people.deceased_person.place_of_death.svm</li>
<li>rel_classifier_people.ethnicity.geographic_distribution.svm</li>
<li>rel_classifier_people.person.ethnicity.svm</li>
<li>rel_classifier_people.person.nationality.svm</li>
<li>rel_classifier_people.person.parents.svm</li>
<li>rel_classifier_people.person.place_of_birth.svm</li>
<li>rel_classifier_people.person.religion.svm</li>
<li>rel_classifier_people.place_of_interment.interred_here.svm</li>
<li>rel_classifier_time.event.includes_event.svm</li>
<li>rel_classifier_time.event.locations.svm</li>
<li>rel_classifier_time.event.people_involved.svm</li>
</ul>

<h2>OpenIE</h2>

<p><strong><a href=""https://github.com/knowitall/openie"">OpenIE</a></strong> from the Univ of Washington will extract relationships from text, representing the output as triples in the form of <code>(Arg1, Arg2, Relation)</code>. For example, given the input sentence:</p>

<blockquote>
  <p>The U.S. president Barack Obama gave his speech on Tuesday to thousands of people.</p>
</blockquote>

<p>OpenIE will extract these binary relations:</p>

<ul>
<li>(Barack Obama, is the president of, the U.S.)</li>
<li>(Barack Obama, gave, his speech)</li>
<li>(Barack Obama, gave his speech, on Tuesday)</li>
<li>(Barack Obama, gave his speech, to thousands of people)</li>
</ul>

<p><em>Note: OpenIE uses <a href=""https://github.com/knowitall/openie/blob/master/LICENSE"">a non-standard open source license</a> that expressly prohibits commercial use.</em></p>

<h2>Stanford Relation Extractor</h2>

<p>The <strong><a href=""http://nlp.stanford.edu/software/relationExtractor.shtml"">Stanford Relation Extractor</a></strong> extracts relations Live_In, Located_In, OrgBased_In, and Work_For. If you want to use a different set of relations, you can train your own relation extractor using the code (details provided on the webpage).</p>

<p>If you want basic dependencies, you can also use the Stanford Dependency Parser:</p>

<p>The <strong><a href=""http://nlp.stanford.edu/software/stanford-dependencies.shtml"">Stanford Dependency Parser</a></strong> (part of the Stanford Parser) will extract grammatical relations between words in a sentence. For example, given this input:</p>

<blockquote>
  <p>Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas</p>
</blockquote>

<p>The Stanford Parser will extract these grammatical dependencies:</p>

<ul>
<li>nsubjpass(submitted, Bills)</li>
<li>auxpass(submitted, were)</li>
<li>agent(submitted, Brownback)</li>
<li>nn(Brownback, Senator)</li>
<li>appos(Brownback, Republican)</li>
<li>prep_of(Republican, Kansas)</li>
<li>prep_on(Bills, ports)</li>
<li>conj_and(ports, immigration)</li>
<li>prep_on(Bills, immigration)</li>
</ul>

<h2>GATE</h2>

<p><strong><a href=""https://gate.ac.uk/"">GATE</a></strong> from the Univ of Sheffield also includes a relation extraction capability, though I've never used it myself. This presentation provides an overview of how it works: <a href=""https://gate.ac.uk/sale/talks/gate-course-may10/track-3/module-11-ml-adv/module-11-relations.pdf"">https://gate.ac.uk/sale/talks/gate-course-may10/track-3/module-11-ml-adv/module-11-relations.pdf</a></p>
",21,12,7775,2014-12-21 10:57:37,https://stackoverflow.com/questions/27588702/need-a-good-relation-extractor
Chunking Stanford Named Entity Recognizer (NER) outputs from NLTK format,"<p>I am using NER in NLTK to find persons, locations, and organizations in sentences. I am able to produce the results like this:</p>

<pre><code>[(u'Remaking', u'O'), (u'The', u'O'), (u'Republican', u'ORGANIZATION'), (u'Party', u'ORGANIZATION')]
</code></pre>

<p>Is that possible to chunk things together by using it?
What I want is like this:</p>

<pre><code>u'Remaking'/ u'O', u'The'/u'O', (u'Republican', u'Party')/u'ORGANIZATION'
</code></pre>

<p>Thanks!</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>You can use the standard NLTK way of representing chunks using <strong>nltk.Tree</strong>. This might mean that you have to change your representation a bit.</p>

<p>What I usually do is represent <strong>NER-tagged</strong> sentences as <strong>lists of triplets</strong>:</p>

<pre><code>sentence = [('Andrew', 'NNP', 'PERSON'), ('is', 'VBZ', 'O'), ('part', 'NN', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('Republican', 'NNP', 'ORGANIZATION'), ('Party', 'NNP', 'ORGANIZATION'), ('in', 'IN', 'O'), ('Dallas', 'NNP', 'LOCATION')]
</code></pre>

<p>I do this when I use an external tool for NER tagging a sentence. Now you can transform this sentence the NLTK representation:</p>

<pre><code>from nltk import Tree


def IOB_to_tree(iob_tagged):
    root = Tree('S', [])
    for token in iob_tagged:
        if token[2] == 'O':
            root.append((token[0], token[1]))
        else:
            try:
                if root[-1].label() == token[2]:
                    root[-1].append((token[0], token[1]))
                else:
                    root.append(Tree(token[2], [(token[0], token[1])]))
            except:
                root.append(Tree(token[2], [(token[0], token[1])]))

    return root


sentence = [('Andrew', 'NNP', 'PERSON'), ('is', 'VBZ', 'O'), ('part', 'NN', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('Republican', 'NNP', 'ORGANIZATION'), ('Party', 'NNP', 'ORGANIZATION'), ('in', 'IN', 'O'), ('Dallas', 'NNP', 'LOCATION')]
print IOB_to_tree(sentence)
</code></pre>

<p>The change in representation kind of makes sense because you certainly need POS tags for NER tagging.</p>

<p>The end result should look like:</p>

<pre><code>(S
  (PERSON Andrew/NNP)
  is/VBZ
  part/NN
  of/IN
  the/DT
  (ORGANIZATION Republican/NNP Party/NNP)
  in/IN
  (LOCATION Dallas/NNP))
</code></pre>
",2,11,2546,2014-12-23 22:46:15,https://stackoverflow.com/questions/27629130/chunking-stanford-named-entity-recognizer-ner-outputs-from-nltk-format
Stanford Parser - train input specification,"<p>In documentation I will see call java class with params:</p>

<blockquote>
  <p>java edu.stanford.nlp.parser.nndep.DependencyParser -tlp
  edu.stanford.nlp.trees.international.pennchinese.ChineseTreebankLanguagePack
  -trainFile chinese/train.conll -devFile chinese/dev.conll -embedFile chinese/embeddings.txt -embeddingSize 50 -model
  nndep.chinese.model.txt.gz</p>
</blockquote>

<p>Where I can find specification on this 3 files?</p>

<p>chinese/train.conll - this is train file (specification on it <a href=""http://ilk.uvt.nl/conll/#dataformat"" rel=""nofollow"">http://ilk.uvt.nl/conll/#dataformat</a>)</p>

<p>chinese/dev.conll - what is it?</p>

<p>chinese/embeddings.txt - what is it?</p>
","java, stanford-nlp","<p>chinese/train.conll, chinese/dev.conll: These are training/dev files in CoNLL 2006 format, as discussed in section 4.1 of the paper: <a href=""http://cs.stanford.edu/~danqi/papers/emnlp2014.pdf"" rel=""nofollow"">http://cs.stanford.edu/~danqi/papers/emnlp2014.pdf</a> . (In general we don't have permission to distribute data sets to others.)</p>

<p>chinese/embeddings.txt: These are word embeddings trained with word2vec as described in section 3.2 of the same paper.</p>
",1,0,355,2014-12-27 20:45:01,https://stackoverflow.com/questions/27671315/stanford-parser-train-input-specification
how to extract elements from tree.productions(),"<p>(1)My goal: To extract left-hand side and right-hand side of a production.</p>

<p>(2)My approach:
I am employing stanford parser and nltk tools to extract parsetree of a sentence. My code is below:</p>

<pre><code>corenlp_dir = ""/home/corenlp-python/stanford-corenlp-full-2013-11-12/""
parser = corenlp.StanfordCoreNLP(corenlp_path=corenlp_dir)

result_json = json.loads(parser.parse(""I have a tree.""))
for sentence in result_json[""sentences""]:
    t = Tree.fromstring(sentence[""parsetree""])
    print t.productions()   # [ROOT -&gt; S, S -&gt; NP VP ., NP -&gt; PRP, PRP -&gt; 'I', VP -&gt; VBP NP, VBP -&gt; 'have', NP -&gt; DT NN, DT -&gt; 'a', NN -&gt; 'tree', . -&gt; '.']

    print t.productions()[1]  # S -&gt; NP VP .
    print type(productions()[1])  # &lt;class 'nltk.grammar.Production'&gt;

    for (i,child) in enumerate(t): 
        print (i,child)  # (0, Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBP', ['have']), Tree('NP', [Tree('DT', ['a']), Tree('NN', ['tree'])])]), Tree('.', ['.'])])) I can only get one tree.
</code></pre>

<p>(3)My question is how I can continue to extract elements from both sides of each <strong>production</strong> , such as <strong>'S'</strong> and <strong>'NP VP .'</strong>. Is there any method can be used to solve this problem?</p>

<p>Could anyone help me and maybe point out some directions?</p>
","python, nlp, nltk, stanford-nlp, context-free-grammar","<p><code>nltk.Tree</code> is actually a subclass of the Python <code>list</code>, so you can access the children of any node <code>c</code> by <code>c[0]</code>, <code>c[1]</code>, <code>c[2]</code>, etc. Note that NLTK trees are not explicitly binary by design, so your notion of ""left"" and ""right"" might have to be enforced somewhere in a contract.</p>

<p>Assuming the tree is binary, you can access the left child of a node with <code>c[0]</code>, and the right with <code>c[1]</code>. For your second task:</p>

<blockquote>
  <p>But what I want to do is to extract the left-hand side of a production and gather right-hand side of all productions with the same left-hand side.</p>
</blockquote>

<p>If I understand correctly, you can traverse the tree and build up a <code>dict</code> as you go, where the keys are left-hand sides and the values are lists of possible right-hand productions. I'm not sure if <code>nltk.Tree</code> objects are hashable / immutable (if not, they wouldn't be usable as <code>dict</code> keys), but you could use the string form of the <code>Tree</code> objects as keys in any case.</p>
",2,2,2053,2014-12-28 11:57:52,https://stackoverflow.com/questions/27676164/how-to-extract-elements-from-tree-productions
NullPointerException with Stanford NLP Spanish POS tagging,"<p>All - </p>

<p>Running Stanford CoreNLP 3.4.1, plus the Spanish models. I have a directory of approximately 100 Spanish raw text documents, UTF-8 encoded. For each one, I execute the following commandline:</p>

<pre><code>java -cp stanford-corenlp-3.4.1.jar:stanford-spanish-corenlp-2014-08-26-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-0.23.jar -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props &lt;propsfile&gt; -file &lt;txtfile&gt;
</code></pre>

<p>The props file looks like this:</p>

<pre><code>annotators = tokenize, ssplit, pos
tokenize.language = es
pos.model = edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger
</code></pre>

<p>For almost every file, I get the following error:</p>

<p>Exception in thread ""main"" java.lang.RuntimeException: Error annotating :
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$15.run(StanfordCoreNLP.java:1287)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1347)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1389)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1459)
Caused by: java.lang.NullPointerException
    at edu.stanford.nlp.tagger.maxent.ExtractorSpanishStrippedVerb.extract(ExtractorFramesRare.java:1626)
    at edu.stanford.nlp.tagger.maxent.Extractor.extract(Extractor.java:153)
    at edu.stanford.nlp.tagger.maxent.TestSentence.getExactHistories(TestSentence.java:465)
    at edu.stanford.nlp.tagger.maxent.TestSentence.getHistories(TestSentence.java:440)
    at edu.stanford.nlp.tagger.maxent.TestSentence.getHistories(TestSentence.java:428)
    at edu.stanford.nlp.tagger.maxent.TestSentence.getExactScores(TestSentence.java:377)
    at edu.stanford.nlp.tagger.maxent.TestSentence.getScores(TestSentence.java:372)
    at edu.stanford.nlp.tagger.maxent.TestSentence.scoresOf(TestSentence.java:713)
    at edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:91)
    at edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:31)
    at edu.stanford.nlp.tagger.maxent.TestSentence.runTagInference(TestSentence.java:322)
    at edu.stanford.nlp.tagger.maxent.TestSentence.testTagInference(TestSentence.java:312)
    at edu.stanford.nlp.tagger.maxent.TestSentence.tagSentence(TestSentence.java:135)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.tagSentence(MaxentTagger.java:998)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.doOneSentence(POSTaggerAnnotator.java:147)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.annotate(POSTaggerAnnotator.java:110)
    at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:67)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:847)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$15.run(StanfordCoreNLP.java:1275)</p>

<p>Any ideas? I haven't even begun to track this down. I'm certain the problem is in POS; tokenize and ssplit run just fine.</p>

<p>P.S. Please don't say ""Upgrade to 3.5.0""; I don't currently have Java 8 installed and don't want to install it yet.</p>

<p>Thanks in advance.</p>
",stanford-nlp,"<p>Yes, it seems like there's a bug in the 3.4.1 Spanish models.</p>

<p>The Spanish 3.5.0 models actually seem to be compatible with Java 7. You can download the models used in 3.5 (<a href=""http://nlp.stanford.edu/software/stanford-spanish-corenlp-2014-10-23-models.jar"" rel=""nofollow""><code>stanford-spanish-corenlp-2014-10-23-models.jar</code></a>) and put that on your classpath instead. This fixed the problem for me running Java 7 locally.</p>
",2,0,695,2015-01-05 19:35:22,https://stackoverflow.com/questions/27786567/nullpointerexception-with-stanford-nlp-spanish-pos-tagging
How to get dependency parse output exactly as online demo?,"<p>How can I programmatically get the same dependency parse using stanford corenlp as seen in the online demo?</p>

<p>I am using the corenlp package to obtain the dependency parse for the following sentence.</p>

<p><strong>Second healthcare worker in Texas tests positive for Ebola , authorities say .</strong></p>

<p>I try to obtain the parse programmatically using the code below</p>

<pre><code>            Properties props = new Properties();
            props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

            String text = ""Second healthcare worker in Texas tests positive for Ebola , authorities say .""; // Add your text here!
            Annotation document = new Annotation(text);
            pipeline.annotate(document);
            String[] myStringArray = {""SentencesAnnotation""};
            List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
            for(CoreMap sentence: sentences) {
                SemanticGraph dependencies = sentence.get(BasicDependenciesAnnotation.class);
                IndexedWord root = dependencies.getFirstRoot();
                System.out.printf(""root(ROOT-0, %s-%d)%n"", root.word(), root.index());
                for (SemanticGraphEdge e : dependencies.edgeIterable()) {
                    System.out.printf (""%s(%s-%d, %s-%d)%n"", e.getRelation().toString(), e.getGovernor().word(), e.getGovernor().index(), e.getDependent().word(), e.getDependent().index());
                }
            }

    }
</code></pre>

<p>I get the following output using the stanford corenlp 3.5.0 package.</p>

<pre><code>root(ROOT-0, worker-3)
amod(worker-3, Second-1)
nn(worker-3, healthcare-2)
prep(worker-3, in-4)
amod(worker-3, positive-7)
dep(worker-3, say-12)
pobj(in-4, tests-6)
nn(tests-6, Texas-5)
prep(positive-7, for-8)
pobj(for-8, ebola-9)
nsubj(say-12, authorities-11)
</code></pre>

<p>But the online demo gives a different answer that marks say as the root and has other relationships like ccomp between words in the parse.</p>

<pre><code>amod(worker-3, Second-1)
nn(worker-3, healthcare-2)
nsubj(tests-6, worker-3)
prep(worker-3, in-4)
pobj(in-4, Texas-5)
ccomp(say-12, tests-6)
acomp(tests-6, positive-7)
prep(positive-7, for-8)
pobj(for-8, Ebola-9)
nsubj(say-12, authorities-11)
root(ROOT-0, say-12)
</code></pre>

<p>How can I resolve my output to match with the online demo?</p>
","nlp, stanford-nlp","<p>The reason for the different output is that if you use the <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""noreferrer"">parser demo</a>, the stand-alone parser distribution is being used and your code uses the entire CoreNLP distribution. While both of them use the same parser and the same models, the default configuration of CoreNLP runs a part-of-speech (POS) tagger before running the parser and the parser incorporates the POS information which can lead to different results in some cases.</p>

<p>In order to get the same results you can disable the POS tagger by changing the list of annotators:</p>

<pre><code>props.put(""annotators"", ""tokenize, ssplit, parse, lemma, ner, dcoref"");
</code></pre>

<p>Note, however, that the lemma, ner and dcoref annotators all require POS tags, so you have to change the order of the annotators.</p>

<p>There is also a <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""noreferrer"">CoreNLP demo</a> which should always produce the same output as your code.</p>
",9,5,3883,2015-01-09 17:19:27,https://stackoverflow.com/questions/27865825/how-to-get-dependency-parse-output-exactly-as-online-demo
Ignore words for lemmatizer,"<p>I would like to use Stanford CoreNLP for lemmatization but I have some words not to be lemmatized. Is there a way to provide this ignore list to the tool? I am following this <a href=""https://stackoverflow.com/questions/1578062/lemmatization-java"">code</a>, and when the program calls <code>this.pipeline.annotate(document);</code>then, that's it; it would be hard to replace the occurrences. One solution is that create a mapping list in which each word to be ignored is paired with lemmatize(word) (i.e., d = {(w1, lemmatize(w1)), (w2, lemmatize(w2), ...} and do the post processing with this mapping list. But it should be easier than this, I guess.</p>

<p>Thanks for the help.</p>
",stanford-nlp,"<p>I think I found the solution with my friend's help.</p>

<pre><code>  for(CoreMap sentence: sentences) {
        // Iterate over all tokens in a sentence
        for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
            System.out.print(token.get(OriginalTextAnnotation.class) + ""\t"");
            System.out.println(token.get(LemmaAnnotation.class));

        }
    }
</code></pre>

<p>You can get original form of the word by calling <code>token.get(OriginalTextAnnotation.class)</code>.</p>
",0,1,406,2015-01-14 21:35:38,https://stackoverflow.com/questions/27952883/ignore-words-for-lemmatizer
Stanford sentiment analysis score java,"<p>I use <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford core NLP</a> library for sentiment analysis. The below code return the class of an example but how can I get the score? for example -0.3 for negative etc</p>

<pre><code>private int getScore(String line) {
    boolean isrun = false;
    StanfordCoreNLP pipeline = null;
    if(!isrun){
        Properties props = getProperties();
        pipeline = new StanfordCoreNLP(props);
        isrun = true;
    }
    Annotation annotation;

    int sentiment = -1;
    if (line != null &amp;&amp; line.length() &gt; 0) {
        annotation = pipeline.process(line);
        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
            Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
            sentiment = RNNCoreAnnotations.getPredictedClass(tree);
        }
    }
    return sentiment;
}
</code></pre>

<p><strong>EDIT</strong></p>

<p>In online <a href=""http://nlp.stanford.edu:8080/sentiment/rntnDemo.html"" rel=""nofollow"">demo</a> when mouse is on root at the graph we can see that the example is negative 72%. How can get this number?</p>
","java, sentiment-analysis, stanford-nlp","<p>0.Download Stanford NLP Core Lib and import external lib stanford-corenlp-3.5.2-models.jar, stanford-corenlp-3.5.2.jar, stanford-corenlp-3.5.2-sources.jar and ejml-0.23.jar into this package.</p>

<p>1.Build this class NLP in Eclipse </p>

<pre><code>import java.util.Properties;
import org.ejml.simple.SimpleMatrix;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class NLP {
static StanfordCoreNLP pipeline;

public static void init() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
    pipeline = new StanfordCoreNLP(props);
}

public static int findSentiment(String tweet) {

    int mainSentiment = 0;
    if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
        int longest = 0;
        Annotation annotation = pipeline.process(tweet);
        for (CoreMap sentence : annotation
                .get(CoreAnnotations.SentencesAnnotation.class)) {
            Tree tree = sentence
                    .get(SentimentAnnotatedTree.class);
            int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
            SimpleMatrix sentiment_new = RNNCoreAnnotations.getPredictions(tree);             
            String partText = sentence.toString();
            if (partText.length() &gt; longest) {
                mainSentiment = sentiment;
                longest = partText.length();
            }
        }
    }
    return mainSentiment;
    }
}
</code></pre>

<p>2.Build a new class to parse your sentence with NLP</p>

<pre><code>import java.util.ArrayList;

public class What2Think {

    public static void main(String[] args) {
        ArrayList&lt;String&gt; tweets = new ArrayList&lt;String&gt;();
        tweets.add(""In this country, \""democracy\"" means pro-government. #irony"");
        NLP.init();
        for(String tweet : tweets) {
            System.out.println(tweet + "" : "" + NLP.findSentiment(tweet));
        }
    }
}
</code></pre>

<p>Run it</p>
",5,4,5586,2015-01-18 21:03:42,https://stackoverflow.com/questions/28014779/stanford-sentiment-analysis-score-java
Stanford NLP Parser gives different result (sentiment) for same statement used in Kaggle Movie review,"<p>I am working on Kaggle Movie Sentiment Analysis and I found the movie reviews has been parsed using Standford Parser.
On exploring the dataset I found same statement giving different setiments-</p>

<pre><code>their parents , wise folks that they are ,  2
their parents , wise folks that they are    3

Genuinely unnerving .   3
Genuinely unnerving 1
</code></pre>

<p>where english is the parsed data and the digit belongs to sentiment.
Now if you check the line just comma and a dot is the difference rest is same but both belongs to different sentiment which is raising a lot of trouble for me to classify.</p>

<p>What should I do in text classification to avoid such problem, if I ignore the duplicate and use the already added sentiment , my classification goes horribly wrong, so what should be the way to solve such scenarios.</p>
","python, scikit-learn, classification, stanford-nlp, kaggle","<p>I assume you are using Bag of Words and the comma and the dot are one of your features (a column in your <code>X</code> matrix). </p>

<pre><code>+-------------------------+-----------+-----------+----+
|    Document/Features    | Genuinely | unnerving | .  |
+-------------------------+-----------+-----------+----+
|  Genuinely unnerving .  |         1 |         1 | 1  |
|  Genuinely unnerving    |         1 |         1 | 0  |
+-------------------------+-----------+-----------+----+
</code></pre>

<p>An ideal algorithm should learn wether these features are relevant or not. For example in the case of Logistic Regression your algorithm would assign a really small weight to the corresponding column, and therefore a <code>1</code> or a <code>0</code> in that column won't change the outcome of the prediction. So you would have something like:</p>

<pre><code>""Genuinely unnerving ."" -&gt; 0.5*1 + -2.3*1 + 0.000001*1 -&gt; Negative
""Genuinely unnerving  "" -&gt; 0.5*1 + -2.3*1 + 0.000001*0 -&gt; Also negative
</code></pre>

<p>In your case it looks like they are having some small effect. Is this really a problem? You have found some special cases were it seems wrong but by looking at the data the algorithm found that sentences with a dot are more negative that sentences without one. May be you should trust that statistically speaking, a dot can change the meaning of a sentence.</p>

<p>It can also happen that you have bad training data or a bad overfitting model. If you really think something is wrong, then you can impose this knowledge on the model by representing the sentences so that they are indistinguishable, for example by ignoring some punctuation.</p>

<p>I think it would be wrong to leave all punctuations out at once, for example a <code>!</code> could be representative of very positive sentiment when accompanied by the word <code>yes</code>, if you strip it from your sentence you would be hiding the model valuable information. But may be it is just the opposite and <code>!</code> is negative in most cases, so it gets a high negative weight after training, which confuses the model when predicting <code>yes!!!</code> like sentences. In this case you could represent your sentence as bigrams, so that the model can separately weight the effect of a single <code>!</code> and a <code>(yes, !)</code> combination.</p>

<p>So in resume, you should try different models and ways to represent your data and see what works.</p>
",1,2,225,2015-01-19 20:41:25,https://stackoverflow.com/questions/28033135/stanford-nlp-parser-gives-different-result-sentiment-for-same-statement-used-i
NLP libraries installation guidelines for java,"<p>I am new to NLP. I need basic idea to get started with installation of it.
I have gone through LingPipe and open NLP installation section, but i did not get why to install maven and additional training sets , models etc.
Any brief explanation of installation would be helpful for me to get started with coding.
Platform - Ubuntu </p>

<p>Sorry if this question is too generic or simple</p>
","java, ubuntu, stanford-nlp, opennlp, lingpipe","<p>I used OpenNLP in my project. I think this instructions will help you to go through OpenNLP Library. Follow this <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html"" rel=""nofollow"">document</a> </p>

<ul>
<li>Download OpenNLP Library and add it to your build path</li>
<li>Download trained models and put it to a folder </li>
<li>modelIn = new FileInputStream(""path"");</li>
</ul>

<p>InputStream modelIn = null;</p>

<pre><code>try {
  modelIn = new FileInputStream(""en-pos-maxent.bin"");
  POSModel model = new POSModel(modelIn);
}
catch (IOException e) {
  // Model loading failed, handle the error
  e.printStackTrace();
}
finally {
  if (modelIn != null) {
    try {
      modelIn.close();
    }
    catch (IOException e) {
    }
  }
}
</code></pre>
",0,0,276,2015-01-20 15:38:35,https://stackoverflow.com/questions/28049338/nlp-libraries-installation-guidelines-for-java
"StanfordCoreNLP : TokenMgrError: Lexical error at line 1, column 14. Encountered: &quot;E&quot; (69), after : &quot;\\&quot;","<p>The complete stacktrace is as follows. Any idea whats wrong?</p>

<p>Exception in thread ""main"" edu.stanford.nlp.ling.tokensregex.parser.TokenMgrError: Lexical error at line 1, column 14.  Encountered: ""E"" (69), after : ""\""
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParserTokenManager.getNextToken(TokenSequenceParserTokenManager.java:1029)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.jj_ntk(TokenSequenceParser.java:3353)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.CoreMapNode(TokenSequenceParser.java:1386)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeBasic(TokenSequenceParser.java:1360)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeGroup(TokenSequenceParser.java:1327)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeDisjConj(TokenSequenceParser.java:1266)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.BracketedNode(TokenSequenceParser.java:1127)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexBasic(TokenSequenceParser.java:833)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexDisjConj(TokenSequenceParser.java:1020)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegex(TokenSequenceParser.java:790)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexWithAction(TokenSequenceParser.java:1643)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.parseSequenceWithAction(TokenSequenceParser.java:37)
        at edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.compile(TokenSequencePattern.java:186)
        at edu.stanford.nlp.patterns.surface.ScorePhrases.runParallelApplyPats(ScorePhrases.java:215)
        at edu.stanford.nlp.patterns.surface.ScorePhrases.applyPats(ScorePhrases.java:326)
        at edu.stanford.nlp.patterns.surface.ScorePhrases.learnNewPhrasesPrivate(ScorePhrases.java:397)
        at edu.stanford.nlp.patterns.surface.ScorePhrases.learnNewPhrases(ScorePhrases.java:177)
        at edu.stanford.nlp.patterns.surface.GetPatternsFromDataMultiClass.iterateExtractApply4Label(GetPatternsFromDataMultiClass.java:1716)
        at edu.stanford.nlp.patterns.surface.GetPatternsFromDataMultiClass.iterateExtractApply(GetPatternsFromDataMultiClass.java:1591)
        at edu.stanford.nlp.patterns.surface.GetPatternsFromDataMultiClass.main(GetPatternsFromDataMultiClass.java:2485)</p>
",stanford-nlp,"<p>I also replied on the github page: We are releasing the new version of the software in a few days. This bug is most probably fixed in that -- I used the files you provided in the github page with the new code and it works. Stay tuned!</p>
",2,2,1542,2015-01-22 08:29:53,https://stackoverflow.com/questions/28084173/stanfordcorenlp-tokenmgrerror-lexical-error-at-line-1-column-14-encountered
Stanford CoreNLP - the egw4-reut.512.clusters cannot be found,"<p>I am using the CoreNLP package to do some annotation on user comments and since I have upgraded to the 3.5.0 version I seem to repeatedly run into the same error:</p>

<blockquote>
  <p>Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... </p>
  
  <p>Loading distsim lexicon from /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters ... </p>
  
  <p>java.lang.RuntimeException: java.io.FileNotFoundException: \u\nlp\data\pos_tags_are_useless\egw4-reut.512.clusters (The system cannot find the path specified) 
      at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setNextObject(ReaderIteratorFactory.java:225) (cue fifty lines of error)</p>
</blockquote>

<p>A few searches here got me these similar questions: </p>

<p><a href=""https://stackoverflow.com/questions/27482591/stanford-ner-error-loading-distsim-lexicon-failed"">Stanford NER Error: Loading distsim lexicon Failed</a> and <a href=""https://stackoverflow.com/questions/25569466/stanford-ner-tagger-generates-file-not-found-exception-with-provided-models"">Stanford NER tagger generates 'file not found' exception with provided models</a> which did not solve my issue: I am exclusively using code and models from the 3.5.0 (via Maven Central). I tried modifying the props file from the NER model and pointing towards another .clusters file in a user directory with no success (exact same error).</p>

<p>The code I use to instantiate the CoreNLP object is pretty standard too, but here it is:</p>

<pre><code>    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    stan = new StanfordCoreNLP(props);
</code></pre>

<p>Now I am thinking that there is something obvious that I am missing. Any help would be greatly appreciated.</p>

<p>A more complete stacktrace (if that can help) is as follows:</p>

<pre><code>Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... Loading distsim lexicon from /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters ... java.lang.RuntimeException: java.io.FileNotFoundException: \u\nlp\data\pos_tags_are_useless\egw4-reut.512.clusters (The system cannot find the path specified)
at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setNextObject(ReaderIteratorFactory.java:225)
at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.&lt;init&gt;(ReaderIteratorFactory.java:161)
at edu.stanford.nlp.objectbank.ReaderIteratorFactory.iterator(ReaderIteratorFactory.java:98)
at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.&lt;init&gt;(ObjectBank.java:404)
at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:242)
at edu.stanford.nlp.ie.NERFeatureFactory.initLexicon(NERFeatureFactory.java:471)
at edu.stanford.nlp.ie.NERFeatureFactory.init(NERFeatureFactory.java:379)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.reinit(AbstractSequenceClassifier.java:171)
at edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2630)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1620)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1675)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1662)
at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2851)
at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:189)
at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:173)
at edu.stanford.nlp.ie.ClassifierCombiner.&lt;init&gt;(ClassifierCombiner.java:113)
at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:64)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$6.create(StanfordCoreNLP.java:617)
at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:267)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:129)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:125)
at rgu.jclos.quilt.utilities.nlp.DependenciesTagger.&lt;init&gt;(DependenciesTagger.java:99)
at rgu.jclos.quilt.eca.approaches.ApproachC_USS.main(ApproachC_USS.java:47)
Caused by: java.io.FileNotFoundException: \u\nlp\data\pos_tags_are_useless\egw4-reut.512.clusters (The system cannot find the path specified)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:131)
    at edu.stanford.nlp.io.EncodingFileReader.&lt;init&gt;(EncodingFileReader.java:78)
    at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setNextObject(ReaderIteratorFactory.java:192)
    ... 23 more
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.FileNotFoundException
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$6.create(StanfordCoreNLP.java:621)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:267)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:129)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:125)
    at rgu.jclos.quilt.utilities.nlp.DependenciesTagger.&lt;init&gt;(DependenciesTagger.java:99)
    at rgu.jclos.quilt.eca.approaches.ApproachC_USS.main(ApproachC_USS.java:47)
Caused by: java.io.FileNotFoundException
    at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:199)
    at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:173)
    at edu.stanford.nlp.ie.ClassifierCombiner.&lt;init&gt;(ClassifierCombiner.java:113)
    at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:64)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$6.create(StanfordCoreNLP.java:617)
    ... 6 more
Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to edu.stanford.nlp.classify.LinearClassifier
at edu.stanford.nlp.ie.ner.CMMClassifier.loadClassifier(CMMClassifier.java:1070)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1620)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1675)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1662)
at edu.stanford.nlp.ie.ner.CMMClassifier.getClassifier(CMMClassifier.java:1116)
at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:195)
... 10 more
</code></pre>
","java, stanford-nlp","<p>It turns out that the problem was caused by Maven.</p>

<p>My code was located in a utility library which was wrapping over the Stanford CoreNLP to provide additional processing, and was working perfectly well by itself. However when adding this project as a dependency to my master project, Maven defaulted to importing version 3.4 of the Stanford CoreNLP library models to the master project, which caused the bug described above.</p>
",0,1,607,2015-01-23 17:15:06,https://stackoverflow.com/questions/28115196/stanford-corenlp-the-egw4-reut-512-clusters-cannot-be-found
Java 8 compilation and execution error,"<p>To use the latest Stanford packages for a NLP project I upgraded Eclipse to Java 8, as in: <a href=""http://eclipse.org/downloads/java8/"" rel=""nofollow"">http://eclipse.org/downloads/java8/</a>. After changing the compliance path to Jdk1.8 and every other corresponding change I ran the program. </p>

<p>This is the error:</p>

<blockquote>
  <p>Error: Main method not found in class edu.stanford.nlp.trees.tregex.Relation$17, please define the main method as: <code>public static void main(String[] args)</code> or a JavaFX application class must extend <code>javafx.application.Application</code></p>
</blockquote>

<p>And here's the program:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
public class Readability 
{
    static String line1;
    static BufferedReader br=new BufferedReader(new InputStreamReader(System.in));
    public static void main(String args[])throws IOException
    {
        Readability rd=new Readability();
        rd.tag(rd.count());
    }      
    public String count()throws IOException
    {
        String line, line1;
        System.out.println(""Enter the name of the file: "");
        String file=br.readLine();
        StringTokenizer st = null;
        StringBuilder sb=new StringBuilder();
        try(FileInputStream input = new FileInputStream(""E:\\""+file))
            {
            int data = input.read();
            while(data != -1)
                {
                sb.append((char)data);
                data = input.read();
                }
            }
        catch(FileNotFoundException e)
        {
            System.err.println(""File Not Found Exception : "" + e.getMessage());
        }
        line=sb.toString();
        double sentencecount=0.0000, syllablecount=0.0000;
        st = new StringTokenizer(line,"" ,(){}[]/.;:'&amp;?!\r\t\n\f"");
        StringTokenizer st1 = new StringTokenizer(line,""\r"");
        double wordcount=st.countTokens();
        System.out.println(wordcount);
        line1=line;//copy for Tagger
        line+="" T"";
        //System.out.println(line);
        char[] array = line.toCharArray();
        for (int i=0;i&lt;array.length-1;i++)
        {
            int turn=i+2;
            if(array[i]=='?')
                sentencecount++; 
            if((array[i]=='.')&amp;&amp;(((int)array[turn]&gt;64&amp;&amp;(int)array[turn]&lt;91)))
                sentencecount++;
            if((array[i]=='!')&amp;&amp;(((int)array[turn]&gt;64&amp;&amp;(int)array[turn]&lt;91)))
                sentencecount++;
            if((array[i]=='\'')&amp;&amp;(((int)array[turn]&gt;64&amp;&amp;(int)array[turn]&lt;91)))
                sentencecount++;
            if((array[i]=='""')&amp;&amp;(((int)array[turn]&gt;64&amp;&amp;(int)array[turn]&lt;91)))
                sentencecount++;
        }
        System.out.println(sentencecount+(st1.countTokens()-1));//To include the last sentence before the 'Enter' is pressed as the ' ' follows a '.'
        //System.out.println(st1.countTokens());
        char[] array1=new char[st.countTokens()];
        while (st.hasMoreTokens()) 
        {
            array1=st.nextToken().toCharArray();
            if(array1.length&gt;2)
            {
            for(int i=0;i&lt;array1.length-1;i++)
            {
                char a=Character.toLowerCase(array1[i]);
                char b=Character.toLowerCase(array1[i+1]);
                //System.out.println(a+"" ""+b);
                if(a=='a'||a=='e'||a=='i'||a=='o'||a=='u')
                {
                    if(b!='a'&amp;&amp;b!='e'&amp;&amp;b!='i'&amp;&amp;b!='o'&amp;&amp;b!='u')
                    {
                        //System.out.println(""Swab"");
                        syllablecount++;
                    }
                }
            }

            char c=Character.toLowerCase(array1[array1.length-1]);
            char d=Character.toLowerCase(array1[array1.length-2]);
            //System.out.println(c+"" ""+d);
            if((c=='a'||c=='i'||c=='o'||c=='u')&amp;&amp;(d!='a'||d!='e'||d!='i'||d!='o'||d!='u'))
            {
                //System.out.println(""Ha!"");
                syllablecount++;
            }
            else if((c=='e')&amp;&amp;(d=='e'))
            {
                //System.out.println(""Hola"");
                syllablecount++;
            }
            }
            else
            {
                //System.out.println(""Woosh"");
                syllablecount++;
            }
        }
        StringTokenizer st2 = new StringTokenizer(line,"" ,(){}[]/.;:'&amp;?!\r\t\n\f"");//As 'The' doesn't come under the syllable count radar
        while (st2.hasMoreTokens()) 
        {
            String a=st2.nextToken();
            //System.out.println(a);
            if(a.equalsIgnoreCase(""the""))
            {
                syllablecount++;
            }
        }
        System.out.println(syllablecount);
        double readability=(206.835-(1.015*(wordcount/sentencecount))-(84.6*(syllablecount/wordcount)));
        System.out.println(""\nReadability Measure:\n\n90-100: Easily Understood by an average 11-year old.\n60-70: Easily understood by a 13-15 year old.\n0-30: Best understood by university graduates."");
        if(readability&gt;0&amp;&amp;readability&lt;100)
            System.out.println(""The readability score, according to Flesch readability measure is: ""+readability);
        else
        System.out.println(""\nThe Readability score, according to Flesch readability measure is: ""+100);
        return line1;
    }
    public void tag(String st)
    {
        //System.out.println(st);
        MaxentTagger tagger = new MaxentTagger(""Tagger/left3words-distsim-wsj-0-18.tagger"");
        String tagged = tagger.tagString(st);    
        System.out.println(tagged);
    }

}
</code></pre>

<p>As for the code: It seeks to get the number of words, sentences and syllables. For using the Stanford POS tagger I needed Java 8 because of the error unsupported major minor version. Thus the changes.</p>
","java, eclipse, stanford-nlp","<p>I just ran this class on Eclipse commenting out the MaxentTagger lines of code (as I don't have this package) and used System.out.println(""Hello World"") and it compiled and ran just fine.</p>

<p>The program looks ok. Right click on the Readability program and hit Run as -> Java Application. My guess is you are hitting Run from the Eclipse toolbar and that is hooked on to some previous setup. If what I stated didn't fix it, then hit Run as -> Run Configurations and check to see if the project and the Main Class being run are correct. Outside of this, your code is fine.</p>
",0,0,719,2015-01-30 17:11:31,https://stackoverflow.com/questions/28241082/java-8-compilation-and-execution-error
StanfordCoreNLP: Why multiple roots for SemanticGraph (e.g. dependency parsing),"<p>In the the definition of the SemanticGraph class which is being used for Dependency Parsing. </p>

<p>Here is the definition of the variable ""roots"" as a collection of vertices: </p>

<pre><code>private final Collection&lt;IndexedWord&gt; roots;
</code></pre>

<p>My question is why <em>collection</em>? In what cases we would need more than one vertex as the root? </p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/semgraph/SemanticGraph.java"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/semgraph/SemanticGraph.java</a></p>
","nlp, stanford-nlp","<p>In all honesty, SemanticGraph has a lot of historical code which was motivated by its initial use in an RTE (Recognizing Textual Entailment) system, not for syntactic dependency parsing, so don't read too much into it all. But, nevertheless, there are various fairly natural use cases (e.g., fragment parsing or the output of semantic graph transformation operations) which can result in disconnected graphs, and, hence, multiple roots. </p>
",5,4,355,2015-01-30 23:10:44,https://stackoverflow.com/questions/28246146/stanfordcorenlp-why-multiple-roots-for-semanticgraph-e-g-dependency-parsing
an index of chinese characters organized by component radicals. stanford core nlp,"<p>I want to use the one described <a href=""https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/trees/international/pennchinese"" rel=""nofollow"">here</a>, part of the Stanford CoreNLP, as it looks promising but I can't understand how it works. I downloaded the entire CoreNLP but the <code>.jar</code> file mentioned in the README document, i.e. <code>chinese_map_utils.jar</code> is nowhere to be found. Do you think they're expecting me to create such a <code>.jar</code> file myself out of the component code they have listed there? That seems a bit absurd. </p>

<p>Essentially what I'm after is a system of breaking down Chinese characters into their component strokes or radicals (I know that not all the parts are called radicals, spare me the pedantics), so if you know of an alternative solution which is actionable then I'd be happy to hear about it. </p>
","java, jar, nlp, stanford-nlp","<p>No need to use this <code>chinese_map_utils.jar</code> — if you have CoreNLP on your classpath, that should be sufficient.</p>

<p>It looks like the class <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/trees/international/pennchinese/RadicalMap.java"" rel=""nofollow""><code>RadicalMap</code></a> may be of interest to you. Execution instructions are included in the class's source code (see the <code>main</code> method).</p>
",1,1,75,2015-01-31 07:03:38,https://stackoverflow.com/questions/28249160/an-index-of-chinese-characters-organized-by-component-radicals-stanford-core-nl
Swapping in Berkley parser in Stanford corenlp,"<p>I was using stanford nlp stack in my experiment and it was working nice until stanford PCFG parser started behaving weird for some of the sentences. I found <a href=""http://tomato.banatao.berkeley.edu:8080/parser/parser.html"" rel=""nofollow"">http://tomato.banatao.berkeley.edu:8080/parser/parser.html</a> the berkley parser giving correct parse tree for the sentences in my dataset. How could i swap in stanford pos tagger by bekley parser and continue using stanford dependency parser. I found here <a href=""http://brenocon.com/blog/2011/09/end-to-end-nlp-packages/"" rel=""nofollow"">http://brenocon.com/blog/2011/09/end-to-end-nlp-packages/</a> that it could be done, but not sure how.</p>

<p>Thanks in advance</p>

<hr>

<p>I have used following configuration for this purpose:</p>

<pre>

    props.put(""parse.type"",""charniak"");
    props.put(""parse.executable"",""src/main/resources/berkeley.bat"");
    props.put(""parse.model"","""");

    /*File: berkeley.bat*/

    @ECHO OFF
    java -jar C:\Users\Arindam\Downloads\berkeleymy.jar -gr C:/Users/Arindam/Downloads/eng_sm6.gr  -inputFile %4  -maxLength 399 -sentence_likelihood -kbest 2

</pre>
","nlp, stanford-nlp","<p>The difficult but clean way to do this would be to build your own annotator which hooks into a programmatic API of the Berkeley parser. You'd basically want to imitate the behavior of the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/ParserAnnotator.java"" rel=""nofollow noreferrer""><code>ParserAnnotator</code></a>, replacing the references to the Stanford <code>ParserQuery</code> implementation with references to Berkeley Parser + code that makes the necessary transformations.</p>

<p>Then using the results returned by the Berkeley Parser (transformed into the Stanford framework), you can use <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/EnglishGrammaticalStructure.html"" rel=""nofollow noreferrer""><code>EnglishGrammaticalStructure</code></a> to convert the Berkeley constituency parse to dependency trees.</p>

<hr>

<p>The less clean but perhaps easier way would be to have the Berkeley parser output a PTB-format parse, and use <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/GrammaticalStructure.html#main-java.lang.String:A-"" rel=""nofollow noreferrer"">the main method of <code>EnglishGrammaticalStructure</code></a> to generate from this CoNLL-format dependency parses.</p>

<hr>

<p>More information on the first option, as requested:</p>

<p>You should make your own annotator which composes with / subclasses <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/ParserAnnotator.java"" rel=""nofollow noreferrer""><code>ParserAnnotator</code></a>. The key method to override is <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/ParserAnnotator.java#L205"" rel=""nofollow noreferrer""><code>ParserAnnotator#doOneSentence</code></a>. Here you can call out to the Berkeley Parser API, parse its results, and call <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/ParserAnnotator.java#L244"" rel=""nofollow noreferrer""><code>ParserAnnotator#finishSentence</code></a> with the properly converted tree. <code>finishSentence</code> should take care of putting the correct annotations in place for you.</p>

<p>You can easily hook in your custom annotator on the main pipeline using a special property. See <a href=""https://stackoverflow.com/a/20127276/176075"">this SO answer</a> for example code (I'm referring to the <code>customAnnotatorClass</code> property).</p>
",2,0,500,2015-01-31 08:31:42,https://stackoverflow.com/questions/28249717/swapping-in-berkley-parser-in-stanford-corenlp
swiftly generate and sort full encoding dictionary and corresponding primary radicals,"<p>Chinese characters, <a href=""http://www.unicode.org/reports/tr38/#N101E4"" rel=""nofollow"">according to the unihan encoding schema</a>, can be indexed by their primary radical. </p>

<p>The <a href=""http://nlp.stanford.edu/software/segmenter.shtml"" rel=""nofollow"">Stanford Word Segmenter</a> has a command that can execute this, as described in their <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/international/pennchinese/RadicalMap.html"" rel=""nofollow"">documentation</a> i.e.</p>

<pre><code>java -cp stanford-segmenter-VERSION.jar
edu.stanford.nlp.trees.international.pennchinese.RadicalMap
-infile whitespace_seperated_chinese_characters.input
&gt; each_character_denoted_by_radical.output
</code></pre>

<p>I want to create a comprehensive table of chinese characters organized by their primary radical, I suppose I can use the function </p>

<p><strong>public static java.util.Set getChars(char ch)</strong> </p>

<p><em>What are the Characters with this primary radical?</em></p>

<p>or </p>

<p><strong>public static char getRadical(char ch)</strong></p>

<p><em>What is the primary radical of this char?</em></p>

<p>But my question is, what is the most efficacious way to accomplish this goal? and furthermore to output the result in the form of a table, <a href=""http://en.wikipedia.org/wiki/Han_unification#Examples_of_language_independent_characters"" rel=""nofollow"">à la this Wikipedia table</a> (not exactly like that table, but, shall we say, suggestive of that table).</p>

<p>That Stanford tool uses the <a href=""http://www.mdbg.net/chindict/chindict.php?page=cedict"" rel=""nofollow"">CC-CEDIT</a> dictionary. Is it possible I could just download that dictionary and feed it in? If so, how?</p>

<p>Maybe than Stanford tool already contains this as <a href=""https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/trees/international/pennchinese"" rel=""nofollow"">part of the code</a>, but how to access it?</p>
","character-encoding, command-line-interface, stanford-nlp","<p>This information is encoded in exactly the form you want in the <a href=""https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/trees/international/pennchinese/RadicalMap.java"" rel=""nofollow""><code>RadicalMap</code> source code</a>.</p>

<p>See the static initializer:</p>

<pre><code>String[] radLists = {""\u4e00\u4e00\u4e01\u4e02\u4e03..."", ""..."", ..., };
</code></pre>

<p>Each string in this list has as its first character a radical, and the remaining characters have that first character as their primary radical.</p>

<p>It's a package-local static variable, so there's not exactly a clean way to access it programmatically.. but you could easily rip its definition out of the source code and use it for whatever need you have.</p>
",1,0,35,2015-01-31 10:56:24,https://stackoverflow.com/questions/28250785/swiftly-generate-and-sort-full-encoding-dictionary-and-corresponding-primary-rad
"In the CoreNLP pipeline, is it possible to use the Coref tool (dcoref) with the new dependency parser tool (depparse)?","<p>This is how you would normally initialize a pipeline to run on some text:</p>

<pre><code>//stanford NLP
static Properties props = new Properties();
static StanfordCoreNLP pipeline;    
static void initStanfordPipeline() {
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref""); // depparse is an option for using a new dependency parsing model
    pipeline = new StanfordCoreNLP(props);
}
</code></pre>

<p>I get the following error when I try 'depparse' instead of 'parse' as an option in the pipeline:</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: annotator ""dcoref"" requires annotator ""parse""
</code></pre>
","java, stanford-nlp","<p>Good question! This currently isn't possible in the pipeline, though it really ought to be. I'll bring it up in our next development meeting.</p>

<p>For now, if you know that your pipeline doesn't require constituency parses, you can easily get around this by setting a property in the pipeline flags: <code>-enforceRequirements false</code>.</p>

<p>However, it looks like you're using <code>dcoref</code>, which does require constituency parses --- so there's no way around using the <code>parse</code> annotator, unfortunately.</p>
",1,3,331,2015-02-03 16:03:17,https://stackoverflow.com/questions/28303398/in-the-corenlp-pipeline-is-it-possible-to-use-the-coref-tool-dcoref-with-the
Detect relation between two persons in text,"<p>Goal is to find all the pairs of persons between which there is <em>any</em> kind of relation in a piece of text. Particularly, if we have this piece of text:</p>

<blockquote>
  <p>Alice Wilson, doctor with more than 30 years of experience in suppressing virus epidemics, has met with the president of Neverland
  country, John Doe, to discuss ways of tackling a new virus.</p>
  
  <p>John Doe will meet next week with the state official Jack Sparrow of some other country to discuss something
  totally-unrelated-to-the-first-part-of-text.</p>
</blockquote>

<p>There is a relation between <code>Alice Wilson</code> and <code>John Doe</code>, as well as <code>John Doe</code> and <code>Jack Sparrow</code>. However, there is no real relation between <code>Alice Wilson</code> and <code>Jack Sparrow</code>, apart from that they both appear in the same text.</p>

<p>Therefore, the resulting pairs would be:</p>

<pre><code>Alice Wilson, John Doe
John Doe, Jack Sparrow
</code></pre>

<p>I have found a way to extract people's names from text using Stanford CoreNLP's Named Entity Recognizer, thus having <code>Alice Wilson</code>, <code>John Doe</code> and <code>Jack Sparrow</code>, but I am not sure how to find relations between them. CoreNLP can perform Part-of-Speech tagging, which can tag words with <code>Subject</code>, <code>Verb</code>, <code>Object</code> etc. Nonetheless, I still don't see a way of tackling all the possible variations where a person's name can be found, e.g.:</p>

<pre><code>John Doe said that... &lt;- 'John Doe' is a Subject
Jack Sparrow introduced John Doe to the senior officials... &lt;- 'John Doe' is an Object
Jack Sparrow, John Doe's cousin, told the press... &lt;- Not even sure what 'John Doe' here is.
</code></pre>

<p>Any ideas (code is welcome, but not necessary) on how to approach this?</p>
","nlp, stanford-nlp, opennlp","<p>You can see if there's a dependency path between the two entities in the sentence. For more info: <a href=""http://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/stanford-dependencies.shtml</a></p>

<p>It won't be 100% accurate but good enough. To improve accuracy, you can prune the paths that are longer than certain lengths or have certain dependencies.</p>

<p>You can also look at open information extraction tools like <a href=""http://reverb.cs.washington.edu"" rel=""nofollow"">http://reverb.cs.washington.edu</a> </p>
",2,1,1228,2015-02-04 12:50:06,https://stackoverflow.com/questions/28321853/detect-relation-between-two-persons-in-text
ssplit.eolonly with Chinese text,"<p>I am trying to parse a raw Chinese text file (one line per sentence) with the Stanford NN Dependency Parser.</p>

<p>For English text I was able to use the 'ssplit' annotator with the 'ssplit.eolonly' option in order to split the document into sentences, however this option seems to fail for Chinese. Parsing works for an input file with a single line, however if there are multiple lines they are treated as a single sentence.</p>

<p>Is there a simple solution to get 'ssplit.eolonly' to work with Chinese? The command I ran was as follows:</p>

<pre><code>java edu.stanford.nlp.pipeline.StanfordCoreNLP \
-annotators segment,ssplit,pos,depparse \
-customAnnotatorClass.segment edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator \
-segment.model edu/stanford/nlp/models/segmenter/chinese/ctb.gz \
-segment.sighanCorporaDict edu/stanford/nlp/models/segmenter/chinese \
-segment.serDictionary edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz \
-segment.sighanPostProcessing true \
-ssplit.eolonly \
-pos.model edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger \
-depparse.model edu/stanford/nlp/models/parser/nndep/CTB_CoNLL_params.txt.gz \
-depparse.language Chinese \
-file in -outputDirectory out
</code></pre>
",stanford-nlp,"<p>Unfortunately, not at the moment (Apr 2015). The current segmenter doesn't support preserving line information. This would be a good thing to fix at some point....</p>
",0,0,347,2015-02-05 06:29:55,https://stackoverflow.com/questions/28337548/ssplit-eolonly-with-chinese-text
How can I get the edges containing the &quot;root&quot; modifier dependency in the Stanford NLP parser?,"<p>I have created the dependency graph for my scenario that takes a text input. </p>

<pre><code>SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
</code></pre>

<p>I am successful in getting the ""num"" modifier dependency using the following code:</p>

<pre><code>List&lt;SemanticGraphEdge&gt; edgesContainingNumModifierDependency = dependencies.findAllRelns(GrammaticalRelation.valueOf(""num""));
</code></pre>

<p>However, I want to find the edges pertaining to the ""root"" and hence, the following </p>

<pre><code>List&lt;SemanticGraphEdge&gt; edgesContainingRootModifierDependency = dependencies.findAllRelns(GrammaticalRelation.valueOf(""root""));
</code></pre>

<p>does not seem to work. </p>

<p>Can anyone explain why? And how I can get the edge for root? </p>
","nlp, stanford-nlp","<p>We don't actually store a <code>SemanticGraphEdge</code> between the root word and a dummy <code>ROOT</code> node. (You can see that the dependency is manually tacked on in public-facing methods <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/semgraph/SemanticGraph.java#L1406"" rel=""nofollow"">like <code>toList</code></a>).</p>

<p>From the <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html"" rel=""nofollow""><code>SemanticGraph</code> documentation</a>:</p>

<blockquote>
  <p>The root is not at present represented as a vertex in the graph. At present you need to get a root/roots from the separate roots variable and to know about it. This should maybe be changed, because otherwise, doing things like simply getting the set of nodes or edges from the graph doesn't give you root nodes or edges.</p>
</blockquote>

<p>You might be able to get what you want, though, with <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html#getFirstRoot--"" rel=""nofollow""><code>SemanticGraph#getFirstRoot</code></a>.</p>
",1,0,234,2015-02-05 21:16:59,https://stackoverflow.com/questions/28354161/how-can-i-get-the-edges-containing-the-root-modifier-dependency-in-the-stanfor
Unknown symbol in nltk pos tagging for Arabic,"<p>I have used nltk to tokenize some arabic text </p>

<p>However, i ended up with some results like </p>

<p>(u'an arabic character/word', '``')
or 
(u'an arabic character/word', ':')</p>

<p>However, they do not provide the `` or : in the documentation. </p>

<p>hence i would like to find out what is this </p>

<pre><code>from nltk.toeknize.punkt import PunktWordTokenizer 

z = ""أنا تسلق شجرة""
tkn = PunkWordTokenizer
sen = tkn.tokenize(z)
tokens = nltk.pos_tag(sent)

print tokens
</code></pre>
","python, nlp, nltk, stanford-nlp, pos-tagger","<p>The default NLTK POS tag is trained on English texts and is supposedly for English text processing, see <a href=""http://www.nltk.org/_modules/nltk/tag.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/tag.html</a>. The docs:</p>

<pre><code>An off-the-shelf tagger is available.  It uses the Penn Treebank tagset:

    &gt;&gt;&gt; from nltk.tag import pos_tag  # doctest: +SKIP
    &gt;&gt;&gt; from nltk.tokenize import word_tokenize # doctest: +SKIP
    &gt;&gt;&gt; pos_tag(word_tokenize(""John's big idea isn't all that bad."")) # doctest: +SKIP
    [('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is',
    'VBZ'), (""n't"", 'RB'), ('all', 'DT'), ('that', 'DT'), ('bad', 'JJ'),
    ('.', '.')]
</code></pre>

<p>And the code for <code>pos_tag</code>:</p>

<pre><code>from nltk.data import load


# Standard treebank POS tagger
_POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'
def pos_tag(tokens):
    """"""
    Use NLTK's currently recommended part of speech tagger to
    tag the given list of tokens.

        &gt;&gt;&gt; from nltk.tag import pos_tag # doctest: +SKIP
        &gt;&gt;&gt; from nltk.tokenize import word_tokenize # doctest: +SKIP
        &gt;&gt;&gt; pos_tag(word_tokenize(""John's big idea isn't all that bad."")) # doctest: +SKIP
        [('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is',
        'VBZ'), (""n't"", 'RB'), ('all', 'DT'), ('that', 'DT'), ('bad', 'JJ'),
        ('.', '.')]

    :param tokens: Sequence of tokens to be tagged
    :type tokens: list(str)
    :return: The tagged tokens
    :rtype: list(tuple(str, str))
    """"""
    tagger = load(_POS_TAGGER)
    return tagger.tag(tokens)
</code></pre>

<p>This works for me to get Stanford tools working in python on Ubuntu 14.4.1:</p>

<pre><code>$ cd ~
$ wget http://nlp.stanford.edu/software/stanford-postagger-full-2015-01-29.zip
$ unzip stanford-postagger-full-2015-01-29.zip
$ wget http://nlp.stanford.edu/software/stanford-segmenter-2015-01-29.zip
$ unzip /stanford-segmenter-2015-01-29.zip
$ python
</code></pre>

<p>and then:</p>

<pre><code>from nltk.tag.stanford import POSTagger
path_to_model= '/home/alvas/stanford-postagger-full-2015-01-30/models/arabic.tagger'
path_to_jar = '/home/alvas/stanford-postagger-full-2015-01-30/stanford-postagger-3.5.1.jar'

artagger = POSTagger(path_to_model, path_to_jar, encoding='utf8')
artagger._SEPARATOR = '/'
tagged_sent = artagger.tag(u""أنا تسلق شجرة"")
print(tagged_sent)
</code></pre>

<p>[out]:</p>

<pre><code>$ python3 test.py
[('أ', 'NN'), ('ن', 'NN'), ('ا', 'NN'), ('ت', 'NN'), ('س', 'RP'), ('ل', 'IN'), ('ق', 'NN'), ('ش', 'NN'), ('ج', 'NN'), ('ر', 'NN'), ('ة', 'PRP')]
</code></pre>

<p>If you have java problems when using Stanford POS tagger, see DELPH-IN wiki: <a href=""http://moin.delph-in.net/ZhongPreprocessing"" rel=""nofollow"">http://moin.delph-in.net/ZhongPreprocessing</a></p>
",4,0,1485,2015-02-06 07:10:22,https://stackoverflow.com/questions/28360402/unknown-symbol-in-nltk-pos-tagging-for-arabic
Stanford CoreNLP Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl,"<p>I am trying to learn the Stanford CoreNLP library. I am using C# with the posted example (<a href=""https://sergeytihon.wordpress.com/2013/10/26/stanford-corenlp-is-available-on-nuget-for-fc-devs/"" rel=""nofollow"">https://sergeytihon.wordpress.com/2013/10/26/stanford-corenlp-is-available-on-nuget-for-fc-devs/</a>).  I loaded the package “Stanford.NLP.CoreNLP” (it added IKVM.NET) via nuget and downloaded the code. Unzipped the .jar models. My directory is correct.  I get the following error:</p>

<pre><code>&gt; edu.stanford.nlp.util.ReflectionLoading.ReflectionLoadingException was
&gt; unhandled HResult=-2146233088 Message=Error creating
&gt; edu.stanford.nlp.time.TimeExpressionExtractorImpl
&gt; Source=stanford-corenlp-3.5.0 StackTrace: at
&gt; edu.stanford.nlp.util.ReflectionLoading.loadByReflection(String
&gt; className, Object[] arguments) at
&gt; edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(String
&gt; className, String name, Properties props) at
&gt; edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(String
&gt; name, Properties props) at
&gt; edu.stanford.nlp.ie.regexp.NumberSequenceClassifier..ctor(Properties
&gt; props, Boolean useSUTime, Properties sutimeProps) at
&gt; edu.stanford.nlp.ie.NERClassifierCombiner..ctor(Boolean
&gt; applyNumericClassifiers, Boolean useSUTime, Properties nscProps,
&gt; String[] loadPaths) at
&gt; edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(Properties
&gt; properties) at edu.stanford.nlp.pipeline.AnnotatorFactories.6.create()
&gt; at edu.stanford.nlp.pipeline.AnnotatorPool.get(String name) at
&gt; edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(Properties A_1,
&gt; Boolean A_2, AnnotatorImplementations A_3) at
&gt; edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(Properties props,
&gt; Boolean enforceRequirements) at
&gt; edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(Properties props) at
&gt; ConsoleApplication1.Program.Main(String[] args) in
&gt; d:\Programming_Code\VisualStudio\visual studio
&gt; 2013\Projects\AutoWikify\ConsoleApplication1\ConsoleApplication1\Program.cs:line
&gt; 30 at System.AppDomain._nExecuteAssembly(RuntimeAssembly assembly,
&gt; String[] args) at
&gt; Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly() at
&gt; System.Threading.ExecutionContext.RunInternal(ExecutionContext
&gt; executionContext, ContextCallback callback, Object state, Boolean
&gt; preserveSyncCtx) at
&gt; System.Threading.ExecutionContext.Run(ExecutionContext
&gt; executionContext, ContextCallback callback, Object state, Boolean
&gt; preserveSyncCtx) at
&gt; System.Threading.ExecutionContext.Run(ExecutionContext
&gt; executionContext, ContextCallback callback, Object state) at
&gt; System.Threading.ThreadHelper.ThreadStart() InnerException:
&gt; edu.stanford.nlp.util.MetaClass.ClassCreationException
&gt; HResult=-2146233088 Message=MetaClass couldn’t create public
&gt; edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties)
&gt; with args [sutime, {sutime.binders=0, annotators=tokenize, ssplit,
&gt; pos, lemma, ner, parse, dcoref}] Source=stanford-corenlp-3.5.0
&gt; StackTrace: at
&gt; edu.stanford.nlp.util.MetaClass.ClassFactory.createInstance(Object[]
&gt; params) at edu.stanford.nlp.util.MetaClass.createInstance(Object[]
&gt; objects) at
&gt; edu.stanford.nlp.util.ReflectionLoading.loadByReflection(String
&gt; className, Object[] arguments) InnerException:
&gt; java.lang.reflect.InvocationTargetException HResult=-2146233088
&gt; Message=”” Source=stanford-corenlp-3.5.0 StackTrace: at __(Object[] )
&gt; at
&gt; Java_sun_reflect_ReflectionFactory.FastConstructorAccessorImpl.newInstance(Object[]
&gt; args) at java.lang.reflect.Constructor.newInstance(Object[] initargs,
&gt; CallerID ) at
&gt; edu.stanford.nlp.util.MetaClass.ClassFactory.createInstance(Object[]
&gt; params) InnerException:
</code></pre>

<p>Here is my code:</p>

<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using java.util;
using java.io;
using edu.stanford.nlp.pipeline;
using Console = System.Console;

namespace ConsoleApplication1
{
class Program
{
static void Main(string[] args)
{
// Path to the folder with models extracted from `stanford-corenlp-3.4-models.jar`
var jarRoot = @”D:\Programming_SDKs\stanford-corenlp-full-2015-01-30\stanford-corenlp-3.5.1-models\”;

// Text for processing
var text = “Kosgi Santosh sent an email to Stanford University. He didn't get a reply.”;

// Annotation pipeline configuration
var props = new Properties();
props.setProperty(“annotators”, “tokenize, ssplit, pos, lemma, ner, parse, dcoref”);
props.setProperty(“sutime.binders”, “0”);

// We should change current directory, so StanfordCoreNLP could find all the model files automatically
var curDir = Environment.CurrentDirectory;
System.IO.Directory.SetCurrentDirectory(jarRoot);
var pipeline = new StanfordCoreNLP(props);
System.IO.Directory.SetCurrentDirectory(curDir);

// Annotation
var annotation = new Annotation(text);
pipeline.annotate(annotation);

// Result – Pretty Print
using (var stream = new ByteArrayOutputStream())
{
pipeline.prettyPrint(annotation, new PrintWriter(stream));
Console.WriteLine(stream.toString());
stream.close();
}
}
}
}
</code></pre>
","c#, nlp, stanford-nlp","<p>3.5.1 is currently (2/11/2015) not supported.  It works with</p>

<p><a href=""http://nlp.stanford.edu/software/stanford-corenlp-full-2014-10-31.zip"" rel=""nofollow"">http://nlp.stanford.edu/software/stanford-corenlp-full-2014-10-31.zip</a> </p>
",1,4,3276,2015-02-08 01:25:58,https://stackoverflow.com/questions/28389564/stanford-corenlp-error-creating-edu-stanford-nlp-time-timeexpressionextractorimp
StanfordCoreNLP: why two different data structures for cons. parse and dependency parse?,"<p>Why Stanford:CoreNLP is using different data structures to represent its  trees (e.g. dep. trees with 'BasicDependenciesAnnotation' and cons. tree with 'TreeAnnotation')? 
It seems like these annotation are representable with the same data structure (like a DAG with labels). Is there any mechanism to cast these to each other? (at least for some of them)</p>

<p>LINK: <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>
","nlp, stanford-nlp","<p>The dependency parses, when collapsed, are not necessarily DAGs. From the <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow"">Stanford Dependencies manual</a>:</p>

<blockquote>
  <p>The collapsed and CCprocessed dependencies are not a DAG. The graphs can contain small cycles between two nodes (only). These don’t seem eliminable given the current representational choices. They occur with relative clauses such as <em>the woman who introduced you</em>. The cycles occur once you wish to represent the referent of <em>who</em>. In the basic plus extras representation, you get <code>rcmod(woman, introduced)</code>, <code>nsubj(introduced, who)</code>, and <code>ref(woman, who)</code>. In the collapsing process, ref arcs are collapsed, and so there is then a two node cycle: <code>rcmod(woman, introduced)</code> and <code>nsubj(introduced, woman)</code>.</p>
</blockquote>
",2,2,127,2015-02-10 15:39:10,https://stackoverflow.com/questions/28435662/stanfordcorenlp-why-two-different-data-structures-for-cons-parse-and-dependenc
Sentence-level to document-level sentiment analysis. Analysing news,"<p>I need to perform sentiment analysis on news articles about a specific topic using the Stanford NLP tool.</p>

<p>Such tool only allows sentence based sentiment analysis while I would like to extract a sentiment evaluation of the whole articles with respect to my topic.</p>

<p>For instance, if my topic is Apple, I would like to know the sentiment of a news article with respect to Apple.</p>

<p>Just computing the average of the sentences in my articles won't do. For instance, I might have an article saying something along the lines of ""Apple is very good at this, and this and that. While Google products are very bad for these reasons"". Such an article would result in a Neutral classification using the average score of sentences, while it is actually a Very positive article about Apple.</p>

<p>On the other hand filtering my sentences to include only the ones containing the word Apple would miss articles along the lines of ""Apple's product A is pretty good. However, it lacks the following crucial features: ..."". In this case the effect of the second sentence would be lost if I were to use only the sentences containing the word Apple.</p>

<p>Is there a standard way of addressing this kind of problems? Is Stanford NLP the wrong tool to accomplish my goal?</p>
","stanford-nlp, sentiment-analysis","<p>Update: You might want to look into 
<a href=""http://blog.getprismatic.com/deeper-content-analysis-with-aspects/"" rel=""nofollow"">http://blog.getprismatic.com/deeper-content-analysis-with-aspects/</a></p>

<p>This is a very active area of research so it would be hard to find an off-the-shelf tool to do this (at least nothing is built in the Stanford CoreNLP). Some pointers: look into aspect-based sentiment analysis. In this case, Apple would be an ""aspect"" (not really but can be modeled that way). Andrew McCallum's group at UMass, Bing Liu's group at UIC, Cornell's NLP group, among others, have worked on this problem.</p>

<p>If you want a quick fix, I would suggest to extract sentiment from sentences that have reference to Apple and its products; use coref (check out dcoref annotator in Stanford CoreNLP), which will increase the recall of sentences and solve the problem of sentences like ""However, it lacks.."".</p>
",4,2,852,2015-02-11 11:38:04,https://stackoverflow.com/questions/28453404/sentence-level-to-document-level-sentiment-analysis-analysing-news
How to use Stanford CoreNLP java library with Ruby for sentiment analysis?,"<p>I'm trying to do <strong>sentiment analysis</strong> on a <strong>large corpus of tweets</strong> in a local MongoDB instance with Ruby on Rails 4, Ruby 2.1.2 and Mongoid ORM.</p>

<p>I've used the freely available <a href=""https://loudelement-free-natural-language-processing-service.p.mashape.com"" rel=""nofollow"">https://loudelement-free-natural-language-processing-service.p.mashape.com</a> API on Mashape.com, however it starts timing out after pushing through a few hundred tweets in rapid fire sequence -- clearly it isn't meant for going through tens of thousands of tweets and that's understandable.</p>

<p>So next I thought I'd <strong>use the Stanford CoreNLP library</strong> promoted here: <a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/code.html</a></p>

<p>The default usage, in addition to using the library in Java 1.8 code, seems to be to use XML input and output files. For my use case this is annoying given I have tens of thousands of short tweets as opposed to long text files. I would want to use CoreNLP like a method and do a tweets.each type of loop.</p>

<p>I guess one way would be to construct an XML file with all of the tweets and then get one out of the Java process and parse that and put it back to the DB, but that feels alien to me and would be a lot of work.</p>

<p>So, I was happy to find on the site linked above a way to <strong>run CoreNLP from the command line and accept the text as stdin</strong> so that I didn't have to start fiddling with the filesystem but rather feed the text as a parameter. However, starting up the JVM separately for each tweet adds a huge overhead compared to using the loudelement free sentiment analysis API.</p>

<p>Now, the code I wrote is ugly and slow but it works. Still, I'm wondering if there's a better way to run the CoreNLP java program from within Ruby without having to start fiddling with the filesystem (creating temp files and giving them as params) or writing Java code?</p>

<p>Here's the code I'm using:</p>

<pre><code>def self.mass_analyze_w_corenlp # batch run the method in multiple Ruby processes
  todo = Tweet.all.exists(corenlp_sentiment: false).limit(5000).sort(follow_ratio: -1) # start with the ""least spammy"" tweets based on follow ratio
  counter = 0

  todo.each do |tweet|
    counter = counter+1

    fork {tweet.analyze_sentiment_w_corenlp} # run the analysis in a separate Ruby process

    if counter &gt;= 5 # when five concurrent processes are running, wait until they finish to preserve memory
      Process.waitall
      counter = 0
    end

  end
end

def analyze_sentiment_w_corenlp # run the sentiment analysis for each tweet object
  text_to_be_analyzed = self.text.gsub(""'""){"" ""}.gsub('""'){' '} # fetch the text field of DB item strip quotes that confuse the command line

  start = ""echo '""
  finish = ""' | java -cp 'vendor/corenlp/*' -mx250m edu.stanford.nlp.sentiment.SentimentPipeline -stdin""
  command_string = start+text_to_be_analyzed+finish # assemble the command for the command line usage below

  output =`#{command_string}` # run the CoreNLP on the command line, equivalent to system('...')
  to_db = output.gsub(/\s+/, """").downcase # since CoreNLP uses indentation, remove unnecessary whitespace
  # output is in the format of ""neutral, ""positive"", ""negative"" and so on

  puts ""Sentiment analysis successful, sentiment is: #{to_db} for tweet #{text_to_be_analyzed}.""

  self.corenlp_sentiment = to_db # insert result as a field to the object
  self.save! # sentiment analysis done!
end
</code></pre>
","java, ruby, twitter, nlp, stanford-nlp","<p>As suggested in the comments by @Qualtagh, I decided to use <strong>JRuby</strong>. </p>

<p>I first attempted to use Java to use MongoDB as the interface (read directly from MongoDB, analyze with Java / CoreNLP and write back to MongoDB), but the MongoDB Java Driver was more complex to use than the Mongoid ORM I use with Ruby, so this is why I felt JRuby was more appropriate. </p>

<p>Doing a REST service for Java would have required me first to learn how to do a REST service in Java, which might have been easy, or then not. I didn't want to spend time figuring that out.</p>

<p>So the code I needed to do to run my code was:</p>

<pre><code>  def analyze_tweet_with_corenlp_jruby
    require 'java'
    require 'vendor/CoreNLPTest2.jar' # I made this Java JAR with IntelliJ IDEA that includes both CoreNLP and my initialization class

    analyzer = com.me.Analyzer.new # this is the Java class I made for running the CoreNLP analysis, it initializes the CoreNLP with the correct annotations etc.
    result = analyzer.analyzeTweet(self.text) # self.text is where the text-to-be-analyzed resides

    self.corenlp_sentiment = result # adds the result into this field in the MongoDB model
    self.save!
    return ""#{result}: #{self.text}"" # for debugging purposes
  end
</code></pre>
",0,4,785,2015-02-11 20:42:31,https://stackoverflow.com/questions/28464095/how-to-use-stanford-corenlp-java-library-with-ruby-for-sentiment-analysis
Stanford NLP: Tokenize output on a single line?,"<p>Can we have a tokenizer output on a single line like that of Apache OpenNLP with the command line tool?
<a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokenizer.shtml</a>
<a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.tokenizer"" rel=""nofollow"">https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.tokenizer</a></p>
",stanford-nlp,"<p>You can use <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/DocumentPreprocessor.html"" rel=""nofollow noreferrer""><code>DocumentPreprocessor</code></a>, either programmatically or from the command line.</p>

<p>From the CLI:</p>

<pre><code>$ echo ""This is a test. And some more."" | java edu.stanford.nlp.process.DocumentPreprocessor 2&gt;/dev/null
This is a test .
And some more .
</code></pre>

<p>You can do the same thing programmatically; see <a href=""https://stackoverflow.com/a/9493264/176075"">this SO answer</a>.</p>
",1,0,144,2015-02-12 03:13:35,https://stackoverflow.com/questions/28468648/stanford-nlp-tokenize-output-on-a-single-line
Stanford NLP: Sentence splitting without tokenization?,"<p>Can I detect sentences via the command line interface of Stanford NLP like Apache OpenNLP?
<a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.sentdetect"" rel=""nofollow"">https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.sentdetect</a></p>

<p>Based on the docs, Stanford NLP requires tokenization as per <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>
",stanford-nlp,"<p>Our pipeline requires that you tokenize first; we use these tokens in the sentence-splitting algorithm. If your text is pre-tokenized, you can use <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/DocumentPreprocessor.html"" rel=""nofollow""><code>DocumentPreproccesor</code></a> and request whitespace-only tokenization.</p>

<p>Let me know if I misunderstood your question.</p>
",1,1,420,2015-02-12 03:28:28,https://stackoverflow.com/questions/28468768/stanford-nlp-sentence-splitting-without-tokenization
Converting Stanford dependency relation to dot format,"<p>I am a newbie to this field. I have dependency relation in this form:</p>

<pre><code>amod(clarity-2, sound-1)
nsubj(good-6, clarity-2)
cop(good-6, is-3)
advmod(good-6, also-4)
neg(good-6, not-5)
root(ROOT-0, good-6)
nsubj(ok-10, camera-8)
cop(ok-10, is-9)
ccomp(good-6, ok-10)
</code></pre>

<p>As mentioned in the links we have to convert this dependency relation to dot format and then use Graphviz for drawing a 'dependency tree'. I am not able to understand how to pass this dependency relation to toDotFormat() function of edu.stanford.nlp.semgraph.SemanticGraph. When I give this string, 'amod(clarity-2, sound-1)' as input to toDotFormat() am getting the output in this form digraph amod(clarity-2, sound-1) { }.
I am trying the solution given here <a href=""https://stackoverflow.com/questions/13468872/how-to-get-a-dependency-tree-with-stanford-nlp-parser/28493020#28493020"">how to get a dependency tree with Stanford NLP parser</a></p>
","parsing, stanford-nlp","<p>You need to call <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html#toDotFormat--"" rel=""nofollow""><code>toDotFormat</code></a> on an entire dependency tree. How have you generated these dependency trees in the first place?</p>

<p>If you're using the StanfordCoreNLP pipeline, adding in the <code>toDotFormat</code> call is easy:</p>

<pre><code>Properties properties = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, depparse"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

String text = ""This is a sentence I want to parse."";
Annotation document = new Annotation(text);

pipeline.annotate(document);

// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

for (CoreMap sentence : sentences) {
  // this is the Stanford dependency graph of the current sentence
  SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
  System.out.println(dependencies.toDotFormat());
}
</code></pre>
",1,1,686,2015-02-13 07:02:34,https://stackoverflow.com/questions/28494188/converting-stanford-dependency-relation-to-dot-format
Get TypedDependencies using StanfordParser Shift Reduce Parser,"<p>I am trying to use the Stanford Shift Reduce Parser with the Spanish model supplied. I am noticing, however, that unlike the Lexicalized Parser, I cannot get the TypedDependencies, despite sending the adequate flag -outputFormat typedDependencies, as it can be seen in lexparser.bat/sh.</p>

<p>Just in case, this is the Java code I'm using to pass the flags and creating the parser.</p>

<pre><code>ShiftReduceParser model = ShiftReduceParser.loadModel(modelPath);
model.setOptionFlags(""-factored"", ""-outputFormat"", ""penn,typedDependencies"");
ArrayList&lt;TaggedWord&gt; taggedWords = new ArrayList&lt;TaggedWord&gt;();
</code></pre>

<p>Thank you</p>
","stanford-nlp, shift-reduce","<p>The problem here is not the ShiftReduceParser, but simply that we don't currently support typed dependencies for Spanish currently - we only have them for English and Chinese.</p>

<p>(Looking ahead, the most likely thing to appear first is support for <a href=""http://universaldependencies.github.io/docs/"" rel=""nofollow"">Universal Dependencies</a> in the <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">Neural Network Dependency Parser</a>. Indeed, you could probably train such a model yourself now.)</p>
",2,0,162,2015-02-16 18:32:43,https://stackoverflow.com/questions/28548053/get-typeddependencies-using-stanfordparser-shift-reduce-parser
StanfordNLP Spanish Tokenizer,"<p>I want to tokenize a text in Spanish  with StanfordNLP and my problem is that the model splits any word matching the pattern ""\d*s "" (a word composed by digits and ending with an ""s"") in two tokens. If the word finished with another letter, such as ""e"", the tokenizer return only one token. </p>

<p>For instance, given the sentence:
""Vendo iPhone 5s  es libre de fabrica esta nuevo sin usar.""</p>

<p>The tokenizer return for the text ""iPhone 5s"" three tokens:""iPhone"", ""5"" and ""s"".</p>

<p>Someone has an idea how could I avoid this behaviour?</p>
","tokenize, stanford-nlp","<p>I suppose you are working with the SpanishTokenizer rather than PTBTokenizer.</p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/international/spanish/process/SpanishTokenizer.html"" rel=""nofollow"">SpanishTokenizer</a> is heavily based on the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/international/french/process/FrenchTokenizer.html"" rel=""nofollow"">FrenchTokenizer</a>, which comes also  from the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/PTBTokenizer.html"" rel=""nofollow"">PTBTokenizer</a> (English).</p>

<p>I've run all three with your sentence and seems that the PTBTokenizer give you the results you need, but not the others.</p>

<p>As all of them are deterministic tokenizers I think you can't avoid that problem because seems to me that the problem is not in the heuristic part which should run later after the deterministic.</p>

<p>A possible workaround may be to use <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/WhitespaceTokenizer.html"" rel=""nofollow"">WhitespaceTokenizer</a>, as long as you don't mind having punctuation tokens or some other gramma rules.</p>
",2,0,1102,2015-02-17 17:54:05,https://stackoverflow.com/questions/28567882/stanfordnlp-spanish-tokenizer
NLP Postagger can&#39;t grok imperatives?,"<p>Stanford NLP postagger claims imperative verbs added to recent version. I've inputted lots of text with abundant and obvious imperatives, but there seems to be no tag for them on output. Must one, after all, train it for this pos? </p>
","stanford-nlp, pos-tagger","<p>There is no special tag for imperatives, they are simply tagged as <code>VB</code>. </p>

<p>The info on the website refers to the fact that we added a bunch of manually annotated imperative sentences to our training data such that the POS tagger gets more of them right, i.e. tags the verb as <code>VB</code>. </p>
",3,1,424,2015-02-18 04:07:05,https://stackoverflow.com/questions/28575795/nlp-postagger-cant-grok-imperatives
CoreNLP MaxentTagger Data Format Error,"<p>I'm using Stanford CoreNLP for NLP processing and am in the process of training the POS tagger with more domain specific data. However, for some reason, the trainer is throwing ""Data format error"" when I run it with the properties file I've got. Here's the context:</p>

<p><strong>Training file</strong></p>

<pre><code>Please#UH let#VBP us#PRP know#VB if#IN you#PRP have#VBP any#DT other#JJ thoughts#NNS that#WDT...
</code></pre>

<p>(Basically a very long 1-line word + tag set.)</p>

<p><strong>Training properties file</strong></p>

<pre><code>         model = special_postagger.tagger
                  arch = words(-1,1),unicodeshapes(-1,1),order(2),suffix(4)
          wordFunction = 
             trainFile = /path/to/POS_trainer1.csv
       closedClassTags = 
closedClassTagThreshold = 40
curWordMinFeatureThresh = 2
                 debug = false
           debugPrefix = 
          tagSeparator = #
              encoding = UTF-8
            iterations = 100
                  lang = 
  learnClosedClassTags = false
      minFeatureThresh = 5
         openClassTags = 
rareWordMinFeatureThresh = 10
        rareWordThresh = 5
                search = qn
                  sgml = false
          sigmaSquared = 0.5
                 regL1 = 1.0
             tagInside = 
              tokenize = true
      tokenizerFactory = 
      tokenizerOptions = 
               verbose = false
        verboseResults = true
  veryCommonWordThresh = 250
              xmlInput = 
            outputFile = 
          outputFormat = slashTags
   outputFormatOptions = 
              nthreads = 1
</code></pre>

<p><strong>Command Run</strong></p>

<pre><code>java edu.stanford.nlp.tagger.maxent.MaxentTagger -prop myProps.props
</code></pre>

<p>But for some reason, I get this error message:</p>

<pre><code>warning: no language set, no open-class tags specified, and no closed-class tags specified; assuming ALL tags are open class tags
TaggerExperiments: adding word/tags
Exception in thread ""main"" java.lang.IllegalArgumentException: Data format error: can't find delimiter ""#"" in word ""as"" (line 2 of /path/to/POS_Trainer1.csv)
at edu.stanford.nlp.tagger.io.TextTaggedFileReader.primeNext(TextTaggedFileReader.java:74)
at edu.stanford.nlp.tagger.io.TextTaggedFileReader.&lt;init&gt;(TextTaggedFileReader.java:34)
at edu.stanford.nlp.tagger.io.TaggedFileRecord.reader(TaggedFileRecord.java:111)
at edu.stanford.nlp.tagger.maxent.ReadDataTagged.&lt;init&gt;(ReadDataTagged.java:52)
at edu.stanford.nlp.tagger.maxent.TaggerExperiments.&lt;init&gt;(TaggerExperiments.java:86)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.trainAndSaveModel(MaxentTagger.java:1140)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.runTraining(MaxentTagger.java:1207)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1839)
</code></pre>
","java, nlp, stanford-nlp, illegalargumentexception","<p>Answering my own question here: the training file <em>must</em> have a <em>perfect</em> format of [word][delimiter][tag], or else it will throw fatal runtime error. You can use whatever delimiter you want, such as the hashtag # symbol, for example, but if there are:</p>

<ul>
<li>whitespaces</li>
<li>missing tags</li>
</ul>

<p>between the [word][delimiter][tag] pattern, it will fail.</p>
",1,0,274,2015-02-19 20:32:51,https://stackoverflow.com/questions/28616317/corenlp-maxenttagger-data-format-error
Does the Stanford NER CRF implementation use sentences in the training phase?,"<p>I am new to CRFs and some of my terminology might be skewed so bear with me. I'm assuming the Stanford NER implements a linear chain CRF. </p>

<p>Let x be a sequence of words and y the sequence of corresponding tags. Call x an example and y its label. A component x_i of x is a word. A component y_i of y is a tag.</p>

<ol>
<li><p>When training the model we provide it with something like: </p>

<pre><code>James    PERSON
lives    O
in       O
Chicago  LOCATION
.        O
Coffee   O
in       O
Trieste  LOCATION
is       O
great    O
.        O 
</code></pre>

<p>Does model use individual sentences as examples? Using the data above is one of the examples: <strong>&lt; Coffee in Trieste is Great . ></strong>? Does this mean that a feature functions cannot depend on words in previous sentences? </p></li>
<li><p>If this is indeed the case, how does the model make sure that each example is indeed a sentence? Does it do any sentence boundary detection? Can it be made to look at e.g. batches of 4 sentences? </p></li>
</ol>

<p>Thank you in advance :) </p>
",stanford-nlp,"<p>Two newlines are considered boundary of an example. Your examples can be anything from phrases to the whole documents. So for your example, if you want two sentences as two examples:</p>

<pre>
James    PERSON
lives    O
in       O
Chicago  LOCATION
.        O


Coffee   O
in       O
Trieste  LOCATION
is       O
great    O
.        O 
</pre>
",3,0,481,2015-02-20 01:29:32,https://stackoverflow.com/questions/28620224/does-the-stanford-ner-crf-implementation-use-sentences-in-the-training-phase
English dictionary in readable format (text or xml),"<p>I am hoping to find a <strong>downloadable (free or paid) English dictionary</strong> preferably from Oxford, Cambridge, Webster in text or XML format to do some NLP.</p>

<p>I hope that each entry has </p>

<ul>
<li>a full part of speech,</li>
<li>pronunciation, </li>
<li>morphology in case of verb and noun</li>
<li>multiple sense/definition entries</li>
</ul>

<p>such as in the following page <a href=""http://www.merriam-webster.com/dictionary/side"" rel=""nofollow"">http://www.merriam-webster.com/dictionary/side</a>.</p>

<p>The actual text of the definition is not important. What I need most is the part of speech, pronunciation, morphology, order of definition entries.</p>

<p><strong>Also wondering</strong>: what does the Stanford NLP toolkit use as lexical resources when it does POS tagging?</p>

<p>Thank you.</p>
","nlp, stanford-nlp","<p><a href=""https://stackoverflow.com/questions/3794454/where-can-i-obtain-an-english-dictionary-with-structured-data"">Here</a> and <a href=""https://english.stackexchange.com/questions/3442/where-can-i-obtain-an-english-dictionary-with-structured-data"">here</a> are the similar questions. In summary:</p>

<ol>
<li><a href=""http://icon.shef.ac.uk/Moby/mpos.html"" rel=""nofollow noreferrer"">Part-of speech dictionary</a> - unfortunately, with quite narrow tag set.</li>
<li><a href=""http://www.speech.cs.cmu.edu/cgi-bin/cmudict"" rel=""nofollow noreferrer"">Pronouncing Dictionary</a></li>
<li>Multiple senses - <a href=""http://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a></li>
</ol>

<p>Morphological dictionary can be found in <a href=""http://nlp.lsi.upc.edu/freeling/index.php?option=com_content&amp;task=view&amp;id=23&amp;Itemid=58"" rel=""nofollow noreferrer"">FreeLing</a> distribution - see data/en/dicc.src. Btw, there are also senses and phonetic dictionaries.</p>

<p>About Stanford POS tagger: they use <a href=""http://www-nlp.stanford.edu/links/statnlp.html#Treebanks"" rel=""nofollow noreferrer"">Penn treebank</a>, <a href=""http://nlp.stanford.edu/software/pos-tagger-faq.shtml#models"" rel=""nofollow noreferrer"">proof</a></p>
",1,1,1645,2015-02-22 11:48:18,https://stackoverflow.com/questions/28657434/english-dictionary-in-readable-format-text-or-xml
Stanford CorpNLP returning wrong results,"<p>I am trying lemmatization with stanford corenlp following  <a href=""https://stackoverflow.com/questions/771918/how-do-i-do-word-stemming-or-lemmatization"">this</a> question. My environment is:-</p>

<ul>
<li>Java 1.7</li>
<li>Eclipse 3.4.0</li>
<li>StandfordCoreNLP version 3.4.1 (<a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow noreferrer"">downloaded from here</a>).</li>
</ul>

<p>my code snippet is:- </p>

<pre><code>//...........lemmatization starts........................

    Properties props = new Properties(); 
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma""); 
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props, false);
    String text = ""painting""; 
    Annotation document = pipeline.process(text);  

    List&lt;edu.stanford.nlp.util.CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    for(edu.stanford.nlp.util.CoreMap sentence: sentences) 

    {    
        for(CoreLabel token: sentence.get(TokensAnnotation.class))
        {       
            String word = token.get(TextAnnotation.class);      
            String lemma = token.get(LemmaAnnotation.class); 
            System.out.println(""lemmatized version :"" + lemma);
        }
    }

    //...........lemmatization ends.........................
</code></pre>

<p>the output i get is:-</p>

<pre><code>lemmatized version :painting
</code></pre>

<p>where i expect </p>

<pre><code>lemmatized version :paint
</code></pre>

<p>Please enlighten me.</p>
","java-7, stanford-nlp, eclipse-3.4, lemmatization","<p>The problem in this example is that the word painting can be the present participle of <em>to paint</em> or a noun and the output of the lemmatizer depends on the part-of-speech tag assigned to the original word. </p>

<p>If you run the tagger only on the fragment <em>painting</em>, then there is no context that could help the tagger (or a human) to decide how the word should be tagged. In this case it picked the tag <code>NN</code> and the lemma of the noun <em>painting</em> is in fact <em>painting</em>.</p>

<p>If you run the same code with the sentence ""I am painting a flower."" the tagger should correctly tag <em>painting</em> as <code>VBG</code> and the lemmatizer should return <em>paint</em>.</p>
",2,0,498,2015-02-23 16:46:18,https://stackoverflow.com/questions/28678811/stanford-corpnlp-returning-wrong-results
Manual tagging of Words using Stanford CorNLP,"<p>I have a resource where i know exactly the types of words. i have to lemmatize them but for correct results, i have to manually tag them. i could not find any code for manual tagging of words. i m using following code but it returns wrong result. i.e ""painting"" for ""painting"" where i expect ""paint"".</p>

<pre><code>*//...........lemmatization starts........................

Properties props = new Properties(); 
props.put(""annotators"", ""tokenize, ssplit, pos, lemma""); 
StanfordCoreNLP pipeline = new StanfordCoreNLP(props, false);
String text = ""painting""; 
Annotation document = pipeline.process(text);  

List&lt;edu.stanford.nlp.util.CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

for(edu.stanford.nlp.util.CoreMap sentence: sentences) 

{    
    for(CoreLabel token: sentence.get(TokensAnnotation.class))
    {       
        String word = token.get(TextAnnotation.class);      
        String lemma = token.get(LemmaAnnotation.class); 
        System.out.println(""lemmatized version :"" + lemma);
    }
}

//...........lemmatization ends.........................*
</code></pre>

<p>i have to run lemmatizer on words and not sentences where pos tagging will be done automatically. so i would first manually tag the words and then find their lemma. help with some sample code or reference to some site would be great help.</p>
","java-7, stanford-nlp, lemmatization","<p>If you know the POS tags in advance you can get the lemmata the following way:</p>

<pre><code>Properties props = new Properties(); 
props.put(""annotators"", ""tokenize, ssplit""); 
StanfordCoreNLP pipeline = new StanfordCoreNLP(props, false);
String text = ""painting"";

Morphology morphology = new Morphology();

Annotation document = pipeline.process(text);  

List&lt;edu.stanford.nlp.util.CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

for(edu.stanford.nlp.util.CoreMap sentence: sentences) {

  for(CoreLabel token: sentence.get(TokensAnnotation.class)) {       
    String word = token.get(TextAnnotation.class);
    String tag = ... //get the tag for the current word from somewhere, e.g. an array
    String lemma = morphology.lemma(word, tag);
    System.out.println(""lemmatized version :"" + lemma);
  }
}
</code></pre>

<p>In case you only want to get the lemma of a single word, you don't even have to run CoreNLP for tokenizing and sentence-splitting, so you could just call the lemma function as following:</p>

<pre><code>String tag = ""VBG"";      
String word = ""painting"";
Morphology morphology = new Morphology();
String lemma = morphology.lemma(word, tag);
</code></pre>
",1,0,339,2015-02-25 16:19:43,https://stackoverflow.com/questions/28724111/manual-tagging-of-words-using-stanford-cornlp
StanfordNLP Tokenizer,"<p>I use StanfordNLP  to tokenize a set of messages written with smartphones. These texts have a lot of typos and do not respect the punctuation rules. 
Very often the blank spaces are missing affecting the tokenization. </p>

<p>For instance, the following sentences miss the blankspace in ""California.This"" and ""university,founded"". </p>

<p><em>Stanford University is located in California.This university is a great university,founded in 1891.</em></p>

<p>The tokenizer returns:</p>

<p><em>{""Stanford"", ""University"", ""is"", ""located"", ""in"", <strong>""California.This""</strong>, ""university"", ""is"", ""a"", ""great"", <strong>""university"", "","", ""founded"",</strong> ""in"", ""1891"", "".""}</em></p>

<p>As observed they split well all the tokens but ""California.This"" (I expect to have three tokens {""California"" ""."" ""this""}).
I took a look at the tokenization rules and I observed that the regular expression for words accepts the punctuation signs used for the end of the sentence in the word.</p>

<p>WORD = {LETTER}({LETTER}|{DIGIT})<em>([.!?]{LETTER}({LETTER}|{DIGIT})</em>)*</p>

<p>I removed the last part and recompiled but the tokenizer still does not change it behaviour. </p>

<p>Does someone have an idea how to avoid this unwanted behaviour?
Or someone, can indicate me another tokenizer that works well with this types of texts? </p>
","tokenize, stanford-nlp, misspelling","<p>I assume you're referring to the <code>.flex</code> file for the tokenizer?</p>

<p>You need to generate new Java code from this specification before building again. Use the <code>flexeverything</code> Ant build task (see our <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/build.xml#L58"" rel=""nofollow"">build spec</a>).</p>

<hr>

<p>You may also find <a href=""https://github.com/brendano/ark-tweet-nlp/blob/master/src/cmu/arktweetnlp/Twokenize.java"" rel=""nofollow""><code>Twokenize</code></a> useful. This is a self-contained tokenizer for tweets. It's part of the <a href=""http://www.ark.cs.cmu.edu/TweetNLP/"" rel=""nofollow"">TweetNLP package</a> from Noah Smith's group at CMU. (Note that this code is GPL v2.)</p>
",1,1,206,2015-02-27 10:27:14,https://stackoverflow.com/questions/28762822/stanfordnlp-tokenizer
How to train a naive bayes classifier with pos-tag sequence as a feature?,"<p>I have two classes of sentences. Each has reasonably distinct pos-tag sequence. How can I train a Naive-Bayes classifier with POS-Tag sequence as a feature? Does Stanford CoreNLP/NLTK (Java or Python) provide any method for building a classifier with pos-tag as a feature?  I know in python <code>NaiveBayesClassifier</code> allows for building a NB classifier but it uses <code>contains-a-word</code> as feature but can it be extended to use pos-tag-sequence as a feature ?</p>
","machine-learning, nltk, stanford-nlp, text-classification, naivebayes","<p>If you know how to train and predict texts (or sentences in your case) using nltk's naive bayes classifier and words as features, than you can easily extend this approach in order to classify texts by pos-tags. This is because the classifier don't care about whether your feature-strings are words or tags. So you can simply replace the words of your sentences by pos-tags using for example nltk's standard pos tagger:</p>

<pre class=""lang-py prettyprint-override""><code>sent = ['So', 'they', 'have', 'internet', 'on', 'computers' , 'now']
tags = [t for w, t in nltk.pos_tag(sent)]
print tags
</code></pre>

<p>['IN', 'PRP', 'VBP', 'JJ', 'IN', 'NNS', 'RB']</p>

<p>As from now you can proceed with the ""contains-a-word"" approach.</p>
",6,6,3649,2015-02-27 11:50:31,https://stackoverflow.com/questions/28764459/how-to-train-a-naive-bayes-classifier-with-pos-tag-sequence-as-a-feature
Annotating a treebank with lexical information (Head Words) in JAVA,"<p>I have a treebank with syntactic parse tree for each sentence as given below:</p>

<p><strong>(S (NP (DT The) (NN government)) (VP (VBZ charges) (SBAR (IN that) (S (PP (IN between) (NP (NNP July) (CD 1971)) (CC and) (NP (NNP July) (CD 1992))) (, ,) (NP (NNP Rostenkowski)) (VP (VBD placed) (NP (CD 14) (NNS people)) (PP (IN on) (NP (NP (PRP$ his) (JJ congressional) (NN payroll)) (SBAR (WHNP (WP who)) (S (VP (VBD performed) (NP (NP (JJ personal) (NNS services)) (PP (IN for) (NP (NP (PRP him)) (CC and) (NP (PRP$ his) (NN family))))))))))))))</strong></p>

<p>I want to annotate the parse tree with lexical information like <strong>headwords</strong> for each node in the parse tree.</p>

<p>Can I do that using StanfordCoreNLP? Please guide me in the right direction. I would prefer a solution that can be implemented in JAVA as I am familiar with JAVA.</p>

<p>Thanks a lot!</p>
","java, nlp, stanford-nlp, lexical-analysis","<p>You can build this using the <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/TreeTransformer.html"" rel=""nofollow""><code>TreeTransformer</code></a> interface. Use a <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/HeadFinder.html"" rel=""nofollow""><code>HeadFinder</code></a> (if you're parsing English, the <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/CollinsHeadFinder.html"" rel=""nofollow""><code>CollinsHeadFinder</code></a>) to retrieve the head word / head constituent at each node.</p>

<p>You can see an example of this kind of work in the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/parser/lexparser/TreeAnnotator.java#L45"" rel=""nofollow""><code>TreeAnnotator</code></a> within the parser.</p>
",1,0,179,2015-02-28 13:38:50,https://stackoverflow.com/questions/28782336/annotating-a-treebank-with-lexical-information-head-words-in-java
In CoreNLP what is the different between the default generated dependency trees?,"<p>Using Stanford CoreNLP, I generate dependency trees using default annotators. I view the XML output with the <a href=""http://nlp.stanford.edu/software/CoreNLP-to-HTML.xsl"" rel=""nofollow"">XSLT transformation</a> provided on the project's website. I see three dependency tree categories each very similar, and they are:</p>

<ul>
<li>Uncollapsed dependencies</li>
<li>Collapsed dependencies</li>
<li>Collapsed dependencies with CC processed</li>
</ul>

<p>See an example - <a href=""http://nlp.stanford.edu/software/example.xml"" rel=""nofollow"">http://nlp.stanford.edu/software/example.xml</a>. I am wondering if anyone has experience with this project and knows the difference between each type of tree? Also, what is CC in this context?</p>
",stanford-nlp,"<p>See section 4 of the Stanford Dependencies manual: <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf#page=12"" rel=""nofollow""><em>Different styles of dependency representation</em></a>. The first three subsections map to basic, collapsed, and CC-processed dependency representations, respectively.</p>
",1,1,408,2015-03-02 05:09:00,https://stackoverflow.com/questions/28803055/in-corenlp-what-is-the-different-between-the-default-generated-dependency-trees
How to get the Stanford parser output as a list of nodes and edges?,"<p>I'm  processing a batch of text files, and I need to use the Stanford parser's output as a numeric list of nodes and edges where <strong>Nodes</strong> have IDs and labels, <strong>edges</strong> consist of two node ids and an <strong>edge weight</strong> like:</p>

<pre class=""lang-none prettyprint-override""><code>Node List:   1  A ,  2  B...
Edge list:  1 2 10, 2 1 10...
</code></pre>

<p>According to the Stanford NLP javadoc -->Class SemanticGraph:</p>

<blockquote>
  <p>There is no mechanism for returning all edges at once (eg edgeSet()). This is intentional. Use edgeIterable() to iterate over the edges if necessary.</p>
</blockquote>

<p>How to do it? I tried this code:</p>

<pre><code>import java.io.*;
import java.util.*;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphEdge;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

public class StanfordCoreNlpSemGraph {
  public static void main(String[] args) throws IOException {
    PrintWriter out;
    if (args.length &gt; 1) {
      out = new PrintWriter(args[1]);
    } else {
      out = new PrintWriter(System.out);
    }
    PrintWriter xmlOut = null;
    if (args.length &gt; 2) {
      xmlOut = new PrintWriter(args[2]);
    }

    StanfordCoreNLP pipeline = new StanfordCoreNLP();
    Annotation annotation;
    if (args.length &gt; 0) {
      annotation = new Annotation(IOUtils.slurpFileNoExceptions(args[0]));
    } else {
      annotation = new Annotation(""This is the first annotation."");
    }

    pipeline.annotate(annotation);
    pipeline.prettyPrint(annotation, out);
    if (xmlOut != null) {
      pipeline.xmlPrint(annotation, xmlOut);
    }
      // An Annotation is a Map.
     // For instance, this gets the parse tree of the first sentence. 

    List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
    if (sentences != null &amp;&amp; sentences.size() &gt; 0) {
      CoreMap sentence = sentences.get(0);
      Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
      out.println();
      out.println(""The first sentence parsed is:"");
      tree.pennPrint(out);
      Object IndexedWord;
      SemanticGraph sg = new SemanticGraph();

      SemanticGraphEdge edge = new SemanticGraphEdge(edge);

      for (SemanticGraphEdge edge : sg.edgeIterable()) 
      {
        int headIndex = edge.getGovernor().index();
        int depIndex = edge.getDependent().index();
        int weight = 1; // ""edge weight""-- should it be the 
        // sum of the weights of the 
        // selected edges? 
        System.out.printf(""%d %d %d%n"", headIndex, depIndex, weight);
      }  
    }
  }
}
</code></pre>

<p>But it throws an error: <code>Duplicate local variable edge  StanfordCoreNlpSemGraph.java    /stan-nlp/src   line 60</code></p>
","java, graph, nodes, stanford-nlp, edges","<p>Here's a basic example of forming the edge list. (The node list part should be easy — you just need to iterate over the tokens in the sentence and print them out.)</p>

<pre><code>SemanticGraph sg = ....
for (SemanticGraphEdge edge : sg.getEdgesIterable()) {
  int headIndex = edge.getGovernor().index();
  int depIndex = edge.getDependent().index();
  int weight = ... // Not sure what ""edge weight"" you want here.
  System.out.printf(""%d %d %d%n"", headIndex, depIndex, weight);
}
</code></pre>
",1,0,767,2015-03-04 22:25:44,https://stackoverflow.com/questions/28866301/how-to-get-the-stanford-parser-output-as-a-list-of-nodes-and-edges
Stanford NLP: Chinese Part of Speech labels?,"<p>I am trying to find a table explaining each label in the Chinese part-of-speech tagger for the 2015.1.30 version.  I couldn't find anything on this topic.  The closest thing I could find was in the ""Morphological features help POS tagging of unknown words across language varieties"" article, but it doesn't explain what VC represent.  I would love to get an updated list.</p>
","python, nlp, stanford-nlp, pos-tagger, part-of-speech","<p>We use the tag set of the (Penn/LDC/Brandeis/UC Boulder) Chinese Treebank.</p>

<p>See here for details on the tag set: <a href=""http://www.cis.upenn.edu/~chinese/"" rel=""nofollow"">http://www.cis.upenn.edu/~chinese/</a></p>

<p>This was documented in the parser FAQ, but I'll add it to the tagger FAQ.</p>
",1,0,790,2015-03-05 16:49:02,https://stackoverflow.com/questions/28883234/stanford-nlp-chinese-part-of-speech-labels
stanford corenlp: Unrecoverable error while loading a tagger model,"<p>I need to use the StanfordTruecaser using the Java API. I have this code:</p>

<pre><code>  String text = ""i love paris. i am with barack obama"";

  //define the properties of the pipeline to be run and add annotators to it
  Properties props = new Properties();
  props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, truecase"");
  pipeline = new StanfordCoreNLP(props);

  //run annotators on document text
  Annotation document = new Annotation(text);
  pipeline.annotate(document);
</code></pre>

<p>But the POS tagger doesn't work well:</p>

<pre><code>Adding annotator pos
Exception in thread ""main"" java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:558)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:267)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:129)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:125)
</code></pre>

<p>I use Eclipse and I have linked the <em>stanford-corenlp-3.4.jar</em> like this:
<img src=""https://i.sstatic.net/Dskig.png"" alt=""enter image description here""></p>
","java, eclipse, stanford-nlp","<p>I just add the <em>stanford-corenlp-3.4-models.jar</em> to the classpath in Eclipse and it works.</p>
",1,0,1756,2015-03-06 15:31:30,https://stackoverflow.com/questions/28902103/stanford-corenlp-unrecoverable-error-while-loading-a-tagger-model
Stanford Word Segmenter for Chinese in Python how to return results without punctuation,"<p>I am trying to segment a Chinese sentence with the Stanford Word Segmenter in Python, but currently the results has punctuation marks in it.  I want to return results without the punctuations, only the words.  What is the best way to do that?  I tried Googling for an answer, but didn't find anything.  </p>
","python, stanford-nlp, punctuation, chinese-locale","<p>I think you'd be better off just removing the punctuation after the text has been segmented; I'm fairly sure the Stanford segmenter takes cues from punctuation in doing its job, so you wouldn't want to do so beforehand. The following works for me on UTF-8 text. For Chinese punctuation, use the Zhon library with regex:</p>

<pre><code>import zhon.hanzi
import re
h_regex = re.compile('[%s]' % zhon.hanzi.punctuation)
intxt = # segmented text with punctuation
outtxt = h_regex.sub('', intxt)
</code></pre>

<p>And depending on the text you're working with, you may also need to remove non-Chinese punctuation:</p>

<pre><code>import string
p_regex = re.compile('[%s]' % re.escape(string.punctuation))
outtext2 = p_regex.sub('', outtxt)
</code></pre>

<p>Then you should be golden.</p>
",3,2,1012,2015-03-06 16:05:59,https://stackoverflow.com/questions/28902758/stanford-word-segmenter-for-chinese-in-python-how-to-return-results-without-punc
Choosing correct word for the given string,"<p>Suppose the given word is"" connnggggggrrrraaatsss"" and we need to convert it to congrats . 
Or for other example is ""looooooovvvvvveeeeee"" should be changed to ""love"" . </p>

<p>Here the given words can be repeated for any number of times but it should be changed to correct form. We need to write a java based program. </p>
","nlp, stanford-nlp","<p>You cannot really check for every word because there are certain words which have more than 1 alphabets in their spelling. So one way you could go is - </p>

<ol>
<li>check for each alphabet in the word and restrict its number of consecutive appearances to two</li>
<li>now check the new spelling on the spell checker, you might want to try HUNspell as it is widely used by many word processing softwares.</li>
</ol>
",2,0,90,2015-03-07 11:15:34,https://stackoverflow.com/questions/28914291/choosing-correct-word-for-the-given-string
python corenlp batch parse,"<p>I am trying to batch parse document using corenlp python wrapper. batch_parse() gives generator, when I try to  iterate over this generator it gives me following error:</p>

<blockquote>
  <p>Invalid maximum heap size: -XmxTrue
  Error: Could not create the Java Virtual Machine.</p>
</blockquote>

<p>Here is my code :</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from corenlp 
import batch_parse 
corenlp_dir = ""stanford-corenlp-full-2014-08-27/"" 
raw_text_directory = ""sample_raw_text/"" 
for value in batch_parse(raw_text_directory, corenlp_dir,True): 
  print value</code></pre>
</div>
</div>
</p>

<p>When I remove for loop over the generator provided by batch_parse, it don't give this error. So, I guess error is relevant to iteration over generator.</p>

<p>How can I resolve this error? </p>
","python, batch-processing, stanford-nlp","<p>It was my mistake. I missed ""raw_output"" in parameter passing of batch_parse. So, it should be like this:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>for value in batch_parse(raw_text_directory, corenlp_dir,raw_output=True):
	print value</code></pre>
</div>
</div>
</p>
",0,0,360,2015-03-10 07:46:52,https://stackoverflow.com/questions/28958425/python-corenlp-batch-parse
Different results performing Part of Speech tagging using Core NLP and Stanford Parser?,"<p>The Part Of Speech (POS) models that Stanford parser and Stanford CoreNlp
uses are different, that's why there is difference in the output of the POS tagging performed through Stanford Parser and CoreNlp.</p>

<ul>
<li>Online Core NLP Output 
<ul>
<li>The/DT man/NN is/VBZ smoking/NN ./.</li>
<li>A/DT woman/NN rides/NNS a/DT horse/NN ./.</li>
</ul></li>
<li>Online Stanford Parser Output
<ul>
<li>The/DT man/NN is/VBZ smoking/VBG ./.</li>
<li>A/DT woman/NN rides/VBZ a/DT horse/NN ./.
similarly more sentences</li>
</ul></li>
</ul>

<p>Is there documentation comparing two models and other detail explanation for the differences ?</p>

<p>It seems output of corenlp is wrong for these cases. Apart from few sentences which I checked during error analysis I guess there would be quite a lot of similar cases where these kind of errors might be.</p>
","stanford-nlp, part-of-speech","<p>This isn't really about CoreNLP, it's about whether you are using the Stanford POS tagger or the Stanford Parser (the PCFG parser) to do the POS tagging. (The PCFG parser usually does POS tagging as part of its parsing algorithm, although it can also use POS tags given from elsewhere.) Both sometimes make mistakes. On average, the POS tagger is a slightly better POS tagger than the parser. But, sometimes the parser wins, and in particular, it sometimes seems like it is better at tagging decisions that involve integrating clause-level information. At any rate, it gets these two examples right - though I bet you could also find some examples that go the other way.</p>

<p>If you want to use the PCFG parser for POS tagging in CoreNLP, simply omit the POS tagger, and move the parser earlier so that POS tags are available for the lemmatizer and regex-based NER:</p>

<pre><code>java -mx3g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,parse,lemma,ner,dcoref -file test.txt
</code></pre>

<p>However, some of our other parsers (NN dependency parser, SR constituency parser) require POS tagging to have been done first.</p>
",5,3,632,2015-03-11 14:36:36,https://stackoverflow.com/questions/28989483/different-results-performing-part-of-speech-tagging-using-core-nlp-and-stanford
Data format for Stanford POS-tagger,"<p>I am re-training the Stanford POS-tagger on my own data. I have trained two other taggers on the same data in the following one-token-per-line format:</p>

<pre><code>word1_TAG
word2_TAG
word3_TAG
word4_TAG
.
</code></pre>

<p>Is this format ok for the Stanford tagger, or does it need to be one-sentence-per-line?</p>

<pre><code>word1_TAG word2_TAG word3_TAG word4_TAG .
</code></pre>

<p>Could using the first format for training and testing affect Stanford tagging results?</p>
","stanford-nlp, dataformat","<p>You should have one sentence per line (your second example).</p>

<p>Using the first format will certainly affect tagging results: you'll effectively build a unigram tagger, in which all tagging is done without any sentence context at all.</p>
",0,0,67,2015-03-11 14:59:07,https://stackoverflow.com/questions/28990059/data-format-for-stanford-pos-tagger
NLP - Error while Tokenization and Tagging etc,"<p>I want to identify all the Tokens and also PartsOfSpeech Tagging using the Stanford NLP jar file. I have added all the required jar files into the build path of the project..The error which I am getting is..</p>

<pre><code>Exception in thread ""main"" java.lang.UnsupportedClassVersionError: edu/stanford/nlp/pipeline/StanfordCoreNLP : Unsupported major.minor version 52.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(Unknown Source)
    at java.security.SecureClassLoader.defineClass(Unknown Source)
    at java.net.URLClassLoader.defineClass(Unknown Source)
    at java.net.URLClassLoader.access$100(Unknown Source)
    at java.net.URLClassLoader$1.run(Unknown Source)
    at java.net.URLClassLoader$1.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at Test.testing(Test.java:19)
    at mainFunction.main(mainFunction.java:29)
</code></pre>
","java, nlp, stanford-nlp","<p>You're trying to use Stanford NLP tools version 3.5 or later using a version of Java 7 or earlier.  Either upgrade to Java 8 or downgrade to Stanford tools version 3.4.1.</p>
",8,2,1682,2015-03-12 10:07:49,https://stackoverflow.com/questions/29006811/nlp-error-while-tokenization-and-tagging-etc
StanfordNLP does not extract relations between entities,"<p>I'm trying out the StanfordNLP Relation Extractor which according to the page at <a href=""http://nlp.stanford.edu/software/relationExtractor.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/relationExtractor.shtml</a> has 4 relations that it can extract : Live_In, Located_In, OrgBased_In, Work_For.</p>

<p>My code is :</p>

<pre><code>Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref, relation"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    String text = ""Mary lives in Boston."";

    Annotation document = new Annotation(text);
    pipeline.annotate(document);

    List&lt;RelationMention&gt; relations = document.get(MachineReadingAnnotations.RelationMentionsAnnotation.class);
</code></pre>

<p>I am expecting to get a Live_In relation but the relations variable is null.</p>

<p>What am I missing in the code?</p>

<p>Thanks</p>
",stanford-nlp,"<p>The <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/machinereading/structure/MachineReadingAnnotations.RelationMentionsAnnotation.html"" rel=""nofollow""><code>RelationMentionsAnnotation</code></a> is a sentence-level annotation. You should first iterate over the sentences in the <code>Annotation</code> object and then try to retrieve the annotation.</p>

<p>Here's a basic example of how to iterate over sentences:</p>

<pre><code>// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
    List&lt;RelationMention&gt; relations = sentence.get(MachineReadingAnnotations.RelationMentionsAnnotation.class);
    // ....
}
</code></pre>
",2,0,269,2015-03-12 17:32:04,https://stackoverflow.com/questions/29016504/stanfordnlp-does-not-extract-relations-between-entities
Instruction for training model in Stanford Core NLP,"<p>I am a novice in the area of sentiment analysis, and I am very interested to learn about training models, could you please explain each of the instructions contained in the following command?</p>

<p>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz</p>

<p>what is the function of:
-numHid 25 
-trainPath train.txt 
-devPath dev.txt 
-train 
-model model.ser.gz</p>

<p>Please could you help me?</p>
","stanford-nlp, sentiment-analysis, training-data","<p>The more complicated options are described in comments in these classes: <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sentiment/RNNOptions.java"" rel=""nofollow""><code>RNNOptions</code></a>, <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sentiment/RNNTrainOptions.java"" rel=""nofollow""><code>RNNTrainOptions</code></a>.</p>

<p>The remainder of the options you listed are paths for reading / writing during training.</p>

<ul>
<li>The <code>-trainPath</code> argument points to a labeled sentiment treebank. The trees in this data will be used to train the model parameters (also to seed the model vocabulary).</li>
<li>The <code>-devPath</code> argument points to a second labeled sentiment treebank. The trees in this data will be used to periodically evaluate the performance of the model. We won't train on this data; it will only be used to test how well the model generalizes to unseen data.</li>
<li>The <code>-model</code> argument specifies where to save the learned sentiment model.</li>
</ul>
",2,0,1301,2015-03-13 14:18:14,https://stackoverflow.com/questions/29034600/instruction-for-training-model-in-stanford-core-nlp
Typed Dependency Parsing in NLTK Python,"<p>I have a sentence</p>

<pre><code>""I shot an elephant in my sleep""
</code></pre>

<p>The typed dependency of the sentence is </p>

<pre><code>nsubj(shot-2, I-1)
det(elephant-4, an-3)
dobj(shot-2, elephant-4)
prep(shot-2, in-5)
poss(sleep-7, my-6)
pobj(in-5, sleep-7)
</code></pre>

<p>How do I get the typed dependency using Stanford Parser (or any parser) using NLTK (preferably, but anthing is fine) in Python ? </p>

<p>Note - I know it is very similar to <a href=""https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk"">this question</a>. But no good answers exist. </p>
","python, nltk, stanford-nlp","<p>There exists a python wrapper for the Stanford parser, you can get it <a href=""https://bitbucket.org/torotoki/corenlp-python"" rel=""nofollow"">here</a>.</p>

<p>It will give you the dependency tree of your sentence.</p>

<hr>

<p>EDIT:</p>

<p>I assume here that you launched a server as said <a href=""https://bitbucket.org/torotoki/corenlp-python"" rel=""nofollow"">here</a>. I also assume that you have installed jsonrpclib.</p>

<p>The following code will produce what you want:</p>

<pre><code>import json
import jsonrpclib

class StanfordNLP:
    def __init__(self, port_number=8080):
        self.server = jsonrpclib.Server(""http://localhost:%d"" % port_number)

    def parse(self, text):
        return json.loads(self.server.parse(text))

nlp = StanfordNLP()
sentence = 'I shot an elephant in my sleep'
result = nlp.parse(sentence)
result['sentences'][0]['indexeddependencies']

&gt;&gt;&gt;
['root', 'ROOT-0', 'shot-2']
['nsubj', 'shot-2', 'I-1']
['det', 'elephant-4', 'an-3']
['dobj', 'shot-2', 'elephant-4']
['poss', 'sleep-7', 'my-6']
['prep_in', 'shot-2', 'sleep-7']
</code></pre>

<hr>

<p>EDIT2:</p>

<p>Now, the Stanford parser has an <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow"">HTTP API</a>. Thus, the python wrapper is not necessary anymore.</p>
",2,0,2850,2015-03-14 14:22:22,https://stackoverflow.com/questions/29049974/typed-dependency-parsing-in-nltk-python
Unsupported major.minor version 52.0 - edu/stanford/nlp/classify/ColumnDataClassifier,"<p>I am using stanford-classifier-3.5.1 jar on my system and trying to run the example classifier only. I am getting the following error:</p>

<pre><code>Exception in thread ""main"" java.lang.UnsupportedClassVersionError: edu/stanford/nlp/classify/ColumnDataClassifier : Unsupported major.minor version 52.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at ClassifierDemoStanford.main(ClassifierDemoStanford.java:20)
</code></pre>

<p>I have tried with Java version 1.6, 1.7 both. The readme file says This software requires Java 5 (JDK 1.6.0+).
Please help. </p>
","java, stanford-nlp","<p>The error message means that the code requires Java 8. I checked out the website and the latest release supports Java 8, since October 2014.</p>

<p>I would estimate that the readme file is out of date. </p>

<p>If you use Java 8 it will run this fine. </p>

<p>If you can't upgrade to Java 8, you might downgrade the version of the classifier jar file to the version before Java 8 was added - version 3.4.1</p>

<p><a href=""http://nlp.stanford.edu/software/classifier.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/classifier.shtml</a></p>
",5,1,4221,2015-03-17 08:47:36,https://stackoverflow.com/questions/29094690/unsupported-major-minor-version-52-0-edu-stanford-nlp-classify-columndataclass
Arabic lemmatization and Stanford NLP,"<p>I try to make lemmatization, ie identifying the lemma and possibly the Arabic root of a verb, for example:
يتصل ==> lemma (infinitive of the verb) ==> اتصل ==> root (triliteral root / Jidr thoulathi)
==> و ص ل</p>

<p>Do you think Stanford NLP can do that?</p>

<p>Best Regards,</p>
","nlp, stanford-nlp, lexical-analysis, stemming, lemmatization","<p>The Stanford Arabic segmenter can't do true lemmatization. However, it is possible to train a new model to do something like stemming:</p>

<ul>
<li>تكتبون ← ت+ كتب +ون</li>
<li>يتصل ← ي+ تصل</li>
</ul>

<p>If it is very important that the output is real Arabic lemmas (""تصل"" is not a true lemma), you might be better off with a tool like MADAMIRA (<a href=""http://nlp.ldeo.columbia.edu/madamira/"">http://nlp.ldeo.columbia.edu/madamira/</a>).</p>

<p><em>Elaboration:</em> The Stanford Arabic segmenter produces its output character-by-character using only these operations (implemented in <code>edu.stanford.nlp.international.arabic.process.IOBUtils</code>):</p>

<ul>
<li>Split a word between two characters</li>
<li>Transform lil- (للـ) into li+ al- (ل+ الـ)</li>
<li>Transform ta (ت) or ha (ه) into ta marbuta (ة)</li>
<li>Transform ya (ي) or alif (ا) into alif maqsura (ى)</li>
<li>Transform alif maqsura (ى) into ya (ي)</li>
</ul>

<p>So lemmatizing يتصل to ي+ اتصل would require implementing an extra rule, i.e., to insert an alif after ya or ta. Lemmatization of certain irregular forms would be completely impossible (for example, نساء ← امرأة).</p>

<p>The version of the Stanford segmenter available for download also only breaks off pronouns and particles:</p>

<p>وسيكتشفونه ← و+ س+ يكتشفون +ه</p>

<p>However, if you have access to the LDC Arabic Treebank or a similarly rich source of Arabic text with morphological segmentation annotated, it is possible to train your own model to remove all morphological affixes, which is closer to lemmatization:</p>

<p>وسيكتشفونه ← و+ س+ ي+ كتشف +ون +ه</p>

<p>Note that ""كتشف"" is not a real Arabic word, but the segmenter should at least consistently produce ""كتشف"" for تكتشفين ,أكتشف ,يكتشف, etc. If this is acceptable, you would need to change the ATB preprocessing script to instead use the morphological segmentation annotations. You could do this by replacing the script called <code>parse_integrated</code> with a modified version like this: <a href=""https://gist.github.com/futurulus/38307d98992e7fdeec0d"">https://gist.github.com/futurulus/38307d98992e7fdeec0d</a></p>

<p>Then follow the instructions for ""TRAINING THE SEGMENTER"" in the README.</p>
",12,5,5318,2015-03-19 17:33:54,https://stackoverflow.com/questions/29151329/arabic-lemmatization-and-stanford-nlp
Difference between tag and class in Stanford NER,"<p>I'm not sure what the difference between tag and class is? </p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow"">NERFeatureFactory</a> mentions:</p>

<pre><code>t - tag
c - class
</code></pre>

<p>The NER <a href=""http://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow"">FAQ</a> seems to use the two terms interchangeably as well? </p>

<p>For example what does the following feature do?</p>

<pre><code>t,c   useTags
</code></pre>

<p>Many thanks in advance!</p>
","nlp, stanford-nlp","<p>As Gabor said, tag is part of speech tag. </p>

<p>However, for c (or class): the features are defined over each class (so, John-NAME and John-PLACE are two different features).</p>

<p>useTags feature would be something like NNP-NAME, RB-PLACE, and so on.</p>
",1,1,128,2015-03-19 21:46:38,https://stackoverflow.com/questions/29155594/difference-between-tag-and-class-in-stanford-ner
How to extract noun phrases from the parsed text,"<p>I have parsed a text with constituency parser copy the result in a text file like below:</p>

<pre><code>(ROOT (S (NP (NN Yesterday)) (, ,) (NP (PRP we)) (VP (VBD went) (PP (TO to)....
(ROOT (FRAG (SBAR (SBAR (IN While) (S (NP (PRP I)) (VP (VBD was) (NP (NP (EX...
(ROOT (S (NP (NN Yesterday)) (, ,) (NP (PRP I)) (VP (VBD went) (PP (TO to.....
(ROOT (FRAG (SBAR (SBAR (IN While) (S (NP (NNP Jim)) (VP (VBD was) (NP (NP (....
(ROOT (S (S (NP (PRP I)) (VP (VBD started) (S (VP (VBG talking) (PP.....
</code></pre>

<p>I need to extract all NounPhrases (NP) from this text file. I wrote the following code that extract only the first NP from each line. However, I need to extract all noun phrases. My Code is:</p>

<pre><code>public class nounPhrase {

    public static int findClosingParen(char[] text, int openPos) {
        int closePos = openPos;
        int counter = 1;
        while (counter &gt; 0) {
            char c = text[++closePos];
            if (c == '(') {

                counter++;
            }
            else if (c == ')') {
                counter--;
            }
        }
        return closePos;
    }

     public static void main(String[] args) throws IOException {

        ArrayList npList = new ArrayList ();
        String line;
        String line1;
        int np;

        String Input = ""/local/Input/Temp/Temp.txt"";

        String Output = ""/local/Output/Temp/Temp-out.txt"";  

        FileInputStream  fis = new FileInputStream (Input);
        BufferedReader br = new BufferedReader(new InputStreamReader(fis,""UTF-8""
        ));
        while ((line = br.readLine())!= null){
        char[] lineArray = line.toCharArray();
        np = findClosingParen (lineArray, line.indexOf(""(NP""));
        line1 = line.substring(line.indexOf(""(NP""),np+1);
        System.out.print(line1+""\n"");
        }
    }
}
</code></pre>

<p>The output is:</p>

<pre><code>(NP (NN Yesterday))...I need other NPs in this line also
(NP (PRP I)).....I need other NPs in this line also
(NP (NNP Jim)).....I need other NPs in this line also
(NP (PRP I)).....I need other NPs in this line also
</code></pre>

<p>My code only takes the first NP on each line with its closing parenthesis but I need to extract all NPs from the text.</p>
","java, stanford-nlp","<p>While writing your own tree parser is a good exercise (!), if you just want results, the easiest way is to use more of the functionality of the Stanford NLP tools, namely <a href=""http://nlp.stanford.edu/software/tregex.shtml"" rel=""nofollow"">Tregex</a>, which is designed for just such things. You can change your final <code>while</code> loop to something like this:</p>

<pre><code>TregexPattern tPattern = TregexPattern.compile(""NP"");
while ((line = br.readLine()) != null) {
    Tree t = Tree.valueOf(line);
    TregexMatcher tMatcher = tPattern.matcher(t);
    while (tMatcher.find()) {
      System.out.println(tMatcher.getMatch());
    }
}
</code></pre>
",3,3,1556,2015-03-21 05:06:20,https://stackoverflow.com/questions/29179453/how-to-extract-noun-phrases-from-the-parsed-text
Stanford Parser and NLTK windows,"<p>I am trying to run Stanford Parser in NLTK in Windows. I am doing it in python. My code for the same is </p>

<pre><code>import os

from nltk.parse import stanford
os.environ['JAVAHOME'] = 'C:/Program Files/Java/jdk1.8.0_25/bin'
os.environ['STANFORD_PARSER'] = 'C:/jars'
os.environ['STANFORD_MODELS'] = 'C:/jars'

parser =stanford.StanfordParser(model_path=""C:/Users/pc/Desktop/Project/englishPCFG.ser.gz"")
sentences = parser.raw_parse_sents((""Hello, My name is Melroy."", ""What is your name?""))


for i in sentences:
    print i
</code></pre>

<p>This is the output it  gave  </p>

<pre><code>listiterator object at 0x03FB6150  
listiterator object at 0x03FB61B0
</code></pre>

<p>I am looking for the following output: </p>

<pre><code>Tree('ROOT', [Tree('S', [Tree('INTJ', [Tree('UH', ['Hello'])]), Tree(',',          [',']), Tree('NP', [Tree('PRP$', ['My']), Tree('NN', ['name'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('ADJP', [Tree('JJ', ['Melroy'])])]), Tree('.', ['.'])])]), Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('PRP$', ['your']), Tree('NN', ['name'])])]), Tree('.', ['?'])])])]
</code></pre>
","python-2.7, nltk, stanford-nlp","<p><code>raw_parse_sents</code> returns a list of listiterators. You can iterate through them like this:</p>

<pre><code>for myListiterator in sentences:
    for t in myListiterator:
        print t

&gt; (ROOT
&gt;   (S
&gt;     (INTJ (UH Hello))
&gt;     (, ,)
&gt;     (NP (PRP$ My) (NN name))
&gt;     (VP (VBZ is) (ADJP (JJ Melroy)))
&gt;     (. .)))
&gt; (ROOT
&gt;   (SBARQ
&gt;     (WHNP (WP What))
&gt;     (SQ (VBZ is) (NP (PRP$ your) (NN name)))
&gt;     (. ?)))
</code></pre>

<p>If you want the exact output format you quoted, you can do it like this:</p>

<pre><code>print [list(i)[0] for i in sentences]

&gt; [Tree('ROOT', [Tree('S', [Tree('INTJ', [Tree('UH', ['Hello'])]), Tree(',', [',']), Tree('NP', [Tree('PRP$', ['My']), Tree('NN', ['name'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('ADJP', [Tree('JJ', ['Melroy'])])]), Tree('.', ['.'])])]), Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('PRP$', ['your']), Tree('NN', ['name'])])]), Tree('.', ['?'])])])]
</code></pre>
",2,1,1247,2015-03-22 21:21:34,https://stackoverflow.com/questions/29200007/stanford-parser-and-nltk-windows
Using The Stanford Parser in Python on Chinese text not working,"<p>I tried this code, but it didn't work:</p>

<pre><code># -*- coding:utf-8 -*-
from nltk.parse import stanford
s = '你好'.decode('utf-8')

print s
parser = stanford.StanfordParser(path_to_jar='stanford-parser.jar',path_to_models_jar='stanford-parser-3.5.1-models.jar')
print parser.raw_parse_sents(s)
</code></pre>

<p>The result prints 你 and 好 as two words:</p>

<pre><code>你好
[Tree('ROOT', [Tree('NP', [Tree('NNP', ['\u4f60'])])]), Tree('ROOT', [Tree('NP', [Tree('NNP', ['\u597d'])])])]
</code></pre>

<p>but on the online parser(<a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/index.jsp</a>), the result is </p>

<p>Parse
(ROOT
  (IP
    (VP (VV 你好))))</p>

<p>How do I fix my code to produce the same result as the online parser?</p>
","java, python, parsing, stanford-nlp, chinese-locale","<p>There are two (ok, three... see ""Update 3"" below for the third) separate things going on:</p>

<p>1) Your code is returning two trees (two <code>ROOT</code>s), but you only expect to get one. This is happening because <code>raw_parse_sents</code> expects a list of sentences, not a single sentence, and if you give it a string, it is parsing each character in the string as if it were its own sentence and returning a list of one-character trees. So either pass <code>raw_parse_sents</code> a list, or use <code>raw_parse</code> instead.</p>

<p>2) You haven't specified a <code>model_path</code>, and the default is English. There are five options for Chinese, but it looks like this one matches the online parser:</p>

<pre><code>parser = stanford.StanfordParser(model_path='edu/stanford/nlp/models/lexparser/xinhuaFactored.ser.gz', path_to_jar='stanford-parser.jar',path_to_models_jar='stanford-parser-3.5.1-models.jar')
</code></pre>

<p>Combining these two changes, I am able to match the online parser (I also had to cast the returned listiterator to a list in order to match your output format):</p>

<pre><code>from nltk.parse import stanford
s = '你好'.decode('utf-8')

print s.encode('utf-8')
parser = stanford.StanfordParser(model_path='edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz', path_to_jar='stanford-parser.jar',path_to_models_jar='stanford-parser-3.5.1-models.jar')
print list(parser.raw_parse(s))

&gt; 你好
&gt; [Tree('ROOT', [Tree('IP', [Tree('VP', [Tree('VV', ['\u4f60\u597d'])])])])]
</code></pre>

<p><strong>Update 1:</strong></p>

<p>I realized you might be looking for an output format more like the one on the website as well, in which case this works:</p>

<pre><code>for tree in parser.raw_parse(s):
    print tree # or print tree.pformat().encode('utf-8') to force an encoding
</code></pre>

<p><strong>Update 2:</strong></p>

<p>Apparently if your version of NLTK is earlier than 3.0.2, <code>Tree.pformat()</code> was <code>Tree.pprint()</code>. From <a href=""https://github.com/nltk/nltk/wiki/Porting-your-code-to-NLTK-3.0"" rel=""nofollow"">https://github.com/nltk/nltk/wiki/Porting-your-code-to-NLTK-3.0</a>:</p>

<blockquote>
  <p>Printing changes (from 3.0.2, see <a href=""https://github.com/nltk/nltk/issues/804"" rel=""nofollow"">https://github.com/nltk/nltk/issues/804</a>):</p>
  
  <ul>
  <li>classify.decisiontree.DecisionTreeClassifier.pp → pretty_format</li>
  <li>metrics.confusionmatrix.ConfusionMatrix.pp → pretty_format</li>
  <li>sem.lfg.FStructure.pprint → pretty_format</li>
  <li>sem.drt.DrtExpression.pretty → pretty_format</li>
  <li>parse.chart.Chart.pp → pretty_format</li>
  <li>Tree.pprint() → pformat</li>
  <li>FreqDist.pprint → pformat</li>
  <li>Tree.pretty_print → pprint</li>
  <li>Tree.pprint_latex_qtree → pformat_latex_qtree</li>
  </ul>
</blockquote>

<p><strong>Update 3:</strong></p>

<p>I am now trying to match the output for the sentence in your comment, <code>'你好，我心情不错今天，你呢？'</code>.</p>

<p>I referred to the <a href=""http://nlp.stanford.edu/software/parser-faq.shtml"" rel=""nofollow"">Stanford Parser FAQ</a> extensively while writing this response and suggest you check it out (especially ""Can you give me some help in getting started parsing Chinese?""). Here's what I've learned:</p>

<p>In general, you need to ""segment"" Chinese text into words (consisting of one or more characters) separated by spaces before parsing it. The online parser does this, and you can see the output of both the segmentation step and the parsing step on the web page. For our test sentence, the segmentation it shows is <code>'你好 ， 我 心情 不错 今天 ， 你 呢 ？'</code>.</p>

<p>If I run this segmentation string through the <code>xinhuaFactored</code> model locally, my output matches the online parser exactly.</p>

<p>So we need to run our text through a word segmenter before running it through the parser. The FAQ recommends the Stanford Word Segmenter, which is probably what the online parser is using anyway: <a href=""http://nlp.stanford.edu/software/segmenter.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/segmenter.shtml</a>.</p>

<p>(As the FAQ mentions, the parser also contains a model <code>xinhuaFactoredSegmenting</code> which does an approximate segmentation as part of the parsing call. However, it calls this method ""reasonable, but not excellent"", and the output doesn't match the online parser anyway, which is our standard.)</p>
",4,3,1497,2015-03-23 01:05:58,https://stackoverflow.com/questions/29201833/using-the-stanford-parser-in-python-on-chinese-text-not-working
Stanford CoreNLP demo and coreference resolution,"<p>Consider the below sentences:
Bats are the only mammals that can fly. They are also among the only mammals known to feed on blood.</p>

<p>Input them in the below link</p>

<p><a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/corenlp/process</a></p>

<p>Coreference output does not show that the word They refers to Bats. Am I missing something basic?</p>
",stanford-nlp,"<p>Stanford's dcoref module has the pronoun 'they' hardcoded to be animate only, and presumably 'bat' is in the inanimate word list.
The animate restriction is probably justified for the newswire training data, but is not valid for general English.
You can change the animate list here <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/dcoref/Dictionaries.java#L155"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/dcoref/Dictionaries.java#L155</a></p>
",4,2,2954,2015-03-23 08:02:00,https://stackoverflow.com/questions/29205498/stanford-corenlp-demo-and-coreference-resolution
"Malt parser, iterate over parsed tree in java","<p>I am trying to extract the tree dependencies from a Malt ConcurrentMaltParserModel. I iterate over the edges like: </p>

<pre><code>SortedSet&lt;ConcurrentDependencyEdge&gt; edges = graph.getEdges();
for (ConcurrentDependencyEdge e : edges) {
      //here I would need to extract the dependency type
}
</code></pre>

<p>I thought I could extract the dependency type in a similar way as StanfordParser, but unfortunately I cant figure out how to do that. </p>
","parsing, nlp, stanford-nlp","<p>First obtain a <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html"" rel=""nofollow""><code>SemanticGraph</code></a> object (e.g. by retrieving the value of a <code>BasicDependenciesAnnotation</code> in the CoreNLP pipeline, or by parsing directly with Stanford Parser). I can elaborate on this more if necessary.</p>

<p>The <code>SemanticGraph</code> provides a simple edge iterable for processing independent graph edges. (See the <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraphEdge.html"" rel=""nofollow""><code>SemanticGraphEdge</code></a> class. Also, note that <code>SemanticGraphEdge.getRelation</code> returns a <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/GrammaticalRelation.html"" rel=""nofollow""><code>GrammaticalRelation</code></a> instance.)</p>

<pre><code>SemanticGraph sg = ....
for (SemanticGraphEdge edge : sg.getEdgesIterable()) {
  int headIndex = edge.getGovernor().index();
  int depIndex = edge.getDependent().index();
  String label = edge.getRelation().getShortName();

  System.out.printf(""%d %d %s%n"", headIndex, depIndex, label);
}
</code></pre>
",1,0,156,2015-03-23 15:17:52,https://stackoverflow.com/questions/29213886/malt-parser-iterate-over-parsed-tree-in-java
Interpreting score of Shift-Reduce Parser parse on Stanford&#39;s LabeledScoredTreeNode,"<p>When parsing a sentence in Stanford, one can get the (negative) log probability of a parse by calling <em>.score</em> on the constituent-based output saved in TreeAnnotation.  So, after creating a Stanford pipeline object called, say, <em>my-basic-annotation object</em> (using Clojure for these examples for the sake of brevity) and then parsing the sentence ""The horse rode past the barn fell.""
like so</p>

<pre><code>&gt;&gt;&gt; (def basic-sentence-annotation (first (.get my-basic-annotation-object CoreAnnotations$SentencesAnnotation)))
&gt;&gt;&gt; sentence-annotation
#&lt;Annotation The horse rode past the barn fell.&gt;
&gt;&gt;&gt; (def basic-parsed  (.get basic-sentence-annotation TreeCoreAnnotations$TreeAnnotation))
&gt;&gt;&gt; basic-parsed
#&lt;LabeledScoredTreeNode (ROOT (S (NP (DT The) (NN horse)) (VP (VBD rode)  
(SBAR (S (NP (IN past) (DT the
) (NN barn)) (VP (VBD fell))))) (. .)))&gt; The horse rode past the barn fell.&gt;
</code></pre>

<p>one can call <em>.score</em> on <em>basic-parsed</em>:</p>

<pre><code>&gt;&gt;&gt; (.score basic-parsed)
-60.86048126220703
</code></pre>

<p>But when I use the Shift Reduce Parser instead and call  <em>.score</em> on the TreeAnnotation I get a very large positive number rather than a negative log probability:</p>

<pre><code>&gt;&gt;&gt; (def sr-sentence-annotation (first (.get my-sr-annotation-object CoreAnnotations$SentencesAnnotation)))
&gt;&gt;&gt; sr-sentence-annotation
#&lt;Annotation The horse rode past the barn fell.&gt;
&gt;&gt;&gt; (def sr-parsed  (.get sr-sentence-annotation TreeCoreAnnotations$TreeAnnotation))
&gt;&gt;&gt; sr-parsed
#&lt;LabeledScoredTreeNode (ROOT (S (NP (NP (DT The) (NN horse)) (VP (VBD rode)   (PP (IN past) (NP (DT the) (NN barn))))) (VP (VBD fell)) (. .)))&gt;
&gt;&gt;&gt; (.score sr-parsed)
6497.833389282227
</code></pre>

<p>I've spent some time looking at the API and the Stanford mailing list for some interpretation of this score, but haven't had any luck (I think the SR parser is too new for people to have encountered this problem yet). Any help would be appreciated.</p>
",stanford-nlp,"<p>Yes, this is expected. The score of a tree output by the shift-reduce parser is the sum of the prediction scores of all the transitions instead of a negative log probability. </p>

<p>The parser uses a multiclass perceptron to predict the transitions and therefore the score of each transition and consequently also the score of a tree can be any number.</p>

<p>See the <a href=""http://nlp.stanford.edu/software/srparser.shtml"" rel=""nofollow"">shift-reduce parser documentation</a> for more information on the parser and references to papers discussing how it works.</p>
",1,0,229,2015-03-23 16:04:17,https://stackoverflow.com/questions/29214885/interpreting-score-of-shift-reduce-parser-parse-on-stanfords-labeledscoredtreen
How would I retrieve the 5 values of positivity from Stanford&#39;s Sentiment Analysis?,"<p>I'm looking to pass a string to get parsed and return the number (1-5) with the highest rating. </p>

<p>For example, this would return 2:</p>

<p><img src=""https://i.sstatic.net/bLKiC.png"" alt=""For example, this would return 2:""></p>
","java, stanford-nlp","<p>I assume you're using the Stanford CoreNLP pipeline with the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/SentimentAnnotator.java#L71"" rel=""nofollow""><code>sentiment</code> annotator</a>.</p>

<p>This annotator sets an annotation on every sentence called <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/sentiment/SentimentCoreAnnotations.ClassName.html"" rel=""nofollow""><code>SentimentCoreAnnotations.ClassName</code></a>. Simply retrieve this value: (untested example)</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, parse, sentiment"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// read some text in the text variable
String text = ... // Add your text here!

// create an empty Annotation just with the given text
Annotation document = new Annotation(text);

// run all Annotators on this text
pipeline.annotate(document);

// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

for (CoreMap sentence : sentences) {
  String sentimentLabel = sentence.get(SentimentCoreAnnotations.ClassName.class);

  // ...
}
</code></pre>
",0,-2,47,2015-03-23 19:46:39,https://stackoverflow.com/questions/29219046/how-would-i-retrieve-the-5-values-of-positivity-from-stanfords-sentiment-analys
How are features generated and used in Stanford NER,"<p>Take as an example two features from <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow"">NERFeatureFactory</a>:</p>

<pre><code>pw, w, c
</code></pre>

<p>and</p>

<pre><code>pc, nc, c
</code></pre>

<p>Questions:</p>

<ol>
<li><p>Using the first feature function we want something like pw = 'in' w = 'Berlin' c = 'LOCATION' which would presumably get a high weight whereas changing c = 'PERSON' would get a low/negative weight. The question is, how are <strong>w</strong> and <strong>pw</strong> picked? Are they hand selected, are they taken from the vicinities of labeled words in the training set, or from the set of all possible words? <em>Is every combination pw, w then considered?</em></p></li>
<li><p>When the second feature function is used in the training phase do the matrices in the forward-backward algorithm become N^3 where N is the number of classes. Or am I missing something?</p></li>
</ol>

<p>Thank you in advance :) ! </p>
","nlp, stanford-nlp","<p>The Stanford NER uses a CRF sequence model.
1. pw and w are all pairs of previous and current words seen during training. 
2. during decoding, if we have the feature template pc, nc, c, then to find the best sequence, it will have to consider N^3 possible combination of classes (for each token and the surrounding tokens).  The default model uses pc, c and considers N^2 combinations.</p>
",4,0,464,2015-03-24 13:36:51,https://stackoverflow.com/questions/29234108/how-are-features-generated-and-used-in-stanford-ner
Exception when training relation extractor model from StanfordNLP,"<p>I'm trying to train my own model for the relation extractor feature from StanfordNLP as described here : <a href=""http://nlp.stanford.edu/software/relationExtractor.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/relationExtractor.shtml</a>. The issue is that when I start the training I get the following exception:</p>

<pre><code>PERCENTAGE OF TRAIN: 1.0
The reader log level is set to SEVERE
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ...done [0.9 sec].
Mar 25, 2015 9:23:36 PM edu.stanford.nlp.ie.machinereading.MachineReading makeResultsPrinters
INFO: Making result printers from
Mar 25, 2015 9:23:36 PM edu.stanford.nlp.ie.machinereading.MachineReading makeResultsPrinters
INFO: Making result printers from edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter
Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassNotFoundException: edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter
        at edu.stanford.nlp.ie.machinereading.MachineReading.makeResultsPrinters(MachineReading.java:771)
        at edu.stanford.nlp.ie.machinereading.MachineReading.makeResultsPrinters(MachineReading.java:756)
        at edu.stanford.nlp.ie.machinereading.MachineReading.makeMachineReading(MachineReading.java:235)
        at edu.stanford.nlp.ie.machinereading.MachineReading.main(MachineReading.java:106)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Unknown Source)
        at edu.stanford.nlp.ie.machinereading.MachineReading.makeResultsPrinters(MachineReading.java:768)
        ... 3 more
</code></pre>

<p>I'm using StanfordNLP 3.5.1 and indeed the class does not exist in the jar. I am using the roth.properties file as described in the link at the top and that contains the line:</p>

<pre><code>relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter
</code></pre>

<p>I tried commenting out that line with the hope that the code will pick up the default printer but that is still RelationExtractorResultsPrinter.</p>

<p>Anyone got this working?</p>

<p>Thanks</p>
",stanford-nlp,"<p>This is definitely our bad! This class will be included in the next release. For now, you can compile the class from the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/machinereading/RelationExtractorResultsPrinter.java"" rel=""nofollow"">source distribution (GitHub link)</a>.</p>
",1,1,260,2015-03-25 19:42:47,https://stackoverflow.com/questions/29265047/exception-when-training-relation-extractor-model-from-stanfordnlp
Detecting language using Stanford NLP,"<p>I'm wondering if it is possible to use <code>Stanford CoreNLP</code> to detect which language a sentence is written in? If so, how precise can those algorithms be?</p>
","nlp, stanford-nlp","<p>Almost certainly there is no language identification in Stanford COreNLP at this moment. 'almost' - because nonexistence is much harder to prove. </p>

<p>EDIT: Nevertheless, below are circumstantial evidences:</p>

<ol>
<li>there is no mention of language identification neither on <a href=""http://nlp.stanford.edu/software/"" rel=""noreferrer"">main
page</a>, nor <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">CoreNLP page</a>, nor in <a href=""http://nlp.stanford.edu/software/corenlp-faq.shtml#languages"" rel=""noreferrer"">FAQ</a> (although there is
a question 'How do I run CoreNLP on other languages?'), nor in <a href=""http://www.surdeanu.info/mihai/papers/acl2014-corenlp.pdf"" rel=""noreferrer"">2014
paper</a> of CoreNLP's authors; </li>
<li>tools that combine several NLP libs
including Stanford CoreNLP use another lib for language
identification, for example <a href=""http://blog.newitfarmer.com/tags/stanford-corenlp%E2%80%A0"" rel=""noreferrer"">DKPro Core ASL</a>; also <a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2013-March/003133.html"" rel=""noreferrer"">other
users</a> talking about language identification and CoreNLP don't mention this capability</li>
<li>source file of CoreNLP contains <code>Language</code>
classes, but nothing related to language identification - you can
check manually for all 84 occurrence of 'language' word <a href=""https://www.google.com/search?q=language%20site%3Ahttp%3A%2F%2Fmavenbrowse.pauldoo.com%2Fcentral%2Fedu%2Fstanford%2Fnlp%2Fstanford-corenlp%2F3.4%2Fstanford-corenlp-3.4-javadoc.jar&amp;ie=utf-8&amp;oe=utf-8#q=language%20site:http://mavenbrowse.pauldoo.com/central/edu/stanford/nlp/stanford-corenlp/3.4/stanford-corenlp-3.4-javadoc.jar&amp;start=30"" rel=""noreferrer"">here</a></li>
</ol>

<p>Try <a href=""http://www.tutorialspoint.com/tika/tika_language_detection.htm"" rel=""noreferrer"">TIKA</a>, or <a href=""http://textcat.sourceforge.net/"" rel=""noreferrer"">TextCat</a>, or <a href=""https://code.google.com/p/language-detection/"" rel=""noreferrer"">Language Detection Library for Java</a> (they report ""99% over precision for 53 languages"").</p>

<p>In general, quality depends on the size of input text: if it is long enough (say, at least several words and not specially chosen), then precision can be pretty good - about 95%.</p>
",11,10,6333,2015-03-26 22:34:00,https://stackoverflow.com/questions/29290107/detecting-language-using-stanford-nlp
Resource that provides number of documents where the term is covered,"<p>I am looking for resources that provides the number of documents a term is covered in. For example, there is about 25 billion documents that contains the term ""the"" in the indexed internet.</p>
","nlp, stanford-nlp, opennlp, corpus","<p>I don't know of any document frequency lists for large corpora such as the web, but there are some term frequency lists available. For example, there are the <a href=""http://wacky.sslmit.unibo.it/doku.php?id=frequency_lists"" rel=""nofollow"">frequency lists from the web corpora compiled by the Web-As-Corpus Kool Yinitiative</a>, which include the 2-billion ukWaC English web corpus. Alternatively, there are the <a href=""http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"" rel=""nofollow"">n-grams from the Google Books Corpus</a>. </p>

<p>It <a href=""http://arxiv.org/abs/0807.3755"" rel=""nofollow"">has been shown</a> that such term frequency counts can be used to reliably approximate document frequency counts. </p>
",2,1,41,2015-03-30 01:45:37,https://stackoverflow.com/questions/29337199/resource-that-provides-number-of-documents-where-the-term-is-covered
Stanford Parser out of memory,"<p>I am trying to run Stanford parser in Ubuntu using python code. My text file is of 500 Mb which i am trying to parse.I have a RAM of 32GB. I am increasing the JVM size, but i don't whether it is actually increasing or not because every-time i am getting this error. Please help me out</p>

<pre><code>WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Exception in thread ""main"" edu.stanford.nlp.parser.common.NoSuchParseException
    at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:398)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:370)
    at edu.stanford.nlp.parser.lexparser.ParseFiles.processResults(ParseFiles.java:271)
    at edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:215)
    at edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:74)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1513)
</code></pre>
","java, python, ubuntu, jvm, stanford-nlp","<p>You should divide the text file into small pieces and give them to the parser one at a time. Since the parser creates an in-memory representation for a whole ""document"" it is given at a time (which is orders of magnitude bigger than the document on disk), it is a very bad idea to try to give it a 500 MB document in one gulp.</p>

<p>You should also avoid super-long ""sentences"", which can easily occur if casual or web-scraped text lacks sentence delimiters, or you are feeding it big tables or gibberish.  The safest way to avoid this issue is to set a parameter limiting the maximum sentence length, such as <code>-maxLength 100</code>.</p>

<p>You might want to try out the neural network dependency parser, which scales better to large tasks: <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/nndep.shtml</a>.</p>
",8,3,4536,2015-03-30 17:09:35,https://stackoverflow.com/questions/29352149/stanford-parser-out-of-memory
What do I need to know on NLP to be able to use and train Stanford NLP for intent analysis?,"<p>Any books, tutorials, course reccommedations would be much appreciated.</p>

<p>I need to know at what level I need to be regarding NLP to be able to comprehend the Stanford NLP and train it to customize it for my app of commercial sentiment analysis.</p>

<p>My goal is not a career in NLP or become an expert in NLP but only to be as much proficient to be able to understand and use the open source NLP frameworks properly and train them for my application.</p>

<p>For this level, what NLP study/training would be needed?</p>

<p>I'm learning c# and .net as well.</p>
","nlp, stanford-nlp","<p>First: to simply use a sentiment model or train on existing data, there is not too much background to learn:</p>

<ul>
<li>Tokenization</li>
<li>Constituency parsing, parse trees, etc.</li>
<li>Basic machine learning concepts (classification, cost functions, training / development sets, etc.)</li>
</ul>

<p>These are well-documented ideas and are all a Google away. It might be worth it to skim the <a href=""https://www.coursera.org/course/nlp"" rel=""nofollow"">Coursera Natural Language Processing course</a> (produced by people here at Stanford!) for the above ideas as well.</p>

<p>After that, the significant task is understanding how the RNTN sentiment model inside CoreNLP works. You don't need to grasp the math fully, I suppose, but the basic recursive nature of the algorithm is important to understand. The best resource is of course the <a href=""http://nlp.stanford.edu/sentiment/"" rel=""nofollow"">original paper</a> (and there's not much else, to be honest).</p>

<hr>

<p>To train your own sentiment model, you'll need your own sentiment data. Producing this data is no small task. The data for the Stanford sentiment model was crowdsourced, and you may need to do something similar if you want to collect anything near the same scale.</p>

<p>The RNTN sentiment paper (linked above) gives some details on the data format. I'm happy to expand on this further if you do wish to create your own data.</p>
",3,-1,768,2015-03-31 06:42:53,https://stackoverflow.com/questions/29362217/what-do-i-need-to-know-on-nlp-to-be-able-to-use-and-train-stanford-nlp-for-inten
Changing CoreNLP settings at runtime,"<p>I'm using Stanford CoreNLP pipeline, and I wonder whether there is a way to edit basic settings without restarting the whole tool (avoiding the reload of the models).</p>

<p>Now I have:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""tokenize.whitespace"", ""true"");
props.setProperty(""annotators"", ""tokenize, ssplit, pos, ..."");
StanfordCoreNLP stanfordPipeline = new StanfordCoreNLP(props);
</code></pre>

<p>I'd like to change the <code>tokenize.whitespace</code> setting on the fly, without restarting everything. Is it possible?</p>
","java, nlp, stanford-nlp","<p>You should only create a new instance of StanfordCoreNLP with other properties; all common annotators and their models won't be reloaded, because StanfordCoreNLP uses static AnnotatorPool (see <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP.java"" rel=""nofollow noreferrer"">src code</a>, line 103), where AnnotatorPool is:</p>
<blockquote>
<p>An object for keeping track of Annotators. Typical use is to allow
multiple  pipelines to share any Annotators in common.</p>
<p>For example, if multiple pipelines exist, and they both need a
ParserAnnotator, it would be bad to load two such Annotators into
memory.  Instead, an AnnotatorPool will only create one Annotator
and allow both pipelines to share it.</p>
</blockquote>
<p>(taken from <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/AnnotatorPool.java"" rel=""nofollow noreferrer"">javadoc</a>)</p>
",1,1,364,2015-04-02 08:40:25,https://stackoverflow.com/questions/29408588/changing-corenlp-settings-at-runtime
edit config file in stanford pos tagger,"<p>i have tagged a simple sentence and this is my code:</p>

<pre><code>package tagger;

import edu.stanford.nlp.tagger.maxent.MaxentTagger;

public class myTag {

public static void main(String[] args) {

    MaxentTagger tagger = new MaxentTagger(""D:/tagger/english-bidirectional-distsim.tagger"");


    String sample = ""i go to school by bus"";

    String tagged = tagger.tagString(sample);

    System.out.println(tagged);
}

}
</code></pre>

<p>this is the output:</p>

<pre><code>    Reading POS tagger model from D:/tagger/english-bidirectional-distsim.tagger    ... done [3.0 sec].
i_LS go_VB to_TO school_NN by_IN bus_NN 
</code></pre>

<p>after editing the properties file it doesn't have any effect at all.
for example i have changed the tag separator to ( * ) but in the output it still prints ( _ ).</p>

<p>how could i use the model config file in eclipse?</p>
","java, nlp, stanford-nlp, maxent","<p>You can load Properties file and pass it to the constructor of MaxEnt, something like this:</p>

<pre><code>Properties props = new Properties();
props.load(new FileReader(""path/to/properties""));
MaxentTagger tagger = new MaxentTagger(""D:/tagger/english-bidirectional-distsim.tagger"", props);
</code></pre>

<p>You can also set properties in <code>props</code> object directly:</p>

<pre><code>props.setProperty(""tagSeparator"", ""*"");
</code></pre>

<p>NB: if you use the original properties file and it fails with exception like </p>

<pre><code>java.io.FileNotFoundException: /u/nl
p/data/pos_tags_are_useless/egw4-reut.512.clusters (No such file or directory)
</code></pre>

<p>then remove <code>arch</code> and <code>trainFile</code> attributes.</p>
",1,2,696,2015-04-03 08:58:00,https://stackoverflow.com/questions/29429137/edit-config-file-in-stanford-pos-tagger
xml format in stanford pos tagger,"<p>i have tagged 20 sentences and this is my code:</p>
<pre><code>public class myTag {

public static void main(String[] args) {

    Properties props = new Properties();

    try {
        props.load(new FileReader(&quot;D:/tagger/english-bidirectional-distsim.tagger.props&quot;));
    } catch (FileNotFoundException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    } catch (IOException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    }
    
    MaxentTagger tagger = new MaxentTagger(&quot;D:/tagger/english-bidirectional-distsim.tagger&quot;,props);
    
    //==================================================================================================
    try (BufferedReader br = new BufferedReader(new FileReader(&quot;C:/Users/chelsea/Desktop/EN/EN.txt&quot;)))
    {

        String sCurrentLine;

        while ((sCurrentLine = br.readLine()) != null) {
            
            String tagged = tagger.tagString(sCurrentLine);
            System.out.println(tagged);
        }

    } catch (IOException e) {
        e.printStackTrace();
    }
    
}

}
</code></pre>
<p>this is the output:</p>
<p><a href=""https://i.sstatic.net/w0ZJx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/w0ZJx.png"" alt=""img"" /></a></p>

<p>as you can see in sentence node it has a Id attribute and here it's constantly=0 which it should not be.i expect the value=0,1,2,3,4,...
i don't understand what is wrong with my code.</p>
","java, xml, xml-parsing, nlp, stanford-nlp","<p>Stanford POS tagger (strictly speaking, sentence splitter that is applied before POS annotator) generates ids for sentences per input text.
So, you ask <code>tagger</code> to tag <code>sCurrentLine</code> consisting of one sentence, this text is split into sentences - actually, just one, with id = 0; then you ask to tag another text - <code>sCurrentLine</code> from the next iteration - and it again is the only sentence and thereby it is the first sentence with id = 0; and so on.</p>

<p>Thus, if you want correct ids, firstly create the whole text, then pass it to <code>tagger</code>. However, if your input text is already split by sentences, it'll be better to leave things as they are (and generate ids by yourself in the loop, if you need them).</p>
",1,0,232,2015-04-04 06:42:33,https://stackoverflow.com/questions/29443556/xml-format-in-stanford-pos-tagger
Natural language interface to database,"<p>I am developing an application in which I need to get result from the database for the user input(natural English text) query.
For starting up I have <strong>tokenized, parsed, NER, POS tagged</strong> the input text (using <strong>Stanford</strong> NLP library).
But what to do next. I mean what practically should I do further so that I can create a proper SQL query from the input text that can de fired on database. How should I process the context of text.</p>

<p>For example I have a database for a company, where I have different tables like employee, department, etc.
<BR> - So when the user types in his query as ""<em>list of employees</em>"", then it should be able to display the list of employees.
<BR> - When user types ""<em>employees from computer department</em>"", then it should display employees with relation to department whose name is computer.</p>

<p>I am using MS-SQL for database (So query needs to be created for SQL from the natural language). And for programming I am using C#. </p>

<p>I have gone through many different documents but all are theoretical. But I didn't find a proper way of implementation. </p>

<p>So any kind of information, data, link, guidance, solution, other way to implement, etc. will be helpful.</p>
","c#, sql-server, database, nlp, stanford-nlp","<p>This is a quite hard question. I doubt it can be answered unambiguously. There can be more than one approach to solve this task. I'm able to tell only about one of them. The simplest one. It's rule-based. A lot of manual mapping work should be done to implement this.</p>

<p>After POS-tagging, a syntax parsing should be done. You should find not only properties of each word but also dependencies between words: links and types of the links between words. Example: <code>""list of employees""</code>. Here <code>""list""</code> is main, <code>""employees""</code> is dependent. The type of the link is <code>possession</code>.</p>

<p>Then a semantic analysis should be performed. You have a syntax tree. It should be transformed into an abstract syntax tree (AST).</p>

<ul>
<li>Every word or phrase (subtree) should be turned into a semantic node. NER is okay. The node <code>word: ""employees""</code> becomes <code>table: EMPLOYEES</code>. The node <code>word: ""list""</code> and the node <code>word: ""of""</code> become kind of <code>action: SELECT</code>. You should have an ontology of your domain. Nodes of syntax tree are mapped into nodes of ontology. Mapping is controlled by rules. Sample rule: <code>if the word is found in the list of tables then replace the node ""word: x"" with the node ""table: x""</code>. Or another one: <code>if the word is ""list"" and a child word is ""of"" then replace these nodes with ""action: SELECT""</code>.</li>
<li>Every edge (syntax link) should be turned into a semantic link. Mapping is also controlled by rules. E.g., subtree <code>table: EMPLOYEES</code> - <code>link: POSSESSION</code> - <code>entity: ID = 234 (""Computer department"") of table DEPARTMENTS</code> shows that the syntax link <code>POSSESSION</code> in this case has meaning <code>SOURCE</code> or <code>FOREIGN KEY</code>. The sample rule is <code>table1 - link: POSSESSION - table2</code> becomes <code>table1 - where FK_FIELD_OF_TABLE1 = PK_FIELD_OF_TABLE2 - table2</code> if there exist corresponding foreign key in table1 and primary key of table2.</li>
</ul>

<p>Your ontology should contain all terms of your domain. Some terms are static (e.g., actions). Some are dynamic and should be queried at runtime while parsing user NL-query (e.g., table names or dictionary tables contents).</p>

<p>Then you construct small queries in SQL at every edge of AST and aggregate results while reaching the top. Sample AST:</p>

<pre><code>    action: SELECT
          |
       table: EMPLOYEES
      /                \
where e.DEP_ID = d.ID   where e.SALARY &gt; ?
      |                              |
entity: ID = 234 (DEPARTMENTS)      constant: 435
</code></pre>

<p>The left edge should be reduced to a constant equality node like the right one. And then a small query <code>select ID from EMPLOYEES where DEP_ID = 234</code> is processed. The resulting list is stored at the <code>table: EMPLOYEES</code> node.</p>

<pre><code>action: SELECT
   |
table: EMPLOYEES (entities: ID in ( 5, 23, 345 ))
   |
where e.SALARY &gt; ?
   |
constant: 435
</code></pre>

<p>Then the right edge (now it's the only one) is processed for every employee:
<code>select ID from EMPLOYEES where SALARY &gt; 435 and ID = 5</code>, <code>select ID from EMPLOYEES where SALARY &gt; 435 and ID = 23</code>, <code>select ID from EMPLOYEES where SALARY &gt; 435 and ID = 345</code>. The resulting list of IDs is replaced.</p>

<p>Of course, this algorithm can be better. For example, you may wish to combine conditions of all edges of a current node. But it would be difficult to construct one query for the whole tree. So you'd better construct small ones (with simple optimizations probably) and then combine the results.</p>

<p>Also, take a look at the last link I give at the end. There are defined semantically-tractable questions. It's very important to limit the user input. These conditions should be hold:</p>

<ul>
<li>The question should contain one of the wh-words (who, where, what etc.) to determine the type of the question (and the type of the answer). This is not obligatory in your case because queries like <code>list of employees</code> are permitted.</li>
<li>Some stop-words are allowed (like <code>the</code>, <code>a</code>, <code>an</code>) and are just ignored.</li>
<li>All other words should have mappings on ontology nodes.</li>
<li>Any link between words in a sentence should be interpretable by the system.</li>
</ul>

<p>If there are any terms that have no mapping then an error should be shown.</p>

<p>Another approach is used at Wolfram|Alpha. <a href=""http://products.wolframalpha.com/api/faqs.html"" rel=""nofollow"">Their FAQ</a> says that their methods ""are unlike traditional NLP"". I don't know what are those methods. But I would have implemented the system like Wolfram|Alpha by using a plenty of simple syntax patterns. E.g., <code>who is X</code> -> <code>show article X</code>, <code>list of X</code> -> <code>select * from X</code>, <code>with salary more than X</code> -> <code>where SALARY &gt; X</code>.</p>

<p>Also, take a look at controlled languages. The results of NL-queries can be ambiguous and unpredictable. The results of controlled language queries are exact.</p>

<p>More theory:</p>

<p><a href=""http://en.wikipedia.org/wiki/Natural_language_user_interface"" rel=""nofollow"">Wikipedia article</a> about natural language interfaces (to databases also).</p>

<p>One of the most consolidated overview works about NLIDB: Androutsopoulos, I., Ritchie, G. and Thanisch, P. Natural Language Interfaces to Databases - An Introduction.</p>

<p><a href=""http://databases.about.com/od/sqlserver/a/englishquery.htm"" rel=""nofollow"">Microsoft English Query</a>: NLIDB implementation from Microsoft.</p>

<p>The definition of semantically-tractable questions is given at this work: Popescu, A-M., Armanasu, A., Etzioni, O., Ko, D., Yates, A. Modern Natural Language Interfaces to Databases: Composing Statistical Parsing with Semantic Tractability. </p>
",1,4,2299,2015-04-05 08:50:44,https://stackoverflow.com/questions/29455623/natural-language-interface-to-database
How to resolve coreferences for a specific Noun phrase in Stanford CoreNLP,"<p>I have a series of rather large text files and am looking to resolve references in each file for a specific noun phrase eg. 'Harry Potter'</p>

<p>I wouldn't want to run the the pipeline in full for every single possibility of reference resolution as that would take far far too long.</p>

<p>Thanks very much!</p>

<p>Here is what I have so far...</p>

<pre><code>import edu.stanford.nlp.io.*;
import edu.stanford.nlp.pipeline.*;

import java.io.*;
import java.util.Properties;

public class Main {

public static void main(String[] args) throws IOException
{
    // SET INPUT AND OUTPUT FILES
    FileOutputStream xmlOut = new FileOutputStream(new File(""nlp.xml""));
    String input_filename = ""weblink_text.txt"";
    String file_contents = IOUtils.slurpFileNoExceptions(input_filename);

    //SET PROPERTIES
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, cleanxml, ssplit, pos, lemma, ner, parse, dcoref"");

    // ANNOTATE AND OUTPUT
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation annotation = new Annotation(file_contents);

    pipeline.annotate(annotation);
    pipeline.xmlPrint(annotation, xmlOut);

    System.out.println(""Completed"");
}
}
</code></pre>
",stanford-nlp,"<p>1) If you only care about the co-reference resolution of pronominal antecedents, I would recommend checking out David Bamman's <a href=""https://github.com/dbamman/book-nlp"" rel=""nofollow"">book-nlp</a>.</p>

<p>It does very fast coref for novel-length texts, but only for pronominal antecedents (which are probably what you are most interested in anyways).</p>

<p>You can then read in the <code>.tokens</code> file and to build your own coreference graph.</p>

<p>2) If you really need to resolve coref for more than that, try setting the <code>dcoref.maxdist</code> parameter to prevent it from looking back to chapter 1 for material from chapter 20, for instance. I would then save some version of your annotated text (serialized, for instance) to load later so that you don't have to keep running this.</p>

<p>[edit]
3) In the relatively near future, there will be a new coref system (<code>hcoref</code>) in the Stanford CoreNLP build (<a href=""https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/hcoref"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/hcoref</a>) that is based off of <code>depparse</code>, which is very much faster. I've been running that on 100 - 500 sentence-long chunks of text for a whole novel and that's been working for me. (there isn't yet an equivalent in <code>hcoref</code> to <code>dcoref.maxdist</code>)</p>

<p>Another note: if the parsing time is also prohibitively expensive, try setting <code>parse.maxlen</code>.</p>
",2,1,270,2015-04-06 06:21:06,https://stackoverflow.com/questions/29466351/how-to-resolve-coreferences-for-a-specific-noun-phrase-in-stanford-corenlp
How to convert a parsed text into a plain text,"<p>I have parsed a text and extracted all noun-phrases using Stanford parser. Now I need to convert my parsed noun phrases into the plain text:</p>

<p>Input:</p>

<pre><code>(NP (DT the) (JJ dallas) (NN country) (NN club))
(NP (NP (CD 25) (NN cent)) (NP (NNP bingo)))
</code></pre>

<p>Expecting Output:</p>

<pre><code>the dallas country club
Cd 25 cent bingo
</code></pre>

<p>Note: I can clean the text in an ugly way that includes lots of ""replace"" methods. However, I prefer cleaning it in a more professional way or by using a tool embedded in Stanford parser API.   </p>
","java, stanford-nlp, text-processing","<p>Can't speak for the Stanford API, but this can be (fairly) easily accomplished with a regular expression, such as the following:</p>

<pre><code>(?&lt;=\([A-Z]+ )[^\(\)]+
</code></pre>

<p>So what does this do?</p>

<ul>
<li>First, we want to make sure that the text we actually want to match against is preceded by an open parentheses, followed by some number of capital letters, and then a space.  For this, we use a lookbehind.  For example, <code>(?&lt;=foo)bar</code> will match the ""bar"" in ""foobar"", but not in ""ackbar"" or just ""bar"".  In our case, we populate the lookbehind with an escaped open parentheses <code>\(</code>, followed by at least one <code>+</code> capital letter <code>[A-Z]</code>, then a single space character <code></code>.</li>
<li>Matching the subsequent text itself could be tricky, because (in theory - again, I don't know how Stanford's parser handles things) phrases could consist of more than one word, or it could be hyphenated or otherwise strangely punctuated, etc.  So we take advantage of the anti-selector, <code>^</code>, which matches everything EXCEPT what is noted within its selector.  For example, <code>[^ABC]</code> would match all characters EXCEPT capital A, B, and C.  So we simply match at least one <code>+</code> character that is not a closing parentheses <code>\)</code>, which will match all characters until we hit the closing parentheses.</li>
<li>A small bug introduced in the above bullet is that this does not account for nested phrases.  Matching simply against the closing parentheses will match the <code>(NP (CD 25</code> in <code>(NP (NP (CD 25))</code>, which is obviously not what we want.  So we also disallow matching against the opening parentheses <code>\(</code> to account for this.</li>
</ul>

<p>All well and good... except Java makes things more difficult than they need to be.</p>

<ul>
<li>First, Java's lookbehind parser doesn't like lookbehinds of unknown possible length, for some reason.  So we have to change the <code>+</code> in <code>[A-Z]+</code> to use a length range, e.g. <code>[A-Z]{2,3}</code>, which will match strings of capital letters 2-3 characters long.  <strong>Note that if the Stanford parser denotes phrases with keys that have more or fewer capital letters than you have noted here, you will have to adjust that range accordingly!</strong></li>
<li>Next, Java regexes must be compiled before use.  Part of said compliation is turning escaped characters into character literals.  But that will turn <code>\(</code> and its counterpart into literal opening and closing parentheses, which the regex engine will then treat as regex parentheses, which will cause it to fail.  So the escaped backslashes themselves must be escaped before compilation, turning each <code>\</code> into a <code>\\</code>.</li>
</ul>

<p>So our final regex reads:</p>

<pre><code>(?&lt;=\\([A-Z]{2,3} )[^\\(\\)]+
</code></pre>

<p>Then it can be fed into Java in some manner like this...</p>

<pre><code>import java.util.regex.Matcher;
import java.util.regex.Pattern;

...

public ArrayList&lt;String&gt; GetMatchesFromNLP(String text) {
  ArrayList&lt;String&gt; matches = new ArrayList&lt;String&gt;();
  Matcher m = Pattern.compile(""(?&lt;=\\([A-Z]{2,3} )[^\\(\\)]+"").matcher(text);
  while (m.find()) {
    matches.add(m.group());
  }
}
</code></pre>

<p>...which will add each regex match into a new element of an ArrayList.</p>

<p>For performance purposes, you may wish to turn that ArrayList into a linked list of some sort, depending on how long your input text is.</p>
",1,0,120,2015-04-06 23:48:42,https://stackoverflow.com/questions/29481476/how-to-convert-a-parsed-text-into-a-plain-text
What tools can I use to find Part Of Speech Patterns,"<p>I am looking for tools to find Part Of Speech patterns on a corpus of documents. I am using the Stanford NLP tools for POS tagging my documents. Now I would like to query these tagged documents and find some specific POS patterns such as for example</p>

<p><strong>NP</strong> is <strong>JJ</strong>  (ex:  the movie is nice)</p>

<p>or <strong>JJ</strong> <strong>NP</strong>  (ex : excellent foie gras) </p>

<p>Is there a tool that can do this for me in a simple and efficient manner or do I need to write my own ? </p>
","nlp, stanford-nlp, part-of-speech","<p>From Stanford CoreNLP, you can also use TokensRegex to match a pattern in a list of tokens: <a href=""http://nlp.stanford.edu/software/tokensregex.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokensregex.shtml</a></p>

<p>For example, your two patterns would be something like:</p>

<p>[{tag:NN}] [{word:is}] [{tag:JJ}]</p>

<p>[{tag:JJ}] [{tag:NN}]</p>

<p>(Side note, but NP is not a POS tag. Likely, really, what you want is [{tag:/N.*/}] and [{lemma:be}] to catch a broader range of cases).</p>
",2,2,968,2015-04-07 15:40:58,https://stackoverflow.com/questions/29495556/what-tools-can-i-use-to-find-part-of-speech-patterns
How to get alignment between sentiment module and constituency parser in CoreNLP,"<p>I'm using Stanford coreNLP to both parse a text and get sentiment information. The two models give two <code>Tree</code> objects, but they are not related. Is there an easy way to navigate the two elements at the same time, so having an alignment between the two at a token level?</p>
","stanford-nlp, sentiment-analysis","<p>You can just use the sentiment tree as a model of both the grammatical parse and the sentiment — it's simply the original parse tree with extra annotations.</p>

<p><em>Explanation:</em> If you're using the Stanford CoreNLP pipeline, the sentiment annotator draws directly from the parse annotator to build its tree. The tree provided by the sentiment annotator is then just the same binarized parse tree with extra sentiment annotations.</p>
",1,0,88,2015-04-08 13:31:59,https://stackoverflow.com/questions/29516115/how-to-get-alignment-between-sentiment-module-and-constituency-parser-in-corenlp
Forcing POS tags in Stanford CoreNLP,"<p>Is there a way to process an already POS-tagged text using Stanford CoreNLP?</p>

<p>For example, I have the sentence in this format</p>

<pre><code>They_PRP are_VBP hunting_VBG dogs_NNS ._.
</code></pre>

<p>and I'd like to annotate with lemma, ner, parse, etc. by forcing the given POS annotation.</p>

<p><strong>Update.</strong> I tried this code, but it's not working.</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma""); 

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
String sentText = ""They_PRP are_VBP hunting_VBG dogs_NNS ._."";
List&lt;CoreLabel&gt; sentence = new ArrayList&lt;&gt;();

String[] parts = sentText.split(""\\s"");
for (String p : parts) {
    String[] split = p.split(""_"");
    CoreLabel clToken = new CoreLabel();
    clToken.setValue(split[0]);
    clToken.setWord(split[0]);
    clToken.setOriginalText(split[0]);
    clToken.set(CoreAnnotations.PartOfSpeechAnnotation.class, split[1]);
    sentence.add(clToken);
}
Annotation s = new Annotation(sentText);
s.set(CoreAnnotations.TokensAnnotation.class, sentence);

Annotation document = new Annotation(s);
pipeline.annotate(document);
</code></pre>
","stanford-nlp, part-of-speech","<p>The POS annotations will certainly be replaced if you include the <code>pos</code> annotator in the pipeline.</p>

<p>Instead, remove the <code>pos</code> annotator and add the option <code>-enforceRequirements false</code>. This will allow the pipeline to run even though an annotator which <code>lemma</code>, etc. depend on (the <code>pos</code> annotator) is not present. Add the following line before pipeline instantiation:</p>

<pre><code>props.setProperty(""enforceRequirements"", ""false"");
</code></pre>

<p>Of course, behavior is undefined if you venture into this area without setting the proper annotations, so make sure you match the annotations made by the relevant annotator (<a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/POSTaggerAnnotator.java#L142"" rel=""nofollow""><code>POSTaggerAnnotator</code></a> in this case).</p>
",0,0,915,2015-04-08 15:28:20,https://stackoverflow.com/questions/29518946/forcing-pos-tags-in-stanford-corenlp
Running Stanford NLP Event parser without filesystem storage,"<p>I am currently playing with Stanford NLP Biomedical Event parser and I was wondering whether it is possible for it to not use the filesystem structure that it currently requires. I have all tokens and parses in memory already, and do not want to write them to disk in order to pass them through the Event parser. Is that possible?</p>
",stanford-nlp,"<p>Long story short, unfortunately, currently there isn't a way of running it without having everything loaded from disk. TODO!</p>
",0,0,75,2015-04-09 14:14:53,https://stackoverflow.com/questions/29541019/running-stanford-nlp-event-parser-without-filesystem-storage
Stanford NLP CoreNLP don&#39;t do sentence split for chinese,"<p>My environment:</p>

<ul>
<li>CoreNLP 3.5.1</li>
<li>stanford-chinese-corenlp-2015-01-30-models</li>
<li>default property file for chinese :<code>StanfordCoreNLP-chinese.properties</code>
<ul>
<li><code>annotators = segment, ssplit</code></li>
</ul></li>
</ul>

<hr>

<p>My testing text is <code>""這是第一個句子。這是第二個句子。""</code>
I get sentence from </p>

<pre><code>val sentences = annotation.get(classOf[SentencesAnnotation])
for (sent &lt;- sentences) {
  count+=1
  println(""sentence{$count} = "" + sent.get(classOf[TextAnnotation]))
}
</code></pre>

<p><strong>It always prints the whole testing text as one sentence , not the expected two here :</strong></p>

<pre><code>sentence1 = 這是第一個句子。這是第二個句子。
</code></pre>

<p>the expected are:</p>

<pre><code>expected sentence1 = 這是第一個句子。
expected sentence2 = 這是第二個句子。
</code></pre>

<hr>

<p>Even the same result if I add more properties like :</p>

<pre><code>ssplit.eolonly = false
ssplit.isOneSentence = false
ssplit.newlineIsSentenceBreak = always
ssplit.boundaryTokenRegex = [.]|[!?]+|[。]|[！？]+
</code></pre>

<hr>

<p>The CoreNLP logs are</p>

<pre><code>Registering annotator segment with class edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
Adding annotator segment
Loading Segmentation Model [edu/stanford/nlp/models/segmenter/chinese/ctb.gz]...Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... Loading Chinese dictionaries from 1 files:
  edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz

loading dictionaries from edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz...Done. Unique words in ChineseDictionary is: 423200
done [56.9 sec].
done. Time elapsed: 57041 ms
Adding annotator ssplit
Adding Segmentation annotation...output: [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]
INFO: TagAffixDetector: useChPos=false | useCTBChar2=true | usePKChar2=false
INFO: TagAffixDetector: building TagAffixDetector from edu/stanford/nlp/models/segmenter/chinese/dict/character_list and edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb
Loading character dictionary file from edu/stanford/nlp/models/segmenter/chinese/dict/character_list
Loading affix dictionary from edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb
這是第一個句子。這是第二個句子。
---&gt;
[這是, 第一, 個, 句子, 。, 這是, 第二, 個, 句子, 。]
done. Time elapsed: 419 ms
</code></pre>

<p>I once saw <a href=""http://webcache.googleusercontent.com/search?q=cache:O-n5_2O6x_AJ:stackoverflow.com.80bola.com/questions/28167258/stanford-nlp-filenotfoundexception-while-processing-chinese-text&amp;hl=zh-TW&amp;gl=tw&amp;strip=0"" rel=""nofollow"">someone</a> get the following log (CoreNLP 3.5.0) ; however oddly I do not have this log:</p>

<pre><code>Adding annotator ssplit edu.stanford.nlp.pipeline.AnnotatorImplementations:ssplit.boundaryTokenRegex=[.]|[!?]+|[。]|[！？]+
</code></pre>

<hr>

<p>What's the problem ? Is there workaround? If unresolvable I can split it myself but I do not know how to integrate my splits into the CoreNLP pipeline.</p>
","nlp, stanford-nlp","<p>OK, I pull off a work around.  </p>

<p>define the ssplit annotator myself.</p>

<p>For convenient I hardcoding the parameter here, though the right way should parse the props.</p>

<pre><code>class MyWordsToSentencesAnnotator extends WordsToSentencesAnnotator(
  true,
  ""[.]|[!?]+|[。]|[！？]+"",
  null,
  null,
  ""never"") {
  def this(name: String, props: Properties) { this() }
}
</code></pre>

<p>and designate the class at property file.</p>

<pre><code>customAnnotatorClass.myssplit = ...
</code></pre>

<hr>

<p>Apparently ,I guess the default CoreNLP Pipeline setting or code has bug?</p>
",2,4,1185,2015-04-09 15:20:06,https://stackoverflow.com/questions/29542569/stanford-nlp-corenlp-dont-do-sentence-split-for-chinese
stanford-nlp How to predict document level,"<p>I am using stanford-corenlp-3.4.1 version. I have the question if we give the sentence has multiple sentence, how do I calculate the prediction.</p>

<p>for eg :
String text = ""IT was very fantastic experience. it was a pathetice experience"";</p>

<p>i am getting the prediction of </p>

<p>IT was very fantastic experience : positive.</p>

<p>it was a pathetice experience : negative.</p>

<p>i  am getting the prediction based on each sentence level.how do i get it document level.</p>

<p>based on reading the total text i need to get it either positive or negative.</p>

<p>Here is the sample code:</p>

<pre><code>Properties props = new Properties();

props.setProperty(""annotators"",""tokenize, ssplit, pos, lemma, parse, sentiment"");

        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        Annotation annotation = new Annotation(""IT was very fantastic experience. it was a pathetice experience"");
        pipeline.annotate(annotation);
        List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
        for (CoreMap sentence : sentences) {
          String sentiment = sentence.get(SentimentCoreAnnotations.ClassName.class);
          System.out.println(sentiment + ""\t"" + sentence);

    }
</code></pre>

<p>Results:</p>

<p>Very positive:  IT was very fantastic experience.
Negative    it was a pathetice experience</p>

<p>thanks</p>
","stanford-nlp, sentiment-analysis","<p>To my knowledge, Stanford NLP does not provide sentiment analysis above the sentence level. One solution would be to compute some sort of mean sentiment value across all sentences in your text, but obviously that's only going to give you a rough idea of the overall sentiment. </p>
",2,0,1072,2015-04-10 00:25:52,https://stackoverflow.com/questions/29551332/stanford-nlp-how-to-predict-document-level
Spanish POS tagging with Stanford NLP - is it possible to get the person/number/gender?,"<p>I'm using Stanford NLP to do POS tagging for Spanish texts. I can get a POS Tag for each word but I notice that I am only given the first four sections of the Ancora tag and it's missing the last three sections for person, number and gender.</p>

<ul>
<li><p>Why does Stanford NLP only use a reduced version of the Ancora tag?</p></li>
<li><p>Is it possible to get the entire tag using Stanford NLP?</p></li>
</ul>

<p>Here is my code (please excuse the jruby...):</p>

<pre><code>props = java.util.Properties.new()
props.put(""tokenize.language"", ""es"")
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"")
props.put(""ner.model"", ""edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz"")
props.put(""pos.model"", ""/stanford-postagger-full-2015-01-30/models/spanish-distsim.tagger"")
props.put(""parse.model"", ""edu/stanford/nlp/models/lexparser/spanishPCFG.ser.gz"")

pipeline = StanfordCoreNLP.new(props)
annotation = Annotation.new(""No sé qué estoy haciendo. Me pregunto si esto va a funcionar."")
</code></pre>

<p>I am getting this as the output:</p>

<blockquote>
  <p>[Text=No CharacterOffsetBegin=0 CharacterOffsetEnd=2 PartOfSpeech=rn
  Lemma=no NamedEntityTag=O] [Text=sé CharacterOffsetBegin=3
  CharacterOffsetEnd=5 PartOfSpeech=vmip000 Lemma=sé NamedEntityTag=O]
  [Text=qué CharacterOffsetBegin=6 CharacterOffsetEnd=9
  PartOfSpeech=pt000000 Lemma=qué NamedEntityTag=O] [Text=estoy
  CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=vmip000
  Lemma=estoy NamedEntityTag=O] [Text=haciendo CharacterOffsetBegin=16
  CharacterOffsetEnd=24 PartOfSpeech=vmg0000 Lemma=haciendo
  NamedEntityTag=O] [Text=. CharacterOffsetBegin=24
  CharacterOffsetEnd=25 PartOfSpeech=fp Lemma=. NamedEntityTag=O]</p>
</blockquote>

<p><s>(I notice that the lemmas are incorrect also, but that's probably an issue for a separate question.</s> Nevermind, I see that Stanford NLP does not support Spanish lemmatization.)</p>
","stanford-nlp, pos-tagger","<blockquote>
  <p>Why does Stanford NLP only use a reduced version of the Ancora tag?</p>
</blockquote>

<p>This was a practical decision made to ensure high tagging accuracy. (Retaining morphological information on tags caused the entire tagger to suffer from data sparsity, and do worse not only on morphological annotation but all over the board.)</p>

<blockquote>
  <p>Is it possible to get the entire tag using Stanford NLP?</p>
</blockquote>

<p>No. You could get quite far doing this with a simple rule-based system, though, or use the Stanford Classifier to train your own morphological annotator. (Feel free to share your code if you pick either path!)</p>
",1,1,1663,2015-04-10 07:53:32,https://stackoverflow.com/questions/29556109/spanish-pos-tagging-with-stanford-nlp-is-it-possible-to-get-the-person-number
Using Tsurgeon for recursive stripping of phrases,"<p>I would like to ""simplify"" phrases by recursively stripping modifiers. For example, given a tree with two embedded PPs</p>

<pre><code>(NP (NP (JJ Microbial) (NN expression)) (PP (IN in) (NP (NP (DT the) (NN rhizosphere)) (PP (IN of) (NP (NNS willows))))))
</code></pre>

<p>I want to derive first</p>

<pre><code>(NP (NP (JJ Microbial) (NN expression)) (PP (IN in) (NP (NP (DT the) (NN rhizosphere)))))
</code></pre>

<p>and second</p>

<pre><code>(NP (NP (JJ Microbial) (NN expression)))
</code></pre>

<p>However, a script like this</p>

<pre><code>PP=pp !&lt;&lt; PP

delete PP
</code></pre>

<p>will delete both PPs right away, because after the first match, Tsurgeon will immediately apply the same pattern again.</p>

<p>Is there a way to force Tsurgeon to apply the operation only once or is there some other trick to accomplish this?</p>
","nlp, stanford-nlp","<p>Try it in two steps:</p>

<ol>
<li>Mark the required node for deletion.</li>
</ol>

<p>Like this:</p>

<pre><code>PP=pp !&lt;&lt; PP !&lt;&lt; DELETE_ME
relabel pp DELETE_ME
</code></pre>

<ol start=""2"">
<li>Delete all marked nodes.</li>
</ol>

<p>Simply:</p>

<pre><code>DELETE_ME=pp
delete pp
</code></pre>
",0,0,36,2015-04-10 15:33:17,https://stackoverflow.com/questions/29565332/using-tsurgeon-for-recursive-stripping-of-phrases
Stanford NLP how to preprocessing the text,"<p>I have a sentence like this ""The people working in @walman are not good""</p>

<p>I have a preprocessed text file which contains the mappings, similar to the following two lines:</p>

<pre><code>@walman   Walman
@text     Test
</code></pre>

<p>For the above sentence I have to read through the text file and replace the word with any matching word found in the text file.</p>

<p>The above sentence will change to ""The people working in Walman are not good""</p>

<p>I am looking for an API available in Standford NLP to read the input text file and replace the text.</p>
","replace, nlp, stanford-nlp","<p>The only NLP-related part here is tokenization. You should read your text file into the map (e.g. <code>HashMap</code> in case of Java), then for each new sentence, you should tokenize it (e.g. by <a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow"">Stanford tokenizer</a>), and check for each token if it is presented in the map; if yes, just replace by the found value from the map, if no, do nothing for this token.</p>

<p>Sample code for tokenization (taken from the link above):</p>

<pre><code>  String arg = ""The people working in @walman is not good"";
  PTBTokenizer&lt;CoreLabel&gt; ptbt = new PTBTokenizer&lt;CoreLabel&gt;(new StringReader(arg),
          new CoreLabelTokenFactory(), """");
  for (CoreLabel label; ptbt.hasNext(); ) {
    label = ptbt.next();
    System.out.println(label);
  }
}
</code></pre>

<p>So, <code>label.toString()</code> gives you the token without any suffixes.</p>
",1,-1,2342,2015-04-10 20:47:17,https://stackoverflow.com/questions/29570412/stanford-nlp-how-to-preprocessing-the-text
Is there a Python wrapper for Stanford Neural Net based dependency parser?,"<p>I know about the Python wrappers for Stanford CoreNLP package but this package does not seem to contain neural net based dependency parser model. Rather it is present in Stanford-parser-full-****-<strong>-</strong> package for which I can't find any Python wrapper. <strong>My Question: Is there a Python wrapper that would parse using Stanford Neural Net based dependency parser?</strong> Any suggestions or directions would be helpful. Thanks!</p>
","python, parsing, nlp, neural-network, stanford-nlp","<p>I don't know of any such wrapper at the moment, and there are no plans at Stanford to build one. (Maybe the NLTK developers would be up for the challenge?)</p>
",1,1,211,2015-04-11 06:52:08,https://stackoverflow.com/questions/29575034/is-there-a-python-wrapper-for-stanford-neural-net-based-dependency-parser
what is distant supervision?,"<p>According to my understanding, Distant Supervision is the process of specifying the concept which the individual words of a passage, usually a sentence, are trying to convey. </p>

<p>For example, a database maintains the structured relationship <code>concerns( NLP, this sentence).</code></p>

<p>Our distant supervision system would take as input the sentence: <code>""This is a sentence about NLP.""</code></p>

<p>Based on this sentence it would recognize the entities, since as a pre-processing step the sentence would have been passed through a named-entity recognizer, <code>NLP</code> &amp; <code>this sentence</code>. </p>

<p>Since our database has it that <code>NLP</code> and <code>this sentence</code> are related by the bond of <code>concern(s)</code> it would identify the input sentence as expressing the relationship <code>Concerns(NLP, this sentence)</code>. </p>

<p>My questions is two fold: </p>

<p>1) What is the use of that? Is it that later our system might see a sentence in ""the wild"" such as <code>That sentence is about OPP</code> and realize that it's seen something similar to that before and thereby realize the novel relationship such that <code>concerns(OPP, that sentence).</code>, based only on the words/ individual tokens?</p>

<p>2) Does it take into account the actual words of the sentence? The verb 'is' and the adverb 'about' for instance, realizing (through WordNet or some other hyponymy system) that this is somehow similar to the higher-order concept ""concerns""?</p>

<p>Does anyone have some code used to generate a distant supervision system that I could look at, i.e. a system that cross references a KB, such as Freebase, and a corpus, such as the NYTimes, and produces a distant supervision database? I think that would go a long way in clarifying my conception of distant supervision. </p>
","nlp, stanford-nlp, supervised-learning, unsupervised-learning","<p>RE 1) Yes, this is exactly right. In the end, what we want is a classifier that takes as input text, and a pair of entity mentions in the text, and tells us what relation holds between those entities in that sentence. Distant supervision is a way of mocking this training data, using ""distant supervision"" from a known knowledge base. But, the end goal is the same as most machine learning tasks: generalize to new sentences.</p>

<p>RE 2) Certainly! Distant supervision only applies to how the training data is generated [1]. Once you've assumed distant supervision, what you're left with is a corpus of (sentence, relation_for_sentence) pairs, and then you extract all of the usual NLP features on the sentence.</p>

<p>[1] To a first approximation -- there are ""distantly supervised"" models (like MultiR and MIML-RE) which don't directly generate fake training data, but incorporate the supervision indirectly into the training procedure itself. But, even in these, there is a factor in the latent-variable model that amounts to a per-sentence classification, and it's just that the output variable is latent rather than naively ""observed"" as in vanilla distant supervision.</p>
",14,20,17576,2015-04-11 08:29:40,https://stackoverflow.com/questions/29575784/what-is-distant-supervision
multiple files input to stanford NER preserving naming for each output,"<p>I have many files, (the NYTimes corpus for '05, '06, &amp; '07) , I want to run them all through the <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">Stanford NER</a>, ""easy"" you might think, ""just follow the commands in the README doc"", but if you thought that just now, you would be mistaken, because my situation is a bit more complicated. I don't want them all outputted into some big jumbled mess, I want to preserve the naming structure of each file, so for example, one file is named <code>1822873.xml</code> and I processed it earlier using the following command:</p>

<pre><code>java -mx600m -cp /home/matthias/Workbench/SUTD/nytimes_corpus/stanford-ner-2015-01-30/stanford-ner-3.5.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile /home/matthias/Workbench/SUTD/nytimes_corpus/1822873.xml -outputFormat inlineXML &gt;&gt; output.curtis
</code></pre>

<p>If I were to follow <a href=""https://stackoverflow.com/questions/27544545/python-or-bash-script-to-pass-all-files-in-a-folder-to-java-command-line"">this question</a>, i.e. many files all listed in the command one after the other, and then pipe that to somewhere, wouldn't it just send them all to the same file? That sounds like a headache disastor of the highest order. </p>

<p>Is there some way to send each file to a seperate output file, so for instance, our old friend <code>1822873.xml</code> would emerge from this process as, say <code>1822873.output.xml</code>, and likewise for each of the other thousand some odd files. Please keep in mind that I'm trying to achieve this <a href=""http://listenonrepeat.com/watch/?hl=en-GB&amp;gl=SG&amp;v=6Z66wVo7uNw?v=6Z66wVo7uNw#Curtis_Mayfield_-_Move_On_Up"" rel=""nofollow noreferrer"">expeditiously</a>.</p>

<p>I guess this should be possible, but what is the best way to do it? with some kind of terminal command, or maybe write a small script?</p>

<p>Maybe one among you has some experience with this type of thing. </p>

<p>Thank you for your consideration. </p>
","java, bash, stanford-nlp","<p><strong>UPDATE</strong></p>

<p>you can do it with a bash script <a href=""https://stackoverflow.com/questions/29588423/bash-script-to-navigate-directory-substructure-and-then-operate-on-xml-files"">like this</a>.</p>

<hr>

<p>@duhaime I tried that but I had an issue with the classifier, also is it possible to formulate the output for that as inline xml?</p>

<p>With respect to my original question, <a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2013-August/003929.html"" rel=""nofollow noreferrer"">check out what I've found</a>:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <blockquote>
        <blockquote>
          <blockquote>
            <p>Unfortunately, there is no option to have multiple input files go to
            multiple output files.  The best you can do in the current situation
            is to run the CRFClassifier once for each input file you have.  If
            you
            have a ton of small files, loading the model will be an expensive
            part
            of this operation, and you might want to use the CRFClassifier
            server
            program and feed files one at a time through the client.  However, I
            doubt that will be worth the effort except in the specific case of
            having very many small files.</p>
            
            <p>We will try to add this as a feature for the next distribution (we
            have a general fix-it day coming up) but no promises.</p>
            
            <p>John</p>
          </blockquote>
        </blockquote>
      </blockquote>
    </blockquote>
  </blockquote>
</blockquote>

<p>My files are all numbered in ascending order, do you think it would be possible to write some kind of bash script with a loop to processes each of them one at a time?</p>
",0,0,755,2015-04-11 11:15:01,https://stackoverflow.com/questions/29577238/multiple-files-input-to-stanford-ner-preserving-naming-for-each-output
stanford nlp pos tagging,"<p>Im doing my POS (part of speech tagging) from this tagger. But when i combine that part to my maven project it doesnt work. Is there a way where i can user stanford directly to do pos without using a seperate tagger? I want the output as same as this. </p>

<pre><code> MaxentTagger tagger = new MaxentTagger(""taggers/left3words-wsj-0-18.tagger"");
        String sample = ""Im so happy about my marks"";
        String tagged = tagger.tagString(sample);
        System.out.println(tagged);
</code></pre>

<p>output:Im/NNP so/RB happy/JJ about/IN my/PRP$ marks/NNS </p>
","java, maven, nlp, stanford-nlp","<p>Of course Stanford CoreNLP can do tagging directly. The following lines of code tag your example, and give you the desired output.</p>

<pre><code>Properties props = new Properties();

props.setProperty(""annotators"",""tokenize, ssplit, pos"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = new Annotation(""I'm so happy about my marks"");
pipeline.annotate(annotation);
List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
    for (CoreLabel token: sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        String word = token.get(CoreAnnotations.TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
        System.out.println(word + ""/"" + pos);
    }
}
</code></pre>
",8,1,4077,2015-04-11 17:28:39,https://stackoverflow.com/questions/29580849/stanford-nlp-pos-tagging
How to solve the UnicodeDecodeError when using stanford parser API in NLTK for python?,"<p>I want to use stanford parser using Python, I use Windows 7, I've installed Python 2.7 and nltk 3.0 and I downloaded the stanford parser from the official site.</p>

<p>I got the javahome environment problem which I solved, then I got this error message: </p>

<blockquote>
  <p>UnicodeDecodeError: 'ascii' codec can't decode byte 0xe9 in position
  0: ordinal not in range(128)</p>
</blockquote>

<p>and I can't find a solution for this problem.</p>

<p>I used the next code : </p>

<pre><code># -*- coding: utf-8 -*-

from nltk.parse import stanford

parser = stanford.StanfordParser(model_path='C:\Program Files (x86)\stanford-parser-full-2015-01-30\edu\stanford\nlp\models\lexparser\englishPCFG.ser.gz')

sent = 'my name is zim'
parser.parse(sent)
</code></pre>

<p>I've looked in stack overflow for a solution but I didn't find one.</p>
","python, unicode, character-encoding, nltk, stanford-nlp","<p>I've found what was the problem that caused the error that I've encountered</p>

<blockquote>
  <p>raise OSError('Java command failed : ' + str(cmd)) OSError: Java command failed :...</p>
</blockquote>

<p>This error is due to the bad interpretation of the address in the following instruction : </p>

<pre><code>parser = stanford.StanfordParser(model_path='C:\Program Files (x86)\stanford-parser-full-2015-01-30\edu\stanford\nlp\models\lexparser\englishPCFG.ser.gz').
</code></pre>

<p>Python or Java interpreted the <code>...\nlp\..</code> as <code>\n lp\...</code>, so as a result, it couldn't find the path.</p>

<p>I've tried a simple solution, I've renamed the folder nlp. And it worked!</p>
",0,0,1176,2015-04-11 19:54:54,https://stackoverflow.com/questions/29582351/how-to-solve-the-unicodedecodeerror-when-using-stanford-parser-api-in-nltk-for-p
"short script to process a directory full of files, one by one, maintaining names","<p>I'm trying to run a command line argument on a directory full of files. The files are named by numbers in ascending order. </p>

<pre><code>1815837.xml
1815838.xml
1815839.xml
1815840.xml
</code></pre>

<p>Would it be possible to write some kind of script to take all the files in the directory and one by one feed them through the following command (the Stanford NER):</p>

<pre><code>java -mx600m -cp /home/matthias/Workbench/SUTD/nytimes_corpus/NER/stanford-ner-2015-01-30/stanford-ner-3.5.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier /home/matthias/Workbench/SUTD/nytimes_corpus/NER/stanford-ner-2015-01-30/classifiers/english.all.3class.distsim.crf.ser.gz -textFile 1815838.xml -outputFormat inlineXML &gt;&gt; 1815838_output.xml
</code></pre>

<p>That code that I'm invoking there outputs the result to the console, so I'm piping it to a specially named file, i.e. <code>&gt;&gt; 1815838_output.xml</code> It's important that I maintain that naming convention. </p>

<p>Is it feasible to run that code on every file in a directory and save the output accordingly with a short java program or a bash script? What would it look like?</p>

<p>This question is tangentially related to a <a href=""https://stackoverflow.com/questions/29577238/multiple-files-input-to-stanford-ner-preserving-naming-for-each-output"">previous inquiry</a>. </p>

<p>My hazy notion is something like this:</p>

<pre><code>*X* = '1815838'

while(still files in directory)
{
   java -mx600m -cp stanford-ner-2015-01-30/stanford-ner-3.5.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier english.all.3class.distsim.crf.ser.gz -textFile *X*.xml -outputFormat inlineXML &gt;&gt; *X* + '_output.xml'

X--

}
</code></pre>

<p>In my mind, that works, but I don't know if that's a real thing or if it would work in real life, I googled and didn't find anything like that, but maybe I didn't know exactly what to ask. Is this reasonable? Can someone maybe show me the way?</p>

<hr>

<p><strong>UPDATE</strong></p>

<pre><code>-rwxr-xr-x 1 matthias matthias 3.8K Apr 10 20:35 1815851.xml*
-rw-r--r-- 1 matthias matthias 4.6K Apr 12 16:25 1815851_output.xml
-rw-r--r-- 1 matthias matthias 5.3K Apr 12 16:25 1815851_output_output.xml
-rwxr-xr-x 1 matthias matthias 3.3K Apr 10 20:35 1815852.xml*
-rw-r--r-- 1 matthias matthias 4.5K Apr 12 16:25 1815852_output.xml
-rw-r--r-- 1 matthias matthias 5.6K Apr 12 16:25 1815852_output_output.xml
-rwxr-xr-x 1 matthias matthias 2.5K Apr 10 20:35 1815853.xml*
-rw-r--r-- 1 matthias matthias 2.9K Apr 12 16:25 1815853_output.xml
-rw-r--r-- 1 matthias matthias 3.3K Apr 12 16:25 1815853_output_output.xml
-rwxr-xr-x 1 matthias matthias 2.4K Apr 10 20:35 1815854.xml*
-rw-r--r-- 1 matthias matthias 2.7K Apr 12 16:25 1815854_output.xml
-rw-r--r-- 1 matthias matthias 2.9K Apr 12 16:25 1815854_output_output.xml
-rwxr-xr-x 1 matthias matthias 2.8K Apr 10 20:35 1815855.xml*
-rw-r--r-- 1 matthias matthias 3.6K Apr 12 16:25 1815855_output.xml
-rw-r--r-- 1 matthias matthias 4.4K Apr 12 16:26 1815855_output_output.xml
</code></pre>

<p>without the loop, but also, curiously, nothing written to output</p>

<pre><code>g=""$(1816001.xml $f .xml)_output.xml""
java -mx600m -cp /home/matthias/Workbench/SUTD/nytimes_corpus/NER/stanford-ner-2015-01-30/stanford-ner-3.5.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier /home/matthias/Workbench/SUTD/nytimes_corpus/NER/stanford-ner-2015-01-30/classifiers/english.all.3class.distsim.crf.ser.gz -textFile $f -outputFormat inlineXML &gt; $g
</code></pre>
","java, bash, stanford-nlp","<p>That's easily done: Assuming your current directory is where the files are:</p>

<pre><code>for f in *.xml ; do
    echo $f | grep -q '_output\.xml$' &amp;&amp; continue # skip output files
    g=""$(basename $f .xml)_output.xml""
    command a_lot_of_arguments $f more_arguments &gt;&gt; $g
done
</code></pre>

<p>Though I wonder whether you want <code>&gt;&gt;</code> or <code>&gt;</code> for redirection. The former will append to the output file if it already exists, for example from a previous run of the same script. The latter will overwrite it.</p>
",1,1,83,2015-04-12 07:50:21,https://stackoverflow.com/questions/29587234/short-script-to-process-a-directory-full-of-files-one-by-one-maintaining-names
MissingMethodException using IKVM,"<p>I'm trying to use Stanford CoreNLP (which is a Java project) in C#.</p>

<p>I found this <a href=""http://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html"" rel=""nofollow"">Nuget package</a> which contains CoreNLP converted to .NET using IKVM, and it's working fine, however I need to do some modifications on the java project as well.</p>

<p>I downloaded CoreNLP from <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">Github</a>, I can build the CoreNLP JAR from Ant, and it's also running fine in eclipse, however <strong>I'm having problems in converting JAR to DLL</strong>. Based on some <a href=""https://ci.appveyor.com/project/sergey-tihon/stanford-nlp-net/build/0.0.1.15"" rel=""nofollow"">build-log</a> that I found in google, I'm doing this:</p>

<pre><code>ikvmc.exe  -version:2.1 ..\lib\joda-time.jar -out:joda-time.dll 
ikvmc.exe  -r:joda-time.dll -version:0.4.7 ..\lib\jollyday-0.4.7.jar -out:jollyday.dll 
ikvmc.exe  -version:0.23 ..\lib\ejml-0.23.jar -out:ejml-0.23.dll 
ikvmc.exe  -version:1.2.10 ..\lib\xom-1.2.10.jar -out:xom.dll 
ikvmc.exe  -version:1.0 ..\lib\javax.json.jar -out:javax.json.dll 
ikvmc.exe  -r:joda-time.dll -r:jollyday.dll -r:ejml-0.23.dll -r:xom.dll -r:javax.json.dll -version:3.5.0 ..\javanlp-core.jar -out:javanlp-core.dll 
</code></pre>

<p>All I get from the following conversions are a few warnings about referenced classed that can't be found:</p>

<pre><code>warning IKVMC0100: Class ""org.apache.xerces.parsers.SAXParser"" not found
warning IKVMC0100: Class ""junit.framework.TestCase"" not found
warning IKVMC0100: Class ""org.apache.xerces.impl.Version"" not found
...
warning IKVMC0100: Class ""junit.framework.TestCase"" not found
warning IKVMC0100: Class ""javax.servlet.http.HttpServlet"" not found
warning IKVMC0100: Class ""javax.servlet.Filter"" not found
warning IKVMC0100: Class ""com.google.protobuf.Descriptors$FileDescriptor$InternalDescriptorAssigner"" not found
warning IKVMC0100: Class ""com.google.protobuf.GeneratedMessage$Builder"" not found
warning IKVMC0100: Class ""com.google.protobuf.GeneratedMessage"" not found
warning IKVMC0100: Class ""com.google.protobuf.MessageOrBuilder"" not found
warning IKVMC0100: Class ""com.google.protobuf.GeneratedMessage$ExtendableBuilder"" not found
warning IKVMC0100: Class ""com.google.protobuf.GeneratedMessage$ExtendableMessage"" not found
warning IKVMC0100: Class ""com.google.protobuf.GeneratedMessage$ExtendableMessageOrBuilder"" not found
warning IKVMC0100: Class ""com.google.protobuf.Internal$EnumLiteMap"" not found
warning IKVMC0100: Class ""com.google.protobuf.ProtocolMessageEnum"" not found
warning IKVMC0100: Class ""junit.framework.TestSuite"" not found
warning IKVMC0100: Class ""junit.framework.Assert"" not found
warning IKVMC0100: Class ""com.apple.eawt.ApplicationAdapter"" not found
warning IKVMC0100: Class ""org.junit.Assert"" not found
warning IKVMC0100: Class ""org.apache.lucene.analysis.core.KeywordAnalyzer"" not found
warning IKVMC0100: Class ""org.apache.lucene.index.IndexWriterConfig"" not found
warning IKVMC0100: Class ""org.apache.lucene.util.Version"" not found
warning IKVMC0100: Class ""org.apache.lucene.store.FSDirectory"" not found
warning IKVMC0100: Class ""org.apache.lucene.index.DirectoryReader"" not found
warning IKVMC0100: Class ""org.apache.lucene.search.IndexSearcher"" not found
warning IKVMC0100: Class ""org.apache.lucene.search.BooleanQuery"" not found
warning IKVMC0100: Class ""org.apache.lucene.search.BooleanClause"" not found
warning IKVMC0100: Class ""org.apache.lucene.search.TermQuery"" not found
warning IKVMC0100: Class ""org.apache.lucene.index.Term"" not found
warning IKVMC0100: Class ""org.apache.lucene.search.BooleanClause$Occur"" not found
warning IKVMC0100: Class ""org.apache.lucene.search.TopDocs"" not found
warning IKVMC0100: Class ""org.apache.lucene.search.ScoreDoc"" not found
warning IKVMC0100: Class ""org.apache.lucene.document.Document"" not found
warning IKVMC0100: Class ""org.apache.lucene.index.IndexWriter"" not found
warning IKVMC0100: Class ""org.apache.lucene.queryparser.classic.ParseException"" not found
warning IKVMC0100: Class ""org.apache.lucene.document.StringField"" not found
warning IKVMC0100: Class ""org.apache.lucene.document.Field$Store"" not found
warning IKVMC0100: Class ""org.apache.lucene.document.Field"" not found
warning IKVMC0100: Class ""org.apache.lucene.search.Query"" not found
warning IKVMC0100: Class ""org.apache.lucene.store.Directory"" not found
warning IKVMC0100: Class ""org.apache.lucene.index.CheckIndex"" not found
warning IKVMC0100: Class ""org.apache.lucene.index.CheckIndex$Status"" not found
warning IKVMC0100: Class ""org.apache.lucene.store.NIOFSDirectory"" not found
warning IKVMC0100: Class ""org.apache.lucene.util.BytesRef"" not found
warning IKVMC0100: Class ""org.apache.lucene.index.IndexReader"" not found
warning IKVMC0100: Class ""com.google.protobuf.Descriptors$FileDescriptor"" not found
warning IKVMC0100: Class ""com.google.protobuf.Descriptors$Descriptor"" not found
warning IKVMC0100: Class ""com.google.protobuf.GeneratedMessage$FieldAccessorTable"" not found
warning IKVMC0100: Class ""com.google.protobuf.Descriptors"" not found
warning IKVMC0100: Class ""junit.framework.AssertionFailedError"" not found
warning IKVMC0100: Class ""org.apache.lucene.document.FieldType"" not found
</code></pre>

<p>P.S.: I also tried converting protobuf.jar, but didn't help with the problem described below.</p>

<p>Problem is: when I replace the Nuget references for my freshly-compiled references (6 dlls), it compiles fine (finds all references), but throws this <strong>MissingMethodException: Additional information: Method not found: 'Void edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(java.util.Properties)'.</strong></p>

<p>Can anyone shed some light? </p>
","stanford-nlp, ikvm, missingmethodexception","<p>I guess I figured it out. Here is what fixed:</p>

<ol>
<li>Removed Nuget Package IKVM 8.0.5449.1</li>
<li>Installed Nuget Package IKVM 8.0.5449.0 (which was the <a href=""http://weblog.ikvm.net/2014/12/02/IKVMNET80ReleaseCandidate0.aspx"" rel=""nofollow"">version</a> I was using for converting my JAR files)</li>
<li>Clean and rebuild.</li>
</ol>

<p>If I understand correctly, the ikvmc.exe I used to convert my JAR files must be the same version of the referenced runtime DLLs. </p>
",1,0,1367,2015-04-16 01:58:35,https://stackoverflow.com/questions/29664079/missingmethodexception-using-ikvm
The inconsistency between the Parser in CoreNLP and the standalone Stanford Parser,"<p>Just found an inconsistency issue that the parsing results of the Parser in CoreNLP and the standalone Stanford Parser are different. </p>

<p>For example, given a sentence ""Microsoft released Windows 10."". </p>

<p>The Parser in CoreNLP (<a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/corenlp/process</a>) will give the following result:
<img src=""https://i.sstatic.net/P2O5V.png"" alt=""enter image description here"">
​
However, the standalone Stanford Parser (<a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a>) will give the following result:
<img src=""https://i.sstatic.net/KIpP6.png"" alt=""enter image description here""></p>

<p>​I also tried to run the codes on my machines. Both the parsers used the same model trained on the same date (englishPCFG.ser.gz, 2015-01-29). But the results given by the two parsers are still different. I tried several other sentences, and it looks that the standalone parser gives better results. </p>

<p>Anyone has idea on this?</p>
","parsing, nlp, stanford-nlp","<p>The parser output can be different depending on whether you run it on a part-of-speech tagged sentence or not.</p>

<p>See the <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#corenlpdiff"" rel=""nofollow"">Parser FAQ</a> for more information.</p>
",1,1,151,2015-04-16 12:13:02,https://stackoverflow.com/questions/29674389/the-inconsistency-between-the-parser-in-corenlp-and-the-standalone-stanford-pars
How can I find grammatical relations of a noun phrase using Stanford Parser or Stanford CoreNLP,"<p>I am using stanford CoreNLP to try to find grammatical relations of noun phrases.</p>

<p>Here is an example: </p>

<p>Given the sentence ""The fitness room was dirty."" </p>

<p>I managed to identify ""The fitness room"" as my target noun phrase. I am now looking for a way to find that the ""dirty"" adjective has a relationship to ""the fitness room"" <strong>and not only to</strong> ""room"".</p>

<p>example code:</p>

<pre><code>private static void doSentenceTest(){
    Properties props = new Properties();
    props.put(""annotators"",""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP stanford = new StanfordCoreNLP(props);

    TregexPattern npPattern = TregexPattern.compile(""@NP"");

    String text = ""The fitness room was dirty."";


    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);
    // run all Annotators on this text
    stanford.annotate(document);

    List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
    for (CoreMap sentence : sentences) {

        Tree sentenceTree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
        TregexMatcher matcher = npPattern.matcher(sentenceTree);

        while (matcher.find()) {
            //this tree should contain ""The fitness room"" 
            Tree nounPhraseTree = matcher.getMatch();
            //Question : how do I find that ""dirty"" has a relationship to the nounPhraseTree


        }

        // Output dependency tree
        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
        GrammaticalStructure gs = gsf.newGrammaticalStructure(sentenceTree);
        Collection&lt;TypedDependency&gt; tdl = gs.typedDependenciesCollapsed();

        System.out.println(""typedDependencies: ""+tdl); 

    }

}
</code></pre>

<p>I used the Stanford CoreNLP on the sentence extracted the root Tree object of it. On this tree object I managed to extract Noun Phrases using a TregexPattern and a TregexMatcher. This gives me a child Tree that contains the actual noun phrase. What I would like to do know is find modifiers of the noun phrase in the original sentence. </p>

<p>The typedDependecies ouptut gives me the following : </p>

<pre><code>typedDependencies: [det(room-3, The-1), nn(room-3, fitness-2), nsubj(dirty-5, room-3), cop(dirty-5, was-4), root(ROOT-0, dirty-5)]
</code></pre>

<p>where I can see <strong>nsubj(dirty-5, room-3)</strong> but I dont have the full noun phrase as dominator.</p>

<p>I hope I am clear enough.
Any help appreciated.</p>
","nlp, stanford-nlp","<p>The typed dependencies <strong>do</strong> show that the adjective 'dirty' applies to 'the fitness room':</p>

<pre><code>det(room-3, The-1)
nn(room-3, fitness-2)
nsubj(dirty-5, room-3)
cop(dirty-5, was-4)
root(ROOT-0, dirty-5)
</code></pre>

<p>the 'nn' tag is the <strong>noun compound modifier</strong>, indicating that 'fitness' is a modifier of 'room'.</p>

<p>You can find detailed information on the dependency tags in the <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""noreferrer"">Stanford dependency manual</a>.</p>
",5,3,2376,2015-04-17 12:42:01,https://stackoverflow.com/questions/29699550/how-can-i-find-grammatical-relations-of-a-noun-phrase-using-stanford-parser-or-s
Stanford Pattern-based Information Extraction,"<p>I really wonder how can I use SPIED (Stanford bootstrapping tool)? Is there any way to have a demo file like what we have for parsers, Ner, .....? The statement in the file is not easy to use... I'm using other tools in my project thanks to demo files...</p>
",stanford-nlp,"<p>Please see the main method of the class GetPatternsFromDataMultiClass. The static run method inside the class is almost like a demo. All you need is a properties file, an example demo properties file is provided with the release. You can also access the example properties <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/data/edu/stanford/nlp/patterns/surface/example.properties"" rel=""nofollow"">here</a>. </p>

<p>You would need to run the class with parameters: "" -props [path-to-properties] ""</p>

<p>SPIED code is different from NER etc. because there is no model released. The code is generic, like CRFs or logistic regression, which you use to train your own model.</p>

<p>An example code to run SPIED is (you can similarly use :</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""true"">
<div class=""snippet-code snippet-currently-hidden"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>GetPatternsFromDataMultiClass&lt;SurfacePattern&gt; model = GetPatternsFromDataMultiClass.&lt;SurfacePattern&gt;run(props);

for(Map.Entry&lt;String, Counter&lt;SurfacePattern&gt;&gt; p : model.getLearnedPatterns().entrySet()){
  System.out.println(""For label "" + p.getKey() + "", the patterns learned are: "");
  for(Map.Entry&lt;SurfacePattern, Double&gt; pat: p.getValue().entrySet()){
    System.out.println(""Pattern "" + pat + "" with score "" + pat.getValue());
  }
  System.out.println(""For label "" + p.getKey() + "", the learned words are:  ""  + model.constVars.getLearnedWords(p.getKey()));
}</code></pre>
</div>
</div>
</p>

<p>For more details on how to use the model for another piece of text, look at explanations of flags loadSavedPatternsWordsDir in the example.properties file.</p>
",2,0,740,2015-04-18 01:55:40,https://stackoverflow.com/questions/29712041/stanford-pattern-based-information-extraction
POS tagger to tag only the start and end of a sentence,"<p>Is it possible to modify the stanford POS tagger so that it only tags the start and end of a sentence and nothing else? </p>

<p>Has this been attempted, or completed, before? Is that code available?</p>

<p>What file needs to be hacked to make this happen?</p>

<p>Is there another tool that provides this functionality relaibly, and relaitively quickly, with good formatting?</p>
","java, stanford-nlp","<p>Not sure if this is still relevant, but I do use the beginPosition() of the first Part Of Speech and the endPosition() of the last part of speech of the current sentence to exactly get the start and end of teh sentence in the input text.</p>

<blockquote>
  <p>Just cast the part of speech to a TaggedWord. You can then easily
  access these properties from the getters , tag() and value().</p>
</blockquote>

<p>Check out this question for more info:</p>

<p><a href=""https://stackoverflow.com/questions/29728347/list-of-part-of-speech-tags-per-sentence-with-pos-tagger-standford-npl-in-c-shar/29733230#29733230"">List of part of speech tags per sentence with POS Tagger Standford NPL in C#</a></p>

<p>Best,
Andrew</p>
",1,0,337,2015-04-19 12:38:34,https://stackoverflow.com/questions/29729961/pos-tagger-to-tag-only-the-start-and-end-of-a-sentence
Stanford NLP: Arabic Part of Speech labels?,"<p>I want to know the meaning of labels the <strong>Arabic</strong> part-of-speech tagger (2015.1.30 version) puts for each word.</p>

<p>like المدرسة/DTNN, I know it differs from the English tagger labels.</p>

<p>I searched their website and documentation but didn't find anything regarding the <strong>Arabic standard labels</strong>.</p>
","arabic, stanford-nlp, pos-tagger","<p>First, I am not sure of this answer; but I hope it will help you.
What you asked for, must be located in the following link : <a href=""http://nlp.stanford.edu/software/parser-arabic-faq.shtml#d"" rel=""nofollow"">POS tag set does the parser use?</a> (but unfortunately there are many broken links!!).<br> 
As they mentioned, you can find the tag set (you called labels) in the following file <a href=""http://catalog.ldc.upenn.edu/docs/LDC2010T13/atb1-v4.1-taglist-conversion-to-PennPOS-forrelease.lisp"" rel=""nofollow"">atb1-v4.1-taglist-conversion-to-PennPOS-forrelease.lisp</a>. As I understand it, they map one or more sequence of English tags to one or more Arabic tag. for example:the mapping: </p>

<blockquote>
  <p>(DET+ADJ_COMP+NSUFF_MASC_DU_GEN DT+JJR)</p>
</blockquote>

<p>means that we map the English sequence of tags (DET+ADJ_COMP+NSUFF_MASC_DU_GEN) to the Arabic tags (DT+JJR). <br> 
Regarding your question (المدرسة/DTNN), they mentioned that it consists of two tags (DT + NN) where DT= (الــ) (pronounced as 'Al')  and NN = (Noun, singular or mass) see <a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow"">Penn Treebank</a> .</p>
",3,3,962,2015-04-19 16:37:59,https://stackoverflow.com/questions/29732675/stanford-nlp-arabic-part-of-speech-labels
Train model using Named entity,"<p>I am looking on standford corenlp using the Named Entity REcognizer.I have different kinds of input text and i need to tag it into my own Entity.So i  started training my own model and it doesnt seems to be working.</p>

<p>For eg: my input text string is ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio <a href=""http://t.co/EqxmY1VmLg"">http://t.co/EqxmY1VmLg</a> <a href=""http://t.co/F0Vefuoj9Q"">http://t.co/F0Vefuoj9Q</a>""</p>

<p>I go through the examples to train my own models and and look for only some words that I am interested in.</p>

<p>My jane-austen-emma-ch1.tsv looks like this</p>

<pre><code>Toyota  PERS
Land Cruiser    PERS
</code></pre>

<p>From the above input text i am only interested in those two words. The one is 
Toyota and the other word is Land Cruiser.</p>

<p>The austin.prop look like this</p>

<pre><code>trainFile = jane-austen-emma-ch1.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Run the following command to generate the ner-model.ser.gz file </p>

<p>java -cp stanford-corenlp-3.4.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop</p>

<pre><code>public static void main(String[] args) {
        String serializedClassifier = ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz"";
        String serializedClassifier2 = ""C:/standford-ner/ner-model.ser.gz"";
        try {
            NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, 
                    serializedClassifier2,serializedClassifier);
            String ss = ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio http://t.co/EqxmY1VmLg http://t.co/F0Vefuoj9Q"";
            System.out.println(""---"");
            List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(ss);
            for (List&lt;CoreLabel&gt; sentence : out) {
              for (CoreLabel word : sentence) {
                System.out.print(word.word() + '/' + word.get(AnswerAnnotation.class) + ' ');
              }
              System.out.println();
            }

        } catch (ClassCastException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }  catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

    }
</code></pre>

<p>Here is the output I am getting</p>

<pre><code>Book/PERS of/PERS 49/O Magazine/PERS Articles/PERS on/O Toyota/PERS Land/PERS Cruiser/PERS 1956-1987/PERS Gold/O Portfolio/PERS http://t.co/EqxmY1VmLg/PERS http://t.co/F0Vefuoj9Q/PERS
</code></pre>

<p>which i think its wrong.I am looking for Toyota/PERS and Land Cruiser/PERS(Which is a multi valued fied.</p>

<p>Thanks for the Help.Any help is really appreciated.</p>
","nlp, stanford-nlp, sentiment-analysis, named-entity-recognition, pos-tagger","<p>The NERClassifier* is word level, that is, it labels words, not phrases. Given that, the classifier seems to be performing fine. If you want, you can hyphenate words that form phrases. So in your labeled examples and in your test examples, you would make ""Land Cruiser"" to ""Land_Cruiser"".</p>
",1,2,2456,2015-04-20 18:43:00,https://stackoverflow.com/questions/29755910/train-model-using-named-entity
stanford coreNLP process many files with a script,"<p><strong>UPDATE</strong></p>

<pre><code>dir=/Users/matthew/Workbench
for f in $dir/Data/NYTimes/NYTimesCorpus_4/*/*/*/*.txt; do
    [[ $f == *.xml ]] &amp;&amp; continue # skip output files
    java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -filelist ""$f"" -outputDirectory .  
done
</code></pre>

<p>this one seems to work better, but I'm getting a <code>io exception file name too long</code> error, what is that about, how to fix it?</p>

<p>I guess the other command in the documentation is disfunctional</p>

<hr>

<p>I was trying to use this script to process my corpus with the Stanford CoreNLP but I keep getting the error </p>

<pre><code>Could not find or load main class .Users.matthew.Workbench.Code.CoreNLP.Stanford-corenlp-full-2015-01-29.edu.stanford.nlp.pipeline.StanfordCoreNLP
</code></pre>

<p>This is the script</p>

<pre><code>dir=/Users/matthew/Workbench
for f in $dir/Data/NYTimes/NYTimesCorpus_3/*/*/*/*.txt; do
    [[ $f == *.xml ]] &amp;&amp; continue # skip output files
    java -mx600m -cp $dir/Code/CoreNLP/stanford-corenlp-full-2015-01-29/stanford-corenlp-VV.jar:stanford-corenlp-VV-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-VV.jar -Xmx2g /Users/matthew/Workbench/Code/CoreNLP/stanford-corenlp-full-2015-01-29/edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit -file ""$f"" -outputDirectory $dir/Data/NYTimes/NYTimesCorpus_3/*/*/*/. 
done
</code></pre>

<p>A very similar one worked for the Stanford NER, that one looked like this:</p>

<pre><code>dir=/Users/matthew/Workbench
for f in $dir/Data/NYTimes/NYTimesCorpus_3/*/*/*/*.txt; do
    [[ $f == *_NER.txt ]] &amp;&amp; continue # skip output files
    g=""${f%.txt}_NER.txt""
    java -mx600m -cp $dir/Code/StanfordNER/stanford-ner-2015-01-30/stanford-ner-3.5.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier $dir/Code/StanfordNER/stanford-ner-2015-01-30/classifiers/english.all.3class.distsim.crf.ser.gz -textFile ""$f"" -outputFormat inlineXML &gt; ""$g""
done
</code></pre>

<p>I can't figure out why I keep getting that error, it seems I've specified all the paths correctly. </p>

<p>I know there's the option <code>-filelist parameter [which] points to a file whose content lists all files to be processed (one per line).</code></p>

<p>but I don't know how exactly that would work in my situation since my directory structure looks like this <code>$dir/Data/NYTimes/NYTimesCorpus_3/*/*/*/*.txt</code> within which there are many files to be processed. </p>

<p>Also is it possible to dynamically specify <code>-outputDirectory</code> they say in the docs <code>You may specify an alternate output directory with the flag</code> but it seems like that would be called once and then be static which would be a nightmare scenario in my case. </p>

<p>I thought maybe I could just write some code to do this, also doesn't work, this is what I tried:</p>

<pre><code>public static void main(String[] args) throws Exception 
{

    BufferedReader br = new BufferedReader(new FileReader(""/home/matthias/Workbench/SUTD/nytimes_corpus/NYTimesCorpus/2005/01/01/1638802_output.txt""));
    try 
    {
        StringBuilder sb = new StringBuilder();
        String line = br.readLine();

        while (line != null) 
        {

            sb.append(line);
            sb.append(System.lineSeparator());
            line = br.readLine();
        }
        String everything = sb.toString();
        //System.out.println(everything);

        Annotation doc = new Annotation(everything);

        StanfordCoreNLP pipeline;

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization,
        // NER, parsing, and coreference resolution
        Properties props = new Properties();

        // configure pipeline
        props.put(
                  ""annotators"", 
                  ""tokenize, ssplit""
                  );

        pipeline = new StanfordCoreNLP(props);

        pipeline.annotate(doc);

        System.out.println( doc );

    }
    finally 
    {
        br.close();
    }

}
</code></pre>
",stanford-nlp,"<p>By far the best way to process a lot of files with Stanford CoreNLP is to arrange to load the system once - since loading all the various models takes 15 seconds or more depending on your computer before any actually document processing is done - and then to process a bunch of files with it. What you have in your update doesn't do that because running CoreNLP is inside the <code>for</code> loop. A good solution is to use the <code>for</code> loop to make a file list and then to run CoreNLP once on the file list. The file list is just a text file with one filename per line, so you can make it any way you want (using a script, editor macro, typing it in yourself), and you can and should check that its contents look correct before running CoreNLP. For your example, based on your update example, the following should work:</p>

<pre><code>dir=/Users/matthew/Workbench
for f in $dir/Data/NYTimes/NYTimesCorpus_3/*/*/*/*.txt; do
    echo $f &gt;&gt; filelist.txt
done
# You can here check that filelist.txt has in it the files you want
java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -filelist filelist
# By default output files are written to the current directory, so you don't need to specify -outputDirectory .
</code></pre>

<p>Other notes on earlier tries:</p>

<ul>
<li><code>-mx600m</code> isn't a reasonable way to run the full CoreNLP pipeline (right through parsing and coref). The sum of all it's models is just too large. <code>-mx2g</code> is fine.</li>
<li>The best way above doesn't fully extend to the NER case. Stanford NER doesn't take a <code>-filelist</code> option, and if you use <code>-textFiles</code> then the files are concatenated and become one output file, which you may well not want. At present, for NER, you may well need to run it inside the <code>for</code> loop, as in your script for that.</li>
<li>I haven't quite decoded how you're getting the error <code>Could not find or load main class .Users.matthew.Workbench.Code.CoreNLP.Stanford-corenlp-full-2015-01-29.edu.stanford.nlp.pipeline.StanfordCoreNLP</code>, but this is happening because you're putting a String (filename?) like that (perhaps with slashes rather than periods) where the <code>java</code> command expects a class name. In that place, there should only be <code>edu.stanford.nlp.pipeline.StanfordCoreNLP</code> as in your updated script or mine.</li>
<li>You can't have a dynamic <code>outputDirectory</code> in one call to CoreNLP. You could get the effect that I think you want reasonably efficiently by making one call to CoreNLP <em>per directory</em> using two nested <code>for</code> loops. The outer <code>for</code> loop would iterate over directories, the inner one make a file list from all the files in that directory, which would then be processed in one call to CoreNLP and written to an appropriate output directory based on the input directory in the outer <code>for</code> loop. Someone with more time or bash-fu than me could try to write that....</li>
<li><p>You can certainly also write your own code to call CoreNLP, but then you're responsible for scanning input directories and writing to appropriate output files yourself. What you have looks basically okay, except the line <code>System.out.println( doc );</code> won't do anything useful - it just prints out the test you began with. You need something like:</p>

<pre><code>PrintWriter xmlOut = new PrintWriter(""outputFileName.xml"");
pipeline.xmlPrint(doc, xmlOut);
</code></pre></li>
</ul>
",4,1,1857,2015-04-21 06:39:06,https://stackoverflow.com/questions/29764703/stanford-corenlp-process-many-files-with-a-script
Stanford Neural Network Dependency Parser -filelist,"<p>I am a newbie in using Stanford DepParser</p>

<p>I want to proceed several sentences with Stanford DepParser.
If there any way instead of <strong>-textFile</strong> to use the list of files like <strong>-filelist</strong>. in CoreNLP</p>

<p>The usual run for command line parameters doesn't give anything</p>

<pre><code>java -cp stanford-parser.jar edu.stanford.nlp.parser.nndep.DependencyParser

EMPTY
</code></pre>
","nlp, stanford-nlp","<p>We don't support this at the moment, unfortunately.</p>

<p>You can just use the <code>depparse</code> model in the CoreNLP pipeline, though, and use the CoreNLP <code>-filelist</code> option as you mention.</p>
",1,0,105,2015-04-22 10:40:56,https://stackoverflow.com/questions/29794998/stanford-neural-network-dependency-parser-filelist
CoreNLP for Original Dependencies with Neural Network Dependency Parsing,"<p>I use Stanford CoreNLP v 3.5.2 in order to get Neural Network Dependency Parsing.</p>

<p>The problem is I don't need Universal Dependencies I need original Dependencies.</p>

<p>Properties file can have <strong>parse.originalDependencies</strong> attribute, however there two questions</p>

<p>NN DepParsing is <strong>depparse</strong> option, and the attributes of <strong>depparse</strong> are not described in <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>

<p>I am not sure that <strong>depparse.originalDependencies</strong> exists.</p>

<p>In addition, how exactly should I define it in properties file</p>

<pre><code>depparse.originalDependencies = 1
or 
depparse.originalDependencies = true
</code></pre>

<p>Thank you</p>
","nlp, stanford-nlp","<p>You need to specify a different parsing model using the <code>-depparse.model</code> option in your CoreNLP invocation. (Note that if you are accessing the parser directly through the <code>edu.stanford.nlp.parser.nndep.DependencyParser</code> class, this option is just called <code>-model</code>.)</p>

<p>Use the following <code>-depparse.model</code> option. This refers to a non-Universal Dependencies model that is distributed with the parser.</p>

<pre><code>-depparse.model ""edu/stanford/nlp/models/parser/nndep/english_SD.gz""
</code></pre>
",1,0,395,2015-04-22 14:46:23,https://stackoverflow.com/questions/29801204/corenlp-for-original-dependencies-with-neural-network-dependency-parsing
How to NER and POS tag a pre-tokenized text with Stanford CoreNLP?,"<p>I'm using the Stanford's CoreNLP Named Entity Recognizer (NER) and Part-of-Speech (POS) tagger in my application. The problem is that my code tokenizes the text beforehand and then I need to NER and POS tag each token. However I was only able to find out how to do that using the command line options but not programmatically.</p>

<p>Can someone please tell me how programmatically can I NER and POS tag pretokenized text using Stanford's CoreNLP?</p>

<p>Edit: </p>

<p>I'm actually using the individual NER and POS instructions. So my code was written as instructed in the tutorials given in the Stanford's NER and POS packages. But I have CoreNLP in my classpath. So I have the CoreNLP in my classpath but using the tutorials in the NER and POS packages. </p>

<p>Edit:</p>

<p>I just found that there are instructions as how one can set the properties for CoreNLP here <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml</a> but I wish if there was a quick way to do what I want with Stanford NER and POS taggers so I don't have to recode everything!</p>
","nlp, stanford-nlp, named-entity-recognition, pos-tagger","<p>If you set the property:</p>

<pre><code>tokenize.whitespace = true
</code></pre>

<p>then the CoreNLP pipeline will tokenize on whitespace rather than the default PTB tokenization. You may also want to set:</p>

<pre><code>ssplit.eolonly = true
</code></pre>

<p>so that you only split sentences on newline characters.</p>
",5,5,1712,2015-04-22 19:33:27,https://stackoverflow.com/questions/29807175/how-to-ner-and-pos-tag-a-pre-tokenized-text-with-stanford-corenlp
where I can find the file sentimentTreesDebug.txt,"<p>I am trying to analyze codes sentiment model anaysis of stanford corenlp, I have found on this page:</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sentiment/SentimentTraining.java"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sentiment/SentimentTraining.java</a></p>

<p>but I can not find the ""sentimentTreesDebug.txt"" file with which to make the experiment, someone could tell me where that file or how it should look?</p>

<p>TrainPath String = ""sentimentTreesDebug.txt"";</p>

<p>I would appreciate very much the help or advice you could give me</p>

<p>this is my problem</p>

<p>xception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: MemoryTreebank.processFile IOException in file sentimentTreesDebug.txt
    at edu.stanford.nlp.trees.MemoryTreebank.processFile(MemoryTreebank.java:300)
    at edu.stanford.nlp.util.FilePathProcessor.processPath(FilePathProcessor.java:84)
    at edu.stanford.nlp.trees.MemoryTreebank.loadPath(MemoryTreebank.java:152)
    at edu.stanford.nlp.trees.Treebank.loadPath(Treebank.java:193)
    at edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithLabels(SentimentUtils.java:67)
    at edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithGoldLabels(SentimentUtils.java:50)
    at edu.stanford.nlp.sentiment.SentimentTraining.main(SentimentTraining.java:170)
Caused by: java.io.FileNotFoundException: sentimentTreesDebug.txt (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.(FileInputStream.java:138)
    at edu.stanford.nlp.trees.MemoryTreebank.processFile(MemoryTreebank.java:212)
    ... 6 more
Java Result: 1</p>
","stanford-nlp, sentiment-analysis","<p>You can download them here:</p>

<p><a href=""http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip</a></p>

<p>A simple dev.txt would look like:</p>

<pre><code>(4 (2 (2 Everything) (2 is)) (4 awesome))
</code></pre>

<p>HTH</p>
",0,1,131,2015-04-23 16:29:47,https://stackoverflow.com/questions/29829380/where-i-can-find-the-file-sentimenttreesdebug-txt
Using other language models with Stanford Core NLP,"<p>I want to use a Dutch model for named entity recognition with <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Core NLP</a>.</p>

<p>I have found a pre-trained model <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">from OpenNLP</a>, but it doesn't seem to be interoperable with CoreNLP.</p>

<p>Why is that? Can we still use Core NLP with other languages than English, Chinese and Spanish?</p>
",stanford-nlp,"<p>CoreNLP currently does not support Dutch. There are some components which work for German and Arabic, but the pipeline is currently only for English, Chinese and Spanish. You can retrain our <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">NER model</a> on the same conllx data, but we have not done this.</p>

<p>The OpenNLP models are not compatible with CoreNLP.</p>
",5,1,1322,2015-04-23 19:20:42,https://stackoverflow.com/questions/29832506/using-other-language-models-with-stanford-core-nlp
How to integrate the GATE Twitter PoS model with Stanford NLP?,"<p>I'm currently using Stanford NLP library for sentiment analysis of a twitter stream (version 3.3.0 but that’s not set.)</p>

<p>I was looking for ways to increase the accuracy when I came across this 
<a href=""https://gate.ac.uk/wiki/twitter-postagger.html"" rel=""nofollow"">https://gate.ac.uk/wiki/twitter-postagger.html</a></p>

<p>I'm relatively new to sentiment analysis but am I right in saying that if I choose this model instead of the default model (which is based off film reviews) I would get an increased accuracy rating ?</p>

<p>If so, how does one go about integrating it with the Stanford NPL library ?</p>

<p>If I am missing any required information here please tell me ! </p>

<p>Regards.</p>
","twitter, machine-learning, stanford-nlp, sentiment-analysis","<p>You can use GATE twitter pos model with stanford package</p>

<pre><code>./corenlp.sh -file tweets.txt -pos.model gate-EN-twitter.model -ssplit.newlineIsSentenceBreak always
</code></pre>

<p>use <code>v3.3.1</code> for GATE</p>
",1,1,394,2015-04-23 22:01:09,https://stackoverflow.com/questions/29835122/how-to-integrate-the-gate-twitter-pos-model-with-stanford-nlp
"Stanford coreNLP : how to get Label, position, and typed dependecies from parse Tree","<p>I am using Stanford coreNLP to parse some text. I get multiple sentences. On these sentences I managed to extract Noun Phrases using TregexPattern. So I get a child Tree that is my Noun Phrase. I also managed to figure out the Head of the noun phrase. </p>

<p>How is it possible to get the position or even the token/coreLabel of that Head in the sentence? </p>

<p>Even better, how is it possible to find the dependency relationships of the Head to the rest of the sentence?</p>

<p>Here's an example : </p>

<pre><code>public void doSomeTextKarate(String text){

    Properties props = new Properties();
    props.put(""annotators"",""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    this.pipeline = pipeline;


    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);
    // run all Annotators on this text
    pipeline.annotate(document);

    List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

    for (CoreMap sentence : sentences) {


        SemanticGraph basicDeps = sentence.get(BasicDependenciesAnnotation.class);
        Collection&lt;TypedDependency&gt; typedDeps = basicDeps.typedDependencies();
        System.out.println(""typedDeps ==&gt;  ""+typedDeps);

        SemanticGraph collDeps = sentence.get(CollapsedDependenciesAnnotation.class);
        SemanticGraph collCCDeps = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);

        List&lt;CoreMap&gt; numerizedTokens = sentence.get(NumerizedTokensAnnotation.class);
        List&lt;CoreLabel&gt; tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);

        Tree sentenceTree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);

        sentenceTree.percolateHeads(headFinder);
        Set&lt;Dependency&lt;Label, Label, Object&gt; &gt; sentenceDeps =   sentenceTree.dependencies();
        for (Dependency&lt;Label, Label, Object&gt; dependency : sentenceDeps) {
            System.out.println(""sentence dep = "" + dependency);

            System.out.println(dependency.getClass() +"" ( "" + dependency.governor() + "", "" + dependency.dependent() +"") "" );
        }


        //find nounPhrases in setence
        TregexPattern pat = TregexPattern.compile(""@NP"");
        TregexMatcher matcher = pat.matcher(sentenceTree);
        while (matcher.find()) {

            Tree nounPhraseTree = matcher.getMatch();
            System.out.println(""Found noun phrase "" + nounPhraseTree);

            nounPhraseTree.percolateHeads(headFinder);

            Set&lt;Dependency&lt;Label, Label, Object&gt; &gt; npDeps = nounPhraseTree.dependencies();
            for (Dependency&lt;Label, Label, Object&gt; dependency : npDeps ) {
                System.out.println(""nounPhraseTree  dep = "" + dependency);
            }


            Tree head = nounPhraseTree.headTerminal(headFinder);
            System.out.println(""head "" + head);


            Set&lt;Dependency&lt;Label, Label, Object&gt; &gt; headDeps = head.dependencies();
            for (Dependency&lt;Label, Label, Object&gt; dependency : headDeps) {
                System.out.println(""head dep "" + dependency);
            }


            //QUESTION : 
            //How do I get the position of ""head"" in tokens or numerizedTokens ?
            //How do I get the dependencies where ""head"" is involved in typedDeps ? 

        }
    }
}
</code></pre>

<p>In other words I would like to query for ALL dependency relationships where the ""head"" word/token/label is involved in the ENTIRE sentence. So I thought I needed to figure out the position of that token in the sentence to correlate it with the typed dependencies but mybe there is some easier way ? </p>

<p>Thanks in advance.</p>

<p>[EDIT]</p>

<p>So I might have found an answer or the beginning of it. </p>

<p>If I call .label() on head I get myself a CoreLabel which is pretty much what I needed to find the rest. I can now iterate over the typed dependencies and search for dependencies where either the dominator label or dependent label has the same index as my headLabel.  </p>

<pre><code>            Tree nounPhraseTree = matcher.getMatch();
            System.out.println(""Found noun phrase "" + nounPhraseTree);

            nounPhraseTree.percolateHeads(headFinder);
            Tree head = nounPhraseTree.headTerminal(headFinder);
            CoreLabel headLabel = (CoreLabel) head.label();

            System.out.println(""tokens.contains(headLabel)"" + tokens.contains(headLabel));

            System.out.println("""");
            System.out.println(""Iterating over typed deps"");
            for (TypedDependency typedDependency : typedDeps) {
                System.out.println(typedDependency.gov().backingLabel());
                System.out.println(""gov pos ""+ typedDependency.gov() + "" - "" + typedDependency.gov().index());
                System.out.println(""dep pos ""+ typedDependency.dep() + "" - "" + typedDependency.dep().index());

                if(typedDependency.gov().index() == headLabel.index() ){

                    System.out.println(""dep or gov backing label equals headlabel :"" + (typedDependency.gov().backingLabel().equals(headLabel) ||
                            typedDependency.dep().backingLabel().equals(headLabel)));  //why does this return false all the time ? 


                    System.out.println("" !!!!!!!!!!!!!!!!!!!!!  HIT ON "" + headLabel + "" == "" + typedDependency.gov());
                }
            }
</code></pre>

<p>So it seems I can only match my head's Label with the one from the typedDeps using the index. I wonder if this the propper way to do this. 
As you can see in my code I also tried to use TypedDependency.backingLabel() to test equality with my headLabel either with the governor or the dependent but it systematically returns false. I wonder why !? </p>

<p>Any feedback appreciated.               </p>
",stanford-nlp,"<p>You can get the position of a CoreLabel within its containing sentence with the <code>CoreAnnotations.IndexAnnotation</code> annotation.</p>

<p>Your method for finding all dependents of a given word seems correct, and is probably the easiest way to do it.</p>
",2,4,2983,2015-04-24 10:09:38,https://stackoverflow.com/questions/29844719/stanford-corenlp-how-to-get-label-position-and-typed-dependecies-from-parse
Does Stanford Core NLP support lemmatization for German?,"<p>I found German parse and pos-tag models which are compatible with Stanford Core NLP. However I was not able to get German lemmatization working. Is there a way to do so?</p>
",stanford-nlp,"<p>Since the version 3.6 is also German supported.
Check it under <a href=""http://stanfordnlp.github.io/CoreNLP/history.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/history.html</a></p>
",0,3,1432,2015-04-25 06:39:47,https://stackoverflow.com/questions/29861925/does-stanford-core-nlp-support-lemmatization-for-german
Stanford Core NLP example code SemanticGraph exception,"<p>I have just tried out the Core NLP example code, included as <em>StanfordCoreNlpDemo.java</em> with the download. When trying to parse a chapter of a book, an exception is thrown from the Semantic Graph:</p>

<pre><code>Exception in thread ""main"" java.lang.NullPointerException
at edu.stanford.nlp.semgraph.SemanticGraph.removeEdge(SemanticGraph.java:122)
at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.expandPPConjunction(UniversalEnglishGrammaticalStructure.java:553)
at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.expandPPConjunctions(UniversalEnglishGrammaticalStructure.java:508)
at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.collapseDependencies(UniversalEnglishGrammaticalStructure.java:807)
at edu.stanford.nlp.trees.GrammaticalStructure.typedDependenciesCollapsed(GrammaticalStructure.java:877)
at edu.stanford.nlp.semgraph.SemanticGraphFactory.makeFromTree(SemanticGraphFactory.java:188)
at edu.stanford.nlp.semgraph.SemanticGraphFactory.generateCollapsedDependencies(SemanticGraphFactory.java:90)
at edu.stanford.nlp.pipeline.ParserAnnotatorUtils.fillInParseAnnotations(ParserAnnotatorUtils.java:51)
at edu.stanford.nlp.pipeline.ParserAnnotator.finishSentence(ParserAnnotator.java:266)
at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:245)
at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:96)
at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:68)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:412)
at epubReader.Test.main(Test.java:68)
</code></pre>

<p>I have narrowed it down to one sentence being responsible, even when parsed alone:</p>

<pre><code>""Generally speaking, he was a scout, and rarely stood at a watch nearer than the four hundred and fiftieth metre, and then only as a cordon commander.""
</code></pre>

<p>Is there anything special about the syntax or semantics of the sentence that I am missing, causing the error?</p>
",stanford-nlp,"<p>No, the sentence is fine and this is unfortunately a bug in our dependency converter. </p>

<p>The part-of-speech tagger outputs a really weird POS sequence causing the parser to produce a completely wrong parse tree which leads to this exception in the constituency-to-dependency converter. </p>

<p>I fixed the bug in the converter but unless you clone the code from <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">GitHub</a> and compile it yourself, this won't help you until the next release.</p>

<p>But you can still get a parse for this sentence either by disabling the POS tagger (see the <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#corenlpdiff"" rel=""nofollow"">parser FAQ</a> for details on how to do this) or by using the <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">neural network dependency parser</a>.</p>
",2,1,451,2015-04-26 13:03:26,https://stackoverflow.com/questions/29877985/stanford-core-nlp-example-code-semanticgraph-exception
Sentiment Analysis in Spanish with Stanford coreNLP,"<p>I'm new here and wanted to know if anyone can help me with the following question.</p>

<p>I'm doing sentiment analysis of text in Spanish and using Stanford CoreNLP but I can not get a positive result.</p>

<p>That is, if I analyze any English text analyzes it perfect to put it in Spanish but the result is always negative</p>

<p>I've been looking how to configure the parser in Spanish, tokenize and everything I found was useless for sentiment analysis.</p>

<p>Someone can tell me if the only thing that works is the tokenize and sentiment does not in Spanish?</p>

<p>This is my properties file so that I managed to find:</p>

<p>annotators = tokenize, ssplit, pos, ner, parse, sentiment</p>

<p>tokenize.language = en</p>

<p>pos.model = edu / stanford / nlp / models / pos-tagger / english / spanish-distsim.tagger</p>

<p>ner.model = edu / stanford / nlp / models / ner / spanish.ancora.distsim.s512.crf.ser.gz
ner.applyNumericClassifiers = false
ner.useSUTime = false</p>

<p>parse.model = edu / stanford / nlp / models / lexparser / spanishPCFG.ser.gz</p>

<p>The code to perform sentiment analysis is typical that you can find in any tutorial</p>

<p>Thank you very much!!</p>
","stanford-nlp, sentiment-analysis","<p>Unfortunately there is no Stanford sentiment model available for Spanish. At the moment all the Spanish words are likely being treated as generic ""unknown words"" by the sentiment analysis algorithm, which is why you're seeing consistently bad performance.</p>

<p>You can certainly train your own model (documented elsewhere on the Internet, I believe..), but you'll need to have Spanish training data to accomplish this.</p>
",0,1,1256,2015-04-27 10:11:18,https://stackoverflow.com/questions/29892848/sentiment-analysis-in-spanish-with-stanford-corenlp
CoreNLP API for N-grams?,"<p>Does CoreNLP have an API for getting unigrams, bigrams, trigrams, etc.?</p>

<p>For example, I have a string <code>""I have the best car ""</code>.  I would love to get:</p>

<pre><code>I
I have
the
the best
car
</code></pre>

<p>based on the string I am passing.</p>
","nlp, stanford-nlp, n-gram, pos-tagger","<p>If you are coding in Java, check out getNgrams* functions in the StringUtils class in CoreNLP.</p>

<p>You can also use CollectionUtils.getNgrams (which is what StringUtils class uses too)</p>
",3,0,2443,2015-04-27 20:54:20,https://stackoverflow.com/questions/29905761/corenlp-api-for-n-grams
Efficient batch processing with Stanford CoreNLP,"<p><strong>Is it possible to speed up batch processing of documents with CoreNLP from command line so that models load only one time? I would like to trim any unnecessarily repeated steps from the process.</strong></p>

<p>I have 320,000 text files and I am trying to process them with CoreNLP. The desired result is 320,000 finished XML file results. </p>

<p>To get from one text file to one XML file, I use the CoreNLP jar file from command line:</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props config.properties 
-file %%~f -outputDirectory MyOutput -outputExtension .xml -replaceExtension`
</code></pre>

<p>This loads models and does a variety of machine learning magic. The problem I face is when I try to loop for every text in a directory, I create a process that by my estimation will complete in 44 days. I literally have had a command prompt looping on my desktop for the last 7 days and I'm nowhere near finished. The loop I run from batch script:</p>

<pre><code>for %%f in (Data\*.txt) do (
    java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props config.properties
    -file %%~f -outputDirectory Output -outputExtension .xml -replaceExtension
)
</code></pre>

<p>I am using these annotators, specified in config.properties:
<br>
<code>annotators = tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment</code></p>
","batch-file, stanford-nlp","<p>I know nothing about Stanford CoreNLP, so I googled for it (you didn't included any link) and in <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">this page</a> I found this description (below ""Parsing a file and saving the output as XML""):</p>

<blockquote>
  <p>If you want to process a list of files use the following command line:</p>
  
  <p>java -cp
  stanford-corenlp-VV.jar:stanford-corenlp-VV-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-VV.jar
  -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP [ -props YOUR CONFIGURATION FILE ] -filelist A FILE CONTAINING YOUR LIST OF FILES</p>
  
  <p>where the -filelist parameter points to a file whose content lists all
  files to be processed (one per line).</p>
</blockquote>

<p>So I guess that you may process your files faster if you store a list of all your text files in a list file:</p>

<pre><code>dir /B *.txt &gt; list.lst
</code></pre>

<p>... and then pass that list in the <code>-filelist list.lst</code> parameter in a single execution of Stanford CoreNLP.</p>
",3,2,1348,2015-04-28 14:04:28,https://stackoverflow.com/questions/29921812/efficient-batch-processing-with-stanford-corenlp
Why POS tagging algorithm tags `can&#39;t` as separate words?,"<p>I'm using <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford Log-linear Part-Of-Speech Tagger</a> and here is the sample sentence that I tag:</p>

<blockquote>
  <p>He can't do that</p>
</blockquote>

<p>When tagged I get this result:</p>

<blockquote>
  <p>He_PRP ca_MD n't_RB do_VB that_DT</p>
</blockquote>

<p>As you can see, <code>can't</code> is split into two words, <code>ca</code> is marked as Modal (MD) and <code>n't</code> is marked as ADVERB (RB)?</p>

<p>I actually get the same result if I use <code>can not</code> separately: <code>can</code> is MD and <code>not</code> is RB, so is this way of breaking up is expected instead of say breaking like <code>can_MD</code> and <code>'t_RB</code>?</p>
","stanford-nlp, pos-tagger","<p>Note: This is not the perfect answer.<br>
I think that the problem originates from the Tokenizer used in Stanford POS Tagger, not from the tagger itself. the Tokenizer (PTBTokenizer) can not handle apostrophe properly:<br>
1- <a href=""https://stackoverflow.com/q/29229780/992406"">Stanford PTBTokenizer token's split delimiter</a>.
 <br>2-  <a href=""https://stackoverflow.com/q/14058399/992406"">Stanford coreNLP - split words ignoring apostrophe</a>.<br>
As they mentioned here <a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow noreferrer"">Stanford Tokenizer</a>, the PTBTokenizer  will tokenizes the sentence : </p>

<blockquote>
  <p>""Oh, no,"" she's saying, ""our $400 blender can't handle something this
  hard!""</p>
</blockquote>

<p>to:</p>

<blockquote>
  <p>...... <br> our<br> $ <br>400 <br>blender<br> <strong>ca</strong><br> <strong>n't</strong><br> handle<br> something</p>
</blockquote>

<p>Try to find a suitable tokenization method and apply it to the tagger as following: </p>

<pre><code>    import java.util.List;
    import edu.stanford.nlp.ling.HasWord;
    import edu.stanford.nlp.ling.Sentence;
    import edu.stanford.nlp.ling.TaggedWord;
    import edu.stanford.nlp.tagger.maxent.MaxentTagger;

    public class Test {

        public static void main(String[] args) throws Exception {
            String model = ""F:/code/stanford-postagger-2015-04-20/models/english-left3words-distsim.tagger"";  
            MaxentTagger tagger = new MaxentTagger(model);
            List&lt;HasWord&gt; sent;
            sent = Sentence.toWordList(""He"", ""can"", ""'t"", ""do"", ""that"", ""."");
            //sent = Sentence.toWordList(""He"", ""can't"", ""do"", ""that"", ""."");
            List&lt;TaggedWord&gt; taggedSent = tagger.tagSentence(sent);
            for (TaggedWord tw : taggedSent) {
                 System.out.print(tw.word() + ""="" +  tw.tag() + "" , "" );

            }

        }

}
</code></pre>

<p>output:<br></p>

<blockquote>
  <p>He=PRP , can=MD , 't=VB , do=VB , that=DT , .=. ,</p>
</blockquote>
",1,0,678,2015-04-30 19:44:30,https://stackoverflow.com/questions/29976457/why-pos-tagging-algorithm-tags-cant-as-separate-words
using Wordnet with stanford nlp,"<p>I'm trying to get WordnetSynAnnotation of a token, it always returns null.</p>

<p>I'm not sure what I'm missing, is there an annotator for WordnetSynAnnotation </p>
","java, stanford-nlp, wordnet","<p>This does not seem to be set by any annotator in the code currently. A code search suggests that it was used in the NER system at one point, but is no longer set by anything.</p>
",1,3,631,2015-05-01 11:38:30,https://stackoverflow.com/questions/29986167/using-wordnet-with-stanford-nlp
How should I figure out the POS tag of &quot;last&quot; in this sentence?,"<p>I'm trying to use Stanford CoreNLP to tag a sentence.</p>

<p>""How long does a soccer game last?""</p>

<p>It seems on CoreNLP demo the token ""last"" is tagged as JJ instead of VB. Is there a way to fix this?</p>
",stanford-nlp,"<p>The short answer is ""no."" CoreNLP provides part of speech tags at a certain high but not perfect accuracy, and it will occasionally make mistakes. Beyond tweaking the tags yourself, there's no easy automatic way to have its accuracy go up.</p>

<p>The longer answer is that you can always re-train the POS tagger on a custom tagged corpus, and then performance will be better on that corpus. This, however, involves a fairly substantial annotation effort tagging a large corpus of text with part of speech tags.</p>
",1,0,32,2015-05-01 20:17:11,https://stackoverflow.com/questions/29994472/how-should-i-figure-out-the-pos-tag-of-last-in-this-sentence
java sentence splitting error,"<p>I want to split sentences from a paragraph using <code>java</code> language. Consider the following sentence. </p>

<blockquote>
  <p>we decided to go to u.s.a, canada,africa etc... from our office. I
  have only rs.1 lakh. So i called my dad and asked some money. he said
  ""No.I wont"" and disconnected the phone</p>
</blockquote>

<p>.</p>

<p>I used <code>stanford tokenizer</code>. Eventhough the sentence <code>we decided to go to u.s.a, canada,africa etc... from our office</code> is a single sentence, output showing that  </p>

<blockquote>
  <p>we decided to go to u.s.a, canada,africa etc...</p>
</blockquote>

<p>is one sentence and </p>

<blockquote>
  <p>from our office</p>
</blockquote>

<p>is another sentence. Rest of the sentences are correctly splitting.</p>

<p>Please note : if the word is <code>etc.,</code> instead of <code>etc...</code> it is working correctly.</p>

<p>Is it possible to tell the program that the words followings <code>etc...</code> are the continuation of the same sentence ? 
I tried some other sentence spliting tools also. But the result is same. Please help.</p>
","java, regex, artificial-intelligence, stanford-nlp, text-segmentation","<p>Use the replace function to replace ... With something unique. *+&amp;1 for example. Then split the string and then replace the unique part with ... Again. </p>
",1,-1,91,2015-05-02 10:22:42,https://stackoverflow.com/questions/30001116/java-sentence-splitting-error
Text tokenization with Stanford NLP : Filter unrequired words and characters,"<p>I use <code>Stanford NLP</code> for string tokenization in my classification tool. I want to get only meaningful words, but I get non-word tokens (like <code>---</code>, <code>&gt;</code>, <code>.</code> etc.) and not important words like <code>am</code>, <code>is</code>, <code>to</code> (stop words). Does anybody know a way to solve this problem?</p>
","java, machine-learning, tokenize, stanford-nlp","<p>This is a very domain-specific task that we don't perform for you in CoreNLP. You should be able to make this work with a regular expression filter and a <a href=""http://en.wikipedia.org/wiki/Stop_words"" rel=""noreferrer"">stopword</a> filter on top of the CoreNLP tokenizer.</p>

<p>Here's <a href=""http://www.ranks.nl/stopwords"" rel=""noreferrer"">an example list of English stopwords</a>.</p>
",5,8,8474,2015-05-03 20:30:26,https://stackoverflow.com/questions/30019054/text-tokenization-with-stanford-nlp-filter-unrequired-words-and-characters
Error when using StanfordCoreNLP,"<p>I'm trying to use Stanford CoreNLP as a library in my java program. I use IntelliJ as the IDE.
I was trying to test the library, so I wrote this code:</p>

<pre><code>import edu.stanford.nlp.pipeline.StanfordCoreNLP;

import java.util.Properties;

/**
 * Created by Benjamin on 15/5/4.
 */
public class SentimentAnaTest {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    }
}
</code></pre>

<p>and it shows the error like this:</p>

<pre><code>Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator pos
Exception in thread ""main"" java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.pipeline.AnnotatorFactories$4.create(AnnotatorFactories.java:292)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:289)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:126)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:122)
    at SentimentAnaTest.main(SentimentAnaTest.java:12)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
Caused by: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:770)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:298)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:263)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:97)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:77)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.posTagger(AnnotatorImplementations.java:59)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$4.create(AnnotatorFactories.java:290)
    ... 10 more
Caused by: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:481)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:765)
    ... 16 more
</code></pre>

<p>I read a solution <a href=""https://stackoverflow.com/questions/22206095/error-in-creating-the-stanfordcorenlp-object"">here</a> but couldn't figure out what the problem was, because I added the library from the Maven Central Repository which already included the ""stanford-corenlp-3.5.2-models.jar"".</p>

<p>From the error message it seemed like the program was trying to load a file from <code>edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger</code> so I download the tagger file from <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow noreferrer"">here</a> and put it into <code>/edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger</code>. However it still didn't work.</p>

<p>Can someone tell me whats this error is about and help me to solve it? Thank you!</p>
","java, stanford-nlp","<p>The tagger file has to be placed into your project root.</p>

<pre><code>project
-- src --&gt; SentimentAnaTest
-- english-left3words/english-left3words-distsim.tagger
</code></pre>

<p>Tested in Eclipse project.</p>
",3,5,5075,2015-05-05 04:58:08,https://stackoverflow.com/questions/30044579/error-when-using-stanfordcorenlp
java.lang.NullPointerException while doing sentimental analysis with stanford-nlp API,"<p>I am new to stanford-nlp API. I am trying to just sentimental analysis with stanford API but it's throwing exception. please see the below logs.</p>

<pre><code>Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.4 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [5.3 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.3 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [4.7 sec].
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1.1 sec].
Adding annotator dcoref
Adding annotator sentiment
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/ejml/simple/SimpleBase
    at edu.stanford.nlp.pipeline.SentimentAnnotator.&lt;init&gt;    (SentimentAnnotator.java:48)
    at  edu.stanford.nlp.pipeline.StanfordCoreNLP$14.create(StanfordCoreNLP.java:850)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:262)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:129)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:125)
    at io.stanford.NLP.findSentiment(NLP.java:30)
    at io.stanford.TestStanford.main(TestStanford.java:8)
Caused by: java.lang.ClassNotFoundException: org.ejml.simple.SimpleBase
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    ... 8 more
</code></pre>
",stanford-nlp,"<p>What is the code that produces this output? My strong suspicion is that you have not included the ""sentiment"" annotator in your annotators list, either in the properties file you are using to run the code, or the properties object you have passed into the annotation pipeline. Without running the sentiment annotator, the document will not have the sentiment annotations attached, and will therefore null pointer when trying to retrieve them.</p>
",1,0,605,2015-05-05 11:41:42,https://stackoverflow.com/questions/30052006/java-lang-nullpointerexception-while-doing-sentimental-analysis-with-stanford-nl
Getting locations in stanford core nlp,"<p>I am using <code>stanford-corenlp-3.2.0.jar</code> and <code>stanford-corenlp-3.2.0-models.jar</code> for identifying locations in a particular sentence. However I have observed that <code>stanford-nlp</code> is not able to identify the location if the word is passed in small case. </p>

<p>For example: <code>""Find a restaurant in London""</code>. Here stanford will identify London as location. </p>

<p>However if the following sentence is passed like: ""Find a restaurant in <strong>london</strong>"", then stanford is unable to identify <code>london</code> as a location. </p>

<p>To resolve this, I am converting first letter of every word in a sentence to capital. However, I am getting other issues if I do this.</p>

<p>Based on answer provided by meskobalazs,
I have downloaded the jar: <strong><code>stanford-corenlp-caseless-2015-04-20-models.jar</code></strong>.</p>

<p>I replaced with the earlier jar: <strong><code>stanford-corenlp-3.2.0-models</code></strong>.</p>

<p>However now I am getting the below exception</p>

<pre><code>SEVERE: Exception sending context initialized event to listener instance of class servlets.NLP_initializer
java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:493)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:260)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:127)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:123)
    at servlets.NLP_initializer.contextInitialized(NLP_initializer.java:34)
    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4887)
    at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5381)
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1559)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1549)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:749)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:283)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:247)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:78)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:62)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:491)
    ... 14 more
Caused by: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:419)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:744)
    ... 19 more
</code></pre>

<p>The place, where I am initializing, while the server starts up is</p>

<pre><code>  public static edu.stanford.nlp.pipeline.StanfordCoreNLP snlp;
    /**
     * @see ServletContextListener#contextInitialized(ServletContextEvent)
     */
    public void contextInitialized(ServletContextEvent arg0) {
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize,ssplit,pos,lemma,parse,ner,dcoref"");
        StanfordCoreNLP snlp = new StanfordCoreNLP(props);
    }
</code></pre>
","java, stanford-nlp","<p>From what I see <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">here</a>, you should try out models which ignore the capitalization of words. You just need to add this models jar file to the existing one: <a href=""http://nlp.stanford.edu/software/stanford-corenlp-caseless-2015-04-20-models.jar"" rel=""nofollow"">caseless models</a>.</p>

<p>For future reference: the jar link may be broken, but the first link goes to the page, where a link for the up to date jar can be found. </p>
",4,5,939,2015-05-07 06:12:58,https://stackoverflow.com/questions/30093222/getting-locations-in-stanford-core-nlp
configuring a separate model jar in stanford nlp,"<p>I have implemented a logic to use stanford nlp to get the location from the particular english sentence. I was using the following jar
stanford-corenlp-3.2.0.jar 
stanford-corenlp-3.2.0-models.jar </p>

<p>The logic that I wrote is following</p>

<pre><code> public static edu.stanford.nlp.pipeline.StanfordCoreNLP snlp;
    /**
     * @see ServletContextListener#contextInitialized(ServletContextEvent)
     */
    public void contextInitialized(ServletContextEvent arg0) {
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize,ssplit,pos,lemma,parse,ner,dcoref"");
        StanfordCoreNLP snlp = new StanfordCoreNLP(props);
    }
</code></pre>

<p>However because of the case sensitive issue, I was adviced to use 
stanford-corenlp-caseless-2015-04-20-models.jar instead of stanford-corenlp-3.2.0.jar. 
From the above code the jar which will be loaded by default is stanford-corenlp-3.2.0-models.jar.</p>

<p>However I want to now configure with the following model i.e. stanford-corenlp-caseless-2015-04-20-models.jar
Please guide me on how to configure it using java code.</p>

<p>I tried the Gabor's solution. However I got the following exception</p>

<pre><code>SEVERE: Exception sending context initialized event to listener instance of class servlets.NLP_initializer
java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:493)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:260)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:127)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:123)
    at servlets.NLP_initializer.contextInitialized(NLP_initializer.java:34)
    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4887)
    at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5381)
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1559)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1549)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:749)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:283)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:247)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:78)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:62)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:491)
    ... 14 more
Caused by: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:419)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:744)
    ... 19 more
</code></pre>
","java, stanford-nlp","<p>See <a href=""http://nlp.stanford.edu/software/corenlp.shtml#caseless"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml#caseless</a></p>

<p>Copying from the documentation:</p>

<blockquote>
  <p>It is possible to run StanfordCoreNLP with tagger, parser, and NER models that ignore capitalization. In order to do this, download the caseless models package. Be sure to include the path to the case insensitive models jar in the -cp classpath flag as well. Then, set properties which point to these models as follows: </p>
  
  <p>-pos.model edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger </p>
  
  <p>-parse.model edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz </p>
  
  <p>-ner.model edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz 
     edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz 
     edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz</p>
</blockquote>

<p>In your code, these paths can be set with:</p>

<pre><code>    props.put(""pos.model"", ""edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger"");
    props.put(""parse.model"", ""edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz"");
    props.put(""ner.model"", ""edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz"");
</code></pre>
",2,0,1722,2015-05-07 16:10:36,https://stackoverflow.com/questions/30106440/configuring-a-separate-model-jar-in-stanford-nlp
How to suppress unmatched words in Stanford NER classifiers?,"<p>I am new to Stanford NLP and NER and trying to train a custom classifier with a data sets of currencies and countries.</p>

<p>My training data in training-data-currency.tsv looks like -</p>

<pre><code>USD CURRENCY
GBP CURRENCY
</code></pre>

<p>And, training data in training-data-countries.tsv looks like -</p>

<pre><code>USA COUNTRY
UK  COUNTRY
</code></pre>

<p>And, classifiers properties look like -</p>

<pre><code>trainFileList = classifiers/training-data-currency.tsv,classifiers/training-data-countries.tsv
ner.model=classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz,classifiers/english.all.3class.distsim.crf.ser.gz
serializeTo = classifiers/my-classification-model.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
#no ngrams will be included that do not contain either the
#beginning or end of the word
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
#the next 4 deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Java code to find the categories is -</p>

<pre><code>LinkedHashMap&lt;String, LinkedHashSet&lt;String&gt;&gt; map = new&lt;String, LinkedHashSet&lt;String&gt;&gt; LinkedHashMap();
NERClassifierCombiner classifier = null;
try {
    classifier = new NERClassifierCombiner(true, true, 
            ""C:\\Users\\perso\\Downloads\\stanford-ner-2015-04-20\\stanford-ner-2015-04-20\\classifiers\\my-classification-model.ser.gz""
            );
} catch (IOException e) {
    // TODO Auto-generated catch block
    e.printStackTrace();
}
List&lt;List&lt;CoreLabel&gt;&gt; classify = classifier.classify(""Zambia"");
for (List&lt;CoreLabel&gt; coreLabels : classify) {
    for (CoreLabel coreLabel : coreLabels) {

        String word = coreLabel.word();
        String category = coreLabel
                .get(CoreAnnotations.AnswerAnnotation.class);
        if (!""O"".equals(category)) {
            if (map.containsKey(category)) {
                map.get(category).add(word);
            } else {
                LinkedHashSet&lt;String&gt; temp = new LinkedHashSet&lt;String&gt;();
                temp.add(word);
                map.put(category, temp);
            }
            System.out.println(word + "":"" + category);
        }

    }

}
</code></pre>

<p>When I run the above code with input as ""USD"" or ""UK"", I get expected result as ""CURRENCY"" or ""COUNTRY"". But, when I input something like ""Russia"", return value is ""CURRENCY"" which is from the first train file in the properties. I am expecting 'O' would be returned for these values which is not present in my training dat.</p>

<p>How can I achieve this behavior? Any pointers where I am going wrong would be really helpful.</p>
","nlp, stanford-nlp, named-entity-recognition","<p>Hi I'll try to help out!</p>

<p>So it sounds to me like you have a list of strings that should be called ""CURRENCY"", and you have a list of strings that should be called ""COUNTRY"", etc...</p>

<p>And you want something to tag strings based off of your list.  So when you see ""RUSSIA"", you want it to be tagged ""COUNTRY"", when you see ""USD"", you want it to be tagged ""CURRENCY"".</p>

<p>I think these tools will be more helpful for you (particularly the first one):</p>

<p><a href=""http://nlp.stanford.edu/software/regexner/"" rel=""nofollow"">http://nlp.stanford.edu/software/regexner/</a></p>

<p><a href=""http://nlp.stanford.edu/software/tokensregex.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokensregex.shtml</a></p>

<p>The NERClassifierCombiner is designed to train on large volumes of tagged sentences and look at a variety of features including the capitalization and the surrounding words to make a guess about a given word's NER label.</p>

<p>But it sounds to me in your case you just want to explicitly tag certain sequences based off of your pre-defined list.  So I would explore the links I provided above.</p>

<p>Please let me know if you need any more help and I will be happy to follow up!</p>
",2,0,991,2015-05-09 15:35:31,https://stackoverflow.com/questions/30141835/how-to-suppress-unmatched-words-in-stanford-ner-classifiers
Annotator for Relationship Extraction,"<p>I have a set of urls in a text file. For each url in that text file, I want to tag the entities and relationships in the text contained in that url. </p>

<p>I am aware of the entity taggers like Stanford NER, NLTK and GATE which can perform the entity tagging. However, I am more interested in relationship extraction. </p>

<p>In order to extract relationships, I am thinking of annotating the text contained in those urls for training purpose. For this, I do not want to do manual annotation. I can write few regex to extract the relationship which I want, however it would be difficult to scale up. </p>

<p>Is there a tool where in I can specify what I want to annotate? </p>

<p>For example: </p>

<blockquote>
  <p>"" Rob is working as the Director of ABC organization. He graduated from
  XYZ University ""</p>
</blockquote>

<p>Here, I want to extract the <em>affiliations</em> relationship, so intuitively I would like to annotate words which describe the affiliations like <em>working</em>, <em>graduated</em>.</p>

<p>Edit:
By ""a set of URLs in the text file"", I mean I have about 200 links to certain webpages in that text file, each of the webpage contains some text. I want to analyse (annotate) that text.</p>
","regex, nlp, nltk, stanford-nlp, gate","<p>There is no PR in GATE that that will pair arguments and create instances
for you. You must therefore create instances that are relevant to your problem.</p>

<p>You can:</p>

<ul>
<li>write a custom PR</li>
<li>or write some JAPE with Java RHS</li>
</ul>

<p>You can probably split your corpus on a training and a test dataset. </p>

<p>You can use the GATE training course about Relation Extration that contains all you need:</p>

<ul>
<li>Slides: <a href=""https://gate.ac.uk/sale/talks/gate-course-jun14/module-3-ml/module-11-relations.pdf"" rel=""nofollow"">https://gate.ac.uk/sale/talks/gate-course-jun14/module-3-ml/module-11-relations.pdf</a></li>
<li>Materials: <a href=""http://gate.ac.uk/sale/talks/gate-course-jun14/module-3-ml/module-11-relation-exercise.zip"" rel=""nofollow"">http://gate.ac.uk/sale/talks/gate-course-jun14/module-3-ml/module-11-relation-exercise.zip</a></li>
</ul>
",1,0,747,2015-05-10 13:23:01,https://stackoverflow.com/questions/30151937/annotator-for-relationship-extraction
how to make a light-weighted stanford-nlp.jar,"<p>I've noticed the whole library is quite large, ~300MB. But I'm only using tokenize, ssplit, pos. How can I make a light library? Many thanks.</p>

<p>Best,
Huang</p>
",stanford-nlp,"<p>If you only want part of speech tags, you can include just the part of speech tagger models; for example, as downloaded from: nlp.stanford.edu/software/tagger.shtml. You can also safely just go ahead and remove unwanted models from the models jar to make it smaller.</p>
",2,0,51,2015-05-11 07:17:03,https://stackoverflow.com/questions/30161684/how-to-make-a-light-weighted-stanford-nlp-jar
NLTK : combining stanford tagger and personal tagger,"<p>The goal of my project is to answer queries such as, for example:
""I am looking for American women between 20 and 30 years old who work in Google""
I then have to process the query and to look into a DB to find the answer.</p>

<p>For this, I would need to combine the Stanford 3-class NERTagger and my own tagger. Indeed, my NER tagger can tag ages, nationalities and gender. But I need the Stanford tagger to tag organizations as I don't have any training file for this.</p>

<p>Right now, I have a code like this:</p>

<pre><code>def __init__(self, q):
    self.userQuery = q
def get_tagged_tokens(self):
    st = NERTagger('C:\stanford-ner-2015-01-30\my-ner-model.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    result = st.tag(self.userQuery.split())[0]
    return result
</code></pre>

<p>And I would like to have something like this:</p>

<pre><code>def get_tagged_tokens(self):
    st = NERTagger('C:\stanford-ner-2015-01-30\my-ner-model.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    st_def = NERTagger('C:\stanford-ner-2015-01-30\classifiers\english.all.3class.distsim.crf.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    tagger = BackoffTagger([st, st_def])
    result = st.tag(self.userQuery.split())[0]
    return result
</code></pre>

<p>This would mean that the tagger first uses my tagger and then the stanford one to tag untagged words. </p>

<p>Is it possible to combine my model with the Stanford model just to tag organizations? If yes, what is the best way to perform this?</p>

<p>Thank you!</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>The new NERClassifierCombiner with Stanford CoreNLP 3.5.2 or the new Stanford NER 3.5.2 has added command line functionality that makes it easy to get this effect with NLTK.</p>

<p>When you provide a list of serialized classifiers, NERClassifierCombiner will run them in sequence.  After one tagger tags the sentence, no other taggers will tag tokens that have already been tagged.  So note in my demo code I provide 2 classifiers as an example.  They are run in the order you place them.  I believe you can put as many as 10 in there if I recall correctly!</p>

<p>First, make sure that you have the latest copy of Stanford CoreNLP 3.5.2 or Stanford NER 3.5.2 , so that you have the right .jar file with this new functionality.</p>

<p>Second, make sure your custom NER model was built with Stanford CoreNLP or Stanford NER, this won't work otherwise!  It should be ok if you used older versions.</p>

<p>Third, I have provided some sample code that should work, the main gist of this is to subclass NERTagger:</p>

<p>If people would like I could look into pushing this to NLTK so it is in there by default!</p>

<p>Here is some sample code (it is a little hacky since I was just rushing this out the door, for instance in NERComboTagger's constructor there is no point to the first argument being classifier_path1, but the code would crash if I didn't put a valid file there):</p>

<pre><code>#!/usr/bin/python

from nltk.tag.stanford import NERTagger

class NERComboTagger(NERTagger):

  def __init__(self, *args, **kwargs):
    self.stanford_ner_models = kwargs['stanford_ner_models']
    kwargs.pop(""stanford_ner_models"")
    super(NERComboTagger,self).__init__(*args, **kwargs)

  @property
  def _cmd(self):
    return ['edu.stanford.nlp.ie.NERClassifierCombiner',
            '-ner.model',
            self.stanford_ner_models,
            '-textFile',
            self._input_file_path,
            '-outputFormat',
            self._FORMAT,
            '-tokenizerFactory',
            'edu.stanford.nlp.process.WhitespaceTokenizer',
            '-tokenizerOptions',
            '\""tokenizeNLs=false\""']

classifier_path1 = ""classifiers/english.conll.4class.distsim.crf.ser.gz""
classifier_path2 = ""classifiers/english.muc.7class.distsim.crf.ser.gz""

ner_jar_path = ""stanford-ner.jar""

st = NERComboTagger(classifier_path1,ner_jar_path,stanford_ner_models=classifier_path1+"",""+classifier_path2)

print st.tag(""Barack Obama is from Hawaii ."".split("" ""))  
</code></pre>

<p>Note the major change in the subclass is what is returned by _cmd .</p>

<p>Also note that I ran this in the unzipped folder stanford-ner-2015-04-20 , so the paths are relative to that.</p>

<p>I get this output:</p>

<pre><code>[('Barack','PERSON'), ('Obama', 'PERSON'), ('is','O'), ('from', 'O'), ('Hawaii', 'LOCATION'), ('.', 'O')]
</code></pre>

<p>Here is a link to the Stanford NER page: </p>

<p><a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/CRF-NER.shtml</a></p>

<p>Please let me know if you need any more help or if there are any errors in my code, I may have made a mistake while transcribing, but it works on my laptop!</p>
",8,1,1274,2015-05-11 08:24:46,https://stackoverflow.com/questions/30162809/nltk-combining-stanford-tagger-and-personal-tagger
How to replace a word by its most representative mention using Stanford CoreNLP Coreferences module,"<p>I am trying to figure out the way to rewrite sentences by ""resolving"" (replacing words with) their coreferences using Stanford Corenlp's Coreference module.</p>

<p>The idea is to rewrite a sentence like the following :</p>

<blockquote>
  <p>John drove to Judy’s house. He made her dinner.</p>
</blockquote>

<p>into </p>

<blockquote>
  <p>John drove to Judy’s house. John made Judy dinner.</p>
</blockquote>

<p>Here's the code I've been fooling around with : </p>

<pre><code>    private void doTest(String text){
    Annotation doc = new Annotation(text);
    pipeline.annotate(doc);


    Map&lt;Integer, CorefChain&gt; corefs = doc.get(CorefChainAnnotation.class);
    List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);


    List&lt;String&gt; resolved = new ArrayList&lt;String&gt;();

    for (CoreMap sentence : sentences) {

        List&lt;CoreLabel&gt; tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);

        for (CoreLabel token : tokens) {

            Integer corefClustId= token.get(CorefCoreAnnotations.CorefClusterIdAnnotation.class);
            System.out.println(token.word() +  "" --&gt; corefClusterID = "" + corefClustId);


            CorefChain chain = corefs.get(corefClustId);
            System.out.println(""matched chain = "" + chain);


            if(chain==null){
                resolved.add(token.word());
            }else{

                int sentINdx = chain.getRepresentativeMention().sentNum -1;
                CoreMap corefSentence = sentences.get(sentINdx);
                List&lt;CoreLabel&gt; corefSentenceTokens = corefSentence.get(TokensAnnotation.class);

                String newwords = """";
                CorefMention reprMent = chain.getRepresentativeMention();
                System.out.println(reprMent);
                for(int i = reprMent.startIndex; i&lt;reprMent.endIndex; i++){
                    CoreLabel matchedLabel = corefSentenceTokens.get(i-1); //resolved.add(tokens.get(i).word());
                    resolved.add(matchedLabel.word());

                    newwords+=matchedLabel.word()+"" "";

                }




                System.out.println(""converting "" + token.word() + "" to "" + newwords);
            }


            System.out.println();
            System.out.println();
            System.out.println(""-----------------------------------------------------------------"");

        }

    }


    String resolvedStr ="""";
    System.out.println();
    for (String str : resolved) {
        resolvedStr+=str+"" "";
    }
    System.out.println(resolvedStr);


}
</code></pre>

<p>The best output I was able to achieve for now is </p>

<blockquote>
  <p>John drove to Judy 's 's Judy 's house . John made Judy 's her dinner . </p>
</blockquote>

<p>which is not very brilliant ... </p>

<p>I'm pretty sure there is a MUCH easier way to do what I am trying to achieve.</p>

<p>Ideally, I would like to reorganize the sentence as a list of CoreLabels, so that I could keep the other data they have attached to them. </p>

<p>Any help appreciated.</p>
","java, nlp, stanford-nlp","<p>The challenge is you need to make sure that the token isn't part of its representative mention. For example, the token ""Judy"" has ""Judy 's"" as its representative mention, so if you replace it in the phrase ""Judy 's"", you'll end up with the double ""'s"". </p>

<p>You can check if the token is part of its representative mention by comparing their indices. You should only replace the token if its index is either smaller than the <code>startIndex</code> of the representative mention, or larger than the <code>endIndex</code> of the representative mention. Otherwise you just keep the token. </p>

<p>The relevant part of your code will now look like this: </p>

<pre><code>            if (token.index() &lt; reprMent.startIndex || token.index() &gt; reprMent.endIndex) {

                for (int i = reprMent.startIndex; i &lt; reprMent.endIndex; i++) {
                    CoreLabel matchedLabel = corefSentenceTokens.get(i - 1); 
                    resolved.add(matchedLabel.word());

                    newwords += matchedLabel.word() + "" "";

                }
            }

            else {
                resolved.add(token.word());

            }
</code></pre>

<p>In addition, and to speed up the process, you can also replace your first if-condition by: </p>

<pre><code>if (chain==null || chain.getMentionsInTextualOrder().size() == 1)
</code></pre>

<p>After all, if the length of the co-reference chain is just 1, there is no use looking for a representative mention. </p>
",9,11,2377,2015-05-12 05:00:51,https://stackoverflow.com/questions/30182138/how-to-replace-a-word-by-its-most-representative-mention-using-stanford-corenlp
StanfordNLP lemmatization cannot handle -ing words,"<p>I've been experimenting with Stanford NLP toolkit and its lemmatization capabilities. I am surprised how it lemmatize some words. For example:</p>

<pre><code>depressing -&gt; depressing
depressed -&gt; depressed
depresses -&gt; depress
</code></pre>

<p>It is not able to transform <code>depressing</code> and <code>depressed</code> into the same lemma. Simmilar happens with <code>confusing</code> and <code>confused</code>, <code>hopelessly</code> and <code>hopeless</code>. I am getting the feeling that the only thing it is able to do is remove the <code>s</code> if the word is in such form (e.g. <code>feels -&gt; feel</code>). Is such behaviour normal for Lematizatiors in English? I would expect that they would be able to transform such variations of common words into a same lemma.</p>

<p>If this is normal, should I rather use stemmers? And, is there a way to use stemmers like Porter (Snowball, etc.) in StanfordNLP? There is no mention of stemmers in their documentation; however, there are some <code>CoreAnnotations.StemAnnotation</code> in the API. If not possible with StanfordNLP which stemmers do you recommend for use in Java?</p>
","java, nlp, stanford-nlp, stemming, lemmatization","<p>Lemmatization crucially depends on the part of speech of the token. Only tokens with the same part of speech are mapped to the same lemma. </p>

<p>In the sentence ""This is confusing"", <code>confusing</code> is analyzed as an adjective, and therefore it is lemmatized to <code>confusing</code>. In the sentence ""I was confusing you with someone else"", by contrast, <code>confusing</code> is analyzed as a verb, and is lemmatized to <code>confuse</code>. </p>

<p>If you want tokens with different parts of speech to be mapped to the same lemma, you can use a stemming algorithm such as <a href=""http://tartarus.org/martin/PorterStemmer/java.txt"" rel=""nofollow"">Porter Stemming</a>, which you can simply call on each token.</p>
",3,2,1731,2015-05-13 09:16:59,https://stackoverflow.com/questions/30210494/stanfordnlp-lemmatization-cannot-handle-ing-words
Stanford NLP - Using Parsed or Tagged text to generate Full XML,"<p>I'm trying to extract data from the PennTreeBank, Wall Street Journal corpus. Most of it already has the parse trees, but some of the data is only tagged.
i.e. wsj_DDXX.mrg and wsj_DDXX.pos files.</p>

<p>I would like to use the already parsed trees and tagged data in these files so as not to use the parser and taggers within CoreNLP, but I still want the output file format that CoreNLP gives; namely, the XML file that contains the dependencies, entity coreference, and the parse tree and tagged data.</p>

<p>I've read many of the java docs but I cannot figure out how to get it the way I described.</p>

<p>For POS, I tried using the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.2/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html"" rel=""nofollow"">LexicalizedParser</a> and it allows me to use the tags, but I can only generate an XML file with the some of the information I want; there is no option for coreference or generating the parse trees. To get it to correctly generate the sub-optimal XML files here, I had to write a script to get rid of all of the brackets within the files. This is the command I use:</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat typedDependenciesCollapsed,wordsAndTags -outputFilesExtension xml -outputFormatOptions xml -writeOutputFiles -outputFilesDirectory my\dir -tokenized -tagSeparator / -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz my\wsj\files\dir</p>
</blockquote>

<p>I also can't generate the data I would like to have for the WSJ data that already has the trees. I tried using what is said <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#s"" rel=""nofollow"">here</a> and I looked at the corresponding Javadocs. I used the command similar to what is described. But I had to write a python program to retrieve the stdout data resulting from analyzing each file and wrote it into a new file. This resulting data is only a text file with the dependencies and is not in the desired XML notation.</p>

<p>To summarize, I would like to use the POS and tree data from these PTB files in order to generate a CoreNLP parse corresponding to what would occur if I used CoreNLP on a regular text file. The pseudo command would be like this:</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.pipeline.CoreNLP -useTreeFile wsj_DDXX.mrg</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.pipeline.CoreNLP -usePOSFile wsj_DDXX.pos</p>
</blockquote>

<p>Edit: fixed a link.</p>
","parsing, nlp, stanford-nlp, pos-tagger","<p>Yes, this is possible, but a bit tricky and there is no out of the box feature that can do this, so you will have to write some code. The basic idea is to replace the <code>tokenize</code>, <code>ssplit</code> and <code>pos</code> annotators (and in case you also have trees the <code>parse</code> annotator) with your code that loads these annotations from your annotated files.</p>

<p>On a very high level you have to do the following:</p>

<ul>
<li>Load your trees with <code>MemoryTreebank</code></li>
<li>Loop through all the trees and for each tree create a sentence <code>CoreMap</code> to which you add
<ul>
<li>a <code>TokensAnnotation</code></li>
<li>a <code>TreeAnnotation</code> and the <code>SemanticGraphCoreAnnotations</code></li>
</ul></li>
<li>Create an <code>Annotation</code> object with a list containing the <code>CoreMap</code> objects for all sentences</li>
<li>Run the StanfordCoreNLP pipeline with the <code>annotators</code> option set to <code>lemma,ner,dcoref</code> and the option <code>enforceRequirements</code> set to <code>false</code>.</li>
</ul>

<p>Take a look at the individual annotators to see how to add the required annotations. E.g. there is a method in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/ParserAnnotatorUtils.java#L29"" rel=""nofollow"">ParserAnnotatorUtils</a> that adds the <code>SemanticGraphCoreAnnotations</code>. </p>
",1,1,665,2015-05-13 15:52:03,https://stackoverflow.com/questions/30219780/stanford-nlp-using-parsed-or-tagged-text-to-generate-full-xml
Processing input before giving input to parser,"<p>What kind of processing should be done to the input which is given to the parser.</p>

<p>As of know i am using stanford parser.jar but there is also stanford coreNLP.jar what is the difference between parser.jar and coreNLP.jar parsing method</p>

<p>As per coreNLP documentation you can pass the operation you want to do as input in the annotators </p>

<p><strong><em>COMMAND:</em></strong> </p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</code></pre>

<p>To use parsing in coreNLP can i pass only parse or should I pass all the annotators  except dcoref </p>

<p><strong><em>i.e.)</em></strong></p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,parse -file input.txt
                                      or
java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</code></pre>

<p>Does the parser.jar has sentence splitting in built in it's jar</p>

<p>Can I give paragraph as input and get sentence and their parsed data as out</p>

<p>or should i give only one sentence at a time<br>
Thank you,</p>
","parsing, stanford-nlp","<p>The CoreNLP annotators can be thought of as a dependency graph. The parser annotator depends on tokenization (<code>tokenize</code>) and sentence splitting (<code>ssplit</code>) only. So, you could run the parser with your first command:</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,parse -file input.txt
</code></pre>

<p>If you know your text is pre-tokenized, the easiest thing to do is to set the options <code>tokenize.whitespace = ""true""</code> in your properties file (or pass it in as a flag: <code>-tokenize.whitespace</code>). To only sentence split at the end of a line, you can set the option (<code>ssplit.eolonly</code>).</p>

<p>But, by default, yes CoreNLP will tokenize and split up your sentence for you. You can just feed in a pile of text, and it will output parsed sentences.</p>
",2,1,257,2015-05-14 05:07:02,https://stackoverflow.com/questions/30229648/processing-input-before-giving-input-to-parser
How to Identify mentions in a text?,"<p>I am looking for rule-based methods or any other methods to identify all mentions in a text. I have found several libraries that give coreferences but no exact options for only mentions. What I want is something like below: </p>

<p>Input text: </p>

<blockquote>
  <p>[This painter]'s indulgence of visual fantasy, and appreciation of different historic architectural styles can be seen in his 1840 Architect's Dream. After a series of paintings on The Last of the Mohicans, [he] made a three year trip to Europe in 1829, but [he] is better known for a trip four years earlier in which [he] journeyed up the Hudson River to the Catskill Mountains. FTP, name [this painter of The Oxbow] and The Voyage of Life series.</p>
</blockquote>

<p>*The square brackets highlight mentions. </p>

<p>How do I find mentions? Also, how is it different from coreferences? It would be really helpful if someone could post links to the concerned papers. </p>
","nlp, stanford-nlp","<p>I think you can get what you want from the standard <code>dcoref</code> annotator. Look at the annotation set by this annotator, <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CorefCoreAnnotations.CorefChainAnnotation.html"" rel=""nofollow""><code>CorefChainAnnotation</code></a>. This is a map from document entities to ""coref chains.""</p>

<p>Each <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CorefChain.html"" rel=""nofollow""><code>CorefChain</code></a> can provide you with a list of mentions for the relevant entity in textual order.</p>
",1,0,484,2015-05-15 20:29:16,https://stackoverflow.com/questions/30268220/how-to-identify-mentions-in-a-text
"software to extract word functions like subject, predicate, object etc","<p>I need to extract relations of the words in a sentence. I'm mostly interested in identifying a subject, predicate and an object. For example, for the follwoing sentence:</p>

<pre><code>She gave him a pen
</code></pre>

<p>I'd like to have:</p>

<pre><code>She_subject gave_predicate him a pen_object.
</code></pre>

<p>Is Stanford NLP can do that? I've tried their <code>relation</code> annotator but it didn't seem to work as I expected? Maybe there's other software that can produce this result?</p>
","nlp, stanford-nlp","<p>According to <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/lex-parser.shtml</a>, Stanford NLP does have a parser which can identify the subject and predicate of a sentence. You can try it out online <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a>. You can use the typed dependencies to identify the subject, predicate, and object.</p>

<p>From the example page, the sentence <em>My dog also likes eating sausage</em> will give you this parse:</p>

<pre><code>(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (NP (NN sausage)))))
    (. .)))
</code></pre>

<p>The parser can also generate dependencies:</p>

<pre><code>poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
</code></pre>

<p>The dependency <code>nsubj</code> shows the main predicate and the subject&mdash;in this case, <em>likes</em> and <em>dog</em>. The numbers give the position of the word in the sentence (one-indexed, for some reason). The <code>dobj</code> dependency shows the relation of the predicate and object. The <code>xcomp</code> dependency gives internal information about the predicate.</p>

<p>This also works when the predicate is not a verb: <em>My dog is large and in charge</em> gives:</p>

<pre><code>poss(dog-2, My-1)
nsubj(large-4, dog-2)
cop(large-4, is-3)
root(ROOT-0, large-4)
cc(large-4, and-5)
conj(large-4, in-6)
pobj(in-6, charge-7)
</code></pre>

<p>This tells us that <code>large</code> is the main predicate (<code>nsubj(large-4, dog-2)</code>), but there was a copula (<code>cop(large-4, is-3)</code>), as well as a conjunction and a preposition with an object. </p>

<p>I'm not familiar with the API, so I can't give exact code. Perhaps someone else who knows the API can do that. The parser is documented at <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/"" rel=""nofollow noreferrer"">the Stanford NLP doc site</a>. You might also find the answer to <a href=""https://stackoverflow.com/a/9606606/3376926"">Tools for text simplification (Java)</a> helpful. There's more information about the dependency format in <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow noreferrer"">The Stanford Dependency Manual</a>.  </p>
",9,10,19167,2015-05-18 17:10:18,https://stackoverflow.com/questions/30309124/software-to-extract-word-functions-like-subject-predicate-object-etc
How can I use Stanford NLP commercially?,"<p>I'm working on the company that makes toy cars that can talk with children. We want to use Stanford Core NLP as a parser. However, it is licensed in GPL: they doesn't allow using the NLP commercially. Can I purchase other license from Stanford NLP group? Or can't I use the NLP commercially?</p>

<p>This might not be appropriate as a public question, but I can't find direct contact to Stanford NLP group.</p>

<p>Thank you in advance.</p>
",stanford-nlp,"<p>You can either use the  software under the GPL license, or you can purchase a commercial license. For the latter, you can contact us at the support email address found <a href=""https://stanfordnlp.github.io/CoreNLP/#license"" rel=""nofollow noreferrer"">here</a>.</p>
",4,4,2467,2015-05-19 04:29:10,https://stackoverflow.com/questions/30316779/how-can-i-use-stanford-nlp-commercially
Setting intercept in Stanford-NLP LogisticClassifier,"<p>I want to instantiate a Stanford-NLP LogisticClassifier using features/weights being read in from a text file (from a classifier trained separately). </p>

<p>The classifier I've trained (in Python, using scikit-learn) consists of weights, features, and also an intercept term. On the Stanford-NLP end, though, the classifier constructor doesn't take an intercept. </p>

<p>Is there any way to incorporate the intercept into my LogisticClassifier?</p>
","stanford-nlp, logistic-regression","<p>Yes; you can simply define a new feature (e.g., ""bias"" or ""intercept""), and set the weight of that to be the intercept value from scikit-learn.</p>
",0,0,104,2015-05-20 22:28:08,https://stackoverflow.com/questions/30361458/setting-intercept-in-stanford-nlp-logisticclassifier
Stanford CoreNLP wrong coreference resolution,"<p>I am still playing with Stanford's CoreNLP and I am encountering strange results on a very trivial test of Coreference resolution. </p>

<p>Given the two sentences :  </p>

<blockquote>
  <p>The hotel had a big bathroom. It was very clean.</p>
</blockquote>

<p>I would expect ""It"" in sentence 2 to be coreferenced by ""bathroom"" or at least ""a big bathroom"" of sentence 1. </p>

<p>Unfortunately it point to ""The hotel"" which in my opinion is wrong.</p>

<p>Is there a way to solve this problem ? Do I need to train anything or is it supposed to work out of the box ?</p>

<pre><code>    Annotation a = getPipeline().getAnnotation(""The hotel had a big bathroom. It was very clean."");

    System.out.println(a.get(CorefChainAnnotation.class));
</code></pre>

<p>output : </p>

<blockquote>
  <p>{1=CHAIN1-[""The hotel"" in sentence 1, ""It"" in sentence 2], 2=CHAIN2-[""a big bathroom"" in sentence 1]}</p>
</blockquote>

<p>Many thanks for your help.</p>
","nlp, stanford-nlp","<p>Like many components in AI, the Stanford coreference system is only correct to a certain accuracy. In the case of coreference this accuracy is actually relatively low (~60 on standard benchmarks in a 0-100 range). To illustrate the difficulty of the problem, consider the following apparently similar sentence with a different coreference judgment:</p>

<blockquote>
  <p>The hotel had a big bathtub. It was very expensive.</p>
</blockquote>
",5,0,806,2015-05-21 00:40:41,https://stackoverflow.com/questions/30362691/stanford-corenlp-wrong-coreference-resolution
CoreNLP ConLL format with CollapsedCCProcessedDependenciesAnnotation,"<p>I am using the recent version of CoreNLP.</p>

<p>My task is to parse a text and get an output in conll format with CollapsedCCProcessedDependenciesAnnotation.</p>

<p>I run the following command</p>

<pre><code>time java -cp $CoreNLP/javanlp-core.jar edu.stanford.nlp.pipeline.StanfordCoreNLP -props $CoreNLP/config.properties -file 12309959  -outputFormat conll


depparse.model = english_SD.gz
</code></pre>

<p>The problem is how to get <code>CollapsedCCProcessedDependenciesAnnotation</code>.</p>

<p>I tried to use 
depparse.extradependencies in config.properties</p>

<p>but there is no parameter for <code>CCProcessedDependenciesAnnotation</code> according to 
<a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/GrammaticalStructure.Extras.html#REF_ONLY_COLLAPSED"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/GrammaticalStructure.Extras.html#REF_ONLY_COLLAPSED</a></p>

<p>Can you propose any solution how I can parse in conll with <code>CollapsedCCProcessedDependenciesAnnotation</code>?</p>
","parsing, stanford-nlp","<p>You can retrieve the CC-processed dependencies programmatically.</p>
<p><a href=""https://stackoverflow.com/questions/5049018/getting-corefrences-with-standard-corenlp-package"">This question</a> should serve as a good example (see the code in the example using the <code>CollapsedCCProcessedDependenciesAnnotation</code>).</p>
<hr />
<p>Gabor's answer from the mailing list explains this behavior very well (i.e., why you can't output collapsed dependencies directly):</p>
<blockquote>
<p>Note that in general the collapsed cc processed dependencies won't output losslessly to conll though, as <strong>the format expects a tree</strong> (every word has a unique parent), and <strong>the dependencies can have multiple heads</strong>.</p>
<p>The output formatter therefore uses the basic dependencies only: <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/CoNLLOutputter.java#L118"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/CoNLLOutputter.java#L118</a>. This could be changed in the code without crashing anything, but the serialized trees would be missing some edges, and ties for which edges are included would be broken somewhat arbitrarily. You may be better off writing your own logic for dumping to conll to fit your particular use case (you can probably copy much of our conll outputter code from above).</p>
</blockquote>
",2,0,992,2015-05-23 08:31:30,https://stackoverflow.com/questions/30410608/corenlp-conll-format-with-collapsedccprocesseddependenciesannotation
NLP- Sentiment Processing for Junk Data takes time,"<p>I am trying to find the Sentiment for the input text. This test is a junk sentence and when I tried to find the Sentiment the Annotation to parse the sentence is taking around 30 seconds. For normal text it takes less than a second. If i need to process around millions of data it will add up the time to process. Any solution to this.</p>

<pre><code>String text = ""Nm n n 4 n n bkj nun4hmnun Onn njnb hm5bn nm55m nbbh n mnrrnut but n rym4n nbn 4nn65 m nun m n nn nun 4nm 5 gm n my b bb b b rtmrt55tmmm5tttn b b bb g bn nn n h r ret n nun bn d. B bbbbbbbbbbr bung NHnhn nn nk, v v v n gain t g 4gnyhimmigration ndn nb NVnb bin uny 7 nbbbbbnn vv bbvb ninn njnj n4 nm n km n n n cb j bun. Nhfnt bn nn. N hm nn nun m bum my b mmmnbjk nn n by nn nun nun n nun nn bn n nhn n nn n n m NH nb4mnm mkn 4 n n n n hm r b rnfngg4d in b nut mmmkmmm5 bbjn n n ij BBM 8u8i by nun n.nn hm n. n4n By 4n4n bunny RN bny hm j mi. Nymmn FBT not mn n n nm g by n n nnm? Rnyb vCard n5 Yu nn n n n n nt .nm mn nt n nb n n n n by y5nnnhyyh h b b nt njj n m f4n re"";
Properties props = new Properties();
            props.setProperty(""annotators"",""tokenize, ssplit, pos,parse,sentiment"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
            Annotation annotation = pipeline.process(text);
</code></pre>

<p>For the NLP sentiment calulation I have to parse the data and i think thats the reason its taking time.</p>

<p>thanks</p>
","nlp, stanford-nlp, sentiment-analysis, pos-tagger","<p>Yes, the standard PCFG parser (the one that is run by default without any other options specified) will choke on this sort of long nonsense data. You might have better luck using the <a href=""http://nlp.stanford.edu/software/srparser.shtml"" rel=""nofollow"">shift-reduce constituency parser</a>, which is substantially faster than the PCFG and nearly as accurate.</p>
",3,1,262,2015-05-23 14:31:29,https://stackoverflow.com/questions/30413885/nlp-sentiment-processing-for-junk-data-takes-time
NLP Shift reduce parser is throwing null pointer Exception for Sentiment calculation,"<p>I am trying to analyze sentiment using nlp.  The version of stanford-nlp I am using is 3.4.1.  I have some junk data to process and it looks like it takes around 45 seconds to process using default PCFG file. </p>

<p>Here is the example:</p>

<pre><code>String text = ""Nm n n 4 n n bkj nun4hmnun Onn njnb hm5bn nm55m nbbh n mnrrnut but n rym4n nbn 4nn65 m nun m n nn nun 4nm 5 gm n my b bb b b rtmrt55tmmm5tttn b b bb g bn nn n h r ret n nun bn d. B bbbbbbbbbbr bung NHnhn nn nk, v v v n gain t g 4gnyhimmigration ndn nb NVnb bin uny 7 nbbbbbnn vv bbvb ninn njnj n4 nm n km n n n cb j bun. Nhfnt bn nn. N hm nn nun m bum my b mmmnbjk nn n by nn nun nun n nun nn bn n nhn n nn n n m NH nb4mnm mkn 4 n n n n hm r b rnfngg4d in b nut mmmkmmm5 bbjn n n ij BBM 8u8i by nun n.nn hm n. n4n By 4n4n bunny RN bny hm j mi. Nymmn FBT not mn n n nm g by n n nnm? Rnyb vCard n5 Yu nn n n n n nt .nm mn nt n nb n n n n by y5nnnhyyh h b b nt njj n m f4n re"";
Properties props = new Properties();

props.setProperty(""annotators"",""tokenize, ssplit, pos,parse,sentiment"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = pipeline.process(text);
</code></pre>

<p>Based on the suggestion here, I tried again with the shift-reduce parser.</p>

<pre><code>Properties props = new Properties();
                    props.setProperty(""annotators"",""tokenize, ssplit, pos,parse,sentiment"");
props.put(""parse.model"", ""com/example/nlp/englishSR.ser.gz"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = pipeline.process(text);
</code></pre>

<p>I have to download the shift-reduce model and put it in the classpath.  The model class is getting loaded but it's throwing a null pointer exception. Any thoughts and suggestions?</p>
","nlp, stanford-nlp, sentiment-analysis, shift-reduce","<p>Is there a specific reason why you are using version 3.4.1 and not the latest version? </p>

<p>If I run your code with the latest version, it works for me (after I change the path to the SR model to <code>edu/stanford/nlp/models/srparser/englishSR.ser.gz</code> but I assume you changed that path on purpose). </p>

<p>Also make sure that the models you downloaded are compatible with your version of CoreNLP. If you downloaded the latest models and try to use them with an older version of CoreNLP, then you will most likely run into problems.</p>
",2,1,467,2015-05-24 07:15:38,https://stackoverflow.com/questions/30421008/nlp-shift-reduce-parser-is-throwing-null-pointer-exception-for-sentiment-calcula
How to not split English into separate letters in the Stanford Chinese Parser,"<p>I am using the Stanford Segmenter at <a href=""http://nlp.stanford.edu/software/segmenter.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/segmenter.shtml</a> in Python. For the Chinese segmenter, whenever it encounters a English word, it will split the word into many characters one by one, but I want to keep the characters together after the segmentation is done.  </p>

<p>For example:</p>

<pre><code>你好abc我好 
</code></pre>

<p>currently will become this after the segmentation </p>

<pre><code>你好 a b c 我 好
</code></pre>

<p>but I want it to become </p>

<pre><code>你好 abc 我 好
</code></pre>

<p>Is there a way to teach the segmenter to do that?  Is there a setting for this?</p>

<p>I Googlged this and to no answer, and tried to hack together a way(spending 6 hours on it) to do this by pulling out English characters from the text and then put them back in after the segmentation is done, but realized it is very difficult to do this in a efficient manner.  Any help on this would be greatly appreciated.</p>
","python, nlp, stanford-nlp, segment, chinese-locale","<p>I don't know about tokenization in mixed language texts, so I propose to use the following hack: go through the text, until you find English word; all text before this word can be tokenized by Chinese tokenizer; English word can be append as another token; repeat. Below is code sample.</p>

<pre><code>import re
pat = re.compile(""[A-Za-z]+"")
for sentence in text:
    sent_tokens = []
    prev_end = 0
    for match in re.finditer(pat, sentence):
        print match.start(0), match.end(0), match.group(0)
        chinese_part = sentence[prev_end:match.start(0)]
        sent_tokens += tokenize(chinese_part)
        sent_tokens.append(match.group(0))
        prev_end = match.end(0)
    last_chinese_part = sentence[prev_end:]
    sent_tokens += tokenize(last_chinese_part)
    print sent_tokens
</code></pre>

<p>I think efficiency to be comparable with sole tokenization by Chinese tokenizer, since the only overhead is caused by application of regex, which is actually just a finite state automaton and works as O(n).</p>
",4,1,577,2015-05-24 16:42:57,https://stackoverflow.com/questions/30425877/how-to-not-split-english-into-separate-letters-in-the-stanford-chinese-parser
Identify prepositons and individual POS,"<p>I am trying to find correct parts of speech for each word in paragraph. I am using Stanford POS Tagger. However, I am stuck at a point.</p>

<p>I want to identify prepositions from the paragraph.</p>

<p>Penn Treebank Tagset says that: </p>

<pre><code>IN  Preposition or subordinating conjunction
</code></pre>

<p>how, can I be sure if current word is be <strong>preposition</strong> or <strong>subordinating conjunction</strong>. How can I extract only prepositions from paragraph in this case?</p>
","nlp, stanford-nlp","<p>I have had some breakthrough to understand if the word is actually preposition or subordinating conjunction.</p>

<p>I have parsed following sentence :</p>

<blockquote>
  <p>She left early because Mike arrived with his new girlfriend.</p>
</blockquote>

<p>(here because is subordinating conjunction )</p>

<p>After POS tagging</p>

<blockquote>
  <p>She_PRP left_VBD early_RB because_IN Mike_NNP arrived_VBD with_IN
  his_PRP$ new_JJ girlfriend_NN ._.</p>
</blockquote>

<p>here , to make sure <strong>because</strong> is a preposition or not I have parsed the sentence.</p>

<p><img src=""https://i.sstatic.net/Z4l4B.png"" alt=""Parse Tree for Sentence 1""></p>

<p>here <strong>because</strong> has direct parent after IN as SBAR(Subordinate Clause) as root.</p>

<p><strong>with</strong> also comes under IN but its direct parent will be <strong>PP</strong> so it is a preposition.</p>

<p><strong>Example 2 :</strong></p>

<blockquote>
  <p>Keep your hand on the wound until the nurse asks you to take it off.
  (here until is coordinating conjunction )</p>
</blockquote>

<p>POS tagging is :</p>

<blockquote>
  <p>Keep_VB your_PRP$ hand_NN on_IN the_DT wound_NN until_IN the_DT
  nurse_NN asks_VBZ you_PRP to_TO take_VB it_PRP off_RP ._.</p>
</blockquote>

<p>So , until and on are marked as <strong>IN</strong>.</p>

<p>However, picture gets clearer when we actually parse the sentence.</p>

<p><strong>So finally I conclude because is subordinating conjunction and with is preposition.</strong></p>

<p>Tried for many variations of sentences .. worked for almost all except some cases for <strong>before</strong> and <strong>after</strong>.
<img src=""https://i.sstatic.net/04qne.png"" alt=""Example 2""></p>
",2,5,633,2015-05-26 06:09:16,https://stackoverflow.com/questions/30450705/identify-prepositons-and-individual-pos
How to collect output from a Python subprocess,"<p>I am trying to make a python process that reads some input, processes it and prints out the result. The processing is done by a subprocess (Stanford's NER), for ilustration I will use 'cat'. I don't know exactly how much output NER will give, so I use run a separate thread to collect it all and print it out. The following example illustrates.</p>

<pre><code>import sys
import threading
import subprocess

#   start my subprocess
cat = subprocess.Popen(
    ['cat'],
    shell=False, stdout=subprocess.PIPE, stdin=subprocess.PIPE,
    stderr=None)


def subproc_cat():
    """""" Reads the subprocess output and prints out """"""
    while True:
        line = cat.stdout.readline()
        if not line:
            break
        print(""CAT PROC: %s"" % line.decode('UTF-8'))

#   a daemon that runs the above function
th = threading.Thread(target=subproc_cat)
th.setDaemon(True)
th.start()

#   the main thread reads from stdin and feeds the subprocess
while True:
    line = sys.stdin.readline()
    print(""MAIN PROC: %s"" % line)
    if not line:
        break
    cat.stdin.write(bytes(line.strip() + ""\n"", 'UTF-8'))
    cat.stdin.flush()
</code></pre>

<p>This seems to work well when I enter text with the keyboard. However, if I try to pipe input into my script (cat file.txt | python3 my_script.py), a racing condition seems to occur. Sometimes I get proper output, sometimes not, sometimes it locks down. Any help would be appreciated!</p>

<p>I am runing Ubuntu 14.04, python 3.4.0. The solution should be platform-independant.</p>
","python, subprocess, stanford-nlp, python-multithreading","<p>Add <code>th.join()</code> at the end otherwise you may kill the thread prematurely before it has processed all the output when the main thread exits: daemon threads do not survive the main thread (or remove <code>th.setDaemon(True)</code> instead of <code>th.join()</code>).</p>
",2,7,638,2015-05-26 07:00:30,https://stackoverflow.com/questions/30451599/how-to-collect-output-from-a-python-subprocess
compare TypedDependencies from Stanford NLP dependency parser tree,"<p>I am trying a semantic match between two sentences by comparing the dependencies.<br>
I am getting two Stanford dependency trees from two different sentences. I want to compare and get a score for the semantic match between the sentences.<br></p>

<pre><code>for(TypedDependency td1 : dependencyList1)
    {
        for(TypedDependency td2 : dependencyList2)
        {
            score = td1.compareTo(td2);
        }
    }
</code></pre>

<p><code>dependencyList1</code> and <code>dependencyList2</code> are the list of all dependencies from sentences1 and sentence 2 respectively.
  I am using a <code>compareTo</code> function which gives out scores of <code>-1,0,1</code>.<br>
I then average out the scores to come up with a final score.<br>
I don't know how these scores are calculated.<br> Is there a better way to compare and identify similar dependencies.<br>
Any help would be appreciated.</p>
","nlp, stanford-nlp, text-mining","<p><code>compareTo()</code> gives you an ordering between dependencies, e.g., for sorting (see <a href=""https://docs.oracle.com/javase/7/docs/api/java/lang/Comparable.html"" rel=""nofollow"">https://docs.oracle.com/javase/7/docs/api/java/lang/Comparable.html</a>). To find similar dependencies, you first need to formalize exactly what you mean by ""similar"", and then make a custom scoring function.</p>

<p>A natural metric, beyond simple equality, is collapsing things like <code>*subj</code> (nsubj, nsubjpass, csubj, csubjpass) and <code>*obj</code> (dobj, iobj). If you care about the endpoints of the arcs, checking for lemma match rather than word match is maybe a good start. Similarity in vector space (e.g., with word2vec or GloVE) is also quite effective.</p>

<p>The list of dependencies, for reference, can be found at: <a href=""http://universaldependencies.github.io/docs/u/dep/index.html"" rel=""nofollow"">http://universaldependencies.github.io/docs/u/dep/index.html</a></p>
",4,0,673,2015-05-28 05:19:30,https://stackoverflow.com/questions/30497786/compare-typeddependencies-from-stanford-nlp-dependency-parser-tree
incompatible types: Object cannot be converted to CoreLabel,"<p>I'm trying to use the Stanford tokenizer with the following example from their website:</p>

<pre><code>import java.io.FileReader;
import java.io.IOException;
import java.util.List;

import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.process.CoreLabelTokenFactory;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.process.PTBTokenizer;

public class TokenizerDemo {

  public static void main(String[] args) throws IOException {
    for (String arg : args) {
      // option #1: By sentence.
      DocumentPreprocessor dp = new DocumentPreprocessor(arg);
      for (List sentence : dp) {
        System.out.println(sentence);
      }
      // option #2: By token
      PTBTokenizer ptbt = new PTBTokenizer(new FileReader(arg),
              new CoreLabelTokenFactory(), """");
      for (CoreLabel label; ptbt.hasNext(); ) {
        label = ptbt.next();
        System.out.println(label);
      }
    }
  }
}
</code></pre>

<p>and I get the following error when I try to compile it:</p>

<pre><code>TokenizerDemo.java:24: error: incompatible types: Object cannot be converted to CoreLabel
        label = ptbt.next();
</code></pre>

<p>Does anyone know what the reason might be? In case you are interested, I'm using Java 1.8 and made sure that CLASSPATH contains the jar file.</p>
",stanford-nlp,"<p>Try parameterizing the <code>PTBTokenizer</code> class. For example:</p>

<pre><code>PTBTokenizer&lt;CoreLabel&gt; ptbt = new PTBTokenizer&lt;&gt;(new FileReader(arg),
          new CoreLabelTokenFactory(), """");
</code></pre>
",2,0,221,2015-05-29 20:59:38,https://stackoverflow.com/questions/30538960/incompatible-types-object-cannot-be-converted-to-corelabel
How to use serialized CRFClassifier with StanfordCoreNLP prop &#39;ner&#39;,"<p>I'm using the StanfordCoreNLP API interface to programmatically do some basic NLP. I need to train a model on my own corpus, but I'd like to use the <code>StanfordCoreNLP</code> interface to do it, because it handles a lot of the dry mechanics behind the scenes and I don't need much specialization there. </p>

<p>I've trained a CRFClassifier that I'd like to use for NER, serialized to a file. Based on the documentation, I'd think the following would work, but it doesn't seem to find my model and instead barfs on not being able to find the standard models (I'm not sure why I don't have those model files, but I'm not concerned about it since I don't want to use them anyway):</p>

<pre><code>    // String constants
    final String serializedClassifierFilename = ""/absolute/path/to/model.ser.gz"";

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, ner"");
    props.setProperty(""ner.models"", serializedClassifierFilename);

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    String fileContents = IOUtils.slurpFileNoExceptions(""test.txt"");
    Annotation document = new Annotation(fileContents);
</code></pre>

<p>Results in:</p>

<pre><code>Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator ner
Loading classifier from /path/build/edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... java.io.FileNotFoundException: edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1554)
</code></pre>

<p>etc., etc.</p>

<p>I know that I don't have their built-in model (again, not sure why.. I just cloned their git repo and compiled with <code>ant compile</code>. Regardless, I don't want to use their model anyway, I want to use the one I trained).</p>

<p>How can I get the StanfordCoreNLP interface to use my model in the <code>ner</code> step? Is possible? Is not possible?</p>
","java, nlp, stanford-nlp","<p>The property name is <code>ner.model</code>, not <code>ner.models</code>, so your code is still trying to load the default models.</p>

<p>Let me know if this is documented incorrectly somewhere.</p>
",2,4,1015,2015-06-02 16:43:45,https://stackoverflow.com/questions/30601875/how-to-use-serialized-crfclassifier-with-stanfordcorenlp-prop-ner
Stanford coreNLP : can a word in a sentence be part of multiple Coreference chains,"<p>The question is in the title. Using Stanford's NLP coref module, I am wondering if a given word can be part of multiple coreference chains. Or can it only be part of one chain. 
Could you give me examples of when this might occur. </p>

<p>Similarly, can a word be part of multiple coreference mentions, or only one.</p>

<p>Thank you.</p>
","nlp, stanford-nlp","<p>A word can be part of multiple coreference mentions. Consider for example the mention ""the new acquisition by Microsoft"". In this case, there are two candidates for mentions: <em>the new acquisition by Microsoft</em> and <em>Microsoft</em>.</p>

<p>From this example it also follows that a word can be part of multiple coreference chains.</p>
",2,0,107,2015-06-05 10:01:08,https://stackoverflow.com/questions/30663716/stanford-corenlp-can-a-word-in-a-sentence-be-part-of-multiple-coreference-chai
Extract list of Persons and Organizations using Stanford NER Tagger in NLTK,"<p>I am trying to extract list of persons and organizations using Stanford Named Entity Recognizer (NER) in Python NLTK.
When I run:</p>

<pre><code>from nltk.tag.stanford import NERTagger
st = NERTagger('/usr/share/stanford-ner/classifiers/all.3class.distsim.crf.ser.gz',
               '/usr/share/stanford-ner/stanford-ner.jar') 
r=st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
print(r) 
</code></pre>

<p>the output is:</p>

<pre><code>[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),
('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),
('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]
</code></pre>

<p>what I want is to extract from this list all persons and organizations in this form:</p>

<pre><code>Rami Eid
Sony Brook University
</code></pre>

<p>I tried to loop through the list of tuples:</p>

<pre><code>for x,y in i:
        if y == 'ORGANIZATION':
            print(x)
</code></pre>

<p>But this code only prints every entity one per line:</p>

<pre><code>Sony 
Brook 
University
</code></pre>

<p>With real data there can be more than one organizations, persons in one sentence, how can I put the limits between different entities?</p>
","python, nltk, stanford-nlp, named-entity-recognition","<p>Thanks to the <a href=""https://stackoverflow.com/questions/13765349/multi-term-named-entities-in-stanford-named-entity-recognizer"">link</a> discovered by @Vaulstein, it is clear that the trained Stanford tagger, as distributed (at least in 2012) <strong>does not chunk named entities</strong>. From <a href=""https://stackoverflow.com/a/13781588/699305"">the accepted answer</a>:</p>
<blockquote>
<p>Many NER systems use more complex labels such as IOB labels, where codes like B-PERS indicates where a person entity starts. The CRFClassifier class and feature factories support such labels, <strong>but they're not used in the models we currently distribute (as of 2012)</strong></p>
</blockquote>
<p>You have the following options:</p>
<ol>
<li><p>Collect runs of identically tagged words; e.g., all adjacent words tagged <code>PERSON</code> should be taken together as one named entity. That's very easy, but of course it will sometimes combine different named entities. (E.g. <code>New York, Boston [and] Baltimore</code> is about three cities, not one.)  <strong>Edit:</strong> This is what Alvas's code does in the accepted anwser. See below for a simpler implementation.</p>
</li>
<li><p>Use <code>nltk.ne_chunk()</code>. It doesn't use the Stanford recognizer but it does chunk entities. (It's a wrapper around an IOB named entity tagger).</p>
</li>
<li><p>Figure out a way to do your own chunking on top of the results that the Stanford tagger returns.</p>
</li>
<li><p>Train your own IOB named entity chunker (using the Stanford tools, or the NLTK's framework) for the domain you are interested in. If you have the time and resources to do this right, it will probably give you the best results.</p>
</li>
</ol>
<p><strong>Edit:</strong> If all you want is to pull out runs of continuous named entities (option 1 above), you should use <code>itertools.groupby</code>:</p>
<pre><code>from itertools import groupby
for tag, chunk in groupby(netagged_words, lambda x:x[1]):
    if tag != &quot;O&quot;:
        print(&quot;%-12s&quot;%tag, &quot; &quot;.join(w for w, t in chunk))
</code></pre>
<p>If <code>netagged_words</code> is the list of <code>(word, type)</code> tuples in your question, this produces:</p>
<pre class=""lang-none prettyprint-override""><code>PERSON       Rami Eid
ORGANIZATION Stony Brook University
LOCATION     NY
</code></pre>
<p>Note again that if two named entities of the same type occur right next to each other, this approach will combine them. E.g. <code>New York, Boston [and] Baltimore</code> is about three cities, not one.</p>
",30,27,27743,2015-06-05 10:49:58,https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk
How can the NamedEntityTag be used as EntityMention in RelationMention in the RelationExtractor?,"<p>I'm trying to train my own <code>NamedEntityRecognizer</code> and <code>RelationExtractor</code>. I've managed the <code>NER</code> model, but the integration with the <code>RelationExtractor</code> is a bit tricky. I get the right <code>NamedEntityTag</code>s, but the <code>RelationMention</code>s found by the are only one-term and with no extra <code>NamedEntity</code> than the default ones. I got input text: </p>

<blockquote>
  <p>America's President Nixon has passed a new law.</p>
</blockquote>

<p>and I got the following output:</p>

<pre><code>[Text=President CharacterOffsetBegin=10 CharacterOffsetEnd=19 PartOfSpeech=NNP Lemma=President NamedEntityTag=PRESIDENT]
[Text=Nixon CharacterOffsetBegin=20 CharacterOffsetEnd=25 PartOfSpeech=NNP Lemma=Nixon NamedEntityTag=PRESIDENT]

Extracted the following MachineReading relation mentions:
RelationMention [type=Live_In, start=0, end=4, {Live_In, 0.3719730539322602; OrgBased_In, 0.22490833335685645; _NR, 0.17474696244295865; Work_For, 0.11754788838692569; Located_In, 0.11082376188099895}
    EntityMention [type=O, objectId=EntityMention-2, hstart=3, hend=4, estart=2, eend=4, headPosition=3, value=""Nixon"", corefID=-1]
    EntityMention [type=LOCATION, objectId=EntityMention-1, hstart=0, hend=1, estart=0, eend=1, headPosition=0, value=""America"", corefID=-1]
]
</code></pre>
","nlp, stanford-nlp","<p>You can match the text from the EntityMention with the NamedEntityText value from the NamedEntityTag.</p>
",0,0,115,2015-06-07 15:35:24,https://stackoverflow.com/questions/30695372/how-can-the-namedentitytag-be-used-as-entitymention-in-relationmention-in-the-re
Stanford Parser: Get Integer value for CARD?,"<p>I am running a small test application using the Stanford Parser.</p>

<p>The parser correctly recognizes cardinals such as ""1990"", ""one"", ""two"", ""three"". I am looking for a way to retrieve the integer values for the annotated texts. Obviously this is especially of interest for the text that initially to not consist of digits like ""one"", ""two"" etc.</p>

<p>Is there a built in feature for this?</p>
","java, nlp, stanford-nlp","<p>The parser doesn't include anything like that but <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">CoreNLP</a> actually has such a functionality.</p>

<p>You can apply the following function to the <code>CoreMap</code> object of each sentence which adds the <code>NumerizedTokensAnnotation</code> to the sentence and the <code>NumericValueAnnotation</code> to each token. </p>

<pre><code>NumberNormalizer.findAndAnnotateNumericExpressions(sentence);
</code></pre>

<p>Unfortunately there doesn't exist any documentation of this feature but you can take a look at the source of <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/NumberNormalizer.java"" rel=""nofollow""><code>NumberNormalizer</code></a> which contains at least some comments and explanations.</p>
",2,1,85,2015-06-07 19:44:39,https://stackoverflow.com/questions/30697753/stanford-parser-get-integer-value-for-card
Lazy parsing with Stanford CoreNLP to get sentiment only of specific sentences,"<p><strong>I am looking for ways to optimize the performance of my Stanford CoreNLP sentiment pipeline. As a result, a want to get sentiment of sentences but only those which contain specific keywords given as an input.</strong></p>

<p>I have tried two approaches:</p>

<p><strong>Approach 1: StanfordCoreNLP pipeline annotating entire text with sentiment</strong></p>

<p>I have defined a pipeline of annotators: tokenize, ssplit, parse, sentiment. I have run it on entire article, then looked for keywords in each sentence and, if they were present, run a method returning keyword value. I was not satisfied though that processing takes a couple of seconds.</p>

<p>This is the code:</p>

<pre><code>List&lt;String&gt; keywords = ...;
String text = ...;
Map&lt;Integer,Integer&gt; sentenceSentiment = new HashMap&lt;&gt;();

Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
props.setProperty(""parse.maxlen"", ""20"");
props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

Annotation annotation = pipeline.process(text); // takes 2 seconds!!!!
List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
for (int i=0; i&lt;sentences.size(); i++) {
    CoreMap sentence = sentences.get(i);
    if(sentenceContainsKeywords(sentence,keywords) {
        int sentiment = RNNCoreAnnotations.getPredictedClass(sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class));
        sentenceSentiment.put(sentence,sentiment);
    }
}
</code></pre>

<p><strong>Approach 2: StanfordCoreNLP pipeline annotating entire text with sentences, separate annotators running on sentences of interest</strong></p>

<p>Because of the weak performance of the first solution, I have defined the second solution. I have defined a pipeline with annotators: tokenize, ssplit. I looked for keywords in each sentence and, if they were present, I have created an annotation only for this sentence and run next annotators on it: ParserAnnotator, BinarizerAnnotator and SentimentAnnotator.</p>

<p>The results were really unsatisfying because of ParserAnnotator. Even if I initialized it with the same properties. Sometimes it took even more time than entire pipeline run on a document in Approach 1.</p>

<pre><code>List&lt;String&gt; keywords = ...;
String text = ...;
Map&lt;Integer,Integer&gt; sentenceSentiment = new HashMap&lt;&gt;();

Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit""); // parsing, sentiment removed
props.setProperty(""parse.maxlen"", ""20"");
props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// initiation of annotators to be run on sentences
ParserAnnotator parserAnnotator = new ParserAnnotator(""pa"", props);
BinarizerAnnotator  binarizerAnnotator = new BinarizerAnnotator(""ba"", props);
SentimentAnnotator sentimentAnnotator = new SentimentAnnotator(""sa"", props);

Annotation annotation = pipeline.process(text); // takes &lt;100 ms
List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
for (int i=0; i&lt;sentences.size(); i++) {
    CoreMap sentence = sentences.get(i);
    if(sentenceContainsKeywords(sentence,keywords) {
        // code required to perform annotation on one sentence
        List&lt;CoreMap&gt; listWithSentence = new ArrayList&lt;CoreMap&gt;();
        listWithSentence.add(sentence);
        Annotation sentenceAnnotation  = new Annotation(listWithSentence);

        parserAnnotator.annotate(sentenceAnnotation); // takes 50 ms up to 2 seconds!!!!
        binarizerAnnotator.annotate(sentenceAnnotation);
        sentimentAnnotator.annotate(sentenceAnnotation);
        sentence = sentenceAnnotation.get(CoreAnnotations.SentencesAnnotation.class).get(0);

        int sentiment = RNNCoreAnnotations.getPredictedClass(sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class));
        sentenceSentiment.put(sentence,sentiment);
    }
}
</code></pre>

<p><strong>Questions</strong></p>

<ol>
<li><p>I wonder why parsing in CoreNLP is not ""lazy""? (In my example that would mean: performed only when sentiment on a sentence is called). Is it from performance reasons?</p></li>
<li><p>How come a parser for one sentence can work almost as long as a parser for entire article (my article had 7 sentences)? Is it possible to configure it in a way that it works faster?</p></li>
</ol>
","java, performance, parsing, stanford-nlp, sentiment-analysis","<p>If you're looking to speed up constituency parsing, the single best improvement is to use the new <a href=""http://nlp.stanford.edu/software/srparser.shtml"" rel=""nofollow"">shift-reduce constituency parser</a>. It is orders of magnitude faster than the default PCFG parser.</p>

<p>Answers to your later questions:</p>

<ol>
<li><em>Why is CoreNLP parsing not lazy?</em> This is certainly possible, but not something that we've implemented yet in the pipeline. We likely haven't seen many use cases in-house where this is necessary. We will happily accept a contribution of a ""lazy annotator wrapper"" if you're interested in making one!</li>
<li><em>How come a parser for one sentence can work almost as long as a parser for an entire article?</em> The default Stanford PCFG parser is <a href=""https://en.wikipedia.org/wiki/CYK_algorithm"" rel=""nofollow"">cubic time complexity</a> with respect to the sentence length. This is why we usually recommend restricting the maximum sentence length for performance reasons. The shift-reduce parser, on the other hand, runs in linear time with respect to the sentence length.</li>
</ol>
",5,3,1346,2015-06-08 16:40:10,https://stackoverflow.com/questions/30714693/lazy-parsing-with-stanford-corenlp-to-get-sentiment-only-of-specific-sentences
Stanford Parser - Factored model and PCFG,"<p>What is the difference between the factored and PCFG models of stanford parser? (In terms of theoretical working and mathematical perspective)</p>
","parsing, nlp, stanford-nlp, sentiment-analysis, text-analysis","<p><a href=""http://nlp.stanford.edu/software/parser-faq.shtml#y"" rel=""nofollow"">This FAQ answer</a> explains the difference in a long paragraph. Relevant parts are quoted below:</p>

<blockquote>
  <p><strong>Can you explain the different parsers?</strong></p>
  
  <p>This answer is specific to English. It mostly applies to other languages although some components are missing in some languages. The file <code>englishPCFG.ser.gz</code> comprises just an unlexicalized PCFG grammar. It is basically the parser described in the ACL 2003 Accurate Unlexicalized Parsing paper.</p>
  
  <p>… The file <code>englishFactored.ser.gz</code> contains two grammars and leads the system to run three parsers. It first runs a (simpler) PCFG parser and then an untyped dependency parser, and then runs a third parser which finds the parse with the best joint score across the two other parsers via a product model. This is described in the NIPS Fast Exact Inference paper.</p>
  
  <p>… For English, although the grammars and parsing methods differ, the average quality of <code>englishPCFG.ser.gz</code> and <code>englishFactored.ser.gz</code> is similar, and so many people opt for the faster <code>englishPCFG.ser.gz</code>, though <code>englishFactored.ser.gz</code> sometimes does better because it does include lexicalization. For other languages, the factored models are considerably better than the PCFG models, and are what people generally use.</p>
</blockquote>

<p>There are links to the papers referenced on <a href=""http://nlp.stanford.edu/software/lex-parser.shtml#Citing"" rel=""nofollow"">the main parser page</a>.</p>
",2,1,575,2015-06-09 03:43:20,https://stackoverflow.com/questions/30722624/stanford-parser-factored-model-and-pcfg
Get list of annotators in Stanford CoreNLP,"<p>I'm customizing Stanford CoreNLP by adding some new Annotators, each one with its requirements. Is there a way to get the list of requirements and satisfactions from the StanfordCoreNLP object?</p>

<p>For example, I instantiate the CoreNLP object:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>I'd like to know, starting from <code>pipeline</code>: (i) there is four annotators (tokenize, ssplit, pos, lemma); (ii) pos depends on tokenize and ssplit, lemma depends on tokenize, ssplit and pos, and so on.</p>

<p>Is it possible?</p>
",stanford-nlp,"<p>Looking at <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP.java"" rel=""nofollow"">the code</a> behind the pipeline, it looks it's not currently possible to get the list of annotators enabled for an already-constructed pipeline (i). All of the relevant members storing this information are private.</p>

<p>You could probably hack something up to get the annotator dependencies (ii), but it wouldn't be pretty. See how the core code does this in the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP.java#L262"" rel=""nofollow""><code>StanfordCoreNLP</code> implementation</a>.</p>
",1,0,569,2015-06-10 12:05:57,https://stackoverflow.com/questions/30755856/get-list-of-annotators-in-stanford-corenlp
Separately tokenizing and pos-tagging with CoreNLP,"<p>I'm having few problems with the way Stanford CoreNLP divides text into sentences, namely:</p>

<ol>
<li>It treats ! and ? (exclamation and question marks) inside a quoted text as a sentence end where it shouldn't, e.g.: He shouted ""Alice! Alice!"" - here it treats the ! after the first Alice as a sentence end and divides the text into two sentences.</li>
<li>It doesn't recognize ellipses as a sentence end.</li>
</ol>

<p>In NLTK we would deal with these problems by simply normalizing text before and after dividing into sentences, that is, replacing the said marks with other symbols before dividing and returning them after to send them down the pipeline in a proper form. </p>

<p>However, the tokenizer in CoreNLP tokenizes before dividing into sentences and that doesn't leave much room to tweak the process. So, my first question: is it possible to ""correct"" the tokenizer without rewriting it to account for such cases?</p>

<p>If it's not, can we at least separate tokenization from the rest of the pipeline (in my case it's pos, lemma, and parse), so that we can change the tokens themselves before sending them further down?</p>

<p>Thanks!</p>
","java, nlp, stanford-nlp","<p>It seems to me that you would be better off separating the tokenization phase from your other downstream tasks (so I'm basically answering Question 2). You have two options:</p>

<ol>
<li><p>Tokenize using the Stanford tokenizer (example from Stanford CoreNLP usage page). The annotators options should only take 'tokenizer' in your case.</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</code></pre>

<p>Once you do this, you can ask the other modules to not tokenize your input. For example, the Stanford Parser has a command-line flag (-tokenized) which you can set to indicate that your input is already tokenized.</p></li>
<li><p>Use a different tokenizer (say, NLTK) to tokenize, and follow the second part of 1.</p></li>
</ol>

<p>Infact, if you use any extrinsic tool to split text into sentences (basically chunks that you don't want to split any further), you have the option of setting a command-line flag in the CoreNLP tools which will not try and split your input. Again for the Stanford Parser, this is done by using the ""-sentences newline"" flag. This is probably the easiest thing to do, provided you have a reliable sentence detector.</p>
",3,3,1234,2015-06-12 17:18:58,https://stackoverflow.com/questions/30808806/separately-tokenizing-and-pos-tagging-with-corenlp
Stanford Segmenter: How to generate Arabic words segments along with tokens/segments char start offsets and lengths?,"<p>Using Stanford Arabic word segmenter, We want to tokenize and segment Arabic text. The ArabicSegmenter does that successfully but the result does not hold the original char offset of the tokens (CoreLabels). Also, the result from ArabicTokenizer, used in ArabicSegmenter class (3.5.2), has all tokens with char offset (i.e. beginning) set to 0.</p>

<p>How to get the Arabic word segments along with the char offset of the resulted CoreLables? </p>
",stanford-nlp,"<p>Character offsets of 0 from <code>ArabicTokenizer</code> are due to a bug that is now fixed in the <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">Github repository</a>. The fix will be included in the next CoreNLP release (3.5.3, scheduled for mid-October 2015). With this fix the <code>CoreLabel</code>s returned from <code>tokenize</code> should be annotated with correct indices into the original source text, as shown in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/test/src/edu/stanford/nlp/international/arabic/process/ArabicTokenizerTest.java#L146"" rel=""nofollow"">this test</a>.</p>

<p>For the time being, <code>ArabicSegmenter</code> remains a string-to-string interface, which means there probably isn't a clean way to retrieve the original character offsets from it.</p>
",2,1,163,2015-06-13 00:25:56,https://stackoverflow.com/questions/30813930/stanford-segmenter-how-to-generate-arabic-words-segments-along-with-tokens-segm
Chinese sentence segmenter with Stanford coreNLP,"<p>I'm using the Stanford coreNLP system with the following command:</p>

<pre><code>java -cp stanford-corenlp-3.5.2.jar:stanford-chinese-corenlp-2015-04-20-models.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -annotators segment,ssplit -file input.txt
</code></pre>

<p>And this is working great on small chinese texts. However, I need to train a MT system which just requires me to segment my input. So I just need to use <code>-annotators segment</code>, but with this parameters the system outputs an empty file. I could run the tool using the <code>ssplit</code> annotator as well but I don't want to do that because my input is a parallel corpora that contains one sentence by line already, and the ssplit will probably not split sentences perfectly and create problems in the parallel data.</p>

<p>Is there a way to tell the system to do the segmentation only, or to tell it that the input already contains a sentence by line exactly?</p>
","java, nlp, tokenize, stanford-nlp","<p>Using Stanford Segmenter instead:</p>

<pre><code>$ wget http://nlp.stanford.edu/software/stanford-segmenter-2015-04-20.zip
$ unzip stanford-segmenter-2015-04-20.zip
$ echo ""应有尽有的丰富选择定将为您的旅程增添无数的赏心乐事"" &gt; input.txt
$ bash stanford-segmenter-2015-04-20/segment.sh ctb input.txt UTF-8 0 &gt; output.txt
$ cat output.txt
应有尽有 的 丰富 选择 定 将 为 您 的 旅程 增添 无数 的 赏心 乐事
</code></pre>

<p>Other than Stanford Segmenter, there are many other segmenter might be more suitable, see <a href=""https://stackoverflow.com/questions/5834371/is-there-any-good-open-source-or-freely-available-chinese-segmentation-algorithm"">Is there any good open-source or freely available Chinese segmentation algorithm available?</a></p>

<hr>

<p>To continue using the Stanford NLP tools for pos tagging:</p>

<pre><code>$ wget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip
$ unzip stanford-postagger-full-2015-04-20.zip
$ cd stanford-postagger-full-2015-01-30/
$ echo ""应有尽有 的 丰富 选择 定 将 为 您 的 旅程 增添 无数 的 赏心 乐事"" &gt; input.txt
$ bash stanford-postagger.sh models/chinese-distsim.tagger input.txt &gt; output.txt
$ cat output.txt 
应有尽有#VV 的#DEC 丰富#JJ 选择#NN 定#VV 将#AD 为#P 您#PN 的#DEG 旅程#NN 增添#VV 无数#CD 的#DEG 赏心#NN 乐事#NN
</code></pre>
",3,2,1453,2015-06-15 05:43:40,https://stackoverflow.com/questions/30838070/chinese-sentence-segmenter-with-stanford-corenlp
How to use Stanford LexParser for Chinese text?,"<p>I can't seem to get the correct input encoding for <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">Stanford NLP's LexParser</a>. </p>

<p><strong>How do I use the Stanford LexParser for Chinese text?</strong></p>

<p>I've done the following to download the tool:</p>

<pre><code>$ wget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip
$ unzip stanford-parser-full-2015-04-20.zip 
$ cd stanford-parser-full-2015-04-20/
</code></pre>

<p>And my input text is in <code>UTF-8</code>:</p>

<pre><code>$ echo ""应有尽有 的 丰富 选择 定 将 为 您 的 旅程 增添 无数 的 赏心 乐事 。"" &gt; input.txt

$ echo ""应有尽有#VV 的#DEC 丰富#JJ 选择#NN 定#VV 将#AD 为#P 您#PN 的#DEG 旅程#NN 增添#VV 无数#CD 的#DEG 赏心#NN 乐事#NN  。#PUNCT"" &gt; pos-input.txt
</code></pre>

<p>According to the <code>README.txt</code>, the parser was trained on:</p>

<blockquote>
  <p>Chinese 
  There are Chinese grammars trained just on mainland material
  from Xinhua and more mixed material from the LDC Chinese Treebank. The
  default input encoding is GB18030.</p>
</blockquote>

<p>So I've tried with the <code>UTF-8</code> file first:</p>

<pre><code>$ bash lexparser-lang.sh Chinese 80 edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz parsed input.txt
Loading parser from serialized file edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz ...  done [1.0 sec].
Parsing file: input.txt
Parsing [sent. 1 len. 16]: 应有尽有 的1�7 丰富 选择 宄1�7 射1�7 丄1�7 悄1�7 的1�7 旅程 增添 无数 的1�7 赏心 乐事 〄1�7
Parsed file: input.txt [1 sentences].
Parsed 16 words in 1 sentences (21.00 wds/sec; 1.31 sents/sec).
</code></pre>

<p>It didn't seem to work. The parser produced this file, <code>input.txt.parsed.80.stp</code></p>

<p>[out]:</p>

<pre><code>$ cat input.txt.parsed.80.stp 
(FRAG (NR 应有尽有) (NR 的1�7) (NT 丰富) (NT 选择) (NN 宄1�7) (NN 射1�7) (NN 丄1�7) (NN 悄1�7) (NR 的1�7) (NT 旅程) (NT 增添) (NN 无数) (NN 的1�7) (NR 赏心) (NR 乐事) (VV 〄1�7))
</code></pre>

<p>Then i'ved tried to encode the sentence into GB18030:</p>

<pre><code>$ bash lexparser-lang.sh Chinese 80 edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz parsed input-gb18030.txt
Loading parser from serialized file edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz ...  done [1.0 sec].
Parsing file: input-gb18030.txt
Parsing [sent. 1 len. 16]: Ӧ�о��� �� �ḻ ѡ�� �� �� Ϊ �� �� �ó� ���� ���� �� ���� ���� ��
Parsed file: input-gb18030.txt [1 sentences].
Parsed 16 words in 1 sentences (19.90 wds/sec; 1.24 sents/sec).
alvas@ubi:~/stanford-parser-full-2015-04-20$ cat input-gb18030.txt.parsed.80.stp 
(IP
  (NP
    (CP
      (IP
        (VP (VV Ӧ�о���)))
      (DEC ��))
    (ADJP (JJ �ḻ))
    (NP (NN ѡ��)))
  (VP (VV ��)
    (VP
      (ADVP (AD ��))
      (PP (P Ϊ)
        (NP
          (DNP
            (NP (PN ��))
            (DEG ��))
          (NP (NN �ó�))))
      (VP (VV ����)
        (NP
          (DNP
            (ADJP (JJ ����))
            (DEG ��))
          (NP (NN ����) (NN ����))))))
  (PU ��))
</code></pre>

<p>It seems like it's working but <strong>how do I convert the file back into UTF8?</strong></p>

<p>I've tried this but it didn't work:</p>

<pre><code>$ cat input-gb18030.txt.parsed.80.stp | python -c ""print raw_input().decode('GB18030').encode('utf8')""
(IP
</code></pre>

<p>Here's some concluding question:</p>

<ul>
<li><strong>How do I convert between GB18030 to UTF8 and UTF8 to GB18030?</strong></li>
<li><strong>How do I use the Stanford LexParser for Chinese UTF8 text?</strong></li>
</ul>
","encoding, utf-8, nlp, stanford-nlp","<p>I followed your steps and it shows that you can simply use encoding convertors to achieve your goal.</p>

<p>I use <code>iconv</code> in my testing.</p>

<pre><code>iconv -f GB18030 -t UTF-8 input2.txt.parsed.80.stp -o output
</code></pre>

<p>Here is my output:</p>

<pre><code>dmk@dmk-debian /t/stanford-parser-full-2015-04-20 ❯❯❯ cat input2.txt.parsed.80.stp
(IP
  (NP
    (CP
      (IP
        (VP (VV Ӧ�о���)))
      (DEC ��))
    (ADJP (JJ �ḻ))
    (NP (NN ѡ��)))
  (VP (VV ��)
    (VP
      (ADVP (AD ��))
      (PP (P Ϊ)
        (NP
          (DNP
            (NP (PN ��))
            (DEG ��))
          (NP (NN �ó�))))
      (VP (VV ����)
        (NP
          (DNP
            (ADJP (JJ ����))
            (DEG ��))
          (NP (NN ����) (NN ����))))))
  (PU ��))

dmk@dmk-debian /t/stanford-parser-full-2015-04-20 ❯❯❯ iconv -f GB18030 -t UTF-8 input2.txt.parsed.80.stp -o output
dmk@dmk-debian /t/stanford-parser-full-2015-04-20 ❯❯❯ cat output
(IP
  (NP
    (CP
      (IP
        (VP (VV 应有尽有)))
      (DEC 的))
    (ADJP (JJ 丰富))
    (NP (NN 选择)))
  (VP (VV 定)
    (VP
      (ADVP (AD 将))
      (PP (P 为)
        (NP
          (DNP
            (NP (PN 您))
            (DEG 的))
          (NP (NN 旅程))))
      (VP (VV 增添)
        (NP
          (DNP
            (ADJP (JJ 无数))
            (DEG 的))
          (NP (NN 赏心) (NN 乐事))))))
  (PU 。))
</code></pre>
",1,2,468,2015-06-15 10:47:47,https://stackoverflow.com/questions/30843237/how-to-use-stanford-lexparser-for-chinese-text
How to extract an unlabelled/untyped dependency tree from a TreeAnnotation using Stanford CoreNLP?,"<p>The target language is Spanish.</p>

<p>The English pipeline has support for typed dependencies whereas the Spanish pipeline, to my knowledge, does not.</p>

<p>The goal is to produce a dependency tree from a TreeAnnotation where the end result is a list of directed edges. Is this possible with CoreNLP 3.4.1 and using Spanish models, if so: how?</p>

<h2>Background</h2>

<p>I'm using Stanford CoreNLP 3.4.1 + (3.5.0 Spanish models for POS tagging) (Due to compatibility reasons, Java 8 cannot be used yet) with the following configuration:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, ner, parse"");
props.setProperty(""tokenize.options"", ""invertible=true,ptb3Escaping=true"");
props.setProperty(""tokenize.language"", ""es"");

props.setProperty(""pos.model"", ""edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger"");
props.setProperty(""ner.model"", ""edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz"");

props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/spanishSR.ser.gz""); //Stanford Parser 3.4.1 shift-reduce models for Spanish. 

props.setProperty(""ner.applyNumericClassifiers"", ""false"");
props.setProperty(""ner.useSUTime"", ""false"");
</code></pre>

<p>Which is then used to create the pipeline and run annotation of a document.</p>

<pre><code>StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
pipeline.annotate(document);

List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

for(CoreMap sentence: sentences) {

    // ... extract start, end position of sentence ...

    for (CoreLabel token: sentence.get(CoreAnnotations.TokensAnnotation.class)) {

        // ... extract POS tags, NER annotations, id ...
    }

    //This works, and I have a tree that is not empty.
    Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
}
</code></pre>

<p>By using a debugger I was able to examine both sentences and tokens and conclude that they have the following content:</p>

<h2>Sentence (keys)</h2>

<p>From edu.stanford.nlp.ling.CoreAnnotations:</p>

<ul>
<li>TextAnnotation</li>
<li>CharacterOffsetBeginAnnotation</li>
<li>CharacterOffsetEndAnnotation</li>
<li>TokensAnnotation</li>
<li>TokenBeginAnnotation</li>
<li>TokenEndAnnotation</li>
<li>SentenceIndexAnnotation</li>
</ul>

<p>From edu.stanford.nlp.trees.TreeCoreAnnotations</p>

<ul>
<li>TreeAnnotation</li>
</ul>

<h2>Tokens (keys)</h2>

<p>From edu.stanford.nlp.ling.CoreAnnotations</p>

<ul>
<li>TextAnnotation</li>
<li>OriginalTextAnnotation</li>
<li>CharacterOffsetBeginAnnotation</li>
<li>CharacterOffsetEndAnnotation</li>
<li>BeforeAnnotation</li>
<li>AfterAnnotation</li>
<li>IndexAnnotation</li>
<li>SentenceIndexAnnotation</li>
<li>PartOfSpeechAnnotation</li>
<li>NamedEntityTagAnnotation</li>
</ul>

<p>From edu.stanford.nlp.trees.TreeCoreAnnotations</p>

<ul>
<li>HeadWordAnnotation - In my experiments: this one always points to itself, i.e. the token where the annotation is retrieved from.</li>
<li>HeadTagAnnotation</li>
</ul>

<p>Thanks in advance!</p>
","java, stanford-nlp","<p>There is no support for Spanish dependency parsing in CoreNLP at the moment. This includes typed dependency conversion from constituency parses.</p>

<p>There is a head finder implemented (but not fully tested). You could hack an untyped dependency converter using this head finder, but we have no guarantees that this will yield a sensible parse.</p>
",1,1,411,2015-06-15 14:07:18,https://stackoverflow.com/questions/30847297/how-to-extract-an-unlabelled-untyped-dependency-tree-from-a-treeannotation-using
What is the default behavior of Stanford NLP&#39;s WordsToSentencesAnnotator when splitting a text into sentences?,"<p>Looking at <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/WordToSentenceProcessor.java"" rel=""nofollow"">WordToSentenceProcessor.java</a>, <code>DEFAULT_BOUNDARY_REGEX = ""\\.|[!?]+"";</code> led me to think that the text would get split into sentences based on <code>.</code>, <code>!</code> and <code>?</code>.</p>

<p>However, if I pass the string <code>D R E L I N. Okay.</code> as input, e.g. using the command line interface:</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP 
             -annotators tokenize,ssplit  -file test.txt
</code></pre>

<p>It returns just one sentence <code>D R E L I N. Okay.</code> instead of  two sentences <code>['D R E L I N.',  'Okay.']</code>. Why?</p>
","nlp, stanford-nlp","<p>It does split on these characters, however only when they appear as their own token and not at the end of an abbreviation such as in ""etc."". So the issue here is not the sentence splitter but the tokenizer which thinks that ""N."" is an abbreviation and therefore does not split ""N."" into two separate tokens. If you know in advance that your text doesn't contain any abbreviations the easiest thing to do is to split all tokens that contain a period at the end before you process them with CoreNLP.</p>

<p>Your input would then be <code>D R E L I N . Okay .</code></p>

<p>In case your input also contains abbreviations, things are a bit more complicated as you'll have to edit the rules of the tokenizer. See ""<a href=""https://stackoverflow.com/questions/30334991/stanford-corenlp-splitting-sentences-abbreviation-exceptions"">stanford corenlp, splitting sentences, abbreviation exceptions</a>"" for a high-level description on how to edit the rules of the tokenizer.</p>
",2,1,119,2015-06-16 04:31:46,https://stackoverflow.com/questions/30858844/what-is-the-default-behavior-of-stanford-nlps-wordstosentencesannotator-when-sp
Coreference resolution using Stanford CoreNLP,"<p>I am new to the Stanford CoreNLP toolkit and trying to use it for a project to resolve coreferences in news texts. In order to use the Stanford CoreNLP coreference system, we would usually create a pipeline, which requires tokenization, sentence splitting, part-of-speech tagging, lemmarization, named entity recoginition and parsing. For example:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// read some text in the text variable
String text = ""As competition heats up in Spain's crowded bank market, Banco Exterior de Espana is seeking to shed its image of a state-owned bank and move into new activities."";

// create an empty Annotation just with the given text
Annotation document = new Annotation(text);

// run all Annotators on this text
pipeline.annotate(document);
</code></pre>

<p>Then we can easily get the sentence annotations with:</p>

<pre><code>List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
</code></pre>

<p>However, I am using other tools for for preprocessing and just need a stand-alone coreference resolution system. It is pretty easy to create tokens and parse tree annotations and set them to the annotation:</p>

<pre><code>// create new annotation
Annotation annotation = new Annotation();

// create token annotations for each sentence from the input file
List&lt;CoreLabel&gt; tokens = new ArrayList&lt;&gt;();
for(int tokenCount = 0; tokenCount &lt; parsedSentence.size(); tokenCount++) {

        ArrayList&lt;String&gt; parsedLine = parsedSentence.get(tokenCount);
        String word = parsedLine.get(1);
        String lemma = parsedLine.get(2);
        String posTag = parsedLine.get(3);
        String namedEntity = parsedLine.get(4); 
        String partOfParseTree = parsedLine.get(6);

        CoreLabel token = new CoreLabel();
        token.setWord(word);
        token.setWord(lemma);
        token.setTag(posTag);
        token.setNER(namedEntity);
        tokens.add(token);
    }

// set tokens annotations to annotation
annotation.set(TokensAnnotation.class, tokens);

// set parse tree annotations to annotation
Tree stanfordParseTree = Tree.valueOf(inputParseTree);
annotation.set(TreeAnnotation.class, stanfordParseTree);
</code></pre>

<p>However, creating sentence annotations is pretty tricky, because to my knowledge there is no document to explain it in full detail. I am able to create the data structure for the sentence annotations and set it to the annotation:</p>

<pre><code>List&lt;CoreMap&gt; sentences = new ArrayList&lt;CoreMap&gt;();
annotation.set(SentencesAnnotation.class, sentences);
</code></pre>

<p>I am sure it cannot be that difficult, but there is no documentation on how to create sentence annotation from tokens annotations, i.e. how to fill the ArrayList with actual sentence annotations.</p>

<p>Any ideas?</p>

<p>Btw, if I use the tokens and parse tree annotations provided by my processing tools and only use the sentence annotations provided by the StanfordCoreNLP pipeline and apply the StanfordCoreNLP stand-alone coreference resolution system I am getting the correct results. So the only part missing for a complete stand-alone coreference resolution system is the ability to create the sentence annotations from the tokens annotations.</p>
","java, nlp, stanford-nlp","<p>There is a <code>Annotation</code> <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/Annotation.java#L92"" rel=""noreferrer"">constructor</a> with a <code>List&lt;CoreMap&gt; sentences</code> argument which sets up the document if you have a list of already tokenized sentences.</p>

<p>For each sentence you want to create a <code>CoreMap</code> object as following.
(Note that I also added a sentence and token index to each sentence and token object, respectively.)</p>

<pre><code>int sentenceIdx = 1;
List&lt;CoreMap&gt; sentences = new ArrayList&lt;CoreMap&gt;();
for (parsedSentence : parsedSentences) {
    CoreMap sentence = new CoreLabel();
    List&lt;CoreLabel&gt; tokens = new ArrayList&lt;&gt;();
    for(int tokenCount = 0; tokenCount &lt; parsedSentence.size(); tokenCount++) {

        ArrayList&lt;String&gt; parsedLine = parsedSentence.get(tokenCount);
        String word = parsedLine.get(1);
        String lemma = parsedLine.get(2);
        String posTag = parsedLine.get(3);
        String namedEntity = parsedLine.get(4); 
        String partOfParseTree = parsedLine.get(6);

        CoreLabel token = new CoreLabel();
        token.setWord(word);
        token.setLemma(lemma);
        token.setTag(posTag);
        token.setNER(namedEntity);
        token.setIndex(tokenCount + 1);
        tokens.add(token);
    }

    // set tokens annotations and id of sentence 
    sentence.set(TokensAnnotation.class, tokens);
    sentence.set(SentenceIndexAnnotation.class, sentenceIdx++);

    // set parse tree annotations to annotation
    Tree stanfordParseTree = Tree.valueOf(inputParseTree);
    sentence.set(TreeAnnotation.class, stanfordParseTree);

    // add sentence to list of sentences
    sentences.add(sentence);
}
</code></pre>

<p>Then you can create an <code>Annotation</code> instance with the <code>sentences</code> list:</p>

<pre><code>Annotation annotation = new Annotation(sentences);
</code></pre>
",5,1,2038,2015-06-20 13:43:21,https://stackoverflow.com/questions/30954649/coreference-resolution-using-stanford-corenlp
How configure Stanford QNMinimizer to get similar results as scipy.optimize.minimize L-BFGS-B,"<p>I want to configurate the QN-Minimizer from Stanford Core NLP Lib to get nearly similar optimization results as scipy optimize L-BFGS-B implementation or get a standard L-BFSG configuration that is suitable for the most things. I set the standard paramters as follow:</p>

<p>The python example I want to copy:</p>

<pre><code>scipy.optimize.minimize(neuralNetworkCost, input_theta, method = 'L-BFGS-B', jac = True)
</code></pre>

<p>My try to do the same in Java:</p>

<pre><code>QNMinimizer qn = new QNMinimizer(10,true) ;
qn.terminateOnMaxItr(batch_iterations);
//qn.setM(10);
output = qn.minimize(neuralNetworkCost, 1e-5, input,15000);
</code></pre>

<p>What I need is a solid and general L-BFSG configuration, that is suitable to solve most problems.</p>

<p>I m also not sure, if I need to set some of these parameters for standard L-BFGS configuration:</p>

<pre><code>useAveImprovement = ?;
useRelativeNorm = ?;
useNumericalZero = ?;
useEvalImprovement = ?; 
</code></pre>

<p>Thanks for your help in advance, I m new on that field.</p>

<p>Resources for Information:
<strong>Stanford Core NLP QNMinimizer:</strong>
<a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.2/edu/stanford/nlp/optimization/QNMinimizer.html#setM-int-"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.2/edu/stanford/nlp/optimization/QNMinimizer.html#setM-int-</a>
<a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/optimization/QNMinimizer.java"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/optimization/QNMinimizer.java</a></p>

<p><strong>Scipy Optimize L-BFGS-B:</strong>
<a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"" rel=""nofollow"">http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html</a>
<a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_l_bfgs_b.html"" rel=""nofollow"">http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_l_bfgs_b.html</a></p>

<p>Thanks in advance!</p>
","java, optimization, machine-learning, scipy, stanford-nlp","<p>What you have should be just fine. (Have you actually had any problems with it?)</p>

<p>Setting termination both on max iterations and max function evaluations is probably overkill, so you might omit the last argument to <code>qn.minimize()</code>, but it seems from the documentation that scipy does use both with a default value of 15000.</p>

<p>In general, using the robustOptions (with a second argument of <code>true</code> as you do) should give a reliable minimizer, similar to the <code>pgtol</code> convergence criterion of scipy. The other options are for special situations or just to experiment with how they work.</p>
",2,1,452,2015-06-21 14:22:53,https://stackoverflow.com/questions/30965541/how-configure-stanford-qnminimizer-to-get-similar-results-as-scipy-optimize-mini
How do I access individual nodes in the dependency tree and constituency tree returned by the Stanford Parser?,"<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

using java.io;
using edu.stanford.nlp.process;
using edu.stanford.nlp.ling;
using edu.stanford.nlp.trees;
using edu.stanford.nlp.parser.lexparser;
using Console = System.Console;

namespace Parser
{   

    class Parser
    {
        //loads the lexical parser
        private static LexicalizedParser LoadLexicalizedParser()
        {
            // Path to models extracted from `stanford-parser-3.5.2-models.jar`
            var jarRoot = @""E:\Project\stanford-parser-full-2015-04-20\stanford-parser-3.5.2-models"";
            var modelsDirectory = jarRoot + @""\edu\stanford\nlp\models"";

            // Loading english PCFG parser from file
            var lp = LexicalizedParser.loadModel(modelsDirectory + @""\lexparser\englishPCFG.ser.gz"");

            return lp;
        }

        //gets the lexical tree for a 'sentence'
        private static Tree GetLexicalTree(LexicalizedParser lp, string sentence)
        {
            string[] words = sentence.Split(' ');   
            // This sample shows parsing a list of correctly tokenized words            
            var rawWords = Sentence.toCoreLabelList(words);
            var tree = lp.apply(rawWords);

            return tree;
        }

        //gets the constituency tree from the lexical 'tree' as a string
        private static string GetConstituencyTree(Tree tree)
        {
            return tree.pennString();
        }

        //gets the dependency tree from the lexical 'tree' as a string
        private static string GetDependencyTree(Tree tree)
        {
            // Extract dependencies from lexical tree
            var tlp = new PennTreebankLanguagePack();
            var gsf = tlp.grammaticalStructureFactory();
            var gs = gsf.newGrammaticalStructure(tree);
            var tdl = gs.typedDependenciesCCprocessed();

            string dependencyTree = String.Empty;

            for (int i = 0; i &lt; tdl.size(); ++i)
                dependencyTree += tdl.get(i) + ""\n"";

            return dependencyTree;
        }


        static void Main()
        {
            var lp = LoadLexicalizedParser();
            string sentence = ""This is an easy sentence."";

            Tree tree = GetLexicalTree(lp, sentence);

            string constituencyTree = GetConstituencyTree(tree);
            string dependencyTree = GetDependencyTree(tree);

            Console.WriteLine(""Constituency Tree\n"" + constituencyTree);
            Console.WriteLine(""Dependency Tree\n"" + dependencyTree);

            //// Extract collapsed dependencies from parsed tree
            //var tp = new TreePrint(""penn,typedDependenciesCollapsed"");
            //tp.printTree(tree);
        }
    }
}
</code></pre>

<p>In this code, I am getting the constituency tree and dependency tree as strings.
But I want to use them using the 'Tree' type itself, ie. I want to access and manipulate the nodes of the variable 'tree'. Is there any way I can do that?
Or do I have to create my own tree data structure and get the individual nodes by processing the strings('constituencyTree' &amp; 'dependencyTree')?</p>

<p>[I need this for a small project that I am doing currently.]</p>
","c#, stanford-nlp, text-parsing","<p>Yes, there exist plenty of data structures to work with constituency trees and dependency trees. For constituency trees, you want to work with the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html"" rel=""nofollow""><code>Tree</code></a> data structure which has many useful built-in functions to traverse trees, get all the terminal nodes, etc. </p>

<p>For dependency trees you can either work with a list of <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/TypedDependency.html"" rel=""nofollow""><code>TypedDependency</code></a> objects where each <code>TypedDependency</code> represents the relation between a governor word and a dependent word, or you can work with a <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html"" rel=""nofollow""><code>SemanticGraph</code></a>. To convert the list of <code>TypedDependency</code> which you named <code>tdl</code> in your example to a <code>SemanticGraph</code>, just pass the list to the constructor:</p>

<pre><code>SemanticGraph sg = new SemanticGraph(tdl);
</code></pre>
",3,2,699,2015-06-24 09:09:46,https://stackoverflow.com/questions/31022381/how-do-i-access-individual-nodes-in-the-dependency-tree-and-constituency-tree-re
Online training for Stanford NER,"<p>I understand that Stanford NER only supports training through a file... is there a way to <em>add more training data</em> at a later stage to <em>update</em> the NER model once it is already trained?</p>

<p>I understand that I can keep all the training datasets from the past and re-train the model, but, I am wondering if there is a way to <em>update</em> the NER model rather than <em>retrain</em> it from scratch.</p>
",stanford-nlp,"<p>For the larger audience: StanfordNER does not support Online Training. Marking this question as closed.</p>
",0,0,163,2015-06-24 18:41:26,https://stackoverflow.com/questions/31034462/online-training-for-stanford-ner
Does the Stanford NLP Parser have methods for semantic role labelling?,"<p>I'm trying to find the semantic labels of english sentences.
I am using the Stanford NLP parser. Does it have methods for this?</p>

<p>I was going through the documentation but the closest thing I could find was:</p>

<pre><code>CoreAnnotations.SemanticWordAnnotation
CoreAnnotations.SemanticTagAnnotation
</code></pre>
","stanford-nlp, semantic-analysis","<p>No, we currently don't have a semantic role labeling (SRL) system in CoreNLP. </p>

<p>Unless you already have a system that explicitly requires semantic role labels, I would recommend taking a look at the <a href=""http://universaldependencies.github.io/docs/"" rel=""noreferrer"">Universal Dependencies</a> representation. Despite the fact that this representation is primarily a syntactic representation, it also encodes a lot semantics and works just as well as SRL for many semantic tasks.</p>

<p>If this alternative does not work for you, then there also exist several SRL systems from other research groups, e.g. the <a href=""http://cogcomp.cs.illinois.edu/page/software_view/Curator"" rel=""noreferrer"">SRL system from UIUC</a>.</p>
",8,4,2215,2015-06-29 14:22:13,https://stackoverflow.com/questions/31118160/does-the-stanford-nlp-parser-have-methods-for-semantic-role-labelling
Generating stanford semantic graph with nodes storing lemma,"<p>I am trying to generate a SemanticGraph and use semgrex to find the specific node.
I would like to use lemma as one of node attribute in semgrex. I saw a relevant question and answer here: </p>

<p><a href=""https://stackoverflow.com/questions/25326451/corenlp-semanticgraph-search-for-edges-with-specific-lemmas"">CoreNLP SemanticGraph - search for edges with specific lemmas</a>  </p>

<p>It is mentioned that</p>

<blockquote>
  <p>Make sure that the nodes are storing lemmas -- see the lemma annotator of CoreNLP (currently available for English, only).</p>
</blockquote>

<p>I current can use pipeline to generate the desired annotation to generate the semantic graph.</p>

<pre><code> Properties props = new Properties();
 props.put(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");
 StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>However, after searching for relevant information, I only find a deprecated function at here:</p>

<p><a href=""https://github.com/chbrown/nlp/blob/master/src/main/java/edu/stanford/nlp/pipeline/ParserAnnotatorUtils.java"" rel=""nofollow noreferrer"">https://github.com/chbrown/nlp/blob/master/src/main/java/edu/stanford/nlp/pipeline/ParserAnnotatorUtils.java</a></p>

<pre><code>public static SemanticGraph generateDependencies(Tree tree,
    boolean collapse,
    boolean ccProcess,
    boolean includeExtras,
    boolean lemmatize,
    boolean threadSafe) {
  SemanticGraph deps = SemanticGraphFactory.makeFromTree(tree, collapse, ccProcess, includeExtras, lemmatize, threadSafe);
  return deps;
}
</code></pre>

<p>which seemed to be removed from newest coreNLP. </p>

<p>Could anyone give some hint on how to generate the semantic graph with nodes that storing the lemmas?</p>
",stanford-nlp,"<p>The function as you have it should work. The lemmas will be stored in the <code>SemanticGraph</code> produced by the <code>parse</code> annotator. You can retrieve the graph after running your text through <code>pipeline.annotate(Annotation)</code> with:</p>

<p><code>sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class)</code></p>

<p>Or, the equivalent for Collapsed and CollapsedCCProcessed dependencies. See <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraphCoreAnnotations.html"" rel=""nofollow"">http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraphCoreAnnotations.html</a>. Note that these are attached to a sentence (CoreMap), not a document (Annotation).</p>

<p>You can then run Semgrex over the graph as usual; e.g., <code>{lemma:/foo/} &gt;arctype {lemma:/bar/}</code>.</p>
",0,0,1395,2015-07-02 06:59:49,https://stackoverflow.com/questions/31177593/generating-stanford-semantic-graph-with-nodes-storing-lemma
depth of node in a parse tree,"<p>I am using stanford-nlp with Java 7 and NetBeans 7.3.1</p>

<pre><code>Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    String text = ""the dog who bit the man"";// Add your text here!
    Annotation document = new Annotation(text);
    pipeline.annotate(document);
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    for(CoreMap sentence: sentences) {
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        String word = token.get(TextAnnotation.class);
        String pos = token.get(PartOfSpeechAnnotation.class);
        String ne = token.get(NamedEntityTagAnnotation.class);       
      }
    Tree tree = sentence.get(TreeAnnotation.class);
    System.out.println(tree);
    System.out.println(tree.depth());
</code></pre>

<p>With this I can get depth of the tree but how can I get the depth of term 'dog' or any other term's depth in this parse tree? </p>
","java, netbeans-7, stanford-nlp","<p>After some research, I came to know that this is a dumb question (very dumb) Sorry :D</p>

<p>Anyway, here's what I learned:</p>

<p>Since the terms/strings of the sentence are represented as leaves in the parse tree, the depth of them will be 0. </p>

<p>So now how to access the term i.e how to iterate the tree to that term/string/leaf ::</p>

<pre><code>for (Tree subtree : tree) { 
          if(subtree.label().value().equals(""term----xxxx""))
          //You can do your own stuff here 
 }
</code></pre>
",0,1,479,2015-07-03 09:21:47,https://stackoverflow.com/questions/31203146/depth-of-node-in-a-parse-tree
Stanford TokensRegex: how to set normalized annotation using normalized output of NER annotation?,"<p>I am creating a TokensRegex annotator to extract the number of floors a building has (just an example to illustrate my question). I have a simple pattern that will recognize both ""4 floors"" and ""four floors"" as instances of my custom entity ""FLOORS"". 
I would also like to add a NormalizedNER annotation, using the normalized value of the number entity used in the expression, but I can't get it to work the way I want to:</p>

<pre><code>ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
normalized = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NormalizedNamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

ENV.defaults[""ruleType""] = ""tokens""

{
  pattern: ( ( [ { ner:NUMBER } ] ) /floor(s?)/ ),
  action: ( Annotate($0, ner, ""FLOORS""), Annotate($0, normalized, $$1.text) ) 
}
</code></pre>

<p>The rules above only set the NormalizedNER fields in the output to the text value of the number, ""4"" and ""four"" for the above examples respectively. Is there a way to use the NUMBER entity's normalized value (""4.0"" both for ""4"" and ""four"") as the normalized value for my ""FLOORS"" entity?</p>

<p>Thanks in advance.</p>
",stanford-nlp,"<p>The correct answer is based on @AngelChang's answer and comment, I'm just posting it here for the sake of ordeliness.</p>

<p>The rule has to be modified so the 2nd Annotate() action's 3rd parameter is <code>$1[0].normalized</code>:</p>

<pre><code>{
  pattern: ( ( [ { ner:NUMBER } ] ) /floor(s?)/ ),
  action: ( Annotate($0, ner, ""FLOORS""), Annotate($0, normalized, $1[0].normalized) ) 
}
</code></pre>

<p>According to @Angel's comment: </p>

<blockquote>
  <p>$1[0].normalized is the ""normalized"" field of the 0th token of the 1st
  capture group (as a CoreLabel). The $$1 gives you back the
  MatchedGroupInfo which has the ""text"" field but not the normalized
  field (since that is on the actual token)</p>
</blockquote>
",0,0,677,2015-07-06 00:04:00,https://stackoverflow.com/questions/31236221/stanford-tokensregex-how-to-set-normalized-annotation-using-normalized-output-o
Stanford CoreNLP Training Examples,"<p>Anyone know where the following files located:</p>

<p>trainFileList = /u/nlp/data/ner/column_data/muc6.ptb.train,
/u/nlp/data/ner/column_data/muc7.ptb.train</p>

<p>I am following the FAQ link <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow"">http://nlp.stanford.edu/software/crf-faq.shtml#a</a></p>

<p>If all I need to do is provide a file with two columns consisting of tokens and class, then that will work. But I am curious about the train files listed in the classifier property files.</p>

<p>serializeTo = english.muc.7class.caseless.distsim.crf.ser.gz</p>

<p>java -mx1g -cp ""$CLASSPATH"" edu.stanford.nlp.ie.NERClassifierCombiner -textFile sample.txt -ner.model classifiers/english.all.3class.distsim.crf.ser.gz,classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz -outputFormat tabbedEntities -textFile sample.txt > sample2.tsv</p>
",stanford-nlp,"<p>Those files are the training data for the MUC-6 and MUC-7 tasks:</p>

<p><a href=""http://cs.nyu.edu/faculty/grishman/muc6.html"" rel=""nofollow"">http://cs.nyu.edu/faculty/grishman/muc6.html</a></p>

<p>They are not distributed by Stanford.  I will see if I can figure out where they are distributed and update this answer.</p>

<p>UPDATE: LDC distributes those files if you want to get a copy, they have copyright issues so you have to purchase them from LDC, that is why we don't distribute them.  Here are some links with more info:</p>

<p><a href=""http://www-nlpir.nist.gov/related_projects/muc/muc_data/muc_data_index.html"" rel=""nofollow"">http://www-nlpir.nist.gov/related_projects/muc/muc_data/muc_data_index.html</a></p>

<p><a href=""https://catalog.ldc.upenn.edu/LDC2003T13"" rel=""nofollow"">https://catalog.ldc.upenn.edu/LDC2003T13</a></p>

<p><a href=""https://catalog.ldc.upenn.edu/LDC2001T02"" rel=""nofollow"">https://catalog.ldc.upenn.edu/LDC2001T02</a></p>
",1,1,1183,2015-07-08 20:16:10,https://stackoverflow.com/questions/31302843/stanford-corenlp-training-examples
StanfordNLP Training Steps Verification and loadClassifier check,"<p>I need help verifying the training steps below and can I add my classifier to -loadClassifier list?</p>

<p>-loadClassifier <strong>sample-ner-model.ser.gz</strong>, classifiers/english.all.3class.distsim.crf.ser.gz,classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz \</p>

<p>sample.txt</p>

<p>The fate of Lehman Brothers, the beleaguered investment bank, hung in the balance on Sunday as Federal Reserve officials and the leaders of major financial institutions continued to gather in emergency meetings trying to complete a plan to rescue the stricken bank.  Several possible plans emerged from the talks, held at the Federal Reserve Bank of New York and led by Timothy R. Geithner, the president of the New York Fed, and Treasury Secretary Henry M. Paulson Jr.</p>

<p>Step 1 Tokenize</p>

<p>java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer sample.txt > sample.tok</p>

<p>The
fate
of
Lehman
Brothers
,
the
beleaguered
investment
bank
,
hung
in
the
balance</p>

<p>. . .</p>

<p>president
of
the
New
York
Fed
,
and
Treasury
Secretary
Henry
M.
Paulson
Jr.
.</p>

<p>Step 2 Classify</p>

<h1>Need a better command to replace EOL ""\n"" with ""\tO\n"" . Perl chomp not working. Edited sample.tzv manually.</h1>

<p>perl -ne 'chomp; print ""$_\tO""' sample.tok > sample.tsv</p>

<p>The 0
fate    0
of  0
Lehman  0
Brothers    0
,   0
the 0
beleaguered 0
investment  0
bank    0
,   0
hung    0
in  0
the 0
balance 0
. . .
president   0
of  0
the 0
New 0
York    0
Fed 0
,   0
and 0
Treasury    0
Secretary   0
Henry   0
M.  0
Paulson 0
Jr. 0
.   0</p>

<p>Step 3 Adjust Properties (sample.prop)</p>

<pre><code># location of the training file
trainFile = sample.tsv
# location where you would like to save (serialize) your
# classifier; adding .gz at the end automatically gzips the file,
# making it smaller, and faster to load
serializeTo = sample-ner-model.ser.gz
. . .
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Step 4 Modify Gold Standard (sample.tsv)</p>

<p>The 0
fate    0
of  0
Lehman  ORG
Brothers    ORG
,   0
the 0
beleaguered 0
investment  0
bank    0
,   0
hung    0
in  0
the 0
balance 0
. . .
president   0
of  0
the 0
New ORG
York    ORG
Fed ORG
,   0
and 0
Treasury    PERS
Secretary   PERS
Henry   PERS
M.  PERS
Paulson PERS
Jr. PERS
.   0</p>

<p>Step 4 Train</p>

<p>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop sample.prop</p>

<p>Step 5 Test and Verify</p>

<p>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier sample-ner-model.ser.gz -testFile sample.tsv</p>

<p>Production Maybe:</p>

<p>java -mx1g edu.stanford.nlp.ie.NERClassifierCombiner -textFile sample.txt -ner.model \ 
-loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz,classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz \
-outputFormat tabbedEntities -textFile sample.txt > sampleNew.tsv</p>
",stanford-nlp,"<p>This seems correct to me.</p>

<p>Yes, if you build a new model with Stanford CoreNLP you can just add it into the list.  </p>

<p>Note that the models are run in order, and earlier NER taggers in the list tag first, and then later models cannot overwrite the tags (e.g. ORG, PER) written by previous ones (except O of course).  So basically where you put the models matters, closer to the front takes priority.</p>

<p>Also ner.combinationMode = HIGH_RECALL will allow every classifier in the list to apply all of their tags.  ner.combinationMode = NORMAL means only the first classifier that applies a tag (e.g. ORG, PER) can apply it.  You can set ner.combinationMode in the .prop file. </p>
",0,0,113,2015-07-09 15:31:55,https://stackoverflow.com/questions/31321994/stanfordnlp-training-steps-verification-and-loadclassifier-check
Stanford CoreNLP Morphology.stemStatic disable lowercase conversion?,"<p>The comments on the stemStatic method of the Morphology class state that it will:</p>

<blockquote>
  <p>return a new WordTag which has the lemma as the value of word().<br>
  The default is to lowercase non-proper-nouns, unless options have
  been set.</p>
</blockquote>

<p>(<a href=""https://github.com/evandrix/stanford-corenlp/blob/master/src/edu/stanford/nlp/process/Morphology.java"" rel=""nofollow"">https://github.com/evandrix/stanford-corenlp/blob/master/src/edu/stanford/nlp/process/Morphology.java</a>)</p>

<p>How/where can I set those options, to disable the lowercase conversion? </p>

<p>I've looked through the source but can't see how I can set options that will affect this static method. Frustratingly, the related static lemmatise method -- lemmaStatic -- includes a boolean parameter to do exactly this...</p>

<p>I'm using v3.3.1 via Maven...</p>

<p>thanks!</p>
","stanford-nlp, stemming","<p>Ok after looking at this for a bit, it seems the right track might be to not use the static method, but instead build a Morphology instance with:</p>

<pre><code>public Morphology(Reader in, int flags) {
</code></pre>

<p>The int flags will set the lexer.options.</p>

<p>Here are the lexer options (from Morpha.java) :</p>

<pre><code>/** If this option is set, print the word affix after a + character */
private final static int print_affixes = 0;  
/** If this option is set, lowercase all tokens */
private final static int change_case = 1;
/** Return the tags on the input words if present?? */
private final static int tag_output= 2;
</code></pre>

<p>The int flags is the bit string for the 3 options, so 7 = 111 , meaning all options will be set to true , 0 = 000 , all options false, 5 = 101 would set print_affixes and tag_output, etc...</p>

<p>Then you can use apply in Morphology.java</p>

<pre><code>public Object apply(Object in) {
</code></pre>

<p>Object in should be a WordTag built with the original word and tag.</p>

<p>Please let me know if you need any further assistance!</p>

<p>We could also change Morphology.java to have the kind of method you want!  The above is if you don't want to play around with customizing Stanford CoreNLP.</p>
",1,0,273,2015-07-09 16:25:35,https://stackoverflow.com/questions/31323159/stanford-corenlp-morphology-stemstatic-disable-lowercase-conversion
How to get noun phrase with multiple words inside it using Stanford NLP Tregex?,"<p>I'm trying to figure out if it is possible to efficently extract NP using condition with multiple words. This is my current code:</p>

<pre><code>public static List&lt;Tree&gt; getNounPhrasesWithMultipleKeywords(Annotation doc,
        List&lt;String&gt; tags) {
    StringBuilder sb = new StringBuilder();
    boolean firstWord = true;

    for (int i = 0; i &lt; tags.size(); i++) {
        String word = tags.get(i);
        String[] splitted = word.split("" "");
        for (String splitWord : splitted) {
            if (!firstWord) {
                sb.append("" &amp;"");
            }
            sb.append("" &lt;&lt; "" + splitWord);
            firstWord = false;
        }

    }
    // sb.append("")"");

    TregexPattern pattern = TregexPattern.compile(""NP &lt; (__""
            + sb.toString() + "")"");

    return getTreeWithPattern(doc, pattern);
}
</code></pre>

<p>Now, lets say that input phrase has got this tree:</p>

<pre><code>(ROOT (S (NP (ADJP (RB Poorly) (VBN controlled)) (NN asthma)) (VP (VBZ is) (NP (DT a) (JJ vicious) (NN disease))) (. .)))
</code></pre>

<p>I want to get only those NP, which contains tags specified in function argument, e.g. for input [""controlled"", ""asthma""] it should return </p>

<pre><code>(NP (ADJP (RB Poorly) (VBN controlled)) (NN asthma))
</code></pre>

<p>But when input is [""injection"", ""controlled"", ""asthma""] it should return nothing.</p>

<p>As you can see, if one of input strings is ""multiple words"", then program splits it into words. I think that there should be better solution for it, but I don't know how it should work.</p>
","java, nlp, stanford-nlp","<p>I think you just need to tweak your pattern a little bit.  You didn't really give a complete specification of what you wanted, but from what I could tell <code>[""controlled"", ""asthma""]</code> should result in a pattern like <code>(NP &lt;&lt; (controlled .. asthma ))</code>, which means ""Noun Phrase containing 'controlled' followed by 'asthma'"".  I'm not sure exactly how you want ""phrases"" to work; Do you want <code>[""controlled asthma""]</code> to mean ""'controlled' followed immediately by 'asthma'"", i.e. <code>(NP &lt;&lt; (controlled . asthma))</code>?</p>

<p>Here's a new version of your function that creates these patterns:</p>

<pre><code>  public static List&lt;Tree&gt; getNounPhrasesWithMultipleKeywords(Annotation doc,
                                                              List&lt;String&gt; tags) {
    List&lt;String&gt; phrases = new ArrayList&lt;String&gt;();

    for (int i = 0; i &lt; tags.size(); i++) {
      String word = tags.get(i);
      String[] splitted = word.split("" "");
      phrases.add(join("" . "", Arrays.asList(splitted)));
    }
    String pattern_str = join("" .. "", phrases);
    TregexPattern pattern = TregexPattern.compile(
      ""NP &lt;&lt; ("" + pattern_str + "")"");
    return getTreeWithPattern(doc, pattern);
  }

  // In Java 8 use String.join.
  public static String join(String sep, Collection&lt;String&gt; strs) {
    System.out.println(strs);
    StringBuilder sb = new StringBuilder();
    boolean first = true;
    for (String s : strs) {
      if (!first) {
        sb.append(sep);
      }
      sb.append(s);
      first = false;
    }
    return sb.toString();
  }
</code></pre>

<p>This function gives the output you specified in your examples.</p>
",1,0,580,2015-07-10 09:43:19,https://stackoverflow.com/questions/31337604/how-to-get-noun-phrase-with-multiple-words-inside-it-using-stanford-nlp-tregex
How to Preserve Original Line Numbering in the Output of Stanford CoreNLP?,"<p>Text corpora are often distributed as large files containing specific documents on each new line.  For instance, I have a file with 10 million product reviews, one per line, and each review contains multiple sentences.  </p>

<p>When processing such files with Stanford CoreNLP, using the command line, for instance</p>

<pre><code>java -cp ""*"" -Xmx16g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma -file test.txt
</code></pre>

<p>the output, whether in text or xml format, will number all sentences from 1 to <code>n</code>, ignoring the original line numbering that separates the documents.  </p>

<p>I would like to keep track of the original file's line numbering (e.g. in xml format, to have an output tree like <code>&lt;original_line id=1&gt;</code>, then <code>&lt;sentence id=1&gt;</code>, then <code>&lt;token id=1&gt;</code>).  Or else, to be able to reset the numbering of sentences at the start of each new line in the original file. </p>

<p>I have tried the answer to a <a href=""https://stackoverflow.com/questions/12140683/stanford-pos-tagger-how-to-preserve-newlines-in-the-output"">similar question</a> about Stanford's POS tagger, without success.  Those options do not keep track of the original line numbers.  </p>

<p>A quick solution could be to split the original file in multiple files, then processing each of them with CoreNLP and the <code>-filelist</code> input option.  However, for large files with millions of documents, creating millions of individual files just to preserve the original line/document numbering seems inefficient.     </p>

<p>I suppose it would be possible to modify the source code of Stanford CoreNLP, but I am unfamiliar with Java.  </p>

<p>Any solution to preserve the original line numbering in the output would be very helpful, whether through the command line or by showing an example Java code that would achieve that.       </p>
","nlp, stanford-nlp","<p>I've dug through the code base, and I can't find a command line flag that will help you.</p>

<p>I wrote some sample Java code that should do the trick.</p>

<p>I put this in DocPerLineProcessor.java, which I put into stanford-corenlp-full-2015-04-20.  I also put a file called sample-doc-per-line.txt which had 4 sentences per line.</p>

<p>First make sure to compile:</p>

<p>cd stanford-corenlp-full-2015-04-20</p>

<p>javac -cp ""*:."" DocPerLineProcessor.java</p>

<p>Here is the command to run:</p>

<p>java -cp ""*:."" DocPerLineProcessor sample-doc-per-line.txt</p>

<p>The output sample-doc-per-line.txt.xml should be the desired xml format, but sentences now have which line number they're on.</p>

<p>Here is the code:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*; 
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;

public class DocPerLineProcessor {
    public static void main (String[] args) throws IOException {
        // set up properties
        Properties props = new Properties();
        props.setProperty(""annotators"",
            ""tokenize, ssplit, pos, lemma, ner, parse"");
        // set up pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // read in a product review per line
        Iterable&lt;String&gt; lines = IOUtils.readLines(args[0]);
        Annotation mainAnnotation = new Annotation("""");
        // add a blank list to put sentences into
        List&lt;CoreMap&gt; blankSentencesList = new ArrayList&lt;CoreMap&gt;();
        mainAnnotation.set(CoreAnnotations.SentencesAnnotation.class,blankSentencesList);
        // process each product review
        int lineNumber = 1;
        for (String line : lines) {
            Annotation annotation = new Annotation(line);
            pipeline.annotate(annotation);
            for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
                sentence.set(CoreAnnotations.LineNumberAnnotation.class,lineNumber);
                mainAnnotation.get(CoreAnnotations.SentencesAnnotation.class).add(sentence);
            }
            lineNumber += 1;
        }
        PrintWriter xmlOut = new PrintWriter(args[0]+"".xml"");
        pipeline.xmlPrint(mainAnnotation, xmlOut);
    }
}
</code></pre>

<p>Now when I run this, the sentence tags also have the appropriate line number.  So the sentences still have a global id, but you can mark which line they came from.  This will also set it up so newline always ends a sentence.</p>

<p>Please let me know if you need any clarification or if I made any errors transcribing my code.</p>
",1,1,663,2015-07-10 14:02:30,https://stackoverflow.com/questions/31342779/how-to-preserve-original-line-numbering-in-the-output-of-stanford-corenlp
Stanford nndep to get parse trees,"<p>Using Stanford CoreNLP, I am trying to parse text using the neural nets dependency parser. It runs really fast (that's why I want to use this and not the <code>LexicalizedParser</code>), and produces high-quality dependency relations. I am also interested in retrieving the parse trees (Penn-tree style) from that too. So, given the GrammaticalStructure, I am getting the root of that (using <code>root()</code>), and then trying to print it out using the <code>toOneLineString()</code> method. However, <code>root()</code> returns the root node of the tree, with an empty/null list of children. I couldn't find anything on this in the instructions or FAQs. </p>

<pre><code> GrammaticalStructure gs = parser.predict(tagged);

  // Print typed dependencies
  System.err.println(gs);

  // get the tree and print it out in the parenthesised form
  TreeGraphNode tree = gs.root();
  System.err.println(tree.toOneLineString());
</code></pre>

<p>The output of this is:</p>

<pre><code>  ROOT-0{CharacterOffsetBeginAnnotation=-1, CharacterOffsetEndAnnotation=-1, PartOfSpeechAnnotation=null, TextAnnotation=ROOT}Typed Dependencies: 
  [nsubj(tell-5, I-1), aux(tell-5, can-2), advmod(always-4, almost-3), advmod(tell-5, always-4), root(ROOT-0, tell-5), advmod(use-8, when-6), nsubj(use-8, movies-7), advcl(tell-5, use-8), amod(dinosaurs-10, fake-9), dobj(use-8, dinosaurs-10), punct(tell-5, .-11)]
  ROOT-0
</code></pre>

<p>How can I get the parse tree too?</p>
","nlp, stanford-nlp","<p>Figured I can use the Shift-Reduce constituency parser made available by Stanford. It's very fast and the results are comparable.</p>
",0,0,153,2015-07-13 10:20:06,https://stackoverflow.com/questions/31380759/stanford-nndep-to-get-parse-trees
bad tokenization in stanford postagger,"<p>I'm trying to use the Stanford POS tagger to tag some French text. To do that, I use the following command:</p>

<blockquote>
  <p>cat file.txt | java -mx10000m -cp 'stanford-postagger.jar:'
  edu.stanford.nlp.tagger.maxent.MaxentTagger -model
  models/french.tagger -sentenceDelimiter newline > output.txt</p>
</blockquote>

<p>(There is one sentence per line.)</p>

<p>But I noticed that the tags were pretty bad, and that the real issue actually comes from the French tokenization itself. I think that the tokenization is done by an English tokenizer.</p>

<p>So I tried to only tokenize the text in French by doing this:</p>

<blockquote>
  <p>cat file.txt | java -mx10000m -cp 'stanford-postagger.jar:'
  edu.stanford.nlp.international.french.process.FrenchTokenizer
  -sentenceDelimiter newline > tokenized.txt</p>
</blockquote>

<p>And there the French tokens are good.</p>

<p>How can I tell the tagger to use the French model for tagging, but also the French tokenizer at the same time?</p>
","nlp, tokenize, stanford-nlp, pos-tagger","<p>You can use the <code>-tokenizerFactory</code> and <code>-tokenizerOptions</code> flags to control tokenization.  The ""Tagging and Testing from the command line"" section of the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/tagger/maxent/MaxentTagger.html"" rel=""nofollow"">javadoc for MaxentTagger</a> has a complete list of available options.</p>

<p>I believe the following command will do what you want:</p>

<pre><code>java -mx10000m -cp 'stanford-postagger.jar:' \
  edu.stanford.nlp.tagger.maxent.MaxentTagger \
  -model models/french.tagger \
  -tokenizerFactory 'edu.stanford.nlp.international.french.process.FrenchTokenizer$FrenchTokenizerFactory' \
  -sentenceDelimiter newline
</code></pre>
",2,2,515,2015-07-14 07:58:57,https://stackoverflow.com/questions/31400985/bad-tokenization-in-stanford-postagger
number possible part of speech of the word,"<p>I know how to get pos of a word in the text but I need to know what would be the possible pos of a word in a sentence for example ""like"" can have 4 part of speechs: verb noun preposition ....
Is it possible to get that from Stanford library? </p>
",stanford-nlp,"<p>Stanford CoreNLP doesn't seem to have an interface to WordNet, but it's pretty easy to do this with one of the other small Java WordNet libraries.  For this example, I used <a href=""http://projects.csail.mit.edu/jwi/"" rel=""nofollow"">JWI 2.3.3</a>.</p>

<p>Besides JWI, you'll need to download a copy of the WordNet database. For example, you can download <a href=""http://wordnetcode.princeton.edu/3.0/"" rel=""nofollow"">WordNet-3.0.tar.gz from Princeton</a>.  Untar the dictionary.</p>

<p>The following code includes a function that returns a list of the possible parts of speech for a word:</p>

<pre><code>import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;

import edu.mit.jwi.Dictionary;
import edu.mit.jwi.item.POS;
import edu.mit.jwi.item.IIndexWord;
import edu.mit.jwi.morph.WordnetStemmer;

public class WNDemo {

  /**
   * Given a dictionary and a word, find all the parts of speech the
   * word can be.
   */
  public static Collection getPartsOfSpeech(Dictionary dict, String word) {
    ArrayList&lt;POS&gt; parts = new ArrayList&lt;POS&gt;();
    WordnetStemmer stemmer = new WordnetStemmer(dict);
    // Check every part of speech.
    for (POS pos : POS.values()) {
      // Check every stem, because WordNet doesn't have every surface
      // form in its database.
      for (String stem : stemmer.findStems(word, pos)) {
        IIndexWord iw = dict.getIndexWord(stem, pos);
        if (iw != null) {
          parts.add(pos);
        }
      }
    }
    return parts;
  }

  public static void main(String[] args) {
    try {
      Dictionary dict = new Dictionary(new File(""WordNet-3.0/dict""));
      dict.open();
      System.out.println(""'like' is a "" + getPartsOfSpeech(dict, ""like""));
    } catch (IOException e) {
      System.err.println(""Error: "" + e);
    }
  }
}
</code></pre>

<p>And the output:</p>

<pre><code>'like' is a [noun, verb, adjective]
</code></pre>
",3,2,414,2015-07-15 15:27:53,https://stackoverflow.com/questions/31434669/number-possible-part-of-speech-of-the-word
stanford-nlp NER from a list of tokens,"<p>Is there a way of using the stanford NER library to take input a list of tokens, and extract NEs?</p>

<p>I have checked the API but it is not explicit. Most of time the input is a String, a document, in both cases tokenization is done behind the scene.</p>

<p>In my case, I really have to do tokenization before and pass the list of tokens to the API. I have noticed that I can do:</p>

<pre><code>List&lt;HasWord&gt; words = new ArrayList&lt;&gt;();

words.add(new Word(""Tesco""));
..... //adding elements to words

List&lt;CoreLabel&gt; labels =classifier.classifySentence(words);
</code></pre>

<p>Is this correct?</p>

<p>Many thanks!!</p>
","nlp, stanford-nlp","<p>Here is one way to solve this issue:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;

public class NERPreToken {
    public static void main (String[] args) {
        Properties props = new Properties();
        props.setProperty(""annotators"",
            ""tokenize, ssplit, pos, lemma, ner"");
        props.setProperty(""tokenize.whitespace"", ""true"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        String [] tokensArray = {""Stephen"",""Colbert"",""hosts"",""a"",""show"",""on"",""CBS"","".""};
        List&lt;String&gt; tokensList = Arrays.asList(tokensArray);
        String docString = String.join("" "",tokensList);
        Annotation annotation = new Annotation(docString);
        pipeline.annotate(annotation);
        List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
        for (CoreMap sentence : sentences) {
            List&lt;CoreLabel&gt; tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);
            for (CoreLabel token : tokens) {
                System.out.println(token.word()+"" ""+token.get(CoreAnnotations.NamedEntityTagAnnotation.class));
            }
        }
    }
}
</code></pre>

<p>The key here is to start with your list of tokens and set the pipeline's property for tokenizing to just tokenize on white space.  Then submit a String with your tokens joined by space.</p>
",2,2,916,2015-07-15 18:17:19,https://stackoverflow.com/questions/31438106/stanford-nlp-ner-from-a-list-of-tokens
Prevent Stanford CoreNLP from Americanizing input,"<p>I noticed that the Stanford CoreNLP is Americanizing input which is breaking some of my code since the character offsets don't add up anymore.</p>

<p>I am using the following code:</p>

<pre><code>String annotators = ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"";
Properties props = new Properties();
props.put(""annotators"",annotators);
this.pipeline = new StanfordCoreNLP(props);
this.annotators = annotators;
Annotation document = new Annotation(text);
pipeline.annotate(document);
pipeline.prettyPrint(document, resp.getWriter());
</code></pre>

<p>With the following input:</p>

<pre><code>But, at the other end of the town, in his own little hut, there dwelt an honourable laborer.
</code></pre>

<p>I get:</p>

<pre><code>[...]
[Text=an CharacterOffsetBegin=70 CharacterOffsetEnd=72 PartOfSpeech=DT]    
[Text=honorable CharacterOffsetBegin=73 CharacterOffsetEnd=83 PartOfSpeech=JJ] 
[Text=laborer CharacterOffsetBegin=84 CharacterOffsetEnd=91 PartOfSpeech=NN]
[...]
(NP (DT an) (JJ honorable) (NN laborer))
[...]
</code></pre>

<p>Notice that the input had the word <code>honourable</code> but the output has the word <code>honorable</code>. Same thing happens with words like <code>colour</code> or <code>harbours</code>.</p>

<p>Is there a way to prevent that behavior? I don't mind it in the lemmas but I'd like to get the original words so offsets matched.</p>
","nlp, stanford-nlp","<p>In code for CoreNLP, you want to add the property:</p>

<pre><code>props.put(""tokenize.options"", ""americanize=false"");
</code></pre>
",3,1,102,2015-07-15 20:09:29,https://stackoverflow.com/questions/31440070/prevent-stanford-corenlp-from-americanizing-input
How to create a GrammaticalRelation in Stanford CoreNLP,"<p>I have recently upgraded to the latest version of Stanford CoreNLP.  The code I previously used to get the subject or object in a sentence was </p>

<pre><code>System.out.println(""subject: ""+dependencies.getChildWithReln(dependencies.getFirstRoot(), EnglishGrammaticalRelations.NOMINAL_SUBJECT));
</code></pre>

<p>but this now returns <code>null</code>.</p>

<p>I have tried creating a relation with </p>

<pre><code>GrammaticalRelation subjreln =
                edu.stanford.nlp.trees.GrammaticalRelation.valueOf(""nsubj"");
</code></pre>

<p>without success.  If I extract a relation using code like</p>

<pre><code>GrammaticalRelation target = (dependencies.childRelns(dependencies.getFirstRoot())).iterator().next();
</code></pre>

<p>Then run the same request, </p>

<pre><code>System.out.println(""target: ""+dependencies.getChildWithReln(dependencies.getFirstRoot(), target));
</code></pre>

<p>then I get the desired result, confirming that the parsing worked fine (I also know this from printing out the full dependencies).</p>

<p>I suspect my problem has to do with the switch to universal dependencies, but I don't know how to create the GrammaticalRelation from scratch in a way that will match what the dependency parser found.</p>
","nlp, stanford-nlp","<p>Since version 3.5.2 the default dependency representation in CoreNLP is <a href=""http://universaldependencies.github.io/docs/"" rel=""nofollow"">Universal Dependencies</a>. This new representation is implemented in a different class (<code>UniversalEnglishGrammaticalRelations</code>) so the <code>GrammaticalStructure</code> objects are now defined somewhere else.</p>

<p>All you have to do to use the new version is to replace <code>EnglishGrammaticalRelations</code> with <code>UniversalGrammaticalRelations</code>:</p>

<pre><code>System.out.println(""subject: ""+dependencies.getChildWithReln(dependencies.getFirstRoot(), UniversalEnglishGrammaticalRelations.NOMINAL_SUBJECT));
</code></pre>

<p>Note, however, that some relations in the new representation are different and might no longer exist (<code>nsubj</code> still does). We are currently compiling <a href=""http://universaldependencies.github.io/docs/en/overview/migration-guidelines.html"" rel=""nofollow"">migration guidelines</a> from the old representation to the new Universal Dependencies relations. It is still incomplete but it already contains all relation names and their class names in CoreNLP.</p>
",0,1,330,2015-07-16 15:15:49,https://stackoverflow.com/questions/31458201/how-to-create-a-grammaticalrelation-in-stanford-corenlp
NLP NER Processing Errors,"<p>here is the tsv file. c2is2r3.tsv</p>

<pre><code>The O
fate    O
of  O
Lehman  ORGANIZATION
Brothers    ORGANIZATION

. . .

New ORGANIZATION
York    ORGANIZATION
Fed ORGANIZATION
,   O
and O
Treasury    TITLE
Secretary   TITLE
Henry   PERSON
M.  PERSON
Paulson PERSON
Jr. PERSON
.   O
</code></pre>

<p>more c2is2r3.prop</p>

<pre><code>trainFile = c2is2r3.tsv
serializeTo = c2is2r3-ner-model.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
</code></pre>

<p>Here is the original sequence</p>

<pre><code>java -cp  stanford-ner-3.5.2.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop c2is2r3.prop


java -cp stanford-ner-3.5.2.jar -mx2g edu.stanford.nlp.ie.NERClassifierCombiner -ner.model c2is2r3-ner-model.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz -ner.useSUTime false -ner.combinationMode HIGH_RECALL -serializeTo c2is2.serialized.ncc.ncc.ser.gz


java -cp stanford-ner-3.5.2.jar -mx1g edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier c2is2.serialized.ncc.ncc.ser.gz -textFile c2is2r3.txt


CRFClassifier invoked on Fri Jul 17 09:51:13 EDT 2015 with arguments:
   -loadClassifier c2is2.serialized.ncc.ncc.ser.gz -textFile c2is2r3.txt
loadClassifier=c2is2.serialized.ncc.ncc.ser.gz
textFile=c2is2r3.txt
Loading classifier from /mnt/hgfs/share/nlp/stanford-ner-2015-04-20/c2is2.serialized.ncc.ncc.ser.gz ... Error deserializing /mnt/hgfs/share/nlp/stanford-ner-2015-04-20/c2is2.serialized.ncc.ncc.ser.gz
Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassCastException: java.util.Properties cannot be cast to [Ledu.stanford.nlp.util.Index;
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1572)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1523)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2987)
Caused by: java.lang.ClassCastException: java.util.Properties cannot be cast to [Ledu.stanford.nlp.util.Index;
    at edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2613)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1451)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1558)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1569)
    ... 2 more
</code></pre>

<p>This is an attempt to use the <strong>NERClassifierCombiner</strong></p>

<pre><code>java -cp stanford-ner-3.5.2.jar -mx1g edu.stanford.nlp.ie.NERClassifierCombiner  -loadClassifier c2is2.serialized.ncc.ncc.ser.gz -testFile c2is2r3.txt
</code></pre>

<p>This is the error stack:</p>

<pre><code>NERClassifierCombiner invoked on Fri Jul 17 10:11:17 EDT 2015 with arguments:
   -loadClassifier c2is2.serialized.ncc.ncc.ser.gz -testFile c2is2r3.txt
testFile=c2is2r3.txt
loadClassifier=c2is2.serialized.ncc.ncc.ser.gz
testFile=c2is2r3.txt
ner.useSUTime=false
ner.model=c2is2r3-ner-model.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz
serializeTo=c2is2.serialized.ncc.ncc.ser.gz
loadClassifier=c2is2.serialized.ncc.ncc.ser.gz
ner.combinationMode=HIGH_RECALL
loading CRF...
loading CRF...
Error on line 1: The fate of Lehman Brothers, the beleaguered investment bank, hung in the balance on Sunday as Federal Reserve officials and the leaders of major financial institutions continued to gather in emergency meetings trying to complete a plan to rescue the stricken bank.  Several possible plans emerged from the talks, held at the Federal Reserve Bank of New York and led by Timothy R. Geithner, the president of the New York Fed, and Treasury Secretary Henry M. Paulson Jr.
Exception in thread ""main"" java.lang.UnsupportedOperationException: Argument array lengths differ: [word, tag, answer] vs. [The, fate, of, Lehman, Brothers,, the, beleaguered, investment, bank,, hung, in, the, balance, on, Sunday, as, Federal, Reserve, officials, and, the, leaders, of, major, financial, institutions, continued, to, gather, in, emergency, meetings, trying, to, complete, a, plan, to, rescue, the, stricken, bank., Several, possible, plans, emerged, from, the, talks,, held, at, the, Federal, Reserve, Bank, of, New, York, and, led, by, Timothy, R., Geithner,, the, president, of, the, New, York, Fed,, and, Treasury, Secretary, Henry, M., Paulson, Jr.]
    at edu.stanford.nlp.ling.CoreLabel.initFromStrings(CoreLabel.java:153)
    at edu.stanford.nlp.ling.CoreLabel.&lt;init&gt;(CoreLabel.java:133)
    at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter$ColumnDocParser.apply(ColumnDocumentReaderAndWriter.java:85)
    at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter$ColumnDocParser.apply(ColumnDocumentReaderAndWriter.java:60)
    at edu.stanford.nlp.objectbank.DelimitRegExIterator.parseString(DelimitRegExIterator.java:67)
    at edu.stanford.nlp.objectbank.DelimitRegExIterator.setNext(DelimitRegExIterator.java:60)
    at edu.stanford.nlp.objectbank.DelimitRegExIterator.&lt;init&gt;(DelimitRegExIterator.java:54)
    at edu.stanford.nlp.objectbank.DelimitRegExIterator$DelimitRegExIteratorFactory.getIterator(DelimitRegExIterator.java:122)
    at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter.getIterator(ColumnDocumentReaderAndWriter.java:54)
    at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.setNextObject(ObjectBank.java:436)
    at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.&lt;init&gt;(ObjectBank.java:415)
    at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:253)
    at edu.stanford.nlp.sequences.ObjectBankWrapper.iterator(ObjectBankWrapper.java:52)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1160)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1111)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1071)
    at edu.stanford.nlp.ie.NERClassifierCombiner.main(NERClassifierCombiner.java:382)
</code></pre>

<p>So not sure what to do next.  Any other combinations.</p>
",stanford-nlp,"<p>During the serialize step you are serializing with:</p>

<p>edu.stanford.nlp.ie.NERClassifierCombiner</p>

<p>During the load step you are loading with:</p>

<p>edu.stanford.nlp.ie.crf.CRFClassifier</p>

<p>So in the second command, use edu.stanford.nlp.ie.NERClassifierCombiner instead and the error should go away.  You serialized an NERClassifierCombiner, but are trying to load it as a CRFClassifier.  Please let me know if you have any other troubles!</p>
",1,0,865,2015-07-16 17:04:45,https://stackoverflow.com/questions/31460407/nlp-ner-processing-errors
UIMA/dkpro: Get type of conjunction,"<p>I am using UIMA in combination with UIMAfit and dkpro and the StanfordParser to parse english sentences.</p>

<p>I can build dependency trees without any problem. For ""and""/""or"" conjunctions, I get a Annotation with class <code>CONJ</code>, which is a subclass of <code>Dependency</code>. As of now, I did not find out, how to check if the found conjunction is an ""AND"" or an ""OR"" conjunction.</p>

<p>Does anybody know how to solve this? I saw examples where the dependencies ""conj_and"" and ""conj_or"" are displayed, but I dont see where they come from.
<a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow"">http://nlp.stanford.edu/software/dependencies_manual.pdf</a></p>

<p>Thanks in advance</p>

<p>Some code for visualization:</p>

<pre><code>// CONJ a;
// StringBuilder sb
Token dependent = a.getDependent();
Token governor = a.getGovernor();

sb.append(""Dependent: "");
sb.append(dependent);
sb.append("", "");

sb.append(""Governor: "");
sb.append(governor);

// How to check type conj_and/conj_or?
</code></pre>
","nlp, stanford-nlp, uima, dkpro-core","<p>The method to get the label of the dependency relation is called</p>

<pre><code>getDependencyType()
</code></pre>

<p>Mind that the <code>conj_or</code> is a ""collapsed dependency"". If you want to get these, you have to explicitly set the dependency mode when invoking the DKPro Core StanfordParser component, e.g. using <code>COLLAPSED</code> or maybe <code>CC_PROPAGATED</code>:</p>

<pre><code>AnalysisEngineFactory.createEngineDescription(StanfordParser.class,
  StanfordParser.PARAM_MODE, StanfordParser.DependenciesMode.COLLAPSED)
</code></pre>

<p>The default setting for this parameter is <code>TREE</code>.</p>

<p>See also: </p>

<ul>
<li><a href=""http://dkpro.github.io/dkpro-core/releases/1.7.0/apidocs/index.html?de/tudarmstadt/ukp/dkpro/core/stanfordnlp/StanfordParser.DependenciesMode.html"" rel=""nofollow"">DKPro Core Stanford.Parser.DEpendenciesMode Javadoc</a></li>
<li><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/index.html?edu/stanford/nlp/trees/GrammaticalStructure.html"" rel=""nofollow"">Stanford CoreNLP GrammaticalStructure Javadoc</a></li>
</ul>

<p><em>Disclosure: I am a member of the DKPro Core team</em>.</p>
",2,0,135,2015-07-20 11:00:35,https://stackoverflow.com/questions/31514775/uima-dkpro-get-type-of-conjunction
Key set for ArrayCoreMap class,"<p>I am having trouble implementing the ArrayCoreMap class: I am having trouble setting a key with a value. This is what I have, but the compiler is showing an error:</p>

<pre><code>CoreMap sentence = new ArrayCoreMap();
sentence.set(""Dog"", ""Thomas"");
</code></pre>

<p>I read the documentation of the API and this is what is says: <code>public &lt;VALUE&gt; VALUE set(java.lang.Class&lt;? extends TypesafeMap.Key&lt;VALUE&gt;&gt; key, VALUE value)</code></p>

<p>So my question is what do I have to do to fix the error. I do not know how to use the <code>TypesafeMap.Key&lt;VALUE&gt;</code> to make my key. The API says this: ""The classes that implement Key are the keys themselves - not instances of those classes."" I do not know what the API means by this. What do I have to do to fix this error?</p>
","java, stanford-nlp","<p>The <code>set</code> method needs its first argument to a type that extends  <code>TypesafeMap.Key&lt;VALUE&gt;</code>, while your first argument is a <code>String</code>. Take a look <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/util/TypesafeMap.Key.html"" rel=""nofollow"">here</a> to see the list of valid types. The key can, for instance, be <code>CoreAnnotations.LemmaAnnotation</code> (implements <code>TypesafeMap.Key&lt;String&gt;</code>).</p>
",0,0,66,2015-07-21 15:47:03,https://stackoverflow.com/questions/31543809/key-set-for-arraycoremap-class
simple scala program gives Error: java.lang.IncompatibleClassChangeError,"<p>I'm using stanford's topic modelling toolkit</p>

<p>This is a simple scala program running on eclipse</p>

<p>Why do i keep getting this error  <code>Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Implementing class</code></p>

<pre><code>import scalanlp.io._;

object Main {
  def main(args: Array[String]) {

    println(""added value""+c);

    val pubmed = CSVFile(""pubmed-oa-subset.csv"");

    println(""Success: "" + pubmed.data.size + "" records"");


  }
}
</code></pre>

<p>Full error here</p>

<pre><code>Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Implementing class
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at scalanlp.io.CSVFile$.CSVFileAsParcel(CSVFile.scala:73)
    at Main$.main(t1.scala:27)
    at Main.main(t1.scala)
</code></pre>
","java, scala, stanford-nlp","<p>The code supplied is correct... The problem is (most probably) a Scala version problem. The jar is compiled with Scala 2.8 (5 years ago), as quote from <a href=""http://nlp.stanford.edu/software/tmt/tmt-0.4/"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/tmt/tmt-0.4/</a>:</p>

<blockquote>
  <p>TMT was written during 2009-10 in what is now a quite old version of
  Scala, using a linear algebra library that is also no longer
  developed. Some people still use it and find it a friendly piece of
  software for LDA and Labeled LDA models, and more power to you, but at
  this point we really can't offer any support or fix any problems.
  Sorry about that.</p>
</blockquote>

<p>The <code>IncompatibleClassChangeError</code> as described here <a href=""https://stackoverflow.com/q/1980452/928952"">What causes java.lang.IncompatibleClassChangeError?</a>, is due to a binary incompatibility. So the only option is to install an old version of Scala, preferable 2.8.0.</p>
",1,0,1107,2015-07-23 09:36:17,https://stackoverflow.com/questions/31583536/simple-scala-program-gives-error-java-lang-incompatibleclasschangeerror
How to initialise JVM with larger maximum heap size using rJava,"<p>I am attempting to make use of the Stanford NLP tools (Java) in R, using the rJava package. When attempting to create a StanfordCoreNLP object I get the following error:</p>

<blockquote>
  <p>Error in .jnew(""edu/stanford/nlp/pipeline/StanfordCoreNLP"", props) : java.lang.OutOfMemoryError: Java heap space</p>
</blockquote>

<p>To resolve this, I have attempted to initialise the JVM with a larger maximum heap size, using variations of the following code:</p>

<pre><code>.jinit(parameters=c(""-Xms1g"",""-Xmx4g""))
</code></pre>

<p>When the maximum heap is set to 1GB using <code>-Xmx1g</code> the JVM loads but I continue to get the OutOfMemoryError. When the maximum heap size is set to 2 or 3 GB (<code>-Xmx2g</code> or <code>-Xmx3g</code>), R will stop responding. When set to 4GB or higher <code>-Xmx4g</code> I will get the following message:</p>

<blockquote>
  <p>Error in .jinit(parameters = c(""-Xms1g"", ""-Xmx4g""), force.init = TRUE) : Cannot create Java virtual machine (-6)</p>
</blockquote>

<p>How do you successfully initialise the JVM using rJava to values larger than 1GB? I am using 32bit versions of Java (v8 u51) and R (v3.2.0)</p>
","r, jvm, stanford-nlp, rjava","<blockquote>
  <p>I am using 32bit versions of Java (v8 u51) and R (v3.2.0)</p>
</blockquote>

<p>That's your problem right there. Switch to 64bit versions.</p>
",0,2,1402,2015-07-24 13:59:13,https://stackoverflow.com/questions/31612488/how-to-initialise-jvm-with-larger-maximum-heap-size-using-rjava
Output &quot;file--token--entity&quot; using Stanford NER,"<p>I want to use Stanford NER in C# to read all files in a folder and output the result into one file in the format ""file  token  entity""</p>

<p>Here is what I have:</p>

<pre class=""lang-c# prettyprint-override""><code>namespace stanfordNER
{
    class Program
    {
        public static CRFClassifier Classifier = CRFClassifier.getClassifierNoExceptions(@""english.all.3class.distsim.crf.ser.gz"");

        static void Main(string[] args)
        {
            Console.WriteLine(""directory address?"");
            string dir = Console.ReadLine();

            //Reads all files in directory
            string[] files = System.IO.Directory.GetFiles(dir);
            foreach (string f in files)
            {
                //Get the document name
                string docNo = Path.GetFileName(Path.GetFullPath(f).TrimEnd(Path.DirectorySeparatorChar));
                Console.WriteLine(docNo);

                string docText = System.IO.File.ReadAllText(f); 

                var classified = Classifier.classifyFile(f).toArray();

                //Error here when running
                //Should output the entities,**this part is the work of Stewart Whiting (STEWH)
                for (int i = 0; i &lt; classified.Length; i++)
                {
                    Triple triple = (Triple)classified[i];

                    int second = Convert.ToInt32(triple.second().ToString());
                    int third = Convert.ToInt32(triple.third().ToString());

                    Console.WriteLine(docNo + '\t' + triple.first().ToString() + '\t' +                              docText.Substring(second, third - second));
                }
            }
        }
    }
}
</code></pre>

<p>I get a invalid cast exception error at ""triple"".  I don't understand how to use the triple function.</p>

<p>example of the output I want:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""true"">
<div class=""snippet-code snippet-currently-hidden"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>wiki-ms      ORGANIZATION    Microsoft Corporation
wiki-ms      LOCATION        Redmond
wiki-ms      LOCATION        Washington
wiki-ms      ORGANIZATION    Microsoft
wiki-ms      ORGANIZATION    Microsoft Office
wiki-ms      ORGANIZATION    Microsoft
wiki-ms      PERSON          Bill Gates
wiki-ms      PERSON          Paul Allen
wiki-ms      ORGANIZATION    Microsoft
wiki-ms      ORGANIZATION    Microsoft</code></pre>
</div>
</div>
</p>

<p>Thanks in advance!  I'm a manufacturing engineer so my programming knowledge is pretty bad.</p>

<p>If you have a way to filter duplicates and/or similar entities that would be an added bonus!</p>

<p>Thanks to Stewart Whiting. <a href=""http://www.stewh.com/"" rel=""nofollow"">His Site</a></p>
","c#, stanford-nlp, named-entity-recognition","<p>I figured it out, just had to change</p>

<p><code>var classified = Classifier.classifyFile(f).toArray();</code></p>

<p>to</p>

<pre><code>var classified = Classifier.classifyToCharacterOffsets(docText).toArray();
</code></pre>

<p>thanks.</p>
",0,0,282,2015-07-24 16:44:04,https://stackoverflow.com/questions/31615731/output-file-token-entity-using-stanford-ner
"How to get &quot;Universal dependencies, enhanced&quot; in response from Stanford coreNLP?","<p>I am playing around with the Stanford coreNLP parser and I am having a small issue that I assume is just something stupid I'm missing due to my lack of experience. I am currently using the node.js stanford-corenlp wrapper module with the latest full Java version of Stanford CoreNLP.</p>

<p>My current results are returning somehting similar to the ""Collapsed Dependencies with CC processed"" data here: <a href=""http://nlp.stanford.edu/software/example.xml"" rel=""nofollow"">http://nlp.stanford.edu/software/example.xml</a></p>

<p>I am trying to figure out how I can get the dependencies titled ""Universal dependencies, enhanced"" as show here: <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/index.jsp</a></p>

<p>If anyone can shed some light on even just what direction I need to research more about, it would be extremely helpful. Currently Google has not been helping much with the specific ""Enhanced"" results and I am just trying to find out what I need to pass,call or include in my annotators to get the results shown at the link above. Thanks for your time!</p>
","nlp, stanford-nlp","<p>Extra (enhanced) dependencies can be enabled in the depparse annotator by using its 'depparse.extradependencies' option.</p>

<p>According to <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a> it is set to NONE by default, and can be set to SUBJ_ONLY or MAXIMAL.</p>
",3,3,2640,2015-07-27 21:05:08,https://stackoverflow.com/questions/31663296/how-to-get-universal-dependencies-enhanced-in-response-from-stanford-corenlp
Error Creating TokenRegex Rules,"<p>I am creating a TokenRegex Rules list. This is what I have:</p>

<pre><code>$STARTING_SEQUENCE = (/start/|/begin/)

{
  ruleType: ""tokens"",
  pattern: ([{lemma:$STARTING_SEQUENCE}]), 
  result: ""START""
}
</code></pre>

<p>When I compile my code, the compiler gives me this error:</p>

<pre><code>Reading TokensRegex rules from tr.txt
Exception in thread ""main"" java.lang.RuntimeException: Error parsing file: tr.txt
at edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.createExtractorFromFile(CoreMapExpressionExtractor.java:258)
at MedicalTranscript.x(MedicalTranscript.java:37)
at MedicalTranscript.main(MedicalTranscript.java:76)
Caused by: java.lang.ClassCastException: edu.stanford.nlp.ling.tokensregex.TokenSequencePattern cannot be cast to java.lang.String
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.CoreMapVarValue(TokenSequenceParser.java:1673)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.AttrValue(TokenSequenceParser.java:1534)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.CoreMapNode(TokenSequenceParser.java:1434)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeBasic(TokenSequenceParser.java:1411)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeGroup(TokenSequenceParser.java:1378)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeDisjConj(TokenSequenceParser.java:1317)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.BracketedNode(TokenSequenceParser.java:1178)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexBasic(TokenSequenceParser.java:884)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexDisjConj(TokenSequenceParser.java:1071)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegex(TokenSequenceParser.java:841)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.BasicValue(TokenSequenceParser.java:383)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.ValueExpression(TokenSequenceParser.java:292)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.Expression(TokenSequenceParser.java:210)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.FieldValue(TokenSequenceParser.java:345)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.CompositeFieldValue(TokenSequenceParser.java:333)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.Rule(TokenSequenceParser.java:122)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.RuleList(TokenSequenceParser.java:107)
at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.getExpressionExtractor(TokenSequenceParser.java:22)
at edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.createExtractorFromFile(CoreMapExpressionExtractor.java:254)
... 2 more
</code></pre>

<p>What am I doing wrong? </p>
","java, stanford-nlp","<p>Try  </p>

<pre><code>$STARTING_SEQUENCE = ""/start|begin/""
</code></pre>

<p>Because it is a regular expression for matching the string field lemma, it needs to be a string expressing a normal regular expression over string.</p>

<p>You may also want to do something with the result or use actions to do some annotation:</p>

<p>Example:</p>

<pre><code>{
  ruleType: ""tokens"",
  pattern: ([{lemma:$STARTING_SEQUENCE}]), 
  action:  ( Annotate($0, ner, ""START"") )
}
</code></pre>
",1,0,437,2015-07-28 00:21:47,https://stackoverflow.com/questions/31665510/error-creating-tokenregex-rules
Natural Logic Inference,"<p>I've been trying to use the Natural Logic Inference component (Naturalli) packaged with Stanford CoreNLP 3.5.2 to extract relation triples...however upon creating a new OpenIE instance I get the following exception:</p>

<pre><code>Could not load affinity model at edu/stanford/nlp/naturalli/: Could not find a part of the path '...\edu\stanford\nlp\naturalli\pp.tab.gz'
</code></pre>

<p>I tried searching for the pp.tab.gz file on the web but I couldn't find it. Then I tried to get around by disabling the affinity:</p>

<pre><code>Properties props = new Properties();
props.put(""ignoreaffinity"", true);
OpenIE ie = new OpenIE(props);
</code></pre>

<p>But then I started getting this following exception:</p>

<pre><code>Could not load clause splitter model at edu/stanford/nlp/naturalli/clauseSplitterModel.ser.gz: Unable to resolve ""edu/stanford/nlp/naturalli/clauseSplitterModel.ser.gz"" as either class path, filename or URL
</code></pre>

<p>Same issue with this file...I couldn't find it anywhere.
Any help regarding how to solve these issues is greatly appreciated! Thanks for everyone in advanced!</p>
","nlp, stanford-nlp","<p>This was recently put up, there are some downloads available here:</p>

<p><a href=""http://nlp.stanford.edu/software/openie.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/openie.shtml</a></p>

<p>I would recommend using the jars pointed to there instead of the Stanford CoreNLP 3.5.2 release.</p>
",1,1,336,2015-07-28 07:04:03,https://stackoverflow.com/questions/31669473/natural-logic-inference
Stanford Core NLP: Entity type non deterministic,"<p>I had built a java parser using Stanford Core NLP. I am finding an issue in getting the consistent results with the CORENLP object. I am getting the different entity types for the same input text. It seems like a bug to me in CoreNLP. Wondering if any of the StanfordNLP users have encountered this issue and found workaround for the same. This is my Service class which I am instantiating and reusing.  </p>

<pre><code>    class StanfordNLPService {
        //private static final Logger logger = LogConfiguration.getInstance().getLogger(StanfordNLPServer.class.getName());
        private StanfordCoreNLP nerPipeline;
       /*
           Initialize the nlp instances for ner and sentiments.
         */
        public void init() {
            Properties nerAnnotators = new Properties();
            nerAnnotators.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
            nerPipeline = new StanfordCoreNLP(nerAnnotators);


        }

        /**
         * @param text               Text from entities to be extracted.

         */
        public void printEntities(String text) {

            //        boolean tracking = PerformanceMonitor.start(""StanfordNLPServer.getEntities"");
            try {

                // Properties nerAnnotators = new Properties();
                // nerAnnotators.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
                // nerPipeline = new StanfordCoreNLP(nerAnnotators); 
               Annotation document = nerPipeline.process(text);
                // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
                List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

                for (CoreMap sentence : sentences) {
                    for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                        // Get the entity type and offset information needed.
                        String currEntityType = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);  // Ner type
                        int currStart = token.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class);    // token offset_start
                        int currEnd = token.get(CoreAnnotations.CharacterOffsetEndAnnotation.class);        // token offset_end.
                        String currPos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);           // POS type
                        System.out.println(""(Type:value:offset)\t"" + currEntityType + "":\t""+ text.substring(currStart,currEnd)+""\t"" + currStart);
                    }
                }
            }catch(Exception e){
                e.printStackTrace();

            }
        }

    }
Discrepancy result: type changed from MISC to O from the initial use.
Iteration 1:
(Type:value:offset) MISC:   Appropriate 100
(Type:value:offset) MISC:   Time    112
Iteration 2:
(Type:value:offset) O:  Appropriate 100
(Type:value:offset) O:  Time    112
</code></pre>
","java, stanford-nlp","<p>I've looked over the code some, and here is a possible way to resolve this:</p>

<p>What you could do to solve this is load each of the 3 serialized CRF's with useKnownLCWords set to false, and serialize them again.  Then supply the new serialized CRF's to your StanfordCoreNLP.</p>

<p>Here is a command for loading a serialized CRF with useKnownLCWords set to false, and then dumping it again:</p>

<p>java -mx600m -cp ""*:."" edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -useKnownLCWords false -serializeTo classifiers/new.english.all.3class.distsim.crf.ser.gz</p>

<p>Put whatever names you want to obviously!  This command assumes you are in stanford-corenlp-full-2015-04-20/ and have a directory classifiers with the serialized CRF's.  Change as appropriate for your set up.</p>

<p>This command should load the serialized CRF, override with the useKnownLCWords set to false, and then re-dump the CRF to new.english.all.3class.distsim.crf.ser.gz</p>

<p>Then in your original code:</p>

<pre><code>nerAnnotators.put(""ner.model"",""comma-separated-list-of-paths-to-new-serialized-crfs"");
</code></pre>

<p>Please let me know if this works or if it's not working, and I can look more deeply into this!</p>
",1,4,738,2015-07-28 14:46:57,https://stackoverflow.com/questions/31679761/stanford-core-nlp-entity-type-non-deterministic
How to load a specific classifier in StanfordCoreNLP,"<p>Wondering is there a way to load specific classier in StanfordCoreNLP. I am trying to resolve an issue where 3 of the classifiers that gets loaded by default the third classifier is not reliably returning the ner tag and resulting in inconsistency in the app. Want to know Loading just the english.all.3class is good enough of for basic named entity tagging and what is the relevance of the other two in the following list.</p>

<blockquote>
  <p>edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz
  edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz 
  edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz</p>
</blockquote>
",stanford-nlp,"<p>Yes you should be fine if you specify the path in the model jar.</p>

<p>Basically you can set ""ner.model"" to a comma separated list of the serialized crf's you wish to use, so if you want to exclude one of them, just supplying the two models you want will work fine.</p>

<p>And to provide some clarity, the three models have been trained on different data sets.  </p>

<p>The all.3class is trained on 7 data sources that have (person, organization, location, none) tagged.</p>

<p>The muc.7class is trained on data from the MUC-7 Named Entity Task, and includes (date,location,money,organization,percent,person,time).  More info:</p>

<p><a href=""https://catalog.ldc.upenn.edu/LDC2001T02"" rel=""nofollow"">https://catalog.ldc.upenn.edu/LDC2001T02</a></p>

<p><a href=""http://www-nlpir.nist.gov/related_projects/muc/proceedings/ne_task.html"" rel=""nofollow"">http://www-nlpir.nist.gov/related_projects/muc/proceedings/ne_task.html</a></p>

<p>The conll.4class is trained on data from the CONLL 2003 NER corpus, and includes (person,organization,location,misc).</p>

<p><a href=""http://www.cnts.ua.ac.be/conll2003/ner/"" rel=""nofollow"">http://www.cnts.ua.ac.be/conll2003/ner/</a></p>
",0,0,1593,2015-07-29 22:04:23,https://stackoverflow.com/questions/31711995/how-to-load-a-specific-classifier-in-stanfordcorenlp
How to retrieve all variants of a lexeme in Java?,"<p>I am searching for a way to retrieve all variants of the lexeme of a specific word.</p>

<p>Example: running -> (run, runs, ran, running…)</p>

<p>I tried out Stanford NLP according to <a href=""https://stackoverflow.com/questions/1578062/lemmatization-java"">this post</a>. However, the lemma-annotator only retrieves the lemma (running -> run), not the complete set of variants. Is there a way to do this with Stanford NLP or another Java Lib/Framework?</p>

<p>Clarification: I do not search for a stemmer. Also, I would like to avoid programming a new algorithm from scratch to crawl WordNet or similar dictionaries.</p>
","java, nlp, stanford-nlp, lemmatization","<p>The short answer is that a standard NLP library or toolkit is unlikely to solve this problem. Like Stanford NLP, most libraries will only provide a mapping from <code>word --&gt; lemma</code>. Note that this is a many-to-one function, i.e., the inverse function is not well-defined in a word space. It is, however, a well defined function from the space of words to the space of sets of words (i.e., it's a one-to-many mapping in word-space).</p>

<p>Without some form of explicit mapping being maintained, it is impossible to <em>generate</em> all the variants from a given lemma. This is a theoretical impossibility because lemmatization is a lossy, one-way function.</p>

<p>You can, however, generate a mapping of <code>lemma --&gt; set-of-words</code> without much coding (and definitely without coding a new algorithm):</p>

<pre><code>// Java
Map&lt;String, Set&lt;String&gt;&gt; inverseLemmaMap = new HashMap&lt;&gt;();

// Guava
Multimap&lt;String, String&gt; inverseLemmaMap = HashMultimap.create();
</code></pre>

<p>Then, as you annotate your corpus using Stanford NLP, you can obtain the lemma and its corresponding token, and populate the above map (or multimap). This way, after a single pass over your dataset, you will have the required inverse lemmatization.</p>

<p>Note that this will be restricted to the corpus/dataset you are using, and not all words in the English language will be included.</p>

<p>Another note is that people often think that an inflection is uniquely determined by the part of speech. This is incorrect:</p>

<pre><code>String s = ""My running was beginning to hurt me. I was running all day.""
</code></pre>

<p>The first instance of <code>running</code> is tagged <code>NN</code>, while the second instance is the present continuous tense of the verb, tagged <code>VBG</code>. This is what I meant by ""lossy, one-way function"" earlier in my answer.</p>
",1,1,365,2015-07-30 12:19:49,https://stackoverflow.com/questions/31723623/how-to-retrieve-all-variants-of-a-lexeme-in-java
Stanford CoreNLP Conll output,"<p>I am using Stanford CoreNLP version 3.4. I want output in Conll format for annotators <code>tokenize</code>, <code>ssplit</code>,<code>pos</code>, <code>lemma</code>, and <code>ner</code>. However on executing the command <code>java -cp stanford-corenlp-3.4.jar:stanford-corenlp-3.4-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-3.4.jar -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -file input.txt -outputFormat conll</code> it shows following error.</p>

<blockquote>
  <blockquote>
    <p>Exception in thread ""main"" java.lang.IllegalArgumentException: No enum constant edu.stanford.nlp.pipeline.StanfordCoreNLP.OutputFormat.CONLL
        at java.lang.Enum.valueOf(Enum.java:236)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$OutputFormat.valueOf(StanfordCoreNLP.java:86)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1167)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1448)</p>
  </blockquote>
</blockquote>

<p>P.S: I don't want to annotate for dependencies.
any suggestions?</p>
","output, stanford-nlp","<p>The CoNLL output formatter was added in version 3.5.0. Upgrade your version and this error should go away.</p>
",2,1,867,2015-08-01 17:32:55,https://stackoverflow.com/questions/31764348/stanford-corenlp-conll-output
What is the Annotation class used to get result data from matched token in Stanford CoreNLP TokensRegex?,"<p>I'm using C# and this is code snippet of how I have tried to get the results based on Stanford Corenlp documentation.</p>

<p>I have no idea which Annotation to use for this:</p>

<pre><code>Annotation document = new Annotation(input);
pipeline.annotate(document);

var sentences = document.get(new CoreAnnotations.SentencesAnnotation().getClass()) as ArrayList;

foreach (CoreMap sentence in sentences)
{
    var tokens = sentence.get(new CoreAnnotations.TokensAnnotation().getClass()) as ArrayList;

    TokenSequencePattern pattern = TokenSequencePattern.compile(""([ner: PERSON]+) /was|is/ /an?/ []{0,3} /painter|artist/"");
    TokenSequenceMatcher matcher = pattern.getMatcher(tokens);

    while (matcher.find())
    {
        String matchedString = matcher.group();
        var matchedTokens = matcher.groupNodes() as ArrayList;

        foreach (CoreLabel matchedToken in matchedTokens)
        {
            //matchedToken.get(new CoreAnnotations.TokensAnnotation().getClass()));
            //Which Annotation class to use in order to get result data from matched token?
        }    
    }
}
</code></pre>
","c#, .net, nlp, stanford-nlp","<p>I'm not sure what you would like to get.  Each token in <code>matchedTokens</code> has the same annotations as other tokens in the sentence.  </p>

<p>If you want to get the first capture group (the <code>([ner: PERSON]+)</code> part), then you should use <code>matcher.group(1)</code> or <code>matcher.groupNodes(1)</code>.  See <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/BasicSequenceMatchResult.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/BasicSequenceMatchResult.html</a> for other functions on the matched result.</p>
",1,0,359,2015-08-02 02:00:18,https://stackoverflow.com/questions/31767931/what-is-the-annotation-class-used-to-get-result-data-from-matched-token-in-stanf
CoreNLP API for N-grams with position,"<p>Does CoreNLP have an API for getting ngrams with position etc.?</p>

<p>For example, I have a string ""I have the best car "". 
if I am using mingrams=1 and maxgrams=2.
I should get the following like below.I know stringutil with ngram function but how to get position.</p>

<pre><code>(I,0)
(I have,0)
(have,1)
(have the,1)
(the,2)
(the best,2) etc etc
</code></pre>

<p>based on the string I am passing.</p>

<p>Any help is really appreciated.</p>

<p>Thanks</p>
","nlp, stanford-nlp, n-gram, pos-tagger","<p>I don't see anything in the utils.  Here is some sample code to help:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*; 
import edu.stanford.nlp.util.*;


public class NGramPositionExample {


    public static List&lt;List&lt;String&gt;&gt; getNGramsPositions(List&lt;String&gt; items, int minSize, int maxSize) {
        List&lt;List&lt;String&gt;&gt; ngrams = new ArrayList&lt;List&lt;String&gt;&gt;();
    int listSize = items.size();
    for (int i = 0; i &lt; listSize; ++i) {
        for (int ngramSize = minSize; ngramSize &lt;= maxSize; ++ngramSize) {
        if (i + ngramSize &lt;= listSize) {
            List&lt;String&gt; ngram = new ArrayList&lt;String&gt;();
            for (int j = i; j &lt; i + ngramSize; ++j) {
            ngram.add(items.get(j));
            }
                    ngram.add(Integer.toString(i));
            ngrams.add(ngram);
        }
        }
    }
    return ngrams;
    }


        public static void main (String[] args) throws IOException {
            String testString = ""I have the best car"";
            List&lt;String&gt; tokens = Arrays.asList(testString.split("" ""));
            List&lt;List&lt;String&gt;&gt; ngramsAndPositions = getNGramsPositions(tokens,1,2);
            for (List&lt;String&gt; np : ngramsAndPositions) {
                System.out.println(Arrays.toString(np.toArray()));
            }
        }
}
</code></pre>

<p>You can just cut and paste that utility method.</p>

<p>This might be a useful functionality to add, so I will put this on our list of things to work on.</p>
",1,1,587,2015-08-05 04:16:33,https://stackoverflow.com/questions/31823347/corenlp-api-for-n-grams-with-position
Algorithms for Natural Language Understanding,"<p>I wanted to know what algorithms I could use for NLU?</p>
<p>For example, let's say I want to start a program, and I have these sentences</p>
<blockquote>
<p>&quot;Let us start&quot;</p>
<p>&quot;Let him start&quot;</p>
</blockquote>
<p>Obviously, the first sentence should start the program, but not the second one (since it doesn't make sense).</p>
<p>Right now, I have am using Stanford's NLP API and have implemented the TokenRegexAnnotator class:</p>
<pre><code>CoreMapExpressionExtractor&lt;MatchedExpression&gt; extractor = CoreMapExpressionExtractor.createExtractorFromFile(env, &quot;tr.txt&quot;);
</code></pre>
<p>So my code &quot;knows&quot; what &quot;Start&quot; should do, that is, &quot;Start&quot; should trigger/start the program. But &quot;Start&quot; could be used with anything, like &quot;Start the car.&quot; In this case, I wouldn't want to &quot;Start&quot; the program because the sentence is about starting a car, not the program. To solve this, I used Stanford's CollapsedDependenciesAnnotation class:</p>
<pre><code>SemanticGraph dependencies = s.get(CollapsedDependenciesAnnotation.class);
Iterable&lt;SemanticGraphEdge&gt; edge_set = dependencies.edgeIterable();
</code></pre>
<p>I used the <code>nsubj</code> dependency to see if the subject was a <code>PRP</code> (pronoun) since I want the program to start only when the subject is a <code>PRP</code>. So when I inputed the sentence &quot;let us start&quot; in my program, the program started. However, when I inputed the sentence &quot;Start the car,&quot; the program didn't start. All is working well...</p>
<p>BUT the program will also start when I input the sentence &quot;Let him start&quot; (as mentioned above). (It starts because &quot;him&quot; is also a pronoun). I do not want the program to start when I input this sentence (because &quot;Let him start&quot; has nothing to do with the starting the program). So how will the program know this? What can I do to solve this problem? Are there algorithms that will let the computer differentiate between &quot;let us start&quot; and &quot;let him start&quot;?</p>
<p>Any ideas on how to solve this problem?</p>
<p>Thank you!</p>
<p>(I hope I am being clear)</p>
","pattern-matching, nlp, semantics, stanford-nlp, lexical-analysis","<p>One way Stanford CoreNLP could help you is its TokensRegex functionality.  With this tool you can write explicit patterns and then tag them in your input text.  Then your code can react based on the presence of certain patterns.</p>

<p>Here are some links with more info:</p>

<p><a href=""http://nlp.stanford.edu/software/tokensregex.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokensregex.shtml</a></p>

<p><a href=""http://nlp.stanford.edu/software/regexner/"" rel=""nofollow"">http://nlp.stanford.edu/software/regexner/</a></p>

<p>I would recommend identifying common expressions that you want to handle that deserve a clear response, and build up so you get decent coverage of what users input.</p>

<p>For instance:</p>

<pre><code>Let us (start|begin).
(Start|begin) the (program|software)
I'm ready to (start|begin)
etc...
</code></pre>

<p>Obviously you could combine these rules and make them increasingly complicated.  But I think a straight forward approach would be to think of the various ways one might express they want to begin and then capture that with rules.</p>
",3,2,1949,2015-08-05 23:46:08,https://stackoverflow.com/questions/31844602/algorithms-for-natural-language-understanding
Additional Named Entity Recognition models for Stanford CoreNLP?,"<p>The <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford CoreNLP</a> library is packaged with models to recognize Time, Location, Organization, Person, Money, Percent, and Dates.  Are there any other general-use models available from other groups that recognize additional things?</p>

<p>Also, if we were to train <a href=""http://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow"">a new model</a> to recognize <em>just</em> band names (for instance), could we run our new model in addition to the packaged ones, or would be have to train the new model to recognize Time, Location, Organization, Person, Money, Percent, Dates, and <em>Bands</em> all together if we wanted to do that?  The documentation does say the existing models themselves cannot be extended.</p>
","nlp, stanford-nlp","<p>You can definitely train a CRFClassifier or RegexNER to recognize band names and incorporate that with the other NER taggers, and your module could exclusively focus on band names.</p>

<p>I would probably recommend using a RegexNER for band names.  Here is the link:</p>

<p><a href=""http://nlp.stanford.edu/software/regexner/"" rel=""nofollow"">http://nlp.stanford.edu/software/regexner/</a></p>

<p>Basically you just create a file with band names, or regular expressions matching band names, and you can then use the standard pipeline to tag text based off of your custom work.</p>

<p>Here is a sample command:</p>

<pre><code>java -mx1g -cp ""*:."" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,ner,regexner"" -file sample_text.txt -regexner.mapping my-band-regexes.txt
</code></pre>
",3,1,462,2015-08-06 21:50:17,https://stackoverflow.com/questions/31866411/additional-named-entity-recognition-models-for-stanford-corenlp
How to get sentiment via stanford corenlp interactive shell?,"<p><a href=""https://i.sstatic.net/vqNI9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vqNI9.png"" alt=""Interactive shell with all annotators including sentiment""></a>I have been trying to get the <em>sentiment value from the stanford corenlp</em>, but it seems in the interactive shell, the sentiment is not given as an output.</p>

<p>I have specified the <em>annotators</em> using the command given in the official website.</p>

<pre><code>java -cp ""*"" -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref, sentiment
</code></pre>

<p>Also, when i tried for getting just the sentiment, then at first asked for other annotators, after providing i didn't give any output at all.</p>

<p>I am getting the interactive shell in both the cases. Like - NLP>
But, there is no sign of sentiment - positive or negative.</p>

<p>Also, if we use the sentimentPipeline instead of nlp.pipeline. It gives the sentiment. Is there any way i can get via the interactive shell or if possible is there any way we can use sentimentPipeline with pexpect?</p>
","stanford-nlp, sentiment-analysis, pexpect, interactive-shell","<p><a href=""https://i.sstatic.net/ZE4YG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZE4YG.png"" alt=""old_version""></a>I was using the older version of Stanford corenlp (3.4.1), but in the recent version (3.5.1), it gives the sentiment.<a href=""https://i.sstatic.net/vOKBW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vOKBW.png"" alt=""new_version""></a></p>
",1,1,321,2015-08-09 09:21:32,https://stackoverflow.com/questions/31902722/how-to-get-sentiment-via-stanford-corenlp-interactive-shell
How to solve decoding while using stanford parser for Chinese text with python,"<p>I want to use Stanford Parser to parse Chinese texts with Python interface. My code is below:</p>

<pre><code>#!~/anaconda/bin/python
# -*- coding: utf-8 -*-

from nltk.parse import stanford

parser = stanford.StanfordParser(path_to_jar='/home/stanford-parser/stanford-parser.jar', path_to_models_jar='/home/stanford-parser/stanford-parser-3.3.0-models.jar',model_path=""/home/stanford-parser/chinesePCFG.ser.gz"",encoding='utf8')

sentences = parser.raw_parse_sents((""我 是 中国 人。"", ""他 来自 美国。""))
print sentences 
</code></pre>

<p>However, when I try to run this code,an decoding error occurs</p>

<pre><code>Traceback (most recent call last):
  File ""/home/test.py"", line 8, in &lt;module&gt;
    sentences = parser.raw_parse_sents((""我 是 中国人。"", ""他 来自 美国。""))
  File ""/home/anaconda/lib/python2.7/site-packages/nltk/parse/stanford.py"", line 176, in raw_parse_sents
    return self._parse_trees_output(self._execute(cmd, '\n'.join(sentences), verbose))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe6 in position 0: ordinal not in range(128)
</code></pre>

<p>I have no idea about why such an error occurs,since my code is edited by <strong>utf-8</strong> just as the second line. Could anyone please help me explain it and solve it? I really need the help from stackoverflow community.</p>
","python, nlp, decode, encode, stanford-nlp","<p>The parser expects a unicode object (you actually told it on creation that you will be using data encoded in UTF-8. However, what you send to it as a parameter are just plain string which are basically just sequences of bytes (in Python 2.x). You can create <a href=""https://docs.python.org/2/howto/unicode.html#unicode-literals-in-python-source-code"" rel=""nofollow"">unicode literals</a> by prepending a string with <code>u</code>, e.g. <code>u""我 是 中国 人""</code></p>

<pre><code>&gt;&gt;&gt; word = u""我 是 中国 人""
&gt;&gt;&gt; type(word)
&lt;type 'unicode'&gt;
&gt;&gt;&gt; print word
我 是 中国 人
</code></pre>

<p>And to convert an existing plain string into a unicode object:</p>

<pre><code>&gt;&gt;&gt; word = ""我 是 中国 人""
&gt;&gt;&gt; type(word)
&lt;type 'str'&gt;
&gt;&gt;&gt; unicode_word = unicode(word, encoding='utf8')
&gt;&gt;&gt; type(unicode_word)
&lt;type 'unicode'&gt;
</code></pre>

<p>If these kind of things cause you trouble, I strongly recommend reading the <a href=""https://docs.python.org/2/howto/unicode.html"" rel=""nofollow"">Unicode HOWTO</a> section of the Python documentation, it will probably make the everything much more clear.</p>

<h3>Bonus</h3>

<p>To convert a plain string representing a <em>Unicode escape</em> sequence to a Unicode string, use the <a href=""https://docs.python.org/2/library/codecs.html#python-specific-encodings"" rel=""nofollow""><code>'unicode_escape'</code> encoding</a>.</p>

<pre><code>&gt;&gt;&gt; type('\u6211')
&lt;type 'str'&gt;
&gt;&gt;&gt; len('\u6211')
6
&gt;&gt;&gt; converted = '\u6211'.decode('unicode_escape')
&gt;&gt;&gt; type(converted)
&lt;type 'unicode'&gt;
&gt;&gt;&gt; len(converted)
1
&gt;&gt;&gt; print converted
我
</code></pre>
",1,0,588,2015-08-10 12:46:15,https://stackoverflow.com/questions/31919980/how-to-solve-decoding-while-using-stanford-parser-for-chinese-text-with-python
NER model to recognize Indian names,"<p>I am planning to use Named Entity Recognition (NER) technique to identify person names (most of which are Indian names) from a given text. I have already explored the CRF-based NER model from Stanford NLP, however it is not quite accurate in recognizing Indian names. Hence I decided to create my own custom NER model via supervised training. I have a fair idea of how to create own NER model using the Stanford NER CRF, but creating a large training corpus with manual annotation is something I would like to avoid, as it is a humongous effort for an individual and secondly obtaining diverse people names from different states of India is also a challenge. Could anybody suggest any automation/programmatic way to prepare a labelled training corpus with at least 100k Indian names?<br>
I have already looked into Facebook and LinkedIn API, but did not find a way to extract 100k number of user's full name from a given location (e.g. India).</p>
","facebook-graph-api, nlp, stanford-nlp, named-entity-recognition, linkedin-api","<p>I ended up doing the following to create NER model to identify Indian names. This may be useful for anybody looking for creating a custom NER model to recognize non-English person names, since most of the publicly available NER models such as the ones from Stanford NLP were trained with English names and hence are more accurate in identifying English (British/American) names. </p>

<ol>
<li>Find an Indian celebrity with Twitter account and having a huge number of followers in Twitter (for my case, I chose Sachin Tendulkar).</li>
<li>Create a program in the language of your choice to call the Twitter REST API (GET followers/list) to get the names of all the followers of the celebrity and save to a file. We can safely assume most of the followers would be Indians. Note that there is an API Rate Limit in place (30 requests per 15 minute window), so the program should be built in to handle that. For our case, we developed the program as a Windows Service which runs every 15 minutes.</li>
<li>Since some Twitter users' names may not be valid person names, it is advisable to add some rule-based logic (like RegEx) to filter seemingly real names and add only those to the file.</li>
<li>Once the file with real names is generated, create another program to create the training data file containing these names labelled/annotated as PERSON as well as non-entity names annotated as OTHER. If you are using Stanford NER CRF Classifier, the program should generate a training (TSV) file  having two columns - one containing the word (token) and the second column mentioning the label.</li>
<li>Once the training corpus is generated programmatically, you can follow the below link to create your custom NER model to recognize Indian names:
<a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#a</a></li>
</ol>
",11,6,11009,2015-08-18 12:53:50,https://stackoverflow.com/questions/32073018/ner-model-to-recognize-indian-names
Example for Stanford NLP Classifier,"<p>I am trying to learn the Stanford NLP Classifier and would like to work on the problem of document classification. Can anyone suggest a place where I can find a working example? I was also looking at the Open NLP libraries and was able to find many working examples, like</p>

<p><a href=""http://tharindu-rusira.blogspot.com/2013/12/opennlp-text-classifier.html"" rel=""nofollow noreferrer"">http://tharindu-rusira.blogspot.com/2013/12/opennlp-text-classifier.html</a></p>

<p>So, as we can see here, it is quite easy to figure out what's going on and create a small working prototype. However, I can't find a simple example for stanford NLP which will show me</p>

<ol>
<li>How to specify training data for a classifier.</li>
<li>How to train a model.</li>
<li>How to ingest test data and run the model.</li>
</ol>

<p>Any suggestions?</p>
","java, machine-learning, stanford-nlp, document-classification","<p>For 1 and 2, have you looked at the examples on the Wiki page <a href=""http://nlp.stanford.edu/wiki/Software/Classifier"" rel=""noreferrer"">http://nlp.stanford.edu/wiki/Software/Classifier</a> ?</p>

<p>For 3, that page also covers testing from the command line. For use in code, the example code in <code>ClassifierDemo.java</code> included in the download should help you get started.</p>
",6,3,6207,2015-08-19 02:21:52,https://stackoverflow.com/questions/32085525/example-for-stanford-nlp-classifier
How to recognize URLs using Stanford CoreNLP,"<p>I am using Stanford CoreNLP to extract various types of information from a given document. I am trying to detect URL patterns and I can see that links beginning with http:// or https:// are recognized properly, but links beginning with ftp://, svn:// etc are broken at ':' and 'ftp' or 'svn' becomes a token instead of the complete link being recognized a token. Due to this, I am not able to use any regex for match.
I know there is a way to tokenize words with whitespaces using tokenize.whitespace.
Is there a way to suppress ':' tokenizing the URL so that the complete link is recognized as a token?</p>
","nlp, stanford-nlp","<p>Unfortunately, there isn't an easy way to just add extra URL patterns, because, for speed reasons, the tokenizer is done as a compiled finite automaton, using JFlex. You can only do it by starting with PTBLexer.flex, editing it, making the new java file with JFlex, setting javac loose on it, etc. For future versions, we're game to add useful patterns that won't detract from accurate tokenization in other places. I've added ""ftp"", ""svn"", and ""svn+ssh"". Anything else you'd like? (You could also put in a pull request.)</p>
",2,0,334,2015-08-22 05:04:33,https://stackoverflow.com/questions/32152520/how-to-recognize-urls-using-stanford-corenlp
Stanford Universal Dependencies on Python NLTK,"<p>Is there any way I can get the Universal dependencies using python, or nltk?I can only produce the parse tree.</p>

<p>Example:</p>

<p>Input sentence:</p>

<pre><code>My dog also likes eating sausage.
</code></pre>

<p>Output:</p>

<pre><code>Universal dependencies

nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
</code></pre>
","python, nlp, nltk, stanford-nlp","<p><a href=""https://github.com/Wordseer/stanford-corenlp-python"" rel=""nofollow"">Wordseer's stanford-corenlp-python fork</a> is a good start as it works with the recent CoreNLP release (3.5.2). However it will give you raw output, which you need manually transform. For example, given you have the wrapper running:</p>

<pre><code>&gt;&gt;&gt; import json, jsonrpclib
&gt;&gt;&gt; from pprint import pprint
&gt;&gt;&gt;
&gt;&gt;&gt; server = jsonrpclib.Server(""http://localhost:8080"")
&gt;&gt;&gt;
&gt;&gt;&gt; pprint(json.loads(server.parse('John loves Mary.')))  # doctest: +SKIP
{u'sentences': [{u'dependencies': [[u'root', u'ROOT', u'0', u'loves', u'2'],
                                   [u'nsubj',
                                    u'loves',
                                    u'2',
                                    u'John',
                                    u'1'],
                                   [u'dobj', u'loves', u'2', u'Mary', u'3'],
                                   [u'punct', u'loves', u'2', u'.', u'4']],
                 u'parsetree': [],
                 u'text': u'John loves Mary.',
                 u'words': [[u'John',
                             {u'CharacterOffsetBegin': u'0',
                              u'CharacterOffsetEnd': u'4',
                              u'Lemma': u'John',
                              u'PartOfSpeech': u'NNP'}],
                            [u'loves',
                             {u'CharacterOffsetBegin': u'5',
                              u'CharacterOffsetEnd': u'10',
                              u'Lemma': u'love',
                              u'PartOfSpeech': u'VBZ'}],
                            [u'Mary',
                             {u'CharacterOffsetBegin': u'11',
                              u'CharacterOffsetEnd': u'15',
                              u'Lemma': u'Mary',
                              u'PartOfSpeech': u'NNP'}],
                            [u'.',
                             {u'CharacterOffsetBegin': u'15',
                              u'CharacterOffsetEnd': u'16',
                              u'Lemma': u'.',
                              u'PartOfSpeech': u'.'}]]}]}
</code></pre>

<p>In case you want to use dependency parser, you can reuse NLTK's DependencyGraph with a bit of effort</p>

<pre><code>&gt;&gt;&gt; import jsonrpclib, json
&gt;&gt;&gt; from nltk.parse import DependencyGraph
&gt;&gt;&gt;
&gt;&gt;&gt; server = jsonrpclib.Server(""http://localhost:8080"")
&gt;&gt;&gt; parses = json.loads(
...    server.parse(
...       'John loves Mary. '
...       'I saw a man with a telescope. '
...       'Ballmer has been vocal in the past warning that Linux is a threat to Microsoft.'
...    )
... )['sentences']
&gt;&gt;&gt;
&gt;&gt;&gt; def transform(sentence):
...     for rel, _, head, word, n in sentence['dependencies']:
...         n = int(n)
...
...         word_info = sentence['words'][n - 1][1]
...         tag = word_info['PartOfSpeech']
...         lemma = word_info['Lemma']
...         if rel == 'root':
...             # NLTK expects that the root relation is labelled as ROOT!
...             rel = 'ROOT'
...
...         # Hack: Return values we don't know as '_'.
...         #       Also, consider tag and ctag to be equal.
...         # n is used to sort words as they appear in the sentence.
...         yield n, '_', word, lemma, tag, tag, '_', head, rel, '_', '_'
...
&gt;&gt;&gt; dgs = [
...     DependencyGraph(
...         ' '.join(items)  # NLTK expects an iterable of strings...
...         for n, *items in sorted(transform(parse))
...     )
...     for parse in parses
... ]
&gt;&gt;&gt;
&gt;&gt;&gt; # Play around with the information we've got.
&gt;&gt;&gt;
&gt;&gt;&gt; pprint(list(dgs[0].triples()))
[(('loves', 'VBZ'), 'nsubj', ('John', 'NNP')),
 (('loves', 'VBZ'), 'dobj', ('Mary', 'NNP')),
 (('loves', 'VBZ'), 'punct', ('.', '.'))]
&gt;&gt;&gt;
&gt;&gt;&gt; print(dgs[1].tree())
(saw I (man a (with (telescope a))) .)
&gt;&gt;&gt;
&gt;&gt;&gt; print(dgs[2].to_conll(4))  # doctest: +NORMALIZE_WHITESPACE
Ballmer     NNP     4       nsubj
has         VBZ     4       aux
been        VBN     4       cop
vocal       JJ      0       ROOT
in          IN      4       prep
the         DT      8       det
past        JJ      8       amod
warning     NN      5       pobj
that        WDT     13      dobj
Linux       NNP     13      nsubj
is          VBZ     13      cop
a           DT      13      det
threat      NN      8       rcmod
to          TO      13      prep
Microsoft   NNP     14      pobj
.           .       4       punct
&lt;BLANKLINE&gt;
</code></pre>

<p>Setting up CoreNLP is not that hard, check <a href=""http://www.eecs.qmul.ac.uk/~dm303/stanford-dependency-parser-nltk-and-anaconda.html"" rel=""nofollow"">http://www.eecs.qmul.ac.uk/~dm303/stanford-dependency-parser-nltk-and-anaconda.html</a> for more details.</p>
",4,3,3349,2015-08-22 07:38:42,https://stackoverflow.com/questions/32153627/stanford-universal-dependencies-on-python-nltk
TypeInitializationException when running Stanford.NLP.CoreNLP example,"<p>I'm trying to run the <strong>F#</strong> sample from <a href=""http://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html"" rel=""nofollow"">http://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html</a>.</p>

<p>The project is .NET porting of Java Stanford NLP libraries, so I'm wondering if this could be an <strong>IKVM</strong> issue rather.</p>

<p>I'm running the sample code inside an <strong>F#</strong> console application for <strong>NET 4.5</strong> and <strong>F# Core 3.1</strong>. I've also downloaded <code>models</code> from the correct jar and changed the path in the code.</p>

<p>This is the raised exception:</p>

<pre>
System.TypeInitializationException was unhandled
Message: An unhandled exception of type 'System.TypeInitializationException' occurred in StanfordNLPNETFs.exe
Additional information: The type initializer for '.$Program' threw an exception.
</pre>

<p>In the second post of the issue I reported on GitHub, there's also a screenshot: <a href=""https://cloud.githubusercontent.com/assets/1194228/9424606/4aef5e0e-48f2-11e5-9690-c2668303d225.png"" rel=""nofollow"">https://cloud.githubusercontent.com/assets/1194228/9424606/4aef5e0e-48f2-11e5-9690-c2668303d225.png</a>.</p>

<p>Thanks in advance.</p>
",".net, f#, stanford-nlp","<p>Most likely you're hitting the <a href=""https://stackoverflow.com/questions/28389564/stanford-corenlp-error-creating-edu-stanford-nlp-time-timeexpressionextractorimp"">known issue</a> of Stanford-NLP distribution that comes and goes with nuget versions/java distributions.</p>

<p>In any case, adding the following property setting to annotation pipeline configuration should allow the given sample code to run successfully with latest <code>Stanford.NLP.CoreNLP ver 3.5.2</code> nuget:</p>

<pre><code>props.setProperty(""ner.useSUTime"", ""false"") |&gt; ignore
</code></pre>
",2,2,648,2015-08-23 05:13:38,https://stackoverflow.com/questions/32163400/typeinitializationexception-when-running-stanford-nlp-corenlp-example
Unrecoverable error while loading a tagger model using nugetpackages,"<p>I have been trying to create and run a simple program using the stanford-corenlp-3.5.2 nugetpackage.</p>
<p>However after looking up some beginner code to start I have found the following code props.setProperty(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, lemma, ner, parse, dcoref&quot;);(Link to code example : <a href=""http://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html"" rel=""nofollow noreferrer"">http://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html</a>)</p>
<p>But whenever the console app loads pos it fires off a runtime error stating that it could not load a tagger.</p>
<p>I am wondering if i am missing any nugetpackages or if there is additional setup I have to go through. (Note. any time i have tried to add say the postagger nuget package i then get an error saying that the class Annotation is referenced in two dlls.)</p>
<p>I have found that if i remove some of the properties the application will run correctly so the new line looks like this
&quot;props.setProperty(&quot;annotators&quot;, &quot;tokenize, ssplit&quot;);</p>
<p>Any help to get past the runtime error so I can continue further analyse of the sample text would be greatly appreciated. Thank You.</p>
<p>Attached picture for reference.(apparently I need more reputation in order to post a pic but when I can I will do so immediately :) Edit I have added the picture now :)</p>
<p>stack trace at line exception is as follows:</p>
<pre><code>at edu.stanford.nlp.pipeline.AnnotatorFactories.4.create()
at edu.stanford.nlp.pipeline.AnnotatorPool.get(String name)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(Properties , Boolean , AnnotatorImplementations )
at edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(Properties props, Boolean enforceRequirements)
at edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(Properties props)
at ConApplicationSabrinaNLP.TestClass.donlp() in c:\Users\Justin\Documents\Visual Studio 2013\Projects\ConApplicationSabrinaNLP\ConApplicationSabrinaNLP\TestClass.cs:line 28
at ConApplicationSabrinaNLP.Program.Main(String[] args) in c:\Users\Justin\Documents\Visual Studio 2013\Projects\ConApplicationSabrinaNLP\ConApplicationSabrinaNLP\Program.cs:line 20
at System.AppDomain._nExecuteAssembly(RuntimeAssembly assembly, String[] args)
at System.AppDomain.ExecuteAssembly(String assemblyFile, Evidence assemblySecurity, String[] args)
at Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly()
at System.Threading.ThreadHelper.ThreadStart_Context(Object state)
at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)
at System.Threading.ThreadHelper.ThreadStart()
</code></pre>
","c#, stanford-nlp, nuget-package","<p>The problem is that you haven't downloaded the models. The Nuget package doesn't work by itself. 
You must download lastest version here <a href=""http://www-nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">POS-Tagger</a></p>

<p>Once you have done that put it in your project directory. Then point the code to it. In the ""3.6.0  2015-12-09  Updated for compatibility"" version the tagger has  a different name, ""english-bidirectional-distsim.tagger"" for example.
Make sure you point the code to the correct folder and files and it will work.</p>

<p>The following is a working example from my windows forms project. </p>

<pre><code>using java.io;
using java.util;
using edu.stanford.nlp.ling;
using edu.stanford.nlp.tagger.maxent;
using Console = System.Console;
using System.IO;
using System.Windows.Forms;

namespace Stanford.NLP.POSTagger.CSharp
{
    class PosTagger

    {
        // get the base folder for the project
        public static string GetAppFolder()
        {
            return Path.GetDirectoryName(Application.ExecutablePath).Replace(@""*your project directory here*\bin\Debug"", string.Empty);
        }

        public void testTagger()
        {
            var jarRoot = Path.Combine(GetAppFolder(), @""packages\stanford-postagger-2015-12-09"");
            Console.WriteLine(jarRoot.ToString());
            var modelsDirectory = jarRoot + @""\models"";

            // Loading POS Tagger
            var tagger = new MaxentTagger(modelsDirectory + @""\english-bidirectional-distsim.tagger"");

            // Text for tagging
            var text = ""A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text""
                       + ""in some language and assigns parts of speech to each word (and other token),""
                       + "" such as noun, verb, adjective, etc., although generally computational ""
                       + ""applications use more fine-grained POS tags like 'noun-plural'."";

            var sentences = MaxentTagger.tokenizeText(new java.io.StringReader(text)).toArray();
            foreach (ArrayList sentence in sentences)
            {
                var taggedSentence = tagger.tagSentence(sentence);
                Console.WriteLine(Sentence.listToString(taggedSentence, false));
            }
        }
    }
}
</code></pre>
",1,4,1046,2015-08-30 19:18:09,https://stackoverflow.com/questions/32300233/unrecoverable-error-while-loading-a-tagger-model-using-nugetpackages
Stanford NN dependency parser: Unrecoverable error while loading a tagger model,"<p>I am trying to test the <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">new Stanford Dependency parser</a> which works with Neural Networks. I am trying to run the demos which are included in the zip file. The files <code>ParserDemo.java</code> and <code>ParserDemo2.java</code> work fine. However the file <code>DependencyParserDemo.java</code>:</p>

<pre><code>import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.parser.nndep.DependencyParser;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import edu.stanford.nlp.trees.GrammaticalStructure;

import java.io.StringReader;
import java.util.List;

/**
 * Demonstrates how to first use the tagger, then use the NN dependency
 * parser. Note that the parser will not work on untagged text.
 *
 * @author Jon Gauthier
 */
public class DependencyParserDemo {
  public static void main(String[] args) {
    String modelPath = DependencyParser.DEFAULT_MODEL;
    String taggerPath = ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"";

    for (int argIndex = 0; argIndex &lt; args.length; ) {
      switch (args[argIndex]) {
        case ""-tagger"":
          taggerPath = args[argIndex + 1];
          argIndex += 2;
          break;
        case ""-model"":
          modelPath = args[argIndex + 1];
          argIndex += 2;
          break;
        default:
          throw new RuntimeException(""Unknown argument "" + args[argIndex]);
      }
    }

    String text = ""I can almost always tell when movies use fake dinosaurs."";

    MaxentTagger tagger = new MaxentTagger(taggerPath);
    DependencyParser parser = DependencyParser.loadFromModelFile(modelPath);

    DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(text));
    for (List&lt;HasWord&gt; sentence : tokenizer) {
      List&lt;TaggedWord&gt; tagged = tagger.tagSentence(sentence);
      GrammaticalStructure gs = parser.predict(tagged);

      // Print typed dependencies
      System.err.println(gs);
    }
  }
}
</code></pre>

<p>Throws an error:</p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:769)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:297)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:262)
    at DependencyParserDemo.main(DependencyParserDemo.java:40)
Caused by: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:448)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:764)
    ... 3 more
</code></pre>

<p>Could someone tell me what am I doing wrong?</p>
","java, nlp, stanford-nlp","<p>It seems like your path to the file ""english-left3words-distsim.tagger"" is not correct. Check if path you provide is correct. You can also try it with absolute path. </p>

<h1>Belphegor replied:</h1>

<blockquote>
  <p>I solved it with absolute path. I first created the following folders in the src folders: 
     edu/stanford/nlp/models/pos-tagger/english-left3words/ 
     and inside I pasted the file english-left3words-distsim.tagger 
     (which is in the POS-tagger file stanford-postagger-full-2015-04-20.zip). 
     After this - it worked.</p>
</blockquote>
",1,0,235,2015-09-01 07:37:15,https://stackoverflow.com/questions/32326065/stanford-nn-dependency-parser-unrecoverable-error-while-loading-a-tagger-model
Sentiment analysis with stanford nlp does not work,"<p>I am trying to use stanford nlp to get the sentiment of a text:
Here is my code:</p>

<pre><code>import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class SentimentAnalyzer {

    public static void main(String[] args) {
        findSentiment("""");
    }

    public static void findSentiment(String line) {
        line = ""I started taking the little pill about 6 years ago."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        int mainSentiment = 0;
        if (line != null &amp;&amp; line.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(line);
            for (CoreMap sentence : annotation
                    .get(CoreAnnotations.SentencesAnnotation.class)) {
                Tree tree = sentence
                        .get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                String partText = sentence.toString();
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        if (mainSentiment == 2 || mainSentiment &gt; 4 || mainSentiment &lt; 0) {
            System.out.println(""Neutral "" + line);
        }
        else{
        }
        /*
         * TweetWithSentiment tweetWithSentiment = new TweetWithSentiment(line,
         * toCss(mainSentiment)); return tweetWithSentiment;
         */

    }
}
</code></pre>

<p>Also I use the instruction from this link:
<a href=""https://blog.openshift.com/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java/"" rel=""nofollow noreferrer"">https://blog.openshift.com/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java/</a></p>

<p>But I get the following error:</p>

<pre><code>Exception in thread ""main"" java.lang.NullPointerException
at edu.stanford.nlp.rnn.RNNCoreAnnotations.getPredictedClass(RNNCoreAnnotations.java:58)
at SentimentAnalyzer.findSentiment(SentimentAnalyzer.java:27)
at SentimentAnalyzer.main(SentimentAnalyzer.java:14)
</code></pre>

<p>which point to this line:</p>

<pre><code>    Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
</code></pre>
","java, stanford-nlp, sentiment-analysis","<p>Use this instead:</p>

<pre><code>Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
</code></pre>

<p><strong>Edit:</strong>
To get positive, negative, and neutral comments, use this snippet:</p>

<pre><code>switch (mainSentiment) {
        case 0:
            return ""Very Negative"";
        case 1:
            return ""Negative"";
        case 2:
            return ""Neutral"";
        case 3:
            return ""Positive"";
        case 4:
            return ""Very Positive"";
        default:
            return """";
        }
</code></pre>
",6,1,1516,2015-09-01 16:08:47,https://stackoverflow.com/questions/32336293/sentiment-analysis-with-stanford-nlp-does-not-work
How to read .tagger file that comes with stanford pos tagger,"<p>I cant read the .tagger file which comes with the stanford maxent tagger for my language with a text editor. How could I view the contents of the .tagger file. </p>
","stanford-nlp, pos-tagger, maxent, treetagger","<p>The .tagger file is a serialized version of a POS tagging model, so it is not human readable.</p>
",1,0,205,2015-09-02 12:27:59,https://stackoverflow.com/questions/32353165/how-to-read-tagger-file-that-comes-with-stanford-pos-tagger
About the Stanford CoreNLP in chinese model,"<p>How to use the chinese model, and I download the ""stanford-corenlp-3.5.2-models-chinese.jar"" in my classpath and I copy </p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.5.2&lt;/version&gt;
    &lt;classifier&gt;models-chinese&lt;/classifier&gt;
&lt;/dependency&gt;
</code></pre>

<p>to pom.xml file. In additional, my input.txt is </p>

<p>因出席中國大陸閱兵引發爭議的國民黨前主席連戰今晚金婚宴，立法院長王金平說，已向連戰恭喜，等一下回南部。
連戰夫婦今晚的50週年金婚紀念宴，正值連戰赴陸出席閱兵引發爭議之際，社會關注會否受到影響。
包括國民黨主席朱立倫、副主席郝龍斌等人已分別對外表示另有行程，無法出席。</p>

<p>then I compile the program using the code </p>

<pre><code>java -cp ""*"" -Xmx1g edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -annotators segment,ssplit -file input.txt
</code></pre>

<p>and the result is as follows. But it gives the following error and how do i solve this problem?</p>

<pre><code>C:\stanford-corenlp-full-2015-04-20&gt;java -cp ""*"" -Xmx1g edu.stanford.nlp.pipelin
e.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -annotators segment,
 ssplit -file input.txt
Registering annotator segment with class edu.stanford.nlp.pipeline.ChineseSegmen
terAnnotator
Adding annotator segment
Loading Segmentation Model ... Loading classifier from edu/stanford/nlp/models/s
egmenter/chinese/ctb.gz ... Loading Chinese dictionaries from 1 file:
  edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
Done. Unique words in ChineseDictionary is: 423200.
done [22.9 sec].

Ready to process: 1 files, skipped 0, total 1
Processing file C:\stanford-corenlp-full-2015-04-20\input.txt ... writing to C:\
stanford-corenlp-full-2015-04-20\input.txt.xml {
  Annotating file C:\stanford-corenlp-full-2015-04-20\input.txt Adding Segmentat
ion annotation ... INFO: TagAffixDetector: useChPos=false | useCTBChar2=true | u
sePKChar2=false
INFO: TagAffixDetector: building TagAffixDetector from edu/stanford/nlp/models/s
egmenter/chinese/dict/character_list and edu/stanford/nlp/models/segmenter/chine
se/dict/in.ctb
Loading character dictionary file from edu/stanford/nlp/models/segmenter/chinese
/dict/character_list
Loading affix dictionary from edu/stanford/nlp/models/segmenter/chinese/dict/in.
ctb
?]?X?u????j???\?L??o??????????e?D?u?s???????B?b?A??k?|???????????A?w?V?s?????A?
??@?U?^?n???C
?s?????????50?g?~???B?????b?A????s??u???X?u?\?L??o???????A???|???`?|?_????v?T?C
?]?A?????D?u?????B??D?u?q?s?y???H?w???O??~???t????{?A?L?k?X?u?C

---&gt;
[?, ], ?, X, ?u????j???, \, ?L??o??????????e?D?u?s???????B?b?A??k?|???????????A?
w?V?s?????A???@?U?^?n???C, , , , ?s?????????, 50, ?, g?, ~, ???B?????b?A????s??u
???X?u?, \, ?L??o???????A???, |, ???, `, ?, |, ?_????v?T?C, , , , ?, ], ?, A????
?D?u???, ??, B??D?u?q, ?, s?y???H?w???O??, ~, ???t????, {, ?, A?L?k?X?u?C]

}
Processed 1 documents
Skipped 0 documents, error annotating 0 documents
Annotation pipeline timing information:
ChineseSegmenterAnnotator: 0.1 sec.
TOTAL: 0.1 sec. for 34 tokens at 485.7 tokens/sec.
Pipeline setup: 0.0 sec.
Total time for StanfordCoreNLP pipeline: 0.1 sec.
</code></pre>
",stanford-nlp,"<p>I edited your question to change the command to the one that you actually used to produce the output shown. It looks like you worked out that the former command:</p>

<pre><code>java -cp ""*"" -Xmx1g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt input.xml
</code></pre>

<p>ran the English analysis pipeline, and that didn't work very well for Chinese text....</p>

<p>The CoreNLP support of Chinese in v3.5.2 is still a little rough, and will hopefully be a bit smoother in the next release. But from here you need to:</p>

<ul>
<li>Specify a properties file for Chinese, giving appropriate models. (If no properties file is specified, CoreNLP defaults to English): <code>-props StanfordCoreNLP-chinese.properties</code></li>
<li>At present, word segmentation of Chinese is not the annotator <code>tokenize</code>, but <code>segment</code>, specified as a custom annotator in <code>StanfordCoreNLP-chinese.properties</code>. (Maybe we'll unify the two in a future release...)</li>
<li>The current <code>dcoref</code> annotator only works for English. There is Chinese coreference, but it is not fully integrated into the pipeline. If you want to use it, you currently have to write some code, as explained <a href=""http://nlp.stanford.edu/software/dcoref.shtml#chinese"" rel=""nofollow noreferrer"">here</a>. So let's delete it. (Again, this should be better integrated in the future). </li>
<li>At that point, things run, but the ugly stderr output you show is that by default the segmenter has VERBOSE turned on, but your output character encoding is not right for our Chinese output. We should have VERBOSE off by default, but you can turn it off with: <code>-segment.verbose false</code></li>
<li>We have no Chinese lemmatizer, so may as well delete that annotator.</li>
<li>Also, <code>CoreNLP</code> needs more than 1GB of RAM. Try 2GB.</li>
</ul>

<p>At this point, all should be good! With the command:</p>

<p><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP  -props StanfordCoreNLP-chinese.properties -annotators segment,ssplit,pos,ner,parse -segment.verbose false -file input.txt
</code></p>

<p>you get the output in <code>input.txt.xml</code>. (I'm not posting it, since it's a couple of thousand lines long....)</p>

<p><strong>Update for CoreNLP v3.8.0:</strong> If using the (current in 2017) CoreNLP v3.8.0, then there are some changes/progress: (i) We now use the annotator <code>tokenize</code> for all languages and it doesn't require loading a custom annotator for Chinese; (ii) verbose segmentation is correctly turned off by default; (iii) [negative progress] the requirements require the <code>lemma</code> annotator prior to <code>ner</code>, even though it is a no-op for Chinese; and (iv) coreference is now available for Chinese, invoked as <code>coref</code>, which requires the prior annotator `mentions, and its statistical models require considerable memory. Put that all together, and you're now good with this command:</p>

<p><code>java -cp ""*"" -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP  -props StanfordCoreNLP-chinese.properties -annotators tokenize,ssplit,pos,lemma,ner,parse,mention,coref -file input.txt
</code></p>
",4,2,1613,2015-09-05 08:48:32,https://stackoverflow.com/questions/32411182/about-the-stanford-corenlp-in-chinese-model
Jersey 2 alternative to ServletContextListener,"<p>I'm trying to initialize the StanfordCoreNLP at the startup of my Jersey 2 webapp.
I found out, that ServletContextListener is the way to do it but i don't have ServletContextListener in jersey 2 right?</p>

<p>So how can i load this code on startup of my jersey 2 webapp:</p>

<pre><code>Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<hr>

<p>Edit</p>

<p>web.xml</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;!-- This web.xml file is not required when using Servlet 3.0 container,
see implementation details http://jersey.java.net/nonav/documentation/latest/jax-rs.html --&gt;
&lt;web-app version=""2.5"" xmlns=""http://java.sun.com/xml/ns/javaee"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd""&gt;
    &lt;servlet&gt;
        &lt;servlet-name&gt;com.crawler.c_api.rest.ApplicationResource&lt;/servlet-name&gt;
        &lt;servlet-class&gt;org.glassfish.jersey.servlet.ServletContainer&lt;/servlet-class&gt;
        &lt;init-param&gt;
            &lt;param-name&gt;jersey.config.server.provider.packages&lt;/param-name&gt;
            &lt;param-value&gt;com.crawler.c_api&lt;/param-value&gt;
        &lt;/init-param&gt;        
        &lt;init-param&gt;
            &lt;param-name&gt;jersey.config.server.provider.classnames&lt;/param-name&gt;
            &lt;param-value&gt;com.crawler.c_api.provider.ResponseCorsFilter;org.glassfish.jersey.filter.LoggingFilter&lt;/param-value&gt;
        &lt;/init-param&gt;
        &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;
    &lt;/servlet&gt;
    &lt;servlet-mapping&gt;
        &lt;servlet-name&gt;com.crawler.c_api.rest.ApplicationResource&lt;/servlet-name&gt;
        &lt;url-pattern&gt;/api/*&lt;/url-pattern&gt;
    &lt;/servlet-mapping&gt;
&lt;/web-app&gt;
</code></pre>

<p>pom.xml</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""&gt;

    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.crawler&lt;/groupId&gt;
    &lt;artifactId&gt;C_API&lt;/artifactId&gt;
    &lt;packaging&gt;war&lt;/packaging&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;C_API&lt;/name&gt;

    &lt;build&gt;
        &lt;finalName&gt;C_API&lt;/finalName&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.5.1&lt;/version&gt;
                &lt;inherited&gt;true&lt;/inherited&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.glassfish.jersey&lt;/groupId&gt;
                &lt;artifactId&gt;jersey-bom&lt;/artifactId&gt;
                &lt;version&gt;${jersey.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.glassfish.jersey.containers&lt;/groupId&gt;
            &lt;artifactId&gt;jersey-container-servlet-core&lt;/artifactId&gt;
            &lt;!-- use the following artifactId if you don't need servlet 2.x compatibility --&gt;
            &lt;!-- artifactId&gt;jersey-container-servlet&lt;/artifactId --&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.glassfish.jersey.media&lt;/groupId&gt;
            &lt;artifactId&gt;jersey-media-moxy&lt;/artifactId&gt;
        &lt;/dependency&gt;


        &lt;dependency&gt;
            &lt;groupId&gt;com.zaxxer&lt;/groupId&gt;
            &lt;artifactId&gt;HikariCP-java6&lt;/artifactId&gt;
            &lt;version&gt;2.2.5&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;5.1.36&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;commons-codec&lt;/groupId&gt;
            &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;
            &lt;version&gt;1.9&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt;
            &lt;artifactId&gt;jackson-mapper-asl&lt;/artifactId&gt;
            &lt;version&gt;1.9.13&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.mindrot&lt;/groupId&gt;
            &lt;artifactId&gt;jbcrypt&lt;/artifactId&gt;
            &lt;version&gt;0.3m&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;net.sf.json-lib&lt;/groupId&gt;
            &lt;artifactId&gt;json-lib&lt;/artifactId&gt;
            &lt;version&gt;2.4&lt;/version&gt;
            &lt;classifier&gt;jdk15&lt;/classifier&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.syncthemall&lt;/groupId&gt;
            &lt;artifactId&gt;boilerpipe&lt;/artifactId&gt;
            &lt;version&gt;1.2.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;xerces&lt;/groupId&gt;
            &lt;artifactId&gt;xercesImpl&lt;/artifactId&gt;
            &lt;version&gt;2.9.1&lt;/version&gt;
        &lt;/dependency&gt; 
        &lt;dependency&gt;
            &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt;
            &lt;artifactId&gt;nekohtml&lt;/artifactId&gt;
            &lt;version&gt;1.9.13&lt;/version&gt;
        &lt;/dependency&gt;



        &lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.5.2&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.5.2&lt;/version&gt;
            &lt;classifier&gt;models&lt;/classifier&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;properties&gt;
        &lt;jersey.version&gt;2.19&lt;/jersey.version&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;/properties&gt;
&lt;/project&gt;
</code></pre>

<p>ApplicationResource</p>

<pre><code>    package com.crawler.c_api.rest;

import com.crawler.c_api.provider.ResponseCorsFilter;
import java.util.logging.Logger;


import org.glassfish.jersey.filter.LoggingFilter;
import org.glassfish.jersey.server.ResourceConfig;
import org.glassfish.jersey.server.ServerProperties;



public class ApplicationResource extends ResourceConfig {

    private static final Logger LOGGER = null;   


    public ApplicationResource() {
            System.out.println(""iansdiansdasdasd"");
        // Register resources and providers using package-scanning.
        packages(""main.java.com.crawler.c_api"");

        // Register my custom provider - not needed if it's in my.package.
        register(ResponseCorsFilter.class);

        // Register an instance of LoggingFilter.
        register(new LoggingFilter(LOGGER, true));

        // Enable Tracing support.
        property(ServerProperties.TRACING, ""ALL"");
    }
}
</code></pre>
","java, jersey, stanford-nlp, jersey-2.0","<p><a href=""http://docs.oracle.com/javaee/7/api/javax/servlet/ServletContextListener.html"" rel=""nofollow""><code>ServletContextListener</code></a> if part of the Servlet APIs. So you can just add</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;javax.servlet&lt;/groupId&gt;
    &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt;
    &lt;version&gt;3.1.0&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre>

<p>But it might not be necessary. There are more ways to deploy a Jersey app aside from just a web.xml. You can extend a <a href=""https://jersey.java.net/apidocs/2.19/jersey/org/glassfish/jersey/server/ResourceConfig.html"" rel=""nofollow""><code>ResourceConfig</code></a>. This will allows you to go completely xml-less even if you wanted to. Everything you configure in the web.xml, for the most part can be configured in the <code>ResourceConfig</code>. </p>

<p>You could even use both web.xml <em>and</em> a <code>ResourceConfig</code>. For example</p>

<pre><code>public class MyApplication extends ResourceConfig {
    public MyApplication() {
        packages(""org.foo.rest;org.bar.rest"");
        // do any other initialization here
    }
}

&lt;web-app&gt;
    &lt;servlet&gt;
        &lt;servlet-name&gt;org.foo.rest.MyApplication&lt;/servlet-name&gt;
    &lt;/servlet&gt;
    ...
    &lt;servlet-mapping&gt;
        &lt;servlet-name&gt;org.foo.rest.MyApplication&lt;/servlet-name&gt;
        &lt;url-pattern&gt;/resources&lt;/url-pattern&gt;
    &lt;/servlet-mapping&gt;
    ...
&lt;/web-app&gt;
</code></pre>

<p>See other deployment options <a href=""https://jersey.java.net/documentation/latest/deployment.html#deployment.servlet.3"" rel=""nofollow"">here</a>. There are a few different ways to deploy an Jersey app.</p>
",4,3,4687,2015-09-05 16:04:55,https://stackoverflow.com/questions/32415092/jersey-2-alternative-to-servletcontextlistener
Stanford core NLP - Grammatical relations missing,"<p>I am trying to update a software that is using Stanford core NLP version 1.3.4 to version 3.5.1. </p>

<p>In the software, there are some English grammatical relations that are used. When I update the code with the new version of the library, the following English grammatical relations are missing in the new version:</p>

<ol>
<li><code>CONTROLLING_SUBJECT</code> - I think that it would be OK if I replace this with <code>SEMANTIC_DEPENDENT</code>.</li>
<li><code>PURPOSE_CLAUSE_MODIFIER</code> - I think that this can be replaced with <code>ADV_CLAUSE_MODIFIER</code>.</li>
<li><code>COMPLEMENTIZER</code> - this might be raplced with <code>MARKER</code> (I think).</li>
<li><code>PARTICIPIAL_MODIFIER</code> and <code>INFINITIVAL_MODIFIER</code> could be replaced with <code>CLAUSAL_COMPLEMENT</code>.</li>
</ol>

<p>I am not sure if this is the case, I have been digging through some documentations, but since there is a big difference between the versions 1.3.4 and 3.5.1 - I am not sure if these missing relations are now part of the relations I mentioned. </p>

<p>Are these replacements I've mentioned above correct? If not, with which other relations can I replace them?</p>
","java, nlp, stanford-nlp","<p>I don't think any of them are <em>exactly</em> correct except for (probably) replacing CONTROLLING_SUBJECT with SEMANTIC_DEPENDENT. Most of them seem like close matches or subsets - I think even if you talked to the Standford you won't get exact replacements. I think you best bet is empirical validation. :(</p>

<p>Sorry, I know this isn't what you want to hear. Your guesses do seem reasonable though.</p>
",1,1,98,2015-09-08 15:01:41,https://stackoverflow.com/questions/32461155/stanford-core-nlp-grammatical-relations-missing
Error Trying to Load DefaultNLP Model in Stanford.NLP.CoreNLP .Net in WebAPI C# Project,"<p>I have been attempting to load the default models I pulled from the suggested ZIP file. Generally, I am loading the annotation into a singleton at the application level so the resources can be shared across all sessions. (in WebAPI OWIN Startup, this is being called from startup.cs). </p>

<p>Trying other methods with relative path references, I was getting this error:</p>

<blockquote>
  <p>unable to resolve
  ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger""
  as either class path, filename or URL</p>
</blockquote>

<p>I am not sure if I am getting closer to or further from a solution. <strong>This is at the root directory of my ASP.NET WebAPI Project:</strong></p>

<p><img src=""https://cloud.githubusercontent.com/assets/7691264/9752735/fb5d4a04-5670-11e5-9b5b-ce97c3adbba2.png"" alt=""image""></p>

<p>However, I am getting the error: </p>

<blockquote>
  <p>Unhandled Execution Error</p>
  
  <p>Description: An unhandled exception occurred during the execution of
  the current web request. Please review the stack trace for more
  information about the error and where it originated in the code. </p>
  
  <p>Exception Details: java.lang.reflect.InvocationTargetException: </p>
  
  <p>Source Error:</p>
  
  <p>Line 43:                 // We should change current directory, so
  StanfordCoreNLP could find all the model files automatically Line 44: 
  Directory.SetCurrentDirectory(HostingEnvironment.MapPath(ModelLocation));
  Line 45:                 pipeline = new StanfordCoreNLP(props); Line
  46:             } Line 47:             finally</p>
  
  <p>Source File: D:\xxx\xxx\xxx\NLP.cs    Line: 45 </p>
  
  <p>Stack Trace: </p>
  
  <p>[InvocationTargetException]    __(Object[] ) +444<br>
  FastConstructorAccessorImpl.newInstance(Object[] args) +28<br>
  java.lang.reflect.Constructor.newInstance(Object[] initargs, CallerID
  ) +133    edu.stanford.nlp.util.ClassFactory.createInstance(Object[]
  params) +108</p>
  
  <p>[ClassCreationException: MetaClass couldn't create public
  edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties)
  with args [sutime, {}]]<br>
  edu.stanford.nlp.util.ClassFactory.createInstance(Object[] params)
  +372    edu.stanford.nlp.util.MetaClass.createInstance(Object[] objects) +34<br>
  edu.stanford.nlp.util.ReflectionLoading.loadByReflection(String
  className, Object[] arguments) +71</p>
  
  <p>[ReflectionLoadingException: Error creating
  edu.stanford.nlp.time.TimeExpressionExtractorImpl]<br>
  edu.stanford.nlp.util.ReflectionLoading.loadByReflection(String
  className, Object[] arguments) +232<br>
  edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(String
  className, String name, Properties props) +80<br>
  edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(String
  name, Properties props) +34<br>
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier..ctor(Properties
  props, Boolean useSUTime, Properties sutimeProps) +57<br>
  edu.stanford.nlp.ie.NERClassifierCombiner..ctor(Boolean
  applyNumericClassifiers, Boolean useSUTime, Properties nscProps,
  String[] loadPaths) +129<br>
  edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(Properties
  properties) +454    edu.stanford.nlp.pipeline.6.create() +46<br>
  edu.stanford.nlp.pipeline.AnnotatorPool.get(String name) +163<br>
  edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(Properties ,
  Boolean , AnnotatorImplementations ) +555<br>
  edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(Properties props,
  Boolean enforceRequirements) +55<br>
  edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(Properties props) +76 
  XXX.XXX.NLP.Start(String modelLocation) in
  D:\xxx\xxx\xxx\NLP.cs:45<br>
  XXX.XXX.Startup.Configuration(IAppBuilder app) in
  D:\xxx\xxx\xxx\Startup.cs:16</p>
  
  <p>[TargetInvocationException: Exception has been thrown by the target of
  aninvocation.]    System.RuntimeMethodHandle.InvokeMethod(Object
  target, Object[] arguments, Signature sig, Boolean constructor) +0<br>
  System.Reflection.RuntimeMethodInfo.UnsafeInvokeInternal(Object obj,
  Object[] parameters, Object[] arguments) +128<br>
  System.Reflection.RuntimeMethodInfo.Invoke(Object obj, BindingFlags
  invokeAttr, Binder binder, Object[] parameters, CultureInfo culture)
  +146    Owin.Loader.&lt;>c__DisplayClass12.b__b(IAppBuilder builder) +93<br>
  Owin.Loader.&lt;>c__DisplayClass1.b__0(IAppBuilder
  builder) +209<br>
  Microsoft.Owin.Host.SystemWeb.OwinAppContext.Initialize(Action 1
  startup) +843<br>
  Microsoft.Owin.Host.SystemWeb.OwinBuilder.Build(Action 1 startup) +51 
  Microsoft.Owin.Host.SystemWeb.OwinHttpModule.InitializeBlueprint()
  +101    System.Threading.LazyInitializer.EnsureInitializedCore(T&amp; target, Boolean&amp; initialized, Object&amp; syncLock, Func 1 valueFactory)
  +141    Microsoft.Owin.Host.SystemWeb.OwinHttpModule.Init(HttpApplication
  context) +172<br>
  System.Web.HttpApplication.RegisterEventSubscriptionsWithIIS(IntPtr
  appContext, HttpContext context, MethodInfo[] handlers) +618<br>
  System.Web.HttpApplication.InitSpecial(HttpApplicationState state,
  MethodInfo[] handlers, IntPtr appContext, HttpContext context) +172<br>
  System.Web.HttpApplicationFactory.GetSpecialApplicationInstance(IntPtr
  appContext, HttpContext context) +419<br>
  System.Web.Hosting.PipelineRuntime.InitializeApplication(IntPtr
  appContext) +343</p>
  
  <p>[HttpException (0x80004005): Exception has been thrown by the target
  of an invocation.]<br>
  System.Web.HttpRuntime.FirstRequestInit(HttpContext context) +579<br>
  System.Web.HttpRuntime.EnsureFirstRequestInit(HttpContext context)
  +120    System.Web.HttpRuntime.ProcessRequestNotificationPrivate(IIS7WorkerRequest
  wr, HttpContext context) +712</p>
</blockquote>

<p>Here is the code I have (NLP.Start() is called in startup.cs):</p>

<pre><code>public static class NLP
{
    private static string _modelLocation = @""~\NLPModels"";
    public static string ModelLocation
    {
        set
        {
            NLP.Start(value);
        }
        get
        {
            return _modelLocation;
        }
    }
    private static StanfordCoreNLP pipeline;
    public static void Start(string modelLocation = null)
    {
        var curDir = Environment.CurrentDirectory;
        if (!string.IsNullOrEmpty(modelLocation))
        {
            _modelLocation = modelLocation;
        }
        try
        {
            // Annotation pipeline configuration
            var props = new Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            props.setProperty(""sutime.binders"", ""0"");
            // We should change current directory, so StanfordCoreNLP could find all the model files automatically
            Directory.SetCurrentDirectory(HostingEnvironment.MapPath(ModelLocation));
            pipeline = new StanfordCoreNLP(props);
        }
        finally
        {
            Directory.SetCurrentDirectory(curDir);
        }
    }

    public static JObject ProcessText(string text)
    {
        var annotation = new Annotation(text);
        using (java.io.StringWriter writer = new java.io.StringWriter())
        {
            pipeline.jsonPrint(annotation, writer);
            return JObject.Parse(writer.toString());
        }
    }
}
</code></pre>
","asp.net, .net, asp.net-web-api, owin, stanford-nlp","<p>After poking around a bit, I found the solution of the same problem.</p>

<p><a href=""https://github.com/sergey-tihon/Stanford.NLP.NET/issues/11"" rel=""nofollow"">https://github.com/sergey-tihon/Stanford.NLP.NET/issues/11</a></p>

<p>If you aren't willing to read through the thread, here was the basic answer. Amend this line of code</p>

<pre><code>props.setProperty(""sutime.binders"", ""0"");
</code></pre>

<p>to</p>

<pre><code>props.setProperty(""ner.useSUTime"", ""0"");
</code></pre>
",0,-1,469,2015-09-09 04:10:09,https://stackoverflow.com/questions/32470774/error-trying-to-load-defaultnlp-model-in-stanford-nlp-corenlp-net-in-webapi-c
Stanford corenlp: top K ngrams with count,"<p>How to use stanford corenlp to get top K ngrams with their count? I know I can write this code using HashMap or Trai but my corpus is pretty large (200K articles each with avg size 30KB) and I want 5grams, so memory requirement will be huge. Thus I was wondering if I can use corenlp for this purpose. 
So given a corpus it should return only top K ngrams in this format:</p>

<p>word1 word2 word3 word4 word5 : frequency</p>

<p>I don't want any probabilistic model.</p>
","nlp, stanford-nlp, n-gram","<p>CoreNLP doesn't have anything to help you store ngrams efficiently.  All it could help you with here would be tokenizing the text (and possibly segmenting the text into sentences, if you care about that).</p>

<p>If your corpus is large enough that you can't just use a hash table to keep the n-gram count, you'll have to use an alternative, more space-efficient representation (e.g. a prefix trie).</p>

<p>For example, I just did a quick test in Clojure where I counted the 5-grams in the Gutenberg King James V Bible.  Using a hashmap to store the counts for 752K distinct 5-grams used 248 MB of heap.  Using a prefix trie to store the counts used 57 MB--a reduction of 77%.</p>

<p>For reference, here's the complete Clojure program using prefix tries:</p>

<pre><code>(ns nlp.core
  (:require [clojure.string :as string]))

(defn tokenize
  ""Very simplistic tokenizer.""
  [text]
  (string/split text #""[\s\:_\-\.\!\,\;]+""))

(defn get-bible-kjv-tokens []
  (tokenize (slurp ""/Users/wiseman/nltk_data/corpora/gutenberg/bible-kjv.txt"")))

(defn ngrams [n tokens]
  (partition n 1 tokens))

(defn build-ngram-trie [n tokens]
  (-&gt;&gt; tokens
       (ngrams n)
       (reduce (fn [trie ngram]
                 (update-in trie ngram #(if % (inc %) 1)))
               {})))

(defn enumerate-trie [trie]
  (if (not (map? trie))
    (list (list trie))
    (apply concat
           (for [[k v] trie]
             (map #(cons k %)
                  (enumerate-trie v))))))

(defn print-trie [trie]
  (doseq [path (enumerate-trie trie)]
    (println (string/join "" "" (butlast path)) "":"" (last path))))


(defn -main []
  (let [ngram-counts (-&gt;&gt; (get-bible-kjv-tokens)
                          (build-ngram-trie 5))]
    (print-trie ngram-counts)))
</code></pre>

<p>And output from the King James V Bible:</p>

<pre><code>$ lein run -m nlp.core | sort -r -k7,7 -n ngrams.txt  | head
And it came to pass : 383
the house of the LORD : 233
the word of the LORD : 219
of the children of Israel : 162
it came to pass when : 142
the tabernacle of the congregation : 131
saith the LORD of hosts : 123
it shall come to pass : 119
And the LORD said unto : 114
And the LORD spake unto : 107
</code></pre>

<p>For some pointers about gaining even more efficiency, the following papers talk about efficient n-gram storage for large corpora:</p>

<p><a href=""http://axon.cs.byu.edu/papers/vandam.smc07.pdf"" rel=""nofollow"">ADtrees for Sequential Data and N-gram Counting</a> - Using a custom data structure.</p>

<p><a href=""http://nlp.cs.berkeley.edu/pubs/Pauls-Klein_2011_LM_paper.pdf"" rel=""nofollow"">Faster and Smaller N-Gram Language Models</a> - ""Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date""</p>
",3,2,879,2015-09-14 12:00:38,https://stackoverflow.com/questions/32564296/stanford-corenlp-top-k-ngrams-with-count
How to customize stanfordNLP tokenizer to ignore asterisk character?,"<p>I'm using stanfordCoreNLP library's tokenizer as a part of my project.For the following string
 <code>abc def *ghi</code>
It is giving the following tokens<code>abc,def,*ghi</code>
But,I want asterisk to be included as in <code>abc,def,*ghi</code>.How to customize PBTTokenizer to acheive this?</p>
","tokenize, stanford-nlp","<p>Please see my answer for this question:</p>

<p><a href=""https://stackoverflow.com/questions/32688640/how-to-set-delimiters-for-ptb-tokenizer/32695789#32695789"">How to set delimiters for PTB tokenizer?</a></p>

<p>You can set the tokenizer to tokenize on white space only:</p>

<pre><code>(command-line) -tokenize.whitespace
(in Java code) props.setProperty(""tokenize.whitespace"", ""true"");
</code></pre>
",1,1,181,2015-09-15 14:14:48,https://stackoverflow.com/questions/32588384/how-to-customize-stanfordnlp-tokenizer-to-ignore-asterisk-character
reducing CRFClassifier model file size,"<p>I am training a chunker using CoreNLP's <code>CRFClassifier</code> and I would like to reduce the size of the generated model file. I thought that I could use the <code>featureCountThreshold</code> property to threshold uncommon features and in this way reduce the file size, but I have tried several thresholds and the file size is always the same, so either I am doing something wrong or I misunderstood the <code>featureCountThreshold</code> property.</p>

<p>This is how I instantiate the <code>CRFClassifier</code>:</p>

<pre><code>val props = new Properties()
props.setProperty(""macro"", ""true"")
props.setProperty(""featureFactory"", ""edu.arizona.sista.chunker.ChunkingFeatureFactory"")
props.setProperty(""featureCountThreshold"", ""10"")
new CRFClassifier[CoreLabel](props)
</code></pre>

<p>The code is in scala, but it should be straightforward.</p>

<p>Is this the right way to reduce the file size? And if not, is there a way to accomplish this?</p>
",stanford-nlp,"<p>For the next person trying to do this:</p>

<p>There are two properties with similar names in CoreNLP: <code>featureCountThreshold</code> and <code>featureCountThresh</code>. <code>featureCountThresh</code> is the correct one for this task.
We were able to reduce a model from 321M to 54M using a <code>featureCountThresh</code> of 10 and still retain almost the same performance.</p>
",0,0,169,2015-09-16 00:01:52,https://stackoverflow.com/questions/32597586/reducing-crfclassifier-model-file-size
NER interfere with REGEXNER,"<p>I use regexner to find named entities that are not in the default set of Stanford NLP and it works fine. However, when I add ner annotator, it annotates tokens that match my regular expression with default tags. How can I overwrite default annotations?</p>

<pre><code>def createNLPPipelineRegex(): StanfordCoreNLP = {
     val props = new Properties()
     props.put(""regexner.mapping"", ""regex.txt"")
     props.put(""annotators"", ""tokenize, ssplit, regexner, pos, lemma, ner"")
     props.put(""tokenize.options"", ""untokenizable=noneKeep,normalizeParentheses=false"")
     new StanfordCoreNLP(props)
</code></pre>

<p>}</p>
","scala, stanford-nlp, named-entity-recognition","<p>If you add regexner after the ner annotator it should work:</p>

<pre><code>props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner"")
</code></pre>
",2,2,396,2015-09-18 00:23:57,https://stackoverflow.com/questions/32642008/ner-interfere-with-regexner
Convert a Tree to SemanticGraph in Stanford parser,"<p>I want to covert a Tree to SemanticGraph in Stanford Parser as followings:</p>

<pre><code>LexicalizedParser lp  = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
LexicalizedParserQuery lpq=lp.lexicalizedParserQuery();

String sentence=""This is a sentence."";

List&lt;CoreLabel&gt; tokenizedSentence = tokenizerFactory.getTokenizer(new StringReader(sentence)).tokenize();
lpq.parse(tokenizedSentence);
Tree depTree = lpq.getBestParse();
SemanticGraph semanticGraph = ParserAnnotatorUtils.generateUncollapsedDependencies(depTree);
</code></pre>

<p><code>ParserAnnotatorUtils.generateUncollapsedDependencies(depTree)</code> works on version 2.0.4. But it does not work on Version 3.5.2.</p>
","parsing, stanford-nlp","<p>You can try something like:</p>

<pre><code>Tree tree = ...
GrammaticalStructureFactory gsf = new UniversalEnglishGrammaticalStructureFactory();
SemanticGraph dependencyGraph = SemanticGraphFactory.generateCollapsedDependencies( gsf.newGrammaticalStructure(tree), GrammaticalStructure.Extras.NONE );
</code></pre>
",2,0,130,2015-09-24 00:25:16,https://stackoverflow.com/questions/32751509/convert-a-tree-to-semanticgraph-in-stanford-parser
Scores for tagged NER results in the StandfordCore NLP.net library,"<p>I'm using the Sandford CoreNLP.NET module and its CRFClassifier to find Named Entities in a document. I am able to get the entities by using classifyWithInlineXML, but does anyone know how to get the entities along with their relevance/confidence scores (0-1)? </p>

<p>Would love an example in C# on how to do this.</p>
","stanford-nlp, named-entity-recognition","<p>Looks like a duplicate of <a href=""https://stackoverflow.com/questions/26612999/display-stanford-ner-confidence-score"">Display Stanford NER confidence score</a> question.</p>

<p>All you need is to rewrite provided sample in C#.</p>
",1,0,626,2015-09-26 10:10:46,https://stackoverflow.com/questions/32796011/scores-for-tagged-ner-results-in-the-standfordcore-nlp-net-library
Is there a maximum number of classes / labels that you can use in Stanford Named Entity Recognizer&#39;s CRFClassifier?,"<p>Is there a maximum number of classes you can use in Stanford’s NER CRFClassifier?</p>

<p>And is there any consequences when you go up to about 100 000 different classes? </p>
",stanford-nlp,"<p>I don't have a strong knowledge of CRF's so I could be wrong about this, but this paper:</p>

<p><a href=""http://people.eng.unimelb.edu.au/tcohn/papers/cohn06ecml.pdf"" rel=""nofollow"">http://people.eng.unimelb.edu.au/tcohn/papers/cohn06ecml.pdf</a></p>

<p>says that training time grows quadratically with respect to the number of classes.  Also if you are building a model with 100,000 classes, you would want substantially more than the 200,000 tokens used to generate the 4-class NER model.</p>

<p>I believe that would lead to a training time measured in centuries or eons!</p>
",2,0,99,2015-09-27 09:22:46,https://stackoverflow.com/questions/32806375/is-there-a-maximum-number-of-classes-labels-that-you-can-use-in-stanford-named
Using openie to reproduce extractions as shown in (Angeli et. al),"<p>I have downloaded the Openie jar files including the model and have it running successfully. However, it is extremely basic out of the box not recognizing multiple clauses nor any examples as elucidated in its foundational paper ""Leveraging Linguistic Structure For Open Domain Information Extraction."" I would like to reproduce the extractions that are shown in the paper. Are there other models and a explanation of the configuration I need to implement to replicate these results?</p>

<p>Thanks </p>
",stanford-nlp,"<p>I was wondering when this would come up... Between the publication of the paper and the release of the code, CoreNLP moved its dependency parser from Stanford Dependencies to Universal Dependencies. Some of the examples in the paper now have incorrect dependency parses  (though some of these are fixed now!), and therefore these errors propagate to the OpenIE system.</p>

<p>However, these should be one-off errors. To test clause splitting, you can try a sentence like ""Cats have tails, and dogs chase cats."" and you should get both (cats; have; tails) and (dogs; chase; cats). And it should certainly work on most of the examples in the paper still. If not, let me know and I'll fix it!</p>
",2,1,153,2015-09-27 20:00:03,https://stackoverflow.com/questions/32812256/using-openie-to-reproduce-extractions-as-shown-in-angeli-et-al
What is the data dictionary in NLP?,"<p>I am new to NLP and its concepts. I am currently trying to use <a href=""https://opennlp.apache.org/"" rel=""nofollow"">OpenNLP</a> from Apache. When I try to use its parser tool as shown in <a href=""http://www.programcreek.com/2012/05/opennlp-tutorial/"" rel=""nofollow"">blog</a>, I get the output like:</p>

<pre><code>Input: Shutdown all active devices

Output: 
(TOP(VP(VB Shutdown)
(NP(DT all)
    (JJ active)
    (NNS devices))))
</code></pre>

<p>I am unable to understand what is the meaning of all the acronyms. Some of them I could understand from <a href=""https://stanbol.apache.org/"" rel=""nofollow"">Stanford's OpenNLP implementation</a> of the same. But not all.</p>

<p>Can anyone point me to some proper direction as to how can I understand it and better, as to how can my code understand what is being said.</p>
","java, machine-learning, nlp, stanford-nlp, opennlp","<p>First of all, learning like in any subject takes time so don't rush it or you will confuse yourself. The output syntax you see is a tree which take on the form of a series of lists and embedded lists. It may remind you of the syntax of a popular LISP such as <a href=""https://en.wikipedia.org/wiki/Scheme_(programming_language)"" rel=""nofollow"">Scheme</a> or <a href=""https://en.wikipedia.org/wiki/Clojure"" rel=""nofollow"">Clojure</a>.</p>

<pre><code>(TOP(VP(VB Shutdown)
(NP(DT all)
    (JJ active)
    (NNS devices))))
</code></pre>

<p>The tags to the left of the words / lists are what is known as POS (<a href=""https://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow"">Part-of-Speech</a>) Tags, that represent the grammatical category the word falls into, essentially a word-category disambiguation. POS tagging is still one of the very difficult research areas of Natural Language Processing as a subject with F1-Scores in their high 90%'s. Your tree snippet built out (with the list below) looks as follows:</p>

<pre><code>(TOP(Verb, non-3rd-person(Verb Shutdown)
(Noun-Phrase (Determiner all)
    (Adjective active)
    (Noun-plural devices))))
</code></pre>

<p>POS Tagging is a great linguistic feature for tasks such as semantic parsing or named entity recognition. Some good resources to learn from include:</p>

<ol>
<li><a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow"">NLTK (Natural Language Toolkit) Book Chapter 5</a></li>
<li><a href=""http://www.amazon.co.uk/Foundations-Statistical-Natural-Language-Processing/dp/0262133601/ref=sr_1_1?ie=UTF8&amp;qid=1443426144&amp;sr=8-1&amp;keywords=foundations%20of%20statistical%20natural%20language%20processing"" rel=""nofollow"">Foundations of Statistical Natural Language Processing</a></li>
<li><a href=""http://link.springer.com/chapter/10.1007/978-94-017-1183-8_4"" rel=""nofollow"">Part of Speech Tagging and Partial Parsing</a></li>
</ol>

<p><strong>List of Part-of-Speech Tags (Penn Treebank corpus)</strong> </p>

<ol start=""3"">
<li><strong>CC</strong> ~ Coordinating conjunction</li>
<li><strong>CD</strong> ~ Cardinal number</li>
<li><strong>DT</strong> ~ Determiner</li>
<li><strong>EX</strong> ~ Existential there</li>
<li><strong>FW</strong> ~ Foreign word</li>
<li><strong>IN</strong> ~ Preposition or subordinating conjunction</li>
<li><strong>JJ</strong> ~ Adjective</li>
<li><strong>JJR</strong> ~ Adjective, comparative</li>
<li><strong>JJS</strong> ~ Adjective, superlative</li>
<li><strong>LS</strong> ~ List item marker</li>
<li><strong>MD</strong> ~ Modal</li>
<li><strong>NN</strong> ~ Noun, singular or mass</li>
<li><strong>NNS</strong> ~ Noun, plural</li>
<li><strong>NNP</strong> ~ Proper noun, singular</li>
<li><strong>NNPS</strong> ~ Proper noun, plural</li>
<li><strong>PDT</strong> ~ Predeterminer</li>
<li><strong>POS</strong> ~ Possessive ending</li>
<li><strong>PRP</strong> ~ Personal pronoun</li>
<li><strong>PRP$</strong> ~ Possessive pronoun</li>
<li><strong>RB</strong> ~ Adverb</li>
<li><strong>RBR</strong> ~ Adverb, comparative</li>
<li><strong>RBS</strong> ~ Adverb, superlative</li>
<li><strong>RP</strong> ~ Particle</li>
<li><strong>SYM</strong> ~ Symbol</li>
<li><strong>TO</strong> ~ to</li>
<li><strong>UH</strong> ~ Interjection</li>
<li><strong>VB</strong> ~ Verb, base form</li>
<li><strong>VBD</strong> ~ Verb, past tense</li>
<li><strong>VBG</strong> ~ Verb, gerund or present participle</li>
<li><strong>VBN</strong> ~ Verb, past participle</li>
<li><strong>VBP</strong> ~ Verb, non-3rd person singular present</li>
<li><strong>VBZ</strong> ~ Verb, 3rd person singular present</li>
<li><strong>WDT</strong> ~ Wh-determiner</li>
<li><strong>WP</strong> ~ Wh-pronoun</li>
<li><strong>WP$</strong> ~ Possessive wh-pronoun</li>
<li><strong>WRB</strong> ~ Wh-adverb</li>
</ol>
",4,1,1801,2015-09-27 20:10:11,https://stackoverflow.com/questions/32812342/what-is-the-data-dictionary-in-nlp
Running Word similarity Glove,"<p>I have tried to run Glove on my Ubuntu machine. It was Ok until it gave me the following error that complains about matlab :</p>

<pre><code>./demo.sh: line 37: matlab: command not found
</code></pre>

<p>Can anybody help me how to set up matlab on my machine to run Glove?</p>
",stanford-nlp,"<p><strong>Solution-1:</strong><br>
matlab command part is only the intrinsic test part of glove tool. 
You don't need to run that part of demo.sh to generate word vectors.  </p>

<p><strong>Solution-2:</strong><br>
Moreover, with new release of GloVe which is <strong>version 1.2</strong>, there is python code to do that. Matlab is not necessary, python will handle it.    </p>

<p>demo.sh has an if elif code part which checks if there exists a matlab running on machine. If exists, it runs matlab, if not checks for octave. If octave exists, it runs octave. If octave doesnot exist it runs with python. demo.sh has following code, and matlab part is the last part of it:</p>

<pre><code>#!/bin/bash

# Makes programs, downloads sample data, trains a GloVe model, and then evaluates it.
# One optional argument can specify the language used for eval script: matlab, octave or [default] python

make
if [ ! -e text8 ]; then
  if hash wget 2&gt;/dev/null; then
    wget http://mattmahoney.net/dc/text8.zip
  else
    curl -O http://mattmahoney.net/dc/text8.zip
  fi
  unzip text8.zip
  rm text8.zip
fi

CORPUS=text8
VOCAB_FILE=vocab.txt
COOCCURRENCE_FILE=cooccurrence.bin
COOCCURRENCE_SHUF_FILE=cooccurrence.shuf.bin
BUILDDIR=build
SAVE_FILE=vectors
VERBOSE=2
MEMORY=4.0
VOCAB_MIN_COUNT=5
VECTOR_SIZE=50
MAX_ITER=15
WINDOW_SIZE=15
BINARY=2
NUM_THREADS=8
X_MAX=10

$BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE &lt; $CORPUS &gt; $VOCAB_FILE
if [[ $? -eq 0 ]]
  then
  $BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE &lt; $CORPUS &gt; $COOCCURRENCE_FILE
  if [[ $? -eq 0 ]]
  then
    $BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE &lt; $COOCCURRENCE_FILE &gt; $COOCCURRENCE_SHUF_FILE
    if [[ $? -eq 0 ]]
    then
       $BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE
       if [[ $? -eq 0 ]]
       then
           **if [ ""$1"" = 'matlab' ]; then
               matlab -nodisplay -nodesktop -nojvm -nosplash &lt; ./eval/matlab/read_and_evaluate.m 1&gt;&amp;2 
           elif [ ""$1"" = 'octave' ]; then
               octave &lt; ./eval/octave/read_and_evaluate_octave.m 1&gt;&amp;2 
           else
               python eval/python/evaluate.py
           fi**
       fi
    fi
  fi
fi
</code></pre>

<p><strong>IN SHORT:</strong>
New version of Glove which is Glove 1.2 solves your problem. Just run it.
matlab, octave or python, one of them is enough for running the script. If you have one of them, demo.sh will handle it.</p>
",2,0,738,2015-09-29 05:42:02,https://stackoverflow.com/questions/32836438/running-word-similarity-glove
Where can I find the code for the Stanford CoreNLP dependency tree visualizer?,"<p>Where can I find the code for the <a href=""http://nlp.stanford.edu:8080/corenlp/"" rel=""nofollow"">Stanford CoreNLP dependency tree visualizer</a>?</p>

<p>I've found it to be an extremely useful tool, however I've also found it to be frequently offline (like it is right now).  I'd like to host a personal instance so I don't have to deal with that.</p>
",stanford-nlp,"<p>As of now, the code for the tangent project 'corenlp.run' for visualising the dependency tree is hosted on github at <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP</a>.</p>

<p>To run your own instance, simply run the command: </p>

<p><code>java -mx4g -cp ""&lt;stanford-corenlp-folder&gt;/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer</code></p>

<p>This will start a server at your <a href=""http://localhost:9000/"" rel=""nofollow"">http://localhost:9000/</a> by default.</p>
",2,1,682,2015-09-30 20:09:46,https://stackoverflow.com/questions/32874737/where-can-i-find-the-code-for-the-stanford-corenlp-dependency-tree-visualizer
Stanford nlp for python,"<p>All I want to do is find the sentiment (positive/negative/neutral) of any given string. On researching I came across Stanford NLP. But sadly its in Java. Any ideas on how can I make it work for python? </p>
","python, stanford-nlp, sentiment-analysis","<h1>Use <a href=""https://github.com/smilli/py-corenlp/"" rel=""nofollow noreferrer""><code>py-corenlp</code></a></h1>
<h2>Download <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">Stanford CoreNLP</a></h2>
<p>The latest version at this time (2020-05-25) is 4.0.0:</p>
<pre><code>wget https://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip https://nlp.stanford.edu/software/stanford-corenlp-4.0.0-models-english.jar
</code></pre>
<p>If you do not have <a href=""https://www.gnu.org/software/wget/"" rel=""nofollow noreferrer""><code>wget</code></a>, you probably have <a href=""https://curl.haxx.se/"" rel=""nofollow noreferrer""><code>curl</code></a>:</p>
<pre><code>curl https://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip -O https://nlp.stanford.edu/software/stanford-corenlp-4.0.0-models-english.jar -O
</code></pre>
<p>If all else fails, use the browser ;-)</p>
<h2>Install the package</h2>
<pre><code>unzip stanford-corenlp-4.0.0.zip
mv stanford-corenlp-4.0.0-models-english.jar stanford-corenlp-4.0.0
</code></pre>
<h2>Start the <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">server</a></h2>
<pre><code>cd stanford-corenlp-4.0.0
java -mx5g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000
</code></pre>
<p>Notes:</p>
<ol>
<li><code>timeout</code> is in milliseconds, I set it to 10 sec above.
You should increase it if you pass huge blobs to the server.</li>
<li>There are <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#command-line-flags"" rel=""nofollow noreferrer"">more options</a>, you can list them with <code>--help</code>.</li>
<li><code>-mx5g</code> should allocate enough <a href=""https://stackoverflow.com/q/14763079/850781"">memory</a>, but YMMV and you may need to modify the option if your box is underpowered.</li>
</ol>
<h2>Install the python package</h2>
<p>The standard package</p>
<pre><code>pip install pycorenlp
</code></pre>
<p>does <em>not</em> work with Python 3.9, so you need to do</p>
<pre><code>pip install git+https://github.com/sam-s/py-corenlp.git
</code></pre>
<p>(See also <a href=""https://stanfordnlp.github.io/CoreNLP/other-languages.html"" rel=""nofollow noreferrer"">the official list</a>).</p>
<h2>Use it</h2>
<pre><code>from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')
res = nlp.annotate(&quot;I love you. I hate him. You are nice. He is dumb&quot;,
                   properties={
                       'annotators': 'sentiment',
                       'outputFormat': 'json',
                       'timeout': 1000,
                   })
for s in res[&quot;sentences&quot;]:
    print(&quot;%d: '%s': %s %s&quot; % (
        s[&quot;index&quot;],
        &quot; &quot;.join([t[&quot;word&quot;] for t in s[&quot;tokens&quot;]]),
        s[&quot;sentimentValue&quot;], s[&quot;sentiment&quot;]))
</code></pre>
<p>and you will get:</p>
<pre><code>0: 'I love you .': 3 Positive
1: 'I hate him .': 1 Negative
2: 'You are nice .': 3 Positive
3: 'He is dumb': 1 Negative
</code></pre>
<h1>Notes</h1>
<ol>
<li>You pass the whole text to the server and it splits it into sentences. It also splits sentences into tokens.</li>
<li>The sentiment is ascribed to each <em>sentence</em>, not the <em>whole text</em>. The <a href=""https://en.wikipedia.org/wiki/Mean"" rel=""nofollow noreferrer"">mean</a> <code>sentimentValue</code> across sentences can be used to estimate the sentiment of the whole text.</li>
<li>The average sentiment of a sentence is between <code>Neutral</code> (2) and <code>Negative</code> (1), the range is from <code>VeryNegative</code> (0) to <code>VeryPositive</code> (4) which appear to be quite rare.</li>
<li>You can <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#stopping-the-server"" rel=""nofollow noreferrer"">stop the server</a> either by typing <kbd>Ctrl-C</kbd> at the terminal you started it from or using the shell command <code>kill $(lsof -ti tcp:9000)</code>. <code>9000</code> is the default port, you can change it using the <code>-port</code> option when starting the server.</li>
<li>Increase <code>timeout</code> (in milliseconds) in server or client if you get timeout errors.</li>
<li><code>sentiment</code> is just <strong>one</strong> annotator, there are <a href=""https://stanfordnlp.github.io/CoreNLP/annotators.html"" rel=""nofollow noreferrer"">many more</a>, and you can request several, separating them by comma: <code>'annotators': 'sentiment,lemma'</code>.</li>
<li>Beware that the sentiment model is somewhat idiosyncratic (e.g., <a href=""https://github.com/stanfordnlp/CoreNLP/issues/351"" rel=""nofollow noreferrer"">the result is different depending on whether you mention David or Bill</a>).</li>
</ol>
<p><strong>PS</strong>. I cannot believe that I added a <strong>9th</strong> answer, but, I guess, I had to, since none of the existing answers helped me (some of the 8 previous answers have now been deleted, some others have been converted to comments).</p>
",70,30,36875,2015-10-01 04:34:36,https://stackoverflow.com/questions/32879532/stanford-nlp-for-python
TokensRegex: Using AND operators,"<p>TokensRegex (a module from Standford CoreNLP library) supports &amp; (AND) operators. As I understand, you can use pattern 'X &amp; Y' to match any sequences containing both X and Y. But when I used the operator in real code, it didn't work as the way I expected. Here is my Java code:</p>

<pre><code>String content = ""data is here and everywhere"";
String pattern = ""data &amp; is"";

TokenizerFactory tf = PTBTokenizer.factory(new CoreLabelTokenFactory(), """");
List&lt;CoreLabel&gt; tokens = tf.getTokenizer(new StringReader(content)).tokenize();
TokenSequencePattern seqPattern = TokenSequencePattern.compile(pattern);
TokenSequenceMatcher matcher = seqPattern.getMatcher(tokens);

if(matcher.find()){
      System.out.println(""Matched""); // &lt;- I expected to have this printed out
} else {
      System.out.println(""Unmatched""); // &lt;- But I've got this instead :(
}
</code></pre>

<p>Would you please tell me what's wrong with my code or my understanding? Thank you in advance.</p>
","pattern-matching, stanford-nlp, string-matching","<p>For the example given, <code>matcher.find()</code> will attempt to find a subsequence in the input token sequence that matches both conditions:</p>

<p><code>data</code>: a sequence of one token with the word <code>data</code></p>

<p><code>is</code>: a sequence of one token with the word <code>is</code></p>

<p>There is obviously no such sequence.  If you want to check if your token sequence include both the word <code>data</code> and the word <code>is</code>, you can try the pattern:</p>

<p><code>String pattern = ""(?: ( []* data []* ) &amp; ( []* is []* ))"";</code></p>

<p>The initial <code>?:</code> indicates it doesn't need to do subgroup capturing, and the <code>[]*</code> indicates a wildcard for any number of optional tokens.  </p>

<p>Although TokensRegex offers AND, it is not really part of normal regular expressions.  It's likely that there are other ways (without the AND) to achieve what you want.</p>
",4,1,329,2015-10-02 02:09:58,https://stackoverflow.com/questions/32899530/tokensregex-using-and-operators
initCoreNLP() method call from the Stanford&#39;s R coreNLP package throws error,"<p>I am trying to use the <code>coreNLP</code> package. I ran the following commands and encounter the <em>GC overhead limit exceeded</em> error. </p>

<pre><code>library(rJava)

downloadCoreNLP()

initCoreNLP()
</code></pre>

<p>Error is like this :  </p>

<blockquote>
  <p>Loading classifier from   edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... Error in rJava::.jnew(""edu.stanford.nlp.pipeline.StanfordCoreNLP"", basename(path)) : 
    java.lang.OutOfMemoryError: GC overhead limit exceeded
  Error during wrapup: cannot open the connection</p>
</blockquote>

<p>I don't know much of Java, can someone help me with this?</p>
","r, stanford-nlp","<p>@indi I ran into the same problem (see <a href=""https://stackoverflow.com/questions/34983149/rs-corenlpinitcorenlp-throws-java-lang-outofmemoryerror/35010559"">R&#39;s coreNLP::initCoreNLP() throws java.lang.OutOfMemoryError</a>) but was able to come up with a more repeatable solution than simply rebooting.</p>

<p>The full syntax for the init command is</p>

<pre><code>initCoreNLP(libLoc, parameterFile, mem = ""4g"", annotators)
</code></pre>

<p>Increasing <code>mem</code> did not help me, but I realized that you and I were both getting stuck with one of the classifiers in the <code>ner</code> annotator (named entity recognition). Since all I needed was parts-of-speech tagging, I replaced the init command with the following:</p>

<pre><code>initCoreNLP(mem = ""8g"", annotators = c(""tokenize"", ""ssplit"", ""pos""))
</code></pre>

<p>This caused the init command to execute in a flash and with no memory problems. BTW, I increased <code>mem</code> to 8g just because I have that much RAM. I'm sure I could have left it at the default 4g and it would have been fine.</p>

<p>I don't know if you need the <code>ner</code> annotator. If not, then explicitly list the <code>annotators</code> argument. Here is a list of the possible values: <a href=""http://stanfordnlp.github.io/CoreNLP/annotators.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/annotators.html</a>. Just pick the ones you absolutely need to get your job done. If you do need <code>ner</code>, then again figure out the minimal set of annotators you need and specify those.</p>

<p>So there you (and hopefully others) go!</p>
",0,2,2319,2015-10-08 12:28:44,https://stackoverflow.com/questions/33015823/initcorenlp-method-call-from-the-stanfords-r-corenlp-package-throws-error
TreebankLanguagePack function in Neural Network Dependency Parser,"<p>If I want to train the Stanford Neural Network Dependency Parser for another language, there is a need for a ""treebankLanguagePack""(TLP) but the information about this TLP is very limited: </p>

<blockquote>
  <p>particularities of your treebank and the language it contains</p>
</blockquote>

<p>If I have my ""treebank"" in another language that follows the same format as PTB,  and my data is using CONLL format. The dependency format follows the ""Universal Dependency"" UD.  Do I need this TLP? </p>
","parsing, neural-network, stanford-nlp","<p>As of the current CoreNLP release, the TreebankLanguagePack is used within the dependency parser only to 1) determine the input text encoding and 2) determine which tokens count as punctuation [1].</p>

<p>Your best bet for a quick solution, then, is probably to stick with the UD English TreebankLanguagePack. You should do this by specifying the property <code>language</code> as <code>""UniversalEnglish""</code> (whether you're accessing the dependency parser via code or command line). If you're using the dependency parser via the CoreNLP main entry point, this property key should be <code>depparse.language</code>.</p>

<hr>

<h3>Technical details</h3>

<p>Two very subtle details follow. You probably don't need to worry about these if you're just trying to hack something together at first, but it's probably good to mention so that you can avoid apocalyptic / head-smashing bugs in the future.</p>

<ul>
<li><strong>Evaluation and punctuation:</strong> If you do choose to stick with UniversalEnglish, be aware that there is a hack in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/parser/nndep/ParsingSystem.java#L128"" rel=""nofollow"">the evaluation code</a> that overrides the punctuation set for English parsing in particular. Any changes you make to punctuation in <code>PennTreebankLanguagePack</code> (the TLP used for the UniversalEnglish language) will be ignored! If you need to get around this, it should be enough to copy and paste the <code>PennTreebankLanguagePack</code> into your own codebase and name it something different.</li>
<li><strong>Potential memory leak:</strong> When building parse results to be returned to the user, the dependency parser <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/parser/nndep/DependencyParser.java#L979"" rel=""nofollow"">draws from a pool of cached <code>GrammaticalRelation</code> objects</a>. This cache does not live-update. This means that if you have relations which aren't formally defined in the language you specified via the <code>language</code> property, they will lead to the instantiation of a new object whenever those relations show up in parser predictions. (This can be a big deal memory-wise if you happen to store the parse objects somewhere.)</li>
</ul>

<p>[1]: Punctuation is excluded during evaluation. This is a standard ""cheat"" used throughout the dependency parsing literature.</p>
",2,1,293,2015-10-08 20:12:50,https://stackoverflow.com/questions/33025113/treebanklanguagepack-function-in-neural-network-dependency-parser
Tree node mapping to GrammaticalStructure dependency,"<p>I'm using the Stanford Core NLP framework 3.4.1 to construct syntactic parse trees of wikipedia sentences. After which I would like to extract out of each parse tree all of the tree fragments of certain length (i.e. at most 5 nodes), but I am having a lot of trouble figuring out how to do that without creating a new GrammaticalStructure for each sub-tree.</p>

<p>This is what I am using to construct the parse tree, most of the code is from TreePrint.printTreeInternal() for conll2007 format which I modified to suit my output needs:</p>

<pre><code>    DocumentPreprocessor dp = new DocumentPreprocessor(new StringReader(documentText));

    for (List&lt;HasWord&gt; sentence : dp) {
        StringBuilder plaintexSyntacticTree = new StringBuilder();
        String sentenceString = Sentence.listToString(sentence);

        PTBTokenizer tkzr = PTBTokenizer.newPTBTokenizer(new StringReader(sentenceString));
        List toks = tkzr.tokenize();
        // skip sentences smaller than 5 words
        if (toks.size() &lt; 5)
            continue;
        log.info(""\nTokens are: ""+PTBTokenizer.labelList2Text(toks));
        LexicalizedParser lp = LexicalizedParser.loadModel(
        ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"",
        ""-maxLength"", ""80"");
        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
        Tree parse = lp.apply(toks);
        GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
        Collection&lt;TypedDependency&gt; tdl = gs.allTypedDependencies();
        Tree it = parse.deepCopy(parse.treeFactory(), CoreLabel.factory());
        it.indexLeaves();

        List&lt;CoreLabel&gt; tagged = it.taggedLabeledYield();
        // getSortedDeps
        List&lt;Dependency&lt;Label, Label, Object&gt;&gt; sortedDeps = new ArrayList&lt;Dependency&lt;Label, Label, Object&gt;&gt;();
        for (TypedDependency dep : tdl) {
            NamedDependency nd = new NamedDependency(dep.gov().label(), dep.dep().label(), dep.reln().toString());
            sortedDeps.add(nd);
        }
        Collections.sort(sortedDeps, Dependencies.dependencyIndexComparator());

        for (int i = 0; i &lt; sortedDeps.size(); i++) {
          Dependency&lt;Label, Label, Object&gt; d = sortedDeps.get(i);

          CoreMap dep = (CoreMap) d.dependent();
          CoreMap gov = (CoreMap) d.governor();

          Integer depi = dep.get(CoreAnnotations.IndexAnnotation.class);
          Integer govi = gov.get(CoreAnnotations.IndexAnnotation.class);

          CoreLabel w = tagged.get(depi-1);

          // Used for both course and fine POS tag fields
          String tag = PTBTokenizer.ptbToken2Text(w.tag());

          String word = PTBTokenizer.ptbToken2Text(w.word());

          if (plaintexSyntacticTree.length() &gt; 0)
              plaintexSyntacticTree.append(' ');
          plaintexSyntacticTree.append(word+'/'+tag+'/'+govi);
        }
        log.info(""\nTree is: ""+plaintexSyntacticTree);
    }
</code></pre>

<p>In the output I need to get something of this format: word/Part-Of-Speech-tag/parentID which is compatible with the output of the <a href=""http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html"" rel=""nofollow noreferrer"" title=""Google Syntactic n-grams"">Google Syntactic N-Grams</a></p>

<p>I can't see to figure out, how I could get the POS tag and parentID from the original syntactic parse tree (stored in the GrammaticalStructure as a dependency list as far as I understand) for only a subset of nodes from the original tree.</p>

<p>I have also seen some mentions about the <a href=""https://stackoverflow.com/questions/19431754/using-stanford-parsercorenlp-to-find-phrase-heads/22841952#22841952"">HeadFinder</a> but as far as I understand that is only useful to construct the GrammaticalStructure, whereas I am trying to use the existing one. 
I have also seen a somwewhat similar issue about <a href=""https://stackoverflow.com/questions/32565312/cast-from-grammaticalstructure-to-tree"">converting GrammaticalStructure to Tree</a> but that is still an open issue and it does not tackle the issue of sub-trees or creating a custom output. Instead of creating a tree from the GrammaticalStructure I was thinking that I could just use the node reference from the tree to get the information I need, but I am basically missing an equivalent of getNodeByIndex() which can get index by node from GrammaticalStructure. </p>

<p><strong>UPDATE:</strong> I have manage to get all of the required information by using the SemanticGraph as suggested in the answer. Here is a basic snippet of code that does that:</p>

<pre><code>    String documentText = value.toString();
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize,ssplit,pos,depparse"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation annotation = new Annotation(documentText);
    pipeline.annotate(annotation);
    List&lt;CoreMap&gt; sentences =  annotation.get(CoreAnnotations.SentencesAnnotation.class);

    if (sentences != null &amp;&amp; sentences.size() &gt; 0) {
        CoreMap sentence = sentences.get(0);
        SemanticGraph sg = sentence.get(SemanticGraphCoreAnnotations.CollapsedDependenciesAnnotation.class);
        log.info(""SemanticGraph: ""+sg.toDotFormat());
       for (SemanticGraphEdge edge : sg.edgeIterable()) {
           int headIndex = edge.getGovernor().index();
           int depIndex = edge.getDependent().index();
           log.info(""[""+headIndex+""]""+edge.getSource().word()+""/""+depIndex+""/""+edge.getSource().get(CoreAnnotations.PartOfSpeechAnnotation.class));
       }
    }
</code></pre>
","java, tree, nlp, stanford-nlp","<p>The Google syntactic n-grams are using dependency trees rather than constituency trees. So, indeed, the only way to get that representation is by converting the tree to a dependency tree. The parent id you get from the constituency parse will be for an intermediate node, rather than another word in the sentence.</p>

<p>My recommendation would be to run the dependency parser annotator (<code>annotators = tokenize,ssplit,pos,depparse</code>), and from the resulting <code>SemanticGraph</code> extract all clusters of 5 neighboring nodes.</p>
",0,0,198,2015-10-09 13:52:26,https://stackoverflow.com/questions/33040054/tree-node-mapping-to-grammaticalstructure-dependency
Stemming option in stanfordcorenlp,"<p>Problem: Is there an option to stem the words using <code>stanford-core-nlp</code>?
I am not able to find one! I am using the stanford-corenlp-3.5.2.jar.</p>

<p><strong>Code:</strong></p>

<pre><code>public class StanfordNLPTester {

  public static void main (String args[]){

    String paragraph = ""A long paragraph here"";

    Properties properties = new Properties();
    properties.put(""annotators"",""tokenize,ssplit,pos,lemma,ner,depparse"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);
    Annotation annotation = new Annotation (paragraph);
    pipeline.annotate(annotation);
    pipeline.prettyPrint(annotation,System.out);
  }
}
</code></pre>
","stanford-nlp, porter-stemmer","<p>You'll need to get this from GitHub: <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""noreferrer"">https://github.com/stanfordnlp/CoreNLP</a></p>

<p>This class will provide what you want:</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/Stemmer.java"" rel=""noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/Stemmer.java</a></p>

<p>The main() method of that class shows example usage of the stemmer.</p>

<p>You can continue to use the stanford-corenlp-3.5.2.jar and just include that one extra class, since everything that class depends on is in the jar.</p>
",8,5,6808,2015-10-10 04:09:15,https://stackoverflow.com/questions/33050169/stemming-option-in-stanfordcorenlp
Stanford LexicalizedParser throws NPE when using in spark,"<p>I am attempting to use stanford's LexicalizedParser within Spark RDD map function.</p>

<p>The algorithm is roughly like this:</p>

<pre><code>val parser = LexicalizedParser.loadModel(englishPCFG.ser.gz)
val parserBroadcast = sparkContext.broadcast(parser) // using Kryo serializer here

someSparkRdd.map { case sentence: List[HasWord] =&gt;
    parserBroadcast.value.parse(sentence) //NPE is being thrown see below
}
</code></pre>

<p>The reason I would like to instantiate parser once (outside the map) and then just broadcast it, is that the map iterates over almost a million sentences, java garbage collector produces too much overhead and whole processing slows down reasonably.</p>

<p>Upon executing map statement, following NullPointerException is being thrown:</p>

<pre><code>java.lang.NullPointerException
    at edu.stanford.nlp.parser.lexparser.BaseLexicon.isKnown(BaseLexicon.java:152)
    at edu.stanford.nlp.parser.lexparser.BaseLexicon.ruleIteratorByWord(BaseLexicon.java:208)
    at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.initializeChart(ExhaustivePCFGParser.java:1343)
    at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.parse(ExhaustivePCFGParser.java:457)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.parseInternal(LexicalizedParserQuery.java:258)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.parse(LexicalizedParserQuery.java:536)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:301)
    at my.class.NounPhraseExtractionWithStanford$$anonfun$extractNounPhrases$3.apply(NounPhraseExtractionWithStanford.scala:28)
    at my.class.NounPhraseExtractionWithStanford$$anonfun$extractNounPhrases$3.apply(NounPhraseExtractionWithStanford.scala:27)
    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
    at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
    at my.class.NounPhraseExtractionWithStanford$.extractNounPhrases(NounPhraseExtractionWithStanford.scala:27)
    at my.class.HBaseDocumentProducerWithStanford$$anonfun$produceDocumentTokens$3.apply(HBaseDocumentProducerWithStanford.scala:104)
    at my.class.HBaseDocumentProducerWithStanford$$anonfun$produceDocumentTokens$3.apply(HBaseDocumentProducerWithStanford.scala:104)
    at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$15.apply(PairRDDFunctions.scala:674)
    at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$15.apply(PairRDDFunctions.scala:674)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
    at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
    at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:64)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>In the source code I see that apparently because of many transient class variables of edu.stanford.nlp.parser.lexparser.BaseLexicon the SerDe performed during broadcast (using Kryo serializer) leaves BaseLexicon half-initialized.</p>

<p>I realize that developers of LexParser didn't had spark in mind when designing it but still I would greatly appreciate any hints on how I could use it in my scenario (with spark that is).</p>
","scala, apache-spark, stanford-nlp","<p>One possible workaround, not 100% sure that it'll work:</p>

<pre><code>class ParseSentence extends (List[HasWord] =&gt; WhateverParseReturns) with Serializable {
  def apply(sentence: List[HasWord]) = ParseSentence.parser.parse(sentence)
}

object ParseSentence {
  val parser = LexicalizedParser.loadModel(englishPCFG.ser.gz)
}

someSparkRdd.map(new ParseSentence)
</code></pre>

<p>This way <code>parser</code> shouldn't need to be serialized/deserialized because it isn't captured as a field of the function object.</p>
",1,1,136,2015-10-12 15:35:01,https://stackoverflow.com/questions/33084944/stanford-lexicalizedparser-throws-npe-when-using-in-spark
Grammar rule extraction from parsed result,"<p>I get following result when i execute stanford parser from nltk.</p>

<pre><code>(S (VP (VB get) (NP (PRP me)) (ADVP (RB now))))
</code></pre>

<p>but i need it in the form </p>

<pre><code>S -&gt; VP
VP -&gt; VB NP ADVP
VB -&gt; get
PRP -&gt; me
RB -&gt; now
</code></pre>

<p>How can I get this result, perhaps using recursive function.
Is there in-built function already?  </p>
","python, recursion, nltk, stanford-nlp","<p>First to navigate a tree, see <a href=""https://stackoverflow.com/questions/27052690/how-to-iterate-through-all-nodes-of-a-tree"">How to iterate through all nodes of a tree?</a> and <a href=""https://stackoverflow.com/questions/14841997/how-to-navigate-a-nltk-tree-tree"">How to navigate a nltk.tree.Tree?</a> :</p>

<pre><code>&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; bracket_parse = ""(S (VP (VB get) (NP (PRP me)) (ADVP (RB now))))""
&gt;&gt;&gt; ptree = Tree.fromstring(bracket_parse)
&gt;&gt;&gt; ptree
Tree('S', [Tree('VP', [Tree('VB', ['get']), Tree('NP', [Tree('PRP', ['me'])]), Tree('ADVP', [Tree('RB', ['now'])])])])
&gt;&gt;&gt; for subtree in ptree.subtrees():
...     print subtree
... 
(S (VP (VB get) (NP (PRP me)) (ADVP (RB now))))
(VP (VB get) (NP (PRP me)) (ADVP (RB now)))
(VB get)
(NP (PRP me))
(PRP me)
(ADVP (RB now))
(RB now)
</code></pre>

<p>And what you're looking for is <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tree.py#L341"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/tree.py#L341</a>:</p>

<pre><code>&gt;&gt;&gt; ptree.productions()
[S -&gt; VP, VP -&gt; VB NP ADVP, VB -&gt; 'get', NP -&gt; PRP, PRP -&gt; 'me', ADVP -&gt; RB, RB -&gt; 'now']
</code></pre>

<p>Note that <code>Tree.productions()</code> returns a <code>Production</code> object, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tree.py#L22"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/tree.py#L22</a> and <a href=""https://github.com/nltk/nltk/blob/develop/nltk/grammar.py#L236"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/grammar.py#L236</a>. </p>

<p>If you want a string form of the grammar rules, you can either do:</p>

<pre><code>&gt;&gt;&gt; for rule in ptree.productions():
...     print rule
... 
S -&gt; VP
VP -&gt; VB NP ADVP
VB -&gt; 'get'
NP -&gt; PRP
PRP -&gt; 'me'
ADVP -&gt; RB
RB -&gt; 'now'
</code></pre>

<p>Or </p>

<pre><code>&gt;&gt;&gt; rules = [str(p) for p in ptree.productions()]
&gt;&gt;&gt; rules
['S -&gt; VP', 'VP -&gt; VB NP ADVP', ""VB -&gt; 'get'"", 'NP -&gt; PRP', ""PRP -&gt; 'me'"", 'ADVP -&gt; RB', ""RB -&gt; 'now'""]
</code></pre>
",5,5,273,2015-10-15 05:59:17,https://stackoverflow.com/questions/33140945/grammar-rule-extraction-from-parsed-result
Using stanford tokenizer,"<p>I'm using Stanford CoreNlp tool for tokenizing the text in a way that the introduced offset of each token is very important (I need the offset of each token to use it later in Brat). The related part of my program is as follow:</p>

<pre><code>pipeline.annotate(annotation);

        List&lt;CoreMap&gt; sentences =annotation.get(CoreAnnotations.SentencesAnnotation.class);
        if (sentences != null &amp;&amp; !sentences.isEmpty()) {
            for (CoreMap sentence : sentences) {
                // CoreMap sentence = sentences.get(0);
                for (CoreMap token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                    // out.println(token+""\t""+token.get(NamedEntityTagAnnotation.class));

                    words = token + ""\t"" + token.get(NamedEntityTagAnnotation.class);
                    String word_offset = token.toShorterString().toString();
                    wordsId.add(words);
                    wordsId1.add(words.substring(0, words.indexOf(""-"")).trim());
                    wordsId2.add(word_offset);
              System.Out.Println(""Text_woffset.txt"",word_offset+""\n"" );
                }
</code></pre>

<p>Input =
""D: Great!</p>

<p>CM: How are you, Daniella? {BR}</p>

<p>{NS}</p>

<p>D: I'm doing good, except for the fact that I'm hearing a little bit of echo.</p>

<p>CM: Oh. {LG} Darn.</p>

<p>D:  Give me a second.</p>

<p>CM: Okay.""</p>

<p>I use the following code to read the input:</p>

<p><code>Text = new Scanner(new File(Input)).useDelimiter(""\\A"").next();</code></p>

<p>With this input I get a wrong offset. For example for the token ""Daniella"" the offset should be [28 36]   but the tool shows me [27, 35] or in the middle of the text the token got 10 to 30 wrong offsets. Would you please let me know the way to cope with such a conversational text using tokenizer? I put the actual text as input (to ensure that the problem is not for using Scanner) but the problem remains the same.</p>
","java, stanford-nlp","<p>What you want is the CharacterOffsetBegin and CharacterOffsetEnd annotations attached to each token. A shorthand for this is <code>CoreLabel.begin()</code> and <code>CoreLabel.end()</code>. A minor tweak to your code: the tokens can be <code>CoreLabel</code>s (a subclass of <code>CoreMap</code>) -- the <code>CoreLabel</code> class has a bunch of utility methods that make working with them much easier.</p>

<p>As a general rule, while in the class hierarchy both CoreLabel and Annotation are subclasses of CoreMap, semantically an Annotation is almost always a document, a CoreMap is almost always a sentence, and a CoreLabel is almost always a token.</p>
",1,1,275,2015-10-15 20:12:26,https://stackoverflow.com/questions/33157682/using-stanford-tokenizer
CRFClassifier doesn&#39;t recognize sentence splitter options,"<p>I'm using CoreNLP to annotate NEs in multiline English text. When doing as follows:</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");
props.put(""ssplit.newlineIsSentenceBreak"", ""always"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
String contentStr = ""John speaks with Martin\n\nJeremy talks to him too."";
Annotation document 
= new  Annotation(contentStr);
pipeline.annotate(document);
List&lt;CoreMap&gt; sents = document.get(SentencesAnnotation.class);
for (int i = 0; i &lt; sents.size(); i++) {
    System.out.println(""sentence "" + i + "" ""+ sents.get(i));
}
</code></pre>

<p>Sentence splitting works fine and recognizes two sentences. However, when I use NER classification as follows:</p>

<pre><code>CRFClassifier classifier = CRFClassifier.getClassifier(""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"", props);
String classifiedStr = classifier.classifyWithInlineXML(contentStr);
</code></pre>

<p>I get the following error message:</p>

<pre><code>Unknown property: |ssplit.newlineIsSentenceBreak|  Unknown property: |annotators|
</code></pre>

<p>and the classifier seems to consider all the text as one sentence resulting in false recognition of an entity ""Martin Jeremy"" instead of two distinct entities.</p>

<p>Any idea what's wrong?</p>
","java, nlp, stanford-nlp","<p>The properties taken by the <code>CRFClassifier.getClassifier</code> are different from the properties taken by <code>StanfordCoreNLP</code> constructor, that's why you get the error that the option is unknown.</p>

<p>It will be set, but it won't be used at run time.</p>

<p>From <a href=""http://isw3.naist.jp/~yusuke-o/manuals/stanford-corenlp-3.2.0-javadoc/edu/stanford/nlp/sequences/SeqClassifierFlags.html"" rel=""nofollow"">here</a>, you will find that you need to set the properties of the <code>SeqClassifierFlags</code>. You need to set <code>tokenizerOptions</code>, and set the option to <code>""tokenizeNLs = true""</code>, which considers new lines as tokens.</p>

<p>Bottom line, set the property as follows, before getting the classifier. It should not give you the error of unknown property, and it should work as intended.</p>

<pre><code>Properties props = new Properties();
props.put(""tokenizerOptions"", ""tokenizeNLs=true"");

CRFClassifier classifier = CRFClassifier.getClassifier(""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"", props);
String classifiedStr = classifier.classifyWithInlineXML(contentStr);
</code></pre>
",2,0,187,2015-10-16 19:16:12,https://stackoverflow.com/questions/33178029/crfclassifier-doesnt-recognize-sentence-splitter-options
&quot;Merged&quot; dependency and constituency tree,"<p>I am working on a research in which I use the <code>CoreNlp</code> in order to parse sentences, using the various available annotators (mainly, constituency and sentiment).</p>

<p>I am now trying to create a ""merged"" trees which include both constituency and dependency information from which I am going to extract grammar (can think about PCFG).</p>

<p>I am trying to get to something like the left trees in the image:</p>

<p><a href=""https://i.sstatic.net/Lmubx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Lmubx.png"" alt=""trees""></a>
(image from <a href=""http://www.tsarfaty.com/pdfs/coling08.pdf"" rel=""nofollow noreferrer"">Relational-Realizational Parsing (Tsarfaty and Sima’an, 2008)</a>)</p>

<p>Is there some ""easy"" way to work with the provided parser outputs (in code that is) to get to something like that?</p>

<p>Alternatively, is there any implementation you may know of based on the Stanford NLP library?</p>

<p>Would <code>GrammaticalStructure</code> be helpful here? Does making a GS for each node and reading it's <code>typedDependencies()</code> at each constituency node make sense here?</p>
","java, stanford-nlp","<p>Created a ""snippet"" (quite a lengthy one) for the code required to create a merged constituency and dependency tree which goes along the lines suggested above - traversing both trees.</p>

<p><a href=""https://bitbucket.org/snippets/tcagan/bnkpe"" rel=""nofollow"">Merge Constituency and Dependency</a></p>

<p>The process in high level:</p>

<ol>
<li><p>Get the constituency tree using parser/pipeline</p></li>
<li><p>Get grammatical structure of the tree and from it, typed dependencies (note - keeping punctuation)</p></li>
<li><p>Convert the typed dependencies into a dependency tree</p></li>
<li><p>Merge the dependency and constituency trees based on the matching node coverage spans.</p></li>
</ol>

<p>The code (classes):</p>

<ul>
<li><p><code>DependencySpan</code> - represents a span and the nodes covering it (used for matching spans)</p></li>
<li><p><code>DependencyTreeNode</code> - a node in dependency tree which holds reference to a core label (which in turn, has a related word index used for getting a span)</p></li>
<li><p><code>MergedNode</code> - a merged constituency + dependency node.</p></li>
<li><p><code>MergeConstituencyAndDependency</code> - actual code to merge trees - works along the lines of algorithm above. Include the main (entry point for testing).</p></li>
</ul>

<p>One thing I am not 100% sure of is why the ""sentiment"" annotator is required - without it creating the grammatical structure throws an error.</p>

<p><strong>A disclaimer</strong> - I am not proficient with the Stanford NLP library and all it's options. The code is loosely based on other examples I've found online and there could be better ways of doing this task.</p>
",0,2,831,2015-10-17 14:53:43,https://stackoverflow.com/questions/33187943/merged-dependency-and-constituency-tree
coreNLP significantly slowing down spark job`,"<p>I'm attempting to make a spark job that does classification through cutting a document into sentences, and then lemmatizing each word in the sentence for logistic regression. However, I'm finding that stanford's annotation class is causing a SERIOUS bottleneck in my spark job (it's taking 20 minutes to process only 500k documents)</p>

<p>Here is the code I'm currently using for sentence parsing and classification</p>

<p>Sentence parsing:</p>

<pre><code>def prepSentences(text: String): List[CoreMap] = {
    val mod = text.replace(""Sr."", ""Sr"") // deals with an edge case
    val doc = new Annotation(mod)
    pipeHolder.get.annotate(doc)
    val sentences = doc.get(classOf[SentencesAnnotation]).toList
    sentences
}
</code></pre>

<p>I then take each coremap and process the lemmas as follows</p>

<pre><code>def coreMapToLemmas(map:CoreMap):Seq[String] = {
      map.get(classOf[TokensAnnotation]).par.foldLeft(Seq[String]())(
    (a, b) =&gt; {
        val lemma = b.get(classOf[LemmaAnnotation])
        if (!(stopWords.contains(b.lemma().toLowerCase) || puncWords.contains(b.originalText())))
      a :+ lemma.toLowerCase
    else a
  }
)
}
</code></pre>

<p>Perhaps there's a class that only involves some of the processing?</p>
","scala, machine-learning, apache-spark, stanford-nlp","<p>Try using <a href=""http://nlp.stanford.edu/software/corenlp.shtml#srparser"" rel=""nofollow"">CoreNLP's Shift Reduce parser implementation</a>.</p>

<p>A basic example (typing this without a compiler):</p>

<pre><code>val p = new Properties()
p.put(""annotators"", ""tokenize ssplit pos parse lemma sentiment"")
// use Shift-Reduce Parser with beam search
// http://nlp.stanford.edu/software/srparser.shtml
p.put(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.beam.ser.gz"")
val corenlp = new StanfordCoreNLP(props)

val text = ""text to annotate""
val annotation = new Annotation(text)
corenlp.annotate(text)
</code></pre>

<p>I work on a production system which uses CoreNLP in a Spark processing pipeline. Using the Shift Reduce parser with Beam search improved the parsing speed of my pipeline by a factor of 16 and reduced the amount of working memory required for parsing. The Shift Reduce parser is linear in runtime complexity, which is better than the standard lexicalized PCFG parser.</p>

<p>To use the shift reduce parser, you'll need the shift reduce models jar which you should put on your classpath (which you can download from CoreNLP's website separately).</p>
",7,2,663,2015-10-21 22:05:02,https://stackoverflow.com/questions/33270080/corenlp-significantly-slowing-down-spark-job
How to write custom rules for sutime in stanford temporal tagger?,"<p>Stanford temporal tagger is working fine for most cases. For example ""I should be in school tomorrow by 9'o clock."" is having 9'o clock as time. But SUTime is not identifying it :(</p>

<p>So I want to add rule based on this. Any suggestions ?</p>

<p>I'm using <a href=""http://nlp.stanford.edu/software/sutime.shtml"" rel=""nofollow"">this</a> </p>
","stanford-nlp, temporal, sutime","<p>I added this rule right below the ""9 o'clock"" rule:</p>

<pre><code>{ ( (?: /the/ /hour/ /of/?)? ([ $INT &amp; { numcompvalue&lt;=24 } ]) (/'/ /o/ /clock/)) =&gt; IsoTime($1[0].numcompvalue, 0, NIL) }
</code></pre>

<p>in: </p>

<p>edu/stanford/nlp/models/sutime/english.sutime.txt</p>

<p>and was able to recognize the pattern you described.</p>

<p>Note that the tokenizer splits ""9'o clock"" into [""9"", ""'"", ""o"", ""clock""].  </p>

<p>In general if you add rules to that file you can add custom patterns for cases you wish to cover.  That file can be found in this jar:</p>

<p>stanford-corenlp-3.5.2-models.jar</p>
",2,0,703,2015-10-22 17:43:30,https://stackoverflow.com/questions/33287288/how-to-write-custom-rules-for-sutime-in-stanford-temporal-tagger
Training a new Stanford part-of-speech tagger from within the NLTK,"<p>I've trained a part-of-speech tagger for an uncommon language (Uyghur) using the Stanford POS tagger and some self-collected training data. I've been using the NLTK's <code>nltk.tag.stanford.POSTagger</code> interface to tag individual sentences in Python. This works well for most of my purposes: (running from <code>/usr/share/stanford-postagger</code>)</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import POSTagger
&gt;&gt;&gt; uy = POSTagger('models/uyghur.tagger', 'stanford-postagger.jar')
&gt;&gt;&gt; uy.tag('Men méning mantini yégenlikimni bilimen .'.split())
[[(u'Men', u'PN1s'), (u'm\xe9ning', u'PN1s.GEN'), (u'mantini', u'N-ACC'), (u'y\xe9genlikimni', u'Vt-PST.dir-1s2'), (u'bilimen', u'Vt-PRS-1s1'), (u'.', u'PUNCT')]]
</code></pre>

<p>I would like to do a ten-fold cross-validation to get a better sense of the accuracy of this tagger, i.e., use each tenth of my complete training data as test data for a tagger trained on the other nine-tenths of the data. Splitting the data set ten ways is no problem in Python, but I don't know if there's a way to train a new tagger from within Python. When I've done it, it's been from the command line using <code>java -mx1g -classpath /usr/share/stanford-postagger/stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -props uyghurtagger.props</code>.</p>

<p>Is it possible to train a new Stanford tagger from within Python using the NLTK interface, or will I need to create it manually via the command line each time?</p>
","python, nlp, nltk, stanford-nlp","<p>This was much simpler to implement than I had thought.</p>

<pre><code>import subprocess32
subprocess32.call(
    ['java', '-mx1g', '-classpath',
     '/usr/share/stanford-postagger/stanford-postagger.jar',
     'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-props',
     'uyghurtagger.props'])
</code></pre>

<p>It's really just as simple as passing a list of the command line arguments to <code>subprocess32.call()</code>. (I am using <code>subprocess32</code> instead of <code>subprocess</code> per the recommendation in the <a href=""https://docs.python.org/2/library/subprocess.html"" rel=""nofollow""><code>subprocess</code> docs</a>).</p>
",1,1,782,2015-10-23 15:45:53,https://stackoverflow.com/questions/33306510/training-a-new-stanford-part-of-speech-tagger-from-within-the-nltk
Stanford NLP pos-tagger model REQUIREMENTS,"<p>I have made a basic application where I use Stanford Parser through OpenIE Lib and when the StanfordCoreNLP is initialized with the specified properties it stops as pos (pos-tagger). I do think all the required models are included so not sure why the process is unable to find the model data.</p>

<pre><code>Properties props = new Properties();       
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation doc = new Annotation(testString);
pipeline.annotate(doc);
for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class))
{
  Collection&lt;RelationTriple&gt; triples = sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
  for (RelationTriple triple : triples) {              
    System.out.println(triple.confidence + ""\t"" +
       triple.subjectLemmaGloss() + ""\t"" +
       triple.relationLemmaGloss() + ""\t"" +
       triple.objectLemmaGloss());
  }
}
</code></pre>

<p>The includes are</p>

<pre><code>stanford-parser.jar
stanford-parser-3.5.2-models.jar
stanford-openie.jar
stanford-openie-models.jar
JDK1.8
</code></pre>

<p>Stack-trace when the StanfordCoreNLP class is initiated.</p>

<pre><code>Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.2 sec].
Exception in thread ""AWT-EventQueue-0"" java.lang.NoSuchFieldError: REQUIREMENTS
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.requires(POSTaggerAnnotator.java:169)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:362)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:131)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:127)
    at org.sjdp.q2sm.GUI.extractTriplets(GUI.java:373)
    at org.sjdp.q2sm.GUI.processSPARQL(GUI.java:353)
    at org.sjdp.q2sm.GUI.actionPerformed(GUI.java:153)
    at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2022)
    at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2348)
    at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:402)
    at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:259)
    at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(BasicButtonListener.java:252)
    at java.awt.AWTEventMulticaster.mouseReleased(AWTEventMulticaster.java:289)
    at java.awt.Component.processMouseEvent(Component.java:6535)
    at javax.swing.JComponent.processMouseEvent(JComponent.java:3324)
    at java.awt.Component.processEvent(Component.java:6300)
    at java.awt.Container.processEvent(Container.java:2236)
    at java.awt.Component.dispatchEventImpl(Component.java:4891)
    at java.awt.Container.dispatchEventImpl(Container.java:2294)
    at java.awt.Component.dispatchEvent(Component.java:4713)
    at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4888)
    at java.awt.LightweightDispatcher.processMouseEvent(Container.java:4525)
    at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4466)
    at java.awt.Container.dispatchEventImpl(Container.java:2280)
    at java.awt.Window.dispatchEventImpl(Window.java:2750)
    at java.awt.Component.dispatchEvent(Component.java:4713)
    at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:758)
    at java.awt.EventQueue.access$500(EventQueue.java:97)
    at java.awt.EventQueue$3.run(EventQueue.java:709)
    at java.awt.EventQueue$3.run(EventQueue.java:703)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:76)
    at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:86)
    at java.awt.EventQueue$4.run(EventQueue.java:731)
    at java.awt.EventQueue$4.run(EventQueue.java:729)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:76)
    at java.awt.EventQueue.dispatchEvent(EventQueue.java:728)
    at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:201)
    at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
    at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
    at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
    at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
    at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)
</code></pre>
","stanford-nlp, pos-tagger","<p>This is almost certainly a class incompatibility bug from including both the parser and the OpenIE system at the same time. It'll be fixed in the next release (3.5.3) when everything syncs up again, but since OpenIE didn't exist at the 3.5.2 release the code in that jar is a bit ""ahead"" of the parser.</p>

<p>The easiest way to run the two at the same time is to run from the Github version of <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">CoreNLP</a>. The command <code>ant jar</code> should create a jar file which contains both the parser and the OpenIE system. The <a href=""http://nlp.stanford.edu/software/stanford-corenlp-models-current.jar"" rel=""nofollow"">most recent models</a> (warning: large download) should have both the parser and OpenIE models.</p>
",0,1,329,2015-10-25 07:16:23,https://stackoverflow.com/questions/33327193/stanford-nlp-pos-tagger-model-requirements
Should I download all the models/classifiers manually for Stanford NLP &quot;hello world&quot;?,"<p>I'm learning how to use the Stanford NLP library (i.e. learning how to write a ""hello world"" application) and I'm using for this purposes following <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">code snippet</a>
But, as usua,l I get a lot of <code>FileNotFoundException</code> exceptions, which means, that I haven't downloaded some libraries.</p>

<p>Whether I <strong>should</strong> download manually all these models and include them in my project as foreign resources OR it would be better to extract somehow all these models from the jar?</p>
","java, nlp, stanford-nlp","<p>As long as you have all the jars provided in the folder: </p>

<p>stanford-corenlp-full-2015-04-20/</p>

<p>in your classpath you should have all the resources you need to start with the toolkit.  </p>

<p>Could you provide more details about the error you're getting?</p>
",1,0,166,2015-10-25 08:25:08,https://stackoverflow.com/questions/33327639/should-i-download-all-the-models-classifiers-manually-for-stanford-nlp-hello-wo
Use lexical PCFG for generating meaningful phrase,"<p>How can i make use of lexical PCFG for generating grammar and thus sentense with more sense.
I can generate phrases from CFG grammar using NLTK library, but it most phrases doesnt make sense, although its grammatically correct.</p>

<pre><code> s=(""""""
    S -&gt; PRP RB VP
    VP -&gt; VBP NP
    NP -&gt; JJ NNS
    RB -&gt; 'forcefully'|'strongly'|'rerely'
    PRP -&gt; 'we'|'you'|'he'
    VBP -&gt; 'actuarize'|'support'|'condemn'
    JJ -&gt; 'black|fair'
    NNS -&gt; 'markets'
    """""")
#load into the grammar

grammar=CFG.fromstring(s)
for sentence in generate(grammar,depth=10):
    print(' '.join(sentence))
</code></pre>
","python, nltk, stanford-nlp","<p>A syntactic grammar will generate grammatical sentences, but it makes no guarantees that the sentences make sense. Really, there's no way to make sentences that make semantic sense -- this would require the computer to understand the meaning of what it's saying on a deeper level than currently possible. You can try to combine  your CFG with an n-gram language model, which should create more locally coherent sentences, but still not necessarily globally coherent.</p>
",2,0,287,2015-10-26 06:21:10,https://stackoverflow.com/questions/33339588/use-lexical-pcfg-for-generating-meaningful-phrase
Dependency parser evaluation with or without punctuation,"<p>I want to evaluate a dependency parser taking into consideration punctuation and not taking into consideration punctuation. How should I define the input data if I do not want to take into consideration punctuation? Should I use the same input data (normal sentences with punctuation) as input, the parser defines all the dependencies incuding punctuation. During evaluation I exclude all dependencies related to periods and commas, etc. Or should I remove punctuation in the input sentences?
Why is the punctuation often not included (CONLL-X) when evaluating a dependency parser?     </p>
","dependencies, nlp, stanford-nlp","<p>The input data should be defined the same regardless of evaluation details.
In standard CoNLL evaluation we simply do not count the arcs leading onto the punctuation tokens. (""Punctuation tokens"" in the standard eval are <code>`` '' . , :</code> . (<a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/parser/nndep/ParsingSystem.java#L130"" rel=""nofollow"">CoreNLP reference</a>))</p>

<p>As to the ""why,"" I don't have a very satisfying answer.. here are a few guesses:</p>

<ol>
<li>SOTA parsers are not so good at determining punctuation dependencies (true). Numbers drop substantially if we include punctuation. Real improvements in natural language parsing may be obscured by changes in punctuation performance, which is undesirable.</li>
<li>Punctuation dependencies are a bit hard to defend, I think* — the ones extant in the current datasets are just a convention, but other analyses of punctuation might also be licensed. (Compare this to an e.g. <code>amod</code> dependency, which can't really be disputed given that we agree on an annotation scheme.)</li>
</ol>

<p>`* I'm not an expert on dependency grammars, so please don't take me too seriously :)</p>
",3,1,442,2015-10-26 11:12:54,https://stackoverflow.com/questions/33344325/dependency-parser-evaluation-with-or-without-punctuation
sentence parsing is running extremely slowly,"<p>I'm attempting to create a Sentence Parser that can read in a document and predict the correct points to break up a sentence while not breaking on unimportant periods such as ""Dr."" or "".NET"", so I've been attempting to use CoreNLP</p>

<p>Upon realizing that PCFG was running way too slowly (and essentially bottlenecking my entire job) I attempted to switch to Shift-Reduce parsing (which according to the coreNLP website is way faster).</p>

<p>However, the SRParser is running extremely slowly and I have no idea why (as PCFG is processing 1000 sentences per second, the SRParser is doing 100).</p>

<p>Here is the code for both. One thing that might be note-worthy is that each ""document"" has about 10-20 sentences, so they're very small:</p>

<p>PCFG parser:</p>

<pre><code>class StanfordPCFGParser {
  val props = new Properties()
  props.put(""annotators"", ""tokenize, ssplit, pos, lemma"")
 val pipeline = new StanfordCoreNLP(props)
  var i = 0
  val time = java.lang.System.currentTimeMillis()

  def parseSentence(doc:String ):List[String] = {
    val tokens = new Annotation(doc)
    pipeline.annotate(tokens)
    val sentences = tokens.get(classOf[SentencesAnnotation]).toList
sentences.foreach(s =&gt;{ if(i%1000==0) println(""parsed "" + i + ""in "" + (java.lang.System.currentTimeMillis() - time)/1000 + "" seconds"" ); i = i+ 1})
sentences.map(_.toString)
  }
}
</code></pre>

<p>Shift-Reduce Parser:</p>

<pre><code>class StanfordShiftReduceParser {
  val p = new Properties()
  p.put(""annotators"", ""tokenize ssplit pos parse lemma "")
  p.put(""parse.model"", ""englishSR.ser.gz"")
  val corenlp = new StanfordCoreNLP(p)
  var i = 0
  val time = java.lang.System.currentTimeMillis()

  def parseSentences(text:String) = {
    val annotation = new Annotation(text)
    corenlp.annotate(annotation)
    val sentences = annotation.get(classOf[SentencesAnnotation]).toList
    sentences.foreach(s =&gt;{ if(i%1000==0) println(""parsed "" + i + ""in "" + (java.lang.System.currentTimeMillis() - time)/1000 + "" seconds"" ); i = i+ 1})
    sentences.map(_.toString)
  }
}
</code></pre>

<p>Here is the code I used for the timing:</p>

<pre><code>val originalParser = new StanfordPCFGParser
println(""starting PCFG"")
var time = getTime
sentences.foreach(originalParser.parseSentence)
time = getTime - time
println(""PCFG parser took "" + time.asInstanceOf[Double] / 1000 + ""seconds for 1000 documents to "" + originalParser.i + ""sentences"")
val srParser = new StanfordShiftReduceParser
println(""starting SRParse"")
time = getTime()
sentences.foreach(srParser.parseSentences)
time = getTime - time
println(""SR parser took "" + time.asInstanceOf[Double] / 1000 + ""seconds for 1000 documents to "" + srParser.i + ""sentences"")
</code></pre>

<p>Which gives me the following output (I've parsed out the ""Untokenizable"" warnings which happen because of questionable data sources)</p>

<pre><code>Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... starting PCFG
done [0.6 sec].
Adding annotator lemma
parsed 0in 0 seconds
parsed 1000in 1 seconds
parsed 2000in 2 seconds
parsed 3000in 3 seconds
parsed 4000in 5 seconds
parsed 5000in 5 seconds
parsed 6000in 6 seconds
parsed 7000in 7 seconds
parsed 8000in 8 seconds
parsed 9000in 9 seconds
PCFG parser took 10.158 seconds for 1000 documents to 9558 sentences
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Adding annotator parse
Loading parser from serialized file englishSR.ser.gz ... done [8.3 sec].
starting SRParse
Adding annotator lemma
parsed 0in 0 seconds
parsed 1000in 17 seconds
parsed 2000in 30 seconds
parsed 3000in 43 seconds
parsed 4000in 56 seconds
parsed 5000in 66 seconds
parsed 6000in 77 seconds
parsed 7000in 90 seconds
parsed 8000in 101 seconds
parsed 9000in 113 seconds
SR parser took 120.506 seconds for 1000 documents to 9558 sentences
</code></pre>

<p>Any help would be greatly appreciated!</p>
","scala, stanford-nlp","<p>If all you need to do is split a block of text into sentences, you only need the <code>tokenize</code> and <code>ssplit</code> annotators. The parser is completely superfluous. So:</p>

<pre><code>props.put(""annotators"", ""tokenize, ssplit"")
</code></pre>
",2,0,264,2015-10-26 22:18:58,https://stackoverflow.com/questions/33356624/sentence-parsing-is-running-extremely-slowly
Get begin poisitions and/or NER from words after parsing,"<p>I am using the new Stanford CoreNLP NN parser. Here's a simplified version of the code:</p>

<pre><code>// Sentence to be parsed
String sentence = ""This is an example sentence."";

// This is where we store the result from the parser. Initially set to ""null"".
GrammaticalStructure gs = null;

// Parse the sentence
DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(sentence));
List&lt;TaggedWord&gt; tagged = null;
for (List&lt;HasWord&gt; sent : tokenizer) {
    tagged = tagger.tagSentence(sent);
    gs = parser.predict(tagged);
}

// Convert the GrammaticalStructure object (the parsing result) into a semantic graph
SemanticGraph semanticGraph = SemanticGraphFactory.generateUncollapsedDependencies(gs);
</code></pre>

<p>Now, when I iterate over the vertices of <code>semanticGraph</code>, I can get the POS tag, but I can't get the NER of the word nor the begin position. So, when I do this:</p>

<pre><code>for (IndexedWord vertex : new ArrayList&lt;&gt;(semanticGraph.vertexSet())){
    String tag = vertex.tag();
    String ner = vertex.ner();
    int beginPosition = vertex.beginPosition();
}
</code></pre>

<p>for <code>tag</code> I get the POS tag correctly, for <code>ner</code> I get <code>null</code> and for <code>beginPostion</code> I always get -1. </p>

<p>How can I do the parsing with correctly preserving the begin position of the word in the original string? And if possible, how do I get the NER? (<code>beginPosition</code> is actually more important in my case)</p>
","java, nlp, stanford-nlp","<p>In your case NER tags don't exist because you are not actually performing such an annotation in your code. I am not sure why <code>beginPosition</code> is not set in the <code>SemanticGraph</code></p>

<p>Using a <code>StanfordCoreNLP</code> pipeline is highly recommended for multiple annotations that depend on each other. It's very easy to (re)configure it to use different annotators through a <code>Properties</code> object. There is also potential for better performance as it can use multiple threads.</p>

<p>Here is a simple example with a pipeline that keeps the for loop from your code. I have tested (CoreNLP 3.5.2) and both <code>ner</code> and <code>beginPosition</code> are set correctly. Since no recognizable entities exist in you example sentence <code>ner</code> is always <code>""O""</code>. Also if you have more than one sentences in your document, you will have to iterate over <code>sentences</code> list.</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

String sentence = ""This is an example sentence."";
Annotation document = new Annotation(sentence);
pipeline.annotate(document);

List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
CoreMap map = sentences.get(0);
SemanticGraph semanticGraph = map.get(CollapsedCCProcessedDependenciesAnnotation.class);

for (IndexedWord vertex : new ArrayList&lt;&gt;(semanticGraph.vertexSet())) {
    String tag = vertex.tag();
    String ner = vertex.ner();
    int beginPosition = vertex.beginPosition();
}
</code></pre>
",3,3,102,2015-10-28 16:06:22,https://stackoverflow.com/questions/33396009/get-begin-poisitions-and-or-ner-from-words-after-parsing
case tag in Universal dependencies,"<p>I am new to NLP.
While studying Universal dependency output of Stanford parser, see case tag.
Unable to find reference to this in the manual</p>

<pre><code>root(ROOT-0, transfer-1)
dep(100-3, $-2)
dobj(transfer-1, 100-3)
case(John-5, to-4)
nmod(100-3, John-5)
case(account-8, from-6)
nmod:poss(account-8, my-7)
nmod(transfer-1, account-8)
acl(account-8, ending-9)
case(1234-11, with-10)
nmod(ending-9, 1234-11)
</code></pre>

<p>Can someone point me to update manual reference or explain significance of case tag</p>
",stanford-nlp,"<p>These are documented in the <a href=""http://universaldependencies.github.io/docs/u/dep/index.html"" rel=""nofollow"">Universal Dependencies manual</a>. The <code>case</code> edge is <a href=""http://universaldependencies.github.io/docs/u/dep/case.html"" rel=""nofollow"">documented here</a>. For English, this is almost always the preposition type of the incoming preposition arc. So, ""<em>in Canada</em>"" would have an incoming edge <code>nmod:in</code> to <em>Canada</em>, and a <code>case</code> edge from <em>Canada</em> to <em>in</em>. The one common special case I've seen is possessives, which are now marked with <code>nmod:poss</code> and have an associated <code>case</code> edge to the ""<em>'s</em>"" token (e.g., <em>Canada 's hokey team</em>).</p>
",2,0,768,2015-10-28 17:57:41,https://stackoverflow.com/questions/33398289/case-tag-in-universal-dependencies
Fastest way to lemmatize sentences,"<p>So I'm currently building a classification pipeline, and at this point the corenlp lemmatizer appears to be a fairly significant bottleneck. I'm trying to figure out if the way in which I am lemmatizing is causing the slowdown or if lemmatization is just slow in general.</p>

<p>Here's my current code:</p>

<pre><code> def singleStanfordSentenceToLemmas(sentence: String): Seq[String] = {
    val doc = new Annotation(sentence)
    pipeline.annotate(doc)
    val tokens = doc.get(classOf[TokensAnnotation]).toList
    tokens.par.foldLeft(Seq[String]())(
      (a, b) =&gt; {
        val lemma = b.get(classOf[LemmaAnnotation])
        if (!(stopWords.contains(b.lemma().toLowerCase) || puncWords.contains(b.originalText())))
          a :+ lemma.toLowerCase
        else a
      }
    )
  }
</code></pre>

<p>And here's the code that creates the pipeline</p>

<pre><code>  val props = new Properties()

  props.put(""annotators"", ""tokenize, ssplit, pos, lemma"")
  val pipeline = new StanfordCoreNLP(props)
</code></pre>

<p>My current theories are</p>

<p>a) the fact that I'm using a fullblown coreNLP object is carrying a lot of overhead that is slowing down everything. Perhaps there is a more minimal class that ONLY lemmatizes?</p>

<p>b) The fact that the lemmatizer requires ssplit, and POS tagging seems pretty intense, since I'm only giving it individual sentences, is there a more efficient way for finding lemmas of individual words?</p>

<p>c) perhaps corenlp is just slow and there might be a faster lemmatizer out there.</p>

<p>Any help would be highly appreciated!</p>
","performance, scala, machine-learning, stanford-nlp","<p>a) Yes, there is certainly overhead there. You can get rid of <em>some</em> of it, but CoreNLP seems (to me) rather inconsistent in separating the core Pipeline Wrappers from the underlying, more direct entities.But you can do:</p>

<pre><code>import edu.stanford.nlp.process.Morphology
val morph = new Morphology()
...
morph.stem(label)
</code></pre>

<p>You will also need siomething like </p>

<pre><code>private lazy val POSTagger =
new MaxentTagger(""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"")
</code></pre>

<p>to previously tag POS, but I think this puts you on the right track.</p>

<p>b) You wont get rid of all this easily. <code>CoreLabel</code> is the main data structure around CoreNLP and is ussed to add more and more data to the same elements. So lemmatization will add lemmas to the same structure. POS tagging will be used bu the lemmatizer in order to differentiate between nouns, verbs, etc and will pick POS tags from there too. </p>

<p>c) Yes, this is the case too. How to deal with this varies a lot with your intent and context. I'm for instance using CoreNLP inside Spark to use the full power of a distributed cluster and I'm also pre-computing and storing some of this data. I hope this gives you some insight.</p>
",1,1,854,2015-11-02 22:53:05,https://stackoverflow.com/questions/33488504/fastest-way-to-lemmatize-sentences
java.lang.NoClassDefFoundError: edu/stanford/nlp/parser/lexparser/LexicalizedParser,"<p>I am working with Stanford Parser API. My system specifications are listed below:</p>

<p>OS: Win8</p>

<p>IDE: .IntelliJIdea14 </p>

<p>JDK: 1.8</p>

<p>Stanford-Parser 3.5.2 Version</p>

<p>I have imported stanford-parser.jar and ejml-0.23.jar in module dependencies (ClassPath). 
 There are some parser models that are saved in a jar file named stanford-parser-3.5.2-models.</p>

<p>The Stanford support team says: </p>

<blockquote>
  <p>""In recent distributions, the models are included in a jar file inside
  the parser distribution. For example, in the 2012-11-12 distribution,
  the models are included in stanford-parser-2.0.4-models.jar The
  easiest way to access these models is to include this file in your
  classpath. The parser will then be able to read the models from that
  jar file. "" </p>
</blockquote>

<p>But I can't import  stanford-parser-3.5.2-models.jar file. So I extracted it, save the model in an appropriate address in D Drive and finally changed the following code:</p>

<pre><code>String parserModel = ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"";

LexicalizedParser lp = LexicalizedParser.loadModel(parserModel);
</code></pre>

<p><strong>To</strong></p>

<pre><code>String parserModel = ""D:\\ MasterofScience\\Tools\\Stanford Dependenct Tree\\models"" +
    ""\\lexparser\\englishPCFG.ser.gz"";

 LexicalizedParser lp = LexicalizedParser.loadModel(parserModel);
</code></pre>

<p>But I give these exception errors:</p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: edu/stanford/nlp/parser/lexparser/LexicalizedParser
at java.lang.Class.getDeclaredMethods0(Native Method)
at java.lang.Class.privateGetDeclaredMethods(Unknown Source)
at java.lang.Class.privateGetMethodRecursive(Unknown Source)
at java.lang.Class.getMethod0(Unknown Source)
at java.lang.Class.getMethod(Unknown Source)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:119)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.parser.lexparser.LexicalizedParser
at java.net.URLClassLoader.findClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
... 6 more
</code></pre>

<p>Even I Don’t change the code, I receive the same exception!
What should I do?</p>
","java, intellij-idea, stanford-nlp","<p>You're missing the parser jar (<code>stanford-parser.jar</code>) in your classpath. Really, you can add both the parser jar and the model jar to your classpath, and then the program should work.</p>
",0,0,1165,2015-11-03 09:30:53,https://stackoverflow.com/questions/33495565/java-lang-noclassdeffounderror-edu-stanford-nlp-parser-lexparser-lexicalizedpar
Stanford Core NLP LexicalizedParser Model,"<p>I am new to NLP.
I am trying a sample program with <code>LexicalizedParser</code> but am not able to locate the model.</p>

<pre><code>String parseModel = ""...../models/lexparser/englishPCFG.ser.gz"";
LexicalizedParser lecicalizedParser = LexicalizedParser.loadModel(parseModel);
</code></pre>

<p>I have the required <code>stanford-core-nlp-3.5.2.jar</code> and the ner jar too in build path of a sample Java application.</p>

<p>I tried referring the absolute path of the core jar and load it but could not. :(</p>

<p>How can I refer to the exact location of this model from my program code?</p>

<p>A big thank you for any help and all help!</p>
","java, stanford-nlp","<p>If you use maven, make sure you include both of these dependencies in you <code>pom.xml</code></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.5.2&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.5.2&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
</code></pre>

<p>This model <code>englishPCFG.ser.gz</code><br>
is inside package <code>edu.stanford.nlp.models.lexparser</code><br>
that is inside <code>stanford-corenlp-3.5.2-models.jar</code></p>

<p>So you should use this path:</p>

<pre><code>String parseModel = ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz""
</code></pre>
",4,5,755,2015-11-03 13:38:01,https://stackoverflow.com/questions/33500572/stanford-core-nlp-lexicalizedparser-model
"Extracting email addresses, phone numbers using Stanford CoreNLP","<p>I have been looking for a solution to extract email addresses, phone numbers, ... from a text using Stanford CoreNLP (RegexNERAnnotator). Can anyone please provide any example?</p>

<p><strong>UPDATE : 04/11/2015:</strong>
Actually i should asked instead if there is a way Stanford RegexNERAnnotator can supports Java Regular expression.</p>

<p>Example Usage:</p>

<pre><code>       final String EMAIL_PATTERN = 
            ""^[_A-Za-z0-9-\\+]+(\\.[_A-Za-z0-9-]+)*@""
            + ""[A-Za-z0-9-]+(\\.[A-Za-z0-9]+)*(\\.[A-Za-z]{2,})$"";

       List&lt;CoreLabel&gt; tokens = ...;
       TokenSequencePattern pattern = TokenSequencePattern.compile(EMAIL_PATTERN);
       TokenSequenceMatcher matcher = pattern.getMatcher(tokens);

       while (matcher.find()) {
         String matchedString = matcher.group();
         List&lt;CoreMap&gt; matchedTokens = matcher.groupNodes();
         ...
       }
</code></pre>

<p><strong>It seems that it doesn't support Java Regular expression:</strong></p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.ling.tokensregex.parser.TokenMgrError: Lexical error at line 1, column 1.  Encountered: ""^"" (94), after : """"
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParserTokenManager.getNextToken(TokenSequenceParserTokenManager.java:1029)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.jj_ntk(TokenSequenceParser.java:3228)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexBasic(TokenSequenceParser.java:784)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexDisjConj(TokenSequenceParser.java:973)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegex(TokenSequenceParser.java:743)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexWithAction(TokenSequenceParser.java:1596)
    at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.parseSequenceWithAction(TokenSequenceParser.java:37)
    at edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.compile(TokenSequencePattern.java:186)
    at edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.compile(TokenSequencePattern.java:169)
</code></pre>
","nlp, stanford-nlp","<p>StackOverflow is not a place for tutorials, or even examples. But, it seems like a regular regex should work, even without needing RegexNER. From a bit of Googling, see <a href=""https://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address"">Using a regular expression to validate an email address</a> for emails. Phone numbers should be as easy as the following long, but straightforward regex:</p>

<pre><code>(\+[0-9]{1,2}(\s*|-)?)?(\(?[0-9]{3}\)?)?(\s*|-)[0-9]{3}(\s*|-)[0-9]{4}
</code></pre>

<p>My guess is that the tokenization from the Stanford Tokenizer would make this harder and not easier.</p>
",5,0,2108,2015-11-03 18:41:04,https://stackoverflow.com/questions/33506672/extracting-email-addresses-phone-numbers-using-stanford-corenlp
Is Stanford NER free for a website?,"<p>I'm working for a company that stores millions of newspapers articles. Subscribers can log on our site and make queries. Our search engine returns articles that match the queries. </p>

<p>I know that in general, a company can use GPL Stanford software commercially as long as all their source code is available and the GPL is preserved in their distribution. The case here is that we dot not distribute any code binaries to clients. Clients only log in, make queries and get results. In this particular case, can we use Stanford NER free of charge or do we have to purchase a commercial license?</p>
",stanford-nlp,"<p>You should seek professional legal council on this, rather than trusting answers on stackoverflow. My impression however is that yes, so long as you're behind a website and not distributing the source code, Stanford CoreNLP is legal to use. Of course, we always appreciate it if buy a license -- it goes to buying us poor grad students things like new chairs and computers :).</p>
",0,-4,259,2015-11-05 17:32:26,https://stackoverflow.com/questions/33551096/is-stanford-ner-free-for-a-website
Is it possible to use the LexicalizedParser class to do &#39;pure&#39; dependency parsing?,"<p>I am trying to use the Stanford NLP toolkit to evaluate dependency grammars. </p>

<p>Is it possible to make the LexicalizedParser class use <strong>only</strong> the rules from the DEPENDENCY_GRAMMAR section of the model file?</p>

<p>Leaving the other sections of the model file blank causes exceptions, and changing the 'doPCFG' flag to 'false' in the model file causes a null pointer exception in ExhaustiveDependencyParser.getBestScore.</p>

<p>It appears that the LexicalizeParser class falls back on the (constituency) PCFG when it cannot parse the input using the dependency rules.</p>

<p>Is this correct? Can this behaviour be switched off so that dependency grammars can be evaluated in isolation? </p>
","parsing, stanford-nlp","<p>What you're looking for is the <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">Stanford Dependency Parser</a>, which only produces a dependency parser and not a constituency parse. The lexicalized parser, by contrast, only produces a constituency parse, which is then converted to a dependency parse with a converter. Therefore, there is no option to not produce this constituency parse and still produce a dependency parse, as there's nothing for the converter to convert.</p>

<p>If you're only doing evaluation of grammatical formalisms, however, you may consider using the dependency converter on gold trees, or using the new Universal Dependencies annotated data.</p>
",2,2,251,2015-11-06 17:39:55,https://stackoverflow.com/questions/33572544/is-it-possible-to-use-the-lexicalizedparser-class-to-do-pure-dependency-parsin
How CFG and google n-gram can be combined to generate sentences,"<p>I have valid list of grammars and lexical items for generating grammatical correct phrases yet meaningless. I want to combine google n-gram to generate only the valid sentences. Is it feasible, is there any paper on this. I am using NLTK and Stanford core nlp tools.</p>
","nltk, stanford-nlp, n-gram","<p>No, it is not feasible. Real sentences have structure and meaning dependencies that go well beyond what can be captured in ngrams.</p>

<p>I suppose you're thinking of generating a random structure by expanding your CFG, then using ngrams to select among the possible vocabulary choices. It's a pretty simple thing to code: Chop off your grammar at the part-of-speech level, generate a ""sentence"" with your CFG as a string of POS tags, and use the ngrams to fill them out one by one.</p>

<p>To work with google's entire 5-gram collection you'll need a lot of disk space and a huge amount of RAM or some clever programming, so I recommend you experiment with one of the NLTK's tagged corpora (e.g., the Brown corpus with the ""universal"" tagset). Starting from any text, it is not hard to collect its ngrams, write a random text generator, and confirm that it produces semi-cohesive but undeniably incoherent (and still mostly ungrammatical) nonsense.</p>
",2,1,282,2015-11-09 06:07:41,https://stackoverflow.com/questions/33603357/how-cfg-and-google-n-gram-can-be-combined-to-generate-sentences
issue recognizing NEs with StanfordNER in python NLTK,"<p>This happens when there is a potential NE followed by a comma, for example if my strings are something like,</p>

<blockquote>
  <p>""These names  Praveen Kumar,,  David Harrison,  Paul Harrison, blah ""</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>""California, United States""</p>
</blockquote>

<p>my output is something as follows, respectively.</p>

<blockquote>
  <p>[[(u'These', u'O'), (u'names', u'O'), (u'Praveen', u'O'), (u'Kumar,,', u'O'), (u'David', u'PERSON'), (u'Harrison,', u'O'), (u'Paul', u'PERSON'), (u'Harrison,', u'O'), (u'blah', u'O')]]</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>[[(u'California,', u'O'), (u'United', u'LOCATION'), (u'States', u'LOCATION')]] </p>
</blockquote>

<p>why it doesn't recognize potential NEs such as ""Praveen Kumar"", ""Harrison"" and ""California""? </p>

<p>Here is how is use it in the code:</p>

<pre><code>from nltk.tag.stanford import NERTagger
st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')

tags = st.tag(""California, United States"".split())
</code></pre>

<p>Is it because I tokenize the input stirng with <code>split()</code> ? How can I resolve this as it's working fine when tried in Java?</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>Since you are doing this through the nltk, use its tokenizers to split your input:</p>

<pre><code>alltext = myfile.read()
tokenized_text = nltk.word_tokenize(alltext)
</code></pre>

<p><strong>Edit:</strong> You're probably better off with the stanford toolkit's own tokenizer, as recommended by the other answer.  So if you'll be feeding the tokens to one of the Stanford tools, tokenize your text like this to get exactly the tokenization that the tools expect:</p>

<pre><code>from nltk.tokenize.stanford import StanfordTokenizer
tokenize = StanfordTokenizer().tokenize

alltext = myfile.read()
tokenized_text = tokenize(alltext)
</code></pre>

<p>To use this method you'll need to have the Stanford tools installed, and the nltk must be able to find them. I assume you have already taken care of this, since you're using the Stanford NER tool.</p>
",2,1,156,2015-11-09 06:26:03,https://stackoverflow.com/questions/33603534/issue-recognizing-nes-with-stanfordner-in-python-nltk
Stanford CoreNLP: input with one sentence per line,"<p>I'm using the Stanford NLP tool for college work. This parser ends the sentences at every point (period) but I need also to close in each line, that is, in each character ' \ n' . by command line, you can use the option "" -sentences "" but so far there is not a similar command for code .</p>

<p>The option setOptionFlags from LexicalizedParser did not work either</p>
","java, parsing, stanford-nlp","<p>Here is some sample code to elaborate on Gabor's answer:</p>

<pre><code>import java.nio.file.Paths;
import java.nio.file.Files;
import java.nio.charset.StandardCharsets;

import java.io.*;
import java.util.*;
import java.nio.file.Paths;
import java.nio.file.Files;
import java.nio.charset.StandardCharsets;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.ling.CoreAnnotations.*;
import edu.stanford.nlp.util.*;

public class ParserExample {

    public static void main (String[] args) throws IOException {
        String text = new String(Files.readAllBytes(Paths.get(args[0])), StandardCharsets.UTF_8);
        Annotation document = new Annotation(text);
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
        props.setProperty(""ssplit.newlineIsSentenceBreak"", ""always"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        pipeline.annotate(document);
    }

}
</code></pre>

<p>args[0] should be the path to your file with one sentence per line</p>

<p>You will need to download Stanford CoreNLP 3.5.2 from this link and put the jars from the download in your classpath: <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>

<p>You can set other options for the parser with props.setProperty()</p>

<p>If you have a file with one sentence per line, you can use </p>

<pre><code>props.setProperty(""ssplit.eolonly"", ""true"");
</code></pre>

<p>if you only want to split on newlines.</p>
",4,4,1390,2015-11-09 08:18:45,https://stackoverflow.com/questions/33604939/stanford-corenlp-input-with-one-sentence-per-line
How to create a Stanford coreNLP model by training?,"<p>I'm very new to Stanford's coreNLP and I'm trying to train it by creating a model. I have a folder that has dev.txt, train.txt, and test.txt as well as a jar file named stanford-corenlp-3.5.1-models.jar. According to <a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool/22635259#22635259"">this</a> question, I can create a model by executing the following command in the terminal:</p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath     dev.txt -train -model model.ser.gz
</code></pre>

<p>However, when I run that in the terminal, I get the following error:</p>

<pre><code>Error: could not find or load main class edu.stanford.nlp.sentiment.SentimentTraining
</code></pre>

<p>Can anyone provide step-by-step instructions of how to go about training CoreNLP? I went on the Stanford <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow noreferrer"">website</a> to see how training is done, but I'm still confused. I thought all I needed to create a model (e.g model.ser.gz) were those three text files and one jar file. </p>

<p>Any help is very appreciated, thank you!</p>
","java, stanford-nlp, sentiment-analysis, training-data","<p>You need to include the CoreNLP jar file in your classpath. So, your java command should look like:</p>

<p><code>java -cp /path/to/corenlp/jar:/path/to/corenlp/library/dependencies -mx8g ...</code></p>

<p>From the root of the CoreNLP distribution, you can just include all the jars in the directory; e.g.,</p>

<p><code>java -cp ""*"" -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz</code></p>
",1,0,2593,2015-11-10 03:01:11,https://stackoverflow.com/questions/33622132/how-to-create-a-stanford-corenlp-model-by-training
Named entities: guidelines that pertain to titles of persons,"<p>I'm working on an annotation task of named entities in a text corpus. I found guidelines in the document <a href=""http://www.dessem.com/sites/default/files/ne99_taskdef_v1_4.pdf"" rel=""nofollow"">1999 Named Entity Recognition Task Definition</a>. In that document, there are guidelines that pertain to titles of persons, in particular the following one: <em>Titles such as “Mr.” and role names such as “President” are not considered part of a person name.</em> For example, in “Mr. Harry Schearer” or “President Harry Schearer”, only Harry Schearer should be tagged as person.</p>

<p>In the Stanford NER though, there are many examples of including titles in the person tag (Captain Weston, Mr. Perry, etc). See <a href=""http://nlp.stanford.edu/software/ner-example/austen.gaz.txt"" rel=""nofollow"">here</a> an example of gazette that they give. In their view of person tags, it seems that even “Mrs. and Miss Bates” should be tagged as a person.</p>

<p>Question: what is the most generally accepted guideline?</p>
","stanford-nlp, named-entity-recognition","<p>If you download Stanford CoreNLP 3.5.2 from here: <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>

<p>and run this command:</p>

<pre><code>java -Xmx6g -cp ""*:."" edu.stanford.nlp.pipeline.StanfordCoreNLP -ssplit.eolonly -annotators tokenize,ssplit,pos,lemma,ner -file ner_examples.txt -outputFormat text
</code></pre>

<p>(assuming you put some sample sentences, one sentence per line in ner_examples.txt)</p>

<p>the tagged tokens will be shown in: ner_examples.txt.out</p>

<p>You can try out some sentences and see how our current NER system handles different situations.  This system is trained on data that does not have titles tagged as PERSON, so our current system in general does not tag the titles as PERSON.</p>
",1,0,534,2015-11-10 15:43:13,https://stackoverflow.com/questions/33633874/named-entities-guidelines-that-pertain-to-titles-of-persons
How to split the result of PTBTokenizer into sentences?,"<p>I know I could use <code>DocumentPreprocessor</code> to split a text into sentence. But it does not provide enough information if one wants to convert the tokenized text back to the original text. So I have to use <code>PTBTokenizer</code>, which has an <code>invertible</code> option. </p>

<p>However, <code>PTBTokenizer</code> simply returns an iterator of all the tokens (<code>CoreLabel</code>s) in a document. It does not split the document into sentences. </p>

<p><a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow"">The documentation</a> says:</p>

<blockquote>
  <p>The output of PTBTokenizer can be post-processed to divide a text into sentences.</p>
</blockquote>

<p>But this is obviously not trivial. </p>

<p>Is there a class in the Stanford NLP library that can take as input a sequence of <code>CoreLabel</code>s, and output sentences? Here's what I mean exactly:</p>

<pre><code>List&lt;List&lt;CoreLabel&gt;&gt; split(List&lt;CoreLabel&gt; documentTokens);
</code></pre>
",stanford-nlp,"<p>I would suggest you use the StanfordCoreNLP class.  Here is some sample code:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.ling.CoreAnnotations.*;
import edu.stanford.nlp.util.*;

public class PipelineExample {

    public static void main (String[] args) throws IOException {
        // build pipeline                                                                                                                                         
        Properties props = new Properties();
        props.setProperty(""annotators"",""tokenize, ssplit, pos"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        String text = "" I am a sentence.  I am another sentence."";
        Annotation annotation = new Annotation(text);
        pipeline.annotate(annotation);
        System.out.println(annotation.get(TextAnnotation.class));
        List&lt;CoreMap&gt; sentences = annotation.get(SentencesAnnotation.class);
        for (CoreMap sentence : sentences) {
            System.out.println(sentence.get(TokensAnnotation.class));
            for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
                System.out.println(token.after() != null);
                System.out.println(token.before() != null);
                System.out.println(token.beginPosition());
                System.out.println(token.endPosition());
            }
        }
    }

}
</code></pre>
",1,0,549,2015-11-13 01:26:18,https://stackoverflow.com/questions/33684475/how-to-split-the-result-of-ptbtokenizer-into-sentences
Questions about creating stanford CoreNLP training models,"<p>I've been working with Stanford's coreNLP to perform sentiment analysis on some data I have and I'm working on creating a training model. I know we can create a training model with the following command: </p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath     dev.txt -train -model model.ser.gz
</code></pre>

<p>I know what goes in the train.txt file. You score sentences and put them in train.txt, something like this:
 <code>(0 (2 Today) (0 (0 (2 is) (0 (2 a) (0 (0 bad) (2 day)))) (..)))</code></p>

<p>But I don't understand what goes in the dev.txt file. 
I read through <a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool"">this</a> question several times to try to understand what goes in dev.txt, but it's still unclear to me. Also, scoring these sentences manually has become a pain, is there a tool available that makes it easier? I'm worried that I've been using the wrong number of parentheses or some other stupid mistake like that. </p>

<p>Also, any suggestions on how long my train.txt file should be? I'm thinking of scoring a 1000 sentences. Is that number too small, too large?</p>

<p>All your help is appreciated :)</p>
","stanford-nlp, sentiment-analysis, training-data, scoring","<ol>
<li><p>dev.txt should be the same as train.txt just with a different set of sentences.  Note that the same sentence should not appear in dev.txt and train.txt.  The development set is used to evaluate the quality of the model you train on the training data.</p></li>
<li><p>We don't distribute a tool for tagging sentiment data.  This class could be helpful in building data: <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/sentiment/BuildBinarizedDataset.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/sentiment/BuildBinarizedDataset.html</a></p></li>
<li><p>Here are the sizes of the train, dev, and test sets used for the sentiment model: train=8544, dev=1101, test=2210</p></li>
</ol>
",1,1,670,2015-11-14 20:08:53,https://stackoverflow.com/questions/33712795/questions-about-creating-stanford-corenlp-training-models
A smaller Stanford NLP Models Jar file,"<p>I am currently utilizing this JAR file for the Stanford NLP models: stanford-corenlp-3.5.2-models.jar</p>

<p>This file is pretty big: its about 340 MB. </p>

<p>I am only using 4 models: <code>tokenize</code>, <code>ssplit</code>, <code>parse</code>, and <code>lemma</code>. Is there any way that I can use a smaller model JAR file (or is there a JAR file for each individual model) because I absolutely need the size of this file to be as small as possible </p>
",stanford-nlp,"<p>You should be fine if you just include the parser's model file in your classpath and the pos tagger's model file.  ""lemma"" requires ""pos"" , so you will need to include that in your list of annotators.</p>

<p>For instance: ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"" and ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" should be all you need.</p>

<p>You could just create that directory structure and include those files in your classpath, or make a jar with just those files in it.  You can definitely cut out most of that jar.</p>

<p>The bottom line is that if you're missing something, your code will crash with a missing resources error.  So you simply need to keep adding files until the code stops crashing.  You definitely don't need a lot of the files in that jar.</p>
",3,2,1363,2015-11-15 02:57:55,https://stackoverflow.com/questions/33715910/a-smaller-stanford-nlp-models-jar-file
How to remove non-valid unicode characters from strings in java,"<p>I am using the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml#Download"" rel=""noreferrer"">CoreNLP Neural Network Dependency Parser</a> to parse some social media content. Unfortunately, the file contains characters which are, according to <a href=""http://www.fileformat.info/"" rel=""noreferrer"">fileformat.info</a>, not valid unicode characters or unicode replacement characters. These are for example <a href=""http://www.fileformat.info/info/unicode/char/d83d/index.htm"" rel=""noreferrer"">U+D83D</a> or <a href=""http://www.fileformat.info/info/unicode/char/FFFD/index.htm"" rel=""noreferrer"">U+FFFD</a>. If those characters are in the file, coreNLP responds with errors messages like this one:</p>

<pre><code>Nov 15, 2015 5:15:38 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: ? (U+D83D, decimal: 55357)
</code></pre>

<p>Based on <a href=""https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java"">this</a> answer, I tried <code>document.replaceAll(""\\p{C}"", """");</code> to just remove those characters. <code>document</code> here is just the document as a string. But that didn't help.</p>

<p>How can I remove those characters out of the string before passing it to coreNLP?</p>

<p><strong>UPDATE (Nov 16th):</strong></p>

<p>For the sake of completeness I should mention that I asked this question only in order to avoid the huge amount of error messages by preprocessing the file. CoreNLP just ignores characters it can't handle, so that is not the problem.</p>
","java, regex, parsing, unicode, stanford-nlp","<p>In a way, both answers provided by <a href=""https://stackoverflow.com/users/5210128/mukesh-kumar"">Mukesh Kumar</a> and <a href=""https://stackoverflow.com/users/4892253/gsusrecovery"">GsusRecovery</a> are helping, but not fully correct. </p>

<pre><code>document.replaceAll(""[^\\u0009\\u000a\\u000d\\u0020-\\uD7FF\\uE000-\\uFFFD]"", """");
</code></pre>

<p>seems to replace all invalid characters. But CoreNLP seems to not support even more. I manually figured them out by running the parser on my whole corpus, which led to this:</p>

<pre><code>document.replaceAll(""[\\uD83D\\uFFFD\\uFE0F\\u203C\\u3010\\u3011\\u300A\\u166D\\u200C\\u202A\\u202C\\u2049\\u20E3\\u300B\\u300C\\u3030\\u065F\\u0099\\u0F3A\\u0F3B\\uF610\\uFFFC]"", """");
</code></pre>

<p>So right now I am running two <code>replaceAll()</code> commands before handing the document to the parser. The complete code snippet is</p>

<pre><code>// remove invalid unicode characters
String tmpDoc1 = document.replaceAll(""[^\\u0009\\u000a\\u000d\\u0020-\\uD7FF\\uE000-\\uFFFD]"", """");
// remove other unicode characters coreNLP can't handle
String tmpDoc2 = tmpDoc1.replaceAll(""[\\uD83D\\uFFFD\\uFE0F\\u203C\\u3010\\u3011\\u300A\\u166D\\u200C\\u202A\\u202C\\u2049\\u20E3\\u300B\\u300C\\u3030\\u065F\\u0099\\u0F3A\\u0F3B\\uF610\\uFFFC]"", """");
DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(tmpDoc2));
for (List&lt;HasWord&gt; sentence : tokenizer) {
    List&lt;TaggedWord&gt; tagged = tagger.tagSentence(sentence);
    GrammaticalStructure gs = parser.predict(tagged);
    System.err.println(gs);
}
</code></pre>

<p>This is not necessarily a complete list of unsupported characters, though, which is why I opened an <a href=""https://github.com/stanfordnlp/CoreNLP/issues/103"" rel=""nofollow noreferrer"">issue</a> on <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">GitHub</a>.</p>

<p>Please note that CoreNLP automatically removes those unsupported characters. The only reason I want to preprocess my corpus is to avoid all those error messages.</p>

<p><strong>UPDATE Nov 27ths</strong></p>

<p><a href=""https://stackoverflow.com/users/235019/christopher-manning"">Christopher Manning</a> just answered the <a href=""https://github.com/stanfordnlp/CoreNLP/issues/103"" rel=""nofollow noreferrer"">GitHub Issue</a> I opened. There are several ways to handle those characters using the class <code>edu.stanford.nlp.process.TokenizerFactory;</code>. Take this code example to tokenize a document:</p>

<pre><code>DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(document));
TokenizerFactory&lt;? extends HasWord&gt; factory=null;
factory=PTBTokenizer.factory();
factory.setOptions(""untokenizable=noneDelete"");
tokenizer.setTokenizerFactory(factory);

for (List&lt;HasWord&gt; sentence : tokenizer) {
    // do something with the sentence
}
</code></pre>

<p>You can replace <code>noneDelete</code>in line 4 with other options. I am citing Manning:</p>

<blockquote>
  <p>""(...) the complete set of six options combining whether to log a warning for none, the first, or all, and whether to delete them or to include them as single character tokens in the output: noneDelete, firstDelete, allDelete, noneKeep, firstKeep, allKeep.""</p>
</blockquote>

<p>That means, to keep the characters without getting all those error messages, the best way is to use the option <code>noneKeep</code>. This way is way more elegant than any attempt to remove those characters.</p>
",8,8,34800,2015-11-15 16:30:12,https://stackoverflow.com/questions/33722024/how-to-remove-non-valid-unicode-characters-from-strings-in-java
Stanford CoreNLP - relation annotator,"<p>I'm trying to use the Stanford CoreNLP relation extractor (<a href=""http://nlp.stanford.edu/software/relationExtractor.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/relationExtractor.shtml</a>). </p>

<p>I've installed the CoreNLP, like it suggested and I tried adding the relation parameter to the annotator pipeline but to no avail. Below is the code that I tried entering through my command line in Linux. </p>

<pre><code>java -cp ""*"" -Xxm1g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse,relation -file &lt;path toinput file&gt;
</code></pre>

<p>The error message is that when it's trying to add the annotator relation, it states: </p>

<pre><code>Unable to resolve ""edu/stanford/nlp/models/supervised_relation_extractor/roth_relation_model_pipelineNER.ser"" as either a class path, filename or URL. 
</code></pre>

<p>I tried searching the web to see if I could just download the roth_relation_model_pipelineNER.ser file and just stick it in the models directory but I can't find it (I also don't know Java very well). </p>

<p>Please let me know if you have any suggestions. Thanks. </p>

<p>In response to @StanfordNLPHelp 's (thank you for your quick reply BTW):</p>

<p>I am running the command with the models.jar file in the same directory and it is picking it up (screenshot below): </p>

<p><a href=""https://i.sstatic.net/MCZGb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MCZGb.png"" alt=""enter image description here""></a></p>

<p>In addition, I downloaded the newest version of the CoreNLP zip file from the website. Unzipped it and also unpacked the models.jar file (stanford-parser-3.5.2-models.jar). I went to the models directory but there was only the lexparser and parser directories there, I didn't see one for supervised_relation_extractor. Attached is a screenshot. 
<a href=""https://i.sstatic.net/RvGBa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RvGBa.png"" alt=""enter image description here""></a></p>

<p>Also, this is a side note but I had a similar problem with the pos annotator as well. I was running into the same issue where it couldn't find the file. But I was able to download a separate .jar file and place it where the program wanted it using this website (question #6 - <a href=""http://nlp.stanford.edu/software/pos-tagger-faq.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/pos-tagger-faq.shtml</a>) and that was able to work. </p>
","java, stanford-nlp","<p>That file is in this jar: stanford-corenlp-3.5.2-models.jar</p>

<p>When you run your command in the Terminal, that .jar file needs to be in the directory you are running the command in for the -cp ""*"" to pick it up.  If you see that error it means that you don't have stanford-corenlp-3.5.2-models.jar in your classpath.</p>
",2,0,417,2015-11-15 19:34:12,https://stackoverflow.com/questions/33724064/stanford-corenlp-relation-annotator
Stanford NNDep parser: java.lang.ArrayIndexOutOfBoundsException,"<p>After training a model, i’m trying to parse the test treebank. Unfortunately, this error keeps popping up:</p>

<pre><code>Loading depparse model file: nndep.model.txt.gz ...
###################
#Transitions: 77
#Labels: 38
ROOTLABEL: root
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 25
        at edu.stanford.nlp.parser.nndep.Classifier.preCompute(Classifier.java:663)
        at edu.stanford.nlp.parser.nndep.Classifier.preCompute(Classifier.java:637)
        at edu.stanford.nlp.parser.nndep.DependencyParser.initialize(DependencyParser.java:1151)
        at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:589)
        at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:493)
        at edu.stanford.nlp.parser.nndep.DependencyParser.main(DependencyParser.java:1245)
</code></pre>

<p>If the pre-trained english model, which ships with the NLP package, is used, that error does not appear. Therefore, there is maybe something wrong with the trained model? There were no errors during training, however. 500 iterations were done (default 20000 takes over 15 hours on my 2,33 GHz Core 2 Duo CPU @ 4 Gb RAM – is such an amount of time normal, by the way?)　Train, dev and test sets are <a href=""http://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1548"" rel=""nofollow"">UD 1.2</a>; word embeddings used are <a href=""http://github.com/wolet/sprml13-word-embeddings"" rel=""nofollow"">these</a>. Seems that this error happens when non-english treebank is used for training (tried swedish and polish UD; <code>-tlp</code> option is not set, using <code>UniversalEnglish</code>).</p>
","nlp, stanford-nlp","<p>Answering my own question, with a hint in a comment by <a href=""https://stackoverflow.com/questions/33726828/stanford-nndep-parser-java-lang-arrayindexoutofboundsexception#comment55226423_33726828"">@Jon Gauthier</a>. It turns out that the <code>-embeddingSize</code> flag is needed <em>also at parsing stage</em> if it was used during training (= other value then the default 50 was used). The <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow noreferrer"">documentation</a> never says that, and in fact only refers to the flag in regards to the training phase, but the error message in the question code actually cryptically hints about the origin of the error, displaying „25“̦ which was the dimensionality of the word embeddings used.</p>
",2,1,336,2015-11-16 00:29:35,https://stackoverflow.com/questions/33726828/stanford-nndep-parser-java-lang-arrayindexoutofboundsexception
extract NP-VP-NP from Stanford dependency parse tree,"<p>I need to extract triplets of the form <code>NP-VP-NP</code> from the dependency parse tree produced as the output of lexalized parsing in Stanford Parser.</p>

<p>Whats the best way to do this.
e.g. If the parse tree is as follows:</p>

<pre><code>(ROOT
  (S
    (S
      (NP (NNP Exercise))
      (VP (VBZ reduces)
        (NP (NN stress)))
      (. .))
    (NP (JJ Regular) (NN exercise))
    (VP (VBZ maintains)
      (NP (JJ mental) (NN fitness)))
    (. .)))
</code></pre>

<p>I need to extract 2 triplets: </p>

<ol>
<li>Exercise-reduces-stress and  </li>
<li>Regular Exercise-maintains-mental fitness</li>
</ol>

<p>Any ideas?</p>
","nlp, stanford-nlp, opennlp","<p>There are two natural options here. One is to run <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html"" rel=""nofollow"">Semgrex</a> over the dependency tree (side note: what you have in the question is a constituency tree), with a pattern like:</p>

<p><code>{pos:/V.*/}=verb &gt;/.subj.*/ {}=subject &gt;/.obj/ {}=object</code></p>

<p>Another option is to use the <a href=""http://nlp.stanford.edu/software/openie.shtml"" rel=""nofollow"">Stanford Open IE</a> system. This will give you a more broad semantics of '(subject; relation; object)' triples, where the relation does not have to be a verb.</p>
",4,3,1215,2015-11-16 10:56:10,https://stackoverflow.com/questions/33733669/extract-np-vp-np-from-stanford-dependency-parse-tree
how to speed up NE recognition with stanford NER with python nltk,"<p>First I tokenize the file content into sentences and then call Stanford NER on each of the sentences. But this process is really slow. I know if I call it on the whole file content if would be faster, but I'm calling it on each sentence as I want to index each sentence before and after NE recognition. </p>

<pre><code>st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
for filename in filelist:
    sentences = sent_tokenize(filecontent) #break file content into sentences
    for j,sent in enumerate(sentences): 
        words = word_tokenize(sent) #tokenize sentences into words
        ne_tags = st.tag(words) #get tagged NEs from Stanford NER
</code></pre>

<p>This is probably due to calling <code>st.tag()</code> for each sentence, but is there any way to make it run faster?</p>

<p><strong>EDIT</strong></p>

<p>The reason that I want to tag sentences separate is that I want to write sentences to a file (like sentence indexing) so that given the ne tagged sentence at a later stage, i can get the unprocessed sentence (i'm also doing lemmatizing here)</p>

<p>file format:</p>

<blockquote>
  <p>(sent_number, orig_sentence, NE_and_lemmatized_sentence)</p>
</blockquote>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>From <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L139"" rel=""noreferrer"">StanfordNERTagger</a>, there is the <code>tag_sents()</code> function, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L68"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L68</a></p>

<pre><code>&gt;&gt;&gt; st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
&gt;&gt;&gt; tokenized_sents = [[word_tokenize(sent) for sent in sent_tokenize(filecontent)] for filename in filelist]
&gt;&gt;&gt; st.tag_sents(tokenized_sents)
</code></pre>
",8,10,5955,2015-11-17 03:17:44,https://stackoverflow.com/questions/33748554/how-to-speed-up-ne-recognition-with-stanford-ner-with-python-nltk
Ignore text inside XML elements when parsing text with Stanford CoreNLP,"<p>I'd like to use Stanford CoreNLP to analyze the text content of XML files.</p>

<p>Here's an example of the kind of XML element I'm analyzing:</p>

<p><code>&lt;cmd&gt;In the new plug-in directory, add a &lt;filepath&gt;cfg/catalog.xml&lt;/filepath&gt; file that specifies the custom XSLT style sheets.&lt;/cmd&gt;
</code></p>

<p>One thing I'd like to check is whether a <code>&lt;cmd&gt;</code> element contains more than one sentence. Now, if I extract the text content of the <code>&lt;cmd&gt;</code> element above, the result is this:</p>

<blockquote>
  <p>In the new plug-in directory, add a cfg/catalog.xml file that specifies the custom XSLT style sheets.</p>
</blockquote>

<p>If I give that piece of text to Stanford CoreNLP, it thinks the text has two sentences because of the dot in <code>cfg/catalog.xml</code>, even though it's really just one sentence.</p>

<p>In this particular example, I could probably just omit the content of the <code>&lt;filepath&gt;</code> element when analyzing the text and it'd work well enough, but that's not necessarily always the case.</p>

<p>Any suggestions on how to best approach this problem on a general level? I guess I'm looking for a way to either ignore the content of <code>&lt;filepath&gt;</code> and similar elements for certain purposes or somehow force them to be recognized as named entities, if that makes any sense.</p>
","xml, nlp, stanford-nlp","<p>You could build an annotator that temporarily replaces the problematic tags/file-names, then restores them after sentence splitting.</p>

<p>If I get a chance I'll write up some example code.</p>
",1,0,154,2015-11-17 14:50:27,https://stackoverflow.com/questions/33759913/ignore-text-inside-xml-elements-when-parsing-text-with-stanford-corenlp
Stanford NNDep parser: features used,"<p>In regards to Stanford’s neural network dependency parser<a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">*̦</a> which features are used during training and testing phases? In practice, which columns in a CONLL<a href=""http://universaldependencies.github.io/docs/format"" rel=""nofollow"">ᶸ</a><a href=""http://ilk.uvt.nl/conll/#dataformat"" rel=""nofollow"">ˣ</a> formatted data set could be substituted with <strong>_</strong> without the parser loosing any accuracy when training? Which columns are never read?</p>

<p>Certainly <code>ID</code>, <code>FORM</code> and <code>HEAD</code> (columns # <strong>1</strong>, <strong>2</strong> &amp; <strong>7</strong>) are a must, as most likely are <code>U/C-POSTAG</code> (# <strong>4</strong>) and <code>DEPREL</code> (# <strong>8</strong>). But how about the columns <code>LEMMA</code>, <code>(X)-POSTAG</code> and <code>FEATS</code> (# <strong>3</strong>, <strong>5</strong> &amp; <strong>6</strong>)? Do they help while training, or whether the treebank contains any information in these is irrelevant for the parser?</p>
","nlp, stanford-nlp","<p>In the current implementation, we only use the following fields. My column indexing begins from 1.</p>

<ul>
<li><code>FORM</code> (column 2)</li>
<li><code>UPOSTAG</code> (column 4) [^1]</li>
<li><code>HEAD</code> (column 7)</li>
<li><code>DEPREL</code> (column 8)</li>
</ul>

<p>[^1]: If parsing with coarse part-of-speech tags (<code>-cPOS</code>), we read column 5 instead.</p>

<p>Everything else can be null, so long as you don't break the CoNLL format (i.e., still include a <code>_</code> in the null column).</p>

<p>See exactly which columns we read here: <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/parser/nndep/Util.java#L165"" rel=""nofollow""><code>edu.stanford.nlp.parser.nndep.Util.loadConllFile</code></a>. Note these are the same for both CoNLL-X and CoNLL-U representations.</p>
",3,3,110,2015-11-18 18:46:53,https://stackoverflow.com/questions/33787955/stanford-nndep-parser-features-used
Error running the open information extraction given by Stanford,"<p>I am trying to run the openIE given by stanford nlp using the command given in the official website: <a href=""http://nlp.stanford.edu/software/openie.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/openie.shtml</a></p>

<pre><code>java -mx1g -cp stanford-openie.jar:stanford-openie-models.jar edu.stanford.nlp.naturalli.OpenIE  mytextfile.txt
</code></pre>

<p>but I am getting the following error:</p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;clinit&gt;(StanfordCoreNLP.java:99)
at edu.stanford.nlp.naturalli.OpenIE.main(OpenIE.java:679)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 2 more
</code></pre>

<p>Again when I run the java code given:</p>

<pre><code>package edu.stanford.nlp.naturalli;

import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.List;
import java.util.Properties;

public class OpenIEDemo {

public static void main(String[] args) throws Exception {
// Create the Stanford CoreNLP pipeline
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize,ssplit,pos,depparse,natlog,openie"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// Annotate an example document.
Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
pipeline.annotate(doc);

// Loop over sentences in the document
for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {

  // Get the OpenIE triples for the sentence
  Collection&lt;RelationTriple&gt; triples = sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);

  // Print the triples
  for (RelationTriple triple : triples) {
    System.out.println(triple.confidence + ""\t"" +
        triple.subjectLemmaGloss() + ""\t"" +
        triple.relationLemmaGloss() + ""\t"" +
        triple.objectLemmaGloss());
  }

  // Alternately, to only run e.g., the clause splitter:
  List&lt;SentenceFragment&gt; clauses = new OpenIE(props).clausesInSentence(sentence);
  for (SentenceFragment clause : clauses) {
    System.out.println(clause.parseTree);
  }
}
}
}
</code></pre>

<p>I get the next error:</p>

<pre><code>Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0,7 sec].
Adding annotator depparse
Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
PreComputed 100000, Elapsed Time: 1.159 (s)
Initializing dependency parser done [3,5 sec].
Adding annotator natlog
Exception in thread ""main"" java.lang.IllegalArgumentException: annotator ""natlog"" requires annotator ""parse""
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:297)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:126)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:122)
at stnfrd.OpenIEDemo.main(OpenIEDemo.java:33)
/home/ue/.cache/netbeans/8.1/executor-snippets/run.xml:53: Java returned: 1
BUILD FAILED (total time: 4 seconds)
</code></pre>

<p>Any help will be appreciated.</p>
",stanford-nlp,"<ol>
<li><p>The first error is you don't have the slf4j jar, which is currently included in the latest version on GitHub: <a href=""https://github.com/stanfordnlp/CoreNLP"">https://github.com/stanfordnlp/CoreNLP</a> or you can find that specific jar here: <a href=""http://www.slf4j.org/download.html"">http://www.slf4j.org/download.html</a></p></li>
<li><p>The second error is caused by ""natlog"" needing ""parse"".  Change ""depparse"" to ""parse"":</p>

<pre><code>props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse,natlog,openie"");
</code></pre></li>
</ol>
",7,6,2449,2015-11-21 16:26:34,https://stackoverflow.com/questions/33845753/error-running-the-open-information-extraction-given-by-stanford
"Stanford CoreNLP: Can I retrieve parent Annotation (i.e., document) from a contained CoreMap (i.e. sentence)?","<p>I'm just getting started with CoreNLP. From all the code samples I've seen (particularly the one on CoreNLP's main website: <a href=""http://nlp.stanford.edu/software/corenlp.shtml#Usage"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml#Usage</a>), I've gathered that Annotation objects hold the annotated document, and CoreMap objects hold the sentences (if ""ssplit"" annotation is enabled).</p>

<p>To keep my code lightweight, I'm only passing CoreMap to one of my functions. However, in one instance I need to retrieve the parent Annotation document object. Is there any backpointer using the CoreMap object, or will I have to pass in the Annotation object to my function as well?</p>
",stanford-nlp,"<p>The overall document is an Annotation.  The Annotation contains a List which contains the sentences.  Each sentence is a CoreMap.  I don't know of any way to get the parent Annotation from a CoreMap, so I would just pass the Annotation object to your function.</p>
",1,0,95,2015-11-22 17:56:13,https://stackoverflow.com/questions/33858119/stanford-corenlp-can-i-retrieve-parent-annotation-i-e-document-from-a-conta
Stanford NER tool -- spaces in training file,"<p>I've been looking through the Stanford NER classifier. I have been able to train a model using a simple file that has spaces only to delimit the items the system expects. For instance,</p>

<p>/a/b/c sanferro 2</p>

<p>/d/e/f ginger 2</p>

<p>However, I run into errors while trying forms such as:</p>

<p>/a/b/c san ferro 2</p>

<p>Here ""san ferro"" is a single ""word"" and ""2"" is the ""answer"" or desired labeling output.
How can I encode spaces? I've tried enclosing a double quotes but that doesn't work.</p>
",stanford-nlp,"<p>Typically you use CoNLL style data to train a CRF.  Here is an example:</p>

<pre><code>-DOCSTART-    O 

John    PERSON
Smith   PERSON
went    O
to      O
France  LOCATION
.       O

Jane    PERSON
Smith   PERSON
went    O
to      O
Hawaii  LOCATION
.       O
</code></pre>

<p>A ""\t"" character separates the tokens and the tags.  You put a blank space in between the sentences.  You use the special symbol ""-DOCSTART-"" to indicate where a new document starts.  Typically you provide a large set of sentences.  This is the case when you are training a CRF.</p>

<p>If you just want to tag certain patterns the same way all the time, you may want to use RegexNER, which is described here: <a href=""http://nlp.stanford.edu/software/regexner/"" rel=""nofollow"">http://nlp.stanford.edu/software/regexner/</a></p>

<p>Here is more documentation on using the NER system: <a href=""http://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/crf-faq.shtml</a></p>
",0,0,324,2015-11-23 00:29:14,https://stackoverflow.com/questions/33862021/stanford-ner-tool-spaces-in-training-file
Training model ignored by stanford CoreNLP,"<p>I've made a small, sample training model to use when performing sentiment analysis with coreNLP. In order to get coreNLP to use this model, I've written the following lines of code: </p>

<pre><code>props = new Properties(); 
props.put(""sample_model-0023-100.00.ser.gz"", ""/home/usr/Documents/coreNLP/"");
props.put(""annotators"", ""tokenize, ssplit, parse, lemma, sentiment""); 
pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>However, it doesn't look like the code is using the model I'm pointing to. I know this because I passed a couple of sentences to it that should get certain scores if the code were to use this model, but I'm getting different scores. Am I missing something in my lines of code that's preventing coreNLP from using the model I've created?</p>
","stanford-nlp, sentiment-analysis, training-data, scoring","<p>You want this:</p>

<pre><code>props.put(""sentiment.model"", ""/path/to/sample_model-0023-100.00.ser.gz"");
</code></pre>
",2,0,88,2015-11-24 04:10:54,https://stackoverflow.com/questions/33885309/training-model-ignored-by-stanford-corenlp
Why does Stanford CoreNLP NER-annotator load 3 models by default?,"<p>When I add the ""ner"" annotator to my StanfordCoreNLP object pipeline, I can see that it loads 3 models, which takes a lot of time:</p>

<pre><code>Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [10.3 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [10.1 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [6.5 sec].
Initializing JollyDayHoliday for SUTime from classpath: edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
</code></pre>

<p>Is there a way to just load a subset that will work equally? Particularly, I am unsure why it is loading the 3-class and 4-class NER models when it has the 7-class model, and I'm wondering if not loading these two will still work. </p>
",stanford-nlp,"<p>You can set which models are loaded in this manner:</p>

<p>command line:</p>

<pre><code>-ner.model model_path1,model_path2
</code></pre>

<p>Java code:</p>

<pre><code> props.put(""ner.model"", ""model_path1,model_path2"");
</code></pre>

<p>Where model_path1 and model_path2 should be something like: </p>

<pre><code>""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz""
</code></pre>

<p>The models are applied in layers.  The first model is run and its tags applied.  Then the second, the third, and so on.  If you want less models, you can put 1 or 2 models in the list instead of the three default, but this will change how the system performs.</p>

<p>If you set ""ner.combinationMode"" to ""HIGH_RECALL"", all models will be allowed to apply all of their tags.  If you set ""ner.combinationMode"" to ""NORMAL"", then a future model cannot apply any tags set by previous models.</p>

<p>All three models in the default were trained on different data.  For instance, the 3-class was trained with substantially more data than the 7-class model.  So each model is doing something different and their results are all being combined to create the final tag sequence.</p>
",3,4,1173,2015-11-24 22:52:28,https://stackoverflow.com/questions/33905412/why-does-stanford-corenlp-ner-annotator-load-3-models-by-default
CoreNLP Training Model Issue,"<p>I'm using Stanford CoreNLP to perform sentiment analysis on some Tweets I'm gathering. I have created a mock training model with one sentence which is scored as follows:
(0 (2 bear)(2 (2 oil)(2 market))).</p>

<p>I'm scoring on a scale of 0 to 4, with 0 being very negative, 2 being neutral and 4 is very positive. 
I am testing on the following two tweets:</p>

<p>bear oil market</p>

<p>bear oil markets</p>

<p>It is assigning the first sentence a 0, which is correct and the second sentence is scored as 2, which is incorrect since this sentence should also be negative. The only difference between the two sentences is the s in markets in the second sentence.</p>

<p>My question is this: is there any way to get around the fact that ANY variation of ANY word is causing the two sentences to be scored differently?</p>
","stanford-nlp, sentiment-analysis, scoring","<p>I think the short answer is ""no"" -- a difference in wording always has a chance of changing the sentiment of a sentence. You can try mitigating the problem by re-training on new data.</p>

<p>Really, if you're running on anything but movie reviews, you should expect the model to degrade in performance at least a little, and occasionally a lot. If you have the training data, it's worth re-training.</p>
",2,0,111,2015-11-25 16:25:15,https://stackoverflow.com/questions/33921575/corenlp-training-model-issue
Stanford NLP: &quot;No annotator named sentiment&quot; error,"<p>I'm trying to apply Sentiment Analysis for a dataset containing tweets that I already collected, for some experiments I run for my thesis. I have followed various tutorials on the Internet and my code is the following so far:</p>

<pre><code>public class SentimentAnalyzer {
    public static StanfordCoreNLP pipeline;

    public SentimentAnalyzer() {
         Properties props = new Properties();
         props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
         pipeline = new StanfordCoreNLP(props);
    }

    public static int getSentiment(String tweet) {
         int mainSentiment = 0;
         if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
             int longest = 0;
             Annotation annotation = pipeline.process(tweet);
             for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
                 Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
                 int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                 String partText = sentence.toString();
                 if (partText.length() &gt; longest) {
                     mainSentiment = sentiment;
                     longest = partText.length();
                 }

             }
        }
        return mainSentiment;
    }
}
</code></pre>

<p>The problem is that when I run this piece of code, Java returns an Illegal Argument Exception:</p>

<pre><code>Adding annotator tokenize
Adding annotator ssplit
Adding annotator parse
Loading parser from serialized file 
edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1,4 sec].
Adding annotator sentiment
Exception in thread ""main"" java.lang.IllegalArgumentException: No annotator named sentiment
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:78)
    at edu.stanfoJava Result: 1
</code></pre>

<p>but every single tutorial lists ""sentiment"" as a valid annotator! The program specifically hangs when calling the StanfordCoreNLP constructor (inside the SentimentAnalyzer constructor) and though I have tried every configuration that I can think of (making the pipeline not-static, creating the pipeline eveytime in getSentiment() method, using a .properties file instead), the problem stays.
I'm using StanfordCoreNLP 3.3.1, along with the models .jar (it's the 3.5.2 version, though) and ejml 0.23 -I have used 3.5.2 version, but with no success.</p>

<p>EDIT:
I replaced 3.3.2 with 3.5.2 and now the error changed:</p>

<pre><code>Loading parser from text file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz Exception in thread ""main"" java.lang.RuntimeException: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz: expecting BEGIN block; got ��
</code></pre>

<p>The rest is unreadable and cannot copy that either. It seems that it's a problem with the models library, at least the directory that the error points, belongs to models. I also updated with the latest library from their GitHub repository, but the error is the same.</p>
","java, stanford-nlp, sentiment-analysis","<p>Turns out that StanfordCoreNLP v. 1.3.4 was left out from a previous implementation since August. When I removed it, the code started working fine.</p>
",0,0,406,2015-12-03 11:22:20,https://stackoverflow.com/questions/34064705/stanford-nlp-no-annotator-named-sentiment-error
Running NLP on Heroku,"<p>If you use Heroku, you know Heroku imposes slugsize limit of 300MB. Our company is trying to run NLP app, which, using Stanford NLP library, is more than 300MB.</p>

<p>Is there anyone who has successfully hosted Stanford NLP process on Heroku? How did you achieve it? If you had to lose the weight by excluding parts of NLP library, how did you do it?</p>
","heroku, nlp, stanford-nlp","<p>The size problem your having is caused by the model files in stanford-corenlp-3.5.2-models.jar. You do not need to keep all the files in stanford-corenlp-3.5.2-models.jar to use the toolkit.  I would suggest assessing what functionality you want to maintain for your server and then remove the models that are not required for that functionality.</p>
",3,1,713,2015-12-03 16:46:36,https://stackoverflow.com/questions/34071597/running-nlp-on-heroku
Stanford Parser: How to include the punctuations?,"<p>I have used Stanford Parser to parse some of my <strong>already tokenized and POS tagged</strong> (by Stanford POS tagger with Gate Twitter model). But the resulting <strong>conll 2007</strong> formatted output does not include any punctuations. Why is that?</p>

<p>The command I have used:</p>

<pre><code>java -mx16g -cp ""*"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline -tokenized -tagSeparator § -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory -escaper edu.stanford.nlp.process.PTBEscapingProcessor -outputFormat conll2007 edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz ..test.tagged &gt; ../test.conll
</code></pre>

<p>e.g. </p>

<p>Original tweet:</p>

<pre><code>bbc sp says they don't understand why the tories aren't 8% ahead in the polls given the current economics stats ; bbc bias ? surely not ?
</code></pre>

<p>POS tagged tweet, used as input for Stanford parser:</p>

<pre><code>bbc§NN sp§NN says§VBZ they§PRP don't§VBP understand§VB why§WRB the§DT tories§NNS aren't§VBZ 8%§CD ahead§RB in§IN the§DT polls§NNS given§VBN the§DT current§JJ economics§NNS stats§NNS ;§: bbc§NN bias§NN ?§. surely§RB not§RB ?§.
</code></pre>

<p>Resulting conll 2007 formatted parse:</p>

<pre><code>1   bbc _   NN  NN  _   2   compound    _   _
2   sp  _   NN  NN  _   3   nsubj   _   _
3   says    _   VBZ VBZ _   0   root    _   _
4   they    _   PRP PRP _   5   nsubj   _   _
5   don't   _   VBP VBP _   3   ccomp   _   _
6   understand  _   VB  VB  _   5   xcomp   _   _
7   why _   WRB WRB _   10  advmod  _   _
8   the _   DT  DT  _   9   det _   _
9   tories  _   NNS NNS _   10  nsubj   _   _
10  aren't  _   VBZ VBZ _   6   ccomp   _   _
11  8%  _   CD  CD  _   12  nmod:npmod  _   _
12  ahead   _   RB  RB  _   15  advmod  _   _
13  in  _   IN  IN  _   15  case    _   _
14  the _   DT  DT  _   15  det _   _
15  polls   _   NNS NNS _   10  nmod    _   _
16  given   _   VBN VBN _   15  acl _   _
17  the _   DT  DT  _   19  det _   _
18  current _   JJ  JJ  _   19  amod    _   _
19  economics   _   NNS NNS _   16  dobj    _   _
20  stats   _   NNS NNS _   19  dep _   _
22  bbc _   NN  NN  _   23  compound    _   _
23  bias    _   NN  NN  _   20  dep _   _
25  surely  _   RB  RB  _   26  advmod  _   _
26  not _   RB  RB  _   16  neg _   _
</code></pre>

<p>As you can see, Most of the punctuations are not included in the parse. But why?</p>
","parsing, nlp, stanford-nlp","<p>Finally, found the answer, use</p>

<pre><code>-outputFormatOptions includePunctuationDependencies
</code></pre>

<p>Have contacted Stanford parser and corenlp support long time ago, no response at all</p>
",1,2,236,2015-12-08 00:11:48,https://stackoverflow.com/questions/34145785/stanford-parser-how-to-include-the-punctuations
Why are the dependencies different from the online demo?,"<p>Give the following sentence:</p>

<blockquote>
  <p>My dog also likes eating sausage.</p>
</blockquote>

<p>The <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">online demo</a> produces the following relations:</p>

<pre><code>nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
</code></pre>

<p>However, my code, which also uses the universal dependencies (UD), produces something different:</p>

<pre><code>nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
amod(sausage-6, eating-5)
dobj(likes-4, sausage-6)
punct(likes-4, .-7)
</code></pre>

<p>Here's my code:</p>

<pre><code>String sentence = ""My dog also likes eating sausage."";
MaxentTagger tagger = new MaxentTagger(""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"");
DependencyParser parser = DependencyParser.loadFromModelFile(""edu/stanford/nlp/models/parser/nndep/english_UD.gz"");

DocumentPreprocessor preprocessor = new DocumentPreprocessor(new StringReader(sentence));
for (List&lt;HasWord&gt; s: preprocessor) {
    List&lt;TaggedWord&gt; taggedWords = tagger.tagSentence(s);
    GrammaticalStructure gs = parser.predict(taggedWords);
    for (TypedDependency d: gs.typedDependencies()) {
        System.out.println(d);
    }
}
</code></pre>

<p>Using <code>typedDependenciesCCprocessed</code>, <code>typedDependenciesCollapsed</code>, and<code>typedDependenciesCollapsedTree</code> will yield the same result. </p>

<p>How do I get the exact same relations as the demo's?</p>
",stanford-nlp,"<p>The online demo is using the constituency parser and converting it to dependencies. The code you linked is using the neural net dependency parser. These are expected to be somewhat different. To get the output of the demo, you should run the Stanford Parser: <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/lex-parser.shtml</a>. From CoreNLP, this means running with annotators <code>tokenize,ssplit,parse</code>.</p>
",1,1,87,2015-12-08 03:34:28,https://stackoverflow.com/questions/34147523/why-are-the-dependencies-different-from-the-online-demo
"Multiple Stanford CoreNLP model files made, which one is the correct one to use?","<p>I made a sentiment analysis model using Standford CoreNLP's library. So I have a bunch of ser.gz files that look like the following: 
<a href=""https://i.sstatic.net/omumK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/omumK.jpg"" alt=""enter image description here""></a></p>

<p>I was wondering what model to use in my java code, but based on a previous <a href=""https://stackoverflow.com/questions/33725007/how-to-get-stanford-corenlp-to-use-a-training-model-you-created"">question</a>,  </p>

<p>I just used the model with the highest F1 score, which in this case is model-0014-93.73.ser.gz. And in my java code, I pointed to the model I want to use by using the following line: </p>

<pre><code>    props.put(""sentiment.model"", ""/path/to/model-0014-93.73.ser.gz."");
</code></pre>

<p>However, by referring to just that model, am I excluding the sentiment analysis from the other models that were made? Should I be referring to all the model files to make sure I ""covered"" all the bases or does the highest scoring model trump everything else?</p>
","stanford-nlp, sentiment-analysis","<p>You should point to only the single highest scoring model. The code has no way to make use of multiple models at the same time.</p>
",1,0,163,2015-12-08 16:35:13,https://stackoverflow.com/questions/34161172/multiple-stanford-corenlp-model-files-made-which-one-is-the-correct-one-to-use
"If I don&#39;t specify a sentiment model in CoreNLP, what will it use to score the data?","<p>I've been creating sentiment analysis models to use with Stanford CoreNLP, and I've been using the one with the highest F1 score in my java code, like so:</p>

<pre><code>props.put(""sentiment.model"", ""/path/to/model-0014-93.73.ser.gz."");
</code></pre>

<p>But if I remove this line, what does CoreNLP use to score the data? Is there a default coreNLP model that's used if the user does not specify a model?</p>
","stanford-nlp, sentiment-analysis","<p>If no model is given, it'll use the default model included in the release trained on the Stanford Sentiment Treebank: <a href=""http://nlp.stanford.edu/sentiment/treebank.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/treebank.html</a></p>
",1,0,75,2015-12-08 19:45:20,https://stackoverflow.com/questions/34164668/if-i-dont-specify-a-sentiment-model-in-corenlp-what-will-it-use-to-score-the-d
Input Penn Treebank constituent trees in a Stanford CoreNLP pipeline,"<p>I am using the <a href=""http://nlp.stanford.edu/software/openie.shtml"" rel=""nofollow"">OpenIE</a> tool from the Stanford NLP libraries to get minimal clauses from a sentence. Here is what I have come up with so far (largely inspired from their demo code):</p>

<pre><code>public static void main(String[] args) {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
    pipeline.annotate(doc);

    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {

        OpenIE split = new OpenIE(props);
        List&lt;SentenceFragment&gt; clauses = split.clausesInSentence(sentence);
        for (SentenceFragment clause : clauses) {
            List&lt;SentenceFragment&gt; short_clauses = split.entailmentsFromClause(clause);
            for (SentenceFragment short_clause : short_clauses){
                System.out.println(short_clause.parseTree);
            }
        }
    }
}
</code></pre>

<p>I now want to use PTB constituent trees as input instead of plain text and then only use the <em>depparse</em>, <em>natlog</em> and <em>openie</em> annotators to get the clauses.</p>

<p>I know that I can use PTB trees as input to the Stanford parser (as explained <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#s"" rel=""nofollow"">here</a>) but have not figured out how to integrate that in the pipeline.</p>
","java, nlp, stanford-nlp","<p>This is I think actually nontrivial. If someone has a clean way to do this in the pipeline, chime in! But, if I were to do it I'd probably just call the component bits of code manually. This means:</p>

<ul>
<li><p>Create a <code>SemanticGraph</code> object from the <code>GrammaticalStructure</code> from the constituency tree.</p></li>
<li><p>Add a lemma annotation to each <code>IndexedWord</code> in the semantic graph. This can be done by calling <code>Morphology#lemma(word, posTag)</code> on each token, and setting the <code>LemmaAnnotation</code> to this.</p></li>
<li><p>Running through the natural logic annotator is going to be tricky. One option is to mock an Annotation object and push it through the usual <code>annotate()</code> method. But, if you don't care too much about the OpenIE system recognizing negation, you can skip this annotator by adding the value <code>Polarity#DEFAULT</code> to each token in the <code>SemanticGraph</code> on the <code>PolarityAnnotation</code> key.</p></li>
<li><p>Now your dependency tree should be ready to pass through the OpenIE annotator. You want to make three calls here:</p>

<ul>
<li><code>OpenIE#clausesInSentence(SemanticGraph)</code> will generate a collection of clauses from a given graph.</li>
<li><code>OpenIE#entailmentsFromClause(SentenceFragment)</code> will generate short entailments from each clause. You want to pass each of the outputs from the above function into this, and collect all the resulting fragments.</li>
<li><code>OpenIE#relationsInFragment(SentenceFragment)</code> will segment a short entailment into a relation triple. It returns an <code>Optional</code> -- most fragments don't segment into any triple. You want to pass each of the short entailments collected from the above call into this function, and collect the relation triples that are defined in the output of this function. These are your OpenIE triples.</li>
</ul></li>
</ul>

<p>Out of curiosity, what are you trying to do in the end? Perhaps there's an easier way to accomplish the same goal.</p>
",2,3,535,2015-12-08 22:51:10,https://stackoverflow.com/questions/34167592/input-penn-treebank-constituent-trees-in-a-stanford-corenlp-pipeline
Stanford NLP sentiment running error,"<p>I downloaded Stanford NLP 3.5.2 at <a href=""http://nlp.stanford.edu/software/stanford-corenlp-full-2015-04-20.zip"" rel=""nofollow"">http://nlp.stanford.edu/software/stanford-corenlp-full-2015-04-20.zip</a></p>

<p>Then I unzip the file and move to the new unzip directory.</p>

<p>I ran as instructed at:</p>

<p><a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/code.html</a></p>

<p><code>java edu.stanford.nlp.sentiment.Evaluate edu/stanford/nlp/models/sentiment/sentiment.ser.gz test.txt</code></p>

<p>Then I have an error:</p>

<p><code>""Error: Could not find or load main class edu.stanford.nlp.sentiment.Evaluate""</code></p>

<p>Followed with some instructions in stackoverflow, I used:</p>

<p><code>java -cp ""*"" edu.stanford.nlp.sentiment.Evaluate edu/stanford/nlp/models/sentiment/sentiment.ser.gz test.txt</code></p>

<p>Then I have another problem:</p>

<pre><code>Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:461)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:313)
    at edu.stanford.nlp.sentiment.SentimentModel.loadSerialized(SentimentModel.java:627)
    at edu.stanford.nlp.sentiment.Evaluate.main(Evaluate.java:72)
</code></pre>

<p>How should I fix the problem? (I am using Java 8 1.8.0_25 on Mac)</p>
","java, stanford-nlp","<p>The <a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">documentation</a> is not up to date. The <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/sentiment/Evaluate.html"" rel=""nofollow"">javadoc</a> says that you have to add <code>-model</code> and <code>-treebank</code> before the corresponding files:</p>

<pre><code>java -cp ""*"" edu.stanford.nlp.sentiment.Evaluate -model edu/stanford/nlp/models/sentiment/sentiment.ser.gz -treebank test.txt
</code></pre>
",1,2,223,2015-12-09 12:13:26,https://stackoverflow.com/questions/34178552/stanford-nlp-sentiment-running-error
How to get JJ and NN (adjective and Noun) from the triples generated StanfordDependencyParser with NLTK?,"<p>i got triples using the following code, but i want to get nouns and adjective from tripples, i tried alot but failed, new to NLTK and python, any help ?</p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser
dp_prsr= StanfordDependencyParser('C:\Python34\stanford-parser-full-2015-04-20\stanford-parser.jar','C:\Python34\stanford-parser-full-2015-04-20\stanford-parser-3.5.2-models.jar', model_path='edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')
word=[]
s='bit is good university'
sentence = dp_prsr.raw_parse(s)
for line in sentence:
    print(list(line.triples()))
</code></pre>

<p>[(('university', 'NN'), 'nsubj', ('bit', 'NN')), (('university', 'NN'), 'cop', ('is', 'VBZ')), (('university', 'NN'), 'amod', ('good', 'JJ'))]</p>

<p>i want to get university and good and BIT and universityi tried the following but couldnt work.</p>

<pre><code>   for line in sentence:
    if (list(line.triples)).__contains__()  == 'JJ':
       word.append(list(line.triples()))
   print(word)
</code></pre>

<p>but i get empty array... please any help.</p>
","parsing, python-3.x, nltk, stanford-nlp, triples","<h2>Linguistically</h2>

<p>What you're looking out for when you look for triplets that contains a <code>JJ</code> and an <code>NN</code> is usually a Noun phrase <code>NP</code> in a context-free grammar.</p>

<p>In dependency grammar, what you're looking for is a triplet that contains the the JJ and NN POS tags in the <strong>arguments</strong>. Most specifically, when you're for a constituent / branch that contains an adjectival modified Noun. From the <code>StanfordDepdencyParser</code> output, you need to look for the <strong>predicate</strong> <code>amod</code>. (If you're confused with what's explained above it is advisable to read up on Dependency grammar before proceeding, see <a href=""https://en.wikipedia.org/wiki/Dependency_grammar"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Dependency_grammar</a>.</p>

<p>Note that the parser outputs the triplets, <code>(arg1, pred, arg2)</code>, where the argument 2 (<code>arg2</code>) depends on argument 1 (<code>arg1</code>) through the predicate (<code>pred</code>) relation; i.e. <code>arg1</code> governs <code>arg2</code> (see, <a href=""https://en.wikipedia.org/wiki/Government_(linguistics)"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Government_(linguistics)</a>)</p>

<hr>

<h2>Pythonically</h2>

<p>Now to the code part of the answer. You want to iterate through a list of tuples (i.e. triplets) so the easiest solution is to specifically assign variables to the tuples as you iterate, then check for the conditions you need see <a href=""https://stackoverflow.com/questions/2191699/find-an-element-in-a-list-of-tuples"">Find an element in a list of tuples</a></p>

<pre><code>&gt;&gt;&gt; x = [(('university', 'NN'), 'nsubj', ('bit', 'NN')), (('university', 'NN'), 'cop', ('is', 'VBZ')), (('university', 'NN'), 'amod', ('good', 'JJ'))]
&gt;&gt;&gt; for arg1, pred, arg2 in x:
...     word1, pos1 = arg1
...     word2, pos2 = arg2
...     if pos1.startswith('NN') and pos2.startswith('JJ') and pred == 'amod':
...             print ((arg1, pred, arg2))
... 
(('university', 'NN'), 'amod', ('good', 'JJ'))
</code></pre>
",2,1,1772,2015-12-10 09:37:09,https://stackoverflow.com/questions/34198237/how-to-get-jj-and-nn-adjective-and-noun-from-the-triples-generated-stanforddep
Stanford NER Tagger in NLTK,"<p>I am trying to import the Stanford Named Entity Recognizer in Python. This is already built in the NLTK package. However, my code below is not working:</p>

<pre><code> from nltk.tag.stanford import NERTagger
 Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
 ImportError: cannot import name NERTagger
</code></pre>

<p>What could be the cause? In all articles I read it works by default. Thank you.</p>
","python, nltk, stanford-nlp","<p>That class has been <a href=""https://github.com/nltk/nltk/blob/develop/ChangeLog#L54"">renamed to <code>StanfordNERTagger</code> in version <code>3.0.3</code></a> (commit <a href=""https://github.com/nltk/nltk/commit/190673c7a6c48620d37f5cd58e5e91851f6edbaf""><code>190673c7</code></a>).</p>

<p>So for <code>nltk &gt;= 3.0.3</code> you need to use this import instead:</p>

<pre><code>from nltk.tag import StanfordNERTagger
</code></pre>

<p><em>(You could also do <code>from nltk.tag.stanford import StanfordNERTagger</code>, but since they now also provide a convenience import in the <code>nltk.tag</code> module, that's probably what they want use to use, that import location should be less prone to future changes like this.)</em></p>
",13,1,5395,2015-12-10 22:23:30,https://stackoverflow.com/questions/34212833/stanford-ner-tagger-in-nltk
Stanford NER crash when called from Python NLTK,"<p>I am trying to tag named entities using the Stanford NER. My code is very simple:</p>

<pre><code> from nltk.tag import StanfordNERTagger
 st = StanfordNERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 
                        'stanford-ner/stanford-ner.jar', 
                         encoding='utf-8')
 st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
</code></pre>

<p>However, I cannot get it to work. What I get back is a Java exception saying that:</p>

<pre><code> Exception in thread ""main"" java.lang.UnsupportedClassVersionError: edu/stanford/nlp/ie/crf/CRFClassifier : Unsupported major.minor version 52.0
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)

raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : [u'/usr/bin/java', '-mx1000m', '-cp', 'stanford-ner/stanford-ner.jar', 
'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 
'stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', '-textFile', '/tmp/tmpq3u0oi', '-outputFormat', 'slashTags', '-tokenizerFactory', 
'edu.stanford.nlp.process.WhitespaceTokenizer', 
'-tokenizerOptions', '""tokenizeNLs=false""']
</code></pre>

<p>Any idea of what could cause this crash? Thank you in advance.</p>
","python, stanford-nlp","<p>You're using an older version of java. What is the output of <code>/usr/bin/java -version</code>? It should be at least Java 8.</p>
",5,3,642,2015-12-10 23:38:22,https://stackoverflow.com/questions/34213777/stanford-ner-crash-when-called-from-python-nltk
How fast is Stanford&#39;s CoreNLP sentiment analysis tool?,"<ol>
<li>I'm trying to find out whether it's feasible for me to use the the CoreNLP sentiment analysis tool (<a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/code.html</a>) on a dataset equivalent in size to about 1 million IMDB reviews.</li>
</ol>

<p>I could not find any absolute metrics anywhere online about average times. I would appreciate if someone could point me to any place about these statistics regarding the speed.</p>

<ol start=""2"">
<li><p>Also, this is what I'm trying - to see if it's possible to estimate a movie rating by using the text alone i.e. by summing up scores for each sentence in a review. Does anything in my idea or in the code snippet below look stupid (should be done better)? I get the feeling that I might be using this tool for something that it's not suited for or I'm doing it the wrong way.</p>

<pre><code>public static double getTextSentimentScore(String text){
Annotation annotation = pipeline.process(text);
double sum = 0;
List&lt;CoreMap&gt; sentences = (List&lt;CoreMap&gt;) annotation.get(CoreAnnotations.SentencesAnnotation.class);
int i = 0;
for (CoreMap sentence : sentences) {
    String sentiment = sentence.get(SentimentCoreAnnotations.SentimentClass.class);
    int sentimentScore = 0;
    if (sentiment.equals(""Very positive""))
        sentimentScore = 5;
    if (sentiment.equals(""Positive""))
        sentimentScore = 4;
    if (sentiment.equals(""Neutral""))
        sentimentScore = 3;
    if (sentiment.equals(""Negative""))
        sentimentScore = 2;
    if (sentiment.equals(""Very negative""))
        sentimentScore = 1;
    sum += sentimentScore;
    System.out.println(sentiment + ""\t"" + sentimentScore);
}
return (sum/sentences.size());
</code></pre>

<p>}</p></li>
</ol>
","stanford-nlp, text-mining, sentiment-analysis","<p>If you run this command:</p>

<pre><code>java -Xmx5g -cp ""stanford-corenlp-full-2015-12-09/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,parse,sentiment -filelist list-of-sample-docs.txt
</code></pre>

<p>the final output will give you timing information</p>

<p>So all you have to do is:</p>

<ol>
<li><p>take 100 IMDB reviews, put them in files named imdb_review_1, imdb_review_2, etc...</p></li>
<li><p>put each filename one file per line in list-of-sample-docs.txts</p></li>
<li><p>run that command and the final output will show total time for each annotator and total time elapsed</p></li>
</ol>
",2,0,414,2015-12-12 07:08:59,https://stackoverflow.com/questions/34237295/how-fast-is-stanfords-corenlp-sentiment-analysis-tool
How to parse more than one sentence from text file using Stanford dependency parse?,"<p>I have a text file which have many line, i wanted to parse all sentences, but it seems like i get all sentences but parse only the first sentence, not sure where m i making mistake.</p>

<pre><code>import nltk
from nltk.parse.stanford import StanfordDependencyParser
dependency_parser = StanfordDependencyParser(  model_path=""edu\stanford\lp\models\lexparser\englishPCFG.ser.gz"")
txtfile =open('sample.txt',encoding=""latin-1"")
s=txtfile.read()
print(s)
result = dependency_parser.raw_parse(s)
for i in result:
print(list(i.triples()))
</code></pre>

<p>but it give only the first sentence parse tripples not other sentences, any help ?</p>

<pre><code>'i like this computer'
'The great Buddha, the .....'
'My Ashford experience .... great experience.'


[[(('i', 'VBZ'), 'nsubj', (""'"", 'POS')), (('i', 'VBZ'), 'nmod', ('computer', 'NN')), (('computer', 'NN'), 'case', ('like', 'IN')), (('computer', 'NN'), 'det', ('this', 'DT')), (('computer', 'NN'), 'case', (""'"", 'POS'))]]
</code></pre>
","parsing, python-3.x, nltk, stanford-nlp, triples","<p>You have to split the text first. You're currently parsing the literal text you posted with quotes and everything. This is evident by this part of the parsing result: <code>(""'"", 'POS')</code></p>

<p>To do that you seem to be able to use <code>ast.literal_eval</code> on each line. Note that an apostrophe (in a word like ""don't"") will ruin the formatting and you'll have to handle the apostrophes yourself with something like <code>line = line[1:-1]</code>:</p>

<pre><code>import ast
from nltk.parse.stanford import StanfordDependencyParser
dependency_parser = StanfordDependencyParser(  model_path=""edu\stanford\lp\models\lexparser\englishPCFG.ser.gz"")

with open('sample.txt',encoding=""latin-1"") as f:
    lines = [ast.litral_eval(line) for line in f.readlines()]

for line in lines:
    parsed_lines = dependency_parser.raw_parse(line)

# now parsed_lines should contain the parsed lines from the file
</code></pre>
",1,0,1021,2015-12-13 09:14:11,https://stackoverflow.com/questions/34249579/how-to-parse-more-than-one-sentence-from-text-file-using-stanford-dependency-par
"How can Stanford CoreNLP Named Entity Recognition capture measurements like 5 inches, 5&quot;, 5 in., 5 in","<p>I'm looking to capture measurements using <a href=""http://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""noreferrer"">Stanford CoreNLP</a>. (If you can suggest a different extractor, that is fine too.)</p>

<p>For example, I want to find <strong>15kg</strong>, <strong>15 kg</strong>, <strong>15.0 kg</strong>, <strong>15 kilogram</strong>, <strong>15 lbs</strong>, <strong>15 pounds</strong>, etc. But among CoreNLPs extraction rules, I don't see one for measurements.</p>

<p>Of course, I can do this with pure regexes, but toolkits can run more quickly, and they offer the opportunity to chunk at a higher level, e.g.  to treat <strong>gb</strong> and <strong>gigabytes</strong> together, and <strong>RAM</strong> and <strong>memory</strong> as building blocks--even without full syntactic parsing--as they build  bigger units  like <strong>128 gb RAM</strong> and <strong>8 gigabytes memory</strong>.</p>

<p>I want an  extractor for this that is  rule-based, not machine-learning-based), but don't see one as part of <a href=""http://nlp.stanford.edu/software/regexner/"" rel=""noreferrer"">RegexNer</a> or elsewhere. How do I go about this?</p>

<p><a href=""https://www-01.ibm.com/support/knowledgecenter/SSPT3X_2.1.1/com.ibm.swg.im.infosphere.biginsights.text.doc/doc/ana_txtan_NamedEntities.html"" rel=""noreferrer"">IBM Named Entity Extraction</a> can do this. The regexes are run in an efficient way rather than passing the text through each one. And the regexes are bundled to express meaningful entities, as for example one that unites all the measurement units into a single concept.</p>
","nlp, stanford-nlp, named-entity-recognition, named-entity-extraction","<p>I don't think a rule-based system exists for this particular task. However, it shouldn't be hard to make with TokensregexNER. For example, a mapping like:</p>

<pre><code>[{ner:NUMBER}]+ /(k|m|g|t)b/ memory?   MEMORY
[{ner:NUMBER}]+ /""|''|in(ches)?/       LENGTH
...
</code></pre>

<p>You could try using vanilla TokensRegex as well, and then just extract out the relevant value with a capture group:</p>

<pre><code>(?$group_name [{ner:NUMBER}]+) /(k|m|g|t)b/ memory?
</code></pre>
",6,7,1432,2015-12-13 14:30:17,https://stackoverflow.com/questions/34252170/how-can-stanford-corenlp-named-entity-recognition-capture-measurements-like-5-in
finding features from a large data set by stanford corenlp,"<p>I am new Stanford NLP. I can not find any good and complete documentation or tutorial. My work is to do sentiment analysis. I have a very large dataset of product reviews. I already distinguished them by positive and negative according to ""starts"" given by the users. Now I need to find the most occurred positive  and negative adjectives as the features of my algorithm. I understand how to do tokenzation, lemmatization and POS taging from <a href=""http://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow"">here</a>. I got files like this.</p>

<p>The review was </p>

<pre><code>Don't waste your money. This is a short DVD and the host is boring and offers information that is common sense to any idiot. Pass on this and buy something else. Very generic
</code></pre>

<p>and the output was.</p>

<pre><code>Sentence #1 (6 tokens):
Don't waste your money.
[Text=Do CharacterOffsetBegin=0 CharacterOffsetEnd=2 PartOfSpeech=VBP Lemma=do]
[Text=n't CharacterOffsetBegin=2 CharacterOffsetEnd=5 PartOfSpeech=RB Lemma=not]
[Text=waste CharacterOffsetBegin=6 CharacterOffsetEnd=11 PartOfSpeech=VB Lemma=waste]
[Text=your CharacterOffsetBegin=12 CharacterOffsetEnd=16 PartOfSpeech=PRP$ Lemma=you]
[Text=money CharacterOffsetBegin=17 CharacterOffsetEnd=22 PartOfSpeech=NN Lemma=money]
[Text=. CharacterOffsetBegin=22 CharacterOffsetEnd=23 PartOfSpeech=. Lemma=.]
Sentence #2 (21 tokens):
This is a short DVD and the host is boring and offers information that is common sense to any idiot.
[Text=This CharacterOffsetBegin=24 CharacterOffsetEnd=28 PartOfSpeech=DT Lemma=this]
[Text=is CharacterOffsetBegin=29 CharacterOffsetEnd=31 PartOfSpeech=VBZ Lemma=be]
[Text=a CharacterOffsetBegin=32 CharacterOffsetEnd=33 PartOfSpeech=DT Lemma=a]
[Text=short CharacterOffsetBegin=34 CharacterOffsetEnd=39 PartOfSpeech=JJ Lemma=short]
[Text=DVD CharacterOffsetBegin=40 CharacterOffsetEnd=43 PartOfSpeech=NN Lemma=dvd]
[Text=and CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=CC Lemma=and]
[Text=the CharacterOffsetBegin=48 CharacterOffsetEnd=51 PartOfSpeech=DT Lemma=the]
[Text=host CharacterOffsetBegin=52 CharacterOffsetEnd=56 PartOfSpeech=NN Lemma=host]
[Text=is CharacterOffsetBegin=57 CharacterOffsetEnd=59 PartOfSpeech=VBZ Lemma=be]
[Text=boring CharacterOffsetBegin=60 CharacterOffsetEnd=66 PartOfSpeech=JJ Lemma=boring]
[Text=and CharacterOffsetBegin=67 CharacterOffsetEnd=70 PartOfSpeech=CC Lemma=and]
[Text=offers CharacterOffsetBegin=71 CharacterOffsetEnd=77 PartOfSpeech=VBZ Lemma=offer]
[Text=information CharacterOffsetBegin=78 CharacterOffsetEnd=89 PartOfSpeech=NN Lemma=information]
[Text=that CharacterOffsetBegin=90 CharacterOffsetEnd=94 PartOfSpeech=WDT Lemma=that]
[Text=is CharacterOffsetBegin=95 CharacterOffsetEnd=97 PartOfSpeech=VBZ Lemma=be]
[Text=common CharacterOffsetBegin=98 CharacterOffsetEnd=104 PartOfSpeech=JJ Lemma=common]
[Text=sense CharacterOffsetBegin=105 CharacterOffsetEnd=110 PartOfSpeech=NN Lemma=sense]
[Text=to CharacterOffsetBegin=111 CharacterOffsetEnd=113 PartOfSpeech=TO Lemma=to]
[Text=any CharacterOffsetBegin=114 CharacterOffsetEnd=117 PartOfSpeech=DT Lemma=any]
[Text=idiot CharacterOffsetBegin=118 CharacterOffsetEnd=123 PartOfSpeech=NN Lemma=idiot]
[Text=. CharacterOffsetBegin=123 CharacterOffsetEnd=124 PartOfSpeech=. Lemma=.]
Sentence #3 (8 tokens):
Pass on this and buy something else.
[Text=Pass CharacterOffsetBegin=125 CharacterOffsetEnd=129 PartOfSpeech=VB Lemma=pass]
[Text=on CharacterOffsetBegin=130 CharacterOffsetEnd=132 PartOfSpeech=IN Lemma=on]
[Text=this CharacterOffsetBegin=133 CharacterOffsetEnd=137 PartOfSpeech=DT Lemma=this]
[Text=and CharacterOffsetBegin=138 CharacterOffsetEnd=141 PartOfSpeech=CC Lemma=and]
[Text=buy CharacterOffsetBegin=142 CharacterOffsetEnd=145 PartOfSpeech=VB Lemma=buy]
[Text=something CharacterOffsetBegin=146 CharacterOffsetEnd=155 PartOfSpeech=NN Lemma=something]
[Text=else CharacterOffsetBegin=156 CharacterOffsetEnd=160 PartOfSpeech=RB Lemma=else]
[Text=. CharacterOffsetBegin=160 CharacterOffsetEnd=161 PartOfSpeech=. Lemma=.]
Sentence #4 (2 tokens):
Very generic
[Text=Very CharacterOffsetBegin=162 CharacterOffsetEnd=166 PartOfSpeech=RB Lemma=very]
[Text=generic CharacterOffsetBegin=167 CharacterOffsetEnd=174 PartOfSpeech=JJ Lemma=generic]
</code></pre>

<p>I already have processed 10000 positive and 10000 negative file like this. Now How can I easily find the most occurred positive and negative features(adjectives)? Do i need to read all the output(processed) file and make a list frequency count of the adjectives like this or is there any easy way by stanford corenlp? </p>
","java, nlp, stanford-nlp","<p>Here is an example of processing an annotated review and storing the adjectives in a Counter.</p>

<p>In the example the movie review  ""The movie was great!  It was a great film."" has a sentiment of ""positive"".</p>

<p>I would suggest altering my code to load in each file and build an Annotation with the file's text and recording the sentiment for that file.</p>

<p>Then you can go through each file and build up a Counter with positive and negative counts for each adjective.</p>

<p>The final Counter has the adjective ""great"" with a count of 2.</p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.stats.Counter;
import edu.stanford.nlp.stats.ClassicCounter;
import edu.stanford.nlp.util.CoreMap;

import java.util.Properties;

public class AdjectiveSentimentExample {

    public static void main(String[] args) throws Exception {

        Counter&lt;String&gt; adjectivePositiveCounts = new ClassicCounter&lt;String&gt;();
        Counter&lt;String&gt; adjectiveNegativeCounts = new ClassicCounter&lt;String&gt;();

        Annotation review = new Annotation(""The movie was great!  It was a great film."");
        String sentiment = ""positive"";

        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        pipeline.annotate(review);
        for (CoreMap sentence : review.get(CoreAnnotations.SentencesAnnotation.class)) {
            for (CoreLabel cl : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                if (cl.get(CoreAnnotations.PartOfSpeechAnnotation.class).equals(""JJ"")) {
                    if (sentiment.equals(""positive"")) {
                        adjectivePositiveCounts.incrementCount(cl.word());
                    } else if (sentiment.equals(""negative"")) {
                        adjectiveNegativeCounts.incrementCount(cl.word());
                    }
                }

            }
        }

        System.out.println(""---"");
        System.out.println(""positive adjective counts"");
        System.out.println(adjectivePositiveCounts);
    }
}
</code></pre>
",1,1,232,2015-12-13 15:06:15,https://stackoverflow.com/questions/34252507/finding-features-from-a-large-data-set-by-stanford-corenlp
Stanford corenlp pause and continue annotation pipeline,"<p>Typically when you use the corenlp annotation pipeline for say NER you would write the following code</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
pipeline.annotate(document);
</code></pre>

<p>I would like to perform sentence splitting i.e. <code>ssplit</code> in the above pipeline.  But then I would like to remove sentences that are too long before I continue the rest of the pipeline.  What I have been doing is performing sentence splitting, filtering the sentences by length, then performing NER by applying the entire pipeline i.e. <code>tokenize, ssplit, pos, lemma, ner</code>.  So essentially I have performed <code>tokenize</code> and <code>ssplit</code> twice.  Is there a more efficient way of doing this?  For example performing <code>tokenize</code> and <code>ssplit</code> then pausing the pipeline to remove sentences that are too long, then resume the pipeline with <code>pos</code>, <code>lemma</code>, and <code>ner</code>.</p>
","java, stanford-nlp","<p>You can create two pipeline objects, with the second one taking the later annotators. So:</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
pipeline.annotate(document);
</code></pre>

<p>Followed by:</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""pos, lemma, ner"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props, false);
pipeline.annotate(document);
</code></pre>

<p>Note, of course, that some of the annotations (e.g., character offsets) will be unintuitive if you delete intermediate sentences.</p>
",1,0,130,2015-12-15 00:32:16,https://stackoverflow.com/questions/34279091/stanford-corenlp-pause-and-continue-annotation-pipeline
ClassNotFoundException when running newest version of stanford-postagger 3.6.0,"<p>I found the Stanford pos tagger and would like to use it in a project I am working on. 
Unfortunately, I get the following error when I try to run it: </p>

<blockquote>
  <p>Exception in thread ""Thread-0"" java.lang.NoClassDefFoundError:
  org/slf4j/LoggerFactory
          at edu.stanford.nlp.io.IOUtils.(IOUtils.java:41)
          at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:765)
          at edu.stanford.nlp.tagger.maxent.MaxentTagger.(MaxentTagger.java:298)
          at edu.stanford.nlp.tagger.maxent.MaxentTagger.(MaxentTagger.java:263)
          at edu.stanford.nlp.tagger.maxent.MaxentTaggerGUI$1.run(MaxentTaggerGUI.java:89)
  Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
          at java.net.URLClassLoader.findClass(Unknown Source)
          at java.lang.ClassLoader.loadClass(Unknown Source)
          at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
          at java.lang.ClassLoader.loadClass(Unknown Source)
          ... 5 more</p>
</blockquote>

<p>The same error occurred whether running within eclipse, command line, or gui. </p>

<p>I discovered after searching for a solution and trying many that didn't help, that the previous version 3.5.2 runs just fine with no errors so I will revert to the previous version for now. </p>

<p>Is it possible there is a problem with the new version? </p>
",stanford-nlp,"<p>We've added slf4j logging to our releases.  In 3.6.0 there is a folder called lib which needs to be added to the CLASSPATH.</p>
",0,0,664,2015-12-15 14:11:56,https://stackoverflow.com/questions/34291393/classnotfoundexception-when-running-newest-version-of-stanford-postagger-3-6-0
Adding custom dictionary,"<p>In Stanford sentiment analysis do we have an option for marking specific/custom words as positive[based on our requirements].</p>

<p>Analysing tweets is giving a negative trend due to the business terms used. Can we handle it to neutralize the negative output due to these words by adding our custom dictionary ? </p>
",stanford-nlp,"<p>The cleanest way to do this would be to retrain the sentiment model. Acquire the <a href=""http://nlp.stanford.edu/sentiment/"" rel=""nofollow"">sentiment training data</a> and manually modify the labels for the words you are concerned about. There are very basic instructions for training <a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">on another Stanford Sentiment page</a>. Then use this trained model as you wish!</p>

<p>A very dirty but possibly faster solution would be to modify the trees you get from the standard model after the fact. For example, you'd search an analyzed tree for words of interest and manually modify their sentiment label. Then apply some heuristic in order to propagate this modification up the tree and possibly alter the sentiment of the whole sentence.</p>
",0,0,819,2015-12-15 16:15:22,https://stackoverflow.com/questions/34294126/adding-custom-dictionary
Why does Stanford POS tagger modify input sentence?,"<p>I took this sentence from Wall Street Journal and passed it through Stanford POS tagger. Strangely, the tagger changed ""theatre"" into ""theater""</p>

<p>The command:</p>

<pre><code>java -classpath stanford-postagger-2015-12-09/stanford-postagger-3.6.0.jar:stanford-postagger-2015-12-09/lib/slf4j-simple.jar:stanford-postagger-2015-12-09/lib/slf4j-api.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -props stanford-postagger-2015-12-09/penn-treebank.props -model /home/minhle/redep/output/dep/penntree.jackknife/jackknife-04.model -testFile format=TREES,test.tree
</code></pre>

<p>The property file:</p>

<pre><code>## adopted english-bidirectional-distsim.tagger.props
## tagger training invoked at Tue Feb 25 01:33:39 PST 2014 with arguments:
                    arch = bidirectional5words,naacl2003unknowns,allwordshapes(-1,1),distsim(stanford-postagger-2015-12-09/egw4-reut.512.clusters.txt,-1,1),distsimconjunction(stanford-postagger-2015-12-09/egw4-reut.512.clusters.txt,-1,1)
            wordFunction = edu.stanford.nlp.process.AmericanizeFunction
         closedClassTags =
 closedClassTagThreshold = 40
 curWordMinFeatureThresh = 2
                   debug = false
             debugPrefix =
            tagSeparator = _
                encoding = UTF-8
              iterations = 100
                    lang = english
    learnClosedClassTags = false
        minFeatureThresh = 2
           openClassTags =
rareWordMinFeatureThresh = 5
          rareWordThresh = 5
                  search = owlqn2
                    sgml = false
            sigmaSquared = 0.5
                   regL1 = 0.75
               tagInside =
                tokenize = true
        tokenizerFactory =
        tokenizerOptions =
                 verbose = false
          verboseResults = true
    veryCommonWordThresh = 250
                xmlInput =
              outputFile =
            outputFormat = slashTags
     outputFormatOptions =
                nthreads = 4
</code></pre>

<p>The input sentence:</p>

<blockquote>
  <p>( (SINV (`` ``) (S-TPC-2 (PP (IN Without) (NP (DT some) (JJ
  unexpected) (`` ``) (FW coup) (FW de) (FW <strong>theatre</strong>) ('' ''))) (, ,)
  (NP-SBJ (PRP I)) (VP (VBP do) (RB n't) (VP (VB see) (SBAR (WHNP-1 (WP
  what)) (S (NP-SBJ-1 (-NONE- <em>T</em>)) (VP (MD will) (VP (VB block) (NP (DT
  the) (NNP Paribas) (NN bid))))))))) (, ,) ('' '') (VP (VBD said) (S-2
  (-NONE- <em>T</em>))) (NP-SBJ (NP (NNP Philippe) (NNP de) (NNP Cholet)) (, ,)
  (NP (NP (NN analyst)) (PP-LOC (IN at) (NP (NP (DT the) (NN brokerage))
  (NP (NNP Cholet) (HYPH -) (NNP Dupont) (CC &amp;) (NNP Cie)))))) (. .)) )</p>
</blockquote>

<p>The output:</p>

<blockquote>
  <p>``_`` Without_IN some_DT unexpected_JJ ``_`` coup_NN de_IN
  <strong>theater_NN</strong> ''_'' ,_, I_PRP do_VBP n't_RB see_VB what_WP will_MD block_VB the_DT Paribas_NNP bid_NN ,_, ''_'' said_VBD Philippe_NNP
  de_IN Cholet_NNP ,_, analyst_NN at_IN the_DT brokerage_NN Cholet_NNP
  -_HYPH Dupont_NNP &amp;_CC Cie_NNP ._.</p>
</blockquote>
",stanford-nlp,"<p>As I understand it, the Stanford POS tagger is trained with US English training data. At runtime we ""Americanize"" the input data in order to make sure it is recognized properly by the tagger. See this line in your configuration file:</p>

<pre><code>wordFunction = edu.stanford.nlp.process.AmericanizeFunction
</code></pre>

<p>If you are accessing CoreNLP programmatically, you can retrieve the pre-Americanized form via <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/CoreLabel.html#originalText--"" rel=""nofollow""><code>CoreLabel.originalText</code></a>. You could also just disable the <code>AmericanizeFunction</code>, but you might see some incorrect outputs as a result.</p>
",2,0,185,2015-12-16 12:23:34,https://stackoverflow.com/questions/34311967/why-does-stanford-pos-tagger-modify-input-sentence
I don&#39;t know where those third-party Python packages have been installed on MacOS?,"<p>I'm currently doing Natural Language Processing with Python. I installed NLTK package by using <code>pip install nltk</code>. I have several Pythons on my Mac, but I use 3.4 mostly.</p>

<p>Now, I've downloaded a segmenter from Stanford that could do Chinese language segmentation(<a href=""http://nlp.stanford.edu/software/segmenter.shtml"" rel=""nofollow"">Stanford Word Segmenter</a>), but it might need me to do some changes on the original NLTK package, namely, I have to write an API file called <code>stanford_segmenter.py</code> and then put it into the <code>nltk/tokenize/</code> directory.</p>

<p>The problem is I don't really know where the NLTK package having been installed, I actually don't know where the <code>lib</code> directory of Python 3.4 is on MacOS. Anyone could help me?</p>
","python, nltk, stanford-nlp","<p>A generic approach to locating a package in the file system is importing it and looking at the <code>__file__</code> attribute.</p>

<p>On the commandline, you can e.g. run</p>

<pre><code>python -c ""import nltk; print(nltk.__file__)""
</code></pre>

<p>It will show you where the top level<code>nltk</code> (probably <code>__init__.py</code>) file is located.</p>
",1,0,112,2015-12-17 10:48:37,https://stackoverflow.com/questions/34332675/i-dont-know-where-those-third-party-python-packages-have-been-installed-on-maco
nltk StanfordNERTagger : NoClassDefFoundError: org/slf4j/LoggerFactory (In Windows),"<p>NOTE: I am using Python 2.7 as part of Anaconda distribution. I hope this is not a problem for nltk 3.1.</p>

<p>I am trying to use nltk for NER as</p>

<pre><code>import nltk
from nltk.tag.stanford import StanfordNERTagger 
#st = StanfordNERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
print st.tag(str)
</code></pre>

<p>but i get </p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
    at edu.stanford.nlp.io.IOUtils.&lt;clinit&gt;(IOUtils.java:41)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1117)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1076)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1057)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3088)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 5 more

Traceback (most recent call last):
  File ""X:\jnk.py"", line 47, in &lt;module&gt;
    print st.tag(str)
  File ""X:\Anaconda2\lib\site-packages\nltk\tag\stanford.py"", line 66, in tag
    return sum(self.tag_sents([tokens]), []) 
  File ""X:\Anaconda2\lib\site-packages\nltk\tag\stanford.py"", line 89, in tag_sents
    stdout=PIPE, stderr=PIPE)
  File ""X:\Anaconda2\lib\site-packages\nltk\internals.py"", line 134, in java
    raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : ['X:\\PROGRA~1\\Java\\JDK18~1.0_6\\bin\\java.exe', '-mx1000m', '-cp', 'X:\\stanford\\stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'X:\\stanford\\classifiers\\english.all.3class.distsim.crf.ser.gz', '-textFile', 'x:\\appdata\\local\\temp\\tmpqjsoma', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf8']
</code></pre>

<p>but i can see that the slf4j jar is there in my lib folder. do i need to update an environment variable?</p>

<p><strong>Edit</strong></p>

<p>Thanks everyone for their help, but i still get the same error. Here is what i tried recently</p>

<pre><code>import nltk
from nltk.tag import StanfordNERTagger 
print(nltk.__version__)
stanford_ner_dir = 'X:\\stanford\\'
eng_model_filename= stanford_ner_dir + 'classifiers\\english.all.3class.distsim.crf.ser.gz'
my_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'
st = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) 
print st._stanford_model
print st._stanford_jar

st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
</code></pre>

<p>and also</p>

<pre><code>import nltk
from nltk.tag import StanfordNERTagger 
print(nltk.__version__)
st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
print st._stanford_model
print st._stanford_jar
st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
</code></pre>

<p>i get</p>

<pre><code>3.1
X:\stanford\classifiers\english.all.3class.distsim.crf.ser.gz
X:\stanford\stanford-ner.jar
</code></pre>

<p>after that it goes on to print the same stacktrace as before. <code>java.lang.ClassNotFoundException: org.slf4j.LoggerFactory</code></p>

<p>any idea why this might be happening? I updated my CLASSPATH as well. I even added all the relevant folders to my PATH environment variable.for example the folder where i unzipped the stanford jars, the place where i unzipped slf4j and even the lib folder inside the stanford folder. i have no idea why this is happening :(</p>

<p><strong>Could it be windows? i have had problems with windows paths before</strong></p>

<p><strong>Update</strong></p>

<ol>
<li><p>The Stanford NER version i have is 3.6.0. The zip file says <code>stanford-ner-2015-12-09.zip</code></p></li>
<li><p>I also tried using the <code>stanford-ner-3.6.0.jar</code> instead of <code>stanford-ner.jar</code> but still get the same error</p></li>
<li><p>When i right click on the <code>stanford-ner-3.6.0.jar</code>, i notice   </p></li>
</ol>

<p><a href=""https://i.sstatic.net/Z8Jlo.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Z8Jlo.png"" alt=""jar properties""></a></p>

<p><strong>i see this for all the files that i have extracted, even the slf4j files.could this be causing the problem?</strong></p>

<ol start=""4"">
<li>Finally, why does the error message say</li>
</ol>

<p><code>java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory</code></p>

<p>i do not see any folder named <code>org</code> anywhere</p>

<p><strong>Update: Env variables</strong></p>

<p>Here are my env variables</p>

<pre><code>CLASSPATH
.;
X:\jre1.8.0_60\lib\rt.jar;
X:\stanford\stanford-ner-3.6.0.jar;
X:\stanford\stanford-ner.jar;
X:\stanford\lib\slf4j-simple.jar;
X:\stanford\lib\slf4j-api.jar;
X:\slf4j\slf4j-1.7.13\slf4j-1.7.13\slf4j-log4j12-1.7.13.jar

STANFORD_MODELS
X:\stanford\classifiers

JAVA_HOME
X:\PROGRA~1\Java\JDK18~1.0_6

PATH
X:\PROGRA~1\Java\JDK18~1.0_6\bin;
X:\stanford;
X:\stanford\lib;
X:\slf4j\slf4j-1.7.13\slf4j-1.7.13
</code></pre>

<p>anything wrong here?</p>
","python, windows, nlp, nltk, stanford-nlp","<h1>EDITED</h1>

<p><strong>Note:</strong> The following answer will only work on:</p>

<ul>
<li>NLTK version 3.1</li>
<li>Stanford Tools compiled since 2015-04-20</li>
</ul>

<p>As both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix.</p>

<p>Always refer to <a href=""https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software</a> for the latest instruction on how to interface Stanford NLP tools using NLTK!!</p>

<hr>

<h1>Step 1</h1>

<p>First update your NLTK to the version 3.1 using</p>

<pre><code>pip install -U nltk
</code></pre>

<p>or (for Windows) download the latest NLTK using <a href=""http://pypi.python.org/pypi/nltk"" rel=""nofollow noreferrer"">http://pypi.python.org/pypi/nltk</a></p>

<p>Then check that you have version 3.1 using:</p>

<pre><code>python3 -c ""import nltk; print(nltk.__version__)""
</code></pre>

<h1>Step 2</h1>

<p>Then download the zip file from <a href=""http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip</a> and unzip the file and save to <code>C:\some\path\to\stanford-ner\</code> (In windows)</p>

<h1>Step 3</h1>

<p>Then set the environment variable for <code>CLASSPATH</code> to <code>C:\some\path\to\stanford-ner\stanford-ner.jar</code></p>

<p>and the environment variable for <code>STANFORD_MODELS</code> to 
<code>C:\some\path\to\stanford-ner\classifiers</code></p>

<p>Or in command line (<strong>ONLY for Windows</strong>):</p>

<pre><code>set CLASSPATH=%CLASSPATH%;C:\some\path\to\stanford-ner\stanford-ner.jar
set STANFORD_MODELS=%STANFORD_MODELS%;C:\some\path\to\stanford-ner\classifiers
</code></pre>

<p>(See <a href=""https://stackoverflow.com/a/17176423/610569"">https://stackoverflow.com/a/17176423/610569</a> for click-click GUI instructions for setting environment variables in Windows)</p>

<p>(See <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk/34112695#34112695"">Stanford Parser and NLTK</a> for details on setting environment variables in Linux)</p>

<h1>Step 4</h1>

<p>Then in python:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag import StanfordNERTagger
&gt;&gt;&gt; st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
&gt;&gt;&gt; st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]
</code></pre>

<p>Without setting the environment variables, you can try:</p>

<pre><code>from nltk.tag import StanfordNERTagger

stanford_ner_dir = 'C:\\some\path\to\stanford-ner\'
eng_model_filename= stanford_ner_dir + 'classifiers\english.all.3class.distsim.crf.ser.gz'
my_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'

st = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) 
st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
</code></pre>

<p>See more detailed instructions on <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk/34112695#34112695"">Stanford Parser and NLTK</a></p>
",11,11,5790,2015-12-18 18:20:22,https://stackoverflow.com/questions/34361725/nltk-stanfordnertagger-noclassdeffounderror-org-slf4j-loggerfactory-in-windo
Stanford CoreNLP version change in pom.xml causing error,"<p>I am using Stanford CoreNLP on Ubuntu 14.04 and facing the following issue when I run the following code:</p>

<p><strong>Java Code:</strong></p>

<pre><code>package com.mycompany.app;

import java.io.*; 
import java.util.*;

/*import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.parser.shiftreduce.ShiftReduceParser;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser;
import edu.stanford.nlp.trees.Tree;*/

import edu.stanford.nlp.tagger.maxent.MaxentTagger;

public class App
{   
    public static void main(String[] args) throws Exception, NoClassDefFoundError
    {
        MaxentTagger tagger = null;
        if(tagger == null)
        {
            tagger = new MaxentTagger(""mymodel.tagger"");
        }
        System.out.println(""Let's do this!"");
    }
}
</code></pre>

<p><strong>Command Run:</strong></p>

<blockquote>
  <p>mvn clean install exec:java -Dexec.mainClass=com.mycompany.app.App</p>
</blockquote>

<p><strong>Terminal Output:</strong></p>

<pre><code>[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ dt_mvn ---
[INFO] Building jar: /home/sidharth/Desktop/dt_mvn/target/dt_mvn-1.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ dt_mvn ---
[INFO] Installing /home/sidharth/Desktop/dt_mvn/target/dt_mvn-1.0-SNAPSHOT.jar to /home/sidharth/.m2/repository/com/mycompany/app/dt_mvn/1.0-SNAPSHOT/dt_mvn-1.0-SNAPSHOT.jar
[INFO] Installing /home/sidharth/Desktop/dt_mvn/pom.xml to /home/sidharth/.m2/repository/com/mycompany/app/dt_mvn/1.0-SNAPSHOT/dt_mvn-1.0-SNAPSHOT.pom
[INFO] 

[INFO] --- exec-maven-plugin:1.4.0:java (default-cli) @ dt_mvn ---
Reading POS tagger model from mymodel.tagger ... [WARNING] 
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:293)
    at java.lang.Thread.run(Thread.java:745)
Caused by: edu.stanford.nlp.io.RuntimeIOException: java.io.StreamCorruptedException: invalid stream header: 00048E4D
    at edu.stanford.nlp.maxent.iis.LambdaSolve.read_lambdas(LambdaSolve.java:726)
    at edu.stanford.nlp.tagger.maxent.LambdaSolveTagger.&lt;init&gt;(LambdaSolveTagger.java:76)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:863)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:767)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:298)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:263)
    at com.mycompany.app.App.main(App.java:22)
    ... 6 more
Caused by: java.io.StreamCorruptedException: invalid stream header: 00048E4D
    at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:806)
    at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:299)
    at edu.stanford.nlp.maxent.iis.LambdaSolve.read_lambdas(LambdaSolve.java:719)
    ... 12 more
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.260s
[INFO] Finished at: Sun Dec 20 01:34:29 IST 2015
[INFO] Final Memory: 23M/228M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.4.0:java (default-cli) on project dt_mvn: An exception occured while executing the Java class. null: InvocationTargetException: java.io.StreamCorruptedException: invalid stream header: 00048E4D -&gt; [Help 1]
</code></pre>

<p><strong>pom.xml:</strong></p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt;
  &lt;artifactId&gt;dt_mvn&lt;/artifactId&gt;
  &lt;packaging&gt;jar&lt;/packaging&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;name&gt;dt_mvn&lt;/name&gt;
  &lt;url&gt;http://maven.apache.org&lt;/url&gt;
  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;3.8.1&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
      &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
      &lt;version&gt;3.5.2&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre>

<p>However, changing version in pom.xml from 3.5.2 to 1.3.0 works correctly. What could be the reason for this?</p>

<p>Thanks!</p>

<p><strong>P.S.</strong> 
If it's of any use, the maven project was created by the following command:</p>

<pre><code>mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=dt_mvn -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
</code></pre>
","java, maven, pom.xml, stanford-nlp","<p>I suspect however you built ""mymodel.tagger"" is incompatible with later versions so the deserialization is failing.  What version did you use to build my_model.tagger?  Does your Maven project fail if you use 1.3.5 ?  I can see changes in our repo in LambdaSolve.java involving serialization in the time period between 1.3.1 and 1.3.5 so I suspect that is the reason.</p>

<p>Also make sure you use Java 1.8 with Stanford CoreNLP 3.5.0 or later.</p>
",0,2,383,2015-12-19 20:15:53,https://stackoverflow.com/questions/34374964/stanford-corenlp-version-change-in-pom-xml-causing-error
how to separate words and special character using stanford tokenize?,"<p>I'm using Stanford CoreNLP tool and I need to separate chain as:
""(see functional requirement number 150)."" </p>

<p>The result of my code is (in corelabels):
[(see, functional, requirement, number, 150).] </p>

<p>when it should be:
[(,see, functional, requirement, number, 150,),.]</p>

<p>The code segment is:</p>

<pre><code>public List&lt;CoreMap&gt; armador(String text){

   Properties props;
   StanfordCoreNLP pipeline;
   props.put(""annotators"", ""tokenize,ssplit,pos"");
   props.put(""ssplit.eolonly"", ""true"");
   props.put(""tokenize.whitespace"", ""true"");

   pipeline = new StanfordCoreNLP(props);
   Annotation document = new Annotation(text);
   pipeline.annotate(document);
   List&lt;CoreMap&gt; result = document.get(CoreAnnotations.SentencesAnnotation.class);  

   return result;
}
</code></pre>

<p>Thank's and sorry my english!</p>
","java, stanford-nlp, tokenize","<p>This is caused by the property:</p>

<pre><code>props.put(""tokenize.whitespace"", ""true"");
</code></pre>

<p>By default, CoreNLP will run Penn Treebank tokenization, which will correctly tokenize out the parentheses. However, the property <code>tokenize.whitespace</code> forces CoreNLP to tokenize only on whitespace tokens.</p>

<p><strong>EDIT</strong> You maybe also should be wary of <code>props.put(""ssplit.eolonly"", ""true"");</code> -- this will only split sentences on newlines.</p>
",0,0,586,2015-12-20 20:43:56,https://stackoverflow.com/questions/34385641/how-to-separate-words-and-special-character-using-stanford-tokenize
Stanford NLP parse tree format,"<p>This may be a silly question, but how does one iterate through a parse tree as an output of an NLP parser (like Stanford NLP)? It's all nested brackets, which is neither an <code>array</code> nor a <code>dictionary</code> or any other collection type I've used.</p>

<pre><code>(ROOT\n  (S\n    (PP (IN As)\n      (NP (DT an) (NN accountant)))\n    (NP (PRP I))\n    (VP (VBP want)\n      (S\n        (VP (TO to)\n          (VP (VB make)\n            (NP (DT a) (NN payment))))))))
</code></pre>
","nlp, stanford-nlp, parse-tree","<p>This particular output format of the Stanford Parser is call the ""bracketed parse (tree)"". It is supposed to be read as a graph with </p>

<ul>
<li>words as nodes (e.g. As, an, accountant)</li>
<li>phrase/clause as labels (e.g. S, NP, VP)</li>
<li>edges are linked hierarchically and</li>
<li>typically the parses TOP or root node is a hallucinated <code>ROOT</code></li>
</ul>

<p>(In this case you can read it as a Directed Acyclic Graph (DAG) since it's unidirectional and non-cyclic)</p>

<p>There are libraries out there to read bracketed parse, e.g. in <code>NLTK</code>'s <code>nltk.tree.Tree</code> (<a href=""http://www.nltk.org/howto/tree.html"" rel=""noreferrer"">http://www.nltk.org/howto/tree.html</a>):</p>

<pre><code>&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; output = '(ROOT (S (PP (IN As) (NP (DT an) (NN accountant))) (NP (PRP I)) (VP (VBP want) (S (VP (TO to) (VP (VB make) (NP (DT a) (NN payment))))))))'
&gt;&gt;&gt; parsetree = Tree.fromstring(output)
&gt;&gt;&gt; print parsetree
(ROOT
  (S
    (PP (IN As) (NP (DT an) (NN accountant)))
    (NP (PRP I))
    (VP
      (VBP want)
      (S (VP (TO to) (VP (VB make) (NP (DT a) (NN payment))))))))
&gt;&gt;&gt; parsetree.pretty_print()
                           ROOT                             
                            |                                
                            S                               
      ______________________|________                        
     |                  |            VP                     
     |                  |    ________|____                   
     |                  |   |             S                 
     |                  |   |             |                  
     |                  |   |             VP                
     |                  |   |     ________|___               
     PP                 |   |    |            VP            
  ___|___               |   |    |    ________|___           
 |       NP             NP  |    |   |            NP        
 |    ___|______        |   |    |   |         ___|_____     
 IN  DT         NN     PRP VBP   TO  VB       DT        NN  
 |   |          |       |   |    |   |        |         |    
 As  an     accountant  I  want  to make      a      payment

&gt;&gt;&gt; parsetree.leaves()
['As', 'an', 'accountant', 'I', 'want', 'to', 'make', 'a', 'payment']
</code></pre>
",5,4,5057,2015-12-21 11:55:45,https://stackoverflow.com/questions/34395127/stanford-nlp-parse-tree-format
Online parser tags different to local MaxentTagger tags,"<p>I'm running the MaxentTagger directly like this:</p>
<pre><code>private void test() {
    MaxentTagger tagger = new MaxentTagger(modelsdir + &quot;wsj-0-18-bidirectional-distsim.tagger&quot;);
    String input = &quot;Someone will trip over that cable.&quot;;
    System.out.println(tagger.tagString(input));
}
</code></pre>
<p>And getting the following output:</p>
<blockquote>
<p>Someone_NN will_NN trip_NN over_IN that_DT cable_NN ._.</p>
</blockquote>
<p>But when using the online parser I get this output:</p>
<blockquote>
<p>Your query</p>
<blockquote>
<p><em>Someone will trip over that cable.</em></p>
</blockquote>
<p>Tagging</p>
<blockquote>
<p>Someone/NN will/MD trip/VB over/RP that/DT cable/NN ./.</p>
</blockquote>
</blockquote>
<p>I tried using different models:</p>
<pre><code>MaxentTagger tagger = new MaxentTagger(models + &quot;english-left3words-distsim.tagger&quot;);
</code></pre>
<blockquote>
<p>Someone_NN will_MD trip_NN over_IN that_DT cable_NN ._.</p>
</blockquote>
<p>but I cant seem to get the same results as the online version.</p>
<p>Why am I getting different tags for words when using the MaxentTagger in comparison to the online version?</p>
<p>Should I be using parameters when initializing the MaxentTagger?</p>
",stanford-nlp,"<p>By ""online parser"" do you mean the CoreNLP demo or the Stanford Parser demo? The latter will assign part-of-speech tags from the parse tree of the sentence, which can be different than those assigned by the part-of-speech tagger.</p>
",0,0,79,2015-12-22 11:16:49,https://stackoverflow.com/questions/34414643/online-parser-tags-different-to-local-maxenttagger-tags
nltk StanfordNERTagger : How to get proper nouns without capitalization,"<p>I am trying to use the StanfordNERTagger and nltk to extract keywords from a piece of text. </p>

<pre><code>docText=""John Donk works for POI. Brian Jones wants to meet with Xyz Corp. for measuring POI's Short Term performance Metrics.""

words = re.split(""\W+"",docText) 

stops = set(stopwords.words(""english""))

    #remove stop words from the list
words = [w for w in words if w not in stops and len(w) &gt; 2]

str = "" "".join(words)
print str
stn = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
stp = StanfordPOSTagger('english-bidirectional-distsim.tagger') 
stanfordPosTagList=[word for word,pos in stp.tag(str.split()) if pos == 'NNP']

print ""Stanford POS Tagged""
print stanfordPosTagList
tagged = stn.tag(stanfordPosTagList)
print tagged
</code></pre>

<p>this gives me </p>

<pre><code>John Donk works POI Brian Jones wants meet Xyz Corp measuring POI Short Term performance Metrics
Stanford POS Tagged
[u'John', u'Donk', u'POI', u'Brian', u'Jones', u'Xyz', u'Corp', u'POI', u'Short', u'Term']
[(u'John', u'PERSON'), (u'Donk', u'PERSON'), (u'POI', u'ORGANIZATION'), (u'Brian', u'ORGANIZATION'), (u'Jones', u'ORGANIZATION'), (u'Xyz', u'ORGANIZATION'), (u'Corp', u'ORGANIZATION'), (u'POI', u'O'), (u'Short', u'O'), (u'Term', u'O')]
</code></pre>

<p>so clearly, things like <code>Short</code> and <code>Term</code> were tagged as <code>NNP</code>. The data that i have contains many such instances where <strong>non <code>NNP</code> words are capitalized</strong>. This might be due to typos or maybe they are headers. I dont have much control over that. </p>

<p>How can i parse or clean up the data so that i can detect a non <code>NNP</code> term even though it may be capitalized? <strong>I dont want terms like <code>Short</code> and <code>Term</code> to be categorized as <code>NNP</code></strong></p>

<p>Also, not sure why <code>John Donk</code> was captured as a person but <code>Brian Jones</code> was not. Could it be due to the other capitalized non <code>NNP</code>s in my data? Could that be having an effect on how the <code>StanfordNERTagger</code> treats everything else?</p>

<p><strong>Update, one possible solution</strong></p>

<p>Here is what i plan to do</p>

<ol>
<li>Take each word and convert to lower case</li>
<li>Tag the lowercase word</li>
<li>If the tag is <code>NNP</code> then we know that the original word must also be an <code>NNP</code></li>
<li>If not, then  the original word was mis-capitalized</li>
</ol>

<p>Here is what i tried to do</p>

<pre><code>str = "" "".join(words)
print str
stp = StanfordPOSTagger('english-bidirectional-distsim.tagger') 
for word in str.split():
    wl = word.lower()
    print wl
    w,pos = stp.tag(wl)
    print pos
    if pos==""NNP"":
        print ""Got NNP""
        print w
</code></pre>

<p>but this gives me error</p>

<pre><code>John Donk works POI Jones wants meet Xyz Corp measuring POI short term performance metrics
john
Traceback (most recent call last):
  File ""X:\crp.py"", line 37, in &lt;module&gt;
    w,pos = stp.tag(wl)
ValueError: too many values to unpack
</code></pre>

<p>i have tried multiple approaches but some error always shows up. <strong>How can i Tag a single word?</strong></p>

<p>I dont want to convert the whole string to lower case and then Tag. If i do that, the <code>StanfordPOSTagger</code> returns an empty string</p>
","python, nlp, nltk, stanford-nlp, pos-tagger","<p>Firstly, see your other question to setup Stanford CoreNLP to be called from command-line or python: <a href=""https://stackoverflow.com/questions/34455749/nltk-how-to-prevent-stemming-of-proper-nouns"">nltk : How to prevent stemming of proper nouns</a>.</p>

<p>For the proper cased sentence we see that the NER works properly:</p>

<pre><code>&gt;&gt;&gt; from corenlp import StanfordCoreNLP
&gt;&gt;&gt; nlp = StanfordCoreNLP('http://localhost:9000')
&gt;&gt;&gt; text = ('John Donk works POI Jones wants meet Xyz Corp measuring POI short term performance metrics. '
... 'john donk works poi jones wants meet xyz corp measuring poi short term performance metrics')
&gt;&gt;&gt; output = nlp.annotate(text, properties={'annotators': 'tokenize,ssplit,pos,ner',  'outputFormat': 'json'})
&gt;&gt;&gt; annotated_sent0 = output['sentences'][0]
&gt;&gt;&gt; annotated_sent1 = output['sentences'][1]
&gt;&gt;&gt; for token in annotated_sent0['tokens']:
...     print token['word'], token['lemma'], token['pos'], token['ner']
... 
John John NNP PERSON
Donk Donk NNP PERSON
works work VBZ O
POI POI NNP ORGANIZATION
Jones Jones NNP ORGANIZATION
wants want VBZ O
meet meet VB O
Xyz Xyz NNP ORGANIZATION
Corp Corp NNP ORGANIZATION
measuring measure VBG O
POI poi NN O
short short JJ O
term term NN O
performance performance NN O
metrics metric NNS O
. . . O
</code></pre>

<p>And for the lowered cased sentence, you will not get <code>NNP</code> for POS tag nor any NER tag:</p>

<pre><code>&gt;&gt;&gt; for token in annotated_sent1['tokens']:
...     print token['word'], token['lemma'], token['pos'], token['ner']
... 
john john NN O
donk donk JJ O
works work NNS O
poi poi VBP O
jones jone NNS O
wants want VBZ O
meet meet VB O
xyz xyz NN O
corp corp NN O
measuring measure VBG O
poi poi NN O
short short JJ O
term term NN O
performance performance NN O
metrics metric NNS O
</code></pre>

<p>So the question to your question should be:</p>

<ul>
<li><strong>What is the ultimate aim of your NLP application?</strong></li>
<li><strong>Why is your input lower-cased? Was it your doing or how the data was provided?</strong></li>
</ul>

<p>And after answering those questions, you can move on to decide what you really want to do with the NER tags, i.e.</p>

<ul>
<li><p><strong>If the input is lower-cased and it's because of how you structured your NLP tool chain</strong>, then</p>

<ul>
<li><strong>DO NOT do that!!!</strong> Perform the NER on the normal text without distortions you've created. It's because the NER was trained on normal text so it won't really work out of the context of normal text.</li>
<li>Also try to not mix it NLP tools from different suites they will usually not play nice, especially at the end of your NLP tool chain</li>
</ul></li>
<li><p><strong>If the input is lower-cased because that's how the original data was</strong>, then: </p>

<ul>
<li>Annotate a small portion of the data, or find annotated data that was lowercased and then retrain a model.</li>
<li>Work around it and train a truecaser with normal text then apply the truecasing model to the lower-cased text. See <a href=""https://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf"" rel=""nofollow noreferrer"">https://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf</a></li>
</ul></li>
<li><p><strong>If the input has erroneous casing, e.g. `Some big Some Small but not all are Proper Noun</strong>, then</p>

<ul>
<li>Try the truecasing solution too.</li>
</ul></li>
</ul>
",7,8,7004,2015-12-23 15:47:02,https://stackoverflow.com/questions/34439208/nltk-stanfordnertagger-how-to-get-proper-nouns-without-capitalization
Stanford coreNLP sentiment without splitting sentences,"<p>I have files I'm feeding to coreNLP's sentiment tagger.  I have already broken the files up into individual sentences and thus want to return one tag per file.  How can I make the java command return one tag.</p>

<p>The command looks like this <code>java -cp ""*"" -mx5g edu.stanford.nlp.sentiment.SentimentPipeline -stdin</code> and outputs as follows:</p>

<pre><code>Annotation pipeline timing information:
TokenizerAnnotator: 0.0 sec.
WordsToSentencesAnnotator: 0.0 sec.
TOTAL: 0.0 sec. for 8 tokens at 296.3 tokens/sec.
Pipeline setup: 0.0 sec.
Total time for StanfordCoreNLP pipeline: 8.7 sec.

C:\stanford-corenlp-full-2015-04-20&gt;java -cp ""*"" -mx5g edu.stanford.nlp.sentiment.SentimentPipeline -stdin
Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.4 sec].
Adding annotator sentiment
Reading in text from stdin.
Please enter one sentence per line.
Processing will end when EOF is reached.

Computer is fun. Not too fun.
  Positive
  Neutral
</code></pre>

<p>How could I make the output a single tag similar to what I did below by removing the punctuation:</p>

<pre><code>Computer is fun Not too fun.
  Positive  
</code></pre>

<p>It seems I should be able to do this easily since there is the <code>-ssplit.isOneSentence</code> and to my understanding the sentiment tagger uses <code>ssplit</code> but I don't know how to rework my command to incorporate it (I have read <a href=""http://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow"">command line documentation</a>).</p>
","java, stanford-nlp","<p>It looks like there was a bug in <code>SentimentPipeline</code> as it shouldn't split sentences within a line when you use the <code>-stdin</code> option. I fixed that now but unless you compile your own version, this won't help you until we release the next version of CoreNLP.</p>

<p>But there is also an alternative (and presumably better) way to get sentiment labels for sentences using a CoreNLP pipeline.</p>

<p>The following command runs the same code as your command but at the same time it allows you to specify more options (including the <code>-ssplit.eolonly</code> option) for the individual annotators. </p>

<pre><code>java -cp ""*"" -mx5g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,parse,sentiment"" -ssplit.eolonly
</code></pre>
",2,1,647,2015-12-27 19:24:34,https://stackoverflow.com/questions/34483978/stanford-corenlp-sentiment-without-splitting-sentences
Train Stanford postagger model,"<p>I am creating a custom post tagger model for Italian language.</p>

<p>I get an error running the command to train the model:</p>

<pre><code>damiano@damiano:~/stanford-postagger$ java -classpath stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -prop /home/damiano/modelli/italian.tagger.props -model italian.tagger -trainFile italian.tagger.train
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
    at edu.stanford.nlp.io.IOUtils.&lt;clinit&gt;(IOUtils.java:41)
    at edu.stanford.nlp.util.StringUtils.argsToProperties(StringUtils.java:938)
    at edu.stanford.nlp.util.StringUtils.argsToProperties(StringUtils.java:891)
    at edu.stanford.nlp.tagger.maxent.TaggerConfig.&lt;init&gt;(TaggerConfig.java:128)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1836)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 5 more
</code></pre>

<p>I found this command inside the README.txt</p>

<p>I also tried with <strong>-classpath stanford-postagger-3.6.0.jar</strong> but I had the same problem. </p>

<p>This is the folder content:</p>

<pre><code>damiano@damiano:~/stanford-postagger$ dir
build.xml  LICENSE.txt  sample-input.txt          stanford-postagger-3.6.0-javadoc.jar  stanford-postagger-gui.bat  stanford-postagger.sh
data       models   sample-output.txt         stanford-postagger-3.6.0-sources.jar  stanford-postagger-gui.sh   TaggerDemo2.java
lib    README.txt   stanford-postagger-3.6.0.jar  stanford-postagger.bat            stanford-postagger.jar  TaggerDemo.java
</code></pre>

<p>What can I do?</p>

<p><strong>EDIT</strong></p>

<p>This is my prop file:</p>

<pre><code>model = /home/damiano/modelli/italian.tagger
arch = generic,suffix(4),prefix(4),unicodeshapes(-1,1),unicodeshapeconjunction(-1,1),words(-2,-2),words(2,2)
trainFile = /home/damiano/modelli/italian.tagger.train
tagSeparator = _
encoding = utf-8
iterations = 100
openClassTags = B BN CC CS DD DE DI DQ DR E EA FB FC FF FS I N PC PD PE PI PP PQ PR RD RI T SA SP XH XM XE XX Ss Sp Sn As Ap An APs APp APn NOs NOp NOn SWs SWp SWn Vip Vip3 Vii Vii3 Vis Vis3 Vif Vif3 Vcp Vcp3 Vci Vci3 Vdp Vdp3 Vg Vp Vf Vm VAip VAip3 VAii VAii3 VAis Vis3 VAif VAif3 VAcp VAcp3 VAci VAci3 VAdp VAdp3 VAg VAp VAf VAm VMip VMip3 VMii VMii3 VMis VMis3 VMif VMif3 VMcp VMcp3 VMci VMci3 VMdp VMdp3 VMg VMp VMf VMm
tokenize = false
</code></pre>
","java, nlp, stanford-nlp","<p>In version 3.6, we started using slf4j as a logging facade, but we unfortunately haven't yet updated a lot of documentation.... The slf4j jars are in the ""lib"" subdirectory.  Try the following command:</p>

<pre><code>$ java -classpath ""stanford-postagger.jar:lib/*"" edu.stanford.nlp.tagger.maxent.MaxentTagger -prop /home/damiano/modelli/italian.tagger.props -model italian.tagger -trainFile italian.tagger.train
</code></pre>
",3,0,831,2015-12-27 23:46:33,https://stackoverflow.com/questions/34486092/train-stanford-postagger-model
What does NER model to find person names inside a resume/CV?,"<p>i just have started with Stanford CoreNLP, I would like to build a custom NER model to find <strong>persons</strong>.</p>

<p>Unfortunately, I did not find a good ner model for italian. I need to find these entities inside a resume/CV document.</p>

<p>The problem here is that document like those can have different structure, for example i can have:</p>

<p><strong>CASE 1</strong></p>

<pre><code>- Name: John

- Surname: Travolta

- Last name: Travolta

- Full name: John Travolta

(so many labels that can represent the entity of the person i need to extract)
</code></pre>

<p><strong>CASE 2</strong></p>

<pre><code>My name is John Travolta and I was born ...
</code></pre>

<p>Basically, i can have structured data (with different labels) or a context where i should find these entities.</p>

<p>What is the best approach for this kind of documents? Can a maxent model work in this case?</p>

<hr>

<h2><strong>EDIT @vihari-piratla</strong></h2>

<p>At the moment, i adopt the strategy to find a pattern that has something on the left and something on the right, following this method i have 80/85% to find the entity.</p>

<p>Example:</p>

<pre><code>Name: John
Birthdate: 2000-01-01
</code></pre>

<p>It means that i have ""Name:"" on the left of the pattern and a <strong>\n</strong> on the right (until it finds the <strong>\n</strong>).
I can create a very long list of patterns like those. I thought about patterns because i do not need names inside ""other"" context. </p>

<p>For example, if the user writes other names inside a <strong>job experience</strong> i do not need them. Because i am looking for the personal name, not others. With this method i can reduce false positives because i will look at specific patterns not ""general names"".</p>

<p>A problem with this method is that i have a big list of patterns (1 pattern = 1 regex), so it does not scale so well if i add others.</p>

<p>If i can train a NER model with all those patterns it will be awesome, but i should use tons of documents to train it well.</p>
","nlp, stanford-nlp, named-entity-recognition","<p>The first case could be trivial, and I agree with Ozborn's suggestion.</p>

<p>I would like to make a few suggestions for case-2.<br>
Stanford NLP provides an excellent English name recognizer, but may not be able to find all the person names. OpenNLP also gives a decent performance, but much lesser than Stanford. There are many other entity recognizers available for English. I will focus here on StanfordNLP, here are a few things to consider.</p>

<ol>
<li><p>Gazettes. You can provide the model with a list of names and also customize how the Gazette entries are matched. Stanford also provides a sloppy match option when set, will allow partial matches with the Gazette entries. Partial matches should work well with the person names. </p></li>
<li><p>Stanford recognizes entities constructively. If in a document, a name like ""John Travolta"" is recognized, then it would also get ""Travolta"" in the same document even if it had no prior idea about ""Travolta"". So, append as much information to the document as possible. Add the names recognized in case-1, in a familiar context like ""My name is John Travolta."" if ""John Travolta"" is recognized by the rules employed in case-1. Adding dummy sentences can improve the recall.</p></li>
</ol>

<p>Making a benchmark for training is a very costly and boring process; you should annotate in the order of tens of thousands of sentences for decent test performance. I am sure that even if you have a model trained on annotated training data, the performance won't be any better than when you have the two steps above implemented. </p>

<p><strong>@edit</strong></p>

<p>Since the asker of this question is interested in unsupervised pattern-based approaches, I am expanding my answer to discuss these.</p>

<p>When supervised data is not available, a method called bootstrapped pattern-learning approach is generally used. The algorithm starts with a small set of seed instances of interest (like a list of books) and outputs more instances of the same type.<br>
Refer the following resources for more information<br></p>

<ul>
<li><a href=""http://nlp.stanford.edu/software/patternslearning.shtml"" rel=""nofollow"">SPIED</a> is a software that uses the above-described technique and is available for download and use.</li>
<li><a href=""http://www.cs.stanford.edu/people/sonal/"" rel=""nofollow"">Sonal Gupta</a> received Ph.D. on this topic, her dissertation is available <a href=""http://www.cs.stanford.edu/people/sonal/sonalgupta_dissertation.pdf"" rel=""nofollow"">here</a>.</li>
<li>For a light introduction on this topic, see these <a href=""http://nlp.stanford.edu/pubs/Gupta_Manning_CoNLL14_slides.pdf"" rel=""nofollow"">slides</a>.</li>
</ul>

<p>Thanks</p>
",7,10,4275,2015-12-28 23:54:43,https://stackoverflow.com/questions/34502517/what-does-ner-model-to-find-person-names-inside-a-resume-cv
How to reproduce the results of Stanford neural parser?,"<p>I would like to run <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">Stanford neural dependency parser</a> which has very impressive performance like 92.0% UAS, 89.7% LAS (Chen &amp; Manning, 2014). I tried to follow their instructions but got sad numbers: 66.2% UAS, 62.0% LAS. Could somebody please tell me what I did wrong?</p>

<p>The commands:</p>

<pre><code>PENN_TEST_PATH=""test.mrg""
CONLL_TEST_PATH=""$PENN_TEST_PATH.dep""

cat penntree/23/* &gt; $PENN_TEST_PATH  

java -cp stanford-parser-full-2014-10-31/stanford-parser.jar edu.stanford.nlp.trees.EnglishGrammaticalStructure -originalDependencies -conllx -treeFile $PENN_TEST_PATH &gt; $CONLL_TEST_PATH
java -cp stanford-parser-full-2014-10-31/stanford-parser.jar edu.stanford.nlp.parser.nndep.DependencyParser -model stanford-parser-full-2014-10-31/PTB_Stanford_params.txt.gz -testFile $CONLL_TEST_PATH
</code></pre>

<p>Output:</p>

<pre><code>Loading depparse model file: stanford-parser-full-2014-10-31/PTB_Stanford_params.txt.gz ...
dict=44392
pos=48
label=46
embeddingSize=50
hiddenSize=200
numTokens=48
preComputed=422468
###################
#Transitions: 91
#Labels: 45
ROOTLABEL: root
PreComputed 100000, Elapsed Time: 1.789 (s)
Initializing dependency parser done [2.6 sec].
Test File: test.mrg.dep
UAS = 66.2110
LAS = 62.0160
DependencyParser tagged 56684 words in 2416 sentences in 3.4s at 16559.7 w/s, 705.8 sent/s.
</code></pre>

<h3>References</h3>

<p>Chen, D., &amp; Manning, C. (2014). A Fast and Accurate Dependency Parser using Neural Networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 740–750). Doha, Qatar: Association for Computational Linguistics.</p>
",stanford-nlp,"<p>I found the problem. I need to call <code>edu.stanford.nlp.trees.EnglishGrammaticalStructure</code> with <code>-basic</code> option.</p>
",2,0,243,2015-12-30 22:11:49,https://stackoverflow.com/questions/34538039/how-to-reproduce-the-results-of-stanford-neural-parser
How to use quote annotator,"<p>Running</p>

<blockquote>
  <p>./corenlp.sh  -annotators quote -outputFormat xml -file input.txt</p>
</blockquote>

<p>on the modified input file</p>

<blockquote>
  <p>""Stanford University"" is located in California. It is a great university, founded in 1891.</p>
</blockquote>

<p>yields the following output:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;?xml-stylesheet href=""CoreNLP-to-HTML.xsl"" type=""text/xsl""?&gt;
&lt;root&gt;
  &lt;document&gt;
    &lt;sentences/&gt;
  &lt;/document&gt;
&lt;/root&gt;
</code></pre>

<p>Maybe I misunderstood the intended use of this annotator, but I expected it to mark the parts of the sentence that is between the "".</p>

<p>When I run the script with the ""usual"" annotators tokenize,ssplit,pos,lemma,ner, they are all working well, but adding quote does not change the output. I use the stanford-corenlp-full-2015-12-09 release.
How can I use the quote annotator and what is it meant to do?</p>
",stanford-nlp,"<p>If you build a StanfordCoreNLP object in Java code and run it with the quote annotator, the final Annotation object will have the quotes.</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.ling.CoreAnnotations.*;
import edu.stanford.nlp.util.*;

public class PipelineExample {

    public static void main (String[] args) throws IOException {
        // build pipeline
        Properties props = new Properties();
        props.setProperty(""annotators"",""tokenize, ssplit, quote"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        String text = ""\""Stanford University\"" is located in California. It is a great university, founded in 1891."";
        Annotation annotation = new Annotation(text);
        pipeline.annotate(annotation);
        System.out.println(annotation.get(CoreAnnotations.QuotationsAnnotation.class));
    }
}
</code></pre>

<p>Currently none of the outputters (json, xml, text, etc...) output the quotes.  I'll make a note we should add this to the output for future versions.</p>
",4,3,1189,2016-01-03 21:38:55,https://stackoverflow.com/questions/34581916/how-to-use-quote-annotator
Python NLTK : Extract lexical head item from Stanford dependency parsed result,"<p>I have a sentence and i want to extract lexical head item, i could do the dependency parsing using Stanford NLP library.</p>

<p><strong>How can i extract main head head in a sentence?</strong> </p>

<p>In the case of the sentence <code>Download and share this tool</code>, the head would be <code>Download</code>.</p>

<p>I've tried the following:</p>

<pre><code> def get_head_word(text):
     standepparse=StanfordDependencyParser(path_to_jar='/home/stanford_resource/stanford-parser-full-2014-06-16/stanford-parser.jar',path_to_models_jar='/home/stanford_resource/stanford-parser-full-2014-06-16/stanford-parser-3.4-models.jar',model_path='/home/stanford_resource/stanford-parser-full-2014-06-16/stanford-parser-3.4-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')
     parsetree=standepparse.raw_parse(text)
     p_tree=list(parsetree)[0]
     print p_tree.to_dot()

 text = 'Download and share this tool'
 get_head_word(text)


output:

digraph G{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (None)""]
0 -&gt; 1 [label=""root""]
1 [label=""1 (Download)""]
1 -&gt; 2 [label=""cc""]
1 -&gt; 3 [label=""conj""]
1 -&gt; 5 [label=""dobj""]
2 [label=""2 (and)""]
3 [label=""3 (share)""]
4 [label=""4 (this)""]
5 [label=""5 (software)""]
5 -&gt; 4 [label=""det""]
}
</code></pre>
","python, nlp, nltk, stanford-nlp, dependency-parsing","<p>To find the dependency head of sentence, simply look for nodes that whose <code>head</code> values points to the <code>root</code> node. In <code>NLTK</code> API to <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/dependencygraph.py"" rel=""nofollow"">DependencyGraph</a>, you can easily look for the node that its head points to the 1st index of the dictionary. </p>

<p>Do note that in dependency parsing unlike typical chomsky normal form / CFG parse trees there might be more than one head to the dependency parse.</p>

<p>But since you're casting the dependency output into a Tree structure, you can do the following:</p>

<pre><code>tree_head = next(n for n in p_tree.node_values() if n['head'] == 1)
</code></pre>

<p>But do note that linguistically, the head in the sentence<code>Download and share this tool</code> should be <code>Download</code> <strong>and</strong> <code>share</code>. But computationally a tree is hierarchical and a normal-form tree would have <code>ROOT-&gt;Download-&gt;and-&gt;share</code> but some parsers might produce this tree too: <code>ROOT-&gt;and-&gt;Download;share</code></p>
",1,2,901,2016-01-04 08:37:25,https://stackoverflow.com/questions/34587293/python-nltk-extract-lexical-head-item-from-stanford-dependency-parsed-result
How can I compare Stanford Core NLP with an algorithm without the trained dataset for the movies?,"<p>I am looking to use the Stanford Core NLP which is trained for movie reviews. I want to compare it with a regular sentiment analysis algorithm which is untrained for movie reviews. Is there a way to use the Stanford Core NLP <a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">Link Here</a> without the trained dataset for the movie reviews or is there any other source I can use to compare directly? </p>
","java, nlp, stanford-nlp, sentiment-analysis","<p>It's unclear what you mean by CoreNLP Sentiment without a trained movie review dataset. The Sentiment model needs some dataset to train on -- untrained it will simply not do anything. It is possible to train it on a dataset which is not movie reviews, but for this you'd have to find or create a suitable dataset. </p>
",0,0,672,2016-01-05 09:11:21,https://stackoverflow.com/questions/34607867/how-can-i-compare-stanford-core-nlp-with-an-algorithm-without-the-trained-datase
Result Difference in Stanford NER tagger NLTK (python) vs JAVA,"<p>I am using both python and java to run the Stanford NER tagger but I am seeing the difference in the results.</p>

<p>For example, when I input the sentence ""Involved in all aspects of data modeling using ERwin as the primary software for this."",</p>

<p>JAVA Result:</p>

<pre><code>""ERwin"": ""PERSON""
</code></pre>

<p>Python Result:</p>

<pre><code>In [6]: NERTagger.tag(""Involved in all aspects of data modeling using ERwin as the primary software for this."".split())
Out [6]:[(u'Involved', u'O'),
 (u'in', u'O'),
 (u'all', u'O'),
 (u'aspects', u'O'),
 (u'of', u'O'),
 (u'data', u'O'),
 (u'modeling', u'O'),
 (u'using', u'O'),
 (u'ERwin', u'O'),
 (u'as', u'O'),
 (u'the', u'O'),
 (u'primary', u'O'),
 (u'software', u'O'),
 (u'for', u'O'),
 (u'this.', u'O')]
</code></pre>

<p>Python nltk wrapper can't catch ""ERwin"" as PERSON.</p>

<p>What's interesting here is both Python and Java uses the same trained data (english.all.3class.caseless.distsim.crf.ser.gz) released in 2015-04-20.</p>

<p>My ultimate goal is to make python work in the same way Java does.</p>

<p>I'm looking at StanfordNERTagger in nltk.tag to see if there's anything I can modify. Below is the wrapper code:</p>

<pre><code>class StanfordNERTagger(StanfordTagger):
""""""
A class for Named-Entity Tagging with Stanford Tagger. The input is the paths to:

- a model trained on training data
- (optionally) the path to the stanford tagger jar file. If not specified here,
  then this jar file must be specified in the CLASSPATH envinroment variable.
- (optionally) the encoding of the training data (default: UTF-8)

Example:

    &gt;&gt;&gt; from nltk.tag import StanfordNERTagger
    &gt;&gt;&gt; st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') # doctest: +SKIP
    &gt;&gt;&gt; st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) # doctest: +SKIP
    [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),
     ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),
     ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]
""""""

_SEPARATOR = '/'
_JAR = 'stanford-ner.jar'
_FORMAT = 'slashTags'

def __init__(self, *args, **kwargs):
    super(StanfordNERTagger, self).__init__(*args, **kwargs)

@property
def _cmd(self):
    # Adding -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions tokenizeNLs=false for not using stanford Tokenizer  
    return ['edu.stanford.nlp.ie.crf.CRFClassifier',
            '-loadClassifier', self._stanford_model, '-textFile',
            self._input_file_path, '-outputFormat', self._FORMAT, '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions','\""tokenizeNLs=false\""']

def parse_output(self, text, sentences):
    if self._FORMAT == 'slashTags':
        # Joint together to a big list    
        tagged_sentences = []
        for tagged_sentence in text.strip().split(""\n""):
            for tagged_word in tagged_sentence.strip().split():
                word_tags = tagged_word.strip().split(self._SEPARATOR)
                tagged_sentences.append((''.join(word_tags[:-1]), word_tags[-1]))

        # Separate it according to the input
        result = []
        start = 0 
        for sent in sentences:
            result.append(tagged_sentences[start:start + len(sent)])
            start += len(sent);
        return result 

    raise NotImplementedError
</code></pre>

<p>Or, if it's because of using different Classifier (In java code, it seems to use AbstractSequenceClassifier, on the other hand, python nltk wrapper uses the CRFClassifier.) is there a way that I can use AbstractSequenceClassifier in python wrapper?</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>Try setting <code>maxAdditionalKnownLCWords</code> to 0 in the properties file (or command line) for CoreNLP, and if possible for NLTK as well. This disables an option which allows the NER system to learn from test-time data a little bit, which could cause occasional mildly different results.</p>
",5,6,1070,2016-01-06 05:56:47,https://stackoverflow.com/questions/34626555/result-difference-in-stanford-ner-tagger-nltk-python-vs-java
pronoun resolution backwards,"<p>The usual coreference resolution works in the following way:</p>

<p>Provided</p>

<pre><code>The man likes math. He really does.
</code></pre>

<p>it figures out that </p>

<pre><code>he 
</code></pre>

<p>refers to </p>

<pre><code>the man.
</code></pre>

<p>There are plenty of tools to do this.</p>

<p>However, is there a way to do it backwards? </p>

<p>For example,</p>

<p>given</p>

<pre><code>The man likes math. The man really does.
</code></pre>

<p>I want to do the pronoun resolution ""backwards,""</p>

<p>so that I get an output like</p>

<pre><code>The man likes math. He really does.
</code></pre>

<p>My input text will mostly be 3~10 sentences, and I'm working with python.</p>
","python, nlp, nltk, stanford-nlp","<p>This is perhaps not really an answer to be happy with, but I think the answer is that there's no such functionality built in anywhere, though you can code it yourself without too much difficulty. Giving an outline of how I'd do it with CoreNLP:</p>

<ol>
<li><p>Still run coref. This'll tell you that ""the man"" and ""the man"" are coreferent, and so you can replace the second one with a pronoun.</p></li>
<li><p>Run the <code>gender</code> annotator from CoreNLP. This is a poorly-documented and even more poorly advertised annotator that tries to attach gender to tokens in a sentence.</p></li>
<li><p>Somehow figure out plurals. Most of the time you could use the part-of-speech tag: plural nouns get the tags NNS or NNPS, but there are some complications so you might also want to consider (1) the existence of conjunctions in the antecedent; (2) the lemma of a word being different from its text; (3) especially in conjunction with 2, the word ending in 's' or 'es' -- this can distinguish between lemmatizations which strip out plurals versus lemmatizations which strip out tenses, etc.</p></li>
<li><p>This is enough to figure out the right pronoun. Now it's just a matter of chopping up the sentence and putting it back together. This is a bit of a pain if you do it in CoreNLP -- the code is just not set up to change the text of a sentence -- but in the worst case you can always just re-annotate a new surface form.</p></li>
</ol>

<p>Hope this helps somewhat!</p>
",9,24,1399,2016-01-06 08:00:09,https://stackoverflow.com/questions/34628224/pronoun-resolution-backwards
CoreNLP 3.6 maven central release,"<p>According to <a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/download.html</a>, the latest version of coreNLP should be available in maven central ""several days"" after available in the download page. In <a href=""https://stanfordnlp.github.io/CoreNLP/history.html"" rel=""nofollow"">https://stanfordnlp.github.io/CoreNLP/history.html</a>, I can see that 3.6 went out in early december.</p>

<p>When can I expect a maven central release of CoreNLP 3.6?</p>
",stanford-nlp,"<p>The Maven release of 3.6.0 should be within the next week.</p>
",1,0,84,2016-01-09 15:51:51,https://stackoverflow.com/questions/34695562/corenlp-3-6-maven-central-release
Installing coreNLP in R,"<p>I'm following the instructions on this link to use coreNLP
<a href=""https://github.com/statsmaths/coreNLP"" rel=""nofollow"">https://github.com/statsmaths/coreNLP</a></p>

<p>However, I found this error</p>

<pre><code>&gt; library(coreNLP)

Error in get(method, envir = home) : 
lazy-load database '/Users/apple/Library/R/3.2/library/coreNLP/R/coreNLP.rdb is  corrupt
In addition: Warning messages:
 1: In .registerS3method(fin[i, 1], fin[i, 2], fin[i, 3], fin[i, 4],  :
 restarting interrupted promise evaluation
 2: In get(method, envir = home) :
 restarting interrupted promise evaluation
 3: In get(method, envir = home) : internal error -3 in R_decompress1
 Error: package or namespace load failed for ‘coreNLP’
</code></pre>
","r, nlp, stanford-nlp, devtools, r-package","<p><strong>After encountering the <code>java.lang.UnsupportedClassVersionError: edu/stanford/nlp/pipeline/StanfordCoreNLP : Unsupported major.minor version 52.0</code> error message:</strong></p>

<p>You need to </p>

<ul>
<li>install java 8, (as superuser), </li>
<li>change the default jvm the operating system uses to this jvm (* see below), </li>
<li>run <code>R CMD javareconf</code> on the command line, and then </li>
<li>set the environment variable LD_LIBRARY_PATH to the directory where libjvm.so is stored. </li>
<li><p>restart R / RStudio</p></li>
<li><p>make sure that a swap file (or swap partition) exists on your machine. call <code>free</code> to check if there is a line in the output that starts with <code>swap</code> and the values on that line are not zero. </p></li>
</ul>

<p>I use ubuntu, my java 8 libjvm.so is here: <code>/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so</code></p>

<p>You can do this in your .Rprofile file. Add this line, perhaps at the bottom of the file:</p>

<p><code>Sys.setenv(LD_LIBRARY_PATH=paste0(Sys.getenv(""LD_LIBRARY_PATH""), "":"", ""/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/"" ))</code></p>

<p>When I do this in R:</p>

<pre><code>R&gt; Sys.getenv(""LD_LIBRARY_PATH"")
[1] ""/usr/local/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/""
R&gt; library(coreNLP)
R&gt; initCoreNLP()
</code></pre>

<p>I get this result:</p>

<pre><code>Searching for resource: config.properties
Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.1 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [5.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.1 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [3.8 sec].
Initializing JollyDayHoliday for SUTime from classpath: edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec].
Adding annotator dcoref
Adding annotator sentiment

R&gt; example(getSentiment)

gtSntmR&gt; getSentiment(annoEtranger) # first Sentence of L'Etranger by A.Camus
  id sentimentValue sentiment
1  1              1  Negative
2  2              2   Neutral

gtSntmR&gt; getSentiment(annoHp) # first Sentence of Harry Potter V1
  id sentimentValue    sentiment
1  1              4 Verypositive
</code></pre>

<p><em>(*) How to see the default jvm on Linux:</em></p>

<pre><code>update-alternatives --display java
</code></pre>

<p>Result</p>

<pre><code>java - auto mode
  link currently points to /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
</code></pre>

<p>To show all available alternatives, use</p>

<pre><code>update-alternatives --list java
</code></pre>

<p>Result (on my machine):</p>

<pre><code>/usr/lib/jvm/java-6-openjdk-amd64/jre/bin/java
/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java
/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
</code></pre>

<p>Change alternatives: </p>

<pre><code>sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
</code></pre>

<p>Just play a bit with update-alternatives.</p>
",4,2,5741,2016-01-11 07:29:59,https://stackoverflow.com/questions/34716207/installing-corenlp-in-r
Stanford NLP - invalid character constants,"<p>I created a project in Eclipse, imported the packages contained in <code>stanford-corenlp-3.6.0.jar</code> and after adding the required JARs I got a few errors, all of which are ""Invalid character constants"". Any approaches to fix this? I'm only analyzing English text. The errors were in the following files:</p>

<ul>
<li><code>ChineseUtils.java</code> line 366</li>
<li><code>Sentence.java</code> line 109</li>
<li><code>State.java</code> line 180</li>
<li><code>WordShapeClassifier.java</code> line 511</li>
</ul>
","java, stanford-nlp","<p>Stanford NLP source code uses non-ASCII Unicode characters. You need to set up Eclipse or just this project so that it is expecting UTF-8 source code.  Here are <a href=""http://stijndewitt.com/2010/05/05/unicode-utf-8-in-eclipse-java/"" rel=""nofollow"">some instructions</a>.</p>
",1,2,118,2016-01-11 20:29:09,https://stackoverflow.com/questions/34730486/stanford-nlp-invalid-character-constants
Unable to load CoreNLP Shift-Reduce model into CoreNLP jar,"<p>I don't understand how to load CoreNLP's Shift-Reduce Constituency Parser (SRCP) from my java app.</p>

<p>I'm using Apache Maven to manage my project's dependencies. Per the docs, the SRCP model is not bundled with CoreNLP, so I have downloaded stanford-srparser-2014-10-23-models.jar separately (<a href=""http://nlp.stanford.edu/software/srparser.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/srparser.shtml</a>) and placed that file in:</p>

<pre><code>~/.m2/repository/edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-srparser-2014-10-23-models.jar 
</code></pre>

<p>That is the same directory as the core dependency jar </p>

<pre><code>~/.m2/repository/edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar
</code></pre>

<p>Here is the relevant portion of my project's pom.xml:</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.5.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.5.2&lt;/version&gt;
        &lt;classifier&gt;models&lt;/classifier&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Compiling is successful:</p>

<pre><code>mvn clean compile
</code></pre>

<p>But when I try to load the app, I receive:</p>

<pre><code>java.lang.reflect.InvocationTargetException
...
Caused by: edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"" as either class path, filename or URL
</code></pre>

<p>I unzipped the compiled project war, and ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"" is not present.</p>

<p>Here is how I'm calling the model in my app:</p>

<pre><code>// Initialize a CoreNLP pipeline
public static Properties props = new Properties();
public static StanfordCoreNLP pipeline;

// Set the CoreNLP pipeline annotators.
props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>How can I update my Maven config to force my CoreNLP dependency to include the srparser model? Keep in mind that I need this configuration to run in other developers' environments, so the solution should be clean and reusable if possible.</p>

<p>Thanks!</p>

<p>EDIT:</p>

<p>In response to @jah's comment, below are the results of <code>mvn dependency:tree</code>. The build succeeds, but the srparser model is not compiled/present:</p>

<pre><code>[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ [REDACTED] ---

Downloading:

...

[INFO] com.[REDACTED].nlp:nlp:war:0.1.0
[INFO] +- com.strategicgains:RestExpress:jar:0.11.2:compile
[INFO] |  +- com.strategicgains:RestExpress-Common:jar:0.11.2:compile
[INFO] |  +- com.strategicgains:DateAdapterJ:jar:1.1.4:compile
[INFO] |  +- com.thoughtworks.xstream:xstream:jar:1.4.7:compile
[INFO] |  |  +- xmlpull:xmlpull:jar:1.1.3.1:compile
[INFO] |  |  \- xpp3:xpp3_min:jar:1.1.4c:compile
[INFO] |  +- io.netty:netty-all:jar:4.0.29.Final:compile
[INFO] |  +- org.owasp.encoder:encoder:jar:1.1.1:compile
[INFO] |  \- com.jcraft:jzlib:jar:1.1.3:compile
[INFO] +- junit:junit:jar:4.11:test
[INFO] |  \- org.hamcrest:hamcrest-core:jar:1.3:test
[INFO] +- edu.stanford.nlp:stanford-corenlp:jar:3.5.2:compile
[INFO] |  +- com.io7m.xom:xom:jar:1.2.10:compile
[INFO] |  |  +- xml-apis:xml-apis:jar:1.3.03:compile
[INFO] |  |  +- xerces:xercesImpl:jar:2.8.0:compile
[INFO] |  |  \- xalan:xalan:jar:2.7.0:compile
[INFO] |  +- joda-time:joda-time:jar:2.1:compile
[INFO] |  +- de.jollyday:jollyday:jar:0.4.7:compile
[INFO] |  |  \- javax.xml.bind:jaxb-api:jar:2.2.7:compile
[INFO] |  +- com.googlecode.efficient-java-matrix-library:ejml:jar:0.23:compile
[INFO] |  \- javax.json:javax.json-api:jar:1.0:compile
[INFO] +- edu.stanford.nlp:stanford-corenlp:jar:models:3.5.2:compile
[INFO] +- org.json:json:jar:20151123:compile
[INFO] +- com.fasterxml.jackson.core:jackson-databind:jar:2.6.4:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.6.0:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-core:jar:2.6.4:compile
[INFO] \- commons-io:commons-io:jar:1.3.2:compile
</code></pre>
","java, maven, stanford-nlp","<p>First, download the srparser jar and place it in your project root: <a href=""http://nlp.stanford.edu/software/stanford-srparser-2014-10-23-models.jar"" rel=""nofollow"">http://nlp.stanford.edu/software/stanford-srparser-2014-10-23-models.jar</a></p>

<p>Second, from the project root, execute the following command to install the srparser model dependency via Maven:</p>

<pre><code>mvn install:install-file -Dfile=stanford-srparser-2014-10-23-models.jar -DgroupId=edu.stanford.nlp -DartifactId=stanford-srparser -Dversion=3.5.2 -Dpackaging=jar
</code></pre>

<p>Note the custom artifactId and lack of classifier in the command -- this is to prevent namespace confusion with the other CoreNLP modules.</p>

<p>Third, add the dependency to the Maven project's pom.xml:</p>

<pre><code>&lt;dependencies&gt;
...
    &lt;dependency&gt;
         &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
         &lt;artifactId&gt;stanford-srparser&lt;/a‌​rtifactId&gt;
         &lt;version&gt;3.5.2&lt;/version&gt;
    &lt;/dependency&gt;
...
&lt;/dependencies&gt;
</code></pre>

<p>Finally, clean install:</p>

<pre><code>mvn clean install
</code></pre>

<p>If you continue to experience issues, it may be helpful to clear your Maven dependencies:</p>

<pre><code>mvn dependency:purge-local-repository
</code></pre>

<p>And don't forget to add the download/install commands to your project README/environment bootstrap file!</p>

<p>(Thanks for your help @jah and @GaborAngeli.)</p>
",4,1,1247,2016-01-13 22:27:00,https://stackoverflow.com/questions/34778266/unable-to-load-corenlp-shift-reduce-model-into-corenlp-jar
Stanford OpenIE with Pronoun Coreference Option,"<p>I was trying to run the OpenIE module through command line with the resolve_coref option, but was getting the following error:</p>

<pre><code>[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.8 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ...
PreComputed 100000, Elapsed Time: 2.091 (s)
Initializing dependency parser done [5.6 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [1.2 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator entitymentions
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
Exception in thread ""main"" java.lang.IllegalArgumentException: annotator ""coref"" requires annotator ""mention""
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:375)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:139)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:135)
at edu.stanford.nlp.naturalli.OpenIE.main(OpenIE.java:697)
</code></pre>

<p>It reports requiring annotator ""mention"", yet it previously added another annotator ""entitymentions"" and there seems to be some aliases resolution issue. On the other hand, I can't find relevant information about the ""mention"" annotator in the documentation.</p>

<p>I used the following command:</p>

<pre><code>java -Xmx20g -cp stanford-corenlp-3.6.0.jar:stanford-corenlp-3.6.0-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar edu.stanford.nlp.naturalli.OpenIE -openie.resolve_coref input.txt
</code></pre>
",stanford-nlp,"<p>Oops; this is indeed a bug. Coref was refactored fairly heavily a few times between the last release and this one, and OpenIE seems to have not kept up with the changes...</p>

<p>It should be fixed in the GitHub version of the code, and will hopefully be incorporated into the Maven version released soon.</p>
",0,0,418,2016-01-13 22:52:32,https://stackoverflow.com/questions/34778597/stanford-openie-with-pronoun-coreference-option
Stanford Relationship training not working,"<p>I am trying to run the relationship trainer as specified at the URL <a href=""http://nlp.stanford.edu/software/relationExtractor.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/relationExtractor.shtml</a> However, it is not able to find the tagger although I have specified the stanford-postagger.jar in the classpath. Any pointers in the right direction on this would be very helpful. </p>

<p>I am running on Windows on the command prompt as given below:</p>

<blockquote>
  <p>D:\01.Jars\Jars_Stanford\stanford-corenlp-full-2015-04-20>java -cp
  ""stanford-ner .jar;stanford-corenlp-3.5.2.jar;stanford-postagger.jar""
  edu.stanford.nlp.ie.mach inereading.MachineReading --arguments
  SuperAnnuation.properties PERCENTAGE OF TRAIN: 1.0 The reader log
  level is set to SEVERE Adding annotator pos Exception in thread ""main""
  java.lang.RuntimeException: edu.stanford.nlp.io.Runti meIOException:
  Unrecoverable error while loading a tagger model
          at edu.stanford.nlp.pipeline.AnnotatorFactories$4.create(AnnotatorFactor
  ies.java:292)
          at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
          at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.j
  ava:289)
          at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java
  :126)
          at edu.stanford.nlp.ie.machinereading.MachineReading.makeMachineReading(
  MachineReading.java:228)
          at edu.stanford.nlp.ie.machinereading.MachineReading.main(MachineReading
  .java:106) Caused by: edu.stanford.nlp.io.RuntimeIOException:
  Unrecoverable error while loa ding a tagger model
          at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTa
  gger.java:770)
          at edu.stanford.nlp.tagger.maxent.MaxentTagger.(MaxentTagger.java:
  298)
          at edu.stanford.nlp.tagger.maxent.MaxentTagger.(MaxentTagger.java:
  263)
          at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnot
  ator.java:97)
          at edu.stanford.nlp.pipeline.POSTaggerAnnotator.(POSTaggerAnnotato
  r.java:77)
          at edu.stanford.nlp.pipeline.AnnotatorImplementations.posTagger(Annotato
  rImplementations.java:59)
          at edu.stanford.nlp.pipeline.AnnotatorFactories$4.create(AnnotatorFactor
  ies.java:290)
          ... 5 more Caused by: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/pos-t
  agger/english-left3words/english-left3words-distsim.tagger"" as either
  class path , filename or URL
          at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSys
  tem(IOUtils.java:481)
          at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTa
  gger.java:765)
          ... 11 more</p>
</blockquote>

<p>And the Superannuation Property file used is given below. This is the default property file provided on the website:</p>

<pre><code>#Below are some basic options. See edu.stanford.nlp.ie.machinereading.MachineReadingProperties class for more options.

# Pipeline options
annotators = pos, lemma, parse
parse.maxlen = 100

# MachineReading properties. You need one class to read the dataset into correct format. See edu.stanford.nlp.ie.machinereading.domains.ace.AceReader for another example.
datasetReaderClass = edu.stanford.nlp.ie.machinereading.domains.roth.RothCONLL04Reader

#Data directory for training. The datasetReaderClass reads data from this path and makes corresponding sentences and annotations.
trainPath = /u/nlp/data/RothCONLL04/conll04.corp

#Whether to crossValidate, that is evaluate, or just train.
crossValidate = false
kfold = 10

#Change this to true if you want to use CoreNLP pipeline generated NER tags. The default model generated with the relation extractor release uses the CoreNLP pipeline provided tags (option set to true).
trainUsePipelineNER=false

# where to save training sentences. uses the file if it exists, otherwise creates it.
serializedTrainingSentencesPath = tmp/roth_sentences.ser

serializedEntityExtractorPath = tmp/roth_entity_model.ser

# where to store the output of the extractor (sentence objects with relations generated by the model). This is what you will use as the model when using 'relation' annotator in the CoreNLP pipeline.
serializedRelationExtractorPath = tmp/roth_relation_model_pipeline.ser

# uncomment to load a serialized model instead of retraining
# loadModel = true

#relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter,edu.stanford.nlp.ie.machinereading.domains.roth.RothResultsByRelation. For printing output of the model.
relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter

#In this domain, this is trivial since all the entities are given (or set using CoreNLP NER tagger).
entityClassifier = edu.stanford.nlp.ie.machinereading.domains.roth.RothEntityExtractor

extractRelations = true
extractEvents = false

#We are setting the entities beforehand so the model does not learn how to extract entities etc.
extractEntities = false

#Opposite of crossValidate. 
trainOnly=true

# The set chosen by feature selection using RothCONLL04:
relationFeatures = arg_words,arg_type,dependency_path_lowlevel,dependency_path_words,surface_path_POS,entities_between_args,full_tree_path

# The above features plus the features used in Bjorne BioNLP09:
# relationFeatures = arg_words,arg_type,dependency_path_lowlevel,dependency_path_words,surface_path_POS,entities_between_args,full_tree_path,dependency_path_POS_unigrams,dependency_path_word_n_grams,dependency_path_POS_n_grams,dependency_path_edge_lowlevel_n_grams,dependency_path_edge-node-edge-grams_lowlevel,dependency_path_node-edge-node-grams_lowlevel,dependency_path_directed_bigrams,dependency_path_edge_unigrams,same_head,entity_counts
</code></pre>
",stanford-nlp,"<p>Try to run with the full Stanford CoreNLP jar, and the associated models jar. These can both be downloaded from the <a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow"">CoreNLP downloads page</a>. Make sure to include both the code jar and the models jar in your classpath!</p>
",1,0,417,2016-01-14 04:30:31,https://stackoverflow.com/questions/34781767/stanford-relationship-training-not-working
Running Stanford corenlp server with custom models,"<p>I've trained a POS tagger and neural dependency parser with Stanford corenlp. I can get them to work via command line, and now would like to access them via a server. </p>

<p>However, the <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow"">documentation</a> for the server doesn't say anything about using custom models. I checked the code and didn't find any obvious way of supplying a configuration file.</p>

<p>Any idea how to do this? I don't need all annotators, just the ones I trained.</p>
",stanford-nlp,"<p>Yes, the server should (in theory) support all the functionality of the regular pipeline. The <code>properties</code> GET parameter is translated into the <code>Properties</code> object you would normally pass into <code>StanfordCoreNLP</code>. Therefore, if you'd like the server to load a custom model, you can just call it via, e.g.:</p>

<pre><code>wget \
  --post-data 'the quick brown fox jumped over the lazy dog' \
  'localhost:9000/?properties={""parse.model"": ""/path/to/model/on/server/computer"", ""annotators"": ""tokenize,ssplit,pos"", ""outputFormat"": ""json""}' -O -
</code></pre>

<p>Note that the server won't garbage-collect this model afterwards though, so if you load too many models there's a good chance you'll run into out-of-memory errors...</p>
",3,2,1194,2016-01-14 05:00:50,https://stackoverflow.com/questions/34782040/running-stanford-corenlp-server-with-custom-models
Stanford OpenIE: How to output dependency path instead of plain text patterns?,"<p>I am looking through the Java source code and wondering if it's easy to modify the system such that the predicate portion of each triple is the dependency path between the two entities instead of the surface form. </p>

<p>Since the natural logic module operates on the dependency trees I suppose there shall be an easy tweak to this demand.</p>

<p>I trace the code in edu.stanford.nlp.naturalli/OpenIE.java to:</p>

<pre><code>// Get the extractions
boolean empty = true;
synchronized (OUTPUT) {
  for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
    for (RelationTriple extraction : sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class)) {
      // Print the extractions
      OUTPUT.println(tripleToString(extraction, docid, sentence));
      empty = false;
    }
  }
}
</code></pre>

<p>Please point me to the implementation of the following step:</p>

<pre><code>sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class)
</code></pre>

<p>Thanks!</p>
",stanford-nlp,"<p>Each relation triple actually does store the dependency structure from which it was generated. Take a look at the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/util/RelationTriple.java#L460"" rel=""nofollow""><code>asDependencyTree()</code> function</a> in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/util/RelationTriple.java"" rel=""nofollow""><code>RelationTriple</code></a>. </p>

<p>Note that this tree is not necessarily a subtree of the original sentence -- e.g., it may be that a subject was moved around to produce a relation triple. If you're looking for a dependency path in the original sentence, you can look up tokens by their <code>IndexAnnotation</code> and compute a dependency path from that.</p>
",0,0,328,2016-01-18 00:41:35,https://stackoverflow.com/questions/34845630/stanford-openie-how-to-output-dependency-path-instead-of-plain-text-patterns
Stanford CoreNLP pipeline coref: parsing some short strings (with few mentions) returns indexoutofbounds exception,"<p><strong>BACKGROUND: I'm importing the Stanford CoreNLP library into my clojure project.  I was using version 3.5.1 but recently jumped directly into version 3.6.0, bypassing 3.5.2.  As part of this update, because I was getting coreference information using the dcoref annotator, I needed to make small modifications so that my program used the coref annotator instead.</strong></p>

<p>In the past (v3.5.1), when I created a pipeline with the following annotators</p>

<p><code>""tokenize, ssplit, pos, lemma, ner, parse, depparse, dcoref, quote, entitymentions""</code>,</p>

<p>I could parse a sentence such as the following without error:</p>

<p>""I ate bread"".</p>

<p>If I remember correctly, extracting the coreference chains from the resulting annotated document would just return an null value, or maybe an empty array.  But that's inconsequential, because at least the annotated document would be created without error.</p>

<p>Now, when I create a pipeline with the following annotators:</p>

<p><code>""tokenize, ssplit, pos, lemma, ner, parse, depparse, mention, coref, quote, entitymentions""</code>,</p>

<p>and then I try to parse that same sentence (or any other sentences with only 1 or 0 ""mentions"") I get an indexoutofboundsexception with the following trace:</p>

<pre><code>actual: java.lang.RuntimeException: Error annotating document with coref
 at edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate (StatisticalCorefSystem.java:79)
    edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate (StatisticalCorefSystem.java:62)
    edu.stanford.nlp.pipeline.CorefAnnotator.annotate (CorefAnnotator.java:100)
    edu.stanford.nlp.pipeline.AnnotationPipeline.annotate (AnnotationPipeline.java:68)
    edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate (StanfordCoreNLP.java:491)
    nlp.core$parse_text.invoke (core.clj:199)
    nlp.focus_scorer.process$lexchain_features.invoke (process.clj:63)
    nlp.focus_scorer.process_test/fn (process_test.clj:49)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    user$eval13163.invoke (form-init7737210093072696705.clj:1)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.eval (Compiler.java:6745)
    clojure.core$eval.invoke (core.clj:3081)
    clojure.main$repl$read_eval_print__7099$fn__7102.invoke (main.clj:240)
    clojure.main$repl$read_eval_print__7099.invoke (main.clj:240)
    clojure.main$repl$fn__7108.invoke (main.clj:258)
    clojure.main$repl.doInvoke (main.clj:258)
    clojure.lang.RestFn.invoke (RestFn.java:1523)
    clojure.tools.nrepl.middleware.interruptible_eval$evaluate$fn__909.invoke (interruptible_eval.clj:58)
    clojure.lang.AFn.applyToHelper (AFn.java:152)
    clojure.lang.AFn.applyTo (AFn.java:144)
    clojure.core$apply.invoke (core.clj:630)
    clojure.core$with_bindings_STAR_.doInvoke (core.clj:1868)
    clojure.lang.RestFn.invoke (RestFn.java:425)
    clojure.tools.nrepl.middleware.interruptible_eval$evaluate.invoke (interruptible_eval.clj:56)
    clojure.tools.nrepl.middleware.interruptible_eval$interruptible_eval$fn__951$fn__954.invoke (interruptible_eval.clj:191)
    clojure.tools.nrepl.middleware.interruptible_eval$run_next$fn__946.invoke (interruptible_eval.clj:159)
    clojure.lang.AFn.run (AFn.java:22)
    java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1142)
    java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:617)
    java.lang.Thread.run (Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
 at java.util.ArrayList$SubList.rangeCheck (ArrayList.java:1217)
    java.util.ArrayList$SubList.get (ArrayList.java:1034)
    edu.stanford.nlp.scoref.Clusterer$State.setClusters (Clusterer.java:349)
    edu.stanford.nlp.scoref.Clusterer$State.&lt;init&gt; (Clusterer.java:322)
    edu.stanford.nlp.scoref.Clusterer.getClusterMerges (Clusterer.java:58)
    edu.stanford.nlp.scoref.ClusteringCorefSystem.runCoref (ClusteringCorefSystem.java:63)
    edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate (StatisticalCorefSystem.java:68)
    edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate (StatisticalCorefSystem.java:62)
    edu.stanford.nlp.pipeline.CorefAnnotator.annotate (CorefAnnotator.java:100)
    edu.stanford.nlp.pipeline.AnnotationPipeline.annotate (AnnotationPipeline.java:68)
    edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate (StanfordCoreNLP.java:491)
    nlp.core$parse_text.invoke (core.clj:199)
    nlp.focus_scorer.process$lexchain_features.invoke (process.clj:63)
    nlp.focus_scorer.process_test/fn (process_test.clj:49)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    user$eval13163.invoke (form-init7737210093072696705.clj:1)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.eval (Compiler.java:6745)
    clojure.core$eval.invoke (core.clj:3081)
    clojure.main$repl$read_eval_print__7099$fn__7102.invoke (main.clj:240)
    clojure.main$repl$read_eval_print__7099.invoke (main.clj:240)
    clojure.main$repl$fn__7108.invoke (main.clj:258)
    clojure.main$repl.doInvoke (main.clj:258)
    clojure.lang.RestFn.invoke (RestFn.java:1523)
    clojure.tools.nrepl.middleware.interruptible_eval$evaluate$fn__909.invoke (interruptible_eval.clj:58)
    clojure.lang.AFn.applyToHelper (AFn.java:152)
    clojure.lang.AFn.applyTo (AFn.java:144)
    clojure.core$apply.invoke (core.clj:630)
    clojure.core$with_bindings_STAR_.doInvoke (core.clj:1868)
    clojure.lang.RestFn.invoke (RestFn.java:425)
    clojure.tools.nrepl.middleware.interruptible_eval$evaluate.invoke (interruptible_eval.clj:56)
clojure.tools.nrepl.middleware.interruptible_eval$interruptible_eval$fn__951$fn__954.invoke (interruptible_eval.clj:191)
    clojure.tools.nrepl.middleware.interruptible_eval$run_next$fn__946.invoke (interruptible_eval.clj:159)
    clojure.lang.AFn.run (AFn.java:22)
    java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1142)
    java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:617)
    java.lang.Thread.run (Thread.java:745)
</code></pre>

<p>Am I possibly doing something wrong?  I realize that the fact that I'm using clojure instead of java might be causing some issue, but I've never had a problem with version 3.5.1.  It would seem that the error is being thrown from the annotation step in edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate, but I'm not sure what I can do about that (other than to have two pipeline objects, one with the coref annotator and one without, parse the sentence without coref, count the mentions, and then parse with coref only if I see more than one mention... which seems a little too much.)</p>
","java, clojure, annotations, indexoutofboundsexception, stanford-nlp","<p>3.6.0 features major changes to coreference.  This issue is a bug in Stanford CoreNLP 3.6.0.  If you re-download the distribution this bug should be fixed in what's up on the site now.  It should also be fixed in the up-coming Maven release.</p>
",1,1,281,2016-01-20 14:19:42,https://stackoverflow.com/questions/34902540/stanford-corenlp-pipeline-coref-parsing-some-short-strings-with-few-mentions
Unexpected Date / DateTime Strings cause exception in Stanford CoreNLP,"<p>According to <a href=""https://github.com/stanfordnlp/CoreNLP/issues/57"" rel=""nofollow"">CoreNLP's Git</a>, the issue has been fixed in some version of CoreNLP, possibly 3.5.1 according to my guess since NER is listed as one of the changed modules in the change notes.  However, 3.5.x requires the jump to Java 1.8 and we are not prepared to do so at the current time.  </p>

<p>Also, disclaimer, I did post to that issue as well, but it may not been seen because the issue has been resolved.  Given that SO is an official forum for support for CoreNLP, I ask here.</p>

<p>So I am asking, what is the change to fix this?  Does it in fact exist in a current version, or is there something else that needs to be done.  I need to fix this without upgrading from the 3.4.1 that I am currently using.</p>

<p>For the record, the string below is supposed to represent Dec 3, 2009 at 10:00 (no seconds are given in that string, so we assume 00 as well).</p>

<p>Here is the stack trace.</p>

<pre><code>java.lang.NumberFormatException: For input string: ""200912031000""
at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
at java.lang.Integer.parseInt(Integer.java:583)
at java.lang.Integer.valueOf(Integer.java:766)
at edu.stanford.nlp.ie.pascal.ISODateInstance.extractDay(ISODateInstance.java:1107)
at edu.stanford.nlp.ie.pascal.ISODateInstance.extractFields(ISODateInstance.java:398)
at edu.stanford.nlp.ie.pascal.ISODateInstance.&lt;init&gt;(ISODateInstance.java:82)
at edu.stanford.nlp.ie.QuantifiableEntityNormalizer.normalizedDateString(QuantifiableEntityNormalizer.java:363)
at edu.stanford.nlp.ie.QuantifiableEntityNormalizer.normalizedDateString(QuantifiableEntityNormalizer.java:338)
at edu.stanford.nlp.ie.QuantifiableEntityNormalizer.processEntity(QuantifiableEntityNormalizer.java:1018)
at edu.stanford.nlp.ie.QuantifiableEntityNormalizer.addNormalizedQuantitiesToEntities(QuantifiableEntityNormalizer.java:1320)
at edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:145)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifySentenceWithGlobalInformation(AbstractSequenceClassifier.java:322)
at edu.stanford.nlp.pipeline.NERCombinerAnnotator.doOneSentence(NERCombinerAnnotator.java:148)
at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:95)
at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:137)
at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:67)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:847)
</code></pre>

<p><strong>EDIT</strong></p>

<p>I am looking at this again because I am addressing some sutime portions of my code currently and I can reproduce by simply doing:</p>

<pre><code>    ISODateInstance idi = new ISODateInstance();
    boolean fields = idi.extractFields(""200912031000"");
    System.out.println(fields);
</code></pre>

<p>Note that <code>true</code> is the printed value.</p>
","datetime, stanford-nlp, sutime","<p>Ok, so let me say why the problem existed.  There were two problems with extractDay() in 3.4.1:</p>

<ol>
<li>Integer.valueOf is used in line 1107. This creates the error we see because the String, if it were to be construed as a number, certainly would be a Long. Long.valueOf is used in later versions.</li>
<li>False should be returned from extractDay because it was unable to do anything with that string. However, the try block (line 1106) is inside the for loop (line 1097) meaning that after a failure, more tokens could be examined leading to the method eventually returning true. This will allow the annotation to be created even though technically no annotation should be created since parsing failed. The try was moved outside of the for block in later versions.</li>
</ol>

<p>So the only answer is update to a later version (although I can't update to a later version still at this time).</p>
",0,1,207,2016-01-21 14:40:30,https://stackoverflow.com/questions/34926699/unexpected-date-datetime-strings-cause-exception-in-stanford-corenlp
"Train Stanford NER with big gazette, memory issue","<p>I have previously trained a german classifier using the Stanford NER and a training-file with 450.000 tokens. Because I had almost 20 classes, this took about 8 hours and I had to cut a lot of features short in the prop file.</p>

<p>I now have a gazette-file with 16.000.000 unique tagged tokens. I want to retrain my classifier under use of those tokens, but I keep running into memory issues. The gazette-txt is 386mb and mostly contains two-token objects (first + second name), all unique.</p>

<p>I have reduced the amount of classes to 5, reduced the amount of tokens in the gazette by 4 million and I've removed all the features listed on the Stanford NER FAQ-site from the prop-file but I still run into the out of memory: java heap space error. I have 16gb of ram and start the jvm with -mx15g -Xmx14g.</p>

<p>The error occurs about 5 hours into the process.</p>

<p>My problem is that I don't know how to further reduce the memory usage without arbitrarily deleting entries from the gazette. Does someone have further suggestions on how I could reduce my memory-usage?</p>

<p>My prop-file looks like this:</p>

<pre><code>trainFile = ....tsv
serializeTo = ...ser.gz
map = word=0,answer=1

useWordPairs=false
useNGrams=false
useClassFeature=true
useWord=true
noMidNGrams=true
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
saveFeatureIndexToDisk=true
qnSize=2
printFeatures=true
useObservedSequencesOnly=true

cleanGazette=true
gazette=....txt
</code></pre>

<p>Hopefully this isnt too troublesome. Thank you in advance!</p>
","java, memory, nlp, stanford-nlp, named-entity-recognition","<p>RegexNER could help you with this:</p>

<p><a href=""http://nlp.stanford.edu/static/software/regexner/"" rel=""nofollow"">http://nlp.stanford.edu/static/software/regexner/</a></p>

<p>Some thoughts:</p>

<ol>
<li><p>start with 1,000,000 entries and see how big of a gazetteer you can handle, or if 1,000,000 is too large shrink it down more.</p></li>
<li><p>sort the entries by how frequent they are in a large corpus and eliminate the infrequent ones</p></li>
<li><p>Hopefully a lot of the rarer entries in your gazetteer aren't ambiguous, so you can just use RegexNER and have a rule based layer in your system that automatically tags them as PERSON</p></li>
</ol>
",1,2,865,2016-01-22 06:33:39,https://stackoverflow.com/questions/34940417/train-stanford-ner-with-big-gazette-memory-issue
How are StanfordNER Classifiers built,"<p>I am working with StanfordNER classifiers. There are 4 classifiers as</p>

<pre><code>english.all.3class.distsim.crf.ser.gz
english.muc.7class.distsim.crf.ser.gz
english.conll.4class.distsim.crf.ser.gz
example.serialized.ncc.ncc.ser.gz
</code></pre>

<p>How are these classifiers built? Since each of them is based on a different corpus, here is my guess</p>

<ol>
<li><p>Train a machine learning classifier like <code>SVM</code> coupled with <code>OVR</code> (for multi label case) on the corpus to detect entities like <code>ORGANIZATION</code>,<code>PERSON</code>,<code>LOCATION</code> etc. This means that the training data would be the entire text of a document in the corpus. For that piece of text we explicitly indicate the <code>ORGANIZATION</code>s,<code>PERSON</code>s and <code>LOCATION</code>s. Thus the classifiers would be able to predict those entities.</p></li>
<li><p>Train a machine learning classifier to link POS tags with entities like <code>ORGANIZATION</code>,<code>PERSON</code>,<code>LOCATION</code>. For example, a classifier can be trained to predict which proper nouns should be <code>ORGANIZATION</code></p></li>
</ol>

<p>Is this the correct big picture? I am just trying to work out how to build my own NER.</p>
","machine-learning, nlp, classification, stanford-nlp, named-entity-recognition","<p>Yes, the models are trained on supervised data. They're 1st order CRFs which do multi-class probabilistic sequence classification (so not OVR, not SVM). You can find an introduction to NER and Stanford NER in particular on <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">the Stanford NER page</a>.</p>
",5,3,216,2016-01-22 14:50:20,https://stackoverflow.com/questions/34949472/how-are-stanfordner-classifiers-built
Stanford NLP: How to lemmatize single word?,"<p>I know how I can annotate a sentence and get the lemma of each word but I don't know how to do it if I just want to lemmatize a <em>single</em> word. I tried</p>

<pre class=""lang-java prettyprint-override""><code>Annotation tokenAnnotation = new Annotation(""wedding"");
List&lt;CoreMap&gt; list = tokenAnnotation.get(SentencesAnnotation.class);

String tokenLemma = list
                        .get(0).get(TokensAnnotation.class)
                        .get(0).get(LemmaAnnotation.class);
</code></pre>

<p>but the <code>tokenAnnotation</code> has only one <code>TextAnnotation</code> key which means <code>list</code> will be <code>null</code>here.</p>

<p>So how can I lemmatize a single word?</p>
",stanford-nlp,"<p>There are two options: you can either annotate you <code>Annotation</code> object through a <code>StanfordCoreNLP</code> pipeline:</p>

<pre class=""lang-java prettyprint-override""><code>StanfordCoreNLP pipeline = new StanfordCoreNLP(new Properties(){{
  setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"");
}});

Annotation tokenAnnotation = new Annotation(""wedding"");
pipeline.annotate(tokenAnnotation);  // necessary for the LemmaAnnotation to be set.
List&lt;CoreMap&gt; list = tokenAnnotation.get(SentencesAnnotation.class);
String tokenLemma = list
                        .get(0).get(TokensAnnotation.class)
                        .get(0).get(LemmaAnnotation.class);
</code></pre>

<p>The other option is to use the SimpleCoreNLP API:</p>

<pre class=""lang-java prettyprint-override""><code>String tokenLemma = new Sentence(""wedding"").lemma(0);
</code></pre>
",3,5,2525,2016-01-23 12:03:00,https://stackoverflow.com/questions/34963203/stanford-nlp-how-to-lemmatize-single-word
Why Stanford parser with nltk is not correctly parsing a sentence?,"<p>I am using Stanford parser with nltk in python and got help from <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk"">Stanford Parser and NLTK</a>  to set up Stanford nlp libraries.</p>

<pre><code>from nltk.parse.stanford import StanfordParser
from nltk.parse.stanford import StanfordDependencyParser
parser     = StanfordParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
dep_parser = StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
one = (""John sees Bill"")
parsed_Sentence = parser.raw_parse(one)
# GUI
for line in parsed_Sentence:
       print line
       line.draw()

parsed_Sentence = [parse.tree() for parse in dep_parser.raw_parse(one)]
print parsed_Sentence

# GUI
for line in parsed_Sentence:
        print line
        line.draw()
</code></pre>

<p>I am getting wrong parse and dependency trees as shown in the example below, it is treating 'sees' as noun instead of verb.</p>

<p><a href=""https://i.sstatic.net/I3hSv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/I3hSv.png"" alt=""Example parse tree""></a>
<a href=""https://i.sstatic.net/C6gL8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C6gL8.png"" alt=""Example dependency tree""></a></p>

<p>What should I do?
It work perfectly right when I change sentence e.g.(one = 'John see Bill').
The correct ouput for this sentence can be viewed from here <a href=""https://stackoverflow.com/questions/10401076/difference-between-constituency-parser-and-dependency-parser"">correct ouput of parse tree</a></p>

<p>Example of correct output is also shown below:</p>

<p><a href=""https://i.sstatic.net/68vBR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/68vBR.png"" alt=""correctly parsed""></a></p>

<p><a href=""https://i.sstatic.net/evbZ8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/evbZ8.png"" alt=""correct dependency parsed tree""></a></p>
","python, parsing, nlp, nltk, stanford-nlp","<p>Once again, no model is perfect (see <a href=""https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag"">Python NLTK pos_tag not returning the correct part-of-speech tag</a>) ;P</p>

<p>You can try a ""more accurate"" parser, using the <code>NeuralDependencyParser</code>.</p>

<p>First setup the parser properly with the correct environment variables (see <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk/34112695#34112695"">Stanford Parser and NLTK</a> and <a href=""https://gist.github.com/alvations/e1df0ba227e542955a8a"" rel=""nofollow noreferrer"">https://gist.github.com/alvations/e1df0ba227e542955a8a</a>), then:</p>

<pre><code>&gt;&gt;&gt; from nltk.internals import find_jars_within_path
&gt;&gt;&gt; from nltk.parse.stanford import StanfordNeuralDependencyParser
&gt;&gt;&gt; parser = StanfordNeuralDependencyParser(model_path=""edu/stanford/nlp/models/parser/nndep/english_UD.gz"")
&gt;&gt;&gt; stanford_dir = parser._classpath[0].rpartition('/')[0]
&gt;&gt;&gt; slf4j_jar = stanford_dir + '/slf4j-api.jar'
&gt;&gt;&gt; parser._classpath = list(parser._classpath) + [slf4j_jar]
&gt;&gt;&gt; parser.java_options = '-mx5000m'
&gt;&gt;&gt; sent = ""John sees Bill""
&gt;&gt;&gt; [parse.tree() for parse in parser.raw_parse(sent)]
[Tree('sees', ['John', 'Bill'])]
</code></pre>

<p>Do note that the <code>NeuralDependencyParser</code> only produces the dependency trees:</p>

<p><a href=""https://i.sstatic.net/FkENU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FkENU.png"" alt=""enter image description here""></a></p>
",7,6,4653,2016-01-23 20:52:03,https://stackoverflow.com/questions/34968716/why-stanford-parser-with-nltk-is-not-correctly-parsing-a-sentence
R&#39;s coreNLP::initCoreNLP() throws java.lang.OutOfMemoryError,"<p><code>coreNLP</code> is an R package for interfacing with Standford's CoreNLP Java libraries. The first line one must execute (after loading the appropriate packages with the <code>library()</code> command) is <code>initCoreNLP()</code>. Unfortunately, this results in the following error: </p>

<blockquote>
  <p>Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... Error in rJava::.jnew(""edu.stanford.nlp.pipeline.StanfordCoreNLP"", basename(path)) : 
    java.lang.OutOfMemoryError: GC overhead limit exceeded</p>
</blockquote>

<p>Note, this is the same problem that is listed here: (<a href=""https://stackoverflow.com/questions/33015823/initcorenlp-method-call-from-the-stanfords-r-corenlp-package-throws-error"">initCoreNLP() method call from the Stanford&#39;s R coreNLP package throws error</a>). In that case, however, the OP found that rebooting his machine made the problem disappear. This is <em>not</em> the case for me; I keep experiencing it even after a reboot.</p>

<p>Has anyone else run into this and can provide a solution or suggestion?</p>

<p>Thanks in advance,
DG</p>

<h1>CONFIG DETAILS:</h1>

<p>R version 3.2.3 (2015-12-10)</p>

<p>rJava version 0.9-7</p>

<p>coreNLP version 0.4-1</p>

<p>Machine: Win 7 with 8GB RAM</p>
","r, stanford-nlp, rjava","<p>Here is some documentation I found:</p>

<p><a href=""https://cran.r-project.org/web/packages/coreNLP/coreNLP.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/coreNLP/coreNLP.pdf</a> </p>

<p>(specifically page 7)</p>

<p>You can specify how much memory you use (from the documentation):</p>

<pre><code>initCoreNLP(libLoc, parameterFile, mem = ""4g"", annotators)
</code></pre>

<p>Add more memory and I would imagine the problem will go away.</p>
",1,2,1427,2016-01-25 00:01:56,https://stackoverflow.com/questions/34983149/rs-corenlpinitcorenlp-throws-java-lang-outofmemoryerror
How to clean sentences for StanfordNER,"<p>I want to use <code>StanfordNER</code> in python to detect named entities. How should i clean up the sentences?</p>

<p>for example, consider</p>

<p><code>qry=""In the UK, the class is relatively crowded with Zacc competing with Abc's Popol (market leader) and  Xyz's Abcvd.""</code></p>

<p>if i do</p>

<pre><code>st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
print st.tag(qry.split())
</code></pre>

<p>i get</p>

<pre><code>[
    (u'In', u'O'), (u'the', u'O'), (u'UK,', u'O'), (u'the', u'O'), 
    (u'class', u'O'), (u'is', u'O'), (u'relatively', u'O'), (u'crowded', u'O'), 
    (u'with', u'O'), (u'Zacc', u'PERSON'), (u'competing', u'O'), (u'with', u'O'), 
    (u""Abc's"", u'O'), (u'Popol', u'O'), (u'(market', u'O'), (u'leader)', u'O'), 
    (u'and', u'O'), (u""Xyz's"", u'O'), (u'Abcvd.', u'O')
]
</code></pre>

<p>`</p>

<p>so only 1 named entities was detected. However, if i do some cleanup by replacing all special characters with spaces</p>

<p><code>qry=""In the UK the class is relatively crowded with Zacc competing with Abc s Popol  market leader and  Xyz s Abcvd""</code></p>

<p>i get</p>

<pre><code>[
    (u'In', u'O'), (u'the', u'O'), (u'UK', u'LOCATION'), (u'the', u'O'), 
    (u'class', u'O'), (u'is', u'O'), (u'relatively', u'O'), (u'crowded', u'O'), 
    (u'with', u'O'), (u'Zacc', u'PERSON'), (u'competing', u'O'), (u'with', u'O'), 
    (u'Abc', u'ORGANIZATION'), (u's', u'O'), (u'Popol', u'PERSON'), (u'market', u'O'), 
    (u'leader', u'O'), (u'and', u'O'), (u'Xyz', u'ORGANIZATION'), (u's', u'O'), (u'Abcvd', u'PERSON')]
</code></pre>

<p>`</p>

<p>so clearly, this is more appropriate. Are there any general rules on how to clean up sentences for <code>StanfordNER</code>? Initially i thought that there is no cleanup required at all!</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>You can use Stanford Tokenizer for your purpose. 
You could use the code below.</p>

<pre><code>from nltk.tokenize.stanford import StanfordTokenizer
token = StanfordTokenizer('stanford-ner-2014-06-16/stanford-ner.jar')
qry=""In the UK, the class is relatively crowded with Zacc competing with Abc's Popol (market leader) and  Xyz's Abcvd.""
tok = token.tokenize(qry)
print tok
</code></pre>

<p>You will get the tokens as you require them.</p>

<blockquote>
  <p>[u'In',<br>
   u'the',<br>
   u'UK',<br>
   u',',<br>
   u'the',<br>
   u'class',<br>
   u'is',<br>
   u'relatively',<br>
   u'crowded',<br>
   u'with',<br>
   u'Zacc',<br>
   u'competing',<br>
   u'with',<br>
   u'Abc',<br>
   u""'s"",<br>
   u'Popol',<br>
   u'-LRB-',<br>
   u'market',<br>
   u'leader',<br>
   u'-RRB-',<br>
   u'and',<br>
   u'Xyz',<br>
   u""'s"",<br>
   u'Abcvd',<br>
   u'.'] </p>
</blockquote>
",4,0,565,2016-01-26 15:07:49,https://stackoverflow.com/questions/35017041/how-to-clean-sentences-for-stanfordner
how to get Coreference Resolution annotation in stanford core nlp toolkit?,"<p>I'm trying to use Stanford Corenlp toolkit to annotate a text. I tried to use the code provided here : <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/</a>
and it works well. The problem is when i want to use <strong>Co-reference Resolution tool embedded in coreNLP toolkit</strong>. It does not work. i used the code that were published by stanford nlp group. code is here below:</p>

<pre><code>public class CorefExample {

  public static void main(String[] args) throws Exception {

  Annotation document = new Annotation(""Barack Obama was born in Hawaii. He is the president. Obama was elected in 2008."");

  Properties props = new Properties();
  props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,mention,coref"");
  StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
  pipeline.annotate(document);
  System.out.println(""---"");
  System.out.println(""coref chains"");
  for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values())          {     
     System.out.println(""\t""+cc);
  }
  for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class))
  {
    System.out.println(""---"");
    System.out.println(""mentions"");
    for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
      System.out.println(""\t""+m);
     }
   }
  }
}
</code></pre>

<p>but when i want to run these codes, i got null,this line : ""<strong>sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)</strong>""
always <strong>return null</strong>, while i am sure that toolkit has annotaed corefrence mentions. 
I really mixed up. what is the solution? how can i receive the coref annottaion in java code?</p>
",stanford-nlp,"<p>If I run the sample code on the coref page with the latest stanford-corenlp-3.6.0.jar it runs to completion, so I am not seeing the null issue you are talking about.</p>

<p>Make sure to use the latest jar available on the website, version 3.6.0</p>

<p>Update:</p>

<p>If you cut and paste the code on this page: </p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/coref.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/coref.html</a></p>

<p>and put into a file called CorefExample.java then do:</p>

<pre><code>javac -cp ""stanford-corenlp-full-2015-12-09/*"" CorefExample.java
java -cp ""stanford-corenlp-full-2015-12-09/*:."" CorefExample
</code></pre>

<p>You should see the mentions printed out.</p>

<p>We've updated the distribution, so also make sure you've downloaded it recently.</p>

<p>If you're still having problems we will have to figure out what is different from what I just described and your set up.  I just cut and paste the code and ran it as described above and I see the mentions printed out (I even added a sentence with no mentions to the sample text) and I get a list with the mentions (or empty list).  So you shouldn't be getting null if you're using that exact code with the latest jar.</p>

<p>It would be helpful to know how you're running the code so we can see what the difference is.</p>
",1,2,2305,2016-01-27 16:28:31,https://stackoverflow.com/questions/35042801/how-to-get-coreference-resolution-annotation-in-stanford-core-nlp-toolkit
CoreNLP tokenizer/sentence splitter misbehaves on HTML input,"<p>I'm using the Stanford CoreNLP pipeline from the command line to dependency parse a large document, and it's very important that each line in the document receive its own dependency tree (otherwise other things get unaligned). This pair of lines is currently causing me grief:</p>

<pre><code>&lt;!-- copy from here --&gt; &lt;a href=""http://strategis.gc.ca/epic/internet/inabc-eac.nsf/en/home""&gt;&lt;img src=""id-images/ad-220x80_01e.jpg"" alt=""Aboriginal Business Canada:
Opening New Doors for Your Business"" width=""220"" height=""80"" border=""0""&gt;&lt;/a&gt; &lt;!-- copy to here --&gt; Small ABC Graphic Instructions 1.
</code></pre>

<p>This is the command I'm using:</p>

<pre><code>java -cp ""*"" -Xmx1g -Xss515m edu.stanford.nlp.pipeline.StanfordCoreNLP \ 
     -annotators tokenize,ssplit,pos,depparse \
     -ssplit.eolonly true \
     -ssplit.newlineIsSentenceBreak always \
     -outputFormat conllu \
     -file ""input.txt""
</code></pre>

<p>And this is the resulting output:</p>

<pre><code>1   &lt;!-- copy from here --&gt; _   _   JJ  _   9   amod    _   _
2   &lt;a href=""http://strategis.gc.ca/epic/internet/inabc-eac.nsf/en/home""&gt;   _   _   JJ  _   9   amod    _   _
3   &lt;img src=""id-images/ad-220x80_01e.jpg"" alt=""Aboriginal Business Canada:
Opening New Doors for Your Business"" width=""220"" height=""80"" border=""0""&gt;    _   _   NN  _   9   compound    _   _
4   &lt;/a&gt;    _   _   NN  _   9   compound    _   _
5   &lt;!-- copy to here --&gt;   _   _   NN  _   9   compound    _   _
6   Small   _   _   JJ  _   9   amod    _   _
7   ABC _   _   NNP _   9   compound    _   _
8   Graphic _   _   NNP _   9   compound    _   _
9   Instructions    _   _   NNS _   0   root    _   _
10  1   _   _   CD  _   9   nummod  _   _
11  .   _   _   .   _   9   punct   _   _
</code></pre>

<p>It looks like the newline character inside the quotation marks in the HTML tag is being interpreted as part of the token, rather than as a sentence break. This is peculiar since I'm using the <code>-ssplit.newlineIsSentenceBreak always</code> flag, which I would expect would force the parser to split up the HTML code. However, even if I didn't need each line to get its own parse, this behavior is troubling because the resulting file is no longer in valid conllu format, since line 3 has only two tab-separated columns (instead of the required 10) and line 4 has only 9.</p>

<p>One workaround I played with was turning each line in the original file into its own file and then feeding them in with the <code>-filelist</code> parameter, but that created too much <code>stdout</code> output that slowed things down and clogged the terminal. My attempts to redirect the output to <code>/dev/null</code> or turn on a ""quiet mode"" were met with failure, but that's probably a question for another post.</p>

<p>I tried double-spacing the file, but that didn't help. Preprocessing the text with <code>sed 's/""/\\""/g'</code> does fix this problem by destroying the pipeline's ability to recognize this as HTML code, but introduces new ones since the parser presumably wasn't trained on escaped quotation marks.</p>

<p>Obviously this is a weird sentence and I don't expect the output to be parsed sensibly, but I do need it be formatted sensibly. Any tips?</p>

<p><strong>Update</strong>
It was suggested to me that I try using the cleanxml annotator to get rid of the HTML tag altogether. This reduces the number of lines in the file, which may result in misalignment later, but since the HTML tags aren't getting parsed sensibly anyway it seems independently advantageous to get rid of them. I'll update again later with whether or not this works for my purposes, but I'm open to other suggestions in the meantime.</p>
",stanford-nlp,"<p>There were two problems here:</p>

<ol>
<li><p>The tokenizer will parse as a single token HTML/XML/SGML tags where the quoted value of an attribute is split over lines. Usually this is a good thing - if this were regular text, what it is doing for this example is actually sensible, keeping the whole <code>img</code> tag together - but it is disastrous if you are wanting to process text as strictly one sentence per line, as in most machine translation corpora. In this case you want to treat each line as a sentence, even if the original sentence breaking was done wrongly, as here.</p></li>
<li><p>If a newline was captured in the value of an attribute, it was left as such, and then on output, this destroyed the integrity of at least the line-oriented CoNLL and CoNLL-U output formats.</p></li>
</ol>

<p>I've added/changed code to address these problems:</p>

<ol>
<li><p>There is a new tokenizer option: <code>-tokenize.options tokenizePerLine</code>, which will disallow the tokenizer from looking across line boundaries when forming tokens or making tokenization decisions. (This option can be combined in a comma-separated list with all the other <code>tokenize.options</code> options.)</p></li>
<li><p>If a newline is captured in the value of an attribute, then it is mapped to <code>U+00A0</code> (non-breaking space). This was already what happened to <code>U+0020</code> (space) and is now also done for newlines. This fixes the CoNLL/CoNLL-U output and maintains the correct invariant for Stanford CoreNLP: Tokens may occasionally contain non-breaking spaces, but never space or newline.</p></li>
</ol>

<p>This problem is fixed in commit 0a17fe4c0fc4ccfb095f474bf113d1df0c6d17cb on <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">the CoreNLP github</a>. If you grab that version -- or minimally update the <code>PTBLexer.class</code> and <code>PTBTokenizer.class</code>, then you will have this new option and should be good. The following command should give you what you want:</p>

<p><code>java -cp ""*"" -Xmx1g edu.stanford.nlp.pipeline.StanfordCoreNLP \
    -annotators tokenize,ssplit,pos,depparse -ssplit.eolonly true \
    -tokenize.options tokenizePerLine -outputFormat conllu -file ""input.txt""
</code></p>

<p>p.s. I guess you were trying to fix things, but if you're using <code>-ssplit.eolonly true</code> then you shouldn't need or see a difference from <code>-ssplit.newlineIsSentenceBreak always</code>. Also, we maybe should make it so that turning on <code>-ssplit.eolonly true</code> automatically turns on <code>-tokenize.options tokenizePerLine</code>, but that is not presently the case....</p>
",1,1,684,2016-01-29 01:38:47,https://stackoverflow.com/questions/35075463/corenlp-tokenizer-sentence-splitter-misbehaves-on-html-input
Sentiment Analysis - What does annotating dataset mean?,"<p>I'm currently working on my final year research project, which is an application which analyzes travel reviews found online, and give out a sentiment score for particular tourist attractions as a result, by conducting aspect level sentiment analysis.</p>

<p>I have a newly scraped dataset from a famous travel website which does not allow to use their API for research/academic purposes. (bummer)</p>

<p>My supervisor said that I might need to get this dataset annotated before using it for the aforementioned purpose. I am kind of confused as to what data annotation means in this context. Could someone please explain what exactly is happening when a dataset is annotated and how it helps in getting sentiment analysis done?</p>

<p>I was told that I might have to get two/three human annotators and get the data annotated to make it less biased. I'm on a tight schedule and I was wondering if there are any tools that can get it done for me? If so, what will be the impact of using such tools over human annotators? I would also like suggestions for such tools that you would recommend.</p>

<p>I would really appreciate a detailed explanation to my questions, as I am stuck with my project progressing to the next step because of this.</p>

<p>Thank you in advance.</p>
","annotations, dataset, nlp, stanford-nlp, sentiment-analysis","<p>To a first approximation, machine learning algorithms (e.g., a sentiment analysis algorithm) is learning to perform a task that humans currently perform by collecting many examples of the human performing the task, and then imitating them. When your supervisor talks about ""annotation,"" they're talking about collecting these examples of a human doing the sentiment annotation task: annotating a sentence for sentiment. That is, collecting pairs of sentences and their sentiment as judged by humans. Without this, there's nothing for the program to learn from, and you're stuck hoping the program can give you something from nothing -- which it never will.</p>

<p>That said, there are tools for collecting this sort of data, or at least helping. Amazon Mechanical Turk and other crowdsourcing platforms are good resources for this sort of data collection. You can also take a look at something like: <a href=""http://www.crowdflower.com/type-sentiment-analysis"" rel=""nofollow"">http://www.crowdflower.com/type-sentiment-analysis</a>.</p>
",2,1,894,2016-01-29 15:02:09,https://stackoverflow.com/questions/35087563/sentiment-analysis-what-does-annotating-dataset-mean
Stanford LexParser Multithreading,"<p>I was using Stanford Lexparser recently. Unfortunately, I have encountered a problem as it take a very long time especially when I pass in a large file. Will multithreading help improve the performance? I know that multithreading can be easily done in command line. However, I would like to multithread it using the API internally. Currently, I am using this code. How I make it multithread?</p>

<pre><code>for (List&lt;HasWord&gt; sentence : new DocumentPreprocessor(fileReader)) {
        parse = lp.apply(sentence);
        TreePrint tp = new TreePrint(""typedDependenciesCollapsed"");
        tp.printTree(parse, pw);
}
</code></pre>
","java, multithreading, nlp, stanford-nlp","<p>You can just use regular old Java thread to annotate documents in parallel. For example:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize,ssplit,parse"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

Annotation ann = new Annotation(""your sentence here"");
for (int i = 0; i &lt; 100; ++i) {
  new Thread() {
    @Override public void run() {
      pipeline.annotate(ann);  // except, you should probably annotate different documents.
      Tree tree = ann.get(SentencesAnnotation.class).get(0).get(TreeAnnotation.class);
    }
  }.start();
}
</code></pre>

<p>Another option is to use the <a href=""http://stanfordnlp.github.io/CoreNLP/simple.html"" rel=""nofollow"">simple API</a>:</p>

<pre><code>for (int i = 0; i &lt; 100; ++i) {
  new Thread() {
    @Override public void run() {
      Tree tree = new Sentence(""your sentence"").parse();
    }
  }.start();
}
</code></pre>

<p>At a high level though, you're unlikely to get a phenomenally huge speedup from multithreading. Parsing is generally slow (O(n^3) wrt the sentence length) and multithreading only gives you max linear speedup in the number of cores. An alternative for making things faster would to be to either use <a href=""http://nlp.stanford.edu/software/srparser.shtml"" rel=""nofollow"">the shift reduce parser</a>, or, if you're ok with dependency and not constituency parses, the <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">Stanford Neural Dependency Parser</a>.</p>
",2,0,176,2016-01-29 16:17:06,https://stackoverflow.com/questions/35089104/stanford-lexparser-multithreading
what are the methods to estimate probabilities of production rules?,"<p>I know that n-gram is useful for finding the probability of words,I want to know, How to estimate probabilities of production rules? How many methods or rules to calculate probabilities of production rules?</p>

<p>I could not find any good blog or something on this topic.Now I am studying on probabilistic context free grammar &amp; CKY parsing algorithm.</p>
","nlp, stanford-nlp, context-free-grammar","<p>As I understand your question, you are asking how to estimate the parameters of a PCFG model from data.</p>

<p>In short, it's easy to make empirical production-rule probability estimates when you have ground-truth parses in your training data. If you want to estimate the probability that <code>S -&gt; NP VP</code>, this is something like <code>Count(S -&gt; NP VP) / Count(S -&gt; *)</code>, where <code>*</code> is any possible subtree.</p>

<p>You can find a much more formal statement in lots of places on the web (search for ""PCFG estimation"" or ""PCFG learning""). Here's a nice one from Michael Collins' lecture notes: <a href=""http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf#page=9"" rel=""nofollow"">http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf#page=9</a></p>
",2,1,203,2016-02-03 04:01:59,https://stackoverflow.com/questions/35168281/what-are-the-methods-to-estimate-probabilities-of-production-rules
Is their any algorithm for sentence parsing using bigram probability?,"<p>I know that the <a href=""https://en.wikipedia.org/wiki/CYK_algorithm"" rel=""nofollow"">CKY algorithm</a> can parse sentences using production rules probabilities. </p>

<p>Is there any sentence parsing algorithm I can use if I only know words bigram probabilities?</p>
","nlp, stanford-nlp, context-free-grammar","<p>You can not induce any sort of grammatical structure from bigram probabilities.* You could use bigrams to build a language model, though, which may serve some of the same purposes as a parser.</p>

<p><sub>* Actually, you might be able to build a very rudimentary chunking algorithm by starting a new chunk whenever bigram probabiity drops below a certain threshold. But this is nowhere near the accuracy / granularity that is going to be useful for most tasks.</sub></p>
",3,2,245,2016-02-03 11:51:11,https://stackoverflow.com/questions/35176283/is-their-any-algorithm-for-sentence-parsing-using-bigram-probability
Time statistics and verbosity of Stanford CoreNLP,"<p>I'm using Stanford CoreNLP to make some annotations (I also added some custom annotator) and I'd like to measure how much time each annotator takes. Is there a way to do so easily?</p>

<p>In addition, I noticed that in the source code of the annotator there is a boolean variable <code>verbose</code>, but I cannot understand how to set it. Is it documented anywhere?</p>
",stanford-nlp,"<p><code>StanfordCoreNLP</code> has a method <code>String timingInformation()</code> which will return information on how much time is being used by each annotator. </p>

<p>(This is unrelated to any extra printing that may be controlled by verbose, but you can often set verbose flags for individual annotators although not quite consistently. Thing like: <code>-parse.debug</code> or <code>-pos.verbose</code> on the command line, or the corresponding things in a properties file. </p>
",1,0,283,2016-02-04 09:22:03,https://stackoverflow.com/questions/35197032/time-statistics-and-verbosity-of-stanford-corenlp
Pass parser parameters when using CoreNLP server,"<p>The parser takes the flag <code>-makeCopulaHead</code>, how can I enable this flag on when using the <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow"">CoreNLP server</a>?</p>

<p>I've tried starting the server with the flag:</p>

<pre><code>java -cp ""*"" -mx4g edu.stanford.nlp.pipeline.StanfordCoreNLPServer -parse.flags "" -makeCopulaHead""
</code></pre>

<p>I have also tried passing it the param as part of the url params in my post request: </p>

<pre><code>properties = {""annotators"": ""..."", ""parse.makeCopulaHead"": ""true""}
properties = {""annotators"": ""..."", ""makeCopulaHead"": ""true""}
properties = {""annotators"": ""..."", ""parse.flags.makeCopulaHead"": ""true""}
</code></pre>

<p><strong>Edit</strong>, and attempts from answer:</p>

<pre><code>properties = {""annotators"": ""..."", ""parse.flags"": "" -makeCopulaHead""}
properties = {""annotators"": ""..."", ""parse.flags"": ""makeCopulaHead""}
</code></pre>
",stanford-nlp,"<p>The right way to do this is the second: pass in the flags as <code>properties = {...}</code> entries. I don't actually know how this particular flag works, but it does seem like the equivalent to your command-line invocation would be:</p>

<pre><code>properties = {""annotators"": ""..."", ""parse.flags"": "" -makeCopulaHead""}
</code></pre>

<p>Perhaps that'll work?</p>

<p>EDIT: The <code>-parse.flags</code> option will only work if you're using the constituency parser + dependency converter (annotator <code>parse</code>) rather than the neural dependency parser (annotator <code>depparse</code>).</p>
",1,1,451,2016-02-05 10:24:39,https://stackoverflow.com/questions/35221700/pass-parser-parameters-when-using-corenlp-server
Stanford CoreNLP Statistical Coref System NullPointerException,"<p>I am trying to apply the statistical coreference system to process a text file with the following command</p>

<pre><code>java -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,cleanxml,ssplit,pos,lemma,ner,parse,coref -file input.txt
</code></pre>

<p>This throws the following error message:</p>

<pre><code>[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator cleanxml
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.8 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.8 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [1.0 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ...
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref

Processing file /home/xilin/Toolkits/stanford-corenlp-full-2015-12-09/input.txt ... writing to /home/xilin/Toolkits/stanford-corenlp-full-2015-12-09/input.txt.out
Annotating file /home/xilin/Toolkits/stanford-corenlp-full-2015-12-09/input.txt
Exception in thread ""main"" java.lang.RuntimeException: Error annotating document with coref
at edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate(StatisticalCorefSystem.java:86)
at edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate(StatisticalCorefSystem.java:63)
at edu.stanford.nlp.pipeline.CorefAnnotator.annotate(CorefAnnotator.java:97)
at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:72)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:534)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:544)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1098)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:877)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1187)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1257)
Caused by: java.lang.NullPointerException
at edu.stanford.nlp.hcoref.Preprocessor.assignMentionIDs(Preprocessor.java:170)
at edu.stanford.nlp.hcoref.Preprocessor.initializeMentions(Preprocessor.java:153)
at edu.stanford.nlp.hcoref.Preprocessor.preprocess(Preprocessor.java:64)
at edu.stanford.nlp.hcoref.CorefDocMaker.makeDocument(CorefDocMaker.java:194)
at edu.stanford.nlp.hcoref.CorefDocMaker.makeDocument(CorefDocMaker.java:154)
at edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate(StatisticalCorefSystem.java:68)
... 9 more
</code></pre>

<p>If I changed the option ""coref"" in the above command to ""dcoref"", the deterministic coreference system runs smoothly. Others have pointed out that this is a <a href=""https://stackoverflow.com/questions/34902540/stanford-corenlp-pipeline-coref-parsing-some-short-strings-with-few-mentions"">BUG</a> in 3.6.0 distribution. I am using the <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">github repository</a>, and I'm getting the latest version. However the bug seems to exist still.</p>
",stanford-nlp,"<p>You need to include the <code>mention</code> annotator before <code>coref</code>. The fact that this appears as a null pointer exception is indeed a bug though. Which Git revision are you using? We've recently changed the way requirements are handled, and this may be a remnant bug from that.</p>
",4,0,319,2016-02-09 06:37:31,https://stackoverflow.com/questions/35285441/stanford-corenlp-statistical-coref-system-nullpointerexception
How do I execute Stanford CoreNLP sentiment analysis within a Java Program?,"<p>I'm using Stanford CoreNLP with the following command line arguments    </p>

<pre><code>java -cp ""*"" -mx5g edu.stanford.nlp.sentiment.SentimentPipeline -file foo.txt
</code></pre>

<p>I obtain the results within the command prompt as well. How do I execute the same in Java?</p>

<p>I can import all the libraries present there but I don't know which function to execute specifically to get the sentiment analysis results. How do I execute the same programmatically?</p>
","java, stanford-nlp","<p>Basically, you execute Java code programmatically using an application programming interfece (API). Stanford CoreNLP offers a demo showing how to use the API. Here's what their <a href=""http://nlp.stanford.edu/software/corenlp-faq.shtml"" rel=""nofollow noreferrer"">FAQ page</a> says:</p>

<blockquote>
  <p>How do I use the API?</p>
  
  <p>A brief demo program included with the download will demonstrate how
  to load the tool and start processing text. When using this demo
  program, be sure to include all of the appropriate jar files in the
  classpath.</p>
</blockquote>

<p>When you start playing with the demo program, check SO for other questions  that have been asked before about running the Stanford CoreNLP demo, for example this one: <a href=""https://stackoverflow.com/questions/20359346/executing-and-testing-stanford-core-nlp-example"">Executing and testing stanford core nlp example</a></p>
",1,0,896,2016-02-09 12:01:47,https://stackoverflow.com/questions/35291574/how-do-i-execute-stanford-corenlp-sentiment-analysis-within-a-java-program
How do I set up a Stanford CoreNLP Server on Windows to return sentiment for text,"<p>I am attempting to set up a local server on Windows with Stanford CoreNLP to calculate sentiment scores for over 1M article and video texts. I don't know Java, so I will need some help.</p>

<p>I successfully installed Stanford CoreNLP 3.6.0, and I have a server running with:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
</code></pre>

<p>Running this http post from my other computer works, and I get an expected response (xxx.xxx.xxx.xxx is the server's IP address):</p>

<pre><code>wget --post-data 'the quick brown fox jumped over the lazy dog' 'xxx.xxx.xxx.xxx:9000/?properties={""tokenize.whitespace"": ""true"", ""annotators"": ""tokenize,ssplit,pos,lemma,parse"", ""outputFormat"": ""json""}' -O -
</code></pre>

<p>However, the response doesn't contain sentiment. The obvious solution would be to add an annotator:</p>

<pre><code>wget --post-data 'the quick brown fox jumped over the lazy dog' 'xxx.xxx.xxx.xxx:9000/?properties={""tokenize.whitespace"": ""true"", ""annotators"": ""tokenize,ssplit,pos,lemma,parse,sentiment"", ""outputFormat"": ""json""}' -O -
</code></pre>

<p>However, on the server side, I get this error:</p>

<pre><code>java.lang.IllegalArgumentException: Unknown annotator: sentiment
at edu.stanford.nlp.pipeline.StanfordCoreNLP.ensurePrerequisiteAnnotators(StanfordCoreNLP.java:281)
at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.getProperties(StanfordCoreNLPServer.java:476)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$CoreNLPHandler.handle(StanfordCoreNLPServer.java:350)
at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
at sun.net.httpserver.AuthFilter.doFilter(Unknown Source)
at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(Unknown Source)
at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
at sun.net.httpserver.ServerImpl$Exchange.run(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.thread.run(Unknown Source)
</code></pre>

<p>The next obvious solution would be to add a parameter to starting the server, which runs:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators ""tokenize,ssplit,pos,lemma,parse,sentiment""
</code></pre>

<p>Running the same http posts from before gives the same exact result and error, respectively.</p>

<p>Am I doing something wrong, or is there some modification to the core code that it needs to work? I don't know Java, so I am unable to make those changes.</p>

<p>As a side note, this similar command starts a console, and seems to load sentiment correctly:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,parse,sentiment""

[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sentiment

Entering interactive shell. Type q RETURN or EOF to quit.
NLP&gt; _
</code></pre>
","java, server, stanford-nlp, sentiment-analysis","<p>Try running with the <a href=""https://github.com/stanfordnlp/CoreNLP"">GitHub version</a> of the code. Your first solution is correct -- the fact that it could not find the sentiment annotator is a bug in the code:</p>

<pre><code>wget --post-data 'the quick brown fox jumped over the lazy dog' 'xxx.xxx.xxx.xxx:9000/?properties={""annotators"": ""tokenize,ssplit,pos,lemma,parse,sentiment"", ""outputFormat"": ""json""}' -O -
</code></pre>

<p>(A side note: the <code>tokenize.whitespace</code> property is in the documentation to show that you can pass in arbitrary properties, but I recommend against using it in production).</p>
",5,6,3566,2016-02-09 23:15:20,https://stackoverflow.com/questions/35304083/how-do-i-set-up-a-stanford-corenlp-server-on-windows-to-return-sentiment-for-tex
Finding start and end point of sentence in a paragraph StanfordCoreNLP,"<p>I was wondering how I can find the start and end position of a sentence in a paragraph using StanfordCoreNLP. Right now I am using DocumentPreprocessor to split the paragraph into sentences. Is it possible to get the start and end index of where the sentence is actually located in the original text?</p>

<p>I am using the code from another question asked on here.</p>

<pre><code>String paragraph = ""My 1st sentence. “Does it work for questions?” My third sentence."";
Reader reader = new StringReader(paragraph);
DocumentPreprocessor dp = new DocumentPreprocessor(reader);
List&lt;String&gt; sentenceList = new ArrayList&lt;String&gt;();

for (List&lt;HasWord&gt; sentence : dp) {
   String sentenceString = Sentence.listToString(sentence);
   sentenceList.add(sentenceString.toString());
}

for (String sentence : sentenceList) {
   System.out.println(sentence);
}
</code></pre>

<p>Taken from: <a href=""https://stackoverflow.com/questions/9492707/how-can-i-split-a-text-into-sentences-using-the-stanford-parser"">How can I split a text into sentences using the Stanford parser?</a></p>

<p>Thanks</p>
","java, indexing, split, stanford-nlp, sentence","<p>The quick and dirty way to do this would be:</p>

<pre><code>import edu.stanford.nlp.simple.*;

Document doc = new Document(""My 1st sentence. “Does it work for questions?” My third sentence."");
for (Sentence sentence : doc.sentences()) {
  System.out.println(sentence.characterOffsetBegin(0) + "" -- "" + sentence.characterOffsetEnd(sentence.length() - 1));
}
</code></pre>

<p>Otherwise, you can extract the <code>CharacterOffsetBeginAnnotation</code> and <code>CharacterOffsetEndAnnotation</code> from a CoreLabel, and use that to find the token's offset in the original text.</p>
",2,0,1037,2016-02-10 00:06:36,https://stackoverflow.com/questions/35304604/finding-start-and-end-point-of-sentence-in-a-paragraph-stanfordcorenlp
Resolving how square brackets vs parenthesis are conceptually handled in pipeline,"<p>When I parse a sentence that contains left and right square brackets, the parse is much, much slower and different than if the sentence contained left and right parentheses and the default normalizeOtherBrackets value of true is kept (I would say 20 seconds vs 3 seconds for the ParserAnnotator to be run).  If this is property is instead set to false, than the parse times of the brackets vs the parentheses are pretty comparable however the parse trees are still very different.  With a true value for text with brackets, the POS is -LRB- whereas the POS is CD for false but in each case the general substructure of the tree is the same.</p>

<p>In the case of my corpus, the brackets are overwhelmingly meant to ""clarify the antecedent"" as described in <a href=""https://www.scribendi.com/advice/square_brackets_curly_brackets_angle_brackets.en.html"" rel=""nofollow"">this site</a>.  However, the PRN phrase-level label exists for parentheses and not for square brackets and so the formation of the tree is inherently different even if they have close to the same function in the sentence.  </p>

<p>So, please explain how the parse times are so different and what can be done to get a proper parse?  Obviously, a simplistic approach would be to replace brackets with parens, but that does not seem like a satisfying solution.  Are there any settings that can provide me with some relief?  Here is my code:</p>

<pre><code>private void execute() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");
    props.setProperty(""tokenize.options"", ""normalizeOtherBrackets=false"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);    

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    long start = System.nanoTime();
    ParserAnnotator pa = new ParserAnnotator(DefaultPaths.DEFAULT_PARSER_MODEL, false, 60, new String[0]);
    pa.annotate(document);
    long end = System.nanoTime();
    System.out.println((end - start) / 1000000 + "" ms"");

    for(CoreMap sentence: sentences) {
        System.out.println(sentence);
        Tree cm = sentence.get(TreeAnnotation.class);
        cm.pennPrint();
        List&lt;CoreLabel&gt; cm2 = sentence.get(TokensAnnotation.class);
        for (CoreLabel c : cm2) {
            System.out.println(c);
        }
    }       
}
</code></pre>
",stanford-nlp,"<p>Yes, this a known (and very tricky) issue, and unfortunately there is no perfect solution for this.</p>

<p>The problem with such sentences is mainly caused by the fact that these parenthetical constructions don't appear in the training data of the parser and by the way the part-of-speech tagger and the parser interact. </p>

<p>If you run the tokenizer with the option <code>normalizeOtherBrackets=true</code>, then the square brackets will be replaced with <code>-LSB-</code> and <code>-RSB-</code>. Now in most (or maybe even in all) of the training examples for the part-of-speech tagger <code>-LSB-</code> has the POS tag <code>-LRB-</code>, and <code>-RSB-</code> has the tag <code>-RRB-</code>. Therefore the POS-tagger, despite taking context into account, basically always assigns these tags to squared brackets.</p>

<p>Now if you run the POS-tagger before the parser, then the parser operates in delexicalized mode which means it tries to parse the sentence based on the POS tags of the individual words instead of based on the words themselves. Assuming that your sentence will have a POS sequence similar to <code>-LRB- NN -RRB-</code> the parser tries to find production rules so that it can parse this sequence. However, the parser's training data doesn't contain such sequences and therefore there aren't any such production rules, and consequently the parser is unable to parse a sentence with this POS sequence.</p>

<p>When this happens, the parser goes into fallback mode which relaxes the constraints on the part-of-speech tags by assigning a very low probability to all the other possible POS tags of every word so that it can eventually parse the sentence according to the production rules of the parser. This step, however, requires a lot of computation and therefore it takes so much longer to parse such sentences.</p>

<p>On the other hand, if you set <code>normalizeOtherBrackets</code> to <code>false</code>, then it won't translate squared brackets to <code>-LSB-</code> and <code>-RSB-</code> and the POS tagger will assign tags to the original <code>[</code> and <code>]</code>. However, our training data was preprocessed such that all squared brackets have been replaced and therefore <code>[</code> and <code>]</code> are treated as unknown words and the POS tagger assigns tags purely based on the context which most likely results in a POS sequence that is part of the training data of our parser and tagger. This in return means that the parser is able to parse the sentence without entering into recovery mode which is a lot faster.</p>

<p>If your data contains a lot of sentences with such constructions, I would recommend that you don't run the POS tagger before the parser (by removing <code>pos</code> from the list of annotators). This should still give you faster but still reliable parses. Otherwise, if you have access to a treebank, you could also manually add a few trees with these constructions and train a new parsing model. I would not set <code>normalizeOtherBrackets</code> to <code>false</code> because then your tokenized input will differ from the training data of the parser and the tagger which will potentially give you worse results.</p>
",1,0,999,2016-02-12 00:34:25,https://stackoverflow.com/questions/35352788/resolving-how-square-brackets-vs-parenthesis-are-conceptually-handled-in-pipelin
Getting NLTK tree leaf values as a string,"<p>I'm trying to get leaf values in the Tree object as a string. The tree object here is the output of the Stanford Parser.</p>

<p>Here is my code:</p>

<pre><code>from nltk.parse import stanford
Parser = stanford.StanfordParser(""path"")


example = ""Selected variables by univariate/multivariate analysis, constructed logistic regression, calibrated the low defaults portfolio to benchmark ratings, performed back""
sentences = Parser.raw_parse(example)
for line in sentences:
    for sentence in line:
        tree = sentence
</code></pre>

<p>And this is how I extract the VP (Verb Phrases) leaves.</p>

<pre><code>VP=[]

VP_tree = list(tree.subtrees(filter=lambda x: x.label()=='VP'))

for i in VP_tree:
    VP.append(' '.join(i.flatten()))
</code></pre>

<p>Here is what i.flatten() looks like: (it returns parsed word list)</p>

<pre><code>(VP
  constructed
  logistic
  regression
  ,
  calibrated
  the
  low
  defaults
  portfolio
  to
  benchmark
  ratings)
</code></pre>

<p>Becuase I could only get them as a list of parsed words, I joined them with '  '. Therefore there is a space between 'regression' and ','. </p>

<pre><code>In [33]: VP
Out [33]: [u'constructed logistic regression , calibrated the low defaults portfolio to benchmark ratings']
</code></pre>

<p>I'd like to get the Verb Phrase as a string (not as a list of parsed words) without having to join them by ' '.</p>

<p>I have looked at methods under Tree class (<a href=""http://www.nltk.org/_modules/nltk/tree.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/tree.html</a>) however got no luck so far.</p>
","python, tree, nlp, nltk, stanford-nlp","<p>To retrieve the strings as per the input positions, you should consider using <a href=""https://github.com/smilli/py-corenlp"" rel=""nofollow"">https://github.com/smilli/py-corenlp</a> instead of the NLTK API to Stanford tools.</p>

<p>First you have to download, install and setup the Stanford CoreNLP, see <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started</a></p>

<p>Then install the python wrapper to CoreNLP, <a href=""https://github.com/smilli/py-corenlp"" rel=""nofollow"">https://github.com/smilli/py-corenlp</a> </p>

<p>Then, <strong>after starting the server</strong> (many people miss this step!), in python, you can do this:</p>

<pre><code>&gt;&gt;&gt; from pycorenlp import StanfordCoreNLP
&gt;&gt;&gt; stanford = StanfordCoreNLP('http://localhost:9000')
&gt;&gt;&gt; text = (""Selected variables by univariate/multivariate analysis, constructed logistic regression, calibrated the low defaults portfolio to benchmark ratings, performed back"")
&gt;&gt;&gt; output = stanford.annotate(text, properties={'annotators': 'tokenize,ssplit,pos,depparse,parse', 'outputFormat': 'json'})
&gt;&gt;&gt; print(output['sentences'][0]['parse'])
(ROOT
  (SINV
    (VP (VBN Selected)
      (NP (NNS variables))
      (PP (IN by)
        (NP
          (NP (JJ univariate/multivariate) (NN analysis))
          (, ,)
          (VP (VBN constructed)
            (NP (JJ logistic) (NN regression)))
          (, ,))))
    (VP (VBD calibrated))
    (NP
      (NP
        (NP (DT the) (JJ low) (NNS defaults) (NN portfolio))
        (PP (TO to)
          (NP (JJ benchmark) (NNS ratings))))
      (, ,)
      (VP (VBN performed)
        (ADVP (RB back))))))
</code></pre>

<p>To retrieve the VP strings as per the input string, you would have to traverse the JSON output using the <code>characterOffsetBegin</code> and <code>characterOffsetEnd</code>:</p>

<pre><code>&gt;&gt;&gt; output['sentences'][0]
{u'tokens': [{u'index': 1, u'word': u'Selected', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 8, u'characterOffsetBegin': 0, u'originalText': u'Selected', u'before': u''}, {u'index': 2, u'word': u'variables', u'after': u' ', u'pos': u'NNS', u'characterOffsetEnd': 18, u'characterOffsetBegin': 9, u'originalText': u'variables', u'before': u' '}, {u'index': 3, u'word': u'by', u'after': u' ', u'pos': u'IN', u'characterOffsetEnd': 21, u'characterOffsetBegin': 19, u'originalText': u'by', u'before': u' '}, {u'index': 4, u'word': u'univariate/multivariate', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 45, u'characterOffsetBegin': 22, u'originalText': u'univariate/multivariate', u'before': u' '}, {u'index': 5, u'word': u'analysis', u'after': u'', u'pos': u'NN', u'characterOffsetEnd': 54, u'characterOffsetBegin': 46, u'originalText': u'analysis', u'before': u' '}, {u'index': 6, u'word': u',', u'after': u' ', u'pos': u',', u'characterOffsetEnd': 55, u'characterOffsetBegin': 54, u'originalText': u',', u'before': u''}, {u'index': 7, u'word': u'constructed', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 67, u'characterOffsetBegin': 56, u'originalText': u'constructed', u'before': u' '}, {u'index': 8, u'word': u'logistic', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 76, u'characterOffsetBegin': 68, u'originalText': u'logistic', u'before': u' '}, {u'index': 9, u'word': u'regression', u'after': u'', u'pos': u'NN', u'characterOffsetEnd': 87, u'characterOffsetBegin': 77, u'originalText': u'regression', u'before': u' '}, {u'index': 10, u'word': u',', u'after': u' ', u'pos': u',', u'characterOffsetEnd': 88, u'characterOffsetBegin': 87, u'originalText': u',', u'before': u''}, {u'index': 11, u'word': u'calibrated', u'after': u' ', u'pos': u'VBD', u'characterOffsetEnd': 99, u'characterOffsetBegin': 89, u'originalText': u'calibrated', u'before': u' '}, {u'index': 12, u'word': u'the', u'after': u' ', u'pos': u'DT', u'characterOffsetEnd': 103, u'characterOffsetBegin': 100, u'originalText': u'the', u'before': u' '}, {u'index': 13, u'word': u'low', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 107, u'characterOffsetBegin': 104, u'originalText': u'low', u'before': u' '}, {u'index': 14, u'word': u'defaults', u'after': u' ', u'pos': u'NNS', u'characterOffsetEnd': 116, u'characterOffsetBegin': 108, u'originalText': u'defaults', u'before': u' '}, {u'index': 15, u'word': u'portfolio', u'after': u' ', u'pos': u'NN', u'characterOffsetEnd': 126, u'characterOffsetBegin': 117, u'originalText': u'portfolio', u'before': u' '}, {u'index': 16, u'word': u'to', u'after': u' ', u'pos': u'TO', u'characterOffsetEnd': 129, u'characterOffsetBegin': 127, u'originalText': u'to', u'before': u' '}, {u'index': 17, u'word': u'benchmark', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 139, u'characterOffsetBegin': 130, u'originalText': u'benchmark', u'before': u' '}, {u'index': 18, u'word': u'ratings', u'after': u'', u'pos': u'NNS', u'characterOffsetEnd': 147, u'characterOffsetBegin': 140, u'originalText': u'ratings', u'before': u' '}, {u'index': 19, u'word': u',', u'after': u' ', u'pos': u',', u'characterOffsetEnd': 148, u'characterOffsetBegin': 147, u'originalText': u',', u'before': u''}, {u'index': 20, u'word': u'performed', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 158, u'characterOffsetBegin': 149, u'originalText': u'performed', u'before': u' '}, {u'index': 21, u'word': u'back', u'after': u'', u'pos': u'RB', u'characterOffsetEnd': 163, u'characterOffsetBegin': 159, u'originalText': u'back', u'before': u' '}], u'index': 0, u'basic-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'Selected'}, {u'dep': u'dobj', u'dependent': 2, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'variables'}, {u'dep': u'case', u'dependent': 3, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'by'}, {u'dep': u'amod', u'dependent': 4, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'univariate/multivariate'}, {u'dep': u'nmod', u'dependent': 5, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'analysis'}, {u'dep': u'punct', u'dependent': 6, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u','}, {u'dep': u'acl', u'dependent': 7, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'constructed'}, {u'dep': u'amod', u'dependent': 8, u'governorGloss': u'regression', u'governor': 9, u'dependentGloss': u'logistic'}, {u'dep': u'dobj', u'dependent': 9, u'governorGloss': u'constructed', u'governor': 7, u'dependentGloss': u'regression'}, {u'dep': u'punct', u'dependent': 10, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u','}, {u'dep': u'dep', u'dependent': 11, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'calibrated'}, {u'dep': u'det', u'dependent': 12, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'the'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'low'}, {u'dep': u'compound', u'dependent': 14, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'defaults'}, {u'dep': u'nsubj', u'dependent': 15, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'portfolio'}, {u'dep': u'case', u'dependent': 16, u'governorGloss': u'ratings', u'governor': 18, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 17, u'governorGloss': u'ratings', u'governor': 18, u'dependentGloss': u'benchmark'}, {u'dep': u'nmod', u'dependent': 18, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'ratings'}, {u'dep': u'punct', u'dependent': 19, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u','}, {u'dep': u'acl', u'dependent': 20, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'performed'}, {u'dep': u'advmod', u'dependent': 21, u'governorGloss': u'performed', u'governor': 20, u'dependentGloss': u'back'}], u'parse': u'(ROOT\n  (SINV\n    (VP (VBN Selected)\n      (NP (NNS variables))\n      (PP (IN by)\n        (NP\n          (NP (JJ univariate/multivariate) (NN analysis))\n          (, ,)\n          (VP (VBN constructed)\n            (NP (JJ logistic) (NN regression)))\n          (, ,))))\n    (VP (VBD calibrated))\n    (NP\n      (NP\n        (NP (DT the) (JJ low) (NNS defaults) (NN portfolio))\n        (PP (TO to)\n          (NP (JJ benchmark) (NNS ratings))))\n      (, ,)\n      (VP (VBN performed)\n        (ADVP (RB back))))))', u'collapsed-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'Selected'}, {u'dep': u'dobj', u'dependent': 2, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'variables'}, {u'dep': u'case', u'dependent': 3, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'by'}, {u'dep': u'amod', u'dependent': 4, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'univariate/multivariate'}, {u'dep': u'nmod:by', u'dependent': 5, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'analysis'}, {u'dep': u'punct', u'dependent': 6, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u','}, {u'dep': u'acl', u'dependent': 7, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'constructed'}, {u'dep': u'amod', u'dependent': 8, u'governorGloss': u'regression', u'governor': 9, u'dependentGloss': u'logistic'}, {u'dep': u'dobj', u'dependent': 9, u'governorGloss': u'constructed', u'governor': 7, u'dependentGloss': u'regression'}, {u'dep': u'punct', u'dependent': 10, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u','}, {u'dep': u'dep', u'dependent': 11, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'calibrated'}, {u'dep': u'det', u'dependent': 12, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'the'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'low'}, {u'dep': u'compound', u'dependent': 14, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'defaults'}, {u'dep': u'nsubj', u'dependent': 15, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'portfolio'}, {u'dep': u'case', u'dependent': 16, u'governorGloss': u'ratings', u'governor': 18, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 17, u'governorGloss': u'ratings', u'governor': 18, u'dependentGloss': u'benchmark'}, {u'dep': u'nmod:to', u'dependent': 18, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'ratings'}, {u'dep': u'punct', u'dependent': 19, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u','}, {u'dep': u'acl', u'dependent': 20, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'performed'}, {u'dep': u'advmod', u'dependent': 21, u'governorGloss': u'performed', u'governor': 20, u'dependentGloss': u'back'}], u'collapsed-ccprocessed-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'Selected'}, {u'dep': u'dobj', u'dependent': 2, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'variables'}, {u'dep': u'case', u'dependent': 3, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'by'}, {u'dep': u'amod', u'dependent': 4, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'univariate/multivariate'}, {u'dep': u'nmod:by', u'dependent': 5, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'analysis'}, {u'dep': u'punct', u'dependent': 6, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u','}, {u'dep': u'acl', u'dependent': 7, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u'constructed'}, {u'dep': u'amod', u'dependent': 8, u'governorGloss': u'regression', u'governor': 9, u'dependentGloss': u'logistic'}, {u'dep': u'dobj', u'dependent': 9, u'governorGloss': u'constructed', u'governor': 7, u'dependentGloss': u'regression'}, {u'dep': u'punct', u'dependent': 10, u'governorGloss': u'analysis', u'governor': 5, u'dependentGloss': u','}, {u'dep': u'dep', u'dependent': 11, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'calibrated'}, {u'dep': u'det', u'dependent': 12, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'the'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'low'}, {u'dep': u'compound', u'dependent': 14, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'defaults'}, {u'dep': u'nsubj', u'dependent': 15, u'governorGloss': u'Selected', u'governor': 1, u'dependentGloss': u'portfolio'}, {u'dep': u'case', u'dependent': 16, u'governorGloss': u'ratings', u'governor': 18, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 17, u'governorGloss': u'ratings', u'governor': 18, u'dependentGloss': u'benchmark'}, {u'dep': u'nmod:to', u'dependent': 18, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'ratings'}, {u'dep': u'punct', u'dependent': 19, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u','}, {u'dep': u'acl', u'dependent': 20, u'governorGloss': u'portfolio', u'governor': 15, u'dependentGloss': u'performed'}, {u'dep': u'advmod', u'dependent': 21, u'governorGloss': u'performed', u'governor': 20, u'dependentGloss': u'back'}]}
</code></pre>

<p>But it doesn't seem to be an easy output to parse to get the character offset since there's no direct link of the parse tree to the offset. Only the dependency triples contains the link to the word ID that links to the offset.</p>

<hr>

<p>To access the tokens and <code>'after'</code> and <code>'before'</code> keys in <code>output['sentences'][0]['tokens']</code> (but sadly no directly link to the parse tree):</p>

<pre><code>&gt;&gt;&gt; tokens = output['sentences'][0]['tokens']
&gt;&gt;&gt; tokens
[{u'index': 1, u'word': u'Selected', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 8, u'characterOffsetBegin': 0, u'originalText': u'Selected', u'before': u''}, {u'index': 2, u'word': u'variables', u'after': u' ', u'pos': u'NNS', u'characterOffsetEnd': 18, u'characterOffsetBegin': 9, u'originalText': u'variables', u'before': u' '}, {u'index': 3, u'word': u'by', u'after': u' ', u'pos': u'IN', u'characterOffsetEnd': 21, u'characterOffsetBegin': 19, u'originalText': u'by', u'before': u' '}, {u'index': 4, u'word': u'univariate/multivariate', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 45, u'characterOffsetBegin': 22, u'originalText': u'univariate/multivariate', u'before': u' '}, {u'index': 5, u'word': u'analysis', u'after': u'', u'pos': u'NN', u'characterOffsetEnd': 54, u'characterOffsetBegin': 46, u'originalText': u'analysis', u'before': u' '}, {u'index': 6, u'word': u',', u'after': u' ', u'pos': u',', u'characterOffsetEnd': 55, u'characterOffsetBegin': 54, u'originalText': u',', u'before': u''}, {u'index': 7, u'word': u'constructed', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 67, u'characterOffsetBegin': 56, u'originalText': u'constructed', u'before': u' '}, {u'index': 8, u'word': u'logistic', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 76, u'characterOffsetBegin': 68, u'originalText': u'logistic', u'before': u' '}, {u'index': 9, u'word': u'regression', u'after': u'', u'pos': u'NN', u'characterOffsetEnd': 87, u'characterOffsetBegin': 77, u'originalText': u'regression', u'before': u' '}, {u'index': 10, u'word': u',', u'after': u' ', u'pos': u',', u'characterOffsetEnd': 88, u'characterOffsetBegin': 87, u'originalText': u',', u'before': u''}, {u'index': 11, u'word': u'calibrated', u'after': u' ', u'pos': u'VBD', u'characterOffsetEnd': 99, u'characterOffsetBegin': 89, u'originalText': u'calibrated', u'before': u' '}, {u'index': 12, u'word': u'the', u'after': u' ', u'pos': u'DT', u'characterOffsetEnd': 103, u'characterOffsetBegin': 100, u'originalText': u'the', u'before': u' '}, {u'index': 13, u'word': u'low', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 107, u'characterOffsetBegin': 104, u'originalText': u'low', u'before': u' '}, {u'index': 14, u'word': u'defaults', u'after': u' ', u'pos': u'NNS', u'characterOffsetEnd': 116, u'characterOffsetBegin': 108, u'originalText': u'defaults', u'before': u' '}, {u'index': 15, u'word': u'portfolio', u'after': u' ', u'pos': u'NN', u'characterOffsetEnd': 126, u'characterOffsetBegin': 117, u'originalText': u'portfolio', u'before': u' '}, {u'index': 16, u'word': u'to', u'after': u' ', u'pos': u'TO', u'characterOffsetEnd': 129, u'characterOffsetBegin': 127, u'originalText': u'to', u'before': u' '}, {u'index': 17, u'word': u'benchmark', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 139, u'characterOffsetBegin': 130, u'originalText': u'benchmark', u'before': u' '}, {u'index': 18, u'word': u'ratings', u'after': u'', u'pos': u'NNS', u'characterOffsetEnd': 147, u'characterOffsetBegin': 140, u'originalText': u'ratings', u'before': u' '}, {u'index': 19, u'word': u',', u'after': u' ', u'pos': u',', u'characterOffsetEnd': 148, u'characterOffsetBegin': 147, u'originalText': u',', u'before': u''}, {u'index': 20, u'word': u'performed', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 158, u'characterOffsetBegin': 149, u'originalText': u'performed', u'before': u' '}, {u'index': 21, u'word': u'back', u'after': u'', u'pos': u'RB', u'characterOffsetEnd': 163, u'characterOffsetBegin': 159, u'originalText': u'back', u'before': u' '}]
</code></pre>
",2,1,5271,2016-02-13 07:38:39,https://stackoverflow.com/questions/35377376/getting-nltk-tree-leaf-values-as-a-string
Stanford CoreNLP lemmatization,"<p>I have written code in java that displays what each of the annotators output such as tokens, pos, semantics, ner but cant figure out how to display lemma is this possible?</p>

<p>So my question is, is it possible to produce a lemma output of what it is doing (the result)?</p>
","java, stanford-nlp","<p><code>CoreLabel</code> class has a <code>lemma()</code> method that returns the lemma. e.g.</p>

<pre><code>// token is a CoreLable instance
String lemma = token.lemma();
</code></pre>
",2,1,654,2016-02-18 11:51:49,https://stackoverflow.com/questions/35480712/stanford-corenlp-lemmatization
How to convert coreNLP generated parse tree into data.tree R package,"<p>I would like the parse tree generated by R package coreNLP into data.tree R package format. The parse tree is generated usng following code:</p>

<pre><code> options( java.parameters = ""-Xmx2g"" ) 
library(NLP)
library(coreNLP)
#initCoreNLP() # change this if downloaded to non-standard location
initCoreNLP(annotators = ""tokenize,ssplit,pos,lemma,parse"")
## Some text.
s &lt;- c(""A rare black squirrel has become a regular visitor to a suburban garden."")
s &lt;- as.String(s)


anno&lt;-annotateString(s)
parse_tree &lt;- getParse(anno)
parse_tree

The output parse tree is as follows:
&gt; parse_tree
[1] ""(ROOT\r\n  (S\r\n    (NP (DT A) (JJ rare) (JJ black) (NN squirrel))\r\n    (VP (VBZ has)\r\n      (VP (VBN become)\r\n        (NP (DT a) (JJ regular) (NN visitor))\r\n        (PP (TO to)\r\n          (NP (DT a) (JJ suburban) (NN garden)))))\r\n    (. .)))\r\n\r\n""
</code></pre>

<p>I found that following posting <a href=""https://stackoverflow.com/questions/33473107/visualize-parse-tree-structure/33536291#33536291"">Visualize Parse Tree Structure</a>
.It coverts openNLP package generated parse tree into a tree format. But the parse tree is different from one generated by coreNLP and also solution doesnot convert to data.tree format which i want.</p>

<p><strong>EDIT</strong>
By adding the 2 lines below we can use the function provided in the posting <a href=""https://stackoverflow.com/questions/33473107/visualize-parse-tree-structure/33536291#33536291"">Visualize Parse Tree Structure</a></p>

<pre><code># this step modifies coreNLP parse tree to mimic openNLP parse tree
parse_tree &lt;- gsub(""[\r\n]"", """", parse_tree)
parse_tree &lt;- gsub(""ROOT"", ""TOP"", parse_tree)

library(igraph)
library(NLP)

parse2graph(parse_tree,  # plus optional graphing parameters
            title = sprintf(""'%s'"", x), margin=-0.05,
            vertex.color=NA, vertex.frame.color=NA,
            vertex.label.font=2, vertex.label.cex=1.5, asp=0.5,
            edge.width=1.5, edge.color='black', edge.arrow.size=0)
</code></pre>

<p>But what i want is to convert the parse tree into <strong>data.tree format</strong> provided by data.tree package</p>
","r, tree, stanford-nlp","<p>Once you have an edgelist, conversion to data.tree is trivial. Replacing only the last bit of the parse2graph function, and moving the styling out of the function:</p>

<pre><code>parse2tree &lt;- function(ptext) {
  stopifnot(require(NLP) &amp;&amp; require(igraph))

  ## Replace words with unique versions
  ms &lt;- gregexpr(""[^() ]+"", ptext)                                      # just ignoring spaces and brackets?
  words &lt;- regmatches(ptext, ms)[[1]]                                   # just words
  regmatches(ptext, ms) &lt;- list(paste0(words, seq.int(length(words))))  # add id to words

  ## Going to construct an edgelist and pass that to igraph
  ## allocate here since we know the size (number of nodes - 1) and -1 more to exclude 'TOP'
  edgelist &lt;- matrix('', nrow=length(words)-2, ncol=2)

  ## Function to fill in edgelist in place
  edgemaker &lt;- (function() {
    i &lt;- 0                                       # row counter
    g &lt;- function(node) {                        # the recursive function
      if (inherits(node, ""Tree"")) {            # only recurse subtrees
        if ((val &lt;- node$value) != 'TOP1') { # skip 'TOP' node (added '1' above)
          for (child in node$children) {
            childval &lt;- if(inherits(child, ""Tree"")) child$value else child
            i &lt;&lt;- i+1
            edgelist[i,1:2] &lt;&lt;- c(val, childval)
          }
        }
        invisible(lapply(node$children, g))
      }
    }
  })()

  ## Create the edgelist from the parse tree
  edgemaker(Tree_parse(ptext))
  tree &lt;- FromDataFrameNetwork(as.data.frame(edgelist))
  return (tree)
}


parse_tree &lt;- ""(ROOT\r\n  (S\r\n    (NP (DT A) (JJ rare) (JJ black) (NN squirrel))\r\n    (VP (VBZ has)\r\n      (VP (VBN become)\r\n        (NP (DT a) (JJ regular) (NN visitor))\r\n        (PP (TO to)\r\n          (NP (DT a) (JJ suburban) (NN garden)))))\r\n    (. .)))\r\n\r\n""
parse_tree &lt;- gsub(""[\r\n]"", """", parse_tree)
parse_tree &lt;- gsub(""ROOT"", ""TOP"", parse_tree)

library(data.tree)

tree &lt;- parse2tree(parse_tree)
tree
SetNodeStyle(tree, style = ""filled,rounded"", shape = ""box"", fillcolor = ""GreenYellow"")
plot(tree)
</code></pre>

<p><a href=""https://i.sstatic.net/RxbQW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RxbQW.jpg"" alt=""enter image description here""></a></p>
",1,4,1346,2016-02-19 03:02:24,https://stackoverflow.com/questions/35496560/how-to-convert-corenlp-generated-parse-tree-into-data-tree-r-package
core nlp truecaseannotator not found,"<p>I just got started with CoreNLP version 3.6.0. I've downloaded this version from <a href=""http://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip"" rel=""nofollow"">this website</a>. Using the commandline pipeline, I've been able to perform the standard pipeline annotators but ran into a problem with the truecase annotator:</p>

<p>Here's a copy of the terminal output:</p>

<blockquote>loadClassifier=edu/stanford/nlp/models/truecase/truecasing.fast.caseless.qn.ser.gz
 mixedCaseMapFile=edu/stanford/nlp/models/truecase/MixDisambiguation.list
classBias=INIT_UPPER:-0.7,UPPER:-0.7,O:0
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""edu/stanford/nlp/models/truecase/truecasing.fast.caseless.qn.ser.gz"" as class path, filename or URL
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1499)
    at edu.stanford.nlp.pipeline.TrueCaseAnnotator.(TrueCaseAnnotator.java:58)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.trueCase(AnnotatorImplementations.java:199)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$10.create(AnnotatorFactories.java:435)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:375)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:139)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:135)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1222)
</blockquote>

<p>Any ideas?</p>
",stanford-nlp,"<p>We tried to make the default models jar a bit smaller and decided to not include this model by default. But it is still contained in the English models jar which you can download from <a href=""http://stanfordnlp.github.io/CoreNLP/history.html"" rel=""nofollow"">release history</a> page.</p>

<p>After you downloaded the jar, make sure to put it in your classpath before you run CoreNLP. The English models jar should also contain everything in <code>stanford-corenlp-3.6.0-models.jar</code>, so you won't need both of them in your classpath.</p>
",1,0,615,2016-02-23 10:26:44,https://stackoverflow.com/questions/35574874/core-nlp-truecaseannotator-not-found
CoreNLP Server does not return entity mentions,"<p>Having downloaded CoreNLP server from <a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow"">here</a> and following <a href=""http://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow"">these instruction</a>, when I include <code>entitymentions</code> as an annotator:</p>

<pre><code>wget --post-data 'Mark Ronson played a concert in New York.' 'localhost:9000/?properties={""tokenize.whitespace"": ""true"", ""annotators"": ""tokenize,ssplit,pos,entitymentions"", ""outputFormat"": ""json""}'
</code></pre>

<p>the returned json is shown below, and although <code>ner</code> was added per token, there's no list of mentions.</p>

<p>Any idea why?</p>

<p>(It is worth mentioning that <a href=""http://corenlp.run/"" rel=""nofollow"">corenlp.run</a> doesn't seem to return them either - seems like the highlights are the results of post-processing).</p>

<pre><code>{
    ""sentences"": [
        {
            ""index"": 0,
            ""parse"": ""SENTENCE_SKIPPED_OR_UNPARSABLE"",
            ""tokens"": [
                {
                    ""index"": 1,
                    ""word"": ""Mark"",
                    ""originalText"": ""Mark"",
                    ""lemma"": ""Mark"",
                    ""characterOffsetBegin"": 0,
                    ""characterOffsetEnd"": 4,
                    ""pos"": ""NNP"",
                    ""ner"": ""PERSON""
                },
                {
                    ""index"": 2,
                    ""word"": ""Ronson"",
                    ""originalText"": ""Ronson"",
                    ""lemma"": ""Ronson"",
                    ""characterOffsetBegin"": 5,
                    ""characterOffsetEnd"": 11,
                    ""pos"": ""NNP"",
                    ""ner"": ""PERSON""
                },
                {
                    ""index"": 3,
                    ""word"": ""played"",
                    ""originalText"": ""played"",
                    ""lemma"": ""play"",
                    ""characterOffsetBegin"": 12,
                    ""characterOffsetEnd"": 18,
                    ""pos"": ""VBD"",
                    ""ner"": ""O""
                },
                {
                    ""index"": 4,
                    ""word"": ""a"",
                    ""originalText"": ""a"",
                    ""lemma"": ""a"",
                    ""characterOffsetBegin"": 19,
                    ""characterOffsetEnd"": 20,
                    ""pos"": ""DT"",
                    ""ner"": ""O""
                },
                {
                    ""index"": 5,
                    ""word"": ""concert"",
                    ""originalText"": ""concert"",
                    ""lemma"": ""concert"",
                    ""characterOffsetBegin"": 21,
                    ""characterOffsetEnd"": 28,
                    ""pos"": ""NN"",
                    ""ner"": ""O""
                },
                {
                    ""index"": 6,
                    ""word"": ""in"",
                    ""originalText"": ""in"",
                    ""lemma"": ""in"",
                    ""characterOffsetBegin"": 29,
                    ""characterOffsetEnd"": 31,
                    ""pos"": ""IN"",
                    ""ner"": ""O""
                },
                {
                    ""index"": 7,
                    ""word"": ""New"",
                    ""originalText"": ""New"",
                    ""lemma"": ""New"",
                    ""characterOffsetBegin"": 32,
                    ""characterOffsetEnd"": 35,
                    ""pos"": ""NNP"",
                    ""ner"": ""LOCATION""
                },
                {
                    ""index"": 8,
                    ""word"": ""York."",
                    ""originalText"": ""York."",
                    ""lemma"": ""York."",
                    ""characterOffsetBegin"": 36,
                    ""characterOffsetEnd"": 41,
                    ""pos"": ""NNP"",
                    ""ner"": ""LOCATION""
                }
            ]
        }
    ]
}
</code></pre>
",stanford-nlp,"<p>For better or worse, we don't currently output the entity mentions to our outputters. The recommended workaround is to post-process the data in the same way the entity mention annotator would: contiguous spans of the same NER are considered an entity mention. I believe all the annotations in the entity mention object are also attached to the component tokens.</p>
",3,4,266,2016-02-23 15:48:53,https://stackoverflow.com/questions/35582020/corenlp-server-does-not-return-entity-mentions
Supervised Learning Approach for Aspect Extraction,"<p>I'm developing an aspect-level sentiment analysis project for online travel reviews of travel domain.</p>

<p>I have a human annotated dataset that has labelled aspect terms, and aspect categories along with their sentiment polarity.</p>

<p>For example;</p>

<pre><code>Sentence:

This beach was a wonderful time for a day party  it had a fun crowd and has a big bar with a great atmosphere. The food was delicious too.
</code></pre>

<p>The above sentence has the following <strong>aspect terms</strong> labelled;</p>

<pre><code>{party#positive C} {crowd#positive C} {bar#positive C} {food#positive C}
</code></pre>

<p>And the following <strong>aspect categories</strong>;</p>

<pre><code>{entertainment#positive C} {accommodation#positive C}
</code></pre>

<p>I want to try a <code>supervised learning</code> approach to train a model to classify aspect terms from sentences. </p>

<p>I'm using <code>Stanford CORENLP</code> library. But confused as to how the training data format should be? and what is the best approach to take.</p>

<p>I have seen people using <code>IOB notation</code> to format training data to train <code>NER</code> systems. Can I use a similar method to get this done? As in, how do I format my training data file to get aspect terms as mentioned above from an input sentence?</p>

<p>If someone can point me in the right direction, I would appreciate that a lot.</p>
","machine-learning, dataset, nlp, stanford-nlp, sentiment-analysis","<p>This problem can be tackled by breaking it down to smaller subtasks. A possible pipeline approach may be:</p>

<ol>
<li><p>The first stage is aspect term extraction which will identify aspect terms in the raw text. This too can be broken down to two  subtasks. Firstly your system will need to label tokens in text that are aspect terms. Let's call the these labelled tokens aspect term mentions. This is called <strong>Named Entity Recognition</strong> (NER). Next, if you have a pre-defined set of aspect term classes, the systems will need to link the aspect term mentions found in the previous task to those classes. This is called <strong>Entity Linking</strong>. It's worth noting that from the example that you give  the labelled dataset is not yet suitable for the above tasks as the labels are not anchored in text. You may be able to create a suitable dataset by guessing which tokens in text do your given labels correspond to. This is similar to the <strong>Distant Supervision</strong> work.</p></li>
<li><p>The next task is aspect term sentiment classification. <strong>Convolutional Neural Networks</strong> have been used for sentence and document sentiment classification but they can probably be adapted for your purposes if at the input you provide a marker for which tokens are being classified. This is called a position embedding in this work: <a href=""http://www.cs.nyu.edu/~thien/pubs/vector15.pdf"" rel=""nofollow"">http://www.cs.nyu.edu/~thien/pubs/vector15.pdf</a></p></li>
</ol>
",3,1,905,2016-02-24 12:19:11,https://stackoverflow.com/questions/35602043/supervised-learning-approach-for-aspect-extraction
CoreNLP local Server crashes with relations (kbp) annotator,"<p>I have downloaded CoreNLP server 3.6.0 from <a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow"">here</a> and followed <a href=""http://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow"">these instruction</a>.</p>

<p>Although it works on <a href=""http://corenlp.run/"" rel=""nofollow"">corenlp.run</a>, when going to <code>localhost:9000</code> and choosing the <code>relations</code> annotator, the server crashes with:</p>

<blockquote>
  <p>edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""edu/stanford/nlp/models/kbp/supervised.ser.gz"" as class path, filename or URL</p>
</blockquote>

<p>Having searched online, <code>supervised.ser.gz</code> is nowhere to be found. Google returns 1 hit - the actual github file where the error above is declared.</p>

<p>What steps does one need to take to have <code>kbp</code> working on a local server?</p>
",stanford-nlp,"<p>Try downloading and compiling the GitHub version of the code, along with the most recent models file. Neither Wikidict nor the KBP relation annotators are in CoreNLP 3.6.0. KBP should work fine with the most recent models jar, but if you want to use the WikiDict models, be sure to download the larger English models download, rather than the default corenlp models download.</p>
",3,0,391,2016-02-25 15:01:12,https://stackoverflow.com/questions/35630911/corenlp-local-server-crashes-with-relations-kbp-annotator
Stanford parser output doesn&#39;t match demo output,"<p>If I use the Stanford CoreNLP neural network dependency parser with the english_SD model, which performed pretty good according to the website (<a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow noreferrer"">link, bottom of the page</a>), it provides completely different results compared to this <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">demo</a>, which I assume is based on the LexicalizedParser (or at least any other one).</p>

<p>If I put the sentence <em>I don't like the car</em> in the demo page, this is the result:</p>

<p><a href=""https://i.sstatic.net/XgQfh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XgQfh.png"" alt=""enter image description here""></a></p>

<p>If I put the same sentence into the neural network parser, it results in this:</p>

<p><a href=""https://i.sstatic.net/koMeb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/koMeb.png"" alt=""enter image description here""></a></p>

<p>In the result of the neural network parser, everything just depends on like. I think it could be due to the different POS-Tags, but I used the CoreNLP Maxent Tagger with the english-bidirectional-distsim.tagger model, so pretty common I think. Any ideas on this?</p>
","parsing, neural-network, stanford-nlp","<p>By default, we use the <code>english-left3words-distsim.tagger</code> model for the tagger which is faster than the bidirectional model but occasionally produces worse results. As both, the constituency parser which is used on the demo page, and the neural network dependency parser which you used, heavily rely on POS tags it is not really surprising that the different POS sequences lead to different parses, especially when the main verb has a function word tag (<code>IN</code> = prepositon) instead of a content word tag (<code>VB</code> = verb, base form).</p>

<p>But also note that the demo outputs dependency parses in the new <a href=""http://universaldependencies.org"" rel=""nofollow"">Universal Dependencies</a> representation, while the <code>english_SD</code> model parses sentences to the old Stanford Dependencies representation. For your example sentence, the correct parses are actually the same but you will see differences for other sentences, especially if they have prepositional phrases which are being treated differently in the new representation. </p>
",2,0,300,2016-02-26 17:07:37,https://stackoverflow.com/questions/35657676/stanford-parser-output-doesnt-match-demo-output
Add ID column to R CoreNLP package tokenizer output using lapply,"<p>I have working code to get tokenizer output from CoreNLP using lapply and do.call. I need help to achieve 2 things if possible:</p>

<ol>
<li>Add Document Id in the apply function itself (Currently code doesn't have this column added)</li>
<li>Achieve the results of do.call in apply function itself (if possible)</li>
</ol>

<p>There is this post  <a href=""https://stackoverflow.com/questions/18357788/parallel-parlapply-setup"">parallel parLapply setup</a> which uses ""lapply"" function. But it just works on the text vector and doesnot take into account and id column. </p>

<p><strong>Code:</strong></p>

<pre><code>#Fake data - Quotes from Great Expectations by Charles Dickens
textcolumn&lt;-c(""The broken heart. You think you will die, but you just keep living, day after day after terrible day."",
              ""We need never be ashamed of our tears."")
DocId &lt;-c(1:length(textcolumn))

options( java.parameters = ""-Xmx2g"" ) 
library(coreNLP)
#initCoreNLP() # change this if downloaded to non-standard location
initCoreNLP(annotators = ""tokenize,ssplit,pos"")

# Function to tokenize
tokenize &lt;- function(textcolumn) {
  tmp&lt;-annotateString(textcolumn)
  tokens&lt;-getToken(tmp)
  colnames(tokens)&lt;-tolower(colnames(tokens))
  tokens[,c(""sentence"", ""id"", ""token"" ,""pos"")]
}

result &lt;- lapply(textcolumn,tokenize)
final &lt;- do.call(rbind,result)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>&gt; final

   sentence id    token  pos
1         1  1      The   DT
2         1  2   broken   JJ
3         1  3    heart   NN
4         1  4        .    .
5         2  1      You  PRP
6         2  2    think  VBP
7         2  3      you  PRP
8         2  4     will   MD
9         2  5      die   VB
10        2  6        ,    ,
11        2  7      but   CC
12        2  8      you  PRP
13        2  9     just   RB
14        2 10     keep  VBP
15        2 11   living   NN
16        2 12        ,    ,
17        2 13      day   NN
18        2 14    after   IN
19        2 15      day   NN
20        2 16    after   IN
21        2 17 terrible   JJ
22        2 18      day   NN
23        2 19        .    .
24        1  1       We  PRP
25        1  2     need  VBP
26        1  3    never   RB
27        1  4       be   VB
28        1  5  ashamed   JJ
29        1  6       of   IN
30        1  7      our PRP$
31        1  8    tears  NNS
32        1  9        .    .
</code></pre>
","r, stanford-nlp, lapply, do.call","<p>I figured out how to add the document id to the output of CoreNLP in the tokenizer function. Since lapply doesn't allow 2 arguments i had to switch to mapply. Also i had to convert the output of the function into a list to get proper output.</p>

<p><strong>Code:</strong></p>

<pre><code>#Fake data - Quotes from Great Expectations by Charles Dickens
textcolumn&lt;-c(""The broken heart. You think you will die, but you just keep living, day after day after terrible day."",
              ""We need never be ashamed of our tears."")
DocId &lt;-c(1:length(textcolumn))

options( java.parameters = ""-Xmx2g"" ) 
library(coreNLP)
#initCoreNLP() # change this if downloaded to non-standard location
initCoreNLP(annotators = ""tokenize,ssplit,pos"")

# Function to tokenize
tokenize &lt;- function(textcolumn,DocId) {
  tmp&lt;-annotateString(textcolumn)
  tokens&lt;-getToken(tmp)
  colnames(tokens)&lt;-tolower(colnames(tokens))
  tokens &lt;- tokens[,c(""sentence"", ""id"", ""token"" ,""pos"")] # keeping only few columns
  colnames(tokens) &lt;- c(""sentence"", ""tokenid"", ""token"" ,""pos"")
  DocId &lt;- rep(DocId,length(tokens[,1]))
  docidtokens&lt;-cbind(DocId,tokens)
  docidtokens &lt;- list(docidtokens) # need to convert into list for proper output
}

result &lt;- mapply(tokenize, textcolumn, DocId)
final &lt;- do.call(rbind,result)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt; print(final, row.names = FALSE)
 DocId sentence tokenid    token  pos
     1        1       1      The   DT
     1        1       2   broken   JJ
     1        1       3    heart   NN
     1        1       4        .    .
     1        2       1      You  PRP
     1        2       2    think  VBP
     1        2       3      you  PRP
     1        2       4     will   MD
     1        2       5      die   VB
     1        2       6        ,    ,
     1        2       7      but   CC
     1        2       8      you  PRP
     1        2       9     just   RB
     1        2      10     keep  VBP
     1        2      11   living   NN
     1        2      12        ,    ,
     1        2      13      day   NN
     1        2      14    after   IN
     1        2      15      day   NN
     1        2      16    after   IN
     1        2      17 terrible   JJ
     1        2      18      day   NN
     1        2      19        .    .
     2        1       1       We  PRP
     2        1       2     need  VBP
     2        1       3    never   RB
     2        1       4       be   VB
     2        1       5  ashamed   JJ
     2        1       6       of   IN
     2        1       7      our PRP$
     2        1       8    tears  NNS
     2        1       9        .    .
</code></pre>
",0,0,227,2016-02-27 19:50:26,https://stackoverflow.com/questions/35674652/add-id-column-to-r-corenlp-package-tokenizer-output-using-lapply
How to make fast stanford Core NLP API?,"<p>I use Stanford Core NLP API; My codes are Below:</p>

<pre><code>public class StanfordCoreNLPTool {


   public static StanfordCoreNLPTool instance;
   private Annotation annotation = new Annotation();
   private Properties props = new Properties();
   private PrintWriter out = new PrintWriter(System.out);;

   private StanfordCoreNLPTool(){}

   public void startPipeLine(String question){
      props = new Properties();
      props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse,mention,
      dcoref, sentiment"");
      annotation = new Annotation(question);
      StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
       // run all the selected Annotators on this text
      pipeline.annotate(annotation);
      pipeline.prettyPrint(annotation, out);
   }

   public static StanfordCoreNLPTool getInstance() {
      if(instance == null){
        synchronized (StanfordCoreNLPTool.class){
            if(instance == null){
                instance = new StanfordCoreNLPTool();
            }
        }
      }
      return instance;
   }
}
</code></pre>

<p>It works fine, but it takes much time; Consider we are using it in a <strong><em>question answering system</em></strong>, so for every new input, <strong>pipeAnnotation</strong> must be run. 
As you know, each time some rules should be fetched, some data trained and etc to yield to an sentence with NLP tags such as POS, NER and ... .</p>

<p>first of all, i wanted to solve the problem with <strong>RMI and EJB</strong>, but it failed, because, <strong>Regardless Of any JAVA Architecture</strong>, for every new sentence, pipeAnnotation should be learnt from the scratch.
look at this log that are printed in my intellij console:</p>

<blockquote>
  <p>Reading POS tagger model from
  edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger
  ... done <strong>[6.1 sec].</strong></p>
  
  <p>Loading classifier from
  edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ...
  <strong>done [8.0 sec].</strong></p>
  
  <p>Loading classifier from
  edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ...
  <strong>done [8.7 sec].</strong> </p>
  
  <p>Loading classifier from
  edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz
  ... <strong>done [5.0 sec].</strong></p>
  
  <p>INFO: Read 25 rules [main] INFO
  edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
  [main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading
  parser from serialized file
  edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ...  done <strong>[4.1
  sec].</strong></p>
</blockquote>

<p>please help me to find a solution to make the program fast.</p>
","performance, parallel-processing, stanford-nlp","<p>The big problem here is that your <code>startAnnotation()</code> method combines two things that should be separated:</p>

<ol>
<li>Constructing an annotation pipeline, including loading large annotator models</li>
<li>Annotating a particular piece of text (a question) and printing the result</li>
</ol>

<p>For making things fast, these two things <strong>must</strong> be separated, as loading an annotation pipeline is slow but should be done only once, while annotating each piece of text is reasonably fast.</p>

<p>The rest is all minor details but FWIW:</p>

<ul>
<li>All the fanciness of the double-checked locking constructing a <code>StanfordCoreNLPTool</code> singleton is doing nothing if you create a new one for every piece of text you annotate. You should construct an annotation pipeline only once, and it might be reasonable to do that as a singleton, but it's probably sufficient to do the initialization in the constructor once you distinguish pipeline construction from text annotation.</li>
<li>The <code>annotation</code> variable can and should be private to the method that annotates one piece of text.</li>
<li>If after these changes you still want model loading to be faster, you could buy an SSD! (My 2012 MBP with an SSD loads models more than 5 times faster than you are reporting.)</li>
<li>If you want to further speed annotation, the main tool is to cut out annotators you don't need or to choose faster versions. E.g., if you're not using coreference, you could delete <code>mention</code> and <code>dcoref</code>.</li>
</ul>
",1,0,1218,2016-03-03 16:16:44,https://stackoverflow.com/questions/35777401/how-to-make-fast-stanford-core-nlp-api
I got a different result when I retrained the sentiment model with Stanford CoreNLP to compare with the related paper&#39;s result,"<p>I downloaded  stanford-corenlp-full-2015-12-09.
And I created a training model with the following command:</p>

<pre><code> java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz
</code></pre>

<p>When I finished training, I found many files in my directory.
<a href=""https://i.sstatic.net/v2fSc.jpg"" rel=""nofollow"">the model list</a></p>

<p>Then I used the evaluation tool from the package and I ran the code like this:</p>

<pre><code>java -cp * edu.stanford.nlp.sentiment.Evaluate -model model-0024-79.82.ser.gz -treebank test.txt
</code></pre>

<p>The test.txt was from trainDevTestTrees_PTB.zip. This is the result about code:</p>

<pre><code>F:\trainDevTestTrees_PTB\trees&gt;java -cp * edu.stanford.nlp.sentiment.Evaluate -model model-0024-79.82.ser.gz -treebank test.txt
EVALUATION SUMMARY
Tested 82600 labels
65331 correct
17269 incorrect
0.790932 accuracy
Tested 2210 roots
890 correct
1320 incorrect
0.402715 accuracy
Label confusion matrix
  Guess/Gold       0       1       2       3       4    Marg. (Guess)
           0     551     340      87      32       6    1016
           1     956    5348    2476     686     191    9657
           2     354    2812   51386    3097     467   58116
           3     146     744    2525    6804    1885   12104
           4       1      11      74     379    1242    1707
Marg. (Gold)    2008    9255   56548   10998    3791

           0        prec=0.54232, recall=0.2744, spec=0.99423, f1=0.36442
           1        prec=0.5538, recall=0.57785, spec=0.94125, f1=0.56557
           2        prec=0.8842, recall=0.90871, spec=0.74167, f1=0.89629
           3        prec=0.56213, recall=0.61866, spec=0.92598, f1=0.58904
           4        prec=0.72759, recall=0.32762, spec=0.9941, f1=0.4518

Root label confusion matrix
  Guess/Gold       0       1       2       3       4    Marg. (Guess)
           0      50      60      12       9       3     134
           1     161     370     147      94      36     808
           2      31     103     102      60      32     328
           3      36      97     123     305     265     826
           4       1       3       5      42      63     114
Marg. (Gold)     279     633     389     510     399

           0        prec=0.37313, recall=0.17921, spec=0.9565, f1=0.24213
           1        prec=0.45792, recall=0.58452, spec=0.72226, f1=0.51353
           2        prec=0.31098, recall=0.26221, spec=0.87589, f1=0.28452
           3        prec=0.36925, recall=0.59804, spec=0.69353, f1=0.45659
           4        prec=0.55263, recall=0.15789, spec=0.97184, f1=0.24561

Approximate Negative label accuracy: 0.638817
Approximate Positive label accuracy: 0.697140
Combined approximate label accuracy: 0.671925
Approximate Negative root label accuracy: 0.702851
Approximate Positive root label accuracy: 0.742574
Combined approximate root label accuracy: 0.722680
</code></pre>

<p>The accuracy about fine-grained and positive/negative was quite different from the paper ""Socher, R., Perelygin, A., Wu, J.Y., Chuang, J., Manning, C.D., Ng, A.Y. and Potts, C., 2013, October. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP) (Vol. 1631, p. 1642)."" 
The paper states the accuracy about fine-grained and positive/negative is higher than mine.
<a href=""https://i.sstatic.net/sdQ84.jpg"" rel=""nofollow"">The records in the paper</a></p>

<p>Were there any problems with my operation? Why was my result different from the paper?</p>
","java, stanford-nlp, sentiment-analysis","<p>The short answer is that the paper used a different system written in Matlab.  The Java system does not match the paper.  Though we do distribute the binary model we trained in Matlab with the English models jar.  So you can RUN the binary model with Stanford CoreNLP, but you cannot TRAIN a binary model with similar performance with Stanford CoreNLP at this time.</p>
",1,2,410,2016-03-03 16:59:09,https://stackoverflow.com/questions/35778325/i-got-a-different-result-when-i-retrained-the-sentiment-model-with-stanford-core
Stanford Parser : frenchFactored.ser.gz,"<p>I am using the Stanford Parser (Version 3.6.0) for French. 
My command line is</p>

<pre><code>java -cp stanford-parser.jar:* edu.stanford.nlp.parser.lexparser.LexicalizedParser -maxlength 30 -outputFormat conll2007 frenchFactored.ser.gz test_french.txt &gt; test_french.conll10
</code></pre>

<p>But I don't get the functions in the output, see : </p>

<p>1   Je  _   CLS CLS _   2   NULL    _   _</p>

<p>2   mange   _   V   V   _   0   root    _   _</p>

<p>3   des _   P   P   _   2   NULL    _   _</p>

<p>4   pommes  _   N   N   _   3   NULL    _   _</p>

<p>5   .   _   PUNC    PUNC    _   2   NULL    _   _</p>

<p>What could have I miss in the command line? </p>
","parsing, stanford-nlp, french","<p>There is a deep learning based French dependency parser in Stanford CoreNLP 3.6.0.</p>

<p>Download Stanford CoreNLP 3.6.0 here:</p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>Also make sure to get the French models jar that is also available on that page.</p>

<p>And then run this command to use the French dependency parser, make sure to have the French models jar in your CLASSPATH:</p>

<pre><code>java -Xmx6g -cp ""*:stanford-corenlp-full-2015-12-09/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-french.properties -file sample-french-document.txt -outputFormat text
</code></pre>
",0,1,744,2016-03-04 13:53:36,https://stackoverflow.com/questions/35797823/stanford-parser-frenchfactored-ser-gz
How to combine the strengths of PCFG (sentence structure) and n-gram models (lexical co occurrence)?,"<p>How to combine the strengths of PCFG (sentence structure) and n-gram models (lexical co-occurrence)?</p>
","nlp, stanford-nlp, n-gram","<p>Have a look at Dan Klein's <a href=""https://www.aclweb.org/anthology/P12-1101"" rel=""nofollow noreferrer"">paper</a>.</p>
",1,0,163,2016-03-05 04:33:20,https://stackoverflow.com/questions/35810053/how-to-combine-the-strengths-of-pcfg-sentence-structure-and-n-gram-models-lex
Obtain word using the POS tags?,"<p>I'm learning to use the CoreNLP and I'd like to know, is there a way to obtain the word using the POS tag. Let me give an example, </p>

<p>Assume the NLP is asked to POS tag ""This is basic testing."" and the result is,</p>

<pre><code>This_DT is_VBZ basic_JJ testing_NN ._. 
</code></pre>

<p>From it, is there way to obtain just the DT's of the sentence and so on? Other than using basic string commands.</p>
",stanford-nlp,"<p>Here is some sample code demonstrating accessing annotations:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.ling.CoreAnnotations.*;
import edu.stanford.nlp.util.*;


public class PipelineExample {

    public static void main (String[] args) throws IOException {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        String text = ""This is basic testing."";
        Annotation annotation = new Annotation(text);
        pipeline.annotate(annotation);
        System.out.println(""---"");
        System.out.println(""text: ""+text);
        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
            for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                System.out.print(""[""+token.word()+"" ""+token.get(CoreAnnotations.PartOfSpeechAnnotation.class)+""]"");
                System.out.println();
            }
        }
    }
}
</code></pre>
",1,0,102,2016-03-05 17:09:29,https://stackoverflow.com/questions/35817105/obtain-word-using-the-pos-tags
Stanford CoreNLP - Phrase level POS,"<p>I'm trying to use the CoreNLP and would like to know if its possible to split the sentence using just the phrase level without going into detail with to the word level. </p>

<p>Basically, I want to analyze a sentence then obtain the phrase tags of it, then obtain the splits into variables. </p>

<p>For example, for a sentence, if it contains the noun phrase(X) and verb phrase (Y), I want to analyze using CoreNLP the X and Y, then obtain X and Y separately into variables. </p>

<p>Any idea on how to do it?</p>
","java, stanford-nlp","<p>There is some sample code in my answer to this question which demonstrates how to access the constituency parse</p>

<p>I provided a sample class called RootFinderExample.java.</p>

<p><a href=""https://stackoverflow.com/questions/34928739/how-to-get-the-root-node-in-stanford-parse-tree/35012082#35012082"">How to get the root node in Stanford Parse-Tree?</a></p>

<p>Here is where the Tree is accessed:</p>

<pre><code>Tree tree = sentence.get(TreeAnnotation.class);
</code></pre>

<p>Here is some documentation on the Tree class:</p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html</a></p>
",1,0,925,2016-03-06 09:34:26,https://stackoverflow.com/questions/35825275/stanford-corenlp-phrase-level-pos
Stanford NLP - VP vs NP,"<p>I have one example where Stanford NLP outputs a weird parse tree for the sentence:</p>

<pre><code>Clean my desk
</code></pre>

<p><a href=""https://i.sstatic.net/B6HNl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B6HNl.png"" alt=""enter image description here""></a></p>

<pre><code>(ROOT
  (NP
    (NP (JJ Clean))
    (NP (PRP$ my) (NN desk))))
</code></pre>

<p>As you can see, it tags the word <code>Clean</code> as an adjective dependent on the verb <code>desk</code> with the whole phrase being tagged as a <code>Noun Phrase</code>, while my expectation is for <code>Clean</code> to be tagged as a verb, and the phase as a <code>Verb Phrase</code>.</p>

<p>The JJ-PRP$-NN combination simply doesn't make sense in English to me. Anyone ever run into something similar? I know that Stanford NLP results sometimes differ based on the sequence (?) of parsing tools run. How to make this tag properly?</p>
",stanford-nlp,"<p>As it happens, if you feed the sentence <code>""Clean my desk""</code> directly to the parser (actually, the 'tokenize', 'ssplit' and 'parse' tools), it gives the following result:</p>

<pre><code>(ROOT (NP (NP (NNP Clean)) (NP (PRP$ my) (NN desk))))
</code></pre>

<p>However, now <code>""Clean""</code> is a Proper Noun - very clever, Stanford. So, if we feed the sentence in with the first word in lowercase - <code>""clean my desk""</code> - we finally get what we are looking for:</p>

<pre><code>(ROOT (S (VP (VB clean) (NP (PRP$ my) (NN desk)))))
</code></pre>

<p>Be careful not to convert the full sentence into lowercase. While testing I've noticed the the word <code>""I""</code> turned into lowercase <code>""i""</code> is tagged as FW (Foreign Word), so only covert the first word to lowercase.</p>
",1,3,1773,2016-03-08 16:10:48,https://stackoverflow.com/questions/35872324/stanford-nlp-vp-vs-np
Stanford CoreNLP API fails to parse some sentences,"<p>I have been trying to use the Stanford CoreNLP API included in the 2015-12-09 release. I start the server using: </p>

<pre><code>java -mx5g -cp ""./*"" edu.stanford.nlp.pipelinStanfordCoreNLPServer
</code></pre>

<p>The server works in general, but fails for some setnences including the following: </p>

<pre><code>""Aside from her specifically regional accent, she reveals by the use of the triad, ``irritable, tense, depressed, a certain pedantic itemization that indicates she has some familiarity with literary or scientific language ( i.e., she must have had at least a high­school education ) , and she is telling a story she has mentally rehearsed some time before.""
</code></pre>

<p>I end up with a result that starts with : </p>

<pre><code>{""sentences"":[{""index"":0,""parse"":""SENTENCE_SKIPPED_OR_UNPARSABLE"",""basic-dependencies"":
</code></pre>

<p>I would greatly appriciate some help in setting this up - am I not including some annotators in the nlp pipeline. </p>

<p>This same sentence works at <a href=""http://corenlp.run/"" rel=""nofollow"">http://corenlp.run/</a></p>
","nlp, stanford-nlp","<p>If you're looking for a dependency parse (like that in <a href=""http://corenlp.run"" rel=""nofollow"">corenlp.run</a>), you should look at the <code>basic-dependencies</code> field rather than the <code>parse</code> field. If you want a constituency parse, you should include the <code>parse</code> annotator in the list of annotators you are sending to the server. By default, the server does not include the parser annotator, as it's relatively slow.</p>
",1,1,482,2016-03-16 17:55:05,https://stackoverflow.com/questions/36043355/stanford-corenlp-api-fails-to-parse-some-sentences
How to use Python API of Stanford NER?,"<h1>Version</h1>

<ul>
<li>Windows 7</li>
<li>Python 2.7.10</li>
<li>NLTK 3.1</li>
<li>Stanford NER 3.6 (2015-12-09)</li>
</ul>

<h1>Issue</h1>

<p>I trained a custom NER model with Stanford NER and got a serialized model. Then I tried using the model to carry out NER on unseen corpus via Python API provided by NLTK.</p>

<p>According to the <a href=""http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford"" rel=""nofollow"">documentation</a>, I should specify the path to model and the path to <code>stanford-ner.jar</code>. However, I need to specify both the path to <code>stanford-ner.jar</code> and the path to <code>slf4j-api.jar</code> because the Stanford NER requires the logging module.</p>

<p>I could not figure out how to specify the two paths in the NLTK API. The constructor takes two arguments where the first one is <code>path/to/model</code> and the second one is <code>path/to/jar</code>. I tried concatenating two jar paths, putting them in a list and a tuple but none of the methods worked.</p>

<p>How can I tell NLTK to find both jars in order to invoke prediction?</p>
","python, jar, nltk, stanford-nlp","<p>Could you provide any code you tried? By looking at the <a href=""http://www.nltk.org/_modules/nltk/tag/stanford.html"" rel=""nofollow"">source code from the nltk.tag.stanford module</a> it looks like you just have to initiate the StanfordNERTagger with the model and the path to the stanford-ner JAR, and the logger is added to the classpath automatically. </p>

<p>This is what the <code>__init__()</code> method from the StanfordTagger superclass does right after setting the model and the engine. It looks for the logger JAR inside the parent folder of the stanford-ner JAR path you provide and adds it to the path implicitly by calling <code>find_jars_within_path()</code> from <code>nltk.internals</code>, which under the hood is appending the folder to <code>os.path</code></p>
",1,0,261,2016-03-17 11:20:03,https://stackoverflow.com/questions/36059026/how-to-use-python-api-of-stanford-ner
Stanford NLP POS Tagger has issues with very simple phrases?,"<p>I found examples of inconsistent behavior in my application using Stanford NLP Parser/POS Tagger and I was able to replicate it online <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/corenlp/process</a> . I am using version 3.60:</p>

<p>Here are the 3 issues I have found so far: </p>

<ul>
<li>Dot with or without inconsistency problem:</li>
</ul>

<p><a href=""https://i.sstatic.net/ELhNf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ELhNf.png"" alt=""NLP Stanford POS Tagger with and without dot""></a></p>

<ul>
<li><p>Verbs that are found as Nouns
<a href=""https://i.sstatic.net/qHKLm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qHKLm.png"" alt=""enter image description here""></a></p></li>
<li><p>Verbs that are tagged as Adjectives
<a href=""https://i.sstatic.net/Qu6MJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qu6MJ.png"" alt=""enter image description here""></a></p></li>
</ul>

<p>I know that language is fairly ambiguous but I would like to know if I can trust this library even for those simple phrases. I would like to also know if I am doing something wrong. I tried the problematic cases of each of an example alone or in other words in separate sentences and the problem persists.</p>

<p>This is the expected behavior:</p>

<p><a href=""https://i.sstatic.net/vQczS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vQczS.png"" alt=""enter image description here""></a></p>

<p>Any help is appreciated! Thanks</p>
","nlp, stanford-nlp, linguistics, part-of-speech","<p>You're not doing anything wrong. You're of course welcome to decide for yourself how much to trust any tool, but I suspect you'll see similar issues with any parser trained empirically/statistically. As to your issues:</p>

<ul>
<li>Periods are treated like any other token in model building, so, yes, they can influence the parse chosen.</li>
<li>There are indeed a lot of ambiguities in English (as there are in all other human languages), and the question of whether to interpret forms ending in <em>ing</em> as verbs, nouns (verbal nouns or gerunds), or adjectives is a common one. The parser does not always get it right.</li>
<li><p>In terms of particular bad choices it made, often they reflect usage/domain mismatches between the parser training data and the sentences you are trying. The training data is predominantly news articles – last millennium news articles for that matter – although we do mix in some other data and occasionally add to it. So:</p>

<ul>
<li>The use of <em>flagging</em> as a verb, common in modern internet developer use, doesn't occur at all in the training data, so it not surprisingly tends to choose JJ for <em>flagging</em>, since that's the analysis of the only cases in the training data.</li>
<li>In news articles <em>drinking</em> is just more commonly a noun, with discussions of <em>underage drinking</em>, <em>coffee drinking</em>, <em>drinking and driving</em>, etc.</li>
</ul></li>
</ul>
",3,1,892,2016-03-20 02:48:42,https://stackoverflow.com/questions/36109717/stanford-nlp-pos-tagger-has-issues-with-very-simple-phrases
CoreNLP Neural Network Dependency Parser - Difference between evaluation during training versus testing,"<p>I am trying to train a new model with the Stanford CoreNLP implementation of the neural network parser of Chen and Manning (2014). During training, I use the <code>-devFile</code> option to do a UAS evaluation on the development set every 100 iterations. After a few thousand iterations I get a fairly good UAS (around 86 per cent). However, after the training is completed and I try to test it on the same development set, I get a UAS of around 15 per cent. I am using the English Universal Dependencies treebank. </p>

<p>Command line options for training:</p>

<pre><code>java edu.stanford.nlp.parser.nndep.DependencyParser -trainFile ~/Datasets/universal-dependencies-1.2/UD_English/en-ud-train.conllu -devFile ~/Datasets/universal-dependencies-1.2/UD_English/en-ud-dev.conllu -embedFile path/to/wordvecs -embeddingSize 100 -model nndep.model.txt.gz  -trainingThreads 2
</code></pre>

<p>Command line options for testing:</p>

<pre><code>java edu.stanford.nlp.parser.nndep.DependencyParser -model nndep.model.txt.gz -testFile ~/Datasets/universal-dependencies-1.2/UD_English/en-ud-dev.conllu
</code></pre>

<p>When I use the provided UD model for English everything works fine, and I get a UAS of around 80 per cent on the development set. This leads me to believe that my trained model is subpar, and that I might have missed some needed step or option. But since the evaluation during training gives pretty good results, I am a bit confused. From my understanding there should not be that big of a difference between these two evaluations.</p>

<p>So, what might be the cause of the large discrepancy between the evaluation during training versus that at testing time?</p>
","nlp, stanford-nlp","<p>Answering my own question.
I found the answer to this problem in <a href=""https://stackoverflow.com/a/33788775/6082557"">another thread</a>, although their problem was a bit different.</p>

<p>When you use an embedding size that is different from the default value of 50, you need to pass the -embeddingSize flag when parsing as well. As noted in the above linked thread, this goes for the hidden size parameter as well.</p>

<p>Doing this resolved the problem, and I get a UAS that is equivalent to the one during training.</p>

<p>So, if you use an word embedding or hidden layer size that is different from the default value, you need to pass those parameters when parsing with the model. </p>
",0,0,474,2016-03-21 09:30:28,https://stackoverflow.com/questions/36127253/corenlp-neural-network-dependency-parser-difference-between-evaluation-during
Dependency tree to triplets,"<p>I came across this paper <a href=""http://swrc.kaist.ac.kr/paper/171.pdf"" rel=""nofollow"">http://swrc.kaist.ac.kr/paper/171.pdf</a>, which describes a method to extract triplets from a dependency tree. This result is exactly I want. However the paper only mentioned it is a ""post order tree traversal"". Is there any open source implementation to extract triplets? For example, ""VRLA is held at Los Angeles in 2016."" should generate ""VRLA, is held at, Los Angeles"" and ""VRLA, is held, in 2016"" </p>
","nlp, stanford-nlp, opennlp","<p>You can try taking a look at the Stanford OpenIE system (part of CoreNLP):</p>

<pre><code>new Sentence(""VRLA is held at Los Angeles in 2016"").openieTriples();
</code></pre>

<p>This should generate triples for (VRLA; be held at; Los Angeles) and (VRLA; be held in; 2016). More documentation on usage can be found on the <a href=""http://stanfordnlp.github.io/CoreNLP/openie.html"" rel=""nofollow"">OpenIE Annotator page</a>. More generally, there are a number of OpenIE systems you can take a look at. <a href=""https://knowitall.github.io/ollie/"" rel=""nofollow"">Ollie</a> is perhaps the most prominent, from the University of Washington.</p>
",2,4,1200,2016-03-22 19:43:57,https://stackoverflow.com/questions/36163741/dependency-tree-to-triplets
HTTP Status 500 - javax.servlet.ServletException: java.lang.NoClassDefFoundError: edu/stanford/nlp/tagger/maxent/MaxentTagger,"<p>I am trying to POS tag my query in jsp. when i run it as a java application, it is working fine but when i run it as jsp, i am getting the following exception.</p>

<pre><code>org.apache.jasper.JasperException: javax.servlet.ServletException: java.lang.NoClassDefFoundError: edu/stanford/nlp/tagger/maxent/MaxentTagger
org.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:548)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:454)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:396)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:340)
javax.servlet.http.HttpServlet.service(HttpServlet.java:729)
org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52
</code></pre>

<p>My jsp code is as below.</p>

<pre><code>    &lt;%@ page language=""java"" contentType=""text/html; charset=UTF-8""
    pageEncoding=""UTF-8""%&gt;
&lt;!DOCTYPE html PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"" ""http://www.w3.org/TR/html4/loose.dtd""&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8""&gt;
&lt;title&gt;Insert title here&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;welcome&lt;/h1&gt;
&lt;%@ page import=""Search.SearchMain"" %&gt;
&lt;%@ page import=""java.util.ArrayList"" %&gt;
&lt;%@ page import=""java.util.List"" %&gt;
&lt;% 
    List&lt;String&gt; res =new ArrayList&lt;String&gt;();
    String q = request.getParameter(""sqry"");
    res = SearchMain.search(q,1);
    for(String r : res){%&gt;
        &lt;%=r%&gt;
    &lt;%}
%&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>My java code is given below</p>

<pre><code>package taggerPOS;

import java.util.ArrayList;
import java.util.List;

import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.Sentence;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;

public class Tagger {
    public static List&lt;String&gt; getTag(String query){
        List&lt;String&gt; nouns = new ArrayList&lt;String&gt;();
        MaxentTagger tagger = new MaxentTagger(""tagger/english-left3words-distsim.tagger"");
        String[] tokens = query.split(""\\s+"");
        List&lt;HasWord&gt; sent = Sentence.toWordList(tokens);
        List&lt;TaggedWord&gt; taggedSent = tagger.tagSentence(sent);
        //System.out.println(taggedSent);
        for (TaggedWord tw : taggedSent) {
              if (tw.tag().startsWith(""NN"")) {
                //System.out.println(tw.word());
                nouns.add(tw.word());
              }
            } 
        return nouns;
    }
    public static void main(String args[]){
        List&lt;String&gt; n = getTag(""This sentence contains two noun phrases"");
        for(String a:n){
            System.out.println(a);
        }
    }
}
</code></pre>

<p>I have added Stanford-POStagger.jar to class path and tagger model is also added.
<a href=""https://i.sstatic.net/P9Rb4.jpg"" rel=""nofollow"">This is my project structure</a></p>
","stanford-nlp, pos-tagger","<p>Try adding your jars to /WEB-INF/lib folder so tomcat can find them</p>

<p>(as mentioned here: <a href=""https://stackoverflow.com/questions/7676352/java-lang-noclassdeffounderror-org-apache-commons-discovery-tools-discoversingl"">java.lang.NoClassDefFoundError: org/apache/commons/discovery/tools/DiscoverSingleton</a>)</p>
",0,0,1347,2016-03-23 12:28:42,https://stackoverflow.com/questions/36178222/http-status-500-javax-servlet-servletexception-java-lang-noclassdeffounderror
Using Stanford CoreNLP for CorefResolution,"<p>I am trying to use Stanford CoreNLP to perform Coref resolution. The version I use is stanford-corenlp-full-2015-12-09. Basically, I have wrote some classes:</p>

<pre><code>import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Properties;


public class CorefResolution {
    public static String corefResolute(String text, List&lt;String&gt; tokenToReplace) {
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        Annotation doc = new Annotation(text);
        pipeline.annotate(doc);

        Map&lt;Integer, CorefChain&gt; corefs = doc.get(CorefCoreAnnotations.CorefChainAnnotation.class);
        System.out.println(corefs);
        List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
        List&lt;String&gt; resolved = new ArrayList&lt;String&gt;();

        for (CoreMap sentence : sentences) {
            List&lt;CoreLabel&gt; tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);

            for (CoreLabel token : tokens) {

                Integer corefClustId = token.get(CorefCoreAnnotations.CorefClusterIdAnnotation.class);
                token.get(Coref)

                if (corefClustId == null) {
                    System.out.println(""NULL NULL NULL\n"");
                    resolved.add(token.word());
                    continue;
                }
                else {
                    System.out.println(""Exist Exist Exist\n"");
                }

                System.out.println(""coreClustId is ""+corefClustId.toString()+""\n"");
                CorefChain chain = corefs.get(corefClustId);

                if (chain == null || chain.getMentionsInTextualOrder().size() == 1) {
                    resolved.add(token.word());
                } else {
                    int sentINdx = chain.getRepresentativeMention().sentNum - 1;
                    CoreMap corefSentence = sentences.get(sentINdx);
                    List&lt;CoreLabel&gt; corefSentenceTokens = corefSentence.get(CoreAnnotations.TokensAnnotation.class);

                    CorefChain.CorefMention reprMent = chain.getRepresentativeMention();

                    if (tokenToReplace.contains(token.word())) {
                        for (int i = reprMent.startIndex; i &lt; reprMent.endIndex; i++) {
                            CoreLabel matchedLabel = corefSentenceTokens.get(i - 1);
                            resolved.add(matchedLabel.word());
                        }
                    } else {
                        resolved.add(token.word());
                    }
                }
            }
        }

        Detokenizer detokenizer = new Detokenizer();
        String resolvedStr = detokenizer.detokenize(resolved);

        return resolvedStr;
    }
}
</code></pre>

<p>Another class</p>

<pre><code>import java.util.Arrays;
import java.util.List;
import java.util.LinkedList;


public class Detokenizer {

    public String detokenize(List&lt;String&gt; tokens) {
        //Define list of punctuation characters that should NOT have spaces before or after
        List&lt;String&gt; noSpaceBefore = new LinkedList&lt;String&gt;(Arrays.asList("","", ""."","";"", "":"", "")"", ""}"", ""]"", ""'"", ""'s"", ""n't""));
        List&lt;String&gt; noSpaceAfter = new LinkedList&lt;String&gt;(Arrays.asList(""("", ""["",""{"", ""\"""",""""));

        StringBuilder sentence = new StringBuilder();

        tokens.add(0, """");  //Add an empty token at the beginning because loop checks as position-1 and """" is in noSpaceAfter
        for (int i = 1; i &lt; tokens.size(); i++) {
            if (noSpaceBefore.contains(tokens.get(i))
                    || noSpaceAfter.contains(tokens.get(i - 1))) {
                sentence.append(tokens.get(i));
            } else {
                sentence.append("" "" + tokens.get(i));
            }

            // Assumption that opening double quotes are always followed by matching closing double quotes
            // This block switches the "" to the other set after each occurrence
            // ie The first double quotes should have no space after, then the 2nd double quotes should have no space before
            if (""\"""".equals(tokens.get(i - 1))) {
                if (noSpaceAfter.contains(""\"""")) {
                    noSpaceAfter.remove(""\"""");
                    noSpaceBefore.add(""\"""");
                } else {
                    noSpaceAfter.add(""\"""");
                    noSpaceBefore.remove(""\"""");
                }
            }
        }
        return sentence.toString();
    }
}
</code></pre>

<p>Another class file</p>

<pre><code>import java.io.*;
import java.nio.charset.Charset;
import java.util.Arrays;
import java.util.List;


public class PlainTextCorefResolver {

    public static void resolveFile(File inputFile, File outputFile) {
        try {
            BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(inputFile), Charset.forName(""UTF-8"")));
            PrintWriter writer = new PrintWriter(outputFile, ""UTF-8"");


            if (inputFile.exists()) System.out.println(""input exist\n"");
            else System.out.println(""input not exist\n"");

            if (outputFile.exists()) System.out.println(""output exist\n"");
            else System.out.println(""output not exist\n"");

            while(true){
                String line = reader.readLine();
                //EOF
                if(line == null)
                    break;
                //Resolve line
                List&lt;String&gt; tokenToReplace = Arrays.asList(""He"", ""he"", ""She"", ""she"", ""It"", ""it"", ""They"", ""they""); //!!!
                String resolvedLine = CorefResolution.corefResolute(line, tokenToReplace);
                writer.println(resolvedLine);
            }
            reader.close();
            writer.close();

        } catch (Exception e){
            System.err.println(""Failed to open/resolve input file ["" +inputFile.getAbsoluteFile()+ ""] in loader"");
            e.printStackTrace();
            return;
        }

    }


    public static void main(String[] args) {
        String inputFileName = ""path/file.txt"";
        String outputFileName =  ""path/file.resolved.txt"";
        File inputFile = new File(inputFileName);
        File outputFile = new File(outputFileName);
        resolveFile(inputFile, outputFile);
    }

}
</code></pre>

<p>However, it doesn't give any useful result. The corefClusterId is always null, thus I always get a bunch of ""NULL NULL NULL"" outputs. </p>

<p>How can I correctly perform coreference resolution to replace such as ""He/he/She/she/It/it/The stadium/..."" with its most typical mention (person or organization's name)?</p>

<p>For example, given:
""Estadio El Madrigal is a stadium in Spain, used since 1923. It is currently mostly used for football matches.""
I want to get
""Estadio El Madrigal is a stadium in Spain, used since 1923. Estadio El Madrigal is currently mostly used for football matches.""</p>
","java, nlp, stanford-nlp","<p>I don't think our coref system is attaching ""Estadio El Madrigal"" to ""It"" in your example.</p>

<p>Here is some example code for accessing the CorefChains and mentions in general.  </p>

<pre><code>import edu.stanford.nlp.hcoref.CorefCoreAnnotations;
import edu.stanford.nlp.hcoref.data.CorefChain;
import edu.stanford.nlp.hcoref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

import java.util.*;

public class CorefExample {

    public static void main(String[] args) throws Exception {

        Annotation document = new Annotation(""John Kerry is the secretary of state.  He ran for president in 2004."");
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,mention,coref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        pipeline.annotate(document);
        System.out.println(""---"");
        System.out.println(""coref chains"");
        for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
            System.out.println(""\t""+cc);
            System.out.println(cc.getMentionMap());
            List&lt;CorefChain.CorefMention&gt; corefMentions = cc.getMentionsInTextualOrder();
            for (CorefChain.CorefMention cm : corefMentions) {
                System.out.println(""---"");
                System.out.println(""full text: ""+cm.mentionSpan);
                System.out.println(""position: ""+cm.position);
                System.out.println(""start index of first word: ""+cm.startIndex);
            }
        }
        for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
            System.out.println(""---"");
            System.out.println(""mentions"");
            for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
                System.out.println(""\t""+m);
            }
        }
    }
}
</code></pre>

<p>======================<br>
<strong>Update</strong><br>
@StanfordNLPHelper, there is the error I get when using ""coref"" rather than ""dcoref"":</p>

<pre><code>INFO: Read 25 rules
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.3 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention
Using mention detector type: rule
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
    at java.util.Arrays.copyOfRange(Arrays.java:3664)
    at java.lang.String.&lt;init&gt;(String.java:207)
    at java.lang.StringBuilder.toString(StringBuilder.java:407)
    at java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3079)
    at java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:2874)
    at java.io.ObjectInputStream.readString(ObjectInputStream.java:1639)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at java.util.HashMap.readObject(HashMap.java:1394)
    at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:324)
    at edu.stanford.nlp.scoref.SimpleLinearClassifier.&lt;init&gt;(SimpleLinearClassifier.java:30)
    at edu.stanford.nlp.scoref.PairwiseModel.&lt;init&gt;(PairwiseModel.java:75)
    at edu.stanford.nlp.scoref.PairwiseModel$Builder.build(PairwiseModel.java:57)
    at edu.stanford.nlp.scoref.ClusteringCorefSystem.&lt;init&gt;(ClusteringCorefSystem.java:31)
    at edu.stanford.nlp.scoref.StatisticalCorefSystem.fromProps(StatisticalCorefSystem.java:48)
    at edu.stanford.nlp.pipeline.CorefAnnotator.&lt;init&gt;(CorefAnnotator.java:66)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.coref(AnnotatorImplementations.java:220)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$13.create(AnnotatorFactories.java:515)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:375)

Process finished with exit code 1
</code></pre>
",2,2,1183,2016-03-24 16:10:20,https://stackoverflow.com/questions/36204856/using-stanford-corenlp-for-corefresolution
UTF-8 issue with CoreNLP server,"<p>I run a <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">Stanford CoreNLP Server</a> with the following command:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
</code></pre>

<p>I try to parse the sentence <code>Who was Darth Vader’s son?</code>. Note that the apostrophe behind <code>Vader</code> is not an ASCII character.</p>

<p>The <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">online demo</a> successfully parse the sentence:</p>

<p><a href=""https://i.sstatic.net/8bgb1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8bgb1.png"" alt=""screenshot of the online webserver of CoreNLP""></a></p>

<p>The server I run on localhost fails:</p>

<p><a href=""https://i.sstatic.net/eh0m1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eh0m1.png"" alt=""screenshot of the localhost webserver of CoreNLP""></a></p>

<p>I also tried to perform the query using Python.</p>

<pre><code>import requests
url = 'http://localhost:9000/'
sentence = 'Who was Darth Vader’s son?'
r=requests.post(url, params={'properties' : '{""annotators"": ""tokenize,ssplit,pos,ner"", ""outputFormat"": ""json""}'}, data=sentence.encode('utf8'))
tree = r.json()
</code></pre>

<p>The last command raises an exception:</p>

<pre><code>ValueError: Invalid control character at: line 1 column 1172 (char 1171)
</code></pre>

<p>However, I noticed occurrences of the character <code>\x00</code> in the text (i.e. <code>r.text</code>). If I remove them, the json parsing succeeds:</p>

<pre><code>import json
tree = json.loads(r.text.replace('\x00', ''))
</code></pre>

<p>Finally, <code>r.encoding</code> is <code>ISO-8859-1</code>, even though I did not use the option <code>-strict</code> to run the server. Note that it does not change anything if I manually replace it by <code>UTF-8</code>.</p>

<p>If I run the same code replacing <code>url = 'http://localhost:9000/'</code> by <code>url = 'http://corenlp.run/'</code>, then everything succeeds. The call <code>r.json()</code> returns a dict, <code>r.encoding</code> is indeed <code>UTF-8</code>, and no character <code>\x00</code> is in the text.</p>

<p>What is wrong with the CoreNLP server I run?</p>
","utf-8, server, stanford-nlp","<p>This is a known bug with the 3.6.0 release. If you build the server from GitHub, it should work properly with UTF-8 characters. Setting the appropriate Content-Type header in the request will also fix this issue (see <a href=""https://github.com/stanfordnlp/CoreNLP/issues/125"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/issues/125</a>).</p>
",2,-1,1052,2016-03-24 17:29:05,https://stackoverflow.com/questions/36206407/utf-8-issue-with-corenlp-server
French dependency parsing using CoreNLP,"<p>I am following the example in this <a href=""http://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow"">link</a>. I have downloaded the french jar from <a href=""http://nlp.stanford.edu/software/stanford-french-corenlp-2016-01-14-models.jar"" rel=""nofollow"">here</a>. When I call it as follows,</p>

<p><code>java -mx1g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-french.properties -annotators tokenize,ssplit,pos,depparse -file french.txt -outputFormat conllu
</code></p>

<p>I always see it loads a english dep-parser model instead of french.</p>

<p><code>Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ...
PreComputed 100000, Elapsed Time: 1.341 (s)
</code></p>

<p>Is this a bug?</p>
","java, stanford-nlp","<p>Update -- I found that the default properties file does not specify a depparse model. So now I give it my own config file and now it works.</p>

<pre><code>annotators = tokenize, ssplit, pos, depparse, parse

tokenize.language = fr

pos.model = edu/stanford/nlp/models/pos-tagger/french/french.tagger

parse.model = edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz

depparse.model = edu/stanford/nlp/models/parser/nndep/UD_French.gz
</code></pre>
",3,1,750,2016-03-25 15:37:05,https://stackoverflow.com/questions/36223002/french-dependency-parsing-using-corenlp
CoreNLP Server assigns &#39;dep&#39; to all dependencies,"<p>the following request to '<a href=""http://corenlp.run"" rel=""nofollow noreferrer"">http://corenlp.run</a>' assigns the label 'dep' to all dependencies. Can someone explain this behavior? Looks like an issue to me or could this be some limitation (rate-limit) from the public endpoint? However, the <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">web-interface</a> returns the correct response. </p>

<pre><code>wget --post-data ""Having earned a doctorate as a physical chemist, Merkel entered politics in the wake of the Revolutions of 1989, briefly serving as a deputy spokesperson for the first democratically-elected East German Government in 1990. Following German reunification in 1990, Merkel was elected to the Bundestag for Stralsund-Nordvorpommern-Rügen in the state of Mecklenburg-Vorpommern, a seat she has held ever since. Merkel was later appointed as the Minister for Women and Youth in 1991 under Chancellor Helmut Kohl, later becoming the Minister for the Environment in 1994. After Kohl was defeated in 1998, Merkel was elected Secretary-General of the CDU before becoming the party's first woman leader two years later in the aftermath of a donations scandal that toppled Wolfgang Schäuble."" 'http://corenlp.run/?properties={""tokenize.whitespace"": ""true"", ""annotators"": ""tokenize,ssplit,pos,lemma,ner,parse, depparse,mention,coref"", ""outputFormat"": ""json"",'timeout': 30000}' -O -
</code></pre>

<p>For other inputs, the parse attribute response looks quite strange. The Web-interface answer is correct again. Example for a wrong parse response:   </p>

<pre><code>""parse"":""(X ... (X their) (X stomachs) (X while) (X simultaneously) (X appealing) (X to) (X their) (X vanity.) (X The) ...)"" 
</code></pre>

<p>I tried the public endpoint, because the latest compiled release suffers from this <a href=""https://stackoverflow.com/questions/36206407/utf-8-issue-with-corenlp-server"">issue</a> and the Github codebase build instructions seems outdated. I totally miss a guide that describes how to build the *.jars provided in their fate bundle <a href=""http://stanfordnlp.github.io/CoreNLP/index.html#download"" rel=""nofollow noreferrer"">here</a> from the Github repo .</p>

<p><strong>UPDATE:</strong> </p>

<p>Just tried the same request with a local instance and the latest CoreNLP Server. Same issue. Only the web-interface returns the correct reponse. If I remove the parse annotator it works. However, I need both annotations. </p>
","nlp, stanford-nlp","<p>Chances are, you're hitting the server's default <code>parse.maxlen</code> limit of 60. You can override it by explicitly setting the property <code>parse.maxlen=&lt;number_of_tokens&gt;</code> in the properties passed to the server. But, beware: sentences longer than this are liable to take a very long time to parse.</p>

<p>If you only need dependencies, I recommend using the <code>depparse</code> annotator instead. This is what the demo at <a href=""http://corenlp.run"" rel=""nofollow"">corenlp.run</a> uses, and why it works on longer sentences.</p>
",0,0,53,2016-03-25 19:56:30,https://stackoverflow.com/questions/36227077/corenlp-server-assigns-dep-to-all-dependencies
Why I was not getting the complete dependency,"<p>I tried both the online demo and download version of the dependency parser. The online demo's Enhanced Dependencies was able to get accurate results. For example, ""Can you recommend me a movie which was directed by James Cameron and Leonardo DiCaprio?"" generated below results:</p>

<p>root ( ROOT-0 , recommend-3 )
aux ( recommend-3 , Can-1 )
nsubj ( recommend-3 , you-2 )
nsubj ( movie-6 , me-4 )
det ( movie-6 , a-5 )
xcomp ( recommend-3 , movie-6 )
nsubjpass ( directed-9 , which-7 )
nsubjpass ( acted-14 , which-7 ) (extra)
auxpass ( directed-9 , was-8 )
acl:relcl ( movie-6 , directed-9 )
case ( Cameron-12 , by-10 )
compound ( Cameron-12 , James-11 )
nmod:agent ( directed-9 , Cameron-12 )
cc ( directed-9 , and-13 )
acl:relcl ( movie-6 , acted-14 ) (extra)
conj:and ( directed-9 , acted-14 )
case ( DiCaprio-17 , by-15 )
compound ( DiCaprio-17 , Leonardo-16 )
nmod:by ( acted-14 , DiCaprio-17 )</p>

<p>Although the two verb ""directed"" and ""acted"" were separated by a few words, the online demo was able to recognize that they were both referring to the noun ""movie"" (acl:relcl ( movie-6 , directed-9 ), acl:relcl ( movie-6 , acted-14 ) (extra)).</p>

<p>However, below is the result I got with the jar files downloaded from Stanford's website. The CoreNLP version is 3.6.0:</p>

<p>root(ROOT-0, recommend-3)
aux(recommend-3, Can-1)
nsubj(recommend-3, you-2)
nsubj(movie-7, me-4)
det(movie-7, a-5)
amod(movie-7, romantic-6)
dobj(recommend-3, movie-7)
nsubjpass(directed-10, movie-7)
ref(movie-7, which-8)
auxpass(directed-10, was-9)
acl:relcl(movie-7, directed-10)
case(Cameron-13, by-11)
compound(Cameron-13, James-12)
nmod:agent(directed-10, Cameron-13)
cc(Cameron-13, and-14)
nmod:agent(directed-10, acted-15)
conj:and(Cameron-13, acted-15)
case(DiCaprio-18, by-16)
compound(DiCaprio-18, Leonardo-17)
nmod:by(acted-15, DiCaprio-18)
punct(recommend-3, ?-19)</p>

<p>And in this case, the parser failed to get the dependency acl:relcl ( movie-6 , acted-14 ) (extra).</p>

<p>This is my constructor code: </p>

<pre><code>public CoreNlpParser() {
        props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, parse, lemma, depparse, ner"");
        props.setProperty(""depparse.extradependencies"", ""MAXIMAL"");
        pipeline = new StanfordCoreNLP(props);
    }
</code></pre>

<p>I thought that by setting ""depparse.extradependencies"" to ""MAXIMAL"" I should be able to
get the same results as the online demo's. What I was missing and how should I properly configure
the annotators? Thanks very much</p>
",stanford-nlp,"<p>This code should more closely match the online demo.  You want to use the PCFG parser, not the NN dependency parser.</p>

<pre><code>import java.io.*;
import java.util.*;
import java.util.Properties;
import java.util.zip.*;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

public class EnhancedDependenciesDemo {

    public static void main (String[] args) {
        Properties props = StringUtils.argsToProperties(args);
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        Annotation ann = new Annotation(""Can you recommend me a movie "" +
                ""which was directed by James Cameron and acted by Leonardo DiCaprio?"");
        pipeline.annotate(ann);
        for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {
            Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
            TreePrint treePrint = new TreePrint(""typedDependenciesCollapsed"");
            treePrint.printTree(tree);
        }
    }
}
</code></pre>
",0,0,325,2016-03-27 04:13:49,https://stackoverflow.com/questions/36243749/why-i-was-not-getting-the-complete-dependency
How to convert typed dependency to semantic graph,"<p>I've retrieved TypedDependency from tree annotation. Now I want to get the graph structure of the dependencies instead of the original list structure. The SemanticGraphEdge class contains four IndexedWord variables which are source, target, gov and dep, also there is a weight variable. How should I determine these variables from a TypedDependency class? Thank you.</p>
",stanford-nlp,"<pre><code>TypedDependency tp;
SemanticGraph semgraph = new SemanticGraph(td);
</code></pre>
",0,0,176,2016-03-28 01:52:48,https://stackoverflow.com/questions/36254813/how-to-convert-typed-dependency-to-semantic-graph
why does google dataproc does not pull coreNLP jars although they are included in POM file?,"<p>My application is a java maven project that uses Spark. Here's the section in my pom that adds stanford coreNLP dependencies:</p>

<pre><code>        &lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.6.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.6.0&lt;/version&gt;
            &lt;classifier&gt;models&lt;/classifier&gt;
        &lt;/dependency&gt;
</code></pre>

<p>I get the following error:</p>

<pre><code>java.lang.NoClassDefFoundError: edu/stanford/nlp/pipeline/StanfordCoreNLP
</code></pre>

<p>there are other dependencies, e.g. Spark, and dataproc pulls them fine. Now I added coreNLP and it works fine on my laptop, but fails in google dataproc.</p>
","java, maven, stanford-nlp, google-cloud-platform, google-cloud-dataproc","<p>Spark classes are ""provided"" in the Dataproc environment because they're considered part of the base distribution, alongside other Hadoop-related packages like <a href=""http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client"" rel=""nofollow"">hadoop-client</a>. Other libraries which aren't part of the base distribution should be packaged as part of your ""fatjar"" using the <a href=""http://maven.apache.org/plugins/maven-shade-plugin/examples/includes-excludes.html"" rel=""nofollow"">Maven shade plugin</a>.</p>

<p>Generally this is a best practice guideline because the ""provided"" environment should be as un-opinionated as possible about version dependencies, and this way you can bring your own version of corenlp as necessary without worrying about version collisions on the Dataproc environment, or even to use your own forked version of the corenlp libraries.</p>
",2,0,119,2016-03-31 15:03:34,https://stackoverflow.com/questions/36337126/why-does-google-dataproc-does-not-pull-corenlp-jars-although-they-are-included-i
stanford parser can&#39;t read german umlauts,"<p>The stanford parser (<a href=""http://nlp.stanford.edu/software/lex-parser.html"" rel=""nofollow"">http://nlp.stanford.edu/software/lex-parser.html</a>), version 3.6.0, comes with trained grammars for Engish, German and other languages. To parse german text the stanford parser provides the tool lexparser-lang.sh</p>

<pre><code>./lexparser-lang.sh
Usage: lexparser-lang.sh lang len grammar out_file FILE...

  lang       : Language to parse (Arabic, English, Chinese, German, French)
  len        : Maximum length of the sentences to parse
  grammar    : Serialized grammar file (look in the models jar)
  out_file   : Prefix for the output filename
  FILE       : List of files to parse
</code></pre>

<p>So I call it with these options:</p>

<pre><code>sadik@sadix:stanford-parser-full-2015-12-09$ ./lexparser-lang.sh German 500 edu/stanford/nlp/models/lexparser/germanFactored.ser.gz factored german_test.txt
</code></pre>

<p>The input file german_test.txt contains a single German sentence:</p>

<pre><code>Fußball findet um 8 Uhr in der Halle statt.
</code></pre>

<p>But the ""ß"" results in a warning and a wrong result. Same with ""ä"", ""ö"" and ""ü"". Now, lexparser-lang.sh is supposed to be designed to deal with German text as input. Is there any option I am missing?</p>

<p><strong>How it is:</strong></p>

<pre><code>[main] INFO edu.stanford.nlp.parser.lexparser.LexicalizedParser - Loading parser from serialized file edu/stanford/nlp/models/lexparser/germanFactored.ser.gz ... 
 done [3.8 sec].
Parsing file: german_test.txt
Apr 01, 2016 12:48:45 AM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable:  (U+9F, decimal: 159)
Parsing [sent. 1 len. 11]: FuÃ ball findet um 8 Uhr in der Halle statt .
Parsed file: german_test.txt [1 sentences].
Parsed 11 words in 1 sentences (32.07 wds/sec; 2.92 sents/sec).
</code></pre>

<p>With a parse tree that looks like crap:</p>

<pre><code>(S (ADV FuÃ) (ADV ball) (VVFIN findet)
  (PP (APPR um) (CARD 8) (NN Uhr))
  (PP (APPR in) (ART der) (NN Halle))
  (PTKVZ statt) ($. .))
</code></pre>

<p><strong>How it should be</strong></p>

<p>When written ""Fussball"", there is no problem (except incorrect orthography)</p>

<pre><code>[main] INFO edu.stanford.nlp.parser.lexparser.LexicalizedParser - Loading parser from serialized file edu/stanford/nlp/models/lexparser/germanFactored.ser.gz ... 
     done [3.5 sec].
    Parsing file: german_test.txt
    Parsing [sent. 1 len. 10]: Fussball findet um 8 Uhr in der Halle statt .
    Parsed file: german_test.txt [1 sentences].
    Parsed 10 words in 1 sentences (40.98 wds/sec; 4.10 sents/sec).
</code></pre>

<p>The correct tree:</p>

<pre><code>(S (NN Fussball) (VVFIN findet)
  (PP (APPR um) (CARD 8) (NN Uhr))
  (PP (APPR in) (ART der) (NN Halle))
  (PTKVZ statt) ($. .))
</code></pre>
","parsing, stanford-nlp","<p>The demo script is not running the tokenizer with the correct character set.  So if your text is pre-tokenized, you can add the option ""-tokenized"" and it will just use space as the token delimiter.</p>

<p>Also you want to tell the parser to use ""-encoding ISO-8859-1"" for German.</p>

<p>Here is the full java command (alter the one found in the .sh script):</p>

<pre><code>java -Xmx2g -cp ""./*"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -maxLength 500 -tLPP edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams -hMarkov 1 -vMarkov 2 -vSelSplitCutOff 300 -uwm 1 -unknownSuffixSize 2 -nodeCleanup 2 -writeOutputFiles -outputFilesExtension output.500.stp -outputFormat ""penn"" -outputFormatOptions ""removeTopBracket,includePunctuationDependencies"" -encoding ISO_8859-1 -tokenized -loadFromSerializedFile edu/stanford/nlp/models/lexparser/germanFactored.ser.gz german_example.txt
</code></pre>

<p>I get this output:</p>

<pre><code>(NUR
  (S (NN Fußball) (VVFIN findet)
    (PP (APPR um) (CARD 8) (NN Uhr))
    (PP (APPR in) (ART der) (NN Halle) (ADJA statt.))))
</code></pre>

<p>UPDATED AGAIN:</p>

<p>Make sure to separate ""statt."" into ""statt ."" since we are now saying the tokens are white space separated.  If we apply this fix we get this parse:</p>

<pre><code>(S (NN Fußball) (VVFIN findet)
  (PP (APPR um) (CARD 8) (NN Uhr))
  (PP (APPR in) (ART der) (NN Halle))
  (PTKVZ statt) ($. .))
</code></pre>

<p>So just to summarize, basically the issue is we need to tell the PTBTokenizer to use ISO_8859-1 and LexicalizedParser to use ISO_8859-1.  </p>

<p>I would recommend just using the full pipeline to accomplish this.</p>

<ol>
<li><p>Download Stanford CoreNLP 3.6.0 from here: </p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/</a></p></li>
<li><p>Download the German model jar from here:</p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/download.html</a></p></li>
<li><p>Run this command:</p>

<pre><code>java -Xmx3g -cp ""stanford-corenlp-full-2015-12-09/*:stanford-corenlp-3.6.0-models-german.jar"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse -props StanfordCoreNLP-german.properties -file german_example_file.txt -outputFormat text
</code></pre></li>
</ol>

<p>This will tokenize and parse the text and use the correct character encoding.</p>
",1,0,607,2016-03-31 23:10:22,https://stackoverflow.com/questions/36345371/stanford-parser-cant-read-german-umlauts
Named Entity Recognition (Ner) - Organization Name Database,"<p>I'm Working on my current graduating project which is Named Entity Recognition For Turkish. The recognizer should catch Turkish words when i work with Person Names and Locations (Sometimes locations can be in different Languages, for example Hilton Hotels in Taksim/Istanbul) all i need add ""Hotel"" in my dataset which is Full of specific location tags like Hotel , Restaurant or Mall. But when its come to Organization Name Tag. I need to find a good dataset of bands , products , company names, But cant figure out how to find or collect this dataset</p>

<p>In stanford nlp tool : <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/ner/process</a></p>

<p>When i type Facebook , Nike , Adidas etc it can find it's organization. So is there any way to have that organization name Dataset ? </p>
","nlp, nltk, stanford-nlp, opennlp, named-entity-recognition","<p>If you are interested in a data resource with these organizations names. You can use one of the knowledge bases KBs available such as </p>

<ul>
<li><a href=""http://wiki.dbpedia.org/OnlineAccess"" rel=""nofollow"">DBpedia</a></li>
<li><a href=""https://www.mpi-inf.mpg.de/de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/"" rel=""nofollow"">YAGO</a></li>
<li><a href=""http://babelnet.org/guide"" rel=""nofollow"">BabelNet</a> It cannot be downloaded only online access.</li>
<li><a href=""https://www.freebase.com/"" rel=""nofollow"">FreeBase</a></li>
</ul>

<p>All of them have names of these organizations and more, you will need some effort to extract the organizations only using their types. For example, <strong>YAGO</strong> has downloadable file with possible entities and their types. You can filter it on  and then you can use hasMeaning data to get all possible names.</p>

<p>Yago and BabelNet have been used to NER or Named Entity Disambiguation system AIDA and Babelfy.</p>

<p><a href=""https://github.com/yago-naga/aida"" rel=""nofollow"">AIDA</a> offers a robust dataset of possible Entity Names, that can be used for NER. </p>
",4,1,5357,2016-04-01 08:10:46,https://stackoverflow.com/questions/36351251/named-entity-recognition-ner-organization-name-database
Stanford classifier cross validation averaged or aggregate metrics,"<p>With <a href=""http://nlp.stanford.edu/software/classifier.shtml"" rel=""nofollow"">Stanford Classifier</a> it is possible to use cross validation by setting the options in the properties file, such as this for 10-fold cross validation:</p>

<pre><code>crossValidationFolds=10
printCrossValidationDecisions=true
shuffleTrainingData=true
shuffleSeed=1
</code></pre>

<p>Running this will output, per fold, the various metrics, such as precision, recall, Accuracy/micro-averaged F1 and Macro-averaged F1.</p>

<p>Is there an option to get an averaged or otherwise aggregated score of all 10 Accuracy/micro-averaged F1 or all 10 Macro-averaged F1 as part of the output?</p>

<p>In Weka, by default the output after 10-fold cross validation includes averaged metrics over all folds. Is such an option also available in Stanford Classifier? Having a final precision, recall or F1 score available and optimizing the parameters against it like in Weka is very useful, and I would like to do this with Stanford Classifier. How?</p>
","nlp, weka, stanford-nlp","<p>When I run with 10 folds, I am seeing that output.  When I run this command:</p>

<pre><code>java -cp ""*"" edu.stanford.nlp.classify.ColumnDataClassifier -prop examples/cheese2007.prop -crossValidationFolds 10
</code></pre>

<p>I see this in the output (after ### Fold 9)</p>

<pre><code>[main] INFO edu.stanford.nlp.classify.ColumnDataClassifier - 181 examples in test set
[main] INFO edu.stanford.nlp.classify.ColumnDataClassifier - Cls 2: TP=109 FN=6 FP=7 TN=59; Acc 0.928 P 0.940 R 0.948 F1 0.944
[main] INFO edu.stanford.nlp.classify.ColumnDataClassifier - Cls 1: TP=59 FN=7 FP=6 TN=109; Acc 0.928 P 0.908 R 0.894 F1 0.901
[main] INFO edu.stanford.nlp.classify.ColumnDataClassifier - Accuracy/micro-averaged F1: 0.92818
[main] INFO edu.stanford.nlp.classify.ColumnDataClassifier - Macro-averaged F1: 0.92224 
[main] INFO edu.stanford.nlp.classify.ColumnDataClassifier - Average accuracy/micro-averaged F1: 0.93429
[main] INFO edu.stanford.nlp.classify.ColumnDataClassifier - Average macro-averaged F1: 0.92247
</code></pre>
",1,1,397,2016-04-01 16:28:48,https://stackoverflow.com/questions/36361348/stanford-classifier-cross-validation-averaged-or-aggregate-metrics
How to load SR parser file in hdfs in the mapper?,"<p>I am trying to use the CoreNLP project in a mapreduce program to find the sentiment of a large number of text stored in <code>hbase</code> tables. I am using the SR parser for parsing. The model file is stored in hdfs at <code>/user/root/englishSR.ser.gz</code>. I  have added the below line in the mapreduce application code</p>

<pre><code> job.addCacheFile(new URI(""/user/root/englishSR.ser.gz#model""));
</code></pre>

<p>Now in the mapper</p>

<pre><code> props.setProperty(""parse.model"", ""./model"");
</code></pre>

<p>I am getting <code>edu.stanford.nlp.io.RuntimeIOException: java.io.StreamCorruptedException: invalid stream header</code>. 
The <code>pom.xml</code> file contains</p>

<pre><code>&lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.4.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.4.1&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
</code></pre>

<p>I have tried adding the file to <code>resources</code> and adding to the <code>maven</code> with all resulting in <code>GC overhead limit exceeded</code> or Java Heap issues.</p>
","java, hadoop, stanford-nlp","<p>I don't know hadoop well, but I suspect that you're confusing CoreNLP about the compression of the SR parser model.</p>

<p>First try this without using Hadoop:</p>

<pre><code>java -mx4g edu.stanford.nlp.parser.shiftreduce.ShiftReduceParser -serializedPath /user/root/englishSR.ser.gz
</code></pre>

<p>See if that loads the parser fine. If so, it should print something like the below and exit (otherwise, it will throw an exception...).</p>

<pre><code>Loading parser from serialized file edu/stanford/nlp/models/srparser/englishSR.ser.gz ... done [10.4 sec].
</code></pre>

<p>If that loads a parser fine, then there is nothing wrong with the model file. I think the problem is then that CoreNLP simply uses whether a file or resource name ends in "".gz"" to decide whether it is gzipped, and so it wrongly interprets the line:</p>

<pre><code>props.setProperty(""parse.model"", ""./model"");
</code></pre>

<p>as saying to load a not-gzipped model.  So I would hope that one or other of the below would work:</p>

<pre><code>cd /user/root ; gunzip englishSR.ser.gz

job.addCacheFile(new URI(""/user/root/englishSR.ser#model""));

props.setProperty(""parse.model"", ""./model"");
</code></pre>

<p>Or:</p>

<pre><code>job.addCacheFile(new URI(""/user/root/englishSR.ser#model.gz""));

props.setProperty(""parse.model"", ""./model.gz"");
</code></pre>
",1,3,166,2016-04-02 07:17:58,https://stackoverflow.com/questions/36370374/how-to-load-sr-parser-file-in-hdfs-in-the-mapper
How to use both the lexicalized and the dependency parser in the StanfordCoreNLP pipeline?,"<p>Suppose I have defined the following StanfordCoreNLP pipeline:</p>

<pre><code>Properties props = new Properties();

props.put(""language"", ""english"");
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");
props.put(""depparse.model"", ""edu/stanford/nlp/models/parser/nndep/english_SD.gz"");
props.put(""parse.originalDependencies"", true);

StanfordCoreNLP pipeline =  new StanfordCoreNLP(props);
</code></pre>

<p>Now, this code will give me the tokens, sentence splitter, POS tags, lemmas, NER and depdencency parse (the NN model). Now, I also want to have a lexicalized parse tree as well. </p>

<p>How can I put this information in the pipeline? Or maybe I have to do it otherwise? What is the optimal way to do this?</p>
","java, nlp, stanford-nlp","<p>Instead of just <code>deparse</code>, add <code>parse</code> to the list of annotators:</p>

<pre><code>props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, depparse"");
</code></pre>

<p>For more details, see <a href=""http://hujiaweibujidao.github.io/blog/2016/03/30/Stanford-NLP/"" rel=""nofollow"">http://hujiaweibujidao.github.io/blog/2016/03/30/Stanford-NLP/</a></p>
",1,2,260,2016-04-03 19:09:13,https://stackoverflow.com/questions/36389892/how-to-use-both-the-lexicalized-and-the-dependency-parser-in-the-stanfordcorenlp
Stanford NLP - OpenIE out of memory when processing list of files,"<p>I'm trying to extract information from several files using the OpenIE tool from Stanford CoreNLP, it gives an out of memory error when several files are passed to the input, instead of just one.</p>

<pre><code>All files have been queued; awaiting termination...
java.lang.OutOfMemoryError: GC overhead limit exceeded
at edu.stanford.nlp.graph.DirectedMultiGraph.outgoingEdgeIterator(DirectedMultiGraph.java:508)
at edu.stanford.nlp.semgraph.SemanticGraph.outgoingEdgeIterator(SemanticGraph.java:165)
at edu.stanford.nlp.semgraph.semgrex.GraphRelation$GOVERNER$1.advance(GraphRelation.java:267)
at edu.stanford.nlp.semgraph.semgrex.GraphRelation$SearchNodeIterator.initialize(GraphRelation.java:1102)
at edu.stanford.nlp.semgraph.semgrex.GraphRelation$SearchNodeIterator.&lt;init&gt;(GraphRelation.java:1083)
at edu.stanford.nlp.semgraph.semgrex.GraphRelation$GOVERNER$1.&lt;init&gt;(GraphRelation.java:257)
at edu.stanford.nlp.semgraph.semgrex.GraphRelation$GOVERNER.searchNodeIterator(GraphRelation.java:257)
at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.resetChildIter(NodePattern.java:320)
at edu.stanford.nlp.semgraph.semgrex.CoordinationPattern$CoordinationMatcher.matches(CoordinationPattern.java:211)
at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.matchChild(NodePattern.java:514)
at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.matches(NodePattern.java:542)
at edu.stanford.nlp.naturalli.RelationTripleSegmenter.segmentVerb(RelationTripleSegmenter.java:541)
at edu.stanford.nlp.naturalli.RelationTripleSegmenter.segment(RelationTripleSegmenter.java:850)
at edu.stanford.nlp.naturalli.OpenIE.relationInFragment(OpenIE.java:354)
at edu.stanford.nlp.naturalli.OpenIE.lambda$relationsInFragments$2(OpenIE.java:366)
at edu.stanford.nlp.naturalli.OpenIE$$Lambda$76/1438896944.apply(Unknown Source)
at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1540)
at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
at edu.stanford.nlp.naturalli.OpenIE.relationsInFragments(OpenIE.java:366)
at edu.stanford.nlp.naturalli.OpenIE.annotateSentence(OpenIE.java:486)
at edu.stanford.nlp.naturalli.OpenIE.lambda$annotate$3(OpenIE.java:554)
at edu.stanford.nlp.naturalli.OpenIE$$Lambda$25/606198361.accept(Unknown Source)
at java.util.ArrayList.forEach(ArrayList.java:1249)
at edu.stanford.nlp.naturalli.OpenIE.annotate(OpenIE.java:554)
at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:71)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:499)
at edu.stanford.nlp.naturalli.OpenIE.processDocument(OpenIE.java:630)
DONE processing files. 1 exceptions encountered.
</code></pre>

<p>I pass the files by input using this call:</p>

<pre><code>java -mx3g -cp stanford-corenlp-3.6.0.jar:stanford-corenlp-3.6.0-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar edu.stanford.nlp.naturalli.OpenIE file1 file2 file3 etc.
</code></pre>

<p>I tried increasing the memory with <code>-mx3g</code> and other variants, and although the amount of processed files increases, it's not much (from 5 to 7, for eg.). Each file individually is processed correctly, so I'm excluding a file with big sentences or many lines.</p>

<p>Is there an option I'm not considering, some OpenIE or Java flag, something that I can use to force a dump to an output, a cleaning, or garbage collection between each file that is processed?</p>

<p>Thank you in advance</p>
","java, stanford-nlp","<p>From the comments above: I suspect this is an issue with too much parallelism and too little memory. OpenIE is a bit memory hungry, especially with long sentences, and so running many files in parallel can take up a fair bit of memory.</p>

<p>An easy fix is to force the program to run single-threaded, by setting the <code>-threads 1</code> flag. If possible, increasing memory should help as well.</p>
",1,3,810,2016-04-05 16:21:49,https://stackoverflow.com/questions/36431900/stanford-nlp-openie-out-of-memory-when-processing-list-of-files
pycorenlp: &quot;CoreNLP request timed out. Your document may be too long&quot;,"<p>I'm trying to run <a href=""https://github.com/smilli/py-corenlp"" rel=""noreferrer"">pycorenlp</a> on a long text and get an <code>CoreNLP request timed out. Your document may be too long</code> error message. How to fix it? Is there any way to increase <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""noreferrer"">Stanford CoreNLP</a>'s timed out?</p>

<p>I don't want to segment the text into smaller texts.</p>

<p>Here is the code I use:</p>

<pre><code>'''
From https://github.com/smilli/py-corenlp/blob/master/example.py
'''
from pycorenlp import StanfordCoreNLP
import pprint

if __name__ == '__main__':
    nlp = StanfordCoreNLP('http://localhost:9000')
    fp = open(""long_text.txt"")
    text = fp.read()
    output = nlp.annotate(text, properties={
        'annotators': 'tokenize,ssplit,pos,depparse,parse',
        'outputFormat': 'json'
    })
    pp = pprint.PrettyPrinter(indent=4)
    pp.pprint(output)
</code></pre>

<p>The Stanford Core NLP Server was launched using:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9000
</code></pre>
","python, timeout, nlp, stanford-nlp","<p>You can add <code>'timeout': '50000'</code> (unit is ms) in the <code>properties</code> dictionary:</p>

<pre><code>output = nlp.annotate(text, properties={
    'timeout': '50000',
    'annotators': 'tokenize,ssplit,pos,depparse,parse',
    'outputFormat': 'json'
})
</code></pre>

<p>Otherwise, you can launch the Stanford Core NLP Server specifying the timeout:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 50000
</code></pre>

<p>(The <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""noreferrer"">documentation</a> doesn't mention the <code>timeout</code> parameter, maybe they forgot to add it, it's at least present in <a href=""http://stanfordnlp.github.io/CoreNLP/history.html"" rel=""noreferrer"">stanford-corenlp-full-2015-12-09, a.k.a. 3.6.0.</a>, which is the latest public release)</p>
",12,8,5216,2016-04-05 19:19:55,https://stackoverflow.com/questions/36435207/pycorenlp-corenlp-request-timed-out-your-document-may-be-too-long
Stanford CoreNLP - Unknown variable: WEEKDAY,"<p>I process the NYT portion of the English Gigaword Corpus with Stanford CoreNLP. While it is still on-going the following message is logged several times:</p>

<p><code>Unknown variable: WEEKDAY</code></p>

<p>Every time this message is logged the memory consumption increases. It is now about 23.8GB. Does someone know what this issue is about?</p>

<p>I'm using Stanford CoreNLP 3.6.0 from Github with commit id <code>4fd28dc4848616e568a2dd6eeb09b9769d1e3f4e</code> and the following models <code>stanford-english-corenlp-2016-01-10-models</code>. My pipeline looks like the this: <code>""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, depparse, mention, coref""</code>.</p>

<p>I already know <a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2014-March/005235.html"" rel=""nofollow"">this</a> question. But no one has answered. </p>
","java, stanford-nlp","<p>The problem should be resolved with this pull-request: <a href=""https://github.com/stanfordnlp/CoreNLP/pull/277"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/pull/277</a></p>
",0,4,415,2016-04-06 11:30:07,https://stackoverflow.com/questions/36449826/stanford-corenlp-unknown-variable-weekday
Ipython from progressbar import ProgressBar Error,"<p>I want to do something according to <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""nofollow"">this github code</a>.</p>

<pre><code>from progressbar import Progressbar, Fraction
ImportError: cannot import name 'Fraction'
</code></pre>

<p>I use ipython 2 or 3</p>
","python, import, stanford-nlp, fractions","<p>The Github repo you linked to contains a file <code>progressbar.py</code>, which is an ancient (2006-era) version of <a href=""https://pypi.python.org/pypi/progressbar"" rel=""nofollow""><code>progressbar</code></a>. <code>progressbar.py</code> contains a <code>Fraction</code> class, while the <code>progressbar</code> module does not. So, you'll need to change your code to point to the <code>progressbar.py</code> file from the Standford NLP Github repo.</p>
",0,0,1255,2016-04-06 19:57:19,https://stackoverflow.com/questions/36461153/ipython-from-progressbar-import-progressbar-error
insert and delete transformation rules to typed dependency,"<p>using stanford coreNLP i have extracted all the type dependencies of a sentence which is in passive voice. Now I want to make it active voice. For this I have to delete and insert some new rule into this. For example if we take sentence like 
"" The cat was chased by the dog."" then the typed dependency representation is as:
det(cat-2, The-1)
nsubjpass(chased-4, cat-2)
auxpass(chased-4, was-3) 
det(dog-7, the-6) 
agent(chased-4, dog-7) 
punct(chased-4, .-8)</p>

<p>A transformation rule to convert the above to active voice would require three deletions and two insertions:
1. Match and Delete:
(a) nsubjpass(??X0, ??X1) 
(b) auxpass(??X0, ??X2) 
(c) agent(??X0, ??X3)</p>

<ol start=""2"">
<li>Insert:
(a) nsubj(??X0, ??X3)
(b) dobj(??X0, ??X1)</li>
</ol>

<p>Here ??X0(chased) ,??X1(cat), ??X2(was) and ??X3(dog)</p>

<p>Now my question is that how can I implement these rule into my java code.</p>
","java, dependencies, stanford-nlp","<p>So you want to change the sentence:</p>

<pre><code>""The cat was chased by the dog.""
</code></pre>

<p>into:</p>

<pre><code>""The dog chased the cat.""
</code></pre>

<p>?</p>

<p>If I were working on this, my first instinct would be to use the rules to generate the new sentence String, and then just rebuild a new Annotation object with that text.</p>

<p>So I would not mess with editing the graph or the other annotations, I would just create the new String I wanted, and the rebuild the dependency graph.</p>

<p>So you could imagine having a rule like:</p>

<pre><code>Pattern: X was VERB by Y --&gt; Y &lt;VERB&gt; X
Example: ""The cat was chased by the dog."" --&gt; ""The dog chased the cat.""
</code></pre>

<p>And follow this algorithm:</p>

<ol>
<li><p>Detect pattern in dependency graph or with regex (in this case ""was VERB by"")</p></li>
<li><p>Create X and Y.  So you could follow the graph to add extra words such as determinants.  So in this case you would follow the determinant edge to expand ""cat"" into ""The cat"" and ""dog"" into ""the dog"".</p></li>
<li><p>Output the new altered String: X  Y, and lowercase everything.  In this case outputting ""the cat chased the dog""</p></li>
<li><p>Then capitalize the first word of your new sentence and add punctuation at the end.  Resulting in ""The cat chased the dog.""</p></li>
<li><p>Then I'd just feed this new String into an Annotation and re-run the pipeline to get a new dependency graph.</p>

<pre><code>Annotation newSentenceAnnotation = new Annotation(""The dog chased the cat."");
pipeline.annotate(newSentenceAnnotation);
</code></pre></li>
</ol>

<p>Here are some links for working with the Semantic Graph:</p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html</a></p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraphEdge.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraphEdge.html</a></p>

<p>If you look at this recent demo we put up, there is a class called DependencyMatchFinder.java which demonstrates accessing a SemanticGraph from an Annotation:</p>

<p><a href=""https://github.com/stanfordnlp/nlp-meetup-demo/blob/master/DependencyMatchFinder.java"" rel=""nofollow"">https://github.com/stanfordnlp/nlp-meetup-demo/blob/master/DependencyMatchFinder.java</a></p>
",0,0,102,2016-04-08 10:00:47,https://stackoverflow.com/questions/36496768/insert-and-delete-transformation-rules-to-typed-dependency
which one is best for parsing between Left corner Parsing algorithm and CYK parsing algorithm ? and Why?,"<p>which one is best for parsing between Left corner Parsing algorithm and CYK parsing algorithm ? and Why ?</p>
","nlp, stanford-nlp","<p>Generally speaking, CYK is a maximum-likelihood parse tree. It never gives you the best performance because of this reason and the fact that it ignores contextual information when assigns the probabilities. You need to modify it to consider more contexts, or integrate it into something else. For example, Left-Corner parser can use a CYK procedure, inside. So the answer to your question is, LC is more powerful than CYK, though it's computationally more expensive. Have a look at Mark Johnson's <a href=""http://arxiv.org/pdf/cs/0008017.pdf"" rel=""nofollow"">paper</a>.</p>
",2,1,190,2016-04-08 14:58:36,https://stackoverflow.com/questions/36502897/which-one-is-best-for-parsing-between-left-corner-parsing-algorithm-and-cyk-pars
How to split sentences using the nltk.parse.stanford library,"<p>I'm trying to use the Stanford Parser from nltk.parse.stanford to do a bunch of NLP tasks. There are certain operations on sentences that I am able to do when I explicitly pass a sentence or a list of sentences as input. <strong>But how do I actually split a large amount of text into sentences</strong>? (Obviously, regex with periods etc. won't work well) </p>

<p>I checked the documentation here and found nothing: <a href=""http://www.nltk.org/api/nltk.parse.html?highlight=stanford#module-nltk.parse.stanford"" rel=""nofollow noreferrer"">http://www.nltk.org/api/nltk.parse.html?highlight=stanford#module-nltk.parse.stanford</a></p>

<p>I found something similar that does the job for java here: <a href=""https://stackoverflow.com/questions/9492707/how-can-i-split-a-text-into-sentences-using-the-stanford-parser"">How can I split a text into sentences using the Stanford parser?</a></p>

<p>I think I need something exactly like this for the python version of the library.</p>
","python, parsing, nlp, nltk, stanford-nlp","<p>First setup Stanford tools and NLTK correctly, e.g. in Linux:</p>

<pre><code>alvas@ubi:~$ cd
alvas@ubi:~$ wget http://nlp.stanford.edu/software/stanford-parser-full-2015-12-09.zip
alvas@ubi:~$ unzip stanford-parser-full-2015-12-09.zip
alvas@ubi:~$ ls stanford-parser-full-2015-12-09
bin                        ejml-0.23.jar          lexparser-gui.sh              LICENSE.txt       README_dependencies.txt  StanfordDependenciesManual.pdf
build.xml                  ejml-0.23-src.zip      lexparser_lang.def            Makefile          README.txt               stanford-parser-3.6.0-javadoc.jar
conf                       lexparser.bat          lexparser-lang.sh             ParserDemo2.java  ShiftReduceDemo.java     stanford-parser-3.6.0-models.jar
data                       lexparser-gui.bat      lexparser-lang-train-test.sh  ParserDemo.java   slf4j-api.jar            stanford-parser-3.6.0-sources.jar
DependencyParserDemo.java  lexparser-gui.command  lexparser.sh                  pom.xml           slf4j-simple.jar         stanford-parser.jar
alvas@ubi:~$ export STANFORDTOOLSDIR=$HOME
alvas@ubi:~$ export CLASSPATH=$STANFORDTOOLSDIR/stanford-parser-full-2015-12-09/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models.jar
</code></pre>

<p>(See <a href=""https://gist.github.com/alvations/e1df0ba227e542955a8a"" rel=""nofollow"">https://gist.github.com/alvations/e1df0ba227e542955a8a</a> for more details and see <a href=""https://gist.github.com/alvations/0ed8641d7d2e1941b9f9"" rel=""nofollow"">https://gist.github.com/alvations/0ed8641d7d2e1941b9f9</a> for windows instructions)</p>

<p>Then, use <a href=""http://www.nltk.org/_modules/nltk/tokenize/punkt.html"" rel=""nofollow"">Kiss and Strunk (2006)</a> to sentence tokenize the text into a list of strings, where each item in the list is a sentence.</p>

<pre><code>&gt;&gt;&gt; from nltk import sent_tokenize, word_tokenize
&gt;&gt;&gt; sentences = 'This is the first sentnece. This is the second. And this is the third'
&gt;&gt;&gt; sent_tokenize(sentences)
['This is the first sentence.', 'This is the second.', 'And this is the third']
</code></pre>

<p>Then feed the document stream into the stanford parser:</p>

<pre><code>&gt;&gt;&gt; list(list(parsed_sent) for parsed_sent in parser.raw_parse_sents(sent_tokenze(sentences)))
[[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['first']), Tree('NN', ['sentence'])])]), Tree('.', ['.'])])])], [Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['second'])])]), Tree('.', ['.'])])])], [Tree('ROOT', [Tree('S', [Tree('CC', ['And']), Tree('NP', [Tree('DT', ['this'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['third'])])])])])]]
</code></pre>
",4,0,3553,2016-04-09 02:41:26,https://stackoverflow.com/questions/36512113/how-to-split-sentences-using-the-nltk-parse-stanford-library
Single query for POS tagging and tokensregex,"<p>I am using Stanford NLP Web Api to get POS tags and tokensregex queries.
What tokensregex is doing actually finding POS tags behind of scene and execute regex queries over it. 
So I need to hit web api 2 times (in my case one for POS tagging and 2 different tokensregex, 3 times to get result for a sentence)</p>

<p>Is there a way to get POS tags and tokensregex with one query against web api ?
I did not see in the documentation. Maybe undocumented feature or any plan for future ?</p>
",stanford-nlp,"<p>This is planned for the future. There's no inherent reason the tokensregex and semgrex endpoints can't deserialize annotated documents, beyond the fact that I haven't implemented it yet. Pull requests are welcome :)!</p>
",0,0,160,2016-04-10 06:46:04,https://stackoverflow.com/questions/36526773/single-query-for-pos-tagging-and-tokensregex
TokensRegex json response,"<p>The TokensRegex response (web api) is as follows with an array list shaped with numerical order.
Is there way to change the format, or any reason it must be that way?
Otherwise it is hard to deserialize it or write a query on the result.</p>

<pre><code>{
  ""sentences"": [
    {
      ""0"": {
        ""text"": ""huge success"",
        ""begin"": 4,
        ""end"": 6
      },
      ""1"": {
        ""text"": ""new venture"",
        ""begin"": 17,
        ""end"": 19
      },
      ""2"": {
        ""text"": ""comfort zone"",
        ""begin"": 26,
        ""end"": 28
      },
      ""length"": 3
    }
  ]
}
</code></pre>
","json, json.net, stanford-nlp","<p>You can use Json.Net's <a href=""http://www.newtonsoft.com/json/help/html/linqtojson.htm"" rel=""nofollow"">LINQ-to-JSON API</a> to deserialize this JSON into something sensible.  </p>

<p>First, define a class <code>Phrase</code> like this:</p>

<pre><code>class Phrase
{
    public string Text { get; set; }
    public int Begin { get; set; }
    public int End { get; set; }
}
</code></pre>

<p>Then you can do this to get a list of phrases:</p>

<pre><code>JObject obj = JObject.Parse(json);

List&lt;Phrase&gt; phrases = 
    obj[""sentences""][0]
        .Children&lt;JProperty&gt;()
        .Where(jp =&gt; jp.Value.Type == JTokenType.Object)
        .Select(jp =&gt; jp.Value.ToObject&lt;Phrase&gt;())
        .ToList();
</code></pre>

<p>Fiddle: <a href=""https://dotnetfiddle.net/hU4iTp"" rel=""nofollow"">https://dotnetfiddle.net/hU4iTp</a></p>
",0,0,66,2016-04-10 08:46:56,https://stackoverflow.com/questions/36527705/tokensregex-json-response
Error while using stanford core nlp,"<p>I m having problem in using the stanford nlp. i am having issues where I'm getting various errors when trying to use the Stanford Core NLP tools.I want to know the sentiment of the sentence passed. But I've not been able to get nlp tools to work when running the code from eclipse with the needed jars added to the classpath,</p>

<p>This is the code i want to execute.</p>

<pre><code>import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.SentimentAnnotator;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.TypesafeMap.Key;

public class sentiment_demo {

    public static void sentiment_analysis(String line)
    {
        //Uses Stanford NLP sentimnet analysis
        //found in latest model released from stanford
        // ver 3.3.1
        //applies sentiment analysis to text 

        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        int mainSentiment = 0;
        if (line != null &amp;&amp; line.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(line);
            for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
                System.out.println(sentence);
                for (Tree token: sentence.get(SentimentCoreAnnotations.AnnotatedTree.class))
                {
                    //System.out.println(token);
                }
                Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                System.out.println(sentiment);
                String partText = sentence.toString();
                //System.out.println(partText);
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        if(mainSentiment==2)
        {
            System.out.println(""Average"");
        }
        else if(mainSentiment&gt;2)
        {
            System.out.println(""Positive"");
        }
        else if(mainSentiment&lt;2)
        {
            System.out.println(""Negative "");
        }

        if (mainSentiment == 2 || mainSentiment &gt; 4 || mainSentiment &lt; 0) {
            //return null;
        }
    }
    public static void main(String[] args)
    {
        sentiment_analysis(""Cristiano Ronaldo, is a Portuguese professional footballer who plays for Spanish club Real Madrid and the Portugal national team. He is a forward and serves as captain for Portugal.Often ranked as the best player in the world and rated by some in the sport as the greatest of all time"");
    }
}
</code></pre>

<p>Here are the libraries i have set 
<a href=""https://i.sstatic.net/bHQJh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bHQJh.jpg"" alt=""enter image description here""></a></p>

<p>I m using eclipse mars. At first it was showing error for ejml library.but then i imported ejml jar file so that error was resolved but it gave rise to this errors now...</p>

<pre><code>Adding annotator tokenize
Adding annotator ssplit
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1.6 sec].
Adding annotator sentiment
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.lang.ClassNotFoundException: edu.stanford.nlp.neural.SimpleTensor
    at edu.stanford.nlp.sentiment.SentimentModel.loadSerialized(SentimentModel.java:470)
    at edu.stanford.nlp.pipeline.SentimentAnnotator.&lt;init&gt;(SentimentAnnotator.java:45)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$14.create(StanfordCoreNLP.java:845)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:260)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:127)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:123)
    at sentiment_demo.sentiment_analysis(sentiment_demo.java:28)
    at sentiment_demo.main(sentiment_demo.java:70)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.neural.SimpleTensor
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at java.io.ObjectInputStream.resolveClass(Unknown Source)
    at java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
    at java.io.ObjectInputStream.readClassDesc(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.readObject(Unknown Source)
    at java.util.TreeMap.buildFromSorted(Unknown Source)
    at java.util.TreeMap.buildFromSorted(Unknown Source)
    at java.util.TreeMap.readObject(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
    at java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.readObject(Unknown Source)
    at java.util.TreeMap.buildFromSorted(Unknown Source)
    at java.util.TreeMap.buildFromSorted(Unknown Source)
    at java.util.TreeMap.readObject(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
    at java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.defaultReadFields(Unknown Source)
    at java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.defaultReadFields(Unknown Source)
    at java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.readObject(Unknown Source)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:298)
    at edu.stanford.nlp.sentiment.SentimentModel.loadSerialized(SentimentModel.java:466)
</code></pre>

<p>i m stuck at it.If someone have any idea of this then please do suggest,it will be of great help.I have searched other similar questions on stackoverflow but still not getting the solution to resolve this.</p>
","stanford-nlp, sentiment-analysis","<p>You appear to be using an old version of CoreNLP (3.3.0) alongside the new models. Try downloading the 3.6.0 code + models.</p>
",1,0,952,2016-04-12 04:24:26,https://stackoverflow.com/questions/36563597/error-while-using-stanford-core-nlp
Can NER Stanford called in java file?,"<p>I've tried to give NER and POS tag from texts with <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">Stanford Named Entity Recognizer (NER)</a> and <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford Log-linear Part-Of-Speech Tagger</a>.</p>

<p>Let me give you an example of POStag</p>

<pre><code>import edu.stanford.nlp.tagger.maxent.MaxentTagger;

public class POSTag {
    static MaxentTagger tagger =  new MaxentTagger(""english_language_pos.tagger"");

    public static void main(String[] args) throws IOException{
        String text = ""Sarah and I go to Samsung office in Korea.""
        String tagged = tagger.tagString(text);

        System.out.println(tagged);
    }
}
</code></pre>

<p>The output: <br></p>

<pre><code>Sarah_NNP and_CC I_PRP go_VBP to_TO Samsung_NNP office_NN in_IN Korea_NNP ._.  
</code></pre>

<p>And for NER Stanford, I have to run in command line:</p>

<pre><code>java -mx500m -cp stanford-ner-3.5.2.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier indonesian_language_ner.gz -textFile E:/file.txt &gt; E:/file_ner.txt
</code></pre>

<p>My question is how can I call Stanford NER in file with java extension(file.java)? Is there any way to call Stanford NER besides from command line?
Or any other tool or library that I can use to give NER that called from java file?</p>

<p>*****edit:<br>
I need the output of NER become like this:</p>

<pre><code>Sarah_PERSON and_O I_O go_O to_O Samsung_ORGANIZATION office_O in_O Korea_PLACE ._O 
</code></pre>

<p>*****edit</p>

<p>**fyi: I use Windows 8.1</p>
","java, stanford-nlp, named-entity-recognition, pos-tagger","<p>You could do something like this:</p>

<pre><code>    String model = ""english.all.3class.distsim.crf.ser.gz"";
    String text = ""file.txt"";

    AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifier(model);
    classifier.classifyAndWriteAnswers(text);
</code></pre>

<p>I hope it helps.</p>
",1,0,1746,2016-04-13 09:36:59,https://stackoverflow.com/questions/36594369/can-ner-stanford-called-in-java-file
Unable to use Stanford NER in python module,"<p>I want to use Python Stanford NER module but keep getting an error,I searched it on internet but got nothing. Here is the basic usage with error.</p>

<pre><code>import ner
tagger = ner.HttpNER(host='localhost', port=8080)
tagger.get_entities(""University of California is located in California,   

United States"")
</code></pre>

<p>Error</p>

<pre><code>Traceback (most recent call last):
File ""&lt;pyshell#3&gt;"", line 1, in &lt;module&gt;
tagger.get_entities(""University of California is located in California, United States"")
File ""C:\Python27\lib\site-packages\ner\client.py"", line 81, in get_entities
tagged_text = self.tag_text(text)
File ""C:\Python27\lib\site-packages\ner\client.py"", line 165, in tag_text
c.request('POST', self.location, params, headers)
File ""C:\Python27\lib\httplib.py"", line 1057, in request
self._send_request(method, url, body, headers)
File ""C:\Python27\lib\httplib.py"", line 1097, in _send_request
self.endheaders(body)
File ""C:\Python27\lib\httplib.py"", line 1053, in endheaders
self._send_output(message_body)
File ""C:\Python27\lib\httplib.py"", line 897, in _send_output
self.send(msg)
File ""C:\Python27\lib\httplib.py"", line 859, in send
self.connect()
File ""C:\Python27\lib\httplib.py"", line 836, in connect
self.timeout, self.source_address)
File ""C:\Python27\lib\socket.py"", line 575, in create_connection
raise err
error: [Errno 10061] No connection could be made because the target machine actively refused it
</code></pre>

<p>Using windows 10 with latest Java installed</p>
","python, python-2.7, nlp, stanford-nlp, named-entity-recognition","<ul>
<li>The Python Stanford NER module is a wrapper for the Stanford NER that
allows you to run python commands to use the NER service.     </li>
<li>The NER
service is a separate entity to the Python module. It is a Java
program. To access this service, via python, or any other way, you
first need to start the service.   </li>
<li>Details on how to start the Java
Program/service can be found here -
<a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/CRF-NER.shtml</a>    </li>
<li><p>The NER comes with
a <code>.bat</code> file for windows and a <code>.sh</code> file for unix/linux. I think
these files start the <code>GUI</code></p></li>
<li><p>To start the service without the <code>GUI</code> you should run a command similar to this:<br>
<code>java -mx600m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz</code><br>
This runs the NER jar, sets the memory, and sets the classifier you want to use. (I think youll have to be in the Stanford NER directory to run this)  </p></li>
<li><p>Once the NER program is running then you will be able to run your python code and query the NER.  </p></li>
</ul>
",1,1,1269,2016-04-16 18:54:59,https://stackoverflow.com/questions/36668340/unable-to-use-stanford-ner-in-python-module
Python - Group Sequential Array Members,"<p>I want to edit my text like this:</p>

<pre><code>arr = [] 
# arr is full of tokenized words from my text
</code></pre>

<p>For example:</p>

<pre><code>""Abraham Lincoln Hotel is very beautiful place and i want to go there with
 Barbara Palvin. Also there are stores like Adidas ,Nike , Reebok.""
</code></pre>

<p>Edit: Basically I want to detect Proper Names and group them by using istitle() and isAlpha() in for statement like:</p>

<pre><code>for i in arr:
    if arr[i].istitle() and arr[i].isAlpha
</code></pre>

<p>In the example arr appened until the next word hasn't his first letter upper case. </p>

<pre><code>arr[0] + arr[1] + arr[2] = arr[0]
#Abraham Lincoln Hotel
</code></pre>

<p>This is what i want with my new arr:</p>

<p><code>['Abraham Lincoln Hotel'] is very beautiful place and i want to go there with</code> <code>['Barbara Palvin']. ['Also'] there are stores like ['Adidas'], ['Nike'],</code> <code>['Reebok'].</code></p>

<p>""Also"" is not problem for me it will be usefull when i try to match with my dataset.</p>
","python, nlp, nltk, stanford-nlp, opennlp","<p>You could do something like this:</p>

<pre><code>sentence = ""Abraham Lincoln Hotel is very beautiful place and i want to go there with Barbara Palvin. Also there are stores like Adidas, Nike, Reebok.""
all_words = sentence.split()
last_word_index = -100
proper_nouns = []
for idx, word in enumerate(all_words):
    if(word.istitle() and word.isalpha()):
        if(last_word_index == idx-1):
            proper_nouns[-1] = proper_nouns[-1] + "" "" + word
        else:
            proper_nouns.append(word)
        last_word_index = idx
print(proper_nouns)
</code></pre>

<p>This code will:</p>

<ul>
<li>Split all the words into a list</li>
<li>Iterate over all of the words and

<ul>
<li>If the last capitalized word was the previous word, it will append it to the last entry in the list</li>
<li>else it will store the word as a new entry in the list</li>
<li>Record the last index that a capitalized word was found</li>
</ul></li>
</ul>
",1,2,126,2016-04-18 07:50:01,https://stackoverflow.com/questions/36688176/python-group-sequential-array-members
Provide parameters to Stanford CoreNLP OpenIE from command line,"<p>I run OpenIE from Stanford CoreNLP through command line with</p>

<pre><code>java -mx1g -cp stanford-corenlp-&lt;version&gt;.jar:stanford-corenlp-&lt;version&gt;-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar edu.stanford.nlp.naturalli.OpenIE -openie.resolve_coref
</code></pre>

<p>But when I want to get output for all annotators I am using</p>

<pre><code>./corenlp.sh -annotators tokenize,ssplit,pos,lemma,ner,parse,natlog,mention,openie,coref
</code></pre>

<p>How can I provide parameter <code>-openie.resolve_coref</code> to OpenIE and get not only it's output?</p>
",stanford-nlp,"<p>You should be able to simply pass the property <code>-openie.resolve_coref</code> using the regular pipeline. I suspect your mistake here is that you have the <code>coref</code> annotator after <code>openie</code>, and so there's no coref information yet when the OpenIE annotator is run.</p>
",3,1,532,2016-04-20 12:10:35,https://stackoverflow.com/questions/36743368/provide-parameters-to-stanford-corenlp-openie-from-command-line
Preventing Stanford Core NLP Server from outputting the text it receives,"<p>I am running a <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">Stanford CoreNLP</a> server:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 50000
</code></pre>

<p>Whenever it receives some text, it outputs it in the shell it is running it. How to prevent this from happening?</p>

<hr>

<p>It that matters, here is the code I use to pass data to Stanford Core NLP Server:</p>

<pre><code>'''
From https://github.com/smilli/py-corenlp/blob/master/example.py
'''
from pycorenlp import StanfordCoreNLP
import pprint

if __name__ == '__main__':
    nlp = StanfordCoreNLP('http://localhost:9000')
    fp = open(""long_text.txt"")
    text = fp.read()
    output = nlp.annotate(text, properties={
        'annotators': 'tokenize,ssplit,pos,depparse,parse',
        'outputFormat': 'json'
    })
    pp = pprint.PrettyPrinter(indent=4)
    pp.pprint(output)
</code></pre>
","nlp, stanford-nlp","<p>There's currently not a way to do this, but you're the second person that's asked. So, it's now in the Github code, and will make it into the next release. For the future, you should be able to set the <code>-quiet</code> flag, and the server will not write to standard out.</p>
",6,4,1274,2016-04-21 20:59:40,https://stackoverflow.com/questions/36780358/preventing-stanford-core-nlp-server-from-outputting-the-text-it-receives
Running Stanford CoreNLP server multithreadedly,"<p>I am running a <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">Stanford CoreNLP</a> server:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 50000
</code></pre>

<p>It seems that it only uses one core when processing texts. Is it possible to run the Stanford CoreNLP server multithreadedly, so that it utilizes more than one core?</p>
","multithreading, nlp, stanford-nlp","<p>This is correct; every request to the server only uses one core. You can get parallelism by making multiple server requests at once. This will run in parallel up to the number of cores on the server (or, the value of <code>-threads</code> passed into the server executable), and after that it'll queue up jobs in a thread pool.</p>
",4,1,1010,2016-04-22 00:12:46,https://stackoverflow.com/questions/36782543/running-stanford-corenlp-server-multithreadedly
SentimentCoreAnnotations.AnnotatedTree cannot be resolved to a type,"<p>I'm trying to get a sentimental score for live tweets using Stanford Core NLP. Currently I'm getting the real-time tweets. But when I feed it to the Stanford Core NLP program i'm getting an error. </p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class NLP {
    static StanfordCoreNLP pipeline;

    public static void init() {
        pipeline = new StanfordCoreNLP(""MyPropFile.properties"");
    }

    public static int findSentiment(String tweet) {

        int mainSentiment = 0;
        if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(tweet);
            for (CoreMap sentence : annotation
                    .get(CoreAnnotations.SentencesAnnotation.class)) {
                Tree tree = sentence
                        .get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                String partText = sentence.toString();
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        return mainSentiment;
    }
}
</code></pre>

<p>I'm calling to this init() and findSentiment() functions from outside the class NLP. </p>

<pre><code>NLP.init();
System.out.println(status.getText() + "" : "" + NLP.findSentiment(status.getText()));
</code></pre>

<p>My error is: <strong>SentimentCoreAnnotations.AnnotatedTree cannot be resolved to a type</strong></p>
","java, twitter, compiler-errors, nlp, stanford-nlp","<p>Try changing </p>

<pre><code>Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
</code></pre>

<p>to </p>

<pre><code>Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
</code></pre>
",7,0,1316,2016-04-23 16:28:15,https://stackoverflow.com/questions/36813383/sentimentcoreannotations-annotatedtree-cannot-be-resolved-to-a-type
Exception in thread &quot;Twitter4J Async Dispatcher[0]&quot; java.lang.NoClassDefFoundError,"<p>I'm working on a project that analyzes real-time tweets and identify user's moods. 
So I'm using twitter4j to receive real-time tweets and feeds those tweets to Stanford’s Core NLP. I'm receiving the real-time tweets correctly. But when I feed those tweets to Stanford's Core NLP i'm getting an run-time error. </p>

<p>PrintSampleStream Class that gets real-time tweets using twitter4j:</p>

<pre><code>import javax.swing.JDialog;
import javax.swing.JOptionPane;

import javax.swing.Timer;

import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;

import twitter4j.*;
import twitter4j.conf.*; 

public class PrintSampleStream {

    private String twitter_handle;

    PrintSampleStream()
    {
        twitter_handle = null;
    }

    PrintSampleStream(String tw)
    {
        twitter_handle = tw;
    }

    public void twitterConnector() throws TwitterException {
         ConfigurationBuilder cb = new ConfigurationBuilder();
          cb.setDebugEnabled(true).setOAuthConsumerKey(""bbbb"")
                  .setOAuthConsumerSecret(""bbbb"")
                  .setOAuthAccessToken(""bbbb"")
                  .setOAuthAccessTokenSecret(""bbbb"");
          TwitterStream twitterStream = new TwitterStreamFactory(cb.build())
                  .getInstance();
        StatusListener listener = new StatusListener() {
            @Override
            public void onStatus(Status status) {
                System.out.println(""@"" + status.getUser().getScreenName() + "" - "" + status.getText());
                NLP.init();
                System.out.println(status.getText() + "" : "" + NLP.findSentiment(status.getText()));
                //storeTweets(status.getText());
                //JOptionPane.showMessageDialog(null, status.getText());
            }

            @Override
            public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice) {
                System.out.println(""Got a status deletion notice id:"" + statusDeletionNotice.getStatusId());
            }

            @Override
            public void onTrackLimitationNotice(int numberOfLimitedStatuses) {
                System.out.println(""Got track limitation notice:"" + numberOfLimitedStatuses);
            }

            @Override
            public void onScrubGeo(long userId, long upToStatusId) {
                System.out.println(""Got scrub_geo event userId:"" + userId + "" upToStatusId:"" + upToStatusId);
            }

            @Override
            public void onStallWarning(StallWarning warning) {
                System.out.println(""Got stall warning:"" + warning);
            }

            @Override
            public void onException(Exception ex) {
                ex.printStackTrace();
            }
        };
        twitterStream.addListener(listener);
        FilterQuery filtre = new FilterQuery();
        String[] keywordsArray = {twitter_handle};
        filtre.track(keywordsArray);
        twitterStream.filter(filtre);
    }
} 
</code></pre>

<p>NLP Class that feeds real-time tweets received from twitter4j to Stanford's Core NLP:</p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class NLP {
    static StanfordCoreNLP pipeline;

    public static void init() {
        pipeline = new StanfordCoreNLP(""MyPropFile.properties"");
    }

    public static int findSentiment(String tweet) {

        int mainSentiment = 0;
        if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(tweet);
            for (CoreMap sentence : annotation
                    .get(CoreAnnotations.SentencesAnnotation.class)) {
                Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                String partText = sentence.toString();
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        return mainSentiment;
    }
}
</code></pre>

<p>My run-time error is:</p>

<pre><code>@laliyaD - Lalinda feels tired
Exception in thread ""Twitter4J Async Dispatcher[0]"" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;clinit&gt;(StanfordCoreNLP.java:99)
    at NLP.init(NLP.java:13)
    at PrintSampleStream$1.onStatus(PrintSampleStream.java:38)
    at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
    at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:105)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    ... 8 more
</code></pre>

<p>Actually I'm getting the real-time tweets from twitter4j. Any help?</p>
","java, twitter, nlp, stanford-nlp, twitter4j","<p>You need to download <code>SLF4J</code> (Simple Logging Facade for Java) and include it in your classpath.</p>

<p>You'll need at least <code>slf4j-api-1.7.21.jar</code> and <code>slf4j-simple-1.7.21.jar</code> in order to be able to actually view log messages from the NLP library.</p>

<p><a href=""http://www.slf4j.org/download.html"" rel=""nofollow"">http://www.slf4j.org/download.html</a></p>
",1,1,378,2016-04-23 19:00:27,https://stackoverflow.com/questions/36815117/exception-in-thread-twitter4j-async-dispatcher0-java-lang-noclassdeffounderr
possible mistake/bug in Stanford CoreNLP and/or NLP parse visualization,"<p>Don't you think it is wrong to tag 'me the meal on flight UA 386 from San Francisco to Denver' in the sentence 'show me the meal on flight UA 386 from San Francisco to Denver' as S?</p>

<p>Image created using NLP Parse Visualization available at <a href=""http://nlpviz.bpodgursky.com/"" rel=""nofollow noreferrer"">http://nlpviz.bpodgursky.com/</a></p>

<p><img src=""https://i.sstatic.net/5Rwhr.png"" alt=""image""></p>
","nlp, stanford-nlp","<p>The Stanford Parser is generally significantly worse at imperatives than it is on other sentences. This is likely just a simple parse error, inherent in the fact that these are imperfect models. </p>

<p>The dependency parser actually seems to also mess up on this sentence; I suspect it's just a hard sentence.</p>
",2,1,363,2016-04-24 23:56:49,https://stackoverflow.com/questions/36830269/possible-mistake-bug-in-stanford-corenlp-and-or-nlp-parse-visualization
Why does my NamedEntityAnnotator for date mentions differ from CoreNLP demo&#39;s output?,"<p>The date detected from my following program gets split into two separate mentions whereas the detected date in the NER output of <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">CoreNLP demo</a> is single as it should be. What should I edit in my program to correct this.</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, entitymentions"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

String text =  ""This software was released on Februrary 5, 2015."";
Annotation document = new Annotation(text);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
      List&lt;CoreMap&gt; mentions = sentence.get(MentionsAnnotation.class);
      if (mentions != null) {
              for (CoreMap mention : mentions) {
                     System.out.println(""== Token="" + mention.get(TextAnnotation.class));
                     System.out.println(""NER="" + mention.get(NamedEntityTagAnnotation.class));
                     System.out.println(""Normalized NER="" + mention.get(NormalizedNamedEntityTagAnnotation.class));
              }
       }
}
</code></pre>

<p>Output from this program:</p>

<pre><code>== Token=Februrary 5,
NER=DATE
Normalized NER=****0205
== Token=2015
NER=DATE
Normalized NER=2015  
</code></pre>

<p>Output from CoreNLP online demo:
<a href=""https://i.sstatic.net/50oef.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/50oef.png"" alt=""enter image description here""></a></p>
","nlp, stanford-nlp, named-entity-recognition","<p>Note that the online demo is showing any sequence of consecutive tokens with the same NER tag as belonging to the same unit.  Consider this sentence:</p>

<pre><code>The event happened on February 5th January 9th.
</code></pre>

<p>This example yields ""February 5th January 9th"" as a single DATE in the online demo.</p>

<p>Yet it recognizes ""February 5th"" and ""January 9th"" as separate entity mentions.</p>

<p>Your sample code is looking at mentions, not NER chunks.  Mentions are not being shown by the online demo.</p>

<p>That being said, I am not sure why SUTime is not joining February 5th and 2015 together in your example.  Thanks for bringing this up, I will look into improving the module to fix this issue in future releases.</p>
",2,0,441,2016-04-25 06:25:39,https://stackoverflow.com/questions/36833373/why-does-my-namedentityannotator-for-date-mentions-differ-from-corenlp-demos-ou
Stanford Parser models,"<p>Stanford CoreNLP contains several models for parsing English sentences.</p>

<ul>
<li>englishSR</li>
<li>english_SD</li>
<li>english_UD (default for depparse annotator)</li>
<li>englishRNN</li>
<li>englishFactored</li>
<li>englishPCFG (default for parse annotator)</li>
<li>englishPCFG.caseless</li>
<li>wsjRNN</li>
<li>wsjFactored</li>
<li>wsjPCFG</li>
</ul>

<p>There are some comparisons in following papers:</p>

<ul>
<li><a href=""http://nlp.stanford.edu/software/stanford-dependencies.shtml#English"" rel=""nofollow"">http://nlp.stanford.edu/software/stanford-dependencies.shtml#English</a></li>
<li><a href=""http://nlp.stanford.edu/pubs/lrecstanforddeps_final_final.pdf"" rel=""nofollow"">http://nlp.stanford.edu/pubs/lrecstanforddeps_final_final.pdf</a></li>
<li><a href=""http://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf"" rel=""nofollow"">http://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf</a></li>
<li><a href=""http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf"" rel=""nofollow"">http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf</a></li>
</ul>

<p>I couldn't find full description and comparison for all models.
Does it exist anywhere? If not I think it is worth to create.</p>
",stanford-nlp,"<p>I can't give a full list (maybe Chris will chime in?), but my understanding is that these models are:</p>

<ul>
<li><p><code>englishSR</code>: The shift reduce model trained on various standard treebanks, and some of Stanford's hand-annotated data. This is the fastest and most accurate model we have, but the model is huge to load.</p></li>
<li><p><code>english_SD</code>: The NN Dependency Parser model for Stanford Dependencies. Deprecated in favor of <code>english_UD</code> -- the Universal Dependencies model.</p></li>
<li><p><code>english_UD</code>: The NN Dependency Parser model for Universal Dependencies. This is the fastest and most accurate way to get dependency trees, but it won't give you constituency parses.</p></li>
<li><p><code>englishRNN</code>: The hybrid PCFG + Neural constituency parser model. More accurate than any of the constituency parsers other than the shift-reduce model, but also noticeably slower.</p></li>
<li><p><code>englishFactored</code>: Not 100% sure what this is, but my impression is that both accuracy and speed-wise it's between <code>englishPCFG</code> and <code>englishRNN</code>.</p></li>
<li><p><code>englishPCFG</code>: A regular old PCFG model for constituency parsing. Fast to load, and faster than any of the constituency models other than the shift-reduce model, but also kind of mediocre accuracy by modern standards. Nonetheless, a good default.</p></li>
<li><p><code>englishPCFG.caseless</code>: A caseless version of the PCFG model.</p></li>
</ul>

<p>I assume the <code>wsj*</code> models are there to reproduce numbers in papers (trained on the proper WSJ splits), but again I'm not 100% sure what they are.</p>

<p>To help chose the right model based on speed, accuracy, and the base memory used by the model:</p>

<ul>
<li><p>Fast: 10x, accurate, high-memory: englishSR</p></li>
<li><p>Medium: 1x, ok accuracy, low-memory: englishPCFG</p></li>
<li><p>Slow: ~0.25x, accurate, low-memory: englishRNN</p></li>
<li><p>Fast: 100x, accurate, low-memory, dependency parses only: english_UD</p></li>
</ul>
",2,2,769,2016-04-25 14:54:14,https://stackoverflow.com/questions/36844102/stanford-parser-models
How to create SemanticGraph object out of a string with CoreNLP?,"<p>Suppose I have a semantic graph <code>g</code>, and then I get its representation through the following two strings:</p>

<pre><code>String sg = g.toCompactString();
String dp = g.typedDependencies().toString();
</code></pre>

<p>Suppose that these strings are now the only thing I have (the object <code>g</code> is not visible anymore). Is it possible to create another object <code>g2</code> which can use the strings <code>sg</code> or <code>dp</code> in some way in order to get the same object? Ideally, it would be great if CoreNLP provided a constructor for this, for example:</p>

<pre><code>SemanticGraph g2 = new SemanticGraph(sg);
</code></pre>

<p>or </p>

<pre><code>SemanticGraph g2 = new SemanticGraph(dp);
</code></pre>

<p>Any ideas how to do this in an efficient way?</p>
","java, nlp, stanford-nlp","<p>Can you provide more context for your problem?</p>

<p>Are you trying to store the graph on disk and load it later and then rebuild the SemanticGraph?  You'd probably be better off using the ProtobufAnnotationSerializer for that use case.</p>

<p>It would help to understand why you are losing access to the SemanticGraph.</p>

<p>If you really wanted to go from a String representation of an edges list to a SemanticGraph you'd have to rebuild the tokens from the String, build SemanticGraphEdges, and then you could use SemanticGraphFactory.makeFromEdges to build a SemanticGraph from a list of edges.</p>

<p>Classes you want to look at:</p>

<pre><code>edu.stanford.nlp.ling.IndexedWord
edu.stanford.nlp.semgraph.SemanticGraphEdge
edu.stanford.nlp.semgraph.SemanticGraphFactory
</code></pre>
",1,1,135,2016-04-26 12:55:43,https://stackoverflow.com/questions/36865607/how-to-create-semanticgraph-object-out-of-a-string-with-corenlp
TokensRegex: Tokens are null after retokenization,"<p>I'm experimenting with Stanford NLP's TokensRegex and try to find dimensions (e.g. 100x120) in a text. So my plan is to first retokenize the input to further split these tokens (using the example provided in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/72096f732942f1969325de2e58ac9270e304a591/src/edu/stanford/nlp/ling/tokensregex/demo/rules/retokenize.rules.txt"" rel=""nofollow"">retokenize.rules.txt</a>) and then to search for the new pattern. </p>

<p>After doing the retokenization, however, only null-values are left that replace the original string:</p>

<pre><code>The top level annotation
[Text=100x120 Tokens=[null-1, null-2, null-3] Sentences=[100x120]]
</code></pre>

<p>The retokenization seems to work fine (3 tokens in result), but the values are lost. What can I do to maintain the original values in the tokens list?</p>

<p>My retokenize.rules.txt file is (as in the demo):</p>

<pre><code>tokens = { type: ""CLASS"", value:""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }
options.matchedExpressionsAnnotationKey = tokens;
options.extractWithTokens = TRUE;
options.flatten = TRUE;
ENV.defaults[""ruleType""] = ""tokens""
ENV.defaultStringPatternFlags = 2
ENV.defaultResultAnnotationKey = tokens

{ pattern: ( /\d+(x|X)\d+/ ), result: Split($0[0], /x|X/, TRUE) }
</code></pre>

<p>The main method:</p>

<pre><code>public static void main(String[] args) throws IOException {
    //...
    text = ""100x120"";
    Properties properties = new Properties();
    properties.setProperty(""tokenize.language"", ""de"");
    properties.setProperty(""annotators"", tokenize,retokenize,ssplit,pos,lemma,ner"");
    properties.setProperty(""customAnnotatorClass.retokenize"", ""edu.stanford.nlp.pipeline.TokensRegexAnnotator"");
    properties.setProperty(""retokenize.rules"", ""retokenize.rules.txt"");
    StanfordCoreNLP stanfordPipeline = new StanfordCoreNLP(properties);
    runPipeline(pipelineWithRetokenize, text);
</code></pre>

<p>}</p>

<p>And the pipeline:</p>

<pre><code>public static void runPipeline(StanfordCoreNLP pipeline, String text) {
    Annotation annotation = new Annotation(text);
    pipeline.annotate(annotation);
    out.println();
    out.println(""The top level annotation"");
    out.println(annotation.toShorterString());
    //...
}
</code></pre>
",stanford-nlp,"<p>Thanks for letting us know.  The CoreAnnotations.ValueAnnotation is not being populated and we'll update TokenRegex to populate the field.</p>

<p>Regardless, you should be able to use TokenRegex to retokenize as you have planned.  Most of the pipeline does not depending on the ValueAnnotation and uses the CoreAnnotations.TextAnnotation  instead.  You can use the CoreAnnotations.TextAnnotation to get the text for the new tokens (each token is a CoreLabel so you can access it using token.word() as well).  </p>

<p>See <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ling/tokensregex/demo/TokensRegexRetokenizeDemo.java"" rel=""nofollow"">TokensRegexRetokenizeDemo</a> for example code on how to get the different annotations out. </p>
",0,0,183,2016-04-27 11:53:13,https://stackoverflow.com/questions/36889011/tokensregex-tokens-are-null-after-retokenization
How to get NN andNNS from a text?,"<p>I want to get NN or NNS from a sample text as given within the script below. To this end, when I use the code below, the output is:</p>

<pre><code>types
synchronization
phase
synchronization
-RSB-
synchronization
-LSB-
-RSB-
projection
synchronization
</code></pre>

<p>Here why am I getting <code>[-RSB-]</code> or <code>[-LSB-]</code>? Should I use a different pattern to get NN or NNS at the same time?</p>

<pre><code>                atic = ""So far, many different types of synchronization have been investigated, such as complete synchronization [8], generalized synchronization [9], phase synchronization [10], lag synchronization [11], projection synchronization [12, 13], and so forth."";

Reader reader = new StringReader(atic);
DocumentPreprocessor dp = new DocumentPreprocessor(reader);        
docs_terms_unq.put(rs.getString(""u""), new ArrayList&lt;String&gt;());
docs_terms.put(rs.getString(""u""), new ArrayList&lt;String&gt;());

for (List&lt;HasWord&gt; sentence : dp) {

List&lt;TaggedWord&gt; tagged = tagger.tagSentence(sentence);
GrammaticalStructure gs = parser.predict(tagged);


Tree x = parserr.parse(sentence); 
System.out.println(x);
TregexPattern NPpattern = TregexPattern.compile(""@NN|NNS"");
TregexMatcher matcher = NPpattern.matcher(x);


while (matcher.findNextMatchingNode()) {

Tree match = matcher.getMatch();
ArrayList hh = match.yield();    
Boolean b = false;

System.out.println(hh.toString());}
</code></pre>
","java, stanford-nlp, text-extraction","<p>I do not know why those are coming up.  But you will get more accurate POS tags if you use the part of speech tagger.  I would suggest just looking directly at the Annotation.  Here is some sample code.</p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

import java.util.Properties;

public class NNExample {

    public static void main(String[] args) {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        String text = ""So far, many different types of synchronization have been investigated, such as complete "" +
                ""synchronization [8], generalized synchronization [9], phase synchronization [10], "" +
                ""lag synchronization [11], projection synchronization [12, 13], and so forth."";
        Annotation annotation = new Annotation(text);
        pipeline.annotate(annotation);
        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
            for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                String partOfSpeechTag = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
                if (partOfSpeechTag.equals(""NN"") || partOfSpeechTag.equals(""NNS"")) {
                    System.out.println(token.word());
                }
            }
        }
    }
}
</code></pre>

<p>And the output I get.</p>

<pre><code>types
synchronization
synchronization
synchronization
phase
synchronization
lag
synchronization
projection
synchronization
</code></pre>
",1,0,161,2016-04-27 12:06:38,https://stackoverflow.com/questions/36889335/how-to-get-nn-andnns-from-a-text
Can not get an Output file in Stanford NER,"<p>I'm new to Stanford NER and have some problems.
I have downloaded Stanford Named Entity Recognizer version 3.6.0. It works, no problem. But I can't get a tagged text as an output file. Read about extracting data on this site: <a href=""http://www.themacroscope.org/2.0/using-the-stanford-named-entity-recognizer-to-extract-data-from-texts/"" rel=""nofollow noreferrer"">http://www.themacroscope.org/2.0/using-the-stanford-named-entity-recognizer-to-extract-data-from-texts</a> (Windows user).
Tried to do the same, but got a few errors in command line:</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/slf4j/LoggerFacto
ry
        at edu.stanford.nlp.io.IOUtils.&lt;clinit&gt;(IOUtils.java:42)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(Abstrac
tSequenceClassifier.java:1484)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExcept
ions(AbstractSequenceClassifier.java:1497)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3015)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        ... 4 more
</code></pre>
<p>Can somebody explain what's wrong and how should I correct it?
Thank you!</p>
","java, nlp, stanford-nlp","<p>You need to include the lib folder in your CLASSPATH.  It contains the necessary jars.</p>
",0,0,358,2016-04-29 18:29:55,https://stackoverflow.com/questions/36945230/can-not-get-an-output-file-in-stanford-ner
Reusable version of DKPro Core pipeline,"<p>I have set up DKPro Core as a web service to take an input and provide a tokenised output.  The service itself is set up as a Jersey resource:</p>

<pre><code>@Path(""/"")
public class MyResource
{

  public MyResource()
  {
    // Nothing here
  }

  @GET
  public String generate(@QueryParam(""q"") final String input)
  {
    try
    {
      final JCasIterable en = iteratePipeline(
        createReaderDescription(StringReader.class, StringReader.PARAM_DOCUMENT_TEXT, input, StringReader.PARAM_LANGUAGE, ""en"")
       ,createEngineDescription(StanfordSegmenter.class)
       ,createEngineDescription(StanfordPosTagger.class)
       ,createEngineDescription(StanfordParser.class)
       ,createEngineDescription(StanfordNamedEntityRecognizer.class)
      );

      final StringBuilder sb = new StringBuilder();
      for (final JCas jCas : en)
      {
        for (final Token token : select(jCas, Token.class))
        {
          sb.append('[');
          sb.append(token.getCoveredText());
          sb.append(' ');
          sb.append(token.getPos().getPosValue());
          sb.append(']');
        }
      }
      return sb.toString();
    }
    catch (final Exception e)
    {
      throw new RuntimeException(""Problem"", e);
    }
  }
}
</code></pre>

<p>Everything works but it is very slow, taking 7-10 seconds for each input.  I assume that this is because the pipeline is being recreated for each request.</p>

<p>How can this code be reworked to move the pipeline creation to the constructor and reduce the load for individual requests?  Note that there could be multiple simultaneous requests so anything that isn't thread-safe will need to be inside the request.</p>
","nlp, stanford-nlp, uima, dkpro-core","<p>Create a single CAS:</p>

<pre><code>JCas jcas = JCasFactory.createJCas();
</code></pre>

<p>Fill the CAS</p>

<pre><code>jcas.setDocumentText(""This is a test"");
jcas.setDocumentLanguage(""en"");
</code></pre>

<p>Create the pipeline once (and keep the engine around for further requests) using</p>

<pre><code>AnalysisEngine engine = createEngine(
   createEngineDescription(...),
   createEngineDescription(...),
   ...);
</code></pre>

<p>If you create the engine implicitly all the time, it has to load models etc over and over again.</p>

<p>Apply the pipeline to the CAS</p>

<pre><code>SimplePipeline.runPipeline(jcas, engine);
</code></pre>

<p>If you want to further speed up processing, then create yourself a pool of CASes and re-use them across multiple requests - creating a CAS from scratch takes a moment.</p>

<p>Some components may be thread-safe, others may not. This is largely up to the implementation of the underlying third-party library. But also the wrappers in DKPro Core are not explicitly built to be thread-safe. For example, in the default configuration, models are loaded and used depending on the document language. If you use the same instance of an analysis engine from multiple threads, this would cause problems. </p>

<p>Again, you should consider creating a pool of pre-instantiated pipelines. You would need quite a bit of memory though, because each instance will be loading their own models. There is some experimental functionality to share models between instances of the same component, but it is not tested too much. Mind that third-party tools may also have implemented their models in a non-thread-safe manner. For model sharing in DKPro Core, see <a href=""https://groups.google.com/d/msg/dkpro-core-user/lHvIN4VfUKk/7gr2ryUgT4MJ"" rel=""nofollow"">this discussion on the mailing list</a>.</p>

<p><em>Disclosure: I am one of the DKPro Core developers.</em></p>
",2,1,337,2016-05-02 17:45:23,https://stackoverflow.com/questions/36988426/reusable-version-of-dkpro-core-pipeline
What treebank was used to train the Stanford CoreNLP Spanish constituency parser?,"<p>I've searched the docs and the FAQs but I have yet to find the answer.  Was the IULA treebank from the Pompeu Fabra Uni used?  <a href=""https://www.iula.upf.edu/recurs01_tbk_uk.htm"" rel=""nofollow"">https://www.iula.upf.edu/recurs01_tbk_uk.htm</a></p>

<p>Thanks.</p>
",stanford-nlp,"<p>The parser was trained on a preprocessed version of the AnCora Spanish 3.0 corpus.</p>

<p>You can find more information about the training data and the preprocessing at</p>

<p><a href=""http://nlp.stanford.edu/software/spanish-faq.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/spanish-faq.html</a> .</p>
",1,0,162,2016-05-04 05:08:23,https://stackoverflow.com/questions/37018925/what-treebank-was-used-to-train-the-stanford-corenlp-spanish-constituency-parser
Stanford CoreNLP - Test command with conll output,"<p>I'm new with CoreNLP and Dependency Parser. I'm a bit lost with the documentation. In this doc page I found how to train a model <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">train doc page</a> but I can't find the command line to test a file in conll format with a pre-trained model and get the ouput in conll. Someone knows the command?</p>

<p>Thanks and regards.</p>
","python, parsing, dependencies, stanford-nlp","<p>I found the command here <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.0/edu/stanford/nlp/parser/nndep/DependencyParser.html"" rel=""nofollow"">Dependency Javadoc</a> so If someone needs know the test command to test a conll file with a trained model and set the output in conll format use this command:</p>

<p>-> Change ""-mx10g"" to the ammount of RAM that you need &lt;-</p>

<p><code>java -mx10g -cp ""*"" edu.stanford.nlp.parser.nndep.DependencyParser -model /path/to/&lt;your.model.txt or your.model.txt.gz&gt; -testFile /path/to/conllu/test/file/en-ud-test.conllu -outFile results.conll</code></p>
",0,0,381,2016-05-05 09:01:56,https://stackoverflow.com/questions/37046494/stanford-corenlp-test-command-with-conll-output
How to keep punctuation in Stanford dependency parser,"<p>I am using Stanford CoreNLP (01.2016 version) and I would like to keep the punctuation in the dependency relations. I have found some ways for doing that when you run it from command line, but I didn't find anything regarding the java code which extracts the dependency relations.</p>

<p>Here is my current code. It works, but no punctuation is included:</p>

<pre><code>Annotation document = new Annotation(text);

        Properties props = new Properties();

        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");

        props.setProperty(""ssplit.newlineIsSentenceBreak"", ""always"");

        props.setProperty(""ssplit.eolonly"", ""true"");

        props.setProperty(""pos.model"", modelPath1);

        props.put(""parse.model"", modelPath );

        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        pipeline.annotate(document);

        LexicalizedParser lp = LexicalizedParser.loadModel(modelPath + lexparserNameEn,

                ""-maxLength"", ""200"", ""-retainTmpSubcategories"");

        TreebankLanguagePack tlp = new PennTreebankLanguagePack();

        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();

        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

        for (CoreMap sentence : sentences) {

            List&lt;CoreLabel&gt; words = sentence.get(CoreAnnotations.TokensAnnotation.class);               

            Tree parse = lp.apply(words);

            GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
            Collection&lt;TypedDependency&gt; td = gs.typedDependencies();

            parsedText += td.toString() + ""\n"";
</code></pre>

<p>Any kind of dependency relation is OK for me, basic, typed, collapsed, etc.
I just want to include the punctuation marks.</p>

<p>Thanks in advance,</p>
","java, nlp, stanford-nlp, dependency-parsing","<p>You are doing quite a bit of extra work here as you are running the parser once through CoreNLP and then again by calling <code>lp.apply(words)</code>.</p>

<p>The easiest way of getting a dependency tree/graph with punctuation marks is by using the CoreNLP option <code>parse.keepPunct</code> as following.</p>

<pre><code>Annotation document = new Annotation(text);
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");
props.setProperty(""ssplit.newlineIsSentenceBreak"", ""always"");
props.setProperty(""ssplit.eolonly"", ""true"");
props.setProperty(""pos.model"", modelPath1);
props.setProperty(""parse.model"", modelPath);
props.setProperty(""parse.keepPunct"", ""true"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

pipeline.annotate(document);

for (CoreMap sentence : sentences) {
   //Pick whichever representation you want
   SemanticGraph basicDeps = sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);
   SemanticGraph collapsed = sentence.get(SemanticGraphCoreAnnotations.CollapsedDependenciesAnnotation.class);
   SemanticGraph ccProcessed = sentence.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
}
</code></pre>

<p>The sentence annotation object stores the dependency trees/graphs as a <code>SemanticGraph</code>. If you want a list of <code>TypedDependency</code> objects, use the method <code>typedDependencies()</code>. For example,</p>

<pre><code>List&lt;TypedDependency&gt; dependencies = basicDeps.typedDependencies();
</code></pre>
",2,2,580,2016-05-10 06:24:50,https://stackoverflow.com/questions/37130722/how-to-keep-punctuation-in-stanford-dependency-parser
NLTK API to Stanford POSTagger works fine for ipython in terminal but not working in Anaconda with spyder,"<p>I have downloaded stanford postagger and parser following the instructions written for below question:</p>

<p><a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk/34112695#34112695"">Stanford Parser and NLTK</a></p>

<p>But when I execute the commands at bottom, it worked perfectly fine for ipython in terminal (Mac OS) but showed error in Spyder(Anaconda) 
(NLTK was unable to find stanford-postagger.jar!) Since I have set <code>CLASSPATH</code> in terminal, I am not sure what went wrong. When I checked </p>

<pre><code>import os
print os.environ.get('CLASSPATH')
</code></pre>

<p>It returned <code>None</code> in Spyder but correct path in terminal. I have also restarted the program and set directory to <code>$HOME</code>. Is there anything I might be missing here?</p>

<pre><code>from nltk.tag.stanford import StanfordPOSTagger
st = StanfordPOSTagger('english-bidirectional-distsim.tagger')
st.tag('What is the airspeed of an unladen swallow ?'.split())
</code></pre>
","python, nltk, stanford-nlp, anaconda, spyder","<p>The problem has nothing to do with python or the nltk; it's a consequence of how OS X starts GUI applications. Basically, the <code>CLASSPATH</code> environment variable is set in your <code>.profile</code> or its kin, but this file is only executed when you are starting Terminal; GUI applications inherit their environment from your login process, which doesn't know <code>CLASSPATH</code>. </p>

<p>There are numerous SO questions about how to deal with this; see <a href=""https://superuser.com/q/476752/127080"">here</a> or <a href=""https://stackoverflow.com/questions/135688/setting-environment-variables-in-os-x"">here</a>. But in your case, there are also a couple of work-arounds that ought to work:</p>

<ol>
<li><p>Start Spyder from the Terminal command line, not via the Launchpad (just type <code>spyder &amp;</code>). Or </p></li>
<li><p>Your python program can also set its own environment (which will be inherited by child processes) prior to launching the Stanford parser, like this:</p>

<pre><code>os.putenv(""CLASSPATH"", ""/path/to/the/parser"")
</code></pre></li>
</ol>
",1,0,182,2016-05-10 20:39:43,https://stackoverflow.com/questions/37148550/nltk-api-to-stanford-postagger-works-fine-for-ipython-in-terminal-but-not-workin
Answers extraction from an unstructured text,"<p>I want to extract some answers for a group of given questions from an unstructured text. I searched a library for this proposal but I haven't found it.</p>

<p>p.s. I have used NLP tools/libraries, such as NLTK, OpenNLP, etc...</p>

<p>thanks in advance</p>
","nlp, nltk, stanford-nlp","<p>You ask for a Question Answering (QA) toolkit. Its rarely to find an open source code or a ready to use system. You can look for scientific articles that describe a QA system and try to replicate it from using the toolkits that you mentioned above. Normally these systems use NER taggger, POS, coreference resolution, etc..</p>
",2,0,289,2016-05-11 19:19:17,https://stackoverflow.com/questions/37171540/answers-extraction-from-an-unstructured-text
How can I get the Syntactic Tree from a SemanticGraph?,"<p>I have a <code>SemanticGraph</code> object and I want to get the syntactic tree (<code>Tree</code> Object) from that <code>SemanticGraph</code>. I know that the <code>SemanticGraph</code> is(can) be generated from a <code>Tree</code>, what I ask for is the inverse.  </p>
","java, nlp, stanford-nlp","<p>I don't think this is supported at this time.  The easiest thing to do is use an Annotation object which will store the SemanticGraph and Tree object for a given sentence together.</p>
",1,0,78,2016-05-11 20:02:54,https://stackoverflow.com/questions/37172265/how-can-i-get-the-syntactic-tree-from-a-semanticgraph
How to omit tokenize and ssplit annotators for Sentiment Analysis,"<p>For the task of sentiment analysis on a text, I am using the following annotators to create a pipeline:</p>

<p>annotators = tokenize, ssplit, parse, sentiment</p>

<p>After reading the documentation on annotators, I realized that tokenize and ssplit take the whole text and break it up into separate sentences to be consdiered for further parsing.
The problem on which I am working currently is sentiment analysis of tweets. Since tweets most of the times do not exceed more than a line, using a tokenize and ssplit annotator before parse seems overkill. </p>

<p>I tried to exclude the first two but it won't let me do giving out a message Exception in thread ""main"" java.lang.IllegalArgumentException: annotator ""parse"" requires annotator ""tokenize""</p>

<p>Is there any way to avoid using the tokenize and ssplit annotators to imrpove efficiency ?</p>
","nlp, stanford-nlp, sentiment-analysis","<p>Yes, if your text is already tokenized and you have a file with one sentence per line, you can tell the tokenizer to split tokens only at spaces and the sentence splitter to split sentences only at newlines.</p>

<p>The option for the tokenizer is <code>-tokenize.whitespace true</code> and the option for the sentence splitter <code>-ssplit.eolonly true</code>.</p>

<p>You can find more information on the options of the <a href=""http://stanfordnlp.github.io/CoreNLP/tokenize.html"" rel=""nofollow"">tokenizer</a> and the <a href=""http://stanfordnlp.github.io/CoreNLP/ssplit.html"" rel=""nofollow"">sentence splitter</a> in the <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">CoreNLP documentation</a>.</p>
",2,0,158,2016-05-12 21:14:19,https://stackoverflow.com/questions/37197503/how-to-omit-tokenize-and-ssplit-annotators-for-sentiment-analysis
How can I effectively build a sentiment model training dataset using Stanford CoreNLP?,"<p>I’m interested in training a new sentiment model with my own dataset.  I know that I need to create a file with sentiment labeled for sentences and their component phrases and words.  </p>

<p>I figured out how to create a tree like the following for the sentence “I do not love you.” via the BuildBinarizedDataset:</p>

<pre><code>(1 (1 I) (1 (1 (1 (1 do) (1 not)) (1 (1 love) (1 you))) (1 .)))
</code></pre>

<p>However, this seems terribly difficult to add labels manually in this format, particularly for phrases within a longer sentence.  It would be far easier if I could generate the following for labeling purposes, then convert when I am ready to train the new model. </p>

<pre><code>sentiment_score pline1

sentiment_score  phrase1

sentiment_score  phrase2

...........................

sentiment_score  phraseN

BLANK ROW

sentiment_score pline2
</code></pre>

<p>The problem is that I can’t figure out how to generate this from a sentence with the parser.  If someone could provide guidance, or direct me to documentation that will explain this process, it would help me tremendously.</p>
","nlp, stanford-nlp","<p>Here is some sample code I wrote to go through a tree and print out every subtree.  So to get the print out you want just use the printSubTrees method I wrote and have it print out everything in your sentiment tree.</p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.Word;
import edu.stanford.nlp.parser.lexparser.LexicalizedParser;
import edu.stanford.nlp.parser.lexparser.TreeBinarizer;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Properties;

public class SubTreesExample {

    public static void printSubTrees(Tree inputTree) {
        ArrayList&lt;Word&gt; words = new ArrayList&lt;Word&gt;();
        for (Tree leaf : inputTree.getLeaves()) {
            words.addAll(leaf.yieldWords());
        }
        System.out.print(inputTree.label()+""\t"");
        for (Word w : words) {
            System.out.print(w.word()+ "" "");
        }
        System.out.println();
        for (Tree subTree : inputTree.children()) {
            printSubTrees(subTree);
        }
    }

    public static void main(String[] args) {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        String text = ""I do not love you."";
        Annotation annotation = new Annotation(text);
        pipeline.annotate(annotation);
        Tree sentenceTree = annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(
                TreeCoreAnnotations.TreeAnnotation.class);
        printSubTrees(sentenceTree);

    }
}
</code></pre>
",1,0,609,2016-05-12 22:26:08,https://stackoverflow.com/questions/37198464/how-can-i-effectively-build-a-sentiment-model-training-dataset-using-stanford-co
Cannot load OpenIE model when using the open source version of CoreNLP,"<p>I downloaded the source code of coreNLP from <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">this page</a> and the model recommended in the README file. I create a new project in eclipse and tried to run openie by it return the following exception:</p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Could not load clause splitter model at edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz
    at edu.stanford.nlp.naturalli.OpenIE.&lt;init&gt;(OpenIE.java:201)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.openie(AnnotatorImplementations.java:272)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$20.create(AnnotatorFactories.java:654)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:89)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:403)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:142)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:138)
    at edu.stanford.nlp.naturalli.demo.Demo.main(Demo.java:37)
Caused by: java.io.InvalidClassException: edu.stanford.nlp.naturalli.ClauseSplitterSearchProblem$8; local class incompatible: stream classdesc serialVersionUID = 4145523451314579506, local class serialVersionUID = -7360029270983346606
    at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:621)
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623)
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:325)
    at edu.stanford.nlp.naturalli.ClauseSplitter.load(ClauseSplitter.java:283)
    at edu.stanford.nlp.naturalli.OpenIE.&lt;init&gt;(OpenIE.java:196)
    ... 7 more
</code></pre>
","java, serialization, nlp, stanford-nlp","<p>I have done the following steps:</p>

<ol>
<li><p>clone the repo from GitHub: <a href=""https://github.com/stanfordnlp/CoreNLP.git"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP.git</a></p></li>
<li><p>download the latest model jar at: <a href=""http://nlp.stanford.edu/software/stanford-corenlp-models-current.jar"" rel=""nofollow"">http://nlp.stanford.edu/software/stanford-corenlp-models-current.jar</a></p></li>
<li><p>cd CoreNLP ; ant</p></li>
<li><p>at this point you should have built the latest version of the code on GitHub</p></li>
<li><p>set CLASSPATH to include CoreNLP/classes, CoreNLP/lib and the latest models jar</p></li>
<li><p>run this command: java -Xmx4g edu.stanford.naturalli.OpenIE</p></li>
</ol>

<p>It works fine, so I don't think there are any problems with what we are currently distributing.</p>
",1,0,211,2016-05-12 23:52:34,https://stackoverflow.com/questions/37199227/cannot-load-openie-model-when-using-the-open-source-version-of-corenlp
Where can I get CoNLL-X training data?,"<p>I'm trying to train the Stanford Neural Network Dependency Parser to check phrase similarity.</p>

<p>The way I tried is:</p>

<pre><code>java edu.stanford.nlp.parser.nndep.DependencyParser -trainFile trainPath -devFile devPath -embedFile wordEmbeddingFile -embeddingSize wordEmbeddingDimensionality -model modelOutputFile.txt.gz
</code></pre>

<p>The error that I got is:</p>

<pre><code>Train File: C:\Users\rohit\Downloads\CoreNLP-master\CoreNLP-master\data\edu\stanford\nlp\parser\trees\en-onetree.txt
Dev File: null
Model File: modelOutputFile.txt.gz
Embedding File: null
Pre-trained Model File: null
################### Train
#Trees: 1
0 tree(s) are illegal (0.00%).
1 tree(s) are legal but have multiple roots (100.00%).
0 tree(s) are legal but not projective (0.00%).
###################
#Word: 3
#POS:3
#Label: 2
###################
#Transitions: 3
#Labels: 1
ROOTLABEL: null
Random generator initialized with seed 1459831358061
Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.parser.nndep.Util.scaling(Util.java:49)
    at edu.stanford.nlp.parser.nndep.DependencyParser.readEmbedFile.  (DependencyParser.java:636)
    at edu.stanford.nlp.parser.nndep.DependencyParser.setupClassifierForTraining(DependencyParser.java:787)
    at edu.stanford.nlp.parser.nndep.DependencyParser.train(DependencyParser.java:676)
    at edu.stanford.nlp.parser.nndep.DependencyParser.main(DependencyParser.java:1247)
</code></pre>

<p>The help embedded within the code says that the training file should be a - ""Path to a training treebank in CoNLL-X format"". </p>

<p>Does anyone know where I can find some CoNLL-X training data to train?
I gave training file but not embedding file and got this error.
My guess is if I give the embedding file it might work.</p>

<p>Please shed some light on which training file &amp; embedding file I should use and where I can find them.</p>
","nlp, stanford-nlp, dependency-parsing","<p><strong>CoNLL-X treebanks</strong></p>

<p>You can get the training data for Danish, Dutch, Portuguese, and Swedish available for free <a href=""http://ilk.uvt.nl/conll/post_task_data.html"" rel=""nofollow"">here</a>. For other languages, you'll probably need to license a treebank from LDC, unfortunately (details for many languages on that page).</p>

<p><a href=""http://universaldependencies.org/"" rel=""nofollow"">Universal Dependencies</a> are in CoNLL-U format, which can usually be converted to CoNLL-X format with some work.</p>

<p>Lastly, there's a large list of treebanks and their availability on <a href=""https://en.wikipedia.org/wiki/Treebank#Syntactic_treebanks"" rel=""nofollow"">this page</a>. You should be able to convert many of the dependency treebanks in this list into CoNLL-X format if they're not already in that format.</p>

<p><strong>Training the Stanford Neural Net Dependency parser</strong></p>

<p>From <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">this page</a>: The embedding file is optional, but the treebank is not. The best treebank and embedding files to use depend on which language and type of text you'd like to parse. Ideally, you would train on as much data as possible in the domain/genre that you're trying to parse.</p>
",2,3,2176,2016-05-19 08:23:40,https://stackoverflow.com/questions/37317749/where-can-i-get-conll-x-training-data
Stanford Dependency Parser - How to get phrase vectors?,"<p>In the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/parser/nndep/DependencyParser.java"" rel=""nofollow"">DependencyParser.java repository</a>, I can see it’s using recursive neural networks.
And from the open lecture (<a href=""http://cs224d.stanford.edu"" rel=""nofollow"">http://cs224d.stanford.edu</a>), I learned that these networks calculate phrase vectors at each node of the parse tree.</p>

<p>I'm trying to make the Parser to output phrase vectors so that I can plot them on the 2-d plane but so far I haven't figured it out. - Can someone please point me to the java object and line numbers where they are calculated? (I suspect that they would be in line 765~)</p>

<pre><code> private void setupClassifierForTraining(List&lt;CoreMap&gt; trainSents, List&lt;DependencyTree&gt; trainTrees, String embedFile, String preModel) {
    double[][] E = new double[knownWords.size() + knownPos.size() + knownLabels.size()][config.embeddingSize];
    double[][] W1 = new double[config.hiddenSize][config.embeddingSize * config.numTokens];
    double[] b1 = new double[config.hiddenSize];
    double[][] W2 = new double[system.numTransitions()][config.hiddenSize];
</code></pre>

<p>And if this is not the correct place to be looking for phrase vectors, I'd really appreciate it if you could point me to the code in the <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">CoreNLP project</a> I should be looking at.</p>
","nlp, stanford-nlp, deep-learning, recurrent-neural-network","<p>Which lecture are you referring to?  </p>

<p>This paper describes the neural network dependency parser we distribute:</p>

<p><a href=""http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf"" rel=""nofollow"">http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf</a></p>

<p>I don't believe it creates phrase embeddings ; it creates embeddings for words, part-of-speech tags, and for dependency labels.</p>
",2,1,224,2016-05-22 01:16:58,https://stackoverflow.com/questions/37369610/stanford-dependency-parser-how-to-get-phrase-vectors
Stanford CoreNLP OpenIE annotator,"<p>I have a question regarding Stanford CoreNLP OpenIE annotator.</p>

<p>I am using Stanford CoreNLP version <strong>stanford-corenlp-full-2015-12-09</strong> in order to extract relations using OpenIE. I don't know much Java that's why I am using the <code>pycorenlp</code> wrapper for Python 3.4. </p>

<p>I want to extract relation between all words of a sentence, below is the code I used. I am also interested in showing the confidence of each triplet:</p>

<pre><code>import nltk
from pycorenlp import *
import collections
nlp=StanfordCoreNLP(""http://localhost:9000/"")
s=""Twenty percent electric motors are pulled from an assembly line""
output = nlp.annotate(s, properties={""annotators"":""tokenize,ssplit,pos,depparse,natlog,openie"",
                                 ""outputFormat"": ""json"",""triple.strict"":""true""})
result = [output[""sentences""][0][""openie""] for item in output]
print(result)
for i in result:
for rel in i:
    relationSent=rel['relation'],rel['subject'],rel['object']
    print(relationSent)
</code></pre>

<p>This is the result i got:</p>

<pre><code>[[{'relationSpan': [4, 6], 'subject': 'Twenty percent electric motors', 'objectSpan': [8, 10], 'relation': 'are pulled from', 'object': 'assembly line', 'subjectSpan': [0, 4]}, {'relationSpan': [4, 6], 'subject': 'percent electric motors', 'objectSpan': [8, 10], 'relation': 'are pulled from', 'object': 'assembly line', 'subjectSpan': [1, 4]}, {'relationSpan': [4, 5], 'subject': 'Twenty percent electric motors', 'objectSpan': [5, 6], 'relation': 'are', 'object': 'pulled', 'subjectSpan': [0, 4]}, {'relationSpan': [4, 5], 'subject': 'percent electric motors', 'objectSpan': [5, 6], 'relation': 'are', 'object': 'pulled', 'subjectSpan': [1, 4]}]]
</code></pre>

<p>And the triplets are:</p>

<pre><code>('are pulled from', 'Twenty percent electric motors', 'assembly line')
('are pulled from', 'percent electric motors', 'assembly line')
('are', 'Twenty percent electric motors', 'pulled')
('are', 'percent electric motors', 'pulled')
</code></pre>

<p>First problem is that the confidence is not showing in the result. Second problem is that I only want to retrieve the triplet that that includes all words of the sentence i.e this triplet: </p>

<pre><code>('are pulled from', 'Twenty percent electric motors', 'assembly line')
</code></pre>

<p>What I’m getting is more than one combination of triplets. I tried to use the option <code>""triple.strict"":""true""</code> because it extracts ""triples only if they consume the entire fragment"" but it is NOT working. </p>

<p>Can anyone advise me on this?</p>
","python, stanford-nlp","<p>You should try this setting:</p>

<pre><code>""openie.triple.strict"":""true""
</code></pre>

<p>Looking through the code it appears at this time the confidence is not stored with the returned json, so you cannot get that from the CoreNLP server.</p>

<p>Since you bring this up I will push a change that will add those to the output json and let you know when that is live on the GitHub.</p>
",6,6,6516,2016-05-22 13:43:55,https://stackoverflow.com/questions/37375137/stanford-corenlp-openie-annotator
How to build a query in Python for StanfordNLP Server?,"<p>I try to implement a filter that sends a document to the StanfordNLP Server. However, I am not sure how the data to be transmitted has to look like. At the moment I am doing it like this:</p>

<pre><code>    values = {
        paragraph: ""true"",
        ""tokenize.whitespace"": ""true"",
        ""annotators"": ""tokenize,ssplit,pos"",
        ""outputFormat"": ""json""
    }

    data     = urllib.urlencode(values)
    req      = urllib2.Request(self.url, data)
    response = urllib2.urlopen(req)
    result   = response.read()
</code></pre>

<p>For a input document:</p>

<pre><code>u'I own a dog but bought this fun and helpful book for two of my favorite people that own a cat for a gift.  What a perrrrrfect gift item it is and both my friends just loved it.  Hoping to have It\\'s a Dog\\'s LIfe soon.Linda Hannawalt'
</code></pre>

<p>The server debug output (<code>data</code>) looks like this:</p>

<pre><code>[/127.0.0.1:36926] API call w/annotators tokenize,ssplit,pos,depparse,lemma,ner,mention,coref,natlog,openie
outputFormat=json&amp;This+remote%2C+for+whatever+reason%2C+was+chosen+by+Time+Warner+to+replace+their+previous+silver+remote%2C+the+Time+Warner+Synergy+V+RC-U62CP-1.12S.++The+actual+function+of+this+CLIKR-5+is+OK%2C+but+the+ergonomic+design+sets+back+remotes+by+20+years.++The+buttons+are+all+the+same%2C+there%27s+no+separation+of+the+number+buttons%2C+the+volume+and+channel+buttons+are+the+same+shape+as+the+other+buttons+on+the+remote%2C+and+it+all+adds+up+to+a+crappy+user+experience.++Why+would+TWC+accept+this+as+a+replacement%3F++This+remote+is+virtually+impossible+to+pick+up+and+use+without+staring+at+it+to+make+sure+where+your+fingers+are.++Heck%2C+you+have+to+feel+around+just+to+figure+out+if+you%27ve+grabbed+it+by+the+top+or+bottom%2C+since+there%27s+no+articulation+in+the+body+of+the+thing+to+tell+you+which+end+is+up.++Horrible%2C+just+horrible+design.++I%27m+skipping+this+and+paying+double+for+a+refurbished+Synergy+V.=true&amp;annotators=tokenize%2Cssplit%2Cpos&amp;tokenize.whitespace=true
</code></pre>

<p>But this is the output that I receive:</p>

<pre><code>&lt;type 'list'&gt;: [{'lemma': u'outputformat', 'originalText': u'outputFormat'}, {'lemma': u'json', 'originalText': u'json'}, {'lemma': u'annotator', 'originalText': u'annotators'}, {'lemma': u'%', 'originalText': u'%'}, {'lemma': u'%', 'originalText': u'%'}, {'lemma': u'2cpos', 'originalText': u'2Cpos'}, {'lemma': u'dog', 'originalText': u'dog'}, {'lemma': u'fun', 'originalText': u'fun'}, {'lemma': u'book', 'originalText': u'book'}, {'lemma': u'people', 'originalText': u'people'}, {'lemma': u'cat', 'originalText': u'cat'}, {'lemma': u'gift', 'originalText': u'gift'}, {'lemma': u'perrrrrfect', 'originalText': u'perrrrrfect'}, {'lemma': u'gift', 'originalText': u'gift'}, {'lemma': u'item', 'originalText': u'item'}, {'lemma': u'friend', 'originalText': u'friends'}, {'lemma': u'%', 'originalText': u'%'}, {'lemma': u'dog', 'originalText': u'Dog'}, {'lemma': u'%', 'originalText': u'%'}, {'lemma': u'life', 'originalText': u'LIfe'}, {'lemma': u'soon.linda', 'originalText': u'soon.Linda'}, {'lemma': u'hannawalt', 'originalText': u'Hannawalt'}, {'lemma': u'tokenize.whitespace', 'originalText': u'tokenize.whitespace'}]
</code></pre>

<p>So the actual text is in there but <code>{'lemma': u'outputformat', 'originalText': u'outputFormat'}</code> is obviously wrong. How would the correct request string look like?</p>

<p>The code of my filter:</p>

<pre class=""lang-py prettyprint-override""><code>def filter(self, paragraph):

    values = {
        paragraph: ""true"",
        ""tokenize.whitespace"": ""true"",
        ""annotators"": ""tokenize,ssplit,pos"",
        ""outputFormat"": ""json""
    }

    data = urllib.urlencode(values)

    req = urllib2.Request(self.url, data)
    response = urllib2.urlopen(req)
    result = response.read()

    result = json.loads(result)

    filtered_tokens = list()

    for sentence in result[""sentences""]:

        for token in sentence[""tokens""]:

            pos = token[""pos""]

            if pos in self.whitelist:
                filtered_tokens.append({
                    ""originalText"": token[""originalText""],
                    ""lemma"": token[""lemma""]
                })

    if self.debug is True:
        print ""Filtered Tokens:   ""
        print filtered_tokens

    return filtered_tokens
</code></pre>
","rest, stanford-nlp","<p>A lot of people like this Python wrapper: <a href=""https://github.com/smilli/py-corenlp"" rel=""nofollow"">https://github.com/smilli/py-corenlp</a> .  We eventually are going to try to release our own code that helps people use Stanford CoreNLP from Python.</p>
",1,0,171,2016-05-24 14:39:51,https://stackoverflow.com/questions/37416787/how-to-build-a-query-in-python-for-stanfordnlp-server
How to parse Penn Tree Bank and get all the child trees using stanford NLP?,"<p>Is there a way to parse the PTB tree below to get all the child trees
for example:</p>

<pre><code>Text   :  Today is a nice day.
PTB : (3 (2 Today) (3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .)))
</code></pre>

<p>Need All child trees possible</p>

<pre><code>Output  : 
(3 (2 Today) (3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .)))
(2 Today)
(3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .))
(3 (2 is) (3 (2 a) (3 (3 nice) (2 day))))
(3 (2 is) (3 (2 a) (3 (3 nice) (2 day))))
(2 is)
(3 (2 a) (3 (3 nice) (2 day)))
(2 a)
(3 (3 nice) (2 day))
(3 nice)
(2 day)
(2 .)
</code></pre>
","java, parsing, nlp, stanford-nlp","<p>The input file for this demo should be one string representation of a tree per line.  This example prints out the subtrees of the first tree.</p>

<p>The Stanford CoreNLP class of interest is Tree.  </p>

<pre><code>import edu.stanford.nlp.trees.*;

import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.InputStreamReader;
import java.io.*;

public class TreeLoadExample {

    public static void printSubTrees(Tree t) {
        if (t.isLeaf())
            return;
        System.out.println(t);
        for (Tree subTree : t.children()) {
            printSubTrees(subTree);
        }
    }


    public static void main(String[] args) throws IOException, FileNotFoundException,
            UnsupportedEncodingException {
        TreeFactory tf = new LabeledScoredTreeFactory();
        Reader r = new BufferedReader(new InputStreamReader(new FileInputStream(args[0]), ""UTF-8""));
        TreeReader tr = new PennTreeReader(r, tf);
        Tree t = tr.readTree();
        printSubTrees(t);
    }
}
</code></pre>
",1,1,552,2016-05-26 00:32:52,https://stackoverflow.com/questions/37449729/how-to-parse-penn-tree-bank-and-get-all-the-child-trees-using-stanford-nlp
Stanford CoreNLP Server disable logging,"<p>I have the feeling that the logging of the server is quite exhaustive. Is there a way to disable or reduce the logging output? It seems that if I send a document to the server it will write the content to <code>stdout</code> which might be a performance killer.</p>
<p>Can I do that somehow?</p>
<hr />
<h1><strong>Update</strong></h1>
<p>I found a way to suppress the output from the server. Still my question is how and if I can do this using a command line argument for the actual server. However for a dirty workaround it seems the following can ease the overhead.</p>
<p>Running the server with</p>
<pre><code>java -mx6g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -prettyPrint false 2&amp;&gt;1 &gt;/dev/null
</code></pre>
<p>where <code>&gt;/dev/null</code> would pipe the output into nothing. Unfortunately this alone did not help. <code>2&amp;&gt;1</code> seems to do the trick here. I confess that I do not know what it's actually doing. However, I compared two runs.</p>
<p><strong>Running with 2&amp;&gt;1 &gt;/dev/null</strong></p>
<pre><code>Processed 100 sentences
Overall time:      2.1797 sec 
Time per sentence: 0.0218 sec 
Processed 200 sentences
Overall time:      6.5694 sec 
Time per sentence: 0.0328 sec 
...
Processed 1300 sentences
Overall time:      30.482 sec 
Time per sentence: 0.0234 sec 
Processed 1400 sentences
Overall time:      32.848 sec 
Time per sentence: 0.0235 sec 
Processed 1500 sentences
Overall time:      35.0417 sec 
Time per sentence: 0.0234 sec 
</code></pre>
<p><strong>Running without additional arguments</strong></p>
<pre><code>ParagraphVectorTrainer - Epoch 1 of 6
Processed 100 sentences
Overall time:      2.9826 sec 
Time per sentence: 0.0298 sec 
Processed 200 sentences
Overall time:      5.5169 sec 
Time per sentence: 0.0276 sec 
...
Processed 1300 sentences
Overall time:      54.256 sec 
Time per sentence: 0.0417 sec 
Processed 1400 sentences
Overall time:      59.4675 sec 
Time per sentence: 0.0425 sec 
Processed 1500 sentences
Overall time:      64.0688 sec 
Time per sentence: 0.0427 sec 
</code></pre>
<p>This was a very shallow test but it appears that this can have quite an impact. The difference here is a factor of 1.828 which is quite a difference over time.</p>
<p>However, this was just a quick test and I cannot guarantee that my results are completely sane!</p>
<p><strong>Further update:</strong></p>
<p>I assume that this has to do with how the JVM is optimizing the code over time but the time per sentence becomes compareable with the one I am having on my local machine. Keep in mind that I got the results below using <code>2&amp;&gt;1 &gt;/dev/null</code> to eliminate the <code>stdout</code> logging.</p>
<pre><code>Processed 68500 sentences
Overall time:      806.644 sec 
Time per sentence: 0.0118 sec 
Processed 68600 sentences
Overall time:      808.2679 sec 
Time per sentence: 0.0118 sec 
Processed 68700 sentences
Overall time:      809.9669 sec 
Time per sentence: 0.0118 sec 
</code></pre>
","stanford-nlp, corenlp-server, stanford-nlp-server","<p>You're now the third person that's asked for this :) -- <a href=""https://stackoverflow.com/questions/36780358/preventing-stanford-core-nlp-server-from-outputting-the-text-it-receives"">Preventing Stanford Core NLP Server from outputting the text it receives</a> . In the HEAD of the GitHub repo, and in versions 3.6.1 onwards, there's a <code>-quiet</code> flag that prevents the server from outputting the text it receives. Other logging can then be configured with SLF4J, if it's in your classpath.</p>
",2,3,1585,2016-05-27 12:27:55,https://stackoverflow.com/questions/37483608/stanford-corenlp-server-disable-logging
Extract tuple containing &#39;PERSON&#39; in StanforNER list result,"<p>I am trying to create a name filter to filter out names from a given text. I am using the StanfordNER and NLTK to do so and doing this actually gives me a list containing tuples. (As shown below)</p>

<pre><code>    [(u'I', u'O'), (u'met', u'O'), (u'with', u'O'), (u'Alan', u'PERSON'), (u'yesterday', u'O')]
</code></pre>

<p>What I want to do is that I just want to extract the exact name that has been tagged as 'PERSON' from the list above which contains tuples to get only 'Alan' at the end of the process. Please help. </p>
","python, nlp, nltk, stanford-nlp","<pre><code>weird_list = [(u'I', u'O'), (u'met', u'O'), (u'with', u'O'), (u'Alan', u'PERSON'), (u'yesterday', u'O')]
for word, tag in weird_list:
    if tag == 'PERSON':
        print word
</code></pre>
",2,0,95,2016-05-29 01:15:18,https://stackoverflow.com/questions/37505245/extract-tuple-containing-person-in-stanforner-list-result
Stanford CoreNLP Simple API Error,"<p>Im just trying to run a simple example to get started with CoreNLP from <a href=""http://stanfordnlp.github.io/CoreNLP/simple.html"" rel=""nofollow"">here</a>. I have added coreNLP and models 3.6 to my pom.xml. I have also added com.google.protobuf(3.0.0-beta-3) to my pom as well.</p>

<p>This what my code looks like:</p>

<pre><code> Document doc = new Document(""add your text here! It can contain multiple sentences."");
        for (Sentence sent : doc.sentences()) {  // Will iterate over two sentences
            // We're only asking for words -- no need to load any models yet

            System.out.println(""The second word of the sentence '"" + sent + ""' is "" + sent.word(1));
            // When we ask for the lemma, it will load and run the part of speech tagger
            System.out.println(""The third lemma of the sentence '"" + sent + ""' is "" + sent.lemma(2));
            // When we ask for the parse, it will load and run the parser
            System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());
            // ...
        }
</code></pre>

<p>And here's the exception:</p>

<blockquote>
  <p>java.lang.NoSuchMethodError:
  edu.stanford.nlp.util.ArrayCoreMap.keySetNotNull()Ljava/util/Set;     at
  edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder(ProtobufAnnotationSerializer.java:377)
    at
  edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder(ProtobufAnnotationSerializer.java:332)
    at edu.stanford.nlp.simple.Document.sentences(Document.java:480)    at
  edu.stanford.nlp.simple.Document.sentences(Document.java:499)</p>
</blockquote>

<p>UPDATE:</p>

<p>I just downgraded protobuf to 2.6.1 and still the same issue. Here what my pom looks like:</p>

<pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;mysql&lt;/groupId&gt;
        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
        &lt;version&gt;5.1.38&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.google.inject&lt;/groupId&gt;
        &lt;artifactId&gt;guice&lt;/artifactId&gt;
        &lt;version&gt;4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.google.inject.extensions&lt;/groupId&gt;
        &lt;artifactId&gt;guice-servlet&lt;/artifactId&gt;
        &lt;version&gt;4.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
        &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
        &lt;version&gt;1.3.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;javax.persistence&lt;/groupId&gt;
        &lt;artifactId&gt;persistence-api&lt;/artifactId&gt;
        &lt;version&gt;1.0.2&lt;/version&gt;
    &lt;/dependency&gt;
        &lt;dependency&gt;
        &lt;groupId&gt;javax.transaction&lt;/groupId&gt;
        &lt;artifactId&gt;jta&lt;/artifactId&gt;
        &lt;version&gt;1.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.jsoup&lt;/groupId&gt;
        &lt;artifactId&gt;jsoup&lt;/artifactId&gt;
        &lt;version&gt;1.8.3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.6.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.6.0&lt;/version&gt;
        &lt;classifier&gt;models&lt;/classifier&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;
        &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt;
        &lt;version&gt;2.6.0&lt;/version&gt;
    &lt;/dependency&gt;

  &lt;/dependencies&gt;
</code></pre>
","java, stanford-nlp","<p>The simple API doesn't work with Proto v.3. It was compiled with 2.6.1, though anything in version 2 should work.</p>
",0,0,357,2016-05-30 21:49:19,https://stackoverflow.com/questions/37534005/stanford-corenlp-simple-api-error
How is feature is different from tag in Stanford-NER?,"<p><a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/CRF-NER.shtml</a>'s FAQ tells we can include our customized feature while training.
On the first place, what features do in NER? How it is different from tag in tsv training file?
As asked in this question <a href=""https://stackoverflow.com/questions/22950995/stanford-ner-customization-to-classify-software-programming-keywords"">Stanford-NER customization to classify software programming keywords</a>, is it right to represent the tags 'Programming_Language', 'Operating_System' in feature column in tsv?</p>

<p>Bit confusing, pls explain.</p>
",stanford-nlp,"<p>The tag is the label you want to apply to the token.  For instance O, PERSON, LOCATION, ORGANIZATION, PROGRAMMING_LANGUAGE.  O means not an entity.</p>

<p>A feature is an aspect of the token stream you want the CRF Classifier to use in its decision.</p>

<p>Consider the sentence ""I went to France last summer.""</p>

<p>The tags would be [O O O LOCATION O O O].</p>

<p>For instance a feature could be the word itself, ""word=France"".</p>

<p>A feature could be the last two words before the current word in the sequence ""word_n-2_n-1=went to"".</p>

<p>Or a feature could be something like the shape of the word ""shape=Xxxxxx""</p>

<p>The point of the features is that the CRF Classifier can find patterns, for instance that words with particular shapes tend to be O, or that particular words tend to belong to particular classes.</p>

<p>You do not need custom features if you simply want to add new categories such as PROGRAMMING_LANGUAGE or OPERATING_SYSTEM.  You just need training data so the system can learn how to label tokens appropriately.</p>
",2,1,369,2016-06-06 06:40:41,https://stackoverflow.com/questions/37650993/how-is-feature-is-different-from-tag-in-stanford-ner
Ontology based search,"<p>I am developing ontology-based web page search engine. We have a lot of web pages hosted by different applications. </p>

<p>All the searchable web pages are captured in ontology along with what information it contains, dimensions of information, URL, parameters, etc. </p>

<p>I wrote one page like google where the user can write search text, and I want to show all meaningful web pages link which matches closely with his query.</p>

<p>My question is more on technology stack.</p>

<ol>
<li><p>What is the best way to index and do a search on ontology? - So far I am thinking of doing it in Solr but not sure how can Index different triples in Solr and what should be my design approach. </p></li>
<li><p>Understanding user search text and translating into a query which can be executed on Ontology. - May be NLP?</p></li>
</ol>

<p>Please advice and it would be great if solution can be explained in detail.</p>
","solr, nlp, stanford-nlp, ontology, protege","<p>NER and Lucene solved my requirement</p>
",0,1,664,2016-06-06 14:39:30,https://stackoverflow.com/questions/37660263/ontology-based-search
How can I extract address from raw text using NLTK in python?,"<p>I have this text </p>

<blockquote>
  <p>'''Hi, Mr. Sam D. Richards lives here, <strong>44 West 22nd Street, New
  York, NY 12345</strong>. Can you contact him now? If you need any help, call
  me on 12345678'''</p>
</blockquote>

<p>. How the address part can be extracted from the above text using NLTK? I have tried <code>Stanford NER Tagger</code>, which gives me only <code>New York</code> as Location. How to solve this?</p>
","python, nltk, stanford-nlp, street-address","<p>Definitely regular expressions :)</p>

<p>Something like</p>

<pre><code>import re

txt = ...
regexp = ""[0-9]{1,3} .+, .+, [A-Z]{2} [0-9]{5}""
address = re.findall(regexp, txt)

# address = ['44 West 22nd Street, New York, NY 12345']
</code></pre>

<p><strong>Explanation:</strong></p>

<p><code>[0-9]{1,3}</code>: 1 to 3 digits, the address number</p>

<p><code>(space)</code>: a space between the number and the street name</p>

<p><code>.+</code>: street name, any character for any number of occurrences</p>

<p><code>,</code>: a comma and a space before the city</p>

<p><code>.+</code>: city, any character for any number of occurrences</p>

<p><code>,</code>: a comma and a space before the state</p>

<p><code>[A-Z]{2}</code>: exactly 2 uppercase chars from A to Z</p>

<p><code>[0-9]{5}</code>: 5 digits</p>

<p><code>re.findall(expr, string)</code> will return an array with all the occurrences found.</p>
",16,16,25634,2016-06-10 10:22:08,https://stackoverflow.com/questions/37745801/how-can-i-extract-address-from-raw-text-using-nltk-in-python
Sentences are not getting splitted using CoreNLP Server,"<p>Having a review text like:</p>

<blockquote>
  <p>""<em>The tutu's was for my neice... She LOVED IT!!!  It fit well and will fit her for some time with the elastic waist.... great quality and very  inexpensive!  I would buy her another easily.</em>""</p>
</blockquote>

<p>and sending it to the CoreNLP Server:</p>

<pre class=""lang-py prettyprint-override""><code>properties = {
    ""tokenize.whitespace"": ""true"",
    ""annotators"": ""tokenize, ssplit, pos, lemma, ner, parse"",
    ""outputFormat"": ""json""
}


if not isinstance(paragraph, str):
    paragraph = unicodedata.normalize('NFKD', paragraph).encode('ascii', 'ignore')

result = self.nlp.annotate(paragraph, properties=properties)
</code></pre>

<p>Is giving me this result:</p>

<pre><code>{  
   u'sentences':[  
      {  
         u'parse':u'SENTENCE_SKIPPED_OR_UNPARSABLE',
         u'index':0,
         u'tokens':[  
            {  
               u'index':1,
               u'word':u'The',
               u'lemma':u'the',
               u'pos':u'DT',
               u'characterOffsetEnd':3,
               u'characterOffsetBegin':0,
               u'originalText':u'The'
            },
            {  
               u'index':2,
               u'word':u""tutu's"",
               u'lemma':u""tutu'"",
               u'pos':u'NNS',
               u'characterOffsetEnd':10,
               u'characterOffsetBegin':4,
               u'originalText':u""tutu's""
            },
            // ...
            {  
               u'index':34,
               u'word':u'easily.',
               u'lemma':u'easily.',
               u'pos':u'NN',
               u'characterOffsetEnd':187,
               u'characterOffsetBegin':180,
               u'originalText':u'easily.'
            }
         ]
      }
   ]
}
</code></pre>

<p>I noticed that sentences are not getting splitted - any idea what the problem could be?</p>

<p>If I am using the <a href=""http://localhost:9000"" rel=""nofollow"">http://localhost:9000</a> webinteface then I see those sentences being splitted correctly..</p>
","python, stanford-nlp","<p>Don't know why but the problem appeared to come from <code>tokenize.whitespace</code>. I just commented it out:</p>

<pre><code>properties = {
    #""tokenize.whitespace"": ""true"",
    ""annotators"": ""tokenize, ssplit, pos, lemma, ner, parse"",
    ""outputFormat"": ""json""
}
</code></pre>
",1,0,124,2016-06-10 13:06:07,https://stackoverflow.com/questions/37749088/sentences-are-not-getting-splitted-using-corenlp-server
Exact Dictionary based Named Entity Recognition with Stanford,"<p>I have a dictionary of named entities, extracted from Wikipedia. I want to use it as the dictionary of an NER. I wanted to know how can I use Stanford-NER with this data of mine.
I have also downloaded Lingpipe, although I have no idea how can I use it. I would appreciate all kinds of information.</p>

<p>Thanks for your helps.</p>
","java, stanford-nlp, named-entity-recognition, named-entity-extraction, lingpipe","<p>You can use dictionary (or regular expression-based) named entity recognition with Stanford CoreNLP. See the <a href=""http://stanfordnlp.github.io/CoreNLP/regexner.html"" rel=""nofollow"">RegexNER annotator</a>. For some applications, we run this with quite large dictionaries of entities. Nevertheless, for us this is typically a secondary tool to using statistical (CRF-based) NER.</p>
",3,3,2492,2016-06-11 11:54:27,https://stackoverflow.com/questions/37763404/exact-dictionary-based-named-entity-recognition-with-stanford
StanfordNLP Openie fails,"<p>I have StanfordNLP up and running.</p>

<p>My maven dependency structure is as follows:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.6.0&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;</code></pre>
</div>
</div>
</p>

<p>My code runs just fine as follows:</p>

<pre><code>@Test
public void testTA() throws Exception
{

    Path p = Paths.get(""s.txt"");

    byte[] encoded = Files.readAllBytes(p);
    String s = new String(encoded);

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse, ner, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = s;

    StringBuffer sb = new StringBuffer();

    sb.append(text);
    sb.append(
            ""\n\n\n\n\n\n\n===================================================================\n\n\n\n\n\n\n\n\n\n\n"");

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and
    // has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    sb.append(
            ""\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+++++++++++++++++++++++SENTENCES++++++++++++++++++++++++++++\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"");
    for (CoreMap sentence : sentences)
    {
        // traversing the words in the current sentence
        // a CoreLabel is a CoreMap with additional token-specific methods
        sb.append(""\n\n\n==============SENTENCE==============\n\n\n"");
        sb.append(sentence.toString());
        sb.append(""\n"");
        for (CoreLabel token : sentence.get(TokensAnnotation.class))
        {
            // this is the text of the token
            sb.append(""\n==============TOKEN==============\n"");
            String word = token.get(TextAnnotation.class);
            sb.append(word);
            sb.append("" : "");
            // this is the POS tag of the token
            String pos = token.get(PartOfSpeechAnnotation.class);
            // this is the NER label of the token
            sb.append(pos);
            sb.append("" : "");
            String lemma = token.get(LemmaAnnotation.class);
            sb.append(lemma);
            sb.append("" : "");
            String ne = token.get(NamedEntityTagAnnotation.class);
            sb.append(ne);
            sb.append(""\n"");

        }

        // this is the parse tree of the current sentence
        Tree tree = sentence.get(TreeAnnotation.class);
        sb.append(""\n\n\n=====================TREE==================\n\n\n"");
        sb.append(tree.toString());

        // this is the Stanford dependency graph of the current sentence
        SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
        sb.append(""\n\n\n"");
        sb.append(dependencies.toString());
    }
</code></pre>

<p>However, when I add openie to the pipeline, the code fails.</p>

<p>props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse, ner, dcoref, openie"");</p>

<p>The error I get is as follows:</p>

<p>annotator ""openie"" requires annotator ""natlog""</p>

<p>Can anyone advise me on this?</p>
","nlp, stanford-nlp","<p>The answer is that annotators in the pipeline can depend on each other. 
Simply add natlog to the pipeline.
Crucially, dependencies must be added first, so </p>

<ul>
<li>natlog must be in the pipeline before openie.</li>
<li>depparse must be in the pipeline before natlog</li>
</ul>

<p>and as an aside,</p>

<ul>
<li>parse must be in the pipeline before dcoref.</li>
</ul>
",1,0,223,2016-06-12 00:26:53,https://stackoverflow.com/questions/37769670/stanfordnlp-openie-fails
Maven build for Stanford CoreNLP and Stanford Parser,"<p>Given that coreNLP is almost 1gb and Stanford Parser (as provided on the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml#Download"" rel=""nofollow"">Stanford website</a>) is 300+mb, is it necessary to include both in my classpath if I am running both in my application?</p>
","nlp, stanford-nlp","<p>It seems that all the demo functions provided in the Stanford demo code work just fine with the following maven dependencies and nothing else. Stanford corenlp is a behemoth, but the stanford-parser is just a small jar of classes. The models from core-nlp jar work with stanford-parser.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.6.0&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.6.0&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
&lt;!-- http://mvnrepository.com/artifact/edu.stanford.nlp/stanford-parser --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-parser&lt;/artifactId&gt;
    &lt;version&gt;3.6.0&lt;/version&gt;
&lt;/dependency&gt;</code></pre>
</div>
</div>
</p>
",0,0,235,2016-06-12 04:28:19,https://stackoverflow.com/questions/37770729/maven-build-for-stanford-corenlp-and-stanford-parser
Stanford NLP: set RegexNERAnnotator to caseInsensitive,"<p>I am identifying qualifications in a large corpus. I am using NamedEntityTagAnnotation.</p>

<p>Problem:</p>

<p>My annotations are read in as case sensitive. I want them to be case insensitive.
Hence</p>

<p>Bachelor's Degree   DEGREE</p>

<p>does not need an additional entry of </p>

<p>Bachelor's degree   DEGREE</p>

<p>I know this is possible. RegexNERAnnotator has a field for ignoreCase. But I don't know how to access RegexNERAnnotator  through the API. </p>

<p>My current code  (which I cadged off the internet and works apart from the case issue) is as follows:</p>

<pre><code>        String prevNeToken = ""O"";
    String currNeToken = ""O"";
    boolean newToken = true;
    for (CoreLabel token : sentence.get(TokensAnnotation.class))
    {
      currNeToken = token.get(NamedEntityTagAnnotation.class);

      String word = token.get(TextAnnotation.class);

      if (currNeToken.equals(""O""))
      {

        if (!prevNeToken.equals(""O"") &amp;&amp; (sbuilder.length() &gt; 0))
        {
          handleEntity(prevNeToken, sbuilder, tokens);
          newToken = true;
        }
        continue;
      }

      if (newToken)
      {
        prevNeToken = currNeToken;
        newToken = false;
        sbuilder.append(word);
        continue;
      }

      if (currNeToken.equals(prevNeToken))
      {
        sbuilder.append("" "" + word);
      }
      else
      {

        handleEntity(prevNeToken, sbuilder, tokens);
        newToken = true;
      }
      prevNeToken = currNeToken;
    }
</code></pre>

<p>Any assistance would be greatly appreciated.</p>
","nlp, stanford-nlp","<p>The answer is in how you set up the pipeline.</p>

<pre><code>    Properties props = new Properties();

    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, regexner, depparse,  natlog,  openie"");


    //props.put(""regexner.mapping"", namedEntityPropertiesPath);

    pipeline = new StanfordCoreNLP(props);
    pipeline.addAnnotator(new TokensRegexNERAnnotator(namedEntityPropertiesPath, true));
</code></pre>

<p>Do <em>not</em> use props.put(""regexner.mapping"", namedEntityPropertiesPath);</p>

<p>Use pipeline.addAnnotator.</p>

<p>The first argument to the constructor is the path to your NER data file. The second is a boolean caseInsensitive.</p>

<p>Note, that this then uses Stanford's NER lists as well as your own. It also uses a more complex NER data file. </p>

<p>See <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.html</a></p>
",3,1,376,2016-06-13 06:39:38,https://stackoverflow.com/questions/37783226/stanford-nlp-set-regexnerannotator-to-caseinsensitive
Extracting phrase n-grams from a sentence corresponding to the main verb,"<p>I am working on a project which needs me to extract similarity between sentences. So given a sentence, I need the phrase n-gram of that sentence, which is 'a combination of the main verb and the noun phrase left and right of the verb'. Any idea how to extract this? I am given the dependency and constituency parse trees of the sentence. I am using Python.</p>

<pre><code>Sample Sentence: My dog also likes eating sausage.
Constituency Parse Tree:
ROOT
(S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (NP (NN sausage)))))
    (. .)))

Dependency Graph:
nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
</code></pre>

<p>Main verb : likes</p>

<p>Left Noun Phrase(NP) : My dog</p>

<p>Right Noun Phrase : sausage.</p>
","python, graph, nlp, stanford-nlp","<p>Have you tried <a href=""http://nlp.stanford.edu/software/openie.html"" rel=""nofollow"">Stanford OpenIE</a>? Or, for that matter, any OpenIE system (<a href=""https://knowitall.github.io/ollie/"" rel=""nofollow"">Ollie</a> / ReVerb / etc.).</p>

<p><strong>Minimal usage (via <a href=""http://stanfordnlp.github.io/CoreNLP/simple.html"" rel=""nofollow"">Simple CoreNLP</a>):</strong></p>

<p><code>new Sentence(""My dog also likes eating sausage."").openieTriples();</code></p>

<p><strong>Pipeline/server usage:</strong> </p>

<p>create a CoreNLP pipeline, and set the annotators to <code>tokenize,ssplit,pos,lemma,depparse,natlog,openie</code>. Then, the Open IE triples should be keyed on <code>RelationTripleAnnotation.class</code> key of a sentence.</p>

<p><strong>Try it out at <a href=""http://corenlp.run"" rel=""nofollow"">corenlp.run</a></strong></p>
",0,1,296,2016-06-13 10:44:22,https://stackoverflow.com/questions/37787708/extracting-phrase-n-grams-from-a-sentence-corresponding-to-the-main-verb
Relationship Extraction (RE) using Stanford API,"<p>I have created a custom Named Entity Recognition(NER) classifier and a custom Relationship Extraction(RE) classifier. In the training data for the RE, I have given it a set of 10 sentences in which I have given the exact entities and the relationship between them.</p>

<p>When I am running the code I am getting the correct relationships for 6 out of the 10 sentences. However, I am not getting the correct relationship in all the sentences. I wanted to understand why is the RE code not able to identify the correct relationships in the sentences even though I have given the exact same sentence in the training data?</p>

<p>For example, the following sentence:</p>

<blockquote>
  <p>The Fund's objective is to help our members achieve the best possible
  RetOue.</p>
</blockquote>

<p>In the training data, the relationship given is </p>

<blockquote>
  <p>Fund  RetOue  build</p>
</blockquote>

<p>Below are all the RelationMentions found in the sentence and it can be seen that the relation beween ""Fund"" and ""RetOut"" is coming as _NR and has a probability of (_NR, 0.6074190677382846) and the actual relation (build, 0.26265263651796966) has a lower probability. The second one in the list below:</p>

<pre><code>RelationMention [type=_NR, start=1, end=9, {_NR, 0.8706606065870188; build, 0.04609463244214589; reply, 0.014127678851794745; cause, 0.01412618987143006; deliver, 0.014028667880335159; calculate, 0.014026673364224201; change, 0.013888249765034161; collaborate, 0.01304730123801706}
    EntityMention [type=RESOURCE, objectId=EntityMention-10, hstart=1, hend=2, estart=1, eend=2, headPosition=1, value=""Fund"", corefID=-1]
    EntityMention [type=ROLE, objectId=EntityMention-11, hstart=8, hend=9, estart=8, eend=9, headPosition=8, value=""members"", corefID=-1]
]

RelationMention [type=_NR, start=1, end=14, {_NR, 0.6074190677382846; build, 0.26265263651796966; collaborate, 0.029635339573025835; reply, 0.020273680468829585; cause, 0.020270355199687763; change, 0.020143296854960534; calculate, 0.019807048865472295; deliver, 0.01979857478176975}
    EntityMention [type=RESOURCE, objectId=EntityMention-10, hstart=1, hend=2, estart=1, eend=2, headPosition=1, value=""Fund"", corefID=-1]
    EntityMention [type=RESOURCE, objectId=EntityMention-12, hstart=13, hend=14, estart=13, eend=14, headPosition=13, value=""RetOue"", corefID=-1]
]

RelationMention [type=_NR, start=1, end=9, {_NR, 0.9088620248226259; build, 0.029826907381364745; cause, 0.01048834533846858; reply, 0.010472406713467062; change, 0.010430417119225247; deliver, 0.010107963031033371; calculate, 0.010090071219976819; collaborate, 0.009721864373838134}
    EntityMention [type=ROLE, objectId=EntityMention-11, hstart=8, hend=9, estart=8, eend=9, headPosition=8, value=""members"", corefID=-1]
    EntityMention [type=RESOURCE, objectId=EntityMention-10, hstart=1, hend=2, estart=1, eend=2, headPosition=1, value=""Fund"", corefID=-1]
]

RelationMention [type=_NR, start=8, end=14, {_NR, 0.6412212367693484; build, 0.0795874107991397; deliver, 0.061375929752833555; calculate, 0.061195561682179045; cause, 0.03964100603702037; reply, 0.039577811103586304; change, 0.03870906323316812; collaborate, 0.038691980622724644}
    EntityMention [type=ROLE, objectId=EntityMention-11, hstart=8, hend=9, estart=8, eend=9, headPosition=8, value=""members"", corefID=-1]
    EntityMention [type=RESOURCE, objectId=EntityMention-12, hstart=13, hend=14, estart=13, eend=14, headPosition=13, value=""RetOue"", corefID=-1]
]

RelationMention [type=_NR, start=1, end=14, {_NR, 0.8650327055005457; build, 0.05264799740623545; collaborate, 0.01878896136615606; reply, 0.012762167223115933; cause, 0.01276049397449083; calculate, 0.012671777715382195; change, 0.012668721250994311; deliver, 0.012667175563079464}
    EntityMention [type=RESOURCE, objectId=EntityMention-12, hstart=13, hend=14, estart=13, eend=14, headPosition=13, value=""RetOue"", corefID=-1]
    EntityMention [type=RESOURCE, objectId=EntityMention-10, hstart=1, hend=2, estart=1, eend=2, headPosition=1, value=""Fund"", corefID=-1]
]

RelationMention [type=_NR, start=8, end=14, {_NR, 0.8687007489440899; cause, 0.019732766828364688; reply, 0.0197319383076219; change, 0.019585387681083893; collaborate, 0.019321463597270272; deliver, 0.018836262558606865; calculate, 0.018763499991179922; build, 0.015327932091782685}
    EntityMention [type=RESOURCE, objectId=EntityMention-12, hstart=13, hend=14, estart=13, eend=14, headPosition=13, value=""RetOue"", corefID=-1]
    EntityMention [type=ROLE, objectId=EntityMention-11, hstart=8, hend=9, estart=8, eend=9, headPosition=8, value=""members"", corefID=-1]
]
</code></pre>

<p>I wanted to understand the reasons I should look out for for this. </p>

<p>Q.1 My assumption was that as entity types are being recognized accurately will help in the  relationship getting recognized accurately. Is it correct?</p>

<p>Q.2 How can I improve my training data to make sure I ge the accurate relationship as the result?</p>

<p>Q.3 Does it matter how many records of each entity type I have defined? Should I maintain equal number of definitions for each relation type? For Example: In my training data if I have 10 exampls of the relationship ""build"", then should I define 10 relations each of the other relationship types as well like for ""cause"", ""reply"" etc.?</p>

<p>Q.4 My assumption is that the correct NER classification of the entity makes a difference in the relationship extraction. Is it correct?</p>
",stanford-nlp,"<p>There are lots of features that can be used by RE for improving the accuracy of the relationship classification that need to be analysed in detail.</p>

<p>Answers to my questions:
A.1. Yes, entity types are being recognized accurately will help in the relationship getting recognized accurately.
A.2. As far as I know, training data needs to be annotated and improved manually.
A.3. As far as I know, yes the number of records defined between entities matters.
A.4. The NER accuracy makes a difference in the RE accuracy.</p>
",0,0,242,2016-06-13 12:12:13,https://stackoverflow.com/questions/37789561/relationship-extraction-re-using-stanford-api
Where can i find the implementation of SPIED tool from stanford corenlp?,"<p>In the paper, ""<em>Improved Pattern Learning for Bootstrapped Entity Extraction. Sonal Gupta and Christopher D. Manning. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning (CoNLL). 2014.</em>""<br>
cited for the tool the link for implementation has been specified as <a href=""http://nlp.stanford.edu/software/patternviz.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/patternviz.shtml</a> but it seems to be taken down.</p>
",stanford-nlp,"<p>Oops! We should maybe fix that; but, in the meantime the new link is: <a href=""http://nlp.stanford.edu/software/patternslearning.html"" rel=""nofollow"">http://nlp.stanford.edu/software/patternslearning.html</a>. The code is distributed with Stanford CoreNLP, so there's no extra download. An example invocation is:</p>

<pre><code>java -cp stanford-corenlp-3.5.1.jar:stanford-corenlp-3.5.1-models.jar:javax.json.jar:joda-time.jar:jollyday.jar edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass -props patterns/example.properties
</code></pre>
",0,0,239,2016-06-15 12:42:09,https://stackoverflow.com/questions/37835795/where-can-i-find-the-implementation-of-spied-tool-from-stanford-corenlp
Running Stanford corenlp server with French models,"<p>I am trying to analyse some French text with the Stanford CoreNLP tool (it's my first time trying to use any StanfordNLP software)</p>

<p>To do so, I have downloaded the v3.6.0 jar and the corresponding <a href=""http://nlp.stanford.edu/software/stanford-french-corenlp-2016-01-14-models.jar"" rel=""nofollow noreferrer"">french models</a>.</p>

<p>Then I run the server with: </p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
</code></pre>

<p>As described in this <a href=""https://stackoverflow.com/a/34782849/4709400"">answer</a>, I call the API with:</p>

<pre><code>wget --post-data 'Bonjour le monde.' 'localhost:9000/?properties={""parse.model"":""edu/stanford/nlp/models/parser/nndep/UD_French.gz"", ""annotators"": ""parse"", ""outputFormat"": ""json""}' -O -
</code></pre>

<p>but I get the following log + error: </p>

<pre><code> [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP  
 Adding annotator tokenize
 [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
 [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP -   Adding annotator ssplit
 [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
 [pool-1-thread-1] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/parser/nndep/UD_French.gz ... 

 edu.stanford.nlp.io.RuntimeIOException: java.io.StreamCorruptedException: invalid stream header: 64696374
    at edu.stanford.nlp.parser.common.ParserGrammar.loadModel(ParserGrammar.java:188)
    at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:212)
    at edu.stanford.nlp.pipeline.ParserAnnotator.&lt;init&gt;(ParserAnnotator.java:115)
    ...
</code></pre>

<p>The solutions proposed <a href=""https://stackoverflow.com/questions/22048384/invalid-stream-header-with-stanford-nlp-library"">here</a> suggest the code and model version differs but I have dowloaded them from the same page (and they both have the same version number in their name) so I am pretty sure they are the same. </p>

<p>Any other hint on what I am doing wrong?</p>

<p><em>(I should also mention that I am not a Java expert, so maybe I forgot a stupid step... )</em></p>
","stanford-nlp, stanford-nlp-server","<p>Ok, after a lot of readings and unsuccessful tries, I found a way to make it work (for <strong>v3.6.0</strong>).  Here are the details, if they can be of any interest to someone else:</p>

<ol>
<li><p>Dowload the code and french models from <a href=""http://stanfordnlp.github.io/CoreNLP/index.html#download"" rel=""noreferrer"">http://stanfordnlp.github.io/CoreNLP/index.html#download</a>. Unzip the code <code>.zip</code> and copy the french model <code>.jar</code> to that directory (do not remove the english models, they have different names anyway)</p></li>
<li><p>cd to that directory and then run the server with: </p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
</code></pre></li>
</ol>

<p><em>(it's a pity that the <code>-prop</code> flag doesn't help here)</em></p>

<ol start=""3"">
<li><p>Call the API repeating the properties listed in the <code>StanfordCoreNLP-french.properties</code>:</p>

<pre><code>wget --header=""Content-Type: text/plain; charset=UTF-8""
     --post-data 'Bonjour le monde.' 
     'localhost:9000/?properties={
       ""annotators"": ""tokenize,ssplit,pos,parse"", 
       ""parse.model"":""edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz"", 
       ""pos.model"":""edu/stanford/nlp/models/pos-tagger/french/french.tagger"", 
       ""tokenize.language"":""fr"", 
       ""outputFormat"": ""json""}' 
  -O -
</code></pre>

<p>which finally gives a 200 response using the French models! </p></li>
</ol>

<p>(NB: don't know how to make it work with the UI (same for utf-8 support))</p>
",10,6,2691,2016-06-15 14:31:19,https://stackoverflow.com/questions/37838374/running-stanford-corenlp-server-with-french-models
how to train a french NER based on stanford-nlp Conditional Random Fields model?,"<p>I discovered the tools of stanford-NLP and found it really interesting.
I'm a french dataminer / datascientist, fond of text analysis and would love to use your tools, but the NER being unavailable in french is quite puzzling to me.</p>

<p>I would love to make my own french NER, perhaps even provide it as a contribution to the package if it is considered worthy, so... could you brief me on the requirements to train a CRF for french NER based on the stanford coreNLP ?</p>

<p>Thank you.</p>
",stanford-nlp,"<p>NB: I am not a developper of the Stanford tools, nor a NLP expert. Just a lambda user that also needed such informations at some point. Also note that part of the information given below are from the official FAQ: <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#a</a></p>

<p>Here are the steps I followed to train my own NER:</p>

<ol>
<li>Install java8</li>
<li><p>Create a train/test sample. It must take the form of <code>.tsv</code> files with the following format:</p>

<pre><code>  Venez    O
  découvrir    O
  lundi    DAY
  le    O
  nouvel    O
  espace    O
  de    O
  vente    O
  ODHOJS    ORGANISATION
</code></pre>

<p>Depending on the original format of your text, you can create this sample with SQL statement or other NLP tools. The labelling is the most complicated part as I don't know other ways to proceed than to do it by hand.</p></li>
<li><p>Train the model with this command:</p>

<pre><code>java -cp ""stanford-ner.jar:lib/*"" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop prop.txt
</code></pre>

<p>where <code>prop.txt</code> is also described <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""noreferrer"">here</a>.</p>

<p>This should create a new <code>.jar</code> containing the newly trained model.</p></li>
<li><p>Test the model performances:</p>

<pre><code>java -cp ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier ner-model.ser.gz -testFile test.tsv &gt; test.res
</code></pre>

<p>The input <code>test.tsv</code> has the same format than the <code>train.tsv</code> file. The output in <code>test.res</code> has an extra column containing the NER predicted class. The last lines also show the summary in terms of precision, recall and F1.</p></li>
<li><p>Finally, you can use your NER on real data:</p>

<pre><code>java -cp ""stanford-ner.jar:lib/*"" -mx5g edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier ner-model.ser.gz  -textFile test.txt -outputFormat inlineXML &gt; test.res
</code></pre></li>
</ol>

<p>Hope it helps.</p>
",8,6,1936,2016-06-16 07:05:02,https://stackoverflow.com/questions/37852084/how-to-train-a-french-ner-based-on-stanford-nlp-conditional-random-fields-model
Having both NER and RegexNER tags in StanfordCoreNLPServer output?,"<p>I am using the StanfordCoreNLPServer to extract some informations from text (such as surfaces, street names)</p>

<p>The street is given by a specifically trained NER model, and the surface by a simple regex via the RegexNER. </p>

<p>Each of them work fine separately but when used together, only the NER result is present in the output, under the <code>ner</code> tag. Why isn't there a <code>regexner</code>tag? Is there a way to also have the RegexNER result?</p>

<p>For information: </p>

<ul>
<li><p>StanfordCoreNLP v3.6.0</p></li>
<li><p>the URL used:</p>

<pre><code>'http://127.0.0.1:9000/'
'?properties={""annotators"":""tokenize,ssplit,pos,ner,regexner"", '
'""pos.model"":""edu/stanford/nlp/models/pos-tagger/french/french.tagger"",'
'""tokenize.language"":""fr"",'
'""ner.model"":""ner-model.ser.gz"", ' # custom NER model with STREET labels
'""regexner.mapping"":""rules.tsv"", ' # SURFACE label
'""outputFormat"": ""json""}'
</code></pre>

<p>as suggested <a href=""https://stackoverflow.com/questions/32642008/ner-interfere-with-regexner"">here</a>, the <code>regexner</code> annotator is <strong>after</strong> the <code>ner</code>, but still...</p></li>
<li><p>The current output (extract):</p>

<pre><code>{u'index': 4, u'word': u'dans', u'lemma': u'dans', u'pos': u'P', u'characterOffsetEnd': 12, u'characterOffsetBegin': 8, u'originalText': u'dans', u'ner': u'O'}
{u'index': 5, u'word': u'la', u'lemma': u'la', u'pos': u'DET', u'characterOffsetEnd': 15, u'characterOffsetBegin': 13, u'originalText': u'la', u'ner': u'O'}
{u'index': 6, u'word': u'rue', u'lemma': u'rue', u'pos': u'NC', u'characterOffsetEnd': 19, u'characterOffsetBegin': 16, u'originalText': u'rue', u'ner': u'STREET'}
{u'index': 7, u'word': u'du', u'lemma': u'du', u'pos': u'P', u'characterOffsetEnd': 22, u'characterOffsetBegin': 20, u'originalText': u'du', u'ner': u'STREET'}
[...]
{u'index': 43, u'word': u'165', u'lemma': u'165', u'normalizedNER': u'165.0', u'pos': u'DET', u'characterOffsetEnd': 196, u'characterOffsetBegin': 193, u'originalText': u'165', u'ner': u'NUMBER'}
{u'index': 44, u'word': u'm', u'lemma': u'm', u'pos': u'NC', u'characterOffsetEnd': 198, u'characterOffsetBegin': 197, u'originalText': u'm', u'ner': u'O'}
{u'index': 45, u'word': u'2', u'lemma': u'2', u'normalizedNER': u'2.0', u'pos': u'ADJ', u'characterOffsetEnd': 199, u'characterOffsetBegin': 198, u'originalText': u'2', u'ner': u'NUMBER'}
</code></pre></li>
<li><p>Expected output : I would like the last 3 items to be labelled with <code>SURFACE</code>, ie the <code>RegexNER</code> result.</p></li>
</ul>

<p>Let me know if more details are needed.</p>
","stanford-nlp, stanford-nlp-server","<p>Ok, things seem to work as I want if I put the <code>regexner</code> first: </p>

<pre><code>""annotators"":""regexner,tokenize,ssplit,pos,ner"",
</code></pre>

<p>seems there is an ordering problem at some stage of the process?</p>
",3,6,1388,2016-06-17 13:36:19,https://stackoverflow.com/questions/37883035/having-both-ner-and-regexner-tags-in-stanfordcorenlpserver-output
Stanford NLP: OutOfMemoryError,"<p>I am annotating and analyzing a series of text files.</p>

<p>The pipeline.annotate method becomes increasingly slow each time it reads a file. Eventually, I get an OutOfMemoryError.</p>

<p><strong>Pipeline is initialized ONCE:</strong></p>

<pre><code>protected void initializeNlp()
{
    Log.getLogger().debug(""Starting Stanford NLP"");


    // creates a StanfordCoreNLP object, with POS tagging, lemmatization,
    // NER, parsing, and
    Properties props = new Properties();

    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner, depparse,  natlog,  openie"");
    props.put(""regexner.mapping"", namedEntityPropertiesPath);

    pipeline = new StanfordCoreNLP(props);


    Log.getLogger().debug(""\n\n\nStarted Stanford NLP Successfully\n\n\n"");
}
</code></pre>

<p><strong>I then process each file using same instance of pipeline</strong> (as recommended elsewhere on SO and by Stanford).</p>

<pre><code>     public void processFile(Path file)
{
    try
    {
        Instant start = Instant.now();

        Annotation document = new Annotation(cleanString);
        Log.getLogger().info(""ANNOTATE"");
        pipeline.annotate(document);
        Long millis= Duration.between(start, Instant.now()).toMillis();
        Log.getLogger().info(""Annotation Duration in millis: ""+millis);

        AnalyzedFile af = AnalyzedFileFactory.getAnalyzedFile(AnalyzedFileFactory.GENERIC_JOB_POST, file);

        processSentences(af, document);

        Log.getLogger().info(""\n\n\nFile Processing Complete\n\n\n\n\n"");



        Long millis1= Duration.between(start, Instant.now()).toMillis();
        Log.getLogger().info(""Total Duration in millis: ""+millis1);

        allFiles.put(file.toUri().toString(), af);


    }
    catch (Exception e)
    {
        Log.getLogger().debug(e.getMessage(), e);
    }

}
</code></pre>

<p>To be clear, I expect the problem is with my configuration. However, I am certain that the stall and memory issues occur at the pipeline.annotate(file) method.</p>

<p>I dispose of all references to Stanford-NLP objects other than pipeline (e.g., CoreLabel) after processing each file. That is, I do not keep references to any Stanford objects in my code beyond the method level.</p>

<p>Any tips or guidance would be deeply appreciated</p>
","nlp, stanford-nlp","<p>OK, that last sentence of the question made me go double check. The answer is that I WAS keeping reference to CoreMap in one of my own classes. In other words, I was keeping in memory all the Trees, Tokens and other analyses for every sentence in my corpus.</p>

<p>In short, keep StanfordNLP CoreMaps for a given number of sentences and then dispose. </p>

<p>(I expect a hard core computational linguist would say there is rarely any need to keep a CoreMap once it has been analyzed, but I have to declare my neophyte status here)</p>
",0,1,188,2016-06-18 19:41:51,https://stackoverflow.com/questions/37900973/stanford-nlp-outofmemoryerror
How do I get started with a project on Text Summarization using NLP?,"<p>My final year engineering project requires me to build an application using Java or Python which summarizes a text document using Natural Language Processing. How do I even begin with the programming of such an application? </p>

<p>Based on some research, I've just noted down that extraction-based summarization will be the best bet for me since it isn't so complex as abstraction based algorithms. Even then, it'd be really helpful if someone would guide me in the right direction to go about this.  </p>
","nlp, stanford-nlp","<p>Text summarization is still an open problem in NLP.</p>

<p>I guess that you might start by asking yourself what is the purpose of the summary:</p>

<ul>
<li>A summary that discriminates a document from other documents</li>
<li>A summary that mines only the frequent patterns </li>
<li>A summary that covers all the topics in the document</li>
<li>etc</li>
</ul>

<p>Because this will influence the way you generate the summary.</p>

<p>But as a start you could use in python the NLTK framework to extract basic elements from a text.
For example you can extract the most frequent words, or the most frequent N-grams( N adjacent words) from the text.</p>

<p>Also a simple way to extract the most relevant sentences is using TF-IDF that stands for term frequency, Inverse document frequency. Basically this function gives higher scores to sentences that tend to appear frequently in one document compared to other document. </p>

<p>Some python libraries that you can use :</p>

<ul>
<li><a href=""http://scikit-learn.org/stable/"" rel=""noreferrer"">sickitlearn</a> that has more advanced features. </li>
<li>Also <a href=""http://rare-technologies.com/text-summarization-with-gensim/"" rel=""noreferrer"">gensim</a> library has a text summarization tutorial (also in python)</li>
<li>You can also use <a href=""https://dato.com/learn/userguide/text/analysis.html"" rel=""noreferrer"">Dato</a> that has as well a text analysis module. </li>
</ul>

<p>Some helpful resources:</p>

<ul>
<li>This book: <a href=""http://nlp.stanford.edu/fsnlp/"" rel=""noreferrer"">Foundations of Statistical Natural Language Processing</a></li>
<li>There is also a coursera course that you can enroll in, in order to understand the basics in text mining:
<a href=""https://www.coursera.org/learn/text-mining"" rel=""noreferrer"">https://www.coursera.org/learn/text-mining</a></li>
<li>Also this coursera course from stanford university (TF-IDF is explained in one of the videos)
<a href=""https://class.coursera.org/nlp/lecture/preview"" rel=""noreferrer"">https://class.coursera.org/nlp/lecture/preview</a></li>
</ul>

<p>Hope this helps.</p>
",11,7,7749,2016-06-21 08:34:01,https://stackoverflow.com/questions/37939341/how-do-i-get-started-with-a-project-on-text-summarization-using-nlp
Scala - spark-corenlp - java.lang.ClassNotFoundException,"<p>I want to run spark-coreNLP <a href=""https://github.com/databricks/spark-corenlp"" rel=""nofollow"">example</a>, but I get an  java.lang.ClassNotFoundException error when running spark-submit.</p>

<p>Here is the scala code, from the github example, which I put into an object, and defined a SparkContext.</p>

<p>analyzer.Sentiment.scala:</p>

<pre><code>package analyzer
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql.functions._
import com.databricks.spark.corenlp.functions._
import sqlContext.implicits._

object Sentiment {
  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName(""Sentiment"")
    val sc = new SparkContext(conf)

        val input = Seq(
                (1, ""&lt;xml&gt;Stanford University is located in California. It is a great university.&lt;/xml&gt;"")
                ).toDF(""id"", ""text"")

        val output = input
            .select(cleanxml('text).as('doc))
            .select(explode(ssplit('doc)).as('sen))
            .select('sen, tokenize('sen).as('words), ner('sen).as('nerTags), sentiment('sen).as('sentiment))

            output.show(truncate = false)
    }
}
</code></pre>

<p>I am using the build.sbt provided by spark-coreNLP - I only modified the scalaVersion and sparkVerison to my own.</p>

<pre><code>version := ""1.0""

scalaVersion := ""2.11.8""

initialize := {
  val _ = initialize.value
  val required = VersionNumber(""1.8"")
  val current = VersionNumber(sys.props(""java.specification.version""))
  assert(VersionNumber.Strict.isCompatible(current, required), s""Java $required required."")
}

sparkVersion := ""1.5.2""

// change the value below to change the directory where your zip artifact will be created
spDistDirectory := target.value

sparkComponents += ""mllib""

spName := ""databricks/spark-corenlp""

licenses := Seq(""GPL-3.0"" -&gt; url(""http://opensource.org/licenses/GPL-3.0""))

resolvers += Resolver.mavenLocal

libraryDependencies ++= Seq(
  ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.6.0"",
  ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.6.0"" classifier ""models"",
  ""com.google.protobuf"" % ""protobuf-java"" % ""2.6.1""
)
</code></pre>

<p>Then, I created my jar by running without issues.</p>

<pre><code>sbt package
</code></pre>

<p>Finally, I submit my job to Spark:</p>

<pre><code>spark-submit --class ""analyzer.Sentiment"" --master local[4] target/scala-2.11/sentimentanalizer_2.11-0.1-SNAPSHOT.jar 
</code></pre>

<p>But I get the following error:</p>

<pre><code>java.lang.ClassNotFoundException: analyzer.Sentiment
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.spark.util.Utils$.classForName(Utils.scala:173)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:641)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
</code></pre>

<p>My file Sentiment.scala is correclty located in a package named ""analyzer"".</p>

<pre><code>    $ find .
    ./src
    ./src/analyzer
    ./src/analyzer/Sentiment.scala
    ./src/com
    ./src/com/databricks
    ./src/com/databricks/spark
    ./src/com/databricks/spark/corenlp
    ./src/com/databricks/spark/corenlp/CoreNLP.scala
    ./src/com/databricks/spark/corenlp/functions.scala
    ./src/com/databricks/spark/corenlp/StanfordCoreNLPWrapper.scala
</code></pre>

<p>When I ran the SimpleApp example from the <a href=""http://spark.apache.org/docs/latest/quick-start.html#self-contained-applications"" rel=""nofollow"">Spark Quick Start</a> , I noticed that MySimpleProject/bin/ contained a SimpleApp.class. MySentimentProject/bin is empty. So I have tried to clean my project (I am using Eclipse for Scala). </p>

<p>I think it is because I need to generate Sentiment.class, but I don't know how to do it - It was done automatically with SimpleApp.scala, and when it ry to run/build with Eclipse Scala, it crashes.</p>
","scala, apache-spark, stanford-nlp","<p>Maybe You should try to add</p>

<pre><code>scalaSource in Compile := baseDirectory.value / ""src""
</code></pre>

<p>to your <code>build.sbt</code>, cause <a href=""http://www.scala-sbt.org/0.13/docs/Howto-Customizing-Paths.html"" rel=""nofollow"">sbt document</a> reads that ""the directory that contains the main Scala sources is by default <code>src/main/scala</code>"".</p>

<p>Or just make your source code in this structure</p>

<pre><code>$ find .
./src
./src/main
./src/main/scala
./src/main/scala/analyzer
./src/main/scala/analyzer/Sentiment.scala
./src/main/scala/com
./src/main/scala/com/databricks
./src/main/scala/com/databricks/spark
./src/main/scala/com/databricks/spark/corenlp
./src/main/scala/com/databricks/spark/corenlp/CoreNLP.scala
./src/main/scala/com/databricks/spark/corenlp/functions.scala
./src/main/scala/com/databricks/spark/corenlp/StanfordCoreNLPWrapper.scala
</code></pre>
",1,1,767,2016-06-22 21:59:53,https://stackoverflow.com/questions/37978991/scala-spark-corenlp-java-lang-classnotfoundexception
Ruby parsing tagged string from stanford core nlp into hash,"<p>I use Stanford CoreNLP to parse basic dependencies in (french or english) sentences. It gives me string results like this:</p>

<blockquote>
  <p>""(ROOT  (SENT    (NP (DET Ce) (NC magasin))    (VN (V est))    (AP (ADJ pourri))    (COORD (CC mais)      (Sint        (NP (DET les) (ADJ vendeuses))        (VN (V sont))        (AP (ADJ géniales))))    (PUNC !)))""</p>
</blockquote>

<p>Now I need this results to be parsed in ruby hash or struct, json or whatever easy to manipulate.</p>

<p>For example in a hash:</p>

<pre><code>{'ROOT'=&gt;{'SENT'=&gt;{'NP'=&gt;{'DET'=&gt;'Ce'},'NC'=&gt;'Magasin'}etc...}}
</code></pre>

<p>I tried several ways to do it but no success.
Could you give me advices or example to achieve this?</p>

<p>Thanks</p>
","ruby, json, parsing, hash, stanford-nlp","<p>Here's a really simple parser implementation. <code>tokenize</code> splits the string into its parts, <code>parse</code> groups the tokens into expressions and feeds them to <code>hasherize</code>, which turns them into a tree.</p>

<pre><code>def tokenize(str)
  str.split(/\s+|([()])/).reject(&amp;:empty?)
end

def hasherize(tokens)
  tokens, hsh = tokens.dup, {}

  while expr = tokens.shift
    _, key, val = expr

    if val
      hsh[key] = val
    elsif key
      tokens, hsh[key] = hasherize(tokens)
    else break
    end
  end

  [ tokens, hsh ]
end


def parse(str)
  tokens = tokenize(str)
    .slice_when {|left,right| left == "")"" || right == ""("" }
  hasherize(tokens.to_a)[1]
end

str = ""(ROOT (SENT (NP (DET Ce) (NC magasin)) (VN (V est)) (AP (ADJ pourri)) (COORD (CC mais) (Sint (NP (DET les) (ADJ vendeuses)) (VN (V sont)) (AP (ADJ géniales)))) (PUNC !)))""
p parse(str)
# =&gt; { ""ROOT"" =&gt; {
#        ""SENT"" =&gt; {
#          ""NP"" =&gt; { ""DET"" =&gt; ""Ce"", ""NC"" =&gt; ""magasin"" },
#          ""VN"" =&gt; { ""V"" =&gt; ""est"" },
#          ""AP"" =&gt; { ""ADJ"" =&gt; ""pourri"" },
#          ""COORD"" =&gt; {
#            ""CC"" =&gt; ""mais"",
#            ""Sint"" =&gt; {
#              ""NP"" =&gt; { ""DET"" =&gt; ""les"", ""ADJ"" =&gt; ""vendeuses"" },
#              ""VN"" =&gt; { ""V"" =&gt; ""sont"" },
#              ""AP"" =&gt; { ""ADJ"" =&gt; ""géniales"" }
#            }
#          },
#          ""PUNC"" =&gt; ""!""
#        }
#      }
#    }
</code></pre>
",0,0,50,2016-06-23 16:04:01,https://stackoverflow.com/questions/37996646/ruby-parsing-tagged-string-from-stanford-core-nlp-into-hash
Scala - Spark-corenlp - java.lang.NoClassDefFoundError,"<p>I want to run spark-coreNLP <a href=""https://github.com/databricks/spark-corenlp"" rel=""nofollow noreferrer"">example</a>, but I get an java.lang.NoClassDefFoundError error when running spark-submit.</p>

<p>Here is the scala code, from the github example, which I put into an object, and defined a SparkContext and SQLContext</p>

<p>main.scala.Sentiment.scala</p>

<pre><code>package main.scala


import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SQLContext

import com.databricks.spark.corenlp.functions._


object SQLContextSingleton {

  @transient  private var instance: SQLContext = _

  def getInstance(sparkContext: SparkContext): SQLContext = {
    if (instance == null) {
      instance = new SQLContext(sparkContext)
    }
    instance
  }
}


object Sentiment {
  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName(""Sentiment"")
    val sc = new SparkContext(conf)
    val sqlContext = SQLContextSingleton.getInstance(sc)
    import sqlContext.implicits._ 


    val input = Seq((1, ""&lt;xml&gt;Stanford University is located in California. It is a great university.&lt;/xml&gt;"")).toDF(""id"", ""text"")

    val output = input
      .select(cleanxml('text).as('doc))
      .select(explode(ssplit('doc)).as('sen))
      .select('sen, tokenize('sen).as('words), ner('sen).as('nerTags), sentiment('sen).as('sentiment))

    output.show(truncate = false)
  }
}
</code></pre>

<p>And my build.sbt (modified from <a href=""https://stackoverflow.com/questions/37978991/scala-spark-corenlp-java-lang-classnotfoundexception"">here</a>)</p>

<pre><code>version := ""1.0""

scalaVersion := ""2.10.6""

scalaSource in Compile := baseDirectory.value / ""src""

initialize := {
  val _ = initialize.value
  val required = VersionNumber(""1.8"")
  val current = VersionNumber(sys.props(""java.specification.version""))
  assert(VersionNumber.Strict.isCompatible(current, required), s""Java $required required."")
}

sparkVersion := ""1.5.2""

// change the value below to change the directory where your zip artifact will be created
spDistDirectory := target.value

sparkComponents += ""mllib""

// add any sparkPackageDependencies using sparkPackageDependencies.
// e.g. sparkPackageDependencies += ""databricks/spark-avro:0.1""
spName := ""databricks/spark-corenlp""

licenses := Seq(""GPL-3.0"" -&gt; url(""http://opensource.org/licenses/GPL-3.0""))

resolvers += Resolver.mavenLocal


libraryDependencies ++= Seq(
  ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.6.0"",
  ""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.6.0"" classifier ""models"",
  ""com.google.protobuf"" % ""protobuf-java"" % ""2.6.1""
)
</code></pre>

<p>I run <code>sbt package</code> without issue, then run Spark with</p>

<p><code>spark-submit --class ""main.scala.Sentiment"" --master local[4] target/scala-2.10/sentimentanalizer_2.10-1.0.jar</code> </p>

<p>The program fails after throwing an exception:</p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: edu/stanford/nlp/simple/Sentence
    at main.scala.com.databricks.spark.corenlp.functions$$anonfun$cleanxml$1.apply(functions.scala:55)
    at main.scala.com.databricks.spark.corenlp.functions$$anonfun$cleanxml$1.apply(functions.scala:54)
    at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:75)
    at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:74)
</code></pre>

<p>Things I tried:</p>

<p>I work with Eclipse for Scala, and I made sure to add all the jars from stanford-corenlp as suggested <a href=""http://stanfordnlp.github.io/CoreNLP/faq.html"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>./stanford-corenlp/ejml-0.23.jar
./stanford-corenlp/javax.json-api-1.0-sources.jar
./stanford-corenlp/javax.json.jar
./stanford-corenlp/joda-time-2.9-sources.jar
./stanford-corenlp/joda-time.jar
./stanford-corenlp/jollyday-0.4.7-sources.jar
./stanford-corenlp/jollyday.jar
./stanford-corenlp/protobuf.jar
./stanford-corenlp/slf4j-api.jar
./stanford-corenlp/slf4j-simple.jar
./stanford-corenlp/stanford-corenlp-3.6.0-javadoc.jar
./stanford-corenlp/stanford-corenlp-3.6.0-models.jar
./stanford-corenlp/stanford-corenlp-3.6.0-sources.jar
./stanford-corenlp/stanford-corenlp-3.6.0.jar
./stanford-corenlp/xom-1.2.10-src.jar
./stanford-corenlp/xom.jar
</code></pre>

<p>I suspect that I need to add something to my command line when submitting the job to Spark, any thoughts?</p>
","scala, apache-spark, stanford-nlp","<p>I was on the right track that my command line was missing something.</p>

<p>spark-submit needs to have all the stanford-corenlp added:</p>

<pre><code>    spark-submit 
--jars $(echo stanford-corenlp/*.jar | tr ' ' ',') 
--class ""main.scala.Sentiment"" 
--master local[4] target/scala-2.10/sentimentanalizer_2.10-1.0.jar 
</code></pre>
",2,1,1317,2016-06-23 17:30:51,https://stackoverflow.com/questions/37998228/scala-spark-corenlp-java-lang-noclassdeffounderror
Training NER model in stanford-nlp,"<p>I have been trying to play around with stanford Core NLP. I would wish to train the my own NER model. From the forums on SO and the official website describes to use a property file to do so. How would I do it via API?.</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, sentiment, regexner"");
props.setProperty(""regexner.mapping"", ""resources/customRegexNER.txt"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);      

String processedQuestion = ""Who is the prime minister of Australia?""

//Annotation annotation = pipeline.process(processedQuestion);
Annotation document = new Annotation(processedQuestion);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
for (CoreMap sentence : sentences) {

    // To get the tokens for the parsed sentence
    for (CoreMap tokens : sentence.get(TokensAnnotation.class)) {           
        String token = tokens.get(TextAnnotation.class);
        String POS = tokens.get(PartOfSpeechAnnotation.class);      
        String NER = tokens.get(NamedEntityTagAnnotation.class);            
        String Sentiment = tokens.get(SentimentClass.class);            
        String lemma = tokens.get(LemmaAnnotation.class);
</code></pre>

<ol>
<li>How &amp; Where do I add the Prop file? </li>
<li>N-gram tokenization (E.g. prime minister to be considered as a single token, later this token is passed for the POS, NER instead of two tokens being passed (prime and minister))?</li>
</ol>
","java, stanford-nlp, tokenize, named-entity-recognition","<p>I think it could work with that code :</p>

<pre><code>val props = new Properties()
  props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner"")
  props.put(""ner.model"", ""/your/path/ner-model.ser.gz"");
  val pipeline = new StanfordCoreNLP(props)
</code></pre>
",1,0,786,2016-06-24 12:08:02,https://stackoverflow.com/questions/38013103/training-ner-model-in-stanford-nlp
Classpath Error when training a model with Stanford NLP,"<p>I get a classpath error when I want to train my NER model :</p>

<blockquote>
  <p>Loading JAR-internal classifier
  /edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ...
  Exception in thread ""main"" java.lang.RuntimeException: Error loading
  classifier from jar file (most likely you are not running this code
  from a jar file or the named classifier is not stored in the jar file)</p>
</blockquote>

<p>I'm using this command line : </p>

<p>java -cp ""stanford-ner.jar:lib/*""  edu.stanford.nlp.ie.crf.CRFClassifier /Users/Desktop/austen.prop</p>

<p>I'm following the instructions from the Stanford NER FAQ <a href=""http://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/crf-faq.shtml</a></p>

<p>Someone know how to run correctly this command ?</p>
","nlp, stanford-nlp","<p>Ok, I fixed my problem, you need to use this command line :</p>

<blockquote>
  <p>java -cp ""stanford-ner.jar:lib/*"" -mx4g
  edu.stanford.nlp.ie.crf.CRFClassifier -prop
  /your/path/austen.txt</p>
</blockquote>

<p>You need to specify the absolute path to your property file to run your model.</p>

<p>For more details follow this good guide : <a href=""https://stackoverflow.com/questions/37852084/how-to-train-a-french-ner-based-on-stanford-nlp-conditional-random-fields-model/37918071#37918071"">how to train a french NER based on stanford-nlp Conditional Random Fields model?</a></p>
",0,0,687,2016-06-24 20:54:20,https://stackoverflow.com/questions/38021980/classpath-error-when-training-a-model-with-stanford-nlp
How do I generate an xml output from standfordner classifier?,"<p>I have used standfordNER classifier to classify text. 
Here is the code.</p>

<pre><code>string docText = fileContent;
        string txt = """";
        var classified = Classifier.classifyToCharacterOffsets(docText).toArray();

        for (int i = 0; i &lt; classified.Length; i++)
        {
            Triple triple = (Triple)classified[i];

            int second = Convert.ToInt32(triple.second().ToString());
            int third = Convert.ToInt32(triple.third().ToString());
            txt = txt + ('\t' + triple.first().ToString() + '\t' + docText.Substring(second, third - second));

            string s = Classifier.classifyWithInlineXML(txt);
            string s1 = Classifier.classifyToString(s, ""xml"", true);
            Panel1.GroupingText = s1;

        }


        Panel1.Visible = true;
</code></pre>

<p>and this is the out put:</p>

<pre><code>LOCATION    Lanka LOCATION  colombo ORGANIZATION microsoft
</code></pre>

<p>But i need an out put in xml format like this</p>

<pre><code>&lt;LOCATION&gt;  Lanka &lt;/LOCATION&gt;   &lt;LOCATION&gt;colombo&lt;/LOCATION&gt;    &lt;ORGANIZATION&gt; microsoft&lt;/ORGANIZATION&gt; 
</code></pre>

<p>In my code i have used ,</p>

<pre><code> string s = Classifier.classifyWithInlineXML(txt);
            string s1 = Classifier.classifyToString(s, ""xml"", true);
</code></pre>

<p>to get the xml ,but its not working. since i m new to this field please do a help for me to resolve this.
Thanks a lot</p>
","xml, stanford-nlp, named-entity-recognition, information-extraction","<p>This sample code should be helpful:</p>

<pre><code>   String content = ""..."";
   String classifierPath = ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"";
   AbstractSequenceClassifier&lt;CoreLabel&gt; asc  = CRFClassifier.getClassifierNoExceptions(classifierPath);
   String result = asc.classifyWithInlineXML(content);
</code></pre>
",1,0,153,2016-06-26 15:08:48,https://stackoverflow.com/questions/38039874/how-do-i-generate-an-xml-output-from-standfordner-classifier
Programmatically training NER Model using .prop file,"<p>I have been to train my ner model using a property file as shown in the tutorial here <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow noreferrer"">LINK</a>. I am using the same prop file but, when I fail to understand as to how to do it programmatically. </p>

<pre><code>props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, sentiment, regexner"");
props.setProperty(""ner.model"", ""resources/NER.prop"");
</code></pre>

<p>the prop file is as shown below :</p>

<pre><code># location of the training file
trainFile = nerTEST.tsv
# location where you would like to save (serialize) your
# classifier; adding .gz at the end automatically gzips the file,
# making it smaller, and faster to load
serializeTo = resources/ner-model.ser.gz

# structure of your training file; this tells the classifier that
# the word is in column 0 and the correct answer is in column 1
map = word=0,answer=1

# This specifies the order of the CRF: order 1 means that features
# apply at most to a class pair of previous class and current class
# or current class and next class.
maxLeft=1

# these are the features we'd like to train with
# some are discussed below, the rest can be
# understood by looking at NERFeatureFactory
useClassFeature=true
useWord=true
# word character ngrams will be included up to length 6 as prefixes
# and suffixes only
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
# the last 4 properties deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Error :</p>

<pre><code> java.io.StreamCorruptedException: invalid stream header: 23206C6F
....
..
Caused by: java.io.IOException: Couldn't load classifier from resources/NER.prop
</code></pre>

<p>From another question on <a href=""https://stackoverflow.com/questions/30601875/how-to-use-serialized-crfclassifier-with-stanfordcorenlp-prop-ner"">SO</a>, I understand you provide the model file directly. But, how can we do that with the help of a property file?</p>
","java, named-entity-recognition, stanford-nlp","<p>You should run this command from the command line:</p>

<pre><code>java -cp ""*"" edu.stanford.nlp.ie.crf.CRFClassifier -prop NER.prop
</code></pre>

<p>If you want to run this in Java code, you could do something like this:</p>

<pre><code>String[] args = new String[]{""-props"", ""NER.prop""};
CRFClassifier.main(args);
</code></pre>

<p>The .prop file is a file specifying the settings for training your model.  Your code is attempting to load the .prop file as a model itself, which is causing the error.</p>

<p>Doing either will generate the final model at resources/ner-model.ser.gz</p>
",2,4,831,2016-06-28 10:00:05,https://stackoverflow.com/questions/38073043/programmatically-training-ner-model-using-prop-file
Stanford Ner : build my own model or use RegexNer?,"<p>I would like some advices for Stanford NER, I'm wondering what it is the best way to detect new entities : </p>

<ul>
<li>Use RegexNer to detect new entities ?</li>
<li>Train my own NER model with new entities ?  </li>
</ul>

<p>Thank you in advance.</p>
",stanford-nlp,"<p>If you can easily generate a large list of the type of entity you want to tag, I would suggest using RegexNER.  For instance if you were trying to tag sports teams, it would probably be easier to just compile a large list of sports team names and directly match.  Building a large training set can take a lot of effort.</p>
",1,1,154,2016-06-28 15:06:56,https://stackoverflow.com/questions/38079899/stanford-ner-build-my-own-model-or-use-regexner
Parsing &quot;1.5 hours&quot; from Stanford Core NLP,"<p>Core NLP is parsing strings like:</p>

<blockquote>
  <p>1.5 hours</p>
</blockquote>

<p>as a one hour duration with the following code:</p>

<pre><code>def getPeriods(text: String): Seq[Period] = {
    parse(text).filter(timexAnn =&gt; {
        val timeExpr: TimeExpression = timexAnn.get(classOf[TimeExpression.Annotation])
        timeExpr.getValue.getType == duration
    }).map(timexAnn =&gt; {
        val timeExpr: TimeExpression = timexAnn.get(classOf[TimeExpression.Annotation])
        val period = timeExpr.getTemporal.getDuration.getJodaTimePeriod
        log.debug(""Parsed period: "" + TimeUtils.getHourMinutePeriodFormatter.print(period))
        period
    })
}
</code></pre>

<p>I am taking the first and only member of the resulting Seq[Period]. I've been playing around with the <a href=""http://nlp.stanford.edu:8080/sutime/process"" rel=""nofollow noreferrer"">online demo</a> and this behavior seems to be expected. Perhaps I have missed something? If not, is there a better alternative?</p>
","java, scala, nlp, stanford-nlp","<p>It appears Core NLP and SuTime do not parse decimal hours. I wrote a simple function in Scala to convert a string like ""1.5 hours"" into a string SuTime understand like ""1 hour and 30 minutes"". I then pass this string to the parser and everyone is happy.</p>

<pre><code>def getReadableDurationString(durationString: String): String = {
    val hoursAndMins = ""([0-9])(\\.[0-9]+) hour[s]?"".r
    val minsOnly = ""[0-9]?(\\.[0-9]+) hour[s]?"".r
    durationString match {
      case hoursAndMins(hours: String, mins: String) =&gt;
        s""${hours.toDouble} hours and ${Math.round(mins.toDouble * 60)} minutes""
      case minsOnly(mins: String) =&gt;
        s""${Math.round(mins.toDouble * 60)} minutes""
      case _ =&gt; durationString
    }
  }
}
</code></pre>
",0,0,119,2016-07-02 15:38:37,https://stackoverflow.com/questions/38161784/parsing-1-5-hours-from-stanford-core-nlp
Problems in setting up my own Stanford CoreNLP server:,"<p>I'm setting up my own stanford parser server following the tutorial in <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/corenlp-server.html</a>. However, when I paste the example code, it comes out an error:</p>

<pre><code>Exception in thread ""Thread-1"" edu.stanford.nlp.io.RuntimeIOException: Could not connect to server: localhost:9000
at edu.stanford.nlp.pipeline.StanfordCoreNLPClient$2.run(StanfordCoreNLPClient.java:393)
Caused by: java.net.ConnectException: Connection refused: connect
at java.net.DualStackPlainSocketImpl.connect0(Native Method)
at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
   at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
at java.net.Socket.connect(Socket.java:589)
at java.net.Socket.connect(Socket.java:538)
at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
at sun.net.www.http.HttpClient.New(HttpClient.java:308)
at sun.net.www.http.HttpClient.New(HttpClient.java:326)
at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1169)
at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1105)
at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:999)
at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:933)
at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1283)
at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1258)
at edu.stanford.nlp.pipeline.StanfordCoreNLPClient$2.run(StanfordCoreNLPClient.java:374)
</code></pre>

<p>Here is my code:</p>

<pre><code>import java.util.*;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLPClient;

public class tagger {
    public static void test(){
        // creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();

        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");

        StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""localhost"", 9000, 2);
        // read some text in the text variable
        String text = ""How are you today?"";
        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);
        // run all Annotators on this text
        pipeline.annotate(document);
    }

    public static void main(String[] args){
        test();
    }
}
</code></pre>
","java, stanford-nlp, stanford-nlp-server, corenlp-server","<p>The code you give works unaltered for me. (It doesn't print anything; you might try putting something like <code>System.out.println(document.toShorterString());</code> at the end of <code>test()</code> while testing it, but it works....)</p>

<p>Did you start the server and leave it running using the first instruction here: <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started</a> ? This just looks like the server isn't running on port 9000 of your local machine.</p>
",0,0,1284,2016-07-02 21:16:23,https://stackoverflow.com/questions/38164545/problems-in-setting-up-my-own-stanford-corenlp-server
Can not figure out how Stanford Dependencies works,"<p>My task is to parse text and find out the main characters in sentences. I need a Stanford Dependencies Parser, but i can't figure out, how and where can i get it. I downloaded CoreNLP as SD is a part of it. What should I do next? Didn't find any tutorials about how SDP works.
I will be very grateful if someone explains me, what i should do.
Thanks!</p>
","java, dependencies, nlp, stanford-nlp","<p>You can run CoreNLP with the following command to generate dependency parses for all the sentences in <code>INPUT.txt</code>. Make sure that you are running this from the CoreNLP directory or otherwise adjust the classpath (<code>-cp</code>)</p>

<pre><code>java -cp ""*"" -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP \
  -annotators ""tokenize,ssplit,pos,depparse"" -file INPUT.txt -outputFormat conllu
</code></pre>

<p>This will parse your sentences to <a href=""http://universaldependencies.org/"" rel=""nofollow"">English Universal Dependencies</a> (a newer dependency representation, based on Stanford Dependencies) and output them in <a href=""http://universaldependencies.org/format.html"" rel=""nofollow"">CoNLL-U format</a>. </p>

<p>If you want to parse the sentences to the old Stanford Dependencies representation, use the following command.</p>

<pre><code>java -cp ""*"" -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP \
  -annotators ""tokenize,ssplit,pos,depparse"" -file INPUT.txt -outputFormat conllu\
  -depparse.model edu/stanford/nlp/models/parser/nndep/PTB_Stanford_params.txt.gz
</code></pre>

<p>You can find more information on how to run CoreNLP on the <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">CoreNLP website.</a></p>
",0,0,194,2016-07-03 21:07:20,https://stackoverflow.com/questions/38174352/can-not-figure-out-how-stanford-dependencies-works
How to convert from Stanford Universal Dependencies to Phrase Grammar?,"<p>In my application I am using Stanford CoreNLP for parsing english text into a graph data structure (Universal Dependencies).</p>

<p>After some modifications of the graph I need to generate a natural language output for which I am using SimpleNLG: <a href=""https://github.com/simplenlg/simplenlg"" rel=""nofollow"">https://github.com/simplenlg/simplenlg</a></p>

<p>However SimpleNLG is using Phrase Grammar.</p>

<p>Therefore in order to successfully use SimpleNLG for natural language generation I need to convert from Universal Dependencies into Phrase Grammar.</p>

<p>What is the easiest way of achieving this?</p>

<p>So far I have only come across this article on this topic:
<a href=""http://delivery.acm.org/10.1145/1080000/1072147/p14-xia.pdf?ip=86.52.161.138&amp;id=1072147&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=642131329&amp;CFTOKEN=21335001&amp;__acm__=1468166339_844b802736ce07dab89064efb7f8ede9"" rel=""nofollow"">http://delivery.acm.org/10.1145/1080000/1072147/p14-xia.pdf?ip=86.52.161.138&amp;id=1072147&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=642131329&amp;CFTOKEN=21335001&amp;<strong>acm</strong>=1468166339_844b802736ce07dab89064efb7f8ede9</a></p>

<p>I am hoping that someone might have some more practical code examples to share on this issue?</p>
","parsing, nlp, stanford-nlp","<p>Phrase-structure trees contain more information than dependency trees and therefore you cannot deterministically convert dependency trees to phrase-structure trees.</p>

<p>But if you are using CoreNLP to parse the sentences, take a look at the <a href=""http://stanfordnlp.github.io/CoreNLP/parse.html"" rel=""nofollow""><code>parse</code></a> annotator. Unlike the dependency parser, this parser also outputs phrase-structure trees, so you can use this annotator to directly parse your sentences to phrase-structure trees. </p>
",0,1,794,2016-07-10 18:57:18,https://stackoverflow.com/questions/38295454/how-to-convert-from-stanford-universal-dependencies-to-phrase-grammar
Stanford ParserAnnotator doesn&#39;t generate annotations,"<p>I'm beginning to learn the Stanford CoreNLP Java API, and am trying to print the syntax tree of a sentence. The syntax tree is supposed to be generated by the ParserAnnotator. In my code (posted below), the ParserAnnotator runs without errors but doesn't generate anything. The error only shows up when the code tries to get the label of the tree's root node, and the tree is revealed to be null. The components that run before it generate their annotations without any problems.</p>

<p>There was one other person on SO who had a problem with the ParserAnnotator, but the issue was with memory. I've increased the memory that I allow Eclipse to use, but the behavior is the same. Running the code in the debugger also did not yield any errors.</p>

<p>Some background information: The sentence I used was ""This is a random sentence."" I recently upgraded from Windows 8.1 to Windows 10.</p>

<pre><code>public static void main(String[] args){
        String sentence = ""This is a random sentence."";
        Annotation doc = initStanford(sentence);
        Tree syntaxTree = doc.get(TreeAnnotation.class);
        printTreePreorder(syntaxTree);
    }
    private static Annotation initStanford(String sentence){
        StanfordCoreNLP pipeline = pipeline(""tokenize, ssplit, parse"");
        Annotation document = new Annotation(sentence);
        pipeline.annotate(document);
        return document;
    }
    private static StanfordCoreNLP pipeline(String components){
        Properties props = new Properties();
        props.put(""annotators"", components);
        return new StanfordCoreNLP(props);
    }
    public static void printTreePreorder(Tree tree){
        System.out.println(tree.label());
        for(int i = 0;i &lt; tree.numChildren();i++){
            printTreePreorder(tree.getChild(i));
        }
    }
</code></pre>
",stanford-nlp,"<p>You're trying to get the tree off of the document (<code>Annotation</code>), rather than the sentences (<code>CoreMap</code>). You can get the sentences with:</p>

<pre><code>Tree tree = doc.get(SentencesAnnotation.class).get(0).get(TreeAnnotation.class)
</code></pre>

<p>I can also shamelessly plug the <a href=""http://stanfordnlp.github.io/CoreNLP/simple.html"" rel=""nofollow"">Simple CoreNLP API</a>:</p>

<pre><code>Tree tree = new Sentence(""this is a sentence"").parse()
</code></pre>
",1,0,67,2016-07-11 19:49:37,https://stackoverflow.com/questions/38315051/stanford-parserannotator-doesnt-generate-annotations
Stanford SemanticGraph get sentence subject,"<p>How does one get the subject of a sentence (in a general way) using the SemanticGraph component from Stanford CoreNLP?</p>

<p>I've tried the code posted below, but the output indicates subject is null.</p>

<pre><code>String sentence = ""Carl has 84 Skittles."";
Annotation doc = InitUtil.initStanford(sentence, ""tokenize, ssplit, pos, lemma, ner, parse"");
SemanticGraph semGraph = doc.get(SENTENCE).get(0).get(DEPENDENCIES);
IndexedWord verb = semGraph.getFirstRoot();
IndexedWord subject = semGraph.getChildWithReln(verb, GrammaticalRelation.valueOf(""nsubj""));
System.out.println(subject);
</code></pre>

<p>If I try the same code replacing the second to last line with the 3 lines below, I get the expected output of ""Carl"". The difference appears to be a private field of <code>GrammaticalRelation</code> called <code>specific</code>, but the value of this field appears to be sentence-specific. My question is how to get the subject in a way that can be applied to all or nearly all sentences.</p>

<pre><code>Set&lt;GrammaticalRelation&gt; relations = semGraph.childRelns(verb);
GrammaticalRelation relation = relations.iterator().next();
IndexedWord subject = semGraph.getChildWithReln(verb, relation);
</code></pre>
",stanford-nlp,"<p>Turns out the problem wasn't with the <code>specific</code> field.</p>

<p><code>SemanticGraph.getChildWIthReln</code> relies on <code>GrammaticalRelation.equals()</code>, which checks if the languages of the two objects are compatible. <code>GrammaticalRelation.valueOf(String)</code> returns a <code>GrammaticalRelation</code> with language as <code>Language.English</code>, while the Stanford Parser uses <code>Language.UniversalEnglish</code>. The two languages are incompatible for some reason. Changing the call to <code>GrammaticalRelation.valueOf(String)</code> to <code>GrammaticalRelation.valueOf(Language, String)</code> solved the problem.</p>
",1,1,178,2016-07-16 13:40:32,https://stackoverflow.com/questions/38411800/stanford-semanticgraph-get-sentence-subject
Using ssplit options for CoreNLP,"<p>According to the documentation, I can use options such as ssplit.isOneSentence for parsing my document into sentences. How exactly do I do this though, given a StanfordCoreNLP object?</p>

<p>Here's my code -</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");
pipeline.annotate(document);
Annotation document = new Annotation(doc);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
</code></pre>

<p>At what point do I add this option and where?
Something like this?</p>

<pre><code>pipeline.ssplit.boundaryTokenRegex = '""' 
</code></pre>

<p>I'd also like to know how to use it for the specific option boundaryTokenRegex</p>

<p>EDIT:</p>

<p>I think this seems more appropriate -</p>

<pre><code>props.put(""ssplit.boundaryTokenRegex"", ""/"""");
</code></pre>

<p>But I still have to verify.</p>
","tokenize, stanford-nlp","<p>The way to do it for tokenizing sentences to end at any instance of a ' "" ' is this -</p>

<pre><code>props.setProperty(""ssplit.boundaryMultiTokenRegex"", ""/\'\'/"");
</code></pre>

<p>or </p>

<pre><code>props.setProperty(""ssplit.boundaryMultiTokenRegex"", ""/\""/"");
</code></pre>

<p>depending on how it is stored. (CoreNLP normalizes it as the former)</p>

<p>And if you want both starting and ending quotes -</p>

<pre><code>props.setProperty(""ssplit.boundaryMultiTokenRegex"",""\/'/'|``\"");
</code></pre>
",1,1,339,2016-07-16 16:51:12,https://stackoverflow.com/questions/38413426/using-ssplit-options-for-corenlp
Using ssplit options for CoreNLP,"<p>According to the documentation, I can use options such as ssplit.isOneSentence for parsing my document into sentences. How exactly do I do this though, given a StanfordCoreNLP object?</p>

<p>Here's my code -</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");
pipeline.annotate(document);
Annotation document = new Annotation(doc);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
</code></pre>

<p>At what point do I add this option and where?
Something like this?</p>

<pre><code>pipeline.ssplit.boundaryTokenRegex = '""' 
</code></pre>

<p>I'd also like to know how to use it for the specific option boundaryTokenRegex</p>

<p>EDIT:</p>

<p>I think this seems more appropriate -</p>

<pre><code>props.put(""ssplit.boundaryTokenRegex"", ""/"""");
</code></pre>

<p>But I still have to verify.</p>
","tokenize, stanford-nlp","<p>The way to do it for tokenizing sentences to end at any instance of a ' "" ' is this -</p>

<pre><code>props.setProperty(""ssplit.boundaryMultiTokenRegex"", ""/\'\'/"");
</code></pre>

<p>or </p>

<pre><code>props.setProperty(""ssplit.boundaryMultiTokenRegex"", ""/\""/"");
</code></pre>

<p>depending on how it is stored. (CoreNLP normalizes it as the former)</p>

<p>And if you want both starting and ending quotes -</p>

<pre><code>props.setProperty(""ssplit.boundaryMultiTokenRegex"",""\/'/'|``\"");
</code></pre>
",1,1,339,2016-07-16 16:51:12,https://stackoverflow.com/questions/38413426/using-ssplit-options-for-corenlp
slf4j issues in stanford core nlp and openccg,"<p>I'm following the <code>ccgbank-README</code> in openCCG when I have the following issue (just part of the log file):</p>

<pre><code>ner-tag-text:
     [echo] NER tagging into file ./data/novel/two-sents.dir/nertext
     [echo] Models: ./stanford-nlp/classifiers/english.all.3class.distsim.crf.ser.gz , ./stanford-nlp/classifiers/english.muc.7class.distsim.crf.ser.gz, ./stanford-nlp/classifiers/english.conll.4class.distsim.crf.ser.gz
     [java] java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
     [java]     at org.apache.tools.ant.taskdefs.ExecuteJava.execute(ExecuteJava.java:194)
     [java]     at org.apache.tools.ant.taskdefs.Java.run(Java.java:771)
     [java]     at org.apache.tools.ant.taskdefs.Java.executeJava(Java.java:221)
</code></pre>

<p>which says that there is a <code>NoClassDefFoundError</code> with <code>org/slf4j/LoggerFactory</code>. I searched the Internet and I found many people had the same problem. So I followed <a href=""https://stackoverflow.com/questions/9030065/classnotfoundexception-org-slf4j-loggerfactory"">this solution</a> by adding </p>

<pre><code>slf4j-api-1.7.2.jar
slf4j-simple-1.7.2.jar
</code></pre>

<p>to my classpath. After that, I have the classpath as follows:</p>

<pre><code>zhao@zhao-ubuntu:~$ echo $CLASSPATH
/home/zhao/slf4j-simple-1.7.21.jar:/home/zhao/slf4j-api-1.7.21.jar
</code></pre>

<p>However, this doesn't solve my problem. </p>
","java, linux, nlp, classpath, stanford-nlp","<p>Problem solved by using an version 1.3.4 of CoreNLP.</p>
",0,0,142,2016-07-18 21:23:30,https://stackoverflow.com/questions/38446231/slf4j-issues-in-stanford-core-nlp-and-openccg
CRF model making is taking too much time,"<p>I am following this <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow"">link</a> for making a <strong>CRF model</strong>. I am using following command for making model.</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre>

<p>Model is made successfully but my training data is very much and it is taking too much time. When I closely observe what is happening in the system. <strong>It is just using only one Core of my computer.</strong> <br></p>

<p>Can I run this command in a way that it should use many cores of my computer? It look like that it is implemented as a single thread. Is there is a support of multi-threading? If yes kindly share.</p>
","nlp, crf, stanford-nlp","<p>Make sure to download the latest version of the code (version 3.6.0).  It should run multi-threaded by default.</p>
",2,0,160,2016-07-19 12:28:11,https://stackoverflow.com/questions/38458606/crf-model-making-is-taking-too-much-time
Stanford CoreNLP: -nthreads flag causes all ner values to be O,"<p>I'm trying to figure out why adding the <code>-nthreads {int}</code> argument to the Stanford CoreNLP (version <code>stanford-corenlp-full-2015-12-09</code>) causes all tokens to have an output NER value of O (= not a named entity). </p>

<p>As a simple example, create a file called <code>sample-file.txt</code> and make its contents say <code>Samuel Huntington</code>. Then run:</p>

<pre><code>java -Xmx6g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -file sample-file.txt -outputFormat json
</code></pre>

<p>This will generate the expected output, with ""Samuel"" recognized as a person:</p>

<pre><code>{
  ""sentences"": [
    {
      ""index"": 0,
      ""parse"": ""SENTENCE_SKIPPED_OR_UNPARSABLE"",
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""Samuel"",
          ""originalText"": ""Samuel"",
          ""lemma"": ""Samuel"",
          ""characterOffsetBegin"": 0,
          ""characterOffsetEnd"": 6,
          ""pos"": ""NNP"",
          ""ner"": ""PERSON"",
          ""before"": """",
          ""after"": "" ""
        }, ...
</code></pre>

<p>If you add <code>-nthreads 8</code> to the command above, however, the output does not indicate Samuel is a person. Full command: </p>

<pre><code>java -Xmx6g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -file sample-file.txt -outputFormat json -nthreads 8 
</code></pre>

<p>Which generates:</p>

<pre><code>{
  ""sentences"": [
    {
      ""index"": 0,
      ""parse"": ""SENTENCE_SKIPPED_OR_UNPARSABLE"",
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""Samuel"",
          ""originalText"": ""Samuel"",
          ""lemma"": ""Samuel"",
          ""characterOffsetBegin"": 0,
          ""characterOffsetEnd"": 6,
          ""pos"": ""NNP"",
          ""ner"": ""O"",
          ""before"": """",
          ""after"": "" ""
        },
</code></pre>

<p>For what it's worth, <code>-nthread {int}</code> (that is, thread with no s) resolves the problem, so I can just use this command. I'll leave this question here in case others try using the -nthreads flag however.</p>

<p>P.S. Here's my CLASSPATH (obtained from <code>echo $CLASSPATH</code>), which contains only the Stanford CoreNLP distribution I downloaded last week: <code>/Users/dduhaime/Desktop/everett/wiki_facts/stanford-corenlp-full-2015-12-09/*:</code></p>
","java, stanford-nlp, corenlp-server","<p>Using <code>-nthread {int}</code> instead of <code>-nthreads {int}</code> generates the expected NER output.</p>
",0,0,215,2016-07-20 03:17:05,https://stackoverflow.com/questions/38471728/stanford-corenlp-nthreads-flag-causes-all-ner-values-to-be-o
How to print out to a file using Stanford Classifier,"<p>I am using <a href=""https://nlp.stanford.edu/software/classifier.shtml"" rel=""nofollow noreferrer"">Stanford Classifier</a> for my project.
This project takes training data to tune the algorithm then test data to classify text inputs into categories.
So the format for test and training data is tab-delimited text which means predictor <code>-TAB- input text</code>
The software prints out the output to <code>stdout</code> (command line).
Is there anyway to output to a text file ? 
I searched the <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html"" rel=""nofollow noreferrer"">javadoc</a> of the project site, and I found <a href=""https://i.sstatic.net/stVYu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/stVYu.png"" alt=""this one""></a>

But I don't know how to use this property.
I tried <code>-csvoutput=%1%n%c</code> on command line
But it gives me java null pointer exception error when I try to run it.</p>
","nlp, classification, stanford-nlp, supervised-learning","<p>If you want to save it to a file just add this to the end of your command:</p>

<pre><code>&gt; output_file.txt
</code></pre>
",0,-1,48,2016-07-21 14:10:53,https://stackoverflow.com/questions/38506579/how-to-print-out-to-a-file-using-stanford-classifier
how to Create Name Entity Recognition and evaluate its performance in terms of precision and recall?,"<p>I am working on aspects identification (explicit and implicit aspects) from a movie review dataset. Here, aspects could be actors, directors, production companies, music, plot, movie type etc. Despite searching a lot, I have come to know I need a NER (name entity recognition) for movie review dataset. Unfortunately, there is no NER available for my dataset. </p>

<p>My Questions are:</p>

<ol>
<li>how can I evaluate my ""Movie NER"" in terms of <em>precision</em>, <em>recall</em> and F1 <em>measure</em>? </li>
<li>what tool should I use for this purpose?</li>
</ol>
","machine-learning, nlp, weka, stanford-nlp, opennlp","<p><strong>Answers:</strong></p>

<ol>
<li><p>If you don't have gold data, first you will need to annotate and create your own gold data set. Then you can use this data for precision, recall, F1 measure calculations.</p></li>
<li><p>For NER purpose you can use Machine Learning based approach. I can suggest you to use <a href=""https://taku910.github.io/crfpp/"" rel=""nofollow"">CRF++</a>. You will need to define your own Tag Set and annotate data using IOB technique and use CRF++ for model training and testing purpose. You can use <a href=""http://www.cnts.ua.ac.be/conll2000/chunking/output.html"" rel=""nofollow"">conlleval.pl</a> to calculate accuracy of the algorithm in terms of precision and recall.</p></li>
</ol>
",3,1,367,2016-07-22 06:51:57,https://stackoverflow.com/questions/38519982/how-to-create-name-entity-recognition-and-evaluate-its-performance-in-terms-of-p
Different result in StanfordNERTagger in python3.5 - Stanford-ner-2015-12-09,"<p>I tried to run a sample sentence:</p>

<pre><code>from nltk.tag import StanfordNERTagger
_model_filename = r'D:/standford/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz'

_path_to_jar = r'D:/standford/stanford-ner-2015-12-09/stanford-ner.jar'

st = StanfordNERTagger(model_filename=_model_filename, path_to_jar=_path_to_jar)

st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) 
</code></pre>

<p>My output was as below in python:</p>

<blockquote>
  <p>[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying',
  'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook',
  'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY',
  'O')]</p>
</blockquote>

<p>while I was expected NY also selected as location based on this <a href=""https://stackoverflow.com/questions/23861355/how-to-install-and-invoke-stanford-nertagger"">reference</a>.</p>

<p>I tried another example as below:</p>

<pre><code>st.tag('Ali is living in London.'.split())
</code></pre>

<p>the result was as below which was correct.</p>

<blockquote>
  <p>[('Ali', 'PERSON'), ('is', 'O'), ('living', 'O'), ('in', 'O'),
  ('London.', 'LOCATION')]</p>
</blockquote>

<p>Do you have any idea why it didn't recognize NY as location in first sentence?</p>

<p>I am using visual studio 2015, Python 3.5, Stanford-ner-2015-12-09</p>
","python-3.x, nlp, nltk, stanford-nlp, named-entity-recognition","<p>Stanford NER tool is trained on properly formatted news text so punctuation is quite important. From the <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">docs</a>:</p>

<blockquote>
  <p>Stanford NER is a Java implementation of a Named Entity Recognizer.
  Named Entity Recognition (NER) labels sequences of words in a text
  which are the names of things, such as person and company names, or
  gene and protein names. It comes with well-engineered feature
  extractors for Named Entity Recognition, and many options for defining
  feature extractors. Included with the download are good named entity
  recognizers for English, particularly for the 3 classes (PERSON,
  ORGANIZATION, LOCATION), and we also make available on this page
  various other models for different languages and circumstances,
  including models trained on just the CoNLL 2003 English training data.</p>
</blockquote>

<p>From the <a href=""http://www.cnts.ua.ac.be/conll2003/ner/"" rel=""nofollow"">CoNLL 2003 doc</a>:</p>

<blockquote>
  <p>The English data is a collection of news wire articles from the
  Reuters Corpus. The annotation has been done by people of the
  University of Antwerp. Because of copyright reasons we only make
  available the annotations. In order to build the complete data sets
  you will need access to the Reuters Corpus. It can be obtained for
  research purposes without any charge from NIST.</p>
</blockquote>

<p>By adding the fullstop to the example sentence, you should get your desired output, but still no model is perfect =)</p>

<pre><code>alvas@ubi:~$ export STANFORDTOOLSDIR=$HOME
alvas@ubi:~$ export CLASSPATH=$STANFORDTOOLSDIR/stanford-ner-2015-12-09/stanford-ner.jar
alvas@ubi:~$ export STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-ner-2015-12-09/classifiers
alvas@ubi:~$ python3
Python 3.5.2 (default, Jul  5 2016, 12:43:10) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from nltk.tag import StanfordNERTagger
&gt;&gt;&gt; st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')
&gt;&gt;&gt; sent = 'Rami Eid is studying at Stony Brook University in NY .'.split()
&gt;&gt;&gt; st.tag(sent)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION'), ('.', 'O')]
&gt;&gt;&gt; sent = 'Rami Eid is studying at Stony Brook University in NY'.split()
&gt;&gt;&gt; st.tag(sent)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]
</code></pre>
",1,0,334,2016-07-22 08:42:59,https://stackoverflow.com/questions/38521971/different-result-in-stanfordnertagger-in-python3-5-stanford-ner-2015-12-09
"NER CRF, Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory","<p>I have downloaded the latest version for NER from this <a href=""http://nlp.stanford.edu/software/stanford-ner-2015-12-09.zip"" rel=""nofollow"">link</a>. Then after extracting it, I have run this command.</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre>

<p>This is not working and getting following exception.</p>

<pre><code>CRFClassifier invoked on Mon Jul 25 06:56:22 EDT 2016 with arguments:
   -prop austen.prop
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
    at edu.stanford.nlp.io.IOUtils.&lt;clinit&gt;(IOUtils.java:42)
    at edu.stanford.nlp.util.StringUtils.argsToProperties(StringUtils.java:942)
    at edu.stanford.nlp.util.StringUtils.argsToProperties(StringUtils.java:891)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2994)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 4 more
</code></pre>

<p>In the folder, <strong>stanford-ner-2015-12-09</strong> there is another folder <strong>lib</strong>, <strong>it already contains slf4j libraries but still it is not executing above command.</strong> I just downloaded and then extracted files and run that command to make a model but this exception is coming. I will be thankful to you if you can help me.</p>
","java, nlp, crf, stanford-nlp","<p>Can you try</p>

<p>UNIX:</p>

<pre><code>java -cp stanford-ner.jar:lib/*:. edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre>

<p>Windows:</p>

<pre><code>java -cp stanford-ner.jar;lib/*;. edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre>
",5,0,909,2016-07-25 11:03:14,https://stackoverflow.com/questions/38566041/ner-crf-exception-in-thread-main-java-lang-noclassdeffounderror-org-slf4j-lo
Using Stanford CoreNLP Python Parser for specific output,"<p>I'm using <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""nofollow"">SCP</a> to get the parse CFG tree for English sentences. </p>

<pre><code>from corenlp import *
corenlp = StanfordCoreNLP()
corenlp.parse(""Every cat loves a dog"")
</code></pre>

<p>My expected output is a tree like this: </p>

<pre><code>(S (NP (DET Every) (NN cat)) (VP (VT loves) (NP (DET a) (NN dog))))
</code></pre>

<p>But what i got is: </p>

<pre><code>(ROOT (S (NP (DT Every) (NN cat)) (VP (VBZ loves) (NP (DT a) (NN dog)))))
</code></pre>

<p>How to change the POS tag as expected and remove the ROOT node?</p>

<p>Thanks</p>
","python, nlp, pos-tagger, stanford-nlp","<p>You can use <a href=""http://www.nltk.org/howto/tree.html"" rel=""nofollow"">nltk.tree</a> module from <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a>.</p>

<pre><code>from nltk.tree import *

def traverse(t):
    try:
        # Replace Labels
        if t.label() == ""DT"":
            t.set_label(""DET"")
        elif t.label() == ""VBZ"":
            t.set_label(""VT"")   
    except AttributeError:
        return

    for child in t:
        traverse(child)

output_tree= ""(ROOT (S (NP (DT Every) (NN cat)) (VP (VBZ loves) (NP (DT a) (NN dog)))))""
tree = ParentedTree.fromstring(output_tree)

# Remove ROOT Element
if tree.label() == ""ROOT"":  
    tree = tree[0]

traverse(tree)
print tree  
# (S (NP (DET Every) (NN cat)) (VP (VT loves) (NP (DET a) (NN dog))))
</code></pre>
",1,0,700,2016-07-25 14:52:07,https://stackoverflow.com/questions/38571004/using-stanford-corenlp-python-parser-for-specific-output
How does StanfordNLP / CoreNLP handle ambigue sentence structures?,"<p>I am using Stanford CoreNLP to parse my sentences and it works surprisingly good. But I am wondering: Since CoreNLP contains a probabilistic parser, how does the software deal with ambiguities?</p>

<p>""I saw the girl with the glasses"".</p>

<p>(1) If I understand it the right way, CoreNLP prints the MOST probably tree. So there is no way to check, if there is a ambiguity, right? 
(2) Does that actually mean, that CoreNLP ignores syntactical ambiguities?</p>
",stanford-nlp,"<p>Yes, CoreNLP will pick one of the two interpretations and return that. Though, it's important to note that the ""most probable tree"" is the one that's <em>syntactically</em> most probable (i.e., most like trees its seen in the training data), rather than most probable based on any sort of real-world knowledge. Chances are, ""I ate the cake with a cherry"" and ""I ate the cake with a fork"" will have the same parse. </p>
",1,0,126,2016-07-26 09:17:09,https://stackoverflow.com/questions/38585791/how-does-stanfordnlp-corenlp-handle-ambigue-sentence-structures
Custom rule for SUTime is not working,"<p>I'm trying to annotate things like <code>in 10 minutes</code>, but for some reason I can't get it work.</p>

<p>I'm using a custom model file for my rules which is loaded after <em>defs.sutime.txt</em>, <em>english.sutime.txt</em> and <em>english.holidays.sutime.txt</em>.</p>

<p>My current rule is this:</p>

<pre><code>{ 
    ( /in/ (?$a [ { temporal::IS_TIMEX_DURATION } ] ) )
    =&gt;
    RelativeTime( $a[0].temporal.value )
}
</code></pre>
","stanford-nlp, sutime","<p>I found the answer:</p>

<pre><code>ENV.defaults[""ruleType""] = ""composite""

{
    ( /in/ (?$a [ { temporal::IS_TIMEX_DURATION } ] ) )
    =&gt;
    TemporalCompose( OFFSET, TIME_REF, $a[0].temporal )
}
</code></pre>
",1,0,212,2016-07-28 00:35:47,https://stackoverflow.com/questions/38625334/custom-rule-for-sutime-is-not-working
How to only remove stopwords when they are not nouns?,"<p>I'm using Solr 5 and need to remove stop words to prevent over-matching and avoid bloating the index with high IDF terms. However, the corpus includes a lot part numbers and name initials like ""Steve A"" and ""123-OR-A"". In those cases, I don't want ""A"" and ""OR"" to get removed by the stopword filter factory as they need to be searchable.</p>

<p>The <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">Stanford POS tagger</a> does a great job detecting that the above examples are nouns, not stop words, but is this the right approach for solving my problem?</p>

<p>Thanks!</p>
","solr, stanford-nlp, pos-tagger","<p>Only you can decide whether this is the right approach. If you can integrate POS tagger in and it gives you useful results - that's good.</p>

<p>But just to give you an alternative, you could look at duplicating your fields and processing them differently. For example, if you see <strong>123-OR-A</strong> being split and stopword-cleaned, that probably means you have <a href=""http://www.solr-start.com/info/analyzers/#WordDelimiterFilterFactory"" rel=""nofollow"">WordDelimiterFilterFactory</a> in your analyzer stack. That factory has a lot of parameters you could try tweaking. Or, you could <strong>copyField</strong> your content to another (<strong>store=false</strong>) field and process it without WordDelimiterFilterFactory all together. Then you search over both copies of your data, possibly with different boost for different fields. </p>
",0,0,137,2016-07-28 23:18:40,https://stackoverflow.com/questions/38648226/how-to-only-remove-stopwords-when-they-are-not-nouns
Stanford NLP Server:: Unknown annotator: sentiment,"<p>I just downloaded and ran Standfor NLP 3.6.0 <a href=""http://stanfordnlp.github.io/CoreNLP/index.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/index.html</a>
 by using the following command:</p>

<pre><code>java -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer --public --port 1062
</code></pre>

<p>I tested other annotators like POS, Tokenizer and they work just ok. However, when I try to run sentiment annotator[ <a href=""http://nlp.stanford.edu/sentiment/"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/</a> ] I get following errors:</p>

<pre><code>java.lang.IllegalArgumentException: Unknown annotator: sentiment
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.ensurePrerequisiteAnnotators(StanfordCoreNLP.java:281)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.getProperties(StanfordCoreNLPServer.java:476)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:350)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
    at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>I uploaded all the files I got from the download page[<a href=""http://stanfordnlp.github.io/CoreNLP/index.html#download]"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/index.html#download]</a> not sure what am I missing?</p>
","java, stanford-nlp, stanford-nlp-server","<p>For those who are experiencing the same issue, please download the project from <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">github</a> and not from their website. Make sure you have the <a href=""http://nlp.stanford.edu/software/stanford-corenlp-models-current.jar"" rel=""nofollow"">models</a> jar, <a href=""http://nlp.stanford.edu/software/stanford-english-corenlp-models-current.jar"" rel=""nofollow"">English</a> and ejml model.</p>

<p>That should do it.</p>
",0,0,241,2016-08-02 18:51:24,https://stackoverflow.com/questions/38728323/stanford-nlp-server-unknown-annotator-sentiment
SemgrexPattern lemma attribute doesn&#39;t seem to work,"<p>Here is a very simple example using <code>SemgrexPattern</code> from Stanford NLP.
I do not understand why it doesn't find any matches with <code>{lemma:/eat/}</code> while it finds a match with <code>{word:/eats/}</code>. I used the <code>LemmaAnnotation</code> class to get the lemma of the verb ""to eat"" and it's ""eat"".</p>

<p>Thank you for your help :)</p>

<pre><code>package Project;
import java.io.File;
import java.util.Scanner;

import edu.stanford.nlp.parser.lexparser.TreebankLangParserParams;
import edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphFactory;
import edu.stanford.nlp.semgraph.semgrex.SemgrexMatcher;
import edu.stanford.nlp.semgraph.semgrex.SemgrexPattern;
import edu.stanford.nlp.trees.GrammaticalStructure;
import edu.stanford.nlp.trees.GrammaticalStructureFactory;
import edu.stanford.nlp.trees.Tree;

public class SemgrexDemo {
  public static void main(String[] args) throws FileNotFoundException {
    String treeString = ""(ROOT (S (NP (NNP John)) (VP (VBZ eats) (NP (NN pizza))) (. .)))"";
    Tree tree = Tree.valueOf(treeString);
    SemanticGraph graph = SemanticGraphFactory.generateUncollapsedDependencies(tree);
    TreebankLangParserParams params = new EnglishTreebankParserParams();
    GrammaticalStructureFactory gsf = params.treebankLanguagePack().grammaticalStructureFactory(params.treebankLanguagePack().punctuationWordRejectFilter(), params.typedDependencyHeadFinder());
    GrammaticalStructure gs = gsf.newGrammaticalStructure(tree);
    System.err.println(graph);
    SemgrexPattern semgrex = SemgrexPattern.compile(""{}=A &lt;&lt;dobj=reln {lemma:/eat/}=B"");
    SemgrexMatcher matcher = semgrex.matcher(graph);
    while (matcher.find()) {
      System.err.println(matcher.getNode(""A"") + "" &lt;&lt;dobj "" + matcher.getNode(""B""));
   }
  }
}
</code></pre>
","java, nlp, pattern-matching, stanford-nlp","<p>The lemmata are not added automatically to the tokens when you parse a tree string to a Tree object, so the lemma attribute of all the nodes in the <code>SemanticGraph</code> is <code>null</code> and therefore <code>{lemma:/eat/}</code> doesn't match any node.</p>

<p>You can add the lemmata using the <code>lemma(String word, String pos)</code> method of the <code>Morphology</code> class:</p>

<pre><code>public static void main(String[] args) throws FileNotFoundException {
  String treeString = ""(ROOT (S (NP (NNP John)) (VP (VBZ eats) (NP (NN pizza))) (. .)))"";
  Tree tree = Tree.valueOf(treeString);
  SemanticGraph graph = SemanticGraphFactory.generateUncollapsedDependencies(tree);

  //add lemmata
  Morphology morphology = new Morphology();
  for (IndexedWord node : graph.vertexSet()) {
    String lemma = morphology.lemma(node.word(), node.tag());
    node.setLemma(lemma);
  }

  System.err.println(graph);
  SemgrexPattern semgrex = SemgrexPattern.compile(""{}=A &lt;&lt;dobj=reln {lemma:/eat/}=B"");
  SemgrexMatcher matcher = semgrex.matcher(graph);
  while (matcher.find()) {
    System.err.println(matcher.getNode(""A"") + "" &lt;&lt;dobj "" + matcher.getNode(""B""));
  }
}
</code></pre>
",1,1,369,2016-08-03 14:36:14,https://stackoverflow.com/questions/38746639/semgrexpattern-lemma-attribute-doesnt-seem-to-work
Stanford coreference possessive pronoun,"<p>Does Stanford's coreference resolution module support resolution for possessive pronouns? For example, consider the following sentences.</p>

<p>""Sandra has 5 candies. Sandra gave all of her candies to Susan.""</p>

<p>Would the coreference module be able to determine that ""her"" refers to ""Sandra""? If so, what are the API calls to do this?</p>
",stanford-nlp,"<p>In general, yes it does. It fails for that particular example. It would need some debugging as to why, but it seems to do with the more complex “all of …” noun phrase. If you instead try:</p>

<p>Sandra has 5 candies. Sandra gave her candies to Susan.</p>

<p>in the demo at:</p>

<p><a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/corenlp/process</a></p>

<p>you can see it working.</p>

<p>There’s nothing special about getting these mentions out in coref chains if they are recognized.  It’s just like regular use of Coref — See, for instance, <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/demo/StanfordCoreNlpDemo.java"" rel=""nofollow"">StanfordCoreNlpDemo.java</a>.</p>
",0,0,248,2016-08-04 00:36:04,https://stackoverflow.com/questions/38755744/stanford-coreference-possessive-pronoun
Unable to instantiate StanfordNERTagger on OS X,"<p>I am trying to instantiate <strong><code>StanfordNERTagger</code></strong>. This is what I am trying:</p>

<pre><code>st = StanfordNERTagger(""/Users/attitude/Desktop/english.all.3class.caseless.distsim.crf.ser.gz"",""/Users/attitude/Desktop/stanford-ner-2015-12-09/stanford-ner.jar"")
</code></pre>

<p>I have set the <code>CLASSPATH</code> variable to <code>/Users/attitude/Desktop/stanford-ner-2015-12-09/stanford-ner.jar</code> (I also tried just the parent folder as value - <code>/Users/attitude/Desktop/stanford-ner-2015-12-09</code>).</p>

<p>However, I am getting this error:</p>

<p><code>LookupError: Could not find stanford-ner.jar jar file at /Users/attitude/Desktop/stanford-ner-2015-12-09/stanford-ner.jar</code>.</p>

<p>I have done everything mentioned in these two answers - <a href=""https://stackoverflow.com/questions/32819573/nltk-why-does-nltk-not-recognize-the-classpath-variable-for-stanford-ner"">this</a> and <a href=""https://stackoverflow.com/questions/32652725/importerror-cannot-import-name-stanfordnertagger-in-nltk"">this</a>. What else do I do now to fix this error?</p>

<p>OS X Yosemite - Python 2.7.</p>
","python, nltk, osx-yosemite, stanford-nlp, pos-tagger","<p><strong>TL;DR</strong>:</p>

<p>Without setting environmental variable, use the keywords arguments, <code>model_filename</code> and <code>path_to_jar</code></p>

<pre><code>from nltk.tag import StanfordNERTagger

stanford_ner_dir = '/home/alvas/stanford-ner/'
eng_model_filename= stanford_ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'
my_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'

st = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) 
st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
</code></pre>

<hr>

<p><strong>In long:</strong> </p>

<p>See <a href=""https://stackoverflow.com/a/34112695/610569"">https://stackoverflow.com/a/34112695/610569</a></p>
",2,0,461,2016-08-07 23:05:19,https://stackoverflow.com/questions/38819371/unable-to-instantiate-stanfordnertagger-on-os-x
Bullets in document getting as a question mark in GATE NLP,"<p>I am new to <code>GATE NLP</code>. I have a document, which contains bullets. When I load it into <code>GATE</code>. Bullets are detected as an unknown type symbol which is printed as <code></code> . I also tried to set the encoding to <code>UTF-8</code>. And I also tryed to load the document programmatically, then bullets gets detected as a  <code>?</code> . </p>

<p>Can anyone explain me this?</p>

<p>Example:</p>

<p><code> Promoted to Senior Member Technical in 2.5 years of experience.</code></p>

<p>Here is the symbol which is in the <code>GATE DEVELOPER UI</code> and the <code>?</code> symbol is shown when I did it ""programmatically"". </p>
","java, encoding, nlp, stanford-nlp, gate","<p>In my experience, <code>doc</code> and <code>docx</code> files usually do not produce <code></code> characters. Bullets are either missing (text formatted as bullet-list) or printed as <code>•</code> (text with raw bullet characters).</p>

<p>See also this related question: <a href=""https://stackoverflow.com/q/33255580/1857897"">Parsing either font style or block of paragraph in GATE</a></p>

<p><code>Pdf</code> files often produce ""<code></code>-bullet characters"" in a GATE document. It may be related to some <em>pdf</em> or <em>Apache PDFBox</em> issues, see e.g. <a href=""https://issues.apache.org/jira/browse/PDFBOX-1713"" rel=""nofollow noreferrer"" title=""[PDFBOX-1713] [PATCH] Bullet character not rendered - ASF JIRA"">this one</a>.</p>

<p>These characters also have a unicode value. In XML, they are encoded for example as <code>&amp;#xf0b7;</code>. In this case, my advice is to trace such characters (they may have different unicode values depending on the original bullet character) and replace them by something printable (e.g. <code>•</code>).</p>

<p>Concerning the <code>?</code> characters: I it is probably caused by your java environment which doesn't support these characters. See e.g.: <a href=""https://stackoverflow.com/q/18856056/1857897"">Why Some Unicode Characters appears to be question mark in the console?</a></p>
",0,1,776,2016-08-08 10:35:41,https://stackoverflow.com/questions/38827011/bullets-in-document-getting-as-a-question-mark-in-gate-nlp
subprocess.Popen works outside but not inside ipyparallel?,"<p>I'm trying to parallelize some code from <a href=""https://github.com/reedcoke/bigDataCamp2016/blob/master/sentiment.py"" rel=""nofollow"">here</a> using <code>ipyparallel</code>. In short, I can make functions that work fine outside of <code>apply_sync()</code>, but I can't seem to get them to work inside it (I swear I had this working earlier, but I can't find a version of the code that isn't broken). A simple example:</p>

<pre><code>def test3(fname = '7_1197_.txt'):
    import subprocess
    command = 'touch data/sentiment/' + fname + '.test'
    child = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
    while True:
        out = child.stdout.read(1)
        if out == '' and child.poll() != None:
            return 
test3() #this works, creates a file with the .test extention
results = view.map_sync(test3, speeches) #this doesn't work. No files created.
</code></pre>

<p>Here's a short version of the function I'm actually going to use. It works fine on its own. In <code>apply_sync()</code> it spins up <code>java</code> processes according to <code>htop</code>, but it doesn't seem to get anything back from those processes. </p>

<pre><code>def test2(fname = '7_1197_.txt'):
    import subprocess

    settings = ' -mx5g edu.stanford.nlp.sentiment.SentimentPipeline'
    inputFile = ' -file data/sentiment/' + fname
    command = 'java ' + settings + inputFile
    child = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
    results = []
    while True:
        out = child.stdout.read(1)
        if out == '' and child.poll() != None:
            return ''.join(results)
        if out != '':
            results.extend(out)
test2() #Works fine, produces output
results = view.map_sync(test2, speeches) #Doesn't work: the results are empty strings.
</code></pre>

<p>I tried a version where I return the command variable. The commands sent to <code>Popen</code> are fine, and they work when pasted manually in the command line. I thought maybe it was just an issue with piping, but changing the command to redirect the output to files with <code>' &gt; '+fname+'.out'</code> doesn't work inside the <code>apply_sync()</code> call either (no output files are produced). </p>

<p>How should I be doing this so I get the <code>stdout</code> from the system calls back?</p>
","python-2.7, subprocess, stanford-nlp, ipython-parallel","<p>I see two potential gotchas. One for the blocking, one for the missing files. For the missing files, you should make sure that your engines and your local session are in the same working directory, or make sure to use absolute paths. A quick way to synchronize paths locally and remotely:</p>

<pre><code>client[:].apply_sync(os.chdir, os.getcwd())
</code></pre>

<p>That says: get the <em>local</em> cwd, then call <code>os.chdir</code> everywhere, so that we all share the same working directory. A quick shortcut for this if you are in an IPython session is:</p>

<pre><code>%px cd {os.getcwd()}
</code></pre>

<p>As for the blocking, my first thought is: are you perhaps using Python 3 when running in parallel? If so, <code>child.stdout.read</code> returns <em>bytes</em> not <em>text</em>. In Python 2, <code>str is bytes</code>, so <code>out == ''</code> will work, but in Python 3, the condition <code>out == ''</code> will never be true because <code>b'' != u''</code>, and your function will never return.</p>

<p>Some more useful bits of info:</p>

<ol>
<li><code>stdout.read(N)</code> will read <em>up to</em> that number of bytes, and truncate if the output is complete. This is useful because <code>read(1)</code> will loop <em>many</em> times, even if the output is all waiting to be read.</li>
<li><code>stdout.read()</code> will only return an empty bytestring if output is finished, so you only need to check that, not <code>child.poll()</code> before returning. (this is true as long as you haven't set NOWAIT on the FD, which is some advanced usage).</li>
<li>if you want to see partial output before the function returns, you can redisplay output on sys.stdout, and see the partial outputs in IPython without waiting for the final result.</li>
</ol>

<p>So here are a couple of implementations of your function, with different goals.</p>

<p>The first one appears to accomplish your current goal using <a href=""https://docs.python.org/3/library/subprocess.html#subprocess.Popen.communicate"" rel=""nofollow"">Popen.communicate</a>, which is the simplest choice if you don't actually want to do anything with partial output and/or have nothing to do in the function wile you are waiting for output:</p>

<pre><code>def simple(fname = '7_1197_.txt'):
    import subprocess
    command = 'echo ""{0}"" &amp;&amp; touch -v data/sentiment/{0}.test'.format(fname)
    child = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
    # if we aren't doing anything with partial outputs,
    # child.communicate() does all of our waiting/capturing for us:
    out, err = child.communicate()
    return out
</code></pre>

<p>(it might be useful to include stderr capturing as well, with <code>stderr=subprocess.PIPE</code> or merge stderr into stdout with <code>stderr=subprocess.STDOUT</code>).</p>

<p>Here's another example, collecting stderr into stdout, and reading in chunks:</p>

<pre><code>def chunked(fname = '7_1197_.txt'):
    import subprocess
    command = 'echo ""{0}"" &amp;&amp; touch data/sentiment/{0}.test'.format(fname)
    child = subprocess.Popen(command, shell=True,
                             stdout=subprocess.PIPE,
                             stderr=subprocess.STDOUT,
                            )
    chunks = []
    while True:
        chunk = child.stdout.read(80) # read roughly one line at a time
        if chunk:
            chunks.append(chunk)
            continue
        else:
            # read will only return an empty bytestring when output is finished
            break
    return b''.join(chunks)
</code></pre>

<p>Note that we can use the <code>if not chunk</code> condition to determine when output is finished, rather than <code>if chunk == ''</code>, since empty bytestrings are Falsy. If we aren't doing something with the partial output, there's really no reason to use this instead of the simpler <code>.communicate()</code> version above.</p>

<p>Finally, here's a version you can use with IPython that, instead of capturing and returning the output, redisplays it, which we can use to display <em>partial</em> output in the client:</p>

<pre><code>def chunked_redisplayed(fname = '7_1197_.txt'):
    import sys, subprocess
    command = 'for i in {{1..20}}; do echo ""{0}""; sleep 0.25; done'.format(fname)
    child = subprocess.Popen(command, shell=True,
                             stdout=subprocess.PIPE,
                             stderr=subprocess.STDOUT,
                            )
    while True:
        chunk = child.stdout.read(80) # read roughly one line at a time
        if chunk:
            sys.stdout.write(chunk.decode('utf8', 'replace'))
            continue
        else:
            # read will only return an empty bytestring when output is finished
            break
</code></pre>

<p>In the client, if you use <code>map_async</code> instead of <code>map_sync</code>, you can check on <code>result.stdout</code>, which is a list of the stdout-streams <em>so far</em>, so you can check on the progress:</p>

<pre><code>amr = view.map_async(chunked_redisplayed, speeches)
amr.stdout # list of stdout text, updated in the background as output is produced
amr.wait_interactive() # waits and shows progress
amr.get() # waits for and returns the actual result
</code></pre>
",1,0,278,2016-08-08 22:59:51,https://stackoverflow.com/questions/38839728/subprocess-popen-works-outside-but-not-inside-ipyparallel
What is difference between Core NLP and Stanford NLP?,"<p>Can anybody let me know the difference between Core NLP</p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/</a></p>

<p>and Stanford NLP</p>

<p><a href=""http://nlp.stanford.edu/"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/</a></p>
",stanford-nlp,"<p>As per my knowledge, the group at Stanford University works on different NLP mechanisms, out of which CoreNLP is the major one. Thus core NLP is nothing but a product outcome of Stanford NLP team research &amp; contributions.</p>
",3,2,1674,2016-08-09 16:22:54,https://stackoverflow.com/questions/38855943/what-is-difference-between-core-nlp-and-stanford-nlp
How to set whitespace tokenizer on NER Model?,"<p>i am creating a custom NER model using CoreNLP 3.6.0</p>

<p>My props are:</p>

<pre><code># location of the training file 
trainFile = /home/damiano/stanford-ner.tsv 
# location where you would like to save (serialize) your 
# classifier; adding .gz at the end automatically gzips the file, 
# making it smaller, and faster to load 
serializeTo = ner-model.ser.gz

# structure of your training file; this tells the classifier that 
# the word is in column 0 and the correct answer is in column 1 
map = word=0,answer=1

# This specifies the order of the CRF: order 1 means that features 
# apply at most to a class pair of previous class and current class 
# or current class and next class. 
maxLeft=1

# these are the features we'd like to train with 
# some are discussed below, the rest can be 
# understood by looking at NERFeatureFactory 
useClassFeature=true 
useWord=true 
# word character ngrams will be included up to length 6 as prefixes 
# and suffixes only  
useNGrams=true 
noMidNGrams=true 
maxNGramLeng=6 
usePrev=true 
useNext=true 
useDisjunctive=true 
useSequences=true 
usePrevSequences=true 
# the last 4 properties deal with word shape features 
useTypeSeqs=true 
useTypeSeqs2=true 
useTypeySequences=true 
wordShape=chris2useLC
</code></pre>

<p>I build with this command:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier  -prop /home/damiano/stanford-ner.prop
</code></pre>

<p>The problem is when i use this model to retrieve the entities inside a textfile. The command is:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier ner-model.ser.gz -textFile file.txt
</code></pre>

<p>Where <strong>file.txt</strong> is:</p>

<pre><code>Hello!
my
name
is
John.
</code></pre>

<p>The output is:</p>

<p>Hello/O !/O 
my/O name/O is/O John/PERSON ./O </p>

<p>As you can see it split ""Hello!"" into two tokens. Same thing for ""John.""</p>

<p>I must use whitespace tokenizer. </p>

<p>How can i set it? </p>

<p>why does CoreNlp is splitting those words in two tokens?</p>
","machine-learning, nlp, stanford-nlp","<p>You set your own tokenizer by specifying the classname to the <code>tokenizerFactory</code> flag/property:</p>

<p><code>tokenizerFactory = edu.stanford.nlp.process.WhitespaceTokenizer$WhitespaceTokenizerFactory</code></p>

<p>You can specify any class that implements <code>Tokenizer&lt;T&gt;</code> interface, but the included <code>WhitespaceTokenizer</code> sounds like what you want. If the tokenizer has options you can specify them with <code>tokenizerOptions</code> For instance, here, if you also specify:</p>

<p><code>tokenizerOptions = tokenizeNLs=true</code></p>

<p>then the newlines in your input will be preserved in the input (for output options that don't convert things always into a one-token-per-line format).</p>

<p>Note: Options like <code>tokenize.whitespace=true</code> apply at the level of CoreNLP. They aren't interpreted (you get a warning saying that the option is ignored) if provided to individual components like CRFClassifier.</p>

<p>As Nikita Astrakhantsev notes, this isn't necessarily a good thing to do. Doing it at test time would only be correct if your training data is also whitespace separated, but otherwise will adversely affect performance. And having tokens like the ones you get from whitespace separation are bad for doing subsequent NLP processing such as parsing.</p>
",4,2,797,2016-08-12 23:07:54,https://stackoverflow.com/questions/38927568/how-to-set-whitespace-tokenizer-on-ner-model
Entities on my gazette are not recognized,"<p>I would like to create a custom NER model. That's what i did:</p>

<p><strong>TRAINING DATA</strong> (stanford-ner.tsv):</p>

<pre><code>Hello    O
!    O
My    O
name    O
is    O
Damiano    PERSON
.    O
</code></pre>

<p><strong>PROPERTIES</strong> (stanford-ner.prop):</p>

<pre><code>trainFile = stanford-ner.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
maxLeft=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useGazettes=true
gazette=gazzetta.txt
cleanGazette=true
</code></pre>

<p><strong>GAZZETTE</strong> gazzetta.txt):</p>

<pre><code>PERSON John
PERSON Andrea
</code></pre>

<p>I build the model via command line with:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier  -prop stanford-ner.prop
</code></pre>

<p>And test with:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier  -loadClassifier ner-model.ser.gz -textFile test.txt
</code></pre>

<p>I did two tests with the following texts:</p>

<p><strong>>>> TEST 1 &lt;&lt;&lt;</strong></p>

<ul>
<li><p>TEXT:
Hello! My name is Damiano and this is a fake text to test.</p></li>
<li><p>OUTPUT
<em>Hello/O !/O
My/O name/O is/O Damiano/PERSON and/O this/O is/O a/O fake/O text/O to/O test/O ./O</em></p></li>
</ul>

<p><strong>>>> TEST 2 &lt;&lt;&lt;</strong></p>

<ul>
<li><p>TEXT:
Hello! My name is John and this is a fake text to test.</p></li>
<li><p>OUTPUT
<em>Hello/O !/O
My/O name/O is/O John/O and/O this/O is/O a/O fake/O text/O to/O test/O ./O</em></p></li>
</ul>

<p>As you can see only ""Damiano"" entity is found. This entity is in my training data but ""John"" (second test) is inside the gazzette. So the question is.</p>

<p>Why does John entity is not recognized ?</p>

<p>Thank you so much in advance.</p>
","machine-learning, nlp, stanford-nlp, named-entity-recognition","<p>As <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#gazette"" rel=""nofollow"">Stanford FAQ</a> says, </p>

<blockquote>
  <p>If a gazette is used, this does not guarantee that words in the
  gazette are always used as a member of the intended class, and it does
  not guarantee that words outside the gazette will not be chosen. It
  simply provides another feature for the CRF to train against. If the
  CRF has higher weights for other features, the gazette features may be
  overwhelmed.</p>
  
  <p>If you want something that will recognize text as a member of a class
  if and only if it is in a list of words, you might prefer either the
  regexner or the tokensregex tools included in Stanford CoreNLP. The
  CRF NER is not guaranteed to accept all words in the gazette as part
  of the expected class, and it may also accept words outside the
  gazette as part of the class.</p>
</blockquote>

<p>Btw, it is not a good practice to test machine learning pipelines in a 'unit-test'-way, i.e. with only one or two examples, because it is supposed to work on much greater volume of data and, more importantly, it is probabilistic by nature.</p>

<p>If you want to check if your gazette file is actually used, it may be better to take existent examples (see the bottom of the page linked above for <code>austen.gaz.prop</code> and <code>austen.gaz.txt</code> examples) and replace multiple names by your own ones, then check. If it fails, firstly try to change your test, e.g. add more names, reformulate text and so on.</p>
",3,5,920,2016-08-13 11:36:59,https://stackoverflow.com/questions/38932299/entities-on-my-gazette-are-not-recognized
R: coreNLP - cannot plot getdependency,"<p>I have a short text data:</p>

<p>""I always prefer old-school guy. I have a PhD degree in science. I am really not interested in finding someone with the same background, otherwise life is gonna be boring.""</p>

<p>I have used the following codes to get the dependence structure of the text above.</p>

<pre><code>dep &lt;- getDependency(anno)
</code></pre>

<p>but when I tried to plot the sentence structure as suggested in book - Humanities Data in R: </p>

<pre><code>plot(anno,3)
</code></pre>

<p>It should give me the sturcture of the third sentence, but I get is an error message:</p>

<pre><code> Error in xy.coords(x, y, xlabel, ylabel, log) : 
'x' and 'y' lengths differ
</code></pre>

<p>I am wondering how can this be properly plotted and get a plot like the following.</p>

<p><a href=""https://i.sstatic.net/q0NN4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q0NN4.jpg"" alt=""enter image description here""></a></p>
","r, nlp, stanford-nlp, stanford-nlp-server","<p>My guess is that you have not loaded the plot function for this kind of object.  It appears that the message is coming from the default plot function.</p>
",0,2,173,2016-08-22 15:00:50,https://stackoverflow.com/questions/39082853/r-corenlp-cannot-plot-getdependency
R: Stanford CoreNLP returnning NAs for getSentiment,"<p>I have the following text data:</p>

<blockquote>
  <p>I always prefer old-school guy. I have a PhD degree in science. I am
  really not interested in finding someone with the same background,
  otherwise life is gonna be boring.</p>
</blockquote>

<p>And I am trying to extract out the sentiment scores of the above text, but what i get is all NAs. </p>

<pre><code> dating3 = annotateString(bio)
 bio.emo = getSentiment(dating3)

   id sentimentValue sentiment
1   1             NA        NA
2   2             NA        NA
3   3             NA        NA
</code></pre>

<p>I do not know why is occuring and googled around but did not find any relevant answers. In the meantime, when i tried the sample data provided within coreNLP package  </p>

<pre><code> getSentiment(annoHp)

   id sentimentValue    sentiment
1  1              4 Verypositive
</code></pre>

<p>It gives me an answer, so I don't know why this is happening. Would greatly appreciate if anyone can offer some insight.</p>
","r, stanford-nlp, stanford-nlp-server, corenlp-server","<p>Hopefully by now you have already found this but for you and anyone else, this is a known bug which is fixed on the GitHub version, see here: <a href=""https://github.com/statsmaths/coreNLP/issues/9"" rel=""nofollow noreferrer"">https://github.com/statsmaths/coreNLP/issues/9</a></p>
",1,1,719,2016-08-23 15:13:40,https://stackoverflow.com/questions/39104937/r-stanford-corenlp-returnning-nas-for-getsentiment
NLTK can&#39;t find the Stanford POS tagger model file,"<p>I am trying to use StanfordPOSTagger from the NLTK. I  downloaded Stanford POS full tagger. I have set </p>

<pre><code>CLASSPATH=/home/waheeb/Stanford_Tools/stanford-postagger-full-2015-12-09  /stanford-postagger.jar
STANFORD_MODELS=home/waheeb/Stanford_Tools/stanford-postagger-full-2015-12-09/models
</code></pre>

<p>When I type the following in python:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag import StanfordPOSTagger
&gt;&gt;&gt; st = StanfordPOSTagger('english-bidirectional-distsim.tagger')
</code></pre>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/home/waheeb/anaconda2/lib/python2.7/site-packages/nltk/tag /stanford.py"", line 136, in __init__
super(StanfordPOSTagger, self).__init__(*args, **kwargs)
File ""/home/waheeb/anaconda2/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 56, in __init__
env_vars=('STANFORD_MODELS',), verbose=verbose)
File ""/home/waheeb/anaconda2/lib/python2.7/site-packages /nltk/internals.py"", line 573, in find_file
file_names, url, verbose))
File ""/home/waheeb/anaconda2/lib/python2.7/site-packages/nltk/internals.py"", line 567, in find_file_iter
raise LookupError('\n\n%s\n%s\n%s' % (div, msg, div))
</code></pre>

<p>LookupError: </p>

<pre><code>=========================================================================
NLTK was unable to find the english-bidirectional-distsim.tagger file!
Use software specific configuration paramaters or set the TANFORD_MODELS  environment variable.
==========================================================================
</code></pre>

<p>Why is that?? </p>
","python, nlp, nltk, stanford-nlp, pos-tagger","<p>You forgot to use <code>export</code> in the command line before calling your python script. I.e. </p>

<pre><code>alvas@ubi:~$ export STANFORDTOOLSDIR=$HOME
alvas@ubi:~$ export CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-12-09/stanford-postagger.jar
alvas@ubi:~$ export STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-12-09/models
alvas@ubi:~$ python
</code></pre>

<p>For more details see <a href=""https://gist.github.com/alvations/e1df0ba227e542955a8a"" rel=""nofollow noreferrer"">https://gist.github.com/alvations/e1df0ba227e542955a8a</a></p>

<hr>

<p>Similar problems includes:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/34037094/setting-nltk-with-stanford-nlp-both-stanfordnertagger-and-stanfordpostagger-fo"">Setting NLTK with Stanford NLP (both StanfordNERTagger and StanfordPOSTagger) for Spanish</a></li>
<li><a href=""https://stackoverflow.com/questions/22930328/error-using-stanford-pos-tagger-in-nltk-python"">Error using Stanford POS Tagger in NLTK Python</a></li>
<li><a href=""https://stackoverflow.com/questions/34692987/cant-make-stanford-pos-tagger-working-in-nltk"">Can&#39;t make Stanford POS tagger working in nltk</a></li>
<li><a href=""https://stackoverflow.com/questions/7344916/trouble-importing-stanford-pos-tagger-into-nltk"">trouble importing stanford pos tagger into nltk</a></li>
<li><a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk"">Stanford Parser and NLTK</a></li>
</ul>
",1,0,704,2016-08-29 15:33:41,https://stackoverflow.com/questions/39210008/nltk-cant-find-the-stanford-pos-tagger-model-file
Calling .jar files from PHP - Stanford NLP - Could not find or load main java class,"<p>I've got a project that is using this package <a href=""https://github.com/agentile/PHP-Stanford-NLP"" rel=""nofollow"">agentile/PHP-Stanford-NLP</a> (PHP interface to Stanford NLP Tools (POS Tagger, NER, Parser) which calls a few .jar files. Everything is working ok on localhost (MAMP) but when I deployed it to laravel forge it is not working anymore. I installed JRE/JDK, Oracle JDK, Oracle JDK 8 in my server.</p>

<p>This is the piece of code I use to call the java files:</p>

<pre><code>$parser = new \StanfordNLP\Parser(
        public_path().'/stanford-parser.jar',
        public_path().'/stanford-parser-3.4.1-models.jar'
);
$parser = $parser-&gt;parseSentence($text);
</code></pre>

<p>This is the piece of code where the error comes from:</p>

<pre><code>$parser = $this-&gt;lexicalized_parser ? 'edu/stanford/nlp/models/lexparser/englishFactored.ser.gz' : 'edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz';
$osSeparator = $this-&gt;php_os == 'windows' ? ';' : ':';
$cmd = $this-&gt;getJavaPath()
     . "" $options -cp \""""
     . $this-&gt;getJar()
     . $osSeparator
     . $this-&gt;getModelsJar()
     . '"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -encoding UTF-8 -outputFormat ""'
     . $this-&gt;getOutputFormat()
     . ""\"" ""
     . $parser
     . "" ""
     . $tmpfname;
$process = proc_open($cmd, $descriptorspec, $pipes, dirname($this-&gt;getJar()));
</code></pre>

<p><a href=""https://github.com/agentile/PHP-Stanford-NLP/blob/51f99f1aaa1c3d5822fe634346b2b4b33a7a6223/src/StanfordNLP/Parser.php#L90"" rel=""nofollow"">https://github.com/agentile/PHP-Stanford-NLP/blob/51f99f1aaa1c3d5822fe634346b2b4b33a7a6223/src/StanfordNLP/Parser.php#L90</a></p>

<p>This is the error:</p>

<pre class=""lang-none prettyprint-override""><code>Error: Could not find or load main class edu.stanford.nlp.parser.lexparser.LexicalizedParser
</code></pre>

<p>EDITED:</p>

<p>This is the <code>$cmd</code> output from localhost:</p>

<pre><code>java -mx300m -classpath */Applications/MAMP/htdocs/mydomainname/public/lib/slf4j-api.jar:/Applications/MAMP/htdocs/mydomainname/public/lib/slf4j-simple.jar:/Applications/MAMP/htdocs/mydomainname/public/stanford-parser.jar:/Applications/MAMP/htdocs/mydomainname/public/stanford-parser-3.4.1-models.jar edu.stanford.nlp.parser.lexparser.LexicalizedParser -encoding UTF-8 -outputFormat wordsAndTags,penn,typedDependencies edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz /private/tmp/phpnlpparserC7ptSf
</code></pre>

<p>This is the <code>$cmd</code> output from production:</p>

<pre><code>java -mx300m -classpath */home/forge/mydomainname.com/public/lib/slf4j-api.jar:/home/forge/mydomainname.com/public/lib/slf4j-simple.jar:/home/forge/mydomainname.com/public/stanford-parser.jar:/home/forge/mydomainname.com/public/stanford-parser-3.4.1-models.jar edu.stanford.nlp.parser.lexparser.LexicalizedParser -encoding UTF-8 -outputFormat wordsAndTags,penn,typedDependencies edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz /tmp/phpnlpparserRdsoE5
</code></pre>
","java, php, laravel, classpath, stanford-nlp","<p>The error message you posted:</p>

<pre class=""lang-none prettyprint-override""><code>Error: Could not find or load main class edu.stanford.nlp.parser.lexparser.LexicalizedParser
</code></pre>

<p>indicates that your class could be found by the <code>java</code> command. Which means your class is not in the <a href=""https://en.wikipedia.org/wiki/Classpath_(Java)"" rel=""nofollow""><code>classpath</code></a>.</p>

<p>The class <code>edu.stanford.nlp.parser.lexparser.LexicalizedParser</code> should be inside <code>stanford-parser.jar</code> which you are manually including in the classpath.</p>

<p>In this scenario (since you said in the comments that the file actually exists) there are two main reasons that could cause the problem:</p>

<ul>
<li><p>You don't have read permission for this file.</p></li>
<li><p>Your file is somehow corrupted or it is not the same one you are using in your local environment (it does not contain the referred class).</p></li>
</ul>

<p>The first cause is unlikely if you uploaded the files with the same user with which you are running the process, in any case it is easy to check and fix.</p>

<p>The second cause can be solved by downloading a clean version and replacing the current one. You can download the new version from <a href=""http://central.maven.org/"" rel=""nofollow""><code>Maven Central</code></a> and replace the one in your server using the following command:</p>

<pre class=""lang-none prettyprint-override""><code>wget http://central.maven.org/maven2/edu/stanford/nlp/stanford-pa‌​rser/3.6.0/stanford-‌​parser-3.6.0.jar &amp;&amp; mv stanford-parser-3.6.0.jar /home/forge/mydomainname.com/public/stanford-parser.jar
</code></pre>
",2,11,825,2016-09-03 10:45:08,https://stackoverflow.com/questions/39306031/calling-jar-files-from-php-stanford-nlp-could-not-find-or-load-main-java-cl
How to reverse engineer an NLP parse tree to arrive at the original sentence?,"<p>Given a parse tree (obtained with <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/corenlp/process</a> pretty print option)</p>

<pre><code>(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>How could I arrive at the original sentence?</p>

<pre><code>You could say that they regularly catch a shower, which adds to their exhilaration and joie de vivre.
</code></pre>

<p>I am thinking of using some regexp magic, but I wonder if Stanford NLP has a built in feature to do this task?</p>
","regex, nlp, nltk, stanford-nlp","<p>You can convert string to <code>Tree</code> using <code>Tree.fromstring()</code>. Now, you can use <code>Tree.leaves()</code> method to get all tokens from the tree.</p>

<p><strong>Code:</strong>  </p>

<pre><code>from nltk import Tree

parse_str = ""(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))""

t = Tree.fromstring(parse_str)

#print t.leaves()
print ' '.join(t.leaves())
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>You could say that they regularly catch a shower , which adds to their exhilaration and joie de vivre .
</code></pre>
",4,2,666,2016-09-04 17:20:30,https://stackoverflow.com/questions/39319547/how-to-reverse-engineer-an-nlp-parse-tree-to-arrive-at-the-original-sentence
How to split an NLP parse tree to clauses (independent and subordinate)?,"<p>Given an NLP parse tree like </p>

<pre><code>(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>Original sentence is ""You could say that they regularly catch a shower, which adds to their exhilaration and joie de vivre.""</p>

<p>How could the clauses be extracted and reverse engineered?
We would be splitting at S and SBAR (to preserve the type of clause, eg subordinated)</p>

<pre><code> - (S (NP (PRP You)) (VP (MD could) (VP (VB say) 
 - (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower))
 - (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to)
   (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW
   de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>to arrive at</p>

<pre><code> - You could say
 - that they regularly catch a shower 
 - , which adds to their exhilaration and joie de vivre.
</code></pre>

<p>Splitting at S and SBAR seems very easy. The problem seems to be stripping away all the POS tags and chunks from the fragments.</p>
","nlp, nltk, grammar, stanford-nlp, clause","<p>You can use <code>Tree.subtrees()</code>. For more information check <a href=""http://www.nltk.org/_modules/nltk/tree.html"" rel=""noreferrer"">NLTK Tree Class</a>.</p>

<p><strong>Code:</strong></p>

<pre><code>from nltk import Tree

parse_str = ""(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))""
#parse_str = ""(ROOT (S (SBAR (IN Though) (S (NP (PRP he)) (VP (VBD was) (ADJP (RB very) (JJ rich))))) (, ,) (NP (PRP he)) (VP (VBD was) (ADVP (RB still)) (ADJP (RB very) (JJ unhappy))) (. .)))""

t = Tree.fromstring(parse_str)
#print t

subtexts = []
for subtree in t.subtrees():
    if subtree.label()==""S"" or subtree.label()==""SBAR"":
        #print subtree.leaves()
        subtexts.append(' '.join(subtree.leaves()))
#print subtexts

presubtexts = subtexts[:]       # ADDED IN EDIT for leftover check

for i in reversed(range(len(subtexts)-1)):
    subtexts[i] = subtexts[i][0:subtexts[i].index(subtexts[i+1])]

for text in subtexts:
    print text

# ADDED IN EDIT - Not sure for generalized cases
leftover = presubtexts[0][presubtexts[0].index(presubtexts[1])+len(presubtexts[1]):]
print leftover
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>You could say 
that 
they regularly catch a shower , 
which 
adds to their exhilaration and joie de vivre
 .
</code></pre>
",11,8,8503,2016-09-04 18:10:51,https://stackoverflow.com/questions/39320015/how-to-split-an-nlp-parse-tree-to-clauses-independent-and-subordinate
CoreNLP : provide pos tags,"<p>I have text that is <strong>already</strong> tokenized, sentence-split, and POS-tagged.</p>

<p>I would like to use CoreNLP to <strong>additionally</strong> annotate lemmas (<code>lemma</code>), named entities (<code>ner</code>), contituency and dependency parse (<code>parse</code>), and coreferences (<code>dcoref</code>).</p>

<p>Is there a combination of commandline options and option file specifications that makes this possible from the command line?</p>

<p>According to <a href=""https://stackoverflow.com/questions/30229648/processing-input-before-giving-input-to-parser"">this question</a>, I can ask the parser to view whitespace as delimiting tokens, and newlines as delimiting sentences by adding this to my properties file:</p>

<pre><code>tokenize.whitespace = true
ssplit.eolonly = true
</code></pre>

<p>This works well, so all that remains is to specify to CoreNLP that I would like to provide POS tags too.</p>

<p>When using the Stanford Parser standing alone, it <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#f"" rel=""nofollow noreferrer"">seems to be possible</a> to have it use existing POS tags, but copying that syntax to the invocation of CoreNLP doesn't seem to work.  For example, this does not work:</p>

<pre><code>java -cp *:./* -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props my-properties-file -outputFormat xml -outputDirectory my-output-dir -sentences newline -tokenized -tagSeparator / -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory -file my-annotated-text.txt
</code></pre>

<p>While <a href=""https://stackoverflow.com/questions/29518946/forcing-pos-tags-in-stanford-corenlp"">this question</a> covers programmatic invocation, I'm invoking CoreNLP form the commandline as part of a larger system, so I'm really asking whether this is possible to achieve this with commandline options.</p>
","parsing, tokenize, pos-tagger, stanford-nlp","<p>I don't think this is possible with command line options.</p>

<p>If you want you can make a custom annotator and include it in your pipeline you could go that route.</p>

<p>Here is some sample code:</p>

<pre><code>package edu.stanford.nlp.pipeline;

import edu.stanford.nlp.util.logging.Redwood;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.util.concurrent.MulticoreWrapper;
import edu.stanford.nlp.util.concurrent.ThreadsafeProcessor;

import java.util.*;

public class ProvidedPOSTaggerAnnotator {

  public String tagSeparator;

  public ProvidedPOSTaggerAnnotator(String annotatorName, Properties props) {
    tagSeparator = props.getProperty(annotatorName + "".tagSeparator"", ""_"");
  }

  public void annotate(Annotation annotation) {

    for (CoreLabel token : annotation.get(CoreAnnotations.TokensAnnotation.class)) {
      int tagSeparatorSplitLength = token.word().split(tagSeparator).length;
      String posTag = token.word().split(tagSeparator)[tagSeparatorSplitLength-1];
      String[] wordParts = Arrays.copyOfRange(token.word().split(tagSeparator), 0, tagSeparatorSplitLength-1);
      String tokenString = String.join(tagSeparator, wordParts);
      // set the word with the POS tag removed
      token.set(CoreAnnotations.TextAnnotation.class, tokenString);
      // set the POS
      token.set(CoreAnnotations.PartOfSpeechAnnotation.class, posTag);
    }
  }
}
</code></pre>

<p>This should work if you provide your token with POS tokens separated by ""_"".  You can change it with the forcedpos.tagSeparator property.</p>

<p>If you set customAnnotator.forcedpos = edu.stanford.nlp.pipeline.ProvidedPOSTaggerAnnotator</p>

<p>to the property file, include the above class in your CLASSPATH, and then include ""forcedpos"" in your list of annotators after ""tokenize"", you should be able to pass in your own pos tags.</p>

<p>I may clean this up some more and actually include it in future releases for people!</p>

<p>I have not had time to actually test this code out, if you try it out and find errors please let me know and I'll fix it!</p>
",2,2,907,2016-09-04 19:39:03,https://stackoverflow.com/questions/39320782/corenlp-provide-pos-tags
Cannot get CoreNLP models files from JBoss Fuse ESB,"<p>I have a problem with <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">CoreNLP</a> using in JBoss Fuse ESB. I installed CoreNLP with it's models on ESB by</p>

<pre><code>install wrap:mvn:edu.stanford.nlp/stanford-corenlp/3.6.0
install wrap:mvn:edu.stanford.nlp/stanford-corenlp/3.6.0//models-english
</code></pre>

<p>Then, when my module tries to use <code>models-english</code> I have error log message</p>

<pre><code>Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:485)[275:org.opengravity.camel.nlp:1.0.0.SNAPSHOT]
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:765)[275:org.opengravity.camel.nlp:1.0.0.SNAPSHOT]
    ... 69 more
</code></pre>

<p>I tried to solve this problem by <code>maven-bundle-plugin</code> configurations</p>

<pre><code>&lt;Embed-Dependency&gt;stanford-corenlp&lt;/Embed-Dependency&gt;
</code></pre>

<p>but unfortunately it didn't help.</p>
","apache-felix, jbossfuse, maven-bundle-plugin, stanford-nlp","<p>First of all I install two modules</p>

<pre><code>install wrap:mvn:edu.stanford.nlp/stanford-corenlp/3.6.0
install wrap:mvn:edu.stanford.nlp/stanford-corenlp/3.6.0//models-english
</code></pre>

<p>and then, as it was advised by Claus Ibsen, I enabled dynamic import on the first bundle <code>dev:dynamic-import XXX</code> where <code>XXX</code> is the <code>stanford-corenlp-3.6.0</code> bundle id. It solved my problem.</p>
",0,0,64,2016-09-04 19:44:24,https://stackoverflow.com/questions/39320839/cannot-get-corenlp-models-files-from-jboss-fuse-esb
Why is Stanford NLP nesting other phrases inside verb phrases?,"<p>I noticed a full parse nests other phrases inside verb phrases like here (although Noun Phrases seem to be standalone)</p>

<pre><code>(ROOT\n  (S\n    (NP (DT The) (JJ quick) (JJ brown) (NN fox))\n    (VP (VBD jumped)\n      (PP (IN over)\n        (NP (DT the) (JJ lazy) (NN dog.))))))
</code></pre>

<p>When I run a simple chunking via Apache OpenNLP, verb phrases are standalone like here</p>

<pre><code>[NP The_DT quick_JJ brown_JJ fox_NN ] [VP jumped_VBD ] [PP over_IN ] [NP the_DT lazy_JJ dog_NN ] ._.
</code></pre>

<p>Although, Apache OpenNLPs full parse is nesting verb phrases too. (<a href=""https://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.parser.parsing.cmdline"" rel=""nofollow"">https://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.parser.parsing.cmdline</a>)</p>

<pre><code>(TOP (NP (NP (DT The) (JJ quick) (JJ brown) (NN fox) (NNS jumps)) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog))) (. .)))
</code></pre>

<p>Is it correct that Stanford NLP nests other phrases inside Verb Phrases?</p>
","nlp, nltk, stanford-nlp, opennlp","<p><a href=""http://udel.edu/~heinz/classes/2012/4-667/materials/JM2008-Chap12.pdf"" rel=""nofollow"">http://udel.edu/~heinz/classes/2012/4-667/materials/JM2008-Chap12.pdf</a></p>

<p>Usually everything but the subject is in the verb phrase.</p>
",1,1,182,2016-09-06 09:21:23,https://stackoverflow.com/questions/39345280/why-is-stanford-nlp-nesting-other-phrases-inside-verb-phrases
stanford corenlp serialization exception,"<p>I am using the Stanfrod coreNLP's ProtobufAnnotationSerializer to serialize/deserialize Annotation objects, it works fine most of the times, but some times the serializer gives me the next Index Out Of Bounds Exception:</p>

<pre><code>    java.lang.IndexOutOfBoundsException: Index: 44, Size: 36
at java.util.ArrayList$SubList.rangeCheck(ArrayList.java:1217)
at java.util.ArrayList$SubList.get(ArrayList.java:1034)
at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.fromProto(ProtobufAnnotationSerializer.java:1513)
at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.fromProto(ProtobufAnnotationSerializer.java:1588)
at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.fromProto(ProtobufAnnotationSerializer.java:1296)
at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.read(ProtobufAnnotationSerializer.java:186)
at com.marca.nlp.relation.RelationDBDeser.main(RelationDBDeser.java:102)
</code></pre>

<p>This exception popups up frequently and for no clear reasons that I could find, here is my serialize/deserialize code:</p>

<p>Serialize: </p>

<pre><code>    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit, pos, lemma, ner, parse""
           + "", depparse, mention, coref, natlog, openie, relation, sentiment"");
    props.setProperty(""openie.resolve_coref"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    ProtobufAnnotationSerializer serializer = new ProtobufAnnotationSerializer();

    String docPath = ""document.proto"";
     try{

            Annotation document = new Annotation(input);

            pipeline.annotate(document);

            FileOutputStream fileOut = new FileOutputStream(docPath);
            ObjectOutputStream out = new ObjectOutputStream(fileOut);
            serializer.write(document, out);
    } catch (Exception ex){
            ex.printStackTrace();
    }
</code></pre>

<p>Deserialize code:</p>

<pre><code>    ProtobufAnnotationSerializer serializer = new ProtobufAnnotationSerializer();
    try{
        String docPath = ""document.proto"";
        FileInputStream fileIn = new FileInputStream(docPath);
        ObjectInputStream in = new ObjectInputStream(fileIn);

        Pair&lt;Annotation, InputStream&gt; docs = serializer.read(in);
        Annotation document = docs.first();
    } catch(Exception ex){
        ex.printStackTrace();
    }
</code></pre>

<p>The exception pops up when I deserialize the object, here is a sample text that causes the problem:</p>

<blockquote>
  <p>Alice Corp. v. CLS Bank International was a legal case about patentable subject matter (patent eligibility)  that the United States Supreme Court heard in 2014, presenting the issue of whether certain claims about a computer-implemented, electronic escrow service for facilitating financial transactions concern abstract ideas ineligible for patent protection.  The patents were held to be invalid because the claims were drawn to an abstract idea, and implementing those claims on a computer was not enough to transform that idea to a patentable invention.  It was the first Supreme Court case on the patent eligibility of software since Bilski v. Kappos in 2010, which was the first such case in three decades.    Alice Corporation (""Alice"") owns four patents on electronic methods and computer programs for financial-trading systems on which trades between two parties who are to exchange payment are settled by a third party in ways that reduce ""counterparty"" or ""settlement"" risk, or the risk that one party will perform while the other will not. According to Alice's account, CLS Bank International and CLS Services Ltd. (collectively ""CLS Bank"") began to use a similar technology in 2002. Alice notified CLS Bank of its probable infringement of Alice's patents, and the companies discussed licensing of the patents.  The relevant claims are in these patents: The three later patents are all derived from the first through continuation and/or continuation-in-part applications. The concept of a third-party to confirm a complete transaction is called escrow, and has been used in finance for thousands of years. The patents in question described how the escrow function could be performed by a general-purpose computer. However, they did not describe how such a computer would work, and did not include any source code or specifications. Australian Ian Shepherd received the patents in 1999, and then formed Alice Corporation to own the patent. However, Alice never produced any such computer system as described, or used the patents in any of its business. As a result the company has been called a patent troll. CLS, a consortium of banks, has actually developed such a computer system that it uses to facilitate US$5 trillion in transactions every day.  US Patent law specifies several requirements before something can receive a patent. 35 U.S.C. §101 of the federal patent law has been interpreted as implicitly disqualifying abstract ideas from being patented and forbidding patents on laws of nature and physical phenomena. 35 U.S.C. §102 requires the submission to be novel, and sets out conditions for what can be used as prior art. 35 U.S.C. §103 requires that patentable claims should not be obvious to someone familiar with the subject. 35 U.S.C. §112 requires (among others) that patents must be clear and detailed enough for someone familiar with the subject to implement it. Alice Corporation's patents have been argued invalid under all of these sections, but the primary litigation focus has been on Section 101 and if Alice's patents claim an abstract idea.  In 2007, CLS Bank sued Alice in the United States District Court for the District of Columbia seeking a declaratory judgment that Alice's patents were invalid and unenforceable and that CLS Bank had not infringed them. Alice countersued CLS Bank for infringement of the patents. After the court had allowed initial, limited discovery on the questions of CLS Bank's operations and its relationship to the allegedly infringing CLS Bank system, the court ruled on the parties' cross-motions for summary judgment, declaring each of Alice's patents invalid because the claims concerned abstract ideas, which are not eligible for patent protection under 35 U.S.C. § 101.  The court stated that a method “directed to an abstract idea of employing an intermediary to facilitate simultaneous exchange of obligations in order to minimize risk” is a “basic business or financial concept,” and that a “computer system merely ‘configured’ to implement an abstract method is no more patentable than an abstract method that is simply ‘electronically’ implemented.”  The district court judge followed Bilski v. Kappos as precedent, in which the 2010 Supreme Court held that certain claims to business methods for hedging against the risk of price fluctuations when trading in commodities markets were not patent-eligible because they covered the abstract idea of hedging against risk.  In the Bilski case the Supreme Court had said that allowing such claims would preempt the use of risk-hedging in all fields and grant a monopoly over an abstract idea.  Alice appealed the decision and the case went to the United States Court of Appeals for the Federal Circuit. A panel of the appeals court decided by 2-1 in July 2012 to reverse the lower court's decision. The panel held that computer-implemented inventions like Alice's are patent-eligible under § 101 unless it is “manifestly evident” that the claims are about an abstract idea; that is, “the single most reasonable understanding is that a claim is directed to nothing more than a fundamental truth or disembodied concept, with no limitations in the claim attaching that idea to a specific application.”  CLS Bank petitioned the same Federal Circuit court for an en banc rehearing. The court granted the petition and vacated the earlier panel's decision in order to decide the following questions: what test should the court adopt to determine whether a computer-implemented invention is a patent-ineligible abstract idea; whether the presence of a computer in a claim could ever make patent-ineligible subject matter patentable; and whether method, system, and media claims should be considered equivalent under § 101. The fractured panel of ten judges issued seven different opinions, with no opinion supported by a majority. Seven of the ten judges upheld the district court's decision that Alice's method claims and computer-readable-medium claims were not patent-eligible, but they did so for conflicting and incompatible reasons. Five of the ten judges upheld the district court's decision that Alice's computer-systems claims were not patent-eligible. The panel did not agree on a standard to determine whether a computer-implemented invention is a patent-ineligible, abstract idea.  In the leading, five-member, concurring opinion by Circuit Judge Lourie, joined by Circuit Judges Dyk, Prost, Reyna, and Wallach, a plurality of the court articulated an analysis of patent-eligibility focused on first identifying the abstract idea or fundamental concept applied by the claim and then determining whether the claim would preempt the abstract idea.  The analysis involves four steps: Regarding ""human contribution,"" the Lourie opinion pointed to four questions to ask, which are potentially subjective factors weighing against patent-eligibility: The Lourie analysis is framed by three common themes in Supreme Court decisions: Chief Judge Rader and Circuit Judges Linn, Moore, and O'Malley filed an opinion concurring in part and dissenting in part, which articulated a patent-eligibility analysis focused on determining whether the claim, as a whole, was limited to an application of an abstract idea, or was merely a recitation of the abstract idea. Under the Rader approach, Alice's patents would have been held patent-eligible because the system claims were limited to a computer-implemented application.  Judge Rader filed ""additional reflections"" to the ruling (not signed by other judges) expressing his read of the statute as allowing very broad patentability under § 101, and his understanding that natural laws are restricted to ""universal constants created, if at all, only by God, Vishnu, or Allah."" Referencing Einstein, he states that ""even gravity is not a natural law."".  —Circuit Judge Kimberly Ann Moore, dissenting in part,   Circuit Judge Moore filed an opinion dissenting in part, in which Chief Judge Rader and Circuit Judges Linn and O'Malley joined, which stated, based on reasoning similar to the Rader approach, that a claim must be considered as a whole and that Alice's computer-systems claims should have been ruled patent-eligible. The opinion cautioned that ""if all of these claims, including the system claims, are not patent-eligible, this case is the death of hundreds of thousands of patents, including all business method, financial system, and software patents as well as many computer implemented and telecommunications patents.""  Circuit Judge Newman filed an opinion concurring in part and dissenting in part, which called for the Federal Circuit to clarify the interpretation of § 101 by affirming three fundamental principles: that § 101 is an inclusive statement of patentable subject matter, that the form of the claim is not relevant to patent-eligibility, and that experimental use of patented subject matter, such as experiments that improve or build upon patented subject matter, compare it with alternatives, seek to understand its mechanisms, or seek to find new applications of it, is not patent-ineligible.  Circuit Judges Linn and O'Malley filed a dissenting opinion that, for procedural reasons, the lower court should have been reversed and all claims ruled patent-eligible. The opinion agreed with the Rader opinion with respect to the computer-systems claims but would have applied the Rader analysis to all the claims and would have had all claims rise or fall together. The opinion called for legislative, rather than judicial, action to address the ""proliferation and aggressive enforcement of low quality software patents"" cited in the many amicus curiae briefs and suggested laws to limit the term of software patents or limit the scope of patents by requiring functional claiming.  The Supreme Court of the United States granted Alice's petition for a writ of certiorari to decide the question ""[w]hether claims to computer-implemented inventions—including claims to systems and machines, processes, and items of manufacture—are directed to patent-eligible subject matter within the meaning of 35 U.S.C. § 101.""  The deep interest of the software industry and patent experts in this divisive issue is evident in the number of companies and groups that had filed amicus curiae briefs urging the Supreme Court to decide this issue, including, among others, Electronic Frontier Foundation, Software Freedom Law Center, Institute of Electrical and Electronics Engineers, Intellectual Property Law Association of Chicago, and Accenture Global Services.  While nearly all such briefs argued that the patent should be invalidated, they disagreed on the reasoning. A brief prepared by Google, Amazon and other companies argued that the patent was of an abstract idea, which actually harms innovation, and that the real innovation lies in detailing out a working system. Microsoft, Adobe and Hewlett-Packard argued it was nothing more than an unpatentable business method (per Bilski v. Kappos) and merely saying to perform it with a computer does not change this fact. Linkedin, Netflix and others, and a separate brief by the Free Software Foundation and others both argued that no software should be patented, as this blocks innovation and scientific collaboration. IBM disagreed with the ""abstract ideas"" reasoning and argued that the patent should instead be struck down for being too obvious. Finally, a consortium of retailer and manufacturers, including Dillard's and Hasbro, simply asked for a clear rule.  The Court heard oral arguments in the case on March 31, 2014,  and issued a ruling on June 19, 2014.  Arguing for Alice Corp. was Carter G. Phillips of Sidley Austin, and arguing for CLS Bank is Mark A. Perry of Gibson, Dunn &amp; Crutcher. The court agreed with those filing amicus curiae briefs and unanimously invalidated the patent. According to The Washington Post, ""while the court struck down what was universally said to be a bad patent, it didn't do much to say what kinds of software should be patentable. In other words, the court decided the most basic conflict in the case, but more or less declined to offer guidance for other, future cases.""  The Electronic Frontier Foundation said the Supreme Court ""reaffirmed that merely adding “a generic computer to perform generic computer functions” does not make an otherwise abstract idea patentable. This statement (and the opinion itself) makes clear that an abstract idea along with a computer doing what a computer normally does is not something our patent system was designed to protect. Admittedly, the Supreme Court did not offer the clearest guidance on when a patent claims merely an abstract idea, but it did offer guidance that should help to invalidate some of the more egregious software patents out there.""  The Software Freedom Law Center said the Supreme Court ""took one more step towards the abolition of patents on software inventions. Upholding its previous positions, the Court held that abstract ideas and algorithms are unpatentable. It also emphasized that one cannot patent “an instruction to apply [an] abstract idea ... using some un-specified, generic computer.”""  The Coalition for Patent Fairness, which advocates for patent reform legislation, said ""neither the ruling—nor any single act by the court or the executive branch—can do what is needed to make the business model of being a patent troll unprofitable and unattractive.""  Some commentators expressed disappointment with the opinion for its failure to define more comprehensively the boundaries between abstract ideas and patent-eligible implementations of ideas. They were particularly critical of Justice Thomas's statement—""In any event, we need not labor to delimit the precise contours of the 'abstract ideas' category in this case. It is enough to recognize that there is no meaningful distinction between the concept of risk hedging in Bilski and the concept of intermediated settlement at issue here. Both are squarely within the realm of 'abstract ideas' as we have used that term.""  For example, Professor Merges said, “To say we did not get an answer is to miss the depth of the non-answer we did get.”  Professor Duffy remarked, ""[T]he Supreme Court has been remarkably resistant to providing clear guidance in this area, and this case continues that trend.""  Perhaps most exasperated of all was Law Comics, which cartooned Justice Thomas saying that ""we need not labor..."" and replied ""yes you do!"" while wagging a finger at him as an accompanying article asserted that his opinion “baulked at the messy, challenging issues surrounding software” and was “not a particularly useful” decision about patenting software.  On the other hand, Professor Stern defended the opinion as ""the expectable price of unanimity in a nine-member tribunal,"" arguing that the ""greater sensed legitimacy and precedential stability"" of a unanimous opinion ""outbalanced"" the shortcomings of a lack of clear guidance as to details; this commentator also asserted that ""it is sensible to make narrow, incremental rulings as to software patent eligibility, because at present we are not so well informed that we can speak with confidence in very broad terms.""</p>
</blockquote>

<p>I am using Stanford CoreNLP 3.6.0
Any thoughts?</p>
","java, nlp, stanford-nlp","<p>Yup, I've replicated the bug. The issue here is with the <code>-openie.resolve_coref</code> flag. The relations produced by OpenIE will actually point to CoreLabels in the original document, which means the deserializer will reconstruct the relation triple from the triple's proto and the deserialized sentence. This is usually ok, because all the tokens in the triple are in the deserialized sentence. But of course, with coref, we can now reference tokens anywhere else in the document. Thus, index out of bounds exceptions.</p>

<p>A fix has been pushed, and should show up on GitHub soon. Note that the new code will no longer deserialize relation triples stored in the old proto format, as the underlying proto file has changed.</p>
",0,1,665,2016-09-07 16:17:47,https://stackoverflow.com/questions/39374946/stanford-corenlp-serialization-exception
Training caseless NER models with Stanford corenlp,"<p>I know how to train an NER model as specified <a href=""http://nlp.stanford.edu/software/crf-faq.html"" rel=""nofollow"">here</a> and have a very successful one in fact. I also know about the 3 provided caseless models as talked about <a href=""http://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""nofollow"">here</a>.  But what if I want to train my own caseless model, what is the trick there?  I have a bunch of all uppercase documents for training.  Do I use the same training process or are there special/different features for the caseless models or are there properties that need to be set?  I can't find a description as to how the provided caseless models were created.</p>
","stanford-nlp, named-entity-recognition","<p>There is only one property change in our models, which is that you want to have it invoke a function that removes case information before words are processed for classification. We do that with this property value (which also maps some words to American spelling):</p>

<p><code>wordFunction = edu.stanford.nlp.process.LowercaseAndAmericanizeFunction</code></p>

<p>but there is also simply:</p>

<p><code>wordFunction = edu.stanford.nlp.process.LowercaseFunction</code></p>

<p>Having more automatic stuff for deciding document format (hard/soft line breaks), case, or even language would be nice, but at present we don't have any of those....</p>
",2,0,513,2016-09-08 19:17:03,https://stackoverflow.com/questions/39398623/training-caseless-ner-models-with-stanford-corenlp
Coreference resolution in python nltk using Stanford coreNLP,"<p>Stanford CoreNLP provides coreference resolution <a href=""http://nlp.stanford.edu/software/dcoref.shtml"" rel=""noreferrer"">as mentioned here</a>, also <a href=""https://stackoverflow.com/questions/30954649/coreference-resolution-using-stanford-corenlp"">this thread</a>, <a href=""https://stackoverflow.com/questions/30362691/stanford-corenlp-wrong-coreference-resolution"">this</a>,   provides some insights about its implementation in Java.</p>

<p>However, I am using python and NLTK and I am not sure how can I use Coreference resolution functionality of CoreNLP in my python code. I have been able to set up StanfordParser in NLTK, this is my code so far.</p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser
stanford_parser_dir = 'stanford-parser/'
eng_model_path = stanford_parser_dir  + ""stanford-parser-models/edu/stanford/nlp/models/lexparser/englishRNN.ser.gz""
my_path_to_models_jar = stanford_parser_dir  + ""stanford-parser-3.5.2-models.jar""
my_path_to_jar = stanford_parser_dir  + ""stanford-parser.jar""
</code></pre>

<p>How can I use coreference resolution of CoreNLP in python?</p>
","python, nlp, nltk, stanford-nlp","<p>As mentioned by @Igor You can try the python wrapper implemented in this GitHub repo: <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""noreferrer"">https://github.com/dasmith/stanford-corenlp-python</a></p>

<p>This repo contains two main files: 
corenlp.py
client.py</p>

<p>Perform the following changes to get coreNLP working:</p>

<ol>
<li><p>In the corenlp.py, change the path of the corenlp folder. Set the path where your local machine contains the corenlp folder and add the path in line 144 of corenlp.py</p>

<p><code>if not corenlp_path:
    corenlp_path = &lt;path to the corenlp file&gt;</code></p></li>
<li><p>The jar file version number in ""corenlp.py"" is different. Set it according to the corenlp version that you have. Change it at line 135 of corenlp.py</p>

<p><code>jars = [""stanford-corenlp-3.4.1.jar"",
                ""stanford-corenlp-3.4.1-models.jar"",
                ""joda-time.jar"",
                ""xom.jar"",
                ""jollyday.jar""]</code></p></li>
</ol>

<p>In this replace 3.4.1 with the jar version which you have downloaded.</p>

<ol start=""3"">
<li><p>Run the command: </p>

<p><code>python corenlp.py</code></p></li>
</ol>

<p>This will start a server</p>

<ol start=""4"">
<li><p>Now run the main client program</p>

<p><code>python client.py</code></p></li>
</ol>

<p>This provides a dictionary and you can access the coref using 'coref' as the key:</p>

<p>For example: John is a Computer Scientist. He likes coding.</p>

<pre><code>{
     ""coref"": [[[[""a Computer Scientist"", 0, 4, 2, 5], [""John"", 0, 0, 0, 1]], [[""He"", 1, 0, 0, 1], [""John"", 0, 0, 0, 1]]]]
}
</code></pre>

<p>I have tried this on Ubuntu 16.04. Use java version 7 or 8. </p>
",10,12,18750,2016-09-09 11:12:47,https://stackoverflow.com/questions/39410282/coreference-resolution-in-python-nltk-using-stanford-corenlp
Read Protobuf Serialization of StanfordNLP Output in Python,"<p>I would like to output the StanfordNLP results in protobuf (since its size is much smaller) and read the results back in python. How should I do that? </p>

<p>I followed the instruction <a href=""http://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow"">here</a> to output the results serialized with <code>ProtobufAnnotationSerializer</code>, like this:</p>

<pre><code>java -cp ""stanford-corenlp-full-2015-12-09/*"" \
edu.stanford.nlp.pipeline.StanfordCoreNLP \
-annotators tokenize,ssplit \
-file input.txt \
-outputFormat serialized \
-outputSerializer \
edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer
</code></pre>

<p>Then use <code>protoc</code> to compile the <code>CoreNLP.proto</code>, which comes with the source code of StanfordNLP, into python modules like this:</p>

<pre><code>protoc --python_out=. CoreNLP.proto
</code></pre>

<p>Then in python I read the files back like this:</p>

<pre><code>import CoreNLP_pb2
doc = CoreNLP_pb2.Document()
doc.ParseFromString(open('input.txt.ser.gz', 'rb').read())
</code></pre>

<p>The parsing fails with the following error message</p>

<pre><code>---------------------------------------------------------------------------
DecodeError                               Traceback (most recent call last)
&lt;ipython-input-213-d8eaeb9c2048&gt; in &lt;module&gt;()
      1 doc = CoreNLP_pb2.Document()
----&gt; 2 doc.ParseFromString(open('imed/s5_tokenized/conv-00000.ser.gz', 'rb').read())

/usr/local/lib/python2.7/dist-packages/google/protobuf/message.pyc in ParseFromString(self, serialized)
    183     """"""
    184     self.Clear()
--&gt; 185     self.MergeFromString(serialized)
    186 
    187   def SerializeToString(self):

/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc in MergeFromString(self, serialized)
   1092         # The only reason _InternalParse would return early is if it
   1093         # encountered an end-group tag.
-&gt; 1094         raise message_mod.DecodeError('Unexpected end-group tag.')
   1095     except (IndexError, TypeError):
   1096       # Now ord(buf[p:p+1]) == ord('') gets TypeError.

DecodeError: Unexpected end-group tag.
</code></pre>

<p>UPDATE:</p>

<p>I asked the author of the serializer Gabor Angeli and got the answer. The protobuf objects were written to the files with <code>writeDelimitedTo</code> in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/83f9965d85851dff4134a31425f7c095e6711ca7/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializer.java#L180"" rel=""nofollow"">this line</a>. Changing it to <code>writeTo</code> would make the output files readable in Python.  </p>
","python, protocol-buffers, stanford-nlp","<p>This question seems to have come up again, so I figured I'd write up a proper answer. The root of the issue is that the proto is written using Java's <code>writeDelimitedTo</code> method, which Google has not implemented for Python. A workaround would be to use the following method to read the proto file (assuming the file is not gziped -- you can replace <code>f.read()</code> with the appropriate code to unzip the file as appropriate):</p>

<pre><code>from google.protobuf.internal.decoder import _DecodeVarint
import CoreNLP_pb2

def readCoreNLPProtoFile(protoFile):
  protos = []
  with open(protoFile, 'rb') as f:
    # -- Read the file --
    data = f.read()
    # -- Parse the file --
    # In Java. there's a parseDelimitedFrom() method that makes this easier
    pos = 0
    while (pos &lt; len(data)):
      # (read the proto)
      (size, pos) = _DecodeVarint(data, pos)
      proto = CoreNLP_pb2.Document()
      proto.ParseFromString(data[pos:(pos+size)])
      pos += size
      # (add the proto to the list; or, `yield proto`)
      protos.append(proto)
  return protos
</code></pre>

<p>The file <code>CoreNLP_pb2</code> is compiled from the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/CoreNLP.proto"" rel=""nofollow noreferrer"">CoreNLP.proto</a> file in the repo with the command:</p>

<pre><code>protoc --python_out /path/to/output/ /path/to/CoreNLP.proto
</code></pre>

<p>Note that as of writing this (version 3.7.0) the format is proto2, not proto3.</p>
",2,1,1408,2016-09-11 05:54:56,https://stackoverflow.com/questions/39433279/read-protobuf-serialization-of-stanfordnlp-output-in-python
StanfordCoreNLP openIE issue,"<p>I am facing the same issue as
 <a href=""https://stackoverflow.com/questions/37375137/stanford-corenlp-openie-annotator"">Stanford CoreNLP OpenIE annotator</a>
I try output = nlp.annotate(s, properties={""annotators"":""tokenize,ssplit,pos,depparse,natlog,openie"", ""outputFormat"": ""json"",""openie.triple.strict"":""true"", ""openie.max_entailments_per_clause"":""1"",""openie.splitter.disable"":""true""})</p>

<p>But still I get 4 clauses</p>

<p>(u'are pulled from', u'Twenty percent electric motors', u'assembly line') (u'are pulled from', u'percent electric motors', u'assembly line') (u'are', u'Twenty percent electric motors', u'pulled') (u'are', u'percent electric motors', u'pulled') 
Am I doing anything wrong? How to get precise triple
('are pulled from', 'Twenty percent electric motors', 'assembly line')</p>
","python, stanford-nlp, stanford-nlp-server","<p>This is actually expected behavior. It was a design decision in the OpenIE system to produce all triples which are logically entailed by the original sentence, even if they are redundant. The idea being that these triples are usually used for something akin to IR-ish lookup, and in these cases it's convenient to not have to do fuzzy matching for whether any of the triples are ""similar enough"" to the query.</p>
",1,-2,634,2016-09-13 09:02:02,https://stackoverflow.com/questions/39466086/stanfordcorenlp-openie-issue
Stanford CoreNLP: output in CONLL format from Java,"<p>I want to parse some German text with Stanford CoreNLP and obtain a CONLL output, so that I can pass the latter to CorZu for coreference resolution.</p>

<p>How can I do that <em>programmatically</em>? </p>

<p>Here is my code so far (which only outputs dependency trees):</p>

<pre><code>Annotation germanAnnotation = new Annotation(""Gestern habe ich eine blonde Frau getroffen"");
Properties germanProperties = StringUtils.argsToProperties(""-props"", ""StanfordCoreNLP-german.properties"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(germanProperties);
pipeline.annotate(germanAnnotation);

StringBuilder trees = new StringBuilder("""");
for (CoreMap sentence : germanAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
     Tree sentenceTree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
     trees.append(sentenceTree).append(""\n"");
}
</code></pre>
","format, dependencies, stanford-nlp","<p>With the following code I managed to save the parsing output in CONLL format.</p>

<pre><code>OutputStream outputStream = new FileOutputStream(new File(""./target/"", OUTPUT_FILE_NAME));
CoNLLOutputter.conllPrint(germanAnnotation, outputStream, pipeline);
</code></pre>

<p>However, the HEAD field was 0 for all words. I am not sure whether there is a problem in parsing or only in the CONLLOutputter. Honestly, I was too annoyed by CoreNLP to investigate further.</p>

<p>I decided and I suggest using ParZu instead. <a href=""https://github.com/rsennrich/ParZu/blob/master/README.md"" rel=""nofollow"">ParZu</a> and <a href=""http://www.cl.uzh.ch/en/research/completed-research/coreferenceresolution.html"" rel=""nofollow"">CorZu</a> are made to work together seamlessly - and they do.
In my case, I had an already tokenized and POS tagged text. This makes things easier, since you will <strong>not</strong> need:</p>

<ul>
<li>A POS-Tagger using the STTS tagset</li>
<li>A tool for morphological analysis</li>
</ul>

<p>Once you have ParZu and CorZu installed, you will only need to run <code>corzu.sh</code> (included in the CorZu download folder). If your text is tokenized and POS tagged, you can edit the script accordingly:</p>

<pre><code>parzu_cmd=""/YourPath/ParZu/parzu -i tagged""
</code></pre>

<p>Last note: make sure to convert your tagged text to the following format, with empty lines signifying sentence boundaries: word [tab] tag [newline]</p>
",0,1,663,2016-09-13 14:44:19,https://stackoverflow.com/questions/39472836/stanford-corenlp-output-in-conll-format-from-java
custom named entity extraction,"<p>I'm trying to implement NER(Named Entity Extraction) using stanford NLP. 
final goal is to convert free text to query format.
I created a custom dictionary and am able to extract entities and build query</p>

<pre><code>people who are from newyork
</code></pre>

<p>I'll build query</p>

<pre><code>     select * from people where region = 'newyork'
</code></pre>

<p>but the issue comes when the statement is negated </p>

<p>people who are not from newyork</p>

<p>How to extract negative scenario from this statement, Is there any way possible even outside of stanford NLP</p>

<p>Any help is appreciated</p>
","nlp, stanford-nlp, opennlp, named-entity-extraction","<p>I know 2 possibilities to implement negation relation:</p>

<ul>
<li>Define custom property ""not a ...""  and apply it everywhere.</li>
<li>Use knowledge database, extract LOCATIONs from data, define ""not from <code>smth</code>"" as ""LOCATION is not <code>smth</code>"".</li>
</ul>

<p>I used second approach successfully, but I was able to restrict my domain to finite set of subjects and relations. I found <a href=""http://robotics.usc.edu/~gkoch/DependencyManual.pdf"" rel=""nofollow"">Stanford's typed dependencies</a> incredibly useful, they might help you too (to find those <code>from smth</code> relations).</p>
",1,2,1585,2016-09-14 08:33:03,https://stackoverflow.com/questions/39485888/custom-named-entity-extraction
Stanford Parser - POS tagging - pronoun as noun?,"<p>I noticed that the Stanford Parser taggs ""anyone"" and ""anybody"" as a noun, whereas they are pronouns; I tried to set ""anyone"" in different contexts, I got the same result. Can anyone tell me if it hapenned to him/her and if there is a way to correct it (I mean perhaps some settings ?).</p>

<p>Thank you!</p>
","stanford-nlp, pos-tagger","<p>This is more a linguistics question than a code question. The analysis of these words is complex. Many linguists would describe them as being a fused determiner and noun, as they fairly transparently are at least historically.</p>

<p>In general we currently use the Penn Treebank standards for tokenization, part-of-speech, and phrasal labels for English. The <a href=""https://catalog.ldc.upenn.edu/LDC99T42"" rel=""nofollow"">Penn Treebank</a> annotates these words as noun (NN) - rightly or wrongly - so that is what our tools currently do.</p>

<p>However, you might be pleased to know that under the <a href=""http://universaldependencies.org/"" rel=""nofollow"">Universal Dependencies</a> guidelines, these words are indeed pronouns (PRON). We are sure to be moving towards greater use of Universal Dependencies in future releases.</p>
",0,0,187,2016-09-15 15:17:00,https://stackoverflow.com/questions/39514714/stanford-parser-pos-tagging-pronoun-as-noun
Stanford CoreNLP merge tokens,"<p>I found the powerful <a href=""http://nlp.stanford.edu/software/regexner.html"" rel=""nofollow"">RegexNER</a> and it's superset <a href=""http://nlp.stanford.edu/software/tokensregex.html"" rel=""nofollow"">TokensRegex</a> from Stanford CoreNLP.<br>
There are some rules that should give me fine results, like the pattern for PERSONs with titles:<br>
""g. Meho Mehic"" or ""gdin. N. Neko"" (g. and gdin. are abbrevs in Bosnian for mr.). </p>

<p>I'm having some trouble with existing tokenizer. It splits some strings on two tokens and some leaves as one, for example, token ""g."" is left as word <code>&lt;word&gt;g.&lt;/word&gt;</code> and token ""gdin."" is split on 2 tokens: <code>&lt;word&gt;gdin&lt;/word&gt;</code> and <code>&lt;word&gt;.&lt;/word&gt;</code>.</p>

<p>That causes trouble with my regex, I have to deal with one-token and multi-token cases (note the two ""maybe-dot""s), RegexNER example:</p>

<pre><code>( /g\.?|gdin\.?/ /\./? ([{ word:/[A-Z][a-z]*\.?/ }]+) ) PERSON
</code></pre>

<p>Also, this causes another issue, with sentence splitting, some sentences are not well recognized so regex fails... For example, when a sentence contains ""gdin."" it will split it on two, so a dot will end the (non-existing) sentence. I managed to bypass this with <code>ssplit.isOneSentence = true</code> for now.</p>

<p>Questions:</p>

<ol>
<li>Do I have to make my own tokenizer, and how? (to merge some tokens like ""gdin."")</li>
<li>Are there any settings I missed that could help me with this?</li>
</ol>
","token, stanford-nlp","<p>Ok I thought about this for a bit and can actually think of something pretty straight forward for your case.  One thing you could do is add ""gdin"" to the list of titles in the tokenizer.</p>

<p>The tokenizer rules are in edu.stanford.nlp.process.PTBLexer.flex (look at line 741)</p>

<p>I do not really understand the tokenizer that well, but clearly there are a list of job titles in there, so they must be cases where it will not split off the period.</p>

<p>This will of course require you to work with a custom build of Stanford CoreNLP.  </p>

<p>You can get the full code at our GitHub:<a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP</a></p>

<p>There are instructions on the main page for building a jar with all of the main Stanford CoreNLP classes.  I think if you just run the ant process it will automatically generate the new PTBLexer.java based on PTBLexer.flex.</p>
",1,0,575,2016-09-20 08:12:59,https://stackoverflow.com/questions/39588902/stanford-corenlp-merge-tokens
Compile error in Java corenlp sentiment score program via py4j in Python,"<p>I mainly use Python and new to Java. However I am trying to write a Java program and make it work in Python via Py4j Python package. Following program is what I adapted from an example. I encountered a compile error. Could you shed some light? I am pretty sure it is basic error. Thanks.  </p>

<pre><code>&gt; compile error: incompatible type: SimpleMatrix cannot be converted to String: return senti_scores.
&gt; intended input in Python: 
app = CoreNLPSentiScore()
app.findSentiment(""I like this book"")
intended output: matrix:    Type = dense , numRows = 5 , numCols = 1
0.016  
0.037  
0.132  
0.618  
0.196  

import java.util.List;
import java.util.Properties;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.ArrayCoreMap;
import edu.stanford.nlp.util.CoreMap;
import py4j.GatewayServer;
import org.ejml.simple.SimpleMatrix;


public class CoreNLPSentiScore {
static StanfordCoreNLP pipeline;

    public static void init() {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
        pipeline = new StanfordCoreNLP(props);

    }

    public static void main(String[] args) {
        CoreNLPSentiScore app = new CoreNLPSentiScore();
        // app is now the gateway.entry_point
        GatewayServer server = new GatewayServer(app);
        server.start();
    }

    //public static void main(String tweet) {
    //public static String findSentiment(String tweet) {
    public String findSentiment(String tweet) {
    //String SentiReturn = ""2"";
    //String[] SentiClass ={""very negative"", ""negative"", ""neutral"", ""positive"", ""very positive""};

    //Sentiment is an integer, ranging from 0 to 4. 
    //0 is very negative, 1 negative, 2 neutral, 3 positive and 4 very positive.
    //int sentiment = 2;
        SimpleMatrix senti_score = new SimpleMatrix();
        if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
            Annotation annotation = pipeline.process(tweet);

            List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
            if (sentences != null &amp;&amp; sentences.size() &gt; 0) {

                ArrayCoreMap sentence = (ArrayCoreMap) sentences.get(0);                
                //Tree tree = sentence.get(SentimentAnnotatedTree.class);  
                Tree tree = sentence.get(SentimentAnnotatedTree.class);  
                senti_score = RNNCoreAnnotations.getPredictions(tree);             

                //SentiReturn = SentiClass[sentiment];
            }
        }
        //System.out.println(senti_score);
        return senti_score;
        //System.out.println(senti_score);
    }

}
</code></pre>
","java, python, stanford-nlp, py4j","<p>Java is object oriented program but it is not like python where everything is considered as object. </p>

<p>In your program mentioned above. There is a method findsentiment is returning SimpleMatrix but in the method declaration it is String. </p>

<p>Solution - You can overide a method toString()  in your class SimpleMatrix and while returning  senti_score return senti_score.toString() </p>
",1,0,63,2016-09-22 18:50:04,https://stackoverflow.com/questions/39646779/compile-error-in-java-corenlp-sentiment-score-program-via-py4j-in-python
"corenlp sentiment Java program via Py4j in Python, raises errors","<p>I made a Java sentiment analysis program to be used in Python via Py4j. I can create the Java object in Python, but try to access the method, it gives java.lang.NullPointerException. Could you help please? Thanks.</p>

<p><strong>Java code</strong>: it compiles correctly, runs without errors.</p>

<pre><code>import java.util.List;
import java.util.Properties;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.ArrayCoreMap;
import edu.stanford.nlp.util.CoreMap;
import py4j.GatewayServer;
import org.ejml.simple.SimpleMatrix;


public class CoreNLPSentiScore {
static StanfordCoreNLP pipeline;

    public static void init() {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
        pipeline = new StanfordCoreNLP(props);

    }

    public static void main(String[] args) {
        CoreNLPSentiScore app = new CoreNLPSentiScore();
        // app is now the gateway.entry_point
        GatewayServer server = new GatewayServer(app);
        server.start();
    }

    //public static void main(String tweet) {
    //public static String findSentiment(String tweet) {
    public String findSentiment(String tweet) {
        //String SentiReturn = ""2"";
        //String[] SentiClass ={""very negative"", ""negative"", ""neutral"", ""positive"", ""very positive""};

        //Sentiment is an integer, ranging from 0 to 4. 
        //0 is very negative, 1 negative, 2 neutral, 3 positive and 4 very positive.
        //int sentiment = 2;
        SimpleMatrix senti_score = new SimpleMatrix();
        if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
            Annotation annotation = pipeline.process(tweet);

            List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
            if (sentences != null &amp;&amp; sentences.size() &gt; 0) {

                ArrayCoreMap sentence = (ArrayCoreMap) sentences.get(0);                
                //Tree tree = sentence.get(SentimentAnnotatedTree.class);  
                Tree tree = sentence.get(SentimentAnnotatedTree.class);  
                senti_score = RNNCoreAnnotations.getPredictions(tree);             

                //SentiReturn = SentiClass[sentiment];
            }
        }
        //System.out.println(senti_score);
        return senti_score.toString();
        //System.out.println(senti_score);
    }

}
</code></pre>

<p><strong>Python code</strong>:</p>

<pre><code>from py4j.java_gateway import JavaGateway
gateway = JavaGateway()
app = gateway.entry_point
test = 'i like this product'
app.findSentiment(test)
</code></pre>

<p><strong>NullPointerException</strong></p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-1-cefdf18ada67&gt;"", line 5, in &lt;module&gt;
    app.findSentiment(test)

  File ""C:\Anaconda3\envs\sandbox\lib\site-packages\py4j\java_gateway.py"", line 1026, in __call__
    answer, self.gateway_client, self.target_id, self.name)

  File ""C:\Anaconda3\envs\sandbox\lib\site-packages\py4j\protocol.py"", line 316, in get_return_value
    format(target_id, ""."", name), value)

Py4JJavaError: An error occurred while calling t.findSentiment.: java.lang.NullPointerException
at CoreNLPSentiScore.findSentiment(CoreNLPSentiScore.java:43)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
at py4j.Gateway.invoke(Gateway.java:280)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:214)
at java.lang.Thread.run(Unknown Source)
</code></pre>
","java, python, stanford-nlp, py4j","<p>I believe you forgot to call init().</p>

<p>This seems to be line #43 (see your stack trace):</p>

<pre><code>Annotation annotation = pipeline.process(tweet);
</code></pre>

<p>pipeline is likely to be null because it is instantiated in init() and init() is not called in the main() method or from Python.</p>
",1,0,107,2016-09-22 20:35:43,https://stackoverflow.com/questions/39648414/corenlp-sentiment-java-program-via-py4j-in-python-raises-errors
German corenlp model defaulting to english models,"<p>I use the following command to serve a  corenlp server for German language models which are downloaded as jar in the classpath , but it does not output german tags or parse but loads only english models:</p>

<pre><code> java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer   -props ./german.prop
</code></pre>

<p>german.prop contents:</p>

<pre><code>annotators = tokenize, ssplit, pos, depparse, parse

tokenize.language = de

pos.model = edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger

ner.model = edu/stanford/nlp/models/ner/german.hgc_175m_600.crf.ser.gz
ner.applyNumericClassifiers = false
ner.useSUTime = false

parse.model = edu/stanford/nlp/models/lexparser/germanFactored.ser.gz
depparse.model = edu/stanford/nlp/models/parser/nndep/UD_German.gz
</code></pre>

<p>client command:</p>

<pre><code>wget --post-data ' Meine Mutter ist aus Wuppertal' 'localhost:9000/?properties""=""{""tokenize.whitespace"":""true"",""annotators"":""tokenize, ssplit, pos, depparse, parse"",""outputFormat"":""text"",""tokenize.language"" :""de"" ,
 ""pos.model"":"" edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger"",
""depparse.model"" : ""edu/stanford/nlp/models/parser/nndep/UD_German.gz"",
""parse.model"" : ""edu/stanford/nlp/models/lexparser/germanFactored.ser.gz""

 }' -O -
</code></pre>

<p>I get following incorrect output:</p>

<pre><code> {""dep"":""dep"",""governor"":4,""governorGloss"":""aus"",""dependent"":5,""dependentGloss"":""Wuppertal""}],""openie"":[{""subject"":""Wuppertal"",""subjectSpan"":[4,5],""relation"":""is ist aus of"",""relationSpan"":[2,4],""object"":""Meine Mutter"",""objectSpan"":[0,2]}],""tokens"":[{""index"":1,""word"":""Meine"",""originalText"":""Meine"",""lemma"":""Meine"",""characterOffsetBegin"":1,""characterOffsetEnd"":6,""pos"":""NNP"",""ner"":""PERSON"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":2,""word"":""Mutter"",""originalText"":""Mutter"",""lemma"":""Mutter"",""characterOffsetBegin"":7,""characterOffsetEnd"":13,""pos"":""NNP"",""ner"":""PERSON"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":3,""word"":""ist"",""originalText"":""ist"",""lemma"":""ist"",""characterOffsetBegin"":14,""characterOffsetEnd"":17,""pos"":""NN"",""ner"":""O"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":4,""word"":""aus"",""originalText"":""aus"",""lemma"":""aus"",""characterOffsetBegin"":18,""characterOffsetEnd"":21,""pos"":""NN"",""ner"":""O"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":5,""word"":""Wuppertal"",""originalText"":""Wuppertal"",""lemma"":""Wuppertal"",""characterOffsetBegin"":22,""characterOffsetEnd"":31,""pos"":""NNP"",""ner"":""LOCATI100%[==========================================================================&gt;] 2,
</code></pre>

<p>in the server log I see it loads english models eventhough it lists german models on startup:</p>

<pre><code>pos.model=edu/stanford/nlp/models/pos-tagger/ge...
parse.model=edu/stanford/nlp/models/lexparser/ger...
tokenize.language=de
depparse.model=edu/stanford/nlp/models/parser/nndep/...
annotators=tokenize, ssplit, pos, depparse, parse
Starting server on port 9000 with timeout of 5000 milliseconds.
StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
[/203.:61563] API call w/annotators tokenize,ssplit,pos,depparse
Die Katze liegt auf der Matte.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.5 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ...
PreComputed 100000, Elapsed Time: 1.396 (s)
</code></pre>

<p>The following question for same error in french models also points to the same problem but even after following , it does not resolve the problem for the server case, I am able to get the correct output without using the server and just using the <code>edu.stanford.nlp.pipeline.StanfordCoreNLP command</code> , it is the server command <code>edu.stanford.nlp.pipeline.StanfordCoreNLPServer</code> which defaults to english:
<a href=""https://stackoverflow.com/questions/36223002/french-dependency-parsing-using-corenlp#"">French dependency parsing using CoreNLP</a></p>
","server, nlp, stanford-nlp, stanford-nlp-server","<p>There have been some issues with getting the foreign language stuff to work on the server.</p>

<p>If you use the latest release available at our GitHub site, it should work.</p>

<p>The GitHub site is here: <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP</a></p>

<p>That link has instructions for building a jar with the latest version of the code.</p>

<p>I ran this command on some sample German text and it looks like it works fine:</p>

<pre><code>wget --post-data '&lt;sample german text&gt;' 'localhost:9000/?properties={""pipelineLanguage"":""german"",""annotators"":""tokenize,ssplit,pos,ner,parse"", ""parse.model"":""edu/stanford/nlp/models/lexparser/germanFactored.ser.gz"",""tokenize.language"":""de"",""pos.model"":""edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger"", ""ner.model"":""edu/stanford/nlp/models/ner/german.hgc_175m_600.crf.ser.gz"", ""ner.applyNumericClassifiers"":""false"", ""ner.useSUTime"":""false""}' -O -
</code></pre>

<p>I should note that the neural net German dependency parser is completely broken and we are working on fixing it soon, so you should just use the German settings I specified in that command.</p>

<p>More info on the server can be found here: <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/corenlp-server.html</a></p>
",2,0,868,2016-09-25 15:36:01,https://stackoverflow.com/questions/39688652/german-corenlp-model-defaulting-to-english-models
Why does the property settings for the StanfordCoreNlp pipeline matters when building dependency parse tree?,"<p>I have been playing around with Stanford-CoreNLP and I figured out that building a dependency parse tree with the following code</p>

<pre><code>String text = ""Are depparse and parse equivalent properties for building dependency parse tree?""
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, parse, lemma, ner"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation document = new Annotation(text);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
    SemanticGraph graph = sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);
    System.out.println(graph.toString(SemanticGraph.OutputFormat.LIST ));
}
</code></pre>

<p>is outputting</p>

<pre><code>root(ROOT-0, tree-11)
cop(tree-11, Are-1)
amod(properties-6, depparse-2)
cc(depparse-2, and-3)
conj(depparse-2, parse-4)
compound(properties-6, equivalent-5)
nsubj(tree-11, properties-6)
case(dependency-9, for-7)
compound(dependency-9, building-8)
nmod(properties-6, dependency-9)
amod(tree-11, parse-10)
punct(tree-11, ?-12)
</code></pre>

<p>However this code</p>

<pre><code>String text = ""Are depparse and parse equivalent properties for building dependency parse tree?""
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, lemma, ner, depparse"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation document = new Annotation(text);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
    SemanticGraph graph = sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);
    System.out.println(graph.toString(SemanticGraph.OutputFormat.LIST ));
}
</code></pre>

<p>outputs</p>

<pre><code>root(ROOT-0, properties-6)
cop(properties-6, Are-1)
compound(properties-6, depparse-2)
cc(depparse-2, and-3)
conj(depparse-2, parse-4)
amod(properties-6, equivalent-5)
case(tree-11, for-7)
amod(tree-11, building-8)
compound(tree-11, dependency-9)
amod(tree-11, parse-10)
nmod(properties-6, tree-11)
punct(properties-6, ?-12)
</code></pre>

<p>So why am I not getting the same outputs with those two methods? Is it possible to change the later code to be equivalent to the first code because loading the constituency parser as well makes the parsing so slow? And how would you recommend setting the properties to get the most accurate dependency parse tree?</p>
",stanford-nlp,"<p>The constituency parser (<code>parse</code> annotator) and dependency parser (<code>depparse</code> annotator) are actually completely different models and code paths. In one case, we're predicting a constituency tree and converting it to a dependency graph. In the other case, we are running a dependency parser directly. In general, <code>depparse</code> is expected to be faster (O(n) vs O(n^3)) and more accurate at producing dependency trees, but will not produce constituency trees.</p>
",1,1,185,2016-09-27 23:05:55,https://stackoverflow.com/questions/39735651/why-does-the-property-settings-for-the-stanfordcorenlp-pipeline-matters-when-bui
what will be CNF form of this probabilistic grammar?,"<p>If PCFG is like,</p>

<pre><code>NP -&gt; ADJ N [0.6]
NP -&gt; N     [0.4]
N  -&gt; cat   [0.2]
N  -&gt; dog   [0.8]
</code></pre>

<p>What will be CNF form? Will it be the following?</p>

<pre><code>NP -&gt; ADJ NP [0.6]
NP -&gt; cat    [0.08]
NP -&gt; dog    [0.32]
</code></pre>

<p>or somethings else?</p>
","nlp, stanford-nlp, chomsky-normal-form, cnf","<pre><code>NP -&gt; ADJ NP [0.6]
NP -&gt; cat    [0.08]
NP -&gt; dog    [0.32]
</code></pre>

<p>Your answer is correct because you need to get the same probability for the result by applying both the original and the converted set of rules (in CNF).</p>
",1,1,282,2016-09-29 11:32:33,https://stackoverflow.com/questions/39769119/what-will-be-cnf-form-of-this-probabilistic-grammar
NoClassDefFoundError StanfordCoreNLP,"<p>I have this problem when i tired to submit topology in <code>local mode</code> ! </p>

<pre><code>ERROR backtype.storm.util - Async loop died!
java.lang.NoClassDefFoundError: edu/stanford/nlp/pipeline/StanfordCoreNLP
at edu.stanford.nlp.pipeline.NLP.init(NLP.java:16) ~[classes/:na]

Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.pipeline.StanfordCoreNLP
at java.net.URLClassLoader$1.run(URLClassLoader.java:217) ~[na:1.6.0_38]
at java.security.AccessController.doPrivileged(Native Method) ~[na:1.6.0_38]
at java.net.URLClassLoader.findClass(URLClassLoader.java:205) ~[na:1.6.0_38]
at java.lang.ClassLoader.loadClass(ClassLoader.java:323) ~[na:1.6.0_38]
at java.lang.ClassLoader.loadClass(ClassLoader.java:268) ~[na:1.6.0_38]
... 20 common frames omitted
</code></pre>

<p>i don't know what can i share to help you to fix it ! please tell me .
I'm using <code>stanford 3.4.1</code> with <code>java 1.6</code> </p>
","java, nlp, stanford-nlp, apache-storm, sentiment-analysis","<p>Make sure the Stanford NLP libraries are on classpath.</p>

<p>If you are running your project using Eclipse, this link might help you - <a href=""https://stackoverflow.com/questions/9528080/error-in-stanford-nlp-core"">Error in stanford nlp core</a></p>
",2,1,1935,2016-09-30 11:48:10,https://stackoverflow.com/questions/39790837/noclassdeffounderror-stanfordcorenlp
sentence index for getOpenIE method in coreNLP for R,"<p>I' using the R wrapper for Stanford's CoreNLP tools, specifically the <code>getOpenIE</code> method in order to extract relation triples. This appears to work fine but I'm a bit confused about the output. Whereas <code>getCoreference</code> returns a dataframe with a sentence column <code>getOpenIE</code> does not and <code>subject_start</code> and <code>subject_end</code> etc. seem to be in-sentence references. 
How can I determine the exact position of those elements in a document?</p>
","r, nlp, stanford-nlp","<p>You want to look at the XML output.  All the getOpenIE/getCoreference functions do is parse the XML anyway.  I had to edit those functions as well to get sentence information.</p>
",0,0,89,2016-10-04 07:17:02,https://stackoverflow.com/questions/39846346/sentence-index-for-getopenie-method-in-corenlp-for-r
How can I convert PCFG in CNF for this grammar?,"<p>Given the following probabilistic context-free grammar - </p>

<pre><code>1.NP -&gt; ADJ N [0.6]
2.NP -&gt; N     [0.4] 
3.N  -&gt; cat   [0.2] 
4.N  -&gt; dog   [0.8]
</code></pre>

<p>what will be the CNF??</p>
","nlp, stanford-nlp, cnf","<p>Given PCFG in CNF is given below.</p>

<pre><code>1.NP -&gt; ADJ N [0.6]
2.NP -&gt; cat   [0.08] 
3.NP -&gt; dog   [0.32] 
</code></pre>

<p>Because you need to get the same probability for the result by applying both the original and the converted set of rules (in CNF).</p>
",3,1,1024,2016-10-04 11:13:56,https://stackoverflow.com/questions/39850826/how-can-i-convert-pcfg-in-cnf-for-this-grammar
BioNLP stanford - tokenization,"<p>I try to tokenize a biomedical text so I decided to use <a href=""http://nlp.stanford.edu/software/eventparser.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/eventparser.shtml</a>. I used the stand-alone program RunBioNLPTokenizer that does what I want.</p>

<p>Now, I want to create my own program that uses Stanford libraries. So, I read the code from RunBioNLPTokenizer describing below.</p>

<pre><code>package edu.stanford.nlp.ie.machinereading.domains.bionlp;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.PrintStream;
import java.util.Collection;
import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.ie.machinereading.GenericDataSetReader;
import edu.stanford.nlp.ie.machinereading.msteventextractor.DataSet;
import edu.stanford.nlp.ie.machinereading.msteventextractor.EpigeneticsDataSet;
import edu.stanford.nlp.ie.machinereading.msteventextractor.GENIA11DataSet;
import edu.stanford.nlp.ie.machinereading.msteventextractor.InfectiousDiseasesDataSet;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.util.StringUtils;

/**
 * Standalone program to run our BioNLP tokenizer and save its output
 */
public class RunBioNLPTokenizer extends GenericDataSetReader {

  public static void main(String[] args) throws IOException {
    Properties props = StringUtils.argsToProperties(args);
    String basePath = props.getProperty(""base.directory"", ""/u/nlp/data/bioNLP/2011/originals/"");

    DataSet dataset = new GENIA11DataSet();
    dataset.getFilesystemInformation().setTokenizer(""stanford"");
    runTokenizerForDirectory(dataset, basePath + ""genia/training"");
    runTokenizerForDirectory(dataset, basePath + ""genia/development"");
    runTokenizerForDirectory(dataset, basePath + ""genia/testing"");

    dataset = new EpigeneticsDataSet();
    dataset.getFilesystemInformation().setTokenizer(""stanford"");
    runTokenizerForDirectory(dataset, basePath + ""epi/training"");
    runTokenizerForDirectory(dataset, basePath + ""epi/development"");
    runTokenizerForDirectory(dataset, basePath + ""epi/testing"");

    dataset = new InfectiousDiseasesDataSet();
    dataset.getFilesystemInformation().setTokenizer(""stanford"");
    runTokenizerForDirectory(dataset, basePath + ""infect/training"");
    runTokenizerForDirectory(dataset, basePath + ""infect/development"");
    runTokenizerForDirectory(dataset, basePath + ""infect/testing"");
  }

  private static void runTokenizerForDirectory(DataSet dataset, String path) throws IOException {
    System.out.println(""Input directory: "" + path);
    BioNLPFormatReader reader = new BioNLPFormatReader();    
    for (File rawFile : reader.getRawFiles(path)) {
      System.out.println(""Input filename: "" + rawFile.getName());
      String rawText = IOUtils.slurpFile(rawFile);

      String docId = rawFile.getName().replace(""."" + BioNLPFormatReader.TEXT_EXTENSION, """");
      String parentPath = rawFile.getParent();

      runTokenizer(dataset.getFilesystemInformation().getTokenizedFilename(parentPath, docId), rawText);
    }
  }

  private static void runTokenizer(String tokenizedFilename, String text) {
    System.out.println(""Tokenized filename: "" + tokenizedFilename);
    Collection&lt;String&gt; sentences = BioNLPFormatReader.splitSentences(text);

    PrintStream os = null;
    try {
      os = new PrintStream(new FileOutputStream(tokenizedFilename));
    } catch (IOException e) {
      System.err.println(""ERROR: cannot save online tokenization to "" + tokenizedFilename);
      e.printStackTrace();
      System.exit(1);
    }

    for (String sentence : sentences) {
      BioNLPFormatReader.BioNLPTokenizer tokenizer = new BioNLPFormatReader.BioNLPTokenizer(sentence);
      List&lt;CoreLabel&gt; tokens = tokenizer.tokenize();
      for (CoreLabel l : tokens) {
        os.print(l.word() + "" "");
      }
      os.println();
    }
    os.close();
  }
}
</code></pre>

<p>I wrote the below code. I achieved to split the text into sentences but I can't use the BioNLPTokenizer as it is used in RunBioNLPTokenizer.</p>

<pre><code>public static void main(String[] args) throws Exception {
  // TODO code application logic here
  Collection&lt;String&gt; c =BioNLPFormatReader.splitSentences("".."");
  for (String sentence : c) {
    System.out.println(sentence);
    BioNLPFormatReader.BioNLPTokenizer x = BioNLPFormatReader.BioNLPTokenizer(sentence);
  }
} 
</code></pre>

<p>I took this error</p>

<blockquote>
  <p>Exception in thread ""main"" java.lang.RuntimeException: Uncompilable source code - edu.stanford.nlp.ie.machinereading.domains.bionlp.BioNLPFormatReader.BioNLPTokenizer has protected access in edu.stanford.nlp.ie.machinereading.domains.bionlp.BioNLPFormatReader</p>
</blockquote>

<p>My question is. How can I tokenize a biomedical sentence according to Stanford libraries without using RunBioNLPTokenizer?</p>
","java, nlp, stanford-nlp","<p>Unfortunately, we made <code>BioNLPTokenizer</code> a <code>protected</code> inner class, so you'd need to edit the source and change its access to <code>public</code>.</p>

<p>Note that <code>BioNLPTokenizer</code> may not be the most general purpose biomedical sentence tokenzier -- I would spot check the output to make sure it is reasonable. We developed it heavily against the BioNLP 2009/2011 shared tasks.</p>
",0,0,189,2016-10-05 10:23:34,https://stackoverflow.com/questions/39871431/bionlp-stanford-tokenization
Noun-mediated relationships not being found in OpenIE,"<p>I'm having difficulty extracting noun-mediated relationships as outlined in <a href=""http://nlp.stanford.edu/pubs/2015angeli-openie.pdf"" rel=""nofollow"">Angeli et al</a>. </p>

<p>When I run OpenIE locally with the input ""US president Barack Obama traveled to India on Monday"" only two relationships are extracted:</p>

<ul>
<li>(US president Barack Obama, traveled on, Monday)</li>
<li>(US president Barack Obama, traveled to, India)</li>
<li><strong>Not found but expected</strong>: (Barack Obama, is president of, US)</li>
</ul>

<p>However, when I run the same input at <a href=""http://corenlp.run/"" rel=""nofollow"">http://corenlp.run/</a>, that third relationship looks to be extracted. Even more interestingly though, if I remove ""Named Entities"" as a possible annotator from corenlp.run, that third relationship is no longer found.</p>

<p>So I guess my question is what is the proper configuration (versions, models, annotators...) needed to properly extract noun-mediated relationships? On my local machine I downloaded v3.6.0, compiled the latest source code from the Master branch on GitHub, and then replaced stanford-corenlp-3.6.0.jar with the previously complied jar file. I then ran the following command from within the v3.6.0 folder:</p>

<pre><code>java -mx1g -cp ""*"" edu.stanford.nlp.naturalli.OpenIE -format ollie
</code></pre>

<p>Any help or insight would be a big help. Thanks so much!</p>
","stanford-nlp, information-extraction","<p>So, the current heuristics in the OpenIE system for extracting these relationships is to only extract them when named entity information is present (which we disable by default to improve speed), or else we drastically over-produce them. You can force-enable them with the flag <code>-triple.all_nominals</code>, but you've been warned :). The other easy option is to set the <code>-resolve_coref</code> flag, which will (1) run and resolve coreference when producing triples, but also (2) implicitly run the NER annotator. The last option is to specify the annotators directly to include NER:</p>

<pre><code>java -mx1g -cp ""*"" edu.stanford.nlp.naturalli.OpenIE -annotators ""tokenize,ssplit,pos,lemma,depparse,ner,natlog,openie"" -format ollie
</code></pre>

<p>Lastly, if you're using the 3.6.0 release, that's now fairly out of date. You're likely to get better results from the HEAD of the GitHub repo -- this is roughly what <a href=""http://corenlp.run"" rel=""nofollow"">corenlp.run</a> tracks.</p>
",1,0,126,2016-10-07 00:51:27,https://stackoverflow.com/questions/39907763/noun-mediated-relationships-not-being-found-in-openie
Extracting specific information from data,"<p>How can i convert a data format like: </p>

<pre><code>James Smith was born on November 17, 1948
</code></pre>

<p>into something like </p>

<pre><code>(""James Smith"", DOB, ""November 17, 1948"")
</code></pre>

<p>without having to rely on positional index of strings</p>

<p>I have tried the following </p>

<pre><code>from nltk import word_tokenize, pos_tag

new = ""James Smith was born on November 17, 1948""
sentences = word_tokenize(new)
sentences = pos_tag(sentences)
grammar = ""Chunk: {&lt;NNP*&gt;&lt;NNP*&gt;}""
cp = nltk.RegexpParser(grammar)
result = cp.parse(sentences)
print(result)
</code></pre>

<p>How to proceed further to get the output in desired fromat.</p>
","python, python-3.x, nltk, stanford-nlp, information-retrieval","<p>You could always use a regular expressions.
The regex <code>(\S+)\s(\S+)\s\bwas born on\b\s(\S+)\s(\S+),\s(\S+)</code> will match and return data from specifically the string format above.</p>

<p>Here's it in action: <a href=""https://regex101.com/r/W2ykKS/1"" rel=""nofollow"">https://regex101.com/r/W2ykKS/1</a></p>

<p>Regex in python:</p>

<pre><code>import re

regex = r""(\S+)\s(\S+)\s\bwas born on\b\s(\S+)\s(\S+),\s(\S+)""
test_str = ""James Smith was born on November 17, 1948""

matches = re.search(regex, test_str)

# group 0 in a regex is the input string

print(matches.group(1)) # James
print(matches.group(2)) # Smith
print(matches.group(3)) # November
print(matches.group(4)) # 17
print(matches.group(5)) # 1948
</code></pre>
",1,2,114,2016-10-08 03:06:32,https://stackoverflow.com/questions/39928277/extracting-specific-information-from-data
Using openIE to extract negation,"<p>I am trying to test OpenIE with Stanford CoreNLP
<a href=""http://nlp.stanford.edu/software/openie.html"" rel=""nofollow"">http://nlp.stanford.edu/software/openie.html</a></p>

<p>I am using the following code based on one of the demos available on <a href=""http://stanfordnlp.github.io/CoreNLP/openie.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/openie.html</a></p>

<pre><code>public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
Properties props = new Properties();

props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
props.setProperty(""openie.triple.strict"", ""false"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
// Annotate an example document.
//File inputFile = new File(""src/test/resources/0.txt"");
//String text = Files.toString(inputFile, Charset.forName(""UTF-8""));
String text = ""Cats do not drink milk."";
Annotation doc = new Annotation(text);
pipeline.annotate(doc);

// Loop over sentences in the document
for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
  // Get the OpenIE triples for the sentence
  Collection&lt;RelationTriple&gt; triples = sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
  // Print the triples
  for (RelationTriple triple : triples) {
    System.out.println(triple.confidence + ""|\t"" +
        triple.subjectLemmaGloss() + ""|\t"" +
        triple.relationLemmaGloss() + ""|\t"" +
        triple.objectLemmaGloss());
  }
}
</code></pre>

<p>}</p>

<p>This counter-intuitively results in the triple </p>

<pre><code>1.0|    cat|    drink|  milk
</code></pre>

<p>being extracted, which is the same result I get using input text ""Cats drink milk."" If I set ""openie.triple.strict"" to ""true"" no triples are extracted at all. Is there a way to extract a triple like cats | do not drink | milk ?</p>
","stanford-nlp, information-extraction","<p>I think you want to set ""openie.triple.strict"" to true to ensure logically warranted triples.  OpenIE does not extract negative relations, it is only designed to find positive ones.  </p>

<p>So you are getting the correct behavior when ""openie.triple.strict"" is set to true (i.e. no relation being extracted).  Note that a relation is extracted for ""Cats drink milk."" when ""openie.triple.strict"" is set to true.</p>
",1,1,736,2016-10-10 10:24:57,https://stackoverflow.com/questions/39956148/using-openie-to-extract-negation
Spark 2.0.1 write Error: Caused by: java.util.NoSuchElementException,"<p>I am trying to attach sentiment value to each message and I have downloaded all stanford core jar files as dependencies:</p>

<pre><code>import sqlContext.implicits._
import com.databricks.spark.corenlp.functions._
import org.apache.spark.sql.functions._

val version = ""3.6.0""
val model = s""stanford-corenlp-$version-models-english"" //
val jars = sc.listJars
if (!jars.exists(jar =&gt; jar.contains(model))) {
import scala.sys.process._
s""wget http://repo1.maven.org/maven2/edu/stanford/nlp/stanford-         
corenlp/$version/$model.jar -O /tmp/$model.jar"".!!
sc.addJar(s""/tmp/$model.jar"")}

val all_messages = spark.read.parquet(""/home/ubuntu/messDS.parquet"")

case class AllMessSent (user_id: Int, sent_at: java.sql.Timestamp, message:    
String)

val messDS = all_messages.as[AllMess]
</code></pre>

<p>Up to this point everything is fine as I can perform computations and save that DS</p>

<pre><code>case class AllMessSentiment = (user_id: Int, sent_at:   
java.sql.Timestamp, message: String, sentiment: Int)

val output = messDS
.select('user_id,'message,'sent_at,
sentiment('message).as('sentiment)).as[AllMessSentiment])

import java.util

output.write.parquet(""/home/ubuntu/AllMessSent.parquet"")
</code></pre>

<p>I can output results as: <code>output.show(truncate = false)</code> where I can see the sentiment score but when writing to csv or parquet the error comes as below, does anyone know how to solve it?:</p>

<pre><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 9, localhost): java.util.NoSuchElementException
at java.util.ArrayList$Itr.next(ArrayList.java:854)
at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43)
at scala.collection.IterableLike$class.head(IterableLike.scala:107)
at scala.collection.AbstractIterable.head(Iterable.scala:54)
at com.databricks.spark.corenlp.functions$$anonfun$sentiment$1.apply(functions.scala:163)
at com.databricks.spark.corenlp.functions$$anonfun$sentiment$1.apply(functions.scala:158)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:85)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException


 at java.util.ArrayList$Itr.next(ArrayList.java:854)

at java.util.ArrayList$Itr.next(ArrayList.java:854)
at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43)
at   
scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43)
at scala.collection.IterableLike$class.head(IterableLike.scala:107)
at scala.collection.AbstractIterable.head(Iterable.scala:54)
at  
com.databricks.spark.corenlp.
functions$$anonfun$sentiment$1.apply(functions.scala:163)
at com.databricks.spark.corenlp.
functions$$anonfun$sentiment$1.apply(functions.scala:158)
at org.apache.spark.sql
.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown   
Source)
at org.apache.spark.sql.execution
.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at  org.apache.spark.sql.execution.
WholeStageCodegenExec$$anonfun$8$$anon$1
.hasNext(WholeStageCodegenExec.scala:370)
at org.apache.spark.sql.execution.datasources.
DefaultWriterContainer$$anonfun$writeRows$1
.apply$mcV$sp(WriterContainer.scala:253)
at org.apache.spark.sql.execution.datasources
.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
 at  org.apache.spark.sql.execution.datasources          
 .DefaultWriterContainer$$anonfun$writeRows$1.
 apply(WriterContainer.scala:252)
 at org.apache.spark.util.Utils$
 .tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)
 at org.apache.spark.sql.execution.datasources
.DefaultWriterContainer.writeRows(WriterContainer.scala:258)
 ... 8 more
</code></pre>
","java, scala, apache-spark, stanford-nlp","<p>I was able to run the algorithm when all messages were splitted into sentences and cleaned of special characters and empty spaces. </p>
",0,0,1454,2016-10-11 17:48:42,https://stackoverflow.com/questions/39983486/spark-2-0-1-write-error-caused-by-java-util-nosuchelementexception
OpenNLP vs Stanford CoreNLP,"<p>I've been doing a little comparison of these two packages and am not sure which direction to go in. What I am looking for briefly is:</p>

<ol>
<li>Named Entity Recognition (people, places, organizations and such).</li>
<li>Gender identification.</li>
<li>A decent training API.</li>
</ol>

<p>From what I can tell, OpenNLP and Stanford CoreNLP expose pretty similar capabilities. However, Stanford CoreNLP looks like it has a lot more activity whereas OpenNLP has only had a few commits in the last six months.</p>

<p>Based on what I saw, OpenNLP appears to be easier to train new models and might be more attractive for that reason alone. However, my question is what would others start with as the basis for adding NLP features to a Java app? I'm mostly worried as to whether OpenNLP is ""just mature"" versus semi-abandoned.</p>
","java, stanford-nlp, opennlp","<p>In full disclosure, I'm a contributor to CoreNLP, so this is a biased answer. But, in my view on your three criteria:</p>

<ol>
<li><p>Named Entity Recognition: I think CoreNLP clearly wins here, both on accuracy and ease-of-use. For one, OpenNLP has a model per NER tag, whereas CoreNLP detects all tags with a single Annotator. Furthermore, temporal resolution with SUTime is a nice perk in CoreNLP. Accuracy-wise, my anecdotal experience is that CoreNLP does better on general-purpose text.</p></li>
<li><p>Gender identification. I think both tools are kind of poorly documented on this front. OpenNLP seems to have a GenderModel class; CoreNLP has a gender Annotator.</p></li>
<li><p>Training API. I suspect the OpenNLP training API is easier-to-use for not off-the-shelf training. But, if all you want to do is, e.g., train a model from a CoNLL file, both should be straightforward. Training speed tends to be faster with CoreNLP than other tools I've tried, but I haven't benchmarked it formally, so take that with a grain of salt.</p></li>
</ol>
",15,16,13266,2016-10-13 16:08:58,https://stackoverflow.com/questions/40025981/opennlp-vs-stanford-corenlp
Stanford CoreNLP - How to setup another language,"<p>I am trying to setup my NLP parser using the stanford library.
On the website I downloaded </p>

<ul>
<li>stanford-corenlp-full-2015-12-09.zip</li>
<li>standford-french-corenlp-2016-01-14-models.jar</li>
</ul>

<p>Now I am facing a problem, how can I indicate my app to use the French model to analyse my sentence.</p>

<p>I actually have this code (working for English sentences)</p>

<pre><code>String text = ""I am very sad"";
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse, sentiment"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    Annotation annotation = pipeline.process(text);
    List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
    for (CoreMap sentence : sentences) {
        String sentiment = sentence.get(SentimentCoreAnnotations.SentimentClass.class);
        System.out.println(sentiment + ""\t"" + sentence);
    }
</code></pre>

<p>Is there a way to indicate in the code that I want the French model (and try to parse a sentence like ""Bonjour, je m'appelle Jean"".</p>

<p>Thanks,
Alexi</p>
","java, nlp, stanford-nlp","<p>The solution is to add the standford french .jar file in the classpath.</p>

<p>Following code is working</p>

<pre><code>String sampleFrenchText = ""Le chat mange la souris"";
Annotation frenchAnnotation = new Annotation(sampleFrenchText);
Properties frenchProperties = StringUtils.argsToProperties(new String[]{""-props"", ""StanfordCoreNLP-french.properties""});
StanfordCoreNLP pipeline = new StanfordCoreNLP(frenchProperties);
pipeline.annotate(frenchAnnotation);
for (CoreMap sentence : frenchAnnotation.get(CoreAnnotations.SentencesAnnotation.class)) {
    Tree sentenceTree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(sentenceTree);
}
</code></pre>
",2,1,1280,2016-10-16 08:36:43,https://stackoverflow.com/questions/40068445/stanford-corenlp-how-to-setup-another-language
Coreference Resolution by CoreNLP CorefChainAnnotation.class not working,"<p>I am using core nlp library to find coreference in my text </p>

<blockquote>
  <p>Tyson lives in New York City with his wife and their two children.</p>
</blockquote>

<p>when I am running this on <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"">Stanford CoreNLP Online demo</a> it's giving me correct <a href=""https://i.sstatic.net/1Yjdy.png"" rel=""nofollow"">output</a></p>

<p>but when I run this text on my machine it's returning null on this line of code</p>

<blockquote>
  <p>Map graph = document.get(CorefChainAnnotation.class);</p>
</blockquote>

<p>Thank you </p>
","parsing, semantic-web, text-mining, stanford-nlp","<p>Look into this complete example - <a href=""http://blog.pengyifan.com/resolve-coreference-using-stanford-corenlp/"" rel=""nofollow noreferrer"">http://blog.pengyifan.com/resolve-coreference-using-stanford-corenlp/</a>. I guess you are missing something as i am unable to understand the exact reason from the code you provided.</p>
",0,0,139,2016-10-18 13:07:30,https://stackoverflow.com/questions/40109181/coreference-resolution-by-corenlp-corefchainannotation-class-not-working
Why the definition of edit Distance algorithm in Stanford NLP course plus 2 not 1,"<p>I am studying the Stanford NLP course by the following slides:<a href=""https://web.stanford.edu/class/cs124/lec/med.pdf"" rel=""nofollow"">https://web.stanford.edu/class/cs124/lec/med.pdf</a>.
The definition of edit Distance algorithm in this slide as following:</p>

<p>Initialization</p>

<pre><code>D(i,0) = i
D(0,j) = j
</code></pre>

<p>Recurrence Relation:</p>

<pre><code> For each  i = 1…M
    For each  j = 1…N


       D(i,j)= min  {D(i-1,j) + 1, D(i,j-1) + 1, 
                     D(i-1,j-1) +   2(if X(i) ≠ Y(j) )  
                                    0(if X(i) = Y(j))}
</code></pre>

<p>why D(i-1,j-1) + 2 not (+1), if X(i) ≠ Y(j). 
I found the definition of edit Distance algorithm in the wikipedia is '+1':<a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow"">https://en.wikipedia.org/wiki/Levenshtein_distance</a>.
Could you guys explain the difference of these two definition, please. I am a new guy to NLP. Thanks!</p>
","algorithm, stanford-nlp, levenshtein-distance","<blockquote>
  <p>When editing one string, what is the minimal number of changes you need to do in order to get another string?</p>
</blockquote>

<p>This is a general, not specific, definition for editing distance. To get a precise definition, you need to define what a ""change"" is.</p>

<ul>
<li>If a ""change"" includes ""replacing one letter by another"", you have +1 in your definition</li>
<li>If a ""change"" doesn't include ""replacing one letter by another"", you have +2 in your definition</li>
</ul>

<p>Example: how many changes do you need to turn <code>feh</code> into <code>fah</code>?</p>

<ul>
<li>One change - just replace <code>e</code> by <code>a</code></li>
<li>Two changes - remove <code>e</code>; then add <code>a</code> in the same place</li>
</ul>

<p>Both answers are useful, and lead to two slightly different definitions of editing distance.</p>
",4,0,201,2016-10-20 09:24:49,https://stackoverflow.com/questions/40150444/why-the-definition-of-edit-distance-algorithm-in-stanford-nlp-course-plus-2-not
pattens in regexNER in core-nlp,"<p>I would like to have a regex pattenr for regexner inside the core-nlp pipeline. my entity/token is </p>

<pre><code>Machine_DS2302
</code></pre>

<p>Where the second part is <code>alphanumeric</code>.</p>

<p>What I have currently is </p>

<pre><code>Machine_.*  MachineNumber
</code></pre>

<p>But, this annotates everything (this is being a wildcard). I would like to add the tag as <code>MachineNumber</code> based on the the <strong>regex</strong> in the second part i.e. if the second part after _ is a number, then assign it the said tag. </p>

<p>The regex pattern </p>

<pre><code>^[a-zA-Z0-9]*$
</code></pre>

<p>But even </p>

<pre><code>Machine_^[a-zA-Z0-9]*$
</code></pre>

<p>Does not work</p>

<p>How would such a pattern look like for the regexNER?</p>
","regex, named-entity-recognition, stanford-nlp","<p>The anchors are redundant, they actually prevent the pattern from matching because <code>^</code> matches a string start location and <code>$</code> matches the string end location.</p>

<p>Since you need to have access to the part after <code>_</code>, you need to also capture, so use a capturing group:</p>

<pre><code>Machine_([a-zA-Z0-9]*)
</code></pre>

<p>The <code>(...)</code> will create a submatch with the alphanumeric value.</p>

<p>Note that you might want to replace <code>*</code> with <code>+</code> if the alphanumeric part should consist of at least 1 char.</p>
",1,1,469,2016-10-21 10:48:10,https://stackoverflow.com/questions/40174663/pattens-in-regexner-in-core-nlp
Stanford NLP pipeline – sequential processing (in Java),"<p>How to correctly use <strong>Stanford NLP pipeline</strong> for two-phase annotation?</p>

<hr>

<p>In the <strong>first phase</strong> I need only <strong>tokenization</strong> and <strong>sentence splitting</strong>, so I use this code:</p>

<pre><code>private Annotation annotatedDocument = null;
private StanfordCoreNLP pipeline = null;

...

public void firstPhase() {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit"");

        pipeline = new StanfordCoreNLP(props);
        annotatedDocument = new Annotation(textDocument);
}
</code></pre>

<p>The <strong>second phase</strong> is optional, so I don't use all annotator in the first phase. The second phase code:</p>

<pre><code>public void secondPhase() {
    POSTaggerAnnotator posTaggerAnot = new POSTaggerAnnotator();
    posAnot.annotate(annotatedDocument);

    // Lemmatization
    MorphaAnnotator morphaAnot = new MorphaAnnotator();
    morphaAnot.annotate(annotatedDocument);
}
</code></pre>

<hr>

<p><strong>First question</strong>: Is this approach using ""stand-alone"" annotators in the second phase correct? Or is there a way to use existing pipeline?</p>

<p><strong>Second question</strong>: I have problem with Correference annotator. I would like use it in the second phase as follow:</p>

<pre><code>CorefAnnotator coref = new CorefAnnotator(new Properties());
</code></pre>

<p>But this constructor seems to be never ending. Constructor without properties doesn't exist, right? Is it some properties setting necessary?</p>
","java, stanford-nlp","<p>There are [at least] 3 ways you can do this:</p>

<ol>
<li><p>The way you described. It's perfectly valid to just call individual annotators, and chain them together. The coref annotator should work with empty properties -- perhaps you need more memory? It's a bit slow to load, and the models are not small.</p></li>
<li><p>If you want to keep using a pipeline, you can create a partial pipeline and set the property <code>enforceRequirements=false</code>. This will do the chaining of annotators for you, but doesn't require their requirements to be satisfied -- i.e., if you know some annotations are already there, you don't have to re-run their corresponding annotators.</p></li>
<li><p>This is a bigger change, but the <a href=""http://stanfordnlp.github.io/CoreNLP/simple.html"" rel=""nofollow"">simple api</a> actually does this sort of lazy evaluation automatically. So, you can just create a <code>Document</code> object, and when you request various annotations, it'll lazily fault them in.</p></li>
</ol>
",1,1,227,2016-10-22 20:52:56,https://stackoverflow.com/questions/40196947/stanford-nlp-pipeline-sequential-processing-in-java
How to get the Stanford-style parse tree (with &quot;noun phrases&quot; and &quot;verb phrases&quot;) in spaCy?,"<p>spaCy provides POS tagging and dependency trees. Is it possible to get what Stanford calls the ""Parse"" tree from it? The difference between these two trees can be seen at the Stanford parser demo at <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/index.jsp</a></p>

<p>Stanford ""Parse"" tree:</p>

<pre><code>(ROOT
  (S
    (NP (NNP John))
    (VP (VBZ likes)
      (NP (PRP him)))
    (. .)))
</code></pre>

<p>Dependency tree: (Provided by spaCy and Stanford parser)</p>

<pre><code>nsubj(likes-2, John-1)
root(ROOT-0, likes-2)
dobj(likes-2, him-3)
</code></pre>

<p>Is it possible to deduce or directly get the parse tree in spaCy?
I have gone through the documentation and I couldn't find any direct API's.</p>
","nlp, stanford-nlp, spacy","<p>Your terminology is a little confused, although it's largely Stanford's fault for its slightly confusing use of terms. A ""parse tree"" is any tree-based representation of a sentence, including both examples you've given above (i.e. a ""dependency tree"" is a kind of parse tree). The kind of tree that you want to get is called a ""constituency tree""; the difference between them is described at <a href=""https://stackoverflow.com/q/10401076/1709587"">Difference between constituency parser and dependency parser</a>.</p>

<h3>Constituency tree</h3>

<pre><code>(ROOT
  (S
    (NP (NNP John))
    (VP (VBZ likes)
      (NP (PRP him)))
    (. .)))
</code></pre>

<h3>Dependency tree</h3>

<pre><code>nsubj(likes-2, John-1)
root(ROOT-0, likes-2)
dobj(likes-2, him-3)
</code></pre>

<p>Unfortunately, spaCy doesn't yet support constituency parsing. They want to eventually - <a href=""https://github.com/explosion/spaCy/issues/59"" rel=""nofollow noreferrer"">there's an open issue</a> - but as of right now the feature doesn't exist.</p>
",2,2,2166,2016-10-27 06:52:12,https://stackoverflow.com/questions/40278029/how-to-get-the-stanford-style-parse-tree-with-noun-phrases-and-verb-phrases
Start/end of regex in TokensRegex,"<p>Suppose I have the following code:</p>

<pre><code>TokenSequencePattern p = TokenSequencePattern.compile(""[{tag:/JJ.*/}] [{tag:/NN.*/}]"");
TokenSequenceMatcher m = tPattern.getMatcher(coreLabelList);
while (tMatcher.find()){
    List&lt;CoreMap&gt; matches = m.groupNodes();
}
</code></pre>

<p>What I would like to capture here is an adjective followed by a noun, i.e. it must start with one adjective and it must end with one noun. For example, if I have ""beautiful scarf"" it should be a match, but if I have ""beautiful scarf with white dots"" it shouldn't be a match. For now, the token regex from above is a match for both of the phrases. How do I specify the exact start of a sequence and it's exact end?</p>
","java, regex, stanford-nlp","<p>You may use</p>

<pre><code>TokenSequencePattern p = TokenSequencePattern.compile(""[tag:/JJ.*/] [tag:/NN.*/]"");
</code></pre>

<p>Testing with <code>A round ball is bouncing very high in to the blue sky.</code> yielded <code>round ball</code> and <code>blue sky</code> substrings.</p>

<p>To only get <em>an entire string match</em>, you need to use <em>anchors</em> if you want to use <code>Matcher#find()</code> (with <code>Matcher#matches()</code>, the anchors are implied).</p>

<p>So, to only match <code>round ball</code> string as a combination of an adjective and a noun, you may use</p>

<pre><code>TokenSequencePattern p = TokenSequencePattern.compile(""^[tag:/JJ.*/] [tag:/NN.*/]$"");
</code></pre>

<p>or</p>

<pre><code>TokenSequencePattern p = TokenSequencePattern.compile(""\\A[tag:/JJ.*/] [tag:/NN.*/]\\z"");
</code></pre>

<p>The <code>^</code> / <code>\A</code> stand for the beginning of a string (also, <code>\A</code> will always match at the beginning of a string) and  <code>$</code> / <code>\z</code> match the end of  a string (note that <code>\z</code> will always match the very end of the string while <code>$</code> - even if you are not using a multiline modifier - allows a trailing newline after it).</p>

<p><strong>Note:</strong> the anchors are tested on CoreNLP 3.7.0. They don't work on some versions (e.g. don't work on CoreNLP 3.5.1, it throws an error: <code>Lexical error at line 1, column 1.  Encountered: ""^"" (94), after : """"</code>)</p>
",1,1,247,2016-10-31 14:13:41,https://stackoverflow.com/questions/40343966/start-end-of-regex-in-tokensregex
"Stanford NLP Text Classifier, Custom Features and Confusion Matrix","<p>I using Stanford NLP Text Classifier (ColumnDataClassifier) from my Java code. I have two main questions.</p>

<p>1-) How do I print more detailed evaluation information such as a confusion matrix.</p>

<p>2-) My code already, does the pre-processing and extracts numeric features (vectors) for terms, such as binary features or TF-IDF values. How can I use those features to train and test the classifier.</p>
","stanford-nlp, text-classification","<ol>
<li><p>I asked a related question in <a href=""https://stackoverflow.com/questions/36361348/stanford-classifier-cross-validation-averaged-or-aggregate-metrics"">here</a>. <code>ColumnDataClassifier</code> does not have an option to output the metrics in a confusion matrix. However, if you look at the code in at <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/classify/ColumnDataClassifier.java"" rel=""nofollow noreferrer"">ColumnDataClassifier.java</a> you can see where the TP, FP, TN, FN are output to the stdin. This place has the raw values that you need. It could be used for a method that aggregates these into a confusion matrix and outputs it after the run, but you would have to write this code yourself.</p></li>
<li><p>The <a href=""http://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">wiki</a> has an example of how to use numerical features with the <code>ColumnDataClassifier</code>. If you use numerical features, take a look at these options from the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html"" rel=""nofollow noreferrer"">API</a> that allow you do apply some transformations:</p>

<pre><code>realValued  boolean false   Treat this column as real-valued and do not perform any transforms on the feature value.    Value
logTransform    boolean false   Treat this column as real-valued and use the log of the value as the feature value. Log
logitTransform  boolean false   Treat this column as real-valued and use the logit of the value as the feature value.   Logit
sqrtTransform   boolean false   Treat this column as real-valued and use the square root of the value as the feature value. Sqrt
</code></pre></li>
</ol>
",1,1,534,2016-11-02 21:03:28,https://stackoverflow.com/questions/40389751/stanford-nlp-text-classifier-custom-features-and-confusion-matrix
Stanford NLP: Understanding the Tregex,"<p>Firstly, Im not sure what Im looking for is <strong>tregex</strong> but im going to try my best to explain my question.</p>

<p>What I want to do is, lets say I've a custom grammar like below.</p>

<pre><code>VP -&gt; V + NP + PP
NP -&gt; NN | PRP
PP -&gt; IN + NP
</code></pre>

<p><strong>V</strong> is all verbs meaning it doesn't matter if its past or present etc.<br>
<strong>+</strong> means concat<br>
<strong>|</strong> means or</p>

<p>And I've a string of postags like below.</p>

<pre><code>VBZ PRP IN NN
</code></pre>

<p>What I want to do is to check if this string is <strong>valid</strong> for the grammar above. I want to do that using the Stanford NLP API since my project uses it. I know I can use a compiler like <strong>jacc</strong> but I want to stick with Stanford NLP.</p>

<p>Thanks.</p>
","parsing, compiler-construction, grammar, stanford-nlp, pos-tagger","<p>I'm afraid that Tregex doesn't do what you want. It's a pattern matcher over tree structures. It isn't a parser. In theory the Stanford Parser can do parsing with any grammar, but in practice, there is no support for hand-written grammars. So you would be best off using a tool like jacc, JavaCC or ANTLR.</p>
",1,0,222,2016-11-02 21:34:21,https://stackoverflow.com/questions/40390215/stanford-nlp-understanding-the-tregex
How to use a custom TokensRegex rules annotator with Stanford CoreNLP Server?,"<p>The TokensRegex rules color annotator (<code>stanford-corenlp-full-2016-10-31/tokensregex/color.rules.txt</code>) loads successfully when using CoreNLP through command line but fails for the web server with  <code>java.lang.IllegalArgumentException: Unknown annotator: color</code>.</p>

<p><strong>Setup</strong></p>

<pre><code># custom.properties
annotators=tokenize,ssplit,pos,lemma,ner,regexner,color
customAnnotatorClass.color = edu.stanford.nlp.pipeline.TokensRegexAnnotator
color.rules = tokensregex/color.rules.txt
</code></pre>

<p><strong>Command Line</strong></p>

<pre><code>$ java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props custom.properties -file ./tokensregex/color.input.txt -outputFormat text
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Registering annotator color with class edu.stanford.nlp.pipeline.TokensRegexAnnotator
...
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator color
[main] INFO edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor - Reading TokensRegex rules from tokensregex/color.rules.txt
[main] INFO edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor - Read 7 rules

# color.input.txt.output
Sentence #1 (9 tokens):
Both blue and light blue are nice colors.
[Text=Both CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=CC Lemma=both NamedEntityTag=O]
[Text=blue CharacterOffsetBegin=5 CharacterOffsetEnd=9 PartOfSpeech=JJ Lemma=blue NamedEntityTag=COLOR NormalizedNamedEntityTag=#0000FF]
...
</code></pre>

<p><strong>Server</strong></p>

<ol>
<li><code>java -mx2g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -c custom.properties</code></li>
<li><p><code>wget --post-data 'Both blue and light blue are nice colors.' 'localhost:9000/?properties={""annotators"":""tokenize,ssplit,pos,lemma,ner,regexner,color"",""outputFormat"":""json""}' -O -</code></p>

<pre><code>HTTP request sent, awaiting response... 500 Internal Server Error
    2016-11-05 14:41:27 ERROR 500: Internal Server Error.

java.lang.IllegalArgumentException: Unknown annotator: color
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.ensurePrerequisiteAnnotators(StanfordCoreNLP.java:304)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.getProperties(StanfordCoreNLPServer.java:713)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:540)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
    at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</code></pre></li>
</ol>

<p><strong>Solution</strong></p>

<p>Include custom annotator properties in the request: <code>wget --post-data 'Both blue and light blue are nice colors.' 'localhost:9000/?properties={""color.rules"":""tokensregex/color.rules.txt"",""customAnnotatorClass.color"":""edu.stanford.nlp.pipeline.TokensRegexAnnotator"",""annotators"":""tokenize,ssplit,pos,lemma,ner,regexner,color"",""enforceRequirements"":""false"",""outputFormat"":""json""}' -O -</code></p>
","stanford-nlp, stanford-nlp-server, corenlp-server","<p>Add</p>

<pre><code>""enforceRequirements"":""false""
</code></pre>

<p>to your request and that should stop this error!</p>
",4,4,1224,2016-11-05 18:52:40,https://stackoverflow.com/questions/40441963/how-to-use-a-custom-tokensregex-rules-annotator-with-stanford-corenlp-server
How to extract Named Entity + Verb from text,"<p>Well, my aim is to extract NE (Person) and a verb connected to it from a text. For example, I have this text:</p>

<blockquote>
  <p>Dumbledore turned and walked back down the street. Harry Potter rolled over inside his blankets without waking up. </p>
</blockquote>

<p>As an ideal result i should get </p>

<blockquote>
  <p>Dumbledore turned walked; Harry Potter rolled</p>
</blockquote>

<p>I use Stanford NER to find and mark persons, then I delete all sentences that don't contain NE. So, in the end I have a 'pure' text that consists only of sentences with names of characters.
After that I use Stanford Dependencies. As the result I get smth like this (CONLLU output-format):</p>

<pre><code>1   Dumbledore  _   _   NN  _   2   nsubj   _   _
2   turned  _   _   VBD _   0   root    _   _
3   and _   _   CC  _   2   cc  _   _
4   walked  _   _   VBD _   2   conj    _   _
5   back    _   _   RB  _   4   advmod  _   _
6   down    _   _   IN  _   8   case    _   _
7   the _   _   DT  _   8   det _   _
8   street  _   _   NN  _   4   nmod    _   _
9   .   _   _   .   _   2   punct   _   _

1   Harry   _   _   NNP _   2   compound    _   _
2   Potter  _   _   NNP _   3   nsubj   _   _
3   rolled  _   _   VBD _   0   root    _   _
4   over    _   _   IN  _   3   compound:prt    _   _
5   inside  _   _   IN  _   7   case    _   _
6   his _   _   PRP$    _   7   nmod:poss   _   _
7   blankets    _   _   NNS _   3   nmod    _   _
8   without _   _   IN  _   9   mark    _   _
9   waking  _   _   VBG _   3   advcl   _   _
10  up  _   _   RP  _   9   compound:prt    _   _
11  .   _   _   .   _   3   punct   _   _
</code></pre>

<p>And that's where all my problems start. I know the person and the verb, but how to extract it from this format I have no idea.
I guess, i can do it this way: find NN/NNP in the table, find its 'parent' and then extract all its 'child'-words. Theoretically it should work. Theoretically.</p>

<p>The question is if anyone can come up with any other idea how to get a person and its action from the text? Or if there any more rational way to do it?</p>

<p>I'll be very grateful for any help!</p>
","java, nlp, stanford-nlp","<p>Here is some sample code to help with your problem:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.util.*;



public class NERAndVerbExample {

  public static void main(String[] args) throws IOException {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse,entitymentions"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    String text = ""John Smith went to the store."";
    Annotation annotation = new Annotation(text);
    pipeline.annotate(annotation);
    System.out.println(""---"");
    System.out.println(""text: "" + text);
    System.out.println("""");
    System.out.println(""dependency edges:"");
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      SemanticGraph sg = sentence.get(SemanticGraphCoreAnnotations.CollapsedDependenciesAnnotation.class);
      for (SemanticGraphEdge sge : sg.edgeListSorted()) {
        System.out.println(
                sge.getGovernor().word() + "","" + sge.getGovernor().index() + "","" + sge.getGovernor().tag() + "","" +
                        sge.getGovernor().ner()
                        + "" - "" + sge.getRelation().getLongName()
                        + "" -&gt; ""
                        + sge.getDependent().word() + "","" +
                        +sge.getDependent().index() + "","" + sge.getDependent().tag() + "","" + sge.getDependent().ner());
      }
      System.out.println();
      System.out.println(""entity mentions:"");
      for (CoreMap entityMention : sentence.get(CoreAnnotations.MentionsAnnotation.class)) {
        int lastTokenIndex = entityMention.get(CoreAnnotations.TokensAnnotation.class).size()-1;
        System.out.println(entityMention.get(CoreAnnotations.TextAnnotation.class) +
                ""\t"" +
                entityMention.get(CoreAnnotations.TokensAnnotation.class)
                        .get(lastTokenIndex).get(CoreAnnotations.IndexAnnotation.class) + ""\t"" +
                entityMention.get(CoreAnnotations.NamedEntityTagAnnotation.class));
      }
    }
  }
}
</code></pre>

<p>I'm hoping to add some syntactic sugar to Stanford CoreNLP 3.8.0 to assist with working with the entity mentions.</p>

<p>To explain this code a bit, basically the entitymentions annotator goes through and groups tokens with the same NER tag together.  So ""John Smith"" gets marked as an entity mention.</p>

<p>If you go through the dependency graph, you can get the index of each word.</p>

<p>Likewise if you access the list of tokens for an entity mention, you can also find the index of each word for the entity mention.</p>

<p>With a little more code you can link those together and form entity mention verb pairs as you were requesting.</p>

<p>As you can see in the current code it is quite cumbersome to access info for an entity mention, so I am going to try to improve that in 3.8.0.</p>
",1,4,910,2016-11-08 05:03:50,https://stackoverflow.com/questions/40479342/how-to-extract-named-entity-verb-from-text
Stanford core NLP models for English language,"<p>I am using stanford corenlp for a task. There are two models ""<strong>stanford-corenlp-3.6.0-models</strong>"" and ""<strong>stanford-english-corenlp-2016-01-10-models</strong>"" on stanford's website. I want to know what is the difference between these two models.</p>
",stanford-nlp,"<p>According to the ""Human languages supported"" section of CoreNLP Overview , the basic distribution provides model files for the analysis of well-edited English,which is the stanford-corenlp-3.6.0-models you mentioned.</p>

<p>But,CoreNLP member also provides a jar that contains all of their English models, which includes various variant models, and in particular has one optimized for working with uncased English (e.g., mostly or all either uppercase or lowercase).The newest one is stanford-english-corenlp-2016-10-31-models and the  previous one is stanford-english-corenlp-2016-01-10-models you mentioned. </p>

<p>Reference:</p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/index.html#programming-languages-and-operating-systems"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/index.html#programming-languages-and-operating-systems</a></p>

<p>(the Stanford CoreNLP Overview page)</p>
",2,1,823,2016-11-11 06:52:45,https://stackoverflow.com/questions/40542631/stanford-core-nlp-models-for-english-language
Is Arabic supported by all annotations in Stanford NLP?,"<p>I've used almost all provided annotations by Stanford NLP on English, but I want to work on Arabic now.</p>

<p>The question is, <strong>Could I use all annotations like</strong> <code>tokenize, ssplit, pos, lemma, ner, parse,dcoref</code> <strong>on Arabic too</strong>?</p>

<p><strong>Update:</strong></p>

<p>I want to know if Arabic has supported annotations like English or not, or is the annotations work on Arabic as well on English?</p>
","java, nlp, stanford-nlp, arabic","<p>Different annotators can produce different annotations. Take a look at this table:</p>

<p><a href=""https://i.sstatic.net/BfMZR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BfMZR.png"" alt=""enter image description here""></a></p>

<p>Reference:</p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/human-languages.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/human-languages.html</a></p>
",4,1,241,2016-11-14 09:25:29,https://stackoverflow.com/questions/40585375/is-arabic-supported-by-all-annotations-in-stanford-nlp
java.lang.NoSuchMethodError: edu.stanford.nlp.util.Generics.newHashMap()Ljava/util/Map;,"<p>I have the following line of code that initializes Stanford lexical parser. </p>

<pre><code>lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
</code></pre>

<p>I get below exception only when I move the code from a Java SE application to a Java EE application. </p>

<pre><code>Caused by: java.lang.NoSuchMethodError: edu.stanford.nlp.util.Generics.newHashMap()Ljava/util/Map;
    at edu.stanford.nlp.parser.lexparser.BinaryGrammar.init(BinaryGrammar.java:223)
    at edu.stanford.nlp.parser.lexparser.BinaryGrammar.readObject(BinaryGrammar.java:211)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</code></pre>

<p>How is this caused and how can I solve it?</p>
","java, jakarta-ee, stanford-nlp, nosuchmethoderror","<p>You can refer to the FAQ : <a href=""http://nlp.stanford.edu/software/corenlp-faq.shtml#nosuchmethoderror"" rel=""noreferrer"">http://nlp.stanford.edu/software/corenlp-faq.shtml#nosuchmethoderror</a></p>

<blockquote>
<pre><code>Caused by: java.lang.NoSuchMethodError: edu.stanford.nlp.util.Generics.newHashMap()Ljava/util/Map;
  at edu.stanford.nlp.pipeline.AnnotatorPool.(AnnotatorPool.java:27)
  at edu.stanford.nlp.pipeline.StanfordCoreNLP.getDefaultAnnotatorPool(StanfordCoreNLP.java:305)
</code></pre>
  
  <p>then this isn't caused by the shiny new Stanford NLP tools that you've just downloaded. It is because you also have old versions of one or more Stanford NLP tools on your classpath.</p>
  
  <p>The straightforward case is if you have an older version of a Stanford NLP tool. For example, you may still have a version of Stanford NER on your classpath that was released in 2009. In this case, you should upgrade, or at least use matching versions. For any releases from 2011 on, just use tools released at the same time -- such as the most recent version of everything :) -- and they will all be compatible and play nicely together.</p>
  
  <p>The tricky case of this is when people distribute jar files that hide other people's classes inside them. People think this will make it easy for users, since they can distribute one jar that has everything you need, but, in practice, as soon as people are building applications using multiple components, this results in a particular bad form of jar hell. People just shouldn't do this. The only way to check that other jar files do not contain conflicting versions of Stanford tools is to look at what is inside them (for example, with the jar -tf command).</p>
  
  <p>In practice, if you're having problems, the most common cause (in 2013-2014) is that you have ark-tweet-nlp on your classpath. The jar file in their github download hides old versions of many other people's jar files, including Apache commons-codec (v1.4), commons-lang, commons-math, commons-io, Lucene; Twitter commons; Google Guava (v10); Jackson; Berkeley NLP code; Percy Liang's fig; GNU trove; and an outdated version of the Stanford POS tagger (from 2011). You should complain to them for creating you and us grief. But you can then fix the problem by using their jar file from Maven Central. It doesn't have all those other libraries stuffed inside.</p>
</blockquote>
",5,0,565,2016-11-14 15:41:22,https://stackoverflow.com/questions/40592531/java-lang-nosuchmethoderror-edu-stanford-nlp-util-generics-newhashmapljava-ut
Resolve Class file for java.util.function.Function not found,"<p>I want to use the Stanford POS-tagger in my android project.</p>
<p>I added the pos-tagger .jar files to my android project in Android Studio:</p>
<ul>
<li>slf4j-api.jar</li>
<li>slf4j-simple.jar</li>
<li>stanford-postagger-3.6.0.jar</li>
</ul>
<p>I can import the library correctly:</p>
<pre><code>import edu.stanford.nlp.tagger.maxent.MaxentTagger;
</code></pre>
<p>But an error occurs when I use the function tagString:</p>
<pre><code>MaxentTagger tagger = new MaxentTagger(&quot;model/english-left3words-distsim.tagger&quot;);
String tagged = tagger.tagString(&quot;Hello World&quot;);//---&gt;&gt;ERROR
Log.i(&quot;tags&quot;,tagged);
</code></pre>
<p>When compiling, I am getting the error:</p>
<blockquote>
<p>Error:(34, 31) error: cannot access Function class file for
java.util.function.Function not found Error:Execution failed for task</p>
<p>':app:compileDebugJavaWithJavac'.
Compilation failed; see the compiler error output for details.</p>
</blockquote>
<p>How can I use the stanford pos-tagger in my Android project correctly?</p>
","java, android, android-studio, jar, stanford-nlp","<p>Are you compiling with Java 8? Recent versions of CoreNLP require Java 8; the error you're seeing is likely from the compiler not finding the newly introduced <code>Function</code> class.</p>
",1,0,1571,2016-11-15 01:32:36,https://stackoverflow.com/questions/40600649/resolve-class-file-for-java-util-function-function-not-found
&#39;Can&#39;t return head of null or leaf Tree&#39; with CoreNLP on Android,"<p>I  want to use CoreNLP in my Android project. But when I create a CoreNLP instance like this:</p>

<pre><code>import java.util.Properties;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class NLP {

    private StanfordCoreNLP pipeline;
    Properties props;

    public NLP() {
        props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, parse, sentiment"");
        pipeline = new StanfordCoreNLP(props);//--&gt;ERROR, SEE BELOW
    }

    public int findSentiment(String line) {
        int mainSentiment = 0;
        if (line != null &amp;&amp; line.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(line);
            for (CoreMap sentence : annotation
                    .get(CoreAnnotations.SentencesAnnotation.class)) {
                Tree tree = sentence
                        .get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                String partText = sentence.toString();
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        return mainSentiment;
    }
}
</code></pre>

<p>The project links to the following .jar files:</p>

<ul>
<li>ejml-0.23.jar    </li>
<li>stanford-corenlp-3.4.1.jar</li>
<li>stanford-corenlp-3.4.1-models.jar</li>
</ul>

<p>On my desktop java environment with java 1.8.0_92, this code runs correctly, but when running the code on an Android (after compiling without error), I am getting the error when the NLP class is instantiated:</p>

<blockquote>
  <p>Caused by: java.lang.IllegalArgumentException: Can't return head of
  null or leaf Tree.
                                                                       at
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:158)
                                                                       at
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:138)
                                                                       at
  edu.stanford.nlp.pipeline.ParserAnnotator.(ParserAnnotator.java:132)
                                                                       at
  edu.stanford.nlp.pipeline.AnnotatorImplementations.parse(AnnotatorImplementations.java:132)
                                                                       at
  edu.stanford.nlp.pipeline.StanfordCoreNLP$10.create(StanfordCoreNLP.java:719)
                                                                       at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
                                                                       at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:292)
                                                                       at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:129)
                                                                       at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:125)</p>
</blockquote>

<p>I am using CoreNLP 3.4.1. It's not the most recent version, but it works with Java 7 on Android. How can I use CoreNLP correctly on Android?</p>
","java, android, sentiment-analysis, stanford-nlp","<h2>Why this problem occurs?</h2>

<p>I was looking for the answer. I have checked the jar. There is a class <code>AbstractCollinsHeadFinder.java</code>. From this class, this error comes</p>

<blockquote>
  <p>edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:158)
  at
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:138)</p>
</blockquote>

<p>There are 2 root causes for this error.</p>

<ol>
<li>If tree is null, then this error occurs.</li>
<li><p>If tree is leaf, then this error occurs.</p>

<pre><code>@Override
public Tree determineHead(Tree t, Tree parent) {
  if (nonTerminalInfo == null) {
    throw new IllegalStateException(""Classes derived from AbstractCollinsHeadFinder must create and fill HashMap nonTerminalInfo."");
  }
  // The error mainly generate for the following condition
  if (t == null || t.isLeaf()) {
    throw new IllegalArgumentException(""Can't return head of null or leaf Tree.""); 
  }
  if (DEBUG) {
    log.info(""determineHead for "" + t.value());
  }

  Tree[] kids = t.children();
  -------------
  -------------
  return theHead;
}
</code></pre></li>
</ol>

<h2>Resource Link:</h2>

<ol>
<li><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/trees/AbstractCollinsHeadFinder.java#L163"" rel=""noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/trees/AbstractCollinsHeadFinder.java#L163</a></li>
</ol>

<hr>

<h2>Check for parameters:</h2>

<p>I have also checked your code. In your setProperty(...), there are some parameters. <strong>Maybe there are some parameter missing</strong>. So, you can create a object by following the code.</p>

<pre><code>// creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<h2>Resource Link:</h2>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/api.html"" rel=""noreferrer"">creates a StanfordCoreNLP object</a></p>

<hr>

<h2>A simple, complete example program:</h2>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;

public class StanfordCoreNlpExample {
    public static void main(String[] args) throws IOException {
        PrintWriter xmlOut = new PrintWriter(""xmlOutput.xml"");
        Properties props = new Properties();
        props.setProperty(""annotators"",
                ""tokenize, ssplit, pos, lemma, ner, parse"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        Annotation annotation = new Annotation(
                ""This is a short sentence. And this is another."");
        pipeline.annotate(annotation);
        pipeline.xmlPrint(annotation, xmlOut);
        // An Annotation is a Map and you can get and use the
        // various analyses individually. For instance, this
        // gets the parse tree of the 1st sentence in the text.
        List&lt;CoreMap&gt; sentences = annotation
                .get(CoreAnnotations.SentencesAnnotation.class);
        if (sentences != null &amp;&amp; sentences.size() &gt; 0) {
            CoreMap sentence = sentences.get(0);
            Tree tree = sentence.get(TreeAnnotation.class);
            PrintWriter out = new PrintWriter(System.out);
            out.println(""The first sentence parsed is:"");
            tree.pennPrint(out);
        }
    }
}
</code></pre>

<h2>Resource Link:</h2>

<ol>
<li><a href=""http://www.surdeanu.info/mihai/papers/acl2014-corenlp.pdf"" rel=""noreferrer"">The Stanford CoreNLP Natural Language Processing Toolkit</a></li>
</ol>
",8,15,394,2016-11-15 06:27:05,https://stackoverflow.com/questions/40603405/cant-return-head-of-null-or-leaf-tree-with-corenlp-on-android
Stanford NLP named entities of more than one token,"<p>I'm experimenting with Stanford Core NLP for named entity recognition. </p>

<p>Some named entities consist of more than one token, for example, Person: ""Bill Smith"". I can't figure out what API calls to use to determine when ""Bill"" and ""Smith"" should be considered a single entity, and when they should be two different entities.</p>

<p>Is there some decent documentation somewhere which explains this?</p>

<p>Here's my current code:</p>

<pre><code>    InputStream is = getClass().getResourceAsStream(MODEL_NAME);
    if (MODEL_NAME.endsWith("".gz"")) {
        is = new GZIPInputStream(is);
    }
    is = new BufferedInputStream(is);

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");

    AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifier(is);
    is.close();

    String text = ""Hello, Bill Smith, how are you?"";

    List&lt;List&lt;CoreLabel&gt;&gt; sentences = classifier.classify(text);
    for (List&lt;CoreLabel&gt; sentence: sentences) {
        for (CoreLabel word: sentence) {
            String type = word.get(CoreAnnotations.AnswerAnnotation.class);
            System.out.println(word + "" is of type "" + type);
        }
    }
</code></pre>

<p>Also, it isn't clear to me why the ""PERSON"" annotation is coming back as AnswerAnnotation, instead of CoreAnnotations.EntityClassAnnotation, EntityTypeAnnotation, or something else.</p>
",stanford-nlp,"<p>You should use the ""entitymentions"" annotator, which will mark continuous sequences of tokens with the same ner tag as an entity.  The list of entities for each sentence will be stored under the  CoreAnnotations.MentionsAnnotation.class key.  Each entity mention itself will be a CoreMap.</p>

<p>Looking over this code could help: </p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java</a></p>

<p>some sample code:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;



public class EntityMentionsExample {

  public static void main (String[] args) throws IOException {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    String text = ""Joe Smith is from Florida."";
    Annotation annotation = new Annotation(text);
    pipeline.annotate(annotation);
    System.out.println(""---"");
    System.out.println(""text: "" + text);
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreMap entityMention : sentence.get(CoreAnnotations.MentionsAnnotation.class)) {
        System.out.print(entityMention.get(CoreAnnotations.TextAnnotation.class));
        System.out.print(""\t"");
        System.out.print(
                entityMention.get(CoreAnnotations.NamedEntityTagAnnotation.class));
        System.out.println();
      }
    }
  }
}
</code></pre>
",2,3,780,2016-11-15 20:13:12,https://stackoverflow.com/questions/40618856/stanford-nlp-named-entities-of-more-than-one-token
Import Stanford nlp Intellij,"<p>I'm having trouble using Stanford Lemmatizer. 
As i'm using Intellij IDE, i try to import it via the Dependencies Windows, but i can't access all the class by that way.</p>

<p>Is there a way to import stanford-english-corenlp-models-current.jar &amp; stanford-corenlp-models-current.jar correctly on Intellij?</p>
","java, intellij-idea, stanford-nlp, lemmatization","<p>As guys mentioned above,you just import the wrong file</p>

<p>First,download the CoreNLP 3.7.0(beta)
<a href=""https://i.sstatic.net/nJ5gg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nJ5gg.png"" alt=""enter image description here""></a> </p>

<p>In the screen shot above,click the red button to download the file,which covers all the things to run the CoreNLP.</p>

<p>Second, right click the module which you want to import jar to to open the ""open module settings"" pannel ,then you may get a picture like this</p>

<p><a href=""https://i.sstatic.net/YtwR1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YtwR1.png"" alt=""enter image description here""></a></p>

<p>click the green plus button to import all the things</p>

<p><a href=""https://i.sstatic.net/2675F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2675F.png"" alt=""enter image description here""></a></p>

<p><strong>Notice:It's the directory I point to you.</strong></p>

<p>Then click ""Apply"" and ""OK""</p>

<p>This is done.Enjoy it!</p>

<p>Reference:</p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/</a>
(CoreNLP official web page)</p>

<p><a href=""http://jingyan.baidu.com/article/fec7a1e5f79e2b1191b4e74f.html"" rel=""nofollow noreferrer"">http://jingyan.baidu.com/article/fec7a1e5f79e2b1191b4e74f.html</a>
(how to import jar in Intellij in Chinese)</p>
",4,3,3386,2016-11-16 06:28:36,https://stackoverflow.com/questions/40625438/import-stanford-nlp-intellij
How to create custom relations using KBPAnnotator?,"<p>Is it possible to somehow define and use custom rule patterns for KBPAnnotator? If I not wrong the annotator reads rules from files located by the following path edu/stanford/nlp/models/kbp/tokensregex and these rule types are hardcoded in the RelationType interface.</p>
",stanford-nlp,"<p>Unfortunately, this is not as easy as it is with many of the other CoreNLP components. The KBP relation annotator evolved primarily as a way to share Stanford's system for the TAC-KBP slot filling shared task, and is therefore very much tuned to that task.</p>

<p>You can likely adapt the Tokensregex part of the classifier to use your own pattern definitions, but this is not yet supported out-of-the-box.</p>
",2,0,211,2016-11-18 10:26:21,https://stackoverflow.com/questions/40674657/how-to-create-custom-relations-using-kbpannotator
Stanford.NLP for .NET not loading models,"<p>I am trying to run the sample code <a href=""https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html"" rel=""nofollow noreferrer"">provided here for Stanford.NLP for .NET</a>.</p>

<p>I installed the package via Nuget, downloaded the CoreNLP zip archive, and extracted stanford-corenlp-3.7.0-models.jar. After extracting, I located the ""models"" directory in stanford-corenlp-full-2016-10-31\edu\stanford\nlp\models.</p>

<p>Here is the code that I am trying to run:</p>

<pre><code> public static void Test1()
    {
        // Path to the folder with models extracted from `stanford-corenlp-3.6.0-models.jar`
        var jarRoot = @""..\..\..\stanford-corenlp-full-2016-10-31\edu\stanford\nlp\models\"";

        // Text for processing
        var text = ""Kosgi Santosh sent an email to Stanford University. He didn't get a reply."";

        // Annotation pipeline configuration
        var props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse, ner,dcoref"");
        props.setProperty(""ner.useSUTime"", ""0"");

        // We should change current directory, so StanfordCoreNLP could find all the model files automatically
        var curDir = Environment.CurrentDirectory;
        Directory.SetCurrentDirectory(jarRoot);
        var pipeline = new StanfordCoreNLP(props);
        Directory.SetCurrentDirectory(curDir);

        // Annotation
        var annotation = new Annotation(text);
        pipeline.annotate(annotation);

        // Result - Pretty Print
        using (var stream = new ByteArrayOutputStream())
        {
            pipeline.prettyPrint(annotation, new PrintWriter(stream));
            Console.WriteLine(stream.toString());
            stream.close();
        }
    }
</code></pre>

<p>I get the following error when I run the code:</p>

<blockquote>
  <p>A first chance exception of type 'java.lang.RuntimeException' occurred in stanford-corenlp-3.6.0.dll
  An unhandled exception of type 'java.lang.RuntimeException' occurred in stanford-corenlp-3.6.0.dll
  Additional information: edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)</p>
</blockquote>

<p>What am I doing wrong? I really want to get this working. :(</p>
","c#, .net, nlp, stanford-nlp","<p>Mikael Kristensen's answer is correct. <code>stanfrod-corenlp-ful-*.zip</code> archive contains files <code>stanford-corenlp-3.7.0-models.jar</code> with models inside (this is a zip archive). In Java world, you add this <code>jar</code> on the class path, and it automatically resolves models' location in the archive.</p>

<p>CoreNLP has a file <a href=""https://github.com/stanfordnlp/CoreNLP/blob/ef653d4f64f82b0395f72d43cc7add8e61752fee/src/edu/stanford/nlp/pipeline/DefaultPaths.java"" rel=""noreferrer"">DefaultPaths.java</a> that specifies path to model file. So when you instantiate <code>StanfordCoreNLP</code> with <code>Properties</code> object that does not specify models location, you should guarantee that models could be found by default path (related to <code>Environment.CurrentDirectory</code>).</p>

<p>The simplest way to guarantee existence of files at path like <code>Environment.CurrentDirectory + ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz""</code> is to unzip a jar archive to the folder, and temporary change the current directory to unzipped folder.</p>

<pre><code>var jarRoot = ""nlp.stanford.edu/stanford-corenlp-full-2016-10-31/jar-modules/"";
...
var curDir = Environment.CurrentDirectory;
Directory.SetCurrentDirectory(jarRoot);
var pipeline = new StanfordCoreNLP(props);
Directory.SetCurrentDirectory(curDir);
</code></pre>

<p>The other way is to specify paths to all models that your pipeline need (it actually depends on the list of <code>annotators</code>).
This option is more complicated because you have to find correct property keys, and specify paths to all used model. But it may be useful if you want to minimize the size of you deployment package.</p>

<pre><code>var props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");
props.put(""ner.model"",
          ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"");
props.put(""ner.applyNumericClassifiers"", ""false"");
var pipeline = new StanfordCoreNLP(props);
</code></pre>
",5,3,3059,2016-11-26 02:58:38,https://stackoverflow.com/questions/40814503/stanford-nlp-for-net-not-loading-models
re-init method of ontoRootGazetteer is not working,"<p>I am new to GATE NLP. I am working on an application which works on <code>GATE NLP</code>.
So, I have created a pipeline, and i am loading it only once in application by creating singleton object. So, Because of this performance of application has increased but when I make any changes in ontology or gazetteer and re-run the application then it is not considering the newly added words,because i made my object singleton through I am loading my pipeline so it considers previously loaded gazetteer and ontology.So, 
I used the following code using it it is taking updated Gazetteer, but not ontology.</p>

<pre><code>  application = CorpusControllerSingleton.getInstance(gapFilePath).getApplicationObject();
            Iterator&lt;ProcessingResource&gt; it = application.getPRs().iterator();
if(isReload){
                System.out.println(""processing resources------&gt;""+it.next());
                while(it.hasNext()){
                    ProcessingResource pr = it.next();
                    if(pr.getName().equals(""RzCIS"") || pr.getName().equals(""RzCs"")) {
                        System.out.println(""PR initialization---&gt;"" +pr.getFeatures());
                        pr.reInit();
                    }
                }

            }
</code></pre>

<p>Can anyone explain me how to re-init ontology ?</p>
","nlp, stanford-nlp, gate","<p>I have used <code>Flexible_Gazetteer</code> so, it has the parameter <code>gazetteerInst</code> which is nothing but a Processing Resource <code>OntoRootGazetteer</code>. So First you need to get all the Processing Resources that you are using in your pipeline . Iterate over it and extract the OntoRootGazetteer from it. After that <code>OntoRootGazetteer</code> has a property <code>gazetterInst</code> whose value is actual a <code>ontology</code>. So, you just need to update that ontology or give the path of the ontology to it. Then use <code>reinit</code> method for <code>ontoRootGazettter</code>which you extracted from the <code>flexibleGazettteer</code>.</p>

<p>Through coding -</p>

<pre><code>application = CorpusControllerSingleton.getInstance(gapFilePath).getApplicationObject();
            Iterator&lt;ProcessingResource&gt; it = application.getPRs().iterator();
            while (it.hasNext()) {
                ProcessingResource pr = it.next();
                if(pr.getName().equals(FLEXIBLE_GAZETTEER)){
                    onto_Root_gazetteer = (ProcessingResource)  pr.getParameterValue(ONTOROOT_PROPERTY);
                    onto_Root_gazetteer.setParameterValue(ONTOROOT_PARAMETER, OntoLoader.getInstance().getOntology());
                    onto_Root_gazetteer.init();
                }
                if(pr.getName().equals(ANNIE_GAZETTEER_CASEINSENSITIVE)) {
                    pr.reInit();
                }
                if(pr.getName().equals(ANNIE_GAZETTEER_CASESENSITIVE)) {
                    pr.reInit();
                }
            }
</code></pre>

<p>Here </p>

<pre><code>private static final String ONTOROOT_PROPERTY = ""gazetteerInst"";
private static final String ONTOROOT_PARAMETER = ""ontology"";
</code></pre>

<p>use this, will solve your problem.</p>
",1,1,87,2016-11-26 04:40:44,https://stackoverflow.com/questions/40814969/re-init-method-of-ontorootgazetteer-is-not-working
re-init method of ontoRootGazetteer is not working,"<p>I am new to GATE NLP. I am working on an application which works on <code>GATE NLP</code>.
So, I have created a pipeline, and i am loading it only once in application by creating singleton object. So, Because of this performance of application has increased but when I make any changes in ontology or gazetteer and re-run the application then it is not considering the newly added words,because i made my object singleton through I am loading my pipeline so it considers previously loaded gazetteer and ontology.So, 
I used the following code using it it is taking updated Gazetteer, but not ontology.</p>

<pre><code>  application = CorpusControllerSingleton.getInstance(gapFilePath).getApplicationObject();
            Iterator&lt;ProcessingResource&gt; it = application.getPRs().iterator();
if(isReload){
                System.out.println(""processing resources------&gt;""+it.next());
                while(it.hasNext()){
                    ProcessingResource pr = it.next();
                    if(pr.getName().equals(""RzCIS"") || pr.getName().equals(""RzCs"")) {
                        System.out.println(""PR initialization---&gt;"" +pr.getFeatures());
                        pr.reInit();
                    }
                }

            }
</code></pre>

<p>Can anyone explain me how to re-init ontology ?</p>
","nlp, stanford-nlp, gate","<p>I have used <code>Flexible_Gazetteer</code> so, it has the parameter <code>gazetteerInst</code> which is nothing but a Processing Resource <code>OntoRootGazetteer</code>. So First you need to get all the Processing Resources that you are using in your pipeline . Iterate over it and extract the OntoRootGazetteer from it. After that <code>OntoRootGazetteer</code> has a property <code>gazetterInst</code> whose value is actual a <code>ontology</code>. So, you just need to update that ontology or give the path of the ontology to it. Then use <code>reinit</code> method for <code>ontoRootGazettter</code>which you extracted from the <code>flexibleGazettteer</code>.</p>

<p>Through coding -</p>

<pre><code>application = CorpusControllerSingleton.getInstance(gapFilePath).getApplicationObject();
            Iterator&lt;ProcessingResource&gt; it = application.getPRs().iterator();
            while (it.hasNext()) {
                ProcessingResource pr = it.next();
                if(pr.getName().equals(FLEXIBLE_GAZETTEER)){
                    onto_Root_gazetteer = (ProcessingResource)  pr.getParameterValue(ONTOROOT_PROPERTY);
                    onto_Root_gazetteer.setParameterValue(ONTOROOT_PARAMETER, OntoLoader.getInstance().getOntology());
                    onto_Root_gazetteer.init();
                }
                if(pr.getName().equals(ANNIE_GAZETTEER_CASEINSENSITIVE)) {
                    pr.reInit();
                }
                if(pr.getName().equals(ANNIE_GAZETTEER_CASESENSITIVE)) {
                    pr.reInit();
                }
            }
</code></pre>

<p>Here </p>

<pre><code>private static final String ONTOROOT_PROPERTY = ""gazetteerInst"";
private static final String ONTOROOT_PARAMETER = ""ontology"";
</code></pre>

<p>use this, will solve your problem.</p>
",1,1,87,2016-11-26 04:40:44,https://stackoverflow.com/questions/40814969/re-init-method-of-ontorootgazetteer-is-not-working
How to get englishPCFG.ser.gz as a single file from Stanford CoreNLP,"<p>I am far to stupid to get the <code>stanford-parser.jar</code> into my build path. Is there a way to obtain just the needed model file <code>englishPCFG.ser.gz</code> on its own, and then to <code>LexicalizedParser.loadModel(""/absolute/path/englishPCFG.ser.gz"");</code>?</p>
",stanford-nlp,"<p>Yes, this is easy. A jar file is just a glorified zip file.  So using either the jar command or your favorite unzipper, do something like:</p>

<pre class=""lang-bash prettyprint-override""><code>unzip stanford-corenlp-3.7.0-models.jar
mv edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz .
rm -rf edu
</code></pre>
",12,4,5353,2016-11-26 15:54:07,https://stackoverflow.com/questions/40820140/how-to-get-englishpcfg-ser-gz-as-a-single-file-from-stanford-corenlp
"How to identify, if a person is talking about himself or others in a sentence","<p>I am new to NLP. I want to identify from a sentence, whether the person is talking about himself or others. Can anyone suggest a way to do so?</p>

<p>I am thinking that based on personal pronouns in sentence we can say if the person is talking about himself or others. </p>

<p>I am looking for ways to identify if the person is talking about himself or others, by using POS tagging or a syntactic parser etc.</p>

<p>Thanks in advance</p>
","nlp, opennlp, stanford-nlp","<p>SO, you're talking about Coreference Resolution system which links multiple mentions of an entity in a document together.</p>

<p>take at look at this <a href=""http://blog.dpdearing.com/2012/11/making-coreference-resolution-with-opennlp-1-5-0-your-bitch/"" rel=""nofollow noreferrer"">blog</a> which is about coreference Resolution in OpenNLP. </p>

<p>Here's the OpenNLP <a href=""https://opennlp.apache.org/documentation/1.6.0/manual/opennlp.html#tools.coref"" rel=""nofollow noreferrer"">Documentation</a></p>

<p>hope this helps.</p>
",0,1,370,2016-11-28 01:22:13,https://stackoverflow.com/questions/40835889/how-to-identify-if-a-person-is-talking-about-himself-or-others-in-a-sentence
How to do Wikification / Entity Linking on the basis of the EntityMentionsAnnotator?,"<p>I am trying to link entity mentions to a knowledge base, for instance DBpedia or Wikidata.</p>

<p>In the end I want to enrich the JSON output with an arbitrary ontology and hereby provide some kind of semantics. But as a first step it should just look something like this:</p>

<pre><code>{

    ""index"": 1,
    ""mention"": ""Barack Obama"",
    ""characterOffsetBegin"": 0,
    ""characterOffsetEnd"": 12,
    ""ner"": ""PERSON"",
    ""before"": """",
    ""after"": "" ""
    ""uri"": ""http://dbpedia.org/page/Barack_Obama""

}
</code></pre>

<p>Is there a way of doing so with the already provided tools of Stanford CoreNLP? I have seen a WikidictAnnotator on GitHub, but unfortunately there is no documentation or whatsoever on what it is and how to use it respectively.</p>

<p>Apart form that, what other possibilities are there? Do I have to use a third-party tool like DBpedia Spotlight in conjunction with the Stanford NE Recognizer in order to realize entity linking?</p>

<p>Thanks in advance!</p>
","java, stanford-nlp, dbpedia, named-entity-recognition, wikidata","<p>After some research I am going to answer my own question. Maybe it helps somebody in the future.</p>

<p>I found a framework called AGDISTIS which provides a mentions to knowledge base functionality. It accepts mentions from the EntityMentionsAnnotator and matches them with the according dbpedia resources. You can even use another index apart from dbpedia.</p>

<p>Nevertheless, I would be happy if someone could explain whether and how wikification is possible just with CoreNLP (see WikidictAnnotator).</p>
",0,1,1034,2016-11-29 12:57:10,https://stackoverflow.com/questions/40866504/how-to-do-wikification-entity-linking-on-the-basis-of-the-entitymentionsannota
How can I stop Stanford CoreNLP from segmenting my sentence,"<p>I have segmented resources and resources that match my segmented sentences.</p>

<p>How can I stop Stanford CoreNLP from segmenting my sentence before generating the parsing tree?</p>

<p>I am doing works on Chinese.</p>
","nlp, stanford-nlp","<p>Your description is not very precise, so I'm not sure if I interpret your question correctly. It sounds like you want to feed the parser a list of tokens without having corenlp doing any tokenisation, right?
If so, it would be useful to know which parser you are using. But with both, you can just feed it a list of tokens and corenlp will not jump in and mess up your tokenisation. I haven't worked with the chinese resources, but the following could help you (if you have done tokenisation before already, and splitting on whitespace results in proper tokenisation):</p>

<pre><code>    String sentence = ""I can't do that ."";
    ArrayList&lt;HasWord&gt; hwl = new ArrayList&lt;HasWord&gt;();
    String[] tokens = sentence.split("" "");
    for (String t : tokens){
     HasWord hw = new Word();
     hw.setWord(t);
     hwl.add(hw);
    }
    LexicalizedParser lexParser = LexicalizedParser.loadModel(""&lt;path to chinese lex parsing here&gt;"",""-maxLength"", ""70"");
    Tree cTree = lexParser.parse(hwl);
    System.out.println(""c tree:"" + cTree);


    DependencyParser parser = DependencyParser.loadFromModelFile(""&lt;chinese model for dep parsing here&gt;"");
    MaxentTagger tagger = new MaxentTagger(""&lt;path to your tagger file goes here"");
    List&lt;TaggedWord&gt; tagged = tagger.tagSentence(hwl);
    GrammaticalStructure gs = parser.predict(tagged);
    System.out.println(""dep tree:"" + gs.typedDependencies());
</code></pre>

<p>Deleting the stderr lines that are written, this results in:</p>

<pre><code>c tree:(ROOT (S (MPN (FM I) (FM can't)) (VVFIN do) (ADJD that) ($. .)))
dep tree:[nsubj(can't-2, I-1), root(ROOT-0, can't-2), xcomp(can't-2, do-3), dobj(do-3, that-4), punct(can't-2, .-5)]
</code></pre>

<p>hope this helps.</p>
",0,0,196,2016-12-03 07:44:51,https://stackoverflow.com/questions/40945468/how-can-i-stop-stanford-corenlp-from-segmenting-my-sentence
StanfordCoreNLP object creation error,"<p>I am facing this issue:</p>

<pre><code>Exception in thread ""main"" java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)

Caused by: java.io.InvalidClassException: edu.stanford.nlp.tagger.maxent.ExtractorDistsim; local class incompatible: stream classdesc serialVersionUID = 1, local class serialVersionUID = 2
at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.readExtractors(MaxentTagger.java:622)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:868)
... 23 more
</code></pre>

<p>at code line</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>Note : I have put the stanford core nlp latest jar file but it didn't work and even tried explicitly adding stanford nlp pos tagger jar file but it didn't work and even tried adding the models jar file but didn't work.</p>

<p>Please help.</p>
","machine-learning, nlp, text-classification, stanford-nlp","<p>Whoever encounters this problem i would suggest them to visit <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP</a> and download the LATEST model files from there it will mostly solve the issue.</p>
",0,0,638,2016-12-05 20:21:55,https://stackoverflow.com/questions/40982653/stanfordcorenlp-object-creation-error
StanfordNLP OpenIE 4 error,"<p>I've been encountering this error:</p>

<p>I ran the OpenIE 4.1 binary but got the following error: </p>

<pre><code>Exception in thread ""main"" java.lang.NullPointerException at 
com.googlecode.clearnlp.tokenization.EnglishTokenizer.protec‌​tEmoticons
(EnglishTokenizer.java:335) at 
com.googlecode.clearnlp.tokenization.EnglishTokenizer.getTok‌​enList(En 
glishTokenizer.java:109) at 
com.googlecode.clearnlp.tokenization.AbstractTokenizer.getTo‌​kens(AbstractTokenizer.java:58) at 
edu.knowitall.tool.tokenize.ClearTokenizer.tokenize(ClearTok‌​enizer.sc ala:22) 
</code></pre>

<p>I've looked up a few sources and found a comment by Yangrui who also had this problem in the past. But there are no solutions. I've checked my openie.4.1.jar file and the com.googlecode.clearnlp.tokenization.EnglishTokenizer.protectEmoticon exists. </p>

<p>Hope someone can help shed some light on this. Thank you in advance.</p>
","java, nlp, stanford-nlp","<p>I've managed to solve this error. The issue lies with the compilation of the OpenIE 4.0 JAR and OpenIE 4.1 JAR files I downloaded from the official website. (<a href=""http://knowitall.github.io/openie/"" rel=""nofollow noreferrer"">http://knowitall.github.io/openie/</a>).</p>

<p>How to solve? Compile the classes yourself.</p>

<ol>
<li>Go to <a href=""https://github.com/knowitall/openie/releases"" rel=""nofollow noreferrer"">https://github.com/knowitall/openie/releases</a></li>
<li>Choose a release. (I chose 4.1.3)</li>
<li>Download the zip file and unzip the file.</li>
<li>Use terminal and redirect to the folder directory.</li>
<li>Run 'sbt package' and it will start compiling.</li>
<li>Your final JAR file will be found in the target folder/scala-2.10/{openie-assembly-VERSION.jar}</li>
</ol>

<p>Note: There could be some changes you need to make to your code if you are running OpenIE as a dependency. The code for test is as follows:</p>

<pre><code>    OpenIE openIE = new OpenIE(new ClearParser(new ClearPostagger(new ClearTokenizer())), new ClearSrl(), false, false);

    Seq&lt;Instance&gt; extractions = openIE.extract(""Obama is the president of the United States"");
    Iterator&lt;Instance&gt; iterator = extractions.iterator();
    while (iterator.hasNext()) {
        Instance inst = iterator.next();
        StringBuilder sb = new StringBuilder();
         sb.append(inst.confidence()).append(""\t\t"")
         .append(inst.extr().arg1().text()).append(""\t\t"")
         .append(inst.extr().rel().text()).append(""\t\t"");

        Iterator&lt;Argument&gt; argIter = inst.extr().arg2s().iterator();
        while (argIter.hasNext()) {
            Part arg = argIter.next();
            sb.append(arg.text()).append(""; "");
        }
         System.out.println(sb.toString());
    }
</code></pre>

<p>I hope this will help someone in the future.</p>
",1,1,110,2016-12-07 14:59:33,https://stackoverflow.com/questions/41020725/stanfordnlp-openie-4-error
Word2Vec - adding constraint to vector representation,"<p>I am trying to adapt the pre-trained Google News word2vec model to my specific domain. For the domain I am looking at, certain words are known to be similar to each other so in an ideal world, the Word2Vec representation of those words should represent that. I understand that I can train the pre-trained model on a corpus of domain-specific data to update the vectors. </p>

<p>However, if I know for certain that certain words are highly similar and should be together, is there a way for me to incorporate that constraint into the word2vec model? Mathematically, I would like to add a term to the loss function of word2vec that provides a penalty if two that I know to be similar are not positioned close to each other in the vector space. Does anyone have advice on how to implement this? Will this require me to unpack the word2vec model or is there a way for me to potentially add that additional term to the loss function?</p>
","nlp, stanford-nlp, word2vec","<p>One approach is to take pre-trained Google News word2vec and use this ""retrofitting"" tool:</p>

<p>Faruqui, Manaal, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. ""Retrofitting word vectors to semantic lexicons."" arXiv preprint arXiv:1411.4166 (2014). <a href=""https://arxiv.org/abs/1411.4166"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1411.4166</a></p>

<blockquote>
  <p>This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed.</p>
</blockquote>

<p>The code is available at <a href=""https://github.com/mfaruqui/retrofitting"" rel=""nofollow noreferrer"">https://github.com/mfaruqui/retrofitting</a> and is straightforward to use (I've personally used it for <a href=""https://arxiv.org/abs/1607.02802"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1607.02802</a>).</p>
",3,5,615,2016-12-11 11:41:51,https://stackoverflow.com/questions/41085755/word2vec-adding-constraint-to-vector-representation
What is Two-Level Morphology?,"<p>In Natural Language Processing what are the two levels of this two-level Morphology framework ?</p>
","nlp, stanford-nlp","<p>There are <em>lexical</em> and <em>surface</em> levels in this framework, hence the name two-level morphology. The surface level is the actual realization of words as they appear in the final form. The lexical (a.k.a dictionary) level corresponds to the combination of roots and affixes that are chained together with boundary markers. For instance, for the present tense third person singular form of the verb &quot;try&quot;:</p>
<pre><code>lexical level: try+s
surface level: tries
</code></pre>
<p>In this framework, these two levels are connected through a series of finite state transducers and form a single combined automaton.</p>
<p>For more information consult this <a href=""https://web.stanford.edu/%7Elaurik/publications/twol-history.pdf"" rel=""nofollow noreferrer"">document</a>.</p>
<p>UPDATE: Also have a look at the documentation for <a href=""https://pytwolc.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">twol</a>, a Python package developed within this linguistic framework. For a detailed theoretical treatment, you can have a look at Koskenniemi's original work <a href=""https://helda.helsinki.fi//bitstream/handle/10138/305218/koskenniemi_1983_Two_Level_Morphology.pdf?sequence=1"" rel=""nofollow noreferrer"">here</a>.</p>
",14,6,2568,2016-12-18 16:23:48,https://stackoverflow.com/questions/41210384/what-is-two-level-morphology
Connection error using Python wrapper for Stanford CoreNLP tools v3.4.1,"<p>I have been trying to use stanford-corenlp-python which is a python wrapper using <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""nofollow noreferrer"">this</a> github repo. </p>

<p>However, while storing the results I get the following error:</p>

<pre><code>    Traceback (most recent call last):
  File ""client.py"", line 14, in &lt;module&gt;
    result = nlp.parse(""Hello world!  It is so beautiful."")
  File ""client.py"", line 11, in parse
    return json.loads(self.server.parse(text))
  File ""/home/kenden/deeshacodes/stanford-corenlp-python/jsonrpc.py"", line 934, in __call__
    return self.__req(self.__name, args, kwargs)
  File ""/home/kenden/deeshacodes/stanford-corenlp-python/jsonrpc.py"", line 906, in __req
    raise RPCTransportError(err)
jsonrpc.RPCTransportError: [Errno 111] Connection refused
</code></pre>

<p>How do I solve this? Also, is there any other way to use coreNLP in python?</p>
","python, stanford-nlp","<p>May be you can try this <a href=""https://github.com/Lynten/stanford-corenlp"" rel=""nofollow noreferrer"">stanfordcorenlp</a>.</p>

<p>It's very simple.</p>

<pre><code>from stanfordcorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP(r'G:/JavaLibraries/stanford-corenlp-full-2016-10-31/')

sentence = 'Guangdong University of Foreign Studies is located in Guangzhou.'
print 'Tokenize:', nlp.word_tokenize(sentence)
print 'Part of Speech:', nlp.pos_tag(sentence)
print 'Named Entities:', nlp.ner(sentence)
print 'Constituency Parsing:', nlp.parse(sentence)
print 'Dependency Parsing:', nlp.dependency_parse(sentence)
</code></pre>

<p>Although convient, it is not as stable as native Python wrapper. ""The project is just a wraper to parse the json data requested from the Java Server backend.""</p>
",0,0,1407,2016-12-21 10:15:12,https://stackoverflow.com/questions/41260313/connection-error-using-python-wrapper-for-stanford-corenlp-tools-v3-4-1
Unpacking list_iterator from nltk.stanford.StanfordDependencyParser inside pandas dataframe,"<p>I am trying to work with the StanfordDependencyParser inside of a pandas DataFrame. </p>

<pre><code>from nltk.parse import stanford
import pandas as pd
dep_parser=stanford.StanfordDependencyParser()
df = pd.DataFrame({'ID' : [0,1,2], 'sentence' : ['This is the first s.', 'This is the 2nd s.', 'This isn''t the third s.']})
df['parsed'] = df.sentence.apply(dep_parser.raw_parse)
print(df)

   ID                sentence                                        parsed
0   0    This is the first s.  &lt;list_iterator object at 0x000000000E849C18&gt;
1   1      This is the 2nd s.  &lt;list_iterator object at 0x000000000E8691D0&gt;
2   2  This isnt the third s.  &lt;list_iterator object at 0x000000000E8696A0&gt;
</code></pre>

<p>But I'd like rather the text representation of the dependency graph inside of DataFrame column instead of the iterator, like so: </p>

<pre><code>    ID                sentence                                        parsed
0   0    This is the first s.  [[(('s.', 'NN'), 'nsubj', ('This', 'DT')),(('s.', 'NN'), 'cop', ('is', 'VBZ')), (('s.', 'NN'), 'det', ('the', 'DT')),(('s.', 'NN'), 'amod', ('first', 'JJ'))]]
                   ...
</code></pre>

<p>I've tried to follow the the nltk documentation by working in steps in pandas, but it results in an attribute error:</p>

<pre><code> df['dep'] = [list(parse.triples()) for parse in df.parsed]
 AttributeError: 'list_iterator' object has no attribute 'triples'
</code></pre>

<p>Is there a way to unpack an iterator that appears as a value in a DataFrame? Any help is welcome. </p>
","python, pandas, nltk, stanford-nlp","<p>A <code>list_iterator</code> is a mechanism for producing lists ""on demand"". It indeed does not have a method <code>triples()</code>, but the list that it produces in your case is indeed a list of triples:</p>

<pre><code>df['dep'] = [list(parse) for parse in df['parsed']]
</code></pre>
",1,1,315,2016-12-22 02:01:19,https://stackoverflow.com/questions/41274728/unpacking-list-iterator-from-nltk-stanford-stanforddependencyparser-inside-panda
How to create a custom model with my own entities,"<p>I have been trying to find some reference material on how to create custom models with my own entities , like if I want to recognize the name of sports from a text.How do I do it?</p>
","nlp, stanford-nlp, named-entity-recognition","<pre><code>    try {
        propFile = new File(System.getProperty(""user.dir"") + ""/src/edu/stanford/nlp/ie/crf/propfile.prop"");
        properties = new Properties();
        properties.load(new FileInputStream(propFile));

        String to = properties.getProperty(""serializeTo"");

        properties.setProperty(""serializeTo"", ""ner-customModel.ser.gz"");
        properties.setProperty(""trainFile"",System.getProperty(""user.dir"") + ""/src/edu/stanford/nlp/ie/crf/outputTokenized.tsv"");
        CRFClassifier crf = new CRFClassifier(properties);
        crf.train();
        String s2 = ""apples are apples"";

        System.out.println(crf.classifyToString(s2));

        crf.serializeClassifier(System.getProperty(""user.dir"") + ""/src/edu/stanford/nlp/ie/crf/ner-customModel.ser.gz"");

    } catch (IOException e) {
        e.printStackTrace();
    }
</code></pre>

<p>and declare the training file and other properties in the properties file.
This worked for me :)</p>
",1,1,792,2016-12-26 16:56:36,https://stackoverflow.com/questions/41334082/how-to-create-a-custom-model-with-my-own-entities
Extract entities from folksonomies,"<p>I am a newbie to NLP and related technologies. I have been researching on decomposing folksonomies such as, hashtags into individual terms (ex:- #harrypotterworld as harry potter world) in order to carry out Named-Entity Recognition.</p>

<p>But I did not come across any available library or previous work I could use for this. Is this achievable or am I following a wrong procedure? If so, are there any available libraries or algorithmic techniques I could use?</p>
","nlp, stanford-nlp","<p>What you are looking for is a compound splitter. As far as I know, this is a problem that does have some implementations, some of which work reasonably well. </p>

<p>Unfortunately most research I know of has been done on languages that tend to compound nouns (ie. German). <em>Fun fact: Hashtag is a compound word itself.</em></p>

<p>I once used this one: <a href=""http://ilps.science.uva.nl/resources/compound-splitter-nl/"" rel=""nofollow noreferrer"">http://ilps.science.uva.nl/resources/compound-splitter-nl/</a> It is an algorithm that works on Dutch. It basically uses a dictionary of uncompounded words an assumes a very uncomplicated grammar for compounding: Something along the lines of: Infixes such as <em>n</em> and <em>s</em> are allowed, and compounded words are always a combination of 2 or more uncompounded words from the dictionary. </p>

<p>I think you could use the given implementation for compounded hashtags, if you provided an English dictionary, and adapted the assumed grammar somewhat (You might not want infixes).</p>
",2,1,55,2016-12-28 05:58:31,https://stackoverflow.com/questions/41356198/extract-entities-from-folksonomies
What are trained models in NLP?,"<p>I am new to Natural language processing. Can anyone tell me what are the trained models in either OpenNLP or Stanford CoreNLP? While coding in java using apache openNLP package, we always have to include some trained models (found here <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">http://opennlp.sourceforge.net/models-1.5/</a> ). What are they?</p>
","java, nlp, stanford-nlp, opennlp","<p>A ""model"" <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">as downloadable for OpenNLP</a> is <a href=""https://en.wikipedia.org/wiki/Statistical_model"" rel=""nofollow noreferrer"">a set of data representing a set of probability distributions</a> used for predicting the structure you want (e.g. <a href=""https://en.wikipedia.org/wiki/Part_of_speech"" rel=""nofollow noreferrer"">part-of-speech</a> tags) from the input you supply (in the case of OpenNLP, typically text files).</p>

<p>Given that natural language is <a href=""https://en.wikipedia.org/wiki/Context-sensitive_grammar#As_model_of_natural_languages"" rel=""nofollow noreferrer"">context-sensitive</a><sup>†</sup>, this model is used in lieu of a rule-based system because it generally works better than the latter for <a href=""http://stp.lingfil.uu.se/~nivre/docs/statnlp.pdf"" rel=""nofollow noreferrer"">a number of reasons</a> which I won't expound here for the sake of brevity. For example, as <a href=""https://stackoverflow.com/questions/41403862/what-are-trained-models-in-nlp#comment-70027854"">you already mentioned</a>, the token <em>perfect</em> could be either a verb (<code>VB</code>) or an adjective (<code>JJ</code>) and this can only be disambiguated in context:</p>

<ul>
<li><em>This answer is perfect</em> &mdash; for this example, the following sequences of POS tags are possible (in addition to many more<sup>‡</sup>):

<ol>
<li><code>DT NN VBZ JJ</code></li>
<li><code>DT NN VBZ VB</code></li>
</ol></li>
</ul>

<p>However, according to a model which accurately represents (""correct"") English<sup>§</sup>, the probability of example 1 is greater than of example 2: <code>P([DT, NN, VBZ, JJ] | [""This"", ""answer"", ""is"", ""perfect""]) &gt; P([DT, NN, VBZ, VB] | [""This"", ""answer"", ""is"", ""perfect""])</code></p>

<hr>

<p><sup>†</sup>In reality, this is quite contentious, but I stress here that I'm talking about natural language as a whole (including semantics/pragmatics/etc.) and not just about natural-language <em>syntax</em>, which (in the case of English, at least) <a href=""https://link.springer.com/chapter/10.1007%2F978-94-009-3401-6_13"" rel=""nofollow noreferrer"">is considered by some to be context-free</a>.</p>

<p><sup>‡</sup>When analyzing language in a data-driven manner, in fact <em>any</em> combination of POS tags is ""possible"", but, given a sample of ""correct"" contemporary English with little noise, tag assignments which native speakers would judge to be ""wrong"" should have an extremely low probability of occurrence.</p>

<p><sup>§</sup>In practice, this means a model trained on a large, diverse corpus of (contemporary) English (or some other target domain you want to analyze) with appropriate tuning parameters (If I want to be even more precise, this footnote could easily be multiple paragraphs long).</p>
",5,1,690,2016-12-30 23:13:53,https://stackoverflow.com/questions/41403862/what-are-trained-models-in-nlp
Stanford Parser - MultiThreading issue - LexicalizedParser,"<p>Firstly, parsing is running smooth on small set of sentences - In order of 200ms to 1s - depending on the sentence size.</p>

<p>What do I want to achieve?</p>

<p>I want to parse 50L sentences in 1-2 hours.</p>

<p>Somehow, I need to convert this -></p>

<pre><code>            for(String sentence: sentences){
               Tree parsed = AnalysisUtilities.getInstance().parseSentence(job).parse;
            }
</code></pre>

<p>into multithreaded calls.
I wrote a multi threaded executor to do this, which looks like this -></p>

<pre><code>                MultiThreadExecutor&lt;String&gt; mte = new MultiThreadExecutor&lt;String&gt;(2, new JobExecutor&lt;String&gt;() {
                @Override
                public void executeJob(String job) {
                    Tree parsed = AnalysisUtilities.getInstance().parseSentence(job).parse;
                    inputTrees.add(parsed);
                }
            }, """");


            for(String sentence: sentences){
                mte.addJob(sentence);
            }
</code></pre>

<p><strong>It works fine on one thread</strong>, but as soon as I give multiple threads it breaks with a exception inside the Stanford parse function. Exception looks like this -> </p>

<blockquote>
  <p>java.lang.ArrayIndexOutOfBoundsException: 3
      at java.util.ArrayList.add(ArrayList.java:441)
      at edu.stanford.nlp.parser.lexparser.BaseLexicon.initRulesWithWord(BaseLexicon.java:300)
      at edu.stanford.nlp.parser.lexparser.BaseLexicon.isKnown(BaseLexicon.java:160)
      at edu.stanford.nlp.parser.lexparser.BaseLexicon.ruleIteratorByWord(BaseLexicon.java:212)
      at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.initializeChart(ExhaustivePCFGParser.java:1299)
      at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.parse(ExhaustivePCFGParser.java:388)
      at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:234)
      at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:189)
      at edu.cmu.ark.AnalysisUtilities.parseSentence(AnalysisUtilities.java:262)
      at edu.cmu.ark.QuestionAsker$1.executeJob(QuestionAsker.java:147)
      at edu.cmu.ark.QuestionAsker$1.executeJob(QuestionAsker.java:144)
      at edu.cmu.ark.MultiThreadExecutor$1.run(MultiThreadExecutor.java:37)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
      at java.lang.Thread.run(Thread.java:745)
  java.lang.RuntimeException: Dependencies not equal: ""Spacious/CD"" -> "".*./CC"" left 0 and ""Spacious/CD"" -> ""easy/RB"" right 1
      at edu.stanford.nlp.parser.lexparser.MLEDependencyGrammar.probTB(MLEDependencyGrammar.java:586)
      at edu.stanford.nlp.parser.lexparser.MLEDependencyGrammar.scoreTB(MLEDependencyGrammar.java:511)
      at edu.stanford.nlp.parser.lexparser.AbstractDependencyGrammar.scoreTB(AbstractDependencyGrammar.java:229)
      at edu.stanford.nlp.parser.lexparser.ExhaustiveDependencyParser.parse(ExhaustiveDependencyParser.java:322)
      at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:244)
      at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:189)
      at edu.cmu.ark.AnalysisUtilities.parseSentence(AnalysisUtilities.java:262)
      at edu.cmu.ark.QuestionAsker$1.executeJob(QuestionAsker.java:147)
      at edu.cmu.ark.QuestionAsker$1.executeJob(QuestionAsker.java:144)
      at edu.cmu.ark.MultiThreadExecutor$1.run(MultiThreadExecutor.java:37)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
      at java.lang.Thread.run(Thread.java:745)
  java.lang.NullPointerException
      at edu.stanford.nlp.parser.lexparser.BiLexPCFGParser.projectHooks(BiLexPCFGParser.java:342)
      at edu.stanford.nlp.parser.lexparser.BiLexPCFGParser.processEdge(BiLexPCFGParser.java:546)
      at edu.stanford.nlp.parser.lexparser.BiLexPCFGParser.processItem(BiLexPCFGParser.java:571)
      at edu.stanford.nlp.parser.lexparser.BiLexPCFGParser.parse(BiLexPCFGParser.java:854)
      at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:255)
      at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:189)
      at edu.cmu.ark.AnalysisUtilities.parseSentence(AnalysisUtilities.java:262)
      at edu.cmu.ark.QuestionAsker$1.executeJob(QuestionAsker.java:147)
      at edu.cmu.ark.QuestionAsker$1.executeJob(QuestionAsker.java:144)
      at edu.cmu.ark.MultiThreadExecutor$1.run(MultiThreadExecutor.java:37)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
      at java.lang.Thread.run(Thread.java:745)</p>
</blockquote>

<p>Is there any way to do it ? I can relate to a previously asked <a href=""https://stackoverflow.com/questions/9286597/stanford-parser-multithread-usage"">question</a> but to no good.</p>
","java, multithreading, nlp, stanford-nlp","<p>Here is an example command that will run the parser in multi-threaded mode:</p>

<pre><code>java -Xmx4g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse -parse.nthreads 4 -ssplit.eolonly -file some-sentences.txt -outputFormat text
</code></pre>
",1,1,231,2017-01-01 12:33:51,https://stackoverflow.com/questions/41415483/stanford-parser-multithreading-issue-lexicalizedparser
Stanford Parser for Python: Output Format,"<p>I am currently using the Python interface for the Stanford Parser. </p>

<pre><code>    from nltk.parse.stanford import StanfordParser
    import os

    os.environ['STANFORD_PARSER'] ='/Users/au571533/Downloads/stanford-parser-full-2016-10-31'
    os.environ['STANFORD_MODELS'] = '/Users/au571533/Downloads/stanford-parser-full-2016-10-31'
    parser=StanfordParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")

    new=list(parser.raw_parse(""The young man who boarded his usual train that Sunday afternoon was twenty-four years old and fat. ""))
    print new
</code></pre>

<p>The output I get looks something like this: </p>

<pre><code>    [Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['The']), Tree('JJ', ['young']), Tree('NN', ['man'])]), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['who'])]), Tree('S', [Tree('VP', [Tree('VBD', ['boarded']), Tree('NP', [Tree('PRP$', ['his']), Tree('JJ', ['usual']), Tree('NN', ['train'])]), Tree('NP', [Tree('DT', ['that']), Tree('NNP', ['Sunday'])])])])])]), Tree('NP', [Tree('NN', ['afternoon'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('NP', [Tree('JJ', ['twenty-four']), Tree('NNS', ['years'])]), Tree('ADJP', [Tree('JJ', ['old']), Tree('CC', ['and']), Tree('JJ', ['fat'])])])]), Tree('.', ['.'])])])]
</code></pre>

<p>However, I only need the part of speech labels, therefore I'd like to have an output in a format that looks like word/tag. </p>

<p>In java it is possible to specify -outputFormat 'wordsAndTags' and it gives exactly what I want. Any hint on how to implement this in Python? </p>

<p>Help would be GREATLY appreciated. 
Thanks!</p>

<p>PS: Tried to use the Stanford POSTagger but it is by far less accurate on some of the words I'm interested in. </p>
","python, parsing, nltk, stanford-nlp","<p>If you look at <a href=""http://www.nltk.org/_modules/nltk/parse/stanford.html"" rel=""nofollow noreferrer"">the NLTK classes for the Stanford parser</a>, you can see that the the <code>raw_parse_sents()</code> method doesn't send the <code>-outputFormat wordsAndTags</code> option that you want, and instead sends <code>-outputFormat Penn</code>.
If you derive your own class from <code>StanfordParser</code>, you could override this method and specify the <code>wordsAndTags</code> format.</p>

<pre><code>from nltk.parse.stanford import StanfordParser

class MyParser(StanfordParser):

        def raw_parse_sents(self, sentences, verbose=False):
        """"""
        Use StanfordParser to parse multiple sentences. Takes multiple sentences as a
        list of strings.
        Each sentence will be automatically tokenized and tagged by the Stanford Parser.
        The output format is `wordsAndTags`.

        :param sentences: Input sentences to parse
        :type sentences: list(str)
        :rtype: iter(iter(Tree))
        """"""
        cmd = [
            self._MAIN_CLASS,
            '-model', self.model_path,
            '-sentences', 'newline',
            '-outputFormat', 'wordsAndTags',
        ]
        return self._parse_trees_output(self._execute(cmd, '\n'.join(sentences), verbose))
</code></pre>
",2,1,1873,2017-01-07 14:28:39,https://stackoverflow.com/questions/41522476/stanford-parser-for-python-output-format
Stanford CorenNLP Phrase POS tags and lemmatization explanation,"<p>I have this result during lemmatization of the phrase:</p>

<blockquote>
  <p>Gathered requirements</p>
</blockquote>

<p>Using the <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">corenlp online tool</a>, POS-tagging and lemmatization of this phrase results in:</p>

<p><a href=""https://i.sstatic.net/XQGIz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XQGIz.png"" alt=""enter image description here""></a></p>

<p>For some reason ""Gathered"" is given a POS-tag of ""JJ"" (""adjective""), which presumably results in the lemma being ""gathered"" rather than ""gather"".</p>

<p>If the input phrase is <code>gathered requirements</code> (i.e. lower-cased), then the POS tag is correctly identified as a verb, and the lemmatization result is what I expected:</p>

<p><a href=""https://i.sstatic.net/lgwR8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lgwR8.png"" alt=""enter image description here""></a></p>

<p>Why is CoreNLP identifying <code>Gathered</code> as an adjective rather than a verb?</p>
","nlp, stanford-nlp","<p>The system checks the probability of POS tag that most happened for the word ""Gathered"" when you write ""Gathered requirments"". Only some kind of words such as named-entities, start of sentence, etc. start with a capital letter. The general reason that ""Gathered"" is more likely to be JJ when you start it with a capital letter is that it was mostly used as adjective not verb in start of sentence. </p>
",2,2,400,2017-01-11 17:28:34,https://stackoverflow.com/questions/41597099/stanford-corennlp-phrase-pos-tags-and-lemmatization-explanation
edu.stanford.nlp.io.RuntimeIOException error using Stanford nlp POS tagger,"<p>I am currently configuring my Java project in order to use Stanford's POS tagger. However I am currently getting the following 2 errors when running the program. </p>

<p>The first being:</p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:791)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:312)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:265)
at TagText.main(TagText.java:10)
</code></pre>

<p>And the second being:</p>

<pre><code>Caused by: java.io.IOException: Unable to open ""taggers/left3words-distsim-wsj-0-18.tagger"" as class path, filename or URL
at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:470)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:789)
... 3 more
</code></pre>

<p>I am using the example found here to build test the POS tagger:</p>

<p><a href=""http://new.galalaly.me//2011/05/tagging-text-with-stanford-pos-tagger-in-java-applications/"" rel=""nofollow noreferrer"">http://new.galalaly.me//2011/05/tagging-text-with-stanford-pos-tagger-in-java-applications/</a></p>
","java, nlp, stanford-nlp, buildpath","<p>Found the solution if anyone else ever needs it. You need to make sure that your MaxentTagger tagger has the correct and updated english file passing through it. </p>
",1,0,917,2017-01-12 16:38:23,https://stackoverflow.com/questions/41618481/edu-stanford-nlp-io-runtimeioexception-error-using-stanford-nlp-pos-tagger
Query Analysis to determine the relationship between words using natural language processing,"<p><strong>Sample Sentence</strong></p>

<p>a) Who is the Ceo of IBM?</p>

<p>b) Where is IBM office located?</p>

<p>The series of operation is applied to above sentence using tokenization, pos-tagging and chunking to extract relationship. </p>

<p><strong>Who</strong> is the <strong>Ceo of IBM</strong> -- (Extracted Tuple)  -->  [Who, Ceo of IBM]</p>

<p><strong>Where</strong> is <strong>IBM office located</strong> -- (Extracted Tuple) --> [Where, IBM Office Located]</p>

<p>From Extracted dependencies how will I determine that what the Question is about? How the <strong>WP</strong> and <strong>WHP</strong> words in the sentence indicate that what kind of query to be made to extract data from the knowledge-based data set.</p>

<p>like in a) <strong>Who</strong> is pointing toward name, place or any another named entity. </p>

<p>and in b)  <strong>Where</strong> is pointing toward name, place or any another named entity. </p>

<p>Any advice using natural language processing techniques or text mining is highly appreciated.</p>
","python, nltk, stanford-nlp","<p>It depends on the variability of input sentences you are expecting. For the examples you give, you could use very simple pattern matching. Just set up a few patterns such as</p>

<pre><code>WHO IS ...? -&gt; [who, ...]  
WHERE IS ...? -&gt; [where, ...]  
WHERE CAN I FIND ...? -&gt; [where, ...]
</code></pre>

<p>And then use string matching to locate those patterns in your input data. You could even use regular expressions if necessary:</p>

<pre><code>s/who is \(.*\)/[who, \1]/
</code></pre>

<p>(using sed-style search and replace here)</p>

<p>This will of course only match those particular examples, but if most of your data looks like it, you might not need a full-blown NLP approach. You can always add more patterns like that, though at some point it might become unmanageable. However, this could get you quite far for your particular problem.</p>

<p>You can of course do a full syntactic analysis, but it might be overkill &amp; too brittle. The right approach depends fully on your use case.</p>
",1,0,241,2017-01-14 15:27:18,https://stackoverflow.com/questions/41651625/query-analysis-to-determine-the-relationship-between-words-using-natural-languag
Stanford CoreNLP server&#39;s JSON response missing RelationExtractor annotations,"<p>I'm processing a simple sentence to test Stanford's <strong>RelationExtractor</strong>:</p>

<blockquote>
  <p>Microsoft is based in New York.</p>
</blockquote>

<p>(it's not)</p>

<p>When I'm annotating the sentence in Java, by directly using the CoreNLP jar files I get the wanted result - CoreNLP finds a <strong>OrgBased_In</strong> relation between <em>Microsoft</em> and <em>New York</em>.</p>

<pre><code>for (CoreMap sentence : sentences) {
    relationType = sentence.get(MachineReadingAnnotations.RelationMentionsAnnotation.class).get(0).type // =&gt; OrgBased_In
}
</code></pre>

<p>However, sending the same sentence into the <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">CoreNLP Server</a> like so:</p>

<pre><code>curl --data 'Microsoft is based in New York.' 'http://localhost:9000/?properties={%22annotators%22%3A%22tokenize%2Cssplit%2Cpos%2Clemma%2Cner%2Cparse%2Cdepparse%2Crelation%22%2C%22outputFormat%22%3A%22json%22}' -o -
</code></pre>

<p>Results in a json response that contains no data on relations whatsoever:</p>

<pre><code>{'sentences': [{'basicDependencies': [{'dep': 'ROOT',
                                   'dependent': 3,
                                   'dependentGloss': 'based',
                                   'governor': 0,
                                   'governorGloss': 'ROOT'},
                                  {'dep': 'nsubjpass',
                                   'dependent': 1,
                                   'dependentGloss': 'Microsoft',
                                   'governor': 3,
                                   'governorGloss': 'based'},
                                  {'dep': 'auxpass',
                                   'dependent': 2,
                                   'dependentGloss': 'is',
                                   'governor': 3,
                                   'governorGloss': 'based'},
                                  {'dep': 'case',
                                   'dependent': 4,
                                   'dependentGloss': 'in',
                                   'governor': 6,
                                   'governorGloss': 'York'},
                                  {'dep': 'compound',
                                   'dependent': 5,
                                   'dependentGloss': 'New',
                                   'governor': 6,
                                   'governorGloss': 'York'},
                                  {'dep': 'nmod',
                                   'dependent': 6,
                                   'dependentGloss': 'York',
                                   'governor': 3,
                                   'governorGloss': 'based'},
                                  {'dep': 'punct',
                                   'dependent': 7,
                                   'dependentGloss': '.',
                                   'governor': 3,
                                   'governorGloss': 'based'}],
            'enhancedDependencies': [{'dep': 'ROOT',
                                      'dependent': 3,
                                      'dependentGloss': 'based',
                                      'governor': 0,
                                      'governorGloss': 'ROOT'},
                                     {'dep': 'nsubjpass',
                                      'dependent': 1,
                                      'dependentGloss': 'Microsoft',
                                      'governor': 3,
                                      'governorGloss': 'based'},
                                     {'dep': 'auxpass',
                                      'dependent': 2,
                                      'dependentGloss': 'is',
                                      'governor': 3,
                                      'governorGloss': 'based'},
                                     {'dep': 'case',
                                      'dependent': 4,
                                      'dependentGloss': 'in',
                                      'governor': 6,
                                      'governorGloss': 'York'},
                                     {'dep': 'compound',
                                      'dependent': 5,
                                      'dependentGloss': 'New',
                                      'governor': 6,
                                      'governorGloss': 'York'},
                                     {'dep': 'nmod:in',
                                      'dependent': 6,
                                      'dependentGloss': 'York',
                                      'governor': 3,
                                      'governorGloss': 'based'},
                                     {'dep': 'punct',
                                      'dependent': 7,
                                      'dependentGloss': '.',
                                      'governor': 3,
                                      'governorGloss': 'based'}],
            'enhancedPlusPlusDependencies': [{'dep': 'ROOT',
                                              'dependent': 3,
                                              'dependentGloss': 'based',
                                              'governor': 0,
                                              'governorGloss': 'ROOT'},
                                             {'dep': 'nsubjpass',
                                              'dependent': 1,
                                              'dependentGloss': 'Microsoft',
                                              'governor': 3,
                                              'governorGloss': 'based'},
                                             {'dep': 'auxpass',
                                              'dependent': 2,
                                              'dependentGloss': 'is',
                                              'governor': 3,
                                              'governorGloss': 'based'},
                                             {'dep': 'case',
                                              'dependent': 4,
                                              'dependentGloss': 'in',
                                              'governor': 6,
                                              'governorGloss': 'York'},
                                             {'dep': 'compound',
                                              'dependent': 5,
                                              'dependentGloss': 'New',
                                              'governor': 6,
                                              'governorGloss': 'York'},
                                             {'dep': 'nmod:in',
                                              'dependent': 6,
                                              'dependentGloss': 'York',
                                              'governor': 3,
                                              'governorGloss': 'based'},
                                             {'dep': 'punct',
                                              'dependent': 7,
                                              'dependentGloss': '.',
                                              'governor': 3,
                                              'governorGloss': 'based'}],
            'index': 0,
            'parse': '(ROOT\n'
                     '  (S\n'
                     '    (NP (NNP Microsoft))\n'
                     '    (VP (VBZ is)\n'
                     '      (VP (VBN based)\n'
                     '        (PP (IN in)\n'
                     '          (NP (NNP New) (NNP York)))))\n'
                     '    (. .)))',
            'tokens': [{'after': ' ',
                        'before': '',
                        'characterOffsetBegin': 0,
                        'characterOffsetEnd': 9,
                        'index': 1,
                        'lemma': 'Microsoft',
                        'ner': 'ORGANIZATION',
                        'originalText': 'Microsoft',
                        'pos': 'NNP',
                        'word': 'Microsoft'},
                       {'after': ' ',
                        'before': ' ',
                        'characterOffsetBegin': 10,
                        'characterOffsetEnd': 12,
                        'index': 2,
                        'lemma': 'be',
                        'ner': 'O',
                        'originalText': 'is',
                        'pos': 'VBZ',
                        'word': 'is'},
                       {'after': ' ',
                        'before': ' ',
                        'characterOffsetBegin': 13,
                        'characterOffsetEnd': 18,
                        'index': 3,
                        'lemma': 'base',
                        'ner': 'O',
                        'originalText': 'based',
                        'pos': 'VBN',
                        'word': 'based'},
                       {'after': ' ',
                        'before': ' ',
                        'characterOffsetBegin': 19,
                        'characterOffsetEnd': 21,
                        'index': 4,
                        'lemma': 'in',
                        'ner': 'O',
                        'originalText': 'in',
                        'pos': 'IN',
                        'word': 'in'},
                       {'after': ' ',
                        'before': ' ',
                        'characterOffsetBegin': 22,
                        'characterOffsetEnd': 25,
                        'index': 5,
                        'lemma': 'New',
                        'ner': 'LOCATION',
                        'originalText': 'New',
                        'pos': 'NNP',
                        'word': 'New'},
                       {'after': '',
                        'before': ' ',
                        'characterOffsetBegin': 26,
                        'characterOffsetEnd': 30,
                        'index': 6,
                        'lemma': 'York',
                        'ner': 'LOCATION',
                        'originalText': 'York',
                        'pos': 'NNP',
                        'word': 'York'},
                       {'after': '',
                        'before': '',
                        'characterOffsetBegin': 30,
                        'characterOffsetEnd': 31,
                        'index': 7,
                        'lemma': '.',
                        'ner': 'O',
                        'originalText': '.',
                        'pos': '.',
                        'word': '.'}]}]}
</code></pre>

<p>I can see on the CoreNLP server terminal that the relation extraction model <em>is</em> loaded.</p>

<pre><code>[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.RelationExtractorAnnotator - Loading relation model from edu/stanford/nlp/models/supervised_relation_extractor/roth_relation_model_pipelineNER.ser
</code></pre>

<p>What am I missing here?</p>

<p>Thanks!</p>
","nlp, stanford-nlp, stanford-nlp-server, corenlp-server","<p>I think ultimately nobody added that output to the JSON for that annotator, which we can do eventually.</p>

<p>Right now the relation extraction we are mainly supporting is the new <code>kbp</code> annotator.  This extracts the relations from the TAC-KBP challenge.</p>

<p>You can find the relation descriptions here:
<a href=""https://tac.nist.gov//2015/KBP/ColdStart/guidelines/TAC_KBP_2015_Slot_Descriptions_V1.0.pdf"" rel=""nofollow noreferrer"">https://tac.nist.gov//2015/KBP/ColdStart/guidelines/TAC_KBP_2015_Slot_Descriptions_V1.0.pdf</a></p>

<p>Here is an example command that I ran:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,mention,entitymentions,coref,kbp -file microsoft-example.txt -outputFormat json
</code></pre>

<p>If you look at the JSON you'll see the proper relation has been extracted.</p>
",3,2,691,2017-01-16 15:17:54,https://stackoverflow.com/questions/41679542/stanford-corenlp-servers-json-response-missing-relationextractor-annotations
FileNotFoundException on tmp/roth_sentences.ser when training Stanford Relation Extractor model,"<p>I'm trying to train my own relation extraction model as described <a href=""http://nlp.stanford.edu/software/relationExtractor.html#training"" rel=""nofollow noreferrer"">here</a> but keep getting a strange error.</p>

<p>My properties file:</p>

<pre><code>#Below are some basic options. See edu.stanford.nlp.ie.machinereading.MachineReadingProperties class for more options.

# Pipeline options
annotators = pos, lemma, parse
parse.maxlen = 100

# MachineReading properties. You need one class to read the dataset into correct format. See edu.stanford.nlp.ie.machinereading.domains.ace.AceReader for another example.
datasetReaderClass = edu.stanford.nlp.ie.machinereading.domains.roth.RothCONLL04Reader

readerLogLevel = INFO
#Data directory for training. The datasetReaderClass reads data from this path and makes corresponding sentences and annotations.
trainPath = ../re-training-data.corp

#Whether to crossValidate, that is evaluate, or just train.
crossValidate = false
kfold = 10

#Change this to true if you want to use CoreNLP pipeline generated NER tags. The default model generated with the relation extractor release uses the CoreNLP pipeline provided tags (option set to true$
trainUsePipelineNER=true

# where to save training sentences. uses the file if it exists, otherwise creates it.
serializedTrainingSentencesPath = tmp/roth_sentences.ser

serializedEntityExtractorPath = tmp/roth_entity_model.ser

# where to store the output of the extractor (sentence objects with relations generated by the model). This is what you will use as the model when using 'relation' annotator in the CoreNLP pipeline.
serializedRelationExtractorPath = tmp/kpl-relation-model-pipeline.ser

# uncomment to load a serialized model instead of retraining
# loadModel = true

#relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter,edu.stanford.nlp.ie.machinereading.domains.roth.RothResultsByRelation. For printing output of the model.
relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter

#In this domain, this is trivial since all the entities are given (or set using CoreNLP NER tagger).
entityClassifier = edu.stanford.nlp.ie.machinereading.domains.roth.RothEntityExtractor

extractRelations = true
extractEvents = false

#We are setting the entities beforehand so the model does not learn how to extract entities etc.
extractEntities = false

#Opposite of crossValidate.
trainOnly=true

# The set chosen by feature selection using RothCONLL04:
relationFeatures = arg_words,arg_type,dependency_path_lowlevel,dependency_path_words,surface_path_POS,entities_between_args,full_tree_path
</code></pre>

<p>Here's what I run in the terminal:</p>

<pre><code>sudo java -cp stanford-corenlp-3.7.0.jar:stanford-corenlp-3.7.0-models.jar edu.stanford.nlp.ie.machinereading.MachineReading --arguments kpl-re-model.properties
</code></pre>

<p>And the the result:</p>

<pre><code>PERCENTAGE OF TRAIN: 1.0
The reader log level is set to INFO
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.8 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec].
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading makeResultsPrinters
INFO: Making result printers from 
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading makeResultsPrinters
INFO: Making result printers from edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading makeResultsPrinters
INFO: Making result printers from 
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading loadOrMakeSerializedSentences
INFO: Parsing corpus sentences...
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading loadOrMakeSerializedSentences
INFO: These sentences will be serialized to /home/ubuntu/stanford-corenlp-full-2016-10-31/tmp/roth_sentences.ser
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.domains.roth.RothCONLL04Reader read
INFO: Reading file: ../re-training-data.corp
Jan 17, 2017 4:55:07 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader preProcessSentences
SEVERE: GenericDataSetReader: Started pre-processing the corpus...
Jan 17, 2017 4:55:07 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader preProcessSentences
INFO: Annotating dataset with edu.stanford.nlp.pipeline.StanfordCoreNLP@5f9d02cb
Jan 17, 2017 4:58:32 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader preProcessSentences
SEVERE: GenericDataSetReader: Pre-processing complete.
Jan 17, 2017 4:58:32 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader parse
SEVERE: Changing NER tags using the CoreNLP pipeline.
Replacing old annotator ""parse"" with signature [edu.stanford.nlp.pipeline.ParserAnnotator#parse.maxlen:100;#] with new annotator with signature [edu.stanford.nlp.pipeline.ParserAnnotator##]
Adding annotator pos
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Jan 17, 2017 4:58:45 PM edu.stanford.nlp.ie.machinereading.MachineReading loadOrMakeSerializedSentences
INFO: Done. Parsed 1183 sentences.
Jan 17, 2017 4:58:45 PM edu.stanford.nlp.ie.machinereading.MachineReading loadOrMakeSerializedSentences
INFO: Serializing parsed sentences to /home/ubuntu/stanford-corenlp-full-2016-10-31/tmp/roth_sentences.ser...
Exception in thread ""main"" java.io.FileNotFoundException: tmp/roth_sentences.ser (No such file or directory)
    at java.io.FileOutputStream.open0(Native Method)
    at java.io.FileOutputStream.open(FileOutputStream.java:270)
    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)
    at edu.stanford.nlp.io.IOUtils.writeObjectToFile(IOUtils.java:77)
    at edu.stanford.nlp.io.IOUtils.writeObjectToFile(IOUtils.java:63)
    at edu.stanford.nlp.ie.machinereading.MachineReading.loadOrMakeSerializedSentences(MachineReading.java:914)
    at edu.stanford.nlp.ie.machinereading.MachineReading.run(MachineReading.java:270)
    at edu.stanford.nlp.ie.machinereading.MachineReading.main(MachineReading.java:111
</code></pre>

<p>The error states that it can't find 'tmp/roth_sentences.ser' but it doesn't make sense because it's supposed to <em>create</em> that file.</p>

<p>Any ideas?</p>

<p>Thanks!
Simon.</p>
","nlp, stanford-nlp","<p>I think if you change <code>tmp/roth_sentences.ser</code> to <code>roth_sentences.ser</code> it should work.  I'm guessing the issue is <code>/home/ubuntu/stanford-corenlp-full-2016-10-31/tmp</code> doesn't exist, so when it tries to write the file it crashes.</p>
",1,0,138,2017-01-17 17:14:33,https://stackoverflow.com/questions/41703155/filenotfoundexception-on-tmp-roth-sentences-ser-when-training-stanford-relation
Loading model in Stanford NER,"<p>I am using Maven. Have dependencies for <em>stanford-corenlp 3.7.0</em> and <em>stanford-corenlp models</em>. The demo code works fine, with some default model. How do I switch models using : <em>CRFClassifier.getClassifierNoExceptions(String loadpath)</em>
I tried a couple of options like</p>

<ul>
<li>/models/edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz</li>
<li>englishPCFG.caseless.ser.gz</li>
<li>/englishPCFG.caseless.ser.gz</li>
</ul>

<p>No luck.</p>
","maven, stanford-nlp","<p>This is a list of available models:</p>

<pre><code>edu/stanford/nlp/models/ner/english.nowiki.3class.caseless.distsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.muc.7class.nodistsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.all.3class.nodistsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.nowiki.3class.nodistsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.conll.4class.nodistsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz
</code></pre>

<p>You need the English models jar available at this link:</p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>For Maven you need stanford-corenlp-3.7.0-models-english.jar as well in your dependencies.</p>
",1,2,1367,2017-01-18 10:25:50,https://stackoverflow.com/questions/41716846/loading-model-in-stanford-ner
How to customize Stanford NER in python?,"<p>I learned how to customize Stanford NER (Named Entity Recognizer) in Java from here:</p>

<p><a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#a</a></p>

<p>But I am developing my project with Python and here I need to train my classier with some custom entities.</p>

<p>I searched a lot for a solution but could not find any. Any idea? If it is not possible, is there any other way to train my classifier with custom entities, i.e, with nltk or others in python?</p>

<p><strong>EDIT: Code addition</strong> 
This is what I did to set up and test Stanford NER which worked nicely:</p>

<pre><code>from nltk.tag.stanford import StanfordNERTagger
path_to_model = ""C:\..\stanford-ner-2016-10-31\classifiers\english.all.3class.distsim.crf.ser""
path_to_jar = ""C:\..\stanford-ner-2016-10-31\stanford-ner.jar""
nertagger=StanfordNERTagger(path_to_model, path_to_jar)
query=""Show  me the best eye doctor in Munich""
print(nertagger.tag(query.split()))
</code></pre>

<p>This code worked successfully. Then, I downloaded the sample austen.prop file and both jane-austen-emma-ch1.tsv and jane-austen-emma-ch2.tsv file and put it in a custom folder in NerTragger library folder. I modified the jane-austen-emma-ch1.tsv file with my custom entity tags. The code of austen.prop file has link to jane-austen-emma-ch1.tsv file. Now, I modified the above code as follow but it is not working:</p>

<pre><code>from nltk.tag.stanford import StanfordNERTagger
path_to_model = ""C:\..\stanford-ner-2016-10-31\custom/austen.prop""
path_to_jar = ""C:\..\stanford-ner-2016-10-31\stanford-ner.jar""
nertagger=StanfordNERTagger(path_to_model, path_to_jar)
query=""Show  me the best eye doctor in Munich""
print(nertagger.tag(query.split()))
</code></pre>

<p>But this code is producing the following error:</p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.StreamCorruptedException: invalid stream header: 236C6F63
    raise OSError('Java command failed : ' + str(cmd))
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1507)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3017)
Caused by: java.io.StreamCorruptedException: invalid stream header: 236C6F63
OSError: Java command failed : ['C:\\Program Files\\Java\\jdk1.8.0_111\\bin\\java.exe', '-mx1000m', '-cp', 'C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\stanford-ner-3.7.0-javadoc.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\stanford-ner-3.7.0-sources.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\stanford-ner-3.7.0.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\stanford-ner.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\lib\\joda-time.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\lib\\jollyday-0.4.9.jar;C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31\\lib\\stanford-ner-resources.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'C:/Users/HP/Desktop/Downloads1/Compressed/stanford-ner-2016-10-31/stanford-ner-2016-10-31/custom/austen.prop', '-textFile', 'C:\\Users\\HP\\AppData\\Local\\Temp\\tmppk8_741f', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf8']
    at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
    at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:301)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1462)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1494)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1505)
    ... 1 more
</code></pre>
","python, nlp, nltk, stanford-nlp","<p>The Stanford NER classifier is a java program. The NLTK's module is only an interface to the java executable. So you train a model exactly as you did before (or as you saw done in the link you provide). </p>

<p>In your code, you are confusing the training of a model with its use to chunk new text. The <code>.prop</code> file contains instructions for training a new model; it is not itself a model. This is what I recommend:</p>

<ol>
<li><p>Forget about python/nltk for the moment, and train a new model from the Windows command line (CMD prompt or whatever): Follow the how-to you mention in your question, to generate a serialized model (<code>.ser</code> file) named <code>ner-model.ser.gz</code> or whatever you decide to call it from your <code>.prop</code> file.</p></li>
<li><p>In your python code, set the <code>path_to_model</code> variable to point to the <code>.ser</code> file you generated in step 1. </p></li>
</ol>

<p>If you really want to control the training process from python, you could use the <a href=""https://docs.python.org/3/library/subprocess.html"" rel=""nofollow noreferrer"">subprocess</a> module to issue the appropriate command line commands. But it sounds like you don't really need this; just try to understand what these steps do so that you can carry them out properly.</p>
",4,1,3131,2017-01-18 14:38:41,https://stackoverflow.com/questions/41722217/how-to-customize-stanford-ner-in-python
Mute Stanford coreNLP logging,"<p>First of all, Java is not my usual language, so I'm quite basic at it. I need to use it for this particular project, so please be patient, and if I have omitted any relevant information, please ask for it, I will be happy to provide it.</p>

<p>I have been able to implement coreNLP, and, seemingly, have it working right, but is generating lots of messages like:</p>

<pre><code>ene 20, 2017 10:38:42 AM edu.stanford.nlp.process.PTBLexer next
ADVERTENCIA: Untokenizable: 【 (U+3010, decimal: 12304)
</code></pre>

<p>After some research (documentation, google, other threads here), I <em>think</em> (sorry, I don't know how I can tell for sure) coreNLP is finding the <code>slf4j-api.jar</code> in my classpath, and logging through it.</p>

<p>Which properties of the JVM can I use to set logging level of the messages that will be printed out?</p>

<p>Also, in which <code>.properties</code> file I could set them? (I already have a <code>commons-logging.properties</code>, a <code>simplelog.properties</code> and a <code>StanfordCoreNLP.properties</code> in my project's resource folder to set properties for other packages).</p>
","java, logging, slf4j, stanford-nlp","<p>Om’s answer is good, but two other possibly useful approaches:</p>

<ul>
<li>If it is just these warnings from the tokenizer that are annoying you, you can (in code or in StanfordCoreNLP.properties) set a property so they disappear: <code>props.setProperty(""tokenize.options"", ""untokenizable=NoneKeep"");</code>.</li>
<li>If slf4j is on the classpath, then, by default, our own Redwoods logger will indeed log through slf4j. So, you can also set the logging level using slf4j.</li>
</ul>
",3,2,1489,2017-01-20 10:15:08,https://stackoverflow.com/questions/41761099/mute-stanford-corenlp-logging
Can I use punctuation in Stanford CoreNLP Named Entities?,"<p>I'm trying to get Stanford Core NLP to recognise an identification code. The problem is the code has punctuation in it. e.g. <code>01.A01.01</code> which causes the input to be separated into three sentences.</p>
<p>The matching expression for this code would be <code>[0-9][0-9][.][a-z,A-Z][0-9][0-9][.][0-9][0-9]</code>. I've tried adding this into my <code>regexner.txt</code> file but it doesn't identify it (presumably because the tokens are across separate sentences?)</p>
<p>I've also tried to match it using a TokenRegex similar to the following (also without any success).</p>
<p><code>/tell/ /me/ /about/ (?$refCode /[0-9][0-9]/ /./ /[a-z,A-Z][0-9][0-9]/ /./ /[0-9][0-9]/ )</code></p>
<p>Some example uses...</p>
<blockquote>
<p>The user has resource 02.G36.63 reserved.</p>
<p>Is 21.J83.02 available?</p>
</blockquote>
<p>Does anyone have any ideas or suggestions?</p>
","nlp, stanford-nlp","<p>I took your sample input and replaced ""\n"" with "" "", to create:</p>

<pre><code>The user has resource 02.G36.63 reserved.  Is 21.J83.02 available?
</code></pre>

<p>I created this rules file (sample-rules.txt):</p>

<pre><code>02.G36.63       ID_CODE    MISC    2
</code></pre>

<p>And I ran this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,regexner -regexner.mapping sample-rules.txt -ssplit.eolonly -tokenize.whitespace -file sample-sentence.txt -outputFormat text
</code></pre>

<p>I got this output:</p>

<pre><code>Sentence #1 (9 tokens):
The user has resource 02.G36.63 reserved.  Is 21.J83.02 available?
[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the NamedEntityTag=O]
[Text=user CharacterOffsetBegin=4 CharacterOffsetEnd=8 PartOfSpeech=NN Lemma=user NamedEntityTag=O]
[Text=has CharacterOffsetBegin=9 CharacterOffsetEnd=12 PartOfSpeech=VBZ Lemma=have NamedEntityTag=O]
[Text=resource CharacterOffsetBegin=13 CharacterOffsetEnd=21 PartOfSpeech=NN Lemma=resource NamedEntityTag=O]
[Text=02.G36.63 CharacterOffsetBegin=22 CharacterOffsetEnd=31 PartOfSpeech=NN Lemma=02.g36.63 NamedEntityTag=ID_CODE]
[Text=reserved. CharacterOffsetBegin=32 CharacterOffsetEnd=41 PartOfSpeech=NN Lemma=reserved. NamedEntityTag=O]
[Text=Is CharacterOffsetBegin=43 CharacterOffsetEnd=45 PartOfSpeech=VBZ Lemma=be NamedEntityTag=O]
[Text=21.J83.02 CharacterOffsetBegin=46 CharacterOffsetEnd=55 PartOfSpeech=NN Lemma=21.j83.02 NamedEntityTag=O]
[Text=available? CharacterOffsetBegin=56 CharacterOffsetEnd=66 PartOfSpeech=NN Lemma=available? NamedEntityTag=O]
</code></pre>

<p>This said to just tokenize on whitespace, so it stopped breaking on the periods.  Also it said to only split sentences on newline, so it is important in the input file to put the entire user request on one line.  You won't get sentences, but you can get a token stream and identify your product codes.</p>

<p>Now if you really want the full power of Stanford CoreNLP and you don't want to have these codes split, you could take the ambitious route and alter the tokenizer PTBLexer.flex file to include all of your id codes.</p>

<p>That file is here in the repo:</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/PTBLexer.flex"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/PTBLexer.flex</a></p>

<p>You'll have to Google around to find instructions on compiling the PTBLexer.flex file into PTBLexer.java.  This site should have the info you need:</p>

<p><a href=""http://www.jflex.de/"" rel=""nofollow noreferrer"">http://www.jflex.de/</a></p>

<p>This would basically mean adding in your id codes and making a few slight edits, and then rebuilding PTBLexer.  Then with your custom tokenizer Stanford CoreNLP would treat your product codes like complete tokens and you could have normal sentence splitting if you want to do something like analyze the dependency structure of your user requests.</p>
",1,1,372,2017-01-20 16:34:09,https://stackoverflow.com/questions/41768321/can-i-use-punctuation-in-stanford-corenlp-named-entities
Stanford NLP - NER &amp; Models,"<p>I was looking at the online demo: <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/ner/process</a>
Try a simple testcase like: <em>John Chambers studied in London (UK) and Mumbai (India).</em>
The 3-class Classifier identifies the Person, the 7-class Classifier does not identify the Person. Seems like I need to run the parser on both the Models: once to identify Person, Location &amp; Organization. And once just for Currency?</p>
",stanford-nlp,"<p>When I run this command it finds all of the appropriate entities on your example:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -file sample-sentence.txt -outputFormat text
</code></pre>

<p>When you run the NERCombinerAnnotator which corresponds to the annotator <code>ner</code> it will run a combination of several models automatically for you.</p>
",1,0,254,2017-01-21 16:36:30,https://stackoverflow.com/questions/41781803/stanford-nlp-ner-models
Filter Stanford Dependency Parser Output,"<p>How can I modify this code to get only one particular output from the code. For example how can I get just <code>'nmod'</code> or <code>'dobj'</code> in output? </p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser
from nltk.tokenize import word_tokenize
from nltk.tree import Tree
stanford_models = 'E:\stanford-parser\stanford-parser-3.7.0-models.jar'
stanford_jar = 'E:\stanford-parser\stanford-parser.jar'
st = StanfordDependencyParser(stanford_models, stanford_jar, encoding='utf-8')
text = 'Randy,Can you send me a schedule of the salary.'
result= st.raw_parse(text)
dep = result.__next__()
list(dep.triples())
</code></pre>

<p>The output is:</p>

<pre><code>[(('send', 'VB'), 'discourse', ('Randy', 'UH')),
 (('send', 'VB'), 'aux', ('Can', 'MD')),
 (('send', 'VB'), 'nsubj', ('you', 'PRP')),
 (('send', 'VB'), 'iobj', ('me', 'PRP')),
 (('send', 'VB'), 'dobj', ('schedule', 'NN')),
 (('schedule', 'NN'), 'det', ('a', 'DT')),
 (('schedule', 'NN'), 'nmod', ('salary', 'NN')),
 (('salary', 'NN'), 'case', ('of', 'IN')),
 (('salary', 'NN'), 'det', ('the', 'DT'))]
</code></pre>
","python, nltk, stanford-nlp","<p>The only thing you have to do is <a href=""https://docs.python.org/3.6/library/functions.html#filter"" rel=""nofollow noreferrer""><code>filter(..)</code></a> and perhaps convert back to a <code>list(..)</code>:</p>

<pre><code>the_triples = list(dep.triples()) #you already have this line
result = filter(lambda v : v[1] == 'nmod' or v[1] == 'dobj',the_triples)
</code></pre>

<p>When you run <a href=""/questions/tagged/python-2.x"" class=""post-tag"" title=""show questions tagged &#39;python-2.x&#39;"" rel=""tag"">python-2.x</a>, <code>result</code> will be a list, if you work with <a href=""/questions/tagged/python-3.x"" class=""post-tag"" title=""show questions tagged &#39;python-3.x&#39;"" rel=""tag"">python-3.x</a>, the result will be a <em>generator</em> (and thus processing is delayed until you really need the values). You can convert the generator to a list by calling <code>list(..)</code> on it.</p>

<p><a href=""https://docs.python.org/3.6/library/functions.html#filter"" rel=""nofollow noreferrer""><code>filter(function,iterable)</code></a> takes as input a function and an iterable. As <code>iterable</code> we feed it the list of triples, as <code>function</code> we use <code>v : v[1] == 'nmod' or v[1] == 'dobj'</code> which is a function that takes the triple and succeeds given the second element of the triple is either <code>'nmod'</code> or <code>'dobj'</code>. So given the function evaluates the triple to <code>True</code>, the element will be emitted, otherwise it will be ignored.</p>
",3,2,262,2017-01-22 21:34:38,https://stackoverflow.com/questions/41796403/filter-stanford-dependency-parser-output
Function vs Content Words,"<p>How do I distinguish between <a href=""https://en.wikipedia.org/wiki/Function_word"" rel=""nofollow noreferrer"">function/structure words and content/lexical words</a>?</p>

<p>I am already using <a href=""http://stanfordnlp.github.io/CoreNLP"" rel=""nofollow noreferrer"">StanfordCoreNLP</a>, so I would like to leverage it, if possible.</p>

<p>More specifically, which <a href=""http://stanfordnlp.github.io/CoreNLP/annotators.html"" rel=""nofollow noreferrer"">annotator</a> should I use and how would it mark content/lexical words?</p>

<p>I tried <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow noreferrer""><code>pos</code></a> but it  does not distinguish between function and content words.</p>

<p>PS. I use the <code>lemma</code> annotator to get the words which I want to ignore.</p>

<p>PPS. I use <a href=""https://stackoverflow.com/a/40496870/850781""><code>pyconlp</code></a>.</p>
","nlp, stanford-nlp","<p>Function words (stop words) are often manually curated because they vary by domain. You can find a general purpose list in NLTK. CoreNLP also has one <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>from nltk.corpus import stopwords
stops = stopwords.words('english')
</code></pre>

<p>However, you should still look at them to see if they make sense for you use case. I have recently been working with technical language, so I removed 'it' from my list because 'IT' is an acronym in this domain and thus a content word.</p>

<p>For your annotator, you could go with the general purpose TokenizerAnnotator which will split your text into ""words"". You can then check each word to see if it exists in your stopword list. If you are working with strings, just try splitting them on whitespace and removing or marking stopwords as a gut check.</p>
",1,2,2294,2017-01-23 17:07:25,https://stackoverflow.com/questions/41811790/function-vs-content-words
Part of speech tagged as &quot;word&quot;,"<p>I'm using the <a href=""http://nlp.stanford.edu/software/tagger.html"" rel=""nofollow noreferrer"">Stanford Part of Speech tagger</a> on some Spanish text. As per their docs the part of speech tags come from this set: <a href=""http://nlp.stanford.edu/software/spanish-faq.shtml#tagset"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/spanish-faq.shtml#tagset</a></p>

<p>Overall, I've found this to be accurate and haven't had an issue. However, I just ran into a small snippet of text: ""Adiós ~ hailey"". This is tagged as follows: <code>Adiós_i ~_word hailey_aq0000</code>. So the <code>~</code> symbol, which I think should get a punctuation tag of <code>f0</code> got a tag of <code>word</code>. That isn't documented or expected. Is this a bug or expected?</p>

<h1>Update</h1>

<p>It turns out the special ""word"" tag appears in other contexts as well. I just saw it for the word <code>it</code> and the word <code>á</code>.</p>
",stanford-nlp,"<p>Thanks for catching this! I've been a bit slow to catch up on documentation.. I just updated <a href=""http://nlp.stanford.edu/software/spanish-faq.shtml#tagset"" rel=""nofollow noreferrer"">the tag list in our documentation</a> to include the new <code>word</code>.</p>

<p>In the CoreNLP 3.7.0 release, we included new Spanish models trained on extra data (specifically, the DEFT Spanish Treebank V2). Some of the new data comes from a discussion forum dataset (Latin American Spanish Discussion Forum Treebank). This dataset uses an extra POS tag, <code>word</code>, to label emoticons and miscellaneous symbols (e.g. the &reg; sign).</p>

<p>(I know, it's a sort of silly choice of name — but we wanted to stick with what the original corpus used.)</p>
",1,1,136,2017-01-24 22:20:50,https://stackoverflow.com/questions/41840023/part-of-speech-tagged-as-word
Stanford CoreNLP and Emoji?,"<p>So far when I tried to use emoji and using the POS tagger, it appeared as unknown symbols, small boxes. Is there a way to get the POS tagger to work with emoji? Emoji (eg <code>😀</code>) the unicode versions.</p>
","java, nlp, stanford-nlp","<p>Provided the character encoding is correct throughout your code, system and the Stanford CoreNLP code, emoji should be represented correctly. However, you'll have two more fundamental problems:</p>

<p>First, emoji are one character long and they are unlikely to be tagged as anything other than an indefinite article. 'a' in English. A smart tokenizer might make better sense of emoji, but I doubt it.</p>

<p>Secondly, and more importantly, POS taggers annotate <a href=""https://en.wikipedia.org/wiki/Part_of_speech"" rel=""nofollow noreferrer"">parts of speech</a>. Emoji are not a part of speech. In the very least, they are an independent, new class of tokens, but certainly not grammatical.</p>

<p>All that said ... you know their character codes ... they're already tagged.</p>
",2,1,693,2017-01-26 17:59:32,https://stackoverflow.com/questions/41879957/stanford-corenlp-and-emoji
Dependency Parsing graph for a paragraph,"<p>I am working on a NLP project. I want to create a dependency parsing graph for an entire paragraph, instead of sentence. Is there an existing method for the same? </p>
","nlp, dependency-parsing, stanford-parser","<p>There is no such dependency parsing for a paragraph but you can use Stanford coreference resolution on the sentences and extract particular dependencies.</p>
",1,1,487,2017-01-30 14:07:30,https://stackoverflow.com/questions/41937898/dependency-parsing-graph-for-a-paragraph
"Stanford NER and POS, Multithreading for a large data","<p>I am trying to use <strong>Stanford NER</strong> and <strong>Stanford POS Tagger</strong> to parse about 23000 documents. I have implemented it using the following pseudocode - </p>

<pre><code>`for each in document:
  eachSentences = PunktTokenize(each)
  #code to generate NER Tagger
  #code to generate POS Taggers on the above output`
</code></pre>

<p>For a 4 core machine, with 15 GB RAM, the run time just for NER is approximately, 945 hours. I have tried to step up things by using the ""threading"" library, but I get the following error-</p>

<pre><code>`Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""removeStopWords.py"", line 75, in partofspeechRecognition
    listOfRes_new = namedEntityRecognition(listRes[min:max])
  File ""removeStopWords.py"", line 63, in namedEntityRecognition
    listRes_ner.append(namedEntityRecognitionResume(eachResSentence))
  File ""removeStopWords.py"", line 50, in namedEntityRecognitionResume
    ner2Tags = ner2.tag(each.title().split())
  File ""/home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 71, in tag
    return sum(self.tag_sents([tokens]), [])
  File ""/home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 98, in tag_sents
    os.unlink(self._input_file_path)
OSError: [Errno 2] No such file or directory: '/tmp/tmpvMNqwB'`
</code></pre>

<p><strong>I am using NLTK version - 3.2.1, Stanford NER,POS - 3.7.0 jar file</strong>, along with the threading module. As far as I can see, this might be due to a thread lock on /tmp. <strong>Please correct me if I am wrong, also what is the best way to run the above using threads or a better way to implement it.</strong></p>

<p>I am using the <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">3 Class Classifier for NER</a> and <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/tagger/maxent/MaxentTagger.html"" rel=""nofollow noreferrer"">Maxent POS Tagger</a></p>

<p>P.S. - Please ignore the name of the Python file, I still haven't removed the stopwords or the punctuations from the original text.</p>

<p>Edit - Using cProfile, and sorting on cumulative time, I got the following top 20 calls</p>

<pre><code>600792 function calls (595912 primitive calls) in 60.795 seconds

Ordered by: cumulative time
List reduced from 3357 to 20 due to restriction &lt;20&gt;

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    1    0.000    0.000   60.811   60.811 removeStopWords.py:1(&lt;module&gt;)
    1    0.000    0.000   58.923   58.923 removeStopWords.py:76(partofspeechRecognition)
   28    0.001    0.000   58.883    2.103 /home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/tag/stanford.py:69(tag)
   28    0.004    0.000   58.883    2.103 /home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/tag/stanford.py:73(tag_sents)
   28    0.001    0.000   56.927    2.033 /home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/internals.py:63(java)
  141    0.001    0.000   56.532    0.401 /usr/lib/python2.7/subprocess.py:769(communicate)
  140    0.002    0.000   56.530    0.404 /usr/lib/python2.7/subprocess.py:1408(_communicate)
  140    0.008    0.000   56.492    0.404 /usr/lib/python2.7/subprocess.py:1441(_communicate_with_poll)
  400   56.474    0.141   56.474    0.141 {built-in method poll}
    1    0.001    0.001   43.522   43.522 removeStopWords.py:69(partofspeechRecognitionRes)
    1    0.000    0.000   15.401   15.401 removeStopWords.py:62(namedEntityRecognition)
    1    0.001    0.001   15.367   15.367 removeStopWords.py:46(namedEntityRecognitionRes)
  141    0.004    0.000    2.302    0.016 /usr/lib/python2.7/subprocess.py:651(__init__)
  141    0.020    0.000    2.287    0.016 /usr/lib/python2.7/subprocess.py:1199(_execute_child)
   56    0.002    0.000    1.933    0.035 /home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/internals.py:38(config_java)
   56    0.001    0.000    1.931    0.034 /home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/internals.py:599(find_binary)
  112    0.002    0.000    1.930    0.017 /home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/internals.py:582(find_binary_iter)
  118    0.009    0.000    1.928    0.016 /home/datascience/pythonEnv/local/lib/python2.7/site-packages/nltk/internals.py:453(find_file_iter)
    1    0.001    0.001    1.318    1.318 /usr/lib/python2.7/pickle.py:1383(load)
    1    0.046    0.046    1.317    1.317 /usr/lib/python2.7/pickle.py:851(load) 
</code></pre>
","python, multithreading, nltk, stanford-nlp","<p>It seems like the Python wrapper is the culprit here. Java implementation is not taking as much time. It's takes approximately what @Gabor Angeli mentioned. Try it.</p>

<p>Hope it helps!</p>
",1,2,755,2017-01-31 04:49:33,https://stackoverflow.com/questions/41949846/stanford-ner-and-pos-multithreading-for-a-large-data
Stanford Relation Extractor custom model selects only one token of relation entities,"<p>I've successfully trained a Relation Extractor model and created a .ser file.</p>

<p>However, I'm running into an issue where the model successfully finds a relation but if one of its entities consists of multiple tokens, <strong>only one token is selected</strong>.
For example, for a relation called <em>Friend_of</em>, and a sentence like:</p>

<blockquote>
  <p>Sam Tarly's best friend is Jon Snow.</p>
</blockquote>

<p>The model will find a relation of type Friend_of between the following entities:</p>

<ul>
<li>Tarly</li>
<li>Jon</li>
</ul>

<p>This causes my tests to mark this as a <strong>false positive</strong> and the model as a whole to get a bad score.</p>

<p>I've tried training a custom NER model using the same training data, and then using this custom NER model to train the RelationExtractor model with the following properties in my props file:</p>

<pre><code>trainUsePipelineNER=true
ner.model=path/to/custom-ner-model.ser.gz
</code></pre>

<p>But that didn't solve the problem.</p>

<p>Is this just a problem of not enough training data or is there something I'm missing here?</p>

<p>Here is the Java code I use to get the relations:</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, depparse, relation"");
props.put(""sup.relation.model"", ""lib/custom-relation-model-pipeline.ser"");
props.put(""pos.ptb3Escaping"", ""false"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

List&lt;Relation&gt; foundRelations = new ArrayList&lt;&gt;();

for (String doc : documents) {
    Annotation document = new Annotation(doc);
    pipeline.annotate(document);
    List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

    for (CoreMap sentence : sentences) {

        List&lt;RelationMention&gt; relationMentions = sentence.get(MachineReadingAnnotations.RelationMentionsAnnotation.class);

        for (RelationMention relation : relationMentions) {
            foundRelations.add(new Relation(relation.getArg(0).getValue(), relation.getType(), relation.getArg(1).getValue()));
        }

    }
}
</code></pre>

<p>Thank you!</p>

<p>Simon.</p>
","nlp, stanford-nlp","<p>So I looked into the MachineReading relation extraction some more.</p>

<p>I think you want to replace <code>getValue()</code> with <code>getExtentString()</code> and see if that helps.</p>

<p>I ran on a sample sentence with our default model:</p>

<p><code>Joe Smith works at Google.</code></p>

<p>And it worked properly.</p>
",1,2,706,2017-02-01 14:33:53,https://stackoverflow.com/questions/41982308/stanford-relation-extractor-custom-model-selects-only-one-token-of-relation-enti
when stanford-nlp OpenIE extract incorrect information from sentence?,"<p>in OpenIE System in stanford-nlp when enter some Sentence it's can Extract a correct information and anther Sentence my be failed to extract the information 
if there any rules to write the Sentence ?
if i want to extract a specific information it's possible or not ?</p>
","java, stanford-nlp","<p>Like any other statistical system, OpenIE has a certain accuracy that's somewhere less than 100%. You should not expect it to work on every sentence perfectly. If you notice a systematic class of mistakes that it makes, I'd be happy to take a look and see if I can do anything to improve the system.</p>
",1,0,94,2017-02-04 12:39:19,https://stackoverflow.com/questions/42040463/when-stanford-nlp-openie-extract-incorrect-information-from-sentence
core-nlp coreference resolution: remaping co-references,"<p>I have been trying to play with the core-nlp co-reference resolution system. The system works as explained in the tutorial. Below is the code for the same: </p>

<pre><code>public static void main(String[] args) throws Exception {
    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,mention,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
</code></pre>

<p>which outputs :</p>

<pre><code>CHAIN3-[""Barack Obama"" in sentence 1, ""He"" in sentence 1]
</code></pre>

<p>What I am trying to get is a map which shows </p>

<pre><code>Key | Value
He : Barack Obama
Obama: Barack Obama
</code></pre>

<p>Is there an inbuilt method to achieve this or do I have to post-process this (Not just the Map)?</p>
","java, stanford-nlp","<p>At the moment there isn't really code for that.  Here is a snippet that will print out the mention gloss, position info, and canonical mention:</p>

<pre><code>for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
    CorefChain.CorefMention representativeMention = cc.getRepresentativeMention();
    for (CorefChain.CorefMention cm : cc.getMentionsInTextualOrder()) {
      String position = ""sentence num: ""+cm.sentNum+"" position: ""+cm.startIndex;
      System.out.println(cm.mentionSpan + ""\t"" + position + ""\t"" + representativeMention.mentionSpan);
}
</code></pre>

<p>}</p>
",1,0,68,2017-02-08 09:02:46,https://stackoverflow.com/questions/42108716/core-nlp-coreference-resolution-remaping-co-references
Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl,"<p>I am running the cort coreference resolution from <a href=""https://github.com/smartschat/cort/blob/master/COREFERENCE.md"" rel=""nofollow noreferrer"">this</a> github repo. Using the syntax to run the system on raw input text as follows:</p>

<pre><code>cort-predict-raw -in *.txt \ 
           -model model.obj \
           -extractor cort.coreference.approaches.mention_ranking.extract_substructures \
           -perceptron cort.coreference.approaches.mention_ranking.RankingPerceptron \
           -clusterer cort.coreference.clusterer.all_ante \
           -corenlp /home/kenden/deeshacodes/corenlp \
</code></pre>

<p>I get the following error :-</p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:57)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:38)
    at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.&lt;init&gt;(NumberSequenceClassifier.java:86)
    at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:132)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:121)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$6.create(AnnotatorFactories.java:273)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:154)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:137)
    at corenlp.JsonPipeline.initializeCorenlpPipeline(JsonPipeline.java:206)
    at corenlp.SocketServer.main(SocketServer.java:102)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
    ... 13 more
Caused by: java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:466)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
    ... 15 more
Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
    at de.jollyday.util.CalendarUtil.&lt;init&gt;(CalendarUtil.java:42)
    at de.jollyday.HolidayManager.&lt;init&gt;(HolidayManager.java:66)
    at de.jollyday.impl.DefaultHolidayManager.&lt;init&gt;(DefaultHolidayManager.java:46)
    at edu.stanford.nlp.time.JollyDayHolidays$MyXMLManager.&lt;init&gt;(JollyDayHolidays.java:148)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:466)
    at java.base/java.lang.Class.newInstance(Class.java:556)
    at de.jollyday.caching.HolidayManagerValueHandler.instantiateManagerImpl(HolidayManagerValueHandler.java:60)
    at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:41)
    at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:13)
    at de.jollyday.util.Cache.get(Cache.java:51)
    at de.jollyday.HolidayManager.createManager(HolidayManager.java:168)
    at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)
    at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)
    at edu.stanford.nlp.time.Options.&lt;init&gt;(Options.java:90)
    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.&lt;init&gt;(TimeExpressionExtractorImpl.java:39)
    ... 20 more
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:532)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:186)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:473)
    ... 39 more
</code></pre>

<p>I have tried corenlp version 3.5.2, 3.6.0 as well as 3.7.0 but nothing works. Where would I be going wrong?</p>
","python, nlp, stanford-nlp","<p>You need to make sure you use the proper dependencies with the proper version.</p>

<p>If you use Stanford CoreNLP 3.7.0, make sure you also have the latest lib and liblocal folders.</p>

<p>I believe this error is because you have an incompatible dependency jar somewhere.</p>

<p>Update:  This is an error due to Java 9.  Add this flag</p>

<pre><code>--add-modules java.se.ee
</code></pre>

<p>and it should go away.</p>
",3,1,11375,2017-02-08 18:11:48,https://stackoverflow.com/questions/42120594/error-creating-edu-stanford-nlp-time-timeexpressionextractorimpl
Stanford Classifier with Real Valued Features,"<p>I'd like to use the <a href=""http://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">Stanford Classifier</a> for text classification. My features are mostly textual, but there are some numeric features as well (e.g. the length of a sentence). </p>

<p>I started off with the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/classify/ClassifierExample.java"" rel=""nofollow noreferrer"">ClassifierExample</a> and replaced the current features by a simple real valued feature <code>F</code> with value <code>100</code> if a stop light is <code>BROKEN</code> and <code>0.1</code> otherwise, which results in the following code (apart from the <code>makeStopLights()</code> function in line 10-16, this is just the code of the original ClassifierExample class):</p>

<pre><code>public class ClassifierExample {

    protected static final String GREEN = ""green"";
    protected static final String RED = ""red"";
    protected static final String WORKING = ""working"";
    protected static final String BROKEN = ""broken"";

    private ClassifierExample() {} // not instantiable

    // the definition of this function was changed!!
    protected static Datum&lt;String,String&gt; makeStopLights(String ns, String ew) {
        String label = (ns.equals(ew) ? BROKEN : WORKING);
        Counter&lt;String&gt; counter = new ClassicCounter&lt;&gt;();
        counter.setCount(""F"", (label.equals(BROKEN)) ? 100 : 0.1);
        return new RVFDatum&lt;&gt;(counter, label);
    }


    public static void main(String[] args) {
        // Create a training set
        List&lt;Datum&lt;String,String&gt;&gt; trainingData = new ArrayList&lt;&gt;();
        trainingData.add(makeStopLights(GREEN, RED));
        trainingData.add(makeStopLights(GREEN, RED));
        trainingData.add(makeStopLights(GREEN, RED));
        trainingData.add(makeStopLights(RED, GREEN));
        trainingData.add(makeStopLights(RED, GREEN));
        trainingData.add(makeStopLights(RED, GREEN));
        trainingData.add(makeStopLights(RED, RED));
        // Create a test set
        Datum&lt;String,String&gt; workingLights = makeStopLights(GREEN, RED);
        Datum&lt;String,String&gt; brokenLights = makeStopLights(RED, RED);
        // Build a classifier factory
        LinearClassifierFactory&lt;String,String&gt; factory = new LinearClassifierFactory&lt;&gt;();
        factory.useConjugateGradientAscent();
        // Turn on per-iteration convergence updates
        factory.setVerbose(true);
        //Small amount of smoothing
        factory.setSigma(10.0);
        // Build a classifier
        LinearClassifier&lt;String,String&gt; classifier = factory.trainClassifier(trainingData);
        // Check out the learned weights
        classifier.dump();
        // Test the classifier
        System.out.println(""Working instance got: "" + classifier.classOf(workingLights));
        classifier.justificationOf(workingLights);
        System.out.println(""Broken instance got: "" + classifier.classOf(brokenLights));
        classifier.justificationOf(brokenLights);
    }

}
</code></pre>

<p>In my understanding of linear classifiers, feature <code>F</code> should make the classification task pretty easy - after all, we just need to check whether the value of <code>F</code> is greater than some threshold. However, the classifier returns <code>WORKING</code> on every instance in the test set. </p>

<p>Now my question is: Have I made something wrong, do I need to change some other parts of the code as well for real-valued features to work or is there something wrong with my understanding of linear classifiers?</p>
","java, machine-learning, classification, stanford-nlp, text-classification","<p>Your code looks fine.  Note that typically with a Maximum Entropy classifier you provide binary valued features (1 or 0).</p>

<p>Here is some more reading on Maximum Entropy classifiers: <a href=""http://web.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers"" rel=""nofollow noreferrer"">http://web.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers</a></p>

<p>Look at slide titled: ""Feature-Based Linear Classifiers"" to see the specific probability calculation for Maximum Entropy classifiers.</p>

<p>Here is the formula for your example case with 1 feature and 2 classes (""works"", ""broken""):</p>

<p><code>probability(c1) = exp(w1 * f1) / total
probability(c2) = exp(w2 * f1) / total
total = exp(w1 * f1) + exp(w2 * f1)</code></p>

<p>w1 is the learned weight for ""works"" and w2 is the learned weight for ""broken""</p>

<p>The classifier selects the higher probability.  Note that f1 = (100 or 0.1) your feature value.</p>

<p>If you consider your specific example data, since you have (2 classes, 1 feature, feature is always positive), it is not possible to build a maximum entropy classifier that will separate that data, it will always guess all one way or the other.</p>

<p>For sake of argument say <code>w1 &gt; w2</code>.</p>

<p>Say <code>v &gt; 0</code> is your feature value (either 100 or 0.1).</p>

<p>Then <code>w1 * v &gt; w2 * v</code>, thus <code>exp(w1 * v) &gt; exp(w2 * v)</code>, so you'll always assign more probability to class1 regardless of what value v has.</p>
",1,1,212,2017-02-09 15:55:23,https://stackoverflow.com/questions/42141223/stanford-classifier-with-real-valued-features
Finding the Head Word in Python,"<p>I need to extract the head words of sentences (more specifically, the head words of the highest noun phrase in a sentence). I am using the Stanford CoreNLP server through py-corenlp to annotate my sentences. The suite has a modification of Michael Collin's head word finding algorithm, but I have not found any method to use it through the server. I would like to avoid reinventing the wheel, so is there any way I can achieve this with existing tools in Python?</p>

<p>Example: </p>

<blockquote>
  <p><em>The <strong>number</strong> of elementary entities in 1 mole of a substance</em> is known as what? </p>
</blockquote>

<pre><code>(ROOT
  (S
    (NP
      (NP (DT The) (NN number))
      (PP (IN of)
        (NP
          (NP (JJ elementary) (NNS entities))
          (PP (IN in)
            (NP
              (NP (CD 1) (NN mole))
              (PP (IN of)
                (NP (DT a) (NN substance))))))))
    (VP (VBZ is)
      (VP (VBN known)
        (PP (IN as)
          (NP (WP what)))))
    (. ?)))
</code></pre>

<p>""The number of elementary entities in 1 mole of a substance"" is the highest noun phrase.</p>

<p>""number"" is the head word of the phrase, which I want to extract.</p>

<hr>

<p>EDIT: Added example.</p>
","python, nlp, stanford-nlp","<p>Looks like it may be easier using the typed dependencies instead of the syntactic parse. Your sentence will be ROOTed with a verb, then find the dependency nsubj or nsubjpas for that verb.
For example:</p>

<pre><code>root ( ROOT-0 , known-13 ) &lt;- Start with this one
det ( number-2 , The-1 )
nsubjpass ( known-13 , number-2 ) &lt;- Then this one
case ( entities-5 , of-3 )
amod ( entities-5 , elementary-4 )
nmod ( number-2 , entities-5 )
case ( mole-8 , in-6 )
nummod ( mole-8 , 1-7 )
nmod ( entities-5 , mole-8 )
case ( substance-11 , of-9 )
det ( substance-11 , a-10 )
nmod ( mole-8 , substance-11 )
auxpass ( known-13 , is-12 )
case ( what-15 , as-14 )
nmod ( known-13 , what-15 )
</code></pre>
",0,0,998,2017-02-10 08:55:53,https://stackoverflow.com/questions/42155178/finding-the-head-word-in-python
"MetaClass couldn&#39;t create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]","<p>I am using the method described on the stanford CoreNLP page <a href=""http://stanfordnlp.github.io/CoreNLP/cmdline.html#classpath"" rel=""nofollow noreferrer"">here.</a></p>

<p>In order to run Stanford CoreNLP from the command line the following command is used :</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</code></pre>

<p>I have run this command from the distribution directory. However. I am getting the following error : </p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:57)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:38)
    at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.&lt;init&gt;(NumberSequenceClassifier.java:86)
    at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:136)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:121)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$6.create(AnnotatorFactories.java:273)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:154)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:137)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1323)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
    ... 12 more
Caused by: java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:466)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
    ... 14 more
Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
    at de.jollyday.util.CalendarUtil.&lt;init&gt;(CalendarUtil.java:42)
    at de.jollyday.HolidayManager.&lt;init&gt;(HolidayManager.java:66)
    at de.jollyday.impl.DefaultHolidayManager.&lt;init&gt;(DefaultHolidayManager.java:46)
    at edu.stanford.nlp.time.JollyDayHolidays$MyXMLManager.&lt;init&gt;(JollyDayHolidays.java:148)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:466)
    at java.base/java.lang.Class.newInstance(Class.java:556)
    at de.jollyday.caching.HolidayManagerValueHandler.instantiateManagerImpl(HolidayManagerValueHandler.java:60)
    at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:41)
    at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:13)
    at de.jollyday.util.Cache.get(Cache.java:51)
    at de.jollyday.HolidayManager.createManager(HolidayManager.java:168)
    at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)
    at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)
    at edu.stanford.nlp.time.Options.&lt;init&gt;(Options.java:90)
    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.&lt;init&gt;(TimeExpressionExtractorImpl.java:39)
    ... 19 more
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:532)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:186)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:473)
    ... 38 more
</code></pre>

<p>My java version is 1.9 and the CoreNLP which I have downloaded is the latest one,i.e.  3.7.0.</p>

<p>Also, I do not have multiple versions of Stanford CoreNLP installed. Is this problem in the version? Where could I be going wrong? </p>
","java, nlp, stanford-nlp","<p>I downloaded the distribution folder from here: <a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>I unzipped it and cd'd into stanford-corenlp-full-2016-10-31</p>

<p>I entered this command:</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt -outputFormat text
</code></pre>

<p>And it worked properly.  Did you add extra jars to the folder?  What type of system are you running this on?</p>

<p>I am running with Java 1.8 and as far as I know Java 1.9 has not been released yet, so that could be an issue as well.</p>
",2,4,1692,2017-02-10 12:39:40,https://stackoverflow.com/questions/42159641/metaclass-couldnt-create-public-edu-stanford-nlp-time-timeexpressionextractorim
How can I expand stanford coreNLP spanish model/dictionary,"<p>I just run a ""hello world"" using Standford Core NLP to get named entities from text. But some places are not recognized properly such as ""Ixhuatlancillo"" or ""Veracruz"", both cities which has to be labeled as LUG (place) are labeled as ORG.
I will like to expand the spanish model or dictionary to add places(cities) from México, and to add person names. How can I do this?</p>

<p>Thanks in advance.</p>
",stanford-nlp,"<p>The fastest and easiest way would be to use the <code>regexner</code> annotator.  You can use this to manually build a dictionary.</p>

<p>Here is an example rule format (separated by tab, the first column can be any number of words)</p>

<p>system administrator    TITLE   MISC    2</p>

<p>token sequence          tag     tags-that-can-be-overwritten  priority</p>

<p>That above rule would mark ""system administrator"" in text as TITLE.</p>

<p>For your case:</p>

<p>Veracruz                LUG     MISC,ORG,PERS      2</p>

<p>This will allow the dictionary to overwrite MISC,ORGS, and PERS.  Without adding extra tags in the third column it won't overwrite previously tagged ner tags.</p>

<p>You might use a command like this to run it:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,regexner -props StanfordCoreNLP-spanish.properties -regexner.mapping /path/to/new_spanish.rules - regexner.ignorecase -regexner.validpospattern ""^(NN|JJ|NNP).*"" -outputFormat text -file sample-text.txt
</code></pre>

<p>Note that <code>regexner.ignorecase</code> means to make caseless matches, and <code>-regexner.validpospattern</code> is saying you should only match sequences with the specified pos tag pattern.</p>

<p>All of this being said, I just ran on the sentence:</p>

<pre><code>Ella fue a Veracruz.
</code></pre>

<p>and it tagged it properly.  Could you let me know what sentence you ran on that caused an incorrect tag for Veracruz?</p>
",1,1,518,2017-02-10 15:18:54,https://stackoverflow.com/questions/42162825/how-can-i-expand-stanford-corenlp-spanish-model-dictionary
Find all references to a supplied noun in StanfordNLP,"<p>I'm trying to parse some text to find all references to a particular item. So, for example, if my item was <code>The Bridge on the River Kwai</code> and I passed it this text, I'd like it to find all the instances I've put in bold.</p>

<blockquote>
  <p><strong>The Bridge on the River Kwai</strong> is a 1957 British-American epic war film
  directed by David Lean and starring William Holden, Jack Hawkins, Alec
  Guinness, and Sessue Hayakawa. <strong>The film</strong> is a work of fiction, but
  borrows the construction of the Burma Railway in 1942–1943 for its
  historical setting. <strong>The movie</strong> was filmed in Ceylon (now Sri Lanka).
  The bridge in the film was near Kitulgala.</p>
</blockquote>

<p>So far my attempt has been to go through all the mentions attached to each CorefChain and loop through those hunting for my target string. If I find the target string, I add the whole CorefChain, as I think this means the other items in that CorefChain also refer to the same thing.</p>

<pre><code>List&lt;CorefChain&gt; gotRefs = new ArrayList&lt;CorefChain&gt;();
String pQuery = ""The Bridge on the River Kwai"";

for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
    List&lt;CorefChain.CorefMention&gt; corefMentions = cc.getMentionsInTextualOrder();
    boolean addedChain = false;
    for (CorefChain.CorefMention cm : corefMentions) {
        if ((!addedChain) &amp;&amp; 
                (pQuery.equals(cm.mentionSpan))) {
            gotRefs.add(cc);
            addedChain = true;
        }
    }
}
</code></pre>

<p>I then loop through this second list of CorefChains, re-retrieve the mentions for each chain and step through them. In that loop I show which sentences have a likely mention of my item in a sentence.</p>

<pre><code>for (CorefChain gr : gotRefs) {
    List&lt;CorefChain.CorefMention&gt; corefMentionsUsing = gr.getMentionsInTextualOrder();
    for (CorefChain.CorefMention cm : corefMentionsUsing) {
        //System.out.println(""Got reference to "" + cm.mentionSpan + "" in sentence #"" + cm.sentNum);
    }
}
</code></pre>

<p>It finds some of my references, but not that many, and it produces a lot of false positives. As might be entirely apparently from reading this, I don't really know the first thing about NLP - am I going about this entirely the wrong way? Is there a StanfordNLP parser that will already do some of what I'm after? Should I be training a model in some way?</p>
","nlp, stanford-nlp","<p>I think a problem with your example is that you are looking for references to a movie title, and there isn't support in Stanford CoreNLP for recognizing movie titles, book titles, etc...</p>

<p>If you look at this example:</p>

<p>""Joe bought a laptop.  He is happy with it.""</p>

<p>You will notice that it connects:</p>

<p>""Joe"" -> ""He""</p>

<p>and </p>

<p>""a laptop"" -> ""it""</p>

<p>Coreference is an active research area and even the best system can only really be expected to produce an F1 of around 60.0 on general text, meaning it will often make errors.</p>
",1,0,55,2017-02-13 23:40:58,https://stackoverflow.com/questions/42215482/find-all-references-to-a-supplied-noun-in-stanfordnlp
How to run StanfordCoreNlpDemo.java,"<p>I successfully compiled StanfordCoreNlpDemo by running:</p>

<pre><code>javac -cp ""*"" StanfordCoreNlpDemo.java
</code></pre>

<p>and it compiled successfully. I then tried to run it with:</p>

<pre><code>java -cp ""*"" StanfordCoreNlpDemo
</code></pre>

<p>I then received the following error:</p>

<blockquote>
  <p>Error: Could not find or load main class StanfordCoreNlpDemo</p>
</blockquote>

<p>I realize this is a CLASSPATH issue so I tried to add the path to the folder:
<code>/some/path/stanford-corenlp-full-2016-10-31/*</code></p>

<p>Nonetheless, I still get the same error. How do I run StanfordCoreNlpDemo.java?</p>
","java, classpath, stanford-nlp","<p>This is not a problem of StanfordCoreNlpDemo program because I ran that code in Netbeans before. The problem seems associated with classpath issue.</p>

<p>Since the <code>StanfordCoreNlpDemo.java</code> file belongs to a package</p>

<pre><code>package package edu.stanford.nlp.pipeline.demo;

public class StanfordCoreNlpDemo {
    public static final void main(String[] args) throws IOException {
        // code goes here
    } 
}
</code></pre>

<p>Then calling the following results in <code>Error: Could not find or load main class TheClassName</code>.</p>

<pre><code>java -cp . StanfordCoreNlpDemo
</code></pre>

<p>It must be called with its fully-qualified name:</p>

<pre><code>java -cp . edu.stanford.nlp.pipeline.demo.StanfordCoreNlpDemo
</code></pre>

<p>And this <code>edu.stanford.nlp.pipeline.demo</code> directory must exist in the classpath. In this example, <code>.</code>, meaning the current directory, is the entirety of classpath. Therefore this particular example must be called from the directory in which <code>edu.stanford.nlp.pipeline.demo</code> exists. </p>

<p>Reference</p>

<ul>
<li><a href=""https://stackoverflow.com/a/29331827/5352399"">https://stackoverflow.com/a/29331827/5352399</a></li>
<li><a href=""https://stackoverflow.com/a/18093929/5352399"">https://stackoverflow.com/a/18093929/5352399</a></li>
</ul>
",1,0,252,2017-02-15 23:36:25,https://stackoverflow.com/questions/42262153/how-to-run-stanfordcorenlpdemo-java
&quot;f was unexpected at this time&quot; - bash script,"<p>I am currently trying to use the Stanford CoreNLP pipeline to run sentiment analysis. I need it to iterate through a folder of individual text files (which contain movie reviews) in order to establish the sentiment of each review.  I have tried to create a batch script in order to iterate through the 3 different folders containing the reviews.  when running the script through the shell runner program i receive the following error:</p>

<p><strong>f was unexpected at this time</strong></p>

<p>the script is as follows:</p>

<pre><code>dir=C:\stanford-corenlp-full-2016-10-31
for f in ""$dir\train\neg"" &amp;&amp; 
    for f in ""$dir\train\pos"" &amp;&amp; 
        for f in ""$dir\train\unsup"" ; do
    echo $f &gt;&gt; filelist.txt
    java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath filelist.txt -devPath dev.txt -train -model model.ser.gz 
done
</code></pre>

<p>this is the first bash script i have ever written so I am assuming my syntax is possibly incorrect somewhere?</p>

<p>Also I am using Windows 10.</p>

<p>Any advice would be amazing, </p>

<p>many thanks</p>

<p>hey guys your advice has been extremely useful.  to try and make my life a bit easier I have tried to convert my script to a batch script so that it shouldnt have any issues with being run on windows.  my new script looks as follows:                                                                           </p>

<pre><code>        @echo off
dir=C:\stanford-corenlp-full-2016-10-31
for %%f in ""%dir\train\neg"" &amp; ""%dir\train\pos"" &amp; ""%dir\train\unsup"" do
    ECHO %%f &gt;&gt; filelist.txt
    java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath filelist.txt -devPath dev.txt -train -model model.ser.gz 
done
pause
</code></pre>

<p>which results in the following error:
""%dir\train\pos"" was unexpected at this time</p>

<p>anyone understand what i've done wrong? I'm assuming it's some sort of a syntax issue I just can't see it. </p>
","bash, stanford-nlp","<p>You don't need multiple for's you can put all the items after the first like this:</p>

<pre><code> for f in ""$dir\train\neg"" ""$dir\train\pos"" ""$dir\train\unsup"" ; do
</code></pre>

<p>Or even:</p>

<pre><code>for f in ""$dir\train\""{neg,pos,unsup}
do
    ....
done
</code></pre>

<p>Also, I would think you would need to replace all single slashes ""\"" with two  ""\\"". but can't say for sure as I don't have Windows.</p>

<p>So it might be</p>

<pre><code> dir=C:\\stanford-corenlp-full-2016-10-31

 for f in ""$dir\\train\\""{neg,pos,unsup}
 do
     ...
 done
</code></pre>

<p>Its unclear whether: </p>

<ul>
<li>you need to call Java 3 times ( once for each dir)</li>
</ul>

<p>Or:</p>

<ul>
<li>you need to put all 3 dirs in filelist.txt  and then call Java at the end</li>
</ul>

<p>Example to call Java 3 times with only one dir in filelist.txt each time:</p>

<pre><code> for f in ""$dir\\train\\""{neg,pos,unsup}
 do 
     #note this ""&gt;"" will over-write the file each time
     echo ""$f"" &gt; filelist.txt  
     java    .... filelist.txt
 done
</code></pre>

<p>Creating a list file with 3 directories then call Java once:</p>

<pre><code> # note this ""&gt;"" will overwrite the file with nothing
 # so it will then be empty
 # so that it doesn't have what's left over from last time
 # that you ran the script
 &gt; filelist.txt  

 for f in ""$dir\\train\\""{neg,pos,unsup}
 do
      #note this ""&gt;&gt;"" will add a line to the file
      echo ""$f"" &gt;&gt; filelist.txt  
 done

 # call Java after you've finished creating the file
 java ..... filelist.txt
</code></pre>

<p>that you need</p>

<p>But, you don't even need the loop (assuming you have the 'ls' command available to you in that environment)</p>

<pre><code>dir=C:\\stanford-corenlp-full-2016-10-31

ls -1 -d ""$dir\\train\\""{neg,pos,unsup} &gt; filelist.txt

java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath filelist.txt -devPath dev.txt -train -model model.ser.gz 
</code></pre>

<p>NOTE 
The {neg,pos,unsup}
syntax will work in bash but not in a windows bat file</p>
",0,1,1608,2017-02-16 14:15:27,https://stackoverflow.com/questions/42276435/f-was-unexpected-at-this-time-bash-script
Stanford CoreNLP: Enhanced++ Dependencies recognition in Java,"<p>Can I directly looking for <strong>nmod:from</strong> and <strong>nmod:of</strong> Enhanced++ Dependencies  in <strong>Java</strong> code?</p>

<pre><code>semanticGraph.getChildrenWithReln(rootToken, UniversalEnglishGrammaticalRelations.NOMINAL_MODIFIER);
</code></pre>

<p>The second parameter of <code>getChildrenWithReln</code> method requires <code>GrammaticalRelation</code>. Unfortunatelly, I find only <code>UniversalEnglishGrammaticalRelations.NOMINAL_MODIFIER</code> and not something like <code>UniversalEnglishGrammaticalRelations.NOMINAL_MODIFIER_FROM</code>.</p>

<hr>

<p><em>My temporary solution:</em></p>

<pre><code>public List&lt;IndexedWord&gt; getChildrenByRelation(IndexedWord root, String shortName, String specific) {
        final List&lt;SemanticGraphEdge&gt; outputEdges = semanticGraphWrapper.get().getOutEdgesSorted(root);
        final List&lt;IndexedWord&gt; tokens = new ArrayList&lt;&gt;();

        GrammaticalRelation relation;
        for (SemanticGraphEdge edge : outputEdges) {
                relation = edge.getRelation();
                if (relation.getShortName().equals(shortName) &amp;&amp; relation.getSpecific().equals(specific))
                    tokens.add(edge.getTarget());
        }

        return tokens;
}
</code></pre>
","java, stanford-nlp","<p>You want to use this method in <code>edu.stanford.nlp.trees.UniversalEnglishGrammaticalRelations:</code></p>

<pre><code>public static GrammaticalRelation getNmod(String prepositionString)
</code></pre>

<p>And supply the preposition you want.</p>
",1,-1,349,2017-02-16 22:57:09,https://stackoverflow.com/questions/42286303/stanford-corenlp-enhanced-dependencies-recognition-in-java
"What is the difference between Viterbi CYK and Probabilistic CYK algorithm, Is there any differences?","<p>I think they are the same concept, 
<a href=""https://courses.engr.illinois.edu/cs498jh/Slides/Lecture10.pdf"" rel=""nofollow noreferrer"">https://courses.engr.illinois.edu/cs498jh/Slides/Lecture10.pdf</a>
Probabilistic CYK algorithm is used the viterbi algorithm to parse, is my concept is correct?</p>
","parsing, nlp, stanford-nlp, viterbi, cyk","<p>Yes, you are correct. You can consider weight CKY is equivalent to Viterbi for parsing. You can see the lecture on Viterbi and Statistical parsing with PCFG from <a href=""http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/syllabus.html"" rel=""nofollow noreferrer"">here</a>. However, Viterbi algorithm can be used to find the most likely sequence of hidden states and probabilistic CYK algorithm is specifically designed for tagging/parsing.</p>
",1,1,1100,2017-02-22 03:58:25,https://stackoverflow.com/questions/42382446/what-is-the-difference-between-viterbi-cyk-and-probabilistic-cyk-algorithm-is-t
Stanford NLP giving exception while running code,"<p>I am trying to run this code using file input and out put to another file:  </p>

<pre><code>import java.util.*;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.neural.rnn.*;
import edu.stanford.nlp.sentiment.*;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

import java.io.BufferedReader;
//import java.io.BufferedWriter;
import java.io.FileReader;
//import java.io.FileWriter;
import java.io.IOException;
import java.io.PrintWriter;
public class TestCoreNLP {

  public static void main(String[] args) throws IOException {
    PrintWriter out =  new PrintWriter(""/home/aims/Desktop/outputNLP1"");

    Properties props=new Properties();
    props.setProperty(""annotators"",""tokenize, ssplit, pos,lemma"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation annotation;  
    String readString = """";
        //PrintWriter pw = null;
    BufferedReader br  = new BufferedReader ( new FileReader ( ""/home/aims/Desktop/testNLP"" )  ) ;
        //pw = new PrintWriter ( new BufferedWriter ( new FileWriter ( ""/home/aims/Desktop/outputNLP"", true )  )  ) ;      
    //String x = """";
        while  (( readString = br.readLine ())  != null)   {
            // pw.println ( readString ) ; 
             //String xx=readString;x=xx;//System.out.println(""OKKKKK""); 
        annotation = new Annotation(readString);
 //System.out.print(readString);
    pipeline.annotate(annotation);    //System.out.println(""LamoohAKA"");
    pipeline.prettyPrint(annotation, out);
    out.println();
    out.println(""The top level annotation"");
    out.println(annotation.toShorterString());
    List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);

    if (sentences != null &amp;&amp; !sentences.isEmpty()) {
        for (int i = 0; i &lt; sentences.size (); i++) {
            CoreMap sentence = sentences.get(i);
            Tree tree = sentence.get(SentimentAnnotatedTree.class);//Tree tree = sentence.get(SentimentAnnotatedTree.class);
            int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
            String sentimentName = sentence.get(SentimentCoreAnnotations.SentimentClass.class);

            out.println();
            out.println(""The sentence is:"");
            out.println(sentence.toShorterString());
            out.println();
            out.println(""Sentiment of \n&gt; \""""+sentence.get(CoreAnnotations.TextAnnotation.class)+""\""\nis: "" + sentiment+"" (i.e., ""+sentimentName+"")"");
            out.println();
          }
    }

    IOUtils.closeIgnoringExceptions(out);
        }
        br.close (  ) ;
   // pw.close (  ) ;
    System.out.println(""Done..."");


  }

}
</code></pre>

<p>The input tp this code is:  </p>

<pre><code>I am glad you are here.
I will see you tomorrow.
I hate you.
Remember me!
I like ice-cream to utmost level of likeness.
</code></pre>

<p>When I ran the code using <code>Eclipse Neon</code>, I got the following error:  </p>

<pre><code>[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.neural.rnn.RNNCoreAnnotations.getPredictedClass(RNNCoreAnnotations.java:83)
    at TestCoreNLP.main(TestCoreNLP.java:48)
</code></pre>

<p>Now I am not understanding why it is happening? What should I do to run this code successfully?</p>
","java, stanford-nlp","<p>You aren't running the sentiment annotator or parser in your pipeline.  Here is a commandline call that shows running a pipeline and getting sentiment.  You can easily adapt it to Java code by setting the properties for your pipeline to match the ones specified by this call.</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse,sentiment -parse.binaryTrees -file example-sentence.txt -outputFormat text
</code></pre>

<p>You need to add the <code>parse</code> and <code>sentiment</code> annotators to your pipeline and you need to make sure that the <code>parse</code> annotator produces binary trees with the <code>parse.binaryTrees</code> property being set to true.</p>

<p>Here is some sample code that shows accessing sentiment:</p>

<pre><code>import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.sentiment.*;
import edu.stanford.nlp.util.*;
import java.util.Properties;

public class SentimentExample {

  public static void main(String[] args) {
    Annotation document = new Annotation(""I liked the first movie.  I hated the second movie."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,sentiment"");
    props.setProperty(""parse.binaryTrees"",""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(sentence.get(CoreAnnotations.TextAnnotation.class));
      System.out.println(sentence.get(SentimentCoreAnnotations.SentimentClass.class));
    }
  }
}
</code></pre>
",2,1,199,2017-02-22 07:47:21,https://stackoverflow.com/questions/42385519/stanford-nlp-giving-exception-while-running-code
Why this exception is getting occurred while running the java code with Spark and Stanford NLP?,"<p>Here is my code: </p>

<pre><code>try
{
    Dataset&lt;Row&gt; df =  spark.sql(""select answer from health where limit 1"");

    nameAndCity = df.toJavaRDD().map(new Function&lt;Row, String&gt;() {

     //   @Override
        public String call(Row row) {
            return row.getString(0);
        }
    }).collect();
}

catch (Exception AnalysisException)
{
    System.out.print(""\nTable is not found\n"");
}
spark.close();
System.out.println(""Spark Done....!"");
for (String name : nameAndCity) 
{
    Annotation document = new Annotation(name);
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,sentiment"");
    //props.setProperty(""parse.binaryTrees"",""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""Sentence Analyzed: "" + "" "" + sentence.get(CoreAnnotations.TextAnnotation.class));
      System.out.println(""Reflected sentiment: "" + "" "" + sentence.get(SentimentCoreAnnotations.SentimentClass.class));
    }
 }
</code></pre>

<p>After running this code, I get the following exception:  </p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:57)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:38)
    at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.&lt;init&gt;(NumberSequenceClassifier.java:86)
    at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:136)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:121)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$6.create(AnnotatorFactories.java:273)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:154)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:137)
    at spark.sparkhive.queryhive.main(queryhive.java:64)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
    ... 12 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
    ... 14 more
Caused by: java.lang.NoClassDefFoundError: de/jollyday/ManagerParameter
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at edu.stanford.nlp.time.Options.&lt;init&gt;(Options.java:87)
    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.&lt;init&gt;(TimeExpressionExtractorImpl.java:39)
    ... 19 more
Caused by: java.lang.ClassNotFoundException: de.jollyday.ManagerParameter
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 24 more
17/02/22 18:26:39 INFO ShutdownHookManager: Shutdown hook called
17/02/22 18:26:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-23b5fac2-27ec-44a6-9ff7-cb2b8bfd8753
</code></pre>

<p>Why I am getting this exception and what I should do to resolve it?</p>
","java, apache-spark, apache-spark-sql, stanford-nlp","<p>This one is the source of your trouble:</p>

<pre><code>java.lang.NoClassDefFoundError: de/jollyday/ManagerParameter
</code></pre>

<p>Your program is looking for this class but can't find it. Check your classpath.</p>

<p>You can ignore the preceding exceptions in the output because they're just reactions to the original exception.</p>

<ul>
<li>ReflectionLoadingException is thrown by a method called loadByReflection, so it seems to be a wrapper exception of some kind and unlikely to tell you much.</li>
<li>Then we have ClassCreationException thrown by createInstance, kind of the same situation. In general if you have a method called doSomething that throws DoSomethingException, the exception is either going to tell you exactly what the problem is (""x must be >= 100"") or wrap the actual exception.</li>
<li>InvocationTargetException is just the exception that the JVM throws when you try to invoke a method using reflection and that method throws an exception. Again it's just a wrapper for the actual exception.</li>
<li>Finally we get to the actual exception, NoClassDefFoundError.</li>
</ul>
",2,0,374,2017-02-22 13:05:21,https://stackoverflow.com/questions/42392322/why-this-exception-is-getting-occurred-while-running-the-java-code-with-spark-an
"Java, Stanford NLP : Unable to validate jar entry per:countries_of_residence.rules on Windows only","<p>I am working on integrating Stanford NLP 3.7.0 on our system and we have no problem on Linux system, but windows development machine is facing a problem. I have added Stanford-parser and its Javadoc in classpath, and models is part of maven. THis is the error we get when we start the project :</p>

<p>Error log :</p>

<pre><code>java.lang.IllegalArgumentException: Unable to validate JAR entry with name edu/stanford/nlp/models/kbp/tokensregex/per:countries_of_residence.rules

                at org.apache.catalina.loader.WebappClassLoaderBase.findResourceInternal(WebappClassLoaderBase.java:3359)

                at org.apache.catalina.loader.WebappClassLoaderBase.findResource(WebappClassLoaderBase.java:1424)

                at org.apache.catalina.loader.WebappClassLoaderBase.getResourceAsStream(WebappClassLoaderBase.java:1652)

                at edu.stanford.nlp.io.IOUtils.findStreamInClasspathOrFileSystem(IOUtils.java:407)

                at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:461)

                at edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:390)

                at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:601)

                at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:405)

                at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:187)

                at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:166)

                at com.tooltank.spring.service.GroupCanvasServiceImpl.&lt;init&gt;(GroupCanvasServiceImpl.java:56)

                at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)

                at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)

                at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)

                at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
Caused by: java.io.IOException: Die Syntax für den Dateinamen, Verzeichnisnamen oder die Datenträgerbezeichnung ist falsch

                at java.io.WinNTFileSystem.canonicalize0(Native Method)

                at java.io.WinNTFileSystem.canonicalize(WinNTFileSystem.java:428)

                at java.io.File.getCanonicalPath(File.java:618)

                at org.apache.catalina.loader.WebappClassLoaderBase.findResourceInternal(WebappClassLoaderBase.java:3352)
</code></pre>

<p>POM.xml :</p>

<pre><code>&lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-parser&lt;/artifactId&gt;
            &lt;version&gt;3.7.0&lt;/version&gt;
        &lt;/dependency&gt;


     &lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.7.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.7.0&lt;/version&gt;
            &lt;classifier&gt;models&lt;/classifier&gt;
        &lt;/dependency&gt;
</code></pre>

<p>Any ideas? THank you. </p>

<p><em>Solution</em></p>

<pre><code>&lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.7.0&lt;/version&gt;
            &lt;classifier&gt;models&lt;/classifier&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
                    &lt;artifactId&gt;stanford-corenlp-3.7.0-models-kbp&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
</code></pre>
","java, windows, parsing, stanford-nlp","<p>There is a known issue where the file names for some KBP relation extractor resources don't work on Windows.  They were made with "":"" in them which is bad on Windows.  We are going to at some point change them to something more Windows friendly.  If you don't use the KBP annotator on Windows I think this issue would go away, you might also need to exclude the stanford-corenlp-3.7.0-models-kbp.jar.</p>
",1,1,104,2017-02-23 09:28:04,https://stackoverflow.com/questions/42411659/java-stanford-nlp-unable-to-validate-jar-entry-percountries-of-residence-rul
Stanford NER - Unable to identify Phone number,"<p>I am training my NER to the entity type Phonenumber whose part of speech is number. However when I test the same data that I have trained, the phone number is not identified by the classifier. <a href=""https://i.sstatic.net/M6v63.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M6v63.png"" alt=""enter image description here""></a></p>

<p>Is that because the part of speech(POS) of phone number is number(CD)?</p>
","java, stanford-nlp","<p>You might want to use <code>regexner</code> instead for this use case.</p>

<p>Consider this sentence (put it in phone-number-example.txt):</p>

<p><code>You can reach the office at 555 555-5555.</code></p>

<p>If you make a <code>regexner</code> rules file like this (note each column is tab separated)</p>

<pre><code>[0-9]{3}\W[0-9]{3}-[0-9]{4}     PHONE_NUMBER    MISC,NUMBER     1
</code></pre>

<p>And run this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,regexner -regexner.mapping phone_number.rules -file phone-number-example.txt -outputFormat text
</code></pre>

<p>It will identify the phone number in the output NER tagging.</p>

<p>One issue to look out for.  You will note the tokenizer turns ""555 555-5555"" into one token.  The first column of the rule file is a regex that matches a token.  The regexner patterns are a space separated list of patterns that match each token you want to ner tag.</p>

<p>So in this example, the rule I made has a ""\W"" to capture the space.  The rule wasn't working when I used ""\s"", etc..so I think there is an issue with writing regexes for tokens that contain spaces.  Typically tokens don't contain spaces for that matter.</p>

<p>So you might want to work around this by expanding on ""\W"" and excluding other characters that you don't want since ""\W"" just means non-word characters.  Also, you can obviously make the pattern I just listed more complicated and capture the various phone number patterns.</p>

<p>More info on RegexNER can be found here:</p>

<p><a href=""http://nlp.stanford.edu/software/regexner.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/regexner.html</a></p>
",1,1,1031,2017-02-23 13:03:23,https://stackoverflow.com/questions/42416550/stanford-ner-unable-to-identify-phone-number
Which settings should be used for TokensregexNER,"<p>When I try regexner it works as expected with the following settings and data;</p>

<pre><code>props.setProperty(""annotators"", ""tokenize, cleanxml, ssplit, pos, lemma, regexner"");
</code></pre>

<blockquote>
  <p>Bachelor of Laws  DEGREE <br>
  Bachelor of (Arts|Laws|Science|Engineering|Divinity)    DEGREE</p>
</blockquote>

<p>What I would like to do is that using TokenRegex. For example </p>

<blockquote>
  <p>Bachelor of Laws  DEGREE<br>
  Bachelor of ([{tag:NNS}] [{tag:NNP}])   DEGREE</p>
</blockquote>

<p>I read that to do this, I should use TokensregexNERAnnotator.</p>

<p>I tried to use it as follows, but it did not work.</p>

<pre><code>Pipeline.addAnnotator(new TokensRegexNERAnnotator(""expressions.txt"", true));
</code></pre>

<p>Or I tried setting annotator in another way,</p>

<pre><code>props.setProperty(""annotators"", ""tokenize, cleanxml, ssplit, pos, lemma, tokenregexner"");    
props.setProperty(""customAnnotatorClass.tokenregexner"", ""edu.stanford.nlp.pipeline.TokensRegexNERAnnotator"");
</code></pre>

<p>I tried to different TokenRegex formats but either annotator could not find the expression or I got SyntaxException.</p>

<p>What is the proper way to use TokenRegex (query on tokens with tags) on NER data file ?</p>

<p>BTW I just see a comment in TokensRegexNERAnnotator.java file. Not sure if it is related pos tags does not work with RegexNerAnnotator.</p>

<pre><code>if (entry.tokensRegex != null) {
    // TODO: posTagPatterns...
    pattern = TokenSequencePattern.compile(env, entry.tokensRegex);
  }
</code></pre>
","named-entity-recognition, stanford-nlp","<p>First you need to make a TokensRegex rule file (sample_degree.rules).  Here is an example:</p>

<pre><code>ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }

{ pattern: (/Bachelor/ /of/ [{tag:NNP}]), action: Annotate($0, ner, ""DEGREE"") }
</code></pre>

<p>To explain the rule a bit, the <code>pattern</code> field is specifying what type of pattern to match.  The <code>action</code> field is saying to annotate every token in the overall match (that is what <code>$0</code> represents), annotate the <code>ner</code> field (note that we specified ner = ... in the rule file as well, and the third parameter is saying set the field to the String ""DEGREE"").</p>

<p>Then make this .props file (degree_example.props) for the command:</p>

<pre><code>customAnnotatorClass.tokensregex = edu.stanford.nlp.pipeline.TokensRegexAnnotator

tokensregex.rules = sample_degree.rules

annotators = tokenize,ssplit,pos,lemma,ner,tokensregex
</code></pre>

<p>Then run this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -props degree_example.props -file sample-degree-sentence.txt -outputFormat text
</code></pre>

<p>You should see that the three tokens you wanted tagged as ""DEGREE"" will be tagged.</p>

<p>I think I will push a change to the code to make <code>tokensregex</code> link to the TokensRegexAnnotator so you won't have to specify it as a custom annotator.
But for now you need to add that line in the .props file.</p>

<p>This example should help in implementing this.  Here are some more resources if you want to learn more:</p>

<p><a href=""http://nlp.stanford.edu/software/tokensregex.shtml#TokensRegexRules"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/tokensregex.shtml#TokensRegexRules</a></p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/SequenceMatchRules.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/SequenceMatchRules.html</a></p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/types/Expressions.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/types/Expressions.html</a></p>
",2,3,667,2017-02-26 16:09:13,https://stackoverflow.com/questions/42470843/which-settings-should-be-used-for-tokensregexner
Custom relation extraction model using Stanford Core NLP doesn&#39;t find any relations,"<p>I trained a custom model for relation extraction using Stanford Core NLP's <a href=""http://nlp.stanford.edu/software/relationExtractor.html"" rel=""nofollow noreferrer"">example</a>. But when I run the model, it doesn't find any relations -- even when I use sentences directly from my training set. I used a verrrry small training set (20 examples) just to make sure I could get the model to train. Even though my training set is ridiculously small, I would still expect the model to work, just very poorly. Why isn't the model able to find any relations?</p>

<p>Also, I wanted to name my relation ""affordance"", but when I try to do so in my dataset, I get a NullPointerException when I try to train. If I change the name of my relation in the dataset to ""kill"" it suddenly works. I'm assuming that since ""kill"" is one of the examples relations Stanford gives, it's been added to some file. Does anyone know how I could rename my relation?</p>

<p>Thank you so much!</p>

<hr>

<p>Example Training Set:</p>

<pre><code>3   Peop    0   O   NNP Alice   O   O   O
3   O   1   O   VBD was O   O   O
3   O   2   O   VBG beginning   O   O   O
3   O   3   O   TO  to  O   O   O
3   O   4   O   VB  get O   O   O
3   O   5   O   RB  very    O   O   O
3   O   6   O   JJ  tired   O   O   O
3   O   7   O   IN  of  O   O   O
3   O   8   O   VBG sitting O   O   O
3   O   9   O   IN  by  O   O   O
3   O   10  O   PRP$    her O   O   O
3   O   11  O   NN  sister  O   O   O
3   O   12  O   IN  on  O   O   O
3   O   13  O   DT  the O   O   O
3   O   14  O   NN  bank    O   O   O
3   O   15  O   .   .   O   O   O

8   14  kill

4   O   0   O   RB  Once    O   O   O
4   O   1   O   CC  or  O   O   O
4   O   2   O   RB  twice   O   O   O
4   O   3   O   PRP she O   O   O
4   O   4   O   VBD had O   O   O
4   O   5   O   VBN peeped  O   O   O
4   O   6   O   IN  into    O   O   O
4   O   7   O   DT  the O   O   O
4   O   8   O   NN  book    O   O   O
4   O   9   O   PRP$    her O   O   O
4   O   10  O   NN  sister  O   O   O
4   O   11  O   VBD was O   O   O
4   O   12  O   VBG reading O   O   O
4   O   13  O   ,   ,   O   O   O
4   O   14  O   CC  but O   O   O
4   O   15  O   PRP it  O   O   O
4   O   16  O   VBD had O   O   O
4   O   17  O   DT  no  O   O   O
4   O   18  O   NNS pictures    O   O   O
4   O   19  O   CC  or  O   O   O
4   O   20  O   NN  conversation    O   O   O
4   O   21  O   .   .   O   O   O

12  8   kill

5   O   0   O   IN  So  O   O   O
5   O   1   O   PRP she O   O   O
5   O   2   O   VBD was O   O   O
5   O   3   O   VBG considering O   O   O
5   O   4   O   IN  in  O   O   O
5   O   5   O   PRP$    her O   O   O
5   O   6   O   JJ  own O   O   O
5   O   7   O   NN  mind    O   O   O
5   O   8   O   ,   ,   O   O   O
5   O   9   O   IN  whether O   O   O
5   O   10  O   DT  the O   O   O
5   O   11  O   NN  pleasure    O   O   O
5   O   12  O   IN  of  O   O   O
5   O   13  O   VBG making  O   O   O
5   O   14  O   DT  a   O   O   O
5   O   15  O   NN  daisy-chain O   O   O
5   O   16  O   MD  would   O   O   O
5   O   17  O   VB  be  O   O   O
5   O   18  O   JJ  worth   O   O   O
5   O   19  O   DT  the O   O   O
5   O   20  O   NN  trouble O   O   O
5   O   21  O   IN  of  O   O   O
5   O   22  O   VBG getting O   O   O
5   O   23  O   RB  up  O   O   O
5   O   24  O   CC  and O   O   O
5   O   25  O   VBG picking O   O   O
5   O   26  O   DT  the O   O   O
5   O   27  O   NNS daisies O   O   O
5   O   28  O   .   .   O   O   O

25  27  kill

6   Peop    0   O   NNP Alice   O   O   O
6   O   1   O   VBD opened  O   O   O
6   O   2   O   DT  the O   O   O
6   O   3   O   NN  door    O   O   O
6   O   4   O   CC  and O   O   O
6   O   5   O   VBD found   O   O   O
6   O   6   O   IN  that    O   O   O
6   O   7   O   PRP it  O   O   O
6   O   8   O   VBD led O   O   O
6   O   9   O   IN  into    O   O   O
6   O   10  O   DT  a   O   O   O
6   O   11  O   JJ  small   O   O   O
6   O   12  O   NN  passage O   O   O
6   O   13  O   ,   ,   O   O   O
6   O   14  O   RB  not O   O   O
6   O   15  O   RB  much    O   O   O
6   O   16  O   JJR larger  O   O   O
6   O   17  O   IN  than    O   O   O
6   O   18  O   DT  a   O   O   O
6   O   19  O   NN  rat-hole    O   O   O
6   O   20  O   .   .   O   O   O

1   3   kill
</code></pre>
","nlp, training-data, stanford-nlp","<p>I figured out the problem with my data set in case anyone else on the internet runs into the same problem.. the relation entity lines e.g. <code>8 14 kill</code> have to be referencing words that have <code>ner</code> tags in column 2! Not being able to name my relations was just a symptom of this as well.</p>
",2,2,454,2017-02-27 23:04:56,https://stackoverflow.com/questions/42497413/custom-relation-extraction-model-using-stanford-core-nlp-doesnt-find-any-relati
Maven failing to assemble WAR - Java Heap Space Issue - Stanford Core NLP,"<p>So i'm getting this error when trying to Clean Install with Maven. I've attempted increasing MAVEN_OPTS with Xmx-512m or w/e it was called, similarly fiddled with Eclipse ini to increase heap space, but all of this to no avail, I keep getting this error:</p>

<pre><code>[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building DiaryAppREST 0.0.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ DiaryAppREST ---
[INFO] Deleting C:\Users\Administrator\workspace\DiaryAppREST\target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ DiaryAppREST ---
[WARNING] Using platform encoding (Cp1252 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory C:\Users\Administrator\workspace\DiaryAppREST\src\main\resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5.1:compile (default-compile) @ DiaryAppREST ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding Cp1252, i.e. build is platform dependent!
[INFO] Compiling 1 source file to C:\Users\Administrator\workspace\DiaryAppREST\target\classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ DiaryAppREST ---
[WARNING] Using platform encoding (Cp1252 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory C:\Users\Administrator\workspace\DiaryAppREST\src\test\resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5.1:testCompile (default-testCompile) @ DiaryAppREST ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ DiaryAppREST ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-war-plugin:3.0.0:war (default-war) @ DiaryAppREST ---
[INFO] Packaging webapp
[INFO] Assembling webapp [DiaryAppREST] in [C:\Users\Administrator\workspace\DiaryAppREST\target\DiaryAppREST-0.0.1-SNAPSHOT]
[INFO] Processing war project
[INFO] Copying webapp resources [C:\Users\Administrator\workspace\DiaryAppREST\WebContent]
[INFO] Webapp assembled in [8913 msecs]
[INFO] Building war: C:\Users\Administrator\workspace\DiaryAppREST\target\DiaryAppREST-0.0.1-SNAPSHOT.war
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 27.019 s
[INFO] Finished at: 2017-02-28T09:52:30+00:00
[INFO] Final Memory: 86M/247M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-war-plugin:3.0.0:war (default-war) on project DiaryAppREST: Error assembling WAR: Problem creating war: Execution exception (and the archive is probably corrupt but I could not delete it): Java heap space -&gt; [Help 1]
[ERROR] 
</code></pre>

<p>I can't understand what is causing this, tried to find information online but can't seem to find this specific problem that anyone else is having. If anyone would be able to help i'd be very much appreciated.</p>
","java, eclipse, maven, war, stanford-nlp","<p>Just figured it out,</p>

<p>Firstly, I had set MAVEN_OPTS incorrectly, I had them set as:</p>

<pre><code>set MAVEN_OPTS=""Xmx 512m""
</code></pre>

<p>After changing this to:</p>

<pre><code>set MAVEN_OPTS=""-Xmx512m""
</code></pre>

<p>You can also add this line into your profile (.profile, .bash_profile, etc) for this to be set in every terminal you open.</p>

<pre><code>export MAVEN_OPTS=""-Xmx512m""
</code></pre>

<p>And then went into the directory the project was in using CMD,
I ran:</p>

<pre><code>mvn clean install -U
</code></pre>

<p>This seemed to do the trick and everything went successfully.</p>

<p>Hope this helps anyone else having problems!</p>
",5,4,16239,2017-02-28 10:03:33,https://stackoverflow.com/questions/42505638/maven-failing-to-assemble-war-java-heap-space-issue-stanford-core-nlp
spaCy-like dependency graph navigation in CoreNLP,"<p>Is it possible to navigate the dependency parse tree in <code>CoreNLP</code> the way one does that in <code>spaCy</code> as described <a href=""https://spacy.io/docs/usage/dependency-parse#navigating"" rel=""nofollow noreferrer"">here</a>? So far I see that token attributes like lemmas, POS tags, etc. are retrievable through an index, e.g. <code>sent.lemmas(5)</code> returns the lemma of the sixth token. I am not sure this exists for dependency heads and relations. Is there an established way of using these apart from navigating the whole tree every time?</p>
","nlp, stanford-nlp, stanford-parser","<p>If you're using the regular API, I believe what you're looking for is the function:</p>

<pre><code>Set&lt;IndexedWord&gt; SemanticGraph#vertexSet()
</code></pre>

<p>This iterates over all of the nodes in a dependency tree [/graph]. each <code>IndexedWord</code> is also a <code>CoreLabel</code>, which means it has all of the functions you know and love for tokens.</p>

<p>From the simple API -- which I gather is what you're using -- you can get a regular old dependency graph with:</p>

<pre><code>SemanticGraph Sentence#dependencyGraph()
</code></pre>
",1,0,495,2017-02-28 10:08:47,https://stackoverflow.com/questions/42505767/spacy-like-dependency-graph-navigation-in-corenlp
Training Stanford POS tagger using multiple text files,"<p>I have a corpus of about 20000 text files and i want to train the tagger using these text files, which is better,to group these text files into one text file(i don't know if it will affect tagging accuracy or not) or to include all these text files in the props file?</p>
","nlp, stanford-nlp, part-of-speech","<p>I don't think it matters.  The code should just load all of the data in, it's just for convenience if you have it split into multiple files.  Also, you can specify different input formats for different files, but this is not going to affect the final model.</p>
",1,0,122,2017-02-28 15:50:02,https://stackoverflow.com/questions/42513179/training-stanford-pos-tagger-using-multiple-text-files
Using CoreNLP ColumnDataClassifier for document classification with a large corpus,"<p>I'm trying to use the CoreNLP <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html"" rel=""nofollow noreferrer"">ColumnDataClassifier</a> to classify a large number of documents. I have a little more than 1 million documents with about 20000 labels.</p>

<p>Is this even possible in terms of memory requirements? (I currently only have 16GB)</p>

<p>Is it somehow possible to train the classifier in an iterative way, splitting the input into many smaller files?</p>
","document-classification, stanford-nlp","<p>As an experiment I ran:</p>

<pre><code>1.) 500,000 documents, each with 100 random words
2.) a label set of 10,000
</code></pre>

<p>This crashed with a memory error even when I gave it 40 GB of RAM.</p>

<p>I also ran:</p>

<pre><code>1.) same 500,000 documents
2.) a label set of 6
</code></pre>

<p>This ran successfully to completion with 16 GB of RAM.</p>

<p>I'm not sure at what point growing the label set will cause a crash, but my advice would be to shrink the possible label set and experiment.</p>
",0,0,213,2017-03-01 15:35:12,https://stackoverflow.com/questions/42535860/using-corenlp-columndataclassifier-for-document-classification-with-a-large-corp
Why DL4J NLP example not getting the jars using maven?,"<p>I am trying to run the program of DL4J examples from the following link:  </p>

<p><a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/Word2VecSentimentRNN.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/Word2VecSentimentRNN.java</a><br>
But while running the code I get the following debug information:</p>

<pre><code>10:48:38.934 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libgomp.so.1 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.935 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libgomp.so.1 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.935 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libgfortran.so.3 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.936 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libgfortran.so.3 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.936 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libopenblas.so.0 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.936 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libopenblas.so.0 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.937 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libnd4j.so in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.937 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libnd4j.so in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.937 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libjnind4j.so in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.937 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-x86_64/libjnind4j.so in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.938 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libjnind4j.so in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner TypeAnnotationsScanner
10:48:38.938 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libjnind4j.so in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner SubTypesScanner
10:48:38.947 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libgomp.so.1 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner TypeAnnotationsScanner
10:48:38.947 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libgomp.so.1 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner SubTypesScanner
10:48:38.947 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libgfortran.so.3 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner TypeAnnotationsScanner
10:48:38.948 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libgfortran.so.3 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner SubTypesScanner
10:48:38.950 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libblas.so.3 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner TypeAnnotationsScanner
10:48:38.950 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libblas.so.3 in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner SubTypesScanner
10:48:38.953 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libnd4j.so in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner TypeAnnotationsScanner
10:48:38.953 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/linux-ppc64le/libnd4j.so in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-linux-ppc64le.jar!/ with scanner SubTypesScanner
10:48:38.954 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/jnind4j.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.954 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/jnind4j.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.965 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libgcc_s_seh-1.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.965 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libgcc_s_seh-1.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.966 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libgfortran-3.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.966 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libgfortran-3.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.970 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libgomp-1.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.970 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libgomp-1.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.970 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libnd4j.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.971 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libnd4j.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.974 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libopenblas.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.974 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libopenblas.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.989 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libquadmath-0.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.989 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libquadmath-0.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.990 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libstdc++-6.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.991 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libstdc++-6.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:38.995 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libwinpthread-1.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:38.996 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/windows-x86_64/libwinpthread-1.dll in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-windows-x86_64.jar!/ with scanner SubTypesScanner
10:48:39.058 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/macosx-x86_64/libiomp5.dylib in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-macosx-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:39.058 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/macosx-x86_64/libiomp5.dylib in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-macosx-x86_64.jar!/ with scanner SubTypesScanner
10:48:39.068 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/macosx-x86_64/libjnind4j.dylib in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-macosx-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:39.069 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/macosx-x86_64/libjnind4j.dylib in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-macosx-x86_64.jar!/ with scanner SubTypesScanner
10:48:39.069 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/macosx-x86_64/libnd4j.dylib in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-macosx-x86_64.jar!/ with scanner TypeAnnotationsScanner
10:48:39.069 [main] DEBUG org.reflections.Reflections - could not scan file org/nd4j/nativeblas/macosx-x86_64/libnd4j.dylib in url jar:file:/home/aims1/.m2/repository/org/nd4j/nd4j-native/0.4.0/nd4j-native-0.4.0-macosx-x86_64.jar!/ with scanner SubTypesScanner
</code></pre>

<p>I have checked my Maven <code>.m2</code> folder, the jars are available there. But why the program fails to access the jars?<br>
How I can resolve this problem? I am using IntelliJ Ultimate.  </p>
","java, maven, intellij-idea, stanford-nlp, dl4j","<p>You can ignore this. It's ust a warning from the reflections library:
<a href=""https://github.com/ronmamo/reflections"" rel=""nofollow noreferrer"">https://github.com/ronmamo/reflections</a></p>

<p>All it's saying is it can't find classfiles for native code (which makes sense)</p>
",1,0,331,2017-03-02 05:27:41,https://stackoverflow.com/questions/42547504/why-dl4j-nlp-example-not-getting-the-jars-using-maven
Java - processing the content of each file within a directory,"<p>I am using Stanford CoreNLP in order to carry out sentiment analysis on 25,000 individual textual movie reviews all contained within one single directory.  In order to do this I need to slightly alter the Stanford code as it only analyses each individual sentence within a single text file. </p>

<p>My attempt at carrying this out is as follows:</p>

<pre><code>import java.io.File;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import org.apache.commons.io.FileUtils;

import com.google.common.io.Files;

import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.util.CoreMap;
import java.io.File;
import java.util.Iterator;
import org.apache.commons.io.*;

/** A simple corenlp example ripped directly from the Stanford CoreNLP website using text from wikinews. */
public class sentimentMain {

  public static void main(String[] args) throws IOException {
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text from the file..
    Iterator it = FileUtils.iterateFiles(new File(""C:\\stanford-corenlp-full-2016-10-31\\train\\neg""), null, false);
    Iterator it1 = FileUtils.iterateFiles(new File(""C:\\stanford-corenlp-full-2016-10-31\\train\\pos""), null, false);
    Iterator it2 = FileUtils.iterateFiles(new File(""C:\\stanford-corenlp-full-2016-10-31\\train\\unsup""), null, false);

    File inputFile  = new File ((String) (it.next()));
    String text = Files.toString(inputFile, Charset.forName(""UTF-8""));
    System.out.println(text);

    //File inputFile = new File(""C:/stanford-corenlp-full-2016-10-31/input.txt"");
    //String text = Files.toString(inputFile, Charset.forName(""UTF-8""));

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);

        System.out.println(""word: "" + word + "" pos: "" + pos + "" ne:"" + ne);
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);
      System.out.println(""parse tree:\n"" + tree);

      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
      System.out.println(""dependency graph:\n"" + dependencies);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map&lt;Integer, CorefChain&gt; graph = 
        document.get(CorefChainAnnotation.class);

  }

}
</code></pre>

<p>of which I receive the following error:</p>

<pre><code>Exception in thread ""main"" java.lang.ClassCastException: java.io.File cannot be cast to java.lang.String
    at sentimentMain.main(sentimentMain.java:46)
</code></pre>

<p>I understand that ""it.next()"" can not be converted to a string, but does anyone know another way I can ensure the content of the files are being input as a string for processing?</p>

<p>Thanks in advance :)</p>
","java, file-io, stanford-nlp","<p>Its a straight forward compilation error, which a decent IDE would have shown you.
variable - ""text"" is not available outside the while loop, It should either be declared before start of while loop or the document declaration should be inside while loop.</p>

<p>Please find below the edited code.</p>

<pre><code>import java.io.File;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import org.apache.commons.io.FileUtils;

import com.google.common.io.Files;

import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.util.CoreMap;
import java.io.File;
import java.util.Iterator;
import org.apache.commons.io.*;

/** A simple corenlp example ripped directly from the Stanford CoreNLP website using text from wikinews. */
public class sentimentMain {

  public static void main(String[] args) throws IOException {
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text from the file..
    Iterator it = FileUtils.iterateFiles(new File(""C:\\stanford-corenlp-full-2016-10-31\\train\\neg""), null, false);
    Iterator it1 = FileUtils.iterateFiles(new File(""C:\\stanford-corenlp-full-2016-10-31\\train\\pos""), null, false);
    Iterator it2 = FileUtils.iterateFiles(new File(""C:\\stanford-corenlp-full-2016-10-31\\train\\unsup""), null, false);

    while(it.hasNext()){

        File inputFile  = new File ((String) (it.next()));
        String text = Files.toString(inputFile, Charset.forName(""UTF-8""));
        System.out.println(text);
    //File inputFile = new File(""C:/stanford-corenlp-full-2016-10-31/input.txt"");
    //String text = Files.toString(inputFile, Charset.forName(""UTF-8""));

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);

        System.out.println(""word: "" + word + "" pos: "" + pos + "" ne:"" + ne);
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);
      System.out.println(""parse tree:\n"" + tree);

      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
      System.out.println(""dependency graph:\n"" + dependencies);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map&lt;Integer, CorefChain&gt; graph = 
        document.get(CorefChainAnnotation.class);

    }
  }

}
</code></pre>
",0,-1,143,2017-03-04 18:17:37,https://stackoverflow.com/questions/42599576/java-processing-the-content-of-each-file-within-a-directory
How to achieve the overall sentiment from a review using Stanford CoreNLP,"<p>I am using Stanford CoreNLP in order to obtain the sentiment analysis of 25,000 movie reviews.  However, I have achieved getting the sentiment of each sentence, of each review, but I was wondering if anyone knew how I could get the sentiment of the overall review instead of each sentence in the review?</p>

<p>The code im using is:</p>

<pre><code>import java.io.*;
import java.util.*;

import edu.stanford.nlp.coref.CorefCoreAnnotations;

import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

/** This class demonstrates building and using a Stanford CoreNLP pipeline. */
public class sentimentMain {

  /** Usage: java -cp ""*"" StanfordCoreNlpDemo [inputFile [outputTextFile [outputXmlFile]]] */
  public static void main(String[] args) throws IOException {
    // set up optional output files
    PrintWriter out;
    if (args.length &gt; 1) {
      out = new PrintWriter(args[1]);
    } else {
      out = new PrintWriter(System.out);
    }
    PrintWriter xmlOut = null;
    if (args.length &gt; 2) {
      xmlOut = new PrintWriter(args[2]);
    }

    // Create a CoreNLP pipeline. To build the default pipeline, you can just use:
    //   StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // Here's a more complex setup example:
    //   Properties props = new Properties();
    //   props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");
    //   props.put(""ner.model"", ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"");
    //   props.put(""ner.applyNumericClassifiers"", ""false"");
    //   StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Add in sentiment
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    File[] files = new File(""C:/stanford-corenlp-full-2016-10-31/dataset"").listFiles();

    String line = null;

    try{
        for (File file : files) {
            if (file.exists()) {
                BufferedReader in = new BufferedReader(new FileReader(file));
                while((line = in.readLine()) != null)
                {
                    Annotation document = new Annotation(line);

                    // run all the selected Annotators on this text
                    pipeline.annotate(document);

                    // this prints out the results of sentence analysis to file(s) in good formats
                    pipeline.prettyPrint(document, out);
                    if (xmlOut != null) {
                      pipeline.xmlPrint(document, xmlOut);
                    }

                    // An Annotation is a Map with Class keys for the linguistic analysis types.
                    // You can get and use the various analyses individually.
                    // For instance, this gets the parse tree of the first sentence in the text.
                    List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
                    if (sentences != null &amp;&amp; ! sentences.isEmpty()) {
                      CoreMap sentence = sentences.get(0);
                      /*out.println(""The keys of the first sentence's CoreMap are:"");
                      out.println(sentence.keySet());
                      out.println();
                      out.println(""The first sentence is:"");
                      out.println(sentence.toShorterString());
                      out.println();
                      out.println(""The first sentence tokens are:"");*/
                      for (CoreMap token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                        //out.println(token.toShorterString());
                      }
                      Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
                      //out.println();
                      //out.println(""The first sentence parse tree is:"");
                      tree.pennPrint(out);
                      //out.println();
                      //out.println(""The first sentence basic dependencies are:"");
                      //out.println(sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class).toString(SemanticGraph.OutputFormat.LIST));
                      //out.println(""The first sentence collapsed, CC-processed dependencies are:"");
                      SemanticGraph graph = sentence.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
                      //out.println(graph.toString(SemanticGraph.OutputFormat.LIST));

                      // Access coreference. In the coreference link graph,
                      // each chain stores a set of mentions that co-refer with each other,
                      // along with a method for getting the most representative mention.
                      // Both sentence and token offsets start at 1!
                      //out.println(""Coreference information"");
                      Map&lt;Integer, CorefChain&gt; corefChains =
                          document.get(CorefCoreAnnotations.CorefChainAnnotation.class);
                      if (corefChains == null) { return; }
                      for (Map.Entry&lt;Integer,CorefChain&gt; entry: corefChains.entrySet()) {
                        //out.println(""Chain "" + entry.getKey());
                        for (CorefChain.CorefMention m : entry.getValue().getMentionsInTextualOrder()) {
                          // We need to subtract one since the indices count from 1 but the Lists start from 0
                          List&lt;CoreLabel&gt; tokens = sentences.get(m.sentNum - 1).get(CoreAnnotations.TokensAnnotation.class);
                          // We subtract two for end: one for 0-based indexing, and one because we want last token of mention not one following.
                          /*out.println(""  "" + m + "", i.e., 0-based character offsets ["" + tokens.get(m.startIndex - 1).beginPosition() +
                                  "", "" + tokens.get(m.endIndex - 2).endPosition() + "")"");*/
                        }
                      }
                      //out.println();
                      out.println(""The first sentence overall sentiment rating is "" + sentence.get(SentimentCoreAnnotations.SentimentClass.class));
                    }
                }
                in.close();
                //showFiles(file.listFiles()); // Calls same method again.
            } else {
                System.out.println(""File: "" + file.getName() + file.toString());
            }
        }
    }catch(NullPointerException e){
        e.printStackTrace();
    }
    IOUtils.closeIgnoringExceptions(out);
    IOUtils.closeIgnoringExceptions(xmlOut);
  }

}
</code></pre>

<p>NOTE: most of the code was commented so only the relevant output was viewed on the console</p>
","java, stanford-nlp","<p>The sentiment model is only designed to run on a sentence and return the sentiment of a sentence, we don't have any methods for getting full document sentiment.</p>
",0,0,302,2017-03-04 23:10:16,https://stackoverflow.com/questions/42602453/how-to-achieve-the-overall-sentiment-from-a-review-using-stanford-corenlp
Stanford parser: how to print also parsing tree and universal dependencies?,"<p>I want to print the parsing tree and the Universal dependencies from a given text line as shown here in their demo at <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a></p>

<p>This is my code</p>

<pre><code>public class ParseDoc {

    private final static String PCG_MODEL = ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"";        

    private final TokenizerFactory&lt;CoreLabel&gt; tokenizerFactory = PTBTokenizer.factory(new CoreLabelTokenFactory(), ""invertible=true"");

    private static final LexicalizedParser parser = LexicalizedParser.loadModel(PCG_MODEL);

    public Tree parse(String str) {                
        List&lt;CoreLabel&gt; tokens = tokenize(str);
        Tree tree = parser.apply(tokens);
        return tree;
    }

    private List&lt;CoreLabel&gt; tokenize(String str) {
        Tokenizer&lt;CoreLabel&gt; tokenizer =
                tokenizerFactory.getTokenizer(
                        new StringReader(str));    
        return tokenizer.tokenize();
    }

    public static void main(String[] args) { 
        String str = ""My dog also likes eating sausage."";
        // Parser parser = new Parser(); 
        Tree tree = parser.parse(str);  

        List&lt;Tree&gt; leaves = tree.getLeaves();
        // Print words and Pos Tags
        for (Tree leaf : leaves) { 
            Tree parent = leaf.parent(tree);
            System.out.print(leaf.label().value() + ""-"" + parent.label().value() + "" "");
        }
        System.out.println(); 
    }
}
</code></pre>

<p>Unfortunately I can only get is the tagging </p>

<pre><code>My-PRP$ dog-NN also-RB likes-VBZ eating-VBG sausage-NN .-.
</code></pre>

<p>which isn't of any use to me.</p>

<p>I want to print the tree:</p>

<pre><code>(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (NP (NN sausage)))))
    (. .)))
</code></pre>

<p>and the universal dependencies:</p>

<pre><code>nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
</code></pre>

<p>How can I achieve this?</p>
","java, parsing, stanford-nlp","<p>Here is some sample code:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.util.*;

import java.util.*;

public class PrintParse {

  public static void main(String[] args) {
    Annotation document =
        new Annotation(""My dog also likes eating sausage."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      Tree constituencyParse = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
      System.out.println(constituencyParse);
      SemanticGraph dependencyParse =
          sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);
      System.out.println(dependencyParse.toList());
    }
  }

}
</code></pre>
",2,1,1189,2017-03-07 14:05:07,https://stackoverflow.com/questions/42650371/stanford-parser-how-to-print-also-parsing-tree-and-universal-dependencies
Error on non English satisfying sentence DL4J and NLP,"<p>I am trying to run the sample program from the Dl4J examples. Here is the program: <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/Word2VecSentimentRNN.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/Word2VecSentimentRNN.java</a><br>
I have done only a simple tweek for getting continuous input through commandline.<br>
Now when I input perfect English sentence then it gives me output the sentiments. But when I type something weird then it throws exception.<br>
Here is the example:  </p>

<pre><code>eweweerfsd dfddfdr
Exception in thread ""main"" org.nd4j.linalg.exception.ND4JIllegalStateException: Invalid shape: Requested INDArray shape [1, 300, 0] contains dimension size values &lt; 1 (all dimensions must be 1 or more)
    at org.nd4j.linalg.factory.Nd4j.checkShapeValues(Nd4j.java:4654)
    at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4644)
    at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:3810)
    at sf.sentiment.analyzer.core.SentimentAnalyser.getDataSet(SentimentAnalyser.java:77)
    at sf.sentiment.analyzer.core.SentimentAnalyser.predict(SentimentAnalyser.java:46)
    at sf.sentiment.analyzer.SentimentAnalysis.main(SentimentAnalysis.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
</code></pre>

<p>I want to know how I can avoid this type of problem? I would like to know I can find whether I should give input to the program or just say that the sentence is not proper? How I can know that there is no spell mistake? In short how to judget the sentence for giving input to the program?<br>
Kindly suggest. I am eager to know the solution.</p>
","java, maven, stanford-nlp, dl4j","<p>If I had to guess the issue is that you are submitting words that aren't in the word2vec vocabulary, so something is going wrong when it can't find a word vector for <code>eweweerfsd</code>.  Simple solutions would be to skip sentences with unknown vocabulary words or remove unknown words or replace the unknown words with a rare word that is in the word2vec vocabulary.</p>
",1,2,369,2017-03-08 07:15:51,https://stackoverflow.com/questions/42665183/error-on-non-english-satisfying-sentence-dl4j-and-nlp
lucene chararrayset not found,"<p>I am using the below solution for removing stopwords while applying stanford NLP.</p>

<p><a href=""https://github.com/jconwell/coreNlp"" rel=""nofollow noreferrer"">https://github.com/jconwell/coreNlp</a></p>

<p>This project has dependency on old version of Lucene ( 3.6.2 )</p>

<p>I need to migrate this code to lucene 5.5.2 in order to utilise latestfeatures of lucene.</p>

<p>While I try to fix the below file ,</p>

<p><a href=""https://github.com/jconwell/coreNlp/blob/master/src/main/java/intoxicant/analytics/coreNlp/StopwordAnnotator.java"" rel=""nofollow noreferrer"">https://github.com/jconwell/coreNlp/blob/master/src/main/java/intoxicant/analytics/coreNlp/StopwordAnnotator.java</a></p>

<p>I observed that the below classes are no longer available in lucene 5.5.2</p>

<pre><code>import org.apache.lucene.analysis.CharArraySet;
import org.apache.lucene.analysis.StopAnalyzer;
</code></pre>

<p>I could not find information on the alternate classes for these from Lucene documentation.</p>

<p>In case if anybody is aware on the right classes to be used from the latest lucene release  , kindly revert back.</p>
","lucene, nlp, stanford-nlp, stop-words","<p>Below are the classes to be used from lucene 5.5.2</p>

<pre><code>import org.apache.lucene.analysis.util.CharArraySet;
import org.apache.lucene.analysis.core.StopAnalyzer;
</code></pre>
",0,-1,273,2017-03-08 12:07:19,https://stackoverflow.com/questions/42670935/lucene-chararrayset-not-found
Java - the method get(Class) is undefined for the type String,"<p>I receive the following error when running my code :</p>

<pre><code>The method get(Class) is undefined for the type String.
</code></pre>

<p>I understand why that I get the issue because the get() method can not be run on a String.  However I was hoping someone could advise me on how to fix this issue? All advice is very much appreciated. The section of code where I get the issue is:</p>

<pre><code>public static void averageSent(String review) 
  { 
        //populate sentence array with each sentence of the review
        String [] sentences = review.split(""[!?.]+"");

        //number of sentences in each review
        int review_sentences = review.split(""[!?.]+"").length;

        //array of sentiments for each review
        String sentiments[] = new String[review_sentences];

        //initialise total
        int total = 0;

        //populate sentiments array
        for (int i=0; i&lt; review_sentences; i++)
            {
                **sentiments[i]=sentences[i].get(SentimentCoreAnnotations.SentimentClass.class).toString();**
            }

        //output sentiments array
        for (int i =0; i &lt; sentiments.length; i++)
        {
            System.out.println(""SENTIMENTS"" +sentiments[i]);
        }

        //total up a score for the array
        for (String s: sentiments)
        {
            ...
        }
    }
</code></pre>

<p>The entire code is :</p>

<pre><code>import java.io.*;
import java.util.*;

import edu.stanford.nlp.coref.CorefCoreAnnotations;

import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.io.EncodingPrintWriter.out;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.pipeline.CoreNLPProtos.Sentiment;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;
import edu.stanford.nlp.sentiment.Evaluate;
import org.apache.commons.io.FileUtils;

/** This class demonstrates building and using a Stanford CoreNLP pipeline. */
public class sentimentMain {

  /** Usage: java -cp ""*"" StanfordCoreNlpDemo [inputFile [outputTextFile [outputXmlFile]]] */
  public static void main(String[] args) throws IOException {

      //ArrayList&lt;String&gt; Sentences = new ArrayList&lt;String&gt;();
      //ArrayList&lt;String&gt; sentence_sentiment = new ArrayList&lt;String&gt;();

    // set up optional output files
    PrintWriter out;
    if (args.length &gt; 1) {
      out = new PrintWriter(args[1]);
    } else {
      out = new PrintWriter(System.out);
    }
    PrintWriter xmlOut = null;
    if (args.length &gt; 2) {
      xmlOut = new PrintWriter(args[2]);
    }
    // Add in sentiment
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    File[] files = new File(""C:/stanford-corenlp-full-2016-10-31/dataset"").listFiles();

    String line = null;

    try{
        for (File file : files) {
            if (file.exists()) {
                BufferedReader in = new BufferedReader(new FileReader(file));
                String str = FileUtils.readFileToString(file);
                while((line = in.readLine()) != null)
                {
                    Annotation document = new Annotation(line);

                    // run all the selected Annotators on this text
                    pipeline.annotate(document);

                    // this prints out the results of sentence analysis to file(s) in good formats
                    pipeline.prettyPrint(document, out);
                    if (xmlOut != null) {
                      pipeline.xmlPrint(document, xmlOut);
                    }

                    List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
                    if (sentences != null &amp;&amp; ! sentences.isEmpty()) {
                      CoreMap sentence = sentences.get(0);
                      for (CoreMap token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                      }
                      Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
                      tree.pennPrint(out);
                      SemanticGraph graph = sentence.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);   
                      Map&lt;Integer, CorefChain&gt; corefChains =
                          document.get(CorefCoreAnnotations.CorefChainAnnotation.class);
                      if (corefChains == null) { return; }
                      for (Map.Entry&lt;Integer,CorefChain&gt; entry: corefChains.entrySet()) {
                        for (CorefChain.CorefMention m : entry.getValue().getMentionsInTextualOrder()) {
                          // We need to subtract one since the indices count from 1 but the Lists start from 0
                          List&lt;CoreLabel&gt; tokens = sentences.get(m.sentNum - 1).get(CoreAnnotations.TokensAnnotation.class);
                        }
                      }
                      out.println(""The first sentence overall sentiment rating is "" + sentence.get(SentimentCoreAnnotations.SentimentClass.class));

                    }
                    }

                      //Sentences.forEach(s -&gt; System.out.println(s));
                      //sentence_sentiment.forEach(s -&gt; System.out.println(s));
                      averageSent(str); 
                in.close();
            } else {
                System.out.println(""File: "" + file.getName() + file.toString());
            }
        }
    }catch(NullPointerException e){
        e.printStackTrace();
    }
    IOUtils.closeIgnoringExceptions(out);
    IOUtils.closeIgnoringExceptions(xmlOut);
  }

  public static void averageSent(String review) 
  { 
        //populate sentence array with each sentence of the review
        String [] sentences = review.split(""[!?.]+"");

        //number of sentences in each review
        int review_sentences = review.split(""[!?.]+"").length;

        //array of sentiments for each review
        String sentiments[] = new String[review_sentences];

        //initialise total
        int total = 0;

        //populate sentiments array
        for (int i=0; i&lt; review_sentences; i++)
            {
                sentiments[i]=sentences[i].get(SentimentCoreAnnotations.SentimentClass.class).toString();
            }

        //output sentiments array
        for (int i =0; i &lt; sentiments.length; i++)
        {
            System.out.println(""SENTIMENTS"" +sentiments[i]);
        }

        //total up a score for the array
        for (String s: sentiments)
        {

            if (s.equals(""Very positive""))
            {
                int veryPos = 4;
                total += veryPos;
            }
            else if (s.equals(""Positive""))
            {
                int pos = 3;
                total += pos;
            }
            else if (s.equals(""Negative""))
            {
                int neg = 2;
                total += neg;
            }
            else if (s.equals(""Very negative""))
            {
                int veryNeg = 1;
                total += veryNeg;
            }
            else if (s.equals(""Neutral""))
            {
                int neu = 0;
                total += neu;
            }

            //System.out.println(""Total "" +total);
    }

  }


}
</code></pre>

<p>The main code used is Stanford CoreNLP sentiment analysis model.  However as this model can only give me the sentiment of individual sentences within movie reviews, I have designed this method so it will allow me to get an average sentiment of an entire review.  The get() method returns the sentiment of the sentence.  It works if the object is from the CoreMap, however I need it to get the sentiment of a string so it can populate a string array of the multiple sentiments for each of the movie reviews.</p>
","java, stanford-nlp","<p>Here is some sample code for going through the sentiment of each sentiment and counting the ""Positive"", ""Negative"", and ""Neutral"" sentences.</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.sentiment.*;
import edu.stanford.nlp.util.*;
import java.util.Properties;

public class AverageDocumentSentiment {

  public static void main(String[] args) {
    Annotation document =
        new Annotation(""...movie review text..."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,sentiment"");
    props.setProperty(""parse.binaryTrees"",""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    int totalNegative = 0;
    int totalPositive = 0;
    int totalNeutral = 0;
    int total = 0;
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      String sentenceSentiment = sentence.get(SentimentCoreAnnotations.SentimentClass.class);
      System.out.println(sentenceSentiment);
      if (sentenceSentiment.equals(""Negative"")) {
        totalNegative++;
      } else if (sentenceSentiment.equals(""Positive"")) {
        totalPositive++;
      } else {
        totalNeutral++;
      }
      total++ ;
    }
  }
}
</code></pre>
",0,0,1064,2017-03-11 14:17:12,https://stackoverflow.com/questions/42736280/java-the-method-getclass-is-undefined-for-the-type-string
Stanford segmenter nltk Could not find SLF4J in your classpath,"<p>I've set up a <code>nltk</code> and <code>stanford</code> environment, and <code>nltk</code> and <code>stanford</code> jars has downloaded, the program with <code>nltk</code> was ok, but I had a trouble with <code>stanford</code> segmenter. just make a simple program via <code>stanford</code> segmenter, I got a error is Could not find <code>SLF4J</code> in your classpath, although I had exported all jars including <code>slf4j-api.jar</code>. Detail as follows</p>

<ul>
<li>Python3.5 NLTK 3.2.2 Standford jars 3.7</li>
<li>OS: Centos</li>
<li><p>environment variable:</p>

<pre><code>export JAVA_HOME=/usr/java/jdk1.8.0_60
export NLTK_DATA=/opt/nltk_data
export STANFORD_SEGMENTER_PATH=/opt/stanford/stanford-segmenter-3.7
export CLASSPATH=$CLASSPATH:$STANFORD_SEGMENTER_PATH/stanford-segmenter.jar
export STANFORD_POSTAGGER_PATH=/opt/stanford/stanford-postagger-full-2016-10-31
export CLASSPATH=$CLASSPATH:$STANFORD_POSTAGGER_PATH/stanford-postagger.jar
export STANFORD_NER_PATH=/opt/stanford/stanford-ner-2016-10-31
export CLASSPATH=$CLASSPATH:$STANFORD_NER_PATH/stanford-ner.jar
export STANFORD_MODELS=$STANFORD_NER_PATH/classifiers:$STANFORD_POSTAGGER_PATH/models
export STANFORD_PARSER_PATH=/opt/stanford/stanford-parser-full-2016-10-31
export CLASSPATH=$CLASSPATH:$STANFORD_PARSER_PATH/stanford-parser.jar:$STANFORD_PARSER_PATH/stanford-parser-3.6.0-models.jar:$STANFORD_PARSER_PATH/slf4j-api.jar:$STANFORD_PARSER_PATH/ejml-0.23.jar
export STANFORD_CORENLP_PATH=/opt/stanford/stanford-corenlp-full-2016-10-31
export CLASSPATH=$CLASSPATH:$STANFORD_CORENLP_PATH/stanford-corenlp-3.7.0.jar:$STANFORD_CORENLP_PATH/stanford-corenlp-3.7.0-models.jar:$STANFORD_CORENLP_PATH/javax.json.jar:$STANFORD_CORENLP_PATH/joda-time.jar:$STANFORD_CORENLP_PATH/jollyday.jar:$STANFORD_CORENLP_PATH/protobuf.jar:$STANFORD_CORENLP_PATH/slf4j-simple.jar:$STANFORD_CORENLP_PATH/xom.jar
export STANFORD_CORENLP=$STANFORD_CORENLP_PATH
</code></pre></li>
</ul>

<p>The program as follows:</p>

<pre><code>from nltk.tokenize import StanfordSegmenter
&gt;&gt;&gt; segmenter = StanfordSegmenter(
    path_to_sihan_corpora_dict=""/opt/stanford/stanford-segmenter-3.7/data/"",
    path_to_model=""/opt/stanford/stanford-segmenter-3.7/data/pku.gz"",
    path_to_dict=""/opt/stanford/stanford-segmenter-3.7/data/dict-chris6.ser.gz""
)... ... ... ... 
&gt;&gt;&gt; res = segmenter.segment(u""北海已成为中国对外开放中升起的一颗明星"")
</code></pre>

<p>The error as follows:</p>

<pre><code>Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.&lt;clinit&gt;(AbstractSequenceClassifier.java:88)
Caused by: java.lang.IllegalStateException: Could not find SLF4J in your classpath
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:190)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.buildChain(RedwoodConfiguration.java:309)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.apply(RedwoodConfiguration.java:318)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.lambda$handlers$535(RedwoodConfiguration.java:363)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.apply(RedwoodConfiguration.java:41)
    at edu.stanford.nlp.util.logging.Redwood.&lt;clinit&gt;(Redwood.java:609)
    ... 1 more
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:364)
    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:381)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:186)
    ... 6 more
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.construct(MetaClass.java:135)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.&lt;init&gt;(MetaClass.java:202)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.&lt;init&gt;(MetaClass.java:69)
    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:360)
    ... 8 more

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 96, in segment
    return self.segment_sents([tokens])
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 123, in segment_sents
    stdout = self._execute(cmd)
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 143, in _execute
    cmd,classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/internals.py"", line 134, in java
    raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : ['/usr/java/jdk1.8.0_60/bin/java', '-mx2g', '-cp', '/opt/stanford/stanford-segmenter-3.7/stanford-segmenter.jar:/opt/stanford/stanford-parser-full-2016-10-31/slf4j-api.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-sighanCorporaDict', '/opt/stanford/stanford-segmenter-3.7/data/', '-textFile', '/tmp/tmpkttpldl6', '-sighanPostProcessing', 'true', '-keepAllWhitespaces', 'false', '-loadClassifier', '/opt/stanford/stanford-segmenter-3.7/data/pku.gz', '-serDictionary', '/opt/stanford/stanford-segmenter-3.7/data/dict-chris6.ser.gz', '-inputEncoding', 'UTF-8']
</code></pre>

<p>Thank you in advance!</p>
","python-3.x, nltk, stanford-nlp","<p>With the current code base if you have the slf4j-api.jar in your CLASSPATH and run the 3.7.0 segmenter you will get this error.  I'm going to push a code change to fix this but for the time being if you remove the slf4j-api.jar from the CLASSPATH this error should go away.</p>
",1,0,692,2017-03-12 04:37:58,https://stackoverflow.com/questions/42743824/stanford-segmenter-nltk-could-not-find-slf4j-in-your-classpath
How to import Stanford POS Tagger,"<p>I'm attempting to make use of the Stanford POS Tagger in Python.</p>

<pre><code>home = 'U:/ManWin/My Documents/Research Project'
from nltk.tag.stanford import StanfordPOSTagger as POS_Tag
_path_to_model = home + '/stanford-postagger/models/english-bidirectional-distsim.tagger' 
_path_to_jar = home + '/stanford-postagger/stanford-postagger.jar'
st = POS_Tag(path_to_model=_path_to_model, path_to_jar=_path_to_jar)
</code></pre>

<p>Have copied the last line from the answer here: <a href=""https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag"">Python NLTK pos_tag not returning the correct part-of-speech tag</a></p>

<p>Getting the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""U:\Python35\site-packages\nltk\tag\stanford.py"", line 136, in __init__
    super(StanfordPOSTagger, self).__init__(*args, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'path_to_model'
</code></pre>

<p>What do I need to change?</p>
","python, nltk, typeerror, stanford-nlp","<p>It seems the name of the parameter <code>path_to_model</code> has changed to <code>model_filename</code>. So, replace the last line with:</p>

<p><code>st = POS_Tag(model_filename=_path_to_model, path_to_jar=_path_to_jar)</code></p>

<p>Or since the parameters are in order, just write:</p>

<p><code>st = POS_Tag(_path_to_model, _path_to_jar)</code></p>
",4,1,5844,2017-03-13 14:39:25,https://stackoverflow.com/questions/42766351/how-to-import-stanford-pos-tagger
What is the purpose of EventMention in Stanford-NLP?,"<p>When using the pipeline annotator, <code>relation</code>, I get back appropriate <code>RelationMention</code> objects for each sentence.  These are binary type objects with two entity mentions and a corresponding relation type.</p>

<p>However, in the code, I also see <code>EventMention</code> objects which can be obtained from the sentence in much the same way.  In the class <code>MachineReadingProperties</code>, I see that extraction of relations and extraction of events both default to true.  However, I am only seeing generated relations and not generated events.</p>

<p>I can find no mention of events in the Stanford documentation, nor does the page describing the relation annotator or how to train a custom relation model describe it.  There are no links to research papers on the event portion, just the link to the Roth and Yih paper on relations.</p>

<p>So do events work with the <code>relation</code> annotator and if so are there any more documents describing them?</p>
",stanford-nlp,"<p>From tracing the code, I see that EventMention is only used in conjunction with the ACE2005 code.  It does not appear to be connected with any of the pipeline annotators and cannot be directly created in that manner.</p>
",0,0,67,2017-03-13 16:17:09,https://stackoverflow.com/questions/42768478/what-is-the-purpose-of-eventmention-in-stanford-nlp
Stanford NLP for Sentiment analysis of tweets stages,"<p>The original tweets have been saved into a file in the following structure:</p>

<blockquote>
  <p>tweet language || tweet</p>
</blockquote>

<p>The following is my pre-processing stage to remove URL's, RT, usernames and any non-alpha numeric character. </p>

<p>def cleanTweets() {</p>

<pre><code>    File dirtyTweets = new File(""result.txt"")
    File cleanTweets = new File(""cleanTweets.txt"")

    try {
        Scanner console = new Scanner(dirtyTweets)

        PrintWriter printWriter = new PrintWriter(new BufferedWriter(new FileWriter(cleanTweets)))
        LinkedHashSet&lt;String&gt; ln = new LinkedHashSet&lt;String&gt;();

        while (console.hasNextLine()) {

            String line = console.nextLine();

            String[] splitter = line.split(""\\|\\|\\|"")
            //Only looks at the english tweets
            if (splitter[0] == ""en"") {

                line = line.replaceFirst(""en"", """")

                String urlIdentifier = ""((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;amp;/~\\+#])?""

                //Removes URL's, RT, Twitter usernames and any non alpha numeric character
                String[] removeNoise = [""RT"", urlIdentifier, ""(?:\\s|\\A)[@]+([A-Za-z0-9-_]+)"", ""[^a-zA-Z0-9 ]""]

                removeNoise.each { noise -&gt;
                    line = line.replaceAll(noise, """").toLowerCase()
                }
                ln.add(line)

            }
        }

        ln.each { line -&gt;
            printWriter.write(line)
            printWriter.println()
        }
        //write to file here
    } catch (IOException e) {
    }
}
</code></pre>

<p>This is then saved into a new file. What would the next stage be for sentiment analysis of these tweets?</p>
","java, twitter, stanford-nlp","<p>Here is some sample code for using the sentiment annotator:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.sentiment.*;
import edu.stanford.nlp.util.*;
import java.util.Properties;

public class SentimentExample {

  public static void main(String[] args) {
    Annotation document = new Annotation(""...insert tweet text here..."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,sentiment"");
    // you might want to enforce treating the entire tweet as one sentence
    //...if so uncomment the line below setting ssplit.eolonly to true
    // also make sure you remove newlines, this will prevent the
    // sentence splitter from dividing the tweet into different sentences
    //props.setProperty(""ssplit.eolonly"",""true"");
    props.setProperty(""parse.binaryTrees"",""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(sentence.get(CoreAnnotations.TextAnnotation.class));
      System.out.println(sentence.get(SentimentCoreAnnotations.SentimentClass.class));
    }
  }
}
</code></pre>
",2,0,355,2017-03-13 23:07:54,https://stackoverflow.com/questions/42775061/stanford-nlp-for-sentiment-analysis-of-tweets-stages
How to generate sentiment treebank in Stanford NLP,"<p>I'm using Sentiment Stanford NLP library for sentiment analytics.</p>

<p>Now I want to generate a treebank from a sentence</p>

<p>input sentence: ""Effective but too-tepid biopic""</p>

<p>output tree bank: (2 (3 (3 Effective) (2 but)) (1 (1 too-tepid) (2 biopic)))</p>

<p>Can anybody show me how to do it ?
Thank all.</p>
","stanford-nlp, sentiment-analysis, penn-treebank","<p>So I had to push a bug fix for the SentimentPipeline.</p>

<p>If you get the latest code from GitHub and use that version: <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP</a></p>

<p>you can issue this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.sentiment.SentimentPipeline -file example-sentences.txt -output PENNTREES
</code></pre>

<p>and you'll get output like this:</p>

<pre><code>I really liked the movie.
(3 (2 I) (3 (2 really) (3 (3 (3 liked) (2 (2 the) (2 movie))) (2 .))))
I hated the movie.
(1 (2 I) (1 (1 (1 hated) (2 (2 the) (2 movie))) (2 .)))
</code></pre>
",0,1,983,2017-03-15 04:42:47,https://stackoverflow.com/questions/42801238/how-to-generate-sentiment-treebank-in-stanford-nlp
Using Stanford Tregex in Python,"<p>I'm a newbie in NLP and Python. I'm trying to extract a subset of noun phrases from parsed trees from StanfordCoreNLP by using the Tregex tool and the Python subprocess library. In particular, I'm trying to find and extract noun phrases that match the following pattern: '(NP[$VP]>S)|(NP[$VP]>S\n)|(NP\n[$VP]>S)|(NP\n[$VP]>S\n)' in the Tregex grammar. </p>

<p>For example, below is the original text, saved in a string named ""text"":</p>

<pre><code>text = ('Pusheen and Smitha walked along the beach. ""I want to surf"", said Smitha, the CEO of Tesla. However, she fell off the surfboard')
</code></pre>

<p>After running the StanfordCoreNLP parser using the Python wrapper, I got the following 3 trees for the 3 sentences:</p>

<pre><code>output1['sentences'][0]['parse']

Out[58]: '(ROOT\n  (S\n    (NP (NNP Pusheen)\n      (CC and)\n      (NNP Smitha))\n    (VP (VBD walked)\n      (PP (IN along)\n        (NP (DT the) (NN beach))))\n    (. .)))'

output1['sentences'][1]['parse']

Out[59]: ""(ROOT\n  (SINV (`` ``)\n    (S\n      (NP (PRP I))\n      (VP (VBP want)\n        (PP (TO to)\n          (NP (NN surf) ('' '')))))\n    (, ,)\n    (VP (VBD said))\n    (NP\n      (NP (NNP Smitha))\n      (, ,)\n      (NP\n        (NP (DT the) (NNP CEO))\n        (PP (IN of)\n          (NP (NNP Tesla)))))\n    (. .)))""

output1['sentences'][2]['parse']

Out[60]: '(ROOT\n  (S\n    (ADVP (RB However))\n    (, ,)\n    (NP (PRP she))\n    (VP (VBD fell)\n      (PRT (RP off))\n      (NP (DT the) (NN surfboard)))))'
</code></pre>

<p>I would like to extract the following 3 noun phrases (one for each sentence) and save them as variables (or lists of tokens) in Python:</p>

<ul>
<li>(NP (NNP Pusheen) \n (CC and) \n (NNP Smitha))</li>
<li>(NP (PRP I))</li>
<li>(NP (PRP she))</li>
</ul>

<p>For your information, I have used of tregex from the command-line with the following code:</p>

<pre><code>cd stanford-tregex-2016-10-31
java -cp 'stanford-tregex.jar:' edu.stanford.nlp.trees.tregex.TregexPattern -f -s '(NP[$VP]&gt;S)|(NP[$VP]&gt;S\n)|(NP\n[$VP]&gt;S)|(NP\n[$VP]&gt;S\n)' /Users/AS/stanford-tregex-2016-10-31/exampletree.txt
</code></pre>

<p>The output was:</p>

<pre><code>Pattern string:
(NP[$VP]&gt;S)|(NP[$VP]&gt;S\n)|(NP\n[$VP]&gt;S)|(NP\n[$VP]&gt;S\n)
Parsed representation:
or
   Root NP
      and
         $ VP
         &gt; S
   Root NP
      and
         $ VP
         &gt; S\n
   Root NP\n
      and
         $ VP
         &gt; S
   Root NP\n
      and
         $ VP
         &gt; S\n
Reading trees from file(s) file path
\# /Users/AS/stanford-tregex-2016-10-31/exampletree.txt
(NP (NNP Pusheen) \n (CC and) \n (NNP Smitha))
\# /Users/AS/stanford-tregex-2016-10-31/exampletree.txt
(NP\n (NP (NNP Smitha)) \n (, ,) \n (NP\n (NP (DT the) (NN spokesperson)) \n   (PP (IN of) \n (NP (DT the) (NNP CIA)))) \n (, ,))
\# /Users/AS/stanford-tregex-2016-10-31/exampletree.txt
(NP (PRP They))
There were 3 matches in total.
</code></pre>

<p>How can I replicate this result in Python? </p>

<p>For your reference, I found the following post via Google, which is relevant to my question but outdated (<a href=""https://mailman.stanford.edu/pipermail/parser-user/2010-July/000606.html"" rel=""noreferrer"">https://mailman.stanford.edu/pipermail/parser-user/2010-July/000606.html</a>):</p>

<p>[parser-user] Variable input to Tregex</p>

<p>Christopher Manning manning at stanford.edu 
Wed Jul 7 17:41:32 PDT 2010
Hi Haiyang,</p>

<p>Sorry, slow reply, things are too busy at the end of the academic year.</p>

<p>On Jun 1, 2010, at 8:56 PM, Haiyang AI wrote:</p>

<blockquote>
  <p>Dear All,</p>
  
  <p>I hope this is the right place to seek help.</p>
</blockquote>

<p>It is, though we can only give very limited help on anything Python specific.....</p>

<p>But this seems to be straightforward (I think). </p>

<p>If what you're wanting is for the pattern to be run on trees being fed in over stdin, you need to add the flag ""-filter"" in the argument list prior to ""NP"".  </p>

<p>If no file is specified after the pattern, and the flag ""-filter"" is not given, then it runs the pattern on a fixed default sentence....</p>

<p>Chris.</p>

<blockquote>
  <p>I'm working on a project related to Tregex. I'm trying to call Tregex from python, but I don't know how to feed data into Tregex, not from conventional file, but from a variable. For example, I'm trying to count the number of ""NP"" from a given variable (e.g. text, already parsed tree, using Stanford Parser), with the following code,</p>
  
  <p>def tregex(text):<br>
      tregex_dir = ""/root/nlp/stanford-tregex-2009-08-30/""
      op = Popen([""java"", ""-mx900m"", ""-cp"", ""stanford-tregex.jar:"", ""edu.stanford.nlp.trees.tregex.TregexPattern"", ""NP""],
          cwd = tregex_dir,
          stdout = PIPE, 
          stdin  = PIPE,
          stderr = STDOUT)
      res = op.communicate(input=text)[0]
      return res</p>
  
  <p>The results are like the following. It didn't search the content from the variable, but somehow falling back to ""using default tree"". Can anyone give me a hand? I have been stuck here for quite a long time. Really appreciate your time and help.
  Pattern string:
  NP
  Parsed representation:
  Root NP
  using default tree
  (NP
    (NP (DT this) (NN wine))
    (CC and)
    (NP (DT these) (NNS snails)))</p>
  
  <p>(NP (DT this) (NN wine))</p>
  
  <p>(NP (DT these) (NNS snails))</p>
  
  <p>There were 3 matches in total.</p>
  
  <p>-- 
  Haiyang AI, Ph.D. student
  Department of Applied Linguistics
  The Pennsylvania State University</p>
  
  <hr>
  
  <p>parser-user mailing list
  parser-user at lists.stanford.edu
  <a href=""https://mailman.stanford.edu/mailman/listinfo/parser-user"" rel=""noreferrer"">https://mailman.stanford.edu/mailman/listinfo/parser-user</a></p>
</blockquote>
","python, parsing, pattern-matching, subprocess, stanford-nlp","<p>Why not use the Stanford CoreNLP server!</p>

<p>1.) Start up the server!</p>

<pre><code>java -Xmx4g edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 - timeout 15000
</code></pre>

<p>2.) Make a python request!</p>

<pre><code>import requests

url = ""http://localhost:9000/tregex""
request_params = {""pattern"": ""(NP[$VP]&gt;S)|(NP[$VP]&gt;S\\n)|(NP\\n[$VP]&gt;S)|(NP\\n[$VP]&gt;S\\n)""}
text = ""Pusheen and Smitha walked along the beach.""
r = requests.post(url, data=text, params=request_params)
print r.json()
</code></pre>

<p>3.) Here are the results!</p>

<pre><code>{u'sentences': [{u'0': {u'namedNodes': [], u'match': u'(NP (NNP Pusheen)\n  (CC and)\n  (NNP Smitha))\n'}}]}
</code></pre>
",7,5,2583,2017-03-15 06:20:24,https://stackoverflow.com/questions/42802406/using-stanford-tregex-in-python
How to use Stanford Open IE with nltk,"<p>I am on an NLP project right now and I need to use Stanford Open information extraction tool with python (nltk if possible). I found <a href=""https://github.com/philipperemy/Stanford-OpenIE-Python"" rel=""nofollow noreferrer"">a python wrapper</a></p>

<p>but it's poorly documented and does not give full functionality interface to Stanford Open IE. Any suggestion?</p>
","nlp, nltk, stanford-nlp","<p>One approach is to use the <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">CoreNLP Server</a>, which outputs OpenIE triples (see, e.g., <a href=""http://corenlp.run"" rel=""nofollow noreferrer"">corenlp.run</a>). Among <a href=""http://stanfordnlp.github.io/CoreNLP/other-languages.html"" rel=""nofollow noreferrer"">other libraries</a>, Stanford's <a href=""https://github.com/stanfordnlp/stanza/tree/master/stanza"" rel=""nofollow noreferrer"">Stanza</a> library is written in Python can call a server instance to get annotations. Make sure to include all the required annotators: <code>tokenize,ssplit,pos,lemma,ner,depparse,natlog,openie</code>.</p>
",4,4,4333,2017-03-17 02:58:31,https://stackoverflow.com/questions/42848438/how-to-use-stanford-open-ie-with-nltk
How to pass a String to AbstractSequenceClassifier.classifyAndWriteAnswersKBest in CoreNLP?,"<p><code>AbstractSequenceClassifier.classifyAndWriteAnswersKBest</code> allows to pass a filename and an <code>ObjectBank&lt;List&lt;IN&gt;&gt;</code>, but it's unclear from <code>ObjectBank</code>'s doc how to create such an <code>ObjectBank</code> without involving a file.</p>

<p>I'm using CoreNLP 3.7.0 with Java 8.</p>
","java, stanford-nlp","<p>You should just use this method instead:</p>

<pre><code>Counter&lt;List&lt;IN&gt;&gt; classifyKBest(List&lt;IN&gt; doc, Class&lt;? extends CoreAnnotation&lt;String&gt;&gt; answerField, int k)
</code></pre>

<p>It will return a mapping of returned sequences to scores.</p>

<p>With this line of code you can turn that counter into a sorted list of sequences:</p>

<pre><code>List&lt;List&lt;IN&gt;&gt; sorted = Counters.toSortedList(kBest);
</code></pre>

<p>I'm not sure exactly what you're trying to do, but typically IN is a CoreLabel.  The key thing here is to turn your String into a list of IN's.  This should be a CoreLabel, but I don't know the full details of the AbstractSequenceClassifier you are working with.</p>

<p>If you want to run your sequence classifier on a sentence, you could first tokenize it with a pipeline and then pass the list of tokens to <code>classifyKBest(...)</code></p>

<p>For instance if in your example you are trying to get the k-best named entity tags:</p>

<pre><code>// set up pipeline
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize"");
StanfordCoreNLP tokenizerPipeline = new StanfordCoreNLP(props);

// get list of tokens for example sentence
String exampleSentence = ""..."";
// wrap sentence in an Annotation object
Annotation annotation = new Annotation(exampleSentence);
// tokenize sentence
tokenizerPipeline.annotate(annotation);
// get the list of tokens
List&lt;CoreLabel&gt; tokens = annotation.get(CoreAnnotations.TokensAnnotation.class);

//...
// classifier should be an AbstractSequenceClassifier

// get the k best sequences from your abstract sequence classifier
Counter&lt;List&lt;CoreLabel&gt;&gt; kBestSequences = classifier.classifyKBest(tokens,CoreAnnotations.NamedEntityTagAnnotation.class, 10)
// sort the k-best examples
List&lt;List&lt;CoreLabel&gt;&gt; sortedKBest = Counters.toSortedList(kBestSequences);
// example: getting the second best list
List&lt;CoreLabel&gt; secondBest = sortedKBest.get(1);
// example: print out the tags for the second best list
System.out.println(secondBest.stream().map(token-&gt;token.get(CoreAnnotations.NamedEntityTagAnnotation.class)).collect(Collectors.joining("" "")));
// example print out the score for the second best list
System.out.println(kBestSequences.getCount(secondBest));
</code></pre>

<p>If you have more questions please let me know and I can help out!</p>


",1,0,100,2017-03-18 10:40:02,https://stackoverflow.com/questions/42873268/how-to-pass-a-string-to-abstractsequenceclassifier-classifyandwriteanswerskbest
Stanford POS tagger doesn&#39;t work with SLF4J in classpath,"<p>I'm using the Stanford POS tagger 3.7.0 in a Java project that also uses the Jena RDF API. Jena requires slf4j-api-1.7.12.jar and slf4j-log4j12-1.7.12.jar, but when trying to call the POS tagger having those jars in the classpath I get the following error:</p>

<pre><code>Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at edu.stanford.nlp.util.logging.Redwood$ConsoleHandler.out(Redwood.java:920)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.&lt;init&gt;(RedwoodConfiguration.java:28)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.empty(RedwoodConfiguration.java:385)
    at util.Splitter.split(Splitter.java:58)
    at core.Main.main(Main.java:23)
Caused by: java.lang.IllegalStateException: Could not find SLF4J in your classpath
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:190)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.buildChain(RedwoodConfiguration.java:309)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.apply(RedwoodConfiguration.java:318)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.lambda$handlers$535(RedwoodConfiguration.java:363)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.apply(RedwoodConfiguration.java:41)
    at edu.stanford.nlp.util.logging.Redwood.&lt;clinit&gt;(Redwood.java:609)
    ... 5 more
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:364)
    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:381)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:186)
    ... 10 more
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.construct(MetaClass.java:135)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.&lt;init&gt;(MetaClass.java:202)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.&lt;init&gt;(MetaClass.java:69)
    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:360)
    ... 12 more
</code></pre>

<p>I've seen that people that had a <a href=""https://stackoverflow.com/questions/42743824/stanford-segmenter-nltk-could-not-find-slf4j-in-your-classpath"">similar problem</a> solved it by removing SLF4J from the classpath. In fact, when I remove the jars the POS tagger works fine, but then Jena stops working.</p>

<p>Is there a way to keep SLF4J in the classpath and still get the POS tagger to work?</p>
","java, stanford-nlp, slf4j, jena","<p>I think the best solution is to just use the full Stanford CoreNLP 3.7.0 release which won't have this issue but will have all the POS tagging functionality.</p>

<p>The full download is available here: </p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>This issue should be resolved in the standalone distributions for Stanford CoreNLP 3.8.0 which we'll try to release in the early summer.</p>
",0,-1,352,2017-03-20 14:45:10,https://stackoverflow.com/questions/42906609/stanford-pos-tagger-doesnt-work-with-slf4j-in-classpath
Stanford NERFeatureFactory description,"<p>Do you know where can I find more details on the description of the Stanford NERFeatureFactory?</p>

<p>I read the one at: <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html</a>
but I do not understand them all (and some have no description).</p>

<p>For example:usePrev, 
useWordPairs,
conjoinShapeNGrams,
useSum, ...
or
(pw,c) (t,c)</p>

<p>There was a <a href=""https://stackoverflow.com/questions/25431094/stanford-ner-features"">similar question 2 years</a> ago without a better description. I was wondering if something new came out since then.</p>

<p>Thanks for your help!</p>
",stanford-nlp,"<p>If you look through the source code of NERFeatureFactory you can see what is going on.</p>

<p>The source code is available here: <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/NERFeatureFactory.java"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/NERFeatureFactory.java</a></p>

<p>For example, <code>useWordPairs</code> creates features for the word under consideration and the previous/next word.  You can see this in the code starting on line 1062...</p>

<p>As an example, consider the features for the word <code>New</code> in this text <code>...from New York...</code>...the useWordPairs feature produces the features <code>New-from-W-PW</code> and <code>New-York-W-NW</code></p>

<p>A lot of the features have descriptions in that file as well.</p>

<p>It's helpful to look through the code and see what is being produced.  For instance the <code>conjoinShapeNGrams</code> feature is producing features that attach the overall shape of the word and substrings of the word.  You can see fully what is going on by looking at the code.</p>

<p>As an example of <code>conjoinShapeNGrams</code>, consider the name <code>Wordsworth</code> which would get features like <code>worth-Xxxxxxxxxx-CNGram-CS</code> , <code>Words-Xxxxxxxxxx-CNGram-CS</code>, etc...</p>

<p>This feature is capturing the presence of a certain substring and word shape together.</p>

<p>(pw, c) refers to ""previous word"" and ""current word"", which is linked to the usePrev flag</p>

<p>(t, c) refers to ""part of speech tag"" and ""current word"", which is linked to the useTags flag</p>

<p>It doesn't look like <code>useSum</code> does anything anymore...</p>
",1,1,577,2017-03-21 21:19:53,https://stackoverflow.com/questions/42938316/stanford-nerfeaturefactory-description
Loading specific model file from CoreNLP Model jar file rather than a local copy of it,"<p>Can refer to the already packaged model in StanfordCoreNLP library jar, instead of having duplicate copy of the model in the project working directory for this purpose?.</p>

<pre><code>Properties configuration = new Properties();
configuration.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
configuration.put(""ner.model"", ""english.all.3class.distsim.crf.ser.gz"");
StanfordCoreNLP coreNLP  = new StanfordCoreNLP(configuration);
</code></pre>

<p>ps: The question was a part of another <strong>SO</strong> question which went unanswered.</p>
","java, maven, stanford-nlp","<p>This path:</p>

<p><code>edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz</code></p>

<p>is in stanford-corenlp-3.7.0-models.jar.</p>

<p>So if you put that in your properties, it will be loaded from the jar.</p>
",1,0,423,2017-03-24 08:14:30,https://stackoverflow.com/questions/42994465/loading-specific-model-file-from-corenlp-model-jar-file-rather-than-a-local-copy
Where in the CoreNLP code are the Penn Treebank part-of-speech symbols themselves actually represented?,"<p>I'm looking specifically for some data structure, enum, or generative process through which the different parts-of-speech are represented internally. I've spent a long time scanning the Javadoc and the source code for a while and can't find what I'm looking for. I would like to access a collection of the tags directly, if possible, if they're stored in some central location. Please forgive me if the question I'm posing constitutes a naive assumption regarding the way CoreNLP pos-tagging operates, but if what I'm describing does exist in some form, this would be very helpful. Thanks!</p>
","java, nlp, stanford-nlp, pos-tagger","<p>I'm not actually sure they're represented explicitly anywhere in the code. The tagger simply outputs them as Strings rather than any sort of fixed enum, and the output space is inferred directly from the training data. The advantage of this being that you can train the exact same model on arbitrary tag sets. And of course the disadvantage you've just run into. :)</p>

<p>However, for English, the tag set should be the Penn Treebank tag set: <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow noreferrer"">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a></p>
",1,3,272,2017-03-25 21:16:03,https://stackoverflow.com/questions/43022047/where-in-the-corenlp-code-are-the-penn-treebank-part-of-speech-symbols-themselve
Saving and Loading Trained Stanford classifier in java,"<p>I have a dataset of 1 million labelled sentences and using it for finding sentiment through Maximum Entropy. I am using Stanford Classifier for the same:-</p>

<pre><code>public class MaximumEntropy {

static ColumnDataClassifier cdc;

public static float calMaxEntropySentiment(String text) {
    initializeProperties();
    float sentiment = (getMaxEntropySentiment(text));
    return sentiment;
}

public static void initializeProperties() {
    cdc = new ColumnDataClassifier(
            ""\\stanford-classifier-2016-10-31\\properties.prop"");
}

public static int getMaxEntropySentiment(String tweet) {

    String filteredTweet = TwitterUtils.filterTweet(tweet);
    System.out.println(""Reading training file"");
    Classifier&lt;String, String&gt; cl = cdc.makeClassifier(cdc.readTrainingExamples(
            ""\\stanford-classifier-2016-10-31\\labelled_sentences.txt""));

    Datum&lt;String, String&gt; d = cdc.makeDatumFromLine(filteredTweet);
    System.out.println(filteredTweet + ""  ==&gt;  "" + cl.classOf(d) + "" "" + cl.scoresOf(d));
    // System.out.println(""Class score is: "" +
    // cl.scoresOf(d).getCount(cl.classOf(d)));
    if (cl.classOf(d) == ""0"") {
        return 0;
    } else {
        return 4;
    }
}
}
</code></pre>

<p>My data is labelled 0 or 1. Now for each tweet the whole dataset is being read and it is taking a lot of time considering the size of dataset.
My query is that is there any way to first train the classifier and then load it when a tweet's sentiment is to be found. I think this approach will take less time. Correct me if I am wrong. 
The following link provides this but there is nothing for JAVA API.
<a href=""https://nlp.stanford.edu/wiki/Software/Classifier#Saving_and_loading_this_classifier"" rel=""nofollow noreferrer"">Saving and Loading Classifier</a>
Any help would be appreciated.</p>
","java, twitter, stanford-nlp, text-classification, maxent","<p>Yes; the easiest way to do this is using Java's default serialization mechanism to serialize a classifier. A useful helper here is the <code>IOUtils</code> class:</p>

<pre><code>IOUtils.writeObjectToFile(classifier, ""/path/to/file"");
</code></pre>

<p>To read the classifier:</p>

<pre><code>Classifier&lt;String, String&gt; cl = IOUtils.readObjectFromFile(new File(""/path/to/file"");
</code></pre>
",2,1,212,2017-03-28 06:22:25,https://stackoverflow.com/questions/43061992/saving-and-loading-trained-stanford-classifier-in-java
Tagging and Training NER dataset,"<p>I have a data set and I want to tag it for Named Entity Recognition. My dataset is in Persian.
I want to know how should I tag expressions like :</p>

<p>*** آقای مهدی کاظمی  = Mr Mehdi Kazemi / Mr will Smith. >>> (names with titles) should I tag all as a person or just the first name and last name should be tagged? (I mean should i also tag ""Mr"")</p>

<p>Mr  >>  b_per     ||     Mr >> O</p>

<p>Mehdi  >>   i_per || Mehdi >> b_per</p>

<p>Kazemi  >> i_per || Kazemi >> i_per</p>

<p>*** بیمارستان نور = Noor hospital >>> Should I tag the name only or the name and hospital both as Named Entity?</p>

<p>*** Eiffel tower / The Ministry of Defense (I mean the us DOD for example) >>> in Persian it is called :
وزارت دفاع    (vezarate defa)
should I only tag Defense ? or all together?</p>

<p>There are many more examples for schools, movies, cities, countries and.... since we use the entity class before the named entity.</p>

<p>I would appreciate if you can help me with tagging this dataset.</p>
","tags, nlp, stanford-nlp, named-entity-recognition, named-entity-extraction","<p>I'll give you some examples from the CoNLL 2003 training data:</p>

<p>""Mr."" is not tagged as part of the person, so titles are ignored.</p>

<p>""Columbia Presbyterian Hospital"" is tagged as (LOC, LOC, LOC)</p>

<p>""a New York hospital"" (O, LOC, LOC, O)</p>

<p>""Ministry of Commerce"" is (ORG, ORG, ORG)</p>

<p>I think ""Eiffel Tower"" should be (LOC, LOC)</p>
",0,-1,997,2017-03-28 12:42:34,https://stackoverflow.com/questions/43069935/tagging-and-training-ner-dataset
How can I detect named entities that have more than 1 word using CoreNLP&#39;s RegexNER?,"<p>I am using the RegexNER annotator in CoreNLP and some of my named entities consist of multiple words. Excerpt from my mapping file:</p>
<blockquote>
<p>RAF inhibitor       DRUG_CLASS</p>
<p>Gilbert's syndrome   DISEASE</p>
</blockquote>
<p>The first one gets detected but each word gets the annotation DRUG_CLASS and there seems to be no way to link the words, like an NER id which both words would have.</p>
<p>The second case does not get detected at all and that's probably because the tokenizer treats the apostrophe after Gilbert as a separate token. Since RegexNER has the tokenization as a dependency, I can't really get around it.</p>
<p>Any suggestions to resolve these cases?</p>
",stanford-nlp,"<p>If you use the <code>entitymentions</code> annotator that will create entity mentions out of consecutive tokens with the same ner tags.  There is the downside that if two entities of the same type are side by side they will be joined together.  We are working on improving the ner system so we may include a new model that finds the boundaries of distinct mentions in these cases, hopefully this will go into Stanford CoreNLP 3.8.0.</p>

<p>Here is some sample code for accessing the entity mentions:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.util.*;

import java.util.*;

public class EntityMentionsExample {

  public static void main(String[] args) {
    Annotation document =
        new Annotation(""John Smith visted Los Angeles on Tuesday."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);

    for (CoreMap entityMention : document.get(CoreAnnotations.MentionsAnnotation.class)) {
      System.out.println(entityMention);
      System.out.println(entityMention.get(CoreAnnotations.TextAnnotation.class));
    }
  }
}
</code></pre>

<p>If you just have your rules tokenized the same way as the tokenizer it will work fine, so for instance the rule should be <code>Gilbert 's syndrome</code>.</p>

<p>So you could just run the tokenizer on all your text patterns and this problem will go away.</p>
",1,2,916,2017-03-29 17:55:02,https://stackoverflow.com/questions/43100750/how-can-i-detect-named-entities-that-have-more-than-1-word-using-corenlps-regex
How to overwrite a tag for a named entity with CoreNLP&#39;s RegexNER without specifying the original tag,"<p>I know that CoreNLP's RegexNER allows me to overwrite a tag using the mapping file. For example; I have the word EGFR which CoreNLP recognizes as an ORGANIZATION. If I have the following line in my mapping file, it still tags it as an ORGANIZATION.</p>

<blockquote>
  <p>EGFR    GENE</p>
</blockquote>

<p>If I change that line to look like the following:</p>

<blockquote>
  <p>EGFR    GENE     ORGANIZATION</p>
</blockquote>

<p>Then CoreNLP tags it as a GENE. </p>

<p>To be able to do this though, I have to know that CoreNLP tags EGFR as an ORGANIZATION and I can't always know that for every word in my mapping file. Now my question is, is there a way to tell the RegexNER to overwrite the tag for EGFR no matter what the original tag is? Something like</p>

<blockquote>
  <p>EGFR    GENE    .*</p>
</blockquote>
",stanford-nlp,"<p>You can provide a comma separated list of tags that can be overwritten.  </p>

<p>For instance:</p>

<pre><code>ORGANIZATION,PERSON,LOCATION,MISC
</code></pre>

<p>will allow it to overwrite all of those tags.  </p>

<p>I don't think there is an overwrite all option at the moment, so you do have to list each type you want overwritten.</p>

<p>If you always want to overwrite everything with what is in your rules you can supply that with this option to the TokensRegexNERAnnotator</p>

<pre><code>regexner.backgroundSymbol ORGANIZATION,PERSON,LOCATION,MISC,O
</code></pre>

<p>And then each rule doesn't have to have a list.</p>
",2,1,305,2017-03-30 21:49:08,https://stackoverflow.com/questions/43128964/how-to-overwrite-a-tag-for-a-named-entity-with-corenlps-regexner-without-specif
How can I obtain NP and VP subtrees in Stanford parser using a Spanish model,"<p>Actually I work in triplets extraction from Spanish text using Java. I need extract those triplets of the form <code>NP-VP-NP</code>. I'm using Stanford Parser CoreNLP v 3.7.0 and Spanish model v 3.7.0 too. My questions is next, Is there a way to extract NP subtrees and VP subtrees from a sentence in the spanish model? I realize that Spanish parser tree form is diferent from english form. </p>

<p>Ex:</p>

<p><code>(ROOT (sentence (sn (spec (da0000 El)) (grup.nom (nc0s000 reino))) (grup.verb (vmm0000 canta) (sadv (spec (rg muy)) (grup.adv (rg bien))) (fp .)))</code></p>
","java, stanford-nlp","<p>You should use the main distribution to make sure you have everything and download the Spanish models </p>

<p>(available here: <a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/download.html</a>)</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.tregex.*;
import edu.stanford.nlp.util.*;

import java.util.*;


public class TregexExample {

  public static void main(String[] args) {
    // set up pipeline
    Properties props = StringUtils.argsToProperties(""-props"", ""StanfordCoreNLP-spanish.properties"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // Spanish example
    Annotation spanishDoc = new Annotation(""...insert Spanish text..."");
    pipeline.annotate(spanishDoc);
    // get first sentence
    CoreMap firstSentence = spanishDoc.get(CoreAnnotations.SentencesAnnotation.class).get(0);
    Tree firstSentenceTree = firstSentence.get(TreeCoreAnnotations.TreeAnnotation.class);
    // use Tregex to match
    String nounPhrasePattern = ""/grup\\.nom/"";
    TregexPattern nounPhraseTregexPattern = TregexPattern.compile(nounPhrasePattern);
    TregexMatcher nounPhraseTregexMatcher = nounPhraseTregexPattern.matcher(firstSentenceTree);
    while (nounPhraseTregexMatcher.find()) {
      nounPhraseTregexMatcher.getMatch().pennPrint();
    }
  }
}
</code></pre>
",1,0,324,2017-04-02 03:13:57,https://stackoverflow.com/questions/43164239/how-can-i-obtain-np-and-vp-subtrees-in-stanford-parser-using-a-spanish-model
How to process tree that i got from syntaxnet?(conll format),"<p>I guess that i need Semgrex from edu.stanford.nlp package. For this task i need to construct Tree from edu.stanford.nlp.trees.Tree and process that tree like </p>

<pre><code>import edu.stanford.nlp.semgraph.semgrex.SemgrexMatcher;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.semgraph.SemanticGraphFactory;

public class SemgrexDemo  {
    public static void main(String[] args) {
        Tree someHowBuiltTree;//idnt know how to construct Tree from conll
        SemanticGraph graph = SemanticGraphFactory.generateUncollapsedDependencies(someHowBuiltTree);
        SemgrexPattern semgrex = SemgrexPattern.compile(""{}=A &lt;&lt;nsubj {}=B"");
        SemgrexMatcher matcher = semgrex.matcher(graph);
    }
}
</code></pre>

<p>Actually i need some suggestions about how to constract tree from conll.</p>
","nlp, stanford-nlp, syntaxnet","<p>You want to load a <code>SemanticGraph</code> from your CoNLL file.</p>

<pre><code>import edu.stanford.nlp.trees.ud.ConLLUDocumentReader;
...

CoNLLUDocumentReader reader = new CoNLLUDocumentReader();
Iterator&lt;SemanticGraph&gt; it = reader.getIterator(IOUtils.readerFromString(conlluFile));
</code></pre>

<p>This will produce an <code>Iterator</code> that will give you a <code>SemanticGraph</code> for each sentence in your file.</p>

<p>It is an open research problem to generate a constituency tree from a dependency parse, so there is no way in Stanford CoreNLP to do that at this time to the best of my knowledge.</p>
",2,3,324,2017-04-04 14:40:58,https://stackoverflow.com/questions/43210411/how-to-process-tree-that-i-got-from-syntaxnetconll-format
StandPOSTagger in Python &quot;Could not find or load main class&quot;,"<p>I recently try to learn nltk package through <a href=""http://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python"" rel=""nofollow noreferrer"">http://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python</a>.
But I faced a question about performing JAVA code in Python:</p>

<pre><code>import os
java_path = ""C:\Program Files (x86)\Java\jre1.8.0_121\\bin\java.exe""
os.environ['JAVAHOME'] = java_path
os.environ['JAVAHOME']
</code></pre>

<p>It turned out:</p>

<pre><code>'C:\\Program Files (x86)\\Java\\jre1.8.0_121\\bin\\java.exe'
</code></pre>

<p>Then I run nltk code:</p>

<pre><code>import nltk
from nltk.tag.stanford import StanfordPOSTagger
english_postagger=StanfordPOSTagger('models/english-bidirectional-distsim.tagger','stanford-postagger.jar')
english_postagger.tag('hi')
</code></pre>

<p>However:</p>

<pre><code>`Error: Could not find or load main class`edu.stanford.nlp.tagger.maxent.MaxentTagger
</code></pre>

<p>I reviewed the documents in 'stanford-postagger.jar', the MaxentTagger file was there:
<a href=""https://i.sstatic.net/uAmHy.jpg"" rel=""nofollow noreferrer"">path to Maxent Tagger</a></p>

<p>May I know how I could set right class path? or other way to solve this problem.
P.S. : I don't have experience in Java, but Python. </p>
","java, python, stanford-nlp, text-mining","<p>The issue is you don't have access to the jars, so this is a CLASSPATH issue.  I'm not positive this will work with <code>nltk</code>, but I've seen previous answers where setting <code>os.environ[""CLASSPATH""]= ""/path/to/stanford-corenlp-full-2016-10-31""</code> solves this.</p>

<p>You can download Stanford CoreNLP 3.7.0 from here: </p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>If you want to use our tools in Python, I would recommend using the Stanford CoreNLP 3.7.0 server and making small server requests (or using the <code>stanza</code> library).</p>

<p>If you use <code>nltk</code> what I believe happens is Python just calls our Java code with <code>subprocess</code> and this can actually be very inefficient since distinct calls reload all of the models.</p>

<p>Here is a previous answer I gave which describes this more thoroughly:</p>

<p><a href=""https://stackoverflow.com/questions/42896027/cannot-use-pycorenlp-for-python3-5-through-terminal/42915635#42915635"">cannot use pycorenlp for python3.5 through terminal</a></p>
",0,0,1329,2017-04-04 15:04:55,https://stackoverflow.com/questions/43210972/standpostagger-in-python-could-not-find-or-load-main-class
Stanford dependency parser training data format,"<p>I would like to add a new language to the Stanford Dependency Parser, but cannot for the life of me figure out how.</p>

<p>In what format should training data be?
How do I generate new language files?</p>
","stanford-nlp, training-data, dependency-parsing","<p>The neural net dependency parser takes in CoNLL-X format data.</p>

<p>There is a description of the format in this paper:</p>

<p><a href=""https://ilk.uvt.nl/~emarsi/download/pubs/14964.pdf"" rel=""nofollow noreferrer"">https://ilk.uvt.nl/~emarsi/download/pubs/14964.pdf</a></p>
",0,0,227,2017-04-05 05:06:53,https://stackoverflow.com/questions/43222250/stanford-dependency-parser-training-data-format
Clause Segmentation using Stanford OpenIE,"<p>I'm in a search of a good tool for segmenting complex sentences into clauses. Since I use CoreNLP tools for parsing, I got to know that OpenIE deals with clause segmentation in the process of extracting the relation triples from a sentence. Currently, I use the sample code provided in the OpenIEDemo class from the github <a href=""https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/naturalli"" rel=""nofollow noreferrer"">repository</a> but it doesn't properly segment the sentence into clauses.
Here is the code: </p>

<pre><code>// Create the Stanford CoreNLP pipeline
Properties props = PropertiesUtils.asProperties(
        ""annotators"", ""tokenize,ssplit,pos,lemma,parse,natlog,openie"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
//Annotate sample sentence
text = ""I don't think he will be able to handle this."";

Annotation doc = new Annotation(text);
pipeline.annotate(doc);

// Loop over sentences in the document
int sentNo = 0;
for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      List&lt;SentenceFragment&gt; clauses = new OpenIE(props).clausesInSentence(sentence);
  for (SentenceFragment clause : clauses) {
    System.out.println(""Clause: ""+clause.toString());
  }
}
</code></pre>

<p>I expect the get as output three clauses:</p>

<ul>
<li>I don't think</li>
<li>he will be able</li>
<li>to handle this</li>
</ul>

<p>instead, the code returns the exact same input: </p>

<ul>
<li>I do n't think he will be able to handle this</li>
</ul>

<p>However, the sentence </p>

<blockquote>
  <p>Obama is born in Hawaii and he is no longer our president.</p>
</blockquote>

<p>gets two clauses:</p>

<ul>
<li>Obama is born in Hawaii and he is no longer our president</li>
<li>he is no longer our president</li>
</ul>

<p>(seems that the coordinating conjunction is a good segmentation indicator)</p>

<p>Is OpenIE generally used for clause segmentation and if so, how to do it properly? </p>

<p>Any other practical approaches/tools on clause segmentation are welcome. Thanks in advance.</p>
","java, nlp, stanford-nlp","<p>So, the clause segmenter is a bit more tightly integrated with OpenIE than the name would imply. The goal of the module is to produce logically entailed clauses, which can then be shortened into logically entailed sentence fragments. Going through your two examples:</p>

<ol>
<li><blockquote>
  <p>I don't think he will be able to handle this.</p>
</blockquote>

<p>None of the three clauses are I think entailed from the original sentence:</p>

<ul>
<li>""I don't think"" -- you likely still ""think,"" even if you don't think something is true.</li>
<li>""He will be able"" -- If you ""think the world is flat,"" it doesn't mean that the world is flat. Similarly, if you ""think he'll be able"" it doesn't mean he'll be able.</li>
<li>""to handle this"" -- I'm not sure this is a clause... I'd group this with ""He will be able to handle this,"" with ""able to"" being treated as a single verb.</li>
</ul></li>
<li><blockquote>
  <p>Obama is born in Hawaii and he is no longer our president.</p>
</blockquote>

<p>Naturally the two clauses should be ""Obama was born in Hawaii"" and ""He is no longer our president."" Nonetheless, the clause splitter outputs the original sentence in place of the first clause, in expectation that the next step of the OpenIE extractor will strip off the ""conj:and"" edge.</p></li>
</ol>
",1,1,910,2017-04-07 13:11:55,https://stackoverflow.com/questions/43279085/clause-segmentation-using-stanford-openie
Starting Stanford CoreNLP server - Java heap size error,"<p>I am trying to start Stanford CoreNLP server the <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started"" rel=""nofollow noreferrer"">link</a></p>

<p>The following command:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
</code></pre>

<p>generates error message:</p>

<pre><code>Invalid maximum heap size: -Xmx4g
The specified size exceeds the maximum representable size.
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
</code></pre>

<p>Just in case, here is my java version: </p>

<pre><code>java version ""1.8.0_31""
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) Client VM (build 25.31-b07, mixed mode)
</code></pre>

<p>Any suggestions?</p>
","java, server, stanford-nlp","<p>This is probably an issue with you running in 32-bit mode instead of 64-bit.</p>

<p>Try adding <code>-d64</code> to your command.</p>

<p>This thread goes into more detail about this kind of error:</p>

<p><a href=""https://stackoverflow.com/questions/909018/avoiding-initial-memory-heap-size-error"">Avoiding Initial Memory Heap Size Error</a></p>
",1,1,762,2017-04-09 03:34:43,https://stackoverflow.com/questions/43302749/starting-stanford-corenlp-server-java-heap-size-error
Extracting relationships out of text,"<p>That is one example text:</p>

<blockquote>
  <p>Last year Jaap van der Meer, TAUS’s founder and director, wrote a provocative blogpost entitled “The Future Does Not Need Translators”, arguing that the quality of MT will keep improving, and that for many applications less-than-perfect translation will be good enough.</p>
</blockquote>

<p>Now I'd like to learn that:</p>

<pre><code>PERSON(Jaap van der Meer) is JOB(founder and director) of ORGANISATION (TAUS)
</code></pre>

<p>What I get from Standford OpenIE and OpenIE is something like:</p>

<pre><code>TAUS    has     founder
Jaap van der Meer       wrote   blogpost
</code></pre>

<p>In general, I'd like to extract the job positions / business relationships between organisations and organisations or people and organisation out of free text like news articles etc.</p>

<p>How can I get this working with Stanford OpenIE?
Is there a better tool to do that?</p>
",stanford-nlp,"<p>You should try using the KBPAnnotator.  This will extract (person, title, job title) relations and (person, organization, works_for) relations among others.</p>

<p>Example command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,regexner,parse,mention,entitymentions,coref,kbp -file example.txt -outputFormat text
</code></pre>
",3,0,135,2017-04-09 08:19:15,https://stackoverflow.com/questions/43304578/extracting-relationships-out-of-text
"Python used to run Java JAR, ClassNotFoundException","<p>I'm using Pythons' <code>subprocess</code> module to run a command.</p>

<p>The command is used to run <strong>java JAR file</strong>.</p>

<p>When I run it via <strong>terminal</strong>, it runs fine producing the desired output.</p>

<p>The <strong>JAVA</strong> command via terminal:</p>

<p><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref,depparse -file input/input.txt</code></p>

<p>I've written a simple Python script to run the same command.</p>

<pre><code>from subprocess import call
def main():
    call(['java', '-cp', '\""*\""','-Xmx2g','edu.stanford.nlp.pipeline.StanfordCoreNLP','-annotators','tokenize,pos,lemma,ner,parse,dcoref,depparse', '-file', 'input/input.txt'])
</code></pre>

<p>Terminal Command (from the same folder where I ran the JAVA Command):</p>

<pre><code>python script.py
</code></pre>

<p>Output here is:</p>

<pre><code>Error: Could not find or load main class     edu.stanford.nlp.pipeline.StanfordCoreNLP
</code></pre>

<p>I'm missing as to what is not same when I run it from python or run it from the terminal?<br>
What is present in the terminals environment that pythons <code>call()</code> misses out?</p>

<p>Any insight or direction would kick start my project!</p>
","java, python, jar, stanford-nlp, classnotfoundexception","<p>When typing <code>""*""</code> in your terminal, it just tells Linux not to expand <code>*</code> to ""all the contents of the current folder"", and just does nothing particular on Windows (double quotes are useful to protect against spaces). In both cases, <code>*</code> is passed to the java command line.</p>

<p>But when passed to <code>subprocess</code> as a <code>list</code> (it's different when passing a string, and I don't recommend doing that), <code>\""*\""</code> (no need for backslashes BTW) is passed to java _literally as <code>""*""</code> so it's not the same as passing <code>*</code></p>

<p>So you have to just change that argument to <code>*</code> like this:</p>

<pre><code>call(['java', '-cp', '*','-Xmx2g', ...
</code></pre>
",3,4,903,2017-04-10 19:29:31,https://stackoverflow.com/questions/43331514/python-used-to-run-java-jar-classnotfoundexception
How can I express an OR relationship between 2 different lemma-tag pairs of a node in Semgrex?,"<p>I am trying to extract a node that should either be the verb <strong>live</strong> or the noun <strong>life</strong> using Semgrex. I have tried the following but got a SemgrexParseException for each one:</p>

<pre><code>{lemma: live; pos: /VB.*/} | {lemma: life; pos: /NN.*/} 

{lemma: live; pos: /VB.*/ | lemma: life; pos: /NN.*/} 

({lemma: live; pos: /VB.*/}) | ({lemma: life; pos: /NN.*/})
</code></pre>

<p>What is the correct Semgrex expression for this case?</p>
",stanford-nlp,"<p>Answering my own question. You need to surround it with square brackets like so:</p>

<pre><code>[{lemma: live; pos: /VB.*/} | {lemma: life; pos: /NN.*/}]
</code></pre>
",0,0,72,2017-04-11 04:26:48,https://stackoverflow.com/questions/43336765/how-can-i-express-an-or-relationship-between-2-different-lemma-tag-pairs-of-a-no
openNLP java - multi term Portuguese NER,"<p>I'm using openNLP API at java for a project that I'm working on. The thing is that with my program i only process words alone with no correspondence.
The code:</p>

<pre><code>String line = input.nextLine();


          InputStream inputStreamTokenizer = new FileInputStream(""/home/bruno/openNLP/apache-opennlp-1.7.2-src/models/pt-token.bin""); 
          TokenizerModel tokenModel = new TokenizerModel(inputStreamTokenizer); 

          //Instantiating the TokenizerME class 
          TokenizerME tokenizer = new TokenizerME(tokenModel); 
          String tokens[] = tokenizer.tokenize(line);


          InputStream inputStream = new FileInputStream(""/home/bruno/openNLP/apache-opennlp-1.7.2-src/models/pt-sent.bin""); 
          SentenceModel model = new SentenceModel(inputStream); 

          //Instantiating the SentenceDetectorME class 
          SentenceDetectorME detector = new SentenceDetectorME(model);  

          //Detecting the sentence
          String sentences[] = detector.sentDetect(line); 

          //Loading the NER-location model 
          //InputStream inputStreamLocFinder = new FileInputStream(""/home/bruno/openNLP/apache-opennlp-1.7.2-src/models/en-ner-location.bin"");       
          //TokenNameFinderModel model = new TokenNameFinderModel(inputStreamLocFinder);

          //Loading the NER-person model 
          InputStream inputStreamNameFinder = new FileInputStream(""/home/bruno/TryOllie/data/pt-ner-floresta.bin"");       
          TokenNameFinderModel model2 = new TokenNameFinderModel(inputStreamNameFinder);

          //Instantiating the NameFinderME class 
          NameFinderME nameFinder2 = new NameFinderME(model2);

          //Finding the names of a location 
          Span nameSpans2[] = nameFinder2.find(tokens);

          //Printing the spans of the locations in the sentence 
          //for(Span s: nameSpans)        
             //System.out.println(s.toString()+""  ""+tokens[s.getStart()]);

          Set&lt;String&gt; x = new HashSet&lt;String&gt;();
          x.add(""event"");
          x.add(""artprod"");
          x.add(""place"");
          x.add(""organization"");
          x.add(""person"");
          x.add(""numeric"");

          SimpleTokenizer simpleTokenizer = SimpleTokenizer.INSTANCE;  
          Span[] tokenz = simpleTokenizer.tokenizePos(line);
          Set&lt;String&gt; tk = new HashSet&lt;String&gt;();
          for( Span tok : tokenz){
              tk.add(line.substring(tok.getStart(), tok.getEnd()));
          }

          for(Span n: nameSpans2)
          {
              if(x.contains(n.getType()))
                  System.out.println(n.toString()+ "" -&gt; "" + tokens[n.getStart()]);

          }
</code></pre>

<p>The  output i get is:</p>

<pre><code>Ficheiro com extensao: file.txt
[1..2) event -&gt; choque[3..4) event -&gt; cadeia[6..7) artprod -&gt; viaturas[13..14) event -&gt; feira[16..18) place -&gt; Avenida[20..21) place -&gt; Porto[24..25) event -&gt; incêndio[2..3) event -&gt; acidente[5..6) artprod -&gt; viaturas[44..45) organization -&gt; JN[46..47) person -&gt; António[47..48) place -&gt; Campos[54..60) organization -&gt; Batalhão[1..2) event -&gt; acidente[6..8) numeric -&gt; 9[11..12) place -&gt; Porto-Matosinhos[21..22) event -&gt; ocorrência[29..30) artprod -&gt; .[4..5) organization -&gt; Sapadores[7..10) organization -&gt; Bombeiros[14..15) numeric -&gt; 15
</code></pre>

<p>What im trying to do is a multi term NER, like Antonio Campos is a person, not Person -> Antonio and Place -> Campos, or Organisation -> Universidade Nova de Lisboa </p>
","java, stanford-nlp","<p>Your are printing the wrong data structure. The span getSart and getEnd will point to the sequence of tokens that are part of the entity. You are printing just the first token.</p>

<p>Also, you are doing tokenization before sentence detection.</p>

<p>Try the following code:</p>

<pre><code>// load the models outside your loop
InputStream inputStream =
    new FileInputStream(""/home/bruno/openNLP/apache-opennlp-1.7.2-src/models/pt-sent.bin"");
SentenceModel model = new SentenceModel(inputStream);

//Instantiating the SentenceDetectorME class 
SentenceDetectorME detector = new SentenceDetectorME(model);

InputStream inputStreamTokenizer =
    new FileInputStream(""/home/bruno/openNLP/apache-opennlp-1.7.2-src/models/pt-token.bin"");
TokenizerModel tokenModel = new TokenizerModel(inputStreamTokenizer);
//Instantiating the TokenizerME class 
TokenizerME tokenizer = new TokenizerME(tokenModel);


//Loading the NER-person model 
InputStream inputStreamNameFinder = new FileInputStream(""/home/bruno/TryOllie/data/pt-ner-floresta.bin"");
TokenNameFinderModel model2 = new TokenNameFinderModel(inputStreamNameFinder);

//Instantiating the NameFinderME class 
NameFinderME nameFinder2 = new NameFinderME(model2);

String line = input.nextLine();

while(line != null) {

  // first we find sentences
  String sentences[] = detector.sentDetect(line);

  for (String sentence :
      sentences) {
    // now we find the sentence tokens
    String tokens[] = tokenizer.tokenize(sentence);

    // now we are good to apply NER
    Span[] nameSpans = nameFinder2.find(tokens);

    // now we can print the spans
    System.out.println(Arrays.toString(Span.spansToStrings(nameSpans, tokens)));

    line = input.nextLine();
  }
}
</code></pre>
",2,2,1154,2017-04-12 11:21:03,https://stackoverflow.com/questions/43368131/opennlp-java-multi-term-portuguese-ner
StanfordNLP: incompatible types: Object cannot be converted to CoreMap,"<p>I am trying Named Entity Recognition using StanfordNLP using <a href=""http://humphreysheil.blogspot.in/2014/10/named-entity-recognition-short-tutorial.html"" rel=""nofollow noreferrer"">this tutorial</a>. I am getting error</p>

<p><code>incompatible types: Object cannot be converted to CoreMap
</code>
I tried type casting it to <code>Object</code> but could not get it working.</p>

<p><strong>Partial code throwing error</strong></p>

<pre><code>  // these are all the sentences in this document
  // a CoreMap is essentially a Map that uses class objects as keys and has values with
  // custom types
  List sentences = document.get(SentencesAnnotation.class);
  StringBuilder sb = new StringBuilder();
for (CoreMap sentence : sentences) {
    // traversing the words in the current sentence, ""O"" is a sensible default to initialise
    // tokens to since we're not interested in unclassified / unknown things..
    String prevNeToken = ""O"";
    String currNeToken = ""O"";
    boolean newToken = true;
    for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
      currNeToken = token.get(NamedEntityTagAnnotation.class);
      String word = token.get(TextAnnotation.class);
      // Strip out ""O""s completely, makes code below easier to understand
      if (currNeToken.equals(""O"")) {
        // LOG.debug(""Skipping '{}' classified as {}"", word, currNeToken);
        if (!prevNeToken.equals(""O"") &amp;&amp; (sb.length() &gt; 0)) {
          handleEntity(prevNeToken, sb, tokens);
          newToken = true;
        }
        continue;
      }

      if (newToken) {
        prevNeToken = currNeToken;
        newToken = false;
        sb.append(word);
        continue;
      }

      if (currNeToken.equals(prevNeToken)) {
        sb.append("" "" + word);
      } else {
        // We're done with the current entity - print it out and reset
        // TODO save this token into an appropriate ADT to return for useful processing..
        handleEntity(prevNeToken, sb, tokens);
        newToken = true;
      }
      prevNeToken = currNeToken;
    }
  }
</code></pre>

<p>I am a noob with NLP.
Thanks in advance.</p>
","java, stanford-nlp","<p>I managed to solve this issue.
Change third line to this</p>

<pre><code>List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
</code></pre>
",1,0,149,2017-04-12 12:10:15,https://stackoverflow.com/questions/43369161/stanfordnlp-incompatible-types-object-cannot-be-converted-to-coremap
Create Collocation with a specific corpus,"<p>I am working on a project and I am trying to do some research.  I want to be able to find the collocation of words, using a specific corpus library.  I was hoping there was some SDK I could use to do this type of work.  I looked at this option:</p>

<p><a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/CollocationFinder.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/CollocationFinder.html</a></p>

<p>and found a class called CollocationFinder but cannot find much documentation or know if there is a way to specify a corpus.  </p>

<p>I also found a project called JXtract (<a href=""http://definingterms.com/projects/Champollion/#smadja93"" rel=""nofollow noreferrer"">http://definingterms.com/projects/Champollion/#smadja93</a>) but it seems to be quite old and the author states that it is sure to have bugs.</p>

<p>Is anyone aware of a good library I could use to develop software in?  Something that will allow for selecting a corpus, as well as maybe selecting different methods for determining a collocation?</p>
","nlp, stanford-nlp, opennlp","<p>As you did not specify that it must be a Java solution, there's <a href=""http://www.nltk.org/_modules/nltk/app/collocations_app.html"" rel=""nofollow noreferrer"">a collocation</a> (<a href=""http://www.nltk.org/_modules/nltk/app/concordance_app.html"" rel=""nofollow noreferrer"">and a concordancer</a>) <a href=""http://www.nltk.org/_modules/nltk/app/collocations_app.html"" rel=""nofollow noreferrer"">app built using NLTK (Python)</a>. It meets your requirements (a good library [for NLP], allow[s] for selecting a corpus, and you can naturally code any different method for determining a collocation - the TK in NLTK stands for ""tool-kit""!)</p>
",0,0,420,2017-04-13 02:38:11,https://stackoverflow.com/questions/43382640/create-collocation-with-a-specific-corpus
Identifying dates of the form \d\d-\d\d-\d\d using regexner,"<p>I am using Stanford regexNer alongwith ner in a pipeline. I want to identify strings of the form [0-9][0-9]-[0-9][0-9]-[0-9][0-9] (e.g., 27-02-16) as date, which ner identifies as a NUMBER. So, I defined a regex in a mapping file and gave it to regexner. But regexNer is not able to identify such strings as dates. The ner for these tokens is still NUMBER.
Following is the mapping file:</p>

<pre><code>[0-9]{2}-[0-9]{2}-[0-9]{2}  date    NUMBER
</code></pre>

<p>I ensured that the columns are tab-separated. I tried several versions of this regex like \d\d-\d\d-\d\d and [0-9][0-9]-[0-9][0-9]-[0-9][0-9], but none of them worked. Any pointers on where I can be wrong? I am using Stanford CoreNLP 3.7. Here the java code I am running.</p>

<pre><code>Properties PROPS = new Properties();

PROPS.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner"");
        StanfordCoreNLP PIPELINE = new StanfordCoreNLP(PROPS);
        PIPELINE.addAnnotator(
                new RegexNERAnnotator(""/home/jyoti/workspace-jee/QA_Rest/src/main/resources/Gazetter.txt""));
</code></pre>

<p>I further investigated and found that the regex is not matching any string only if it consists wholly of integers. I tried prefixing it with alphabet and it worked (i.e., a\d\d-\d\d-\d\d matched a14-07-12).</p>
",stanford-nlp,"<p>How are you running this, because your original rule works fine for me.</p>

<p>I issued this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,regexner -regexner.mapping date-rules.txt -file date-example.txt -outputFormat text
</code></pre>
",0,0,204,2017-04-14 11:20:55,https://stackoverflow.com/questions/43410522/identifying-dates-of-the-form-d-d-d-d-d-d-using-regexner
Quote Annotator get author,"<p>In the following text:</p>

<blockquote>
  <p>John said, ""There's an elephant outside the window.""</p>
</blockquote>

<p>Is there a simple way to figure out that the quote ""There's an elephant outside the window."" belongs to John?</p>
",stanford-nlp,"<p>We've just added a module for handling this.</p>

<p>You'll need to get the latest code from GitHub.</p>

<p>Here is some sample code:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.coref.*;
import edu.stanford.nlp.coref.data.*;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.util.*;

import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class QuoteAttributionExample {

  public static void main(String[] args) {
    Annotation document =
        new Annotation(""John said, \""There's an elephant outside the window.\"""");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions,quote,quoteattribution"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    for (CoreMap quote : document.get(CoreAnnotations.QuotationsAnnotation.class)) {
      System.out.println(quote);
      System.out.println(quote.get(QuoteAttributionAnnotator.MentionAnnotation.class));
    }
  }
}
</code></pre>

<p>This is still under development, we will probably add some code to make it easier to get the actual text span that links to the quote soon.</p>
",2,1,216,2017-04-14 12:55:10,https://stackoverflow.com/questions/43411968/quote-annotator-get-author
How to escape characters in the RegexNER mapping file in CoreNLP?,"<p>I have the following line in my text:</p>
<blockquote>
<p>Mutation in the deafness (mitochondrial) modifier 2 gene</p>
</blockquote>
<p>And I've tried the following lines in my RegexNER mapping file to tag <strong>deafness (mitochondrial) modifier 2</strong> as GENE but they've both failed</p>
<blockquote>
<p>deafness (mitochondrial) modifier 2     GENE</p>
<p>deafness \(mitochondrial\) modifier 2     GENE</p>
</blockquote>
<p>Seems that the problem is escaping the parenthesis characters, because it matches when I remove the parentheses from the text and the mapping file. What is the correct way of escaping characters in the RegexNER mapping file?</p>
",stanford-nlp,"<p>The parentheses get transformed by the tokenizer into:</p>

<p><code>-LRB-</code> and <code>-RRB-</code></p>

<p>so you want: <code>-LRB- mitochondrial -RRB-</code> to match <code>(mitochondrial)</code></p>

<p>Note also that the tokenizer creates a token for each of the parentheses.</p>
",2,1,216,2017-04-15 17:39:27,https://stackoverflow.com/questions/43429229/how-to-escape-characters-in-the-regexner-mapping-file-in-corenlp
Get original sentences from CoreNLP,"<p>I'm going through my data and want to split it into sentences. I'm using pycorenlp.</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')
output = nlp.annotate(text, properties={
    'annotators': 'tokenize,ssplit',
    'outputFormat': 'json'
})
for tempsentence in output['sentences']:
     # store important sentences ...
</code></pre>

<p>Now I store some sentences which are important for my application.
Some of those contain "" or ' and it seems that CoreNLP changes those sentences. "" are converted into -LRB- and -RRB- if I remember that correctly.</p>

<p>Is it possible that I can get the orignial sentence from CoreNLP (since I need to do another CoreNLP run later on and if "" is now gone, my data doesn't look orgininal and the 2nd CoreNLP run doesn't seem to recognise some quotiations anymore.</p>
","nlp, stanford-nlp","<ol>
<li><p>Download and install the stanza library: <a href=""https://github.com/stanfordnlp/stanza"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/stanza</a></p></li>
<li><p>The returned result will have the original tokens.</p></li>
</ol>

<p>example:</p>

<pre><code>from stanza.nlp.corenlp import CoreNLPClient
client = CoreNLPClient(server='http://localhost:9000', default_annotators=['ssplit', 'tokenize'])
result = client.annotate(""..."")
for sentence in result.sentences:
  for token in sentence.tokens:
    print token.word + ""\t"" + token.originalText
</code></pre>
",2,1,644,2017-04-16 14:59:35,https://stackoverflow.com/questions/43438548/get-original-sentences-from-corenlp
When does CRFClassifier start showing results different from text matching?,"<p>I'm very new to NLP and just learned about CRFClassifiers for Named Entity Recognition. I followed this tutorial to train my classifier <a href=""https://nlp.stanford.edu/software/crf-faq.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/crf-faq.html</a></p>

<p>At this point, it ""looks"" no different than if I had a giant set of named entities and for each word in the test file, I could check if it exists in the giant set.</p>

<p>I'm trying to develop an intuition about when and how the results of this classifier will start to look different than that. Under what circumstances and in what way are CRFClassifiers better?</p>
",stanford-nlp,"<p>I can think of 2 examples off of the top of my head:</p>

<ol>
<li>Ambiguous term:</li>
</ol>

<p><code>The actor Jeremy London was cast in the show.</code></p>

<ol start=""2"">
<li>Terms not in your dictionary:</li>
</ol>

<p><code>She has worked for Xytrocorp for the last 3 years.</code></p>

<p><code>They had a meeting with Joe Rarelastname last week.</code></p>

<p>I would say especially the second is a major reason you'd want a CRFClassifier.</p>
",0,0,59,2017-04-19 00:39:02,https://stackoverflow.com/questions/43484362/when-does-crfclassifier-start-showing-results-different-from-text-matching
StanfordNLP : ArrayIndexOutOfBoundsException for Named Entity Recognition,"<p>I am trying to learn NER using <a href=""https://humphreysheil.blogspot.in/2014/10/named-entity-recognition-short-tutorial.html"" rel=""nofollow noreferrer"">this</a> short Named Entity Recognition tutorial. But I am unable to run the code successfully. I provided one entry in location.txt file as mentioned there.</p>

<p>I am getting <code>ArrayIndexOutOfBoundsException</code> error.</p>

<pre><code>09:32:09.431 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator regexner

java.lang.ArrayIndexOutOfBoundsException: 1

at  edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.readEntries(TokensRegexNERAnnotator.java:696)
at edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.readEntries(TokensRegexNERAnnotator.java:593)
at edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.&lt;init&gt;(TokensRegexNERAnnotator.java:294)
at edu.stanford.nlp.pipeline.AnnotatorImplementations.tokensRegexNER(AnnotatorImplementations.java:135)
at edu.stanford.nlp.pipeline.AnnotatorFactories$7.create(AnnotatorFactories.java:305)
at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:154)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:137)
</code></pre>

<p>Kindly help me.
Thanks in advance.</p>
","java, nlp, stanford-nlp","<p>Here is an example rexegner rule:</p>

<pre><code>London    LOCATION    MISC    1
</code></pre>

<p>Make sure the 4 columns are separated by a ""\t"" character not spaces.</p>
",1,0,102,2017-04-20 04:23:06,https://stackoverflow.com/questions/43510163/stanfordnlp-arrayindexoutofboundsexception-for-named-entity-recognition
Stanford NER not tagging date and time,"<p>I am using Stanford NER tagger in python. It is not tagging dates and time. Rather returns O on every word.
My sentence was:</p>

<p>""What sum of money will earn an interest of $ 162 in 3 years at the rate of 12% per annum""</p>

<p>The result I got after tagging was-</p>

<pre><code>[('What', 'O'), ('sum', 'O'), ('of', 'O'), ('money', 'O'), ('will', 'O'), ('earn', 'O'), ('an', 'O'), ('interest', 'O'), ('of', 'O'), ('$', 'O'), ('162', 'O'), ('in', 'O'), ('3', 'O'), ('years', 'O'), ('at', 'O'), ('the', 'O'), ('rate', 'O'), ('of', 'O'), ('12%', 'O'), ('per', 'O'), ('annum', 'O')]
</code></pre>

<p>How to fix this?</p>
","python, stanford-nlp, named-entity-recognition","<ol>
<li><p>Download and install Stanford NLP Group's Python library <code>stanza</code> .</p>

<p>GitHub: <a href=""https://github.com/stanfordnlp/stanza"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/stanza</a></p></li>
<li><p>With Stanford CoreNLP 3.7.0, start a server:</p>

<p>command: <code>java -Xmx4g edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000</code></p>

<p>Stanford CoreNLP 3.7.0: <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>(Note: make sure CLASSPATH contains all of the jars in the download folder)</p></li>
<li><p>Issue a request to the Java Stanford CoreNLP server started in Step 2:</p>

<pre><code>from stanza.nlp.corenlp import CoreNLPClient

client = CoreNLPClient(server='http://localhost:9000', default_annotators=['ssplit', 'tokenize', 'lemma', 'pos', 'ner'])

annotated = client.annotate(""..text to annotate..."")

for sentence in annotated.sentences:
  print ""---""
  print sentence.tokens
  print sentence.ner_tags
</code></pre>

<p>We are working on having the Python library handle starting and stopping the server for Stanford CoreNLP 3.8.0.</p></li>
</ol>
",3,1,1262,2017-04-21 03:55:25,https://stackoverflow.com/questions/43533701/stanford-ner-not-tagging-date-and-time
Can Stanford Parser ignore case?,"<p>I've been playing with <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">http://corenlp.run/</a> and noticed that it is case sensitive. 
For example, it tags ""i"" as FW versus ""I"" as PRP.
Can I train it to ignore case? More generally, how do I go about training it for non-well formed sentences?</p>
","stanford-nlp, stanford-parser","<p>CorenNLP has case insensitive models that you can use for English only. They call them <a href=""https://stanfordnlp.github.io/CoreNLP/caseless.html"" rel=""nofollow noreferrer"">caseless models</a>. Read the warning there if you use version 3.6.0</p>

<p>A couple of points mentioned in the link:</p>

<ol>
<li>You can fix the case of letters in your text and use the normal models. You may use <code>TrueCaseAnnotator</code> for that.</li>
<li><p>To train your own case insensitive models with CoreNLP you can specify a token preprocessor that ignores case</p>

<pre><code>wordFunction = edu.stanford.nlp.process.LowercaseFunction
</code></pre></li>
</ol>
",2,1,397,2017-04-21 19:06:26,https://stackoverflow.com/questions/43550228/can-stanford-parser-ignore-case
Stanford NER for phrases or compound entities,"<p>I noticed that corenlp.run can identify ""10am tomorrow"" and parse it out as time. But the training tutorial and the docs I've seen only allow for 1 word per line. How do I get it to understand a phrase. 
On a related note, is there a way to tag compound entities?</p>
","stanford-nlp, stanford-parser","<p>Time related phrases like that are recognized by the SUTime library.  More details can be found here: <a href=""https://nlp.stanford.edu/software/sutime.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/sutime.html</a></p>

<p>There is functionality for extracting entities after the <code>ner</code> tagging has been done.</p>

<p>For instance if you have tagged a sentence: <code>Joe Smith went to Hawaii .</code> as <code>PERSON PERSON O O LOCATION O</code> you can extract out <code>Joe Smith</code> and <code>Hawaii</code>.  This requires the <code>entitymentions</code> annotator.</p>

<p>Here is some example code:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.util.*;

import java.util.*;

public class EntityMentionsExample {

  public static void main(String[] args) {
    Annotation document =
        new Annotation(""John Smith visited Los Angeles on Tuesday."");
    Properties props = new Properties();
    //props.setProperty(""regexner.mapping"", ""small-names.rules"");
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);

    for (CoreMap entityMention : document.get(CoreAnnotations.MentionsAnnotation.class)) {
      System.out.println(entityMention);
      //System.out.println(entityMention.get(CoreAnnotations.TextAnnotation.class));
      System.out.println(entityMention.get(CoreAnnotations.EntityTypeAnnotation.class));
    }
  }
}
</code></pre>
",2,1,518,2017-04-22 00:23:47,https://stackoverflow.com/questions/43553867/stanford-ner-for-phrases-or-compound-entities
"Improving on the basic, existing GloVe model","<p>I am using GloVe as part of my research. I've downloaded the models from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">here</a>. I've been using GloVe for sentence classification. The sentences I'm classifying are specific to a particular domain, say some STEM subject. However, since the existing GloVe models are trained on a general corpus, they may not yield the best results for my particular task. </p>

<p>So my question is, how would I go about loading the retrained model and just retraining it a little more on my own corpus to learn the semantics of my corpus as well? There would be merit in doing this were it possible. </p>
","nlp, text-classification, glove","<p>After a little digging, I found <a href=""https://github.com/stanfordnlp/GloVe/issues/62"" rel=""nofollow noreferrer"">this issue</a> on the git repo. Someone suggested the following:</p>

<blockquote>
  <p>Yeah, this is not going to work well due to the optimization setup. But what you can do is train GloVe vectors on your own corpus and then concatenate those with the pretrained GloVe vectors for use in your end application.</p>
</blockquote>

<p>So that answers that.</p>
",2,7,2345,2017-04-25 18:15:28,https://stackoverflow.com/questions/43618145/improving-on-the-basic-existing-glove-model
Connect to Dedicated Server - CoreNLP,"<p>I am looking for the python code required to connect to a local instantiation of the Stanford CoreNLP Server. </p>

<p>I have successfully connected and communicated with Stanford's server where you need to enter the following into the command line prompt:</p>

<p><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000</code></p>

<p>and have <code>nlp = StanfordCoreNLP('http://localhost:9000')</code> in your python code.</p>

<p>I now have a local CoreNLP server operating on CentOS 6. I use the terminal prompt: <code>sudo service corenlp start</code> and get the system response <code>CoreNLP server started.</code> Which I assume means my local instantiation of the CoreNLP server is listening for any requests. I now need to know how to communicate with my local server.</p>

<p>Can anyone enlighten me on what I need to replace <code>nlp = StanfordCoreNLP('http://localhost:9000')</code> with in order to be able to talk to my local CoreNLP Server?</p>
","python, python-3.x, stanford-nlp, python-3.6, corenlp-server","<p>You should use the Stanford NLP group's Python library <code>stanza</code>:</p>

<p><a href=""https://github.com/stanfordnlp/stanza"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/stanza</a></p>

<p>Hopefully over the next month there will be better documentation and support for starting and stopping the server in Python.</p>
",0,0,1674,2017-04-25 18:56:31,https://stackoverflow.com/questions/43618807/connect-to-dedicated-server-corenlp
Use GET params to provide the Web interface with a specific text to annotate,"<p>I would like to link to my instance of CoreNLP server, with a specified text (and possibly, a specified set of annotators). (i.e. without having to paste the text then click on Submit)</p>

<p>Is there a way to do this?</p>

<p>(I know and use the API version, but I'm looking for the Web visualisation)</p>
","stanford-nlp, corenlp-server","<p>So, answering my own question: this is now possible (I believe from CoreNlp 3.8), after a successfully merged pull request from yours truly.</p>

<p>This is the relevant pull request: <a href=""https://github.com/stanfordnlp/CoreNLP/pull/423"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/pull/423</a></p>
",0,0,63,2017-04-27 16:30:58,https://stackoverflow.com/questions/43663335/use-get-params-to-provide-the-web-interface-with-a-specific-text-to-annotate
Java - Stanford NLP - Process all files in directory,"<p>I am using Stanford to do some NER analysis on txt files. The problem so far is that I have been to read all files in a directory. I have just been able to read simple Strings. What should be the next step to read several files? I tried with Iterator but it did not work. </p>

<p>Please see my code below:</p>

<blockquote>
  <p>Blockquote</p>
</blockquote>

<pre><code>import java.io.*;
import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import edu.stanford.nlp.ie.AbstractSequenceClassifier;
import edu.stanford.nlp.ie.NERClassifierCombiner;
import edu.stanford.nlp.pipeline.SentimentAnnotator;
import edu.stanford.nlp.ie.crf.CRFClassifier;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.process.PTBEscapingProcessor;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.PrintWriter;
import java.io.File;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import org.apache.commons.io.FileUtils;

import com.google.common.io.Files;
import org.apache.commons.io.*;


public class NLPtest2 {

    public static void main(String[] args) throws IOException {

        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse, ner, dcoref, sentiment"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        //how can we read all documents in a directory instead of just a String??

       String text = ""I work at Lalalala Ltd. It is awesome""; 

        Annotation annotation = new Annotation(text);

        pipeline.annotate(annotation);

       // Annotation annotation = pipeline.process(text);
        List&lt;CoreMap&gt; sentences = annotation.get(SentencesAnnotation.class);

        for (CoreMap sentence : sentences) {
            String sentiment = sentence.get(SentimentCoreAnnotations.SentimentClass.class);
            System.out.println(sentiment + ""\t"" + sentence);
           // System.out.println(annotation.get(CoreAnnotations.QuotationsAnnotation.class));// dont need it
                  // traversing the words in the current sentence
                  // a CoreLabel is a CoreMap with additional token-specific methods
                  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
                    // this is the text of the token
                    String word = token.get(TextAnnotation.class);

                    // this is the POS tag of the token
                    String pos = token.get(PartOfSpeechAnnotation.class);
                    // this is the NER label of the token

                    String ne = token.get(NamedEntityTagAnnotation.class); 
                    System.out.println( ""Text:""+ word +""//""+""Part of Speech:""+ pos + ""//""+ ""Entity Recognition:""+ ne);
                  }

            }   
        }
    }
</code></pre>
","java, stanford-nlp","<pre><code>import edu.stanford.nlp.io.*;
import edu.stanford.nlp.util.*;

import java.util.*;

public class ReadFiles {

  public static void main(String[] args) {

    List&lt;String&gt; filePaths = IOUtils.linesFromFile(args[0]);
    for (String filePath : filePaths) {
      String fileContents = IOUtils.stringFromFile(filePath);
    }
  }
}
</code></pre>
",-1,-1,236,2017-04-28 22:42:03,https://stackoverflow.com/questions/43689819/java-stanford-nlp-process-all-files-in-directory
StanfordNLP - ArrayIndexOutOfBoundsException at TokensRegexNERAnnotator.readEntries(TokensRegexNERAnnotator.java:696),"<p>I want to identify following as SKILL using stanfordNLP's TokensRegexNERAnnotator. </p>

<p><code>AREAS OF EXPERTISE
Areas of Knowledge
Computer Skills
Technical Experience
Technical Skills</code></p>

<p>There are many more sequence of text like above. </p>

<p>Code - </p>

<pre><code>    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.addAnnotator(new TokensRegexNERAnnotator(""./mapping/test_degree.rule"", true));
    String[] tests = {""Bachelor of Arts is a good degree."", ""Technical Skill is a must have for Software Developer.""};
    List tokens = new ArrayList&lt;&gt;();

    // traversing each sentence from array of sentence.
    for (String txt : tests) {
         System.out.println(""String is : "" + txt);

         // create an empty Annotation just with the given text
         Annotation document = new Annotation(txt);

         pipeline.annotate(document);
         List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

         /* Next we can go over the annotated sentences and extract the annotated words,
         Using the CoreLabel Object */
      for (CoreMap sentence : sentences) {
         for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
             System.out.println(""annotated coreMap sentences : "" + token);
             // Extracting NER tag for current token
             String ne = token.get(NamedEntityTagAnnotation.class);
             String word = token.get(CoreAnnotations.TextAnnotation.class);
             System.out.println(""Current Word : "" + word + "" POS :"" + token.get(PartOfSpeechAnnotation.class));
             System.out.println(""Lemma : "" + token.get(LemmaAnnotation.class));
             System.out.println(""Named Entity : "" + ne);
    }
  }
</code></pre>

<p>My regex rule file is -</p>

<p>$SKILL_FIRST_KEYWORD = ""/area of/|/areas of/|/technical/|/computer/|/professional/""
$SKILL_KEYWORD  = ""/knowledge/|/skill/|/skills/|/expertise/|/experience/""</p>

<p>tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }</p>

<p>{
     ruleType: ""tokens"",
     pattern: ($SKILL_FIRST_KEYWORD + $SKILL_KEYWORD),
     result: ""SKILL""
}</p>

<p>I am getting <code>ArrayIndexOutOfBoundsException</code> error. I guess there is something wrong with my rule file. Can somebody please point me where am I making mistake?</p>

<p>Desired Output - </p>

<p><strong>AREAS OF EXPERTISE - SKILL</strong></p>

<p><strong>Areas of Knowledge - SKILL</strong></p>

<p><strong>Computer Skills - SKILL</strong></p>

<p>and so on.</p>

<p>Thanks in advance.</p>
","java, nlp, stanford-nlp","<p>You should be using the TokensRegexAnnotator not the TokensRegexNERAnnotator.</p>

<p>You should review these threads for more info:</p>

<p><a href=""https://stackoverflow.com/questions/43447585/tokensregex-rules-to-get-correct-output-for-named-entities/43532621#43532621"">TokensRegex rules to get correct output for Named Entities</a></p>

<p><a href=""https://stackoverflow.com/questions/43521697/getting-output-in-the-desired-format-using-tokenregex"">Getting output in the desired format using TokenRegex</a></p>
",1,0,171,2017-04-29 04:52:03,https://stackoverflow.com/questions/43691901/stanfordnlp-arrayindexoutofboundsexception-at-tokensregexnerannotator-readentr
Tokenizer Training with StanfordNLP,"<p>So my requirement is verbally simple. I need StanfordCoreNLP default models along with my custom trained model, based on custom entities. In a final run, I need to be able to isolate specific phrases from a given sentence (RegexNER will be used)</p>

<p>Following are my efforts :-</p>

<p><strong>EFFORT I :-</strong>
So I wanted to use the StanfordCoreNLP CRF files, tagger files and ner model files, along with my custom trained ner models.
I tried to find if there is any official way of doing this, but didnt get anything. There is a property ""ner.model"" for StanfordCoreNLP pipeline, but it will skip the default ones if used.</p>

<p><strong>EFFORT II :-</strong>
Next (might not be the smartest thing ever. Sorry! Just a guy trying to make ends meet!) , I extracted the model <pre><code>stanford-corenlp-models-3.7.0.jar</code></pre> , and copied all :-</p>

<pre><code>
*.ser.gz (Parser Models)
*.tagger (POS Tagger)
*.crf.ser.gz (NER CRF Files)
</code></pre>

<p>and tried to put Comma Separated Values with properties ""parser.model"", ""pos.model"" and ""ner.model"" respectively, as follows :-</p>

<pre><code>
parser.model=models/ner/default/anaphoricity_model.ser.gz,models/ner/default/anaphoricity_model_conll.ser.gz,models/ner/default/classification_model.ser.gz,models/ner/default/classification_model_conll.ser.gz,models/ner/default/clauseSearcherModel.ser.gz,models/ner/default/clustering_model.ser.gz,models/ner/default/clustering_model_conll.ser.gz,models/ner/default/english-embeddings.ser.gz,models/ner/default/english-model-conll.ser.gz,models/ner/default/english-model-default.ser.gz,models/ner/default/englishFactored.ser.gz,models/ner/default/englishPCFG.caseless.ser.gz,models/ner/default/englishPCFG.ser.gz,models/ner/default/englishRNN.ser.gz,models/ner/default/englishSR.beam.ser.gz,models/ner/default/englishSR.ser.gz,models/ner/default/gender.map.ser.gz,models/ner/default/md-model-dep.ser.gz,models/ner/default/ranking_model.ser.gz,models/ner/default/ranking_model_conll.ser.gz,models/ner/default/sentiment.binary.ser.gz,models/ner/default/sentiment.ser.gz,models/ner/default/truecasing.fast.caseless.qn.ser.gz,models/ner/default/truecasing.fast.qn.ser.gz,models/ner/default/word_counts.ser.gz,models/ner/default/wsjFactored.ser.gz,models/ner/default/wsjPCFG.ser.gz,models/ner/default/wsjRNN.ser.gz
ner.model=models/ner/default/english.all.3class.caseless.distsim.crf.ser.gz,models/ner/default/english.all.3class.distsim.crf.ser.gz,models/ner/default/english.all.3class.nodistsim.crf.ser.gz,models/ner/default/english.conll.4class.caseless.distsim.crf.ser.gz,models/ner/default/english.conll.4class.distsim.crf.ser.gz,models/ner/default/english.conll.4class.nodistsim.crf.ser.gz,models/ner/default/english.muc.7class.caseless.distsim.crf.ser.gz,models/ner/default/english.muc.7class.distsim.crf.ser.gz,models/ner/default/english.muc.7class.nodistsim.crf.ser.gz,models/ner/default/english.nowiki.3class.caseless.distsim.crf.ser.gz,models/ner/default/english.nowiki.3class.nodistsim.crf.ser.gz
pos.model=models/tagger/default/english-left3words-distsim.tagger
</code></pre>

<p>But, I get the following exception :-</p>

<pre><code>
Caused by: edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
Caused by: java.io.StreamCorruptedException: invalid stream header: EFBFBDEF
</code></pre>

<p><strong>EFFORT III :-</strong>
I thought I will be able to handle with RegexNER, and I was successful to some extent. Just that the entities that it learns through RegexNER, it doesn't apply to forthcoming expressions. Eg: It will find the entity ""CUSTOM_ENTITY"" inside a text, but if i put a RegexNER like <pre><code> ( [ {ner:CUSTOM_ENTITY} ] /with/ [ {ner:CUSTOM_ENTITY} ] ) </code></pre> it never succeeds in finding the right phrase.</p>

<p>Really need help here!!! I don't wanna train the complete model again, Stanford guys got over a GB of model information which is useful to me. Just that I want to add custom entities too.</p>
","nlp, stanford-nlp","<p>First of all make sure your CLASSPATH has the proper jars in it.</p>

<p>Here is how you should include your custom trained NER model:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -ner.model &lt;csv-of-model-paths&gt; -file example.txt
</code></pre>

<p><code>-ner.model</code> should be set to a comma separated list of all models you want to use.</p>

<p>Here is an example of what you could put:</p>

<pre><code>edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz,/path/to/custom_model.ser.gz
</code></pre>

<p>Note in my example that all of the standard models will be run, and then finally your custom model will be run.  Make sure your custom model is in the CLASSPATH.</p>

<p>You also probably need to add this to your command: <code>-ner.combinationMode HIGH_RECALL</code>.  By default the NER combination will only use the tags for a particular class from the first model.  So if you have model1,model2,model3 only model1's LOCATION will be used.  If you set things to <code>HIGH_RECALL</code> then model2 and model3's LOCATION tags will be used as well.</p>

<p>Another thing to keep in mind, model2 can't overwrite decisions by model1.  It can only overwrite ""O"".  So if model1 says that a particular token is a LOCATION, model2 can't say it's an ORGANIZATION or a PERSON or anything.  So the order of the models in your list matters.</p>

<p>If you want to write rules that use entities found by previous rules, you should look at my answer to this question:</p>

<p><a href=""https://stackoverflow.com/questions/43447585/tokensregex-rules-to-get-correct-output-for-named-entities/43532621#43532621"">TokensRegex rules to get correct output for Named Entities</a></p>
",1,1,355,2017-04-30 20:06:00,https://stackoverflow.com/questions/43710811/tokenizer-training-with-stanfordnlp
Stanford NLP: Keeping punctuation tokens?,"<p>I am looking for sentences such as </p>

<blockquote>
  <p>Bachelors Degree in early childhood teaching, psychology</p>
</blockquote>

<ul>
<li>I annotate the text using the Stanford Parser. </li>
<li>I then iterate each sentence and identify ""Bachelor's Degree"" using NER (named entity recognition).</li>
<li>By processing triples, I can see that the object follows ""BE IN"" and is likely to be a college major.</li>
<li>So I send the object phrase for further analysis. My trouble is that I don't know how to separate </li>
</ul>

<blockquote>
  <p>early childhood teaching</p>
</blockquote>

<p>from</p>

<blockquote>
  <p>psychology</p>
</blockquote>

<p>My code for this procedure loops through the object triple and keeps it if certain POS requirements are met.</p>

<pre><code>private void processTripleObject(List&lt;CoreLabel&gt; objectPhrase )
{
    try
    {
        StringBuilder sb = new StringBuilder();
        for(CoreLabel token: objectPhrase)
        {
            String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);

            TALog.getLogger().debug(""pos: ""+pos+""  word ""+token.word());
            if(!matchDegreeNameByPos(pos))
            {
                return;
            }

            sb.append(token.word());
            sb.append(SPACE);
        }

        IdentifiedToken itoken = new IdentifiedToken(IdentifiedToken.SKILL, sb.toString());

    }
    catch(Exception e)
    {
        TALog.getLogger().error(e.getMessage(),e);
    }
</code></pre>

<p>Since the comma between teaching and psychology is not in the tokens, I don't know how to recognize the divide.</p>

<p>Can anyone advise?</p>
","java, nlp, stanford-nlp","<p>Note that <code>token.get(CoreAnnotations.PartOfSpeechAnnotation.class)</code> will return the token if no POS tag was found. Tested with CoreNLP 3.7.0 and <code>""tokenize ssplit pos""</code> annotators. You can then check if <code>pos</code> is in a String with punctuation points you are interested in. E.g this some code I just tested:</p>

<pre><code>String punctuations = "".,;!?"";
for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
    for (CoreLabel token: sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        // pos could be ""NN"" but could also be "",""
        String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
        if (punctuations.contains(pos)) {
            // do something with it
        }
    }
}
</code></pre>
",2,1,674,2017-05-02 03:54:54,https://stackoverflow.com/questions/43729848/stanford-nlp-keeping-punctuation-tokens
How to modify TokenRegex rule in StanfordNLP?,"<p>I have rule file for tokenregex as</p>

<p><code>$EDU_FIRST_KEYWORD = (/Education/|/Course[s]?/|/Educational/|/Academic/|/Education/ /and/?|/Professional/|/Certification[s]?/ /and/?)</code></p>

<p><code>$EDU_LAST_KEYWORD  = (/Background/|/Qualification[s]?/|/Training[s]?/|/Detail[s]?/|/Record[s]?/)</code>
<code>tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }</code></p>

<p><code>{ ruleType: ""tokens"", pattern: ( $EDU_FIRST_KEYWORD $EDU_LAST_KEYWORD ?),
  result: ""EDUCATION""
}</code></p>

<p>I want to match <code>EDU_FIRST_KEYWORD</code> followed by <code>EDU_LAST_KEYWORD</code>. If it does not match both parts, then check if <code>EDU_FIRST_KEYWORD</code> matches in given string.</p>

<p>E.g. 1. Training &amp; Courses</p>

<p>Matched Output: EDUCATION (as it matched Courses, which should not happen)</p>

<p><strong>Expected Output: no output</strong> </p>

<p>It is because it <strong>does not match either first part of string or complete string.</strong></p>

<ol start=""2"">
<li>Educational Background</li>
</ol>

<p>Matched Output: EDUCATION</p>

<p>Expected Output: EDUCATION</p>

<p>I tried changing <code>pattern: ( $EDU_FIRST_KEYWORD $EDU_LAST_KEYWORD ?)</code> to 
<code>pattern: ( $EDU_FIRST_KEYWORD + $EDU_LAST_KEYWORD ?)</code> but it does not help.</p>

<p>I tried stanfordNLP tokenregex documentation, but could not get how to achieve this. Can somebody help me changing rule file?
Thanks in advance.</p>
","java, stanford-nlp","<p>You want to use the <code>matches()</code> method of TokenSequenceMatcher to have your rule run against the entire String.</p>

<p>If you use <code>find()</code> it will search the entire string...if you use <code>matches()</code> it will see if the entire string matches the pattern.</p>

<p>At this time I am not sure if the TokensRegexAnnotator can perform full string matches on sentences, so you probably need to use some code like this:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.util.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.tokensregex.Env;
import edu.stanford.nlp.ling.tokensregex.TokenSequencePattern;
import edu.stanford.nlp.ling.tokensregex.TokenSequenceMatcher;
import edu.stanford.nlp.pipeline.*;

import java.util.*;

public class TokensRegexExactMatch {

  public static void main(String[] args) {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation annotation = new Annotation(""Training &amp; Courses"");
    pipeline.annotate(annotation);
    //System.err.println(IOUtils.stringFromFile(""course.rules""));
    Env env = TokenSequencePattern.getNewEnv();
    env.bind(""$EDU_WORD_ONE"", ""/Education|Educational|Courses/"");
    env.bind(""$EDU_WORD_TWO"", ""/Background|Qualification/"");
    TokenSequencePattern pattern = TokenSequencePattern.compile(env, ""$EDU_WORD_ONE $EDU_WORD_TWO?"");
    List&lt;CoreLabel&gt; tokens = annotation.get(CoreAnnotations.TokensAnnotation.class);
    TokenSequenceMatcher matcher = pattern.getMatcher(tokens);
    // matcher.matches()
    while (matcher.find()) {
      System.err.println(""---"");
      String matchedString = matcher.group();
      List&lt;CoreMap&gt; matchedTokens = matcher.groupNodes();
      System.err.println(matchedTokens);
    }
  }
}
</code></pre>
",1,0,194,2017-05-02 07:50:45,https://stackoverflow.com/questions/43732780/how-to-modify-tokenregex-rule-in-stanfordnlp
Dedicated CoreNLP Server Control Issues,"<p><strong>Question:</strong> How can I confirm whether or not my ""Dedicated Server"" is running properly?</p>

<p><strong>Background:</strong> I am working to get a 'Dedicated CoreNLP Server' running on a stand-alone Linux system. This system is a laptop running CentOS 7. This OS was chosen because the directions for a <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">Dedicated CoreNLP Server</a> specifically state that they apply to CentOS. </p>

<p>I have followed the directions for the Dedicated CoreNLP Server step-by-step (outlined below): </p>

<ol>
<li>Downloaded CoreNLP 3.7.0 from the Stanford CoreNLP website (not GitHub) and placed/extracted it into the <a href=""https://i.sstatic.net/tKOqy.png"" rel=""nofollow noreferrer"">/opt/corenlp</a> folder.</li>
<li><p>Installed <code>authbind</code> and created a user called 'nlp' with super user privileges and bind it to port 80</p>

<p><code>sudo mkdir -p /etc/authbind/byport/</code></p>

<p><code>sudo touch /etc/authbind/byport/80</code></p>

<p><code>sudo chown nlp:nlp /etc/authbind/byport/80</code></p>

<p><code>sudo chmod 600 /etc/authbind/byport/80</code></p></li>
<li><p>Copy the startup script from the source jar at path <code>edu/stanford/nlp/pipeline/demo/corenlp</code> to <code>/etc/init.d/corenlp</code></p></li>
<li><p>Give executable permissions to the startup script: <code>sudo chmod a+x /etc/init.d/corenlp</code></p></li>
<li><p>Link the script to <code>/etc/rc.d/</code>: <code>ln -s /etc/init.d/corenlp /etc/rc.d/rc2.d/S75corenlp</code></p></li>
</ol>

<p>Completing these steps is supposed to allow me to run the command <code>sudo service corenlp start</code> in order to run the dedicated server. When I run this command in the terminal I get the output ""CoreNLP server started"" which <strong>IS</strong> consistent with the the start up script ""corenlp"". I then run the start command again and get this same response, which is <strong>NOT</strong> consistent with the start up script. From what I can tell, if the server is actually running and I try to start it again I should get the message ""CoreNLP server is already running!"" This leads me to believe that my server is not actually functioning as it is intended to. </p>

<p><strong>Is this command properly starting the server? How can I tell?</strong></p>

<p>Since the ""proper"" command was not functioning as I thought it should, I used the command <code>sudo systemctl *start* corenlp.service</code> and checked the service's status with <code>sudo systemctl *status* corenlp.service</code>. I am not sure if this is an appropriate way in which to start and stop a 'Dedicated CoreNLP Server' but I can control the service. I just do not know if I am actually starting and stopping my dedicated server. </p>

<p><strong>Can I use <code>systemctl</code> command to operate my <code>Dedicated CoreNLP Server</code>?</strong></p>
","linux, terminal, centos, stanford-nlp","<p>Please read the comments below the originally posted question. This was the back and forth between @GaborAngeli and myself which lead my question/problem being solved. </p>

<p>The two critical steps I took in order to get my instantiation of the CoreNLP server running locally on my machine after following all the directions on how to setup a dedicated server, which are outlined on Stanford CoreNLP's webpage, are as follows:</p>

<ol>
<li><p>Made two modifications to the ""corenlp"" start-up script. (1) added sudo to the beginning because the user ""nlp"" needs permissions to certain files on the system (2) changed the first folder path from /usr/local/bin/authbind to /usr/bin/authbind. <code>authbind</code> installation must've changed since the start up script was written.</p>

<p><code>nohup su ""$SERVER_USER"" -c ""sudo /usr/bin/authbind --deep java -Djava.net.preferIPv4Stack=true -Djava.io.tmpdir_""$CORENLP_DIR"" -cp ""$CLASSPATH"" -mx15g edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 80""</code></p></li>
<li><p>If you were to attempt to start the server with the change above you would not successfully run server because sudo usage requires a password input. In order to allow sudo privileges without a required password entry you need to edit the sudoers file (I did this under the root user b/c you need permissions to change or even view this document). my sudoers file was located in /etc. There is a part that says <code>## Allows people in group wheel to run all commands</code> and below that is a section that says <code>##Same thing without a password</code>. You just need to remove the comment mark (#) form in front of the next line which says <code>%wheel     ALL+(ALL)     NOPASSWD:  ALL</code>. Save this file. <strong>BE CAREFUL IN EDITING THIS FILE AS IT MAY CAUSE SERIOUS ISSUES. MAKE ONLY THE NECESSARY CHANGE OUTLINED ABOVE</strong></p></li>
</ol>

<p>Those two steps allowed me to successfully run my dedicated server. My system runs on CentOS 7.</p>

<p><strong>HELPFUL TIP:</strong> From my discussion with @GaborAngeli I learned that within the 'corenlp' folder (/opt/corenlp if you followed the directions correctly) you can open the <code>stderr.log</code> file to help you in trouble shooting your server. This outputs what you would see if you were to run the server in the command window. If there is an error it is output here too, which is extremely helpful.</p>
",2,1,393,2017-05-02 18:04:02,https://stackoverflow.com/questions/43744429/dedicated-corenlp-server-control-issues
Stanford NLP Tagger via NLTK - tag_sents splits everything into chars,"<p>I'm hoping someone has experience with this as I'm unable to find any comments online besides a bug report from 2015 regarding the NERtagger which is probably the same.</p>

<p>Anyway, I'm trying to batch process text to get around the poor performing base tagger.  From what I understand, tag_sents should help.</p>

<pre><code>from nltk.tag.stanford import StanfordPOSTagger
from nltk import word_tokenize
import nltk

stanford_model = 'stanford-postagger/models/english-bidirectional-distsim.tagger'
stanford_jar = 'stanford-postagger/stanford-postagger.jar'
tagger = StanfordPOSTagger(stanford_model, stanford_jar)
tagger.java_options = '-mx4096m'
text = ""The quick brown fox jumps over the lazy dog.""
print tagger.tag_sents(text)
</code></pre>

<p>Except no matter what I pass to the tag_sents method, the text gets split up into chars instead of words.  Anyone know why it doesn't work properly?  This works as expected...</p>

<pre><code>tag(text)
</code></pre>

<p>I tried splitting the sentence into tokens as well to see if that helped but same treatment</p>
","python, nlp, nltk, stanford-nlp","<p>The <code>tag_sents</code> function takes a list of list of strings.</p>

<pre><code>tagger.tag_sents(word_tokenize(""The quick brown fox jumps over the lazy dog.""))
</code></pre>

<p>Here's a useful idiom:</p>

<pre><code> tagger.tag_sents(word_tokenize(sent) for sent in sent_tokenize(text))
</code></pre>

<p>where <code>text</code> is a string.</p>
",3,2,1480,2017-05-02 21:21:21,https://stackoverflow.com/questions/43747451/stanford-nlp-tagger-via-nltk-tag-sents-splits-everything-into-chars
Extraction of the sentence of a file based on the Stanford Open IE triplet in the form of Json,"<p>I am trying to extract a sentence from a file based on the Stannford OpenIE triplet. I have it in the form of json file. </p>

<pre><code>""openie"": [
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""drive back to"",
      ""relationSpan"": [3, 6],
      ""object"": ""Missouri night"",
      ""objectSpan"": [6, 10]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""drive back to"",
      ""relationSpan"": [3, 6],
      ""object"": ""Missouri"",
      ""objectSpan"": [6, 7]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""decided"",
      ""relationSpan"": [1, 2],
      ""object"": ""drive back to Missouri night"",
      ""objectSpan"": [3, 10]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""drive back to"",
      ""relationSpan"": [3, 6],
      ""object"": ""Missouri same night"",
      ""objectSpan"": [6, 10]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""drive to"",
      ""relationSpan"": [3, 6],
      ""object"": ""Missouri"",
      ""objectSpan"": [6, 7]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""decided"",
      ""relationSpan"": [1, 2],
      ""object"": ""drive to Missouri"",
      ""objectSpan"": [3, 7]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""decided"",
      ""relationSpan"": [1, 2],
      ""object"": ""drive back to Missouri"",
      ""objectSpan"": [3, 7]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""decided"",
      ""relationSpan"": [1, 2],
      ""object"": ""drive to Missouri same night"",
      ""objectSpan"": [3, 10]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""drive to"",
      ""relationSpan"": [3, 6],
      ""object"": ""Missouri same night"",
      ""objectSpan"": [6, 10]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""decided"",
      ""relationSpan"": [1, 2],
      ""object"": ""drive to Missouri night"",
      ""objectSpan"": [3, 10]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""decided"",
      ""relationSpan"": [1, 2],
      ""object"": ""drive"",
      ""objectSpan"": [3, 4]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""decided"",
      ""relationSpan"": [1, 2],
      ""object"": ""drive back to Missouri same night"",
      ""objectSpan"": [3, 10]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""decided"",
      ""relationSpan"": [1, 2],
      ""object"": ""drive back"",
      ""objectSpan"": [3, 5]
    },
    {
      ""subject"": ""Missouri"",
      ""subjectSpan"": [6, 7],
      ""relation"": ""is  at_time"",
      ""relationSpan"": [9, 10],
      ""object"": ""night"",
      ""objectSpan"": [9, 10]
    },
    {
      ""subject"": ""We"",
      ""subjectSpan"": [0, 1],
      ""relation"": ""drive to"",
      ""relationSpan"": [3, 6],
      ""object"": ""Missouri night"",
      ""objectSpan"": [6, 10]
    }
  ],
</code></pre>

<p>I need to extract this specific sentence from the text file from which this json was extracted. 
I am trying to do a text summarization.</p>

<p>Please help.</p>

<p>Thanks in advance</p>
","json, python-3.x, stanford-nlp","<p>The Stanford CoreNLP output attaches the <code>openie</code> output to each sentence.</p>

<p>For instance if you loaded the json into Python you might get this:</p>

<p><code>sample_json['sentences'][0]['openie']</code></p>

<p>You can access info about the sentence from that json.</p>

<p>For instance:</p>

<p><code>sample_json['sentences'][0]['tokens']</code></p>
",0,1,271,2017-05-03 04:45:46,https://stackoverflow.com/questions/43751262/extraction-of-the-sentence-of-a-file-based-on-the-stanford-open-ie-triplet-in-th
How do I get BeautifulSoup to show me specific strings?,"<p>My XML document structure is like this:</p>

<p><code>root
   document
       sentences
           sentence id
               tokens
                   token id
                      word
                      lemma
                      POS
                      NER</code></p>

<p>Here is an example of the children of <code>token id</code>:</p>

<pre><code>        &lt;word&gt;Denmark&lt;/word&gt;
        &lt;lemma&gt;denmark&lt;/lemma&gt;
        &lt;CharacterOffsetBegin&gt;0&lt;/CharacterOffsetBegin&gt;
        &lt;CharacterOffsetEnd&gt;7&lt;/CharacterOffsetEnd&gt;
        &lt;POS&gt;NN&lt;/POS&gt;
        &lt;NER&gt;LOCATION&lt;/NER&gt;
</code></pre>

<p>I want to filter out details of only those words that have the NER tag ""LOCATION"". I tried this:</p>

<pre><code>soup = BeautifulSoup(markup,""lxml-xml"")
print(soup.find_all('NER'))
</code></pre>

<p>But this gives me:</p>

<pre><code>[&lt;NER&gt;LOCATION&lt;/NER&gt;, &lt;NER&gt;O&lt;/NER&gt;, &lt;NER&gt;NUMBER&lt;/NER&gt;, &lt;NER&gt;O&lt;/NER&gt;]
</code></pre>

<p>I want:</p>

<pre><code>denmark, LOCATION
</code></pre>

<p>How do I get that? I looked into the documentation but I cannot find a way out.</p>
","xml, beautifulsoup, stanford-nlp","<p>One option would be to locate the <code>NER</code> tags with <code>LOCATION</code> text and get to it's parent:</p>

<pre><code>for ner in soup('NER', text='LOCATION'):
    token = ner.parent

    print(token.word.get_text(), token.ner.get_text())
</code></pre>
",1,0,74,2017-05-05 07:02:25,https://stackoverflow.com/questions/43798529/how-do-i-get-beautifulsoup-to-show-me-specific-strings
Number name entity recognition in Stanford,"<p>I have a problem in which I'm trying to recognize the number name entity from a text using Stanford , in case I have for example 20 million It's retrieving like this ""Number"":[""20-5"",""million-6""], How can I optimize the answer so 20 millions comes together? and How can I ignore the index number like (5,6) in the above example? I'm using java language. </p>

<pre><code>    public void extractNumbers(String text) throws  IOException {
    number = new HashMap&lt;String, ArrayList&lt;String&gt;&gt;();
    n= new ArrayList&lt;String&gt;();
    edu.stanford.nlp.pipeline.Annotation document = new edu.stanford.nlp.pipeline.Annotation(text);
    pipeline.annotate(document);
    List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
    for (CoreMap sentence : sentences) {
        for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {

            if (!token.get(CoreAnnotations.NamedEntityTagAnnotation.class).equals(""O"")) {

                if (token.get(CoreAnnotations.NamedEntityTagAnnotation.class).equals(""NUMBER"")) {
                  n.add(token.toString());
        number.put(""Number"",n);
                }
            }

        }

    }
</code></pre>
","nlp, stanford-nlp, opennlp","<p>To get the exact text from any object of <code>CoreLabel</code> class simply use <code>token.originalText()</code> instead of <code>token.toString()</code></p>

<p>If you need anything else from these tokens, take a look at <code>CoreLabel</code>'s  <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/CoreLabel.html"" rel=""nofollow noreferrer"">javadoc</a>.</p>
",0,1,421,2017-05-06 04:19:38,https://stackoverflow.com/questions/43816678/number-name-entity-recognition-in-stanford
Killing Stanford core nlp process,"<p>I launch Stanford Core NLP server using the following command (on Ubuntu 16.04):</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
</code></pre>

<p>I would like to kill this server once I am done using it. Simply closing terminal does not help. It does not release memory. Is there way to kill it and release memory without rebooting computer?</p>
","linux, ubuntu, stanford-nlp","<p>You can always <code>CTRL-C</code> in the terminal window to stop the server.</p>

<p>You could also <code>ps aux | grep StanfordCoreNLPServer</code> to find the pid and then kill the process manually.</p>

<p>When the server is started it should create a shutdown key and you can send that message to the server to close the server.  This isn't working on my Macbook Pro (maybe a permission issue ??) but I've seen it work on other machines.</p>

<p>Here is the command:</p>

<pre><code>wget ""localhost:9000/shutdown?key=`cat /tmp/corenlp.shutdown`"" -O -
</code></pre>

<p>Note the shutdown key is stored at <code>/tmp/corenlp.shutdown</code></p>

<p>If you use the the <code>-server_id server0</code> option the shutdown key will be stored at this path <code>/tmp/corenlp.shutdown.server0</code></p>
",3,1,1020,2017-05-07 01:04:24,https://stackoverflow.com/questions/43826851/killing-stanford-core-nlp-process
How to generate custom training data for Stanford relation extraction,"<p>I have trained a custom classifier to understand named entities in finance domain. I want to generate custom training data like shown in below link 
<a href=""http://cogcomp.cs.illinois.edu/Data/ER/conll04.corp"" rel=""nofollow noreferrer"">http://cogcomp.cs.illinois.edu/Data/ER/conll04.corp</a></p>

<p>I can mark the custom relation by hand but want to generate the data format like conll first with my custom named entities. </p>

<p>I have also tried the parser in the following way but that does not generate the relation training data like  Roth and Yih's data mentioned in link <a href=""https://nlp.stanford.edu/software/relationExtractor.html#training"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/relationExtractor.html#training</a>.</p>

<p>java -mx150m -cp ""stanford-parser-full-2013-06-20/*:"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat ""penn"" edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz stanford-parser-full-2013-06-20/data/testsent.txt >testsent.tree</p>

<p>java -mx150m -cp ""stanford-parser-full-2013-06-20/*:"" edu.stanford.nlp.trees.EnglishGrammaticalStructure -treeFile testsent.tree -conllx</p>

<p>Following is the output of custom ner run separate with the following python code</p>

<pre><code>'java -mx2g -cp ""*"" edu.stanford.nlp.ie.NERClassifierCombiner '\
                '-ner.model classifiers\custom-model.ser.gz '\
                'classifiers/english.all.3class.distsim.crf.ser.gz,'\
                'classifiers/english.conll.4class.distsim.crf.ser.gz,'\
                'classifiers/english.muc.7class.distsim.crf.ser.gz ' \
                '-textFile '+ outtxt_sent +  ' -outputFormat inlineXML  &gt; ' + outtxt + '.ner'

output:

&lt;PERSON&gt;Charles Sinclair&lt;/PERSON&gt; &lt;DESG&gt;Chairman&lt;/DESG&gt; &lt;ORGANIZATION&gt;-LRB- age 68 -RRB- Charles was appointed a&lt;/ORGANIZATION&gt; &lt;DESG&gt;non-executive director&lt;/DESG&gt; &lt;ORGANIZATION&gt;in&lt;/ORGANIZATION&gt;
</code></pre>

<p>So the NER is working standalone fine even i have java code to test it out.</p>

<p>Here is the detailed code for relation data generation</p>

<pre><code>Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions"");
        props.setProperty(""ner.model"", ""classifiers/custom-model.ser.gz,classifiers/english.all.3class.distsim.crf.ser.gz,classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz"");
        // set up Stanford CoreNLP pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // build annotation for a review
        Annotation annotation = new Annotation(""Charles Sinclair Chairman -LRB- age 68 -RRB- Charles was appointed a non-executive director"");
        pipeline.annotate(annotation);
        int sentNum = 0;

.............. Rest of the code is same as yours

output:
0   PERSON  0   O   NNP/NNP Charles/Sinclair    O   O   O
0   PERSON  1   O   NNP Chairman    O   O   O
0   PERSON  2   O   -LRB-/NN/CD/-RRB-/NNP/VBD/VBN/DT    -LRB-/age/68/-RRB-/Charles/was/appointed/a  O   O   O
0   PERSON  3   O   JJ/NN   non-executive/director  O   O   O

O   3   member_of_board //I will modify the relation once the data generated with proper NER

The Ner tagging is ok now.  
 props.setProperty(""ner.model"", ""classifiers/classifiers/english.all.3class.distsim.crf.ser.gz,classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz,"");
</code></pre>

<p>Custom NER problem solved.</p>
",stanford-nlp,"<p>This link shows an example of the data: <a href=""http://cogcomp.cs.illinois.edu/Data/ER/conll04.corp"" rel=""nofollow noreferrer"">http://cogcomp.cs.illinois.edu/Data/ER/conll04.corp</a></p>

<p>I don't think there is a way to produce this in Stanford CoreNLP.</p>

<p>After you tag the data, you need to loop through the sentences and print out the tokens in that same format, including the part-of-speech tag and the ner tag.  It appears most of the columns have a ""O"" in them.</p>

<p>For each sentence that has a relationship you need to print out the a line after the sentence in the relation format.  For instance this line indicates the previous sentence has the Live_In relationship:</p>

<pre><code>7    0    Live_In
</code></pre>

<p>Here is some example code to generate the output for a sentence.  You will need to set the pipeline to use your <code>ner</code> model instead by setting the <code>ner.model</code> property to the path of your custom model.  WARNING: There may be some bugs in this code, but it should show how to access the data you need from the StanfordCoreNLP data structures.</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

import java.util.*;
import java.util.stream.Collectors;

public class CreateRelationData {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation = new Annotation(""Joe Smith lives in Hawaii."");
    pipeline.annotate(annotation);
    int sentNum = 0;
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      int tokenNum = 1;
      int elementNum = 0;
      int entityNum = 0;
      CoreMap currEntityMention = sentence.get(CoreAnnotations.MentionsAnnotation.class).get(entityNum);
      String currEntityMentionWords = currEntityMention.get(CoreAnnotations.TokensAnnotation.class).stream().map(token -&gt; token.word()).
          collect(Collectors.joining(""/""));
      String currEntityMentionTags =
          currEntityMention.get(CoreAnnotations.TokensAnnotation.class).stream().map(token -&gt; token.tag()).
              collect(Collectors.joining(""/""));
      String currEntityMentionNER = currEntityMention.get(CoreAnnotations.EntityTypeAnnotation.class);
      while (tokenNum &lt;= sentence.get(CoreAnnotations.TokensAnnotation.class).size()) {
        if (currEntityMention.get(CoreAnnotations.TokensAnnotation.class).get(0).index() == tokenNum) {
          String entityText = currEntityMention.toString();
          System.out.println(sentNum+""\t""+currEntityMentionNER+""\t""+elementNum+""\t""+""O\t""+currEntityMentionTags+""\t""+
              currEntityMentionWords+""\t""+""O\tO\tO"");
          // update tokenNum
          tokenNum += (currEntityMention.get(CoreAnnotations.TokensAnnotation.class).size());
          // update entity if there are remaining entities
          entityNum++;
          if (entityNum &lt; sentence.get(CoreAnnotations.MentionsAnnotation.class).size()) {
            currEntityMention = sentence.get(CoreAnnotations.MentionsAnnotation.class).get(entityNum);
            currEntityMentionWords = currEntityMention.get(CoreAnnotations.TokensAnnotation.class).stream().map(token -&gt; token.word()).
                collect(Collectors.joining(""/""));
            currEntityMentionTags =
                currEntityMention.get(CoreAnnotations.TokensAnnotation.class).stream().map(token -&gt; token.tag()).
                    collect(Collectors.joining(""/""));
            currEntityMentionNER = currEntityMention.get(CoreAnnotations.EntityTypeAnnotation.class);
          }
        } else {
          CoreLabel token = sentence.get(CoreAnnotations.TokensAnnotation.class).get(tokenNum-1);
          System.out.println(sentNum+""\t""+token.ner()+""\t""+elementNum+""\tO\t""+token.tag()+""\t""+token.word()+""\t""+""O\tO\tO"");
          tokenNum += 1;
        }
        elementNum += 1;
      }
      sentNum++;
    }
    System.out.println();
    System.out.println(""O\t3\tLive_In"");
  }
}
</code></pre>
",1,0,949,2017-05-07 07:19:22,https://stackoverflow.com/questions/43828851/how-to-generate-custom-training-data-for-stanford-relation-extraction
Type of Properties method in Stanford Core NLP,"<p>I am building a front end to parse some text files using Stanford Core NLP in C#. I open a file selection dialog and select some text files. Then the following method works from there on.</p>

<pre><code>    using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;
using System.IO;
using System.Text.RegularExpressions;
using java.util;
using java.io;
using edu.stanford.nlp.pipeline;

namespace Parser_SVO
{
    public partial class Form1 : Form
    {
        public static List&lt;string&gt; textFiles = new List&lt;string&gt;();
        public Form1()
        {
            InitializeComponent();
        }

        private void button1_Click(object sender, EventArgs e)
        {
            OpenFileDialog openFileDialog1 = new OpenFileDialog();
            openFileDialog1.ShowReadOnly = true;
            openFileDialog1.Filter = ""Text Files|*.txt"";
            if (openFileDialog1.ShowDialog() == System.Windows.Forms.DialogResult.OK)
            {
                textFiles.AddRange(openFileDialog1.FileNames);
            }
            parseText();
        }
        public static void parseText()
        {
            label2.Text = ""Stanford Parser...."";
            // Path to the folder with models extracted from `stanford-corenlp-3.7.0-models.jar`
            string jarRoot = """";
            string prettyPrint = """";
            if (textFiles.Count != 0)
            {
                jarRoot = Path.GetDirectoryName(textFiles[0]) + @""\Models\"";
                prettyPrint = Path.GetDirectoryName(textFiles[0]);
                Directory.CreateDirectory(prettyPrint + @""\PrettyPrint\"");
                prettyPrint = prettyPrint + @""\PrettyPrint\"";
            }
            // Annotation pipeline configuration
            var props = Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            props.setProperty(""ner.useSUTime"", ""0"");

            // We should change current directory, so StanfordCoreNLP could find all the model files automatically
            var curDir = Environment.CurrentDirectory;
            Directory.SetCurrentDirectory(jarRoot);
            var pipeline = new StanfordCoreNLP(props);
            Directory.SetCurrentDirectory(curDir);
            foreach (string file in textFiles)
            {
                label3.Text = file;
                // Text for processing
                var text = System.IO.File.ReadAllText(file);
                // Annotation
                var annotation = new Annotation(text);
                pipeline.annotate(annotation);
                // Result - Pretty Print
                string output = prettyPrint + Path.GetFileName(file);
                using (var stream = new ByteArrayOutputStream())
                {
                    pipeline.prettyPrint(annotation, new PrintWriter(stream));
                    System.IO.File.AppendAllText(output, stream.toString()+Environment.NewLine);
                    stream.close();
                }
            }

        }
    }
}
</code></pre>

<p>I have modified the example from official StanfordCoreNLP .Net port <a href=""https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html"" rel=""nofollow noreferrer"">here</a>. 
Since I am using Windows Forms instead of Console application, this line of code is creating problem: <code>var props = Properties();</code>. I am not sure how to find the namespace of this method to provide a complete namespace.class.method path to disambiguate.
Another minor problem is that I want to update label text as in <code>label2.Text = ""Stanford Parser...."";</code> but visual studio says that ""An object reference is required"" while I am in the same class (Forms1.cs). Your help will be greatly appreciated.</p>
","c#, winforms, stanford-nlp","<p>The Properties() class is java.util.Properties.
Simply remove <code>static</code> from the method name to access windows forms objects like text box or label.</p>
",1,0,349,2017-05-07 10:28:25,https://stackoverflow.com/questions/43830397/type-of-properties-method-in-stanford-core-nlp
py-corenlp - TypeError: string indices must be integers,"<p>I followed this answer:
<a href=""https://stackoverflow.com/questions/32879532/stanford-nlp-for-python/40496870#40496870"">Stanford nlp for python</a></p>

<pre><code>from pycorenlp import StanfordCoreNLP
from newspaper import Article

url = u'newsArticle.example.html'
nlp = StanfordCoreNLP('http://localhost:9000')
article = Article(url)
article.download()
article.parse()


LARGE_TEXT=article.text


res = nlp.annotate(LARGE_TEXT,
               properties={
                   'annotators': 'sentiment',
                   'outputFormat': 'json',
                   'timeout': 1000,
               })
for s in res[""sentences""]:
    print (""%d: '%s': %s %s"" % (
        s[""index""],
        "" "".join([t[""word""] for t in s[""tokens""]]),
        s[""sentimentValue""], s[""sentiment""]))
</code></pre>

<p>I used a longer text as input and encountered following error:</p>

<pre><code> for s in res[""sentences""]:
TypeError: string indices must be integers
</code></pre>
","python, python-3.x, stanford-nlp","<p>The problem was <strong>'timeout': 1000</strong></p>

<p>I changed it to <strong>'timeout': 10000</strong></p>
",2,2,880,2017-05-08 14:01:16,https://stackoverflow.com/questions/43849689/py-corenlp-typeerror-string-indices-must-be-integers
How to import created NER to Stanford CoreNLP?,"<p>I have created my own NER with the help from this <a href=""https://nlp.stanford.edu/software/crf-faq.html"" rel=""nofollow noreferrer"">link</a>.</p>

<p>So, I have downloaded StanfordNER, and tokenized and created tsv file from my own corpus, then created NER.</p>

<p>My next step is to import this NER into the CoreNLP.
How do I import this created NER to the Stanford CoreNLP server?
Is there a specific folder where I can paste the NER?</p>
",stanford-nlp,"<p>You need to start the server with the <code>-serverProperties</code> option and point it to a properties file.</p>

<p>You should include <code>ner.model = /path/to/custom_model.ser.gz</code> to have it use your custom model.  Make sure the annotators list has <code>annotators = tokenize,ssplit,pos,lemma,ner</code></p>
",1,1,84,2017-05-10 15:27:25,https://stackoverflow.com/questions/43896842/how-to-import-created-ner-to-stanford-corenlp
When training NER with BIO chunks what would be the most suitable approach in following case?,"<p>In my case I want to tag following under DISEASE and SYMPTOM tags </p>

<p><em>Eg : Osgood-Schlatter disease is a painful inflammation of the upper portion of the tibia.</em></p>

<p>where I have successfully trained as <code>Osgood-Schlatter/b-disease disease/i-disease</code> (I have used BIO NER tag approach)</p>

<p>However I am not sure whether is it correct to tag remaining part <strong>""painful inflammation of the upper portion of the tibia""</strong> as Symptom itself.</p>

<p>as <code>painful/b-symptom  inflammation/i-symptom of/i-symptom the/i-symptom upper/i-symptom portion/i-symptom of/i-symptom the/i-symptom tibia/i-symptom</code></p>

<p>I would like any opinion whether to use my current approach or should I follow POS tag based parsing approach or any other suitable approach?</p>
","nlp, stanford-nlp","<p>For the similar use case in financial domain, i have got fairly good results with this approach.
But I had also used POS tags and distsim approach to train the model.
That way you provide more features to learn from.</p>
",0,0,58,2017-05-12 06:04:55,https://stackoverflow.com/questions/43930386/when-training-ner-with-bio-chunks-what-would-be-the-most-suitable-approach-in-fo
ConllReader (Like RothCONLL04Reader) throws exception while reading relation training data with custom NER and custom relation,"<p>In continuation of the following question.
<a href=""https://stackoverflow.com/questions/43828851/how-to-generate-custom-training-data-for-stanford-relation-extraction"">How to generate custom training data for Stanford relation extraction</a></p>

<p>Thanks to StanfordNLPHelp i am able to generate relation data with custom ner and on top of it regexner.</p>

<pre><code>I had to run my custom model at the end because otherwise it will misclassify lots of ORGANIZATION PERSON etc. 
Example custom NER classes. 

""DEGREE"", ""DESG""
</code></pre>

<p>Example of relation training data.</p>

<pre><code>0   ELECTEDBODY 0   O   NNP/IN/NNP  BOARD/OF/DIRECTORS  O   O   O
0   ORGANIZATION    1   O   NNP Board   O   O   O
0   O   2   O   NNS committees  O   O   O
0   O   3   O   JJ  key O   O   O
0   ORGANIZATION    4   O   NN/NN/NN/NN/NNP/NN  N/Nomination/committee/A/Audit/committee    O   O   O
0   O   5   O   NN  R   O   O   O
0   MISC    6   O   NN  Remuneration    O   O   O
0   O   7   O   NN  committee   O   O   O
0   O   8   O   NNP EFFECTIVE   O   O   O
0   O   9   O   NNP LEADERSHIP  O   O   O
0   O   10  O   CC  AND O   O   O
0   O   11  O   JJ  STRONG  O   O   O
0   O   12  O   NN  GOVERNANCE  O   O   O
0   O   13  O   NNP George  O   O   O
0   O   14  O   NNP Weston  O   O   O
0   DESG    15  O   NNP/NNP Chief/Executive O   O   O
0   O   16  O   -LRB-   -LRB-   O   O   O
0   O   17  O   NN  age O   O   O
0   NUMBER  18  O   CD  52  O   O   O
0   O   19  O   -RRB-   -RRB-   O   O   O
0   PERSON  20  O   NNP George  O   O   O
0   O   21  O   VBD was O   O   O
0   O   22  O   VBN appointed   O   O   O
0   O   23  O   TO  to  O   O   O
0   O   24  O   DT  the O   O   O
0   ELECTEDBODY 25  O   NN  board   O   O   O
0   DATE    26  O   IN/CD   in/1999 O   O   O
0   O   27  O   CC  and O   O   O
0   O   28  O   VBD took    O   O   O
0   O   29  O   RP  up  O   O   O
0   O   30  O   PRP$    his O   O   O
0   O   31  O   JJ  current O   O   O
0   O   32  O   NN  appointment O   O   O
0   O   33  O   IN  as  O   O   O
0   DESG    34  O   NNP/NNP Chief/Executive O   O   O
0   O   35  O   IN  in  O   O   O
0   DATE    36  O   NNP/CD  April/2005  O   O   O
0   O   37  O   .   .   O   O   O

20  34  cur_desg 
20  36  cur_desg_from
</code></pre>

<p>I am trying to train custom relation model and added my custom relation classes.</p>

<pre><code>ex: relation class -&gt; **cur_desg** (current designation) between entities (**PERSON, DESG**)
**Here is the relevant section of my properties file to train the relation classifier.**

datasetReaderClass = com.samrat.nlp.ie.re.CustomConllReader
entityClassifier = com.samrat.nlp.ie.re.CustomConllExtractor
relationResultsPrinters = com.samrat.nlp.ie.re.RelationResultPrinter

serializedTrainingSentencesPath = custom_relation_sentences.ser
serializedEntityExtractorPath = custom_relation_model.ser
serializedRelationExtractorPath = custom-relation-model-pipeline.ser
</code></pre>

<p>Relevant section of Code CustomConllReader</p>

<pre><code>private String getNormalizedNERTag(String ner) {
        ......
        }  else if(ner.equalsIgnoreCase(""degree"")) {
            return ""DEGREE"";
        }
        else if(ner.equalsIgnoreCase(""electedbody"")) {
            return ""ELECTEDBODY"";
        }
...............
</code></pre>

<p><strong>Problem 1</strong> 
    (CustomConllReader throws exception at following line while reading training data)</p>

<pre><code>Span span = new Span(entity1.getExtentTokenStart(), entity2.getExtentTokenEnd());
</code></pre>

<p>Relevant portion of CustomConllReader (It is almost same as RothCONLL04Reader)</p>

<pre><code>case 3: // relation
                System.out.println(currentLine);
                String type = pieces.get(2);
                List&lt;ExtractionObject&gt; args = new ArrayList&lt;&gt;();
                EntityMention entity1 = indexToEntityMention.get(pieces.get(0));
                EntityMention entity2 = indexToEntityMention.get(pieces.get(1));
                args.add(entity1);
                args.add(entity2);
                Span span = new Span(entity1.getExtentTokenStart(), entity2.getExtentTokenEnd());
                // identifier = ""relation"" + sentenceID + ""-"" + sentence.getAllRelations().size();
                identifier = RelationMention.makeUniqueId();
                RelationMention relationMention = new RelationMention(identifier,
                        sentence, span, type, null, args);
                AnnotationUtils.addRelationMention(sentence, relationMention);
                break;
</code></pre>

<p>Exception</p>

<pre><code>    INFO: Reading file: tagged-training-relation-data-conll04.corp
20  34  cur_desg 
20  36  cur_desg_from
0   2   cur_desg
Exception in thread ""main"" java.io.IOException
    at edu.stanford.nlp.ie.machinereading.GenericDataSetReader.parse(GenericDataSetReader.java:138)
    at com.wipro.nlp.ie.re.CustomConllReader.main(CustomConllReader.java:292)
Caused by: java.lang.NullPointerException
    at com.wipro.nlp.ie.re.CustomConllReader.readSentence(CustomConllReader.java:144)
    at com.wipro.nlp.ie.re.CustomConllReader.read(CustomConllReader.java:55)
    at edu.stanford.nlp.ie.machinereading.GenericDataSetReader.parse(GenericDataSetReader.java:136)
    ... 1 more
</code></pre>

<p>The exception thrown on sentence 3 while parsing the relation (0 2 cur_desg)</p>

<pre><code>3   PERSON  0   O   NNP/NNP John/Bason  O   O   O
3   O   1   O   NNP Finance O   O   O
3   ELECTEDBODY 2   O   NNP Director    O   O   O
3   O   3   O   -LRB-   -LRB-   O   O   O
3   O   4   O   NN  age O   O   O
3   NUMBER  5   O   CD  59  O   O   O
3   O   6   O   -RRB-   -RRB-   O   O   O
3   PERSON  7   O   NNP John    O   O   O
3   O   8   O   VBD was O   O   O
3   O   9   O   VBN appointed   O   O   O
3   O   10  O   IN  as  O   O   O
3   O   11  O   NNP Finance O   O   O
3   ELECTEDBODY 12  O   NNP Director    O   O   O
3   O   13  O   IN  in  O   O   O
3   DATE    14  O   NNP/CD  May/1999    O   O   O
3   O   15  O   .   .   O   O   O

0   2   cur_desg
0   14  cur_desg_from
</code></pre>

<p><strong>This problem is solved, my training data has extra line break in between i am able to build a custom relation classifier. 
But now while using that custom relation classifier it does not understand any custom NER tags or custom relations.</strong></p>

<p>Separate question here below. (for making custom relation classifier understand custom ner tags and relations in new sentences)
<a href=""https://stackoverflow.com/questions/43954566/custom-relation-classifier-does-not-understand-any-custom-ner-tags-and-does-not"">Custom Relation Classifier does not understand any Custom NER tags and does not find any relations</a></p>
",stanford-nlp,"<p>The exception was thrown due to extra line break in between.
There has to be exactly two line breaks in the input tagged training data like below.</p>

<pre><code>PERSON  0   O   NNP/NNP John/Bason  O   O   O
3   O   1   O   NNP Finance O   O   O
3   ELECTEDBODY 2   O   NNP Director    O   O   O
3   O   3   O   -LRB-   -LRB-   O   O   O
3   O   4   O   NN  age O   O   O
3   NUMBER  5   O   CD  59  O   O   O
3   O   6   O   -RRB-   -RRB-   O   O   O
3   PERSON  7   O   NNP John    O   O   O
3   O   8   O   VBD was O   O   O
3   O   9   O   VBN appointed   O   O   O
3   O   10  O   IN  as  O   O   O
3   O   11  O   NNP Finance O   O   O
3   ELECTEDBODY 12  O   NNP Director    O   O   O
3   O   13  O   IN  in  O   O   O
3   DATE    14  O   NNP/CD  May/1999    O   O   O
3   O   15  O   .   .   O   O   O

0   2   cur_desg
0   14  cur_desg_from

5   O   0   O   PRP He  O   O   O
5   O   1   O   VBD was O   O   O
5   O   2   O   RB  previously  O   O   O
5   O   3   O   DT  the O   O   O
5   O   4   O   NN  finance O   O   O
5   DESG    5   O   NN  director    O   O   O
5   O   6   O   IN  of  O   O   O
5   ORGANIZATION    7   O   NNP Bunzl   O   O   O
5   O   8   O   NN  plc O   O   O
5   O   9   O   CC  and O   O   O
5   O   10  O   VBZ is  O   O   O
</code></pre>
",0,0,120,2017-05-12 08:23:47,https://stackoverflow.com/questions/43932872/conllreader-like-rothconll04reader-throws-exception-while-reading-relation-tra
Train Chinese Segmenter with custom sources,"<p>I want to train the Chines Segenter with new data and i produced a dictionary and a serialized treebank text file.</p>

<p>My problem is that i do not understand or find documentations about the difference between:</p>

<p>-sighanCorporaDict data </p>

<p>and</p>

<p>-trainFile train.txt</p>

<p>Can somebody help me out with this problem. My Chinese datasets are Buddhist ancient texts which makes it hard to replace ressources like -sighanCorporaDict?</p>

<p>All the best</p>

<p>Andreas</p>
",stanford-nlp,"<p>There is documentation here for training your own Chinese segmenter:</p>

<p><a href=""https://nlp.stanford.edu/software/segmenter-faq.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/segmenter-faq.html</a></p>

<p><code>sighanCorporaDict</code> is a directory with resources the segmenter needs...this should be set to the <code>data</code> directory in the segmenter distribution</p>

<p><code>trainFile</code> should be a list of sentences that have been properly segmented (words separated by space).</p>
",1,0,112,2017-05-12 08:50:27,https://stackoverflow.com/questions/43933366/train-chinese-segmenter-with-custom-sources
Load Custom NER Model Stanford CoreNLP,"<p>I have created my own NER model with Stanford's ""Stanford-NER"" software and by following <a href=""https://nlp.stanford.edu/software/crf-faq.html#a"" rel=""noreferrer"">these</a> directions. </p>

<p>I am aware that CoreNLP loads three NER models out of the box in the following order:</p>

<ol>
<li><code>edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz</code></li>
<li><code>edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz</code></li>
<li><code>edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz</code></li>
</ol>

<p>I now want to include my NER model in the list above and have the text tagged by my NER model first.</p>

<p>I have found two previous StackOverflow questions regarding this topic and they are <a href=""https://stackoverflow.com/questions/41232187/stanford-openie-using-customized-ner-model"">'Stanford OpenIE using customized NER model'</a> and <a href=""https://stackoverflow.com/questions/33905412/why-does-stanford-corenlp-ner-annotator-load-3-models-by-default?rq=1"">'Why does Stanford CoreNLP NER-annotator load 3 models by default?'</a></p>

<p>Both of these posts have good answers. The general message of the answers is that you have to edit code within a file. </p>

<p><strong>Stanford OpenIE using customized NER model</strong></p>

<p>From this post it says to edit <code>corenlpserver.sh</code> but I cannot find this file within the Stanford CoreNLP downloaded software. Can anyone point me to this file's location? </p>

<p><strong>does Stanford CoreNLP NER-annotator load 3 models by default?</strong></p>

<p>This post says that I can use the argument of <code>-ner.model</code> to specifically call which NER models to load. I added this argument to the initial server command (<code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 -ner.model *modlefilepathhere*</code>). This did not work as the server still loaded all three models. </p>

<p>It also states that you have to change some java code though it does not specifically call out where to make the change. </p>

<p>Do I need to modify or add this code <code>props.put(""ner.model"", ""model_path1,model_path2"");</code> to a specific class file in the CoreNLP software? </p>

<p><strong>QUESTION:</strong> From my research it seems that I need to add/modify some code to call my unique NER model. These 'edits' are outlined above and this information has been pulled from other StackOverflow questions. What files specifically do I need to edit? Where exactly are these files located (i.e. edu/Stanford/nlp/...etc)?</p>

<p><strong>EDIT:</strong> My system is running on a local server and I'm using the API pycorenlp in order to open a pipeline to my local server and to make requests against it. the two critical lines of python/pycorenlp code are:</p>

<ol>
<li><code>nlp = StanfordCoreNLP('http://localhost:9000')</code></li>
<li><code>output = nlp.annotate(evalList[line], properties={'annotators': 'ner, openie','outputFormat': 'json', 'openie.triple.strict':'True', 'openie.max_entailments_per_clause':'1'})</code></li>
</ol>

<p>I do <em>NOT</em> think this will affect my ability to call my unique NER model but I wanted to present all the situational data I can in order to obtain the best possible answer.</p>
","java, python, python-3.x, nlp, stanford-nlp","<p>If you want to customize the pipeline the server uses, create a file called <code>server.properties</code> (or you can call it whatever you want).</p>

<p>Then add this option when you start the server <code>-serverProperties server.properties</code> with the java command.</p>

<p>In that .properties file you should include <code>ner.model = /path/to/custom_model.ser.gz</code></p>

<p>In general you can customize the pipeline the server will use in that .properties file.  For instance you can also set the list of annotators in it with the line <code>annotators = tokenize,ssplit,pos,lemma,ner,parse</code> etc...</p>

<p>UPDATE to address comments:</p>

<ol>
<li><p>In your java command you don't need the <code>-ner.model /path/to/custom_model.ser.gz</code></p></li>
<li><p>A .properties file can have an unlimited amount of properties settings in it, one setting per line (blank lines are ignored, as are #'d out lines)</p></li>
<li><p>When you run a Java command, it default looks for files in the directory you are running the command.  So if your command includes <code>-serverProperties server.properties</code> it is going to assume that the file <code>server.properties</code> is in the same directory the command is running from.  If you supply an absolute path instead <code>-serverProperties /path/to/server.properties</code> you can run the command from anywhere.</p></li>
<li><p>So just to be clear you could start the server with this command (run in the folder with all the jars):</p></li>
</ol>

<p><code>java -Xmx8g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 -serverProperties server.properties</code></p>

<p>and <code>server.properties</code> should be a file like this:</p>

<pre><code>ner.model = /path/to/custom_model.ser.gz
</code></pre>

<p><code>server.properties</code> could look like this:</p>

<pre><code>annotators = tokenize,ssplit,pos,lemma,ner,depparse
ner.model = /path/to/custom_model.ser.gz
parse.maxlen = 100
</code></pre>

<p>just as an example...you should put all settings into <code>server.properties</code></p>

<ol start=""5"">
<li>I made some comments about accessing the StanfordCoreNLP server from Python in a previous answer:</li>
</ol>

<p><a href=""https://stackoverflow.com/questions/42896027/cannot-use-pycorenlp-for-python3-5-through-terminal/42915635#42915635"">cannot use pycorenlp for python3.5 through terminal</a></p>

<p>You appear to be using the pycorenlp library which I don't really know about.  2 other options are some code I show in that answer or the <code>stanza</code> package we make.  Details in that answer above.</p>
",3,6,3831,2017-05-12 16:29:01,https://stackoverflow.com/questions/43942476/load-custom-ner-model-stanford-corenlp
Set options in Stanford CoreNLP tokenizer,"<p>I adapted Prof. Mannings code sample from <a href=""https://stackoverflow.com/questions/11832490/stanford-core-nlp-java-output"">here</a> to read in a file, tokenize, part-of-speech-tag, and lemmatize it.</p>

<p>Now I came across the issue of untokenizable characters, and I would like to use the ""untokenizable"" option and set it to ""noneKeep"".</p>

<p>Other questions on StackOverflow explain that I would need to instantiate the tokenizer myself. However, I am not sure how to do that so that the following tasks (POS tagging etc.) are still performed as needed. Can anyone point me in the right direction?</p>

<pre><code>// expects two command line parameters: one file to be read, one to write to

import java.io.*;
import java.util.*;

import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

public class StanfordCoreNlpDemo {

  public static void main(String[] args) throws IOException {
    PrintWriter out;
    out = new PrintWriter(args[1]);

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation annotation;
    annotation = new Annotation(IOUtils.slurpFileNoExceptions(args[0]));

    pipeline.annotate(annotation);
    pipeline.prettyPrint(annotation, out);
  }
}
</code></pre>
","java, stanford-nlp","<p>Add this to your code:</p>

<p><code>props.setProperty(""tokenize.options"", ""untokenizable=allKeep"");</code></p>

<p>The 6 options for untokenizable are:</p>

<p><code>noneDelete, firstDelete, allDelete, noneKeep, firstKeep, allKeep</code></p>
",4,0,988,2017-05-12 19:42:44,https://stackoverflow.com/questions/43945368/set-options-in-stanford-corenlp-tokenizer
Could not find stanford-postagger.jar jar file at,"<p>I am trying to use the StanfordTokenizer <code>tokenize()</code> in my script, but it seems like it can't find the jar in the <code>CLASSPATH</code> I ordered.</p>

<p>I tried to change the <code>_JAR = 'stanford-postagger.jar'</code> to <code>C:\Program Files\JetBrains\PyCharm 2017.1.2\stanford-postagger-2016-10-31\'stanford-postagger.jar'</code> but it doesn't seem to work. </p>

<p>This is my script:</p>

<pre><code>from nltk.tokenize.stanford import StanfordTokenizer
def AnalyzeText(text):
     t = StanfordTokenizer(path_to_jar='C:\Program Files\JetBrains\PyCharm 2017.1.2\stanford-postagger-2016-10-31\stanford-postagger.jar')
     return t.tokenize(text)
</code></pre>

<p><img src=""https://i.sstatic.net/Avrw1.png"" alt=""This is the error I get""></p>

<p>I do have updated <code>nltk</code>. I also downloaded the <code>stanford-postagger</code> as you see. I can't understand what the problem is.</p>
","python, nltk, stanford-nlp","<p>It seems like the ClASSPATH was okay. NLTK needs a JDK 1.8 version.</p>

<pre><code>import os
java_path = ""C:/Program Files/Java/jdk1.8.0_131/bin/java.exe""
os.environ['JAVAHOME'] = java_path
</code></pre>
",1,1,2214,2017-05-15 15:15:04,https://stackoverflow.com/questions/43983040/could-not-find-stanford-postagger-jar-jar-file-at
Is there a way to load the wiki-fasttext model faster with load_word2vec_format,"<p>Loading the wiki-fasttext model with the gensim library takes <strong>six</strong> minutes. </p>

<p>I'm aware of ways to cache the model but I'm looking for ways to speedup the initial model loading. The specific api is below:</p>

<pre><code>en_model = KeyedVectors.load_word2vec_format(os.path.join(root_dir, model_file))
</code></pre>

<p>Granted, wiki-fasttext a very large model, however I have load the same model in many languages.  </p>
","nlp, stanford-nlp, gensim, fasttext","<p>You can try to use <code>limit=vector_num</code> argument for load <code>vector_num</code> word vectors from a file. You will not load all the vectors, but you can speed up the loading process.</p>
",2,1,1439,2017-05-15 17:07:37,https://stackoverflow.com/questions/43985180/is-there-a-way-to-load-the-wiki-fasttext-model-faster-with-load-word2vec-format
Why does Stanford CoreNLP server split named entities into single tokens?,"<p>I'm using this command to post the data (a bit of copy pasta from the stanford site):</p>

<pre><code>wget --post-data 'Barack Obama was President of the United States of America in 2016' 'localhost:9000/?properties={""annotators"": ""ner"", ""outputFormat"": ""json""}' -O out.json
</code></pre>

<p>The response looks like this:</p>

<pre><code>{
    ""sentences"": [{
        ""index"": 0,
        ""tokens"": [{
            ""index"": 1,
            ""word"": ""Barack"",
            ""originalText"": ""Barack"",
            ""lemma"": ""Barack"",
            ""characterOffsetBegin"": 0,
            ""characterOffsetEnd"": 6,
            ""pos"": ""NNP"",
            ""ner"": ""PERSON"",
            ""before"": """",
            ""after"": "" ""
        }, {
            ""index"": 2,
            ""word"": ""Obama"",
            ""originalText"": ""Obama"",
            ""lemma"": ""Obama"",
            ""characterOffsetBegin"": 7,
            ""characterOffsetEnd"": 12,
            ""pos"": ""NNP"",
            ""ner"": ""PERSON"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 3,
            ""word"": ""was"",
            ""originalText"": ""was"",
            ""lemma"": ""be"",
            ""characterOffsetBegin"": 13,
            ""characterOffsetEnd"": 16,
            ""pos"": ""VBD"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 4,
            ""word"": ""President"",
            ""originalText"": ""President"",
            ""lemma"": ""President"",
            ""characterOffsetBegin"": 17,
            ""characterOffsetEnd"": 26,
            ""pos"": ""NNP"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 5,
            ""word"": ""of"",
            ""originalText"": ""of"",
            ""lemma"": ""of"",
            ""characterOffsetBegin"": 27,
            ""characterOffsetEnd"": 29,
            ""pos"": ""IN"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 6,
            ""word"": ""the"",
            ""originalText"": ""the"",
            ""lemma"": ""the"",
            ""characterOffsetBegin"": 30,
            ""characterOffsetEnd"": 33,
            ""pos"": ""DT"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 7,
            ""word"": ""United"",
            ""originalText"": ""United"",
            ""lemma"": ""United"",
            ""characterOffsetBegin"": 34,
            ""characterOffsetEnd"": 40,
            ""pos"": ""NNP"",
            ""ner"": ""LOCATION"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 8,
            ""word"": ""States"",
            ""originalText"": ""States"",
            ""lemma"": ""States"",
            ""characterOffsetBegin"": 41,
            ""characterOffsetEnd"": 47,
            ""pos"": ""NNPS"",
            ""ner"": ""LOCATION"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 9,
            ""word"": ""of"",
            ""originalText"": ""of"",
            ""lemma"": ""of"",
            ""characterOffsetBegin"": 48,
            ""characterOffsetEnd"": 50,
            ""pos"": ""IN"",
            ""ner"": ""LOCATION"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 10,
            ""word"": ""America"",
            ""originalText"": ""America"",
            ""lemma"": ""America"",
            ""characterOffsetBegin"": 51,
            ""characterOffsetEnd"": 58,
            ""pos"": ""NNP"",
            ""ner"": ""LOCATION"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 11,
            ""word"": ""in"",
            ""originalText"": ""in"",
            ""lemma"": ""in"",
            ""characterOffsetBegin"": 59,
            ""characterOffsetEnd"": 61,
            ""pos"": ""IN"",
            ""ner"": ""O"",
            ""before"": "" "",
            ""after"": "" ""
        }, {
            ""index"": 12,
            ""word"": ""2016"",
            ""originalText"": ""2016"",
            ""lemma"": ""2016"",
            ""characterOffsetBegin"": 62,
            ""characterOffsetEnd"": 66,
            ""pos"": ""CD"",
            ""ner"": ""DATE"",
            ""normalizedNER"": ""2016"",
            ""before"": "" "",
            ""after"": """",
            ""timex"": {
                ""tid"": ""t1"",
                ""type"": ""DATE"",
                ""value"": ""2016""
            }
        }]
    }]
}
</code></pre>

<p>Am I doing something wrong? I have Java client code that would at least recognize <code>Barack Obama</code> and <code>United States of America</code> as full NERs, but using the service it seems to treat each token separately. Any ideas why?</p>
","java, nlp, stanford-nlp","<p>You should add the <code>entitymentions</code> annotator to your list of annotators.</p>
",2,1,80,2017-05-15 17:45:29,https://stackoverflow.com/questions/43985744/why-does-stanford-corenlp-server-split-named-entities-into-single-tokens
Stanford CoreNLP depparse throwing OutofMemoryException,"<p>I am using the Stanford CoreNLP (in JAVA) for some Information extraction (using OpenIE annotators). PFB my code -</p>

<pre><code>public void getInformation(String fileName){
    Properties prop = new Properties();
    prop.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, depparse, natlog, openie"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(prop);
    Annotation annotation = new Annotation(IOUtils.slurpFileNoExceptions(fileName));
    pipeline.annotate(annotation);
    pipeline.prettyPrint(annotation, out_data);
    System.out.println(""============================="");
    System.out.println(""The top level annotation"");
    System.out.println(annotation.toString());

    List&lt;CoreMap&gt; sentences = (List&lt;CoreMap&gt;) annotation.get(CoreAnnotations.SentencesAnnotation.class);
    if(sentences!=null &amp;&amp; !sentences.isEmpty())
    {
        CoreMap sentence = sentences.get(0);

        Collection&lt;RelationTriple&gt; triples = sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);

        // Print the triples
        for(RelationTriple triple : triples) 
        {
            System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
        }
    }
}
</code></pre>

<p>But am getting the following error <strong>(java.lang.OutOfMemoryError: Java heap space)</strong> while running my code.</p>

<pre><code>INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
at edu.stanford.nlp.parser.nndep.Classifier.preCompute(Classifier.java:661)
at edu.stanford.nlp.parser.nndep.Classifier.preCompute(Classifier.java:643)
at edu.stanford.nlp.parser.nndep.DependencyParser.initialize(DependencyParser.java:1168)
at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:605)
at edu.stanford.nlp.parser.nndep.DependencyParser.loadFromModelFile(DependencyParser.java:498)
at edu.stanford.nlp.pipeline.DependencyParseAnnotator.&lt;init&gt;(DependencyParseAnnotator.java:57)
at edu.stanford.nlp.pipeline.AnnotatorImplementations.dependencies(AnnotatorImplementations.java:273)
at edu.stanford.nlp.pipeline.AnnotatorFactories$18.create(AnnotatorFactories.java:480)
at edu.stanford.nlp.simple.Document$5.get(Document.java:154)
at edu.stanford.nlp.simple.Document$5.get(Document.java:148)
at edu.stanford.nlp.simple.Document.runDepparse(Document.java:946)
at edu.stanford.nlp.simple.Document.runNatlog(Document.java:966)
at edu.stanford.nlp.simple.Document.runOpenie(Document.java:986)
at edu.stanford.nlp.simple.Sentence.openieTriples(Sentence.java:890)
at edu.stanford.nlp.simple.Sentence.openieTriples(Sentence.java:900)
at com.automatics.nlp.OpenIEDemo.main(OpenIEDemo.java:18)
</code></pre>

<p>How should I overcome this exception?</p>
","java, stanford-nlp","<p>When you run your program you need to give it at least 2 GB of RAM, possibly more depending on what other Stanford CoreNLP annotators you are using.  You should keep adding RAM until the crash goes away.</p>
",0,0,561,2017-05-16 11:13:39,https://stackoverflow.com/questions/43999921/stanford-corenlp-depparse-throwing-outofmemoryexception
CoreNLP Server only on localhost,"<p>When I start the <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">CoreNLP Server</a> on Linux with:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
</code></pre>

<p>It is reachable at <code>http://localhost:9000/</code> (and also via <code>127.0.0.1:9000</code>). However, it is also reachable through my outside IP address, which I do not want.</p>

<p>I tried using the <code>-backends 127.0.0.1:9000</code> option but this does not help.</p>

<p>Is there a way to specify in CoreNLP Server (and not in the OS firewall rules) to only serve on localhost and 127.0.0.1, but not on any other IP address on the system, so that outside connections cannot connect to it?</p>
","java, stanford-nlp","<p>It's not possible to bind to a specific IP address through the command line arguments. In looking at the source code found <a href=""https://github.com/mhewett/stanford-corenlp-node/blob/master/java/src/com/lemlabs/nlp/StanfordCoreNLPServer.java#L161"" rel=""nofollow noreferrer"">here</a> you can see that it doesn't specify a bind address. By default it will bind to all addresses. Only option you have is to run a firewall of some sort and filter/block access to the port from all IP's except local ones.</p>
",1,1,480,2017-05-16 19:24:28,https://stackoverflow.com/questions/44009938/corenlp-server-only-on-localhost
tag word with NLTK stanford NER,"<p>I would like tag only a word and not a sentence with NLTK standford NER.</p>

<p>With <code>stNER.tag(word)</code> I have this output: </p>

<pre><code>[[('o', 'O')], [('u', 'O')], [('r', 'O')]]
</code></pre>

<p>and I want <code>[('Our','O')]</code>.</p>
","python, python-3.x, nltk, stanford-nlp","<p>I believe problem is that you provide <code>word</code> as a <strong>string object not list</strong>. Probably you should pass it like <code>stNER.tag(word.split())</code>. <code>word.split()</code> will return list which is an iterable object that this function requires.</p>

<p>But it's only guess you should provide bigger context (imports, type of variable <code>word</code>)</p>
",1,0,356,2017-05-23 08:46:38,https://stackoverflow.com/questions/44129987/tag-word-with-nltk-stanford-ner
how to calculate the accuracy when working with corenlp,"<p>please let me know if I am unclear,</p>

<p>I found some projects in the GitHub written with scala or java with the aim of getting sentiment of the text using corenlp,</p>

<p>I had already tried other approaches to get the sentiment of the text, the approach was like this,
we had training data, so we trained data and make a model then we could evaluate our model with testing data, so the test data had an accuracy,</p>

<p>with regard to this, why no one is interested in calculating the accuracy of the result when they are working with corenlp?</p>

<p>may I ask you some ideas or approach to finding the accuracy when working with corenlp?</p>

<p>some examples:
<a href=""https://github.com/fmguler/SentimentAnalysis"" rel=""nofollow noreferrer"">sentiment1</a></p>

<p><a href=""https://github.com/peoplehum/coreNLP-Sentiment-Analysis-PredictionIO-Template"" rel=""nofollow noreferrer"">sentiment2</a></p>

<p><a href=""https://github.com/anirudh985/TwitterSentimentAnalysis"" rel=""nofollow noreferrer"">sentiment3</a></p>

<p><a href=""https://github.com/anirudh985/TwitterSentimentAnalysis"" rel=""nofollow noreferrer"">sentiment4</a></p>
","stanford-nlp, sentiment-analysis","<p>After searching I found my answer,</p>

<p>the answere is as simple as this: corenlp is not a library for classification like this,
I mean it reports the analysis of the text.
but we can have both training data and testing data again then evaluate our accuracy. its a bit time consuming but worth it if you need to work with corenlp.</p>

<p>more explanations to how to do it <a href=""https://stackoverflow.com/questions/33712795/questions-about-creating-stanford-corenlp-training-models?rq=1"">link</a></p>
",0,0,398,2017-05-25 00:48:07,https://stackoverflow.com/questions/44170614/how-to-calculate-the-accuracy-when-working-with-corenlp
opennlp vs corenlp : Market reach - popularity,"<p>I am just doing the comparative study of open source NLP tools, and got an idea about the features/services of openNLP and coreNLP engines. In the recent past, I see that no contribution made for openNLP forum, where as coreNLP forum is still going active. So I wanted to understand if stanford:coreNLP has become more popular and been widely used in commercial applications? Anyone has an idea about it?</p>
","parsing, nlp, stanford-nlp, opennlp","<p>Apache OpenNLP is actively developed. Take a look at the commit history [1], there are commits done almost everyday by different contributors and they cut four releases this years (1.7.0, 1.7.1, 1.7.2, and just recently 1.8.0).</p>

<p>OpenNLP is licensed under company friendly Apache License 2.0, compared to CoreNLP which is licensed under GPL which is difficult to use in commercial software (e.g. software being distributed must be released under GPL as well), but they are selling commercial licenses.</p>

<p>OpenNLP is developed mostly by companies which run it in their production systems, where CoreNLP is made by a researchers at Stanford.</p>

<p>CoreNLP has a quite a few dependencies which are pulled into your project, where OpenNLP has zero dependencies.</p>

<p>OpenNLP can support you with the following tasks:</p>

<ul>
<li>Sentence Detection</li>
<li>Tokenization</li>
<li>Chunking</li>
<li>Named Entity Recognition</li>
<li>Pos Tagging</li>
<li>Parsing</li>
<li>Stemming</li>
<li>Language Model</li>
<li>Lemmatization</li>
<li>Document classification</li>
</ul>

<p>OpenNLP is highly customizable, easy to train on user data, has support for training on many publicly available corpora and features built-in evaluation to measure performance of every component.</p>

<p>CoreNLP supports these tasks:</p>

<ul>
<li>Sentence Detection</li>
<li>Tokenization</li>
<li>Named Entity Recognition</li>
<li>Pos Tagging</li>
<li>Parsing (also dependency parsing)</li>
<li>Sentiment</li>
<li>Coreference</li>
<li>Lemmatization</li>
<li>Relation Extraction</li>
</ul>

<p>[1] <a href=""https://github.com/apache/opennlp/commits/master"" rel=""noreferrer"">https://github.com/apache/opennlp/commits/master</a></p>
",8,3,953,2017-05-25 08:26:44,https://stackoverflow.com/questions/44175810/opennlp-vs-corenlp-market-reach-popularity
Stanford NLP Lexparser loadModel(),"<p>I'm currently using the below C# code to create my Lexparser with success:</p>

<pre><code>return LexicalizedParser.loadModel(projectDir + @""StanfordResources/lexparser/englishPCFG.ser.gz"");
</code></pre>

<p>But due to deployment reasons I would rather embed the 'englishPCFG.ser.gz' file as some sort of resource either into the assembly or as a Resource.resx.</p>

<p>So I try to read my byte[] file as so:</p>

<pre><code>ObjectInputStream stream = new ObjectInputStream(new ByteArrayInputStream(Resource.englishPCFG_ser));
        return LexicalizedParser.loadModel(stream);
</code></pre>

<p>But I get the below error:</p>

<pre><code>java.io.StreamCorruptedException: invalid stream header: 1F8B0800
</code></pre>

<p>Is there another way to load this rather than from a file path or am I doing a silly?</p>
","c#, stanford-nlp, java-io, objectinputstream","<p><code>1F8B0800</code> is the GZIP header, which makes sense, given the name of the file you're trying to read. So you need to put <code>java.util.zip.GZIPInputStream</code> between the <code>ByteArrayInputStream</code> and <code>ObjectInputStream</code>:</p>

<pre><code>new ObjectInputStream(new GZIPInputStream(new ByteArrayInputStream(Resource.englishPCFG_ser)))
</code></pre>
",1,0,119,2017-05-25 09:48:36,https://stackoverflow.com/questions/44177375/stanford-nlp-lexparser-loadmodel
how to do classification on the result of stanford-core nlp,"<p>I have a couple of questions about core nlp and doing classification,</p>

<p>firstly I should say that I have read this questions but still, I am confused:</p>

<p><a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool?noredirect=1&amp;lq=1"">link1</a></p>

<p><a href=""https://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">link2</a></p>

<p><a href=""https://stackoverflow.com/questions/32085525/example-for-stanford-nlp-classifier"">link3</a></p>

<p><a href=""https://stackoverflow.com/questions/33712795/questions-about-creating-stanford-corenlp-training-models?rq=1"">link4</a></p>

<p>and some others related to this,</p>

<p>but my confusion:
when I faced with these links, I was happy that I can do something on the result of the corenlp I got to classify them, then gain the accuracy,</p>

<p>my result like other results of corenlp is something like this:</p>

<p><a href=""https://i.sstatic.net/uK7DG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uK7DG.png"" alt=""enter image description here""></a></p>

<p>but now In these links they are talking about doing labling then using stanford-classifier.</p>

<p>so it seems stanford-classifier is something that do classification on the data like other classification methods, and so there is no way to do classification on the result we get of corenl,</p>

<p>may I ask you to critique me, and share your information regarding this</p>

<p>many thanks</p>
","stanford-nlp, sentiment-analysis, text-classification","<p>Actually the right answer is that, YES stanford classifier is a supervised algorithm,</p>

<p>so if anyone want to do classification on the result of corenlp, it needs some coding , like for example I firstly did the corenlp for very negative ones, then I made the document as the text for very negative text,</p>

<p>then I made another document for very positive, and so on,</p>

<p>finally I had for example two document of the result of corenlp for positive and negative,</p>

<p>Then I used those document for the stanford classifier</p>

<p>hope helps somebody which is new to this area</p>
",0,1,503,2017-05-27 03:57:37,https://stackoverflow.com/questions/44212814/how-to-do-classification-on-the-result-of-stanford-core-nlp
Only Get Tokenized Sentences as Output from Stanford Core NLP,"<p>I need to split sentences. I'm using the <code>pycorenlp</code> wrapper for python3. I've started the server from my jar directory using: <code>java -mx4g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000</code></p>
<p>I've run the following commands:</p>
<pre><code>from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')    
text = 'Pusheen and Smitha walked along the beach. Pusheen wanted to surf, but fell off the surfboard.'
output = nlp.annotate(text, properties={'annotators': 'tokenize,ssplit', 'outputFormat': 'text'})
print (output)
</code></pre>
<p>which gave the following output:</p>
<pre><code>Sentence #1 (8 tokens):
Pusheen and Smitha walked along the beach.
[Text=Pusheen CharacterOffsetBegin=0 CharacterOffsetEnd=7]
[Text=and CharacterOffsetBegin=8 CharacterOffsetEnd=11]
[Text=Smitha CharacterOffsetBegin=12 CharacterOffsetEnd=18]
[Text=walked CharacterOffsetBegin=19 CharacterOffsetEnd=25]
[Text=along CharacterOffsetBegin=26 CharacterOffsetEnd=31]
[Text=the CharacterOffsetBegin=32 CharacterOffsetEnd=35]
[Text=beach CharacterOffsetBegin=36 CharacterOffsetEnd=41]
[Text=. CharacterOffsetBegin=41 CharacterOffsetEnd=42]
Sentence #2 (11 tokens):
Pusheen wanted to surf, but fell off the surfboard.
[Text=Pusheen CharacterOffsetBegin=43 CharacterOffsetEnd=50]
[Text=wanted CharacterOffsetBegin=51 CharacterOffsetEnd=57]
[Text=to CharacterOffsetBegin=58 CharacterOffsetEnd=60]
[Text=surf CharacterOffsetBegin=61 CharacterOffsetEnd=65]
[Text=, CharacterOffsetBegin=65 CharacterOffsetEnd=66]
[Text=but CharacterOffsetBegin=67 CharacterOffsetEnd=70]
[Text=fell CharacterOffsetBegin=71 CharacterOffsetEnd=75]
[Text=off CharacterOffsetBegin=76 CharacterOffsetEnd=79]
[Text=the CharacterOffsetBegin=80 CharacterOffsetEnd=83]
[Text=surfboard CharacterOffsetBegin=84 CharacterOffsetEnd=93]
[Text=. CharacterOffsetBegin=93 CharacterOffsetEnd=94]
</code></pre>
<p>I need the output in the following format:</p>
<pre><code>Pusheen and Smitha walked along the beach.
Pusheen wanted to surf, but fell off the surfboard.
</code></pre>
","python, nlp, stanford-nlp, tokenize","<p>Try the <a href=""https://github.com/nltk/nltk/pull/1249"" rel=""nofollow noreferrer"">new ""shiny"" Stanford CoreNLP API in NLTK</a> =)</p>

<p>First:</p>

<pre><code>pip install -U nltk[corenlp]
</code></pre>

<p>On command-line: </p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
</code></pre>

<p>Then in Python, the standard usage is:</p>

<pre><code>&gt;&gt;&gt; from nltk.parse.corenlp import CoreNLPParser
&gt;&gt;&gt; stanford = CoreNLPParser('http://localhost:9000')
&gt;&gt;&gt; text = 'Pusheen and Smitha walked along the beach. Pusheen wanted to surf, but fell off the surfboard.'

# Gets you the tokens.
&gt;&gt;&gt; ' '.join(next(stanford.raw_parse(text)).leaves())
u'Pusheen and Smitha walked along the beach . Pusheen wanted to surf , but fell off the surfboard .'

# Gets you the Tree object.
&gt;&gt;&gt; next(stanford.raw_parse(text))
Tree('ROOT', [Tree('S', [Tree('S', [Tree('NP', [Tree('NNP', ['Pusheen']), Tree('CC', ['and']), Tree('NNP', ['Smitha'])]), Tree('VP', [Tree('VBD', ['walked']), Tree('PP', [Tree('IN', ['along']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['beach'])])])]), Tree('.', ['.'])]), Tree('NP', [Tree('NNP', ['Pusheen'])]), Tree('VP', [Tree('VP', [Tree('VBD', ['wanted']), Tree('PP', [Tree('TO', ['to']), Tree('NP', [Tree('NN', ['surf'])])])]), Tree(',', [',']), Tree('CC', ['but']), Tree('VP', [Tree('VBD', ['fell']), Tree('PRT', [Tree('RP', ['off'])]), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['surfboard'])])])]), Tree('.', ['.'])])])

# Gets you the pretty png tree.
&gt;&gt;&gt; next(stanford.raw_parse(text)).draw()
</code></pre>

<p>[out]:</p>

<p><a href=""https://i.sstatic.net/N2VhR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/N2VhR.png"" alt=""enter image description here""></a></p>

<hr>

<p>To get the tokenized sentence, you'll need some finesse:</p>

<pre><code>&gt;&gt;&gt; from nltk.parse.corenlp import CoreNLPParser
&gt;&gt;&gt; stanford = CoreNLPParser('http://localhost:9000')

# Using the CoreNLPParser.api_call() function, ...
&gt;&gt;&gt; stanford.api_call
&lt;bound method CoreNLPParser.api_call of &lt;nltk.parse.corenlp.CoreNLPParser object at 0x107131b90&gt;&gt;

# ... , you can get the JSON output from the CoreNLP tool.
&gt;&gt;&gt; stanford.api_call(text, properties={'annotators': 'tokenize,ssplit'})
{u'sentences': [{u'tokens': [{u'index': 1, u'word': u'Pusheen', u'after': u' ', u'characterOffsetEnd': 7, u'characterOffsetBegin': 0, u'originalText': u'Pusheen', u'before': u''}, {u'index': 2, u'word': u'and', u'after': u' ', u'characterOffsetEnd': 11, u'characterOffsetBegin': 8, u'originalText': u'and', u'before': u' '}, {u'index': 3, u'word': u'Smitha', u'after': u' ', u'characterOffsetEnd': 18, u'characterOffsetBegin': 12, u'originalText': u'Smitha', u'before': u' '}, {u'index': 4, u'word': u'walked', u'after': u' ', u'characterOffsetEnd': 25, u'characterOffsetBegin': 19, u'originalText': u'walked', u'before': u' '}, {u'index': 5, u'word': u'along', u'after': u' ', u'characterOffsetEnd': 31, u'characterOffsetBegin': 26, u'originalText': u'along', u'before': u' '}, {u'index': 6, u'word': u'the', u'after': u' ', u'characterOffsetEnd': 35, u'characterOffsetBegin': 32, u'originalText': u'the', u'before': u' '}, {u'index': 7, u'word': u'beach', u'after': u'', u'characterOffsetEnd': 41, u'characterOffsetBegin': 36, u'originalText': u'beach', u'before': u' '}, {u'index': 8, u'word': u'.', u'after': u' ', u'characterOffsetEnd': 42, u'characterOffsetBegin': 41, u'originalText': u'.', u'before': u''}], u'index': 0}, {u'tokens': [{u'index': 1, u'word': u'Pusheen', u'after': u' ', u'characterOffsetEnd': 50, u'characterOffsetBegin': 43, u'originalText': u'Pusheen', u'before': u' '}, {u'index': 2, u'word': u'wanted', u'after': u' ', u'characterOffsetEnd': 57, u'characterOffsetBegin': 51, u'originalText': u'wanted', u'before': u' '}, {u'index': 3, u'word': u'to', u'after': u' ', u'characterOffsetEnd': 60, u'characterOffsetBegin': 58, u'originalText': u'to', u'before': u' '}, {u'index': 4, u'word': u'surf', u'after': u'', u'characterOffsetEnd': 65, u'characterOffsetBegin': 61, u'originalText': u'surf', u'before': u' '}, {u'index': 5, u'word': u',', u'after': u' ', u'characterOffsetEnd': 66, u'characterOffsetBegin': 65, u'originalText': u',', u'before': u''}, {u'index': 6, u'word': u'but', u'after': u' ', u'characterOffsetEnd': 70, u'characterOffsetBegin': 67, u'originalText': u'but', u'before': u' '}, {u'index': 7, u'word': u'fell', u'after': u' ', u'characterOffsetEnd': 75, u'characterOffsetBegin': 71, u'originalText': u'fell', u'before': u' '}, {u'index': 8, u'word': u'off', u'after': u' ', u'characterOffsetEnd': 79, u'characterOffsetBegin': 76, u'originalText': u'off', u'before': u' '}, {u'index': 9, u'word': u'the', u'after': u' ', u'characterOffsetEnd': 83, u'characterOffsetBegin': 80, u'originalText': u'the', u'before': u' '}, {u'index': 10, u'word': u'surfboard', u'after': u'', u'characterOffsetEnd': 93, u'characterOffsetBegin': 84, u'originalText': u'surfboard', u'before': u' '}, {u'index': 11, u'word': u'.', u'after': u'', u'characterOffsetEnd': 94, u'characterOffsetBegin': 93, u'originalText': u'.', u'before': u''}], u'index': 1}]} 

&gt;&gt;&gt; output_json = stanford.api_call(text, properties={'annotators': 'tokenize,ssplit'})
&gt;&gt;&gt; len(output_json['sentences'])
2
&gt;&gt;&gt; for sent in output_json['sentences']:
...     start_offset = sent['tokens'][0]['characterOffsetBegin'] # Begin offset of first token.
...     end_offset = sent['tokens'][-1]['characterOffsetEnd'] # End offset of last token.
...     sent_str = text[start_offset:end_offset]
...     print sent_str
... 
Pusheen and Smitha walked along the beach.
Pusheen wanted to surf, but fell off the surfboard.
</code></pre>
",3,3,2261,2017-05-31 18:52:11,https://stackoverflow.com/questions/44292616/only-get-tokenized-sentences-as-output-from-stanford-core-nlp
Processing NLTK Stanford POS Tagger output,"<p>Im working with NLTK Stanford Pos Tagger with my own model to tag line of sentences in text file. The output of my tagger is like :</p>

<pre><code>sentences = [((Word,WordTag),....(Word,WordTag)]
</code></pre>

<p>Im processing Indonesian language, Im doing 2 step after doing Pos tag :</p>

<ol>
<li>Stop Word Removal</li>
<li>Stemming</li>
</ol>

<p>I've got a list of stopword in text file(<code>stopword.txt</code>), and Stem the sentence. 
So far I have done the tagging part. I dont have any ideas how to filter the word in <code>sentences</code>  and remove the word if they in <code>stopword.txt</code> and stem the word in <code>sentences</code></p>

<p>So far, i've tried this code to remove the word but still not removing the word and its word tag: </p>

<pre><code>stopWords = getStopWordList('id_stopword.txt')
filtered_sentences = [w for w in sentences if not w in stopWords]
    filtered_sentences = []
    for w in sentences:
        if w not in stopWords:
            filtered_sentences.append(w)
</code></pre>
","python, nltk, stanford-nlp","<p>I have to assume that your function <code>getStopWordList()</code> correctly returns a list of strings. (Have you verified that?)</p>

<p>The code you posted won't run because it has indentation errors. But the indented bit doesn't matter because you don't need it. It clearly just repeats the logic of the line before it. So I've just ignored it.</p>

<p>To do the filtering you need to change this:</p>

<pre><code>filtered_sentences = [w for w in sentences if not w in stopWords]
</code></pre>

<p>to this:</p>

<pre><code>filtered_sentences = [(w,t) for (w,t) in sentences if not w in stopWords]
</code></pre>
",0,0,480,2017-06-04 09:07:54,https://stackoverflow.com/questions/44352440/processing-nltk-stanford-pos-tagger-output
How to initialize stanfordNLP pipeline once and use many times without initializing again?,"<p>I want to initialize stanfordNLP pipelince once and use it many times without initializing it again, to improve the execution time.</p>

<p>Is it possible?</p>

<p>I have code:</p>

<pre><code>    public static boolean isHeaderMatched(String string) {

    // creates a StanfordCoreNLP object.
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");

    RedwoodConfiguration.current().clear().apply();
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);

    Annotation document = new Annotation(string);

    // use the pipeline to annotate the document we created
    pipeline.annotate(document);
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    CoreMapExpressionExtractor extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, ""./app/utils/Summarizer/mapping/career_objective.rule"", ""./app/utils/Summarizer/mapping/personal_info.rule"", ""./app/utils/Summarizer/mapping/education.rule"", ""./app/utils/Summarizer/mapping/work_experience.rule"", ""./app/utils/Summarizer/mapping/certification.rule"", ""./app/utils/Summarizer/mapping/publication.rule"", ""./app/utils/Summarizer/mapping/award_achievement.rule"", ""./app/utils/Summarizer/mapping/hobbies_interest.rule"", ""./app/utils/Summarizer/mapping/lang_known.rule"", ""./app/utils/Summarizer/mapping/project_details.rule"", ""./app/utils/Summarizer/mapping/skill-set.rule"", ""./app/utils/Summarizer/mapping/misc_header.rule"");

    boolean flag = false;
    for (CoreMap sentence : sentences) {
        List&lt;MatchedExpression&gt; matched = extractor.extractExpressions(sentence);
        //System.out.println(""Probable Header is : "" + matched);
        Set&lt;String&gt; uniqueMatchedKeyWordSet = DocumentParserUtil.removeDuplicate(matched);
        System.out.println(""Matched: "" + uniqueMatchedKeyWordSet + "" and Size of MatchedSet: "" + uniqueMatchedKeyWordSet.size());

        //checked if the more than half the no. of word in header(string) is matched
        if ((matched.size() &gt;= uniqueMatchedKeyWordSet.size()) &amp;&amp; !matched.isEmpty() &amp;&amp; matched.size() &gt;= Math.floorDiv(string.split(""\\s"").length, 2)) {
                //System.out.println(""This is sure a header!"");
            flag = true;
        } else {
            flag = false;
        }
  /*for(MatchedExpression phrase: matched){
    System.out.println(""matched header type: "" + phrase.getValue().get());
  }*/
    }
    return flag;
}
</code></pre>

<p>I want to execute this part of code to be executed only at first call of above method to load the model.</p>

<pre><code>    // creates a StanfordCoreNLP object.
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");

    RedwoodConfiguration.current().clear().apply();
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
</code></pre>

<p>Thanks in advance.</p>
","java, stanford-nlp","<p>The following is an example of what you can do:</p>

<pre><code>public class Example {
    private static StanfordCoreNLP pipeline;
    private static Env env;

    static {
        // creates a StanfordCoreNLP object.
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");

        RedwoodConfiguration.current().clear().apply();
        pipeline = new StanfordCoreNLP(props);

        env = TokenSequencePattern.getNewEnv();
        env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
        env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    }

    public static boolean isHeaderMatched(String string) {
        Annotation document = new Annotation(string);

        // use the pipeline to annotate the document we created
        pipeline.annotate(document);
        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

        CoreMapExpressionExtractor extractor = CoreMapExpressionExtractor.createExtractorFromFiles(env, ""./app/utils/Summarizer/mapping/career_objective.rule"", ""./app/utils/Summarizer/mapping/personal_info.rule"", ""./app/utils/Summarizer/mapping/education.rule"", ""./app/utils/Summarizer/mapping/work_experience.rule"", ""./app/utils/Summarizer/mapping/certification.rule"", ""./app/utils/Summarizer/mapping/publication.rule"", ""./app/utils/Summarizer/mapping/award_achievement.rule"", ""./app/utils/Summarizer/mapping/hobbies_interest.rule"", ""./app/utils/Summarizer/mapping/lang_known.rule"", ""./app/utils/Summarizer/mapping/project_details.rule"", ""./app/utils/Summarizer/mapping/skill-set.rule"", ""./app/utils/Summarizer/mapping/misc_header.rule"");

        boolean flag = false;
        for (CoreMap sentence : sentences) {
            List&lt;MatchedExpression&gt; matched = extractor.extractExpressions(sentence);
            //System.out.println(""Probable Header is : "" + matched);
            Set&lt;String&gt; uniqueMatchedKeyWordSet = DocumentParserUtil.removeDuplicate(matched);
            System.out.println(""Matched: "" + uniqueMatchedKeyWordSet + "" and Size of MatchedSet: "" + uniqueMatchedKeyWordSet.size());

            // checked if the more than half the no. of word in header(string) is matched
            if ((matched.size() &gt;= uniqueMatchedKeyWordSet.size()) &amp;&amp; !matched.isEmpty() &amp;&amp; matched.size() &gt;= Math.floorDiv(string.split(""\\s"").length, 2)) {
                flag = true;
            } else {
                flag = false;
            }

        }

        return flag;
    }

}
</code></pre>

<p>In the above code the <code>static</code> block will be executed when the class is loaded. If you do not wish for this behavior then allow access to an <code>init</code> method, like the following:</p>

<pre><code>public class Example {
    private static StanfordCoreNLP pipeline;
    private static Env env;

    public static init() {
        // creates a StanfordCoreNLP object.
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");

        RedwoodConfiguration.current().clear().apply();
        pipeline = new StanfordCoreNLP(props);

        env = TokenSequencePattern.getNewEnv();
        env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
        env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    }

    public static boolean isHeaderMatched(String string) {
        // code left out for brevity
    }

}
</code></pre>

<p>Which can be called from another class using:</p>

<pre><code>Example.init();
Example.isHeaderMatched(""foobar"");
</code></pre>

<hr>

<p>While writing this answer I noticed a possible flaw in your logic. The following code may not produce the behavior you desire.</p>

<pre><code>boolean flag = false;
for (CoreMap sentence : sentences) {
    List&lt;MatchedExpression&gt; matched = extractor.extractExpressions(sentence);
    //System.out.println(""Probable Header is : "" + matched);
    Set&lt;String&gt; uniqueMatchedKeyWordSet = DocumentParserUtil.removeDuplicate(matched);
    System.out.println(""Matched: "" + uniqueMatchedKeyWordSet + "" and Size of MatchedSet: "" + uniqueMatchedKeyWordSet.size());

    // checked if the more than half the no. of word in header(string) is matched
    if ((matched.size() &gt;= uniqueMatchedKeyWordSet.size()) &amp;&amp; !matched.isEmpty() &amp;&amp; matched.size() &gt;= Math.floorDiv(string.split(""\\s"").length, 2)) {
        flag = true;
    } else {
        flag = false;
    }

}
</code></pre>

<p>You're iterating over every <code>CoreMap</code> in the <code>List&lt;CoreMap&gt;</code> collection <code>sentences</code>. Every iteration you set <code>flag</code> to the result of the conditional, this is where the problem lies. The boolean <code>flag</code> will only reflect the result of the last <code>sentence</code> run through the conditional. If you need to know the result for each <code>sentence</code> then you should have a list of booleans to keep track of the results, otherwise remove the loop and just check the last sentence (because that's what your loop is doing anyways).</p>
",2,0,182,2017-06-05 02:31:20,https://stackoverflow.com/questions/44360925/how-to-initialize-stanfordnlp-pipeline-once-and-use-many-times-without-initializ
How do I specify my own NER classifier in StanfordNLP coreference resolution tagging?,"<p>I have trained my own NER classifier and now I want to use that in doing the coreference resolution. How do I do this? I tried this:
<code>
java -Xmx5g -cp stanford-corenlp-3.7.0.jar:stanford-corenlp-models-3.7.0.jar:* edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ner,coref -loadClassifier ./classifiers/my_classifier.ser.gz -coref.algorithm neural -file my_file.txt
</code>
But it didn't work and error message: 
<code>
java.io.IOException: Unable to open ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"" as class path, filename or URL
</code>
So it seems that it is still not using my NER classifier. </p>
",stanford-nlp,"<p>Suppose I have the custom model stored at:</p>

<pre><code>/Users/stanfordnlphelp/custom_model.ser.gz
</code></pre>

<p>Then you want to add this option:</p>

<pre><code>-ner.model /Users/stanfordnlphelp/custom_model.ser.gz
</code></pre>
",0,0,557,2017-06-05 21:52:04,https://stackoverflow.com/questions/44378274/how-do-i-specify-my-own-ner-classifier-in-stanfordnlp-coreference-resolution-tag
Get all nouns to an array set using core NLP,"<p>I'm a beginner to NLP tools. I need to get all the nouns from below code to an array. How can I do this ?</p>

<pre><code>public String s = ""I like java and python"";

public static Set&lt;String&gt; nounPhrases = new HashSet&lt;&gt;();

public void me() {

    Document doc = new Document("" "" + s);
    for (Sentence sent : doc.sentences()) {

        System.out.println(""The parse of the sentence '"" + sent + ""' is "" + sent.parse());



    }
</code></pre>

<p>This code gives me the below output.</p>

<pre><code>The parse of the sentence 'I like java and python' is (ROOT (S (NP (PRP I)) (VP (VBP like) (NP (NN java) (CC and) (NN python)))))
</code></pre>

<p>I need all </p>

<blockquote>
  <p>NN</p>
</blockquote>

<p>to my array set nounPhrases . How can I do this ?</p>
","java, arrays, loops, set, stanford-nlp","<p>Try this.</p>

<pre><code>Pattern pat = Pattern.compile(""\\(NN\\s+(\\w+)\\)"");
Matcher m = pat.matcher(sent.parse().toString());
while (m.find())
    nounPhrase.add(m.group(1));
System.out.println(nounPhrase);
</code></pre>

<p>result:</p>

<pre><code>[python, java]
</code></pre>
",0,0,91,2017-06-10 12:07:38,https://stackoverflow.com/questions/44473149/get-all-nouns-to-an-array-set-using-core-nlp
"TypeError:DataType float32 for attr &#39;Tindices&#39; not in list of allowed values: int32, int64","<p>I am doing Stanford's CS224n course. I get an error in assignment2  q2_parser_model.py in my dependency parser</p>

<pre><code>== Initializing==

    Loading data... took 2.17 seconds
    Building parser... took 0.04 seconds
    Loading pretrained embeddings... took 2.16 seconds
    Vectorizing data... took 0.06 seconds
    Preprocessing training data...
    1000/1000 [==============================] - 1s     
    Building model...
    Traceback (most recent call last):
      File ""q2_parser_model.py"", line 286, in &lt;module&gt;
        main()
      File ""q2_parser_model.py"", line 252, in main
        model = ParserModel(config, embeddings)
      File ""q2_parser_model.py"", line 237, in __init__
        self.build()
      File ""/home/jarvis/My projects/Machine Learning/CS224n/My assignments/assignment2/model.py"", line 109, in build
        self.pred = self.add_prediction_op()
      File ""q2_parser_model.py"", line 149, in add_prediction_op
        x = self.add_embedding()
      File ""q2_parser_model.py"", line 119, in add_embedding
        features = tf.nn.embedding_lookup(embedding, self.input_placeholder)
      File ""/home/jarvis/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py"", line 110, in embedding_lookup
        validate_indices=validate_indices)
      File ""/home/jarvis/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1293, in gather
        validate_indices=validate_indices, name=name)
      File ""/home/jarvis/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 582, in apply_op
        _Attr(op_def, input_arg.type_attr))
      File ""/home/jarvis/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
        "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
    TypeError: DataType float32 for attr 'Tindices' not in list of allowed values: int32, int64
</code></pre>

<p>Following is the code snippet and line where I am getting error</p>

<pre><code>def add_embedding(self):
        embedding = tf.Variable(self.pretrained_embeddings, name = ""embedding"")
    --&gt; features = tf.nn.embedding_lookup(embedding, self.input_placeholder)
        embeddings = tf.reshape(features, [-1, self.config.n_features * 
        self.config.embedding_size])
        ### END YOUR CODE
        return embeddings
</code></pre>
","python, python-2.7, tensorflow, stanford-nlp","<p>Your <code>self.input_placeholder</code> must be passed to <code>tf.nn.embedding_lookup</code> as an array of <code>int32</code> or <code>int64</code> , so you could just:</p>

<pre><code>features = tf.nn.embedding_lookup(embedding, 
                            np.asarray(self.input_placeholder, dtype=np.int32))
</code></pre>
",2,0,3567,2017-06-11 07:34:10,https://stackoverflow.com/questions/44481437/typeerrordatatype-float32-for-attr-tindices-not-in-list-of-allowed-values-in
Why the two demo of Stanford CoreNLP gives different results?,"<pre><code>Cruella De Vil is a fur-loving maniac in which Disney movie? 
</code></pre>

<p>this sentence gives different results on <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">http://corenlp.run/</a> and <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/corenlp/process</a>. Is there some setting that causes the difference? I am able to get the same results using the latest downloaded model as with <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">http://corenlp.run/</a> but <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/corenlp/process</a> seems to be more accurate for this specific statement.</p>
",stanford-nlp,"<p>The older demo is not really supported any more.  We are generally using <a href=""http://corenlp.run"" rel=""nofollow noreferrer"">http://corenlp.run</a> .  In fact, we may shut down the older demos.  </p>

<p>The two demos use different models.  If you want to try to match the results of the older demo, you'll probably have to search through the history of releases and look at older NER models for releases such as 3.5.1 etc...Ultimately we believe the current model is better, but in individual cases an older model can perform better.</p>
",1,0,295,2017-06-13 00:17:05,https://stackoverflow.com/questions/44510529/why-the-two-demo-of-stanford-corenlp-gives-different-results
&quot;Enhancing&quot; CoreNLP Sentiment Analysis Results,"<p>I am trying to perform sentiment analysis on a large number of product reviews using CoreNLP (Java).  Overall, I find the accuracy of the analysis to be pretty good.  From what I read, the model I'm using was initially created using movie reviews (I think), so it's not 100% suited for analyzing product reviews.  I was wondering the best way to go about ""enhancing"" the accuracy of my analysis.</p>

<p>The main thing I was thinking about was that in addition to the text of the product review, I also have a user-provided star rating.  The values range from 1-5, 1 star being the lowest.  I was hoping there was a way to take the star rating into account when generating the sentiment score, since it more accurately reflects the users' feelings on a particular product.  Is there a way I can best have the star rating factor in to the sentiment analysis scoring in CoreNLP?  My analysis code looks something like this:</p>

<pre><code>List&lt;ProductReview&gt; reviews = this.reviewRepository.findAll();
        for (ProductReview review : reviews) {
            Properties props = new Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment"");
            props.put(""ner.model"", ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"");

            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

            int starRating = review.getStarRating();
            String reviewText = review.getTitle() + "" : "" + review.getReviewText();
            if (!StringUtils.isEmpty(reviewText)) {
                int longest = 0;
                int mainSentiment = 0;
                Annotation annotation = pipeline.process(reviewText);
                String sentimentStr = null;
                List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
                for (CoreMap sentence : sentences) {
                    Tree sentimentTree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);

                    int sentiment = RNNCoreAnnotations.getPredictedClass(sentimentTree) - 2;
                    String partText = sentence.toString();
                    if (partText.length() &gt; longest) {
                        mainSentiment = sentiment;
                        sentimentStr = sentence.get(SentimentCoreAnnotations.SentimentClass.class);

                        longest = partText.length();
                    }
                }
            }
        }
</code></pre>

<p>How could I best incorporate the star ratings (or other info, such as votes on the most useful product reviews, etc.) into the analysis being performed by CoreNLP?  Is this something I would have to do separately?  Or is there a way to incorporate the additional data directly into the sentiment analysis engine?</p>
","java, stanford-nlp, sentiment-analysis","<p>There are a few enhancements possible.</p>

<p>/1. <strong>Improvised training set and contextual sentiment analysis</strong>:
Some features might get classified as positive in a movie review context, but might be negative in product review context. You shall retrain your data on your context. Method specified <a href=""https://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow noreferrer"">here</a></p>

<blockquote>
  <p>Models can be retrained using the following command using the PTB
  format dataset:</p>
  
  <p>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25
  -trainPath train.txt -devPath dev.txt -train -model model.ser.gz</p>
</blockquote>

<p>A good discussion on training dataset can be found <a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool"">here</a>.</p>

<p>/2.<strong>Getting the contextual training and testing data :</strong>  Your product reviews data can act as training set as well as testing set. Select the reviews with extreme polarities ( 1 star POOREST, and 5 star GREAT ) as your training data, to improvide further on the content, you can select 1 and 5 star reviews which have been marked as helpful by the community.
Using this data generated your PTB dataset classifying the reviews as POSITIVE and NEGATIVE ( Neutral would be a hard thing to achieve by using 2-3-4 star rated reviews, as they can introduce noise ).</p>

<p>/3. Use 80% of your dataset as training set, and 20% as testing set. The 1 star rated reviews shall mostly get classified as NEGATIVE and 5 star shall mostly get classified as positive.
Post this, you can use the trained model to analyze sentiment of other reviews, <strong>your sentiment score</strong> ( say 0 for negative sentiment, and 5 for very positive sentiment,  or -1 for negative to +1 for very positive) <strong>will have a positive correlation with actual star rating</strong> provided along with that review. If there is a <strong>sentiment disparity</strong>, e.g. a text review comes out as having positive sentiment, but has 1 star rating, you may want to log such cases, and improvise your classification.</p>

<p>/4. <strong>Improvising using other data sources and classifiers</strong>:  <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">Vader sentiment</a> (in python ) is a very good classifier specially attuned for social media and things like product reviews. You may or may not chose to use it as a comparative classifier ( to cross match or have double set of your results, from corenlp+vader), but you can surely use its amazon reviews dataset as mentioned <a href=""https://github.com/cjhutto/vaderSentiment#resources-and-dataset-descriptions"" rel=""nofollow noreferrer"">here</a>:</p>

<blockquote>
  <p>amazonReviewSnippets_GroundTruth.txt FORMAT: the file is tab delimited
  with ID, MEAN-SENTIMENT-RATING, and TEXT-SNIPPET</p>
  
  <p>DESCRIPTION: includes 3,708 sentence-level snippets from 309 customer
  reviews on 5 different products. The reviews were originally used in
  Hu &amp; Liu (2004); we added sentiment intensity ratings. The ID and
  MEAN-SENTIMENT-RATING correspond to the raw sentiment rating data
  provided in 'amazonReviewSnippets_anonDataRatings.txt' (described
  below).</p>
  
  <p>amazonReviewSnippets_anonDataRatings.txt FORMAT: the file is tab
  delimited with ID, MEAN-SENTIMENT-RATING, STANDARD DEVIATION, and
  RAW-SENTIMENT-RATINGS</p>
  
  <p>DESCRIPTION: Sentiment ratings from a minimum of 20 independent human
  raters (all pre-screened, trained, and quality checked for optimal
  inter-rater reliability).</p>
</blockquote>

<p>The datasets are available in the tgz file here:
<a href=""https://github.com/cjhutto/vaderSentiment/blob/master/additional_resources/hutto_ICWSM_2014.tar.gz"" rel=""nofollow noreferrer"">https://github.com/cjhutto/vaderSentiment/blob/master/additional_resources/hutto_ICWSM_2014.tar.gz</a></p>

<p>It follows the pattern <code>reviewindex_part  polarity  review_snippet</code></p>

<pre><code>1_19    -0.65   the button was probably accidentally pushed to cause the black screen in the first place.
1_20    2.85    but, if you're looking for my opinion of the apex dvd player, i love it!
1_21    1.75    it practically plays almost everything you give it.
</code></pre>
",4,5,1115,2017-06-14 19:28:08,https://stackoverflow.com/questions/44553166/enhancing-corenlp-sentiment-analysis-results
Understanding Stanford CoreNLP TokensRegex syntax for arbitrary phrase matching,"<p>So I've been working with NLP recently and I'm having trouble using their regex syntax for anything other than extremely simple matches like /test/. The end game I'm searching for is to match phrases that contain specific words somewhere in their contents. The speech patterns I'm trying to model are very structured but, because human speech is variable, the phrases could start a myriad of ways but as long as the phrase contains certain key words I want to count it. So as an example a regex I'm trying to model in TokensRegex is:</p>

<pre><code>.*(show).*(cars).*(\d{0,9})(km|mi).*
</code></pre>

<p>which would match a phrase like:</p>

<p><code>""please show me all cars within 100 km of me""</code></p>

<p>Its not apparent to me from their documentation that a phrase that complex can be constructed. If it can I'm having trouble seeing how to convert it to their syntax. The closest I've come to what seems to match is this:</p>

<pre><code>""[]*/(show)/[]*/(cars)/[]*[word&gt;=0]/(kilometer|miles)/[]""
</code></pre>

<p>this a snip of the code I'm using to check for matches:</p>

<pre><code>//value is the regex, tokens is the List&lt;CoreLabel&gt;s of text to try and match
pattern = TokenSequencePattern.compile(value);
TokenSequenceMatcher matcher = pattern.getMatcher(tokens);
while (matcher.find()) {
    String matchedString = matcher.group();
    System.out.println(matchedString);
    return true;
}
</code></pre>

<p>When I debug it I see the elements inside the matcher is a list of CoreLabels</p>

<pre><code>(show,me,all,cars,within,a,hundred,kilometers,of,me) 
</code></pre>

<p>and the pattern is compiled into a list of SequencePattern</p>

<pre><code> (*,TextAnnotation:/show/,*,TextAnnotation:/cars/,*,TextAnnotation GE 0.0, TextAnnotation:/(kilometer[s]?|mile[s]?)/,*)
</code></pre>

<p>To me that seems like it should match but it doesn't. Even something as stripped down as:</p>

<pre><code>show me all cars
</code></pre>

<p>with the regex:</p>

<pre><code>[]/show/[]/cars/[]
</code></pre>

<p>doesn't match so it makes me lean towards not setting up the regex right. Is there something I'm not understand about the limitations of TokensRegex or am I not constructing the expressions correctly. Any help would be much appreciated, thank you!</p>
","nlp, stanford-nlp","<p>It turns out I didn't need TokensRegex directly for my problem. The key thing I was trying to solve was picking out numbers in a phrase and convert them but I realized that I could use CoreNLP's NERClassifierCombiner to pick them ouy,replace them, and use plain regular expressions to match the updated input phrase. Example of what I did is below, for phrases like ""Show me all cars within fifteen kilometers"" this converts it to ""Show me all cars within 15 kilometers"":</p>

<pre><code>        Sentence sentence = new Sentence(eventName);
        String serializedClassifier = ""english.muc.7class.distsim.crf.ser.gz"";
        NERClassifierCombiner combiner = null;
        try {
            combiner = new NERClassifierCombiner(serializedClassifier);
        } catch (IOException e) {
            e.printStackTrace();
        }
        List&lt;String&gt; reconstructedEventTokens = new ArrayList&lt;&gt;();
        for (CoreLabel cl : combiner.classify(sentence.asCoreLabels())) {
            if(cl.ner() == KBPRelationExtractor.NERTag.NUMBER.name){
                reconstructedEventTokens.add(cl.get(CoreAnnotations.NumericCompositeValueAnnotation.class).toString());
            }else{
                reconstructedEventTokens.add(cl.originalText());
            }
        }
        String newEvent = String.join("" "",reconstructedEventTokens);
        System.out.println(""matching phrase to check: ""+newEvent);
        Pattern pattern = Pattern.compile(value);
        Matcher matcher = pattern.matcher(newEvent);
</code></pre>

<p>Took some more digging into the library to find the NER toolkit but its working like a charm now! Hope this helps someone else that's trying to find numbers or other entities in their phrases.</p>
",0,0,822,2017-06-17 02:38:06,https://stackoverflow.com/questions/44600257/understanding-stanford-corenlp-tokensregex-syntax-for-arbitrary-phrase-matching
Wikipedia entity annotator not working in Stanford coreNLP,"<p>I have setup a local server of Stanford coreNLP library on my system. Although, I am able to get the responses as one can get on the <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">demo site</a>. But in my server <code>wikipedia entity</code> option is not working. The server gives the following error:</p>

<pre><code>java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""edu/stanford/nlp/models/kbp/wikidict.tab.gz"" as class path, filename or URL
        at edu.stanford.nlp.pipeline.WikidictAnnotator.&lt;init&gt;(WikidictAnnotator.java:81)
        at edu.stanford.nlp.pipeline.AnnotatorImplementations.link(AnnotatorImplementations.java:296)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$63(StanfordCoreNLP.java:517)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getDefaultAnnotatorPool$65(StanfordCoreNLP.java:533)
        at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:118)
        at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:146)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:447)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:146)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:133)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.mkStanfordCoreNLP(StanfordCoreNLPServer.java:319)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$500(StanfordCoreNLPServer.java:50)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:642)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:748)
Caused by: edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""edu/stanford/nlp/models/kbp/wikidict.tab.gz"" as class path, filename or URL
        at edu.stanford.nlp.io.IOUtils$GetLinesIterable$1.getReader(IOUtils.java:802)
        at edu.stanford.nlp.io.IOUtils$GetLinesIterable$1.&lt;init&gt;(IOUtils.java:760)
        at edu.stanford.nlp.io.IOUtils$GetLinesIterable.iterator(IOUtils.java:758)
        at edu.stanford.nlp.pipeline.WikidictAnnotator.&lt;init&gt;(WikidictAnnotator.java:58)
        ... 22 more
Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/kbp/wikidict.tab.gz"" as class path, filename or URL
        at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
        at edu.stanford.nlp.io.IOUtils$GetLinesIterable.getStream(IOUtils.java:750)
        at edu.stanford.nlp.io.IOUtils$GetLinesIterable.access$000(IOUtils.java:719)
        at edu.stanford.nlp.io.IOUtils$GetLinesIterable$1.getReader(IOUtils.java:792)
        ... 25 more
</code></pre>

<p>I have downloaded <code>wikidict.tab.gz</code> file, but I am unable to understand how to use it.</p>
","nlp, stanford-nlp","<p>@sv_jan5's answer will work fine, but perhaps easier is downloading and including in your classpath the <code>English (KBP)</code> models jar from the download page (<a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a>). The naming is somewhat cryptic -- KBP stands for ""Knowledge Base Population,"" which subsumed the entity linking models and the new relation extraction models.</p>

<p>Direct link is <a href=""http://nlp.stanford.edu/software/stanford-english-kbp-corenlp-2017-06-09-models.jar"" rel=""nofollow noreferrer"">here</a> for the 3.8.0 version (warning: 500MB download).</p>
",4,1,662,2017-06-20 03:47:35,https://stackoverflow.com/questions/44643501/wikipedia-entity-annotator-not-working-in-stanford-corenlp
Extract Entities and Relationships,"<p>Given text documents (student essays with about 100 words per essay) I want to extract entities and relationships important to the context of the sentence (maybe by considering <strong>Noun Phrase</strong> and <strong>Verb Phrase</strong>) to automatically score the answer. </p>

<p>Are there any popular algorithms/tools that I can use to perform this task?</p>
","python, nlp, stanford-nlp, opennlp","<p>It would be helpful if you could be more specific, but in general this problem is known as <strong>Information Extraction</strong>. One example software package that deals with it is Standford NLP's <a href=""https://nlp.stanford.edu/software/openie.html"" rel=""nofollow noreferrer"">open information extraction</a> system. Example use:</p>

<p><a href=""https://i.sstatic.net/CLG6C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CLG6C.png"" alt=""enter image description here""></a></p>
",1,0,586,2017-06-26 01:15:54,https://stackoverflow.com/questions/44752007/extract-entities-and-relationships
Specific Part of Speech labels for Java Stanford NLP,"<p>What are the set of PoS labels produced by Standford NLP (including PoS for punctuation tokens), and its description?</p>

<p>I know this question has been asked several times, such as in:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels"">Java Stanford NLP: Part of Speech labels?</a></li>
<li><a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow noreferrer"">http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a></li>
<li><a href=""http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf"" rel=""nofollow noreferrer"">http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf</a></li>
</ul>

<p>but those answers list some typical PoS labels which are <strong>not</strong> specific to Standfor NLP. For instance, none of those answers list the <code>-LRB-</code> PoS label used by Stanford NKLP for the <code>(</code> punctuation.</p>

<p>Where can I find this list of PoS labels in the source code of the Stanford NLP?</p>

<p>Also, what are some token examples annotated with the <code>SYM</code> PoS label?</p>

<p>Also, how to know if a token is a punctuation?
<a href=""http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf"" rel=""nofollow noreferrer"">Here</a> they define <code>isPunctation == true if its PoS is :|,|.|“|”|-LRB-|-RRB-|HYPH|NFP|SYM|PUNC</code>. However Stanford NLP does not have all these PoS.</p>
",stanford-nlp,"<p>It is the Penn Treebank POS set, but many descriptions of this tag set seem to omit punctuation marks. Here is a complete list of tags:</p>

<p><a href=""https://www.eecis.udel.edu/~vijay/cis889/ie/pos-set.pdf"" rel=""nofollow noreferrer"">https://www.eecis.udel.edu/~vijay/cis889/ie/pos-set.pdf</a></p>

<p>(But parentheses are tagged as -LRB- and -RRB-, not sure why they don't mention this in the documentation.)</p>
",4,2,1088,2017-06-26 15:49:41,https://stackoverflow.com/questions/44763607/specific-part-of-speech-labels-for-java-stanford-nlp
python 3.5 nltk Stanford segmenter windows 10,"<p>I have setup the environment variables as instructed by the official NLTK wiki. I run into the following error in my very first example. Here are the codes:</p>

<pre><code>from nltk.tokenize import StanfordSegmenter
datapath = ""D:/Coding/stanford-segmenter/""
corporadict = datapath+""data/""
modelpath = datapath + ""data/pku.gz""
dictpath = datapath + ""data/dict-chris6.ser.gz""
segmenter = StanfordSegmenter(path_to_sihan_corpora_dict=corporadict,path_to_model=modelpath,path_to_dict=dictpath)
res = segmenter.segment(u""这是斯坦福中文分词器"")
</code></pre>

<p>but Python gives me the following error. 
Traceback (most recent call last):</p>

<pre><code>File ""D:/Video data/data_processed/ugctext/test_stanford.py"", line 19, in &lt;module&gt;
res = segmenter.segment(u""这是斯坦福中文分词器"")
File ""C:\Python35\lib\site-packages\nltk\tokenize\stanford_segmenter.py"", line 164, in segment
return self.segment_sents([tokens])
File ""C:\Python35\lib\site-packages\nltk\tokenize\stanford_segmenter.py"", line 192, in segment_sents
stdout = self._execute(cmd)
File ""C:\Python35\lib\site-packages\nltk\tokenize\stanford_segmenter.py"", line 211, in _execute
stdout, _stderr = java(cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)
File ""C:\Python35\lib\site-packages\nltk\internals.py"", line 129, in java
p = subprocess.Popen(cmd, stdin=stdin, stdout=stdout, stderr=stderr)
File ""C:\Python35\lib\subprocess.py"", line 947, in __init__
restore_signals, start_new_session)
File ""C:\Python35\lib\subprocess.py"", line 1198, in _execute_child
args = list2cmdline(args)
File ""C:\Python35\lib\subprocess.py"", line 751, in list2cmdline
needquote = ("" "" in arg) or (""\t"" in arg) or not arg
TypeError: argument of type 'NoneType' is not iterable
</code></pre>

<p>can anyone help me with this issue? Thanks!</p>
","nltk, stanford-nlp","<p>For some reason <code>list2cmdline(args)</code> in <code>subprocess.py</code> is returning <code>[None]</code>, and it is not being handled properly. I would guess that it's a problem with the <code>java()</code> call in <code>stanford_segmenter.py</code>. </p>

<p>From <a href=""https://nlp.stanford.edu/software/segmenter.html"" rel=""nofollow noreferrer"">here</a> you can see that the code was updated to require Java 8 back in 2014. If your Java version is lower than this, it could be the problem.</p>
",0,0,403,2017-06-28 04:32:24,https://stackoverflow.com/questions/44793697/python-3-5-nltk-stanford-segmenter-windows-10
cannot unzipp the stanford corenlp files,"<p>i downloaded the stanford corenlp archive from <a href=""http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip"" rel=""nofollow noreferrer"">stanford corenlp full</a></p>

<p>using uGet <a href=""https://i.sstatic.net/DnDaQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DnDaQ.png"" alt=""downloading ""></a> it is full downloaded but when i try to extract it it shows me  <strong>An error occurred while extracting files</strong>
i tried to unzipped from the terminal and it doesn't work also </p>

<p>help please</p>
","ubuntu, zip, stanford-nlp, unzip","<p>You may need to be more specific about the error. Have you tried another program to unzip it? Have you tried re-downloading the file? The md5 I have for the file on my computer is: <code>5134febe18bdbd830f3e8d5aff024b19</code>. If this doesn't match, then perhaps you have a corrupted version of the file.</p>
",1,0,86,2017-06-28 11:18:21,https://stackoverflow.com/questions/44800954/cannot-unzipp-the-stanford-corenlp-files
Stanford Classifier: What are non ngram activeFeatures used to determine scoreOf Datum?,"<p>I have number of classifiers to determine whether event descriptions fall into certain categories, i.e. a rock concert, a jazz evening, classical music, etc or not. I have created a servlet which uses the LinearClassifier scoresOf function to return a score for the event description's datum.</p>

<p>In order to look at cases which return unexpected results, I adapted the scoreOf function (public Counter scoresOf(Datum example)) in order to get an array of the individual features and their scores, so I could understand how the final score was arrived at. This works for the most part, i.e. I mostly have lines like:-<br>
1-#-jazz    -0.6317620789568879<br>
1-#-saxo    -0.2449097451977173  </p>

<p>as I'd expect. However I also have a couple, which I don't understand:-<br>
CLASS   1.4064007882810108<br>
1-Len-31-Inf    0.4569598446321162  </p>

<p>Can anybody please help by explaining what these are and how these scores are determined? (I really thought I was just working on a score built up from the weighted components of my description string).</p>

<p>(I appreciate that ""CLASS"" &amp; ""Len-xx"" are set as properties for the classifier, I just don't understand why they then show up as scored elements in their own right)</p>
",stanford-nlp,"<p>For what you want for seeing feature weights, you might also look at LinearClassifier's <code>justificationOf()</code>. I think it's the same as what you've been writing....</p>

<p>For the questions: </p>

<p>The CLASS feature acts as a class prior or bias term. It will have a more positive weight to the extent that the class is more common in the data overall. You will get this feature iff you use the <code>useClassFeature</code> property. But it's generally a good idea to have it.</p>

<p>The 1-Len feature looks at the length of the String that is column 1.  31-Inf is a length of over 30. This will again have weights as to whether such a length is indicative or not of a particular class. This is employed iff you use the <code>binnedLengths</code> feature. This is useful only if there is some general correlation between field length and the target class.</p>
",0,0,63,2017-06-30 08:36:57,https://stackoverflow.com/questions/44842149/stanford-classifier-what-are-non-ngram-activefeatures-used-to-determine-scoreof
NLTK tokenizer and Stanford corenlp tokenizer cannot distinct 2 sentences without space at period (.),"<p>I have 2 sentences in my dataset:</p>

<p>w1 = I am Pusheen the cat.I am so cute. # no space after period <br>
w2 = I am Pusheen the cat. I am so cute. # with space after period</p>

<p>When I use NKTL tokenizer (both word and sent), nltk cannot distinct the between cat.I.</p>

<p>Here is word tokenize</p>

<pre><code>&gt;&gt;&gt; nltk.word_tokenize(w1, 'english')
['I', 'am', 'Pusheen', 'the', 'cat.I', 'am', 'so', 'cute']
&gt;&gt;&gt; nltk.word_tokenize(w2, 'english')
['I', 'am', 'Pusheen', 'the', 'cat', '.', 'I', 'am', 'so', 'cute']
</code></pre>

<p>and sent tokenize</p>

<pre><code>&gt;&gt;&gt; nltk.sent_tokenize(w1, 'english')
['I am Pusheen the cat.I am so cute']
&gt;&gt;&gt; nltk.sent_tokenize(w2, 'english')
['I am Pusheen the cat.', 'I am so cute']
</code></pre>

<p>I would like to ask how to fix that ? i.e: make nlkt detect as w2 while in my dataset, sometime word and punctuation are stick together.</p>

<p>Update: 
Tried Stanford CoreNLP 3.7.0, they also cannot distinct 'cat.I' as 'cat', '.', 'I'</p>

<pre><code>meow@meow-server:~/projects/stanfordcorenlp$ java edu.stanford.nlp.process.PTBTokenizer sample.txt
I
am
Pusheen
the
cat.I
am
so
cute
.
PTBTokenizer tokenized 9 tokens at 111.21 tokens per second.
</code></pre>
","python, nlp, nltk, stanford-nlp, tokenize","<p>It's implemented this way on purpose -- a period with no space after it usually doesn't signify the end of a sentence (think about the periods in phrases such as &quot;version 4.3&quot;, &quot;i.e.&quot;, &quot;A.M.&quot;, etc.). If you have a corpus in which ends of sentences with no space after the full stop is a common occurrence, you'll have to preprocess the text with a regular expression or some such before sending it to NLTK.</p>
<p>A good rule-of-thumb might be that usually a lowercase letter followed by a period followed by an uppercase letter usually signifies the end of a sentence. To insert a space after the period in such cases, you could use a regular expression, e.g.</p>
<pre><code>import re
w1 = re.sub(r'([a-z])\.([A-Z])', r'\1. \2', w1)
</code></pre>
<p>where</p>
<ul>
<li><code>[a-z]</code> matches any lowercase character</li>
<li><code>\\.</code> matches the full stop</li>
<li><code>[A-Z]</code> matches any uppercase character</li>
<li><code>\1</code> is a <a href=""https://stackoverflow.com/questions/20802056/python-regular-expression-1/20802130"">reference to the first group</a> in (parentheses)</li>
<li><code>\2</code> is a reference to the second group in (parentheses)</li>
</ul>
",8,5,2144,2017-07-01 08:04:17,https://stackoverflow.com/questions/44858741/nltk-tokenizer-and-stanford-corenlp-tokenizer-cannot-distinct-2-sentences-withou
regex for pos tagged text for extract location,"<p>i am using stanford pos tagger for tagging tweet content. i need to extract locations like ""the golden gate bridge"" or ""tiburon blvd"", etc. i have some rules for detecting location, that are:</p>

<pre><code>1.&lt;NN&gt;+ 
2.&lt;DT&gt;?&lt;JJ&gt;?&lt;1&gt; ----&gt; &lt;1&gt;:it means the rule number 1
3.&lt;CD&gt;?&lt;2&gt;
4.&lt;2&gt; &lt;CD&gt;?
5.(3|4) &lt;CC | PE&gt; (3|4)
</code></pre>

<p>The “+” sign indicates the presence of a tag at least one or
more times, the “?” sign indicates the presence of a tag zero or
one time, and the “|” sign indicates the presence of one of the
two tags. 
and we have Nouns (NN), Determiners
(DT), Adjectives (JJ), Cardinal Numbers (CD), Conjunctions
(CC), and Possessive Endings (PE) from pos tagger.</p>

<p>A sample pos tagged sentence: ""this/DT overturned/VBN tanker/NN in/IN marin/NN has/VBZ created/VBN a/DT huge/JJ jam/NN on/IN wb/NN 580/CD clear/JJ across/IN the/DT richmond/JJ san/NN rafael/NN bridge/NN &amp;/CC Four/CD""</p>

<p>can any one help me with creating regex for each rule in java?</p>
","java, regex, pattern-matching, stanford-nlp, pos-tagger","<p>While there probably are better / more efficient solutions, this should work:</p>

<pre><code>1. (\w+/NN)(\s(\w+/NN))*
2. (\w+/DT\s)?(\w+/JJ\s)?(\w+/NN)(\s(\w+/NN))*
3. (\w+/CD\s)?(\w+/DT\s)?(\w+/JJ\s)?(\w+/NN)(\s(\w+/NN))*
4. (\w+/DT\s)?(\w+/JJ\s)?(\w+/NN)(\s(\w+/NN))*(\s\w+/CD)?
5. ((&lt;3&gt;)|(&lt;4&gt;))\s((\w+/CC)|(\w+/PE))\s((&lt;3&gt;)|(&lt;4&gt;))
</code></pre>

<p>A little explanation:
<code>(\w+/NN)</code> extracts any combination of characters (at least one) followed by a forward slash and ""NN"" (e.g. tanker/NN). If we combine that with a whitespace (\s), we can extract any arbitrary number of consecutive nouns.</p>

<p>For the second rule we add two optional parts which extract "".../DT"" and "".../JJ"" and combine it with rule one. Rule three just adds another optional part which extracts "".../CD"" and combine it with rule two. For rule four, we append an optional "".../CD"" to rule two. </p>

<p>For the fifth rule you have to replace <code>&lt;3&gt;</code> and <code>&lt;4&gt;</code> with the respective rule. Otherwise it would look a bit complex. The middle part extracts "".../CC"" or "".../PE"" and we append and prepend a pattern to match either rule three or rule four.</p>
",0,1,577,2017-07-04 11:08:07,https://stackoverflow.com/questions/44904358/regex-for-pos-tagged-text-for-extract-location
Stanford CoreNLP BasicPipelineExample doesn&#39;t work,"<p>I'm trying to get started with Stanford CoreNLP and can't even get past the very first simple example from here.</p>

<blockquote>
  <p><a href=""https://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/api.html</a></p>
</blockquote>

<p>Here is my code:</p>

<pre><code>package stanford.corenlp;

import java.io.File;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import com.google.common.io.Files;

import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.util.CoreMap;
import java.util.logging.Level;
import java.util.logging.Logger;

    private void test2() {
        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""Now is the time for all good men to come to the aid of their country."";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

    }

  public static void main(String[] args) throws IOException {
      StanfordNLP nlp = new StanfordNLP();
      nlp.test2();
  }

}
</code></pre>

<p>Here is the stacktrace:</p>

<pre><code>Adding annotator tokenize
No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator pos
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:791)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:312)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:265)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:85)
    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:73)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.posTagger(AnnotatorImplementations.java:55)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$42(StanfordCoreNLP.java:496)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getDefaultAnnotatorPool$65(StanfordCoreNLP.java:533)
    at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:118)
    at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:146)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:447)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:146)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:133)
    at stanford.corenlp.StanfordNLP.test2(StanfordNLP.java:93)
    at stanford.corenlp.StanfordNLP.main(StanfordNLP.java:108)
Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:789)
    ... 16 more
C:\Users\Greg\AppData\Local\NetBeans\Cache\8.2\executor-snippets\run.xml:53: Java returned: 1
BUILD FAILED (total time: 0 seconds)
</code></pre>

<p>What am I missing?</p>
",stanford-nlp,"<p>First of all you need to add to the class path stanford-corenlp-3.8.0.jar.  That makes the red error marks in NetBeans go away.  But you also need to add stanford-corenlp-3.8.0-models.jar to the class path to prevent the error I documented.  Adding the folder it resides in to the classpath doesn't work.  Details like this should never be left out of beginner documentation!</p>

<p>Now if you continue with the example and add in the new stuff, more errors occur.  For example, the code will then look like:</p>

<pre><code>package stanford.corenlp;

    import java.io.File;
    import java.io.IOException;
    import java.nio.charset.Charset;
    import java.util.List;
    import java.util.Map;
    import java.util.Properties;

    import com.google.common.io.Files;

    import edu.stanford.nlp.dcoref.CorefChain;
    import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
    import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
    import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
    import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
    import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
    import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
    import edu.stanford.nlp.ling.CoreLabel;
    import edu.stanford.nlp.pipeline.Annotation;
    import edu.stanford.nlp.pipeline.StanfordCoreNLP;
    import edu.stanford.nlp.semgraph.SemanticGraph;
    import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
    import edu.stanford.nlp.trees.Tree;
    import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
    import edu.stanford.nlp.util.CoreMap;
    import edu.stanford.nlp.util.PropertiesUtils;
    import java.util.logging.Level;
    import java.util.logging.Logger;

        private void test2() {
            // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
            Properties props = new Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(
            PropertiesUtils.asProperties(
                ""annotators"", ""tokenize,ssplit,pos,lemma,parse,natlog"",
                ""ssplit.isOneSentence"", ""true"",
                ""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"",
                ""tokenize.language"", ""en""));

            // read some text in the text variable
            String text = ""Now is the time for all good men to come to the aid of their country."";

            // create an empty Annotation just with the given text
            Annotation document = new Annotation(text);

            // run all Annotators on this text
            pipeline.annotate(document);

            // these are all the sentences in this document
            // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
            List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

            for (CoreMap sentence: sentences) {
                // traversing the words in the current sentence
                // a CoreLabel is a CoreMap with additional token-specific methods
                for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
                    // this is the text of the token
                    String word = token.get(TextAnnotation.class);
                    // this is the POS tag of the token
                    String pos = token.get(PartOfSpeechAnnotation.class);
                    // this is the NER label of the token
                    String ne = token.get(NamedEntityTagAnnotation.class);

                    System.out.println(""word=""+word +"", pos=""+pos +"", ne=""+ne);
                }

                // this is the parse tree of the current sentence
                Tree tree = sentence.get(TreeAnnotation.class);

                // this is the Stanford dependency graph of the current sentence
                SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
            }

            // This is the coreference link graph
            // Each chain stores a set of mentions that link to each other,
            // along with a method for getting the most representative mention
            // Both sentence and token offsets start at 1!
            Map&lt;Integer, CorefChain&gt; graph = 
                document.get(CorefChainAnnotation.class);
        }

      public static void main(String[] args) throws IOException {
          StanfordNLP nlp = new StanfordNLP();
          nlp.test2();
      }

    }
</code></pre>

<p>And the stack trace becomes:</p>

<pre><code>run:
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.6 sec].
Adding annotator lemma
Adding annotator parse
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"" as class path, filename or URL
    at edu.stanford.nlp.parser.common.ParserGrammar.loadModel(ParserGrammar.java:187)
    at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:219)
    at edu.stanford.nlp.pipeline.ParserAnnotator.&lt;init&gt;(ParserAnnotator.java:121)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.parse(AnnotatorImplementations.java:115)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$50(StanfordCoreNLP.java:504)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getDefaultAnnotatorPool$65(StanfordCoreNLP.java:533)
    at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:118)
    at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:146)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:447)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:146)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:133)
    at stanford.corenlp.StanfordNLP.test2(StanfordNLP.java:95)
    at stanford.corenlp.StanfordNLP.main(StanfordNLP.java:145)
Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"" as class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:309)
    at edu.stanford.nlp.parser.common.ParserGrammar.loadModel(ParserGrammar.java:184)
    ... 14 more
C:\Users\Greg\AppData\Local\NetBeans\Cache\8.2\executor-snippets\run.xml:53: Java returned: 1
BUILD FAILED (total time: 1 second)
</code></pre>

<p>I finally got past this point by downloading and adding to the classpath stanford-english-corenlp-2017-06-09-models.jar which you can get from the ""English"" download link here:</p>

<blockquote>
  <p><a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a></p>
</blockquote>

<p>You need to do this despite the message on the Download page saying that everything needed for English is already provided in the corenlp download!  </p>
",3,3,2489,2017-07-04 16:49:52,https://stackoverflow.com/questions/44910934/stanford-corenlp-basicpipelineexample-doesnt-work
Linear Chain Conditional Random Field Sequence Model - NER,"<p>I am confused with what a linear chain CRF implementation exactly is. While some people say that ""The Linear Chain CRF restricts the features to depend on only the current(i) and previous label(i-1), rather than arbitrary labels throughout the sentence"" , some people say that it restricts the features to depend on the current(i) and future label(i+1). </p>

<p>I am trying to understand the implementation that goes behind the Stanford NER Model. Can someone please explain what exactly the linear chain CRF Model is?</p>
","nlp, stanford-nlp","<p>Both models would be linear chain CRF models. The important part about the ""linear chain"" is that the features depend only on the current label and one direct neighbour in the sequence. Usually this would be the previous label (because that corresponds with reading order), but it could also be the future label. Such a model model would basically process the sentence backwards, and I have never seen this in the literature, but it would still be a linear chain CRF).</p>

<p>As far as I know, the Stanford NER model is based on a model that uses the current and the previous label, but it also uses an extension that can also look to labels further back. It is therefore not a strict linear-chain model, but uses an extension described in this paper:</p>

<blockquote>
  <p>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. <a href=""http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf</a></p>
</blockquote>
",3,1,247,2017-07-09 05:14:06,https://stackoverflow.com/questions/44993214/linear-chain-conditional-random-field-sequence-model-ner
How do I compile and run a java file from CMD that is dependent on JAR files?,"<p>Please bear with me as this is my first post and I am still not very familiar with CMD and and java commands. I have already looked at the 20+ posts about this topic but none seem to have the answer, as I continue to run into errors. Currently I have a some code (TestCoreNLP) that runs in NetBeans. However, when I try to run the program form CMD, no luck. </p>

<p>I have tried running the .JAR using this command:</p>

<pre><code>C:\Users\Forrest_Hunter\Documents\NetBeansProjects\ParsingEngine\dist&gt;java -jar ParsingEngine.jar ""C:\Users\Forrest_Hunter\Desktop\Summer Project\Unassigned Cases\input.xml"" 
</code></pre>

<p>however I get this error...</p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: edu/stanford/nlp/pipeline/StanfordCoreNLP
    at Test.TestCoreNLP.main(TestCoreNLP.java:25)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.pipeline.StanfordCoreNLP
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    ... 1 more
</code></pre>

<p>I have also tried to compile the code my code using this command (with success in my compilation):</p>

<pre><code>C:\Users\Forrest_Hunter\Documents\NetBeansProjects\ParsingEngine&gt;javac -classpath .;stanford-corenlp-3.8.0.jar src/Test/TestCoreNLP.java
</code></pre>

<p>But when I try to then run the code with this command:</p>

<pre><code>C:\Users\Forrest_Hunter\Documents\NetBeansProjects\ParsingEngine&gt;java -classpath .;stanford-corenlp-3.8.0.jar src/Test/TestCoreNLP
</code></pre>

<p>...I get this error:</p>

<pre><code>Error: Could not find or load main class src.Test.TestCoreNLP
</code></pre>

<p>From my understanding from all the other threads that I have read, I think that there is something wrong with my classpath, and that it doesn't point to the location of the needed JAR files, however I honestly am not sure how to fix it. </p>
","java, cmd, jar, stanford-nlp","<p>Have to make some presumptions based on your question.</p>

<p>you invoke java command line by supplying the main class file with its complete package structure.</p>

<p>so in your case I presume your package is <code>Test</code>, and class file <code>TestCoreNLP</code></p>

<p>you would invoke it as 
</p>

<pre class=""lang-java prettyprint-override""><code>java Test.TestCoreNLP
</code></pre>

<p>or in your case
</p>

<pre class=""lang-java prettyprint-override""><code>java -classpath .;stanford-corenlp-3.8.0.jar Test.TestCoreNLP
</code></pre>

<p>Your classpath definition is fine, 2 things to check. </p>

<p>Your current directory should be the one that contains the directory <code>Test</code>, there is a class file <code>TestCoreNLP.class</code> in that directory.</p>

<p>If the class is inside the Jar, then make sure that you are just invoking it with the package-name.class-name</p>
",0,1,88,2017-07-11 17:51:00,https://stackoverflow.com/questions/45041450/how-do-i-compile-and-run-a-java-file-from-cmd-that-is-dependent-on-jar-files
How to recognize a named entity that is lowcase such as kobe bryant by CoreNLP?,"<p>I got a problem that CoreNLP can only recognize named entity such as Kobe Bryant that is beginning with a uppercase char, but can't recognize kobe bryant as a person!!! So how to recognize a named entity that is beginning with a lowercase char by CoreNLP ???? Appreciate it !!!!</p>
","java, stanford-nlp","<p>First off, you do have to accept that it is harder to get named entities right in lowercase or inconsistently cased English text than in formal text, where capital letters are a great clue. (This is also one reason why Chinese NER is harder than English NER.) Nevertheless, there are things that you must do to get CoreNLP working fairly well with lowercase text – the default models are trained to work well on well-edited text.</p>

<p>If you are working with properly edited text, you should use our default English models. If the text that you are working with is (mainly) lowercase or uppercase, then you should use one of the two solutions presented below. If it's a real mixture (like much social media text), you might use the truecaser solution below, or you might gain by using <em>both</em> the cased and caseless NER models (as a long list of models given to the <code>ner.model</code> property).</p>

<p><strong>Approach 1: Caseless models.</strong> We also provide English models that ignore case information. They will work much better on all lowercase text.</p>

<p><strong>Approach 2: Use the truecaser.</strong> We provide a <code>truecase</code> annotator, which attempts to convert text into formally edited capitalization. You can apply it first, and then use the regular annotators.</p>

<p>In general, it's not clear to us that one of these approaches usually or always wins. You can try both.</p>

<p><strong>Important:</strong> To have available the extra components invoked below, you need to have downloaded <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""noreferrer"">the English models jar</a>, and to have it available on your classpath.</p>

<p>Here's an example. We start with a sample text:</p>

<pre><code>% cat lakers.txt
lonzo ball talked about kobe bryant after the lakers game.
</code></pre>

<p>With the default models, no entities are found and all their words just get a common noun tag. Sad!</p>

<pre><code>% java edu.stanford.nlp.pipeline.StanfordCoreNLP -file lakers.txt -outputFormat conll -annotators tokenize,ssplit,pos,lemma,ner
% cat lakers.txt.conll 
1   lonzo   lonzo   NN  O   _   _
2   ball    ball    NN  O   _   _
3   talked  talk    VBD O   _   _
4   about   about   IN  O   _   _
5   kobe    kobe    NN  O   _   _
6   bryant  bryant  NN  O   _   _
7   after   after   IN  O   _   _
8   the the DT  O   _   _
9   lakers  laker   NNS O   _   _
10  game    game    NN  O   _   _
11  .   .   .   O   _   _
</code></pre>

<p>Below, we ask to use the caseless models, and then we're doing pretty well: All the name words are now recognized as proper nouns, and the two person names are recognized. But the team name is still missed.</p>

<pre><code>% java edu.stanford.nlp.pipeline.StanfordCoreNLP -outputFormat conll -annotators tokenize,ssplit,pos,lemma,ner -file lakers.txt -pos.model edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger -ner.model edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz
% cat lakers.txt.conll 
1   lonzo   lonzo   NNP PERSON  _   _
2   ball    ball    NNP PERSON  _   _
3   talked  talk    VBD O   _   _
4   about   about   IN  O   _   _
5   kobe    kobe    NNP PERSON  _   _
6   bryant  bryant  NNP PERSON  _   _
7   after   after   IN  O   _   _
8   the the DT  O   _   _
9   lakers  lakers  NNPS    O   _   _
10  game    game    NN  O   _   _
11  .   .   .   O   _   _
</code></pre>

<p>Instead, you can run truecasing prior to POS tagging and NER:</p>

<pre><code>% java edu.stanford.nlp.pipeline.StanfordCoreNLP -outputFormat conll -annotators tokenize,ssplit,truecase,pos,lemma,ner -file lakers.txt -truecase.overwriteText
% cat lakers.txt.conll 
1   Lonzo   Lonzo   NNP PERSON  _   _
2   ball    ball    NN  O   _   _
3   talked  talk    VBD O   _   _
4   about   about   IN  O   _   _
5   Kobe    Kobe    NNP PERSON  _   _
6   Bryant  Bryant  NNP PERSON  _   _
7   after   after   IN  O   _   _
8   the the DT  O   _   _
9   Lakers  Lakers  NNPS    ORGANIZATION    _   _
10  game    game    NN  O   _   _
11  .   .   .   O   _   _
</code></pre>

<p>Now, the organization <em>Lakers</em> is recognized, and in general nearly all the entity words are tagged as proper nouns with the correct entity label, but it fails to get <em>ball</em>, which remains a common noun. Of course, this is a fairly hard word to get right in caseless text, since <em>ball</em> is a quite frequent common noun.</p>
",7,2,1097,2017-07-14 07:46:48,https://stackoverflow.com/questions/45097507/how-to-recognize-a-named-entity-that-is-lowcase-such-as-kobe-bryant-by-corenlp
"I was able to compile my java code from PowerShell, but cannot run it","<p>I was able to compile and run my java code from CMD, however when I try to run the same commands in PS, I am getting error messages. I have read and been told that CMD commands will work in PS, but the CMD commands are not working in PS </p>

<p>Here is the line that I am using to execute my program:</p>

<pre><code>java -classpath .;stanford-corenlp-3.8.0.jar;stanford-corenlp-3.8.0-
javadoc.jar;stanford-corenlp-3.8.0-models.jar;stanford-corenlp-3.8.0-
models.jar Test.TestCoreNLP
</code></pre>

<p>I am running the command from the directory where my needed JAR files are located. The error message says...</p>

<pre><code>The command stanford-corenlp-3.8.0-models.jar was not found, but does exist 
in the current location. Windows PowerShell does not load commands from the 
current If you trust this command, instead type: "".\stanford-corenlp-3.8.0-
models.jar"".
</code></pre>

<p>Made the change and the code looks like this now.</p>

<pre><code>java -classpath .\;stanford-corenlp-3.8.0.jar;stanford-corenlp-3.8.0-
javadoc.jar;stanford-corenlp-3.8.0-models.jar;stanford-corenlp-3.8.0-
models.jar Test.TestCoreNLP
</code></pre>

<p>Still getting the exact same error message. I have also tried going up a directory and no luck. I have looked all over StackOverflow and I have done my research.</p>

<p>Any help would be much appreciated. 
Thanks.</p>
","java, powershell, stanford-nlp","<p>Using <code>.\</code> would work for one file, but since you have a number of files, you should reference the current directory in each one of those files.</p>

<pre><code>java -classpath .\stanford-corenlp-3.8.0.jar;.\stanford-corenlp-3.8.0-javadoc.jar;.\stanford-corenlp-3.8.0-models.jar;.\stanford-corenlp-3.8.0-models.jar .\Test.TestCoreNLP
</code></pre>

<p>Java 6 also supports wildcards, <a href=""https://stackoverflow.com/questions/219585/including-all-the-jars-in-a-directory-within-the-java-classpath"">as this answer indicates</a>, so you might try simply this.</p>

<pre><code>java -cp "".\*"" .\Test.TestCoreNLP
</code></pre>
",0,-3,768,2017-07-14 13:23:14,https://stackoverflow.com/questions/45104154/i-was-able-to-compile-my-java-code-from-powershell-but-cannot-run-it
How to execute Spark UDF in parallel without repartitioning,"<p>I have a small Hive Table with 15 million rows saved over HDFS (parquet/1152 files - over 30GB).</p>

<p>I am doing LDA over scientific abstracts. Therefore, the first step is extracting some noun phrases/chunk phrases by using StanfordNLP which I wrote an UDF to achieve this goal.</p>

<p>Now performance wise, there are two scenarios which each has very different results.</p>

<p><strong>Scenario 1:</strong></p>

<pre class=""lang-scala prettyprint-override""><code>val hiveTable = hivecontext.sql(""""""
SELECT ab AS text,
          pmid AS id
  FROM scientific.medline      
    LIMIT 15000000
"""""")
</code></pre>

<p>Then I call my UDF over my <code>hiveTable</code>:</p>

<pre class=""lang-scala prettyprint-override""><code>val postagsDF = hiveTable.withColumn(""words"", StanfordNLP.posPhrases(col(""text"")))
</code></pre>

<p>Now if I trigger any action/transformation such as .count() or do CountVectorizer() on ""<strong>postagsDF</strong>"" I see 2 stages. One with the appropriate number of tasks (partitions) and the other stage with only one task. First one ends very fast after doing some Input/Shuffle writes but the second one with only one task takes a long time. It seems that my UDF is being executed in this stage which only has one task. (takes hours, no resource activity during its completion) </p>

<p><strong>Scenario 2:</strong></p>

<pre class=""lang-scala prettyprint-override""><code>val hiveTable = hivecontext.sql(""""""
SELECT ab AS text,
          pmid AS id
  FROM scientific.medline      
    LIMIT 15000000
"""""")
</code></pre>

<p>I repartition my <code>DataFrame</code> to the exact number of partitions detected by spark based on the number of parquets. (I can choose any other number but the number seems OK since I have over 500 cores available - 2 tasks per core)</p>

<pre class=""lang-scala prettyprint-override""><code>val repartitionedDocDF = docDF.repartition(1152)
</code></pre>

<p>Now calling my UDF over my <code>hiveTable</code>:</p>

<pre class=""lang-scala prettyprint-override""><code>val postagsDF = hiveTable.withColumn(""words"", StanfordNLP.posPhrases(col(""text"")))
</code></pre>

<p>However, any action/transformation this time will be four stages. Two of the stages (let's say count) are 1152 tasks and two of them are single task. I can see my UDF is being executed in one of those stages with 1152 tasks by all the executors using my entire cluster properly.</p>

<p><strong>Results of scenario number 1:</strong>
Looking at my cluster there is not much going on during the long-running single-task stage. There is no cpu usage, no memory, no network and no IO activity. Just one executor with one task that is applying my UDF over each document/column.</p>

<p><strong>Benchmark:</strong> Scenario number 1 takes 3-4 hours to finish only 1 million rows. (I couldn't wait to see how much it takes for 15 million rows)</p>

<p><strong>Results of scenario number 2:</strong>
Looking at my cluster I can clearly see all my resources are being utilised. All my nodes are almost at the full capacity.</p>

<p><strong>Benchmark:</strong> Scenario number 2 takes over 30 minutes for 15 million rows. </p>

<p><a href=""https://i.sstatic.net/8P8WM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8P8WM.png"" alt=""enter image description here""></a></p>

<p><strong>Real questions</strong></p>

<ol>
<li><p>What just happened? I thought UDFs over Dataframe would run in parallel by default. Maybe do repartition if the number of partions/tasks are more or less than total number of cores, but at least parallel over the default 200 partitions/tasks. I just want to understand why the UDf in my case is single task and ignoring both the default 200 and actual partition size. (It is not just about performance, it's single task job vs multi tasks job)</p></li>
<li><p>Is there any other way to make UDF to be executed over all the executors in parallel without calling repartition. I have nothing against repartitioning, but it is very expensive operation which I don't think it should be the only way of making UDF runs in parallel. Even when I repartition to the exact same number of partitions/files I still have to watch over 20GB shuffle reads and writes fly over my cluster.</p></li>
</ol>

<p>I have read everything about repartitioning and UDFs but I couldn't find similar issue which one can't run the UDF by default in parallel unless it does repartitioning. (simple UDF when you cast a type of a column from int to bigint might not be visible, but when you do NLP it really is visible)</p>

<p>My cluster size: 30 nodes (16core/32G) - Spark 1.6 Cloudera CDH 5.11.1
Spark: <code>--driver-cores 5 --driver-memory 8g --executor-cores 5 --executor-memory 5g --num-executors 116</code></p>

<p>Many thanks,</p>

<p><strong>UPDATE</strong>: </p>

<p>I ran the same code without the LIMIT clause and it did it in 18 minutes! So the LIMIT was the reason (more on this in the answer):</p>

<p><a href=""https://i.sstatic.net/EFVpD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EFVpD.png"" alt=""enter image description here""></a></p>
","apache-spark, parallel-processing, apache-spark-sql, stanford-nlp, user-defined-functions","<p>The problem here is specifically related to <code>LIMIT</code> clause you use in your query, and has nothing to do with <code>UDF</code>. <code>LIMIT</code> clause repartitions all resulting data to a single partition, therefore it is not suitable for large samples.</p>

<p>If you want to avoid the problem and somehow reduce number of records it is best to sample your data first:</p>

<pre class=""lang-scala prettyprint-override""><code>val p: Double = ???

spark.sql(s""SELECT * FROM df TABLESAMPLE($p percent)"")
</code></pre>

<p>or:</p>

<pre class=""lang-scala prettyprint-override""><code>spark.table(""df"").sample(false, p)
</code></pre>

<p>where <code>p</code> is a desired fraction of records.</p>

<p>Please keep in mind that sampling with exact number of values will suffer from the same issues as <code>LIMIT</code> clause.</p>
",8,4,5282,2017-07-17 19:30:14,https://stackoverflow.com/questions/45152226/how-to-execute-spark-udf-in-parallel-without-repartitioning
CoreNLP Stanford Dependency Format,"<blockquote>
  <p>Bills on ports and immigration were submitted by Senator Brownback,
  Republican of Kansas</p>
</blockquote>

<p>From the above sentence, I am looking to obtain the following typed dependencies:</p>

<pre><code>nsubjpass(submitted, Bills)
auxpass(submitted, were)
agent(submitted, Brownback)
nn(Brownback, Senator)
appos(Brownback, Republican)
prep_of(Republican, Kansas)
prep_on(Bills, ports)
conj_and(ports, immigration)
prep_on(Bills, immigration)
</code></pre>

<p>This <em>should be possible</em> as per Table 1, Figure 1 on the documentation for <a href=""https://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""nofollow noreferrer"">Stanford Dependencies</a>.</p>

<p>Using the below code I have only been able to achieve the following dependency makeup (code outputs this):</p>

<pre><code>root(ROOT-0, submitted-7)
nmod:on(Bills-1, ports-3)
nmod:on(Bills-1, immigration-5)
case(ports-3, on-2)
cc(ports-3, and-4)
conj:and(ports-3, immigration-5)
nsubjpass(submitted-7, Bills-1)
auxpass(submitted-7, were-6)
nmod:agent(submitted-7, Brownback-10)
case(Brownback-10, by-8)
compound(Brownback-10, Senator-9)
punct(Brownback-10, ,-11)
appos(Brownback-10, Republican-12)
nmod:of(Republican-12, Kansas-14)
case(Kansas-14, of-13)
</code></pre>

<p><strong>Question</strong> - How do I achieve the desired output above? </p>

<p><strong>Code</strong></p>

<pre><code>public void processTestCoreNLP() {
    String text = ""Bills on ports and immigration were submitted "" +
            ""by Senator Brownback, Republican of Kansas"";

    Annotation annotation = new Annotation(text);
    Properties properties = PropertiesUtils.asProperties(
            ""annotators"", ""tokenize,ssplit,pos,lemma,depparse""
    );

    AnnotationPipeline pipeline = new StanfordCoreNLP(properties);

    pipeline.annotate(annotation);

    for (CoreMap sentence : annotation.get(SentencesAnnotation.class)) {
        SemanticGraph sg = sentence.get(EnhancedPlusPlusDependenciesAnnotation.class);
        Collection&lt;TypedDependency&gt; dependencies = sg.typedDependencies();
        for (TypedDependency td : dependencies) {
            System.out.println(td);
        }
    }
}
</code></pre>
","nlp, stanford-nlp","<p>If you want to get the CCprocessed and collapsed Stanford Dependencies (SD) for a sentence through the NN dependency parser, you'll have to set a property to circumvent a small bug in CoreNLP. </p>

<p><strong>However</strong>, please note that we are <strong>no longer maintaining the Stanford Dependencies code</strong> and unless you have really good reasons to use SD, we'd recommend using Universal Dependencies for any new projects. Take a look at the <a href=""http://www.universaldependencies.org"" rel=""nofollow noreferrer"">Universal Dependencies (UD) documentation</a> and <a href=""https://nlp.stanford.edu/~sebschu/pubs/schuster-manning-lrec2016.pdf"" rel=""nofollow noreferrer"">Schuster and Manning (2016)</a> for more information on the UD representation.</p>

<p>To obtain the CCprocessed and collapsed SD representation, set the <code>depparse.language</code> property as follows:</p>

<pre><code>public void processTestCoreNLP() {
  String text = ""Bills on ports and immigration were submitted "" +
        ""by Senator Brownback, Republican of Kansas"";

  Annotation annotation = new Annotation(text);
  Properties properties = PropertiesUtils.asProperties(
        ""annotators"", ""tokenize,ssplit,pos,lemma,depparse"");

  properties.setProperty(""depparse.language"", ""English"")

  AnnotationPipeline pipeline = new StanfordCoreNLP(properties);

  pipeline.annotate(annotation);

  for (CoreMap sentence : annotation.get(SentencesAnnotation.class)) {
    SemanticGraph sg = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
    Collection&lt;TypedDependency&gt; dependencies = sg.typedDependencies();
    for (TypedDependency td : dependencies) {
      System.out.println(td);
    }
  }
}
</code></pre>
",1,2,1003,2017-07-19 23:07:16,https://stackoverflow.com/questions/45202486/corenlp-stanford-dependency-format
How are Words converted to Vectors in Stanford NER,"<p>I am looking at Stanford NER and want to know how the words are represented. Are they converted to vectors using Word2Vec or Glove when training the model using linear CRF. </p>

<p>A Little more study shows me that the data is stored into a CRFDatum structure. Can anyone please elaborate on this?</p>
","stanford-nlp, word2vec","<p>Well, now I know how the old-school AI people feel...</p>

<p>Back in the Old Days (including when the NER system was built), before neural networks took off, statistical ML converted discrete outputs into vectors using custom-built featurizers. For language, this usually resulted in a very long but sparse vector of one-hot features. For example, a featurizer might assign each word a one-hot representation: 1 at the index corresponding to the word, and zero elsewhere. For NER, these features were usually things like the characters in the word (one-hot encoded), prefixes and suffixes of length $k$, word shape, part-of-speech tag, etc.</p>

<p>In Stanford's code, these sparse vectors are usually represented as <code>Counter</code> objects of one form or another, which then get passed into a <code>Datum</code> object and converted into a more densely packed <code>Dataset</code> object, which is fed into the optimizer (usually, <code>QNMinimizer</code>, implementing L-BFGS).</p>
",2,0,339,2017-07-20 21:06:28,https://stackoverflow.com/questions/45225180/how-are-words-converted-to-vectors-in-stanford-ner
How to get protobuf extension field in ProtobufAnnotationSerializer,"<p>I am a new to protocol-buffers and try to figure out how to extend a message type in the Stanford CoreNLP library as described here: <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializer.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializer.html</a></p>

<p>The problem: I can set the extension field but i can't get it. I boiled the problem down to the code below. In the original message the field name is <code>[edu.stanford.nlp.pipeline.myNewField]</code> but is replaced by the field number <code>101</code> in the deserialized message.</p>

<p>How can i get the value of myNewField?</p>

<p>PS: This post <a href=""https://stackoverflow.com/questions/28815214/how-to-set-get-protobufs-extension-field-in-go"" title=""This post"">https://stackoverflow.com/questions/28815214/how-to-set-get-protobufs-extension-field-in-go</a> suggests that it should be as easy as calling <code>getExtension(MyAppProtos.myNewField)</code></p>

<p>custom.proto</p>

<pre><code>syntax = ""proto2"";

package edu.stanford.nlp.pipeline;

option java_package = ""com.example.my.awesome.nlp.app"";
option java_outer_classname = ""MyAppProtos"";

import ""CoreNLP.proto"";

extend Sentence {
    optional uint32 myNewField = 101;
}
</code></pre>

<p>ProtoTest.java</p>

<pre><code>import com.example.my.awesome.nlp.app.MyAppProtos;
import com.google.protobuf.ExtensionRegistry;
import com.google.protobuf.InvalidProtocolBufferException;

import edu.stanford.nlp.pipeline.CoreNLPProtos;
import edu.stanford.nlp.pipeline.CoreNLPProtos.Sentence;

public class ProtoTest {

    static {
        ExtensionRegistry registry = ExtensionRegistry.newInstance();
        registry.add(MyAppProtos.myNewField);
        CoreNLPProtos.registerAllExtensions(registry);
    }

    public static void main(String[] args) throws InvalidProtocolBufferException {

        Sentence originalSentence = Sentence.newBuilder()
                .setText(""Hello world!"")
                .setTokenOffsetBegin(0)
                .setTokenOffsetEnd(12)
                .setExtension(MyAppProtos.myNewField, 13)
                .build();

        System.out.println(""Original:\n"" + originalSentence);

        byte[] serialized = originalSentence.toByteArray();

        Sentence deserializedSentence = Sentence.parseFrom(serialized);
        System.out.println(""Deserialized:\n"" + deserializedSentence);

        Integer myNewField = deserializedSentence.getExtension(MyAppProtos.myNewField);
        System.out.println(""MyNewField: "" + myNewField);
    }
}
</code></pre>

<p>Output:</p>

<pre><code>Original:
tokenOffsetBegin: 0
tokenOffsetEnd: 12
text: ""Hello world!""
[edu.stanford.nlp.pipeline.myNewField]: 13

Deserialized:
tokenOffsetBegin: 0
tokenOffsetEnd: 12
text: ""Hello world!""
101: 13

MyNewField: 0
</code></pre>

<p><strong>Update</strong>
Because this question was about extending CoreNLP message types and using them with the <code>ProtobufAnnotationSerializer</code>, here is what my extended serializer looks like:</p>

<pre><code>import java.io.IOException;
import java.io.InputStream;
import java.util.Set;

import com.example.my.awesome.nlp.app.MyAppProtos;
import com.google.protobuf.ExtensionRegistry;

import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.CoreNLPProtos;
import edu.stanford.nlp.pipeline.CoreNLPProtos.Sentence;
import edu.stanford.nlp.pipeline.CoreNLPProtos.Sentence.Builder;
import edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.Pair;

public class MySerializer extends ProtobufAnnotationSerializer {

    private static ExtensionRegistry registry;

    static {
        registry = ExtensionRegistry.newInstance();
        registry.add(MyAppProtos.myNewField);
        CoreNLPProtos.registerAllExtensions(registry);
    }

    @Override
    protected Builder toProtoBuilder(CoreMap sentence, Set&lt;Class&lt;?&gt;&gt; keysToSerialize) {

        keysToSerialize.remove(MyAnnotation.class);
        Builder builder = super.toProtoBuilder(sentence, keysToSerialize);
        builder.setExtension(MyAppProtos.myNewField, 13);

        return builder;
    }

    @Override
    public Pair&lt;Annotation, InputStream&gt; read(InputStream is)
            throws IOException, ClassNotFoundException, ClassCastException {

        CoreNLPProtos.Document doc = CoreNLPProtos.Document.parseDelimitedFrom(is, registry);
        return Pair.makePair(fromProto(doc), is);
    }

    @Override
    protected CoreMap fromProtoNoTokens(Sentence proto) {

        CoreMap result = super.fromProtoNoTokens(proto);
        result.set(MyAnnotation.class, proto.getExtension(MyAppProtos.myNewField));

        return result;
    }
}
</code></pre>
","protocol-buffers, stanford-nlp","<p>The mistake was that i didn't provide the <code>parseFrom</code> call with the extension registry.</p>

<p>Changing <code>Sentence deserializedSentence = Sentence.parseFrom(serialized);</code> to <code>Sentence deserializedSentence = Sentence.parseFrom(serialized, registry);</code> did the job!</p>
",5,1,2639,2017-07-24 08:56:22,https://stackoverflow.com/questions/45276236/how-to-get-protobuf-extension-field-in-protobufannotationserializer
similarity measurement among names?,"<p>I have a list of names with me and iam trying to find the most similar 5 names from the list of any given name as a query.
I thought of applying word2vec or else using Text.similar() from nltk.
but iam not sure whether these will work for names as well.</p>

<p>any similarity measure would work for me.
any suggestions?
this not for any project but just i wanted to learn new things.</p>
","nlp, nltk, stanford-nlp, word2vec","<p>Since you added NLTK, I assume you are fine working in Python.<br>
Check out the <a href=""https://github.com/jamesturk/jellyfish"" rel=""nofollow noreferrer"">Jellyfish</a> library which contains 10 different algorithms for comparing strings. Some of them will compare just the characters while others will try to guess how a string would be pronounced and help you identify other phrases that are very differently spelt but would sound similar.<br>
The actual algorithms are all written in C and so this library is pretty efficient!<br>
I think you will find the Jaro-Winkler distance to be most useful. Also check out <a href=""http://users.cecs.anu.edu.au/~Peter.Christen/publications/tr-cs-06-02.pdf"" rel=""nofollow noreferrer"">this paper</a>.</p>
",4,0,1892,2017-07-26 09:35:02,https://stackoverflow.com/questions/45322996/similarity-measurement-among-names
extract a linguistic structure based on POS tagged sentence using Stanford nlp in JAVA,"<p>I am new in Natural Language Processing (NLP), I want to do part-of-speech tagging (POS) and then do find a specific structure within a text. I could manage POS tagging using Stanford-NLP but, I do not know how to extract this structure:</p>

<blockquote>
  <p><code>NN/NNS + IN + DT + NN/NNS/NNP/NNPS</code></p>
</blockquote>

<pre><code>public static void main(String args[]) throws Exception{
    //input File
    String contentFilePath = """";
    //outputFile
    String triplesFilePath = contentFilePath.substring(0, contentFilePath.length()-4)+""_postagg.txt"";

    //document to POS tagging
    String content = getFileContent(contentFilePath);

    Properties props = new Properties();

    props.setProperty(""annotators"",""tokenize, ssplit, pos"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // Annotate the document.
    Annotation doc = new Annotation(content);
    pipeline.annotate(doc);


    // Annotate the document.
    List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
    for (CoreMap sentence : sentences) {
        for (CoreLabel token: sentence.get(CoreAnnotations.TokensAnnotation.class)) {
            String word = token.get(CoreAnnotations.TextAnnotation.class);
            // this is the POS tag of the token
            String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
            System.out.println(word + ""/"" + pos);
        } }}}
</code></pre>
","java, nlp, stanford-nlp","<p>You can simply iterate over your sentence and check for the POS tags. If they match your requirements, you can extract this structure. The code for that could look like this:</p>

<pre><code>for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) { 
    List&lt;CoreLabel&gt; tokens = sentence.get(TokensAnnotation.class);
    for(int i = 0; i &lt; tokens.size() - 3; i++) {
        String pos = tokens.get(i).get(PartOfSpeechAnnotation.class);
        if(pos.equals(""NN"") || pos.equals(""NNS"")) {
            pos = tokens.get(i + 1).getString(PartOfSpeechAnnotation.class);
            if(pos.equals(""IN"")) {
                pos = tokens.get(i + 2).getString(PartOfSpeechAnnotation.class);
                if(pos.equals(""DT"")) {
                    pos = tokens.get(i + 3).getString(PartOfSpeechAnnotation.class);
                    if(pos.contains(""NN"")) {
                        //We have a match starting at index i and ending at index i + 3
                        String word1 = tokens.get(i).getString(TextAnnotation.class);
                        String word2 = tokens.get(i + 1).getString(TextAnnotation.class);
                        String word3 = tokens.get(i + 2).getString(TextAnnotation.class);
                        String word4 = tokens.get(i + 3).getString(TextAnnotation.class);
                        System.out.println(word1 + "" "" + word2 + "" "" + word3 + "" "" + word4);
                    }
                }
            }
        }
    }   
}
</code></pre>
",1,1,246,2017-07-31 11:52:02,https://stackoverflow.com/questions/45415212/extract-a-linguistic-structure-based-on-pos-tagged-sentence-using-stanford-nlp-i
Inconsistent dependecy parsing?,"<p>I have the following two sentences I'm analysing using the same Stanford CoreNLP (3.8.0) pipeline. </p>

<p>What I don't understand is why the dependency parser builds different trees even though the sentences are grammatically identical. Is there a way to enforce consistency ?</p>

<h2>Example 1</h2>

<pre><code>S1: “Admin Account” means all or any account created in connection with this website.
S2: “Admin Account” means all or any user created in connection with this website.
S3: “Admin Account” means all or any cat created in connection with this website.
S4: “Admin Account” means all or any dog created in connection with this website.
</code></pre>

<p>These get parsed into the following:</p>

<p><a href=""https://i.sstatic.net/LqHn2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LqHn2.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/uObQo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uObQo.png"" alt=""enter image description here""></a></p>

<hr>

<h2>Example 2</h2>

<p>Here is another example using a variation of the same sentence that introduces a nominal phrase.</p>

<p><a href=""https://i.sstatic.net/CdzQD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CdzQD.png"" alt=""enter image description here""></a></p>

<hr>

<p>Here is how I run the corenlp server</p>

<pre><code>java -mx20g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 35000 -parse.model edu/stanford/nlp/models/srparser/englishSR.beam.ser.gz -tokenize.language en -tagger edu/stanford/nlp/models/pos-tagger/english-bidirectional/english-bidirectional-distsim.tagger -depparse.model edu/stanford/nlp/models/parser/nndep/english_SD.gz
</code></pre>
","stanford-nlp, dependency-parsing","<p>The answer is simple but probably quite disappointing: Stanford CoreNLP is driven by a complex statistical model trained on manually annotated examples (and so are all modern dependency parsers), so it sometimes will output different structures for different inputs, sometimes even if they are very similar and have in fact the same underlying structure. As far as I know, there are no rules that would enforce consistent behaviour, it is just expected that the massive amount of consistently annotated training data results in consistency in <em>most</em> real-life cases (and this happens, doesn't it?).</p>

<p>Internally the parser weighs evidence for many candidate parses and multiple factors can influence this. You can imagine this as various structures competing for being chosen. Sometimes two alternative readings can have very similar probabilities assigned by the parser. In such situations even very small differences in other parts of the sentence are likely to influence the final decision on labelling and attachment that takes place in other parts (think butterfly effect).</p>

<p><em>Account</em> is an inanimate noun, probably most often used as an object or in passive constructs. <em>User</em> is usually animate, so it is more likely to play the role of agens. It is hard to guess what exactly the parser “thinks” when seeing these sentences but the context in which nouns usually appear can have deciding role (CoreNLP also deals with word embeddings).</p>

<p><strong>What you can do to enforce consistency?</strong> Theoretically you could add in extra training examples to a training corpus and train the parser yourself (mentioned here: <a href=""https://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/nndep.shtml</a>). I guess this is might be not trivial, I'm also not sure if the original training corpus is publicly available. Some parsers offer a possibility of post-training an existing model. I've faced issues similar to yours and managed to overcome them by post-training in Spacy dependency parser (see the discussion under <a href=""https://github.com/explosion/spaCy/issues/1015"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/1015</a> if you're interested).</p>

<p><strong>What could have happened in these examples?</strong></p>

<p>Each of these has been mislabelled. I think the main verb ‘means’ should be pointed to its clausal complement (clause headed with ‘created’) with a <code>ccomp</code> dependency (<a href=""http://universaldependencies.org/u/dep/ccomp.html"" rel=""nofollow noreferrer"">http://universaldependencies.org/u/dep/ccomp.html</a>) but this just never happened. Perhaps more importantly, “all or any account” should be a subject of this clause, which is also not reflected in any of these structures. The parser guessed that this phrase is either an adverb modifier (which is kinda weird) or a direct object (account means all). 
My guess is that the linking of ‘means’ with its dependents is heavily influenced by other parser guesses (this is a complex probabilistic model, all of the decisions made within a sentence may have influence on probability of decisions taken in other parts).</p>
",2,1,273,2017-07-31 16:38:01,https://stackoverflow.com/questions/45421172/inconsistent-dependecy-parsing
Stanford Parser version 3.8.0,"<p>Dears.
I downloaded Stanford Parser version 3.8.0 from <a href=""https://nlp.stanford.edu/software/lex-parser.shtml#Download"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/lex-parser.shtml#Download</a>. However,  I do not know how to install it on Microsoft Windows Operating System 8.1. whenever i click on a file, it says ""How do you want to open this type of file?""and the message asks me to choose an app. Would you please help me in this regard.
Thank you</p>
","parsing, stanford-nlp","<p>I assume you have Java installed on your machine. Then you can simply double-click on <em>lexparser-gui.bat</em>, which opens a terminal and displays a GUI where you can start your parsing.</p>
",0,0,185,2017-07-31 22:00:48,https://stackoverflow.com/questions/45425955/stanford-parser-version-3-8-0
Query classification for Virtual Assistant in Java?,"<p>This is my first time posting to Stack Overflow, so please let know if I should be more thorough when asking questions in the future.</p>

<p>Currently I am working on a Virtual Assistant application for Android using Java, and although it is going well so far, I am unsure how to approach classifying user input. So far I have implemented the <a href=""http://nlp.stanford.edu:8080/parser/index.jsp/%20%22Stanford%20NLP%20Parser"" rel=""nofollow noreferrer"">Stanford NLP Parser</a> within the program, so that clause, phrase, and word labels can be applied to the raw text. This has allowed me to have the program recognise direct questions and extract the subject from it, simply by searching for the occurrence of certain tags.</p>

<pre><code>(ROOT
  (SBARQ &lt;--- Indicates that the sentence is a question
    (WHNP (WP Who))
      (SQ (VBD were)
        (NP (DT the) (FW samurai))) &lt;--- Subject of question
      (. ?)))
</code></pre>

<p>Although this feels like a step forward, I hope to eventually have the assistant capable of classifying different types of questions, (weather related questions, time/date related questions, etc) while also being capable of recognising questions that are not as direct but are asking for the same information (e.g. ""can you tell me about the samurai?"" as opposed to ""who were the samurai?""). Doing this by just using the Stanford NLP Parser and looking for certain tags seems like a very difficult task. Does anyone have any advice on alternative approaches I could take?</p>

<p>Thank-you!</p>
","java, android, machine-learning, nlp, stanford-nlp","<p>With regards to virtual assistants or chatbots this is usually called <strong>intent classification</strong>. There's a pile of ways to do this, but generally you provide labelled examples and train a model to differentiate them. Here's some example data from <a href=""https://chatbotslife.com/text-classification-using-algorithms-e4d50dcba45"" rel=""nofollow noreferrer"">a blog post</a> on the topic:</p>

<pre><code># 3 classes of training data
training_data = []
training_data.append({""class"":""greeting"", ""sentence"":""how are you?""})
training_data.append({""class"":""greeting"", ""sentence"":""how is your day?""})
training_data.append({""class"":""greeting"", ""sentence"":""good day""})
training_data.append({""class"":""greeting"", ""sentence"":""how is it going today?""})

training_data.append({""class"":""goodbye"", ""sentence"":""have a nice day""})
training_data.append({""class"":""goodbye"", ""sentence"":""see you later""})
training_data.append({""class"":""goodbye"", ""sentence"":""have a nice day""})
training_data.append({""class"":""goodbye"", ""sentence"":""talk to you soon""})

training_data.append({""class"":""sandwich"", ""sentence"":""make me a sandwich""})
training_data.append({""class"":""sandwich"", ""sentence"":""can you make a sandwich?""})
training_data.append({""class"":""sandwich"", ""sentence"":""having a sandwich today?""})
training_data.append({""class"":""sandwich"", ""sentence"":""what's for lunch?""})
</code></pre>

<p>While your training data is specific to your application, in principle it's not different from automatically categorizing emails or news articles.</p>

<p>A easy-to-use baseline algorithm for text classification is <strong>Naive Bayes</strong>. More recent methods include using <strong>Word Mover's Distance</strong> or neural networks. </p>

<p>The part where you extract the subject is also called <strong>slot detection</strong>, and ""intent and slot"" architectures for assistants are common. Even if you want to build something from scratch, looking at configuration screens for chatbot platforms like <a href=""http://rasa-nlu.readthedocs.io/en/stable/tutorial.html"" rel=""nofollow noreferrer"">rasa</a> may be helpful to get an idea of how to use training data.</p>
",2,3,170,2017-08-04 14:22:05,https://stackoverflow.com/questions/45509244/query-classification-for-virtual-assistant-in-java
Unexpected format when running StanfordPOSTagger with NLTK for Chinese,"<p>I have installed Python 3.6.0, NLTK 3.2.4, and downloaded Stanford POS Tagger 3.8.0.</p>

<p>Then I tried running the following script:</p>

<pre><code>#!/usr/bin/env python3

from nltk.tag import StanfordPOSTagger


st = StanfordPOSTagger('chinese-distsim.tagger')
print(st.tag('这 是 斯坦福 中文 分词器 测试'.split()))
</code></pre>

<p>and the output is in an unexpected format:</p>

<pre><code>[('', '这#PN'), ('', '是#VC'), ('', '斯坦福#NR'), ('', '中文#NN'), ('', '分词器#NN'), ('', '测试#NN')]
</code></pre>

<p>The tagger does do its job, but the words and their parts of speech are not separated as a pair, but joined by a '#' to form single strings. Is this the format specially for Chinese, or is there something wrong?</p>
","python, python-3.x, nlp, nltk, stanford-nlp","<h1>TL;DR</h1>

<p>Set a different <code>_SEPARATOR</code>:</p>

<pre><code>from nltk.tag import StanfordPOSTagger

st = StanfordPOSTagger('chinese-distsim.tagger')
st._SEPARATOR = '#'
print(st.tag('这 是 斯坦福 中文 分词器 测试'.split()))
</code></pre>

<hr>

<h1>Better Solution</h1>

<p>Hold out for a while, wait for NLTK v3.2.5 where there will be a very simple interface to the Stanford tokenizers that are standardize across different languages. </p>

<p><strong>There'll be no delimiter involved since the tags and tokens are transferred through a json from a REST interface</strong> =)</p>

<p>Also, the <code>StanfordSegmenter</code> and <code>StanfordTokenizer</code> classes will be deprecated in v3.2.5, see  </p>

<ul>
<li><a href=""https://github.com/nltk/nltk/pull/1735#issuecomment-306137326"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/pull/1735#issuecomment-306137326</a> </li>
<li><a href=""https://github.com/nltk/nltk/pull/1771"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/pull/1771</a></li>
</ul>

<p>First upgrade your <code>nltk</code> version:</p>

<pre><code>pip install -U nltk
</code></pre>

<p>Download and start the Stanford CoreNLP server:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31
wget http://nlp.stanford.edu/software/stanford-chinese-corenlp-2016-10-31-models.jar
wget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-chinese.properties 

java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-serverProperties StanfordCoreNLP-chinese.properties \
-preload tokenize,ssplit,pos,lemma,ner,parse \
-status_port 9001  -port 9001 -timeout 15000
</code></pre>

<p>Then in NLTK v3.2.5:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger
&gt;&gt;&gt; from nltk.tokenize.stanford import CoreNLPTokenizer
&gt;&gt;&gt; stpos, stner = CoreNLPPOSTagger('http://localhost:9001'), CoreNLPNERTagger('http://localhost:9001')
&gt;&gt;&gt; sttok = CoreNLPTokenizer('http://localhost:9001')

&gt;&gt;&gt; sttok.tokenize(u'我家没有电脑。')
['我家', '没有', '电脑', '。']

# Without segmentation (input to`raw_string_parse()` is a list of single char strings)
&gt;&gt;&gt; stpos.tag(u'我家没有电脑。')
[('我', 'PN'), ('家', 'NN'), ('没', 'AD'), ('有', 'VV'), ('电', 'NN'), ('脑', 'NN'), ('。', 'PU')]
# With segmentation
&gt;&gt;&gt; stpos.tag(sttok.tokenize(u'我家没有电脑。'))
[('我家', 'NN'), ('没有', 'VE'), ('电脑', 'NN'), ('。', 'PU')]

# Without segmentation (input to`raw_string_parse()` is a list of single char strings)
&gt;&gt;&gt; stner.tag(u'奥巴马与迈克尔·杰克逊一起去杂货店购物。')
[('奥', 'GPE'), ('巴', 'GPE'), ('马', 'GPE'), ('与', 'O'), ('迈', 'O'), ('克', 'PERSON'), ('尔', 'PERSON'), ('·', 'O'), ('杰', 'O'), ('克', 'O'), ('逊', 'O'), ('一', 'NUMBER'), ('起', 'O'), ('去', 'O'), ('杂', 'O'), ('货', 'O'), ('店', 'O'), ('购', 'O'), ('物', 'O'), ('。', 'O')]
# With segmentation
&gt;&gt;&gt; stner.tag(sttok.tokenize(u'奥巴马与迈克尔·杰克逊一起去杂货店购物。'))
[('奥巴马', 'PERSON'), ('与', 'O'), ('迈克尔·杰克逊', 'PERSON'), ('一起', 'O'), ('去', 'O'), ('杂货店', 'O'), ('购物', 'O'), ('。', 'O')]
</code></pre>
",1,2,947,2017-08-07 15:06:53,https://stackoverflow.com/questions/45550167/unexpected-format-when-running-stanfordpostagger-with-nltk-for-chinese
Stanford NLP: print parser tree in tree format,"<p>this is a silly question, but how can i print the result of the NLP parse tree in tree format?</p>

<p>This is my code:</p>

<pre><code>public static void main(String[] args) {

        Annotation document =
                new Annotation(""My dog also likes eating sausage."");
            Properties props = new Properties();
            props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
            pipeline.annotate(document);
            for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
              Tree constituencyParse = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
              System.out.println(constituencyParse);

        }
    }
</code></pre>

<p>Upon Execution, i get following result in the Eclipse console:</p>

<pre><code>(ROOT (S (NP (PRP$ My) (NN dog)) (ADVP (RB also)) (VP (VBZ likes) (NP (JJ eating) (NN sausage))) (. .)))
</code></pre>

<p>Is there any way to print this in the actual tree format i.e., something like this?</p>

<pre><code>(ROOT
 (S
   (NP (PRP$ My) (NN dog))
   (ADVP (RB also))
   (VP (VBZ likes)
     (S
       (VP (VBG eating)
        (NP (NN sausage)))))
(. .)))
</code></pre>
","tree, stanford-nlp","<p>The following statement will print the tree in print format.</p>

<pre><code>tree.pennPrint();
</code></pre>
",0,0,505,2017-08-08 11:58:18,https://stackoverflow.com/questions/45567764/stanford-nlp-print-parser-tree-in-tree-format
"Parts of Speech, Stanford Core NLP","<p><strong>It may be that he is bargaining for the Chancellorship, which he is certainly not fit for.</strong></p>

<p>In the above mentioned sentence, the Stanford Parser refers the word <strong>bargaining</strong> <em>Parts of Speech</em> as <strong>NN</strong> which signifies that this word is a <strong>noun</strong> here , however as per the use of the word in the above sentence, it should have been a <strong>VERB</strong> .</p>

<p>Could anyone clarify on this.</p>
",stanford-nlp,"<p>You are right, in your sentence above, the word is a verb. The Stanford POS Tagger calculates tags based on a bidirectional approach. The POS tag of a word is calculated based on the context it appears in, meaning that two words before and two words after are considered. Based on that the algorithm outputs the tag which is most likely to be correct. The tagger does not claim the output to be correct. It is just more likely, that <em>bargaining</em> is a noun in this sentence.</p>

<p>You could train your own POS tagger, if you really want a correct output. But that requires a lot of effort. For more information, take a look <a href=""http://renien.com/blog/training-stanford-pos-tagger/"" rel=""nofollow noreferrer"">here</a> and <a href=""https://medium.com/@klintcho/training-a-swedish-pos-tagger-for-stanford-corenlp-546e954a8ee7"" rel=""nofollow noreferrer"">here</a>.</p>
",0,-2,236,2017-08-08 17:45:08,https://stackoverflow.com/questions/45574851/parts-of-speech-stanford-core-nlp
"Why does the Stanford NER demo convert &#39;this year&#39; to 2017, whereas my CoreNLP server does not?","<p>I have set up a CoreNLP server and am using Stanford NER to extract time periods from sentences.</p>

<p>If I use the online interactive demo at corenlp.run to parse the sentence </p>

<blockquote>
  <p>'Last year something happened.'</p>
</blockquote>

<p><a href=""https://i.sstatic.net/dxdB9.png"" rel=""nofollow noreferrer"">it shows 'DATE' and '2016'</a>.
However, my own server, set up with the latest release of CoreNLP, <a href=""https://i.sstatic.net/JL9qw.png"" rel=""nofollow noreferrer"">only shows 'DATE'</a>. What's more, when I use Python Requests to query my server's API with the same sentence, the first two tokens in the response contain the fields <code>'timex': {'type': 'DATE','tid': 't1', 'altValue': 'THIS P1Y OFFSET P-1Y'}</code> and <code>'normalizedNER': 'THIS P1Y OFFSET P-1Y'</code>.</p>

<p>If I just have to deal with the fact that my output is not as good as the demo's, then where is the Stanford NER or timex3 documentation explaining what <code>THIS P1Y OFFSET P-1Y</code> means or describing what other possible responses I might get in the <code>normalizedNER</code> field?</p>

<p>Here is the entire API response</p>

<pre><code>[
{'word': 'Last', 'after': ' ', 'originalText': 'Last', 'timex': {'type': 'DATE', 'tid': 't1', 'altValue': 'THIS P1Y OFFSET P-1Y'}, 'pos': 'JJ', 'ner': 'DATE', 'lemma': 'last', 'normalizedNER': 'THIS P1Y OFFSET P-1Y', 'before': '', 'index': 1, 'characterOffsetBegin': 0, 'characterOffsetEnd': 4},
{'word': 'year', 'after': ' ', 'originalText': 'year', 'timex': {'type': 'DATE', 'tid': 't1', 'altValue': 'THIS P1Y OFFSET P-1Y'}, 'pos': 'NN', 'ner': 'DATE', 'lemma': 'year', 'normalizedNER': 'THIS P1Y OFFSET P-1Y', 'before': ' ', 'index': 2, 'characterOffsetBegin': 5, 'characterOffsetEnd': 9},
{'word': 'something', 'before': ' ', 'originalText': 'something', 'ner': 'O', 'lemma': 'something', 'after': ' ', 'characterOffsetEnd': 19, 'index': 3, 'characterOffsetBegin': 10, 'pos': 'NN'},
{'word': 'happened', 'before': ' ', 'originalText': 'happened', 'ner': 'O', 'lemma': 'happen', 'after': '', 'characterOffsetEnd': 28, 'index': 4, 'characterOffsetBegin': 20, 'pos': 'VBD'}, 
{'word': '.', 'before': '', 'originalText': '.', 'ner': 'O', 'lemma': '.', 'after': '', 'characterOffsetEnd': 29, 'index': 5, 'characterOffsetBegin': 28, 'pos': '.'}
]
</code></pre>
","stanford-nlp, named-entity-recognition","<p>If you closely look at the request made to corenlp server in the interactive demo, then you will see that current date is also sent as ""date"" parameter with the request. 
For eg. If your sentence is ""I went to school today."", then ""today"" has normalized ner is ""2017-19-09"" (current date).
If you dont pass the ""date"" parameter, ""today"" won't have the exact date as normalized ner.</p>

<p>Hope it makes sense.
<a href=""https://i.sstatic.net/qsGPu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qsGPu.png"" alt=""enter image description here""></a></p>
",0,0,313,2017-08-15 11:28:41,https://stackoverflow.com/questions/45691933/why-does-the-stanford-ner-demo-convert-this-year-to-2017-whereas-my-corenlp-s
NLP always returns sentiment as -1,"<p>NLP library always returns sentiment integer as -1</p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class NLP {
    static StanfordCoreNLP pipeline;

    public static void init() {
        pipeline = new StanfordCoreNLP(""MyPropFile.properties"");
    }

    public static int findSentiment(String tweet) {

        int mainSentiment = 0;
        if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(tweet);
            for (CoreMap sentence : annotation
                    .get(CoreAnnotations.SentencesAnnotation.class)) {
                Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                String partText = sentence.toString();
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        return mainSentiment;
    }
}
</code></pre>

<p>No matter what sentence i pass it always returns as -1.
Example : ""Google is good"" returns -1
          ""Google is bad"" returns -1</p>
","java, nlp, stanford-nlp","<p>Simply change this line</p>

<pre><code>Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
</code></pre>

<p>to this</p>

<pre><code>Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
</code></pre>

<p>Now it should work.</p>
",1,1,66,2017-08-16 09:37:03,https://stackoverflow.com/questions/45710057/nlp-always-returns-sentiment-as-1
Unable to set up my own Stanford CoreNLP server with error &quot;Could not delete shutdown key file&quot;,"<p>I try to set up my own Stanford CoreNLP server following the <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">official guide</a>. However, I am not able to start the server using the following command:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
</code></pre>

<p>I paste the error messages below:</p>

<pre><code>my_server_name$ java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
Exception in thread ""main"" java.lang.IllegalStateException: Could not delete shutdown key file
at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.&lt;init&gt;(StanfordCoreNLPServer.java:195)
at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1323)
[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.
</code></pre>

<p>The main problem is the IllegalSstateException: Could not delete shutdown key file. I just wonder whether the cause of this problem is the sudo access. The official guide doesn't explicitly state this command needs sudo access. </p>

<p>I want to ask 1) whether the above command requires the sudo access and 2) if that command doesn't need sudo access, what could be the potential error for my IllegalSstateException. </p>

<p>Thanks.</p>

<p>PS: I am running on a server with Ubuntu 16.04.3 LTS. </p>
","nlp, stanford-nlp","<p>This error happens when the shutdown key file already exists on your filesystem, you're starting a new CoreNLP server instance, and it can't delete the old shutdown key file. Are you running the server as two different users?</p>

<p>More generally, do you have permissions to the directory stored in java property <code>java.io.tmpdir</code>? This is, traditionally, <code>/tmp</code> on Linux machines. The shutdown key is stored in:</p>

<pre><code> System.getProperty(""java.io.tmpdir"") + File.separator + ""corenlp.shutdown""
</code></pre>

<p>So, for a Linux system:</p>

<pre><code>/tmp/corenlp.shutdown
</code></pre>

<p>The error says that this file exists, and cannot be deleted by Java. You should check your permissions on this file, and that should help you debug what's wrong.</p>

<p>An easy workaround, in the worst case, is to set the tmpdir yourself when starting the server. For example:</p>

<pre><code>java -Djava.io.tmpdir=/path/to/tmp -mx4g edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9000
</code></pre>
",6,4,2302,2017-08-25 16:49:05,https://stackoverflow.com/questions/45886128/unable-to-set-up-my-own-stanford-corenlp-server-with-error-could-not-delete-shu
How to train word2vec with your own vocab,"<p>I am getting error while training word2vec with my own vocabulary. I am also not getting why its happening.</p>

<p>Code:</p>

<pre><code>from gensim.models import word2vec
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

sentences = word2vec.LineSentence('test_data')

model = word2vec.Word2Vec(sentences, size=20)
model.build_vocab(sentences,update=True)
model.train(sentences)

print model.most_similar(['course'])
</code></pre>

<p>It throws an error</p>

<pre><code>2017-08-27 16:50:04,590 : INFO : precomputing L2-norms of word weight vectors
Traceback (most recent call last):
  File ""tryword2vec.py"", line 23, in &lt;module&gt;
    print model.most_similar(['course']) 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 1285, in most_similar
    return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 97, in most_similar
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word 'course' not in vocabulary""
</code></pre>

<p>test_data contains:</p>

<blockquote>
  <p>Bachelor of Engg is a course. M.Tech is a course. ME is a course.
  B.Tech is a course. Bachelor of Arts is a course. Fashion Design is a
  course. Multimedia is a course. Mechanical engg is a course. Computer
  Science is a course. Electronics is a cource. Engineering is a course.
  MBA is a course. BBA is a course.</p>
</blockquote>

<p>Any help is appreciated?</p>
","nlp, stanford-nlp, word2vec, doc2vec","<p>The reason you are not getting the error is because the word <strong>course</strong> is not in the vocabulary. Instead the word present is <b>course.</b> </p>

<p>There is a period <strong>"".""</strong> at the end of course.</p>

<p>check your vocabulary 
<code>model.wv.vocab</code></p>

<pre><code>{u'a': &lt;gensim.models.keyedvectors.Vocab at 0x7fe086c461d0&gt;,
 u'course.': &lt;gensim.models.keyedvectors.Vocab at 0x7fe0b4704f90&gt;,
 u'is': &lt;gensim.models.keyedvectors.Vocab at 0x7fe086ba0d10&gt;}
</code></pre>

<p>And do hide your <strong>api</strong> keys.</p>
",1,2,1273,2017-08-27 11:32:49,https://stackoverflow.com/questions/45904499/how-to-train-word2vec-with-your-own-vocab
Basic and enhanced dependencies give different results in Stanford coreNLP,"<p>I am using dependency parsing of coreNLP for a project of mine. The basic and enhanced dependencies are different result for a particular dependency.
I used the following code to get enhanced dependencies.</p>

<pre><code>val lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
lp.setOptionFlags(""-maxLength"", ""80"")
val rawWords = edu.stanford.nlp.ling.Sentence.toCoreLabelList(tokens_arr:_*)
val parse = lp.apply(rawWords)
val tlp = new PennTreebankLanguagePack()
val gsf:GrammaticalStructureFactory = tlp.grammaticalStructureFactory()
val gs:GrammaticalStructure = gsf.newGrammaticalStructure(parse)
val tdl = gs.typedDependenciesCCprocessed()
</code></pre>

<p>For the following example, </p>

<pre><code>Account name of ramkumar.
</code></pre>

<p>I use simple API to get basic dependencies. The dependency i get between
(account,name) is (compound). But when i use the above code to get enhanced dependency i get the relation between (account,name) as (dobj).</p>

<p>What is the fix to this? Is this a bug or am i doing something wrong? </p>
","parsing, nlp, stanford-nlp, dependency-parsing","<p>When I run this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse -file example.txt -outputFormat json
</code></pre>

<p>With your example text in the file <code>example.txt</code>, I see <code>compound</code> as the relationship between both of those words for both types of dependencies.</p>

<p>I also tried this with the <code>simple API</code> and got the same results.</p>

<p>You can see what <code>simple</code> produces with this code:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.semgraph.SemanticGraphFactory;
import edu.stanford.nlp.simple.*;

import java.util.*;

public class SimpleDepParserExample {

  public static void main(String[] args) {
    Sentence sent = new Sentence(""...example text..."");
    Properties props = new Properties();
    // use sent.dependencyGraph() or sent.dependencyGraph(props, SemanticGraphFactory.Mode.ENHANCED) to see enhanced dependencies
    System.out.println(sent.dependencyGraph(props, SemanticGraphFactory.Mode.BASIC));
  }

}
</code></pre>

<p>I don't know anything about any Scala interfaces for Stanford CoreNLP.  I should also note my results are using the latest code from GitHub, though I presume Stanford CoreNLP 3.8.0 would also produce similar results.  If you are using an older version of Stanford CoreNLP that could be a potential cause of the error.</p>

<p>But running this example in various ways using Java I don't see the issue you are encountering.</p>
",1,0,706,2017-08-29 06:41:34,https://stackoverflow.com/questions/45932370/basic-and-enhanced-dependencies-give-different-results-in-stanford-corenlp
edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException when using CoreNLP with Jython,"<p>I'm trying to use CoreNLP run off of Jython 2.7.1, ran using IntelliJ IDEA, with the following test code:</p>

<pre><code>from edu.stanford.nlp.simple import *
s = Sentence(""This is a test."")
ss = s.nerTags()
print(s)
print(ss)
</code></pre>

<p>Log:</p>

<pre><code>Connected to pydev debugger (build 172.4155.5)
[MainThread] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.4 sec].
[MainThread] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.8 sec].
[MainThread] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].
[MainThread] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].
[MainThread] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
Traceback (most recent call last):
  File ""C:\Users\user\.IntelliJIdea2017.2\config\plugins\python\helpers\pydev\pydevd.py"", line 1599, in &lt;module&gt;
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Users\user\.IntelliJIdea2017.2\config\plugins\python\helpers\pydev\pydevd.py"", line 1026, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:/Users/user/IdeaProjects/nlptest/src/test.py"", line 3, in &lt;module&gt;
    ss = s.nerTags()
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:57)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:38)
    at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.&lt;init&gt;(NumberSequenceClassifier.java:86)
    at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:136)
    at edu.stanford.nlp.pipeline.NERCombinerAnnotator.&lt;init&gt;(NERCombinerAnnotator.java:91)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:70)
    at edu.stanford.nlp.simple.Document$2.get(Document.java:115)
    at edu.stanford.nlp.simple.Document$2.get(Document.java:109)
    at edu.stanford.nlp.simple.Document.runNER(Document.java:886)
    at edu.stanford.nlp.simple.Sentence.nerTags(Sentence.java:528)
    at edu.stanford.nlp.simple.Sentence.nerTags(Sentence.java:536)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
</code></pre>

<p>I added, using my IDE, the CoreNLP directory to my CLASSPATH and it seems that Jython can find it. There are several similar questions on SO on about error but none of their solutions (if they had any) actually worked for me. Has anyone else found a solution to this problem?
(For the record, CoreNLP works fine when run off of the command line.)</p>

<p>Edit (my attempt using Bash on Windows):</p>

<pre><code>user@user:~/uh$ export JYTHONPATH=""/home/user/uh/stanford-corenlp-full-2017-06-09/*:""    
user@user:~/uh$ jython
Jython 2.7.0 (default:9987c746f838, Apr 29 2015, 02:25:11)
[OpenJDK 64-Bit Server VM (Oracle Corporation)] on java1.8.0_131
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from edu.stanford.nlp.simple import *
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named edu
&gt;&gt;&gt;
</code></pre>
","java, stanford-nlp, jython, bash-on-windows","<p>My further testing shows that this is purely a Jython problem and a cross-platform one at that. I tested similar setups on Windows, Ubuntu, and Mac resulting in the same error. Even running pure Java code designed to return the initial pipeline to Jython returns the same error. However, the error can be avoided by setting ""ner.useSUTime"" to false with the following:</p>

<pre><code>props.setProperty(""ner.useSUTime"", ""0"")
</code></pre>
",2,0,504,2017-09-01 23:32:03,https://stackoverflow.com/questions/46009115/edu-stanford-nlp-util-reflectionloadingreflectionloadingexception-when-using-co
Why do I get UnsupportedOperationException with Stanford CoreNLP,"<p>I would like to find the head word of each phrase (constituent) from a <code>Tree</code> in <code>Stanford CoreNLP</code>, but when I try <code>tree.Parent()</code> for any of the <code>constituents</code>, I get <code>UnsupportedOperationException</code>. What am I doing wrong?</p>

<p>Here is my code:</p>

<pre><code>List&lt;Tree&gt; allConstituents = new ArrayList&lt;&gt;();
    private Tree parseTree;

  List&lt;CoreMap&gt; sentences = LoadAndParse(language, filetype, modelPath, text);

            for (CoreMap sentence : sentences) {
                Tree parse = sentence.get(TreeAnnotation.class);
                allConstituents = parseTree.subTreeList();

            for (int i = 0; i &lt; allConstituents.size(); i++) {
                    Tree constituentTree = allConstituents.get(i);
                    HeadFinder headFinder = new SemanticHeadFinder();
                    String head = constituentTree.headTerminal(headFinder, constituentTree.parent());

                }
              }
</code></pre>

<p>Here is an example that I have:</p>

<pre><code>Your tasks are challenging:
</code></pre>

<p>I get 13 as the size of the <code>parseTree.subTreeList()</code>, but for all of them, I get the <code>UnsupportedOperationException</code> on the <code>constituentTree.parent()</code> method. Can anyone help me what is the correct way to get the semantic head of ""all"" constituents in the tree?</p>
","java, stanford-nlp, parse-tree","<p>I'm not sure if it is really an answer which works for all, but in my case it was helpful:</p>

<p>Use the main <code>Tree</code> which includes the whole sentence as the second input for all constituents; that is:</p>

<pre><code>                String head = constituentTree.headTerminal(headFinder, parseTree);
</code></pre>
",0,0,82,2017-09-04 08:16:28,https://stackoverflow.com/questions/46032899/why-do-i-get-unsupportedoperationexception-with-stanford-corenlp
Finding the best preposition for a verb,"<p>I have the task of sentence completion, I have the subj, verb, adverb or subject and all I need is the appropriate preposition in between. Is there any NLP tool that can give distribution over the prepositions that can go with the verb?</p>

<p>Best </p>
","python, nlp, nltk, stanford-nlp, wordnet","<p>Here's how to get frequency counts for all verb-preposition pairs in the Brown corpus, and then look up the ones for the verb ""go"". First the counts:</p>

<pre><code>import nltk
from nltk.corpus import brown
prepchoices = nltk.ConditionalFreqDist((v[0], p[0]) 
    for (v, p) in nltk.bigrams(brown.tagged_words(tagset=""universal"")) 
        if v[1] == ""VERB"" and p[1] == ""ADP"") 
</code></pre>

<p>""ADP"" stands for ""adposition"", i.e. preposition or post-position. Now let's look at what we've got:</p>

<pre><code>&gt;&gt;&gt; prepchoices[""go""]
FreqDist({'to': 96, 'with': 20, 'into': 18, 'through': 8, 'on': 8, 'for': 7, 
'in': 5, 'out': 4, 'around': 4, 'from': 4, ...})
</code></pre>

<p>You can get the top choices, in descending order of frequency, with <code>most_common()</code>:</p>

<pre><code>&gt;&gt;&gt; print(prepchoices[""go""].most_common(5))
[('to', 96), ('with', 20), ('into', 18), ('through', 8), ('on', 8)]
</code></pre>

<p>I didn't do any stemming of the verbs (""goes"" and ""went"" were counted as separate words), or even case-folding. You could add them, but the above should already give you a decent picture of the distribution.</p>
",4,1,1318,2017-09-05 02:40:30,https://stackoverflow.com/questions/46046256/finding-the-best-preposition-for-a-verb
How does a Transition-based Dependency parser decide which operation to do next in its configuration stage?,"<p>I understand that the model uses previously trained Part of Speech tagging during its configuration stage. But what if most of the words are new, how would the parser decide its operation then? </p>
","nlp, stanford-nlp, dependency-parsing","<p>I'd like to flesh @Quantum's answer out into a detailed one as follows: </p>

<p>Before 2014 many parsers were depending on a manually designed set of feature templates, and such methods have two drawbacks: 1) they required a lot of expertise and are usually incomplete; 2) most of the runtime is consumed by the feature extraction part of the configuration stage. After Chen and Mannning published their paper, <a href=""https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf"" rel=""nofollow noreferrer"">A Fast and Accurate Dependency Parser using Neural Networks</a>, almost all parsers are relying on neural networks. </p>

<p>Let's see how Chen and Manning did the job. </p>

<p><a href=""https://i.sstatic.net/q5Pba.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q5Pba.png"" alt=""enter image description here""></a></p>

<p>As illustrated in the above diagram, the output of the neural network is a distribution after a softmax function, then it is a simple classification problem depending on some given information. The given information contains mainly three parts: the top 3 words on the stack and buffer, and the two leftmost/rightmost children of the top two words on the stack, and the leftmost and rightmost grandchildren; the POS tags of the above; and the arc labels of all children/grandchildren. </p>

<p>The inputs are embedded into a matrix and transformed by two matrices(and as shown in the picture a cube function) to become the logits and then the distribution of three elements atop of the network. </p>

<p>HTH :)</p>

<p>References: 1) <a href=""https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf"" rel=""nofollow noreferrer"">A Fast and Accurate Dependency Parser using Neural Networks</a>, 2) <a href=""https://youtu.be/7rp2c7JVymE"" rel=""nofollow noreferrer"">CMU Neural Nets for NLP 2017 (12): Transition-based Dependency Parsing</a></p>
",1,2,174,2017-09-05 16:07:17,https://stackoverflow.com/questions/46059280/how-does-a-transition-based-dependency-parser-decide-which-operation-to-do-next
How to index Spark CoreNLP analysis?,"<p>I have been using the <a href=""https://github.com/databricks/spark-corenlp"" rel=""nofollow noreferrer"">Stanford CoreNLP wrapper for Apache Spark</a> to do NEP analysis and found it works well. However, i want to extend the simple example to where I can map the analysis back to an original dataframe id. See below, I have added two more row to the simple example.</p>

<pre><code>val input = Seq(
  (1, ""&lt;xml&gt;Apple is located in California. It is a great company.&lt;/xml&gt;""),
  (2, ""&lt;xml&gt;Google is located in California. It is a great company.&lt;/xml&gt;""),
  (3, ""&lt;xml&gt;Netflix is located in California. It is a great company.&lt;/xml&gt;"")
).toDF(""id"", ""text"")

input.show()

input: org.apache.spark.sql.DataFrame = [id: int, text: string]
+---+--------------------+
| id|                text|
+---+--------------------+
|  1|&lt;xml&gt;Apple is loc...|
|  2|&lt;xml&gt;Google is lo...|
|  3|&lt;xml&gt;Netflix is l...|
+---+--------------------+
</code></pre>

<p>I can then run this dataframe through the Spark CoreNLP wrapper to do both sentiment and NEP analysis.</p>

<pre><code>val output = input
  .select(cleanxml('text).as('doc))
  .select(explode(ssplit('doc)).as('sen))
  .select('sen, tokenize('sen).as('words), ner('sen).as('nerTags), sentiment('sen).as('sentiment))
</code></pre>

<p>However, in the output below i have lost the connection back to the original dataframe row ids.</p>

<pre><code>+--------------------+--------------------+--------------------+---------+
|                 sen|               words|             nerTags|sentiment|
+--------------------+--------------------+--------------------+---------+
|Apple is located ...|[Apple, is, locat...|[ORGANIZATION, O,...|        2|
|It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
|Google is located...|[Google, is, loca...|[ORGANIZATION, O,...|        3|
|It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
|Netflix is locate...|[Netflix, is, loc...|[ORGANIZATION, O,...|        3|
|It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
+--------------------+--------------------+--------------------+---------+
</code></pre>

<p>Ideally, I want something like the following: </p>

<pre><code>+--+---------------------+--------------------+--------------------+---------+
|id|                  sen|               words|             nerTags|sentiment|
+--+---------------------+--------------------+--------------------+---------+
| 1| Apple is located ...|[Apple, is, locat...|[ORGANIZATION, O,...|        2|
| 1| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
| 2| Google is located...|[Google, is, loca...|[ORGANIZATION, O,...|        3|
| 2| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
| 3| Netflix is locate...|[Netflix, is, loc...|[ORGANIZATION, O,...|        3|
| 3| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
+--+---------------------+--------------------+--------------------+---------+
</code></pre>

<p>I have tried to create a UDF but am unable to make it work.</p>
","scala, apache-spark, stanford-nlp","<p>Using UDF defined in the <a href=""https://github.com/databricks/spark-corenlp,"" rel=""nofollow noreferrer"">Stanford CoreNLP wrapper for Apache Spark</a> you can use the following code to produced the desired output</p>

<pre><code>val output = input.withColumn(""doc"", cleanxml('text).as('doc))
  .withColumn(""sen"", ssplit('doc).as('sen))
  .withColumn(""sen"", explode($""sen""))
  .withColumn(""words"", tokenize('sen).as('words))
  .withColumn(""ner"", ner('sen).as('nerTags))
  .withColumn(""sentiment"", sentiment('sen).as('sentiment))
  .drop(""text"")
  .drop(""doc"").show()
</code></pre>

<p>will produce the following Dataframe</p>

<pre><code>+--+---------------------+--------------------+--------------------+---------+
|id|                  sen|               words|             nerTags|sentiment|
+--+---------------------+--------------------+--------------------+---------+
| 1| Apple is located ...|[Apple, is, locat...|[ORGANIZATION, O,...|        2|
| 1| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
| 2| Google is located...|[Google, is, loca...|[ORGANIZATION, O,...|        3|
| 2| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
| 3| Netflix is locate...|[Netflix, is, loc...|[ORGANIZATION, O,...|        3|
| 3| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
+--+---------------------+--------------------+--------------------+---------+
</code></pre>
",0,1,356,2017-09-11 14:28:24,https://stackoverflow.com/questions/46158125/how-to-index-spark-corenlp-analysis
Dynamically add properties to StanfordCoreNLP Annotator or Pipeline,"<p>Below my situation.</p>

<p>I have a class TextProcessor that process a text. I need to find the coreferences in such a text and then extract the informations with the Stanford's tool OpenIE. I use this two pipelines:</p>

<blockquote>
  <p><strong>""tokenize,ssplit,pos,lemma,ner,parse,mention,coref""</strong> for coreferences.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p><strong>""tokenize,ssplit,pos,lemma,depparse,natlog,openie""</strong> for Information Extraction.</p>
</blockquote>

<p>It requires lot of time to use them separately for analyzing a single text, but for the moment I have to do so cause using them together requires a large amount of memory and the pipeline would exeed my memory's bounds. </p>

<pre><code>public class TextProcessor(){
    Properties props;
    StanfordCoreNLP pipeline;

    public TextProcessor() {
        props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,mention,coref"");
        pipeline = new StanfordCoreNLP(props);
    }


    // Performs NER and COREF 
     public void process(String text) {
         Annotation document = new Annotation(malware.getDescription());
         pipeline.annotate(document);

         // Process text (tokenization, pos, lemma, ner, coref)....
     }

     public void extractInformation(String document) {
         props = new Properties();
         props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
         pipeline = new StanfordCoreNLP(props);

         Annotation doc = new Annotation(document);
         pipeline.annotate(doc);

         // Extract informations from doc ...
    }
</code></pre>

<p>Is there a way to put together the two pipelines dynamically? I mean, something like this:</p>

<blockquote>
  <p>1) <strong>""tokenize,ssplit,pos,lemma,ner,depparse,mention,coref</strong>""</p>
  
  <p>2) ""tokenize,ssplit,pos,lemma,ner,depparse,mention,coref,<strong>natlog,openie</strong>"".</p>
</blockquote>

<p>I tried to return an Annotation object from the first method <code>process(String text)</code> and then add the other three properties to it in the method  <code>extractInformation(String text)</code>, like this:</p>

<pre><code>     public Annotation process(String text) {
         Annotation document = new Annotation(malware.getDescription());
         pipeline.annotate(document);

         // Process text (tokenization, pos, lemma, ner, coref)....
         return document;
     }

     public void extractInformation(Annotation document) {
         props.setProperty(""annotators"",""depparse,natlog,openie"");
         pipeline = new StanfordCoreNLP(props);
         pipeline.annotate(document);

         // Extract informations from doc ...
    }
</code></pre>

<p>But I get this error:</p>

<p><code>annotator ""depparse"" requires annotation ""TextAnnotation"". The usual requirements for this annotator are: tokenize,ssplit,pos</code>.</p>

<p>I thought that adding the new three properties (depparse, natlog, openie) to an already annotated document (with tokenize,ssplit,pos) would work, but it didn't. </p>

<p>So, is there a way to add those properties to the oldest pipeline avoiding to perform again all the pipeline (plus the new properties) and avoid the memory to exceed its bounds?</p>

<p><br><br>
<strong>UPDATE</strong></p>

<p>All I needed to do was</p>

<pre><code>     public Annotation process(String text) {
         Annotation document = new Annotation(malware.getDescription());
         pipeline.annotate(document);

         // Process text (tokenization, pos, lemma, ner, coref)....
         StanfordCoreNLP.clearAnnotatorPool(); // &lt;-- Added: to get rid of the models and solve the memory issue
         return document;
     }

     public void extractInformation(Annotation document) {
         props.setProperty(""annotators"",""natlog,openie"");

         props.setProperty(""enforceRequirements"", ""false"") //&lt;-- Added

         pipeline = new StanfordCoreNLP(props);
         pipeline.annotate(document);

         // Extract informations from doc ...
    }
</code></pre>

<p>Alternatively, you can use: </p>

<pre><code>pipeline = new StanfordCoreNLP(props, false);
</code></pre>

<p>in extractInformation(Annotation document).</p>
","java, nlp, stanford-nlp","<p>It sounds like you want to build a first pipeline, run it on a set of documents, clear the memory, and then build a second pipeline and run it on the set of documents.</p>

<p>If you run the second pipeline on the same set of Annotations, it will just pick up where the first pipeline finished.  But you need to set <code>enforceRequirements</code> to <code>false</code> so the second pipeline won't crash.  Also after you are done using the first pipeline you should run <code>StanfordCoreNLP.clearAnnotatorPool();</code> to get rid of the models or you won't solve the memory issue.</p>
",4,1,728,2017-09-13 09:54:07,https://stackoverflow.com/questions/46194378/dynamically-add-properties-to-stanfordcorenlp-annotator-or-pipeline
"For Tokensregex, does a rule need to be token type to use Annotate?","<p>I am going over some older code Tokensregex code and I am faced with a situation where some characters are not being tokenized by the PTBTokenizer.  In particular, I am looking at currency symbols.  So for example, ₱ would not be a token whereas some others are such as $ would.</p>

<p>Well I want to try to write a text type rule instead of token type to try to catch this symbol otherwise in a capture group and then do something like <code>Annotate($0, ner, ""MONEY"")</code> to capture a string such as ₱240.</p>

<p>When I attempt this I get:</p>

<blockquote>
  <p>... 49 more Caused by: java.lang.ClassCastException:
  edu.stanford.nlp.ling.tokensregex.TokenSequencePattern cannot be cast
  to java.lang.String   at
  edu.stanford.nlp.ling.tokensregex.SequenceMatchRules$TextPatternExtractRuleCreator.create(SequenceMatchRules.java:666)
    at
  edu.stanford.nlp.ling.tokensregex.SequenceMatchRules.createExtractionRule(SequenceMatchRules.java:331)
    at
  edu.stanford.nlp.ling.tokensregex.SequenceMatchRules.createRule(SequenceMatchRules.java:321)
    at
  edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.Rule(TokenSequenceParser.java:141)
    at
  edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.RuleList(TokenSequenceParser.java:125)
    at
  edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.updateExpressionExtractor(TokenSequenceParser.java:32)
    at
  edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.createExtractorFromFiles(CoreMapExpressionExtractor.java:292)
    ... 52 more</p>
</blockquote>

<p>So can I do the above, create a MONEY ner annotation. if the currency symbol is missed by the tokenizer?</p>

<p><strong>EXAMPLE</strong></p>

<p>Text rule attempt at doing what I want (create ner annotation called CURRENCY for a string containing a peso monetary value)</p>

<pre><code>ENV.defaults[""ruleType""] = ""text""
{ text: /(₱\d+)/ =&gt; Annotate($0, ner, ""CURRENCY"")}
</code></pre>

<p>Token rule successfully doing what I want (because yen is a recoginized token).  This creates a yen monetary string with ner annotation of CURRENCY.</p>

<pre><code>ENV.defaults[""ruleType""] = ""tokens""
ENV.defaults[""matchWithResults""] = TRUE

# Set default string pattern flags (to case-insensitive)
ENV.defaultStringPatternFlags = 2

ENV.defaults[""stage""] = 0

# Ex: ¥3000
{   
pattern:  ([{ word: ""¥"" }] $NUMBER_COMMA_SEP $LARGE_NUMBERS?),
action: (Annotate($0, ner, ""CURRENCY""))
}
</code></pre>

<p>ner is defined as:</p>

<pre><code>ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
</code></pre>

<p>and then:</p>

<pre><code>$NUMBER_COMMA_SEP = ""$NUMBER_NON_CD | ([{ tag: /CD/ } &amp; $VALID_NUMERIC_CHARS] [{ tag: /CD/; word: /,\d+(\.\d+)?/ }]*)""
$LARGE_NUMBERS = ""/thousand|million|mil|mn|billion|bil|bn|trillion/""
</code></pre>
",stanford-nlp,"<p>You need to make sure the tokenizer is not deleting untokenizable tokens.</p>

<p>command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,tokensregex -tokensregex.rules example-rules.txt -props StanfordCoreNLP-spanish.properties -tokenize.options ""untokenizable=allKeep"" -file example.txt -outputFormat text
</code></pre>

<p>example-rules.txt</p>

<pre><code>ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }

{ pattern: ( /₱/ /[0-9]+/ ) , action: (Annotate($0, ner, ""CURRENCY"") ) }
</code></pre>

<p>If you run on text with that symbol with the tokenizer configured properly it will create a distinct token for that symbol.</p>
",1,0,331,2017-09-18 22:18:54,https://stackoverflow.com/questions/46288962/for-tokensregex-does-a-rule-need-to-be-token-type-to-use-annotate
Stanford corenlp interactive tregex tool results differ from the constituency parse,"<p>Can anyone explain why nlp tags from tregex response differs from the tags obtained in constituency parse as shown in the figure below.</p>

<p><a href=""https://i.sstatic.net/PNXlI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PNXlI.png"" alt=""enter image description here""></a></p>

<p>In the above figure, <strong>engineer</strong> is tagged as <strong>NN</strong> by <em>constituency parse</em> annotator, but <em>tregex</em> outputs it as <strong>NNP</strong>.</p>

<p>Is it because the annotator pipeline used to perform constituency parse uses a different parse model compared to the pipeline used to perform tregex?</p>
",stanford-nlp,"<p>It appears different pipelines are being used.</p>

<p>When you run the standard annotation process it will use the pipeline you specify, which it appears in your example includes the <code>pos</code> annotator.  Since the <code>pos</code> annotator's tags are being used, you are seeing the <code>NN</code>.</p>

<p>When you submit a <code>tregex</code> request, it is simply running a pipeline with <code>tokenize,ssplit,parse</code> (you can see this in the code for StanfordCoreNLPServer.java , which has a specific tregex handler)  </p>

<p>This means it is using the constituency parser's part of speech tagging, which produces a different result than the dedicated part-of-speech tagger's results.  In this case the constituency parser applies the tag of <code>NNP</code>.  I should note if you use the shift reduce parser it will require the part of speech tags to be provided by the part of speech tagger, whereas the lexical parser has the ability to create it's own part of speech tags.</p>
",0,0,70,2017-09-19 07:12:42,https://stackoverflow.com/questions/46294162/stanford-corenlp-interactive-tregex-tool-results-differ-from-the-constituency-pa
Standard way to start and stop StanfordCoreNLP server in python?,"<p>I used subprocess to start CoreNLP server,</p>

<ol>
<li><p><code>command = 'java -mx4g -cp ""*;stanford-corenlp-full-2017-06-09/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000'</code></p></li>
<li><p><code>server  = subprocess.Popen(command, stdout=subprocess.PIPE)</code></p></li>
</ol>

<p>to stop the server i have used : </p>

<ol start=""3"">
<li><code>server.kill()</code></li>
</ol>

<p>for the first time running there is no problem,</p>

<p>if i try to start all the above steps again, then it is giving an error:</p>

<blockquote>
  <p>error is: </p>
</blockquote>

<pre><code>    java.net.BindException: Address already in use: bind
    at sun.nio.ch.Net.bind0(Native Method)
    at sun.nio.ch.Net.bind(Unknown Source)
    at sun.nio.ch.Net.bind(Unknown Source)
    at sun.nio.ch.ServerSocketChannelImpl.bind(Unknown Source)
    at sun.nio.ch.ServerSocketAdaptor.bind(Unknown Source)
    at sun.net.httpserver.ServerImpl.&lt;init&gt;(Unknown Source)
    at sun.net.httpserver.HttpServerImpl.&lt;init&gt;(Unknown Source)
    at sun.net.httpserver.DefaultHttpServerProvider.createHttpServer(Unknown Source)
    at com.sun.net.httpserver.HttpServer.create(Unknown Source)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.run(StanfordCoreNLPServer.java:1277)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1369)
</code></pre>

<p>could anyone help me out. </p>
","python-3.x, subprocess, stanford-nlp","<p>You should send a shutdown message with the shutdown key.  Here is an example command line call:</p>

<pre><code>wget ""localhost:9000/shutdown?key=`cat /tmp/corenlp.shutdown`"" -O -
</code></pre>

<p>You could execute such a command with <code>subprocess</code> or <code>os.system</code> etc...</p>

<p>Note that the shutdown key is located at <code>/tmp/corenlp.shutdown</code> unless you specify a different name.</p>

<p>If you want to be nicer you could also use the <code>requests</code> library:</p>

<pre><code>import requests

from commands import getoutput

url = ""http://localhost:9000/shutdown?""
shutdown_key = getoutput(""cat /tmp/corenlp.shutdown"")
r = requests.post(url,data="""",params={""key"": shutdown_key})
</code></pre>

<p>That will transmit the shutdown message to the server as well.</p>
",3,1,1509,2017-09-19 10:44:07,https://stackoverflow.com/questions/46298404/standard-way-to-start-and-stop-stanfordcorenlp-server-in-python
How to find the future tense of a word using stanford nlp,"<p>I want to ultimately determine whether the tense of a sentence is the future tense. My strategy is to find the head verb using StanfordCoreNLP. Then to examine the tense of the verb itself and the auxillary verbs to find if the sentence is future tense.</p>

<p>Do you know how I can decide the tense of a specific verb and the auxillary verbs associated with it?</p>

<p>Thanks</p>
","machine-learning, nlp, stanford-nlp","<p>POS tags partly give you the tense. 
Here is full list of POS tags and their description: 
<a href=""https://i.sstatic.net/yEHjq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yEHjq.png"" alt=""enter image description here""></a></p>

<p>As you can see ""VBD"" is ""past tense"". All the followings have VBD label somewhere: </p>

<p><a href=""https://i.sstatic.net/OTaEU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OTaEU.png"" alt=""enter image description here""></a></p>

<p>For future tense you can use modal label ""MD"":</p>

<p><a href=""https://i.sstatic.net/zx0MT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zx0MT.png"" alt=""enter image description here""></a></p>

<p>For case with compound verbs, I'd use shallow-parser with some rules: </p>

<p><a href=""https://i.sstatic.net/tHIiO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tHIiO.png"" alt=""enter image description here""></a></p>

<p>In this case ""is going to be"" is an indicator for future tense. Here is the complete list of shallow-parse labels: (from: <a href=""http://cogcomp.org/page/demo_view/ShallowParse"" rel=""nofollow noreferrer"">http://cogcomp.org/page/demo_view/ShallowParse</a>)
<a href=""https://i.sstatic.net/rocr1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rocr1.png"" alt=""enter image description here""></a></p>

<p>Here is a demo if you wanna work more with it: <a href=""http://nlp.cogcomp.org/"" rel=""nofollow noreferrer"">http://nlp.cogcomp.org/</a> </p>
",3,2,1265,2017-09-19 14:48:48,https://stackoverflow.com/questions/46303537/how-to-find-the-future-tense-of-a-word-using-stanford-nlp
How to extract/identify word or text from the given text using stanford-nlp or OpenNLP via java,"<p>I am about to extract some information from the RAW tests published in Social Media, News sites, blogs which are related to a specific field such as politics, WAR, DRUGS etc. So I am already started to use some open source libs such as stanford-nlp, apache OpenNLP as well as a commercial licensed tool called lexalytics.</p>

<p>According to my project, we are analyzing text posted in publicly and generating some results and doing mining based on some parameters to identify those post are related to what category.</p>

<p>But I need to extract topics from the given text using stanford-nlp library. Topic means text or sentences related to EDUCATION, POLITICS such that. Already I am able to extract entities like text/sentences contain LOCATION, DATE, PERSON, MONEY such a way.</p>

<p>Same topic extraction coming with lexalytics as well which is licenses tool. </p>

<p>your help appreciated.</p>

<p>Thanks.</p>
","java, nlp, stanford-nlp, opennlp","<p>Topic extraction from text documents can be done using generative modeling where words distributed are assumed a prior based on topic(s) for which algorithms like LDA are deployed  </p>

<p>Open Calais API <code>http://www.opencalais.com/opencalais-api/</code> gives you single/multiple topics with associated confidence values on which the document was based.</p>
",0,0,977,2017-09-21 09:26:20,https://stackoverflow.com/questions/46340205/how-to-extract-identify-word-or-text-from-the-given-text-using-stanford-nlp-or-o
CoreNLP&#39;s GenderAnnotation is unable to label names written in proper format,"<p>Given the name ""David"" presented in three different ways (""DAVID david David""), CoreNLP is only able to mark #1 and #2 as MALE despite the fact that #3 is the only one marked as a PERSON. I'm using the standard model provided originally and I attempted to implement the suggestions listed <a href=""https://stackoverflow.com/a/31355961/6706943"">here</a> but 'gender' is not allowed before NER anymore. My test is below with the same results in both Java and Jython (Word, Gender, NER Tag):</p>

<pre><code>DAVID, MALE, O
david, MALE, O
David, None, PERSON
</code></pre>
",stanford-nlp,"<p>This is a bug in Stanford CoreNLP 3.8.0.</p>

<p>I have made some modifications to the GenderAnnotator and submitted them.  They are available now on GitHub.  I am still working on this, so probably over the next day or so there will be further changes, but I think this bug is fixed now.  You will also need the latest version of the models jar which was just updated that contains the name lists.  I believe shortly I will build another models jar with larger name lists.</p>

<p>The new version of GenderAnnotator requires the <code>entitymentions</code> annotator to be used.  Also, the new version logs the gender of both the CoreMap for the entity mention and for each token of the entity mention.</p>

<p>You can learn how to work with the latest version of Stanford CoreNLP off of GitHub here: <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a></p>
",1,0,158,2017-09-24 23:46:07,https://stackoverflow.com/questions/46395891/corenlps-genderannotation-is-unable-to-label-names-written-in-proper-format
How to split the text properly in python for glove?,"<p>In glove, punctuation like '.' is counted as a word. but in the case of u.s. and u.k. .it cannot be separated.</p>

<p>For example, there is a sentence.</p>

<p>he's going to u.s..</p>

<p>What glove want is ['he', ''s', 'going', 'to', 'u.s.', '.']
Are there any good ways to split that?</p>
","python, split, nlp, stanford-nlp","<p>You should split the input the same way the input used in training was split. If you are using pre-trained vectors and don't know how they were generated, you can train your own vectors or ask the creator how they tokenized their input.</p>

<p>Also, as a note, sentences don't end with a double period even if the last word is an abbreviation. </p>

<pre><code>wrong: He's going to the U.S..
right: He's going to the U.S.
</code></pre>

<p>You can read a more detailed explanation of that <a href=""http://www.quickanddirtytips.com/education/grammar/ending-sentence-abbreviation"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Also note that in modern English it's very common to not use periods in abbreviations - as an example, <a href=""https://www.theguardian.com/international"" rel=""nofollow noreferrer"">The Guardian</a> has sections for ""US News"" and  ""UK News"", without periods. As a practical matter, I think you don't need to worry about this particular issue unless it comes up a lot in your specific dataset.</p>
",1,0,375,2017-09-25 06:35:17,https://stackoverflow.com/questions/46398947/how-to-split-the-text-properly-in-python-for-glove
Preparing data for (stanford) Deepdive (ValueError),"<p>I started using Stanford-Deepdive a while ago. 
I am currently facing the problem, that deepdive will interpret some of the rows he gets as incomplete. </p>

<pre><code>Value Error: Expected 6 attributes, but found 5 in input row:
&lt;Row()&gt;
</code></pre>

<p>I already had this problem with another data-set. At this set there were some rows, that contained ""\n"" within the text. So i removed that and everything went flawlessly.</p>

<p>For my new set of data I am removing ""\n"", ""\t"", and any occurence of multiple spaces. Also I replace any empty text value by ""EMPTY"" - still the error refuses to go away.</p>

<p>Are there any other formatting errors or characters that I need to take care of?
Is my way of approaching this reasonable? </p>
","postgresql, nlp, special-characters, stanford-nlp","<p>I found the problem. It was caused by a singular TAB (\t) entry. I replaced that by a singe SPACE and in the end it would not be a valid antry anymore </p>

<p>so if you use some text for deepdive you will want to treat etrys consisting of a single SPACE as if they were empty.</p>
",0,1,42,2017-09-28 08:25:04,https://stackoverflow.com/questions/46464353/preparing-data-for-stanford-deepdive-valueerror
How to work around 100K character limit for the StanfordNLP server?,"<p>I am trying to parse book-length blocks of text with StanfordNLP. The http requests work great, but there is a non-configurable 100KB limit to the text length, MAX_CHAR_LENGTH in StanfordCoreNLPServer.java.</p>

<p>For now, I am chopping up the text before I send it to the server, but even if I try to split between sentences and paragraphs, there is some useful coreference information that gets lost between these chunks. Presumably, I could parse chunks with large overlap and link them together, but that seems (1) inelegant and (2) like quite a bit of maintenance.</p>

<p>Is there a better way to configure the server or the requests to either remove the manual chunking or preserve the information across chunks?</p>

<p>BTW, I am POSTing using the python requests module, but I doubt that makes a difference unless a corenlp python wrapper deals with this problem somehow.</p>
",stanford-nlp,"<p>You should be able to start the server with the flag <code>-maxCharLength -1</code> and that'll get rid of the sentence length limit. Note that this is inadvisable in production: arbitrarily large documents can consume arbitrarily large amounts of memory (and time), especially with things like coref.</p>

<p>The list of options to the server should be accessible by calling the server with <code>-help</code>, and are <a href=""https://github.com/stanfordnlp/CoreNLP/blob/76f2bcdcde0d2266e7ab50ebb95a629c407f5922/src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java#L84"" rel=""nofollow noreferrer"">documented in code here</a>.</p>
",1,3,1469,2017-10-11 01:09:44,https://stackoverflow.com/questions/46678204/how-to-work-around-100k-character-limit-for-the-stanfordnlp-server
How to use the openie.triple.strict option in StanfordNLP parser?,"<p>I am new to NLP and want an example to help me understand how to use the openie.triple.strict option under the openie property of the StanfordNLP parser.</p>
","stanford-nlp, information-extraction","<p>The way to the same is:-</p>

<p>Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,mention,coref, natlog, openie"");</p>

<p>props.setProperty(""openie.resolve_coref"", ""false""); //default = true</p>

<p>Thanks <a href=""https://stackoverflow.com/users/1473431/gabor-angeli"">Gabor Angeli</a> for the tip!</p>
",1,1,496,2017-10-11 06:38:40,https://stackoverflow.com/questions/46681390/how-to-use-the-openie-triple-strict-option-in-stanfordnlp-parser
Stanford POS Tagger to return more then one tag,"<p>I am implementing POS tagging with the Stanford POS Tagger. However, some of the words can have multiple tags. For example word <code>heat</code>, can be noun or verb. However POS tagger return only one value for current sentence - NOUN. Is it possible to return all possible POS tags using Stanford POS Tagger. Which means that for the word <code>heat</code>, I can get NOUN and VERB?</p>
",stanford-nlp,"<p>The POS tagger is designed to tag a word with the proper POS tag based on the context of the sentence.</p>
",2,0,203,2017-10-11 11:03:48,https://stackoverflow.com/questions/46686617/stanford-pos-tagger-to-return-more-then-one-tag
stanford NLP API error in java maven project,"<p>I have been working on this project for a while. I'm using stanford NLP. I've added its dependency in my maven project. it used to work completely fine but now it shows this error: (I even downloaded similar project from git to find out if something is wrong done by me yet same message is displayed for that too. please let me know the problem)</p>

<pre><code>Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos

    Exception in thread ""main"" java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:493)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:260)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:127)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:123)
        at CoreNlpExample.main(CoreNlpExample.java:17)
    Caused by: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:749)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:283)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:247)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:78)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:62)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:491)
        ... 5 more
    Caused by: java.io.IOException: Unable to resolve ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
        at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:419)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:744)
        ... 10 more
</code></pre>
","java, maven, stanford-nlp","<p>You need to include the models jar in your classpath. You can either download this manually, or include it with maven via the instructions at <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a> (replace version as appropriate):</p>

<pre><code>&lt;dependencies&gt; &lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.8.0&lt;/version&gt; 
  &lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.8.0&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt; 
  &lt;/dependency&gt; 
&lt;/dependencies&gt;
</code></pre>
",1,1,354,2017-10-12 04:12:36,https://stackoverflow.com/questions/46701294/stanford-nlp-api-error-in-java-maven-project
"NLTK Stanford Segmentor, how to set CLASSPATH","<p>I'm trying to use the Stanford Segementer bit from the NLTK Tokenize package. However, I run into issues just trying to use the basic test set. Running the following:</p>

<pre><code># -*- coding: utf-8 -*-
from nltk.tokenize.stanford_segmenter import StanfordSegmenter
seg = StanfordSegmenter()
seg.default_config('zh')
sent = u'这是斯坦福中文分词器测试'
print(seg.segment(sent))
</code></pre>

<p>Results in this error: 
<a href=""https://i.sstatic.net/bXjyE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bXjyE.png"" alt=""Error""></a></p>

<p>I got as far as to add...</p>

<pre><code>import os
javapath = ""C:/Users/User/Folder/stanford-segmenter-2017-06-09/*""
os.environ['CLASSPATH'] = javapath
</code></pre>

<p>...to the front of my code, but that didn't seem to help. </p>

<p>How do I get the segmentor to run properly? </p>
","java, python, classpath, nltk, stanford-nlp","<p>Note: This solution would <strong>only work for</strong>:</p>

<ul>
<li>NLTK v3.2.5 (v3.2.6 would have an even simpler interface)</li>
<li>Stanford CoreNLP (version >= 2016-10-31)</li>
</ul>

<hr>

<p>First you have to get Java 8 properly installed first and if Stanford CoreNLP works on command line, the Stanford CoreNLP API in NLTK v3.2.5 is as follows.</p>

<p><strong>Note:</strong> You have to start the CoreNLP server in terminal <strong>BEFORE</strong> using the new CoreNLP API in NLTK.</p>

<h1>English</h1>

<p>In terminal:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31

java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-preload tokenize,ssplit,pos,lemma,parse,depparse \
-status_port 9000 -port 9000 -timeout 15000
</code></pre>

<p>In Python:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger
&gt;&gt;&gt; stpos, stner = CoreNLPPOSTagger(), CoreNLPNERTagger()
&gt;&gt;&gt; stpos.tag('What is the airspeed of an unladen swallow ?'.split())
[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]
&gt;&gt;&gt; stner.tag('Rami Eid is studying at Stony Brook University in NY'.split())
[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]
</code></pre>

<h1>Chinese</h1>

<p>In terminal:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31
wget http://nlp.stanford.edu/software/stanford-chinese-corenlp-2016-10-31-models.jar
wget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-chinese.properties 

java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-serverProperties StanfordCoreNLP-chinese.properties \
-preload tokenize,ssplit,pos,lemma,ner,parse \
-status_port 9001  -port 9001 -timeout 15000
</code></pre>

<p>In Python</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger
&gt;&gt;&gt; from nltk.tokenize.stanford import CoreNLPTokenizer
&gt;&gt;&gt; stpos, stner = CoreNLPPOSTagger('http://localhost:9001'), CoreNLPNERTagger('http://localhost:9001')
&gt;&gt;&gt; sttok = CoreNLPTokenizer('http://localhost:9001')

&gt;&gt;&gt; sttok.tokenize(u'我家没有电脑。')
['我家', '没有', '电脑', '。']

# Without segmentation (input to`raw_string_parse()` is a list of single char strings)
&gt;&gt;&gt; stpos.tag(u'我家没有电脑。')
[('我', 'PN'), ('家', 'NN'), ('没', 'AD'), ('有', 'VV'), ('电', 'NN'), ('脑', 'NN'), ('。', 'PU')]
# With segmentation
&gt;&gt;&gt; stpos.tag(sttok.tokenize(u'我家没有电脑。'))
[('我家', 'NN'), ('没有', 'VE'), ('电脑', 'NN'), ('。', 'PU')]

# Without segmentation (input to`raw_string_parse()` is a list of single char strings)
&gt;&gt;&gt; stner.tag(u'奥巴马与迈克尔·杰克逊一起去杂货店购物。')
[('奥', 'GPE'), ('巴', 'GPE'), ('马', 'GPE'), ('与', 'O'), ('迈', 'O'), ('克', 'PERSON'), ('尔', 'PERSON'), ('·', 'O'), ('杰', 'O'), ('克', 'O'), ('逊', 'O'), ('一', 'NUMBER'), ('起', 'O'), ('去', 'O'), ('杂', 'O'), ('货', 'O'), ('店', 'O'), ('购', 'O'), ('物', 'O'), ('。', 'O')]
# With segmentation
&gt;&gt;&gt; stner.tag(sttok.tokenize(u'奥巴马与迈克尔·杰克逊一起去杂货店购物。'))
[('奥巴马', 'PERSON'), ('与', 'O'), ('迈克尔·杰克逊', 'PERSON'), ('一起', 'O'), ('去', 'O'), ('杂货店', 'O'), ('购物', 'O'), ('。', 'O')]
</code></pre>

<h1>German</h1>

<p>In terminal:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31

wget http://nlp.stanford.edu/software/stanford-german-corenlp-2016-10-31-models.jar
wget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-german.properties

java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-serverProperties StanfordCoreNLP-german.properties \
-preload tokenize,ssplit,pos,ner,parse \
-status_port 9002  -port 9002 -timeout 15000
</code></pre>

<p>In Python:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger
&gt;&gt;&gt; stpos, stner = CoreNLPPOSTagger('http://localhost:9002'), CoreNLPNERTagger('http://localhost:9002')

&gt;&gt;&gt; stpos.tag('Ich bin schwanger'.split())
[('Ich', 'PPER'), ('bin', 'VAFIN'), ('schwanger', 'ADJD')]

&gt;&gt;&gt; stner.tag('Donald Trump besuchte Angela Merkel in Berlin.'.split())
[('Donald', 'I-PER'), ('Trump', 'I-PER'), ('besuchte', 'O'), ('Angela', 'I-PER'), ('Merkel', 'I-PER'), ('in', 'O'), ('Berlin', 'I-LOC'), ('.', 'O')]
</code></pre>

<h1>Spanish</h1>

<p>In terminal:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31

wget http://nlp.stanford.edu/software/stanford-spanish-corenlp-2016-10-31-models.jar
wget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-spanish.properties

java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-serverProperties StanfordCoreNLP-spanish.properties \
-preload tokenize,ssplit,pos,ner,parse \
-status_port 9003  -port 9003 -timeout 15000
</code></pre>

<p>In Python:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger
&gt;&gt;&gt; stpos, stner = CoreNLPPOSTagger('http://localhost:9003'), CoreNLPNERTagger('http://localhost:9003')

&gt;&gt;&gt; stner.tag(u'Barack Obama salió con Michael Jackson .'.split())
[(u'Barack', u'PERS'), (u'Obama', u'PERS'), (u'sali\xf3', u'O'), (u'con', u'O'), (u'Michael', u'PERS'), (u'Jackson', u'PERS'), (u'.', u'O')]

&gt;&gt;&gt; stpos.tag(u'Barack Obama salió con Michael Jackson .'.split())
[(u'Barack', u'np00000'), (u'Obama', u'np00000'), (u'sali\xf3', u'vmis000'), (u'con', u'sp000'), (u'Michael', u'np00000'), (u'Jackson', u'np00000'), (u'.', u'fp')]
</code></pre>

<h1>French</h1>

<p>In terminal:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31

wget http://nlp.stanford.edu/software/stanford-french-corenlp-2016-10-31-models.jar
wget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-french.properties

java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-serverProperties StanfordCoreNLP-french.properties \
-preload tokenize,ssplit,pos,parse \
-status_port 9004  -port 9004 -timeout 15000
</code></pre>

<p>In Python:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import CoreNLPPOSTagger
&gt;&gt;&gt; stpos = CoreNLPPOSTagger('http://localhost:9004')
&gt;&gt;&gt; stpos.tag('Je suis enceinte'.split())
[(u'Je', u'CLS'), (u'suis', u'V'), (u'enceinte', u'NC')]
</code></pre>

<h1>Arabic</h1>

<p>In terminal:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31

wget http://nlp.stanford.edu/software/stanford-arabic-corenlp-2016-10-31-models.jar
wget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-arabic.properties

java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-serverProperties StanfordCoreNLP-french.properties \
-preload tokenize,ssplit,pos,parse \
-status_port 9005  -port 9005 -timeout 15000
</code></pre>

<p>In Python:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import CoreNLPPOSTagger
&gt;&gt;&gt; from nltk.tokenize.stanford import CoreNLPTokenizer
&gt;&gt;&gt; sttok = CoreNLPTokenizer('http://localhost:9005')
&gt;&gt;&gt; stpos = CoreNLPPOSTagger('http://localhost:9005')
&gt;&gt;&gt; text = u'انا حامل'
&gt;&gt;&gt; stpos.tag(sttok.tokenize(text))
[('انا', 'DET'), ('حامل', 'NC')]
</code></pre>
",3,2,1873,2017-10-13 16:18:07,https://stackoverflow.com/questions/46734119/nltk-stanford-segmentor-how-to-set-classpath
stanford nlp api for java: how to get the name as full not in parts,"<p>the aim of my code is to submit a document (be it pdf or doc file) and get all the text in it. give the text to be analysed by stanford nlp. the code works just fine. but suppose there is name in the document eg: ""Pardeep    Kumar"" . the output recieved for it, is as follows:</p>

<blockquote>
  <p>Pardeep       NNP          PERSON</p>
  
  <p>Kumar       NNP          PERSON</p>
</blockquote>

<p>but i want it to be like this: </p>

<blockquote>
  <p>Pardeep Kumar      NNP          PERSON</p>
</blockquote>

<p>how do i do that?how do i check two words placed adjacently that actually make one name or anything similar? how do i not let them be split in different words?</p>

<p>here is my code:</p>

<pre><code>public class readstuff {

      public static void analyse(String data) {

            // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
            Properties props = new Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");

            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);


            // create an empty Annotation just with the given text
            Annotation document = new Annotation(data);

            // run all Annotators on this text
            pipeline.annotate(document);

            List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

            // System.out.println(""word""+""\t""+""POS""+""\t""+""NER"");
            for (CoreMap sentence : sentences) {

                // traversing the words in the current sentence
                // a CoreLabel is a CoreMap with additional token-specific methods

                for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                    // this is the text of the token
                    String word = token.get(CoreAnnotations.TextAnnotation.class);
                    // this is the POS tag of the token
                    String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
                    // this is the NER label of the token
                    String ne = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);

                    if(ne.equals(""PERSON"") || ne.equals(""LOCATION"") || ne.equals(""DATE"") )
                    {

                        System.out.format(""%32s%10s%16s"",word,pos,ne);
                        System.out.println();
                    //System.out.println(word +""       \t""+pos +""\t""+ne);
                    }

                }
            }
        }

    public static void main(String[] args) throws FileNotFoundException, IOException, TransformerConfigurationException{

        JFileChooser window=new JFileChooser();
        int a=window.showOpenDialog(null);

        if(a==JFileChooser.APPROVE_OPTION){
            String name=window.getSelectedFile().getName();
            String extension = name.substring(name.lastIndexOf(""."") + 1, name.length());
            String data = null;

            if(extension.equals(""docx"")){
                XWPFDocument doc=new XWPFDocument(new FileInputStream(window.getSelectedFile()));
                XWPFWordExtractor extract= new XWPFWordExtractor(doc);
                //System.out.println(""docx file reading..."");
                data=extract.getText();
                //extract.getMetadataTextExtractor();
            }
            else if(extension.equals(""doc"")){
                HWPFDocument doc=new HWPFDocument(new FileInputStream(window.getSelectedFile()));
                WordExtractor extract= new WordExtractor(doc);
                //System.out.println(""doc file reading..."");
                data=extract.getText();
            }
            else if(extension.equals(""pdf"")){
                //System.out.println(window.getSelectedFile());
                PdfReader reader=new PdfReader(new FileInputStream(window.getSelectedFile()));
                int n=reader.getNumberOfPages();
                for(int i=1;i&lt;n;i++)
                {
                    //System.out.println(data);
                data=data+PdfTextExtractor.getTextFromPage(reader,i );
                }
            }
            else{
                System.out.println(""format not supported"");
            }

        analyse(data);  
        }
    }



}
</code></pre>
","java, stanford-nlp","<p>You want to use the <code>entitymentions</code> annotator.</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.util.*;

import java.util.*;

public class EntityMentionsExample {

  public static void main(String[] args) {
    Annotation document =
        new Annotation(""John Smith visited Los Angeles on Tuesday. He left Los Angeles on Wednesday."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);

    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreMap entityMention : sentence.get(CoreAnnotations.MentionsAnnotation.class)) {
        System.out.println(entityMention);
        System.out.println(entityMention.get(CoreAnnotations.EntityTypeAnnotation.class));
      }
    }
  }
}
</code></pre>
",1,1,687,2017-10-17 10:04:55,https://stackoverflow.com/questions/46787542/stanford-nlp-api-for-java-how-to-get-the-name-as-full-not-in-parts
How to replace a token (CoreLabel) in a sentence (CoreMap) using Stanford NLP?,"<p>As usual, I traverse the sentences of an annotated document in a for-loop (Java):</p>

<pre><code>for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
    ...
}
</code></pre>

<p>Then, inside that, I remove a word from the sentence (e.g. ""teacher"") using the word index, I set the new word text to be ""John"" using the <code>CoreLabel</code> method <code>setWord()</code> and in the end I add the renewed word in the sentence in the same index:</p>

<pre><code>sentence.get(CoreAnnotations.TokensAnnotations.class).remove(token.get(CoreAnnotations.IndexAnnotation.class));
token.setWord(""John"");
sentence.get(CoreAnnotations.TokensAnnotation.class).add(token.get(CoreAnnotations.IndexAnnotation.class),token);
</code></pre>

<p>The problem is that the sentence stays as is. Even if I print the sentence text right after the removal, it won't change. Am I doing something wrong? Is there a more reasonable way?</p>
","java, nlp, stanford-nlp","<p>I'm going to venture that even though you've changed the word, you haven't changed the <code>originalText</code>. In general, you should be a bit wary of these sorts of transformations -- they can have all sorts of bizarre effects (e.g., your character offsets will be broken), but if you're feeling brave and want to fix the bug at hand, you should be able to fix it by setting:</p>

<pre><code>token.setOriginalText(""John"");
</code></pre>
",1,0,269,2017-10-18 15:01:40,https://stackoverflow.com/questions/46813335/how-to-replace-a-token-corelabel-in-a-sentence-coremap-using-stanford-nlp
Is it possible to use StanfordCoreNLP to get constituency parsing while using a preexisting tokenization?,"<p>As per the title of this post, I would like to know if it is possible to use StanfordCoreNLP to get a constituency parsing of a text while using a pre-existing, potentially externaly provided tokenization of the text. I am looking to do this on texts written in French. I guess that if it were possible, only the file containing the description the tokens would be necessary, since a version of the text could be reconstructed from this file.
I suppose it might be possible if one were to program in java, and directly use the java classes, but since I do not know much java, I wanted to know if such a thing was possible using command-line instruction.
Has anyone knowledge about such a thing?</p>

<p>I searched for an answer to this question by googling, and browsing the StanfordCoreNLP site (<a href=""https://nlp.stanford.edu/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/</a>), especially this page <a href=""https://nlp.stanford.edu/software/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/</a>, but did not find what I was looking for. When looking for a way to get info about the software, I found that we are told to ask a question on StackOverflow.</p>

<p>Now for a precise formulation of my question: is there a way to use the StanfordCoreNLP with the command-line interface so as to get constituency parsing info on a text written in French while forcing StanfordCoreNLP to respect a pre-existing, input tokenization of that text? If the answer is yes, where can I document myself about such a way?</p>

<p><strong>Edit:</strong>
<em>Example :</em>
I will provide an example of such a thing being done on a text written in English :</p>

<p>Raw text :
« John went on a trip; which was quite nice. » </p>

<p>Tokenized text :
«  John went on a trip ; which was quite nice . »
(Here, the difference from the raw text is that the punctuation marks were separated from their respective preceding word)</p>

<p>Constituency parsing of the text :
«  (ROOT (S (NP (NNP John)) (VP (VBD went) (PP (IN on) (NP (NP (DT a) (NN trip)) (: ;) (SBAR (WHNP (WDT which)) (S (VP (VBD was) (ADJP (RB quite) (JJ nice)))))))) (. .))) »</p>

<p>As you can see, the constituency parsing can be seen as an annotation of the result of the tokenization step. I currently know how to use the StanfordCoreNLP suite to compute constituency parsing information, among other type of information, by providing the raw text, but I guess that, in order to achieve that, the StanfordCoreNLP suite carries out its own tokenization step.</p>

<p>I would like to know if there is a way to force the StanfordCoreNLP suite to use / respect a pre-defined tokenization of a text in French.</p>

<p><strong>Edit 2:</strong></p>

<p>Thanks for the answer. Incidentally, that made me learn how one can parametrize the different annotators used during a StanfordCoreNLP's pipeline process, using the ""{annotator_name}.{option_name}"" format on the command line; so next time I will be able to better understand the StanfordCoreNLP's documentation when browsing it.</p>
",stanford-nlp,"<p>Use the <code>tokenize.whitespace</code> option, and provide your text tokenized by whitespace.  That option will only create words separated by whitespace.</p>
",1,1,163,2017-10-19 14:59:25,https://stackoverflow.com/questions/46832933/is-it-possible-to-use-stanfordcorenlp-to-get-constituency-parsing-while-using-a
Stanford Chinese Word Segmenter in Android Studio,"<p>I am just starting with the Chinese Word Segmenter, and I would like to use it in my Android application (primarily to parse the Tatoeba example sentences). I'm not really sure where to start on this, and am looking for documentation and/or examples of using it on Android. Additionally, I imported the .jar as a library in Android Studio, but I am having trouble adding the Source and Javadoc to the library, the primary problem being that the library doesn't show up in the External Libraries folder in the Project View. Some good starting questions I have are:</p>

<ul>
<li>Which classes do I need to use to segment text?</li>
<li>How does the segmenter handle English names?</li>
<li>Is there a page for the documentation besides <a href=""https://nlp.stanford.edu/software/segmenter-faq.html"" rel=""nofollow noreferrer"">this</a>? (I need documentation for using it in Java, not just as a command line tool)</li>
<li>Are there any examples of using the segmenter in Android?</li>
<li>Do I also need to have the CoreNLP library?</li>
<li>Are there simpler alternatives to the Stanford segmenter?</li>
</ul>

<p>Sorry for such a basic question but I am really not understanding how to use it at this point</p>
","java, android, stanford-nlp","<p>You need to include a jar in your CLASSPATH with the code and you need a jar with these files in it (they can all be found in the Chinese models jar):</p>

<pre><code>edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
edu/stanford/nlp/models/segmenter/chinese/ctb.gz
StanfordCoreNLP-chinese.properties
</code></pre>

<p>You could include this jar from the main distribution to get the code:</p>

<pre><code>stanford-corenlp-3.8.0.jar
</code></pre>

<p>The file I referenced above is available with the Chinese models jar found here:  <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>You are going to have to create some small jars if you want to integrate this with an Android app since there are very strict size requirements.  I would advise cutting out most of the code that is not necessary for just running the segmenter.</p>

<p>If you use the stanford-corenlp-3.8.0.jar , this is some sample code:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;

public class PipelineExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = StringUtils.argsToProperties(""-props"", ""StanfordCoreNLP-chinese.properties"");
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation = new Annotation(""...Chinese text to segment..."");
    // annotate the review
    pipeline.annotate(annotation);
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token);
      }
    }
  }
}
</code></pre>

<p>You will want to do some work to shrink the jars down to the bare minimum, ultimately this will require removing lots of classes and making sure things still run properly.</p>

<p>You could also download the standalone segmenter, which runs the same process, more info here:</p>

<p><a href=""https://nlp.stanford.edu/software/segmenter.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/segmenter.html</a></p>

<p>It might be easier to use the standalone segmenter distribution.  It will have a demo called SegDemo.java which shows Java API usage in that case.  The sample code I provided above will not work if you use the classes from the standalone segmenter.</p>
",0,0,342,2017-10-25 17:19:25,https://stackoverflow.com/questions/46938447/stanford-chinese-word-segmenter-in-android-studio
Stanford CRFClassifier performance evaluation output,"<p>I'm following this FAQ <a href=""https://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/crf-faq.shtml</a> for training my own classifier and I noticed that the performance evaluation output does not match the results (or at least not in the way I expect).
Specifically this section</p>

<p><code>CRFClassifier tagged 16119 words in 1 documents at 13824.19 words per second.
         Entity P       R       F1      TP  FP  FN
       MYLABEL  1.0000  0.9961  0.9980  255 0   1
         Totals 1.0000  0.9961  0.9980  255 0   1
</code></p>

<p>I expect <code>TP</code> to be all instances where the predicted label matched the golden label, <code>FP</code> to be all instances where <code>MYLABEL</code> was predicted but the golden label was <code>O</code>, <code>FN</code> to be all instances where <code>O</code> was predicted but the golden was <code>MYLABEL</code>.</p>

<p>If I calculate those numbers myself from the output of the program, I get completely different numbers with no relation to what the program prints. I've tried this with various test files. 
I'm using <code>Stanford NER - v3.7.0 - 2016-10-31</code></p>

<p>Am I missing something?</p>
","stanford-nlp, named-entity-recognition, crf","<p>The F1 scores are over entities not labels.</p>

<p>Example:</p>

<pre><code>(Joe, PERSON) (Smith, PERSON) (went, O) (to, O) (Hawaii, LOCATION) (., O).
</code></pre>

<p>In this example there are two possible entities:</p>

<pre><code>Joe Smith   PERSON
Hawaii      LOCATION
</code></pre>

<p>Entities are created by taking all adjacent tokens with the same label.  (Unless you use a more complicated BIO labeling scheme ; BIO schemes have tags like I-PERSON and B-PERSON to indicate whether a token is the beginning of an entity, etc...).</p>
",1,0,343,2017-10-25 19:03:00,https://stackoverflow.com/questions/46940195/stanford-crfclassifier-performance-evaluation-output
Stanford Core NLP train tagger using Java API,"<p>Does anyone know if it's possible to train a Stanford tagger using the Java API? I'm only finding examples of people doing it through the command line. That should imply that there exists an API method somewhere, but I can't find it.</p>
",stanford-nlp,"<p>You can put all of your training properties in a <code>.properties</code> file and then call <code>MaxentTagger.main(""-props"", ""/path/to/training.properties"")</code>.  I don't see any easier way to do this in the Java API.</p>
",1,0,56,2017-10-31 18:40:32,https://stackoverflow.com/questions/47042462/stanford-core-nlp-train-tagger-using-java-api
how to give flag option in stanford-nlp program?,"<p>The site suggest that i can use several flags 
<a href=""https://nlp.stanford.edu/software/openie.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/openie.html</a></p>

<p>But how to use it, I tried doing it this way</p>

<pre><code>import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/**
 * A demo illustrating how to call the OpenIE system programmatically.
*/
public class OpenIEDemo {

public static void main(String[] args) throws Exception {
// Create the Stanford CoreNLP pipeline
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
props.setProperty(""openieformat"",""ollie"");
props.setProperty(""openieresolve_coref"",""1"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// Annotate an example document.
Annotation doc = new Annotation(""Obama was born in Hawaii. He is our president."");
pipeline.annotate(doc);

// Loop over sentences in the document
for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
  // Get the OpenIE triples for the sentence
  Collection&lt;RelationTriple&gt; triples = sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
  // Print the triples
  for (RelationTriple triple : triples) {
    System.out.println(triple.confidence + ""\t"" +
        triple.subjectLemmaGloss() + ""\t"" +
        triple.relationLemmaGloss() + ""\t"" +
        triple.objectLemmaGloss());
  }
}
 }
}
</code></pre>

<p>I have added</p>

<pre><code>props.setProperty(""openieformat"",""ollie"");
props.setProperty(""openieresolve_coref"",""1"");
</code></pre>

<p>But its not working</p>
",stanford-nlp,"<p>For StanfordCoreNLP, flags/properties for individual annotators are set with an <code>annotator.flag</code> name. And boolean flags have value ""false"" or ""true"". So, what you have is close to right, but needs to be:</p>

<p><code>props.setProperty(""openie.format"",""ollie"");
props.setProperty(""openie.resolve_coref"",""true"");</code></p>
",0,0,79,2017-11-04 07:00:44,https://stackoverflow.com/questions/47108335/how-to-give-flag-option-in-stanford-nlp-program
How to extract name from string using nltk,"<p>I am trying to extract name(Indian) from unstructured string.</p>

<p>Here come my code:</p>

<pre><code>text = ""Balaji Chandrasekaran Bangalore |  Senior Business Analyst/ Lead Business Analyst An accomplished Senior Business Analyst with a track record of handling complex projects in given period of time, exceeding above the expectation. Successful at developing product road maps and leading cross-functional software teams from prototype to release. Professional Competencies Systems Development Life Cycle (SDLC) Agile methodologies Business process improvement Requirements gathering &amp; Analysis Project Management UML Specification UI &amp; UX (Wireframe Designing) Functional Specification Test Scenario Creation SharePoint Admin Work History Senior Business Analyst (Aug 2012 Current) YouBox Technology pvt ltd, Chennai Translating business goals, feature concepts and customer needs into prioritized product requirements and use cases. Expertized in designing innovative wireframes combining user experience analysis and technology models. Extensive Experience in implementing soft wares for Shipping/Logistics firms to handle CRM, Finance, Logistics, Operations, Intermodal, and documentation. Strong interpersonal skills, highly adept at diplomatically facilitating discussions and negotiations with stakeholders. Education Bachelor of Engineering: Electronics &amp; Communication, 2011 CES Tech Hosur Accomplishment Successful onsite implementation at various locations around the globe for Europe Shipping Company. - (Pre Study, General Design, and Functional Specification) Organized Business Analyst Forum and conducted various activities to develop skill sets of Business Analysts.""
if text != """":
    grammar = """"""PERSON: {&lt;NNP&gt;}""""""
    chunkParser = nltk.RegexpParser(grammar)
    tagged = nltk.pos_tag(nltk.word_tokenize(text))
    tree = chunkParser.parse(tagged)

    for subtree in tree.subtrees():
        if subtree.label() == ""PERSON"": 
            pronouns.append(' '.join([c[0] for c in subtree]))

    print(pronouns)
</code></pre>

<blockquote>
  <p>['Balaji', 'Chandrasekaran', 'Bangalore', '|','Senior', 'Business',
  'Analys', '/', 'Lead', 'Business', 'Analyst', 'Senior', 'Business',
  'Analyst', 'Successful', 'Development', 'Life', 'Cycle', 'SDLC',
  'Agile', 'Business', 'Requirements', 'Analysis', 'Project',
  'Management', 'UML', 'Specification', 'UI', 'UX', 'Wireframe',
  'Designing', 'Functional', 'Specification', 'Test', 'Scenario',
  'Creation', 'SharePoint', 'Admin', 'Work', 'History', 'Senior',
  'Business', 'Analyst', 'Aug', 'Current', 'Technology', 'Chennai',
  'Translating', 'CRM', 'Finance', 'Logistics', 'Operations',
  'Intermodal', 'Education', 'Bachelor', 'Engineering', 'Electronics',
  'Communication', 'Accomplishment', 'Successful', 'Mediterranean',
  'Ship', 'Company', 'MSC', 'Georgia', 'MSC', 'Cambodia', 'MSC',  'MSC',
  'South', 'Successful', 'Stake', 'MSC', 'Geneva', 'Switzerland', 'Pre',
  'Study', 'General', 'Design', 'Functional', 'Specification', 'O',
  'Business', 'Analyst', 'Forum', 'Business']</p>
</blockquote>

<p>But actually i need to get only <strong>Balaji Chandrasekaran</strong> , I even try to use Standford ner lib.Which fails to pick <strong>Balaji Chandrasekaran</strong></p>

<p>Can any one help to extract name from the un strcuture string, or suggest me any good tutorial to do that.</p>

<p>Thank you in advance.</p>
","python, nlp, nltk, stanford-nlp, named-entity-extraction","<p>Like I said in the comments, you would have to create your own corpora for Indian names and test your text against that. The NLTK Book teaches you how to do this in <a href=""http://www.nltk.org/book/ch02.html"" rel=""nofollow noreferrer"">Chapter 2</a> (Section 1.9 to be exact).</p>

<pre><code>from nltk.corpus import PlaintextCorpusReader

# You can use a regular expression to find the files, or pass a list of files
files = "".*\.txt""

new_corpus = PlaintextCorpusReader(""/path/"", files)
corpus  = nltk.Text(new_corpus.words())
</code></pre>

<p>See also: <a href=""https://stackoverflow.com/q/4951751"">Creating a new corpus with NLTK</a></p>
",1,3,5058,2017-11-05 15:09:59,https://stackoverflow.com/questions/47123094/how-to-extract-name-from-string-using-nltk
duplicate props file when training new StanfordCoreNLP tagger in scala,"<p>I'm trying to train a new StanfordCoreNLP tagger using the following line of code:</p>

<p><code>MaxentTagger.main(Seq(""-props"", ""src/resources/tagger/mycustom.tagger.props"").toArray)</code>.</p>

<p>The data files inside <code>src/resources/tagger/mydata.txt</code>. But a second copy of the props file keeps magically appearing at the the same level as <code>src</code>. By second copy, I mean exacty the same, with the same custom file name and the same custom values inside the file. I tried deleting this file several times, but it keeps reappearing.</p>

<p>Alternatively, I tried deleting the one inside my <code>src/resources/tagger/</code> file and using only the one at the same level as <code>src</code>. But the opposite thing happened. When I tried to save the trained model inside the resources folder, a second copy of the props file appeared along with it.</p>
",stanford-nlp,"<p>That is normal.  The tagger stores a copy of the properties used for the model along with the model as a record and to assist with recreating the training process if one needed to.</p>
",0,0,30,2017-11-09 00:22:51,https://stackoverflow.com/questions/47191969/duplicate-props-file-when-training-new-stanfordcorenlp-tagger-in-scala
"Stanford NLP NER, Sentiment, SUTime Performance Issue","<p>The text in the main method seem to be taking more than 2 seconds to return NER. I am not an expert in NLP and this code is not at all scalable. I have added comments in 2 places where the bottleneck i have identified. Can you please suggest improvements to improve the performance of the program. </p>

<p>Thanks.</p>

<pre><code> public class NERSentimentUtil
{
private static final Logger logger = Logger.getLogger(NERSentimentUtil.class);

private static final String serializedClassifier7 = ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz"";
private static final String serializedClassifier4 = ""edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz"";
private static final String serializedClassifier3 = ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"";

private static NERClassifierCombiner ncc;
private static StanfordCoreNLP pipeline;

static
{       
    try
    {
        ncc = new NERClassifierCombiner(serializedClassifier3,serializedClassifier4,serializedClassifier7);
    } catch (IOException e) {
        e.printStackTrace();
        logger.error(e);
    }
}

static
{               
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment, sutime"");
    /*props.setProperty(""ner.useSUTime"", ""0"");*/

    String defs_sutime = ""/edu/stanford/nlp/models/sutime/defs.sutime.txt"";
    String holiday_sutime = ""/edu/stanford/nlp/models/sutime/english.holidays.sutime.txt"";
    String _sutime = ""/edu/stanford/nlp/models/sutime/english.sutime.txt"";

    String sutimeRules = defs_sutime + "","" + holiday_sutime + "","" + _sutime;
    props.setProperty(""ner.useSUTime"", ""true"");
    props.setProperty(""-sutime.rules"", sutimeRules);
    props.setProperty(""sutime.binders"", ""0"");
    props.setProperty(""sutime.markTimeRanges"", ""false"");
    props.setProperty(""sutime.includeRange"", ""false"");
    props.setProperty(""customAnnotatorClass.sutime"", ""edu.stanford.nlp.time.TimeAnnotator"");
    props.setProperty(""parse.maxlen"", ""20"");
    //props.setProperty(""ner.applyNumericClassifiers"", ""false"");
    //props.setProperty(""nthreads"", ""16"");
    //props.setProperty(""threads"", ""16"");
    //props.setProperty(""parse.nthreads"",""16"");
    //props.setProperty(""ssplit.eolonly"",""true"");

    props.setProperty(""-parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    RedwoodConfiguration.current().clear().apply();
    pipeline = new StanfordCoreNLP(props);
    //RedwoodConfiguration.empty().capture(System.err).apply();
}

//A sentiment score of 0 or 1 is negative, 2 neutral and 3 or 4 positive.
private static int getScore(int score)
{
    if(score&lt;2)
        return -1;
    else if(score==2)
        return 0;
    else
        return 1;       
}

public static HashMap&lt;String,Object&gt; getStanford(String s, long dateString)//""2013-07-14""
{   
    int finalScore =0;

    HashMap&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();

    HashMap&lt;String, Integer&gt; dateMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; dateCountMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, String&gt; dateSentenceMap = new HashMap&lt;String, String&gt;();

    HashMap&lt;String, Integer&gt; personMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; personCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Integer&gt; orgMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; orgCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Integer&gt; locationMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; locationCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Article_Location&gt; locationArticleMap = new HashMap&lt;String, Article_Location&gt;();

    ArrayList&lt;Articel_Ner&gt; organisationlist = new ArrayList&lt;Articel_Ner&gt;();
    ArrayList&lt;Articel_Ner&gt; personlist = new ArrayList&lt;Articel_Ner&gt;();
    ArrayList&lt;Artilcle_Ner_Date&gt; datelist = new ArrayList&lt;Artilcle_Ner_Date&gt;();
    ArrayList&lt;Article_NerLocation&gt; locationList = new ArrayList&lt;Article_NerLocation&gt;();     

    try
    {
        Annotation annotation = pipeline.process(s);//1/3 rd time is taken up by this line

        List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);

        for (CoreMap sentence : sentences)
        {
             String str = sentence.toString();

             int score = getSentiment(sentence);

             finalScore+=score;
             boolean dFlag = true;

             List&lt;Triple&lt;String,Integer,Integer&gt;&gt; triples = ncc.classifyToCharacterOffsets(str);

             for (Triple&lt;String,Integer,Integer&gt; trip : triples)
             {
                 String ne = trip.first();
                 String word = str.substring(trip.second(), trip.third).toLowerCase();

                 switch(ne)
                 {
                    case ""LOCATION"":                        
                        extractLocation(locationMap, locationCountMap, locationArticleMap, score, word);
                        break;

                    case ""ORGANIZATION"":                        
                        extractOrg(orgMap, orgCountMap, score, word);                       
                        break;

                    case ""PERSON"":                      
                        extractPerson(personMap, personCountMap, score, word);
                        break;

                    case ""DATE"":
                        if(dFlag)
                        {
                         extractSUDate(dateString, dateMap, dateCountMap, dateSentenceMap, str, score);
                         dFlag = false;
                        }
                        break;

                    default:
                        break;
                 }
             }
        }
            //2/3rd of the time taken by these 4 methods:: can be obtimized
        mapDate(dateMap, dateCountMap, dateSentenceMap, datelist);
        mapLocation(locationMap, locationCountMap, locationArticleMap, locationList);   
        mapOrg(orgMap, orgCountMap, organisationlist);  
        mapPerson(personMap, personCountMap, personlist);
        //
    }
    catch(Exception e)
    {
        logger.error(e);
        logger.error(s);
        e.printStackTrace();
    }

    if(finalScore&gt;0)
        finalScore = 1;
    else if(finalScore&lt;0)
        finalScore = -1;
    else
        finalScore = 0;

    map.put(""ORGANISATION"", organisationlist);
    map.put(""PERSON"", personlist);
    map.put(""DATE"", datelist);
    map.put(""LOCATION"", locationList);
    map.put(""SENTIMENT"", finalScore);

    return map;
}

private static void extractPerson(HashMap&lt;String, Integer&gt; personMap, HashMap&lt;String, Integer&gt; personCountMap,
        int score, String word)
{       
    if(personMap.get(word)!=null)
    {
        personMap.put(word, personMap.get(word)+score);
        personCountMap.put(word, personCountMap.get(word)+1);
    }
    else
    {
        personMap.put(word, score);
        personCountMap.put(word, 1);
        //personSentenceMap.put(pname, str);
    }   
}

private static void extractOrg(HashMap&lt;String, Integer&gt; orgMap, HashMap&lt;String, Integer&gt; orgCountMap,
        int score, String word)
{
    if(orgMap.get(word)!=null)
    {
        orgMap.put(word, orgMap.get(word)+score);
        orgCountMap.put(word, orgCountMap.get(word)+1);                             
    }
    else
    {
        orgMap.put(word, score);
        orgCountMap.put(word, 1);
        //orgSentenceMap.put(oname, str);
    }
}

private static void extractLocation(HashMap&lt;String, Integer&gt; locationMap,
        HashMap&lt;String, Integer&gt; locationCountMap,
        HashMap&lt;String, Article_Location&gt; locationArticleMap,
        int score,
        String word)
{
    if(locationMap.get(word)!=null)
    {
        locationMap.put(word, locationMap.get(word)+score);
        locationCountMap.put(word, locationCountMap.get(word)+1);                               
    }
    else
    {
        Article_Location articleLocation = LocationUtil.getLocation(word);

        locationMap.put(word, score);
        locationCountMap.put(word, 1);
        locationArticleMap.put(word, articleLocation);
    }   
}

private static void extractSUDate(long dateString,
        HashMap&lt;String, Integer&gt; dateMap,
        HashMap&lt;String, Integer&gt; dateCountMap,
        HashMap&lt;String, String&gt; dateSentenceMap, 
        String str,
        int score) {

    Annotation dateAnnotation = new Annotation(str);
    dateAnnotation.set(CoreAnnotations.DocDateAnnotation.class, FormatUtil.getDate(dateString));
    pipeline.annotate(dateAnnotation);

    for(CoreMap timex:dateAnnotation.get(TimeAnnotations.TimexAnnotations.class))
    {
        TimeExpression timeExpression = timex.get(TimeExpression.Annotation.class);

         if(timeExpression!=null &amp;&amp; timeExpression.getTemporal()!=null &amp;&amp;
            timeExpression.getTemporal().getTimexValue()!=null)
         {           
             String word = checkDate(timeExpression.getTemporal().getTimexValue());

             if(word!=null)
             {
                 if(dateMap.get(word)!=null)
                 {
                     dateMap.put(word, dateMap.get(word)+score);
                     dateCountMap.put(word, dateCountMap.get(word)+1);
                     dateSentenceMap.put(word, dateSentenceMap.get(word)+"" ""+str);
                 }
                 else
                 {
                     dateMap.put(word, score);
                     dateCountMap.put(word, 1);
                     dateSentenceMap.put(word, str);
                 }                       
             }
         }
    }
}

private static int getSentiment(CoreMap sentence) {
    Tree annotatedTree = sentence.get(SentimentAnnotatedTree.class);
     int localScore = RNNCoreAnnotations.getPredictedClass(annotatedTree);
     int score = getScore(localScore);       
    return score;
}   


private static void mapLocation(HashMap&lt;String, Integer&gt; locationMap,
        HashMap&lt;String, Integer&gt; locationCountMap,
        HashMap&lt;String, Article_Location&gt; locationArticleMap,
        ArrayList&lt;Article_NerLocation&gt; locationList)
{       
    for(Map.Entry&lt;String, Integer&gt; entry : locationMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Article_Location articleLocation = locationArticleMap.get(key);

        Article_NerLocation l1 = new Article_NerLocation();
        if(value&gt;=1)
            l1.setNerSentiment(1);
        else if(value&lt;=-1)
            l1.setNerSentiment(-1);
        else
            l1.setNerSentiment(0);            

        l1.setKeyword(key);
        l1.setCount(locationCountMap.get(key));

        if(articleLocation!=null)
        {                   
            l1.setNerCountry(articleLocation.getCountryCode());
            l1.setNerLatLong(articleLocation.getLatitude()+"",""+articleLocation.getLongitude());
            l1.setTimeZone(articleLocation.getTimeZone());
            l1.setCountryName(articleLocation.getCountryName());
        }

        locationList.add(l1);
    }
}

private static void mapDate(HashMap&lt;String, Integer&gt; dateMap,
        HashMap&lt;String, Integer&gt; dateCountMap,
        HashMap&lt;String, String&gt; dateSentenceMap,
        ArrayList&lt;Artilcle_Ner_Date&gt; datelist)
{               
    for(Map.Entry&lt;String, Integer&gt; entry : dateMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Artilcle_Ner_Date d1 = new Artilcle_Ner_Date();

        if(value&gt;=1)
            d1.setNerSentiment(1);
        else if(value&lt;=-1)
            d1.setNerSentiment(-1);
        else
            d1.setNerSentiment(0);

        d1.setKeyword(key);
        d1.setCount(dateCountMap.get(key));
        d1.setSentence(dateSentenceMap.get(key));
        d1.setNerDateTheme1(SummaryThemeUtil.getSTByDate(dateSentenceMap.get(key)));
        datelist.add(d1);
    }   
}   

private static void mapOrg(HashMap&lt;String, Integer&gt; orgMap,
        HashMap&lt;String, Integer&gt; orgCountMap,
        ArrayList&lt;Articel_Ner&gt; organisationlist) 
{
    for(Map.Entry&lt;String, Integer&gt; entry : orgMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Articel_Ner o1 = new Articel_Ner();
        if(value&gt;=1)
            o1.setNerSentiment(1);
        else if(value&lt;=-1)
            o1.setNerSentiment(-1); 
        else
            o1.setNerSentiment(0);            


        o1.setKeyword(key);
        o1.setCount(orgCountMap.get(key));
        organisationlist.add(o1);            
    }       
}

private static void mapPerson(HashMap&lt;String, Integer&gt; personMap,
        HashMap&lt;String, Integer&gt; personCountMap,
        ArrayList&lt;Articel_Ner&gt; personlist)
{
    for(Map.Entry&lt;String, Integer&gt; entry : personMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Articel_Ner p1 = new Articel_Ner();
        if(value&gt;=1)
            p1.setNerSentiment(1);
        else if(value&lt;=-1)
            p1.setNerSentiment(-1);
        else
            p1.setNerSentiment(0);            

        p1.setKeyword(key);
        p1.setCount(personCountMap.get(key));
        personlist.add(p1);      
    }
}   

private static String checkDate(String date)
{               
    if(date.length()&lt;10)
        return null;
    else if(date.length()&gt;10)
        date = date.substring(0,10);

    if (date.matches(""\\d{4}-\\d{2}-\\d{2}""))
        return date; 
    else
        return null;
}

public static void main(String args[])
{
    String text = ""Lets meet on every 2nd week. Night is young. Happy new Year. The festival will be held on the following dates are 18 Feb 1997, the 20th of july and 4 days from today."";
    long pre = System.currentTimeMillis();
    HashMap&lt;String, Object&gt; map = getStanford(text, 1508745558);
    long post = System.currentTimeMillis();
    long diff = post-pre;

    System.out.println(diff);
    System.out.println(map);
}
}
</code></pre>
","stanford-nlp, sentiment-analysis, named-entity-recognition, sutime","<p>After days and days of sore black eyes. Here is where the problem is:</p>

<ul>
<li><p>Stanford ""<strong>parse</strong>"" model whether <strong>PCFG</strong> or <strong>SRparser</strong> both are CPU killers. You will never be able to scale. At best i was doing 70 docs/second. This is with 15 threads that i was able to manage on tomcat. The docs where being consumed from RabbitMQ. Machine Intel Xeon 8Core VM with 15 GB RAM. The CPU was always 90%.</p></li>
<li><p>So if you want to do <strong>NER</strong>,<strong>sentiment</strong>,<strong>sutime</strong>. Its better to use separate libraries and not use stanford for all 3. For NER you can use <strong>NERClassifierCombiner</strong> from stanford. For sentiment you can use <strong>weka</strong>. For extracting dates you can use <strong>natty</strong>. </p></li>
<li><p>Now we are able to do <strong>2,000 docs/second</strong>. </p></li>
</ul>
",0,-1,539,2017-11-09 09:35:26,https://stackoverflow.com/questions/47198333/stanford-nlp-ner-sentiment-sutime-performance-issue
Cannot run Python module from command line,"<p>I am attempting to install a package from github (<a href=""https://github.com/sina-al/pynlp"" rel=""nofollow noreferrer"">https://github.com/sina-al/pynlp</a>). To run the package, the instructions say to run <code>python3 -m pynlp
</code>. However, when I run this I receive the error:</p>

<pre><code>adamg:~ adamg$ python3 -m pynlp
/usr/local/opt/python3/bin/python3.5: Error while finding spec for 'pynlp.__main__' (&lt;class 'ImportError'&gt;: No module named 'corenlp_protobuf'); 'pynlp' is a package and cannot be directly executed
</code></pre>

<p>How can this be corrected?</p>
","python, stanford-nlp","<p>Did you follow all the instructions on GitHub? 1) downloading the Stanford CoreNLP 
2) set an environmental variable <code>CORE_NLP</code> that points to it </p>

<p>then:
3) <code>pip3 install corenlp_protobuf</code> (if it's missing)</p>
",1,0,414,2017-11-11 22:54:27,https://stackoverflow.com/questions/47243677/cannot-run-python-module-from-command-line
Detailed Sentiment Score in Stanford CoreNLP,"<p>On the StanfordCore NLP website there is the following demo:<a href=""http://nlp.stanford.edu:8080/sentiment/rntnDemo.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/sentiment/rntnDemo.html</a> </p>

<p>The demo gives a sentence a detailed sentiment score from 0 to 4. </p>

<p>I understand how to get a ""positive"" or ""negative"" assessment using command line, similar to this:
<a href=""https://i.sstatic.net/F5s41.jpg"" rel=""nofollow noreferrer"">Screenshot from corenlp.run showing a positive sentiment analysis</a></p>

<p>I have seen this question already, but I am interested how the analysis shown in the attached screenshot is created. <a href=""https://stackoverflow.com/questions/20368101/getting-sentiment-analysis-result-using-stanford-core-nlp-java-code"">Getting sentiment analysis result using stanford core nlp java code</a></p>

<p>Is there a way in Stanford CoreNLP to return a score (i.e. 0-4) for a given sentence so show its degree of positivity or negativity?</p>

<p>Thanks!</p>
","java, stanford-nlp, sentiment-analysis","<p>There are multiple ways to get that kind of info.</p>

<p>Also I should note that the there is a direct mapping:</p>

<p>""Very negative"" = 0
""Negative"" = 1
""Neutral"" = 2
""Positive"" = 3
""Very positive"" = 4</p>

<p>Here is a sample command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,sentiment -file example-1.txt -outputFormat json
</code></pre>

<p>In the file <code>example-1.txt.json</code> you'll see a lot of sentiment related fields for the sentence, including <code>sentimentValue</code>.</p>

<p>There is more info about this at this GitHub issue:</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/issues/465"" rel=""noreferrer"">https://github.com/stanfordnlp/CoreNLP/issues/465</a></p>
",6,3,4099,2017-11-13 21:31:20,https://stackoverflow.com/questions/47273885/detailed-sentiment-score-in-stanford-corenlp
Stanford Classifier ColumnDataClassifier,"<p>I am using the Maximum Entropy algorithm provided by the Stanford Classifier in order to perform a customized Named Entity Recognition.
The output file provides 5 columns --> word \t ground-truth \t label \t P(clAnswer) \t P(goldAnswer))</p>

<p>Which is the difference between <strong>P(clAnswer)</strong> and <strong>P(goldAnswer)</strong> and how are these calculated?</p>
","stanford-nlp, maxent","<p>P(clAnswer) is the probability the model gives the guess.
P(goldAnswer) is the probability the model gives the true gold answer.</p>

<p>If you want to understand the algorithm behind the classifier you can find resources at this link: <a href=""https://nlp.stanford.edu/software/classifier.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/classifier.shtml</a></p>

<p>I should note that it is standard to use the CRFClassifier to train NER models.  There is exhaustive documentation here about training an NER model:</p>

<p><a href=""https://nlp.stanford.edu/software/crf-faq.html#a"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/crf-faq.html#a</a></p>
",0,0,191,2017-11-14 15:58:58,https://stackoverflow.com/questions/47290107/stanford-classifier-columndataclassifier
how to use pos tag as feature in Stanford NER training?,"<p>I am trying to use the useTags and related features in training Stanford NER CRF model. However, although I have specified in the .prop file that I will use this feature, CoreAnnotations.PartOfSpeechAnnotation.class does not seem to return anything and hence the training does not use this feature at all. Is there something I did wrong that it wasn't using this feature? Thanks!</p>
","nlp, stanford-nlp, named-entity-recognition","<p>You need to specify which column in your training/test data has the pos tag and add the pos tags to the CoNLL.</p>

<p>You specify that column in this part of the properties:</p>

<p><code>map = word=0,answer=1,tag=2</code></p>

<p>(for example if you added the tags in the 3rd column)</p>
",1,0,868,2017-11-16 20:13:06,https://stackoverflow.com/questions/47338317/how-to-use-pos-tag-as-feature-in-stanford-ner-training
stanford core nlp -- &quot;last year&quot; not recognized as a date,"<p>I am running Stanford Core NLP locally and comparing the results with the web interface at <a href=""http://http://corenlp.run/"" rel=""nofollow noreferrer"">http://http://corenlp.run/</a>. For the test sentence, ""The economy grew by 2% last year"", the web interface identifies ""last year"" as a date entity.
<a href=""https://i.sstatic.net/zXSmx.jpg"" rel=""nofollow noreferrer"">NER</a></p>

<p>But my local instance fails to recognize ""last year"" as a date:</p>

<pre><code>{'after': ' ',
  'before': ' ',
  'characterOffsetBegin': 23,
  'characterOffsetEnd': 27,
  'index': 7,
  'lemma': 'last',
  'ner': **'O'**,
  'originalText': 'Last',
  'pos': 'JJ',
  'word': 'Last'},
 {'after': '',
  'before': ' ',
  'characterOffsetBegin': 28,
  'characterOffsetEnd': 32,
  'index': 8,
  'lemma': 'year',
  'ner': **'O'**,
  'originalText': 'Year',
  'pos': 'NN',
  'word': 'Year'}
</code></pre>

<p>Strangely, if I change ""last year"" to ""last month"", ""last month"" does get recognized by my local instance as a date.</p>

<pre><code>{'after': ' ',
  'before': ' ',
  'characterOffsetBegin': 23,
  'characterOffsetEnd': 27,
  'index': 7,
  'lemma': 'last',
  'ner': **'DATE'**,
  'normalizedNER': 'Last Month ',
  'originalText': 'Last',
  'pos': 'JJ',
  'word': 'Last'},
 {'after': '',
  'before': ' ',
  'characterOffsetBegin': 28,
  'characterOffsetEnd': 33,
  'index': 8,
  'lemma': 'Month',
  'ner': **'DATE'**,
  'normalizedNER': 'Last Month ',
  'originalText': 'Month',
  'pos': 'NNP',
  'word': 'Month'}
</code></pre>

<p>Help with understanding and resolving this will be appreciated!!</p>
",stanford-nlp,"<p>If I use the latest code from GitHub with the latest models, and run this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -file example-1.txt -outputFormat text
</code></pre>

<p>On your example, I find ""last year"" marked as date.  I would presume this is true for 3.8.0 as well.</p>
",0,1,199,2017-11-17 07:53:55,https://stackoverflow.com/questions/47345761/stanford-core-nlp-last-year-not-recognized-as-a-date
Get most similar words using GloVe,"<p>I am new to GloVe. I successfully ran their <a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">demo.sh</a> as given in their website. After running demo I got several files created such as <code>vocab</code>, <code>vectors</code> etc. But they haven't any documentation or anything that describes what files we need to use and how to use to find most similar words.</p>

<p>Hence, please help me to find the most similar words given a word in GloVe (using cosine similarity)? (e.g., like <code>most.similar</code> in Gensim word2vec)</p>

<p>Please help me!</p>
","nlp, stanford-nlp, word-embedding","<p>It doesn't really matter how word vectors are generated, you can always calculate cosine similarity between the words. The easiest way to achieve what you asked for is (considering you have gensim):</p>

<pre><code>python -m gensim.scripts.glove2word2vec –input &lt;GloVe vector file&gt; –output &lt;Word2vec vector file&gt;
</code></pre>

<p>This will convert glove vector file to w2v format. You can do it manually too - just add extra line to your GloVe file containing total number of vectors and their dimensionality at the top of your file. It looks something a kin of:</p>

<pre><code>180000 300
&lt;The rest of your file&gt;
</code></pre>

<p>After that you can just load the file into gensim and everything is working as if it is a regular w2v model. </p>
",2,1,3235,2017-11-18 11:25:19,https://stackoverflow.com/questions/47365480/get-most-similar-words-using-glove
Find sentences with describing context using stanford NLP,"<p>Is there any way to find those sentences that are describing objects?</p>

<p>For example sentences like ""This is a good product"" or ""You are very beautiful""</p>

<p>I guess I can create an algorithm by using TokenSequencePattern and filter with POS some patterns like PRONUN + VERB + ADJECTIVE but don't think would be something reliable.</p>

<p>I am asking you if there is something out of the box, what I am trying to do is to identify review comments on a webpage.</p>
","machine-learning, stanford-nlp, sentiment-analysis","<p>Instead of POS tagging, you would achieve better results by dependency parsing. By using that instead of POS tagging &amp; patterns as you mentioned, you will have richer and accurate information about the sentence structure.
Example:</p>

<p><a href=""https://demos.explosion.ai/displacy/?text=The%20product%20was%20really%20very%20good.&amp;model=en_core_web_sm&amp;cpu=0&amp;cph=0"" rel=""nofollow noreferrer"">https://demos.explosion.ai/displacy/?text=The%20product%20was%20really%20very%20good.&amp;model=en_core_web_sm&amp;cpu=0&amp;cph=0</a></p>

<p><a href=""https://i.sstatic.net/cVS6W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cVS6W.png"" alt=""enter image description here""></a></p>

<p>Stanford NLP does support <a href=""https://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""nofollow noreferrer"">depedency parsing</a>.
Apart from that you can also use the brilliant <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">SpaCy</a>.</p>
",1,0,189,2017-11-19 20:08:28,https://stackoverflow.com/questions/47381480/find-sentences-with-describing-context-using-stanford-nlp
Finding best speed and accuracy combination for parsing,"<p>I have experience on different cases englishPCFG pos-tagger is more accurate then any others. The model used on <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a> </p>

<p>Due to speed, I would like to get parse result with depparse (english_UD)</p>

<p>Is there a way to use englishPCFG pos-tagger (without parse as it is slow to compare with english_UD) with depparse to combine pos-tagger accuracy and depparse speed ?</p>
","nlp, stanford-nlp","<p>No.  The parser produces a part-of-speech tag sequence as a part of its total parsing process.  You cannot get the tag sequence without completely parsing the sentence.</p>

<p>Your best bet is to use a Stanford CoreNLP pipeline, use the dedicated part-of-speech tagger and the neural network dependency parser in the pipeline.</p>

<p><code>-annotators tokenize,ssplit,pos,lemma,depparse</code></p>
",0,0,56,2017-11-24 07:19:14,https://stackoverflow.com/questions/47468169/finding-best-speed-and-accuracy-combination-for-parsing
With which treebank are the available StanfordCoreNLP French models trained?,"<p>As per the title of this post, I would like to have a maximum of information regarding the dataset that is being used to train the StanfordCoreNLP French models that are made available on this page (<a href=""https://stanfordnlp.github.io/CoreNLP/history.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/history.html</a>). My ultimate aim is to know the set of tag that I can expect to be output by the stanford core nlp tool when using it to characterize a text written in French. I was told that a model is trained using a treebank. For for the French language, there is 6 of them (<a href=""http://universaldependencies.org/"" rel=""nofollow noreferrer"">http://universaldependencies.org/</a>, section for the French language) :
- FTB
- Original
- Sequoia
- ParTUT
- PUD
- Spoken
So I would like to know which of them was used to train which French model.</p>

<p>I have first asked this question on the mailing list dedicated to the java nlp users (java-nlp-user@lists.stanford.edu), but to no avail up until now.</p>

<p>So, again, assuming it is one the treebanks described above that was indeed used to train the stanford core nlp French models available at the link posted above, which one is it? Alternatively, who (name and surname) would know the answer to this question, if no one here knows?</p>
",stanford-nlp,"<p>For all who are curious about this, here is some info about the datasets used for French in Stanford CoreNLP:</p>

<pre><code>French POS tagger: CC (Crabbe and Candito) modified French Treebank
French POS tagged (UD version): UD 1.3
French Constituency Parser: CC modified French Treebank
French NN Dependency Parser: UD 1.3
</code></pre>

<p>Also note that the constituency parser <code>parse</code> cannot translate constituency parses into dependency parses the way the English constituency parser can.</p>
",0,0,184,2017-11-28 10:12:44,https://stackoverflow.com/questions/47528937/with-which-treebank-are-the-available-stanfordcorenlp-french-models-trained
How do I process multiple sentences in one go using the simple CoreNLP server,"<p>If I send two sentences (delimited by newline) in the body of a POST request to the <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">simple CoreNLP server</a>, the two sentences get analyzed as one, and the returned list of JSON object contains only one element. What settings do I have to pass to the server, so that each sentences is analyzed separately, and results are returned as a list of two JSON objects, one for each sentence?</p>

<p><a href=""https://i.sstatic.net/6e0Mz.jpg"" rel=""nofollow noreferrer"">Screenshot of a POST request and the returned JSON array containing only one element</a></p>

<p>Setting the property ""ssplit.eonly"":""true"" does not seem to help either:</p>

<p><a href=""https://i.sstatic.net/qIrBn.jpg"" rel=""nofollow noreferrer"">Turning on ssplit.eonly does not help</a></p>
",stanford-nlp,"<p>Add <code>""ssplit.eolonly"": ""true""</code> to your JSON.</p>
",0,0,290,2017-11-29 13:20:14,https://stackoverflow.com/questions/47553840/how-do-i-process-multiple-sentences-in-one-go-using-the-simple-corenlp-server
NLTK CoreNLPDependencyParser: Failed to establish connection,"<p>I'm trying to use the Stanford Parser through NLTK, following the example <a href=""http://www.nltk.org/api/nltk.parse.html#nltk.parse.corenlp.CoreNLPDependencyParser%20tutorial%20here"" rel=""noreferrer"">here</a>.</p>

<p>I follow the first two lines of the example (with the necessary import)</p>

<pre><code>from nltk.parse.corenlp import CoreNLPDependencyParser
dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')
parse, = dep_parser.raw_parse('The quick brown fox jumps over the lazy dog.')
</code></pre>

<p>but I get an error saying:</p>

<pre><code>[...] Failed to establish a new connection: [Errno 61] Connection refused""
</code></pre>

<p>I realize that it must be an issue with trying to connect to the url given as input to the constructor.</p>

<pre><code>dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')
</code></pre>

<p>What url should I be connecting to, if not this? If this is correct, what is the issue?</p>
","python, nlp, nltk, stanford-nlp","<p>You need to first download and run the CoreNLP server on <code>localhost:9000</code>.  </p>

<p>1) download CoreNLP at <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a><br>
2) unzip the files to some directory then run the following command in the that directory to start the server</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
</code></pre>

<p>Ref: <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/corenlp-server.html</a></p>

<p>The result of the above code is like</p>

<pre><code>&gt;&gt;&gt; print(parse.to_conll(4))
The DT  4   det
quick   JJ  4   amod
brown   JJ  4   amod
fox NN  5   nsubj
jumps   VBZ 0   ROOT
over    IN  9   case
the DT  9   det
lazy    JJ  9   amod
dog NN  5   nmod
.   .   5   punct
</code></pre>

<hr>

<p>You can also start the server via NLTK API (need to configure the <code>CORENLP_HOME</code> environment variable first)  </p>

<pre><code>os.environ[""CORENLP_HOME""] = ""dir""
client = corenlp.CoreNLPClient()
# do something
client.stop()
</code></pre>
",4,6,3580,2017-12-01 00:15:12,https://stackoverflow.com/questions/47584738/nltk-corenlpdependencyparser-failed-to-establish-connection
"Maven choosing either stanford-corenlp models or non-models, not both","<p>The documentation says to include:</p>

<pre><code>    &lt;dependencies&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.8.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.8.0&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>Unfortunately, when trying to build an RPM, the assembly plugin outputs:</p>

<p>[INFO] [INFO] Reading assembly descriptor: src/main/assembly/assembly.xml
[INFO] [INFO] lib/stanford-corenlp-3.8.0.jar already added, skipping</p>

<p>and we only get the first jar, not the models one, in the final result.</p>

<p>I know this is something on our end, since it seems to be working fine for other people, but does anyone have any ideas what could be going wrong? Searching for anything related to maven skipping the ""classifier"" part hasn't worked since the word classifier is used in so many contexts.</p>
","maven, stanford-nlp","<p>We eventually found the answer to this--in our assembly.xml, we had this:</p>

<pre><code>&lt;dependencySet&gt;
  &lt;outputFileNameMapping&gt;${artifact.artifactId}-${artifact.version}.${artifact.extension}&lt;/outputFileNameMapping&gt;
  &lt;outputDirectory&gt;/lib&lt;/outputDirectory&gt;
  &lt;useProjectArtifact&gt;true&lt;/useProjectArtifact&gt;
  &lt;useTransitiveDependencies&gt;true&lt;/useTransitiveDependencies&gt;
  &lt;unpack&gt;false&lt;/unpack&gt;
  &lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependencySet&gt;
</code></pre>

<p></p>

<p>The outputFileNameMapping value was overriding the default, and missing the segment that would include the classifier: <code>${dashClassifier?}</code></p>

<p>The current Maven default is: <code>${module.artifactId}-${module.version}${dashClassifier?}.${module.extension}</code></p>

<p>so we just removed the outputFileNameMapping declaration and it fixed it.</p>

<p><a href=""https://maven.apache.org/plugins/maven-assembly-plugin/assembly.html"" rel=""nofollow noreferrer"">https://maven.apache.org/plugins/maven-assembly-plugin/assembly.html</a></p>
",0,0,157,2017-12-01 19:10:11,https://stackoverflow.com/questions/47599977/maven-choosing-either-stanford-corenlp-models-or-non-models-not-both
Best Approach for Custom Information Extraction (NER),"<p>I'm trying to extract locations from blobs of text (NER/IE) and have tried many solutions all which are far too innaccurate spacy, Stanford etc etc.</p>

<p>All really are only about 80-90% accurate on my dataset (spacy was like 70%), another problem I'm having is not having a probability that means anything for these entities so I don't know confidence and can't proceed accordingly.</p>

<p>I tried a super naive approach of splitting my blobs into singular words then extracting surrounding context as features, also used a location placename lookup (30/40k location placenames) as a feature aswell. Then I used just a classifier(XGDBoost) and the results where much better once I trained the classifier on about 3k manually labelled datapoints (100k total only 3k where locations). 95% precision for states/countries and about 85% for cities.</p>

<p>This approach sucks obviously but why is it outperforming everything I have tried? I think the black box approach to NER just isn't working for my data problem, I tried spacy custom training and it really just didn't seem like it was going to work. Not having a confidence in the entity is kind of killer also as the probability they give you for that is almost meaningless.</p>

<p>Is there someway I can approach this problem a little better to improve my results even more? shallow nlp for like 2/3/4-grams? Another problem I have with my approach is the output of the classifier isnt some sequential entity, its literally just classified word blobs which somehow need to be clustered back into one entity i.e : -> San Francisco, CA is just 'city','city', '0','state' with no concept of them being the same entity</p>

<p>spacy example:</p>

<p>example blob:</p>

<pre><code>About Us - Employment Opportunities Donate Donate Now The Power of Mushrooms Enhancing Response Where We Work Map Australia Africa Asia Pacific Our Work Agriculture Anti - Trafficking and Gender - based Violence Education Emergency Response Health and Nutrition Rural and Economic Development About Us Who We Are Annual Report Newsletters Employment Opportunities Video Library Contact Us Login My Profile Donate Join Our Email List Employment Opportunities Annual Report Newsletters Policies Video Library Contact Us Employment Opportunities Current Career Opportunity Internships Volunteer Who We Are Our History Employment Opportunities with World Hope International Working in Service to the Poor Are you a professional that wants a sense of satisfaction out of your job that goes beyond words of affirmation or a pat on the back ? You could be a part of a global community serving the poor in the name of Jesus Christ . You could use your talents and resources to make a significant difference to millions . Help World Hope International give a hand up rather than a hand out . Career opportunities . Internship opportunities . Volunteer Why We Work Here World Hope International envisions a world free of poverty . Where young girls aren ’ t sold into sexual slavery . Where every child has enough to eat . Where men and women can earn a fair and honest wage , and their children aren ’ t kept from an education . Where every community in Africa has clean water . As an employee of World Hope International , these are the people you will work for . Regardless of their religious beliefs , gender , race or ethnic background , you will help shine the light of hope into the darkness of poverty , injustice and oppression . Find out more by learning about the of World Hope International and reviewing a summary of our work in the most recent history annual report . Equal Opportunity Employer World Hope International is both an equal opportunity employer and a faith - based religious organization . We hire US employees without regard to race , color , ancestry , national origin , citizenship , age , sex , marital status , parental status , membership in any labor organization , political ideology or disability of an otherwise qualified individual . We hire national employees in our countries of operation pursuant to the law of the country where we hire the employees . The status of World Hope International as an equal opportunity employer does not prevent the organization from hiring US staff based on their religious beliefs so that all US staff share the same religious commitment . Pursuant to the United States Civil Rights Act of 1964 , Section 702 ( 42 U . S . C . 2000e 1 ( a ) ) , World Hope International has the right to , and does , hire only candidates whose beliefs align with the Apostle ’ s Creed . Apostle ’ s Creed : I believe in Jesus Christ , Gods only Son , our Lord , who was conceived by the Holy Spirit , born of the Virgin Mary , suffered under Pontius Pilate , was crucified , died , and was buried ; he descended to the dead . On the third day he rose again ; he ascended into heaven , he is seated at the right hand of the Father , and he will come again to judge the living and the dead . I believe in the Holy Spirit , the holy catholic church , the communion of saints , the forgiveness of sins , the resurrection of the body , and the life everlasting . AMEN . Christian Commitment All applicants will be screened for their Christian commitment . This process will include a discussion of : The applicant ’ s spiritual journey and relationship with Jesus Christ as indicated in their statement of faith The applicant ’ s understanding and acceptance of the Apostle ’ s Creed . Statement of Faith A statement of faith describes your faith and how you see it as relevant to your involvement with World Hope International . It must include , at a minimum , a description of your spiritual disciplines ( prayer , Bible study , etc . ) and your current fellowship or place of worship . Applicants can either incorporate their statement of faith into their cover letter content or submit it as a separate document . 519 Mt Petrie Road Mackenzie , Qld 4156 1 - 800 - 967 - 534 ( World Hope ) + 61 7 3624 9977 CHEQUE Donations World Hope International ATTN : Gift Processing 519 Mt Petrie Road Mackenzie , Qld 4156 Spread the Word Stay Informed Join Email List Focused on the Mission In fiscal year 2015 , 88 % of all expenditures went to program services . Find out more . Privacy Policy | Terms of Service World Hope Australia Overseas Aid Fund is registered with the ACNC and all donations over $ 2 are tax deductible . ABN : 64 983 196 241 © 2017 WORLD HOPE INTERNATIONAL . All rights reserved .'
</code></pre>

<p>and the results:</p>

<pre><code>('US', 'GPE')
('US', 'GPE')
('US', 'GPE')
('the', 'GPE')
('United', 'GPE')
('States', 'GPE')
('Jesus', 'GPE')
('Christ', 'GPE')
('Pontius', 'GPE')
('Pilate', 'GPE')
('Faith', 'GPE')
('A', 'GPE')
</code></pre>
","python, entity, stanford-nlp, spacy, information-extraction","<p>Even the best Deep Learning based NER systems only achieve an F1 of 92.0 these days.  Deep Learning based systems (CNN-BiLSTM-CRF) should outperform Stanford CoreNLP's plain CRF sequence tagger.  Recently there have been even more advancements involving integrating language models.  You might want to look at AllenNLP.</p>

<p>But if you want super high accuracy like 99.0%, you're going to have integrate rule-based approaches for the time being.</p>

<p>I think rule-based processing could be helpful.  For instance, you can write a pattern that says ""city city O , state"" should be merged together into one entity.  Also, you might want to consider discarding entities that don't appear in your dictionary of location/places.  Or discard entities that aren't in a location dictionary but are in another type.  But I find it hard to believe many unknown string sequences are location place names you care about extracting.  I would think people names are the most likely to be outside of dictionaries.</p>

<p>UIUC's NLP tools have some dictionaries in them if you download their software.</p>

<p>When running StanfordCoreNLP, using the <code>ner,regexner,entitymentions</code> annotators will allow automatic grouping together of consecutive NE tags into entities.  Full info on the pipeline here: <a href=""https://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""noreferrer"">https://stanfordnlp.github.io/CoreNLP/cmdline.html</a></p>

<p>Also, remember the out-of-the-box versions of these systems are typically trained on news articles from the last 15 years.  Retraining on data closer to your set is essential.  Ultimately you might be best off just writing some rules that do dictionary based extraction. </p>

<p>You can look into Stanford CoreNLP's TokensRegex and RegexNER functionality to see how to use Stanford CoreNLP for that purpose.</p>

<p>TokensRegex: <a href=""https://nlp.stanford.edu/software/tokensregex.html"" rel=""noreferrer"">https://nlp.stanford.edu/software/tokensregex.html</a>
RegexNER: <a href=""https://nlp.stanford.edu/software/regexner.html"" rel=""noreferrer"">https://nlp.stanford.edu/software/regexner.html</a></p>
",7,5,2864,2017-12-02 00:08:05,https://stackoverflow.com/questions/47603200/best-approach-for-custom-information-extraction-ner
How to use stanford word tokenizer in NLTK?,"<p>I am searching way to use stanford word tokenizer in nltk, I want to use because when I compare results of stanford and nltk word tokenizer, they both are different. I know there might be way to use stanford tokenizer, like we can stanford POS Tagger and NER in NLTK. </p>

<p>Is it possible to do use stanford tokenizer without running server?  </p>

<p>Thanks</p>
","python, nltk, stanford-nlp, tokenize","<p>Outside of NLTK, you can use the <a href=""https://github.com/stanfordnlp/python-stanford-corenlp"" rel=""nofollow noreferrer""><em>official</em> Python interface that's recently release by Stanford NLP</a>:</p>

<h1>Install</h1>

<pre><code>cd ~
wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31
pip3 install -U https://github.com/stanfordnlp/python-stanford-corenlp/archive/master.zip
</code></pre>

<h1>Setup Environment</h1>

<pre><code># On Mac
export CORENLP_HOME=/Users/&lt;username&gt;/stanford-corenlp-full-2016-10-31/

# On linux
export CORENLP_HOME=/home/&lt;username&gt;/stanford-corenlp-full-2016-10-31/
</code></pre>

<h1>In Python</h1>

<pre><code>&gt;&gt;&gt; import corenlp
&gt;&gt;&gt; with corenlp.client.CoreNLPClient(annotators=""tokenize ssplit"".split()) as client:
...     ann = client.annotate(text)
... 
[pool-1-thread-4] INFO CoreNLP - [/0:0:0:0:0:0:0:1:55475] API call w/annotators tokenize,ssplit
Chris wrote a simple sentence that he parsed with Stanford CoreNLP.
&gt;&gt;&gt; sentence = ann.sentence[0]
&gt;&gt;&gt; 
&gt;&gt;&gt; [token.word for token in sentence.token]
['Chris', 'wrote', 'a', 'simple', 'sentence', 'that', 'he', 'parsed', 'with', 'Stanford', 'CoreNLP', '.']
</code></pre>
",2,2,8700,2017-12-04 00:10:31,https://stackoverflow.com/questions/47624742/how-to-use-stanford-word-tokenizer-in-nltk
extracting VPs/NPs connected with conjunctions using Tregex for Stanford Parser,"<p>I want to split the trees based on conjunctions and commas. For example, when I have <code>VP and VP</code> or <code>NP and NP</code> or <code>VP, VP</code> or <code>NP,NP</code>, I would like to extract each VP or NP separately. I have the following code:</p>

<pre><code> List&lt;Tree&gt; subtrees = constituent.subTreeList();

                for (int i = 0; i &lt; subtrees.size(); i++) {
                    String s = ""@VP $+ CC $+ @VP"";
                    TregexPattern p = TregexPattern.compile(s);
                    TregexMatcher m = p.matcher(subtrees.get(i));
                    while (m.find()) {
                        m.getMatch().pennPrint();
                        Tree foundTree = m.getMatch();
                        System.out.println(m.getMatch());
                    }
                }
</code></pre>

<p>But it doesn't work for the following text. What is wrong with my code?</p>

<pre><code>(VP (VP (VB manage) (NP (NP (DT the) (JJ entire) (NN life) (NN cycle)) (PP (IN of) (NP (PRP$ your) (NNS APIs))))) (CC and) (VP (VB expose) (NP (PRP$ your) (NNS APIs)) (PP (TO to) (NP (JJ third-party) (NNS developers)))))
</code></pre>
","java, parsing, stanford-nlp","<p>The main problem here is that chained Tregex relations (following the tradition of tgrep and tgrep2) have a special non-associative semantics: <code>A r1 B r2 C [r3 D]</code> means <code>A r1 B</code> and <code>A r2 C</code> and <code>A r3 D</code>. (This usually makes sense for the core use case of <code>A &lt; B &lt; C</code> meaning an A node with B and C children. To get another grouping, you need to use parentheses. In particular, the pattern you want here is <code>""@VP $+ (CC $+ @VP)""</code>.</p>

<p>This is documented in the <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/tregex/TregexPattern.html"" rel=""nofollow noreferrer"">Tregex Javadoc</a> under the list of relations, but I realize that this is an easy mistake to make, especially since the semantics is quite non-standard relative to typical mathematical or programming language expressions.</p>

<p>There are then some other improvements to be made, as noted by @dantiston. You should compile the pattern just once outside loops, as for a regular regex. Also, you're much better off just letting Tregex iterate over the nodes of a tree rather than constructing a full list of all subtrees. Here's some good example code:</p>

<pre><code>Tree t2 = Tree.valueOf(""(VP (VP (VB manage) (NP (NP (DT the) (JJ entire) (NN life) (NN cycle)) (PP (IN of) (NP (PRP$ your) (NNS APIs))))) (CC and) (VP (VB expose) (NP (PRP$ your) (NNS APIs)) (PP (TO to) (NP (JJ third-party) (NNS developers)))))"");
List&lt;Tree&gt; trees = Collections.singletonList(t2);

String s = ""@VP $+ (@CONJP|CC $+ @VP)"";
TregexPattern p = TregexPattern.compile(s);
for (Tree t : trees) {
  TregexMatcher m = p.matcher(t);
  while (m.findNextMatchingNode()) {
    Tree foundTree = m.getMatch();
    System.out.println(foundTree);
  }
}
</code></pre>
",1,0,148,2017-12-08 14:55:04,https://stackoverflow.com/questions/47716808/extracting-vps-nps-connected-with-conjunctions-using-tregex-for-stanford-parser
using Stanford CoreNLP via web,"<p>Whenever I need to use Stanford CoreNLP I normally download it and launch it on local machine. In some circumstance I cannot really install it (some restrictions). I wonder if it is possible to use instead:</p>

<pre><code>http://nlp.stanford.edu:8080/corenlp/
</code></pre>

<p>I tried it with pycorenlp wrapper and it did not work:</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp_st = StanfordCoreNLP('http://nlp.stanford.edu:8080/corenlp/')
parser_output = nlp_st.annotate('Grass is green.', properties={
          'annotators': 'tokenize,ssplit,pos,depparse,parse,ner',
          'outputFormat': 'json'})
</code></pre>

<p>Here is what it returned:</p>

<pre><code>'\n\n\n\n&lt;html lang=""en-US"" xml:lang=""en-US"" xmlns=""http://www.w3.org/1999/xhtml""&gt;\n&lt;head&gt;\n    &lt;meta http-equiv=""Content-Type"" content=""application/xhtml+xml; charset=utf-8""/&gt;\n\n    &lt;link href=""http://nlp.stanford.edu/nlp.css"" rel=""stylesheet"" \n          type=""text/css"" /&gt;\n  &lt;title&gt;Stanford CoreNLP&lt;/title&gt;\n&lt;style type=""text/css""&gt;\n&lt;!--\n#Footer {\nposition: relative;\nbottom: 0px;\n}\n--&gt;\n&lt;/style&gt;\n\n  &lt;link rel=""icon"" type=""image/x-icon"" href=""/ner/favicon.ico"" /&gt;\n  &lt;link rel=""shortcut icon"" type=""image/x-icon"" \n        href=""/ner/favicon.ico"" /&gt;\n\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;div&gt;\n&lt;h1&gt;Stanford CoreNLP&lt;/h1&gt;\n&lt;FORM name=""myform"" METHOD=""POST"" ACTION=""process"" accept-charset=""UTF-8""&gt;\n  &lt;table&gt;\n    &lt;tr&gt;&lt;td&gt;\n      Output format:\n      \n      &lt;select name=""outputFormat""&gt;\n        &lt;option value=""visualise""  &gt;Visualise&lt;/option&gt;\n        &lt;option value=""pretty""  &gt;Pretty print&lt;/option&gt;\n        &lt;option value=""xml""  &gt;XML&lt;/option&gt;\n        &lt;option value=""json""  &gt;JSON&lt;/option&gt;\n        &lt;option value=""conll""  &gt;CoNLL&lt;/option&gt;\n      &lt;/select&gt;\n    &lt;/td&gt;&lt;/tr&gt;\n  \n    &lt;tr&gt;&lt;td colspan=2&gt;\n      &lt;br&gt;Please enter your text here:&lt;br&gt;&lt;br&gt;\n      &lt;textarea valign=top name=""input"" \n                style=""width: 400px; height: 8em"" rows=31 cols=7&gt;&lt;/textarea&gt;\n    &lt;/td&gt;&lt;/tr&gt;\n\n    &lt;tr&gt;&lt;td align=left&gt;\n      &lt;input type=""submit"" name=""Process""/&gt;\n        &lt;input type=""button"" value=""Clear""\n               onclick=""this.form.elements[\'input\'].value=\'\'""/&gt;\n    &lt;/td&gt;&lt;/tr&gt;\n  &lt;/table&gt;\n&lt;/FORM&gt;\n&lt;/div&gt;\n  &lt;div id=""Footer""&gt;\r\n    Copyright &amp;copy; 2015, \r\n    &lt;a href=""http://nlp.stanford.edu/software/corenlp.shtml""&gt;Stanford University&lt;/a&gt;, All Rights Reserved.\r\n  &lt;/div&gt;\r\n&lt;/body&gt;\r\n&lt;/html&gt;\r\n'
</code></pre>

<p>Should I use it with post and get methods? What is proper format?</p>
","python, stanford-nlp","<p>Please do not hit the Stanford servers programmatically! They're not set up for any real amounts of traffic, and are really only for demos. There are some Docker images you should be able to easily set up on the cloud (e.g., <a href=""https://hub.docker.com/r/motiz88/corenlp/"" rel=""nofollow noreferrer""><code>motizz88/corenlp</code></a>), documented at: <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#docker"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#docker</a>:</p>

<blockquote>
  <p>If running the server under docker, the container’s port 9000 has to be published to the host. Give a command like: docker run -p 9000:9000 --name coreNLP --rm -i -t motiz88/corenlp. If, when going to localhost:9000/, you see the error This site can’t be reached. localhost refused to connect, then this is what you failed to do!</p>
</blockquote>
",1,0,427,2017-12-12 17:49:39,https://stackoverflow.com/questions/47778649/using-stanford-corenlp-via-web
The probability distribution of two words in a file using java 8,"<p>I need the number of lines that contain two words. For this purpose I have written the following code:
The input file contains <code>1000 lines</code> and about <code>4,000 words</code>, and it takes about 4 hours.
Is there a library in <code>Java</code> that can do it faster?
Can I implement this code using <code>Appache Lucene</code> or <code>Stanford Core NLP</code> to achieve less run time?</p>

<pre><code>ArrayList&lt;String&gt; reviews = new ArrayList&lt;String&gt;();
ArrayList&lt;String&gt; terms = new ArrayList&lt;String&gt;();
Map&lt;String,Double&gt; pij = new HashMap&lt;String,Double&gt;();

BufferedReader br = null;
FileReader fr = null;
try 
    {
        fr = new FileReader(""src/reviews-preprocessing.txt"");
            br = new BufferedReader(fr);
            String line;
            while ((line= br.readLine()) != null) 
            {
            for(String term : line.split("" ""))
                {
                    if(!terms.contains(term))
                        terms.add(term);
                }
                reviews.add(line);
            }
        } 
        catch (IOException e) { e.printStackTrace();} 
        finally 
        {
            try 
            {
                if (br != null)
                    br.close();
                if (fr != null)
                    fr.close();
            } 
            catch (IOException ex) { ex.printStackTrace();}    
    }
long Count = reviews.size();
for(String term_i : terms)
    {
        for(String term_j : terms)
            {
                if(!term_i.equals(term_j))
                {
                    double p = (double) reviews.parallelStream().filter(s -&gt; s.contains(term_i) &amp;&amp; s.contains(term_j)).count();
                    String key = String.format(""%s_%s"", term_i,term_j);
                    pij.put(key, p/Count);
                }
            }
    }
</code></pre>
","java-8, lucene, stanford-nlp, probability-distribution","<p>Your first loop getting the distinct words relies on <code>ArrayList.contains</code>, which has a linear time complexity, instead of using a <code>Set</code>. So if we assume <em>nd</em> distinct words, it already has a time complexity of “<em>number of lines</em>”×<em>nd</em>.</p>

<p>Then, you are creating <em>nd</em>×<em>nd</em> word combinations and probing all 1,000 lines for the presence of these combination. In other words, if we only assume 100 distinct words, you are performing 1,000×100 + 100×100×1,000 = 10,100,000 operations, if we assume 500 distinct words, we’re talking about 250,500,000 already.</p>

<p>Instead, you should just create the combinations actually existing in a line and collect them into the map. This will only process those combinations actually existing and you may improve this by only checking either of each “a_b”/“b_a” combination, as the probability of both is identical. Then, you are only performing “<em>number of lines</em>”×“<em>word per line</em>”×“<em>word per line</em>” operations, in other words, roughly 16,000 operations in your case.</p>

<p>The following method combines all words of a line, only keeping one of the “a_b”/“b_a” combination, and eliminates duplicates so each combination can count as a line.</p>

<pre><code>static Stream&lt;String&gt; allCombinations(String line) {
    String[] words = line.split("" "");
    return Arrays.stream(words)
        .flatMap(word1 -&gt;
            Arrays.stream(words)
                  .filter(words2 -&gt; word1.compareTo(words2)&lt;0)
                  .map(word2 -&gt; word1+'_'+word2))
        .distinct();
}
</code></pre>

<p>This method can be use like</p>

<pre><code>List&lt;String&gt; lines = Files.readAllLines(Paths.get(""src/reviews-preprocessing.txt""));
double ratio = 1.0/lines.size();
Map&lt;String, Double&gt; pij = lines.stream()
        .flatMap(line -&gt; allCombinations(line))
        .collect(Collectors.groupingBy(Function.identity(),
                                       Collectors.summingDouble(x-&gt;ratio)));
</code></pre>

<p>It ran through my copy of “War and Peace” within a few seconds, without needing any attempt to do parallel processing. Not much surprising, “and_the” was the combination with the highest probability.</p>

<p>You may consider changing the line</p>

<pre><code>String[] words = line.split("" "");
</code></pre>

<p>to</p>

<pre><code>String[] words = line.toLowerCase().split(""\\W+"");
</code></pre>

<p>to generalize the code to work with different input, handling multiple spaces or other punctuation characters and ignoring the case.</p>
",6,1,346,2017-12-13 07:24:44,https://stackoverflow.com/questions/47787594/the-probability-distribution-of-two-words-in-a-file-using-java-8
how to retrieve subtrees while parsing in nlp,"<p>I would like to retrieve the sub tress while parsing the sentence like below:</p>
<pre><code>sentence = &quot;All new medications must undergo testing before they can be 
             prescribed&quot;
parser = stanford.StanfordParser()
tree_parse = parser.raw_parse(sentence)
for i, sub_tree in enumerate(tree_parse[0].subtrees()):
   if sub_tree.label() in [&quot;S&quot;]:
      sub_list = sub_tree
      print(sub_list)
</code></pre>
<p>What I am expecting is to access the subtree labeled &quot;S&quot; individually like below:</p>
<h1>first subtree</h1>
<pre><code>(S
  (NP (DT All) (JJ new) (NNS medications))
  (VP
    (MD must)
    (VP
      (VB undergo)
</code></pre>
<h1>second subtree</h1>
<pre><code>(S
    (VP
      (VBG testing)
      (SBAR
        (IN before)
    
</code></pre>
<h1>3rd subtree</h1>
<pre><code>(S
          (NP (PRP they))
          (VP (MD can) (VP (VB be) (VP (VBN prescribed)))))))))))
</code></pre>
<p>But the actual output is like below:</p>
<pre><code> (NP (DT All) (JJ new) (NNS medications))
  (VP
  (MD must)
  (VP
    (VB undergo)
    (S
      (VP
        (VBG testing)
        (SBAR
          (IN before)
          (S
            (NP (PRP they))
            (VP (MD can) (VP (VB be) (VP (VBN prescribed))))))))))
 How to access the sub tress individually like accessing items in a list?
</code></pre>
","python, parsing, nlp, nltk, stanford-nlp","<p>You are already getting the subtrees: A subtree contains everything below its root, so the output you show is correctly retrieved as the ""subtree"" below the top-level <code>S</code>. Your will then output the subtree dominating ""testing before they can be prescribed"", and finally the lower-most <code>S</code>, dominating ""they can be prescribed"".</p>

<p>Incidentally, you can get the <code>S</code> subtrees directly by specifying a <a href=""http://www.nltk.org/api/nltk.html#nltk.tree.Tree.subtrees"" rel=""nofollow noreferrer"">filter</a>:</p>

<pre><code>for sub_tree in tree_parse[0].subtrees(lambda t: t.label() == ""S""):
    print(sub_tree)
</code></pre>
",1,1,435,2017-12-14 12:24:57,https://stackoverflow.com/questions/47813330/how-to-retrieve-subtrees-while-parsing-in-nlp
TypeError: &#39;&lt;&#39; not supported between instances of &#39;NoneType&#39; and &#39;str&#39; using Pyner for Name entity recognition,"<p>I am trying to pass an email string to Pyner to pull out all the entities into a dictionary. I can verify my setup works with this returning two PERSON entities </p>

<pre><code>import ner
tagger = ner.SocketNER(port=9191, output_format='slashTags')
t = ""My daughter Sophia goes to the university of California. James also goes there""
print(type(t))
test = tagger.get_entities(t)
person_ents = test['PERSON']
for i in person_ents:
    print(i)
</code></pre>

<p>This outputs as expected</p>

<pre><code>Sophia
James
</code></pre>

<p>The only difference is here that I have email text here instead I can verify it's a string </p>

<pre><code>print(type(firstEmail))

test = tagger.get_entities(firstEmail)
person_ents = test['PERSON']
print (type(person_ents))
for i in person_ents:
    print(i)
</code></pre>

<p>This returns the following error</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-79-ff847452c8df&gt; in &lt;module&gt;()
      3 
      4 
----&gt; 5 test = tagger.get_entities(firstEmail)
      6 person_ents = test['PERSON']
      7 print (type(person_ents))

~/anaconda3/envs/nlp/lib/python3.6/site-packages/ner-0.1-py3.6.egg/ner/client.py in get_entities(self, text)
     90         else: #inlineXML
     91             entities = self.__inlineXML_parse_entities(tagged_text)
---&gt; 92         return self.__collapse_to_dict(entities)
     93 
     94     def json_entities(self, text):

~/anaconda3/envs/nlp/lib/python3.6/site-packages/ner-0.1-py3.6.egg/ner/client.py in __collapse_to_dict(self, pairs)
     71         """"""
     72         return dict((first, list(map(itemgetter(1), second))) for (first, second)
---&gt; 73             in groupby(sorted(pairs, key=itemgetter(0)), key=itemgetter(0)))
     74 
     75     def get_entities(self, text):

TypeError: '&lt;' not supported between instances of 'NoneType' and 'str'
</code></pre>

<p>Any idea how what's wrong</p>
","string, python-3.x, stanford-nlp, named-entity-recognition","<p>The issue here is that NER is setup so that when the output is set to SlashTags it output a dictionary format. However the text is parsed with slash characters where a named entity occurs and this character is then used to separate dictionary entity before the dictionary is generated. As a result if any slashes occur in your text data you need to parse this out. </p>

<p>Something like</p>

<pre><code>#text is your string
text = text.replace('/', '-')
</code></pre>

<p>This shouldn't be an issue in NLP terms as dates should still be picked out with this format. But if some key part of your analysis requires this tag to be there this solution might not be suitable. I can't verify if this issue exists in the java implementation but it's possible</p>
",0,1,818,2018-01-04 12:00:09,https://stackoverflow.com/questions/48094827/typeerror-not-supported-between-instances-of-nonetype-and-str-using-pyn
Using foreign language APIs in Rascal?,"<p>Is there a way to invoke a foreign language API in Rascal? In particular, I've been thinking about the <a href=""https://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow noreferrer"">Stanford Core NLP</a> that has a Java API.</p>
","stanford-nlp, rascal","<p>Rascal has an excellent Java API. Essentially, a foreign function is defined as an ordinary Rascal function prefixed with the keyword <code>java</code> and an attribute <code>javaClass</code> that defines the class where the function is implemented.</p>

<p>Take the <code>size</code> function on <code>List</code>s as an example. In Rascal's <code>List</code> module <code>size</code> is defined as follows:</p>

<pre><code>@javaClass{org.rascalmpl.library.Prelude}
public java int size(list[&amp;T] lst);
</code></pre>

<p>In the java class <code>org.rascalmpl.library.Prelude</code>, the method <code>size</code> is implemented like this:</p>

<pre><code>public IValue size(IList lst)
{
   return values.integer(lst.length());
}
</code></pre>

<p>Note that all Rascal values are implemented as (immutable) <code>IValue</code>s and that some marshaling is unavoidable.</p>

<p><em>Final note</em>: interfacing with an NLP library is very interesting (and is actually on our bucket list) but be aware to preserve Rascal's spirit of immutable data and mostly functional solutions. This has to be taken into account when designing the Rascal API for such a library.</p>
",2,2,93,2018-01-06 12:22:30,https://stackoverflow.com/questions/48127287/using-foreign-language-apis-in-rascal
Stanford JavaNLP RegexNERAnnotator Apostrophe,"<p><strong>RegexNERAnnotator cannot seem to identify apostrophes.</strong></p>

<pre><code>    Properties properties = new Properties();
    properties.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions,regexner,tokensregex"");
    properties.put(""regexner.mapping"", ""regexfile.txt"");
    properties.put(""regexner.ignorecase"", ""true"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);
</code></pre>

<p>In regexfile.txt,</p>

<pre><code>Bachelor of (Arts|Laws|Science|Engineering) DEGREE
Lalor   LOCATION    PERSON
Labor   ORGANIZATION
</code></pre>

<p>It is able to identify Bachelor of Arts. Unfortunately, after i changed it to, </p>

<pre><code>Bachelor's of (Arts|Laws|Science|Engineering)   DEGREE
Lalor   LOCATION    PERSON
Labor   ORGANIZATION
</code></pre>

<p>It will not be able to identify Bachelor's of Arts as a DEGREE.</p>

<p>Any help will be greatly appreciated. Thanks in advance. :)</p>
","nlp, stanford-nlp","<p>The RegexNERAnnotator requires the tokenizer in order to work.</p>

<p>Consider a sentence containing the phrase ""Bachelor's of Arts"".
The tokenization process will divide the word Bachelor from the apostrophe, creating two different tokens. </p>

<p>Within the tab separated file regexfile.txt, whitespaces denote a new token.
This means that your custom rule will only match a token which is exactly the word ""Bachelor's"". This will not happen due to the tokenizer.</p>

<p>Write rules where each token you want to match is separated with a whitespace and everything will work.</p>

<pre><code>Bachelor 's of (Arts|Laws|Science|Engineering)   DEGREE
Lil ' Jon    RAPPER
</code></pre>
",1,0,62,2018-01-07 15:29:10,https://stackoverflow.com/questions/48138642/stanford-javanlp-regexnerannotator-apostrophe
Ambiguous Entity in stanfors NER,"<p>I am working on Stanford NER, My question is regarding ambiguous entities.
For example, I have 2 sentences:</p>

<blockquote>
  <ol>
  <li>I love oranges.</li>
  <li>Orange is my dress code for tomorrow.</li>
  </ol>
</blockquote>

<p>How can i train these 2 sentences to give out, </p>

<blockquote>
  <p>first orange as Fruit,  second orange as Color.</p>
</blockquote>

<p>Thanks</p>
","python-3.x, stanford-nlp, named-entity-recognition","<p>Scrape data from sites like wikipedia,etc. and create a scoring model and then use it for context prediction.</p>
",0,1,210,2018-01-08 11:21:56,https://stackoverflow.com/questions/48149281/ambiguous-entity-in-stanfors-ner
Ambiguous Entity in stanfors NER,"<p>I am working on Stanford NER, My question is regarding ambiguous entities.
For example, I have 2 sentences:</p>

<blockquote>
  <ol>
  <li>I love oranges.</li>
  <li>Orange is my dress code for tomorrow.</li>
  </ol>
</blockquote>

<p>How can i train these 2 sentences to give out, </p>

<blockquote>
  <p>first orange as Fruit,  second orange as Color.</p>
</blockquote>

<p>Thanks</p>
","python-3.x, stanford-nlp, named-entity-recognition","<p>Scrape data from sites like wikipedia,etc. and create a scoring model and then use it for context prediction.</p>
",0,1,210,2018-01-08 11:21:56,https://stackoverflow.com/questions/48149281/ambiguous-entity-in-stanfors-ner
"Training Stanford-NER-CRF, control number of iterations and regularisation (L1,L2) parameters","<p>I was looking through StanfordNER documentation/FAQ but I can't find anything related to specifying the maximum number of iterations in training and also the value of the regularisation parameters L1 and L2.</p>

<p>I saw an answer on which is suggested to set, for instance: </p>

<p><code>maxIterations=10</code> </p>

<p>in the properties file, but that did not gave any results.</p>

<p>Is it possible to set these parameters?</p>
","nlp, stanford-nlp, crf, named-entity-recognition","<p>I had to dig in the code but found it, so basically StanfordNER supports many different numerical optimization algorithms. One can see which ones are implemented and can be used to train the CRF by looking into the 
 <code>getMinimizer()</code> method in the <code>CRFClassifier.java</code> file.</p>

<p>I configured my properties file to use the Orthant-Wise Limited-memory Quasi-Newton, by setting: </p>

<p><code>useOWLQN = true</code></p>

<p>The L1-prior can be set with:</p>

<p><code>priorLambda = 10</code></p>

<p>An useful trick is to play with the convergence tolerance parameter TOL, which is checked at each iteration: <code>|newest_val - previous_val| / |newestVal| &lt; TOL</code>, the <code>TOL</code> is controlled by:</p>

<p><code>tolerance = 0.01</code></p>

<p>Yet another useful parameter is to explicitly control the maximum number of iterations for which the learning algorithm should run:</p>

<p><code>maxQNItr = 100</code></p>
",0,0,479,2018-01-09 10:27:28,https://stackoverflow.com/questions/48166110/training-stanford-ner-crf-control-number-of-iterations-and-regularisation-l1-l
Library works in IntelliJ doesn&#39;t but doesn&#39;t work in Android studio,"<p>I have java project based on <strong>Stanford Natural Language Processing</strong> libraries, which i want to redo in android studio, the libraries are causing issues. does anyone know how to import natural language processing library Stanford CoreNLP to android studio?</p>

<p>I've googled it, there doesn't seem to be a solution for this yet.</p>

<p>i have imported NLP libraries, and added them to android studio gradle file, i get this error when i run the app. <a href=""https://github.com/amirdora/StanfordCoreNLP"" rel=""nofollow noreferrer"">Github link</a> This is a sample test app, just making libraries work and then i'll add them to main project.</p>

<p><a href=""https://i.sstatic.net/2m6re.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2m6re.png"" alt=""enter image description here""></a></p>
","java, android, intellij-idea, stanford-nlp, libraries","<p>Update your Android SDK to version 26 or higher, and make sure Android Studio is using it.</p>
",1,-1,173,2018-01-09 15:37:04,https://stackoverflow.com/questions/48171681/library-works-in-intellij-doesnt-but-doesnt-work-in-android-studio
Lemmatization of Spanish sentences In Stanford CoreNLP,"<p>How can I use Stanford-NLP to lemmatize words or is this even a possibility in coreNLP? </p>

<p>According to this website (<a href=""https://stanfordnlp.github.io/CoreNLP/human-languages.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/human-languages.html</a>) Lemmatization is not an option--but I'm hoping that this is a neglected page that needs to be updated. </p>

<p>Additionally, I've seen related questions but about German: <a href=""https://stackoverflow.com/questions/29861925/does-stanford-core-nlp-support-lemmatization-for-german"">Does Stanford Core NLP support lemmatization for German?</a></p>

<p><strong>How can I lemmatize spanish words in CoreNLP?</strong></p>
","nlp, stanford-nlp","<p>According to GitHub, this is not functionality that exists in Stanford Core NLP. </p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/issues/137"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/issues/137 </a></p>
",1,1,862,2018-01-10 06:00:03,https://stackoverflow.com/questions/48181177/lemmatization-of-spanish-sentences-in-stanford-corenlp
How to extract contained words by TokensRegex,"<p>for example, all the words contained ""b""</p>

<p>In a sentence: ""A boy is going to take a bus""</p>

<p>the result should be ""boy"" ""bus""</p>

<p>Here is my rules but it does not work in this case.</p>

<pre><code>{ 
 ruleType: ""tokens"",
 pattern: ( [ { word:/b/ } ]),
 result: Format(""%s"", """")
}
</code></pre>
",stanford-nlp,"<p>I expect you need the regex to include wildcards before and after the ""b"": <code>/.*b.*/</code>:</p>

<pre><code>{ 
 ruleType: ""tokens"",
 pattern: ( [ { word:/.*b.*/ } ]),
 result: Format(""%s"", """")
}
</code></pre>
",1,0,44,2018-01-10 06:56:15,https://stackoverflow.com/questions/48181864/how-to-extract-contained-words-by-tokensregex
Is there any part of speech tagger and tokenizer of Tamil language?,"<p>I am a beginner in natural language processing. I have to work on different languages that Tamil is one of them. Could I ask from experts whether there is any Tamil language tokenizer code (java,c,python or etc.) and part of speech tagger codes that I use it for my research? </p>

<p>I really appreciate if I can get some experts' opinion here. Any help is appreciated.</p>

<p>Thanks</p>
","nlp, stanford-nlp, opennlp, tamil","<p>I have found one tool for tokenization <a href=""http://anoopkunchukuttan.github.io/indic_nlp_library/"" rel=""nofollow noreferrer"">Indic NLP Library</a>. It supports Tamil.</p>

<hr>

<p>I found no POS tagger tools available on the internet, but I have found some papers:</p>

<p>2008 <a href=""https://pdfs.semanticscholar.org/d721/793e92770c37dd676cd74905add5291c9c81.pdf?_ga=2.156283001.399233541.1517161769-1057452481.1494151046"" rel=""nofollow noreferrer"">Morpheme based Language Model
for Tamil Part-of-Speech Tagging</a></p>

<p>2009 <a href=""https://link.springer.com/content/pdf/10.1007/978-3-642-00831-3_2.pdf"" rel=""nofollow noreferrer"">CRF Models for Tamil Part of Speech
Tagging and Chunking</a></p>

<p>2009 <a href=""https://pdfs.semanticscholar.org/ffb7/494c35b766d0cac1a87298ea4f9bf00f5ad2.pdf?_ga=2.146764662.399233541.1517161769-1057452481.1494151046"" rel=""nofollow noreferrer"">Improvement of Rule Based Morphological
Analysis and POS Tagging in Tamil Language
via Projection and Induction Techniques</a></p>

<p><strong>Maybe you can contact the authors for help.</strong></p>

<hr>

<p>Or if you can speak Tamil, search on the internet(especially university websites) in Tamil, you may find some resources and tools.</p>
",4,3,1418,2018-01-10 09:55:26,https://stackoverflow.com/questions/48184642/is-there-any-part-of-speech-tagger-and-tokenizer-of-tamil-language
Stanford CoreNLP OpenIE by sentence?,"<p>I'm currently using the OpenIE system from Stanford CoreNLP using its Java command line interface</p>

<pre><code>java -mx32g -cp stanford-corenlp-3.8.0.jar:stanford-corenlp-3.8.0-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar edu.stanford.nlp.naturalli.OpenIE test_file.txt -threads 8 -resolve_coref true
</code></pre>

<p>My test file contains 50,000 sentences, one per line.</p>

<p>The OpenIE result would be a list of tuples for all sentences. Is there a flag which I can set to have a correspondence between each tuple and the particular sentence? (e.g., some sentences may have no extractions, some may have more than one. How can I know which is which?)</p>

<p>My current solution is to have 50,000 files, with one sentence per file. But this is incredibly slow, as models have to be reloaded with every file.</p>

<p>Thanks.</p>

<p>Edit:</p>

<p>I realized that the -filelist flag makes processing much faster, which is a good thing. But the output unfortunately still does not differentiate between the different files.</p>
",stanford-nlp,"<p>You should be able to get sentence info if you output using the Reverb format (<code>-format reverb</code>). In addition, I expect you'll want to force the tokenizer to split sentences on newlines (<code>-ssplit.newlineIsSentenceBreak always</code>). For example, the following command should work, adapted from your example:</p>

<pre><code>java -mx8g -cp stanford-corenlp-3.8.0.jar:stanford-corenlp-3.8.0-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar \
    edu.stanford.nlp.naturalli.OpenIE \
    -threads 8 -resolve_coref true \
    -ssplit.newlineIsSentenceBreak always \
    -format reverb \
    input.txt
</code></pre>

<p>For the following input file:</p>

<pre><code>George Bush was born in Texas 
Obama was born in Hawaii
</code></pre>

<p>I get the following output on stdout (you can redirect it to a file with the <code>-output &lt;filename&gt;</code> flag):</p>

<pre><code>input.txt   0   George Bush was born    0   2   2   3   3   4   1.000   George Bush was born in Texas   NNP NNP VBD VBN IN NNP  George Bush be  bear
input.txt   0   George Bush was born in Texas   0   2   2   5   5   1.000   George Bush was born in Texas   NNP NNP VBD VBN IN NNP  George Bush be bear in  Texas
input.txt   1   Obama   was born in Hawaii  0   1   1   4   4   5   1.000   Obama was born in Hawaii    NNP VBD VBN IN NNP  Obama   be bear in  Hawaii
input.txt   1   Obama   was born    0   1   1   2   2   3   1.000   Obama was born in Hawaii    NNP VBD VBN IN NNP  Obama   be  bear
</code></pre>

<p>The second line is the sentence index; the full list of tab-separated columns is documented <a href=""https://github.com/knowitall/reverb/blob/master/README.md"" rel=""nofollow noreferrer"">on the ReVerb README</a>:</p>

<ol>
<li>The filename (or stdin if the source is standard input)</li>
<li>The sentence number this extraction came from.</li>
<li>Argument1 words, space separated</li>
<li>Relation phrase words, space separated</li>
<li>Argument2 words, space separated</li>
<li>The start index of argument1 in the sentence. For example, if the value is i, then the first word of argument1 is the i-1th word in the sentence.</li>
<li>The end index of argument1 in the sentence. For example, if the value is j, then the last word of argument1 is the jth word in the sentence.</li>
<li>The start index of relation phrase.</li>
<li>The end index of relation phrase.</li>
<li>The start index of argument2.</li>
<li>The end index of argument2.</li>
<li>The confidence that this extraction is correct. The higher the number,  the more trustworthy this extraction is.</li>
<li>The words of the sentence this extraction came from, space-separated.</li>
<li>The part-of-speech tags for the sentence words, space-separated.</li>
<li>The chunk tags for the sentence words, space separated. These represent a shallow parse of the sentence.</li>
<li>A normalized version of arg1. See the BinaryExtractionNormalizer javadoc for details about how the normalization is done.</li>
<li>A normalized version of rel.</li>
<li>A normalized version of arg2.</li>
</ol>
",3,0,1686,2018-01-17 02:39:51,https://stackoverflow.com/questions/48292805/stanford-corenlp-openie-by-sentence
Cannot Initialize CoreNLP in R,"<p>I am unable to access <code>coreNLP</code> in R on a Mac running High Sierra. I am uncertain what the problem is, but it seems that every time I try again to get <code>coreNLP</code> to work, I am faced with a different error. I have JDK 9.0.4. Please see my code below for what I am attempting to do, and the error that stops me.</p>

<p>My previous attempt I was able to get <code>initCoreNLP()</code> to run and load some elements of the packages, but it would fail on others. When I then attempted to run <code>annotateString()</code>, it would throw the error <code>Error Must initialize with 'int CoreNLP'!</code>.</p>

<p>I have downloaded and re-downloaded the <code>coreNLP</code> Java archive many times and still no luck! See image for contents of my <code>coreNLP</code> R package folder located at <code>/Library/Frameworks/R.framework/Versions/3.4/Resources/library/coreNLP</code>.</p>

<p>Do you know how I can successfully initialize <code>coreNLP</code>?</p>

<pre><code>dyn.load(""/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/lib/server/libjvm.dylib"")

library(NLP)
library(coreNLP)

&gt; downloadCoreNLP()
trying URL 'http://nlp.stanford.edu/software//stanford-corenlp-full-2015-12-09.zip'
Content type 'application/zip' length 403157240 bytes (384.5 MB)
==================================================
downloaded 384.5 MB

&gt; initCoreNLP()
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP.properties
Error in rJava::.jnew(""edu.stanford.nlp.pipeline.StanfordCoreNLP"", basename(path)) : 
  edu.stanford.nlp.io.RuntimeIOException: ERROR: cannot find properties file ""StanfordCoreNLP.properties"" in the classpath!
</code></pre>

<p><a href=""https://i.sstatic.net/Qx32P.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qx32P.png"" alt=""Contents of coreNLP R Package folder""></a></p>
","java, r, stanford-nlp, rjava","<p>Per our discussion.</p>

<p>My sense is your Java / R configuration dependency issue. Thus, it appears that <code>rJava</code> is dependent on the version of <code>java</code> used and <code>coreNLP</code> is dependent on <code>rJava</code>.</p>

<pre><code>java &lt;- rJava &lt;- coreNLP
</code></pre>

<p>thus we can set the dlynlib version to 1.8.X, uninstall rJava, reinstall rJava then reinstall coreNLP.</p>

<h3>Setup a particular version of java in RStudio</h3>

<pre><code>dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/server/libjvm.dylib')

remove.packages(""rJava"")
install.packages(""rJava"")

ipak &lt;- function(pkg){
  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, ""Package""])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

# usage
packages &lt;- c(""NLP"", ""coreNLP"", ""rJava"")
ipak(packages)

.jinit()
.jcall(""java/lang/System"",""S"",""getProperty"",""java.version"")

# run the follwoing command once
# downloadCoreNLP() # &lt;- Takes a while...

initCoreNLP()
example(getSentiment)
sIn &lt;- ""Mother died today. Or, maybe, yesterday; I can't be sure.""
annoObj &lt;- annotateString(sIn)
</code></pre>
",1,2,1519,2018-01-29 06:57:23,https://stackoverflow.com/questions/48495923/cannot-initialize-corenlp-in-r
How to suppress Stanford CoreNLP Redwood logging in MATLAB?,"<p>The thread <a href=""https://stackoverflow.com/questions/21851217/how-to-shutdown-stanford-corenlp-redwood-logging"">How to shutdown Stanford CoreNLP Redwood logging?</a> supposedly resolved my question in Java. I would like to do the same in MATLAB, but the code(s) given in that thread doesn't work. Please suggest a complete solution, starting with what to import, setting properties, etc.</p>

<p>My code is the following:</p>

<pre><code>import java.io.*;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;

tagger = MaxentTagger('./english-left3words-distsim.tagger');
</code></pre>

<p>which logs on the command line:</p>

<pre><code>Reading POS tagger model from ./english-left3words-distsim.tagger ... done [3.6 sec].
</code></pre>

<p>taken from CreateTagger.m in my package:
<a href=""https://github.com/jzsfvss/POSTaggerSML"" rel=""nofollow noreferrer"">https://github.com/jzsfvss/POSTaggerSML</a>.</p>
","matlab, logging, stanford-nlp","<p>I'm not very up on calling Java from MatLab, but I think it would work and remove the loading message to use instead the following (where the final <code>false</code> argument says to not print loading):</p>

<pre><code>import edu.stanford.nlp.util.StringUtils;

...
tagger = MaxentTagger('./english-left3words-distsim.tagger', StringUtils.argsToProperties({'-model', './english-left3words-distsim.tagger'}), false);
</code></pre>

<p>If that doesn't work, perhaps the easiest thing would be to make an slf4j config file <code>simplelogger.properties</code> with the line:</p>

<pre><code>org.slf4j.simpleLogger.log.edu.stanford.nlp.tagger.maxent.MaxentTagger=warn
</code></pre>

<p>and to make sure that file is on the Java classpath (presumably with a call to <code>javaaddpath</code>) so that the tagger component no longer prints INFO messages like the file loading.</p>
",0,0,251,2018-01-30 02:42:11,https://stackoverflow.com/questions/48513078/how-to-suppress-stanford-corenlp-redwood-logging-in-matlab
Stanford NLP 3.9.0: Does using CoreEntityMention combine adjacent entity mentions?,"<p>I am testing out getting entity mentions the new 3.9.0 way with CoreEntityMention.  I do something like:</p>

<pre><code>    CoreDocument document = new CoreDocument(text);
    stanfordPipe = createNerPipeline();
    stanfordPipe.annotate(document);

    for (CoreSentence sentence : document.sentences()) {
        logger.debug(""Found sentence {}"", sentence);
        if (sentence.entityMentions() == null) continue;
        for (CoreEntityMention cem : sentence.entityMentions()) {
            logger.debug(""Found em {}"", stringify(cem));            
        }
    }
</code></pre>

<p>When I iterate through entity mentions using <code>sentence.entityMentions()</code> I see that some of the entity mentions produced are multi-token entity mentions.  The old way of getting entity mentions, and correct me if I am wrong, is that you have to iterate over CoreLabel and therefore have to combine the multi-token entity mentions yourself.</p>

<p>So is there some new method that did not exist before to combine adjacent tokens with the same ner label?  Or have I missed older ways to combine multi-token entity mentions?</p>
","stanford-nlp, named-entity-recognition","<p>Hi thanks for using the new interface!</p>

<p>Yes, the CoreEntityMention is supposed to represent a full entity mention.  This was some new syntax added to help make it easier to work with our code.</p>

<p>Traditionally there has been a need for things like sentence.get(CoreAnnotations.TokensAnnotation.class)...etc...so we tried to add some wrapper classes so people could use the pipeline interface but not have the cumbersome syntax.</p>

<p>With this newly debuted syntax, you can write:</p>

<pre><code>sentence.tokens();
</code></pre>

<p>Regarding entity mentions, if the sentence is ""Joe Smith went to Hawaii."" you would get two entity mentions:</p>

<p>Joe Smith  (2 tokens)
Hawaii (1 token)</p>

<p>Traditionally the <code>ner</code> annotator would tag every token in the sentence with it's named entity type.  Then a separate <code>entitymentions</code> annotator would build <code>Mention</code> annotations which were <code>CoreMap</code> representations of full entity mentions (e.g. Joe Smith).</p>

<p>I've seen a lot of people over the years ask ""How do I go from a tagged sequence of tokens to the full entity mentions?""  So in response to this we tried to make it a lot easier to just extract the full entity's referred to in the sentence.</p>

<p>I should also note that for the most part the older ways should still work.  Updated documentation is on the way as we work on finalizing the 3.9.0 release!</p>
",2,0,328,2018-02-05 22:04:36,https://stackoverflow.com/questions/48632256/stanford-nlp-3-9-0-does-using-coreentitymention-combine-adjacent-entity-mention
How to add a label to all words in a file?,"<p>I have a file containing words, I want to read this file and add a label in front of all words. The label should be added on the right side of the words. eg. <code>book - ""O""</code>, <code>Berlin - ""O""</code>. How to do it in python? I have tried this code but not given my answer.</p>

<pre><code>inp = open('Dari.pos', 'r')
out = open('DariNER.txt', 'w')

for line in iter(inp):
    word= line.__add__(""O"")
    out.write(word)
inp.close()
out.close()
</code></pre>
","python, python-3.x, python-2.7, stanford-nlp, nltk-trainer","<p>If I understand the correct output format word-O, you can try something like this:</p>

<pre><code>words = open('filename').read().split()
labeled_words = [word+""-O"" for word in words]

# And now user your output format, each word a line, separate by tabs, whatever.
# For example new lines
with open('outputfile','w') as output:
    output.write(""\n"".join(labeled_words))
</code></pre>
",0,-6,251,2018-02-06 13:47:11,https://stackoverflow.com/questions/48644605/how-to-add-a-label-to-all-words-in-a-file
NLP - Sentence does not follow any of the grammar rule in Syntactic parsing,"<p>I have grammar extracted from Treebank dataset from nltk library. Now the parser will use these rules to parse the sentence. So for example I have grammar like below:</p>

<pre><code>A-&gt;B C
B-&gt;'b'
C-&gt;'c'
D-&gt;'d'
</code></pre>

<p>Now suppose I have sentence like <code>b c</code> so the parser will make parse tree like below</p>

<pre><code>    A
   / \
  B   C
  |   |
  b   c
</code></pre>

<p>So like this I have all the grammar from training dataset. Now for testing assume a completely new sentence is there like <code>""c d""</code></p>

<p>Parse tree for above sentence will be</p>

<pre><code>  C   D
  |   |
  c   c
</code></pre>

<p>and  the parser will stop as there is no rule for <code>A-&gt;C D</code></p>

<p>So how to parse these kind of sentences because training grammar does not have any grammar like <code>A -&gt; C D</code></p>

<p>So the question is how to parse sentence if it completely new and grammar rule is not present in training data set? I am using probabilistic grammar.</p>
","python, parsing, nlp, stanford-nlp, text-parsing","<p>One of the things you could try is to cut varies parts of the unresolvable input sentence off and deduct a certain confidence score, then try to match it with existing grammar. if the confidence level drop below a certain threshold you would simply return a message about not being able to parse and drop this sentence. While in the same time you could log these sentences and come up with new grammars.</p>

<p>This is what I did for one of the personal assistant projects, trying to give an answer to all unseen or even invalid input is virtually impossible.</p>
",0,0,257,2018-02-07 06:30:53,https://stackoverflow.com/questions/48657359/nlp-sentence-does-not-follow-any-of-the-grammar-rule-in-syntactic-parsing
(java.lang.VerifyError: Rejecting class) error when adding StanfordCoreNLP library in an Android Studio project,"<p>I am developing an Android application (I am a beginner). I want to use Stanford CoreNLP 3.8.0 library in my app to extract the part of speech, the lemma, the parser and so on from the user sentences. </p>

<p>I have tried a simple java code in NetBeans by following this youtube tutorial <a href=""https://www.youtube.com/watch?v=9IZsBmHpK3Y"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=9IZsBmHpK3Y</a>, and it is working perfectly.</p>

<p>The jar files that I imported to the NetBeans project are: stanford-corenlp-3.8.0.jar and stanford-corenlp-3.8.0-models.jar.</p>

<p>And this is the java source code: </p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

import java.util.List;
import java.util.Properties;

public class CoreNlpExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""What is the Weather in Bangalore right now?"";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

        List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

        for (CoreMap sentence : sentences) {
            // traversing the words in the current sentence
            // a CoreLabel is a CoreMap with additional token-specific methods
            for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                // this is the text of the token
                String word = token.get(CoreAnnotations.TextAnnotation.class);
                // this is the POS tag of the token
                String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
                // this is the NER label of the token
                String ne = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);

                System.out.println(String.format(""Print: word: [%s] pos: [%s] ne: [%s]"", word, pos, ne));
            }
        }
    }
} 
</code></pre>

<p>I wanted to try the same code in Android Studio but I am facing a problem with adding these external libraries in my Android Studio 3.0.1 project. </p>

<p>I have read on some websites that I need to reduce the size of the jar files, and I did that and made sure that the reduced jars are still working fine in the Netbeans project. But I am still facing problems in Android studio and this is the error that I am getting: </p>

<pre><code>java.lang.VerifyError: Rejecting class edu.stanford.nlp.pipeline.StanfordCoreNLP that attempts to sub-type erroneous class edu.stanford.nlp.pipeline.AnnotationPipeline (declaration of 'edu.stanford.nlp.pipeline.StanfordCoreNLP' appears in /data/app/com.example.fatimah.nlpapplication-bhlUJOCUwLhSbkWE7NBERA==/split_lib_dependencies_apk.apk)
</code></pre>

<p>Any suggestions on how I can fix this and import Stanford library successfully?</p>
","java, android, android-studio, jar, stanford-nlp","<p>I have been able to overcome this issue and import Stanford CoreNLP library successfully by doing three steps:</p>

<ol>
<li><p>reducing the size of stanford-corenlp-3.8.0-models.jar.</p></li>
<li><p>making the minimum SDK = 25 in build.gradle :
minSdkVersion 25</p></li>
<li><p>including the following line in gradle.properties:
android.enableD8=true</p></li>
</ol>
",0,-1,411,2018-02-07 16:59:39,https://stackoverflow.com/questions/48669330/java-lang-verifyerror-rejecting-class-error-when-adding-stanfordcorenlp-libra
How to extract all types of nouns in Java?,"<p>I want to get all types of nouns from a text how can I get?  </p>

<pre><code>import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import java.io.BufferedReader;
import java.io.FileReader;


public class Noun_Code {


    public static void main(String[] args) {
        try{

            FileReader file = new FileReader(""C:\\Users\\NaB33L NaQ33B!\\Desktop\\TaggerDemo.java"");
            @SuppressWarnings(""resource"")
            BufferedReader reader = new BufferedReader(file);

            String text = """";
            String line = reader.readLine();
            while(line!=null){
                text +=line;
                line = reader.readLine();
            }
            System.out.println(text);
            String tagged;

            MaxentTagger LibAddress =  new MaxentTagger(""F:\\stanford-postagger-2015-04-20\\stanford-postagger-2015-04-20\\models/english-left3words-distsim.tagger"");
            tagged = LibAddress.tagString(text);

            System.out.println(""Frequency : ""+tagged);

            String[] words = tagged.split("" "");

            String[] keyword1 = new String[words.length];
            int len=keyword1.length;
        for(int i = 0;i&lt;words.length;i++)
        {
            int length= words[i].length();
            char chr1 = (char) (words[i].charAt(length-3));
            char chr2 = (char) (words[i].charAt(length-2));
            char chr3 = (char) (words[i].charAt(length-1));
            if(chr1=='N' &amp;&amp; chr2=='N' &amp;&amp; chr3=='P')
            {
            keyword1[i] = words[i]; 
            System.out.println(keyword1[i]);
            }
            else
            {
            keyword1[i] = ""-1"";
            }
        }
            int var =0;
        for(int i = 0;i&lt;keyword1.length;i++)
        {
            if(keyword1[i].equalsIgnoreCase(""-1""))
            {
            var=var+1;
            }
        }
            len=len-var;
            String[] original = new String[len];
            String[] temp = new String[len];
            int e=0;
        for(int i = 0;i&lt;keyword1.length;i++)
        {
            if(keyword1[i].equalsIgnoreCase(""-1"")){}
            else
            {original[e] = keyword1[i];
            temp[e] = keyword1[i];
            e=e+1;
            }
        }
        }
        catch(Exception ex){System.out.println(""Exception :""+ex);}
}
}   
</code></pre>

<p>Please guide me, to get all types of nouns. above is all the code which I am using. I want all the below noun types:
<strong>Common noun</strong>
A common noun is a noun that refers to people or things in general, e.g. boy, country, bridge, city, birth, day, happiness.</p>

<p><strong>Proper noun</strong>
A proper noun is a name that identifies a particular person, place, or thing, e.g. Steven, Africa, London, Monday. In written English, proper nouns begin with capital letters.</p>

<p><strong>Concrete noun</strong>
A concrete noun is a noun which refers to people and to things that exist physically and can be seen, touched, smelled, heard, or tasted. Examples include dog, building, coffee, tree, rain, beach, tune.</p>

<p><strong>Abstract noun</strong>
An abstract noun is a noun which refers to ideas, qualities, and conditions - things that cannot be seen or touched and things which have no physical reality, e.g. truth, danger, happiness, time, friendship, humour.</p>

<p><strong>Collective nouns</strong>
Collective nouns refer to groups of people or things, e.g. audience, family, government, team, jury. In American English, most collective nouns are treated as singular, with a singular verb: The whole family was at the table.</p>
","java, nlp, stanford-nlp","<p>Use this to get all the Nouns from a text; This code will extract all the Nouns having type NN, NNS, NNP, NNPS, PRP etc
Note: For this <strong>OpenNLP</strong> was used.
Here is the link of <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow noreferrer"">Alphabetical list of part-of-speech tags used in the Penn Treebank Project</a>:</p>
<pre><code>public void getNounPhrases(Parse p) {
        if (p.getType().equals(&quot;NN&quot;) || p.getType().equals(&quot;NNS&quot;) || p.getType().equals(&quot;NNP&quot;) || p.getType().equals(&quot;NNPS&quot;)|| p.getType().equals(&quot;PRP&quot;)) {
             nounPhrases.add(p.getCoveredText());
        }
</code></pre>
",0,2,954,2018-02-09 18:09:07,https://stackoverflow.com/questions/48711753/how-to-extract-all-types-of-nouns-in-java
How to group up NER tags in order to get data from sentence as a whole?,"<p>Through the CoreNLP library, upon calling <code>ner()</code> on a <code>CoreLabel</code> I receive a string indicating its named entity tag (such as <code>PERSON</code> or <code>DATE</code>).</p>

<p>However, I know of no way of comparing tokens in a sentence against each other. For example: (text of tokens surrounded in backticks)</p>

<pre><code>`Ellen` PERSON
`Wexler `PERSON
`,` O
`February` DATE
`9` DATE
`,` DATE
`2016` DATE
</code></pre>

<p><strong>Through CoreNLP, How do I group up the person tags in order to get the name <code>Ellen Wexler</code>? Or the date tags in order to get <code>February 9, 2016</code>, or another representation that I could eventually turn into a Date/Calendar object in Java?</strong> I have looked at the example given <a href=""https://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow noreferrer"">here</a>, however that only finds the ner tags for each individual core label. It does not provide me a way to group consecutive, identical ner tags together.</p>

<p><strong>What I have tried:</strong>
I have written a for loop that iterates over the sentence and finds X number of consecutive, identical ner tags (so if X is 2 and the ner tag is <code>PERSON</code>, it will find 2 consecutive PERSONs). In this scenario, that is <code>Ellen Wexler</code>. However, this breaks down when punctuation comes into play, as punctuation, depending on context, is given the ner tag of its adjacent tokens. In addition, there must be some way to do this through CoreNLP.</p>

<p><strong>My Resarch</strong>:
<a href=""https://stackoverflow.com/questions/45349957/merge-consecutive-tokens-with-same-ner-tag-in-corenlp-conll-output"">This</a> similar question has not been answered. The CoreNLP home page provides no answer, as it only provides an example regarding analysis of individual core labels/tokens. </p>
","java, nlp, stanford-nlp, named-entity-recognition","<p>More traditionally you want to use the <code>entitymentions</code> annotator.</p>

<p>In version 3.9.0 which has just been beta-released, the <code>ner</code> annotator will automatically create entity mentions which link tokens together that belong to the same entity mention.</p>

<p>You can see some example usage of a new API to see how to easily access the entity mentions.  Some of the features of this class aren't in the beta of 3.9.0 on the site, but will be added in an updated version very soon.</p>

<p>Helpful demo code:</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/examples/BasicPipelineExample.java"" rel=""noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/examples/BasicPipelineExample.java</a></p>
",6,1,1428,2018-02-10 20:00:53,https://stackoverflow.com/questions/48725027/how-to-group-up-ner-tags-in-order-to-get-data-from-sentence-as-a-whole
When will Stanford Core NLP version 3.9.0 be on Maven Central?,"<p>The Stanford Core NLP says that version 3.9.0 is available but I can't see this on Maven Central yet. </p>
",stanford-nlp,"<p>3.9.0 is still in a beta phase, but we're working on putting it up on Maven Central very soon.  Hopefully early next week!</p>

<p>Update: Stanford CoreNLP 3.9.1 is now out on Maven Central !</p>
",3,2,73,2018-02-14 12:03:41,https://stackoverflow.com/questions/48786809/when-will-stanford-core-nlp-version-3-9-0-be-on-maven-central
NLP - Speed of Named Entity Recognition (StanfordNER),"<p>I'm doing Natural Language Processing (NLP) in Python 3 and more specifically Named Entity Recognition (NER) on the Harry Potter set of books. I'm using StanfordNER, which works pretty well but takes incredible amounts of time...</p>

<p>I have done some research online on why it would be this slow but I can't seem to find anything that truly suits my code, and I honestly think the problem lays more in the (bad) way I have written the code.</p>

<p>So here's what I wrote for now :</p>

<pre><code>import string
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk.tag.stanford as st

tagger = st.StanfordNERTagger('_path_/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz', '_path_/stanford-ner-2017-06-09/stanford-ner.jar')

#this is just to read the file

hp = open(""books/hp1.txt"", 'r', encoding='utf8')
lhp = hp.readlines()

#a small function I wrote to divide the book in sentences

def get_sentences(lbook):
    sentences = []
    for k in lbook:
        j = sent_tokenize(k)
        for i in j:
            if bool(i):
                sentences.append(i)
    return sentences

#a function to divide a sentence into words

def get_words(sentence):
    words = word_tokenize(sentence)
    return words

sentences = get_sentences(lhp)

#and now the code I wrote to get all the words labeled as PERSON by the StanfordNER tagger

characters = []
    for i in sentence:
    characters = [tag[0] for tag in tagger.tag(get_words(sentences[i])) if tag[1]==""PERSON""]
    print(characters)
</code></pre>

<p>Now the problem, as I explained, is that the code takes huge amounts of time... So I'm wondering, is it normal or <strong>can I save time by rewriting the code in a better way ?</strong> If so, could you help me out ?</p>
","python, python-3.x, nlp, stanford-nlp, named-entity-recognition","<p>The bottleneck is the <code>tagger.tag</code> method, it has a big overhead. Therefore, calling it for every sentence results in a really slow program. Unless there's an additional need for splitting the book into sentences, I'd process the whole text at once:</p>

<pre><code>with open('books/hp1.txt', 'r') as content_file:
    all_text = content_file.read()
    tags = tagger.tag(word_tokenize(all_text))
    characters = [tag[0] for tag in tags if tag[1] == ""PERSON""]
    print(characters)
</code></pre>

<p>Now if what you wanted to know is, say, what sentence each character is mentioned in, then you could first get the characters' names in <code>characters</code> like in the code above, and then loop through the sentences checking if an element from <code>characters</code> exists there.</p>

<p>If file size is a concern (although a .txt file of most books shouldn't be a problem to load into memory), then instead of reading the whole book, you could read a number <code>n</code> of sentences at once. From your code, modify your for loop like so:</p>

<pre><code>n = 1000
for i in range(0, len(sentences), n):
    scs = '. '.join(sentences[i:i + n])
    characters = [tag[0] for tag in tagger.tag(get_words(scs)) if tag[1]==""PERSON""]
</code></pre>

<p>The general idea is to minimize the calls to <code>tagger.tag</code> for its big overhead.</p>
",3,0,606,2018-02-15 22:00:52,https://stackoverflow.com/questions/48817017/nlp-speed-of-named-entity-recognition-stanfordner
What is the difference between the different GloVe models?,"<p><a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a></p>

<p>I'm trying to use GloVe for summarizing music reviews, but I'm wondering which version is the best for my project. Will ""glove.840B.300d.zip"" give me a more accurate text summarization since it used way more tokens? Or perhaps the Wikipedia 2014 + Gigaword 5 is more representative than Common Crawl? Thanks!</p>
","nlp, deep-learning, stanford-nlp","<p>Unfortunately I don't think anyone can give you a better answer for this than:</p>

<p>""try several options, and see which one works the best""</p>

<p>I've seen work that uses the Wikipedia 2014 + Gigaword 100d vectors that produced SOTA results for reading comprehension.  Without experimentation, it's difficult to say conclusively which corpus is closer to your music review set, or what the impact of larger dimensional word embeddings will be.</p>

<p>This is just random advice, but I guess I would suggest trying in this order:</p>

<p>100d from Wikipedia+Gigaword
300d from Wikipedia+Gigaword
300d from Common Crawl</p>

<p>You might as well start with the smaller dimensional embeddings while prototyping, and then you could experiment with larger embeddings to see if you get a performance enhancement.</p>

<p>And in the spirit of promoting other group's work, I would definitely say you should look at these ELMo vectors from AllenNLP:</p>

<p><a href=""http://allennlp.org/elmo"" rel=""nofollow noreferrer"">http://allennlp.org/elmo</a></p>

<p>They look very promising!</p>
",4,1,2924,2018-02-18 00:17:44,https://stackoverflow.com/questions/48847221/what-is-the-difference-between-the-different-glove-models
Test Maximum Entropy classifier,"<p>Is it possible to classify new data trough the Stanford Maximum Entropy classifier WITHOUT creating an external file including all the features?</p>

<p>In other words i have a test file in the following format:</p>

<p><strong>token1 \t feature1_1 \t ... \t feature1_N \t goldLabel1</strong></p>

<p><strong>...</strong></p>

<p><strong>tokenM \t featureM_1 \t ... \t featureM_N \t goldLabelM</strong></p>

<p>I was wondering if it is possible to use a data structure to include test data
without creating an external file.</p>
","nlp, stanford-nlp, text-classification","<p>If you review this method (line 409 in ColumnDataClassifier)</p>

<pre><code>private Pair&lt;GeneralDataset&lt;String,String&gt;, List&lt;String[]&gt;&gt; readDataset(String filename, boolean inTestPhase) {
</code></pre>

<p>you can see how the code goes from a  file path to a <code>Pair&lt;GeneralDataset&lt;String,String&gt;, List&lt;String[]&gt;&gt;</code></p>

<p>That is the key data object needed for evaluation.</p>

<p>If you review this method (line 2158 in ColumnDataClassifier) you can see how the evaluation is done</p>

<p><code>public Pair&lt;Double, Double&gt; testClassifier(String testFile) {</code></p>

<p>If you review the <code>main()</code> method (line 2011) you will see an example of the <code>ColumnDataClassifier</code> being built.</p>

<p>By looking at these three methods you can write additional code to do what you want to do and avoid writing to disk.</p>
",0,0,134,2018-02-23 12:10:10,https://stackoverflow.com/questions/48947590/test-maximum-entropy-classifier
Sentiment analysis using bigrams,"<p>So I have some reviews that I am trying to classify into positive or negative. I am trying to use NLTK and Stanford coreNLP to do so. I am able to do it on unigrams but its not working for bigrams. I tried the following for bigrams</p>

<pre><code>def classifySentence(sen):
  wn_lem = WordNetLemmatizer()
  pos = 0
  neg = 0
  stop_words = set(stopwords.words('english'))
  filtered_review = [token for token in nltk.word_tokenize(sen) if not token in stop_words]


  for token in nltk.bigrams(filtered_review):
      #lemma = wn_lem.lemmatize(token)
      # print(""lemma=""+token)
      if len(wn.synsets(token))&gt;0:
          synset = wn.synsets(token)[0]
          #print(""synset.name=""+synset.name())

          sent = swn.senti_synset(synset.name())

          #print(""Sentiment of ""+token+"" ""+str(sent))

          pos = pos + sent.pos_score()
          neg = neg + sent.neg_score()
          # print (token + ""(pos_score): "" + str(pos) +""\n"")
          # print (token + ""(neg_score): "" + str(neg) +""\n"")
  #print (filtered_review)
  JoinedTokens = ' '.join(wo for wo in filtered_review)
  return [JoinedTokens, pos, neg]
</code></pre>

<p>I was wondering if someone could suggest me ways to do this. I would like to use NLTK or can also use stanfordcoreNLP. I am also open to using other python packages but just need some guidance
I have written some code for using it but it didn't work either. The code I wrote</p>

<pre><code>def StanfordBigrams():
  nlp = StanfordCoreNLP('http://localhost:9000')
  operations = {'annotators': 'tokenize,lemma,pos,sentiment', 'outputFormat': 'json'}
  string = ""not bad""
  tok = nltk.word_tokenize(string)
  bigrams = nltk.bigrams(tok)
  res = nlp.annotate(str(bigrams),operations)
  for s in res[""sentences""]: 
          for token in s[""tokens""]:
              print(""Sentiment: ""+str(s[""sentiment""])+""SentimentValue: ""+str(s[""sentimentValue""]))
              print (token)
</code></pre>

<p>I would be grateful if someone could point me in the right direction.</p>
","python, nltk, stanford-nlp, sentiment-analysis","<p>Are you training a sentiment classifier, or just trying to use one? Technically, I suspect your error is in <code>wn.synset(bigram)</code> -- I doubt the thing returned from <code>nltk.bigrams</code> is a word that can be passed into WordNet.</p>

<p>But, more importantly, you probably want to pass your whole sentence into a sentiment classifier -- bigrams aren't going to have sentiments annotated on them in things like SentiWordNet, and the trained sentiment classifiers are going to have a much easier time on sentences than they are on short snippets. You should be able to get sentiment for <em>some</em> of the bigrams in the sentence from Stanford's sentiment tree (vs just the sentiment value at the root). See the <code>sentimentTree</code> field on the JSON output from the CoreNLP server.</p>
",0,2,1896,2018-02-24 13:24:20,https://stackoverflow.com/questions/48963354/sentiment-analysis-using-bigrams
How to do dependency parsing in R language?,"<p>I did POSTagging. But how to do dependency parsing? Can I get the code for it in R language ?</p>
","machine-learning, nlp, stanford-nlp, opennlp","<p>Probably you already figured out. if not, how I did dependency parsing in R:</p>

<pre><code>library(cleanNLP)
library(udpipe)

s &lt;- ""I prefer the morning flight through Denver""

cnlp_init_udpipe()
doc &lt;- cnlp_annotate(s, as_strings = TRUE)
cnlp_get_dependency(doc, get_token = FALSE)
</code></pre>
",1,0,529,2018-02-25 03:38:42,https://stackoverflow.com/questions/48969988/how-to-do-dependency-parsing-in-r-language
CoreNLP - NER and SUTime to only recognize absolute dates,"<p>I'm working with the Named Entity Recognition annotator of CoreNLP.</p>

<p>My problem is that I would like to not recognize as entities relative dates.
My goal is to connect dates with events</p>

<blockquote>
  <p>Some interesting dates are 18 Feb 1997, the 20th of july, the year 1992, 4 days from today and Monday the 13th.</p>
</blockquote>

<p>In this example I would like to highlight ""18 Feb 1997"", ""20th of july"" and ""1992"".
Even if some of these dates are not complete, they can still be used to search for events.</p>

<p>On the other hand ""4 days from today"" and ""Monday the 13th"" are not interesting for me: the reasons are that the first it is relative to the current date (or the date the text has been written), while the second one is too generic.</p>

<p>Is there a simple way to tell the NER annotator to discard relative dates? </p>

<p>Thank you</p>
","stanford-nlp, named-entity-recognition, sutime","<p>I found the following solution, which works very well in my case.</p>

<p>Each token representing a Time/Date Named Entity has an annotation field containing its normalized form.</p>

<p>The absolute dates that I want to recognize will have a normalized form which follows the following pattern:</p>

<ul>
<li>18 Feb 1997 -> 1997/02/18</li>
<li>20th of July -> XXXX/07/20</li>
<li>1992 -> 1992</li>
</ul>

<p>Using a REGEX it is possible to discard annotations which do not have a normalized form like this.</p>

<pre><code>(\d{4}|X{4})((\/\d{2}(\/\d{2})?)?)
</code></pre>
",1,1,622,2018-03-06 08:56:05,https://stackoverflow.com/questions/49126827/corenlp-ner-and-sutime-to-only-recognize-absolute-dates
serializeTo parameter in ColumnDataClassifier,"<p>I am currently performing a text classification using the ColumnDataClassifier by Stanford NLP group.
I would like to perform the training stage serializing the model through the serializeTo parameter included in the prop file.</p>

<p>Classification results obtained performing training and test stages through the same command line are different from those ones obtained applying the serialized classifier on a new test document. Why this happens?</p>

<p>Example:</p>

<p><strong>First classification</strong></p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -prop myfile.prop </p>

<p>where in myfile.prop i added values for trainFile and testFile.</p>

<p><a href=""https://i.sstatic.net/kMBQ2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kMBQ2.png"" alt=""enter image description here""></a></p>

<p><strong>Second classification</strong></p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -prop myfile2.prop</p>

<p>where in myfile2.prop i added values for trainFile and serializeTo. I am not including any testFile in myfile2.prop. Once i finish the training stage i want to classify new data using the classifier serialized during the training phase.</p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -loadClassifier MyClassifier -testFile myTestFile</p>

<p><a href=""https://i.sstatic.net/hAO7p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hAO7p.png"" alt=""enter image description here""></a></p>

<p>As you can see results are different. In particular, the serialized classifier associates almost all the instances to the class O (the default one).</p>
","nlp, stanford-nlp, text-classification","<p>Well, i resolved the problem by myself. Writing the following line of code leads to unexpected results:</p>

<p><code>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -loadClassifier MyClassifier -testFile myTestFile</code></p>

<p>This is the right command:</p>

<p><code>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -prop file.prop -loadClassifier MyClassifier -testFile myTestFile</code></p>

<p>In other words, the property file must be included even if it used only during the training stage.</p>
",0,-1,112,2018-03-07 12:07:48,https://stackoverflow.com/questions/49151576/serializeto-parameter-in-columndataclassifier
What is &quot;unk&quot; in the pretrained GloVe vector files (e.g. glove.6B.50d.txt)?,"<p>I found ""unk"" token in the glove vector file glove.6B.50d.txt downloaded <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">from https://nlp.stanford.edu/projects/glove/</a>. Its value is as follows:</p>

<pre><code>unk -0.79149 0.86617 0.11998 0.00092287 0.2776 -0.49185 0.50195 0.00060792 -0.25845 0.17865 0.2535 0.76572 0.50664 0.4025 -0.0021388 -0.28397 -0.50324 0.30449 0.51779 0.01509 -0.35031 -1.1278 0.33253 -0.3525 0.041326 1.0863 0.03391 0.33564 0.49745 -0.070131 -1.2192 -0.48512 -0.038512 -0.13554 -0.1638 0.52321 -0.31318 -0.1655 0.11909 -0.15115 -0.15621 -0.62655 -0.62336 -0.4215 0.41873 -0.92472 1.1049 -0.29996 -0.0063003 0.3954
</code></pre>

<p>Is it a token to be used for unknown words or is it some kind of abbreviation?</p>
","neural-network, deep-learning, nlp, word-embedding, glove","<p><strong>The <code>unk</code> token in the pretrained GloVe files is not an unknown token!</strong></p>

<p>See this <a href=""https://groups.google.com/forum/#!searchin/globalvectors/unk|sort:date/globalvectors/9w8ZADXJclA/hRdn4prm-XUJ"" rel=""noreferrer"">google groups thread</a> where Jeffrey Pennington (GloVe author) writes:</p>

<blockquote>
  <p>The pre-trained vectors do not have an unknown token, and currently the code just ignores out-of-vocabulary words when producing the co-occurrence counts.</p>
</blockquote>

<p>It's an embedding learned like any other on occurrences of ""unk"" in the corpus (which appears to happen occasionally!)</p>

<p>Instead, Pennington suggests (in the same post):</p>

<blockquote>
  <p>...I've found that just taking an average of all or a subset of the word vectors produces a good unknown vector.</p>
</blockquote>

<p>You can do that with the following code (should work with any pretrained GloVe file):</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

GLOVE_FILE = 'glove.6B.50d.txt'

# Get number of vectors and hidden dim
with open(GLOVE_FILE, 'r') as f:
    for i, line in enumerate(f):
        pass
n_vec = i + 1
hidden_dim = len(line.split(' ')) - 1

vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)

with open(GLOVE_FILE, 'r') as f:
    for i, line in enumerate(f):
        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)

average_vec = np.mean(vecs, axis=0)
print(average_vec)
</code></pre>

<p>For <code>glove.6B.50d.txt</code> this gives:</p>

<pre><code>[-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011
  0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802
  0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912
  0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227
  0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107
 -0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544
 -0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303
 -0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408
  0.00653003  0.01685157]
</code></pre>

<p>And because it is fairly compute intensive to do this with the larger glove files, I went ahead and computed the vector for <code>glove.840B.300d.txt</code> for you:</p>

<pre><code>0.22418134 -0.28881392 0.13854356 0.00365387 -0.12870757 0.10243822 0.061626635 0.07318011 -0.061350107 -1.3477012 0.42037755 -0.063593924 -0.09683349 0.18086134 0.23704372 0.014126852 0.170096 -1.1491593 0.31497982 0.06622181 0.024687296 0.076693475 0.13851812 0.021302193 -0.06640582 -0.010336159 0.13523154 -0.042144544 -0.11938788 0.006948221 0.13333307 -0.18276379 0.052385733 0.008943111 -0.23957317 0.08500333 -0.006894406 0.0015864656 0.063391194 0.19177166 -0.13113557 -0.11295479 -0.14276934 0.03413971 -0.034278486 -0.051366422 0.18891625 -0.16673574 -0.057783455 0.036823478 0.08078679 0.022949161 0.033298038 0.011784158 0.05643189 -0.042776518 0.011959623 0.011552498 -0.0007971594 0.11300405 -0.031369694 -0.0061559738 -0.009043574 -0.415336 -0.18870236 0.13708843 0.005911723 -0.113035575 -0.030096142 -0.23908928 -0.05354085 -0.044904727 -0.20228513 0.0065645403 -0.09578946 -0.07391877 -0.06487607 0.111740574 -0.048649278 -0.16565254 -0.052037314 -0.078968436 0.13684988 0.0757494 -0.006275573 0.28693774 0.52017444 -0.0877165 -0.33010918 -0.1359622 0.114895485 -0.09744406 0.06269521 0.12118575 -0.08026362 0.35256687 -0.060017522 -0.04889904 -0.06828978 0.088740796 0.003964443 -0.0766291 0.1263925 0.07809314 -0.023164088 -0.5680669 -0.037892066 -0.1350967 -0.11351585 -0.111434504 -0.0905027 0.25174105 -0.14841858 0.034635577 -0.07334565 0.06320108 -0.038343467 -0.05413284 0.042197507 -0.090380974 -0.070528865 -0.009174437 0.009069661 0.1405178 0.02958134 -0.036431845 -0.08625681 0.042951006 0.08230793 0.0903314 -0.12279937 -0.013899368 0.048119213 0.08678239 -0.14450377 -0.04424887 0.018319942 0.015026873 -0.100526 0.06021201 0.74059093 -0.0016333034 -0.24960588 -0.023739101 0.016396184 0.11928964 0.13950661 -0.031624354 -0.01645025 0.14079992 -0.0002824564 -0.08052984 -0.0021310581 -0.025350995 0.086938225 0.14308536 0.17146006 -0.13943303 0.048792403 0.09274929 -0.053167373 0.031103406 0.012354865 0.21057427 0.32618305 0.18015954 -0.15881181 0.15322933 -0.22558987 -0.04200665 0.0084689725 0.038156632 0.15188617 0.13274793 0.113756925 -0.095273495 -0.049490947 -0.10265804 -0.27064866 -0.034567792 -0.018810693 -0.0010360252 0.10340131 0.13883452 0.21131058 -0.01981019 0.1833468 -0.10751636 -0.03128868 0.02518242 0.23232952 0.042052146 0.11731903 -0.15506615 0.0063580726 -0.15429358 0.1511722 0.12745973 0.2576985 -0.25486213 -0.0709463 0.17983761 0.054027 -0.09884228 -0.24595179 -0.093028545 -0.028203879 0.094398156 0.09233813 0.029291354 0.13110267 0.15682974 -0.016919162 0.23927948 -0.1343307 -0.22422817 0.14634751 -0.064993896 0.4703685 -0.027190214 0.06224946 -0.091360025 0.21490277 -0.19562101 -0.10032754 -0.09056772 -0.06203493 -0.18876675 -0.10963594 -0.27734384 0.12616494 -0.02217992 -0.16058226 -0.080475815 0.026953284 0.110732645 0.014894041 0.09416802 0.14299914 -0.1594008 -0.066080004 -0.007995227 -0.11668856 -0.13081996 -0.09237365 0.14741232 0.09180138 0.081735 0.3211204 -0.0036552632 -0.047030564 -0.02311798 0.048961394 0.08669574 -0.06766279 -0.50028914 -0.048515294 0.14144728 -0.032994404 -0.11954345 -0.14929578 -0.2388355 -0.019883996 -0.15917352 -0.052084364 0.2801028 -0.0029121689 -0.054581646 -0.47385484 0.17112483 -0.12066923 -0.042173345 0.1395337 0.26115036 0.012869649 0.009291686 -0.0026459037 -0.075331464 0.017840583 -0.26869613 -0.21820338 -0.17084768 -0.1022808 -0.055290595 0.13513643 0.12362477 -0.10980586 0.13980341 -0.20233242 0.08813751 0.3849736 -0.10653763 -0.06199595 0.028849555 0.03230154 0.023856193 0.069950655 0.19310954 -0.077677034 -0.144811
</code></pre>
",31,24,10774,2018-03-12 16:20:44,https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt
Text Processing Tools for German and Spanish Languages,"<p>I'm trying to process text in German and Spanish languages. Working on English text is straight forward because of myriad NLP packages on this language. But it's not easy for other languages. I Found some packages for German text but I don't know which one is more accurate. Also, It's more difficult to find NLP package for Spanish text considering that there are some special characters in this language. Some steps that I need to do on the text are: Sentence Splitting, Tokenizing, Pos tagging and Stemming. In other words, I am looking for something that works on one or both of these two languages in Java.</p>

<p>Any information on this topic is appreciated.. </p>
","java, nlp, stanford-nlp, text-mining, linguistics","<p>I can recommend you <strong>Freeling</strong>, check its <a href=""http://nlp.lsi.upc.edu/freeling/demo/demo.php"" rel=""nofollow noreferrer"">Freeling_online_demo</a>, it includes Sentence Splitting, Tokenizing, Pos tagging and other functionalities for several language. I dont know how good it's for german but for analyze spanish is the best tool I know. I've just used Freeling via python+command line, but there are interfaces for java too, for example <a href=""https://github.com/TALP-UPC/FreeLing/tree/master/APIs/java"" rel=""nofollow noreferrer"">Freeling_jaVa_API</a>.</p>

<p>Good luck!</p>
",1,0,589,2018-03-13 08:22:28,https://stackoverflow.com/questions/49251361/text-processing-tools-for-german-and-spanish-languages
Stanford CoreNLP 3.9.1 Chinese models does not load,"<p>I have been using the <strong>Stanford CoreNLP</strong> for Chinese language processing.</p>

<p>I upgraded to the latest version 3.9.1, and found out that the Chinese segmenter (and the ssplit, pos)doe not work</p>

<p>this is my <strong>""StanfordCoreNLP.Properties""</strong> file (located under the ""resources"" folder)</p>

<pre><code># Pipeline options - lemma is no-op for Chinese but currently needed because coref demands it (bad old requirements system)
annotators = tokenize, ssplit, pos

# segment
tokenize.language = zh
segment.model = edu/stanford/nlp/models/segmenter/chinese/ctb.gz
segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
segment.sighanPostProcessing = true

# sentence split
ssplit.boundaryTokenRegex = [.\u3002]|[!?\uFF01\uFF1F]+

# pos
pos.model = edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger

# ner
ner.language = chinese
ner.model = edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz
ner.applyNumericClassifiers = true
ner.useSUTime = false

# regexner
ner.fine.regexner.mapping = edu/stanford/nlp/models/kbp/chinese/cn_regexner_mapping.tab
ner.fine.regexner.noDefaultOverwriteLabels = CITY,COUNTRY,STATE_OR_PROVINCE

# parse
parse.model = edu/stanford/nlp/models/srparser/chineseSR.ser.gz

# depparse
depparse.model    = edu/stanford/nlp/models/parser/nndep/UD_Chinese.gz
depparse.language = chinese

# coref
coref.sieves = ChineseHeadMatch, ExactStringMatch, PreciseConstructs, StrictHeadMatch1, StrictHeadMatch2, StrictHeadMatch3, StrictHeadMatch4, PronounMatch
coref.input.type = raw
coref.postprocessing = true
coref.calculateFeatureImportance = false
coref.useConstituencyTree = true
coref.useSemantics = false
coref.algorithm = hybrid
coref.path.word2vec =
coref.language = zh
coref.defaultPronounAgreement = true
coref.zh.dict = edu/stanford/nlp/models/dcoref/zh-attributes.txt.gz
coref.print.md.log = false
coref.md.type = RULE
coref.md.liberalChineseMD = false

# kbp
kbp.semgrex = edu/stanford/nlp/models/kbp/chinese/semgrex
kbp.tokensregex = edu/stanford/nlp/models/kbp/chinese/tokensregex
kbp.language = zh
kbp.model = none

# entitylink
entitylink.wikidict = edu/stanford/nlp/models/kbp/chinese/wikidict_chinese.tsv.gz
</code></pre>

<p><strong>This is the code that utilise the Stanford CoreNLP</strong></p>

<pre><code>public class CoreNlp {

    private static StanfordCoreNLP pipeline = new StanfordCoreNLP();
    private static HashSet&lt;String&gt; meaningless = new HashSet&lt;&gt;(Arrays.asList(""AD"",""AS"",""BA"",""CC"",""CS"",""DEC"",""DEG"",""DER"",""DEV"",""DT"",""ETC"",""IJ"",
            ""LB"",""LC"",""MSP"",""ON"",""P"",""PN"",""PU"",""SB"",""SP"",""VC"",""VE""));
    public static List&lt;String&gt; annotating(String linea){
        List&lt;String&gt; words = new ArrayList&lt;&gt;();

        if(linea == null){
            return words;
        }

        String text = clean(linea);
        if(Util.isNull(text)){
            return words;
        }

        CoreDocument document = new CoreDocument(text);
        CoreNlp.pipeline.annotate(document);

        for (CoreLabel token:  document.tokens()) {
            String word = token.word();
            String pos = token.tag(); 
            if(meaningless.contains(pos)) {
                continue;
            }

            words.add(word);
        }

        return words;
    }

    private static String clean(String myString) {
        StringBuilder newString = new StringBuilder(myString.length());
        for (int offset = 0; offset &lt; myString.length();)
        {
            int codePoint = myString.codePointAt(offset);
            offset += Character.charCount(codePoint);
            // Replace invisible control characters and unused code points
            switch (Character.getType(codePoint))
            {
                case Character.CONTROL:     // \p{Cc}
                case Character.FORMAT:      // \p{Cf}
                case Character.PRIVATE_USE: // \p{Co}
                case Character.SURROGATE:   // \p{Cs}
                case Character.UNASSIGNED:  // \p{Cn}
                case Character.SPACE_SEPARATOR: // \p{Zs}
                case Character.LINE_SEPARATOR: // \p{Zl}
                case Character.PARAGRAPH_SEPARATOR: // \p{Zp}
                    newString.append("""");
                    break;
                default:
                    newString.append(Character.toChars(codePoint));
            }
        }
        return newString.toString();
    }
}
</code></pre>

<p><strong>This is loading log:</strong></p>

<pre><code>2018-03-13 16:22:54.178  INFO 1424 --- [io-10301-exec-5] e.stanford.nlp.pipeline.StanfordCoreNLP  : Searching for resource: StanfordCoreNLP.properties ... found.
2018-03-13 16:22:54.179  INFO 1424 --- [io-10301-exec-5] e.stanford.nlp.pipeline.StanfordCoreNLP  : Adding annotator tokenize
2018-03-13 16:22:54.194  INFO 1424 --- [io-10301-exec-5] e.s.nlp.pipeline.TokenizerAnnotator      : No tokenizer type provided. Defaulting to PTBTokenizer.
2018-03-13 16:22:54.280  INFO 1424 --- [io-10301-exec-5] e.stanford.nlp.pipeline.StanfordCoreNLP  : Adding annotator ssplit
2018-03-13 16:22:54.318  INFO 1424 --- [io-10301-exec-5] e.stanford.nlp.pipeline.StanfordCoreNLP  : Adding annotator pos
2018-03-13 16:22:55.241  INFO 1424 --- [io-10301-exec-5] e.s.nlp.tagger.maxent.MaxentTagger       : Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].
</code></pre>

<p>it seems that the Chinese models had not been loaded. </p>

<p>as a result, the default model (English) has been used for segment, ssplit and pos; thus causing Chinese language processing failed</p>

<p>please advise, </p>

<p>thanks</p>
",stanford-nlp,"<p>I have solved this problem by the following changes in the ""CoreNlp"" class:</p>

<pre><code>private static StanfordCoreNLP pipeline = new StanfordCoreNLP(""StanfordCoreNLP"");
</code></pre>

<p>the loading logs now look like this </p>

<pre><code>2018-03-15 12:50:40.821  INFO 1460 --- [io-10301-exec-7] e.stanford.nlp.pipeline.StanfordCoreNLP  : Searching for resource: StanfordCoreNLP.properties ... found.
2018-03-15 12:50:41.185  INFO 1460 --- [io-10301-exec-7] e.stanford.nlp.pipeline.StanfordCoreNLP  : Adding annotator tokenize
2018-03-15 12:50:52.337  INFO 1460 --- [io-10301-exec-7] e.s.nlp.ie.AbstractSequenceClassifier    : Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... done [10.7 sec].
2018-03-15 12:50:52.393  INFO 1460 --- [io-10301-exec-7] e.stanford.nlp.pipeline.StanfordCoreNLP  : Adding annotator ssplit
2018-03-15 12:50:52.419  INFO 1460 --- [io-10301-exec-7] e.stanford.nlp.pipeline.StanfordCoreNLP  : Adding annotator pos
2018-03-15 12:50:53.292  INFO 1460 --- [io-10301-exec-7] e.s.nlp.tagger.maxent.MaxentTagger       : Loading POS tagger from edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger ... done [0.8 sec].
2018-03-15 12:50:53.362  INFO 1460 --- [io-10301-exec-7] e.s.nlp.wordseg.ChineseDictionary        : Loading Chinese dictionaries from 1 file:
2018-03-15 12:50:53.362  INFO 1460 --- [io-10301-exec-7] e.s.nlp.wordseg.ChineseDictionary        :   edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
2018-03-15 12:50:53.657  INFO 1460 --- [io-10301-exec-7] e.s.nlp.wordseg.ChineseDictionary        : Done. Unique words in ChineseDictionary is: 423200.
2018-03-15 12:50:53.797  INFO 1460 --- [io-10301-exec-7] edu.stanford.nlp.wordseg.CorpusChar      : Loading character dictionary file from edu/stanford/nlp/models/segmenter/chinese/dict/character_list [done].
2018-03-15 12:50:53.806  INFO 1460 --- [io-10301-exec-7] e.stanford.nlp.wordseg.AffixDictionary   : Loading affix dictionary from edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb [done].
</code></pre>
",0,0,408,2018-03-13 10:15:57,https://stackoverflow.com/questions/49253519/stanford-corenlp-3-9-1-chinese-models-does-not-load
Extracting the person names in the named entity recognition in NLP using Python,"<p>I have a sentence for which i need to identify the Person names alone:</p>

<p>For example:</p>

<pre><code>sentence = ""Larry Page is an American business magnate and computer scientist who is the co-founder of Google, alongside Sergey Brin""
</code></pre>

<p>I have used the below code to identify the NERs.</p>

<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk
print(ne_chunk(pos_tag(word_tokenize(sentence))))
</code></pre>

<p>The output i received was:</p>

<pre><code>(S
  (PERSON Larry/NNP)
  (ORGANIZATION Page/NNP)
  is/VBZ
  an/DT
  (GPE American/JJ)
  business/NN
  magnate/NN
  and/CC
  computer/NN
  scientist/NN
  who/WP
  is/VBZ
  the/DT
  co-founder/NN
  of/IN
  (GPE Google/NNP)
  ,/,
  alongside/RB
  (PERSON Sergey/NNP Brin/NNP))
</code></pre>

<p>I want to extract all the person names, such as</p>

<pre><code>Larry Page
Sergey Brin
</code></pre>

<p>In order to achieve this, I refereed this <a href=""https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk"">link</a> and tried this. </p>

<pre><code>from nltk.tag.stanford import StanfordNERTagger
st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz','/usr/share/stanford-ner/stanford-ner.jar')
</code></pre>

<p>However i continue to get this error: </p>

<pre><code>LookupError: Could not find stanford-ner.jar jar file at /usr/share/stanford-ner/stanford-ner.jar
</code></pre>

<p>Where can i download this file?</p>

<p>As informed above, the result that i am expecting in the form of list or dictionary is :</p>

<pre><code>Larry Page
Sergey Brin
</code></pre>
","python, nlp, nltk, stanford-nlp","<h1>In Long</h1>

<p>Please read these <strong>carefully</strong>:</p>

<ul>
<li><a href=""https://stackoverflow.com/a/49345866/610569"">https://stackoverflow.com/a/49345866/610569</a> </li>
<li><a href=""https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk"">Extract list of Persons and Organizations using Stanford NER Tagger in NLTK</a></li>
</ul>

<p>Understand the solution, don't just copy and paste.</p>

<hr>

<h1>TL;DR</h1>

<p>In terminal:</p>

<pre><code>pip install -U nltk

wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31

java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-preload tokenize,ssplit,pos,lemma,parse,depparse \
-status_port 9000 -port 9000 -timeout 15000
</code></pre>

<p>In Python</p>

<pre><code>from nltk.tag.stanford import CoreNLPNERTagger

def get_continuous_chunks(tagged_sent):
    continuous_chunk = []
    current_chunk = []

    for token, tag in tagged_sent:
        if tag != ""O"":
            current_chunk.append((token, tag))
        else:
            if current_chunk: # if the current chunk is not empty
                continuous_chunk.append(current_chunk)
                current_chunk = []
    # Flush the final current_chunk into the continuous_chunk, if any.
    if current_chunk:
        continuous_chunk.append(current_chunk)
    return continuous_chunk


stner = CoreNLPNERTagger()
tagged_sent = stner.tag('Rami Eid is studying at Stony Brook University in NY'.split())

named_entities = get_continuous_chunks(tagged_sent)
named_entities_str_tag = [("" "".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]


print(named_entities_str_tag)
</code></pre>

<p>[out]:</p>

<pre><code>[('Rami Eid', 'PERSON'), ('Stony Brook University', 'ORGANIZATION'), ('NY', 'LOCATION')]
</code></pre>

<p>You might find this help too: <a href=""https://stackoverflow.com/questions/7558908/unpacking-a-list-tuple-of-pairs-into-two-lists-tuples"">Unpacking a list / tuple of pairs into two lists / tuples</a></p>
",11,5,18663,2018-03-20 15:03:38,https://stackoverflow.com/questions/49387699/extracting-the-person-names-in-the-named-entity-recognition-in-nlp-using-python
Java and Stanford Parser,"<p>I used StanfordCoreNLP jar file library to split English paragraphs into sentences but I could retrieve the split sentences as CoreMap Object, but I want to convert those split sentences of type CoreMap to type String, is there anyway to achieve this task. The bold text in the code shows the area where CoreMap is used and I want the sentences retrieved to convert it to String</p>

<p>The code snippet:</p>

<pre><code>    props.setProperty(""annotators"",""tokenize,ssplit"");
    //put that in a pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    //a data structure for the annotation
    Annotation document = new Annotation(text);

    // run the pipeline on that data structure
    pipeline.annotate(document);

    // access the annotations which has worked on a sentence 
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    PrintStream printStream = new PrintStream(new FileOutputStream(""/home/sakshi/Desktop/Admin_System/translate.en""));
    PrintStream console = System.out; // To store the reference to default output stream to use it to restore the default std output stream
    System.setOut(printStream);// To change the default output stream
    **for (CoreMap sentence : sentences) {
        System.out.println(sentence);**

    }
    System.setOut(console);
    response.setContentType(""text/plain"");
    response.getWriter().write(text);
</code></pre>
","java, stanford-nlp","<p><code>whatever.toString()</code>
since toString works for every java Object since every object inherits from java.lang.Object</p>
",-1,0,97,2018-03-21 11:36:43,https://stackoverflow.com/questions/49405325/java-and-stanford-parser
What does representation matrix of context word mean in SkipGram?,"<p>I am learning Stanford NLP Course and I have issue understanding a concept in Skipgram from picture below.</p>

<p><a href=""https://i.sstatic.net/0XwOX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0XwOX.jpg"" alt=""enter image description here""></a>
From left to right, the first column vector is one-hot encoder, the second is the word embedding matrix from 1-layer neural network, the third is word representation vector. However, when it comes to the fourth one, which is a matrix with 'v by d' dimension. Not sure if I listen it correctly, but the speaker said this is a representation of context word and these three matrix are identical? </p>

<p>My questions are:
1. Why these three matrix are identical but the three multiplication results are different?
2. How do we get this matrix (v by d dimension)?</p>

<p>The link to the lecture is: </p>

<p><a href=""https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=1481s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=1481s</a></p>
","machine-learning, nlp, deep-learning, stanford-nlp, word2vec","<p>Before answering your question I have to add a bit of background for the sake of argument from previous slides. First, the optimization is on the probability of one word co-occurring with another word: the center word and a context word. The vector representations could be shared between these two but practically we have two collections of matrixes (list of word vectors) 1. center word vectors (first red matrix on the left) 2. context word vectors (three red matrices in the middle).</p>

<p>The picture in this question shows how we estimate the probabilities with the multiplication of two kinds of vectors and the softmax normalization. Now the questions:</p>

<blockquote>
  <ol start=""2"">
  <li>How do we get this matrix (v by d dimension)?</li>
  </ol>
</blockquote>

<p>As it was mentioned before, this can be the same matrix as word vectors but transposed. Or, you can imagine that we learning two vectors for each word: 1. center 2. context </p>

<p>The context word-vectors in calculations are used in its transposed form:</p>

<pre><code>(center words-vectors, v)  W : (d,V)
(outside words-vectors, uT) W': (V,d)
</code></pre>

<p><code>V</code> being the size of vocabulary and <code>d</code> the dimension size of the vectors. (these are the parameters which we want to learn from data)</p>

<p>Notice how dimensions change in each matrix multiplication:</p>

<pre><code>      W: (d,V)
      x: (V,1)
v = W.x: (d,1) 
     W': (V,d)
   W'.v: (V,1)
</code></pre>

<p><code>x</code> is the one-hot encoding of the center word, <code>W</code> is the list of all word vectors. <code>W.x</code> multiplication basically select the right word vector out of this list. The final result is a list of all possible dot-product of context word vector and the center word vector. The one-hot vector of the true observed context word selects the intended results. Then, based on the loss, updates will be backpropagated through the computation flow updating <code>W</code> and <code>W'</code>.</p>

<blockquote>
  <ol>
  <li>Why these three matrix are identical but the three multiplication results are different?</li>
  </ol>
</blockquote>

<p>The square and two rhombi in the middle are representing one matrix. The three multiplications are happening in three different observations. Although they represent the same matrix, on each observation parameters (<code>W</code> and <code>W'</code>) change using backpropagations. That is why the results are different on three multiplications. </p>

<p><strong>UPDATE FROM CHAT</strong>
However, your expectations are valid, the presentation could show exactly the same results in these multiplications. Because the objective function is the sum of all co-occurrence probabilities in one window. </p>
",2,3,1827,2018-03-25 06:02:28,https://stackoverflow.com/questions/49472999/what-does-representation-matrix-of-context-word-mean-in-skipgram
Speed up annotation time in CoreNLP sentiment,"<p>In my dataset I have 100,000 text files, and I am trying to process them with CoreNLP. The desired result is 100,000 finished text file results, which has classified each sentence as having either a positive, negative or neutral sentiment. 
To get from one text file to annother text file, I use the CoreNLP jar file, which is used from the command line below.    </p>

<pre><code> java -cp ""*"" -mx5g edu.stanford.nlp.sentiment.SentimentPipeline -fileList list.txt
</code></pre>

<p>It takes a very long time to do this, since I can't get the model to take every file in the filelist, but it will take the single path lines as input into the model. </p>

<p>I have furthermore tried to implement some of the other approaches in this link, but I can't get the result need from these.
<a href=""https://stanfordnlp.github.io/CoreNLP/cmdline.html#classpath"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/cmdline.html#classpath</a></p>

<p>Is there a better and faster way to do this and speed up the process?</p>
","java, command-line, nlp, stanford-nlp, sentiment-analysis","<p>Try this command:</p>

<pre><code>java -Xmx14g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse,sentiment -parse.model edu/stanford/nlp/models/srparser/englishSR.ser.gz -outputFormat text -filelist list.txt 
</code></pre>

<p>It will use the faster shift-reduce parser.  This will run through each file in <code>list.txt</code> (1 file per line) and process it.</p>
",1,1,120,2018-03-27 12:43:08,https://stackoverflow.com/questions/49513178/speed-up-annotation-time-in-corenlp-sentiment
maximum number of classes for ColumnDataClassifier,"<p>Is there a limit on the maximum number of classes i can have in using ColumnDataClassifier? I have about addresses that I want to assign to 10k orgs, but i kept running into memory issue even after I set the -xmx number to maximum. </p>
","classification, stanford-nlp, text-classification","<p>There isn't an explicit limit for the size of the label set, but 10k is an extremely large set, and I am not surprised you are having memory issues.  You should try some experiments with substantially smaller label sets (~ 100 labels) and see if your issues go away.  I don't know how many labels will practically work, but I doubt it's anywhere near 10,000.  I would try much smaller sets just to understand how the memory usage is growing at the label set size grows.</p>

<p>You may have to have a hierarchy of labels and different classifiers.  You could imagine the first label being ""California-organization"", and then having a second classifier to select the various California organizations, etc...</p>
",0,0,274,2018-03-27 18:10:10,https://stackoverflow.com/questions/49519757/maximum-number-of-classes-for-columndataclassifier
Running stanford ner tagger in pycharm is not working,"<p>I am new to pycharm professional ide, python and nltk.
i want to use stanfordNertagger for my project work. When I use the following code in python file in pycharm professional</p>

<pre><code>from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize
stanford_classifier = '/home/PycharmProjects/Stanford-NER-Python/stanford/stanford-ner-2015-12-09/classifiers\english.all.3class.distsim.crf.ser.gz'
stanford_ner_path = '/home/PycharmProjects/Stanford-NER-Python/stanford/stanford-ner-2015-12-09/stanford-ner.jar'

# Creating Tagger Object
st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')

text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'

tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)

print classified_text
</code></pre>

<p>The error shown is </p>

<pre><code>Traceback (most recent call last):
  File ""/home/premchikkus/PycharmProjects/Stanford-NER-Python/main.py"", line 9, in &lt;module&gt;
    st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')
  File ""/home/premchikkus/PycharmProjects/Stanford-NER-Python/venv/local/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 168, in __init__
    super(StanfordNERTagger, self).__init__(*args, **kwargs)
  File ""/home/premchikkus/PycharmProjects/Stanford-NER-Python/venv/local/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 53, in __init__
    verbose=verbose)
  File ""/home/premchikkus/PycharmProjects/Stanford-NER-Python/venv/local/lib/python2.7/site-packages/nltk/internals.py"", line 719, in find_jar
    searchpath, url, verbose, is_regex))
  File ""/home/premchikkus/PycharmProjects/Stanford-NER-Python/venv/local/lib/python2.7/site-packages/nltk/internals.py"", line 635, in find_jar_iter
    (name_pattern, path_to_jar))
LookupError: Could not find stanford-ner.jar jar file at /home/PycharmProjects/Stanford-NER-Python/stanford/stanford-ner-2015-12-09/stanford-ner.jar
</code></pre>

<p>I already set Environment variables in pycharm ide to 
CLASSPATH = /home/PycharmProjects/Stanford-NER-Python/stanford/stanford-ner-2015-12-09/classifiers\english.all.3class.distsim.crf.ser.gz;/home/PycharmProjects/Stanford-NER-Python/stanford/stanford-ner-2015-12-09/stanford-ner.jar</p>

<p>I am using nltk version 3.2.1<br>
python 2.7<br>
ubuntu 16.04<br></p>

<p>Thanks in advance.</p>
","python, pycharm, nltk, stanford-nlp","<p>Nevermind. I got it.
The location for stanford_classifier and stanford_ner_path were wrong and need to pass first two arguments in StandfordNERTagger() no need of ascii.</p>
",0,0,457,2018-03-28 14:54:00,https://stackoverflow.com/questions/49537804/running-stanford-ner-tagger-in-pycharm-is-not-working
Annotating text with NER: Exception: couldn&#39;t read TokensRegexNER,"<p>I'm trying to annotate text with Stanford CoreNLP v3.9.1 in Java.
The annotators used are : tokenize, ssplit, pos, lemma, ner</p>

<p>I've included the model jar from <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a>.
Both english models are included in my project (normal + kbp).</p>

<p>However, after loading the <code>english.muc.7class.distsim.crf.ser.gz</code> classifier, the following exception is thrown: <code>Couldn't read TokensRegexNER from edu/stanford/nlp/models/kbp/regexner_caseless.tab</code>.</p>

<p>After opening the download jar model <code>stanford-english-kbp-corenlp-2018-02-27-models.jar</code>, the correct path to <code>regexner_caseless.tab</code> is <code>edu/stanford/nlp/models/kbp/english/regexner_caseless.tab</code> (notice the <code>english</code> subpath).</p>

<p>How do I make Stanford CoreNLP use the correct path?</p>
",stanford-nlp,"<p>You are missing the main models jar that comes with the distribution.</p>

<p><code>stanford-corenlp-2018-02-27-models.jar</code></p>
",5,4,1451,2018-03-31 01:37:24,https://stackoverflow.com/questions/49583377/annotating-text-with-ner-exception-couldnt-read-tokensregexner
How to get a parse in a bracketed format (without POS tags)?,"<p>I want to parse a sentence to a binary parse of this form (Format used in the <a href=""https://nlp.stanford.edu/projects/snli/"" rel=""nofollow noreferrer"">SNLI</a> corpus):</p>

<p>sentence:""A person on a horse jumps over a broken down airplane.""</p>

<p>parse: ( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )</p>

<p>I'm unable to find a parser which does this.</p>

<p>note: This question has been asked earlier(<a href=""https://stackoverflow.com/questions/44742809/how-to-get-a-binary-parse-in-python"">How to get a binary parse in Python</a>). But the answers are not helpful. And I was unable to comment because I do not have the required reputation.</p>
","python, nlp, stanford-nlp","<p>Here is some sample code which will erase the labels for each node in the tree.</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

import java.util.*;

public class PrintTreeWithoutLabelsExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,parse"");
    // use faster shift reduce parser
    props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    props.setProperty(""parse.maxlen"", ""100"");
    props.setProperty(""parse.binaryTrees"", ""true"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for text
    Annotation annotation = new Annotation(""The red car drove on the highway."");
    // annotate the review
    pipeline.annotate(annotation);
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      Tree sentenceConstituencyParse = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
      for (Tree subTree : sentenceConstituencyParse.subTrees()) {
        if (!subTree.isLeaf())
          subTree.setLabel(CoreLabel.wordFromString(""""));
      }
      TreePrint treePrint = new TreePrint(""oneline"");
      treePrint.printTree(sentenceConstituencyParse);
    }
  }
}
</code></pre>
",0,0,431,2018-04-06 03:54:17,https://stackoverflow.com/questions/49685032/how-to-get-a-parse-in-a-bracketed-format-without-pos-tags
BLEU score for generation task,"<p>after I generate the new text using LSTM or RNN how can I measure the quality of the new text.can i use BLEU score?, but BLEU is used to evaluate the line-to-line generation task which focuses on the semantic relevance between two lines. here I do not have the target to compare between two sentences because I totally got a new text.   </p>
","nlp, stanford-nlp","<p>BLEU score can only be used to evaluate candidate text outputs against one or more reference outputs.</p>

<p>It's not at all clear from your question what kind of text generation task you're attempting. If you're training an RNN language model on monolingual data and sampling sentences from it, you could evaluate it by using your LM to compute perplexity on a test document that you know is well-written.</p>
",0,0,238,2018-04-10 17:34:33,https://stackoverflow.com/questions/49759797/bleu-score-for-generation-task
Could you pleas help me in the following stanford-nlp OpenIE,"<p>I run the same demo example on the website with the following sentence:
""Hudson was born in Hampstead, which is a suburb of London.""</p>

<p>and give me the following,</p>

<p>Hudson  be      bear</p>

<p>and I was expecting the following relations:</p>

<p>(Hudson, was born in, Hampstead)</p>

<p>(Hampstead, is a suburb of, London)</p>

<blockquote>
  <p></p>
</blockquote>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;

import java.util.Collection;
import java.util.Properties;

/** A demo illustrating how to call the OpenIE system programmatically.
 */
public class OpenIEDemo {

  public static void main(String[] args) throws Exception {
    // Create the Stanford CoreNLP pipeline
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    //tokenize,ssplit,pos,lemma,depparse,natlog,openie
    //tokenize,ssplit,pos,lemma,ner,regexner,parse,mention,entitymentions,coref,kbp
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    Annotation doc = new Annotation(args[0]);
    pipeline.annotate(doc);
   // Loop over sentences in the document
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get the OpenIE triples for the sentence
      Collection&lt;RelationTriple&gt; triples =
	          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}</code></pre>
</div>
</div>
</p>

<p>Thank you for your help</p>
","java, nlp, stanford-nlp","<p>So, the system is not wrong, though certainly undergenerating possible relations. <code>Hudson be bear</code> is just asserting that Hudson was born (a true fact). This in particular was caused by the ref edge from Hampstead -ref-> which. This should be fixed in subsequent versions of the code.</p>

<p>In general though, like all NLP systems, OpenIE has a certain accuracy rate that's under 100%, and you should never expect the system to produce completely correct output. Especially for a task like Open IE, where even getting agreement on what ""correct"" means is difficult.</p>
",0,0,181,2018-04-13 16:10:14,https://stackoverflow.com/questions/49821201/could-you-pleas-help-me-in-the-following-stanford-nlp-openie
Anaphora resolution in stanford-nlp using python,"<p>I am trying to do anaphora resolution and for that below is my code.</p>

<p>first i navigate to the folder where i have downloaded the stanford module. Then i run the command in command prompt to initialize stanford nlp module</p>

<pre><code>java -mx4g -cp ""*;stanford-corenlp-full-2017-06-09/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
</code></pre>

<p>After that i execute below code in Python</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')
</code></pre>

<p>I want to change the sentence <code>Tom is a smart boy. He know a lot of thing.</code> into <code>Tom is a smart boy. Tom know a lot of thing.</code> and there is no tutorial or any help available in Python.</p>

<p>All i am able to do is annotate by below code in Python</p>

<p>coreference resolution</p>

<pre><code>output = nlp.annotate(sentence, properties={'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})
</code></pre>

<p>and by parsing for coref </p>

<pre><code>coreferences = output['corefs']
</code></pre>

<p>i get below JSON</p>

<pre><code>coreferences

{u'1': [{u'animacy': u'ANIMATE',
   u'endIndex': 2,
   u'gender': u'MALE',
   u'headIndex': 1,
   u'id': 1,
   u'isRepresentativeMention': True,
   u'number': u'SINGULAR',
   u'position': [1, 1],
   u'sentNum': 1,
   u'startIndex': 1,
   u'text': u'Tom',
   u'type': u'PROPER'},
  {u'animacy': u'ANIMATE',
   u'endIndex': 6,
   u'gender': u'MALE',
   u'headIndex': 5,
   u'id': 2,
   u'isRepresentativeMention': False,
   u'number': u'SINGULAR',
   u'position': [1, 2],
   u'sentNum': 1,
   u'startIndex': 3,
   u'text': u'a smart boy',
   u'type': u'NOMINAL'},
  {u'animacy': u'ANIMATE',
   u'endIndex': 2,
   u'gender': u'MALE',
   u'headIndex': 1,
   u'id': 3,
   u'isRepresentativeMention': False,
   u'number': u'SINGULAR',
   u'position': [2, 1],
   u'sentNum': 2,
   u'startIndex': 1,
   u'text': u'He',
   u'type': u'PRONOMINAL'}],
 u'4': [{u'animacy': u'INANIMATE',
   u'endIndex': 7,
   u'gender': u'NEUTRAL',
   u'headIndex': 4,
   u'id': 4,
   u'isRepresentativeMention': True,
   u'number': u'SINGULAR',
   u'position': [2, 2],
   u'sentNum': 2,
   u'startIndex': 3,
   u'text': u'a lot of thing',
   u'type': u'NOMINAL'}]}
</code></pre>

<p>Any help on this?</p>
","python, nlp, stanford-nlp, linguistics, pycorenlp","<p>Here is one possible solution that uses the data structure output by CoreNLP. All the information is provided. This is not intended as a full solution and extensions are probably required to deal with all situations, but this is a good starting point.</p>

<pre><code>from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')


def resolve(corenlp_output):
    """""" Transfer the word form of the antecedent to its associated pronominal anaphor(s) """"""
    for coref in corenlp_output['corefs']:
        mentions = corenlp_output['corefs'][coref]
        antecedent = mentions[0]  # the antecedent is the first mention in the coreference chain
        for j in range(1, len(mentions)):
            mention = mentions[j]
            if mention['type'] == 'PRONOMINAL':
                # get the attributes of the target mention in the corresponding sentence
                target_sentence = mention['sentNum']
                target_token = mention['startIndex'] - 1
                # transfer the antecedent's word form to the appropriate token in the sentence
                corenlp_output['sentences'][target_sentence - 1]['tokens'][target_token]['word'] = antecedent['text']


def print_resolved(corenlp_output):
    """""" Print the ""resolved"" output """"""
    possessives = ['hers', 'his', 'their', 'theirs']
    for sentence in corenlp_output['sentences']:
        for token in sentence['tokens']:
            output_word = token['word']
            # check lemmas as well as tags for possessive pronouns in case of tagging errors
            if token['lemma'] in possessives or token['pos'] == 'PRP$':
                output_word += ""'s""  # add the possessive morpheme
            output_word += token['after']
            print(output_word, end='')


text = ""Tom and Jane are good friends. They are cool. He knows a lot of things and so does she. His car is red, but "" \
       ""hers is blue. It is older than hers. The big cat ate its dinner.""

output = nlp.annotate(text, properties= {'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})

resolve(output)

print('Original:', text)
print('Resolved: ', end='')
print_resolved(output)
</code></pre>

<p>This gives the following output:</p>

<pre><code>Original: Tom and Jane are good friends. They are cool. He knows a lot of things and so does she. His car is red, but hers is blue. It is older than hers. The big cat ate his dinner.
Resolved: Tom and Jane are good friends. Tom and Jane are cool. Tom knows a lot of things and so does Jane. Tom's car is red, but Jane's is blue. His car is older than Jane's. The big cat ate The big cat's dinner.
</code></pre>

<p>As you can see, this solution doesn't deal with correcting the case when a pronoun has a sentence-initial (title-case) antecedent (""The big cat"" instead of ""the big cat"" in the last sentence). This depends on the category of the antecedent - common noun antecedents need lowercasing, while proper noun antecedents wouldn't.
Some other ad hoc processing might be necessary (as for the possessives in my test sentence). It also presupposes that you will not want to reuse the original output tokens, as they are modified by this code. A way around this would be to make a copy of the original data structure or create a new attribute and change the <code>print_resolved</code> function accordingly.
Correcting any resolution errors is also another challenge!</p>
",6,5,4443,2018-04-24 14:54:54,https://stackoverflow.com/questions/50004797/anaphora-resolution-in-stanford-nlp-using-python
stanford nlp coreference resolution error: Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: File doesn&#39;t exist: example_file.txt,"<p>I downloaded stanfordCoreNLP module version <code>stanford-corenlp-full-2018-02-27</code> from the download page and unzipped the file. created a example_file.txt file in the directory where it was extracted. I added the text <code>My name is Sam. I want to be an astronaut. I had snacks a while ago.</code>. I navigated to the folder it was extracted to and tried to run the example code given for co-reference resolution in the command line</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/coref.html"" rel=""nofollow noreferrer"">stanfordNLP page</a></p>

<pre><code>java -Xmx5g -cp stanford-corenlp-3.9.1.jar:stanford-corenlp-3.9.1-sources.jar:* edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,mention,coref -coref.algorithm neural -file example_file.txt
</code></pre>

<p>I am getting below error message</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: File doesn't exist: example_file.txt
    at edu.stanford.nlp.io.FileSequentialCollection$FileSequentialCollectionIterator.primeNextFile(FileSequentialCollection.java:364)
    at edu.stanford.nlp.io.FileSequentialCollection$FileSequentialCollectionIterator.&lt;init&gt;(FileSequentialCollection.java:269)
    at edu.stanford.nlp.io.FileSequentialCollection.iterator(FileSequentialCollection.java:238)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1166)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1010)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1365)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1427)
</code></pre>

<p>Any help on this?</p>
","java, ubuntu, stanford-nlp","<p>Java lookup resources within the classpath that you defined with <code>-cp</code> option. The directory that contains <code>example_file.txt</code> should probably be included in it</p>

<p><code>-cp "".:stanford-corenlp-3.9.1.jar:stanford-corenlp-3.9.1-sources.jar:*""</code></p>

<p>The dot added to the class path means <code>this directory</code> which apparently contains your file. Also, double quotes prevent the shell to expand the wildcard at the end that it should not be there in my opinion unless it contains jars relevant to the app. At most, it could be <code>*.jar</code>.</p>
",1,0,155,2018-04-27 11:51:00,https://stackoverflow.com/questions/50061815/stanford-nlp-coreference-resolution-error-exception-in-thread-main-java-lang
How can I run command line CoreNLP in multi-threads?,"<p>I need to parse a lot of documents (around 0.3 million).
As suggested in the stanford web, I created a file named filelist.txt which contains paths of all the files to be parsed. </p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/cmdline.html</a></p>

<p>Then I called the CoreNLP as below.</p>

<pre><code>java -mx20g -cp ""$SCRIPT/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse -ssplit.eolonly tokenize.whitespace true -filelist filelist.txt -outputDirectory $OUTDIR
</code></pre>

<p>But the CPU usage is just 100%, which means CoreNLP seems not to use multi-threads. Thus, the parsing is too slow (approximately 10sec per document.).</p>

<p>When I run CoreNLP without -filelist option, it runs as multi-threads.</p>

<p>Is there any options or ways to use multi threads in CoreNLP?</p>
",stanford-nlp,"<p>I believe the command line argument <code>-threads k</code> should annotate a file list on <code>k</code> threads.</p>
",0,0,393,2018-04-28 03:33:18,https://stackoverflow.com/questions/50072568/how-can-i-run-command-line-corenlp-in-multi-threads
Stanford NLP: Retrieving the updated annotation for action in SequenceMatchRules,"<p>I'm using the Sequence Match Rules as part of TokenRegex in the Stanfords CoreNLP library and having some issues retrieving an updated annotation based upon the evaluation of the action in the match rules. </p>

<pre><code>rule = { type: ""CLASS"" , value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" 

{  
   ""ruleType"":""tokens"",
   ""pattern"": ( /This/ /should/ /match/ ) ,
   ""action"": (  Annotate($0, rule, ""RetrieveThisValue"" ) ),
   ""result"":""This is the result""
}
</code></pre>

<p>How do I retrieve the value <code>""RetrieveThisValue""</code> from the annotated coremap. According to the docs <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/types/Expressions.html"" rel=""nofollow noreferrer"">here</a> I would have thought I could retrieve the value from the matched expression CoreMap. When I use something like <code>matchedexpression.get(0).getAnnotation().get(CoreAnnotations.TokensAnnotation.class).toString()</code> I get the results field ""This is the result"" and I do not get the ""RetrieveThisValue"" as well.</p>

<p>I can find the ""RetrieveThisValue"" deep in the extract function of the MatchedExpression. </p>

<p>How do I retrieve ""RetrieveThisValue"" upon matching an expression? </p>
","java, stanford-nlp","<p>Rules file: this-should-match.rules</p>

<pre><code>ruleClass = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$GoldAnswerAnnotation"" }

{
   ""pattern"": ( /This/ /should/ /match/ ),
   ""action"": ( Annotate($0, ruleClass, ""this should match!"") ),
   ""result"": ""This is the result!""
}
</code></pre>

<p>Code:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.util.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class TokensRegexExampleTwo {

  public static void main(String[] args) {

    // set up properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""this-should-match.rules"");
    props.setProperty(""tokensregex.caseInsensitive"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // set up text to annotate
    Annotation annotation = new Annotation(""This should match."");

    // annotate text
    pipeline.annotate(annotation);

    // print out found entities
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" +
            token.get(edu.stanford.nlp.ling.CoreAnnotations.GoldAnswerAnnotation.class));
      }
    }
  }
}
</code></pre>

<p>NOTE: I don't think you should use the GoldAnswer annotation, you should probably make up a brand new annotation class to handle your use case.  But I was just using that as an example.</p>
",1,1,134,2018-05-01 13:17:56,https://stackoverflow.com/questions/50117218/stanford-nlp-retrieving-the-updated-annotation-for-action-in-sequencematchrules
StanfordCoreNLPClient don&#39;t work as expected on sentiment analysis,"<p>Stanford CoreNLP version 3.9.1</p>

<p>I have a problem getting <code>StanfordCoreNLPClient</code> work the same way as <code>StanfordCoreNLP</code> when doing sentiment analysis.</p>

<pre><code>public class Test {

  public static void main(String[] args) {
    String text = ""This server doesn't work!"";

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, sentiment"");

    //If I uncomment this line, and comment out the next one, it works                            
    //StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);

    Annotation annotation = new Annotation(text);
    pipeline.annotate(annotation);
    CoreDocument document = new CoreDocument(annotation);           
    CoreSentence sentence = document.sentences().get(0);

    //outputs null when using StanfordCoreNLPClient
    System.out.println(RNNCoreAnnotations.getPredictions(sentence.sentimentTree())); 

    //throws null pointer when using StanfordCoreNLPClien (reason of course is that it uses the same method I called above, I assume)   
    System.out.println(RNNCoreAnnotations.getPredictionsAsStringList(sentence.sentimentTree()));  

}
</code></pre>

<p>}</p>

<p>Output using <code>StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2)</code>:</p>

<pre><code> null
 Exception in thread ""main"" java.lang.NullPointerException
at edu.stanford.nlp.neural.rnn.RNNCoreAnnotations.getPredictionsAsStringList(RNNCoreAnnotations.java:68)
at tomkri.mastersentimentanalysis.preprocessing.Test.main(Test.java:35)
</code></pre>

<p>Output using <code>StanfordCoreNLP pipeline = new StanfordCoreNLP(props)</code>:</p>

<pre><code>     Type = dense , numRows = 5 , numCols = 1
     0.127  
     0.599  
     0.221  
     0.038  
     0.015  

     [0.12680336652661395, 0.5988695516384742, 0.22125584263055106, 0.03843574738131668, 0.014635491823044227]
</code></pre>

<p>Other annotations than sentiment works in both cases (at least those I have tried).</p>

<p>The server starts fine, and I am able to use from my web browser. When using it there I also get output of sentiment scores (on each subtree in the parse) in json format.</p>
",stanford-nlp,"<p>My solution, in case anyone else need it.</p>

<p>I tried to get the required annotation by making http request to the server with JSON response:</p>

<pre><code>HttpResponse&lt;JsonNode&gt; jsonResponse = Unirest.post(""http://localhost:9000"")
       .queryString(""properties"", ""{\""annotators\"":\""tokenize, ssplit, pos, lemma, ner, parse, sentiment\"",\""outputFormat\"":\""json\""}"")
       .body(text)
       .asJson();

String sentTreeStr = jsonResponse.getBody().getObject().
                getJSONArray(""sentences"").getJSONObject(0).getString(""sentimentTree"");

System.out.println(sentTreeStr); //prints out sentiment values for tree and all sub trees.
</code></pre>

<p>But not all annotation data is available. For example, you don't get the probability distribution over all the possible
sentiment values, only the probability of the sentiment most likely (the sentiment with highest probability).</p>

<p>If you need that, this is a solution:</p>

<pre><code>HttpResponse&lt;InputStream&gt; inStream = Unirest.post(""http://localhost:9000"")
        .queryString(
                ""properties"", 
                ""{\""annotators\"":\""tokenize, ssplit, pos, lemma, ner, parse, sentiment\"",""
                + ""\""outputFormat\"":\""serialized\"",""
                + ""\""serializer\"": \""edu.stanford.nlp.pipeline.GenericAnnotationSerializer\""}""
        )
        .body(text)
        .asBinary();

GenericAnnotationSerializer  serializer = new GenericAnnotationSerializer ();
try{
        ObjectInputStream in = new ObjectInputStream(inStream.getBody());
        Pair&lt;Annotation, InputStream&gt; deserialized = serializer.read(in);
        Annotation annotation = deserialized.first();           

        //And now we are back to a state as if we were not running CoreNLP as server.
        CoreDocument doc = new CoreDocument(annotation);         
        CoreSentence sentence = document.sentences().get(0);
        //Prints out same output as shown in question  
        System.out.println(
            RNNCoreAnnotations.getPredictions(sentence.sentimentTree()));

} catch (UnirestException ex) {
       Logger.getLogger(SentimentTargetExtractor.class.getName()).log(Level.SEVERE, null, ex);
   }    
</code></pre>
",1,0,317,2018-05-01 20:28:04,https://stackoverflow.com/questions/50123357/stanfordcorenlpclient-dont-work-as-expected-on-sentiment-analysis
Stanford NLP Tokens Regex -- doesn&#39;t recognize NER,"<p>I'm just barely getting started with Tokens Regex. I haven't really found an intro or tutorial that gives me what I need. (If I've missed something, links are appreciated!)</p>

<p>The super short, bare-bones idea is that I want to do something like using</p>

<p><code>pattern: ( ( [ { ner:PERSON } ]) /was/ /born/ /on/ ([ { ner:DATE } ]) )</code></p>

<p>(from <a href=""https://nlp.stanford.edu/software/tokensregex.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/tokensregex.html</a>)</p>

<p>to match ""John Smith was born on March 1, 1999"", and then be able to extract ""John Smith"" as the person and ""March 1, 1999"" as the date.</p>

<p>I've cobbled together the following from a couple of web searches. I can get the simple Java regex <code>/John/</code> to match, but nothing I've tried (all copied from web searches for examples, and tweaked a bit) matches when I use an NER.</p>

<p>EDIT for clarity: (Success/failure at the moment is true/false from <code>matcher2.matches()</code> in the code below.)</p>

<p>I don't know if I need to explicitly mention some model or an annotation or something, or if I'm missing something else, or if I'm just approaching it entirely the wrong way.</p>

<p>Any insights are much appreciated! Thanks!</p>

<pre><code>import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.tokensregex.TokenSequenceMatcher;
import edu.stanford.nlp.ling.tokensregex.TokenSequencePattern;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.junit.Test;

public class StanfordSandboxTest {
    private static final Log log = LogFactory.getLog(StanfordSandboxTest.class);

    @Test
    public void testFirstAttempt() {

        Properties props2;
        StanfordCoreNLP pipeline2;
        TokenSequencePattern pattern2;
        Annotation document2;
        List&lt;CoreMap&gt; sentences2;
        TokenSequenceMatcher matcher2;
        String text2;

        props2 = new Properties();
        props2.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner, parse, dcoref"");
        pipeline2 = new StanfordCoreNLP(props2);
        text2 = ""March 1, 1999"";
        pattern2 = TokenSequencePattern.compile(""pattern: (([{ner:DATE}])"");
        document2 = new Annotation(text2);
        pipeline2.annotate(document2);
        sentences2 = document2.get(CoreAnnotations.SentencesAnnotation.class);
        matcher2 = pattern2.getMatcher(sentences2);
        log.info(""testFirstAttempt: Matches2: "" + matcher2.matches());

        props2 = new Properties();
        props2.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner, parse, dcoref"");
        pipeline2 = new StanfordCoreNLP(props2);
        text2 = ""John"";
        pattern2 = TokenSequencePattern.compile(""/John/"");
        document2 = new Annotation(text2);
        pipeline2.annotate(document2);
        sentences2 = document2.get(CoreAnnotations.SentencesAnnotation.class);
        matcher2 = pattern2.getMatcher(sentences2);
        log.info(""testFirstAttempt: Matches2: "" + matcher2.matches());
    }
}
</code></pre>
",stanford-nlp,"<p>Sample code:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.util.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class TokensRegexExampleTwo {

  public static void main(String[] args) {

    // set up properties
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,tokensregex"");
    props.setProperty(""tokensregex.rules"", ""multi-step-per-org.rules"");
    props.setProperty(""tokensregex.caseInsensitive"", ""true"");

    // set up pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // set up text to annotate
    Annotation annotation = new Annotation(""Joe Smith works for Apple Inc."");

    // annotate text
    pipeline.annotate(annotation);

    // print out found entities
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
        System.out.println(token.word() + ""\t"" + token.ner());
      }
    }
  }
}
</code></pre>

<p>sample rules file:</p>

<pre><code>ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }

$ORGANIZATION_TITLES = ""/inc\.|corp\./""

$COMPANY_INDICATOR_WORDS = ""/company|corporation/""

ENV.defaults[""stage""] = 1

{ pattern: (/works/ /for/ ([{pos: NNP}]+ $ORGANIZATION_TITLES)), action: (Annotate($1, ner, ""RULE_FOUND_ORG"") ) }

ENV.defaults[""stage""] = 2

{ pattern: (([{pos: NNP}]+) /works/ /for/ [{ner: ""RULE_FOUND_ORG""}]), action: (Annotate($1, ner, ""RULE_FOUND_PERS"") ) }
</code></pre>

<p>This will apply NER tags to ""Joe Smith"" and ""Apple Inc."".  You can adapt this to your specific case.  Please let me know if you want to do something more advanced than just apply NER tags.  Note: make sure you put those rules in a file called: ""multi-step-per-org.rules"".</p>
",1,0,657,2018-05-02 21:17:10,https://stackoverflow.com/questions/50143681/stanford-nlp-tokens-regex-doesnt-recognize-ner
Sentiment analysis using spark and Stanford NLP API,"<p>When I wanted to do a sentiment analysis project I searched alot online, and atlast I landed on this website, which explained the code but what it did not explain is how to use spark with respect to the code, I mean where to add the code. 
Website :<a href=""http://stdatalabs.blogspot.in/2017/09/twitter-sentiment-analysis-using-spark.html?m=1"" rel=""nofollow noreferrer"">http://stdatalabs.blogspot.in/2017/09/twitter-sentiment-analysis-using-spark.html?m=1</a></p>

<p>It will be of great help, if anyone can explain me completely, as Iam a begginer and this my first project on big data.
Thank you.</p>
","apache-spark, bigdata, stanford-nlp, sentiment-analysis","<p>In the bottom there is a link to the github (<a href=""https://github.com/stdatalabs/sparkNLP-elasticsearch"" rel=""nofollow noreferrer"">https://github.com/stdatalabs/sparkNLP-elasticsearch</a>) you should check that out (literally)</p>

<p>The main class is 
<code>com.stdatalabs.SparkES.TwitterSentimentAnalysis</code> according to the pom.xml </p>

<p>So running <code>mvn package</code> will yield you an executable .jar (user <code>java -jar</code>)</p>

<p>Running the jar will prompt you for some twitter config (keys, etc) and saves to a local es cluster using hardcoded index (&amp; mapping) <code>twitter_020717/tweet</code></p>

<p>You can now alter the code anyway you want, build, run, and check the results.</p>
",2,2,328,2018-05-03 20:43:27,https://stackoverflow.com/questions/50163569/sentiment-analysis-using-spark-and-stanford-nlp-api
How to predict probability of a sentence?,"<p>How to determine the probability of a sentence ""what is a cat"" ? with associated PCFG :</p>

<pre><code>Rule , Probability
S -&gt; NP VB
NN -&gt; CAT , 1
DT -&gt; what , 1
VB-&gt;is , .5
VB-&gt;be , .5
</code></pre>

<p>How can this pcfg with sentence be represented as hidden markov model ?</p>

<p>Each node in the model is ""what"" , ""is"" , ""a"" , ""cat"" ? , how to model the probability connections between the nodes from PCFG ?</p>
","nlp, stanford-nlp, probability, hidden-markov-models, markov-models","<p>A PCFG defines (i) a distribution over parse trees and (ii) a distribution over sentences.</p>

<p>The probability of a <strong>parse tree</strong> given by a PCFG is:</p>

<p><img src=""https://i.sstatic.net/RLRws.gif"" alt=""""></p>

<p>where the parse tree <em>t</em> is described as a multiset of rules <em>r</em> (it is a multiset because a rule can be used several times to derive a tree).</p>

<p>To compute the probability of a <strong>sentence</strong>, you have to consider every possible way of deriving the sentence and sum over their probabilities.</p>

<p><img src=""https://i.sstatic.net/1kyWK.gif"" alt=""""></p>

<p>where <img src=""https://i.sstatic.net/sc8W8.gif"" alt="""">  means that the string of terminals (the yield) of <em>t</em> is the sentence <em>s</em>.</p>

<p>In your example, the probability of ""what is a cat"" is 0 because you cannot generate it with your grammar. Here's another example with a toy grammar:</p>

<pre><code>Rule            Probability
S -&gt; NP VP      1
NP -&gt; they      0.5
NP -&gt; fish      0.5
VP -&gt; VBP NP    0.5
VP -&gt; MD VB     0.5
VBP -&gt; can      1
MD -&gt; can       1
VB -&gt; fish      1
</code></pre>

<p>The sentence ""they can fish"" has two possible parse trees:</p>

<pre><code>(S (NP they) (VP (MD can) (VB fish)))
(S (NP they) (VP (VBP can) (NP fish)))
</code></pre>

<p>with probabilities:</p>

<pre><code>S   NP    VP     MD   VB
1 * 0.5 * 0.5 *  1  * 1 = 1 / 4
</code></pre>

<p>and </p>

<pre><code>S   NP    VP     VBP   NP
1 * 0.5 * 0.5 *  1  *  0.5  = 1 /8
</code></pre>

<p>so its probability is the sum or probabilities of both parse trees (3/8).</p>

<p>It turns out that in the general case, it is too computationally expensive to enumerate each possible parse tree for a sentence.
However, there is a O(n^3) algorithm to compute the sum efficiently: the <a href=""http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf"" rel=""nofollow noreferrer"">inside-outside algorithm</a> (see pages 17-18), pdf by Michael Collins.</p>

<p>Edit: corrected trees.</p>
",4,4,1816,2018-05-04 09:22:57,https://stackoverflow.com/questions/50171512/how-to-predict-probability-of-a-sentence
Extracting age related information from using nlp,"<p>I am new to NLP and I have been trying to extract age related information from raw text. I googled and didn't get any reliable library in any language for this requirement. It would be great if I can get any help in this. I am open to any language and it is not a constraint. It can be in Java, Python or any other language too. Any help would be much appreciated. Thanks in advance. Cheers!</p>

<p>Update:</p>

<p>I tried adding the annotators, mentioned by Stanford help, to my java parser and I am facing below exception :</p>

<pre><code>    ERROR: cannot create CorefAnnotator!
    java.lang.RuntimeException: Error creating coreference system
    at 

 edu.stanford.nlp.scoref.StatisticalCorefSystem.fromProps(StatisticalCorefSystem.java:58)
    at edu.stanford.nlp.pipeline.CorefAnnotator.&lt;init&gt;(CorefAnnotator.java:66)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.coref(AnnotatorImplementations.java:220)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$13.create(AnnotatorFactories.java:515)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:375)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:139)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:135)
    at com.dateparser.SUtime.SUAgeParser.makeNumericPipeline(SUAgeParser.java:85)
    at com.dateparser.SUtime.SUAgeParser.&lt;clinit&gt;(SUAgeParser.java:60)
Caused by: java.lang.RuntimeException: Error initializing coref system
    at edu.stanford.nlp.scoref.StatisticalCorefSystem.&lt;init&gt;(StatisticalCorefSystem.java:36)
    at edu.stanford.nlp.scoref.ClusteringCorefSystem.&lt;init&gt;(ClusteringCorefSystem.java:24)
    at edu.stanford.nlp.scoref.StatisticalCorefSystem.fromProps(StatisticalCorefSystem.java:48)
    ... 9 more
Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/hcoref/md-model.ser"" as class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:485)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:323)
    at edu.stanford.nlp.hcoref.md.DependencyCorefMentionFinder.&lt;init&gt;(DependencyCorefMentionFinder.java:38)
    at edu.stanford.nlp.hcoref.CorefDocMaker.getMentionFinder(CorefDocMaker.java:149)
    at edu.stanford.nlp.hcoref.CorefDocMaker.&lt;init&gt;(CorefDocMaker.java:61)
    at edu.stanford.nlp.scoref.StatisticalCorefSystem.&lt;init&gt;(StatisticalCorefSystem.java:34)
    ... 11 more
</code></pre>

<p>I upgraded to version 1.6.0 and also added stanford-corenlp-models-current.jar to the classpath. Please let me know if I am missing something</p>

<p>Update 1:</p>

<p>The exception was fixed after upgrading to 3.9.1. But I am getting the ouput as per:duration relation instead of per:age</p>

<pre><code>private static AnnotationPipeline makePipeline() {

    Properties props = new Properties();
    props.setProperty(""annotators"",
             ""tokenize,ssplit,pos,lemma,ner,depparse,coref,kbp"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    return pipeline;


}

public static void parse(String str) {
    try {
        Annotation doc = new Annotation(str);
        pipeline.annotate(doc);
        ArrayList&lt;CoreMap&gt; resultRelations = new ArrayList&lt;CoreMap&gt;();
        List&lt;CoreMap&gt; mentionsAnnotations = doc.get(MentionsAnnotation.class);
        for (CoreMap currentCoreMap : mentionsAnnotations) {
            System.out.println(currentCoreMap.get(TextAnnotation.class));
            System.out.println(currentCoreMap.get(CharacterOffsetBeginAnnotation.class));
            System.out.println(currentCoreMap.get(CharacterOffsetEndAnnotation.class));
            System.out.println(currentCoreMap.get(NamedEntityTagAnnotation.class));
        }
    } catch (Exception e) {

    }
}
</code></pre>

<p>Is this normal behaviour or am I doing something wrong?</p>
","nlp, nltk, stanford-nlp, opennlp","<p>You may find the KBP relation extractor useful.</p>

<p>Example text:</p>

<pre><code>Joe Smith is 58 years old.
</code></pre>

<p>Command:</p>

<pre><code>java -Xmx12g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,kbp -file example.txt -outputFormat text
</code></pre>

<p>This should attach <code>Joe Smith</code> to <code>58 years old</code> with the <code>per:age</code> relation.</p>
",0,0,367,2018-05-07 08:55:47,https://stackoverflow.com/questions/50210424/extracting-age-related-information-from-using-nlp
Stanford NLP : Corpus of coreference resolution,"<p>I was simply wondering on which corpus was trained the english statistical coreference resolution system of Stanford NLP. Would it be effective if used on novels ?</p>
","stanford-nlp, corpus","<p>The coreference model is trained on the CoNLL 2012 coreference data set, which is related to the OntoNotes 5.0 data set.</p>

<p>Here is the link to the data:</p>

<p><a href=""http://conll.cemantix.org/2012/data.html"" rel=""nofollow noreferrer"">http://conll.cemantix.org/2012/data.html</a></p>
",0,-1,161,2018-05-07 14:37:22,https://stackoverflow.com/questions/50216789/stanford-nlp-corpus-of-coreference-resolution
how to deploy stanford coreNLP server to an online webservice e.g. Heroku?,"<p>I would like to deploy the Stanford CoreNLP server so that I can access it as an API from another app of mine. This is probably rather trivial but there are not many resources regarding this around. Being a rather new developer, and not one experienced in Java, could someone point me to how I could get started with deploying it?</p>
","java, heroku, stanford-nlp","<p>Perhaps easiest is to use a pre-built Docker image, e.g., as defined in <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#docker"" rel=""nofollow noreferrer"">the docker section of the server documentation</a>:</p>

<pre><code>docker run -p 9000:9000 --name coreNLP --rm -i -t motiz88/corenlp
</code></pre>

<p>With the docker image at <a href=""https://hub.docker.com/r/motiz88/corenlp/"" rel=""nofollow noreferrer"">https://hub.docker.com/r/motiz88/corenlp/</a></p>
",1,0,311,2018-05-12 09:09:36,https://stackoverflow.com/questions/50304590/how-to-deploy-stanford-corenlp-server-to-an-online-webservice-e-g-heroku
Python - Extract relation of entities (noun phrases) from unstructured-based text (NLP) using NLTK,"<p>From a unstructured text, I have extracted all necessary entities and stored it in a dictionary using stanford POS tagger. Now I want to extract the relation between them to build my own Ontology in the form of triplets (Entity1,Entity2,relation). I tried the stanford dependencies parser, but I don't know how to extract these triplets. </p>

<p>For example:
The front diffusers comprise pivotable flaps that are arranged between boundary walls of air ducts.</p>

<p>I want to have the relation (front diffusers, pivotable flaps, comprise); (pivotable flaps, boundary walls of air ducts, arrange);</p>

<p>Another example: The cargo body comprises a container having a floor, a top wall, a front wall, side walls and a rear door.</p>

<p>My expected relations are (cargo body, container, comprise); (container, floor, have); (container,top wall, have); (container, front wall, have); (container, side walls, have); (container, rear door, have). </p>

<p>What can I do with the stanford dependencies parser to achieve my goal? This means how to navigate the dependencies parse tree and get the results?</p>
","python, nlp, stanford-nlp, information-extraction","<p>You are on correct path with using dependency parsers. You just need to dig in little deeper to extract the structure you are looking for. From what I can see, the dependency parser has all the information that you are looking for:</p>

<pre><code> (ROOT
  (S
    (NP (DT The) (JJ front) (NNS diffusers))
    (VP (VBP comprise)
      (NP
        (NP (JJ pivotable) (NNS flaps))
        (SBAR
          (WHNP (WDT that))
          (S
            (VP (VBP are)
              (VP (VBN arranged)
                (PP (IN between)
                  (NP
                    (NP (NN boundary) (NNS walls))
                    (PP (IN of)
                      (NP (NN air) (NNS ducts)))))))))))
    (. .)))
</code></pre>

<p>Here is what you actually need right from the parser itself:</p>

<pre><code>nsubj(comprise-4, diffusers-3)
root(ROOT-0, comprise-4)
amod(flaps-6, pivotable-5)
dobj(comprise-4, flaps-6)
</code></pre>

<p>Just study different sentences and you will be able to extract the info in whichever format you wish to get it.</p>
",1,1,805,2018-05-17 18:00:27,https://stackoverflow.com/questions/50397837/python-extract-relation-of-entities-noun-phrases-from-unstructured-based-tex
What&#39;s the tags meaning of Stanford dependency parser(3.9.1)?,"<p>I used the Stanford dependency parser(3.9.1) to parse a sentence, and I got the result as the following. </p>

<pre><code>    [[(('investigating', 'VBG'), 'nmod', ('years', 'NNS')),
  (('years', 'NNS'), 'case', ('In', 'IN')),
  (('years', 'NNS'), 'det', ('the', 'DT')),
  (('years', 'NNS'), 'amod', ('last', 'JJ')),
  (('years', 'NNS'), 'nmod', ('century', 'NN')),
  (('century', 'NN'), 'case', ('of', 'IN')),
  (('century', 'NN'), 'det', ('the', 'DT')),
  (('century', 'NN'), 'amod', ('nineteenth', 'JJ')),
  (('investigating', 'VBG'), 'nsubj', ('Planck', 'NNP')),
  (('investigating', 'VBG'), 'aux', ('was', 'VBD')),
  (('investigating', 'VBG'), 'dobj', ('problem', 'NN')),
  (('problem', 'NN'), 'det', ('the', 'DT')),
  (('problem', 'NN'), 'nmod', ('radiation', 'NN')),
  (('radiation', 'NN'), 'case', ('of', 'IN')),
  (('radiation', 'NN'), 'amod', ('black-body', 'JJ')),
  (('radiation', 'NN'), 'acl', ('posed', 'VBN')),
  (('posed', 'VBN'), 'advmod', ('first', 'RB')),
  (('posed', 'VBN'), 'nmod', ('Kirchhoff', 'NNP')),
  (('Kirchhoff', 'NNP'), 'case', ('by', 'IN')),
  (('Kirchhoff', 'NNP'), 'advmod', ('earlier', 'RBR')),
  (('earlier', 'RBR'), 'nmod:npmod', ('years', 'NNS')),
  (('years', 'NNS'), 'det', ('some', 'DT')),
  (('years', 'NNS'), 'amod', ('forty', 'JJ'))]]
</code></pre>

<p>Some of the tags meaning such as 'nmod' and 'acl' are missing in the <a href=""https://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow noreferrer"">StanfordDependencyManual</a>.The newest manual version I can find is 3.7.0. I also find some explanation at a <a href=""https://wiki.opencog.org/w/Dependency_relations#Standard_list_of_dependency_relations"" rel=""nofollow noreferrer"">Standard_list_of_dependency_relations</a>
But it still missed some tags. </p>

<p>Hence, my question is where can I find the newest version of the explanation of these tags? Thanks!</p>
","nltk, stanford-nlp","<p>For the last few versions, the Stanford parser has been generating <a href=""http://universaldependencies.org/"" rel=""noreferrer"">Universal Dependencies</a> rather than Stanford Dependencies. The new relation set can be found <a href=""http://universaldependencies.org/docsv1/en/dep/index.html"" rel=""noreferrer"">here</a>, and are listed below (for version 1 -- version 2 seems to be a work-in-progress still?):</p>

<pre><code>acl: clausal modifier of noun
acl:relcl: relative clause modifier
advcl: adverbial clause modifier
advmod: adverbial modifier
amod: adjectival modifier
appos: appositional modifier
aux: auxiliary
auxpass: passive auxiliary
case: case marking
cc: coordination
cc:preconj: preconjunct
ccomp: clausal complement
compound: compound
compound:prt: phrasal verb particle
conj: conjunct
cop: copula
csubj: clausal subject
csubjpass: clausal passive subject
dep: dependent
det: determiner
det:predet: predeterminer
discourse: discourse element
dislocated: dislocated elements
dobj: direct object
expl: expletive
foreign: foreign words
goeswith: goes with
iobj: indirect object
list: list
mark: marker
mwe: multi-word expression
name: name
neg: negation modifier
nmod: nominal modifier
nmod:npmod: noun phrase as adverbial modifier
nmod:poss: possessive nominal modifier
nmod:tmod: temporal modifier
nsubj: nominal subject
nsubjpass: passive nominal subject
nummod: numeric modifier
parataxis: parataxis
punct: punctuation
remnant: remnant in ellipsis
reparandum: overridden disfluency
root: root
vocative: vocative
xcomp: open clausal complement
</code></pre>

<p>Although no longer maintained, you can get the old dependency format by setting the property <code>depparse.language</code> to <code>English</code> (see, e.g., <a href=""https://stackoverflow.com/questions/45202486/corenlp-stanford-dependency-format"">here</a>):</p>

<pre><code>properties.setProperty(""depparse.language"", ""English"")
</code></pre>
",8,2,2016,2018-05-20 02:29:38,https://stackoverflow.com/questions/50431155/whats-the-tags-meaning-of-stanford-dependency-parser3-9-1
how to map sentences to user declared methods based on the meaning of that sentence?,"<p>I have to map user defined sentences to particular methods.
for example, if there is sentence ""Tune to 87.5 FM station"" and i have a method ""Tune(MediaDevices mediaDevice, double value)"". I need to identify through the sentence that this method is to be called and parameters should be FM for mediaDevice and 87.5 for value. </p>

<p>I have tried using NLP dependency parser to identify the Action i.e ""Tune"" in this case and the values,i.e ""FM"" and ""87.5"". 
But how to map these action and value to this Method. Also there will be many methods related to tuner, so which method needs to be called is an issue.</p>
","c#, reflection, stanford-nlp","<pre><code>Type classType = Type.GetType(""ClassName""); //In case method is in any other class.
ConstructorInfo classConstructor = classType.GetConstructor(Type.EmptyTypes);
object classObject = classConstructor.Invoke(new object[]{});
MethodInfo tune= magicType.GetMethod(""Tune"");
tune.Invoke(classObject , new object[]{arg1,arg2});
</code></pre>

<p>Include reflection.
Hope it helps.</p>
",0,-1,56,2018-05-22 10:48:51,https://stackoverflow.com/questions/50465814/how-to-map-sentences-to-user-declared-methods-based-on-the-meaning-of-that-sente
"With the Simple API in Stanford CoreNLP, is there a way to get multi-token entity mentions?","<p>This question is very similar to <a href=""https://stackoverflow.com/questions/48632256/stanford-nlp-3-9-0-does-using-coreentitymention-combine-adjacent-entity-mention"">my question</a>, however due to the way SO works, I think it is better to ask a new question rather than just continue a thread.</p>

<p>CoreNLP has the <a href=""https://stanfordnlp.github.io/CoreNLP/simple.html"" rel=""nofollow noreferrer"">Simple API</a> which allows for quicker access to various components of the NLP pipeline.  The way to get named entities appears to be:</p>

<ol>
<li>Form a document annotation from the text</li>
<li>Get the sentences from the document object</li>
<li>Use <code>nerTags()</code> from the sentences object to get the token-by-token ner labeling.</li>
</ol>

<p>Via other mechanisms, as talked about in the question link above, one can retrieve full multi-token entity mentions such as George Washington, which is an entity mention composed of 2 tokens.  Is there a way using the simple api to get these multi-token entity mentions?</p>
",stanford-nlp,"<p>Yes, though it gives you less information than the full API, returning only the String spans of the mention. See <a href=""https://github.com/stanfordnlp/CoreNLP/blob/91d520a46042d26ed91bff4562056f7584829984/src/edu/stanford/nlp/simple/Sentence.java#L567"" rel=""nofollow noreferrer""><code>Sentence#mentions(String)</code></a> and <a href=""https://github.com/stanfordnlp/CoreNLP/blob/91d520a46042d26ed91bff4562056f7584829984/src/edu/stanford/nlp/simple/Sentence.java#L598"" rel=""nofollow noreferrer""><code>Sentence#mentions()</code></a>.</p>

<p>If you want to get more information about a mention, you'll have to either use the regular API, or re-implement the logic in these functions. You can also try mucking around in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/91d520a46042d26ed91bff4562056f7584829984/src/edu/stanford/nlp/simple/Sentence.java#L307"" rel=""nofollow noreferrer"">the raw Proto</a>, which will certainly have all the information you could possibly want, but in a less-than-pleasant proto interface. The proto definition is <a href=""https://github.com/stanfordnlp/CoreNLP/blob/91d520a46042d26ed91bff4562056f7584829984/src/edu/stanford/nlp/pipeline/CoreNLP.proto"" rel=""nofollow noreferrer"">here</a>.</p>
",1,0,138,2018-05-25 12:52:40,https://stackoverflow.com/questions/50529708/with-the-simple-api-in-stanford-corenlp-is-there-a-way-to-get-multi-token-entit
Stanford NER Tagger and NLTK - not working [OSError: Java command failed ],"<p>Trying to run Stanford NER Taggerand NLTK from a jupyter notebook. 
I am continuously getting </p>

<pre><code>OSError: Java command failed
</code></pre>

<p>I have already tried the hack at 
    <a href=""https://gist.github.com/alvations/e1df0ba227e542955a8a"" rel=""nofollow noreferrer"">https://gist.github.com/alvations/e1df0ba227e542955a8a</a>
and thread 
    <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk/34112695#34112695"">Stanford Parser and NLTK</a> </p>

<p>I am using </p>

<pre><code>NLTK==3.3
Ubuntu==16.04LTS 
</code></pre>

<p>Here is my python code:</p>

<pre><code>Sample_text = ""Google, headquartered in Mountain View, unveiled the new Android phone""

sentences = sent_tokenize(Sample_text)
tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]

PATH_TO_GZ = '/home/root/english.all.3class.caseless.distsim.crf.ser.gz'

PATH_TO_JAR = '/home/root/stanford-ner.jar'

sn_3class = StanfordNERTagger(PATH_TO_GZ,
                       path_to_jar=PATH_TO_JAR,
                              encoding='utf-8')

annotations = [sn_3class.tag(sent) for sent in tokenized_sentences]
</code></pre>

<p>I got these files using following commands:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip
wget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip
wget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip
# Extract the zip file.
unzip stanford-ner-2015-04-20.zip 
unzip stanford-parser-full-2015-04-20.zip 
unzip stanford-postagger-full-2015-04-20.zip
</code></pre>

<p>I am getting the following error:</p>

<pre><code>CRFClassifier invoked on Thu May 31 15:56:19 IST 2018 with arguments:
   -loadClassifier /home/root/english.all.3class.caseless.distsim.crf.ser.gz -textFile /tmp/tmpMDEpL3 -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions ""tokenizeNLs=false"" -encoding utf-8
tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer
Unknown property: |tokenizerFactory|
tokenizerOptions=""tokenizeNLs=false""
Unknown property: |tokenizerOptions|
loadClassifier=/home/root/english.all.3class.caseless.distsim.crf.ser.gz
encoding=utf-8
Unknown property: |encoding|
textFile=/tmp/tmpMDEpL3
outputFormat=slashTags
Loading classifier from /home/root/english.all.3class.caseless.distsim.crf.ser.gz ... Error deserializing /home/root/english.all.3class.caseless.distsim.crf.ser.gz
Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassCastException: java.util.ArrayList cannot be cast to [Ledu.stanford.nlp.util.Index;
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1380)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1331)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2315)
Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to [Ledu.stanford.nlp.util.Index;
    at edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2164)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1249)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1366)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1377)
    ... 2 more

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-15-5621d0f8177d&gt; in &lt;module&gt;()
----&gt; 1 ne_annot_sent_3c = [sn_3class.tag(sent) for sent in tokenized_sentences]

/home/root1/.virtualenv/demos/local/lib/python2.7/site-packages/nltk/tag/stanford.pyc in tag(self, tokens)
     79     def tag(self, tokens):
     80         # This function should return list of tuple rather than list of list
---&gt; 81         return sum(self.tag_sents([tokens]), [])
     82 
     83     def tag_sents(self, sentences):

/home/root1/.virtualenv/demos/local/lib/python2.7/site-packages/nltk/tag/stanford.pyc in tag_sents(self, sentences)
    102         # Run the tagger and get the output
    103         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,
--&gt; 104                                        stdout=PIPE, stderr=PIPE)
    105         stanpos_output = stanpos_output.decode(encoding)
    106 

/home/root1/.virtualenv/demos/local/lib/python2.7/site-packages/nltk/__init__.pyc in java(cmd, classpath, stdin, stdout, stderr, blocking)
    134     if p.returncode != 0:
    135         print(_decode_stdoutdata(stderr))
--&gt; 136         raise OSError('Java command failed : ' + str(cmd))
    137 
    138     return (stdout, stderr)

OSError: Java command failed : [u'/usr/bin/java', '-mx1000m', '-cp', '/home/root/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/home/root/english.all.3class.caseless.distsim.crf.ser.gz', '-textFile', '/tmp/tmpMDEpL3', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf-8']
</code></pre>
","nltk, stanford-nlp, named-entity-recognition","<p>Download Stanford <a href=""http://Download%20Stanford%20Named%20Entity%20Recognizer%20version%203.9.1"" rel=""nofollow noreferrer"">Named Entity Recognizer version 3.9.1</a>: see ‘Download’ section from <a href=""https://nlp.stanford.edu/software/CRF-NER.shtml#Download"" rel=""nofollow noreferrer"">The Stanford NLP website</a>.</p>

<p>Unzip it and move 2 files ""ner-tagger.jar"" and ""english.all.3class.distsim.crf.ser.gz"" to your folder</p>

<p>Open jupyter notebook or ipython prompt in your folder path and run the following python code:</p>

<pre><code>import nltk
from nltk.tag.stanford import StanfordNERTagger

sentence = u""Twenty miles east of Reno, Nev., "" \
    ""where packs of wild mustangs roam free through "" \
    ""the parched landscape, Tesla Gigafactory 1 "" \
    ""sprawls near Interstate 80.""

jar = './stanford-ner.jar'

model = './english.all.3class.distsim.crf.ser.gz'

ner_tagger = StanfordNERTagger(model, jar, encoding='utf8')

words = nltk.word_tokenize(sentence)

# Run NER tagger on words
print(ner_tagger.tag(words))
</code></pre>

<p>I tested this on NLTK==3.3 and Ubuntu==16.0.6LTS</p>
",1,1,1898,2018-05-31 11:07:28,https://stackoverflow.com/questions/50622897/stanford-ner-tagger-and-nltk-not-working-oserror-java-command-failed
NER is over writing the custom NERin stanford NLP,"<p>In the stanford nlp, I used a pattern to match the phone number in regexner. But the NER is over writing it as Number.</p>

<p>If I remove the ner annotation then it is showing as PHONE_NUMBER.
Can any one of you please help me. </p>

<p>Thanks in Advance.</p>

<p>Here is my regexner line:</p>

<pre><code>^(?:(?:\+|0{0,2})91(\s*[\-]\s*)?|[0]?)?[789]\d{9}$  PHONENUMBER
</code></pre>
",stanford-nlp,"<p>java command:</p>

<pre><code>java -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -file phone-number-example.txt -outputFormat text -ner.fine.regexner.mapping phone-number-regex.rules
</code></pre>

<p>example text:</p>

<pre><code>I will call him at 555-555-5555
</code></pre>

<p>format of rules file:</p>

<pre><code>555-555-5555    PHONE_NUMBER    NUMBER  1
</code></pre>

<p>(note the columns are tab delimited)</p>

<p>The fine-grained NER will be applied after the statistical NER.  You can also build a custom <code>regexner</code> and run it after the statistical model.  The key is telling it to overwrite the NUMBER tag (which is indicated in the third column).</p>
",0,0,102,2018-06-04 10:25:14,https://stackoverflow.com/questions/50678418/ner-is-over-writing-the-custom-nerin-stanford-nlp
Add domain-specific entities to spaCy or Stanford NLP training set,"<p>We would like to add some custom entities to the training set of either Stanford NLP or spaCy, before re-training the model. We are willing to label our custom entities, but we would like to add these to the existing training set, so as to not spend too much time labeling.</p>

<p>We assume that the NLP model was trained on a large labeled data set, which includes labels for words that are labeled ""O"" (""other"", i.e. nothing of interest) as well as words that are labeled ""DATE"", ""PERSON"", ""ORGANIZATION"", etc. We have a custom set of ORGANIZATION words, but we would like to add these to all the other labeled data, before re-training the model.</p>

<p>Is this possible? How can we do this? Do we have to get the labeled dataset that the models were trained on, so we can add our own data? If so, how can we do that?</p>

<p>We have built prototypes using both Stanford NLP and spaCy, so an answer for either one works for us.</p>
","stanford-nlp, spacy","<p>For spaCy, you should just be able to call <code>nlp.update()</code>. This will make a weight update against the current weights, allowing you to resume training. If you want to make many updates, you might want to parse some text with the original model and mix that through your training, to avoid the ""catastrophic forgetting"" problem.</p>
",2,0,447,2018-06-04 17:36:19,https://stackoverflow.com/questions/50685950/add-domain-specific-entities-to-spacy-or-stanford-nlp-training-set
Get token string from tokenID using Stanford Parser in GATE,"<p>I am trying to use some Java RHS to get the string value of dependent tokens using Stanford dependency parser in GATE, and add them as features of a new annotation. </p>

<p>I am having problems targeting just the 'dependencies' feature of the token, and getting the string value from the tokenID.</p>

<p>Using below specifying only 'depdencies' also throws a java null pointer error: </p>

<pre><code>for(Annotation lookupAnn : tokens.inDocumentOrder())
  {
   FeatureMap lookupFeatures  = lookupAnn.getFeatures();
   token = lookupFeatures.get(""dependencies"").toString();  
  }
</code></pre>

<p>I can use below to get all the features of a token,</p>

<pre><code>gate.Utils.inDocumentOrder
</code></pre>

<p>but it returns all features, including the dependent tokenID's; i.e:</p>

<pre><code>dependencies = [nsubj(8390), dobj(8394)]
</code></pre>

<p>I would like to get just the dependent token's string value from these tokenID's.</p>

<p>Is there any way to access dependent token string value and add them as a feature to the annotation?</p>

<p>Many thanks for your help</p>
","java, stanford-nlp, gate","<p>Here is a working JAPE example. It only printns to the GATE's message window (std out), It doesn't create any new annotations with features you asked for. Please finish it yourself...</p>

<p><code>Stanford_CoreNLP</code> plugin has to be loaded in GATE to make this JAPE file loadable. Otherwise you will get class not found exception for <code>DependencyRelation</code> class. </p>

<pre><code>Imports: {
  import gate.stanford.DependencyRelation;
}

Phase: GetTokenDepsPhase
Input: Token
Options: control = all
Rule: GetTokenDepsRule
(
  {Token}
): token
--&gt; 
:token {
  //note that tokenAnnots contains only a single annotation so the loop could be avoided...
  for (Annotation token : tokenAnnots) {
    Object deps = token.getFeatures().get(""dependencies"");

    //sometimes the dependencies feature is missing - skip it
    if (deps == null) continue;

    //token.getFeatures().get(""string"") could be used instead of gate.Utils.stringFor(doc,token)...
    System.out.println(""Dependencies for token "" + gate.Utils.stringFor(doc, token));

    //the dependencies feature has to be typed to List&lt;DependencyRelation&gt;
    List&lt;DependencyRelation&gt; typedDeps = (List&lt;DependencyRelation&gt;) deps;
    for (DependencyRelation r : typedDeps) {

      //use DependencyRelation.getTargetId() to get the id of the target token
      //use inputAS.get(id) to get the annotation for its id
      Annotation targetToken = inputAS.get(r.getTargetId());

      //use DependencyRelation.getType() to get the dependency type
      System.out.println(""  "" +r.getType()+ "": "" +gate.Utils.stringFor(doc, targetToken));
    }
  }
}
</code></pre>
",2,1,248,2018-06-05 07:51:08,https://stackoverflow.com/questions/50694878/get-token-string-from-tokenid-using-stanford-parser-in-gate
How to get dependency information about a word?,"<p>I have already successfully parsed sentences to get dependency information using stanford parser (version 3.9.1(run it in IDE Eclipse）) with command ""TypedDependencies"", but how could I get depnedency information about a single word( it's parent, siblings and children)?  I have searched javadoc, it seems Class semanticGraph is used to do this job, but it need a IndexedWord type as input, how do I get IndexedWord? Do you have any simple samples?</p>
","nlp, stanford-nlp, text-processing","<p>You can create a <code>SemanticGraph</code> from a <code>List</code> of <code>TypedDependencies</code> and then you can use the methods <code>getChildren(IndexedWord iw)</code>, <code>getParent(IndexedWord iw)</code>, and <code>getSiblings(IndexedWord iw)</code>. (See the javadoc of <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html"" rel=""nofollow noreferrer""><code>SemanticGraph</code></a>).</p>

<p>To get the <code>IndexedWord</code> of a specific word, you can, for example, use the <code>SemanticGraph</code> method <code>getNodeByIndex(int i)</code>, which will return the <code>IndexNode</code> of the i-th token in a sentence.</p>
",0,0,103,2018-06-06 07:47:35,https://stackoverflow.com/questions/50714922/how-to-get-dependency-information-about-a-word
How to cluster Named Entity using StanfordNER using python,"<p>Stanford NER provides it NER jars to detect POS tags and NERs. But I am facing one issue with one of the sentences when trying to parse. The sentence is as follows:</p>

<pre><code>Joseph E. Seagram &amp; Sons, INC said on Thursday that it is merging its two United States based wine companies
</code></pre>

<p>Below is my code</p>

<pre><code>st = StanfordNERTagger('./stanford- ner/classifiers/english.all.3class.distsim.crf.ser.gz',
                       './stanford-ner/stanford-ner.jar',
                       encoding='utf-8')
ne_in_sent = []
with open(""./CCAT/2551newsML.txt"") as fd:
    lines = fd.readlines()
    for line in lines:
        print(line)
        tokenized_text = word_tokenize(line)
        classified_text = st.tag(tokenized_text)
        ne_tree = stanfordNE2tree(classified_text)
        for subtree in ne_tree:
            # If subtree is a noun chunk, i.e. NE != ""O""
            if type(subtree) == Tree:
                ne_label = subtree.label()
                ne_string = "" "".join([token for token, pos in subtree.leaves()])
                ne_in_sent.append((ne_string, ne_label))
                print(ne_in_sent)
</code></pre>

<p>when I parse it I get the following entities as the organization.
(Joseph E. Seagram &amp; Sons, Organization) and (Inc, Organization)</p>

<p>Also for some other texts in the file like</p>

<pre><code>TransCo has a very big plane. Transco is moving south.
</code></pre>

<p>It differentiates the organizations due to capitalization hence I get
2 entities (TransCo, organization) and (Transco, organization).</p>

<p>Is it possible to convert these into one entity?</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>Used Cosine similarity checker to check the similarity</p>

<p>ref: <a href=""https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings"">Calculate cosine similarity given 2 sentence strings</a></p>
",0,1,546,2018-06-07 08:47:04,https://stackoverflow.com/questions/50736830/how-to-cluster-named-entity-using-stanfordner-using-python
NLTK -&gt; Using Stanford Dependency Parser -&gt;,"<p>There seems to be a lot of contradicting documentation for NLTK (where is the definitive source for NLTK/StanfordNLP documentation?). </p>

<p>My question: what is the preferred method to call the StanfordParser from nltk? This is my code, but something is incorrect in the java call.</p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser
import os

parser_home = '/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/'

# os.environ['CLASSPATH'] = parser_home

parser = StanfordDependencyParser(
    model_path = parser_home + 'stanford-parser.jar',
    path_to_models_jar = parser_home +  'stanford-parser-3.9.1-models.jar',
    verbose = True
)

result = parser.raw_parse('Here is an example sentence.')
</code></pre>

<p>Here's my error. Any help appreciated. I haven't found an exact match to mine. I'm setting the classpath, but I'm not sure that's required.</p>

<pre><code>[Found stanford-parser\.jar: /Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser.jar]
[Found stanford-parser-(\d+)(\.(\d+))+-models\.jar: /Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser-3.9.1-models.jar]
/Users/myname/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: The StanfordDependencyParser will be deprecated
Please use nltk.parse.corenlp.StanforCoreNLPDependencyParser instead.
  if sys.path[0] == '':

SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Exception in thread ""main"" java.lang.RuntimeException: /Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser.jar: expecting BEGIN block; got PK��aL    META-INF/��PKPK��aLMETA-INF/MANIFEST.MFE��
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.confirmBeginBlock(LexicalizedParser.java:536)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTextFile(LexicalizedParser.java:546)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:406)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:186)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1400)

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-18-052e46a6f6aa&gt; in &lt;module&gt;()
----&gt; 1 result = parser.raw_parse('Here is an example sentence.')

~/anaconda3/envs/nlp/lib/python3.6/site-packages/nltk/parse/stanford.py in raw_parse(self, sentence, verbose)
    132         :rtype: iter(Tree)
    133         """"""
--&gt; 134         return next(self.raw_parse_sents([sentence], verbose))
    135 
    136     def raw_parse_sents(self, sentences, verbose=False):

~/anaconda3/envs/nlp/lib/python3.6/site-packages/nltk/parse/stanford.py in raw_parse_sents(self, sentences, verbose)
    150             '-outputFormat', self._OUTPUT_FORMAT,
    151         ]
--&gt; 152         return self._parse_trees_output(self._execute(cmd, '\n'.join(sentences), verbose))
    153 
    154     def tagged_parse(self, sentence, verbose=False):

~/anaconda3/envs/nlp/lib/python3.6/site-packages/nltk/parse/stanford.py in _execute(self, cmd, input_, verbose)
    216                 cmd.append(input_file.name)
    217                 stdout, stderr = java(cmd, classpath=self._classpath,
--&gt; 218                                       stdout=PIPE, stderr=PIPE)
    219 
    220             stdout = stdout.replace(b'\xc2\xa0', b' ')

~/anaconda3/envs/nlp/lib/python3.6/site-packages/nltk/__init__.py in java(cmd, classpath, stdin, stdout, stderr, blocking)
    134     if p.returncode != 0:
    135         print(_decode_stdoutdata(stderr))
--&gt; 136         raise OSError('Java command failed : ' + str(cmd))
    137 
    138     return (stdout, stderr)

OSError: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', '/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser-3.9.1-models.jar:/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser-3.9.1-javadoc.jar:/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/ejml-0.23.jar:/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser-3.9.1-sources.jar:/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/slf4j-api.jar:/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser-3.9.1-models.jar:/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser.jar:/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/slf4j-api-1.7.12-sources.jar', 'edu.stanford.nlp.parser.lexparser.LexicalizedParser', '-model', '/Users/myname/Documents/nlp/stanford-parser-full-2018-02-27/stanford-parser.jar', '-sentences', 'newline', '-outputFormat', 'conll2007', '-encoding', 'utf8', '/var/folders/kg/y1g8nszj77z0pm6mzplqv7580000gp/T/tmp93uyyya_']
</code></pre>
","java, python, nltk, stanford-nlp","<p>After digging around, it seems that the <code>StanfordDependencyParser</code> class has been deprecated in NLTK:</p>

<ul>
<li><a href=""https://github.com/nltk/nltk/issues/1510"" rel=""nofollow noreferrer"">Discussion on GitHub of people having similar issues</a></li>
<li><a href=""https://github.com/nltk/nltk/issues/1839"" rel=""nofollow noreferrer"">Proposal of more elegant interface</a></li>
</ul>

<p>The new, improved way:</p>

<p>First, download the full CoreNLP files from <a href=""https://stanfordnlp.github.io/CoreNLP/index.html#download"" rel=""nofollow noreferrer"">here</a>, then start a CoreNLP server (I chose port 9010) in the downloaded folder by running the below command. The folder looks like the <code>stanford-parser-full-2018-02-27</code> directory, for you:</p>

<p><code>$ java -mx1g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9010 -timeout 15000</code></p>

<p>Then, run this code:</p>

<pre><code>from nltk.parse.corenlp import CoreNLPParser
parser = CoreNLPParser(url='http://localhost:{somePort}'
next(
     parser.raw_parse('The quick brown fox sucks at jumping.')
     ).pretty_print()
                ROOT
                 |
                 S
       __________|__________________________
      |                     VP              |
      |                 ____|___            |
      |                |        PP          |
      |                |     ___|_____      |
      |                |    |         S     |
      |                |    |         |     |
      NP               |    |         VP    |
  ____|__________      |    |         |     |
 DT   JJ    JJ   NN   VBZ   IN       VBG    .
 |    |     |    |     |    |         |     |
The quick brown fox  sucks  at     jumping  .
</code></pre>

<p>Also, fun fact, once the server is running, you can navigate to <code>http://localhost:9010</code> (or whatever port you've chosen) and view a nice little interface to tinker around with. </p>
",5,4,3165,2018-06-15 21:41:10,https://stackoverflow.com/questions/50883000/nltk-using-stanford-dependency-parser
How to write scripts to keep punctuation in Stanford dependency parser,"<p>In order to get some specific dependency information I write a java script to parse sentences rather than directly use ParserDemo.java that Stanford Parser 3.9.1 provided. But I found punctuation is missing after got typedDependencies. Is there any function to get punctuation in Stanford Parser?
<strong>I had to write a script to parse sentences myself for the reason that I need to create a SemanticGraph from a List of TypedDependencies, in order to use methods in SemanticGraph to get evey single tokens dependent information(include punctuation).</strong></p>

<pre><code>public class ChineseFileTest3 {

public static void main(String[] args){

    String modelpath = ""edu/stanford/nlp/models/lexparser/xinhuaFactored.ser.gz"";
    LexicalizedParser lp = LexicalizedParser.loadModel(modelpath);
    String textFile = ""data/chinese-onesent-unseg-utf8.txt"";
    demoDP(lp,textFile);

}
public static void demoDP(LexicalizedParser lp, String filename){

for(List&lt;HasWord&gt; sentence : new DocumentPreprocessor(filename)) {

    Tree t = lp.apply(sentence);

    ChineseGrammaticalStructure gs = new ChineseGrammaticalStructure(t);
    Collection&lt;TypedDependency&gt; tdl = gs.typedDependenciesCollapsed();
    System.out.println(tdl);

}
}
}
</code></pre>
","nlp, stanford-nlp, text-processing","<p>I would suggest not using the parser standalone but instead just running a pipeline.  That will maintain the punctuation.</p>

<p>There is comprehensive documentation about using the Java API for pipelines here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/api.html</a></p>

<p>You need to set the properties for Chinese.  A quick way to do that is with this line of code</p>

<pre><code>Properties props = StringUtils.argsToProperties(""-props"", ""StanfordCoreNLP-chinese.properties"");
</code></pre>
",0,1,116,2018-06-16 04:24:52,https://stackoverflow.com/questions/50884939/how-to-write-scripts-to-keep-punctuation-in-stanford-dependency-parser
Execution time of Stanford CoreNLP on other languages,"<p>I need to extract sentences, tokens, pos tags and lemma from English and German text of a big corpora. So, I used the Stanford CoreNLP tool. Its output is perfect. However, the problem is the time complexity. The English one executes quickly but the German model takes a long time to annotate the text. I initialize the models with these codes:</p>
<pre><code>// To initialize English model
propsEN = new Properties();
propsEN.setProperty(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, lemma&quot;);
propsEN.setProperty(&quot;tokenize.language&quot;, &quot;en&quot;);
corenlpEN = new StanfordCoreNLP(propsEN);


// To initialize German model
propsDE = new Properties();
propsDE.setProperty(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, lemma&quot;);
propsDE.setProperty(&quot;tokenize.language&quot;, &quot;de&quot;);
corenlpDE = new StanfordCoreNLP(propsDE);
</code></pre>
<p>To represent the difference in execution times, I computed the length of each text and the time each model takes to run on the text. In order to calculate the execution time, I used System.currentTimeMillis() instruction:</p>
<h1>Executing the Stanford CoreNLP model on English Text:</h1>
<blockquote>
<p>English text length=1587   --- Elapse time=57</p>
<p>English text length=15906 --- Elapse time=160</p>
<p>English text length=44286 --- Elapse time=3287</p>
<p>English text length=19814 --- Elapse time=1809</p>
<p>English text length=1427  --- Elapse time=166</p>
<p>English text length=56787 --- Elapse time=2374</p>
</blockquote>
<h1>Executing the Stanford CoreNLP model on German Text:</h1>
<blockquote>
<p>German text length=979 --- Elapse time=401</p>
<p>German text length=22039  ---  Elapse time=15285</p>
<p>German text length=30632  --- Elapse time=21659</p>
<p>German text length=42019  --- Elapse time=21767</p>
<p>German text length=2944  --- Elapse time=2005</p>
<p>German text length=76248 --- Elapse time=48857</p>
</blockquote>
<p>Why does German model take several times? Have I made any mistake? Is there any solution to solve the problem?</p>
<p>Any information about this topic is appreciated.</p>
",stanford-nlp,"<p>I don't know if this will help, but you're not using the German part of speech tagger.  You can set that with the <code>pos.model</code> property.</p>

<p>Here is a list of options (make sure you have the German models jar):</p>

<pre><code>edu/stanford/nlp/models/pos-tagger/german/german-fast.tagger
edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger
edu/stanford/nlp/models/pos-tagger/german/german-fast-caseless.tagger
edu/stanford/nlp/models/pos-tagger/german/german-ud.tagger
</code></pre>

<p>Also there is no <code>lemma</code> for German.</p>
",1,0,502,2018-06-19 09:27:12,https://stackoverflow.com/questions/50924921/execution-time-of-stanford-corenlp-on-other-languages
Issue with ProtobufAnnotationSerializer - Stanford CoreNLP,"<p>I tried to use <code>ProtobufAnnotationSerializer</code> to serialize an <code>Annotation</code> object  as follow:</p>

<pre><code>String text = ""Stanford University is located in California. It is a great university, founded in 1891."";
Annotation document = new Annotation(text);
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse,depparse"");    
StanfordCoreNLP pip = new StanfordCoreNLP(props);
pip.annotate(document);

ProtobufAnnotationSerializer serializer = new ProtobufAnnotationSerializer();
FileOutputStream fileOut = new FileOutputStream(""path/to/anno.ser"");
ObjectOutputStream out = new ObjectOutputStream(fileOut);
serializer.write(document, out);
</code></pre>

<p>This bug came out:</p>

<pre><code>Exception in thread ""main"" java.lang.VerifyError: Bad type on operand stack
Exception Details:
  Location:
    com/google/protobuf/GeneratedMessageV3$ExtendableMessage.getExtension(Lcom/google/protobuf/Extension;I)Ljava/lang/Object; @3: invokevirtual
  Reason:
    Type 'com/google/protobuf/Extension' (current frame, stack[1]) is not assignable to 'com/google/protobuf/ExtensionLite'
  Current Frame:
    bci: @3
    flags: { }
    locals: { 'com/google/protobuf/GeneratedMessageV3$ExtendableMessage', 'com/google/protobuf/Extension', integer }
    stack: { 'com/google/protobuf/GeneratedMessageV3$ExtendableMessage', 'com/google/protobuf/Extension', integer }
  Bytecode:
    0x0000000: 2a2b 1cb6 0024 b0                      

    at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder(ProtobufAnnotationSerializer.java:611)
    at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:579)
    at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.write(ProtobufAnnotationSerializer.java:184)
at xxxxxxxxxxxxxx.main(xxxxxxx.java:303)  \\ line: serializer.write(document, out);
</code></pre>

<p>I think that there is an inconsistency between CoreNLP ProtobufAnnotationSerializer and protobuf package. I am using version 3.9.1 downloaded directly from <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">CoreNLP home page</a>, and I even tried some alternative solutions but none of them work. I tried:</p>

<ul>
<li>versions 3.9 3.8</li>
<li>download the package and its dependencies directly from maven</li>
<li>download and build (with ant) the source code on github.</li>
</ul>

<p>The error occurs with other languages (i tested with French) and even when calling the server.</p>
","java, stanford-nlp, protobuf-java","<p>Oooops my bad ... one of my dependencies includes protobuf-lite which cause the collision with corenlp ... Thanks a lot @GaborAngeli </p>
",0,1,219,2018-06-21 13:19:46,https://stackoverflow.com/questions/50969598/issue-with-protobufannotationserializer-stanford-corenlp
Stanford CoreNLP Classifier: NER training context,"<p>In Stanford's CoreNLP Classifier, all of the examples that I have seen have included words (denoted by O) that one does not want to recognize. For example, below ""certain"" and ""before"" are not recognized as Assets:</p>

<pre><code>certain O       O
Apple   ASSET   ASSET
products       ASSET   ASSET
macOS   ASSET   ASSET
before  O       O
</code></pre>

<p>1) Do I need words that provide context like ""certain"" and ""before""?</p>

<p>2) Does order matter? Could I, rather than the order ""certain, Apple, products, macOS, before"" do ""before, certain, Apple, macOS, products""?</p>

<p>3) If context is necessary, once I have added enough training data, could I just add more Assets without context?</p>
",stanford-nlp,"<p>Ad 1. Context is helpful if your classification is context-dependent.</p>

<p>Ad 2. Under the hood Stanford CoreNLP Classifier uses CRF (<a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Field</a>) algorithm which uses the order of words to classify correctly as well.</p>

<p>Ad 3. See pt. 1. - necessity depends on your problem and your data. You could reuse previous contexts and see if that improves or degrades the classification accuracy.</p>
",1,0,63,2018-06-25 15:45:21,https://stackoverflow.com/questions/51027347/stanford-corenlp-classifier-ner-training-context
Error in automating binarizer of stanford parser (Python),"<p>I am using Stanford Parser to generate trees from the given data in the required annotated format. I am able to do it for a single file by running:</p>

<pre><code>./lexparser.sh input_file &gt; output_file
</code></pre>

<p>from the <code>stanford-parser-full-2018-02-27</code> folder. But, I have multiple files so I tried to automate it using python as follows :</p>

<pre><code>import os
import glob
import subprocess

for movie in glob.glob(""../full_movies/annotated/*.txt""):
    tree = subprocess.call(['./lexparser.sh', os.path.basename(movie)])
    with open(""../full_movies/trees/"" + os.path.basename(movie), ""w"") as fid:
        fid.write(tree)
</code></pre>

<p>But it shows the following error :</p>

<blockquote>
  <p>SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
  SLF4J: Defaulting to no-operation (NOP) logger implementation
  SLF4J: See <a href=""http://www.slf4j.org/codes.html#StaticLoggerBinder"" rel=""nofollow noreferrer"">http://www.slf4j.org/codes.html#StaticLoggerBinder</a> for further details.
  Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""movie_name.txt"" as class path, filename or URL
      at edu.stanford.nlp.io.IOUtils.slurpFileNoExceptions(IOUtils.java:1316)
      at edu.stanford.nlp.sentiment.BuildBinarizedDataset.main(BuildBinarizedDataset.java:171)
  Caused by: java.io.IOException: Unable to open ""movie_name.txt"" as class path, filename or URL
      at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
      at edu.stanford.nlp.io.IOUtils.readerFromString(IOUtils.java:637)
      at edu.stanford.nlp.io.IOUtils.slurpFile(IOUtils.java:1159)
      at edu.stanford.nlp.io.IOUtils.slurpFile(IOUtils.java:1184)
      at edu.stanford.nlp.io.IOUtils.slurpFileNoExceptions(IOUtils.java:1314)
      ... 1 more</p>
</blockquote>

<p>I am unable to understand this. Is it a mistake in my python script or something else is wrong here?
If you want to see the contents of <code>./lexparser.sh</code> let me know.
Thanks in advance.</p>
","python, stanford-nlp","<p>I think your problem is simple you just give Filenames to the lex parser, because you call basename on it. But since lexparser isn’t in the same directory as the files you need to give the full path to the files. Just use movie and delete the os.path.basename call.</p>
",1,0,80,2018-07-01 05:18:33,https://stackoverflow.com/questions/51120617/error-in-automating-binarizer-of-stanford-parser-python
Converting props (properties) from Java to Python3 using StanfordCoreNLPServer,"<p>I'm converting <strong>Java</strong> code that uses <strong>StanfordCoreNLP</strong> to <strong>Python</strong> that accesses a <strong>StanfordCoreNLPServer</strong>. I'm not a Java developer at all. </p>

<p>The java code instantiates a StanfordCoreNLP <code>pipeline</code> using the following properties. </p>

<pre><code>    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner"");
    props.put(""ner.model"", serializedClassifier);
    props.put(""pos.model"", posModel);
    props.put(""tokenize.language"", ""de"");
    props.put(""ssplit.isOneSentence"", ""true"");
    props.put(""ssplit.language"", ""de"");
    props.put(""lemma.language"", ""de"");
    props.put(""regexner.mapping"", Init.REGEXNER);
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>I'm instantiating the Python pipeline as follows: </p>

<pre><code>    self.nlp = StanfordCoreNLP(host, port=port, timeout=30000)
    self.props = {
        'annotators': 'tokenize, ssplit, pos, lemma, ner, regexner',
        'pipelineLanguage': 'de',
        'tokenizeLanguage': 'de',
        'outputFormat': 'json', 
    }

def annotate(self, sentence):
    return json.loads(self.nlp.annotate(sentence, properties=self.props))
</code></pre>

<p><strong>The question is</strong>, I'm not sure how to convert the java properties (such as <code>ner.model</code>) to Python dict keys. I presume it would be something like <code>nerModel</code> ... however, when passing the <code>self.props</code> object in - nonsense keys DO NOT ERROR. So, I don't know if the key is valid or not.  </p>

<p>I'm not seeing documentation on the properties for Python anywhere on the Stanford page. </p>
","java, python-3.x, stanford-nlp","<p>When you start the server you can use the <code>-serverProperties</code> option and submit a properties file that will customize the pipeline the server uses.</p>

<p>Full details here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/corenlp-server.html</a></p>
",1,0,69,2018-07-02 10:03:12,https://stackoverflow.com/questions/51133613/converting-props-properties-from-java-to-python3-using-stanfordcorenlpserver
How i can extract NP subtrees in stanford parser using arabic model?,"<p>from 
(ROOT (S (VP (VBD ذهب) (NP (NNP احمد)) (PP (IN الي) (NP (DTNN المدرسه))))))</p>
","java, stanford-nlp","<p>In general I recommend using the full Stanford CoreNLP.</p>

<p>You can get that here: <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/</a></p>

<p>Here is some example code for finding particular constituents with the Java API:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.util.*;

import java.util.*;

public class ConstituentExample {

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = StringUtils.argsToProperties(""-props"", ""StanfordCoreNLP-arabic.properties"");
    // set up Stanford CoreNLP pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // build annotation for a review
    Annotation annotation =
        new Annotation(""..."");
    // annotate
    pipeline.annotate(annotation);
    // get tree
    Tree tree =
        annotation.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(TreeCoreAnnotations.TreeAnnotation.class);
    System.out.println(tree);
    Set&lt;Constituent&gt; treeConstituents = tree.constituents(new LabeledScoredConstituentFactory());
    for (Constituent constituent : treeConstituents) {
      if (constituent.label() != null &amp;&amp;
          (constituent.label().toString().equals(""VP"") || constituent.label().toString().equals(""NP""))) {
        System.err.println(""found constituent: ""+constituent.toString());
        System.err.println(tree.getLeaves().subList(constituent.start(), constituent.end()+1));
      }
    }
  }
}
</code></pre>
",0,-3,144,2018-07-08 16:01:00,https://stackoverflow.com/questions/51233772/how-i-can-extract-np-subtrees-in-stanford-parser-using-arabic-model
undefined reference to &#39;check_nan&#39;,"<p>I'm trying to compile the file (<code>glove.c</code> from Stanford NLP <a href=""https://github.com/stanfordnlp/GloVe/blob/master/src/glove.c"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/blob/master/src/glove.c</a>) but I'm getting an undefined reference error.</p>

<pre><code>aerin@capa:~/Desktop/GloVe/src$ gcc -pthread  glove.c -o glove.out -lm

/tmp/ccZMsGyg.o: In function `glove_thread':
glove.c:(.text+0x9d7): undefined reference to `check_nan'
glove.c:(.text+0xa6a): undefined reference to `check_nan'
collect2: error: ld returned 1 exit status
</code></pre>

<p>I can't find the answer about ""check_nan"" on google.  What flag should I use to compile this file? Any tip will greatly help!</p>
","c, gcc, stanford-nlp","<p>If you look in the makefile for the full project, you'll see that some options are being passed that you aren't including:</p>

<pre><code>CFLAGS = -lm -pthread -Ofast -march=native -funroll-loops -Wall -Wextra -Wpedantic
...
glove : $(SRCDIR)/glove.c
    $(CC) $(SRCDIR)/glove.c -o $(BUILDDIR)/glove $(CFLAGS)
</code></pre>

<p>The flag in pariticular you're missing is <code>-Ofast</code>.  Add that and it should compile fine.</p>

<p>Alternately, just run <code>make</code> from the top level directory to build everything.</p>
",1,1,773,2018-07-10 18:01:05,https://stackoverflow.com/questions/51271557/undefined-reference-to-check-nan
How can I use Java in Google Colab,"<p>I want to use Stanford CoreNLP in my Google Colab Notebook. For that I need Java. Is there a way to install Java on those machines?</p>

<p>What I currently have is:</p>

<pre><code>!pip install StanfordCoreNLP
from stanfordcorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('stanford-corenlp', lang='de', memory='4g')
...
nlp.close()
</code></pre>

<p>and I get the error:</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'java': 'java'
</code></pre>
","java, python, stanford-nlp, google-colaboratory","<p>try this  </p>

<pre><code>import os       #importing os to set environment variable
def install_java():
  !apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null      #install openjdk
  os.environ[""JAVA_HOME""] = ""/usr/lib/jvm/java-8-openjdk-amd64""     #set environment variable
  !java -version       #check java version
install_java()
</code></pre>
",21,10,33474,2018-07-11 13:51:25,https://stackoverflow.com/questions/51287258/how-can-i-use-java-in-google-colab
"TypeError: stat: path should be string, bytes, os.PathLike or integer, not _io.TextIOWrapper","<p>I found the following code on a python tutorial website:</p>
<pre><code>from nltk.tag import StanfordNERTagger

stanford_classifier = open(&quot;english.all.3class.distsim.crf.ser.gz&quot;)
stanford_ner_path = open(&quot;stanford-ner.jar&quot;)

st = StanfordNERTagger(stanford_classifier, stanford_ner_path)
</code></pre>
<p>The error is as follows:</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File &quot;C:/Users/DELL7810/AppData/Local/Programs/Python/Python37/stanpar.py&quot;, line 9, in &lt;module&gt;
st = StanfordNERTagger(stanford_classifier, stanford_ner_path)
 File &quot;C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\tag\stanford.py&quot;, line 180, in __init__
super(StanfordNERTagger, self).__init__(*args, **kwargs)
File &quot;C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\tag\stanford.py&quot;, line 63, in __init__
verbose=verbose)
File &quot;C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\internals.py&quot;, line 721, in find_jar
searchpath, url, verbose, is_regex))
File &quot;C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\internals.py&quot;, line 632, in find_jar_iter
if os.path.isfile(path_to_jar):
File &quot;C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\genericpath.py&quot;, line 30, in isfile
st = os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not _io.TextIOWrapper
</code></pre>
","python, python-3.x, file-io, typeerror, stanford-nlp","<h2>Your problem</h2>

<p>As you can see in this <a href=""https://kite.com/python/docs/nltk.tag.StanfordNERTagger"" rel=""nofollow noreferrer""><strong>documentation page</strong></a>, <code>StanfordNERTagger</code> takes file paths as arguments:</p>

<pre><code>StanfordNERTagger(path_to_model, path_to_jar)
</code></pre>

<p>Your code crashes because <code>open()</code> will give you file objects and this is not what <code>StanfordNERTagger</code> is expecting as arguments.</p>

<h2>Solution</h2>

<p>Directly give your paths as arguments to <code>StanfordNERTagger</code>, like this:</p>

<pre><code>st = StanfordNERTagger(""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\stanford-ner-2018-02-27\classifiers\english.all.3class.distsim.crf.ser.gz"", ""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\stanford-ner-2018-02-27\stanford-ner.jar"")
</code></pre>
",2,5,100467,2018-07-14 09:35:10,https://stackoverflow.com/questions/51337167/typeerror-stat-path-should-be-string-bytes-os-pathlike-or-integer-not-io-t
Quantifying sentiment analysis using python,"<p>I've been using NLTK in python for doing sentiment analysis, it only has positive, neutral and negative class, what if we want to do sentiment analysis and having a number to show how much a sentence can be negative or positive. Sort of seeing it as a regression problem. Is there any pre-trained library out there to do so? </p>
","python, nlp, nltk, stanford-nlp, sentiment-analysis","<p>I know of a few ways to do this:</p>

<ul>
<li><strong>Vader</strong> returns score as a gradation (between zero and one)</li>
<li><strong>Stanford NLP</strong> returns a categorical classification (i.e. 0, 1, 2, 3).</li>
</ul>

<h1>An NLTK way:</h1>

<pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer as sia
sentences = ['This is the worst lunch I ever had!',
             'This is the best lunch I have ever had!!',
             'I don\'t like this lunch.',
             'I eat food for lunch.',
             'Red is a color.',
             'A really bad, horrible book, the plot was .']

hal = sia()
for sentence in sentences:
    print(sentence)
    ps = hal.polarity_scores(sentence)
    for k in sorted(ps):
        print('\t{}: {:&gt;1.4}'.format(k, ps[k]), end='  ')
    print()
</code></pre>

<p>Example output:</p>

<pre><code>This is the worst lunch I ever had!
    compound: -0.6588   neg: 0.423      neu: 0.577      pos: 0.0  
</code></pre>

<h1>A Stanford-NLP, Python way:</h1>

<p>(Note that this way requires you to start an instance of the CoreNLP server to run e.g.: <code>java -mx1g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000</code>)</p>

<pre><code>from pycorenlp import StanfordCoreNLP
stanford = StanfordCoreNLP('http://localhost:9000')

for sentence in sentences:
    print(sentence)
    result = stanford.annotate(sentence,
                               properties={
                                'annotators': 'sentiment',
                                'outputFormat': 'json',
                                'timeout': '5000'
                               })
    for s in result['sentences']:
        score = (s['sentimentValue'], s['sentiment'])
    print(f'\tScore: {score[0]}, Value: {score[1]}')
</code></pre>

<p>Example output:</p>

<pre><code>This is the worst lunch I ever had!
    Score: 0, Value: Verynegative
</code></pre>
",2,2,1063,2018-07-14 21:05:46,https://stackoverflow.com/questions/51343373/quantifying-sentiment-analysis-using-python
Speeding up Stanford Dependency Parses in Python,"<p>Is there a faster way to implement the <code>CoreNLPParser</code> or <strong>should I interact with the API through another library?</strong> Or should I dust off the Java books?</p>

<p>I have a corpus of 6500 sentences that I'm running through the <code>CoreNLPParser</code> method in <code>nltk.parse.corenlp</code>. I have isolated everything else I'm doing from my main project to test the <code>tree_height</code> function I wrote previously. However, the speed is the same--in fact, this process takes more than 15 minutes to complete. </p>

<p>Here's my <code>tree_height</code> function:</p>

<pre><code>from nltk.parse.corenlp import CoreNLPParser
Parser = CoreNLPParser(url='http://localhost:9000')
def tree_height(tokenized_sent):
    ddep = Parser.raw_parse(tokenized_sent)
    for i in ddep:
        sent_height = i.height()
    return sent_height
</code></pre>

<p>I am parsing Spanish sentences and have previously started the CoreNLP server using the following command:</p>

<p><code>java -mx10g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-spanish.properties  -port 9000 -timeout 15000</code></p>

<p>I have also played with changing the <code>mx3g</code> part to <code>mx5g</code>, which doesn't seem to make a much of a difference. </p>

<p>I've seen <a href=""https://github.com/nltk/nltk/issues/1390"" rel=""nofollow noreferrer"">this discussion on GitHub</a> and am running a recent version of StanfordCoreNLP.</p>

<p><strong>--- Update ---</strong></p>

<p>I was concerned that the reason my script was performing slowly was because of inefficiencies or poorly written code--so here is what I've tried to do to find the inefficiencies with my code:</p>

<ol>
<li>Iterating over all the data (from a pandas dataframe) <strong>without calling any NLP functions</strong> takes about 20 seconds. </li>
<li>Iterating over all the data and <strong>only</strong> sentence tokenizing all the data takes ~30 seconds</li>
<li>In my latest attempt I added all the tokenized sentences to a variable and iteratively called the <code>tree_height</code> function on each one and <strong>found no difference in the speed</strong> (taking as long as before I started isolating code).</li>
</ol>
","python, nltk, stanford-nlp","<p>Ok so here is a description of a Python interface we are developing.  To get the latest version you'll have to download from GitHub and follow the install instructions (which are easy to follow !!)</p>

<p><strong>Go to GitHub and clone the Python interface repo:</strong></p>

<p><a href=""https://github.com/stanfordnlp/python-stanford-corenlp"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/python-stanford-corenlp</a></p>

<p><strong>cd into the directory and type</strong> <code>python setup.py install</code></p>

<p>(soon we'll set this up with <code>conda</code> and <code>pip</code> etc..., but for now it's still under development...you can get an older version on pip right now)</p>

<p><strong>in a separate terminal window, start up a Java server:</strong></p>

<pre><code>java -Xmx4g edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-spanish.properties -port 9000 -timeout 15000
</code></pre>

<p>NOTE: make sure to have all of the necessary jars in your <code>CLASSPATH</code> or run with the <code>-cp ""*""</code> option from a directory with all of the appropriate jars.</p>

<p><strong>run this Python code:</strong></p>

<pre><code>import corenlp
client = corenlp.CoreNLPClient(start_server=False, annotators=[""tokenize"", ""ssplit"", ""pos"", ""depparse""])
# there are other options for ""output_format"" such as ""json""
# ""conllu"", ""xml"" and ""serialized""
ann = client.annotate(u""..."", output_format=""text"")
</code></pre>

<p><code>ann</code> will contain the final annotated info (including the dependency parse)...this should be dramatically faster than what you are reporting...please try it out and let me know.</p>
",2,3,733,2018-07-16 17:03:06,https://stackoverflow.com/questions/51366811/speeding-up-stanford-dependency-parses-in-python
Can&#39;t implement options for Stanford CoreNLP&#39;s OpenIE annotator,"<p>I've been successfully running the Stanford CoreNLP annotators in Eclipse but have had issues implementing the provided options for the OpenIE annotator. Originally I thought it was an error only with the openie.filelist option and tried specifying the filepath different ways, but then I noticed that other options, such as openie.format, didn't work either. </p>

<p>Here is the code included below.</p>

<pre><code>package main.java.com.nlptools.corenlp;
import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations;
import edu.stanford.nlp.util.CoreMap;
import java.util.*;

public class OpenIETest {

  public static void main(String[] args) throws Exception {
    // Pipeline property setup
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
    props.setProperty(""openie.format"", ""reverb"");
    props.setProperty(""openie.filelist"", ""src/OpenIETestDoc.txt"");
    // Create corenlp pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Build and annotate an example document
    Annotation annotation = 
        new Annotation(""Usaine Bolt may play football trial in Australia. China's economic growth cools amid trade tensions. Trump is under fire after Putin meeting."");
    pipeline.annotate(annotation);

    // Loop for each sentence in the document
    for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
      // Get triples from sentence
       Collection&lt;RelationTriple&gt; triples =
          sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
      // Print triples
      for (RelationTriple triple : triples) { 
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }
    }
  }
}
</code></pre>

<p>When I run the code, no error or warning messages show, only the relationship tuples formed from the example document. The expected output would be a TSV in ReVerb format of the tuples formed from the filelist I included. How can I get the added properties to work? Thanks for any help and/or insight!</p>
","java, eclipse, stanford-nlp","<p>The instructions for using the standalone OpenIE system are here:</p>

<p><a href=""https://nlp.stanford.edu/software/openie.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/openie.html</a></p>

<p>Those options don't work when using OpenIE as an annotator in a pipeline, they work when using the OpenIE's main() method.</p>

<p>Sample commands are at that link above.</p>

<pre><code>java -mx1g -cp ""*"" edu.stanford.nlp.naturalli.OpenIE  /path/to/file1  /path/to/file2
</code></pre>

<p>In Java code you should directly call OpenIE's main() method.</p>
",0,1,228,2018-07-17 07:25:55,https://stackoverflow.com/questions/51375524/cant-implement-options-for-stanford-corenlps-openie-annotator
CRFClassifier: loading model from a stream gives exception &quot;invalid stream header: 1F8B0800&quot;,"<p>I am trying to load a <code>CRFClassifier</code> model from a file. This way works:</p>

<pre><code>// this works
classifier = CRFClassifier.getClassifier(""../res/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz"");
</code></pre>

<p><strong>When I want to use stream</strong>, however, I get <code>invalid stream header: 1F8B0800</code> exception:</p>

<pre><code>// this throws an exception
String modelResourcePath = ""../res/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz"";
BufferedInputStream stream = new BufferedInputStream(new FileInputStream(modelResourcePath));
classifier = CRFClassifier.getClassifier(stream);
</code></pre>

<p>Exception:</p>

<pre><code>Exception in thread ""main"" java.io.StreamCorruptedException: invalid stream header: 1F8B0800
at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:866)
at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:358)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1473)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1456)
at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2890)
at com.sv.research.ner.stanford.StanfordEntityExtractor.&lt;init&gt;(StanfordEntityExtractor.java:34)
at com.sv.research.ner.stanford.StanfordEntityExtractor.main(StanfordEntityExtractor.java:59)
</code></pre>

<p>I would expect both ways to be equivalent. My reason to load through a stream is that ultimately I want to load the model from JAR resources using:</p>

<pre><code>stream = ClassLoader.getSystemClassLoader().getResourceAsStream(modelResourcePath));
</code></pre>
",stanford-nlp,"<p>The way the classifier you are trying to use was serialized via GZIPInputStream as far as I could see from their sources.</p>

<p>So can you try deserializing the way that they serialize, like this: </p>

<pre><code>BufferedInputStream stream = new BufferedInputStream(new GZIPInputStream(new FileInputStream(modelResourcePath)));
</code></pre>

<p>Cheers</p>
",0,0,243,2018-07-18 09:31:32,https://stackoverflow.com/questions/51398494/crfclassifier-loading-model-from-a-stream-gives-exception-invalid-stream-heade
CRFClassifier: loading model from a stream gives exception &quot;invalid stream header: 1F8B0800&quot;,"<p>I am trying to load a <code>CRFClassifier</code> model from a file. This way works:</p>

<pre><code>// this works
classifier = CRFClassifier.getClassifier(""../res/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz"");
</code></pre>

<p><strong>When I want to use stream</strong>, however, I get <code>invalid stream header: 1F8B0800</code> exception:</p>

<pre><code>// this throws an exception
String modelResourcePath = ""../res/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz"";
BufferedInputStream stream = new BufferedInputStream(new FileInputStream(modelResourcePath));
classifier = CRFClassifier.getClassifier(stream);
</code></pre>

<p>Exception:</p>

<pre><code>Exception in thread ""main"" java.io.StreamCorruptedException: invalid stream header: 1F8B0800
at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:866)
at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:358)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1473)
at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1456)
at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2890)
at com.sv.research.ner.stanford.StanfordEntityExtractor.&lt;init&gt;(StanfordEntityExtractor.java:34)
at com.sv.research.ner.stanford.StanfordEntityExtractor.main(StanfordEntityExtractor.java:59)
</code></pre>

<p>I would expect both ways to be equivalent. My reason to load through a stream is that ultimately I want to load the model from JAR resources using:</p>

<pre><code>stream = ClassLoader.getSystemClassLoader().getResourceAsStream(modelResourcePath));
</code></pre>
",stanford-nlp,"<p>The way the classifier you are trying to use was serialized via GZIPInputStream as far as I could see from their sources.</p>

<p>So can you try deserializing the way that they serialize, like this: </p>

<pre><code>BufferedInputStream stream = new BufferedInputStream(new GZIPInputStream(new FileInputStream(modelResourcePath)));
</code></pre>

<p>Cheers</p>
",0,0,243,2018-07-18 09:31:32,https://stackoverflow.com/questions/51398494/crfclassifier-loading-model-from-a-stream-gives-exception-invalid-stream-heade
How to import Stanford CoreNLP library to Android Studio,"<p>How can i add Stanford Core Natural Language Processes library into Android Studio project</p>
","android, stanford-nlp","<p>You can get it from <strong>maven repository</strong> - <a href=""https://mvnrepository.com/artifact/edu.stanford.nlp/stanford-corenlp/3.9.1"" rel=""nofollow noreferrer"">Here</a></p>

<p>Or just import by adding this app level build.gradle</p>

<pre><code>compile ""edu.stanford.nlp:stanford-corenlp:3.9.1""
</code></pre>

<p>In my honest opinion I <strong>wouldn't recommend</strong> to use it android as its <strong>too heavy</strong> </p>

<p>On Github they also have suggested to use dedicated back end <strong>server</strong> to utilize its functionality - <a href=""https://github.com/stanfordnlp/CoreNLP/issues/623#issuecomment-364255895"" rel=""nofollow noreferrer"">Github Issue</a></p>
",3,1,1903,2018-07-20 06:28:14,https://stackoverflow.com/questions/51436326/how-to-import-stanford-corenlp-library-to-android-studio
Using POS Taggers will raise wrong format in German,"<p>I encountered some question about annotation German corpus, while it's normal for English corpus.
For example:</p>

<p><strong>Original Sentence :</strong>
( Foto unten ) RI Director Kjell-Åke Åkesson ( Schweden ) , Mitglied des NID-Tea    ms , bei der Impfung eines Kindes in Indien .</p>

<p><strong>Annotation Sentence :</strong></p>

<ol>
<li>(_XY Foto_NN unten_ADV )_CARD RI_NE Director_NE Kjell-胈NE</li>
<li>ke_XY 胈XY</li>
<li>kesson_NE (_VVFIN Schweden_NE )<em>NE ,</em>$, Mitglied_NN des_ART
NID-Teams_NN ,_$, be    i_APPR der_ART Impfung_NN eines_ART
Kindes_NN in_APPR Indien_NE ._$.</li>
</ol>

<p>In this case,the char ""Å"" will give rise to a change and cause a newline, therefore,the whole corpus would increase by 4000 lines.</p>

<p>By the way, my program command is as follow:</p>

<pre><code>java -mx8g -classpath stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -tokenize false -sentenceDelimiter newline -model models/german-hgc.tagger -textFile /data/mmyin/wmt17_de_en/test_1 &gt; /data/mmyin/wmt17_de_en/test_pos
</code></pre>
",stanford-nlp,"<p>Hi I would suggest using the full pipeline.</p>

<p>You can download it here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/</a></p>

<p>Here is a sample command for running on German text:</p>

<pre><code>java -Xmx4g edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-german.properties -annotators tokenize,ssplit,pos -file example.txt -outputFormat text
</code></pre>
",0,0,95,2018-07-20 08:02:20,https://stackoverflow.com/questions/51437842/using-pos-taggers-will-raise-wrong-format-in-german
Running GloVe on Windows,"<p>Is it possible to install GloVe on Windows 10?</p>
<p>From <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a> :</p>
<blockquote>
<p>Compile the source:  <code>cd GloVe-1.2 &amp;&amp; make</code></p>
</blockquote>
<p>But how can I run &quot;make&quot; in Windows?</p>
","windows, nlp, stanford-nlp","<p>I guess, there are two possible workarounds for you. You could install something like <a href=""http://www.mingw.org/"" rel=""nofollow noreferrer"">MinGW</a> and build official implementation of Glove on your Windows system. Or you could try installing Glove as a Python package (like <a href=""https://github.com/maciejkula/glove-python"" rel=""nofollow noreferrer"">this one</a> or like <a href=""https://pypi.org/project/glove/"" rel=""nofollow noreferrer"">this one</a>) through <code>pip</code> or <code>conda</code> or something, and work with Glove through Python. But, as the authors of these packages say, they could <em>contain a tremendous amount of bugs</em>. So trying to build the official Glove should be the best solution.</p>

<p>But, actually, if you do not need to train the model, installing Glove is not necessary, and you can just download pre-trained models from <a href=""http://vectors.nlpl.eu/repository/#"" rel=""nofollow noreferrer"">here</a>, for example.</p>
",1,2,2842,2018-07-22 08:50:12,https://stackoverflow.com/questions/51463453/running-glove-on-windows
How to train NER to recognize that a word is not an entity?,"<p>I may have worded my question poorly, but basically I have been training new models using spaCy for NER.  I have trained some custom entities and it's doing a really great job when I test it.  However, when I send it something that shouldn't be recognized as an entity, it seems to guess one of the entities anyways.  I am guessing it's because I never trained it what would = O(I think that's how stanford does it).</p>

<p>Here is a sample of my training data, does this look right?  Do I need to just add trash values and set the entity as O?</p>

<pre><code>[ ""644663"" , {""entities"": [[0,6, ""CARDINAL""]]}],
[ ""871448"" , {""entities"": [[0,6, ""CARDINAL""]]}],
[ ""6/26/1967"" , {""entities"": [[0,9, ""DATE""]]}],
[ ""1/21/1969"" , {""entities"": [[0,9, ""DATE""]]}],
[ ""GORDON GARDIN"" , {""entities"": [[0,13, ""PERSON""]]}],
[ ""CANDRA CARDINAL"" , {""entities"": [[0,15, ""PERSON""]]}],
[ ""FIAT"" , {""entities"": [[0,4, ""CARMAKE""]]}],
[ ""FORD"" , {""entities"": [[0,4, ""CARMAKE""]]}]
</code></pre>
","machine-learning, nlp, stanford-nlp, spacy, named-entity-recognition","<p>You're correct in that the problem is that you haven't shown the system anything that's <em>not</em> an entity. You don't want to add ""trash values"" however. Spacy expects your training strings to be strings with entities in context, not just singular examples of entities. So one training example should look more like:</p>

<p><code>[ ""My uncle drives a Ford"" , {""entities"": [(18,22, ""CARMAKE"")]}]</code></p>

<p>This will allow your system to train to recognize entities in context, and recognize more entities than just the specific training examples you give it (e.g. a well trained system would be able to recognize ""Chrysler"" and ""Toyota"" as car makes in addition to Ford and Fiat). Spacy has more in-depth <a href=""https://spacy.io/usage/training#example-new-entity-type"" rel=""nofollow noreferrer"">examples</a> for training custom entities, so I'd recommend you check that out.</p>
",2,0,698,2018-07-25 14:28:18,https://stackoverflow.com/questions/51521429/how-to-train-ner-to-recognize-that-a-word-is-not-an-entity
How do I set flags in code in Stanford Information Extraction,"<p>Using Stanford information extraction we have the following java code</p>

<pre><code>Properties props = new Properties();
props.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitymentions,depparse,natlog,openie"");
props.put(""tokenize.options"", ""latexQuotes=false"");
props.put(""openie"",""triple.extract=false,triple.all_nominals=false"")
pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>I want set some of the OpenIE flags (in props or something similar) available <a href=""https://nlp.stanford.edu/software/openie.html"" rel=""nofollow noreferrer"">here</a>.  So for example, the flag -triple.strict=false and -triple.all_nominals=true.  The approach I've taken is shown above and is</p>

<pre><code>props.put(""openie"",""triple.extract=false,triple.all_nominals=false"")
</code></pre>

<p>It's not clear if this is correct or working.  Assuming these appear in props, how are they set there?  If not, how do I set them in code?</p>

<p>Thanks!</p>
","stanford-nlp, information-extraction","<p>You were almost right. You should do:</p>

<pre><code>props.put(""openie.triple.extract"", ""false"");
props.put(""openie.triple.all_nominals"", ""false"")
</code></pre>
",1,1,89,2018-07-25 23:35:52,https://stackoverflow.com/questions/51529071/how-do-i-set-flags-in-code-in-stanford-information-extraction
How to escape regular expression special characters in CoreNLP TokenRegex pattern?,"<p>How to escape special characters (e.g.<code>$</code> or <code>?</code>) in CoreNLP TokenRegex pattern (e.g. <code>/$[0-9,]+/</code> where <code>$</code> refer to the currency symbol). For example, in java the pattern would be <code>\\$[0-9,]+</code> .</p>

<p>Is there a fuction similar to <code>Pattern.quote()</code> in CoreNLP TokenRegex?</p>
","java, stanford-nlp","<p>For special characters inside of regular expressions -- as in your example of <code>/$[0-9,]+/</code> -- this is the same escape character as Javan's Pattern. That is, '\'. So in your case, <code>/\$[0-9,]+/</code>.</p>

<p>For TokensRegex special characters, you can always quote them. For example: <code>foo ""["" citation needed ""]""</code>.</p>
",2,1,224,2018-07-25 23:43:56,https://stackoverflow.com/questions/51529128/how-to-escape-regular-expression-special-characters-in-corenlp-tokenregex-patter
How to read rules from a file,"<p>I am trying to match the sentence against rules.</p>

<p>I am able to compile multiple rules and match it against <code>CoreLabel</code> using the following method :</p>

<pre><code>TokenSequencePattern pattern1 = TokenSequencePattern.compile(""([{tag:/NN.*//*}])"");
TokenSequencePattern pattern2 = TokenSequencePattern.compile(""([{tag:/NN.*//*}])"");

List&lt;TokenSequencePattern&gt; tokenSequencePatterns = new ArrayList&lt;&gt;();
tokenSequencePatterns.add(pattern1);
tokenSequencePatterns.add(pattern2);

MultiPatternMatcher multiMatcher = TokenSequencePattern.getMultiPatternMatcher(tokenSequencePatterns);
List&lt;SequenceMatchResult&lt;CoreMap&gt;&gt; matched=multiMatcher.findNonOverlapping(tokens);
</code></pre>

<p>I have many rules inside a file. Is there any way to load the rule file?</p>

<p>I have seen a method to load the rules from file using the following method:</p>

<pre><code>CoreMapExpressionExtractor extractor = CoreMapExpressionExtractor.createExtractorFromFiles(TokenSequencePattern.getNewEnv(), ""en.rules"");
List&lt;MatchedExpression&gt; matched = extractor.extractExpressions((CoreMap)sentence);
</code></pre>

<p>But it accepts <code>CoreMap</code> as its argument. But I need to match it against <code>CoreLabel</code></p>
",stanford-nlp,"<p>Please see this comprehensive write up on TokensRegex:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokensregex.html</a></p>
",1,0,105,2018-07-27 16:46:51,https://stackoverflow.com/questions/51562312/how-to-read-rules-from-a-file
Train NER model in Stanford NLP,"<p>Exception while executing Train the model. Please find the steps that I have followed to train NER model,</p>

<p><strong>Step1</strong> : Create the training file like,</p>

<pre><code>the 0
Greenland   LOC
whale   0
is  0
deposed 0
,   0
-   0
the 0
great   0
sperm   0
whale   0
now 0
reigneth    0
!   0
</code></pre>

<p>and save as ""TrainingFile.tsv"" file.</p>

<p><strong>Step2:</strong> Created a .prop file we could use to train the first classifier.</p>

<p>Custom-ner.prop:</p>

<pre><code>trainFile = TrainingFile.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
maxLeft=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Step3: Build the classifier by executing the below code,</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier \
-prop propforclassifierone.prop
</code></pre>

<p><strong>While perform the Step3 throws an exception like,</strong></p>

<p>Exception :</p>

<pre><code>useSequences=true
wordShape=chris2useLC
useTypeySequences=true
useDisjunctive=true
noMidNGrams=true
serializeTo=ner-model.ser.gz
maxNGramLeng=6
useNGrams=true
usePrev=true
useNext=true
maxLeft=1
trainFile=directories2-10combined.tsv
map=word=0,answer=1
useWord=true
useTypeSeqs=true
=\
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.FileN
otFoundException: directories2-10combined.tsv (The system cannot find the file s
pecified)
        at edu.stanford.nlp.io.IOUtils.inputStreamFromFile(IOUtils.java:509)
        at edu.stanford.nlp.io.IOUtils.readerFromFile(IOUtils.java:550)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setN
extObject(ReaderIteratorFactory.java:189)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.&lt;ini
t&gt;(ReaderIteratorFactory.java:161)
        at edu.stanford.nlp.objectbank.ResettableReaderIteratorFactory.iterator(
ResettableReaderIteratorFactory.java:98)
        at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.&lt;init&gt;(ObjectBank.j
ava:414)
        at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:253)
        at edu.stanford.nlp.sequences.ObjectBankWrapper.iterator(ObjectBankWrapp
er.java:45)
        at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1585)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequence
Classifier.java:758)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequence
Classifier.java:746)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3034)
Caused by: java.io.FileNotFoundException: directories2-10combined.tsv (The syste
m cannot find the file specified)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(Unknown Source)
        at java.io.FileInputStream.&lt;init&gt;(Unknown Source)
        at edu.stanford.nlp.io.IOUtils.inputStreamFromFile(IOUtils.java:503)
        ... 11 more
</code></pre>
","java, nlp, stanford-nlp","<p>Are you sure, you are using the correct files?</p>

<p>In your example in Step 2 you have a properties file called <code>Custom-ner.prop</code>, while in Step 3 you call a command with a file called <code>propforclassifierone.prop</code>. 
Also, your training file seems to be named differently: <code>TrainingFile.tsv</code> vs. <code>directories2-10combined.tsv</code>.</p>

<p>Plus, make sure, your training file is in your class path (respective in the same directory as the jar file) or provide the absolute path of your training file.</p>
",0,0,324,2018-07-30 07:30:07,https://stackoverflow.com/questions/51588988/train-ner-model-in-stanford-nlp
Adding additional words in word2vec or Glove (maybe using gensim),"<p>I have two pretrained word embeddings: <code>Glove.840b.300.txt</code> and <code>custom_glove.300.txt</code></p>

<p>One is pretrained by Stanford and the other is trained by me.
Both have different sets of vocabulary. To reduce oov, I'd like to add words that don't appear in file1 but do appear in file2 to file1.
How do I do that easily?</p>

<p>This is how I load and save the files in gensim 3.4.0.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model = KeyedVectors.load_word2vec_format('path/to/thefile')
model.save_word2vec_format('path/to/GoogleNews-vectors-negative300.txt', binary=False)
</code></pre>
","nlp, gensim, word2vec, glove","<p>I don't know an <em>easy</em> way. </p>

<p>In particular, word-vectors that weren't co-trained together won't have compatible/comparable coordinate-spaces. (There's no one right place for a word – just a relatively-good place compared to the other words that are in the same model.)</p>

<p>So, you can't just append the missing words from another model: you'd need to transform them into compatible locations. Fortunately, it seems to work to use some set of shared anchor-words, present in both word-vector-sets, to learn a transformation – then apply that the words you want to move over.</p>

<p>There's a class, <code>[TranslationMatrix][1]</code>, and <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">demo notebook</a> in gensim showing this process for language-translation (an application mentioned in the original word2vec papers). You could concievably use this, combined with the ability to <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add"" rel=""nofollow noreferrer"">append extra vectors to a gensim <code>KeyedVectors</code></a> instance, to create a new set of vectors with a superset of the words in either of your source models.</p>
",4,2,2578,2018-07-30 20:52:57,https://stackoverflow.com/questions/51602111/adding-additional-words-in-word2vec-or-glove-maybe-using-gensim
What is the default number of threads in stanford-corenlp,"<p>What is the default number of threads in stanford-corenlp?  Specifically, the named entity extractor, and then the information extractor.  Also, I would like both to use a single thread for debugging purposes, how do I set this?</p>

<p>Thanks!</p>
","multithreading, stanford-nlp","<p>Default is 1 thread.</p>

<p>There are two ways to run Stanford CoreNLP in a multi-threaded mode.</p>

<p>1.) each thread handles a separate document</p>

<p>2.) each thread handles a separate sentence</p>

<p>Suppose you have 4 cores.</p>

<p>If you want each thread to handle a separate document, use the <code>-threads 4</code> option (assuming you want to use 4).</p>

<p>So you might run this command:</p>

<pre><code>java -Xmx14g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,kbp -threads 4 -fileList sample-files.txt -outputFormat text
</code></pre>

<p>Multiple annotators can process sentences in parallel.  Here is an example of setting the named entity processor to use multiple threads.</p>

<pre><code>java -Xmx14g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,kbp -ner.nthreads 4 -fileList sample-filelist-16.txt -outputFormat text
</code></pre>

<p>The following annotators can work on multiple sentences at the same time:</p>

<pre><code>name       example configuration

depparse   -depparse.nthreads 4
ner        -ner.nthreads 4
parse      -parse.nthreads 4
</code></pre>

<p>Note that while the <code>ner</code> annotator can run in multi-threaded mode, it uses several sub-annotators that cannot.  So you are really only getting the statistical model run in parallel.  The pattern matching rules modules do not operate in multi-threaded mode.</p>
",5,4,2035,2018-08-01 14:41:46,https://stackoverflow.com/questions/51636158/what-is-the-default-number-of-threads-in-stanford-corenlp
Unable to Start Stanford CoreNLP server with Shift-Reduce Parser,"<p>I have setup coreNLP server on local and can be started using command:</p>
<pre><code>java -mx5g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer  -timeout 100000 
</code></pre>
<p>But default constituency parser is set with following warning message:</p>
<blockquote>
<p>warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz</p>
<p>using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead</p>
</blockquote>
<p>I downloaded the shift reduce parser, extracted jar file and pasted <strong>edu</strong> folder in project directory.</p>
<p>To double check, I manually traversed to edu/stanford/nlp/models/lexparser to make sure <strong>englishSR.ser.gz</strong> exists.</p>
<p>Also, created <strong>.properties</strong> file and used -serverProperties option when starting server again.</p>
<p><strong>StanfordCoreNLP.properties</strong> file contains:</p>
<pre><code>parser.model = edu/stanford/nlp/models/srparser/englishSR.ser.gz
</code></pre>
<ul>
<li>Command to run server is now:</li>
</ul>
<blockquote>
<pre><code>java -mx5g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 100000 -serverProperties &quot;StanfordCoreNLP.properties&quot;
</code></pre>
</blockquote>
<p>No luck and I am still getting the same warning.
Please comment if more clarification is needed.</p>
",stanford-nlp,"<p>You should put the full English models jar in the directory where you are running this command.</p>

<p>The <code>-cp ""*""</code> is saying to look at all jars in that directory. </p>

<p>You can find that English models jar here: </p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a></p>
",2,1,563,2018-08-01 17:44:36,https://stackoverflow.com/questions/51639330/unable-to-start-stanford-corenlp-server-with-shift-reduce-parser
Documentation for training a named entity recognizer model from an IOB annotated train set,"<p>Is there any documentation which can be used to find out which properties should be set for training a Stanford NER model from a train set with IOB annotation tags in Java? </p>

<p>Thanks in advance.</p>
","java, stanford-nlp, named-entity-recognition","<p>There is a detailed example located here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/ner.html</a></p>

<p>see:
""Training or retraining new models""</p>
",1,0,82,2018-08-02 18:39:30,https://stackoverflow.com/questions/51660233/documentation-for-training-a-named-entity-recognizer-model-from-an-iob-annotated
problems with using regexner to override existing named entities while maintaining entitymentions,"<p>I am using Stanford CoreNLP 3.9.1 on a Mac running Java version 1.8.0_101. I have CoreNLP running and have been able to leverage most of the annotators. I am trying to fine-tune my named entity recognition and having problems over-riding existing mappings.</p>

<p>Below is the content of properties file:
    # This file is launched with the following command:
    # Marks-MacBook-Pro-4:stanford-corenlp-full-2018-02-27 moranmarkd$ java -cp ""*"" -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -props ner-3.props</p>

<pre><code>annotators = tokenize,ssplit,pos,lemma,ner

# RegexNER mappings file - this is different than just the regexner file...this should incorporate with Named Entity Mentions
ner.additional.regexner.mapping = regexner-1.txt

# Where to find the list of input files
filelist = CL-Cleaned-Infiles.txt

# Where to place and how to format output
outputDirectory = ../../CL-Cleaned-CoreNLP-ner3-json
outputFormat = json
</code></pre>

<p>Below is the content of regexner-1.txt
    John Deere\tORGANIZATION\tPERSON\n
    New Holland\tORGANIZATION\tCOUNTRY\n</p>

<p>I have tried using both the regexner annotator at the end of the pipeline and invoking additional mapping (as above) and not able to override the defaults identification of John Deere as a person and New Holland as a country.</p>

<p>I have also tried invoking all of these same settings from a long command line with no success either:
    java -cp ""*"" -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -anotators tokenize,ssplit,pos,lemma,ner -ner.additional.regexner.mapping regexner-1.txt -filelist CL-Cleaned-Infiles.txt -outputDirector ../../CL-Cleaaned-CoreNLP-ner3-json -outputFormat json</p>

<p>My preference was to do the additional mapping instead of regexner to be able to still impact Entity Mentions (as I understand the pipeline).</p>

<p>Any advice on how to troubleshoot, what to correct, or what to try is welcome.</p>

<p>Best,</p>

<p>Mark</p>
",stanford-nlp,"<p>the current documentation matches the current code available on GitHub, so Stanford CoreNLP 3.9.1 doesn't have the refined ner pipeline (which was just added a couple weeks ago).  I'll make a note of this on the page.</p>

<p>We are working to get Stanford CoreNLP 3.9.2 out fairly soon (matter of weeks).</p>

<p>It's very straight forward to create a jar with the code from GitHub.  The instructions are on the main page.</p>

<p>If you have any other questions please let me know.</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP</a></p>

<p>I ran this command on your example with your rules and got the proper results (using the latest code):</p>

<pre><code>java -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -ner.additional.regexner.mapping override_example.rules -file override_example.txt -outputFormat text
</code></pre>
",0,1,417,2018-08-05 16:35:57,https://stackoverflow.com/questions/51696241/problems-with-using-regexner-to-override-existing-named-entities-while-maintaini
NLTK and Stanford Dependency Parser - How to get word position?,"<p><strong>Can I get the word's position when using Stanford's dependency parser via NLTK</strong> as shown in <a href=""https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk"">this SO question</a>?</p>

<p>This is the <a href=""https://www.nltk.org/_modules/nltk/parse/stanford.html"" rel=""nofollow noreferrer"">source code</a> for reference</p>

<p>~~~</p>

<p><strong>Example:</strong></p>

<p>When I use Stanford's Dependency Parser via NLTK, using the example in the SO post referenced above I get a list of tuples like this:</p>

<pre><code>[((u'shot', u'VBD'), u'nsubj', (u'I', u'PRP')),
((u'shot', u'VBD'), u'dobj', (u'elephant', u'NN')),
((u'elephant', u'NN'), u'det', (u'an', u'DT')),
((u'shot', u'VBD'), u'prep', (u'in', u'IN')),
((u'in', u'IN'), u'pobj', (u'sleep', u'NN')),
((u'sleep', u'NN'), u'poss', (u'my', u'PRP$'))]
</code></pre>

<p>whereas when I use the online tool I also get a pointer to the word's position, see the digits in the text below:</p>

<pre><code>nsubj(shot-2, I-1)
det(elephant-4, an-3)
dobj(shot-2, elephant-4)
prep(shot-2, in-5)
poss(sleep-7, my-6)
pobj(in-5, sleep-7)
</code></pre>

<p>The info about the word position is non-trivial in some specific cases*, so is it still possible to get it via NLTK? </p>

<p>(*) for the specific cases: think technical texts where jargon acronyms are substituted with plain English keywords to ease the parser's job</p>
","python, nltk, stanford-nlp","<p>Not sure if there is a way to get this from the triples directly. But, if I recall correctly, you call <code>deps.triples()</code> on your dependencies to get them in this triple format. On that dependencies object (deps above), you can also call <code>deps.get_by_address(i)</code> to get the word at the specified index. You could try if these are connected (i.e. whatever object you get from .get_by_address(position) and every item in the deps.triples()). If so, you can make a dictionary before from dep triple to position.
And .get_by_address() is 1-based (not 0-based), as the 0 is always the root node.</p>

<p>EDIT: Just found out that .triples() just seems to return a list of tuples, doesn't look like anything fancy from which you can retrieve for ex. position info. The following may help you though (sorry for the German example):</p>

<pre><code>s = 'Ich werde nach Hause gehen .'
res = depParser.parse(s.split()) # can use a simple .split since my input is already tokenised
deps = res.__next__()
traverse(deps, 0) # 0 is always the root node
</code></pre>

<p>traversing then goes as follows:</p>

<pre><code>def traverse(deps, addr):

dep = deps.get_by_address(addr)
print(dep)
for d in dep['deps']:
    for addr2 in dep['deps'][d]:
        traverse(deps, addr2)
</code></pre>

<p>Which should just recursively walk through all dependencies in the graph, and gives me the following output:</p>

<pre><code>{'word': None, 'head': None, 'address': 0, 'lemma': None, 'feats': None, 'ctag': 'TOP', 'deps': defaultdict(&lt;class 'list'&gt;, {'root': [3]}), 'tag': 'TOP', 'rel': None}
{'word': 'nach', 'head': 0, 'address': 3, 'lemma': '_', 'rel': 'root', 'ctag': 'VBP', 'feats': '_', 'deps': defaultdict(&lt;class 'list'&gt;, {'dobj': [5], 'nsubj': [2]}), 'tag': 'VBP'}
{'word': 'gehen', 'head': 3, 'address': 5, 'lemma': '_', 'rel': 'dobj', 'ctag': 'NN', 'feats': '_', 'deps': defaultdict(&lt;class 'list'&gt;, {'amod': [4]}), 'tag': 'NN'}
{'word': 'Hause', 'head': 5, 'address': 4, 'lemma': '_', 'rel': 'amod', 'ctag': 'JJ', 'feats': '_', 'deps': defaultdict(&lt;class 'list'&gt;, {}), 'tag': 'JJ'}
{'word': 'werde', 'head': 3, 'address': 2, 'lemma': '_', 'rel': 'nsubj', 'ctag': 'NNP', 'feats': '_', 'deps': defaultdict(&lt;class 'list'&gt;, {'compound': [1]}), 'tag': 'NNP'}
{'word': 'Ich', 'head': 2, 'address': 1, 'lemma': '_', 'rel': 'compound', 'ctag': 'NNP', 'feats': '_', 'deps': defaultdict(&lt;class 'list'&gt;, {}), 'tag': 'NNP'}
</code></pre>

<p>In a slightly different format than the .triples() you are using, but hope this helps.</p>
",1,2,529,2018-08-06 10:58:15,https://stackoverflow.com/questions/51706023/nltk-and-stanford-dependency-parser-how-to-get-word-position
Dependency Parsing using Stanford Dependency Parser,"<p>i am trying to extract main verb in a sentence and i followed this <a href=""https://stackoverflow.com/questions/19751230/how-can-we-extract-the-main-verb-from-a-sentence"">question</a> , i am expecting output in this format </p>

<pre><code>nsubj(swim-4, Parrots-1)
aux(swim-4, do-2)
neg(swim-4, not-3)
root(ROOT-0, swim-4)
</code></pre>

<p>but i am getting output in this way </p>

<pre><code>[&lt;DependencyGraph with 94 nodes&gt;]
</code></pre>

<p>i did following </p>

<pre><code>  dependencyParser = stanford.StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
  print (list(dependencyParser.raw_parse(noiseLessInput)))
</code></pre>

<p>i think i am doing something wrong, how can i achieve desired ouput</p>
","machine-learning, nlp, stanford-nlp, dependency-parsing","<p>yeah, found how to do that through <a href=""https://stackoverflow.com/questions/39340907/converting-output-of-dependency-parsing-to-tree"">this question</a>, but it is not showing root attribute, that's the only issue now </p>

<pre><code>  dependencyParser = stanford.StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
result = dependencyParser.raw_parse(noiseLessInput)
dep = result.__next__()
for triple in dep.triples():
 print(triple[1], ""("", triple[0][0], "", "", triple[2][0], "")"")
</code></pre>
",1,0,353,2018-08-10 12:09:28,https://stackoverflow.com/questions/51786224/dependency-parsing-using-stanford-dependency-parser
"Glove word embedding model parameters using tex2vec in R, and display training output (epochs) after every n iterations","<p>I am using text2vec package in R for training word embedding (Glove Model) as:</p>

<pre><code>library(text2vec)
library(tm)

prep_fun = tolower
tok_fun = word_tokenizer
tokens = docs %&gt;%  # docs: a collection of text documents  
prep_fun %&gt;% 
tok_fun

it = itoken(tokens, progressbar = FALSE)

stopword &lt;- tm::stopwords(""SMART"")
vocab = create_vocabulary(it,stopwords=stopword) 

vectorizer &lt;- vocab_vectorizer(vocab)

tcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 6)

x_max &lt;- min(50,max(10,ceiling(length(vocab$doc_count)/100)))
glove_model &lt;- GlobalVectors$new(word_vectors_size = 200, vocabulary = vocab, x_max = x_max,learning_rate = 0.1) 

word_vectors &lt;- glove_model$fit_transform(tcm, n_iter = 1000, convergence_tol = 0.001)
</code></pre>

<p>When I run this code I get the following output:
<a href=""https://i.sstatic.net/e7TBg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/e7TBg.png"" alt=""enter image description here""></a></p>

<p>My questions are:</p>

<ol>
<li>Is it possible to have output after every n iterations, i.e. output for epoch 50, 100, 150 and so on.</li>
<li>Any suggestion for optimal values for word_vectors_size, x_max and learning_rate? for example for 10,000 documents, what is the best value for those parameters? </li>
</ol>

<p>I appreciate your response.</p>

<p>Many thanks,
Sam</p>
","r, word2vec, text2vec, glove","<p>There is a member of the <code>GlobalVectors</code> class called <code>n_dump_every</code>. You can set it to some number and the history of word embeddings will be saved. Then it can be retrieved with <code>get_history()</code> function</p>

<pre><code>glove_model &lt;- GlobalVectors$new(word_vectors_size = 200, vocabulary = vocab, x_max = 100,learning_rate = 0.1) 
glove_model$n_dump_every = 10
word_vectors &lt;- glove_model$fit_transform(tcm, n_iter = 1000, convergence_tol = 0.001)
trace = glove_model$get_history()
</code></pre>

<p>Regarding second question - </p>

<ul>
<li>you may try to vary learning rate a bit (usually decrease), but default one should be ok (keep track of the value of cost function).</li>
<li>the more data you have the larger value you can provide for <code>word_vectors_size</code>. For wikipedia size 300 is usually enough. For smaller datasets you may start with 20-50. You really need to experiment with this.</li>
</ul>
",0,0,1021,2018-08-14 00:51:28,https://stackoverflow.com/questions/51832333/glove-word-embedding-model-parameters-using-tex2vec-in-r-and-display-training-o
coreNLPDependencyParser output explanation,"<p>i am running coreNLPDependencyParser for a sentence </p>

<blockquote>
  <p>The quick brown fox jumps over the lazy dog.</p>
</blockquote>

<p>and i am getting output in this way </p>

<pre><code>The     DT      4       det
quick   JJ      4       amod
brown   JJ      4       amod
fox     NN      5       nsubj
jumps   VBZ     0       ROOT
over    IN      9       case
the     DT      9       det
lazy    JJ      9       amod
dog     NN      5       nmod
.       .       5       punct
</code></pre>

<p>i ran the same input in stanfordDependencyParser and the output is same with different representation</p>

<p><strong>My question is</strong>
if you see the third column it is giving some score sort of thing, i assumed it to be depth in the tree but its not correct </p>

<p>it is not mentioned anywhere what exactly the score is .</p>

<p>you can see tree <a href=""http://www.nltk.org/api/nltk.parse.html?highlight=stanford"" rel=""nofollow noreferrer"">here</a></p>

<p>please enlighten me on output representation?</p>
","nlp, stanford-nlp, dependency-parsing","<p>Each value in the third column is a directed edge in the dependency tree. For example:</p>

<ul>
<li>The head/governor of ""quick"" is token 4: ""fox"", and ""quick"" is a modifier of ""fox"" (amod)</li>
<li>The head/governor of ""fox"" is token 5: ""jumps"" and ""fox"" is the subject of ""jumps"" (nsubj)</li>
</ul>

<p>The value 0 is reserved for the root of the tree, usually the main verb of the sentence.</p>
",1,1,163,2018-08-14 08:00:33,https://stackoverflow.com/questions/51836500/corenlpdependencyparser-output-explanation
Stanford Name Entity Recognizer(NER) using PyNER not working,"<p>I am trying using Stanford's Name Entity Recognizer(NER). </p>

<p>I downloaded the zip file from : <a href=""https://github.com/dat/pyner"" rel=""nofollow noreferrer"">https://github.com/dat/pyner</a>.</p>

<p>Installed it using: python setup.py install.</p>

<p>Now when I am running the below command, I am getting blank output</p>

<pre><code>import ner
tagger =ner.SocketNER(host='localhost',port=31752,output_format='slashTags')
tagger.get_entities(""University of California is located in California, United States"")

Output:
{}
</code></pre>

<p>am I missing anything?</p>
","python-3.x, nlp, nltk, stanford-nlp, spacy","<p>The <a href=""https://github.com/dat/pyner"" rel=""nofollow noreferrer"">https://github.com/dat/pyner</a> tool is badly outdated. </p>

<p>If you're using NLTK, first update your <code>nltk</code> version:</p>

<pre><code>pip3 install -U nltk
</code></pre>

<p>Then still in terminal:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip
unzip stanford-corenlp-full-2018-02-27.zip
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -preload tokenize,ssplit,pos,lemma,ner,parse,depparse -status_port 9000 -port 9000 -timeout 15000 &amp;
</code></pre>

<p>Then in Python3:</p>

<pre><code>&gt;&gt;&gt; from nltk.parse import CoreNLPParser
&gt;&gt;&gt; tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
&gt;&gt;&gt; tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
&gt;&gt;&gt; 
&gt;&gt;&gt; tagger.tag(tokens)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'STATE_OR_PROVINCE')]
</code></pre>

<hr>

<h1>For Windows</h1>

<p>You can use the above using <code>powershell</code> (which you really do so) but if you like to click on your mouse.</p>

<p><strong>Step 1:</strong> Download the zip file from <a href=""http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip</a></p>

<p><strong>Step 2:</strong> Unzip it</p>

<p><strong>Step 3:</strong> Open command prompt and go to the folder where file has been unzipped</p>

<p><strong>Step 4:</strong> run command: <code>pip3 install -U nltk</code></p>

<p><strong>Step 5:</strong> Now run command:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -preload tokenize,ssplit,pos,lemma,ner,parse,depparse -status_port 9000 -port 9000 -timeout 15000 &amp;
</code></pre>

<p>Then in Python3:</p>

<pre><code>&gt;&gt;&gt; from nltk.parse import CoreNLPParser
&gt;&gt;&gt; tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
&gt;&gt;&gt; tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
&gt;&gt;&gt; 
&gt;&gt;&gt; tagger.tag(tokens)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'STATE_OR_PROVINCE')]
</code></pre>
",2,0,424,2018-08-23 05:02:15,https://stackoverflow.com/questions/51978633/stanford-name-entity-recognizerner-using-pyner-not-working
Stanford CoreNLP - lemmas are not recognised correctly,"<p>I’m using the coreNLP tools from the command line to tag some files containing text in German. I need to get the token, pos, lemma and ner annotations. For this purpose I’m using the following command:</p>

<p>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP <strong>-annotators tokenize,ssplit,pos,lemma,ner</strong> -filelist $dir/filelist.input  -outputFormat conll --add-modules java.se.ee -ner.useSUTime 0 <strong>-outputFormatOptions word,pos,lemma,ner</strong> -outputDirectory $dir/tagged_articles -replaceExtension -props StanfordCoreNLP-german.properties</p>

<p>However, the lemmas I’m getting are just not right. Here is an example of a tagged file:</p>

<p>Auch     ADV    auch    O</p>

<p>eine     ART     eine   O</p>

<p>ausgereifte ADJA    ausgereifte O</p>

<p>Technik NN  technik O</p>

<p>kann    VMFIN   kann    O</p>

<p>jedoch  ADV jedoch  O</p>

<p>an  APPR    a   O</p>

<p>ihre    PPOSAT  ihre    O</p>

<p>Grenzen NN  grenzen O</p>

<p>stoßen  VVINF   stoßen  O</p>

<p>The lemmas for some of those words should be: ist -> sein / Textmengen -> Textmenge / enormen -> enorm / Grenzen -> Grenze. So there is obviously something wrong and I’m wondering what it could be. Any hint is highly appreciated!</p>

<p>I am using the following German model: stanford-german-corenlp-2018-02-27-models.jar</p>

<p>According to the README file, the version of the coreNLP tools is ""2018-02-27    3.9.1”</p>

<p>java version ""10.0.1"" 2018-04-17</p>

<p>Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)</p>
","java, command-line, nlp, stanford-nlp","<p>By now, lemmas are only supported for English: </p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/human-languages.html"" rel=""nofollow noreferrer"">Supported human languages</a></p>

<p>You could try using a different lemmatizer and add the lemmas manually. </p>
",0,0,300,2018-08-23 14:32:40,https://stackoverflow.com/questions/51988251/stanford-corenlp-lemmas-are-not-recognised-correctly
Stanford OpenNLP extract only those names that are mentioned in relation to (identified) organisation,"<p>With the Stanford NER tagger, I am able to extract all PERSONs and ORGANISATIONS as expected. Here is a short snippet:</p>

<pre><code>    ss=tagger.get_entities(text)
    xorg=unique_list(ss.get('ORGANIZATION'))
    xper=unique_list(ss.get('PERSON'))
    out= (xorg,xperson)
    #out is written to database
</code></pre>

<p>My question is how do I extract only those PERSON names which have a relation to named ORGANISATION? Specifically, I want the output as a triplet: PERSON, RELATION, ORGANISATION.</p>

<p>For either ""Enron Chairman Kenneth Lay"" OR ""Kenneth Lay, Chairman, Enron"" I expect the output to read as (Kenneth Lay) (Chairman) (Enron).</p>

<p>Any help will be useful.</p>
","python-3.x, stanford-nlp, opennlp, named-entity-recognition","<p>Plain NER is just about finding (named) entities and label them correctly. Your task is called relation extraction. You should look at following links:</p>

<p><a href=""https://nlp.stanford.edu/software/relationExtractor.html"" rel=""nofollow noreferrer"" title=""Stanford Relation Extractor"">Stanford Relation Extractor</a> extracts relations between entities: <code>Live_In</code>, <code>Located_In</code>, <code>OrgBased_In</code>, <code>Work_For</code>, and <code>None</code>.</p>

<p><a href=""https://nlp.stanford.edu/software/openie.html"" rel=""nofollow noreferrer"" title=""Stanford OpenIE"">Stanford OpenIE</a> is able to extract arbitrary binary relations from text. Thus, doing NER isn't necessary beforehand.</p>

<p>Maybe one of these tools helps you with your task.</p>
",0,0,75,2018-08-24 15:46:14,https://stackoverflow.com/questions/52007847/stanford-opennlp-extract-only-those-names-that-are-mentioned-in-relation-to-ide
Stanford&#39;s coreNLP Name Entity Recogniser throwing error 500 Server Error: Internal Server Error for url,"<p>I have a set of text files. I am using Stanford's coreNLP Name Entity Recogniser to extract details of the lines where patient name is mentioned out of those files. When I am running NER on a single sentence, it is printing results correctly but when I am running it on set of files, it is printing the results along with error and also I am not able to write the results on a text file because of this:</p>

<pre><code>500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cner%22%2C+%22ssplit.isOneSentence%22%3A+%22true%22%7D
</code></pre>

<p>Here is the code which I am using:</p>

<pre><code>import re
import os
from nltk.parse import CoreNLPParser
tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')

def name_detail_extracter():    

    data_location=""D:\Data"" # folder containing all the data
    for root, dirs, files in os.walk(data_location):
    for filename in files:
        with open(os.path.join(root, filename), encoding=""utf8"",mode=""r"") as f:
            patient_name_check=re.compile(r"".*\s+(patient name)\s*:*\s*(.*)"",re.I)                
            for line_number, line in enumerate(f, 1):

                patient_name_matches=patient_name_check.findall(line)
                for match in patient_name_matches:

                    name_details=match[1]
                    tokens = name_details.split()
                    result=tagger.tag(tokens)
                    for m in result:
                        print(m)

name_detail_extracter()
</code></pre>
","python-3.x, nlp, stanford-nlp","<p>The issue has been resolved as there were some empty tokens getting passed to NER, so now I have put a check for them.</p>

<pre><code>for match in patient_name_matches:
    name_details=match[1]
    tokens = name_details.split()
    if tokens: # this is the check which I put
        result=tagger.tag(tokens)
        for m in result:
            print(m)
</code></pre>
",1,2,728,2018-08-27 00:44:48,https://stackoverflow.com/questions/52031337/stanfords-corenlp-name-entity-recogniser-throwing-error-500-server-error-inter
Tree structure from Stanford CoreNLP parser,"<p>I am trying to run StanfordCoreNLP parser and I have the following code:</p>

<pre><code>from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')

def depparse(text):
    parsed=""""
    output = nlp.annotate(text, properties={
      'annotators': 'depparse',
      'outputFormat': 'json'
      })

    for i in output[""sentences""]:
        for j in i[""basicDependencies""]:
            parsed=parsed+str(j[""dep""]+'('+ j[""governorGloss""]+' ')+str(j[""dependentGloss""]+')'+' ')
        return parsed
text='I shot an elephant in my sleep'
depparse(text)
</code></pre>

<p>This gives me output as:
<code>'ROOT(ROOT shot) nsubj(shot I) det(elephant an) dobj(shot elephant) case(sleep in) nmod:poss(sleep my) nmod(shot sleep) '</code></p>

<p>To convert the relationships into tree, I am encountered one stackoverflow post <a href=""https://stackoverflow.com/questions/34395127/stanford-nlp-parse-tree-format"">Stanford NLP parse tree format</a>. However, the output of the parser is in ""bracketed parse (tree)"". Hence, I am not sure how can I achieve it. I tried changing the outputformat as well but it gives an error.</p>

<p>I also found <a href=""https://stackoverflow.com/questions/34964878/python-generate-a-dictionarytree-from-a-list-of-tuples/35049729"">Python - Generate a dictionary(tree) from a list of tuples</a> and implemented</p>

<pre><code>    list_of_tuples = [('ROOT','ROOT', 'shot'),('nsubj','shot', 'I'),('det','elephant', 'an'),('dobj','shot', 'elephant'),('case','sleep', 'in'),('nmod:poss','sleep', 'my'),('nmod','shot', 'sleep')]

nodes={}

for i in list_of_tuples:
    rel,parent,child=i
    nodes[child]={'Name':child,'Relationship':rel}

forest=[]

for i in list_of_tuples:
    rel,parent,child=i
    node=nodes[child]

    if parent=='ROOT':# this should be the Root Node
            forest.append(node)
    else:
        parent=nodes[parent]
        if not 'children' in parent:
            parent['children']=[]
        children=parent['children']
        children.append(node)

print forest
</code></pre>

<p>I got the following output <code>[{'Name': 'shot', 'Relationship': 'ROOT', 'children': [{'Name': 'I', 'Relationship': 'nsubj'}, {'Name': 'elephant', 'Relationship': 'dobj', 'children': [{'Name': 'an', 'Relationship': 'det'}]}, {'Name': 'sleep', 'Relationship': 'nmod', 'children': [{'Name': 'in', 'Relationship': 'case'}, {'Name': 'my', 'Relationship': 'nmod:poss'}]}]}]</code></p>
","python, dictionary, nested, nlp, stanford-nlp","<p>A bit off-topic indeed (this is not really an answer to your original question, but to your last comment). Posting it as an answer because the code wouldn't really fit nicely into a comment. But by just changing your depparse function slightly, you can get it in the desired format:</p>

<pre><code>def depparse(text):
parsed=""""
output = nlp.annotate(text, properties={
  'annotators': 'depparse',
  'outputFormat': 'json'
  })
for i in output['sentences']: # not sure if there can be multiple items here. If so, it just returns the first one currently.
    return [tuple((dep['dep'], dep['governorGloss'], dep['dependentGloss'])) for dep in i['basicDependencies']]
</code></pre>
",0,0,1678,2018-09-01 19:04:53,https://stackoverflow.com/questions/52131295/tree-structure-from-stanford-corenlp-parser
How to split Japanese text?,"<p>What is the best way of splitting Japanese text using Java?
For Example, for the below text:</p>

<pre><code>こんにちは。私の名前はオバマです。私はアメリカに行く。
</code></pre>

<p>I need the following output:</p>

<pre><code>こんにちは
私の名前はオバマです
私はアメリカに行く
</code></pre>

<p>Is it possible using Kuromoji? </p>
","text, stanford-nlp, opennlp, cjk, kuromoji","<p>You can use java.text.BreakIterator.</p>

<pre><code>String TEXT = ""こんにちは。私の名前はオバマです。私はアメリカに行く。"";
BreakIterator boundary = BreakIterator.getSentenceInstance(Locale.JAPAN);
boundary.setText(TEXT);
int start = boundary.first();
for (int end = boundary.next();
     end != BreakIterator.DONE;
     start = end, end = boundary.next()) {
     System.out.println(TEXT.substring(start, end));
}
</code></pre>

<p>The output of this program is:</p>

<pre><code>こんにちは。
私の名前はオバマです。
私はアメリカに行く。
</code></pre>

<p>You cannot use Kuromoji to look for Japanese sentence boundaries. It can split a sentence into words.</p>
",4,3,1661,2018-09-03 08:31:22,https://stackoverflow.com/questions/52145954/how-to-split-japanese-text
How to make a tree from the output of a dependency parser?,"<p>I am trying to make a tree (nested dictionary) from the output of dependency parser. The sentence is ""I shot an elephant in my sleep"". I am able to get the output as described on the link:
<a href=""https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk"">How do I do dependency parsing in NLTK?</a></p>

<pre><code>nsubj(shot-2, I-1)
det(elephant-4, an-3)
dobj(shot-2, elephant-4)
prep(shot-2, in-5)
poss(sleep-7, my-6)
pobj(in-5, sleep-7)
</code></pre>

<p>To convert this list of tuples into nested dictionary, I used the following link:
<a href=""https://stackoverflow.com/questions/39495924/how-to-convert-python-list-of-tuples-into-tree"">How to convert python list of tuples into tree?</a></p>

<pre><code>def build_tree(list_of_tuples):
    all_nodes = {n[2]:((n[0], n[1]),{}) for n in list_of_tuples}
    root = {}    
    print all_nodes
    for item in list_of_tuples:
        rel, gov,dep = item
        if gov is not 'ROOT':
            all_nodes[gov][1][dep] = all_nodes[dep]
        else:
            root[dep] = all_nodes[dep]
    return root
</code></pre>

<p>This gives the output as follows:</p>

<pre><code>{'shot': (('ROOT', 'ROOT'),
  {'I': (('nsubj', 'shot'), {}),
   'elephant': (('dobj', 'shot'), {'an': (('det', 'elephant'), {})}),
   'sleep': (('nmod', 'shot'),
    {'in': (('case', 'sleep'), {}), 'my': (('nmod:poss', 'sleep'), {})})})}
</code></pre>

<p>To find the root to leaf path, I used the following link: <a href=""https://stackoverflow.com/questions/47302382/return-root-to-specific-leaf-from-a-nested-dictionary-tree"">Return root to specific leaf from a nested dictionary tree</a></p>

<p>[Making the tree and finding the path are two separate things]The second objective is to find the root to leaf node path like done <a href=""https://stackoverflow.com/questions/47302382/return-root-to-specific-leaf-from-a-nested-dictionary-tree]"">Return root to specific leaf from a nested dictionary tree</a>. 
But I want to get the root-to-leaf (dependency relationship path)
So, for instance, when I will call recurse_category(categories, 'an') where categories is the nested tree structure and 'an' is the word in the tree, I should get <code>ROOT-nsubj-dobj</code> (dependency relationship till root) as output.</p>
","python, dictionary, nlp, nltk, stanford-nlp","<p>This converts the output to the nested dictionary form. I will keep you updated if I can find the path as well. Maybe this, is helpful.</p>

<pre><code>list_of_tuples = [('ROOT','ROOT', 'shot'),('nsubj','shot', 'I'),('det','elephant', 'an'),('dobj','shot', 'elephant'),('case','sleep', 'in'),('nmod:poss','sleep', 'my'),('nmod','shot', 'sleep')]

nodes={}

for i in list_of_tuples:
    rel,parent,child=i
    nodes[child]={'Name':child,'Relationship':rel}

forest=[]

for i in list_of_tuples:
    rel,parent,child=i
    node=nodes[child]

    if parent=='ROOT':# this should be the Root Node
            forest.append(node)
    else:
        parent=nodes[parent]
        if not 'children' in parent:
            parent['children']=[]
        children=parent['children']
        children.append(node)

print forest
</code></pre>

<p>The output is a nested dictionary,</p>

<p><code>[{'Name': 'shot', 'Relationship': 'ROOT', 
                                  'children': 
                                            [{'Name': 'I', 'Relationship': 'nsubj'}, 
                                            {'Name': 'elephant', 'Relationship': 
                                             'dobj', 
                                            'children': 
                                                      [{'Name': 'an', 
                                                      'Relationship': 'det'}]}, 
                                            {'Name': 'sleep', 'Relationship': 
                                            'nmod', 
                                            'children': 
                                                      [{'Name': 'in', 
                                                      'Relationship': 'case'}, 
                                                      {'Name': 'my', 'Relationship': 
                                                      'nmod:poss'}]}]}]</code></p>

<p>The following function can help you to find the root-to-leaf path:</p>

<pre><code>def recurse_category(categories,to_find):
    for category in categories: 
        if category['Name'] == to_find:
            return True, [category['Relationship']]
        if 'children' in category:
            found, path = recurse_category(category['children'], to_find)
            if found:
                return True, [category['Relationship']] + path
    return False, []
</code></pre>
",0,5,4398,2018-09-03 11:17:06,https://stackoverflow.com/questions/52148690/how-to-make-a-tree-from-the-output-of-a-dependency-parser
There should be an edit in installation manual for zsh and Mac users,"<p>Zsh doesn't recognise 'realpath', so 'readlink' should be used instead. </p>

<p>So that, instead:</p>

<pre><code>export CLASSPATH=""$CLASSPATH:javanlp-core.jar:stanford-corenlp-models-current.jar"";
for file in `find lib -name ""*.jar""`; do export CLASSPATH=""$CLASSPATH:`realpath $file`""; done
</code></pre>

<p>there should be:</p>

<pre><code>export CLASSPATH=""$CLASSPATH:javanlp-core.jar:stanford-corenlp-models-current.jar"";
for file in `find lib -name ""*.jar""`; do export CLASSPATH=""$CLASSPATH:`readlink $file`""; done
</code></pre>

<p>It would be nice if Stanford-NLP site maintainers mentioned it in the installation manual. Thanks!</p>
",stanford-nlp,"<p>I don't think this is a Zsh issue; <code>realpath</code> is a program, not a builtin. On my mac, it lives at:</p>

<pre><code>/usr/local/bin/realpath
</code></pre>

<p>I believe you can install it with:</p>

<pre><code>brew install coreutils
</code></pre>
",0,0,31,2018-09-04 16:27:56,https://stackoverflow.com/questions/52170533/there-should-be-an-edit-in-installation-manual-for-zsh-and-mac-users
EM score in SQuAD Challenge,"<p>The <a href=""https://rajpurkar.github.io/SQuAD-explorer/"" rel=""noreferrer"">SQuAD Challenge</a> ranks the results against the F1 and EM scores. There is a lot of information about the F1 score (a function of precision and recall). But what would the EM score be?</p>
","tensorflow, machine-learning, deep-learning, stanford-nlp, reinforcement-learning","<blockquote>
<p><strong>Exact match.</strong> This  metric  measures  the  percentage of predictions
that  match any one of the ground truth answers exactly.</p>
</blockquote>
<p>According to <a href=""https://arxiv.org/pdf/1606.05250.pdf"" rel=""noreferrer"">here</a>.</p>
",25,19,9058,2018-09-07 20:09:29,https://stackoverflow.com/questions/52229059/em-score-in-squad-challenge
Why do CoreNLP ner tagger and ner tagger join the separated numbers together?,"<p>Here is the code snippet:   </p>

<pre><code>In [390]: t
Out[390]: ['my', 'phone', 'number', 'is', '1111', '1111', '1111']

In [391]: ner_tagger.tag(t)
Out[391]: 
[('my', 'O'),
 ('phone', 'O'),
 ('number', 'O'),
 ('is', 'O'),
 ('1111\xa01111\xa01111', 'NUMBER')]
</code></pre>

<p>What I expect is: </p>

<pre><code>Out[391]: 
[('my', 'O'),
 ('phone', 'O'),
 ('number', 'O'),
 ('is', 'O'),
 ('1111', 'NUMBER'),
 ('1111', 'NUMBER'),
 ('1111', 'NUMBER')]
</code></pre>

<p>As you can see the artificial phone number is joined by \xa0 which is said to be a non-breaking space. Can I separate that by setting the CoreNLP without changing other default rules. </p>

<p>The ner_tagger is defined as: </p>

<pre><code>ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
</code></pre>
","python, nlp, nltk, stanford-nlp, pycorenlp","<h1>TL;DR</h1>

<p>NLTK was reading the list of tokens into a string and before passing it to the CoreNLP server. And CoreNLP retokenize the inputs and concatenated the number-like tokens with <code>\xa0</code> (non-breaking space).</p>

<hr>

<h1>In Long</h1>

<p>Lets walk through the code, if we look at the <code>tag()</code> function from <code>CoreNLPParser</code>, we see that it calls the <code>tag_sents()</code> function and converted the input list of strings into a string before calling the <code>raw_tag_sents()</code> which allows <code>CoreNLPParser</code> to re-tokenized the input, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L348"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L348</a>:</p>

<pre><code>def tag_sents(self, sentences):
    """"""
    Tag multiple sentences.
    Takes multiple sentences as a list where each sentence is a list of
    tokens.

    :param sentences: Input sentences to tag
    :type sentences: list(list(str))
    :rtype: list(list(tuple(str, str))
    """"""
    # Converting list(list(str)) -&gt; list(str)
    sentences = (' '.join(words) for words in sentences)
    return [sentences[0] for sentences in self.raw_tag_sents(sentences)]

def tag(self, sentence):
    """"""
    Tag a list of tokens.
    :rtype: list(tuple(str, str))
    &gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
    &gt;&gt;&gt; tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
    &gt;&gt;&gt; parser.tag(tokens)
    [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
    ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]
    &gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
    &gt;&gt;&gt; tokens = ""What is the airspeed of an unladen swallow ?"".split()
    &gt;&gt;&gt; parser.tag(tokens)
    [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
    ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
    ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
    """"""
    return self.tag_sents([sentence])[0]
</code></pre>

<p>And when calling then the <code>raw_tag_sents()</code> passes the input to the server using the <code>api_call()</code>:</p>

<pre><code>def raw_tag_sents(self, sentences):
    """"""
    Tag multiple sentences.
    Takes multiple sentences as a list where each sentence is a string.

    :param sentences: Input sentences to tag
    :type sentences: list(str)
    :rtype: list(list(list(tuple(str, str)))
    """"""
    default_properties = {'ssplit.isOneSentence': 'true',
                          'annotators': 'tokenize,ssplit,' }

    # Supports only 'pos' or 'ner' tags.
    assert self.tagtype in ['pos', 'ner']
    default_properties['annotators'] += self.tagtype
    for sentence in sentences:
        tagged_data = self.api_call(sentence, properties=default_properties)
        yield [[(token['word'], token[self.tagtype]) for token in tagged_sentence['tokens']]
                for tagged_sentence in tagged_data['sentences']]
</code></pre>

<p><strong>So the question is how to resolve the problem and get the tokens as it's passed in?</strong></p>

<p>If we look at the options for the Tokenizer in CoreNLP, we see the <code>tokenize.whitespace</code> option:</p>

<ul>
<li><a href=""https://stanfordnlp.github.io/CoreNLP/tokenize.html#options"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokenize.html#options</a></li>
<li><a href=""https://stackoverflow.com/questions/36440495/preventing-tokens-from-containing-a-space-in-stanford-corenlp"">Preventing tokens from containing a space in Stanford CoreNLP</a> </li>
</ul>

<p>If we make some changes to the allow additional <code>properties</code> before calling <code>api_call()</code>, we can enforce the tokens as it's passed to the CoreNLP server joined by whitespaces, e.g. changes to the code:</p>

<pre><code>def tag_sents(self, sentences, properties=None):
    """"""
    Tag multiple sentences.

    Takes multiple sentences as a list where each sentence is a list of
    tokens.

    :param sentences: Input sentences to tag
    :type sentences: list(list(str))
    :rtype: list(list(tuple(str, str))
    """"""
    # Converting list(list(str)) -&gt; list(str)
    sentences = (' '.join(words) for words in sentences)
    if properties == None:
        properties = {'tokenize.whitespace':'true'}
    return [sentences[0] for sentences in self.raw_tag_sents(sentences, properties)]

def tag(self, sentence, properties=None):
    """"""
    Tag a list of tokens.

    :rtype: list(tuple(str, str))

    &gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
    &gt;&gt;&gt; tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
    &gt;&gt;&gt; parser.tag(tokens)
    [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
    ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]

    &gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
    &gt;&gt;&gt; tokens = ""What is the airspeed of an unladen swallow ?"".split()
    &gt;&gt;&gt; parser.tag(tokens)
    [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
    ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
    ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
    """"""
    return self.tag_sents([sentence], properties)[0]

def raw_tag_sents(self, sentences, properties=None):
    """"""
    Tag multiple sentences.

    Takes multiple sentences as a list where each sentence is a string.

    :param sentences: Input sentences to tag
    :type sentences: list(str)
    :rtype: list(list(list(tuple(str, str)))
    """"""
    default_properties = {'ssplit.isOneSentence': 'true',
                          'annotators': 'tokenize,ssplit,' }

    default_properties.update(properties or {})

    # Supports only 'pos' or 'ner' tags.
    assert self.tagtype in ['pos', 'ner']
    default_properties['annotators'] += self.tagtype
    for sentence in sentences:
        tagged_data = self.api_call(sentence, properties=default_properties)
        yield [[(token['word'], token[self.tagtype]) for token in tagged_sentence['tokens']]
                for tagged_sentence in tagged_data['sentences']]
</code></pre>

<p>After changing the above code:</p>

<pre><code>&gt;&gt;&gt; from nltk.parse.corenlp import CoreNLPParser
&gt;&gt;&gt; ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
&gt;&gt;&gt; sent = ['my', 'phone', 'number', 'is', '1111', '1111', '1111']
&gt;&gt;&gt; ner_tagger.tag(sent)
[('my', 'O'), ('phone', 'O'), ('number', 'O'), ('is', 'O'), ('1111', 'DATE'), ('1111', 'DATE'), ('1111', 'DATE')]
</code></pre>
",1,2,442,2018-09-10 02:19:35,https://stackoverflow.com/questions/52250268/why-do-corenlp-ner-tagger-and-ner-tagger-join-the-separated-numbers-together
How to Get the Probability value of all the entity mentions,"<p>Lets consider this text:</p>

<blockquote>
  <p><em>""John Joseph lives in America. He works in federal government""</em></p>
</blockquote>

<p>I am getting the following entity mentions for this text:</p>

<ol>
<li><code>John Joseph</code></li>
<li><code>America</code></li>
<li><code>He</code></li>
<li><code>Federal government</code></li>
</ol>

<p>Now I am trying to get the probability value for each entity mentions. When I look at the <code>Java APIs</code>, I noticed we can get the probability value for each tokens (John, Joseph, America, He, Federal &amp; Government). </p>

<p>But how do we get the probability value of ""John Joseph"" &amp; ""Federal Government"" (More than one token)? Is there any Java API available to get this value?</p>

<p>Any help would be appreciated.</p>
",stanford-nlp,"<p>We are going to add a new feature in Stanford CoreNLP 3.9.2 that attaches the minimal token-label probability for the entity.  We won't provide any score for anything derived via rules though, because it's not clear what confidence would be appropriate.  So for instance ""Joe Smith"" might get the probability that the ""Joe"" token has for PERSON.  This seems like a reasonable heuristic.</p>

<p>We aim for this new release to be out before the end of the month!  I'll update this answer with more details upon release.</p>
",0,0,61,2018-09-12 16:41:28,https://stackoverflow.com/questions/52299938/how-to-get-the-probability-value-of-all-the-entity-mentions
`nltk` CoreNLPParser: prevent splitting at hyphens in POS tagger,"<p>I am using the <code>nltk</code> <code>CoreNLPParser</code> with the Stanford NLP server for POS tagging as described in <a href=""https://stackoverflow.com/a/51981566/3881984"">this answer</a>.</p>

<p>This tagger treats words with hyphens as multiple words, for example dates like <code>2007-08</code> are tagged as <code>CP, :, CP</code>. However, my model uses words with hyphen as one token. Is it possible using the <code>CoreNLPParser</code> to prevent splitting at hyphens?</p>
","python, parsing, nlp, nltk, stanford-nlp","<h1>TL;DR</h1>

<pre><code>from nltk.parse.corenlp import GenericCoreNLPParser

class CoreNLPParser(GenericCoreNLPParser):
    _OUTPUT_FORMAT = 'penn'
    parser_annotator = 'parse'

    def make_tree(self, result):
        return Tree.fromstring(result['parse'])

    def tag_sents(self, sentences, properties=None):
        """"""
        Tag multiple sentences.

        Takes multiple sentences as a list where each sentence is a list of
        tokens.

        :param sentences: Input sentences to tag
        :type sentences: list(list(str))
        :rtype: list(list(tuple(str, str))
        """"""
        # Converting list(list(str)) -&gt; list(str)
        sentences = (' '.join(words) for words in sentences)
        if properties == None:
            properties = {'tokenize.whitespace':'true'}
        return [sentences[0] for sentences in self.raw_tag_sents(sentences, properties)]

    def tag(self, sentence, properties=None):
        """"""
        Tag a list of tokens.

        :rtype: list(tuple(str, str))

        &gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
        &gt;&gt;&gt; tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
        &gt;&gt;&gt; parser.tag(tokens)
        [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
        ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]

        &gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
        &gt;&gt;&gt; tokens = ""What is the airspeed of an unladen swallow ?"".split()
        &gt;&gt;&gt; parser.tag(tokens)
        [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
        ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
        ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
        """"""
        return self.tag_sents([sentence], properties)[0]

    def raw_tag_sents(self, sentences, properties=None):
        """"""
        Tag multiple sentences.

        Takes multiple sentences as a list where each sentence is a string.

        :param sentences: Input sentences to tag
        :type sentences: list(str)
        :rtype: list(list(list(tuple(str, str)))
        """"""
        default_properties = {'ssplit.isOneSentence': 'true',
                              'annotators': 'tokenize,ssplit,' }

        default_properties.update(properties or {})

        # Supports only 'pos' or 'ner' tags.
        assert self.tagtype in ['pos', 'ner']
        default_properties['annotators'] += self.tagtype
        for sentence in sentences:
            tagged_data = self.api_call(sentence, properties=default_properties)
            yield [[(token['word'], token[self.tagtype]) for token in tagged_sentence['tokens']]
                    for tagged_sentence in tagged_data['sentences']]

pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
sent = ['My', 'birthday', 'is', 'on', '09-12-2050']
print(pos_tagger.tag(sent))
</code></pre>

<p>[out]:</p>

<pre><code>[('My', 'PRP$'), ('birthday', 'NN'), ('is', 'VBZ'), ('on', 'IN'), ('09-12-2050', 'CD')]
</code></pre>

<hr>

<h1>In Long</h1>

<p>See </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/52250268/why-do-corenlp-ner-tagger-and-ner-tagger-join-the-separated-numbers-together"">Why do CoreNLP ner tagger and ner tagger join the separated numbers together?</a></li>
<li><a href=""https://github.com/nltk/nltk/issues/2112"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/issues/2112</a></li>
</ul>
",1,1,595,2018-09-24 14:47:10,https://stackoverflow.com/questions/52481933/nltk-corenlpparser-prevent-splitting-at-hyphens-in-pos-tagger
extract name entity from unstructured data,"<p>I have highly unstructured data and I want to extract full name out of It. The data is something like this </p>

<pre><code>txt = "" 663555 murphy rd suite 106 richardson tx 7508 usa 111 it park indore 452 010 india ph 91 987 4968420 123456789 sumeetlogikviewcom  Nirali Khoda cofounder analytics pvt ltd ideata  a comprehensive data analytics platform""

text = ""dicictay  8 8 8 bf infotech pvt ltd manager infotech pvt ltd  redefining technologies 91 12345 12345 zoeb fatemi ""
</code></pre>

<p>I tried spacy and standfordNER but It is not giving good results. It gives me name from address like this </p>

<pre><code>en = spacy.load('en_core_web_md')

txt = txt.title().strip()

sents = en(txt)

people = [ee for ee in sents.ents if ee.label_ == 'PERSON']
</code></pre>

<p>out put is this :</p>

<pre><code>[663555 Murphy Rd Suite, Analytics Pvt Ltd Ideata]
</code></pre>

<p>expected output :</p>

<pre><code>[Nirali Khoda]
</code></pre>

<p>Help would be appreciated. Thanks :) </p>
","python-3.x, nlp, stanford-nlp, spacy","<p>Before I start, I want to mention that I used <code>spacy.load(""en_core_web_lg"")</code> for my code instead - this seems to affect the results of the parsing quite significantly, so this could be an initial approach for your problem. <br/>
I also had no running installation of StanfordNER locally, so I used their provided <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">web interface instead</a>.</p>

<p>The problem with NER is problematic in this case - as you kind of already mentioned - due to the fact that your ""sentences"" lack any cohesive structure. The problem is that mos NER accuracy comes from <em>context information</em> which is clearly lacking in your case. <br/>
This is also nicely visualized by parsing one of the sentences from your examples in the web interface mentioned above: The parsed sentence tree looks very scary (obviously), and there is not much we can take from there.</p>

<p>I also parsed the first sentence with SpaCy, and got the following result when looking at the recognized entities:</p>

<pre><code>663555 DATE
106 Richardson PERSON
Tx GPE
7508 DATE
Usa GPE
111 CARDINAL
Park Indore GPE
452 010 CARDINAL
India GPE
91 CARDINAL
987 CARDINAL
123456789 DATE
Sumeetlogikviewcom PERSON
Nirali Khoda Cofounder Analytics Pvt Ltd Ideata ORG
Comprehensive Data Analytics Platform ORG
</code></pre>

<p>As we can see, the problem is two-fold here: Not only is the instance with your name in it mislabeled (<code>ORG</code> instead of <code>PERSON</code>), but it also shows that the initial split into different entities is problematic.</p>

<p>I am assuming that you have some way of accessing the data extraction pipeline, and are not ""blindly"" taking these from somewhere else. This is specifically important so you can introduce some form of separation between different containers; albeit most preprocessor have some form of boilerblate (that removes HTML tags and ""unifies"" them), some form of separation might do you good: I slightly altered your input to the following:</p>

<pre><code>txt = "" 663555 murphy rd suite 106 richardson tx 7508 usa , 111 it park indore 452 010 india ph 91 987 4968420 123456789 , sumeetlogikviewcom ,  Nirali Khoda , cofounder analytics pvt ltd , ideata  a comprehensive data analytics platform""
</code></pre>

<p>Then, I performed the same processing again, and - look at that - ended up with the following result:</p>

<pre><code>663555 DATE
106 Richardson PERSON
Tx GPE
7508 DATE
Usa GPE
111 CARDINAL
Park Indore GPE
452 010 CARDINAL
India GPE
91 CARDINAL
987 CARDINAL
123456789 DATE
Sumeetlogikviewcom PERSON
Nirali Khoda PERSON
Cofounder Analytics Pvt Ltd ORG
Ideata   ORG
</code></pre>

<p>This time, the result is both correctly split up, as well as (more) correctly classified. Obviously you are still not getting perfect results, but that is seldomly the case with NER.</p>

<p>If you want to only recognize names, you can also ""manually parse"" them, regardless of the underlying entities, with a more crude approach: You might want to let SpaCy or CoreNLP split the different entities, and then - regardless of the actual tag - check for each entity whether it contains a token that is contained in a set of common first/last names (data for the U.S., for example, can be found <a href=""https://names.mongabay.com/"" rel=""nofollow noreferrer"">here</a>). I am sure there exist more comprehensive lists, and this might be a good substitution, if you are literally only looking for names. Of course, this is unlikely to perfectly solve your problem as well (think of Toyota, which is incidentally also a very common last name in Japanese; or something like Mr. Propper, which (to a computer) might as well be a ""person"" as well).</p>
",2,1,1209,2018-09-26 07:22:10,https://stackoverflow.com/questions/52512144/extract-name-entity-from-unstructured-data
What&#39;s the difference between the &#39;originalText&#39; and &#39;word&#39; keys in a token?,"<p>When using <code>CoreNLPParser</code> from <code>NLTK</code> with <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">CoreNLP Server</a>, the resulting <em>tokens</em> contain both an <code>'originalText'</code> key and a <code>'word'</code> key.</p>

<p><strong>What's the difference between the two? Is there any documentation about them?</strong></p>

<p>I've only found <a href=""https://github.com/nltk/nltk/issues/1746"" rel=""nofollow noreferrer"">this issue</a>, which mentioned the <code>origintalText</code> key, but it doesn't answer my questions.</p>

<pre><code>from nltk.parse.corenlp import CoreNLPParser 

corenlp_parser = CoreNLPParser('http://localhost:9000', encoding='utf8')
text = u'我家没有电脑。'

result = corenlp_parser.api_call(text, {'annotators': 'tokenize,ssplit'})
print(result)
</code></pre>

<p>prints</p>

<pre><code>{
   ""sentences"":[
      {
         ""index"":0,
         ""tokens"":[
            {
               ""index"":1,
               ""word"":""我家"",
               ""originalText"":""我家"",
               ""characterOffsetBegin"":0,
               ""characterOffsetEnd"":2
            },
            {
               ""index"":2,
               ""word"":""没有"",
               ""originalText"":""没有"",
               ""characterOffsetBegin"":2,
               ""characterOffsetEnd"":4
            },
            {
               ""index"":3,
               ""word"":""电脑"",
               ""originalText"":""电脑"",
               ""characterOffsetBegin"":4,
               ""characterOffsetEnd"":6
            },
            {
               ""index"":4,
               ""word"":""。"",
               ""originalText"":""。"",
               ""characterOffsetBegin"":6,
               ""characterOffsetEnd"":7
            }
         ]
      }
   ]
}
</code></pre>

<p>Update:</p>

<p>It seems the <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/simple/Token.html"" rel=""nofollow noreferrer""><code>Token</code></a> implements <code>HasWord</code> and <code>HasOriginalText</code></p>
","nltk, stanford-nlp","<p>A <code>word</code> is transformed a little bit to make it, e.g., possible to print it in an S-Expression (i.e., a parse tree). So, parentheses and other braces become tokens like <code>-LRB-</code> (left round brace). In addition, quotes are normalized to be backticks (``) and forward ticks ('') and some other little things.</p>

<p><code>originalText</code>, by contrast, is the literal original text of the token that can be used to reconstruct the original sentence.</p>
",3,2,162,2018-09-28 11:46:06,https://stackoverflow.com/questions/52554675/whats-the-difference-between-the-originaltext-and-word-keys-in-a-token
missing stop words from spacy en_core_web_lg,"<p>I downloaded <a href=""https://github.com/explosion/spacy-models/releases//tag/en_core_web_lg-2.0.0"" rel=""nofollow noreferrer"">en_core_web_lg</a>(en_core_web_lg-2.0.0) but when I load it and used it on spacy. But it seems to miss lots of basic common stop words such as ""be"", ""a"" etc. Am I missing correct version ?</p>

<pre><code>import nltk
n = nltk.corpus.stopwords.words('english')
""be"" in n
</code></pre>

<p>O/P: True</p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_lg"")
nlp.vocab[""be""].is_stop
</code></pre>

<p>O/P: False</p>
","python, nlp, nltk, stanford-nlp, spacy","<p>You've probably run into this bug:</p>

<p><a href=""https://github.com/explosion/spaCy/issues/1574"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/1574</a></p>

<p>Stop words are missing for the en_core_web_md and en_core_web_lg models, but your code will work as expected with en_core_web_sm.</p>
",2,1,1356,2018-09-30 13:29:00,https://stackoverflow.com/questions/52578323/missing-stop-words-from-spacy-en-core-web-lg
How to link NE with it&#39;s dependent?,"<p>I need to extract descriptions of locations from a text. For now, I am trying to get location with it's adjectival modifier.<br>
For example from  </p>

<blockquote>
  <p>In compact Durham you don't need transport to get around.  </p>
</blockquote>

<p>I want to get  </p>

<blockquote>
  <p>compact Durham  </p>
</blockquote>

<p>I have <code>CoreEntityMention</code> and <code>SemanticGraph</code> of my sentence. I can get index of NE's token to find <code>IndexedWord</code> in <code>SemanticGraph</code>, but NE may contain more than one token so I don't know hot to build the link. I saw <a href=""https://stackoverflow.com/questions/40479342/how-to-extract-named-entity-verb-from-text"">this similar question</a>, but didn't understand suggested solution. Do need I to check dependence for each token?<br>
Here is my approach written in Kotlin (no big difference from Java):  </p>

<pre><code>    val dependencies = mutableListOf&lt;String&gt;()
    val depGraph = entityMention.sentence().dependencyParse()

    for (token in entityMention.tokens()) {
        val node = depGraph.getNodeByIndex(token.index())
        for (dependence in depGraph.childPairs(node)) {
            if (dependence.first.shortName == ""amod"") {
                dependencies.add(dependence.second.toString())
            }
        }
    }
</code></pre>

<p>Is it correct and simplest way?</p>
","nlp, stanford-nlp, named-entity-recognition","<p>Yes, at the moment I think the best thing you can do is iterate through the tokens of the entity mention since dependencies exist between tokens.</p>

<p>I'll note this question, and maybe we can add some code to make this easier in the future.</p>
",0,0,49,2018-10-08 17:46:24,https://stackoverflow.com/questions/52707579/how-to-link-ne-with-its-dependent
Can&#39;t use OLLIE open information extraction method in Stanford Core NLP OpenIE,"<p>I am facing trying to extract triples using OLLIE with Stanford Core NLP's OpenIE tools.</p>

<p>I've installed both <strong>stanford-corenlp-3.9.1</strong> as well <strong>stanford-corenlp-3.9.2</strong> to try to extract triples from text.</p>

<p>For <strong>stanford-corenlp-3.9.1</strong>:</p>

<ul>
<li>Can only extract information using default method, despite adding the flag ""-format ollie"" or ""-openie.format ollie""</li>
<li><p>I've tested it with this sentence</p>

<blockquote>
  <p>Some people say Barack Obama was not born in the United States.</p>
</blockquote>

<p>Which should yield this:</p>

<blockquote>
  <p>(Barack Obama; was not born in; the United States)[attrib=Some people say]</p>
</blockquote>

<p>This is the example to test if the OpenIE methids is indeed ollie. But I get no triples instead. It does work for other sentences however, but the output is that of the default method.</p></li>
</ul>

<p>For <strong>stanford-corenlp-3.9.2</strong>:</p>

<ul>
<li><p>I was unable to extract any triples at all, but get this error instead.</p>

<pre><code>'java.lang.IllegalArgumentException: annotator ""openie"" requires annotation ""CorefChainAnnotation"". The usual requirements for this annotator are: tokenize,ssplit,pos,lemma,depparse,natlog'
</code></pre></li>
</ul>

<p>EDITED:</p>

<ol>
<li>Turns out OLLIE wasn't supported in Stanford OpenIE, and the flags merely changes the output to in OLLIE's format instead.</li>
<li>Able to run 3.9.2 version (see reply below).</li>
</ol>
","nlp, stanford-nlp","<p>So, Stanford OpenIE is not the same as Ollie; it just has an option to output in a format that is similar to (technically a subset of) the Ollie format.</p>

<p>The Stanford OpenIE system is described in <a href=""https://nlp.stanford.edu/pubs/2015angeli-openie.pdf"" rel=""nofollow noreferrer"">Angeli et al. ""Leveraging Linguistic Structure For Open Domain Information Extraction""</a>. Ollie is described in <a href=""https://www.aclweb.org/anthology/D12-1048"" rel=""nofollow noreferrer"">Mausam et al. ""Open Language Learning for Information Extraction""</a>.</p>

<p>RE the missed extraction: Stanford's system models negation and false statements as a first order phenomenon, where it won't extract negated facts. This is to avoid cases where the downstream application has to disambiguate between a negated relation and a non-negated relation (e.g., what if a relation is in a double-negative context?). Therefore, both because of the ""some people say"" modifier and because of the negation, the system doesn't return anything.</p>

<p>RE the exception: you're missing <code>mention,coref</code> as an annotator in your annotators list. Are you calling this from the command line, or from the annotation pipeline? If from the command line, can you include the command you used to run the program?</p>
",1,0,525,2018-10-10 20:45:36,https://stackoverflow.com/questions/52748528/cant-use-ollie-open-information-extraction-method-in-stanford-core-nlp-openie
Python NLTK: Stanford NER tagger error message: NLTK was unable to find the java file,"<p>Trying to get Stanford NER working with Python. Followed some instructions on the web, but got the error message: ""NLTK was unable to find the java file!
Use software specific configuration paramaters or set the JAVAHOME environment variable."" What was wrong? Thank you! </p>

<pre><code>from nltk.tag.stanford import StanfordNERTagger
from nltk.tokenize import word_tokenize

model = r'C:\Stanford\NER\classifiers\english.muc.7class.distsim.crf.ser.gz'
jar = r'C:\Stanford\NER\stanford-ner-3.9.1.jar'

ner_tagger = StanfordNERTagger(model, jar, encoding = 'utf-8')

text = 'While in France, Christine Lagarde discussed short-term stimulus ' \
       'efforts in a recent interview with the Wall Street Journal.'

words = word_tokenize(text)
classified_words = ner_tagger.tag(words)
</code></pre>
","python, nltk, stanford-nlp, named-entity-recognition","<p>Found the solution on the web. Replace the path with your own. </p>

<blockquote>
<pre><code> import os

 java_path = ""C:/../../jdk1.8.0_101/bin/java.exe""   
 os.environ['JAVAHOME'] = java_path
</code></pre>
</blockquote>

<p>or:</p>

<blockquote>
<pre><code>import nltk

nltk.internals.config_java('C:/../../jdk1.8.0_101/bin/java.exe')
</code></pre>
</blockquote>

<p>Source: <a href=""https://tianyouhu.wordpress.com/2016/09/01/problem-of-nltk-with-stanfordtokenizer/"" rel=""nofollow noreferrer"">https://tianyouhu.wordpress.com/2016/09/01/problem-of-nltk-with-stanfordtokenizer/</a></p>
",0,0,1316,2018-10-18 04:21:36,https://stackoverflow.com/questions/52866988/python-nltk-stanford-ner-tagger-error-message-nltk-was-unable-to-find-the-java
What are the preprocessing steps to be taken before passing text into Stanford NER tagger?,"<p>Initially I had followed preprocessing steps like, stop words removal, HTML stripping, removing punctuation. However when I don't do this, the NER seems to perform better. Can anyone tell me what are preprocessing steps to be followed?</p>
","python, nlp, stanford-nlp","<p>The only thing StanfordNER needs is clean text, by clean I mean, no HTML or any other kind of document meta-tags. Also, you shouldn't remove stop-words, these might be useful for the model in deciding which label to give to a certain word.</p>

<p>Just have a file with clean text:</p>

<pre><code>echo ""Switzerland, Davos 2018: Soros accuses Trump of wanting a 'mafia state' and blasts social media."" &gt; test_file.txt
</code></pre>

<p>Then you will call stanford-ner.jar a pass it a trained model, e.g: <code>classifiers/english.all.3class.distsim.crf.ser.gz</code> and an input file, e.g.: <code>test_file.txt</code></p>

<p>Like this:</p>

<pre><code>java -cp stanford-ner-2017-06-09/stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile test_file.txt -outputFormat inlineXML
</code></pre>

<p>This should output something like this:</p>

<pre><code>Switzerland LOCATION
,   O
Davos   PERSON
2018    O
:   O
Soros   PERSON
accuses O
Trump   PERSON
of  O
wanting O
a   O
`   O
mafia   O
state   O
'   O
and O
blasts  O
social  O
media   O
.   O
</code></pre>

<p>As you can see you don't even need to handle tokenisation (e.g., find each unique token/word in the sentence) StanfordNER does that for you.</p>

<p>Another useful feature is to set up StanfordNER as a webservice:</p>

<pre><code>java -mx2g -cp stanford-ner-2017-06-09/stanford-ner.jar edu.stanford.nlp.ie.NERServer -loadClassifier my_model.ser.gz -textFile -port 9191 -outputFormat inlineXML
</code></pre>

<p>Then you can simple telnet or POST a sentence a get it back tagged:</p>

<pre><code>telnet 127.0.0.1 9191
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
Switzerland, Davos 2018: Soros accuses Trump of wanting a 'mafia state' and blasts social media.

&lt;LOCATION&gt;Switzerland&lt;/LOCATION&gt;, &lt;PERSON&gt;Davos&lt;/PERSON&gt; 2018: &lt;PERSON&gt;Soros&lt;/PERSON&gt; accuses &lt;PERSON&gt;Trump&lt;/PERSON&gt; of wanting a 'mafia state' and blasts social media.

Connection closed by foreign host.
</code></pre>
",1,0,355,2018-10-27 01:13:43,https://stackoverflow.com/questions/53017947/what-are-the-preprocessing-steps-to-be-taken-before-passing-text-into-stanford-n
Coreference Resolution with CoreNLP,"<p>I am trying to get CoreNLP to access CorefChains. My intention is that words like ""he, she, ..."" will be substituted by their best mention, but I am not able to access the CorefChains (they are always null). </p>

<pre><code>    public static void main (String [] args) {
         Properties props = new Properties();
         props.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,dcoref"");
         props.put(""dcoref.score"", true);
         StanfordCoreNLP corefPipeline = new StanfordCoreNLP(props);
         String text = ""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."";
         Annotation document = new Annotation(text);
         corefPipeline.annotate(document);
         // Chains is always null
         Map&lt;Integer, CorefChain&gt; chains = document.get(CorefCoreAnnotations.CorefChainAnnotation.class);
}
</code></pre>
","java, stanford-nlp, nlp","<p>I think it is an import classes issue. This one is working fine:</p>

<pre><code>import java.util.Map;
import java.util.Properties;

import edu.stanford.nlp.coref.CorefCoreAnnotations;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;


public class App {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,dcoref"");
        props.put(""dcoref.score"", true);
        StanfordCoreNLP corefPipeline = new StanfordCoreNLP(props);
        String text = ""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."";
        Annotation document = new Annotation(text);
        corefPipeline.annotate(document);
        // Chains is always null
        Map&lt;Integer, CorefChain&gt; chains = document.get(CorefCoreAnnotations.CorefChainAnnotation.class);
        System.out.println(chains);
    }
}
</code></pre>

<p>And the output:</p>

<pre><code>{1=CHAIN1-[""Barack Obama"" in sentence 1, ""He"" in sentence 2, ""the president"" in sentence 2, ""Obama"" in sentence 3], 2=CHAIN2-[""Hawaii"" in sentence 1], 6=CHAIN6-[""2008"" in sentence 3]}
</code></pre>

<p>Here is what I have in pom.xml:</p>

<pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.9.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.9.2&lt;/version&gt;
        &lt;classifier&gt;models&lt;/classifier&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>
",0,0,465,2018-10-27 12:28:48,https://stackoverflow.com/questions/53021903/coreference-resolution-with-corenlp
CoreNLP: seek out if a word is singular or plural,"<p>I want to to test if a word is singular or plural. Given a word like ""her"" or ""ours"", both words share the same part of speech tag (""PRP$""). Is there an easy way to tell by the token?</p>
","java, stanford-nlp","<p>He is an example that parses a sentence and, for each token found, decides whether is singular or plural. Basically, the idea is to use the lemmatizing capabilities:</p>

<pre><code>import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class StanfordLemmatizer {

    protected StanfordCoreNLP pipeline;

    public StanfordLemmatizer() {
        // Create StanfordCoreNLP object properties, with POS tagging
        // (required for lemmatization), and lemmatization
        Properties props;
        props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma"");

        // StanfordCoreNLP loads a lot of models, so you probably
        // only want to do this once per execution
        this.pipeline = new StanfordCoreNLP(props);
    }

    public void lemmatize(String documentText) {

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(documentText);

        // run all Annotators on this text
        this.pipeline.annotate(document);

        // Iterate over all of the sentences found
        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
        for (CoreMap sentence : sentences) {
            // Iterate over all tokens in a sentence
            for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
                String lemma = token.lemma();
                if (lemma != null) {
                    System.out.println(""Lemma: "" + lemma);
                    String originalText = token.originalText();
                    System.out.print(""Token: '"" + originalText + ""'"");
                    if (lemma.equalsIgnoreCase(originalText)) {
                        System.out.println("" is singular"");
                    } else {
                        System.out.println("" is plural"");
                    }
                }
            }
        }
    }

    public static void main(String[] args) {
        StanfordLemmatizer lemmatizer = new StanfordLemmatizer();
        lemmatizer.lemmatize(""The only creatures that are evolved enough to convey pure love are dogs and infants"");
    }
}
</code></pre>
",0,0,526,2018-10-28 11:07:27,https://stackoverflow.com/questions/53030817/corenlp-seek-out-if-a-word-is-singular-or-plural
dependency parsing (bracket format) - spanish - using nltk and stanford-nlp tag,"<p>I am trying to parse a plain text corpus of Spanish to get a result like SNLI corpus (used for entailment), I´ve ttached an extract of snli corpus below. </p>

<p>The church has cracks in the ceiling.
( ( The church ) ( ( has ( cracks ( in ( the ceiling ) ) ) ) . ) )
(ROOT (S (NP (DT The) (NN church)) (VP (VBZ has) (NP (NP (NNS cracks)) (PP (IN in) (NP (DT the) (NN ceiling))))) (. .)))</p>

<p>I tried the following code but the output was not good at all.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from nltk import Tree
from functools import reduce
from nltk.parse.corenlp import CoreNLPParser

def binarize(tree):
    """"""
    Recursively turn a tree into a binary tree.
    """"""
    if isinstance(tree, str):
        return tree
    elif len(tree) == 1:
        return binarize(tree[0])
    else:
        label = tree.label()
        return reduce(lambda x, y: Tree(label, (binarize(x), binarize(y))), tree)
    
parser = CoreNLPParser(url='http://localhost:9002')
#parse, = parser.raw_parse('you could say that they regularly catch a shower, which adds to their exhilaration and joie de vivre')
parse, = parser.raw_parse('si idioma no es elegido entonces elegir español por defecto.')
print(parse)
t = parse
bt = binarize(t)
print(bt)</code></pre>
</div>
</div>
</p>
","python, nltk, stanford-nlp, pos-tagger","<p>My output is produced using the <code>stanford-spanish-corenlp-2017-06-09-models.jar</code> which can be downloaded here: <a href=""https://nlp.stanford.edu/software/lex-parser.shtml#Download"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/lex-parser.shtml#Download</a></p>

<p>For some reason, using newer versions of the <code>models.jar</code> file create different results. </p>

<p>Make sure and put the Spanish <code>.jar</code> into the folder with the rest of Stanford Core NLP (I used the latest <code>2018-10-05</code>).</p>

<p>Then, when you start the Stanford Core NLP server, make sure and start it in Spanish:</p>

<pre><code> java -mx3g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-spanish.properties -port 9000 -timeout 15000
</code></pre>

<p>Note that the Spanish instance of the <code>CoreNLPTagger</code> uses a different tag set which is detailed on the <a href=""https://nlp.stanford.edu/software/spanish-faq.shtml"" rel=""nofollow noreferrer"">Spanish FAQ</a> page.</p>

<pre><code>from nltk.parse.corenlp import CoreNLPParser

parser = CoreNLPParser(url='http://localhost:9000')

parsed = parser.raw_parse('si idioma no es elegido entonces elegir español por defecto.')

for node in parsed:
    print(node)
</code></pre>

<p>Example output below:</p>

<pre><code>(ROOT
  (sentence
    (S
      (S
        (conj (cs si))
        (sn (grup.nom (nc0s000 idioma)))
        (neg (rn no))
        (grup.verb (vsip000 es)))
      (S (participi (aq0000 elegido))))
    (S
      (sadv (grup.adv (rg entonces)))
      (S
        (infinitiu (vmn0000 elegir))
        (s.a (grup.a (aq0000 español)))
        (sp (prep (sp000 por)) (sn (grup.nom (nc0s000 defecto))))))
    (fp .)))
</code></pre>
",0,1,621,2018-10-29 14:36:50,https://stackoverflow.com/questions/53047808/dependency-parsing-bracket-format-spanish-using-nltk-and-stanford-nlp-tag
How to improve speed of the program for large data in python,"<p>I am trying to calculate the prediction probability. I have wrote a program which is calculating but speed is very slow and taking so much time for large dataset. </p>

<p>The aim is to calculate each prediction probability in the SVM model by using <code>LinearSVC</code> and <code>OneVsRestClassifier</code> but getting the error</p>

<p><code>AttributeError: 'LinearSVC' object has no attribute 'predict_proba'</code></p>

<p>Due to the above error, I have tried below</p>

<p><strong>Code</strong></p>

<pre><code>from sklearn import svm

model_1 = svm.SVC(kernel='linear', probability=True)

from sklearn.preprocessing import LabelEncoder

X_1 = df[""Property Address""]
lb = LabelEncoder()
X_2 = lb.fit_transform(X_1)

y_1 = df[""Location_Name""]
y_2 = lb.fit_transform(y_1)

test_1 = test[""Property Address""]
lb = LabelEncoder()
test_1 = lb.fit_transform(test_1)

X_2= X_2.reshape(-1, 1)
y_2= y_2.reshape(-1, 1)
test_1 = test_1.reshape(-1, 1)

model_1.fit(X_2, y_2)

results = model_1.predict_proba(test_1)[0]

# gets a dictionary of {'class_name': probability}
prob_per_class_dictionary = dict(zip(model.classes_, results))
</code></pre>

<p>Is there any other way for the same task? please suggest</p>
","python, python-3.x, machine-learning, stanford-nlp","<p>You could use sklearns <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV"" rel=""nofollow noreferrer"">CalibratedClassifierCV</a> if you need to use to the <code>predict_proba</code> method. </p>

<p>Or you could use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""nofollow noreferrer"">Logistic Regression</a>. </p>

<p>If your issue is related to speed, try consider using the <code>LinearSVC</code> in <code>sklearn.svm</code> instead of <code>SVC(kernel='linear')</code>. It is faster.</p>
",2,-1,579,2018-10-30 12:10:24,https://stackoverflow.com/questions/53064011/how-to-improve-speed-of-the-program-for-large-data-in-python
Segmenting sentence into subsentences with CoreNLP,"<p>I am working on the following problem: I would like to split sentences into subsentences using Stanford CoreNLP. The example sentence could be:</p>

<pre><code>""Richard is working with CoreNLP, but does not really understand what he is doing""
</code></pre>

<p>I would now like my sentence to be split into single ""S"" as shown in the tree diagram below:</p>

<p><a href=""https://i.sstatic.net/UyLba.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UyLba.png"" alt=""enter image description here""></a></p>

<p>I would like the output to be a list with the single ""S"" as follows:</p>

<pre><code>['Richard is working with CoreNLP', ', but', 'does not really understand what', 'he is doing']
</code></pre>

<p>I would be really thankful for any help :)</p>
","stanford-nlp, dependency-parsing, nlp, pycorenlp","<p>I suspect the tool you're looking for is <a href=""https://nlp.stanford.edu/software/tregex.html"" rel=""nofollow noreferrer"">Tregex</a>, described in more detail in the power point <a href=""https://nlp.stanford.edu/software/tregex/The_Wonderful_World_of_Tregex.ppt/"" rel=""nofollow noreferrer"">here</a> or the <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/tregex/TregexPattern.html"" rel=""nofollow noreferrer"">Javadoc</a> of the class itself.</p>

<p>In your case, I believe the pattern you're looking for is simply <code>S</code>. So, something like:</p>

<pre><code>tregex.sh “S” &lt;path_to_file&gt;
</code></pre>

<p>where the file is a Penn Treebank formatted tree -- that is, something like <code>(ROOT (S (NP (NNS dogs)) (VP (VB chase) (NP (NNS cats)))))</code>.</p>

<p>As an aside: I believe the fragment ""<em>, but</em>"" is not actually a sentence, as you've hightlighted in the figure. Rather, the node you've highlighted subsumes the whole sentence ""<em>Richard is working with CoreNLP, but does not really understand what he is doing</em>"". Tregex would then print out this whole sentence as one of the matches. Similarly, ""<em>does not really understand what</em>"" is not a sentence unless it subsumes the entire SBAR: ""<em>does not understand what he is doing</em>"". </p>

<p>If you want just the ""leaf"" sentences (i.e., a sentence that's not subsumed by another sentence), you can try a pattern more like:</p>

<pre><code>S !&gt;&gt; S
</code></pre>

<hr>

<p>Note: I haven't tested the patterns -- use at your own risk!</p>
",2,3,464,2018-11-05 13:07:35,https://stackoverflow.com/questions/53155057/segmenting-sentence-into-subsentences-with-corenlp
How to get location from text using OpenNLP?,"<p>I am using  chunking for tagging the data and get the location from the text initially i try to extract noun phrase from next when we use noun phrase name also mentioned as noun phrase so it can't use.then i moved to location ner of core nlp
i try to run the below code</p>

<pre><code> * @param args the command line arguments
 */
public static void main(String[] args) {
    // TODO code application logic here
    try {
</code></pre>

<p>InputStream inputStreamTokenizer = new 
         FileInputStream(""D:\project\Relation Extraction in Text Document\Libraray\parsing/en-token.bin""); 
      TokenizerModel tokenModel = new TokenizerModel(inputStreamTokenizer); </p>

<pre><code>  //String paragraph = ""Mike and Smith are classmates""; 
  String paragraph = ""Tutorialspoint is located in Hyderabad""; 

  //Instantiating the TokenizerME class 
  TokenizerME tokenizer = new TokenizerME(tokenModel); 
  String tokens[] = tokenizer.tokenize(paragraph); 

  //Loading the NER-location moodel 
  InputStream inputStreamNameFinder = new 
     FileInputStream(""D:\\project\\Relation Extraction in Text Document\\Libraray\\parsing/en-ner-location.bin"");       
  TokenNameFinderModel location = new TokenNameFinderModel(inputStreamNameFinder); 

  //Instantiating the NameFinderME class 
  NameFinderME nameFinder;      
        nameFinder = new NameFinderME(location);

  //Finding the names of a location 
  Span nameSpans[] = nameFinder.find(tokens);        
  //Printing the spans of the locations in the sentence 
 for(Span s: nameSpans)        
     System.out.println(s.toString()+""  ""+tokens[s.getStart()]);
</code></pre>

<p>I Got an error that ""java.lang.UnsupportedOperationException: Not supported yet.""</p>

<p>An error symbol at""  nameFinder = new NameFinderME(location);"" saying that ""exmp.TokenNameFinderModel cannot be converted to opennlp.tools.namefind.TokenNameFinderModel"" what is reson for it</p>
","java, location, stanford-nlp, opennlp","<p>You have incorrect imports, here is a working version:</p>

<pre><code>import java.io.FileInputStream;
import java.io.InputStream;

import opennlp.tools.namefind.NameFinderME;
import opennlp.tools.namefind.TokenNameFinderModel;
import opennlp.tools.tokenize.TokenizerME;
import opennlp.tools.tokenize.TokenizerModel;
import opennlp.tools.util.Span;
</code></pre>

<p>and the output: <code>[4..5) location  Hyderabad</code></p>
",0,0,496,2018-11-07 16:26:00,https://stackoverflow.com/questions/53193671/how-to-get-location-from-text-using-opennlp
natural language logic in stanford corenlp,"<p>How does one use the natural logic component of Stanford CoreNLP?</p>

<p>I am using CoreNLP 3.9.1 and I fed natlog as an annotator in command line, but I don't seem to see any natlog result in the output, i.e. <code>OperatorAnnotation</code> and <code>PolarityAnnotation</code>, according to <a href=""https://stanfordnlp.github.io/CoreNLP/natlog.html"" rel=""nofollow noreferrer"">this link</a>. Does that have anything to do with the outputFormat? I've tried xml and json, but neither has any output on natural logic. The other stuff (tokenization, dep parse) is in there though. </p>

<p>Here is my command:</p>

<pre><code>./corenlp.sh -annotators tokenize,ssplit,pos,lemma,depparse,natlog -file natlog.test -outputFormat xml
</code></pre>

<p>Thanks in advance.</p>
",stanford-nlp,"<p>This code snippet works for me:</p>

<pre class=""lang-java prettyprint-override""><code>import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.pipeline.Annotation;
// this is the polarity annotation!
import edu.stanford.nlp.naturalli.NaturalLogicAnnotations.PolarityDirectionAnnotation;
// not the one below!
// import edu.stanford.nlp.ling.CoreAnnotations.PolarityAnnotation;
import edu.stanford.nlp.util.PropertiesUtils;

import java.io.*;
import java.util.*;

public class test {

    public static void main(String[] args) throws FileNotFoundException, UnsupportedEncodingException {

        // code from: https://stanfordnlp.github.io/CoreNLP/api.html#generating-annotations
        StanfordCoreNLP pipeline = new StanfordCoreNLP(
                PropertiesUtils.asProperties(
                // **add natlog here**
                        ""annotators"", ""tokenize,ssplit,pos,lemma,parse,depparse,natlog"", 
                        ""ssplit.eolonly"", ""true"",
                        ""tokenize.language"", ""en""));

        // read some text in the text variable
        String text = ""Every dog sees some cat"";

        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

        // these are all the sentences in this document
        // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

        for(CoreMap sentence: sentences) {
            // traversing the words in the current sentence
            // a CoreLabel is a CoreMap with additional token-specific methods
            for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
                // this is the text of the token
                String word = token.get(TextAnnotation.class);
                // this is the POS tag of the token
                String pos = token.get(PartOfSpeechAnnotation.class);
                // this is the NER label of the token
                String ne = token.get(NamedEntityTagAnnotation.class);
                // this is the polarity label of the token
                String pol = token.get(PolarityDirectionAnnotation.class);
                System.out.print(word + "" ["" + pol + ""] "");
            }
            System.out.println();
        }
    }
}
</code></pre>

<p>The output will be: 
Every [up] dog [down] sees [up] some [up] cat [up]</p>
",0,0,217,2018-11-07 21:37:01,https://stackoverflow.com/questions/53198202/natural-language-logic-in-stanford-corenlp
glove most similar to multiple words,"<p>I am supposed to do some exercises with python glove, most of it doesn't give me any problems but now i am supposed to find the 5 most similar words to ""norway - war + peace"" from the ""glove-wiki-gigaword-100"" package. But when i run my code it just says that the 'word' is not in the vocabulary. Now I'm guessing that this is some kind of formatting, but i don't know how to use it.</p>

<pre><code>import gensim.downloader as api
model = api.load(""glove-wiki-gigaword-100"")  # download the model and return as object ready for use

bests = model.most_similar(""norway - war + peace"", topn= 5)

print(""5 most similar words to 'norway - war + peace':"")

for best in bests:
    print(best)
</code></pre>
","python, nlp, gensim, glove","<p>Gensim's model word2vec only deals with previously seen words. Here you give an entire sentence... What you want to do is:</p>

<ol>
<li>get vectors v1, v2 and v3 for resp. words ""norway"", ""war"" and ""peace"".</li>
<li>Compute the math: v = v1 -v2 + v3.</li>
<li>get the most_similar words to v.  </li>
</ol>

<p>To do so, you will need these functions: <code>model.wv.most_similar()</code> and <code>model.wv.similar_by_vector()</code>. Note that <code>model.wv.most_similar()</code> does something similar to these three steps but in a more complicated way using a set of positive words and a set of negative words. See the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors"" rel=""nofollow noreferrer"">documentation</a> for details.</p>
",2,0,3672,2018-11-13 13:10:21,https://stackoverflow.com/questions/53281744/glove-most-similar-to-multiple-words
Using GLOVEs pretrained glove.6B.50.txt as a basis for word embeddings R,"<p>I'm trying to convert textual data into vectors using GLOVE in r. My plan was to average the word vectors of a sentence, but I can't seem to get to the word vectorization stage. I've downloaded the glove.6b.50.txt file and it's parent zip file from: <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a> and I have visited text2vec's website and tried running through their example where they load wikipedia data. But I dont think its what I'm looking for (or perhaps I am not understanding it). I'm trying to load the pretrained embeddings into a model so that if I have a sentence (say 'I love lamp') I can iterate through that sentence and turn each word into a vector that I can then average (turning unknown words into zeros) with a function like vectorize(word). How do I load the pretrained embeddings into a glove model as my corpus (and is that even what I need to do to accomplish my goal?)</p>
","r, word-embedding, text2vec, glove","<p>I eventually figured it out. The embeddings matrix is all I needed. It already has the words in their vocab as rownames, so I use those to determine the vector of each word. </p>

<p>Now I need to figure out how to update those vectors!</p>
",1,1,1459,2018-11-17 05:18:54,https://stackoverflow.com/questions/53348473/using-gloves-pretrained-glove-6b-50-txt-as-a-basis-for-word-embeddings-r
"compare NER library from Stanford coreNLP, SpaCy And Google cloud","<p>I want to recognise <strong>person</strong> name from text. But i'm getting confused which NLP library I have to use for NER. I find out following best NLP library for NER
1. Stanford coreNLP
2. Spacy
3. Google cloud.</p>

<p>I unable to find out which library will give more accurate result and good performance. Please help me here.</p>
","nlp, stanford-nlp, spacy, named-entity-recognition, google-natural-language","<p>spaCy have the industrial-strength in terms of NLP and obviously faster and accurate in terms of NER. It is also bundled with multi-lingual models. Check <a href=""https://spacy.io/usage/linguistic-features#section-named-entities"" rel=""nofollow noreferrer"">spaCy</a></p>

<p>Also AllenNLP comes with state-of-the-art NER model but slightly complex to use. Check <a href=""http://demo.allennlp.org/named-entity-recognition"" rel=""nofollow noreferrer"">AllenNLP demo</a></p>

<p>If paywall is not the issue then I would suggest to go with Google's Cloud Natural Language (of course it is faster and accurate).</p>

<p>I have personally used spaCy and AllenNLP. I would say go with spaCy if you are seeking to just start with.</p>

<p>Hope this helps.</p>
",2,0,5020,2018-11-22 12:06:20,https://stackoverflow.com/questions/53430654/compare-ner-library-from-stanford-corenlp-spacy-and-google-cloud
"How to pick out a the subject, predicate, and object and adjectives in a sentence","<p>I want to extract the subject, predicate, and object of a sentence and find out which adjectives go to the subject, predicate, or object with Stanford CoreNLP in java code.</p>

<p>I have tried to use the dependency parser to solve this by finding the dependency index, checking the dependency tag if it equals amod, then adding it to an ArrayList, but with this method sometimes the adjective's dependency tag is not amod and is nmod, and other tags may come up.</p>

<p>With determining the object and predicate, I have used a similar method as above. I have checked if it is det, and if it is any other tags that mean it is a predicate or object. However, sometimes different tags come up and it is not efficient to have to parse every tag that somewhat means it is a predicate pointing to the object.</p>

<p>So my question is, how to I get the subject, predicate, and object of a sentence and each's adjectives but not need to check each tag?</p>

<p>For the above mentioned attempts, I have used Stanford CoreNLP Simple API, but I am OK with the standard API if it is truly needed.</p>
","java, nlp, stanford-nlp","<p>You should try out the <code>openie</code> annotator which will find (subject, predicate, object) triples.</p>

<p>example command:</p>

<pre><code>java -Xmx5g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,natlog,openie -file basic-example.txt -outputFormat text
</code></pre>

<p>example:</p>

<pre><code>The tall man ate the delicious pizza.
</code></pre>

<p>example output:</p>

<pre><code>1.0     man     ate     pizza
1.0     man     ate     delicious pizza
1.0     tall man        ate     pizza
1.0     tall man        ate     delicious pizza
</code></pre>
",1,0,1038,2018-11-22 16:39:56,https://stackoverflow.com/questions/53435202/how-to-pick-out-a-the-subject-predicate-and-object-and-adjectives-in-a-sentenc
NLP Entity Recognition Inquiry,"<p>I am working on an NLP Chatbot project. The Chatbot will need to process requests like the following:</p>

<p>""I want to go to Penn Station from Back Bay Station"" and ""I want to go from Back Bay Station to Penn Station""</p>

<p>In each case, I want to extract the source train station as ""Back Bay Station"" and the destination as ""Penn Station."" However, because of the sentence re-ordering, I am not sure how to do this.</p>

<p>Any advice, including examples, would be much appreciated.</p>
","nlp, nltk, stanford-nlp, spacy, rasa-nlu","<p>Two ways. </p>

<ol>
<li>Heuristics: Look for words like 'to' and 'from' and similar before the entities. You might have to spend some time creating a library of these prepositions or subordinating conjunctions but that will do the job.</li>
<li>Use more sophisticated <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">deep parsers</a> that can do this job for you. You might have to still fall back to heuristics here as well, but you can get much more information this way. I am suggesting this option because I don't know how wide your problem statement is. If it is just about 'to' and 'from' then stick to option 1 </li>
</ol>
",1,0,83,2018-11-23 19:34:15,https://stackoverflow.com/questions/53452118/nlp-entity-recognition-inquiry
I have a question regarding practical implementation of Named Entity Recognition in NLP,"<p>If we Consider two Named Entity Relation systems, one based on the use of word embedding and the other using both word and character embedding jointly. How can we intuitively conclude which model is better for NER task? Is any illustration possible for above case?</p>
","nlp, stanford-nlp, named-entity-recognition","<p>If I understand your question correctly, you may train (if they require training) your models on some annotated dataset like CONL-2003 <a href=""https://www.clips.uantwerpen.be/conll2003/ner/"" rel=""nofollow noreferrer"">https://www.clips.uantwerpen.be/conll2003/ner/</a> and compare their accuracies using the test-part of the dataset.</p>
",0,0,87,2018-11-26 09:48:45,https://stackoverflow.com/questions/53478429/i-have-a-question-regarding-practical-implementation-of-named-entity-recognition
Force Stanford CoreNLP Parser to Prioritize &#39;S&#39; Label at Root Level,"<p>Greetings NLP Experts,</p>
<p>I am using the Stanford CoreNLP software package to produce constituency parses, using the most recent version (3.9.2) of the English language models JAR, downloaded from the <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">CoreNLP Download page</a>. I access the parser via the Python interface from the NLTK module nltk.parse.corenlp. Here is a snippet from the top of my main module:</p>
<pre><code>import nltk
from nltk.tree import ParentedTree
from nltk.parse.corenlp import CoreNLPParser

parser = CoreNLPParser(url='http://localhost:9000')
</code></pre>
<p>I also fire up the server using the following (fairly generic) call from the terminal:</p>
<pre><code>java -mx4g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer
-annotators &quot;parse&quot; -port 9000 -timeout 30000
</code></pre>
<p>The parser that CoreNLP selects by default (when the full English model is available) is the Shift-Reduce (SR) parser, which <a href=""https://stackoverflow.com/questions/29241123/pcfg-vs-sr-parser"">is sometimes claimed</a> to be both more accurate and faster than the CoreNLP PCFG parser. Impressionistically, I can corroborate that with my own experience, where I deal almost exclusively with Wikipedia text.</p>
<p>However, I have noticed that often the parser will erroneously opt for parsing what is in fact a complete sentence (i.e., a finite, matrix clause) as a subsentential constituent instead, often an <code>NP</code>. In other words, the parser should be outputting an <code>S</code> label at root level <code>(ROOT (S ...))</code>, but something in the complexity of the sentence's syntax pushes the parser to say a sentence is not a sentence <code>(ROOT (NP ...))</code>, etc.</p>
<p>The parses for such problem sentences also always contain another (usually glaring) error further down in the tree. Below are a few examples. I'll just paste in the top few levels of each tree to save space. Each is a perfectly acceptable English sentence, and so the parses should all begin <code>(ROOT (S ...))</code>. However, in each case some other label takes the place of <code>S</code>, and the rest of the tree is garbled.</p>
<blockquote>
<p><strong>NP:</strong> An estimated 22–189 million school days are missed annually due to a cold. <code>(ROOT (NP (NP An estimated 22) (: --) (S 189 million school days are missed annually due to a cold) (. .)))</code></p>
<p><strong>FRAG:</strong> More than one-third of people who saw a doctor received an antibiotic prescription, which has implications for antibiotic resistance. <code>(ROOT (FRAG (NP (NP More than one-third) (PP of people who saw a doctor received an antibiotic prescription, which has implications for antibiotic resistance)) (. .)))</code></p>
<p><strong>UCP:</strong> Coffee is a brewed drink prepared from roasted coffee beans, the seeds of berries from certain Coffea species. <code>(ROOT (UCP (S Coffee is a brewed drink prepared from roasted coffee beans) (, ,) (NP the seeds of berries from certain Coffea species) (. .)))</code></p>
</blockquote>
<p>At long last, here is my question, which I trust the above evidence proves is a useful one: <strong>Given that my data contains a negligible number of fragments or otherwise ill-formed sentences, how can I impose a high-level constraint on the CoreNLP parser such that its algorithm gives priority to assigning an <code>S</code> node directly below <code>ROOT</code>?</strong></p>
<p>I am curious to see whether imposing such a constraint when processing data (that one knows to satisfy it) will also cure other myriad ills observed in the parses produced. From what I understand, the solution would not lie in specifying a <code>ParserAnnotations.ConstraintAnnotation</code>. Would it?</p>
","python, nlp, nltk, stanford-nlp","<p>You can specify a certain range has to be marked a certain way.  So you can say the entire range has to be an 'S'.  But I think you have to do this in Java code.</p>

<p>Here is example code that shows setting constraints.</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/itest/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserITest.java"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/itest/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserITest.java</a></p>
",1,1,355,2018-11-27 23:25:27,https://stackoverflow.com/questions/53509779/force-stanford-corenlp-parser-to-prioritize-s-label-at-root-level
Error while embedding: could not convert string to float: &#39;ng&#39;,"<p>I am working on Pre trained word vectors using GloVe method. Data contains vectors on Wikipedia data. While embedding data i am getting error stating that could not convert string to float: 'ng'</p>

<p>I tried going through data but there i was not able to find symbol 'ng'</p>

<pre><code># load embedding as a dict
def load_embedding(filename):
    # load embedding into memory, skip first line
    file = open(filename,'r', errors = 'ignore')
    # create a map of words to vectors
    embedding = dict()
    for line in file:
        parts = line.split()
        # key is string word, value is numpy array for vector
        embedding[parts[0]] = np.array(parts[1:], dtype='float32')
    file.close()
    return embedding
</code></pre>

<p>Here is the error report. Please guide me further.</p>

<pre><code>runfile('C:/Users/AKSHAY/Desktop/NLP/Pre-trained GloVe.py', wdir='C:/Users/AKSHAY/Desktop/NLP')
C:\Users\AKSHAY\AppData\Local\conda\conda\envs\py355\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):

  File ""&lt;ipython-input-1-d91aa5ebf9f8&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/AKSHAY/Desktop/NLP/Pre-trained GloVe.py', wdir='C:/Users/AKSHAY/Desktop/NLP')

  File ""C:\Users\AKSHAY\AppData\Local\conda\conda\envs\py355\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""C:\Users\AKSHAY\AppData\Local\conda\conda\envs\py355\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/AKSHAY/Desktop/NLP/Pre-trained GloVe.py"", line 123, in &lt;module&gt;
    raw_embedding = load_embedding('glove.6B.50d.txt')

  File ""C:/Users/AKSHAY/Desktop/NLP/Pre-trained GloVe.py"", line 67, in load_embedding
    embedding[parts[0]] = np.array(parts[1:], dtype='float32')

ValueError: could not convert string to float: 'ng'
</code></pre>
","python, machine-learning, nlp, glove","<p>ValueError: could not convert string to float: 'ng' </p>

<p>For addressing the problem above, add <strong>encoding='utf8'</strong> to the function as follows:</p>

<pre><code>file = open(filename,'r', errors = 'ignore', encoding='utf8')
</code></pre>
",3,2,4880,2018-11-29 14:32:17,https://stackoverflow.com/questions/53541327/error-while-embedding-could-not-convert-string-to-float-ng
"Vector Space Model - query vector [0, 0.707, 0.707] calculated","<p>I'm reading the book ""Introduction to Information Retrieval ""(Christopher Manning) and I'm stuck on the chapter 6 when it introduces the query ""jealous gossip"" for which it indicated that the vector unit associated is [0, 0.707, 0.707] ( <a href=""https://nlp.stanford.edu/IR-book/html/htmledition/queries-as-vectors-1.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/IR-book/html/htmledition/queries-as-vectors-1.html</a> ) considering the terms affect, jealous and gossip.
I tried to calculate it by computing the tf<em>idf assuming that:
- Tf is equal to 1 for jealous and gossip
- Idf is always equal to 0 if we calculate it as log(N/df) with N=1(I have only 1 query and it is my document), df=1 for jealous and gossip => log(1)=0
Since the idf is 0, it turns out that the tf</em>idf is 0.
So I decided to compute every weight of the query vector with the raw tf divided by the euclidean length. In this case the Euclidean length is sqrt(1+1)=1. 
I can't obtain the formula by which it decided that [0, 0.707, 0.707] is the query vector. 
Can someone help me? </p>
","stanford-nlp, information-retrieval, tf-idf","<p>I haven't worked through the problem, but I think the issue might be that <code>sqrt(1+1)</code> is <code>sqrt(2)</code>, so when you normalize, each of the 1s become <code>1/sqrt(2) = 0.707</code>.</p>
",0,0,352,2018-12-02 22:09:12,https://stackoverflow.com/questions/53585068/vector-space-model-query-vector-0-0-707-0-707-calculated
Map CorefChain to CoreEntityMention in Stanford Core NLP,"<p>I want to construct a lookup map with the type <code>Map&lt;CorefChain, CoreEntityMention&gt;</code> using       </p>

<pre><code>Map&lt;Integer, Integer&gt; mapping = document.annotation().get(CoreAnnotations.CorefMentionToEntityMentionMappingAnnotation.class);  
</code></pre>

<p>where <code>document</code> is a <code>CoreDocument</code>.    </p>

<p>I have tried to get a set of CorefChains and a list of CoreEntityMentions to build such a map, but the indices do not seem to match up.  </p>

<pre><code>Map&lt;Integer, CorefChain&gt; chains = document.corefChains();
List&lt;CoreEntityMention&gt; entities = document.entityMentions();   
</code></pre>

<hr>

<p>Example: </p>

<p>Sentence:</p>

<pre><code> ""ʿAmrān is a small city in western central Yemen. It is the capital of the 'Amran 
  Governorate, and was formerly in the Sana'a Governorate. It is located 52.9 
  kilometres by road northwest of the Yemeni capital of Sana'a. According to the 
  2004 census it had a population of 76,863, and an estimated population of 
  90,792 in 2012.""       
</code></pre>

<p>chains:</p>

<pre><code>{
    1=CHAIN1-[""a small city in western central Yemen"" in sentence 1, ""It"" in sentence 2, ""It"" in sentence 3], 
    2=CHAIN2-[""western central Yemen"" in sentence 1], 
    4=CHAIN4-[""the capital of the ` Amran Governorate"" in sentence 2], 
    5=CHAIN5-[""the ` Amran Governorate"" in sentence 2], 
    6=CHAIN6-[""the Sana'a Governorate"" in sentence 2, ""Sana'a"" in sentence 3], 
    7=CHAIN7-[""52.9"" in sentence 3], 
    10=CHAIN10-[""52.9 kilometres"" in sentence 3], 
    11=CHAIN11-[""road northwest of the Yemeni capital of Sana'a"" in sentence 3], 
    12=CHAIN12-[""the Yemeni capital of Sana'a"" in sentence 3], 13=CHAIN13-[""76,863"" in sentence 4], 
    14=CHAIN14-[""90,792"" in sentence 4], 15=CHAIN15-[""the 2004 census"" in sentence 4, ""it"" in sentence 4], 
    17=CHAIN17-[""a population of 76,863 , and an estimated population of 90,792 in 2012"" in sentence 4], 
    18=CHAIN18-[""a population of 76,863"" in sentence 4], 
    19=CHAIN19-[""an estimated population of 90,792 in 2012"" in sentence 4], 
    20=CHAIN20-[""2012"" in sentence 4]
}
</code></pre>

<p>entities:</p>

<pre><code>[Yemen, Sana'a Governorate, 52.9, Yemeni, Sana'a, 2004, 76,863, 90,792, 2012]
</code></pre>

<p>mapping:</p>

<pre><code>{16=8, 7=2, 8=4, 13=5, 14=6, 15=7}
</code></pre>
",stanford-nlp,"<p>Just to be clear, there are two types of mentions:</p>

<pre><code>coref mentions
entity mentions
</code></pre>

<p>All <code>entity mentions</code> should be <code>coref mentions</code>, but not all <code>coref mentions</code> are <code>entity mentions</code>.</p>

<p>As you found there is a map from <code>coref mentions</code> to <code>entity mentions</code>.</p>

<p>You should be able to see the chain id with the <code>corefClusterID</code> property of the coref mention.  So you have a map of <code>coref mention</code> to <code>entity mention</code>, and you can convert the <code>coref mention</code> to the chain id by accessing the <code>corefClusterID</code> of the coref mention.</p>
",0,-1,51,2018-12-05 14:53:00,https://stackoverflow.com/questions/53634990/map-corefchain-to-coreentitymention-in-stanford-core-nlp
Best tool for text representation to deep learning,"<p>so I wanna ask you which is the best tool used to prepare my text to deep learning?</p>

<p>What is the difference between <code>Word2Vec</code>, <code>Glove</code>, <code>Keras</code>, <code>LSA</code>...</p>
","keras, deep-learning, word2vec, lsa, glove","<p>You should use a pre-trained embedding to represent the sentence into a vector or a matrix. There are a lot of sources where you can find pre-trained embeddings that use different dataset (for instance all the Wikipedia) to train their models. These models can have different length, but normally each word is represented with 100 or 300 dimensions.</p>

<p><a href=""http://vectors.nlpl.eu/repository/"" rel=""nofollow noreferrer"">Pre-trained embeddings</a>
<a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""nofollow noreferrer"">Pre-trained embeddings 2</a></p>
",1,-2,51,2018-12-11 10:49:39,https://stackoverflow.com/questions/53722492/best-tool-for-text-representation-to-deep-learning
Possible error with Stanford POS Tagger and classifying intent and the replies,"<p>I have a specific usecase, where a person would say something like this: </p>

<ul>
<li><strong><em>""Hey (Trigger Word), note in object history XYZ""</em></strong> or:</li>
<li><strong><em>""Hey (Trigger Word), record in object diagnosis that PQR""</em></strong></li>
<li>(""object"" as used in the example is a placeholder and can be replaced with words like 'Maintenance/Patient', etc.)</li>
</ul>

<p>I would like to recognize the intent and the slots. </p>

<p>Then I use Stanford Parser to parse the sentence, e.g. parsing <strong><em>""Note in object history object was last updated in may twenty eighteen""</em></strong> gives this list-of-tuple:</p>

<pre><code>[('Note', 'VB'),
 ('in', 'IN'),
 ('object', 'NN'),
 ('history', 'NN'),
 ('object', 'NN'),
 ('was', 'VBD'),
 ('last', 'RB'),
 ('updated', 'VBN'),
 ('in', 'IN'),
 ('may', 'MD'),
 ('twenty', 'CD'),
 ('eighteen', 'CD')]
</code></pre>

<ol>
<li><p>Now, my point is how can I use this information to get the necessary output:</p>

<ul>
<li>Where to note <em>(we have a field in DB: Object History)</em> and</li>
<li>What to note <em>(object was last updated in may twenty eighteen)</em>.</li>
</ul></li>
<li><p>Another issue is since the input of the NLP is from an ASR system, the capitalization is missing. And the POS Tagger mis-tags 'note' as 'NN' (instead of 'VB'). Ideally 'note'/'record' should be a verb. How do I solve this probable error?</p></li>
</ol>
","nlp, speech-recognition, stanford-nlp, pos-tagger","<p>You can use the TrueCaseAnnotator to fix case issues:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/truecase.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/truecase.html</a></p>

<p>In general you probably just want to use TokensRegex and write rules patterns to handle these templates.  More info here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokensregex.html</a></p>
",1,0,58,2018-12-12 15:24:32,https://stackoverflow.com/questions/53746225/possible-error-with-stanford-pos-tagger-and-classifying-intent-and-the-replies
Creating Words Dictionary and Mapping to other languages,"<p>I am creating an app, where user can search in multiple languages. Let's say I have a website which host a very big novel. may be consisting of thousands of para graphs. some where between 20,000 to 30,000 para graphs. </p>

<p>A user can read the novel in multiple languages. He can search in multiple languages. For example he searches ""LOVE"" in English, I will show him all the paragraphs containing ""LOVE"" in them. </p>

<p>Now, if user switches to French, and searches ""Amour"" (French for Love), I will show him all paragraphs containing ""Amour"" in them. </p>

<p>I can do it by creating two versions of the novel, one in french and other in English. Indeed, I will have the translations. But, when user is reading the novel I will give him an option to click on any word and see it's translation to other languages. In this case, if he is reading in English, I will show him translation of that particular word in French and vise versa. </p>

<p>This means I want to keep a word to word mapping between different languages. </p>

<p>One way of doing is to create a map my self, which is a lot of work. another way could be some API calls, for example Google Translator. 
Can you suggest best approach? Any existing API?
Some Google Terms to go into the right direction for this task would also be helpful. </p>
","nlp, stanford-nlp, google-translate, google-translator-toolkit","<p>Elasticsearch can be good solution to your problem.</p>

<ol>
<li>Simply index your content in their native language (index it well)</li>
<li>Then you can use Elasticsearch Phonetic Plugin for multi-lingual search (<a href=""https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-phonetic.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-phonetic.html</a>)</li>
</ol>

<p>for the other part, where you want to do word by word translation, I think google translate or other translation api can be integrated or maybe elasticsearch as well.</p>

<p>I hope this helps.</p>
",2,2,456,2018-12-16 10:26:29,https://stackoverflow.com/questions/53801282/creating-words-dictionary-and-mapping-to-other-languages
SUTime SequenceMatchRules for &quot;c. DATE - DATE BC&quot;,"<p>I'm fighting with Stanford's SequenceMatchRules for recognizing the following input as two dates:</p>

<blockquote>
  <p>Anaximander (c. 610 – c. 546 BC) was a pre-Socratic Greek philosopher
  who lived in Miletus, a city of Ionia (in modern-day Turkey).</p>
</blockquote>

<p>(taken from the Pantheon dataset, e.g. <a href=""http://pantheon.media.mit.edu"" rel=""nofollow noreferrer"">http://pantheon.media.mit.edu</a>)</p>

<p><em>'546 BC'</em> works just fine, but I also want to recognize <em>'610'</em> as <em>'610 BC'</em> (preferably NOT as a duration).</p>

<p>What I did so far just to get things going:</p>

<p>Modified <code>english.sutime.txt</code>:</p>

<p>Changed</p>

<pre><code>$POSSIBLE_YEAR = ( $YEAR /a\.?d\.?|b\.?c\.?/? | $INT /a\.?d\.?|b\.?c\.?/ | $INT1000TO3000 );
</code></pre>

<p>to</p>

<pre><code>$POSSIBLE_YEAR = ( $YEAR /a\.?d\.?|b\.?c\.?/? | $INT /a\.?d\.?|b\.?c\.?/ | /c\.\ / $INT | $INT1000TO3000 );
</code></pre>

<p>And in the <code>pattern: ( $POSSIBLE_YEAR)...</code> extraction rule:</p>

<pre><code>          Tag($0, ""YEAR_ERA"",
            :case {
               $0 =~ ( $INT /a\.?d\.?/ ) =&gt; ERA_AD,
               $0 =~ ( $INT /b\.?c\.?/ ) =&gt; ERA_BC,
               :else =&gt; ERA_UNKNOWN
            }
          )
</code></pre>

<p>to</p>

<pre><code>          Tag($0, ""YEAR_ERA"",
            :case {
               $0 =~ ( $INT /a\.?d\.?/ ) =&gt; ERA_AD,
               $0 =~ ( /c\.\ / $INT ) =&gt; ERA_BC,
               $0 =~ ( $INT /b\.?c\.?/ ) =&gt; ERA_BC,
               :else =&gt; ERA_UNKNOWN
            }
          )
</code></pre>

<p>First it's ugly, second it didn't work at all.</p>

<p>Where should I begin to get this right?</p>

<p>I'm using the <code>stanford-corenlp-full-2018-10-05</code>.</p>

<p>I should mention that Pantheon is not perfectly normalized, so I have to deal with additional stuff like CE/BCE, missing spaces around dates etc later. Therefore an extendable approach would be great.</p>
","regex, date, stanford-nlp, sutime","<p>I think this rule would match <code>c. 610</code> ... if it sees the pattern it will attach the corresponding IsoDate to it.  Please let me know if that works or not...if not I can figure out what's broken.</p>

<pre><code>{ (/c\./ (/[0-9]{3,4}/)) =&gt; IsoDate($1[0].numcompvalue, NIL, NIL, 0, FALSE) }
</code></pre>

<p>Here is the constructor for IsoDate that takes in era for reference:</p>

<pre><code>public IsoDate(Number y, Number m, Number d, Number era, Boolean yearEraAdjustNeeded) {
  this.year = (y != null)? y.intValue():-1;
  this.month = (m != null)? m.intValue():-1;
  this.day = (d != null)? d.intValue():-1;
  this.era = (era != null)? era.intValue():ERA_UNKNOWN;
  if (yearEraAdjustNeeded != null &amp;&amp; yearEraAdjustNeeded &amp;&amp; this.era == ERA_BC) {
    if (this.year &gt; 0) {
      this.year--;
    }
  }
  initBase();
}
</code></pre>

<p>If that rule works, it should demonstrate how to match a text pattern and attach the desired year.  It might be easiest to just write a <code>pantheon_rules.txt</code> file and add it your list of SUTime rules that covers everything you want, once you have that basic rule down you can extend it to match the cases you want.  I could also work on adding some rules for handling these cases into the official release at some point.</p>
",1,1,98,2018-12-17 19:20:18,https://stackoverflow.com/questions/53821741/sutime-sequencematchrules-for-c-date-date-bc
CoreNLP: English_SD model last update time,"<blockquote>
  <p>Since version 3.5.2 the Stanford Parser and Stanford CoreNLP output grammatical relations in the Universal Dependencies v1 representation by default.</p>
</blockquote>

<p>I wonder if Stanford still improving the <code>English_SD</code> parser model or it's concentrated on improving <code>English_UD</code> instead. What was the last time <code>English_SD</code> got updated?</p>

<p><a href=""https://mailman.stanford.edu/pipermail/java-nlp-announce/2017-January.txt"" rel=""nofollow noreferrer"">Mailing list archive</a> tells me there are <code>new neural dependency parsing models for English</code> released in 3.7.0, but I'm not sure if it's SD and/or UD models.</p>
",stanford-nlp,"<p>We are not updating SD any more, that description is a reference to a new UD model.</p>
",1,0,22,2018-12-25 08:39:15,https://stackoverflow.com/questions/53920690/corenlp-english-sd-model-last-update-time
How do I get word embedding using CoreNlp from Stanford?,"<p>I am using CoreNlp to get the information extraction from a large text. However, its using the ""triple"" approach where a single sentence produce many output which is good, but there are some sentences that doesn't make sense. I tried to eliminate this by running another unsupervised NLP and try to utilize function in CoreNlp, yet I stuck at getting word vector form CoreNlp. Can anyone point where do I need to start searching for codes that do the word embedding in CoreNlp? Also I am newbie in java and IT. </p>

<p>There are some open libraries like glove, word2vec, text2vec, but I noticed glove already been used in CoreNlp (correct me if wrong). </p>
","java, vectorization, stanford-nlp, word-embedding","<p>since training your own model from scratch might turn out to be a time-consuming task, you could just download pretrained vectors from:
<a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a></p>

<p>however, there is an example with dl4j here that might do to trick:
<a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/glove/GloVeExample.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/glove/GloVeExample.java</a></p>
",1,0,1178,2018-12-31 09:27:04,https://stackoverflow.com/questions/53985757/how-do-i-get-word-embedding-using-corenlp-from-stanford
How to set tokenizer options when using the simple CoreNLP API?,"<p>I'm aware of the tokenizer options that are available in CoreNLP and I know how to set them in the standard version. </p>

<p>Is there way to pass the options, e.g. the <code>untokenizable=noneKeep</code>, when using the Simple CoreNLP interfaces?</p>
","java, stanford-nlp","<p>You can build a Document with properties.</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.simple.*;

import java.util.*;

public class SimpleExample {

    public static void main(String[] args) {
        Properties props = new Properties();
        props.setProperty(""tokenize.options"", ""untokenizable=allKeep"");
        Document doc = new Document(props, ""Joe Smith was born in California.  He moved to Chicago last year."");
        for (Sentence sent : doc.sentences()) {
            System.out.println(sent.tokens());
            System.out.println(sent.nerTags());
            System.out.println(sent.parse());
        }
    }

}
</code></pre>
",1,5,130,2019-01-06 11:24:57,https://stackoverflow.com/questions/54061020/how-to-set-tokenizer-options-when-using-the-simple-corenlp-api
Entity Mention Detection is not working properly with TokensRegex,"<p>entitymention doesn't seem to work. I followed similar approach mentioned here by adding <code>entitymentions</code> as one of the <code>annotators</code></p>

<p><a href=""https://stackoverflow.com/questions/43100750/how-can-i-detect-named-entities-that-have-more-than-1-word-using-corenlps-regex"">How can I detect named entities that have more than 1 word using CoreNLP&#39;s RegexNER?</a></p>

<p>Input : ""Here is your 24 USD""</p>

<p>I have a TokensRegex:</p>

<pre><code>{ ruleType: ""tokens"", pattern: ([{ner:""NUMBER""}] + [{word:""USD""}]), action: Annotate($0, ner, ""NEW_MONEY""), result: ""NEW_MONEY_RESULT"" }
</code></pre>

<p>Init Pipeline:</p>

<pre><code>props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,tokensregex,entitymentions"");
props.setProperty(""tokensregex.rules"", ""basic_ner.rules"");
</code></pre>

<p>I still got 2 CoreEntityMention instead of just 1.</p>

<p>Both of them have the same value for <code>edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation</code> which is <code>NEW_MONEY</code></p>

<p>but they have different  <code>edu.stanford.nlp.ling.CoreAnnotations$EntityMentionIndexAnnotation</code></p>

<p>which is <code>0</code> for <code>24</code></p>

<p><code>1</code> for <code>USD</code></p>

<p>How can they be merged since they both have same entity tag annotation.</p>

<p><code>3.9.2</code> version of stanford library is used.</p>
",stanford-nlp,"<p>The issue is that numbers have a normalized name entity tag.</p>

<p>Here is a rules file that will work:</p>

<pre><code># these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
normNER = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NormalizedNamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

# rule for recognizing company names
{ ruleType: ""tokens"", pattern: ([{ner:""NUMBER""}] [{word:""USD""}]), action: (Annotate($0, ner, ""NEW_MONEY""), Annotate($0, normNER, ""NEW_MONEY"")), result: ""NEW_MONEY"" }
</code></pre>

<p>You should not add an extra <code>tokensregex</code> annotator and <code>entitymentions</code> annotator at the end.  The <code>ner</code> annotator will run these as sub-annotators.</p>

<p>Here is an example command:</p>

<pre><code>java -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -ner.additional.tokensregex.rules new_money.rules -file new_money_example.txt -outputFormat text
</code></pre>

<p>More documentation here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokensregex.html</a></p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/ner.html</a></p>
",1,0,150,2019-01-11 17:24:53,https://stackoverflow.com/questions/54151316/entity-mention-detection-is-not-working-properly-with-tokensregex
When we run coreference resolution program it will throw an error how can i solve?,"<p>I am new to coreference resolution when we run the program it will throw an error i m very confused to resolve please help me with it</p>

<pre><code>    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
</code></pre>

<p>The error occured is mentioned below</p>

<pre><code>&gt; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
at java.util.Arrays.copyOfRange(Unknown Source)
at java.lang.String.&lt;init&gt;(Unknown Source)
at edu.stanford.nlp.util.StringUtils.splitOnChar(StringUtils.java:537)
at edu.stanford.nlp.coref.data.Dictionaries.loadGenderNumber(Dictionaries.java:405)
at edu.stanford.nlp.coref.data.Dictionaries.&lt;init&gt;(Dictionaries.java:676)
at edu.stanford.nlp.coref.data.Dictionaries.&lt;init&gt;(Dictionaries.java:576)
at edu.stanford.nlp.coref.CorefSystem.&lt;init&gt;(CorefSystem.java:32)
at edu.stanford.nlp.pipeline.CorefAnnotator.&lt;init&gt;(CorefAnnotator.java:66)
at edu.stanford.nlp.pipeline.AnnotatorImplementations.coref(AnnotatorImplementations.java:196)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$53(StanfordCoreNLP.java:555)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$24/544724190.apply(Unknown Source)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$69(StanfordCoreNLP.java:625)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$37/1673605040.get(Unknown Source)
at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:495)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:201)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:194)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:181)
at test.test.main(test.java:17)
</code></pre>
","java, nlp, stanford-nlp, opennlp","<p>You need to run your Java process with more memory.  On the command line you typically do this with <code>java -Xmx5g ...</code>.  I am not sure specifically how much memory you should use for your code, but I think something around <code>4g</code> should be fine.  It's possible less will work as well.</p>
",0,0,61,2019-01-15 17:08:58,https://stackoverflow.com/questions/54203702/when-we-run-coreference-resolution-program-it-will-throw-an-error-how-can-i-solv
averaging a sentence’s word vectors in Keras- Pre-trained Word Embedding,"<p>I am new to Keras.</p>

<p>My goal is to create a <strong>Neural Network Multi-Classification for Sentiment Analysis</strong> for tweets.</p>

<p>I used <code>Sequential</code> in <code>Keras</code> to build my model. </p>

<p>I want to use <strong>pre-trained word embeddings</strong> in the first layer of my model, specifically <code>gloVe</code>. </p>

<p>Here is my model currently:</p>

<pre><code>model = Sequential()
model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))
model.add(LSTM(100, stateful=False))
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))
</code></pre>

<p><code>embedding_matrix</code> is filled by the vectors coming from the file <code>glove.840B.300d.txt</code></p>

<p>Since my input to the neural network model is <strong>sentences</strong> (or tweets), and after consulting some theory, I want for the layer after the Embedding layer, after taking every word vector in the tweet, to <strong>average the sentence’s word vectors</strong>.</p>

<p>Currently what I use is <code>LSTM</code>, I want to replace it with this technique of averaging technique or <code>p-means</code>. I wasn't able to find this in <code>keras</code> documentation. </p>

<p>I'm not sure if this is the right place to ask this, but all help will be appreciated.</p>
","python, tensorflow, keras, glove","<p>You can use the <code>mean</code> function from Keras' backend and wrap it in a <code>Lambda</code> layer to average the embeddings over the words. </p>

<pre><code>import keras
from keras.layers import Embedding
from keras.models import Sequential
import numpy as np
# Set parameters
vocab_size=1000
max_length=10
# Generate random embedding matrix for sake of illustration
embedding_matrix = np.random.rand(vocab_size,300)

model = Sequential()
model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], 
input_length=max_length, trainable=False))
# Average the output of the Embedding layer over the word dimension
model.add(keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1)))

model.summary()
</code></pre>

<p>Gives: </p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_6 (Embedding)      (None, 10, 300)           300000    
_________________________________________________________________
lambda_6 (Lambda)            (None, 300)               0         
=================================================================
Total params: 300,000
Trainable params: 0
Non-trainable params: 300,000
</code></pre>

<p>Furthermore, you can use the <code>Lambda</code> layer to wrap arbitrary functions that operate on tensors in a Keras layer and add them to your model. If you are using the TensorFlow backend, you have access to tensorflow ops as well:</p>

<pre><code>import tensorflow as tf    
model = Sequential()
model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], 
input_length=max_length, trainable=False))
model.add(keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)))
# same model as before
</code></pre>

<p>This can help to implement more custom averaging functions. </p>
",3,5,4526,2019-01-16 12:53:37,https://stackoverflow.com/questions/54217503/averaging-a-sentence-s-word-vectors-in-keras-pre-trained-word-embedding
Overriding default env settings for TokensRegex in stanford,"<p>What is the setting to over ride <code>edu.stanford.nlp.ling.CoreAnnotations$TextAnnotation</code> to 
<code>edu.stanford.nlp.ling.CoreAnnotations$OriginalTextAnnotation</code>
when token regex rules are added.</p>

<p>Sample example:</p>

<p><code>#123456</code> is tagged as MONEY in Stanford hence to over ride the NER behavior I wrote the the rule which tags <code>123456</code> as NUMBER instead of MONEY. As a side effect following <code>£20.49</code> is now being tagged as NUMBER.</p>

<p>I debugged the code and realized that when pattern is applied <code>edu.stanford.nlp.ling.CoreAnnotations$TextAnnotation</code> is used to match. Hence in the case when <code>£20.49</code> is the input <code>£</code> is the value of <code>edu.stanford.nlp.ling.CoreAnnotations$OriginalTextAnnotation</code> and <code>#</code> is the value of <code>edu.stanford.nlp.ling.CoreAnnotations$TextAnnotation</code>. </p>

<p>Is there an environment setting to change this behavior? </p>

<p>Sample rule</p>

<pre><code># make all patterns case-sensitive
ENV.defaultStringMatchFlags = 0
ENV.defaultStringPatternFlags = 0

# these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

normalizedValue = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NormalizedNamedEntityTagAnnotation"" }

{ ruleType: ""tokens"", pattern: (([{word:""#""}]) ([{ner:""MONEY""}])), action: (Annotate($1, ner, ""IGNORE""), Annotate($2, ner, ""NUMBER""), Annotate($0, normalizedValue, ""TOKENS_REGEX"")), result: ""NUMBER"" }

</code></pre>
",stanford-nlp,"<p>You should use the latest version on GitHub or version 3.9.2.  Currency is no longer normalized, so the pound sign will no longer be turned into ""#"" by default.</p>

<p>You should be able to do something like</p>

<pre><code>originalWord = { type: ""CLASS"", value: edu.stanford.nlp.ling.CoreAnnotations$OriginalTextAnnotation }
</code></pre>

<p>Then you can replace <code>word</code> in your rule with <code>originalWord</code> .</p>
",0,0,58,2019-01-17 02:40:02,https://stackoverflow.com/questions/54228221/overriding-default-env-settings-for-tokensregex-in-stanford
Remove stopwords list from list in Python (Natural Language Processing),"<p>I have been trying to remove stopwords using python 3 code but my code does not seem to work,I want to know how to remove stop words from the below list.  The example structure is as below:</p>

<pre><code>    from nltk.corpus import stopwords

    word_split1=[['amazon','brand','- 
    ','solimo','premium','almonds',',','250g','by','solimo'],
    ['hersheys','cocoa', 'powder', ',', '225g', 'by', 'hersheys'], 
    ['jbl','t450bt','extra','bass','wireless','on- 
    ear','headphones','with','mic','white','by','jbl','and']]
</code></pre>

<p>I am trying to remove stop words and tried the below is my code and i would appreciate if anyone can help me rectify the issue.. here is the code below</p>

<pre><code>    stop_words = set(stopwords.words('english'))

    filtered_words=[]
    for i in word_split1:
        if i not in stop_words:
            filtered_words.append(i)
</code></pre>

<p>I get error:</p>

<pre><code>    Traceback (most recent call last):
    File ""&lt;ipython-input-451-747407cf6734&gt;"", line 3, in &lt;module&gt;
    if i not in stop_words:
    TypeError: unhashable type: 'list'
</code></pre>
","python-3.x, nlp, stanford-nlp, opennlp","<p>You have a list of lists.</p>

<p>Try:</p>

<pre><code>word_split1=[['amazon','brand','- ','solimo','premium','almonds',',','250g','by','solimo'],['hersheys','cocoa', 'powder', ',', '225g', 'by', 'hersheys'],['jbl','t450bt','extra','bass','wireless','on-ear','headphones','with','mic','white','by','jbl','and']]
stop_words = set(stopwords.words('english'))
filtered_words=[]
for i in word_split1:
    for j in i:
        if j not in stop_words:
            filtered_words.append(j)
</code></pre>

<hr>

<p>or flatten your list.</p>

<p><strong>Ex:</strong></p>

<pre><code>from itertools import chain    

word_split1=[['amazon','brand','- ','solimo','premium','almonds',',','250g','by','solimo'],['hersheys','cocoa', 'powder', ',', '225g', 'by', 'hersheys'],['jbl','t450bt','extra','bass','wireless','on-ear','headphones','with','mic','white','by','jbl','and']]
stop_words = set(stopwords.words('english'))
filtered_words=[]
for i in chain.from_iterable(word_split1):
    if i not in stop_words:
        filtered_words.append(i)
</code></pre>

<p>or</p>

<pre><code>filtered_words = [i for i in chain.from_iterable(word_split1) if i not in stop_words]
</code></pre>
",1,0,924,2019-01-17 10:27:59,https://stackoverflow.com/questions/54233854/remove-stopwords-list-from-list-in-python-natural-language-processing
Stanford NLP POS tag X in Spanish,"<p>I am doing POS tag in a text in Spanish and for some words the tagger marks it with the <code>X</code> tag.</p>

<pre><code>    static public void main(String[] args) {
        String text = ""Posteriormente, el desarrollo urbanístico estuvo marcado por el aumento de la población debido a la inmigración desde otras partes de España, lo que conllevó diversos proyectos urbanísticos como el Plan Comarcal de 1953 o el Plan General Metropolitano de 1976. Igualmente, la adecuación del espacio urbano de la ciudad se ha visto favorecida entre los siglos XIX y XXI por diversos eventos celebrados en la ciudad, como la Exposición Universal de 1888, la Internacional de 1929, el XXXV Congreso Eucarístico Internacional de 1952, los Juegos Olímpicos de 1992 y el Fórum Universal de las Culturas de 2004."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos"");
        props.setProperty(""pos.model"", ""spanish.tagger"");
        props.setProperty(""pos.maxlen"", ""50"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        Annotation annotation = new Annotation(text);
        pipeline.annotate(annotation);
        List&lt;CoreMap&gt; sentences = annotation.get(SentencesAnnotation.class);
        for(CoreMap sentence: sentences) {
            for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
                String word = token.get(TextAnnotation.class);
                System.out.println(word + "" "" + token.get(PartOfSpeechAnnotation.class));
            }
        }
}
</code></pre>

<p>outputs :</p>

<pre><code>Posteriormente rg
, fc
el da0000
desarrollo nc0s000
urbanístico aq0000
estuvo vmis000
marcado aq0000
por sp000
el da0000
aumento nc0s000
de sp000
la da0000
población nc0s000
debido vmp0000
a sp000
la da0000
inmigración nc0s000
desde sp000
otras di0000
partes nc0p000
de sp000
España np00000
, fc
lo da0000
que pr000000
conllevó vmis000
diversos di0000
proyectos nc0p000
urbanísticos aq0000
como cs
el da0000
Plan np00000
Comarcal np00000
de sp000
1953 w
o cc
el da0000
Plan np00000
General aq0000
Metropolitano np00000
de sp000
1976 w
. fp
Igualmente X
, X
la X
adecuación X
del X
espacio X
urbano X
de X
la X
ciudad X
se X
ha X
visto X
favorecida X
entre X
los X
siglos X
XIX X
y X
XXI X
por X
diversos X
eventos X
celebrados X
en X
la X
ciudad X
, X
como X
la X
Exposición X
Universal X
de X
1888 X
, X
la X
Internacional X
de X
1929 X
, X
el X
XXXV X
Congreso X
Eucarístico X
Internacional X
de X
1952 X
, X
los X
Juegos X
Olímpicos X
de X
1992 X
y X
el X
Fórum X
Universal X
de X
las X
Culturas X
de X
2004 X
. X
</code></pre>

<p>You will see that at the end there is a lot of tokens with the ""X"" tag.</p>

<p>Even things that look pretty obvious (like ""las""). </p>

<p>What does that token mean? I looked it up in the <a href=""https://nlp.stanford.edu/software/spanish-faq.shtml#tagset"" rel=""nofollow noreferrer"">site</a> but couldn't find anything about it.</p>
","java, stanford-nlp","<p>You set <code>pos.maxlen = 50</code> so it stops tagging after 50 tokens.</p>
",0,0,304,2019-01-21 20:48:01,https://stackoverflow.com/questions/54297565/stanford-nlp-pos-tag-x-in-spanish
Why does a Sentence parse throw an exception only the first time?,"<p>I am using the <code>edu.stanford.nlp.simple</code> package to generate parse trees for sentences in several different languages. The English and Chinese models produce the expected results, e.g.</p>

<pre><code>&gt; val s = new Sentence(""The quick brown fox jumps over the lazy dog."")
&gt; s.parse

res1: edu.stanford.nlp.trees.Tree = (ROOT (NP (NP (DT The) (JJ quick) (JJ brown) (NN fox)) (NP (NP (NNS jumps)) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog))))))
</code></pre>

<p>(I am using Scala here but that shouldn't make a difference.)</p>

<p>Other languages like German, however, exhibit a strange behavior:</p>

<pre><code>&gt; val p = new Properties()
&gt; p.load(IOUtils.readerFromString(""StanfordCoreNLP-german.properties""))
&gt; val s = new Sentence(""Ich hoffe, dass es funktionieren wird."")
&gt; s.parse(p)

10:48:34.127 [main] INFO  e.s.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/germanFactored.ser.gz ... done  [1.4sec].                                                                                                                                    
java.lang.NullPointerException                                                                                                                            
  edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:672)                                                   
  edu.stanford.nlp.simple.Document.runParse(Document.java:933)                                                                                            
  edu.stanford.nlp.simple.Sentence.parse(Sentence.java:637)                                                                                               
  ammonite.$sess.cmd3$.&lt;init&gt;(cmd3.sc:1)                                                                                                                  
  ammonite.$sess.cmd3$.&lt;clinit&gt;(cmd3.sc)

&gt; s.parse(p)
res4: edu.stanford.nlp.trees.Tree = (ROOT (S (PPER Ich) (VVFIN hoffe) ($, ,) (S (KOUS dass) (PPER es) (VP (VVINF funktionieren)) (VAFIN wird)) ($. .)))
</code></pre>

<p>I have examined the properties <code>p</code> to verify that they haven't changed -- the parser consistently throws a <code>NullPointerException</code> on the first invocation and then works correctly afterwards for the same sentence.</p>

<p>I have had a look through the source for CoreNLP but can't find an explicit reason why this might be happening... I wonder if I'm missing something?</p>

<p>I am using Stanford-CoreNLP version <code>3.9.1</code>. The foreign language models I'm referencing are the ones from the Maven repository, also discussed on the <a href=""https://stanfordnlp.github.io/CoreNLP/human-languages.html"" rel=""nofollow noreferrer"">Stanford CoreNLP website</a>.</p>
","java, scala, nlp, stanford-nlp","<p>Confirmed as a bug -- @StanfordNLPHelp's fix works for me.</p>
",0,0,63,2019-01-23 16:56:02,https://stackoverflow.com/questions/54332128/why-does-a-sentence-parse-throw-an-exception-only-the-first-time
ValueError: need more than 0 values to unpack - Glove,"<p>I downloaded the Glove model 
<a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a></p>

<p>and I train the Glove model use my own corpus, I didn't change anything else other than adding my own corpus. but it gives the following errors. </p>

<pre><code>01/30/19 - 11:15.01AM, iter: 015, cost: 0.062701
$ python eval/python/evaluate.py
Traceback (most recent call last):
  File ""eval/python/evaluate.py"", line 110, in &lt;module&gt;
    main()
  File ""eval/python/evaluate.py"", line 33, in main
    evaluate_vectors(W_norm, vocab, ivocab)
  File ""eval/python/evaluate.py"", line 66, in evaluate_vectors
    ind1, ind2, ind3, ind4 = indices.T
ValueError: need more than 0 values to unpack
</code></pre>
",glove,"<p>Although this error occurs, The corpus is generated correctly.</p>
",2,2,171,2019-01-30 06:00:50,https://stackoverflow.com/questions/54434179/valueerror-need-more-than-0-values-to-unpack-glove
How to feed CoreNLP some pre-labeled Named Entities?,"<p>I want to use Standford CoreNLP to pull out Coreferences and start working on the Dependencies of pre-labeled text.  I eventually hope to build graph nodes and edges between related Named Entities.  I am working in python, but using nltk's java functions to call the ""edu.stanford.nlp.pipeline.StanfordCoreNLP"" jar directly (which is what nltk does behind the scenes anyway).</p>

<p>My pre-labeled text is in this format:</p>

<pre><code>PRE-LABELED:  During his youth, [PERSON: Alexander III of Macedon] was tutored by [PERSON: Aristotle] until age 16.  Following the conquest of [LOCATION: Anatolia], [PERSON: Alexander] broke the power of [LOCATION: Persia] in a series of decisive battles, most notably the battles of [LOCATION: Issus] and [LOCATION: Gaugamela].  He subsequently overthrew [PERSON: Persian King Darius III] and conquered the [ORGANIZATION: Achaemenid Empire] in its entirety.
</code></pre>

<p>What I tried to do is tokenize my sentences myself, building a list of tuples in IOB format: [ (""During"",""O""), (""his"",""O""), (""youth"",""O""), (""Alexander"",""B-PERSON""), (""III"",""I-PERSON""), ...]</p>

<p>However, I can't figure out how to tell CoreNLP to take this tuple list as a starting point, building additional Named Entities that weren't initially labeled and finding coreferences on these new, higher-quality tokenized sentences.  I obviously tried simply striping out my labels, and letting CoreNLP do this by itself, but CoreNLP is just not as good at finding the Named Entities as the human-tagged pre-labeled text.</p>

<p>I need an output as below.  I understand that it will be difficult to use Dependencies to get Edges in this way, but I need to see how far I can get.</p>

<pre><code>DESIRED OUTPUT:
[Person 1]:
Name: Alexander III of Macedon
Mentions:
* ""Alexander III of Macedon""; Sent1 [4,5,6,7] # List of tokens
* ""Alexander""; Sent2 [6]
* ""He""; Sent3 [1]
Edges:
* ""Person 2""; ""tutored by""; ""Aristotle""

[Person 2]:
Name: Aristotle
[....]
</code></pre>

<p><strong>How can I feed CoreNLP some pre-identified Named Entities, and still get help with additional Named Entities, with Coreference, and with Basic Dependencies?</strong></p>

<p>P.S. Note that this is not a duplicate of <a href=""https://stackoverflow.com/questions/11333903/nltk-named-entity-recognition-with-custom-data"">NLTK Named Entity Recognition with Custom Data</a>.  I'm not trying to train a new classifier with my pre-labeled NER, I'm only trying to add CoreNLP's to my own when running coreference (including mentions) and dependencies on a given sentence.</p>
","python, nltk, stanford-nlp, named-entity-recognition","<p>The answer is to make a Rules file with <a href=""https://stanfordnlp.github.io/CoreNLP/ner.html#additional-tokensregexner-rules"" rel=""nofollow noreferrer""><strong>Additional TokensRegexNER Rules</strong></a>.</p>

<p>I used a regex to group out the labeled names.  From this I built a rules tempfile which I passed to the corenlp jar with <code>-ner.additional.regexner.mapping mytemprulesfile</code>.</p>

<pre><code>Alexander III of Macedon    PERSON      PERSON,LOCATION,ORGANIZATION,MISC
Aristotle                   PERSON      PERSON,LOCATION,ORGANIZATION,MISC
Anatolia                    LOCATION    PERSON,LOCATION,ORGANIZATION,MISC
Alexander                   PERSON      PERSON,LOCATION,ORGANIZATION,MISC
Persia                      LOCATION    PERSON,LOCATION,ORGANIZATION,MISC
Issus                       LOCATION    PERSON,LOCATION,ORGANIZATION,MISC
Gaugamela                   LOCATION    PERSON,LOCATION,ORGANIZATION,MISC
Persian King Darius III     PERSON      PERSON,LOCATION,ORGANIZATION,MISC
Achaemenid Empire           ORGANIZATION    PERSON,LOCATION,ORGANIZATION,MISC
</code></pre>

<p><em>I have aligned this list for readability, but these are tab-separated values.</em></p>

<p>An interesting finding is that some multi-word pre-labeled entities stay multi-word as originally labeled, whereas running corenlp without the rules files will sometimes split these tokens into separate entities.</p>

<p>I had wanted to specifically identify the named-entity tokens, figuring it would make coreferences easier, but I guess this will do for now.  How often are entity names identical but unrelated within one document, anyway?</p>

<p><strong>Example</strong> <em>(execution takes ~70secs)</em></p>

<pre><code>import os, re, tempfile, json, nltk, pprint
from subprocess import PIPE
from nltk.internals import (
    find_jar_iter,
    config_java,
    java,
    _java_options,
    find_jars_within_path,
)

def ExtractLabeledEntitiesByRegex( text, regex ):
    rgx = re.compile(regex)
    nelist = []
    for mobj in rgx.finditer( text ):
        ne = mobj.group('ner')
        try:
            tag = mobj.group('tag')
        except IndexError:
            tag = 'PERSON'
        mstr = text[mobj.start():mobj.end()]
        nelist.append( (ne,tag,mstr) )
    cleantext = rgx.sub(""\g&lt;ner&gt;"", text)
    return (nelist, cleantext)

def GenerateTokensNERRules( nelist ):
    rules = """"
    for ne in nelist:
        rules += ne[0]+'\t'+ne[1]+'\tPERSON,LOCATION,ORGANIZATION,MISC\n'
    return rules

def GetEntities( origtext ):
    nelist, cleantext = ExtractLabeledEntitiesByRegex( origtext, '(\[(?P&lt;tag&gt;[a-zA-Z]+)\:\s*)(?P&lt;ner&gt;(\s*\w)+)(\s*\])' )

    origfile = tempfile.NamedTemporaryFile(mode='r+b', delete=False)
    origfile.write( cleantext.encode('utf-8') )
    origfile.flush()
    origfile.seek(0)
    nerrulefile = tempfile.NamedTemporaryFile(mode='r+b', delete=False)
    nerrulefile.write( GenerateTokensNERRules(nelist).encode('utf-8') )
    nerrulefile.flush()
    nerrulefile.seek(0)

    java_options='-mx4g'
    config_java(options=java_options, verbose=True)
    stanford_jar = '../stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar'
    stanford_dir = os.path.split(stanford_jar)[0]
    _classpath = tuple(find_jars_within_path(stanford_dir))

    cmd = ['edu.stanford.nlp.pipeline.StanfordCoreNLP',
        '-annotators','tokenize,ssplit,pos,lemma,ner,parse,coref,coref.mention,depparse,natlog,openie,relation',
        '-ner.combinationMode','HIGH_RECALL',
        '-ner.additional.regexner.mapping',nerrulefile.name,
        '-coref.algorithm','neural',
        '-outputFormat','json',
        '-file',origfile.name
        ]

    # java( cmd, classpath=_classpath, stdout=PIPE, stderr=PIPE )
    stdout, stderr = java( cmd, classpath=_classpath, stdout=PIPE, stderr=PIPE )    # Couldn't get working- stdin=textfile
    PrintJavaOutput( stdout, stderr )

    origfilenametuple = os.path.split(origfile.name)
    jsonfilename = origfilenametuple[len(origfilenametuple)-1] + '.json'

    os.unlink( origfile.name )
    os.unlink( nerrulefile.name )
    origfile.close()
    nerrulefile.close()

    with open( jsonfilename ) as jsonfile:
        jsondata = json.load(jsonfile)

    currentid = 0
    entities = []
    for sent in jsondata['sentences']:
        for thisentity in sent['entitymentions']:
            tag = thisentity['ner']
            if tag == 'PERSON' or tag == 'LOCATION' or tag == 'ORGANIZATION':
                entity = {
                    'id':currentid,
                    'label':thisentity['text'],
                    'tag':tag
                }
                entities.append( entity )
                currentid += 1

    return entities

#### RUN ####
corpustext = ""During his youth, [PERSON:Alexander III of Macedon] was tutored by [PERSON: Aristotle] until age 16.  Following the conquest of [LOCATION: Anatolia], [PERSON: Alexander] broke the power of [LOCATION: Persia] in a series of decisive battles, most notably the battles of [LOCATION: Issus] and [LOCATION: Gaugamela].  He subsequently overthrew [PERSON: Persian King Darius III] and conquered the [ORGANIZATION: Achaemenid Empire] in its entirety.""

entities = GetEntities( corpustext )
for thisent in entities:
    pprint.pprint( thisent )
</code></pre>

<p><strong>Output</strong></p>

<pre><code>{'id': 0, 'label': 'Alexander III of Macedon', 'tag': 'PERSON'}
{'id': 1, 'label': 'Aristotle', 'tag': 'PERSON'}
{'id': 2, 'label': 'his', 'tag': 'PERSON'}
{'id': 3, 'label': 'Anatolia', 'tag': 'LOCATION'}
{'id': 4, 'label': 'Alexander', 'tag': 'PERSON'}
{'id': 5, 'label': 'Persia', 'tag': 'LOCATION'}
{'id': 6, 'label': 'Issus', 'tag': 'LOCATION'}
{'id': 7, 'label': 'Gaugamela', 'tag': 'LOCATION'}
{'id': 8, 'label': 'Persian King Darius III', 'tag': 'PERSON'}
{'id': 9, 'label': 'Achaemenid Empire', 'tag': 'ORGANIZATION'}
{'id': 10, 'label': 'He', 'tag': 'PERSON'}
</code></pre>
",0,0,702,2019-01-30 15:08:06,https://stackoverflow.com/questions/54443634/how-to-feed-corenlp-some-pre-labeled-named-entities
Drop in performance between stanford nlp 3.7.0 and 3.9.2,"<p>There seems to be a performance drop between in java for stanford nlp version 3.7.0 and 3.9.2.</p>

<p>I am running the following pipeline</p>

<p><code>props.put(""annotators"", ""tokenize, ssplit, pos, ner, parse, sentiment"");</code></p>

<p>and the following properties</p>

<pre><code>props.put(""ner.model"",
            ""edu/stanford/nlp/models    /ner/english.all.3class.distsim.crf.ser.gz"");
props.put(""ner.useSUTime"", ""false"");
props.put(""ner.applyNumericClassifiers"", ""false"");
</code></pre>

<p>When I upgraded from version 3.7.0 to 3.9.2 I am seeing CPU spikes and a fall in performance. Don't have numbers at the minute but seems to be around 5 times slower.</p>

<p>I am parsing small amount of text. A small news site article. </p>

<p>Maybe I should be using a different model?
Anyone else notice this?</p>

<p>Edit:
I have noticed that the 3.9.2 version loads model data for RegexNERAnnotator but 3.7.0 does not, seen this in the logs, not sure if this has an impact.</p>
",stanford-nlp,"<p>Yes, the reason is the rules-based NER is run by default now.</p>

<p>If you don't want the fine-grained named entities, you can deactivate it with</p>

<p><code>props.put(""ner.applyFineGrained"", ""false"");</code></p>
",1,0,101,2019-02-01 10:13:32,https://stackoverflow.com/questions/54477292/drop-in-performance-between-stanford-nlp-3-7-0-and-3-9-2
Getting Stanford NLP to recognise named entities with multiple words,"<p>First off let me say that I am a complete newbie with NLP. Although, as you read on, that is probably going to become strikingly apparent.</p>

<p>I'm parsing Wikipedia pages to find all mentions of the page title. I do this by going through the CorefChainAnnotations to find ""proper"" mentions - I then assume that the most common ones are talking about the page title. I do it by running this:</p>

<pre><code>    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    String content = ""Abraham Lincoln was an American politician and lawyer who served as the 16th President of the United States from March 1861 until his assassination in April 1865. Lincoln led the United States through its Civil War—its bloodiest war and perhaps its greatest moral, constitutional, and political crisis."";
    Annotation document = new Annotation(content);
    pipeline.annotate(document);
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
        List&lt;CorefChain.CorefMention&gt; corefMentions = cc.getMentionsInTextualOrder();
        for (CorefChain.CorefMention cm : corefMentions) {
            if (cm.mentionType == Dictionaries.MentionType.PROPER) {
                log(""Proper ref using "" + cm.mentionSpan + "", "" + cm.mentionType);
            }
        }
    }
</code></pre>

<p>This returns:</p>

<pre><code>Proper ref using the United States
Proper ref using the United States
Proper ref using Abraham Lincoln
Proper ref using Lincoln
</code></pre>

<p>I know already that ""Abraham Lincoln"" is definitely what I am looking for and I can surmise that because ""Lincoln"" appears a lot as well then that must be another way of talking about the main subject. (I realise right now the most common named entity is ""the United States"", but once I've fed it the whole page it works fine).</p>

<p>This works great until I have a page like ""Gone with the Wind"". If I change my code to use that:</p>

<pre><code>String content = ""Gone with the Wind has been criticized as historical revisionism glorifying slavery, but nevertheless, it has been credited for triggering changes to the way African-Americans are depicted cinematically."";
</code></pre>

<p>then I get no Proper mentions back at all. I suspect this is because none of the words in the title are recognised as named entities.</p>

<p>Is there any way I can get Stanford NLP to recognise ""Gone with the Wind"" as an already-known named entity? From looking around on the internet it seems to involve training a model, but I want this to be a known named entitity just for this single run and I don't want the model to remember this training later.</p>

<p>I can just imagine NLP experts rolling their eyes at the awfulness of this approach, but it gets better! I came up with the great idea of changing any occurences of the page title to ""Thingamijig"" before passing the text to Stanford NLP, which works great for ""Gone with the Wind"" but then fails for ""Abraham Lincoln"" because (I think) the NER longer associates ""Lincoln"" with ""Thingamijig"" in the corefMentions.</p>

<p>In my dream world I would do something like:</p>

<pre><code>    pipeline.addKnownNamedEntity(""Gone with the Wind"");
</code></pre>

<p>But that doesn't seem to be something I can do and I'm not exactly sure how to go about it.</p>
","java, stanford-nlp","<p>You can submit a dictionary with any phrases you want and have them recognized as named entities.</p>

<pre><code>java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -ner.additional.regexner.mapping additional.rules -file example.txt -outputFormat text
</code></pre>

<p>additional.rules</p>

<pre><code>Gone With The Wind    MOVIE    MISC    1
</code></pre>

<p>Note that the columns above should be tab-delimited.  You can have as many lines as you'd like in the <code>additional.rules</code> file.</p>

<p>One warning, EVERY TIME that token pattern occurs it will be tagged.</p>

<p>More details here: <a href=""https://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/ner.html</a></p>
",2,1,376,2019-02-01 17:41:02,https://stackoverflow.com/questions/54484544/getting-stanford-nlp-to-recognise-named-entities-with-multiple-words
Is there a way to remove a word from a KeyedVectors vocab?,"<p>I need to remove an invalid word from the vocab of a ""gensim.models.keyedvectors.Word2VecKeyedVectors"". </p>

<p>I tried to remove it using <code>del model.vocab[word]</code>, if I print the <code>model.vocab</code> the word disappeared, but when I run <code>model.most_similar</code> using other words the word that I deleted is still appearing as similar. 
So how can I delete a word from <code>model.vocab</code> in a way that affect the <code>model.most_similar</code> to not bring it?</p>
","gensim, word2vec, embedding, glove","<p>There's no existing method supporting the removal of individual words. </p>

<p>A quick-and-dirty workaround might be to, at the same time as removing the <code>vocab</code> entry, noting the <code>index</code> of the existing vector (in the underlying large vector array), and also changing the string in the <code>kv_model.index2entity</code> list at that index to some plug value (like say, <code>'***DELETED***'</code>). </p>

<p>Then, after performing any <code>most_similar()</code>, discard any entries matching <code>'***DELETED***'</code>. </p>
",2,2,1204,2019-02-14 19:26:53,https://stackoverflow.com/questions/54697748/is-there-a-way-to-remove-a-word-from-a-keyedvectors-vocab
"Apart from Keras and Spacy, can I use Stanford Core NLP for Deep Learning?","<p>I'm trying to perform sentiment analysis over twitter data using Deep Learning ( RNN ). I know that there are various other deep learning libraries out there like TF, keras , gensim etc., but i wanted to know if it is possible to perform deep learning using the CoreNLP Library. </p>

<p><a href=""https://github.com/charlescc9/deep-learning-sentiment-analysis"" rel=""nofollow noreferrer"">https://github.com/charlescc9/deep-learning-sentiment-analysis</a></p>

<p>This person above tries to compare gensim, tensorflow and core nlp for deep learning. But there's barely any documentation and i can't understand how to run the file (or) the dependecies required . Please help me out here.</p>
","deep-learning, nlp, stanford-nlp, lstm","<p>I have used RNN for the same reason before, and here is what I did:</p>
<h2>Getting Ready</h2>
<ol>
<li>Download the coreNLP package. You can do it from <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">here</a>.</li>
<li>Install <a href=""https://pypi.org/project/pycorenlp/"" rel=""nofollow noreferrer"">pycorenlp wrapper</a> by running <code>pip install pycorenlp</code>.</li>
<li>Install <code>Java&gt;=1.8</code> if it isn't installed.</li>
</ol>
<h2>Usage</h2>
<p>Now, let's see how to use it:</p>
<ol>
<li>Extract the downloaded zip file into your project's directory</li>
<li>Open the terminal and run the following:
<code>java -mx5g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000</code></li>
<li>Now, the server is running at <code>localhost:9000</code> by default. Now, you can write your program.</li>
</ol>
<p>Here is a simple example:</p>
<pre><code>&gt;&gt;&gt; from pycorenlp import StanfordCoreNLP
&gt;&gt;&gt;
&gt;&gt;&gt; sentence = &quot;NLP is great&quot;
&gt;&gt;&gt; nlp = StanfordCoreNLP('http://localhost:9000')
&gt;&gt;&gt; res = nlp.annotate(sentence, properties={ 'annotators': 'sentiment',
...                                           'outputFormat': 'json',
...                                           'timeout': 10000,})
&gt;&gt;&gt; #you can get the class by:
&gt;&gt;&gt; klass = res[&quot;sentences&quot;][0][&quot;sentimentValue&quot;]
</code></pre>
",0,-1,382,2019-02-16 10:24:22,https://stackoverflow.com/questions/54722074/apart-from-keras-and-spacy-can-i-use-stanford-core-nlp-for-deep-learning
Is it possible to freeze only certain embedding weights in the embedding layer in pytorch?,"<p>When using GloVe embedding in NLP tasks, some words from the dataset might not exist in GloVe. Therefore, we instantiate random weights for these unknown words.</p>

<p>Would it be possible to freeze weights gotten from GloVe, and train only the newly instantiated weights?</p>

<p>I am only aware that we can set:
model.embedding.weight.requires_grad = False</p>

<p>But this makes the new words untrainable..</p>

<p>Or are there better ways to extract semantics of words.. </p>
","python, nlp, pytorch, word-embedding, glove","<h1>1. Divide embeddings into two separate objects</h1>
<p>One approach would be to use two separate embeddings <strong>one for pretrained</strong>, another for the one <strong>to be trained</strong>.</p>
<p>The GloVe one should be frozen, while the one for which there is no pretrained representation would be taken from the trainable layer.</p>
<p>If you format your data that for pretrained token representations it is in smaller range than the tokens without GloVe representation it could be done. Let's say your pretrained indices are in the range [0, 300], while those without representation are [301, 500]. I would go with something along those lines:</p>
<pre><code>import numpy as np
import torch


class YourNetwork(torch.nn.Module):
    def __init__(self, glove_embeddings: np.array, how_many_tokens_not_present: int):
        self.pretrained_embedding = torch.nn.Embedding.from_pretrained(glove_embeddings)
        self.trainable_embedding = torch.nn.Embedding(
            how_many_tokens_not_present, glove_embeddings.shape[1]
        )
        # Rest of your network setup

    def forward(self, batch):
        # Which tokens in batch do not have representation, should have indices BIGGER
        # than the pretrained ones, adjust your data creating function accordingly
        mask = batch &gt; self.pretrained_embedding.num_embeddings

        # You may want to optimize it, you could probably get away without copy, though
        # I'm not currently sure how
        pretrained_batch = batch.copy()
        pretrained_batch[mask] = 0

        embedded_batch = self.pretrained_embedding(pretrained_batch)

        # Every token without representation has to be brought into appropriate range
        batch -= self.pretrained_embedding.num_embeddings
        # Zero out the ones which already have pretrained embedding
        batch[~mask] = 0
        non_pretrained_embedded_batch = self.trainable_embedding(batch)

        # And finally change appropriate tokens from placeholder embedding created by
        # pretrained into trainable embeddings.
        embedded_batch[mask] = non_pretrained_embedded_batch[mask]

        # Rest of your code
        ...
</code></pre>
<p>Let's say your pretrained indices are in the range [0, 300], while those without representation are [301, 500].</p>
<h1>2. Zero gradients for specified tokens.</h1>
<p>This one is a bit tricky, but I think it's pretty concise and easy to implement. So, if you obtain the indices of tokens which got no GloVe representation, you can explicitly zero their gradient after backprop, so those rows will not get updated.</p>
<pre><code>import torch

embedding = torch.nn.Embedding(10, 3)
X = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])

values = embedding(X)
loss = values.mean()

# Use whatever loss you want
loss.backward()

# Let's say those indices in your embedding are pretrained (have GloVe representation)
indices = torch.LongTensor([2, 4, 5])

print(&quot;Before zeroing out gradient&quot;)
print(embedding.weight.grad)

print(&quot;After zeroing out gradient&quot;)
embedding.weight.grad[indices] = 0
print(embedding.weight.grad)
</code></pre>
<p>And the output of the second approach:</p>
<pre><code>Before zeroing out gradient
tensor([[0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0833, 0.0833, 0.0833],
        [0.0417, 0.0417, 0.0417],
        [0.0833, 0.0833, 0.0833],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417]])
After zeroing out gradient
tensor([[0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417]])
</code></pre>
",20,16,8933,2019-02-28 11:23:39,https://stackoverflow.com/questions/54924582/is-it-possible-to-freeze-only-certain-embedding-weights-in-the-embedding-layer-i
Create word embeddings without keeping fastText Vector file in the repository,"<p>I am trying to embed a sentence with the help of <a href=""https://github.com/facebookresearch/InferSent"" rel=""nofollow noreferrer"">Infersent</a>, and Infersent uses <a href=""https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip"" rel=""nofollow noreferrer"">fastText</a> vectors for word embedding. The fastText vector file is close to 5 GiB.</p>

<p>When we keep the fastText vector file along with the code repository it makes the repository size huge, and makes the code difficult to share/deploy (even creating a docker container).</p>

<p>Is there any method to avoid keeping the vector file along with the repository, but reuse it for embedding new sentences?</p>
","nlp, fasttext, glove","<p>What kind of sentences are you embedding, is it the same domain as the one on which fastText embeddings were generated?</p>

<p>Try to get a representation of your data in tokens i.e, a set of all tokens, or some representations of the most common tokens that appear in the sentences you want to embed using fastText.</p>

<p>Compute the overlap of your tokens with the tokens in fastText, remove the ones from fastText which don't appear in your data representation.</p>

<p>I did that recently and went from a 1.4GB file with some pre-trained word embeddings to 200 MB, mainly because the overlap with my corpus was around 10%.</p>
",1,1,199,2019-03-05 17:52:22,https://stackoverflow.com/questions/55008804/create-word-embeddings-without-keeping-fasttext-vector-file-in-the-repository
Illegal Hardware Instruction Error when using GloVe,"<p>I am trying to train GloVe embeddings. In the GloVe implementation from <a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">stanfordnlp</a> there are 4 scripts to run. However, running the second script, <code>coocur</code>, results in an <code>Illegal Hardware Instruction</code>-Error. I don't understand how this error is produced. </p>

<p>With the input file <code>3.txt</code> my commands look like this:</p>

<pre><code>$ ./vocab_count -min-count 1 -verbose 2 &lt; 3.txt &gt; vocab.txt
BUILDING VOCABULARY
Processed 8354 tokens.
Counted 3367 unique words.
Using vocabulary of size 3367.

$ ./cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 &lt; 3.txt &gt; cooccurrence.bin
zsh: illegal hardware instruction  ./cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 &lt; 3.tx
</code></pre>

<p>I am running these commands on a remote server (Debian GNU/Linux 9 (stretch)). When I run the same commands on the same data locally (18.04.2 LTS (Bionic Beaver)), there is no problem. What could be the cause of this?</p>
","nlp, stanford-nlp, word-embedding, glove, illegal-instruction","<p>I've hit the same issue in recent days. </p>

<p>The Docker image was built on a server using Jenkins. It has been running fine until the underlying cluster host orchestration software and physical hardware was upgraded. </p>

<p>My solution has been to remove the build of GloVe from the Dockerfile and instead put the build/make inside a script which runs when the container starts. </p>

<p>The actual cause of the error may be caused by the <code>CFLAGS</code>: <code>-march=native</code> set in the Glove Makefile: <a href=""https://github.com/stanfordnlp/GloVe/blob/07d59d5e6584e27ec758080bba8b51fce30f69d8/Makefile#L4"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/blob/07d59d5e6584e27ec758080bba8b51fce30f69d8/Makefile#L4</a> This will cause the GloVe build to rely on the underlying CPU instruction set on which the Docker image is built. </p>

<p>There's a discussion of this further here: <a href=""https://stackoverflow.com/questions/54039176/mtune-and-march-when-compiling-in-a-docker-image"">mtune and march when compiling in a docker image</a></p>
",2,0,1095,2019-03-08 11:15:09,https://stackoverflow.com/questions/55062077/illegal-hardware-instruction-error-when-using-glove
"Masked language model processing, deeper explanation","<p>I'm looking to BERT model (<a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">you can found the description here</a>) in detail and I'm getting problem to understand clearly the need to keep or replace random word 20% of the time instead or just use [MASK] token always for the masked language model.</p>

<p>We try to train the bidirectional technique and the article explains ""[MASK] token is never seen during fine-tuning"" but it is two different steps for me, we train first bidirectional and after we downstream task.</p>

<p>If someone can explain to me where I'm wrong in my comprehension.</p>
","nlp, stanford-nlp","<p>If you don't use random replacement during training your network won't learn to extract useful features from non-masked tokens.</p>

<p>in other words, if you only use masking and try to predict them, it will be a waste of resources for your network to extract good features for the non-masked tokens(remember that your network is as good as your task and it will try to find the easiest way to solve your task)</p>
",1,0,1080,2019-03-08 15:09:40,https://stackoverflow.com/questions/55066010/masked-language-model-processing-deeper-explanation
How to use StanfordNLP Python package to do dependency parsing?,"<p>I am trying to use the new NN-based parser at <a href=""https://github.com/stanfordnlp/stanfordnlp"" rel=""nofollow noreferrer"">here</a> to find all adjective phrases in a sentence (e.g., <code>good</code> and <code>extremely good</code> in <code>The weather is extremely good</code>), however, it's very lack of documentation and I could not get it working. My current code is</p>

<pre><code>import stanfordnlp
nlp = stanfordnlp.Pipeline()
doc = nlp(""The weather is extremely good"")
doc.sentences[0].print_dependencies()
</code></pre>

<p>which gives me</p>

<pre><code>('The', '2', 'det')
('weather', '5', 'nsubj')
('is', '5', 'cop')
('extremely', '5', 'advmod')
('good', '0', 'root')
</code></pre>

<p>But it is not clear how to extract the information I need, as this does not seem to be a tree structure. Does anyone have an idea?</p>
","parsing, nlp, stanford-nlp","<p>At this time there is not Python support for constituency parses which is what you want.  This is just returning the dependency parses (a different type of parse).</p>

<p>You can use <code>stanfordnlp</code> to communicate with a Java server and get constituency parses in that manner.</p>

<p>There is example code here for accessing the constituency parses:</p>

<p><a href=""https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html</a></p>
",0,3,1950,2019-03-11 02:52:02,https://stackoverflow.com/questions/55094637/how-to-use-stanfordnlp-python-package-to-do-dependency-parsing
Importing StanfordNER Tagger Google Colab,"<p>I am having some issues when trying to import StanfordNER Tagger to use for NER. Here is my code (took portions of this from other posts here):</p>

<pre><code>import os
def install_java():
  !apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null
  os.environ[""JAVA_HOME""] = ""/usr/lib/jvm/java-8-openjdk-amd64""
  !java -version
install_java()

!pip install StanfordCoreNLP
from stanfordcorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('stanford-corenlp', lang='en', memory='4g')
</code></pre>

<p>The error I am getting highlights the last line of code telling me:</p>

<pre><code>OSError: stanford-corenlp is not a directory.
</code></pre>

<p>Any help would be great!</p>

<p>Edit: Here is another line of code that worked for me. For what inside StanfordNERTagger, load those files into Colab and give the path name. Do the same for what I originally asked as my problem above. Worked for me.</p>

<pre><code>from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize



st = StanfordNERTagger('/content/english.muc.7class.distsim.crf.ser.gz',
                   '/content/stanford-ner.jar',
                   encoding='utf-8')

text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'

tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)

print(classified_text)
</code></pre>
","python, stanford-nlp, google-colaboratory","<p>The following code downloads all the required files and also sets the environment:</p>

<pre class=""lang-py prettyprint-override""><code>from nltk.tag.stanford import StanfordNERTagger
from nltk.tokenize import word_tokenize
import nltk

!wget 'https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip'
!unzip stanford-ner-2018-10-16.zip

nltk.download('punkt')

st = StanfordNERTagger('/content/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',
                       '/content/stanford-ner-2018-10-16/stanford-ner.jar',
                       encoding='utf-8')

text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'

tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)
</code></pre>
",3,5,1684,2019-03-12 19:54:35,https://stackoverflow.com/questions/55129698/importing-stanfordner-tagger-google-colab
Error with NLTK package and other dependencies,"<p>I have installed the NLTK package and other dependencies and set the environment variables as follows:</p>

<pre><code>STANFORD_MODELS=/mnt/d/stanford-ner/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz:/mnt/d/stanford-ner/stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz:/mnt/d/stanford-ner/stanford-ner-2018-10-16/classifiers/english.conll.4class.distsim.crf.ser.gz

CLASSPATH=/mnt/d/stanford-ner/stanford-ner-2018-10-16/stanford-ner.jar
</code></pre>

<p>When I try to access the classifier like below:</p>

<pre><code>stanford_classifier = os.environ.get('STANFORD_MODELS').split(':')[0]

stanford_ner_path = os.environ.get('CLASSPATH').split(':')[0]

st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')
</code></pre>

<p>I get the following error.  But I don't understand what is causing this error.</p>

<pre><code>Error: Could not find or load main class edu.stanford.nlp.ie.crf.CRFClassifier
OSError: Java command failed : ['/mnt/c/Program Files (x86)/Common 
Files/Oracle/Java/javapath_target_1133041234/java.exe', '-mx1000m', '-cp', '/mnt/d/stanford-ner/stanford-ner-2018-10-16/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/mnt/d/stanford-ner/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz', '-textFile', '/tmp/tmpaiqclf_d', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf8']
</code></pre>
","python, nlp, stanford-nlp, named-entity-recognition","<p>I found the answer for this issue. I am using NLTK == 3.4. From NLTK ==3.3 and above Stanford NLP (POS, NER , tokenizer) are not loaded as part of nltk.tag but from nltk.parse.corenlp.CoreNLPParser. The stackoverflow answer is available in stackoverflow.com/questions/13883277/stanford-parser-and-nltk/… and the github link for official documentation is github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK.</p>

<p>Additional information if you are facing timeout issue from the NER tagger or any other parser of coreNLP API, please increase the timeout limit as stated in <a href=""https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK/_compare/3d64e56bede5e6d93502360f2fcd286b633cbdb9...f33be8b06094dae21f1437a6cb634f86ad7d83f7"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK/_compare/3d64e56bede5e6d93502360f2fcd286b633cbdb9...f33be8b06094dae21f1437a6cb634f86ad7d83f7</a> by dimazest.</p>
",1,1,210,2019-03-15 14:38:15,https://stackoverflow.com/questions/55185021/error-with-nltk-package-and-other-dependencies
Stanford NLP sentiment prediction bug; differs from live demo,"<p>I wanted to find a sentiment for the following sentence (tweet):</p>

<blockquote>
  <p>After today's turnaround by #Boeing , the $SPX is heading for the best weekly gain since November led by #tech stocks that are flying!</p>
</blockquote>

<p>Some experimenting with NLP sentiment predictions lead me to the following code:</p>

<pre><code>val pipeline = {
  val props = new Properties()
  props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse, sentiment"")
  props.setProperty(""outputFormat"", ""json"")
  new StanfordCoreNLP(props)
}
val text = ""After today's turnaround by #Boeing , the $SPX is heading for the best weekly gain since November led by #tech stocks that are flying!""
val annotation: Annotation = pipeline.process(text)
val sentences = annotation.get(classOf[CoreAnnotations.SentencesAnnotation])
pipeline.annotate(annotation)
println(JSONOutputter.jsonPrint(sentences.head.get(classOf[SentimentCoreAnnotations.SentimentAnnotatedTree]))
</code></pre>

<p>which returns negative sentiment contrary to the intuitions and results from live demo <a href=""http://nlp.stanford.edu:8080/sentiment/rntnDemo.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/sentiment/rntnDemo.html</a> (positive with 40% probability).</p>

<p>What am I missing here?</p>
","machine-learning, nlp, stanford-nlp","<p>Lemmatizer is an normally optional in a typical NLP pipeline. One has to do extrinsic/intrinsic evaluation with and without various components in the pipeline. Try dropping lemmatizer and adding NER</p>
",0,0,49,2019-03-20 14:36:31,https://stackoverflow.com/questions/55263332/stanford-nlp-sentiment-prediction-bug-differs-from-live-demo
Stanford NLP : Constituency parser in French,"<p>I want to use the Stanford NLP constituency parser in my Python program.<br>
My server is working well.</p>

<hr>

<p><strong>What I want</strong><br>
I want to obtain the result that I have when using the server in a web browser :</p>

<p><a href=""https://i.sstatic.net/7lG3Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7lG3Z.png"" alt=""The result I want""></a></p>

<hr>

<p><strong>My code</strong><br>
Here is my code :</p>

<pre><code>from stanfordnlp.server import CoreNLPClient
from nltk.tree import Tree
with CoreNLPClient(annotators=[ 'tokenize','ssplit','pos','parse'],
                   timeout=30000,
                   output_format=""json"",
                   properties={'tokenize.language' :'fr',
                               'pos.model' : 'edu/stanford/nlp/models/pos-tagger/french/french.tagger',
                               'parse.model' : 'edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz'}) as client :
    ann = client.annotate(text)

output = ann['sentences'][0]['parse']
parsetree = Tree.fromstring(output)
parsetree.pretty_print()
</code></pre>

<hr>

<p><strong>My current result</strong><br>
My current result is like that :</p>

<pre><code>                         ROOT                 
                          |                    
                         SENT                 
    ______________________|________________    
   |           NP                          |  
   |      _____|__________                 |   
   |     |     |          PP               |  
   |     |     |      ____|____            |   
   VN    |     |     |         NP          |  
   |     |     |     |     ____|_______    |   
   V    DET    NC    P   ADV   P  DET  NC PUNC
   |     |     |     |    |    |   |   |   |   
Cherche les enfants  de moins  de  3  ans  .  
</code></pre>

<p>We can see that the structure is not the same ...<br>
Do you have an idea why my result is different ?  </p>
","python, stanford-nlp","<p>I finally found the solution :</p>

<p>Here are the basecode that define the pipeline for french :</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-french.properties"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-french.properties</a></p>

<p>So the parse model to use in order to have the same result is :</p>

<pre><code>parse.model = edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz
</code></pre>
",0,0,1068,2019-03-21 15:06:07,https://stackoverflow.com/questions/55283519/stanford-nlp-constituency-parser-in-french
Stanford coreNLP splitting paragraph sentences without whitespace,"<p>I faced a problem with stanford's Sentence annotator.
As an input I've got the text, which contains sentences, but there is no whitespace after dot in some parts of it. Like this:</p>
<blockquote>
<p>Dog loves cat.Cat loves mouse. Mouse hates everybody.</p>
</blockquote>
<p>So when I'm trying to use SentenceAnnotator - I'm getting 2 sentences</p>
<blockquote>
<p>Dog loves cat.Cat loves mouse.</p>
<p>Mouse hates everybody.</p>
</blockquote>
<p>Here is my code</p>
<pre><code>Annotation doc = new Annotation(t);
Properties props = new Properties();
props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,parse,coref&quot;);
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
pipeline.annotate(doc);
List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
</code></pre>
<p>I also tried to add property</p>
<pre><code>props.setProperty(&quot;ssplit.boundaryTokenRegex&quot;, &quot;\\.&quot;);
</code></pre>
<p>but no effect.</p>
<p>Maybe I'm missing something?
Thanks!</p>
<p>UPD
Also I tried to tokenize text using PTBTokenizer</p>
<pre><code>PTBTokenizer ptbTokenizer = new PTBTokenizer(
        new FileReader(classLoader.getResource(&quot;simplifiedParagraphs.txt&quot;).getFile())
        ,new WordTokenFactory()
        ,&quot;untokenizable=allKeep,tokenizeNLs=true,ptb3Escaping=true,strictTreebank3=true,unicodeEllipsis=true&quot;);
List&lt;String&gt; strings = ptbTokenizer.tokenize();
</code></pre>
<p>but tokenizer thinks that cat.Cat is single word and doesn't split it.</p>
","java, nlp, stanford-nlp","<p>This is a pipeline where the sentence splitter is going to identify sentence boundaries for the tokens provided by the tokenizer, but the sentence splitter only groups adjacent tokens into sentences, it doesn't try to merge or split them.</p>

<p>As you found, I think that the <code>ssplit.boundaryTokenRegex</code> property would tell the sentence splitter to end a sentence when it sees ""."" as a token, but this doesn't help in cases where the tokenizer hasn't split the ""."" apart from surrounding text into a separate token.</p>

<p>You will need to either:</p>

<ul>
<li>preprocess your text (insert a space after ""cat.""),</li>
<li>postprocess your tokens or sentences to split cases like this, or</li>
<li>find/develop a tokenizer that can split ""cat.Cat"" into three tokens.</li>
</ul>

<p>None of the standard English tokenizers, which are typically intended to be used with newspaper text, have been developed to handle this kind of text.</p>

<p>Some related questions:</p>

<p><a href=""https://stackoverflow.com/questions/51693199/does-the-nltk-sentence-tokenizer-assume-correct-punctuation-and-spacing"">Does the NLTK sentence tokenizer assume correct punctuation and spacing?</a></p>

<p><a href=""https://stackoverflow.com/questions/42445842/how-to-split-text-into-sentences-when-there-is-no-space-after-full-stop"">How to split text into sentences when there is no space after full stop?</a></p>
",2,0,1010,2019-03-26 08:34:00,https://stackoverflow.com/questions/55352808/stanford-corenlp-splitting-paragraph-sentences-without-whitespace
How to set Stanford-NLP simple API in french?,"<p>I'm trying to use stanford-nlp in french in netbeans.
I'm using netbeans 10.0 and stanford-nlp 3.9.2. </p>

<p>I'm using maven and I set this dependency in my pom.</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.9.2&lt;/version&gt;
    &lt;type&gt;jar&lt;/type&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.9.2&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.9.2&lt;/version&gt;
    &lt;classifier&gt;models-french&lt;/classifier&gt;
&lt;/dependency
</code></pre>

<p>And there is an example code in java.</p>

<pre><code>Document doc = new Document(props_fr,""Ceci est mon texte en français. Il contient plusieurs phrases."");
        for (Sentence sent : doc.sentences()) {
            System.out.println(sent.parse());
        }
</code></pre>

<p>I expect the ouput to be (using <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/</a>): </p>

<blockquote>
  <p>(ROOT(SENT(NP (PRO Ceci))(VN (V est))(NP (DET mon)NC texte))(PP (P
  en)(NP (NC français)))(PUNC .)))</p>
  
  <p>(ROOT(SENT(VN (CLS Il) (V contient))(NP (DET plusieurs) (NC
  phrases))(PUNC .)))</p>
</blockquote>

<p>But the actual output is: </p>

<blockquote>
  <p>(ROOT (NP (NP (NNP Ceci) (NNP est)) (NP (NP (NN mon) (NN texte)) (PP
  (IN en) (NP (NN français)))) (. .))) (ROOT (NP (NP (NN Il)) (NP (JJ
  contient) (NNS plusieurs) (NNS phrases)) (. .)))</p>
</blockquote>
","java, netbeans, stanford-nlp","<p>Did you try to load the StanfordCoreNLP-french.properties?</p>

<pre><code>Properties props = new Properties();
props.load(IOUtils.readerFromString(""StanfordCoreNLP-french.properties""));
StanfordCoreNLP corenlp = new StanfordCoreNLP(props);

Annotation ann = corenlp.process(""Ceci est mon texte en français. Il contient plusieurs phrases."");
Document doc = new Document(props, ann);
</code></pre>
",0,0,485,2019-04-08 09:32:55,https://stackoverflow.com/questions/55570301/how-to-set-stanford-nlp-simple-api-in-french
The meaning of hyperparameters in glove,"<p>The following is hyperparameters in <a href=""https://github.com/stanfordnlp/GloVe/blob/master/demo.sh"" rel=""nofollow noreferrer"">demo.sh of glove</a>. What is the meaning of <code>VERBOSE</code>, <code>MEMORY</code>, <code>WINDOW_SIZE</code> and <code>BINARY</code>.</p>

<p>The <code>WINDOW_SIZE</code> is 15, is that means chose 15 words from right and chose 15 words from left?</p>

<pre><code>VERBOSE=2        ?
MEMORY=4.0       ?
VOCAB_MIN_COUNT=5
VECTOR_SIZE=128
MAX_ITER=15
WINDOW_SIZE=15   ?
BINARY=2         ?
NUM_THREADS=8
X_MAX=100
</code></pre>
","nlp, hyperparameters, glove","<p><code>VERBOSE</code> is a regular parameter for model training nowadays, its value tells the function how much information to print while training the model. Usually <code>0</code> means no intermediate information, <code>1</code> means minimal and <code>2</code> means a lot more detail. (check: <a href=""https://github.com/stanfordnlp/GloVe/blob/master/src/cooccur.c"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/blob/master/src/cooccur.c</a>) for more details about what are printed.</p>

<p><code>MEMORY</code>: I'm not quite sure about this but I think it has to do with the memory usage for the model training. (feel free to correct &amp; update)</p>

<p><code>WINDOW_SIZE</code>: yes, it's the context size (check: <a href=""https://github.com/stanfordnlp/GloVe/blob/master/src/cooccur.c"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/blob/master/src/cooccur.c</a>)</p>

<p><code>BINARY</code>: It's a switch option for file output type. <code>0</code> for text file, <code>1</code> for binary, and <code>2</code> for both (check: <a href=""https://github.com/stanfordnlp/GloVe/blob/master/src/glove.c"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/blob/master/src/glove.c</a>).</p>
",1,1,679,2019-04-09 03:39:15,https://stackoverflow.com/questions/55584776/the-meaning-of-hyperparameters-in-glove
CoreNLP: Can it tell whether a noun refers to a person?,"<p>Can CoreNLP determine whether a common noun (as opposed to a proper noun or proper name) refers to a person out-of-the-box? Or if I need to train a model for this task, how do I go about that? </p>

<p>First, I am <em>not</em> looking for coreference resolution, but rather a building block for it. Coreference by definition depends on the context, whereas I am trying to evaluate whether a word <em>in isolation</em> is a subset of ""person"" or ""human"". For example:</p>

<pre><code>is_human('effort') # False
is_human('dog') # False
is_human('engineer') # True
</code></pre>

<p>My naive attempt to use Gensim's and spaCy's pre-trained word vectors failed to rank ""engineer"" above the other two words.</p>

<pre><code>import gensim.downloader as api
word_vectors = api.load(""glove-wiki-gigaword-100"") 
for word in ('effort', 'dog', 'engineer'):
    print(word, word_vectors.similarity(word, 'person'))

# effort 0.42303842
# dog 0.46886832
# engineer 0.32456854
</code></pre>

<p>I found the following lists from <a href=""https://nlp.stanford.edu/software/dcoref.html"" rel=""nofollow noreferrer"">CoreNLP</a> promising. </p>

<pre><code>dcoref.demonym                   // The path for a file that includes a list of demonyms 
dcoref.animate                   // The list of animate/inanimate mentions (Ji and Lin, 2009)
dcoref.inanimate 
dcoref.male                      // The list of male/neutral/female mentions (Bergsma and Lin, 2006) 
dcoref.neutral                   // Neutral means a mention that is usually referred by 'it'
dcoref.female 
dcoref.plural                    // The list of plural/singular mentions (Bergsma and Lin, 2006)
dcoref.singular
</code></pre>

<p>Would these work for my task? And if so, how would I access them from the <a href=""https://pypi.org/project/corenlp-python/"" rel=""nofollow noreferrer"">Python wrapper</a>? Thank you.</p>
","nlp, stanford-nlp, pycorenlp","<p>I would suggest trying <a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a> instead and see:</p>

<ol>
<li>if enough of your terms are covered by WordNet and</li>
<li>if the terms you want are hyponyms of <code>person.n.01</code>.</li>
</ol>

<p>You'd have to expand this a bit to cover multiple senses, but the gist would be:</p>

<pre><code>from nltk.corpus import wordnet as wn

# True
wn.synset('person.n.01') in wn.synset('engineer.n.01').lowest_common_hypernyms(wn.synset('person.n.01'))

# False
wn.synset('person.n.01') in wn.synset('dog.n.01').lowest_common_hypernyms(wn.synset('person.n.01'))

</code></pre>

<p>See the NLTK docs for <code>lowest_common_hypernym</code>: <a href=""http://www.nltk.org/howto/wordnet_lch.html"" rel=""nofollow noreferrer"">http://www.nltk.org/howto/wordnet_lch.html</a></p>
",1,0,438,2019-04-10 22:40:43,https://stackoverflow.com/questions/55622251/corenlp-can-it-tell-whether-a-noun-refers-to-a-person
Stanford NLP Dedicated Server Max Character Limit,"<p>I'm trying to create a keyword extractor that goes through some documents at work and grabs all the main keywords. For the majority of my documents, it works great as they are emails or small documents, but I am starting to get a lot of documents that are timing out.</p>

<p>To fix the timing out, I thought that I could just use the sentence splitting annotator to build a list of sentences and then send a comfortable amount of sentences at a time. The only problem with this idea is that the server is telling me that it can only take 100000 characters.</p>

<p>I am using the Server API. How can I go about updating the server to take more characters or only grabbing up to 100000 characters of a string without losing any of the information or integrity of a sentence (e.g. not cutting off half a sentence at the end)?</p>

<p>NOTE: <a href=""https://stackoverflow.com/questions/46678204/how-to-work-around-100k-character-limit-for-the-stanfordnlp-server"">This</a> (How to work around 100K character limit for the StanfordNLP server?
) does not work for me. I believe it relates to an older version. Either way, I have tried to add <code>-maxCharLength -1</code> to my start script and it doesn't do anything.</p>

<p>My start script currently is </p>

<p><code>java -mx8g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000 -quiet true
pause</code></p>
","c#, stanford-nlp","<p>It should definitely work if you just set maxCharLength to a very large number and are using Stanford CoreNLP 3.9.2.  I tested this out and tokenized a document with 220000 characters for instance.  I think the ""-1"" is causing issues when running at the command line.</p>
",1,1,315,2019-04-12 05:17:24,https://stackoverflow.com/questions/55644922/stanford-nlp-dedicated-server-max-character-limit
Encoding problem while training my own Glove model,"<p>I am training a GloVe model with my own corpus and I have troubles to save it/load it in an <code>utf-8</code> format.</p>

<p>Here what I tried: </p>

<pre><code>from glove import Corpus, Glove

#data
lines = [['woman', 'umbrella', 'silhouetted'], ['person', 'black', 'umbrella']]

#GloVe training
corpus = Corpus() 
corpus.fit(lines, window=4)
glove = Glove(no_components=4, learning_rate=0.1)
glove.fit(corpus.matrix, epochs=10, no_threads=8, verbose=True)
glove.add_dictionary(corpus.dictionary)
glove.save('glove.model.txt')
</code></pre>

<p>The saved file <code>glove.model.txt</code> is unreadable and I can't succeed to save it with a <code>utf-8</code> encoding.</p>

<p>When I try to read it, for exemple by converting it in a Word2Vec format:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove2word2vec(glove_input_file=""glove.model.txt"", 
word2vec_output_file=""gensim_glove_vectors.txt"")    

model = KeyedVectors.load_word2vec_format(""gensim_glove_vectors.txt"", binary=False)
</code></pre>

<p>I have the following error:</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>Any idea on how I could use my own GloVe model ?  </p>
","python, encoding, nlp, word-embedding, glove","<p>I just found myself a way to save the data with an <code>utf-8</code> format, I'm sharing it here in case someone faces the same problem</p>

<p>Instead of using the glove saving method <code>glove.save('glove.model.txt')</code> try to simulate by yourself a glove record:</p>

<pre><code>with open(""results_glove.txt"", ""w"") as f:
    for word in glove.dictionary:
        f.write(word)
        f.write("" "")
        for i in range(0, vector_size):
            f.write(str(glove.word_vectors[glove.dictionary[word]][i]))
            f.write("" "")
        f.write(""\n"")
</code></pre>

<p>Then you will be able to read it. </p>
",2,1,1452,2019-04-15 16:13:35,https://stackoverflow.com/questions/55693318/encoding-problem-while-training-my-own-glove-model
How to connect with coreNLP server with username password?,"<p>I have created coreNLP server in Google App Engine and I have provided username and password over there  using following properties: -port **** -username <strong>user</strong> -password <strong>pass</strong>.</p>

<p>And from Python, I have come to know to pass username and password in properties.</p>

<p>First I am creating NLP object using following function and passing URL:</p>

<pre><code>sNLP = StanfordCoreNLP(serverurl)
</code></pre>

<p>Then I am calling annotate with data and property , and my username and password is there in props.</p>

<pre><code>props: {
    ""annotators"": ""pos,sentiment"",
    ""pipelineLanguage"": ""en"",
    ""outputFormat"": ""json"",
    ""username"": ""**user**"",
    ""password"": ""***pass***"",
    ""maxCharLength"": -1
}

sNLP.annotate(data, props)
</code></pre>

<p>before providing authentication it was working fine, but after that I am not getting any response from server.</p>

<p>So should I pass username and password while creating sNLP object? If yes then how? I haven't found any documentation related to that.</p>
","python, stanford-nlp","<p>A solution for this will soon be in the next release of our Python code which should be very soon, though it is not in the current version.</p>

<p><code>pip install stanfordnlp</code></p>

<p>Repo is here: <a href=""https://github.com/stanfordnlp/stanfordnlp"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/stanfordnlp</a></p>

<p>If you clone that and checkout <code>dev</code> and run <code>pip install -e .</code> you can get the latest <code>dev</code> branch code.</p>

<p>Example:</p>

<pre><code>from stanfordnlp.server import CoreNLPClient

with CoreNLPClient(annotators=""tokenize,ssplit,pos"", memory=""4G"", server_id='my_server_id', username='user-1234', password='1234') as client:
    ann = client.annotate('Hello World!', output_format='text', username='user-1234', password='1234')
    print(ann)
</code></pre>

<p>If you want to attempt other solutions, note that this code is setting <code>auth</code> to a <code>requests.auth.HTTPBasicAuth(username, password)</code> when making a POST request with the <code>requests</code> library.</p>
",0,0,193,2019-04-17 11:01:10,https://stackoverflow.com/questions/55726031/how-to-connect-with-corenlp-server-with-username-password
Load a part of Glove vectors with gensim,"<p>I have a word list like<code>['like','Python']</code>and I want to load pre-trained Glove word vectors of these words, but the Glove file is too large, is there any fast way to do it? </p>

<p><strong>What I tried</strong></p>

<p>I iterated through each line of the file to see if the word is in the list and add it to a dict if True. But this method is a little slow.</p>

<pre><code>def readWordEmbeddingVector(Wrd):
    f = open('glove.twitter.27B/glove.twitter.27B.200d.txt','r')
    words = []
    a = f.readline()
    while a!= '':
        vector = a.split()
        if vector[0] in Wrd:
            words.append(vector)
            Wrd.remove(vector[0])
        a = f.readline()
    f.close()
    words_vector = pd.DataFrame(words).set_index(0).astype('float')
    return words_vector
</code></pre>

<p>I also tried below, but it loaded the whole file instead of vectors I need</p>

<pre class=""lang-py prettyprint-override""><code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('word2vec.twitter.27B.200d.txt')
</code></pre>

<p><strong>What I want</strong></p>

<p>Method like <code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</code> but I can set a word list to load.</p>
","python, gensim, word-embedding, glove","<p>There's no existing <code>gensim</code> support for filtering the words loaded via <code>load_word2vec_format()</code>. The closest is an optional <code>limit</code> parameter, which can be used to limit how many word-vectors are read (ignoring all subsequent vectors). </p>

<p>You could conceivably create your own routine to perform such filtering, using the source code for <code>load_word2vec_format()</code> as a model. As a practical matter, you might have to read the file twice: 1st, to find out exactly how many words in the file intersect with your set-of-words-of-interest (so you can allocate the right-sized array without trusting the declared size at the front of the file), then a second time to actually read the words-of-interest.</p>
",0,0,954,2019-04-19 15:25:32,https://stackoverflow.com/questions/55764137/load-a-part-of-glove-vectors-with-gensim
run stanford parser interactively (using stdin and stdout) or run it as a server,"<p>I found it inefficient to reboot the parser when new input comes, so I'd like to run the parser interactively--read the input from stdin and print result to stdout. However, the instruction given on the official website <a href=""https://nlp.stanford.edu/software/parser-faq.shtml#x"" rel=""nofollow noreferrer"">Can I have the parser run as a filter?</a>   seems not compatible with options (for example, <code>-port</code>).</p>

<p>I know that CoreNLP can be run as a server but it can not receive POS tagged  text as input so I won't use it.</p>

<p>Here is what I'm trying:</p>

<pre><code>class myThread(threading.Thread):
def __init__(self,inQueue,outQueue):
    threading.Thread.__init__(self)

    self.cmd=['java.exe',
              '-mx4g',
              '-cp','*',
              'edu.stanford.nlp.parser.lexparser.LexicalizedParser',
              '-model', 'edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz',
              '-sentences', 'newline',
              '-outputFormat', 'conll2007', 
              '-tokenized',
              '-tagSeparator','/',
              '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer',
              '-tokenizerMethod', 'newCoreLabelTokenizerFactory',
              '-encoding', 'utf8']
    self.subp=subprocess.Popen(cmd,stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
    self.inQueue=inQueue
    self.outQueue=outQueue
def run(self):
    while True:
        rid,sentence=self.inQueue.get()
        print(u""Receive sentence %s""%sentence)
        sentence=sentence.replace(""\n"","""")
        self.subp.stdin.write((sentence+u'\n').encode('utf8'))
        self.subp.stdin.flush()
        print(""start readline"")
        result=self.subp.stdout.readline()
        print(""end readline"")
        print(result)
        self.outQueue.put((rid,result))
</code></pre>
","subprocess, stanford-nlp","<p>I think you're confusing things a bit. Both CoreNLP and Stanford Parser have an option to run as a command-line filter, reading from stdin and writing to stdout. However, only CoreNLP separately provides a webservice implementation. </p>

<p>Options like <code>port</code> only make sense for the latter.</p>

<p>So, at the moment, I agree that you have a valid use case (wanting to input pre-tagged text) but at present there isn't webservice support for it. The easiest path forward would be to write a simple webservice implementation for the parser. For us, it could happen sometime, but there are a bunch of other current priorities. Anyone else is welcome to write one. :)</p>
",0,0,77,2019-04-27 16:48:33,https://stackoverflow.com/questions/55882752/run-stanford-parser-interactively-using-stdin-and-stdout-or-run-it-as-a-server
How to annotate multiple Stanford CoreNLP CoreDocuments more efficiently?,"<p>I am annotating an huge amount of Strings as CoreDocuments through Stanford Corenlp. StanfordCoreNLP pipelines have an internal feature for multithreaded annotating to optimize the process however as far as i can see CoreDocument objects cant use that feature in the version i run,- which is stanford-corenlp-full-2018-10-05.</p>

<p>Since I could not make Pipelines Annotate collections of CoreDocuments I instead tried to optimize the individual annotations by placing them inside multithreaded methods. I have no Issues with the multithreaded environment. I receive all results back as expected, my only drawback is the time consumption. I tried about 7 different implementation and these were the 3 fastest:</p>

<pre class=""lang-java prettyprint-override""><code>//ForkJoinPool is initialized in the main method in my application
private static ForkJoinPool executor = new ForkJoinPool(Runtime.getRuntime().availableProcessors(), ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, false);

   public static ConcurrentMap&lt;String, CoreDocument&gt; getMultipleCoreDocumentsWay1(Collection&lt;String&gt; str) {
        ConcurrentMap&lt;String, CoreDocument&gt; pipelineCoreDocumentAnnotations = new MapMaker().concurrencyLevel(2).makeMap();
        str.parallelStream().forEach((str1) -&gt; {
            CoreDocument coreDocument = new CoreDocument(str1);
            pipeline.annotate(coreDocument);
            pipelineCoreDocumentAnnotations.put(str1, coreDocument);
            System.out.println(""pipelineCoreDocumentAnnotations size1: "" + pipelineCoreDocumentAnnotations.size() + ""\nstr size: "" + str.size() + ""\n"");
        });
        return pipelineCoreDocumentAnnotations;
    }


     public static ConcurrentMap&lt;String, CoreDocument&gt; getMultipleCoreDocumentsWay4(Collection&lt;String&gt; str) {
        ConcurrentMap&lt;String, CoreDocument&gt; pipelineCoreDocumentAnnotations = new MapMaker().concurrencyLevel(2).makeMap();
        str.parallelStream().forEach((str1) -&gt; {
            try {
                ForkJoinTask&lt;CoreDocument&gt; forkCD = new RecursiveTask() {
                    @Override
                    protected CoreDocument compute() {
                        CoreDocument coreDocument = new CoreDocument(str1);
                        pipeline.annotate(coreDocument);
                        return coreDocument;
                    }
                };
                forkCD.invoke();
                pipelineCoreDocumentAnnotations.put(str1, forkCD.get());
                System.out.println(""pipelineCoreDocumentAnnotations2 size: "" + pipelineCoreDocumentAnnotations.size() + ""\nstr size: "" + str.size() + ""\n"");
            } catch (InterruptedException | ExecutionException ex) {
                Logger.getLogger(Parsertest.class.getName()).log(Level.SEVERE, null, ex);
            }
        });
        return pipelineCoreDocumentAnnotations;
    }

    public static ConcurrentMap&lt;String, CoreDocument&gt; getMultipleCoreDocumentsWay7(ConcurrentMap&lt;Integer, String&gt; hlstatsSTR) {
        RecursiveDocumentAnnotation recursiveAnnotation = new RecursiveDocumentAnnotation(hlstatsSTR, pipeline);
        ConcurrentMap&lt;String, CoreDocument&gt; returnMap = new MapMaker().concurrencyLevel(2).makeMap();
        executor.execute(recursiveAnnotation);
        try {
            returnMap = recursiveAnnotation.get();
        } catch (InterruptedException | ExecutionException ex) {
            Logger.getLogger(Parsertest.class.getName()).log(Level.SEVERE, null, ex);
        }
        System.out.println(""reached end\n"");
        return returnMap;
    }
RecursiveDocumentAnnotation class:

    public class RecursiveDocumentAnnotation extends RecursiveTask&lt;ConcurrentMap&lt;String, CoreDocument&gt;&gt; {

    private String str;
    private StanfordCoreNLP nlp;
    private static ConcurrentMap&lt;String, CoreDocument&gt; pipelineCoreDocumentAnnotations;
    private static ConcurrentMap&lt;Integer, String&gt; hlstatsStrMap;

    public static ConcurrentMap&lt;String, CoreDocument&gt; getPipelineCoreDocumentAnnotations() {
        return pipelineCoreDocumentAnnotations;
    }

    public RecursiveDocumentAnnotation(ConcurrentMap&lt;Integer, String&gt; hlstatsStrMap, StanfordCoreNLP pipeline) {
        this.pipelineCoreDocumentAnnotations = new MapMaker().concurrencyLevel(2).makeMap();
        this.str = hlstatsStrMap.get(0);
        this.nlp = pipeline;
        this.hlstatsStrMap = hlstatsStrMap;
    }

    public RecursiveDocumentAnnotation(ConcurrentMap&lt;Integer, String&gt; hlstatsStrMap, StanfordCoreNLP pipeline,
            ConcurrentMap&lt;String, CoreDocument&gt; returnMap) {
        this.str = hlstatsStrMap.get(returnMap.size());
        this.nlp = pipeline;
        this.hlstatsStrMap = hlstatsStrMap;
        this.pipelineCoreDocumentAnnotations = returnMap;
    }

    @Override
    protected ConcurrentMap&lt;String, CoreDocument&gt; compute() {
        CoreDocument coreDocument = new CoreDocument(str);
        nlp.annotate(coreDocument);
        pipelineCoreDocumentAnnotations.put(str, coreDocument);
        System.out.println(""hlstatsStrMap size: "" + hlstatsStrMap.size() + ""\npipelineCoreDocumentAnnotations size: "" + pipelineCoreDocumentAnnotations.size()
                + ""\n"");
        if (pipelineCoreDocumentAnnotations.size() &gt;= hlstatsStrMap.size()) {
            return pipelineCoreDocumentAnnotations;
        }
        RecursiveDocumentAnnotation recursiveAnnotation = new RecursiveDocumentAnnotation(hlstatsStrMap, nlp, pipelineCoreDocumentAnnotations);
        recursiveAnnotation.fork();
        return recursiveAnnotation.join();
    }    } 
</code></pre>

<p>Time parallel1: 336562 ms.</p>

<p>Time parallel4: 391556 ms.</p>

<p>Time parallel7: 491639 ms.</p>

<p>What honnestly would be the greatest would be if the pipeline by itself could do the multi annotation somehow, however as long as I would not know how to achieve this I hope somebody could perharps explain me how to optimize the CoreDocument annotations individually.
PS: Mashing all the strings together into a single coredocument for annotation would also not be what i want since i need the Coredocuments individually for comparrisions afterwards.</p>
","java, multithreading, stanford-nlp","<p>I didn't time this, but you could try this sample code (add test Strings to the list of Strings)...it should work on 4 documents at the same time:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;

import java.util.*;
import java.util.function.*;
import java.util.stream.*;


public class MultiThreadStringExample {

    public static class AnnotationCollector&lt;T&gt; implements Consumer&lt;T&gt; {

        List&lt;T&gt; annotations = new ArrayList&lt;T&gt;();

        public void accept(T ann) {
            annotations.add(ann);
        }
    }

    public static void main(String[] args) throws Exception {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse"");
        props.setProperty(""threads"", ""4"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        AnnotationCollector&lt;Annotation&gt; annCollector = new AnnotationCollector&lt;Annotation&gt;();
        List&lt;String&gt; exampleStrings = new ArrayList&lt;String&gt;();
        for (String exampleString : exampleStrings) {
            pipeline.annotate(new Annotation(exampleString), annCollector);
        }
        Thread.sleep(10000);
        List&lt;CoreDocument&gt; coreDocs =
                annCollector.annotations.stream().map(ann -&gt; new CoreDocument(ann)).collect(Collectors.toList());
        for (CoreDocument coreDoc : coreDocs) {
            System.out.println(coreDoc.tokens());
        }
    }

}
</code></pre>
",1,1,491,2019-04-29 19:38:15,https://stackoverflow.com/questions/55909786/how-to-annotate-multiple-stanford-corenlp-coredocuments-more-efficiently
how to run stanford corenlp server on google colab?,"<p>I want to use stanford corenlp for obtaining dependency parser of sentences. In order to using stanford corenlp in python, we need to do the below steps:</p>

<ol>
<li>Install java</li>
<li>Download stanford-corenlp-full-2018-10-05 and extract it.</li>
<li>Change directory to stanford-corenlp-full-2018-10-05 folder with ""cd"" command.</li>
<li>Run  this command in the current directory: 

<blockquote>
  <p>""java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 75000"" .</p>
</blockquote></li>
</ol>

<p>After that, stanford-corenlp server will run at '<a href=""http://localhost:9000"" rel=""nofollow noreferrer"">http://localhost:9000</a>' .
Finally we can call CoreNLPDependencyParser() in python script like this:</p>

<pre><code>dependency_parser = CoreNLPDependencyParser(url='http://localhost:9000')
</code></pre>

<p>Now , i want to run stanford-corenlp server on google colab. I uplaoded stanford-corenlp-full-2018-10-05 folder to google drive and mount google drive on google colab. Then I installed java with below function :</p>

<pre><code>import os       
def install_java():
  !apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null     
  os.environ[""JAVA_HOME""] = ""/usr/lib/jvm/java-8-openjdk-amd64""     
  !java -version    
install_java()
</code></pre>

<p>Now, i don't know how run aforementioned java command and gain localhost address.</p>

<p>Is there any way to do that?</p>
","java, stanford-nlp, google-colaboratory","<p>To connect from a remote machine to a server running on Google Colab, you need to use <a href=""https://ngrok.com/"" rel=""nofollow noreferrer"">ngrok</a>.</p>

<p>Assuming your server is running on an existing notebook, create a new notebook and run the following code (which I found from <a href=""https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py"" rel=""nofollow noreferrer"">here</a>):</p>

<pre><code>import os
import subprocess
import json
import time
import requests


def _get_ngrok_tunnel():
    while True:
        try:
            tunnels_json = requests.get(""http://localhost:4040/api/tunnels"").content
            public_url = json.loads(tunnels_json)['tunnels'][0]['public_url']
            return public_url
        except Exception:
            print(""Can't get public url, retrying..."")
            time.sleep(2)


def _warmup_ngrok_tunnel(public_url):
    while requests.get(public_url).status_code &gt;= 500:
        print(""Tunnel is not ready, retrying..."")
        time.sleep(2)


def expose_port_on_colab(port):
    os.system(""apt-get install net-tools"")
    # check that port is open
    while not ("":{} "".format(port) in str(subprocess.check_output(""netstat -vatn"", shell=True))):
        print(""Port {} is closed, retrying..."".format(port))
        time.sleep(2)

    # run ngrok
    os.system(""wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"")
    os.system(""unzip ngrok-stable-linux-amd64.zip"")
    os.system(""./ngrok http {0} &amp;"".format(port))
    public_url = _get_ngrok_tunnel()
    _warmup_ngrok_tunnel(public_url)

    print(""Open {0} to access your {1} port"".format(public_url, port))

</code></pre>

<p>Then invoke <code>expose_port_on_colab</code> function with the port that the server is listening to, this function will give you a URL that you can use to connect to the server</p>

<p><a href=""https://i.sstatic.net/Q9rLw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Q9rLw.png"" alt=""enter image description here""></a></p>
",2,1,4015,2019-05-06 07:59:13,https://stackoverflow.com/questions/56001048/how-to-run-stanford-corenlp-server-on-google-colab
Nlp library that reconstruct sentense,"<p>I Looking for an nlp library that can reconstruct word or sentences.</p>

<p>I do not know if space or nltk ca do it.</p>

<p>For example :</p>

<p>th i is a tes t =>  this is a test </p>

<p>mornung => morning</p>

<p>Let'5 pl3 y =>  Let's play</p>

<p>Thanks in advance  for your help</p>
","python-3.x, machine-learning, nlp, stanford-nlp","<p>You can use packages like,</p>

<p>1)Gingerit : <a href=""https://github.com/Azd325/gingerit"" rel=""nofollow noreferrer"">https://github.com/Azd325/gingerit</a></p>

<p>2)Spacy_grammar : <a href=""https://github.com/tokestermw/spacy_grammar"" rel=""nofollow noreferrer"">https://github.com/tokestermw/spacy_grammar</a></p>

<p>3)Language tool : <a href=""https://github.com/languagetool-org/languagetool"" rel=""nofollow noreferrer"">https://github.com/languagetool-org/languagetool</a></p>
",0,-2,161,2019-05-08 10:47:47,https://stackoverflow.com/questions/56038963/nlp-library-that-reconstruct-sentense
What&#39;s the major difference between glove and word2vec?,"<p>What is the difference between word2vec and glove? 
Are both the ways to train a word embedding? if yes then how can we use both?</p>
","machine-learning, nlp, word2vec, word-embedding, glove","<p>Yes, they're both ways to train a word embedding. They both provide the same core output: one vector per word, with the vectors in a useful arrangement. That is, the vectors' relative distances/directions roughly correspond with human ideas of overall word relatedness, and even relatedness along certain salient semantic dimensions.</p>
<p>Word2Vec does incremental, 'sparse' training of a neural network, by repeatedly iterating over a training corpus.</p>
<p>GloVe works to fit vectors to model a giant word co-occurrence matrix built from the corpus.</p>
<p>Working from the same corpus, creating word-vectors of the same dimensionality, and devoting the same attention to meta-optimizations, the quality of their resulting word-vectors will be roughly similar. (When I've seen someone confidently claim one or the other is definitely better, they've often compared some tweaked/best-case use of one algorithm against some rough/arbitrary defaults of the other.)</p>
<p>I'm more familiar with Word2Vec, and my impression is that Word2Vec's training better scales to larger vocabularies, and has more tweakable settings that, if you have the time, might allow tuning your own trained word-vectors more to your specific application. (For example, using a small-versus-large <code>window</code> parameter can have a strong effect on whether a word's nearest-neighbors are 'drop-in replacement words' or more generally words-used-in-the-same-topics. Different downstream applications may prefer word-vectors that skew one way or the other.)</p>
<p>Conversely, some proponents of GLoVe tout that it does fairly well without needing metaparameter optimization.</p>
<p>You probably wouldn't use both, unless comparing them against each other, because they play the same role for any downstream applications of word-vectors.</p>
",24,28,18388,2019-05-10 06:10:19,https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec
"Read GloVe pre-trained embeddings into R, as a matrix","<p>Working in R. I know the pre-trained GloVe embeddings (e.g., ""glove.6B.50d.txt"") can be found here: <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a>. However, I've had zero luck reading this text file into R so that the product is the word embedding matrix of words by vectors. Has anyone successfully done this, either pulling from a saved .txt file or from the site itself, and if so how was that text converted to a matrix in R?</p>
","r, nlp, word-embedding, text2vec, glove","<p>The text file is already in a tabular form, just use <code>read.csv(""path/to/glove.6B.50d.txt"", sep = "" "")</code> - note that the field/cell separator, in this case, is a space, not a comma.</p>
",1,1,1738,2019-05-10 11:33:41,https://stackoverflow.com/questions/56076874/read-glove-pre-trained-embeddings-into-r-as-a-matrix
Do some HPC clusters cache only one result when running Stanford CoreNLP?,"<p>I am using the Stanford CoreNLP library for a Java project. I created a class called StanfordNLP and instantiated two different objects and initalized the constructors with different strings as parameters. I am using the POS tagger to get adjective-noun sequences. However, the output of the program only shows me the results from the first object. Every StanfordNLP object was initialized with a different string but every object returns the same results as the first object. I'm new to Java, so I can't tell if there is a problem with my code or if there is a problem with the HPC cluster that it's running on.</p>
<p>Instead of returning a list of strings from the StanfordNLP class method, I tried using a getter. I've also tried setting the first StanfordNLP object to null so it doesn't reference anything then created the other objects. Nothing works.</p>
<pre><code>/* in main */
List&lt;String&gt; pos_tokens0 = new ArrayList&lt;String&gt;();
List&lt;String&gt; pos_tokens1 = new ArrayList&lt;String&gt;();

String text0 = &quot;Mary little lamb white fleece like snow&quot;
StanfordNLP snlp0 = new StanfordNLP(text0);
pos_tokens0 = snlp0.process();

String text1 = &quot;Everywhere little Mary went fluffy lamb ate green grass&quot;
StanfordNLP snlp1 = new StanfordNLP(text1);
pos_tokens1 = snlp1.process();


/* in StanfordNLP.java */
public class StanfordNLP {

    private static List&lt;String&gt; pos_adjnouns = new ArrayList&lt;String&gt;();
    private String documentText = &quot;&quot;;

    public StanfordNLP() {}
    public StanfordNLP(String text) { this.documentText = text; }

    public List&lt;String&gt; process() {     
        Properties props = new Properties();
        props.setProperty(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, lemma, ner, depparse&quot;);
        props.setProperty(&quot;coref.algorithm&quot;, &quot;neural&quot;);
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);    
        Annotation document = new Annotation(documentText);
        pipeline.annotate(document);

        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
        List&lt;String[]&gt; corpus_temp = new ArrayList&lt;String[]&gt;();
        int count = 0;
    
        for(CoreMap sentence: sentences) {
            for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
                String[] data = new String[2];
                String word = token.get(TextAnnotation.class);
                String pos = token.get(PartOfSpeechAnnotation.class);
                count ++;

                data[0] = word;
                data[1] = pos;         
                corpus_temp.add(data);
            }           
        }
    
        String[][] corpus = corpus_temp.toArray(new String[count][2]);
    
        // corpus contains string arrays with a word and its part-of-speech.
        for (int i=0; i&lt;(corpus.length-3); i++) { 
            String word = corpus[i][0];
            String pos = corpus[i][1];
            String word2 = corpus[i+1][0];
            String pos2 = corpus[i+1][1];

            // find adjectives and nouns (eg, &quot;fast car&quot;)
            if (pos.equals(&quot;JJ&quot;)) {         
                if (pos2.equals(&quot;NN&quot;) || pos2.equals(&quot;NNP&quot;) || pos2.equals(&quot;NNPS&quot;)) {
                    word = word + &quot; &quot; + word2;
                    pos_adjnouns.add(word);
                }
            }
        }
        return pos_adjnouns;
}
</code></pre>
<p>Expected output for pos_tokens0 is &quot;little lamb, white fleece&quot;.Expected output for pos_tokens1 is &quot;little Mary, fluffy lamb, green grass&quot;. But the actual output for both variables is &quot;little lamb, white fleece&quot;.</p>
<p>Any idea why this might be happening? I ran a simple Java jar file with a main.java and myclass.java on an HPC server and can't replicate this problem. So, it doesn't seem like the HPC server has issues with multiple objects of the same class.</p>
","machine-learning, nlp, stanford-nlp, hpc","<p>The problem looks like it is simply that your <code>pos_adjnouns</code> variable is <code>static</code>, and so is shared between all instances of <code>StanfordNLP</code>…. Try removing the <code>static</code> keyword and see if things work as you expect then.</p>

<p>But like that still isn't right, since you'd have an instance variable and on multiple calls to <code>process()</code>, things would keep being added to the <code>pos_adjnouns</code> list. Two other things you should do are:</p>

<ol>
<li>Make <code>pos_adjnouns</code> a method variable in the <code>process()</code> method</li>
<li>Conversely, initializing a StanfordCoreNLP pipeline is expensive, so you should move that out of the <code>process()</code> method and do it in the class constructor. It'd probably be better for things to be exactly opposite and for the constructor to initialize a pipeline, and for the <code>process()</code> method to take a <code>String</code> to analyze.</li>
</ol>
",1,0,52,2019-05-17 06:04:31,https://stackoverflow.com/questions/56180504/do-some-hpc-clusters-cache-only-one-result-when-running-stanford-corenlp
What&#39;s the difference between the new StanfordNLP native Python package and the python wrapper to Core-NLP?,"<p>Can anyone shed some light on the difference between the neural pipeline used in the new native Python StanfordNLP package: <a href=""https://stanfordnlp.github.io/stanfordnlp/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanfordnlp/</a></p>

<p>and the python wrapper to the Java coreNLP package <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/</a>?</p>

<p>Are these two different implementations? I saw that the StanfordNLP package has native neural implementations but also had a wrapper to the CoreNLP package and was wondering why you would need this wrapper if everything was migrated to python anyway?</p>
","python, nlp, stanford-nlp","<p>The two systems are completely distinct. The Python-native neural pipeline roughly corresponds to <a href=""https://nlp.stanford.edu/pubs/qi2018universal.pdf"" rel=""nofollow noreferrer"">Universal Dependency Parsing from Scratch</a>, with the Tensorflow parser used there reproduced in PyTorch. It provides a fully neural pipeline for many languages from sentence splitting through dependency parsing, exploiting <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">UD</a> resources, but doesn't (at present) support other things such as NER, coreference, relation extraction, open IE, and hand-written pattern matching, and is only trained on UD resources. <a href=""https://www.aclweb.org/anthology/P14-5010"" rel=""nofollow noreferrer"">CoreNLP</a>, which you can use through this or other Python wrappers, does provide all of these other components for a handful of languages, and some models, including English, are trained on much more data. It has the advantages and disadvantages of many pre-neural components (fast tokenizer! purely heuristic sentence-splitting). Most likely, if you're working with formal English text, you'll currently still do better with CoreNLP. In a bunch of other circumstances, you'll do better with the Python stanfordnlp.</p>
",1,1,1514,2019-05-19 06:54:52,https://stackoverflow.com/questions/56205454/whats-the-difference-between-the-new-stanfordnlp-native-python-package-and-the
CoreNLP Road Map,"<p>The road map for CoreNLP is unclear.  Is it in maintenance mode? I'm happy to see emphasis on StanfordNLP, but the lack of visibility into the direction is concerning. If the new neural models are better, will wee see them wrapped in the Java CoreNLP API's?</p>
",stanford-nlp,"<p>CoreNLP is not yet in maintenance mode. We are going to put in some quite significant (and compatibility-breaking) changes over the summer. Among other things, we're going to convert across to using UDv2 (from the current UDv1), we're going to make tokenization changes to English and perhaps other languages to better align with UD and ""new"" (since about 2004!) Penn Treebank tokenization, and we'll have more consistent availability and use of word vectors. These changes should increase compatibility between the Java and Python packages, and over time also make it possible for us to use more data to train Python stanfordnlp models. Now that the Python stanfordnlp v0.2 is out, work on CoreNLP should pick up.</p>

<p>On the other hand, most of the research energy in the Stanford NLP group has now moved to exploring neural models built in Python on top of the major deep learning frameworks. (Hopefully that's not a surprise to hear!) It is therefore less likely that major new components will be added to CoreNLP. It's hard to predict the future, but it is reasonable to expect that CoreNLP will head more in the direction of being a stable, efficient-on-CPU NLP package, rather than something implementing the latest neural models. </p>
",1,1,72,2019-05-19 14:55:51,https://stackoverflow.com/questions/56209004/corenlp-road-map
How to generate custom triples with OpenIEDemo.java provided by stanford-nlp,"<p>I have trained custom NER and Relation extraction model and I have checked generating triples with corenlp server but when I'm using <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/naturalli/OpenIEDemo.java"" rel=""nofollow noreferrer"">OpenIEDemo.java</a>
to generate triples it's generating triples having relations ""has"" and ""have"" only but not the relations on which I have trained my Relation Extraction model on.</p>

<p>I'm loading custom NER and Relation Extraction model while running the same script. Here is my OpenIEDemo.java file...</p>

<pre><code>package edu.stanford.nlp.naturalli;

import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.PropertiesUtils;

import java.util.Collection;
import java.util.List;
import java.util.Properties;

/**
 * A demo illustrating how to call the OpenIE system programmatically.
 * You can call this code with:
 *
 * &lt;pre&gt;
 *   java -mx1g -cp stanford-openie.jar:stanford-openie-models.jar edu.stanford.nlp.naturalli.OpenIEDemo
 * &lt;/pre&gt;
 *
 */
public class OpenIEDemo {

  private OpenIEDemo() {} // static main

  public static void main(String[] args) throws Exception {

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, depparse, natlog, openie"");
    props.setProperty(""ner.model"", ""./ner/ner-model.ser.gz"");
    props.setProperty(""sup.relation.model"", ""./relation_extractor/relation_model_pipeline.ser.ser"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    String text;
    if (args.length &gt; 0) {
      text = args[0];
    } else {
      text = ""Obama was born in Hawaii. He is our president."";
    }
    Annotation doc = new Annotation(text);
    pipeline.annotate(doc);

    // Loop over sentences in the document
    int sentNo = 0;
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""Sentence #"" + ++sentNo + "": "" + sentence.get(CoreAnnotations.TextAnnotation.class));

      // Print SemanticGraph
      System.out.println(sentence.get(SemanticGraphCoreAnnotations.EnhancedDependenciesAnnotation.class).toString(SemanticGraph.OutputFormat.LIST));

      // Get the OpenIE triples for the sentence
      Collection&lt;RelationTriple&gt; triples = sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);

      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }

      // Alternately, to only run e.g., the clause splitter:
      List&lt;SentenceFragment&gt; clauses = new OpenIE(props).clausesInSentence(sentence);
      for (SentenceFragment clause : clauses) {
        System.out.println(clause.parseTree.toString(SemanticGraph.OutputFormat.LIST));
      }
      System.out.println();
    }
  }
}
</code></pre>

<p>Thanks in advance.</p>
","java, python, nlp, stanford-nlp","<p>As OpenIE module of stanfordCoreNLP not using custom relation model(don't know why) I can not use custom relation extraction model with this code instead I had to run SanfordCoreNLP pipeline adding path for my custom NER and Relation Extraction model in server.properties file and generate triples. If someone know the reason why OpenIE is not using custom Relation Extraction model please comment, it will be very useful for others. </p>
",1,1,200,2019-05-21 12:42:12,https://stackoverflow.com/questions/56238567/how-to-generate-custom-triples-with-openiedemo-java-provided-by-stanford-nlp
"StanfordNLP, CoreNLP, spaCy - different dependency graphs","<p>I'm trying to use simple rules/patterns defined over a dependency graph to extract very basic informations from sentences (e.g., triples such as subject->predicate->object). I started using <a href=""https://stanfordnlp.github.io/stanfordnlp/pipeline.html"" rel=""nofollow noreferrer"">StanfordNLP</a> since it was easy to set up and utlizes the GPU for better performance. However, I've noticed that for some sentences, the resulting dependency graph looked not as I would have expected -- I'm no expert though. I therefore tried two other solutions: <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">spaCy</a> and <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">Stanford CoreNLP</a> (I understand that these are maintained by different groups?)</p>

<p>For the example sentence <em>""Tom made Sam believe that Alice has cancer.""</em> I've printed the dependencies for all three approaches. CoreNLP and spaCy yield the same dependencies, and they are different from the ones of StanfordNLP. Hence, I'm inclined to swich to CoreNLP and spaCy (another advantage would be that they come with NER out of the box).</p>

<p>Does anyone have some more experience or feedback that would help where to go from here? I don't expect that CoreNLP and spaCy will always yield in the same dependency graphs, but in the example sentence, considering <code>Sam</code> as <code>obj</code> as StandfordNLP is doing compared to being <code>nsubj</code> (CoreNLP, spaCy) seems to be a significant difference</p>

<pre><code>Format:
token   dependency_tag   parent_token

StanfordNLP
Tom     nsubj   made
made    ROOT    ROOT
Sam     obj     made
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  obj     has
.       punct   made

CoreNLP
Tom     nsubj   made
made    ROOT    ROOT
Sam     nsubj   believe
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  dobj    has
.       punct   made

spaCy
Tom     nsubj   made
made    ROOT    ROOT
Sam     nsubj   believe
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  dobj    has
.       punct   made
</code></pre>
","nlp, stanford-nlp, spacy, dependency-parsing","<p>Not sure how to address your questions but I'd recommend you carefully read the documentation for the Stanford CoreNLP: <a href=""https://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/lex-parser.shtml</a></p>

<p>Within the package there are several grammatical and dependency parsers that you can use. Just looking at the grammatical parses, there is an option to retrieve k-best parses and if you process dependencies on them you will most likely get different dependencies for each.</p>

<p>This has to do both with inaccuracies in the parser and ambiguities in natural language.</p>
",2,0,1465,2019-05-23 03:04:44,https://stackoverflow.com/questions/56267537/stanfordnlp-corenlp-spacy-different-dependency-graphs
How do we use the output file generated after training a Stanford NER tagger using custom dataset?,"<p>After following the steps in this <a href=""https://nlp.stanford.edu/software/crf-faq.html#a"" rel=""nofollow noreferrer"">Stanford NLP FAQ</a> , I was able to generate a zip file of the model. But in the documentation they're using a TSV file to calculate the accuracy of prediction against already annotated file , but no documentation whatsoever is there as to how to test it against a new file!</p>

<p>Command used to generate the model was </p>

<pre><code> java -Xmx10240m -cp 'path_to_stanford-ner.jar' edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code></pre>

<p>where austen.prop is the properties which affect the training</p>

<p>Beginner in Java here , excuse if it's a silly question</p>
","nltk, stanford-nlp","<p>The solution is to take the input file whichever you want to test against the model <strong>and to convert it into a TSV file</strong> which can be fed to the ner model by the following command</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier ner-model.ser.gz -testFile converted_to_tsv.tsv
</code></pre>

<p>Here's a small script to convert a file to TSV in python:</p>

<pre><code>import json
import re
file = filepath
for line in open(file, mode=""r"",encoding = 'utf8'):
    regex = '[ ]'  
    with open('output.tsv','w+') as output_file:

        for line in list(filter(bool, file.splitlines())):

            for word in re.split(split_regex,line):
                print(word+""\tO"")
                output_file.write(word+""\tO""+""\n"")
</code></pre>
",0,3,115,2019-06-07 11:30:56,https://stackoverflow.com/questions/56493505/how-do-we-use-the-output-file-generated-after-training-a-stanford-ner-tagger-usi
Why do I get a TypeError when importing a textfile line by line for sentiment analysis instead of using a sentence hard-coded?,"<p>I am trying to analyze the sentiment of each given sentence from a text file line by line. The code is working whenever I am using the hard coded sentences from the first question linked. When I use the text file input, I get the <code>TypeError</code>.</p>

<p>This is related to the question asked <a href=""https://stackoverflow.com/questions/32879532/stanford-nlp-for-python"">here</a>. And the line by line from text file code is coming from <a href=""https://stackoverflow.com/questions/52160274/how-to-iterate-through-each-line-of-a-text-file-and-get-the-sentiment-of-those-l/56450803#56450803"">this</a> question:</p>

<p>The first one works, the second with the text-file <code>(""I love you. I hate him. You are nice. He is dumb"")</code> does not work. Here is the code :</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')
results = []    
with open(""c:/nlp/test.txt"",""r"") as f:
    for line in f.read().split('\n'):
        print(""Line:"" + line)
        res = nlp.annotate(line,
                   properties={
                       'annotators': 'sentiment',
                       'outputFormat': 'json',
                       'timeout': 1000,
                   })
        results.append(res)      

for res in results:             
    s = res[""sentences""]         
    print(""%d: '%s': %s %s"" % (
        s[""index""], 
        "" "".join([t[""word""] for t in s[""tokens""]]),
        s[""sentimentValue""], s[""sentiment""]))
</code></pre>

<p>I get this error:</p>

<blockquote>
  <p>line 21, in </p>
  
  <p>s[""index""],</p>
  
  <p>TypeError: list indices must be integers or slices, not str</p>
</blockquote>
","python, stanford-nlp, sentiment-analysis, pycorenlp","<p>Looks like I solved the problem. As londo pointed out: This line sets <code>S</code> as <code>List</code>, but it should be <code>dict</code>, just like in the original code:   </p>

<pre><code>s = res[""sentences""] 
</code></pre>

<p>I moved the code into the same loop where the file is read and analyzed line by line and I print the result directly there. So the new code looks like this:</p>

<pre><code>from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')

with open(""c:/nlp/test.txt"",""r"") as f:
    for line in f.read().split('\n'):
        res = nlp.annotate(line,
                    properties={
                        'annotators': 'sentiment',
                        'outputFormat': 'json',
                        'timeout': 15000,
                   }) 
        for s in res[""sentences""]:
            print(""%d: '%s': %s %s"" % (
            s[""index""], 
            "" "".join([t[""word""] for t in s[""tokens""]]),
            s[""sentimentValue""], s[""sentiment""]))
</code></pre>

<p>The result looks just as intended and without any error message:</p>

<pre><code>0: 'I love you .': 3 Positive
0: 'I hate him .': 1 Negative
0: 'You are nice .': 3 Positive
0: 'He is dumb .': 1 Negative
</code></pre>
",0,1,71,2019-06-07 14:41:56,https://stackoverflow.com/questions/56496475/why-do-i-get-a-typeerror-when-importing-a-textfile-line-by-line-for-sentiment-an
Is there a way to use stanford-nlp-pos-tagger in a .NET project in UWP. If not what are the alternatives,"<p>While trying to install Stanford.NLP.POSTagger in .NET project in Universal Windows Platform, I get this error:</p>

<blockquote>
  <p>NU1202: Package Stanford.NLP.POSTagger 3.9.2 is not compatible with uap10.0.15063 (UAP,Version=v10.0.15063). Package
  Stanford.NLP.POSTagger 3.9.2 supports: net
  (.NETFramework,Version=v0.0)
  NU1202: Package IKVM 8.1.5717 is not compatible with uap10.0.15063 (UAP,Version=v10.0.15063). Package IKVM 8.1.5717 supports: net
  (.NETFramework,Version=v0.0)</p>
</blockquote>

<p>Is there a work around for this? Or I might use some other POS tagger libraries?</p>
","uwp, nlp, stanford-nlp","<p>Try upgrading your UWP project to target <strong>at least SDK 16299 (Fall Creators Update)</strong>. This way the library could be referenced. Unfortunately it means you will not be able to support Windows 10 Mobile.</p>

<p>Right-click your UWP project in Solution Explorer and select Properties. There you select the <strong>Min</strong> and <strong>Target</strong> versions to at least 16299.</p>

<p><a href=""https://i.sstatic.net/m1FhR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/m1FhR.png"" alt=""Targeting drop downs""></a></p>
",1,0,63,2019-06-10 08:03:30,https://stackoverflow.com/questions/56522664/is-there-a-way-to-use-stanford-nlp-pos-tagger-in-a-net-project-in-uwp-if-not-w
Stanford typed dependencies using coreNLP in python,"<p>In <a href=""https://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""noreferrer"">Stanford Dependency Manual</a> they mention  ""Stanford typed dependencies"" and particularly the type ""neg"" - negation  modifier. It is also available when using Stanford enhanced++ parser using the website. for example, the sentence: </p>

<blockquote>
  <p>""Barack Obama was not born in Hawaii""</p>
</blockquote>

<p><a href=""https://i.sstatic.net/dEo0q.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/dEo0q.png"" alt=""enter image description here""></a> </p>

<p>The parser indeed find neg(born,not) </p>

<p>but when I'm using the <code>stanfordnlp</code> python library, the only dependency parser I can get will parse the sentence as  follow:</p>

<pre><code>('Barack', '5', 'nsubj:pass')

('Obama', '1', 'flat')

('was', '5', 'aux:pass')

('not', '5', 'advmod')

('born', '0', 'root')

('in', '7', 'case')

('Hawaii', '5', 'obl')
</code></pre>

<p>and the code that generates it: </p>

<pre><code>import stanfordnlp
stanfordnlp.download('en')  
nlp = stanfordnlp.Pipeline()
doc = nlp(""Barack Obama was not born in Hawaii"")
a  = doc.sentences[0]
a.print_dependencies()
</code></pre>

<p>Is there a way to get similar results to the enhanced dependency parser or any other Stanford parser that result in typed dependencies that will give me the negation modifier?</p>
","python, parsing, nlp, stanford-nlp","<p>It is to note the python library stanfordnlp is not just a python wrapper for StanfordCoreNLP. </p>

<h2>1. Difference StanfordNLP / CoreNLP</h2>

<p>As said on the <a href=""https://github.com/stanfordnlp/stanfordnlp"" rel=""noreferrer"">stanfordnlp Github repo</a>:</p>

<blockquote>
  <p>The Stanford NLP Group's official Python NLP library. It contains
  packages for running our latest fully neural pipeline from the CoNLL
  2018 Shared Task and for accessing the Java Stanford CoreNLP server.</p>
</blockquote>

<p>Stanfordnlp contains a new set of neural networks models, trained on the CONLL 2018 shared task. The online parser is based on the CoreNLP 3.9.2 java library. Those are two different pipelines and sets of models, as explained <a href=""https://github.com/stanfordnlp/stanfordnlp/issues/67"" rel=""noreferrer"">here</a>. </p>

<p>Your code only accesses their neural pipeline trained on CONLL 2018 data. This explains the differences you saw compared to the online version. Those are basically two different models.</p>

<p>What adds to the confusion I believe is that both repositories belong to the user named stanfordnlp (which is the team name). Don't be fooled between the java stanfordnlp/CoreNLP and the python stanfordnlp/stanfordnlp. </p>

<p>Concerning your 'neg' issue, it seems that in the python libabry stanfordnlp, they decided to consider the negation with an 'advmod' annotation altogether. At least that is what I ran into for a few example sentences.</p>

<h2>2. Using CoreNLP via stanfordnlp package</h2>

<p>However, you can still get access to the CoreNLP through the stanfordnlp package. It requires a few more steps, though. Citing the Github repo,</p>

<blockquote>
  <p>There are a few initial setup steps.</p>
  
  <ul>
  <li>Download Stanford CoreNLP and models for the language you wish to    use. <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""noreferrer"">(you can download CoreNLP and the language models here)</a></li>
  <li>Put the model jars in the distribution folder</li>
  <li>Tell the python code where Stanford CoreNLP is located: export CORENLP_HOME=/path/to/stanford-corenlp-full-2018-10-05</li>
  </ul>
</blockquote>

<p>Once that is done, you can start a client, with code that can be found in the <a href=""https://github.com/stanfordnlp/stanfordnlp/blob/master/demo/corenlp.py"" rel=""noreferrer"">demo</a>  :</p>

<pre><code>from stanfordnlp.server import CoreNLPClient 

with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]

    # get the dependency parse of the first sentence
    print('---')
    print('dependency parse of first sentence')
    dependency_parse = sentence.basicDependencies
    print(dependency_parse)

    #get the tokens of the first sentence
    #note that 1 token is 1 node in the parse tree, nodes start at 1
    print('---')
    print('Tokens of first sentence')
    for token in sentence.token :
        print(token)
</code></pre>

<p>Your sentence will therefore be parsed if you specify the 'depparse' annotator (as well as the prerequisite annotators tokenize, ssplit, and pos). 
Reading the demo, it feels that we can only access basicDependencies. I have not managed to make Enhanced++ dependencies work via stanfordnlp.</p>

<p>But the negations will still appear if you use basicDependencies !</p>

<p>Here is the output I obtained using stanfordnlp and your example sentence. It is a DependencyGraph object, not pretty, but it is unfortunately always the case when we use the very deep CoreNLP tools. You will see that between nodes 4 and 5 ('not' and 'born'), there is and edge 'neg'. </p>

<pre><code>node {
  sentenceIndex: 0
  index: 1
}
node {
  sentenceIndex: 0
  index: 2
}
node {
  sentenceIndex: 0
  index: 3
}
node {
  sentenceIndex: 0
  index: 4
}
node {
  sentenceIndex: 0
  index: 5
}
node {
  sentenceIndex: 0
  index: 6
}
node {
  sentenceIndex: 0
  index: 7
}
node {
  sentenceIndex: 0
  index: 8
}
edge {
  source: 2
  target: 1
  dep: ""compound""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 2
  dep: ""nsubjpass""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 3
  dep: ""auxpass""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 4
  dep: ""neg""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 7
  dep: ""nmod""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 8
  dep: ""punct""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 7
  target: 6
  dep: ""case""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
root: 5

---
Tokens of first sentence
word: ""Barack""
pos: ""NNP""
value: ""Barack""
before: """"
after: "" ""
originalText: ""Barack""
beginChar: 0
endChar: 6
tokenBeginIndex: 0
tokenEndIndex: 1
hasXmlContext: false
isNewline: false

word: ""Obama""
pos: ""NNP""
value: ""Obama""
before: "" ""
after: "" ""
originalText: ""Obama""
beginChar: 7
endChar: 12
tokenBeginIndex: 1
tokenEndIndex: 2
hasXmlContext: false
isNewline: false

word: ""was""
pos: ""VBD""
value: ""was""
before: "" ""
after: "" ""
originalText: ""was""
beginChar: 13
endChar: 16
tokenBeginIndex: 2
tokenEndIndex: 3
hasXmlContext: false
isNewline: false

word: ""not""
pos: ""RB""
value: ""not""
before: "" ""
after: "" ""
originalText: ""not""
beginChar: 17
endChar: 20
tokenBeginIndex: 3
tokenEndIndex: 4
hasXmlContext: false
isNewline: false

word: ""born""
pos: ""VBN""
value: ""born""
before: "" ""
after: "" ""
originalText: ""born""
beginChar: 21
endChar: 25
tokenBeginIndex: 4
tokenEndIndex: 5
hasXmlContext: false
isNewline: false

word: ""in""
pos: ""IN""
value: ""in""
before: "" ""
after: "" ""
originalText: ""in""
beginChar: 26
endChar: 28
tokenBeginIndex: 5
tokenEndIndex: 6
hasXmlContext: false
isNewline: false

word: ""Hawaii""
pos: ""NNP""
value: ""Hawaii""
before: "" ""
after: """"
originalText: ""Hawaii""
beginChar: 29
endChar: 35
tokenBeginIndex: 6
tokenEndIndex: 7
hasXmlContext: false
isNewline: false

word: "".""
pos: "".""
value: "".""
before: """"
after: """"
originalText: "".""
beginChar: 35
endChar: 36
tokenBeginIndex: 7
tokenEndIndex: 8
hasXmlContext: false
isNewline: false
</code></pre>

<h2>2. Using CoreNLP via NLTK package</h2>

<p>I will not go into details on this one, but there is also a solution to access the CoreNLP server via the NLTK library , if all else fails. It does output the negations, but requires a little more work to start the servers.
Details on <a href=""https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK"" rel=""noreferrer"">this page</a></p>

<h2>EDIT</h2>

<p>I figured I could also share with you the code to get the DependencyGraph into a nice list of 'dependency, argument1, argument2' in a shape similar to what stanfordnlp outputs.</p>

<pre><code>from stanfordnlp.server import CoreNLPClient

text = ""Barack Obama was not born in Hawaii.""

# set up the client
with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]

    # get the dependency parse of the first sentence
    dependency_parse = sentence.basicDependencies

    #print(dir(sentence.token[0])) #to find all the attributes and methods of a Token object
    #print(dir(dependency_parse)) #to find all the attributes and methods of a DependencyGraph object
    #print(dir(dependency_parse.edge))

    #get a dictionary associating each token/node with its label
    token_dict = {}
    for i in range(0, len(sentence.token)) :
        token_dict[sentence.token[i].tokenEndIndex] = sentence.token[i].word

    #get a list of the dependencies with the words they connect
    list_dep=[]
    for i in range(0, len(dependency_parse.edge)):

        source_node = dependency_parse.edge[i].source
        source_name = token_dict[source_node]

        target_node = dependency_parse.edge[i].target
        target_name = token_dict[target_node]

        dep = dependency_parse.edge[i].dep

        list_dep.append((dep, 
            str(source_node)+'-'+source_name, 
            str(target_node)+'-'+target_name))
    print(list_dep)
</code></pre>

<p>It ouputs the following </p>

<pre><code>[('compound', '2-Obama', '1-Barack'), ('nsubjpass', '5-born', '2-Obama'), ('auxpass', '5-born', '3-was'), ('neg', '5-born', '4-not'), ('nmod', '5-born', '7-Hawaii'), ('punct', '5-born', '8-.'), ('case', '7-Hawaii', '6-in')]
</code></pre>
",7,6,3896,2019-06-10 13:54:55,https://stackoverflow.com/questions/56527814/stanford-typed-dependencies-using-corenlp-in-python
How to add stanford corenlp library in Eclipse/Netbeans IDE for Java NLP Project?,"<p>I am trying to do a NLP project using Java for that I want to set up Stanford Core NLP in eclipse or netbeans IDE for developement.How should I do my initial Setup ?</p>

<p>I have tried some installation methods but its not working.I have also used Maven, but its not working.</p>

<ol>
<li>Missing artifact edu.stanford.nlp:stanford-corenlp:jar:${stanford.corenlp.version}   pom.xml /stanford-coreNLP   line 22 Maven Dependency Problem</li>
</ol>
","java, eclipse, netbeans, nlp, stanford-nlp","<p>Looks like your version placeholder cannot be resolved. Either state the version explicitly</p>

<pre><code>&lt;!-- https://mvnrepository.com/artifact/edu.stanford.nlp/stanford-corenlp --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.9.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Or define the version in your pom.xml as a property:</p>

<pre><code>&lt;project&gt;
&lt;properties&gt;
    &lt;stanford.corenlp.version&gt;3.9.2&lt;/stanford.corenlp.version&gt;
&lt;/properties&gt;
...
&lt;/project&gt;
</code></pre>
",1,0,954,2019-06-14 10:04:19,https://stackoverflow.com/questions/56595903/how-to-add-stanford-corenlp-library-in-eclipse-netbeans-ide-for-java-nlp-project
Encoding issue while annotating a sentence in Spanish with cleanNLP and stanford-corenlp backend,"<p>I'm trying to annotate a sentence in Spanish with <code>cleanNLP</code> and <code>stanford-corenlp</code> backend. When I inspect the output tokens I notice that all non-ascii characters were removed and the words with these characters were splited.</p>

<p>Here is a repoducible example:</p>

<pre class=""lang-r prettyprint-override""><code>&gt; library(cleanNLP)
&gt; 
&gt; cnlp_init_corenlp(
+   language = ""es"", 
+   lib_location = ""C:/path/to/stanford-corenlp-full-2018-10-05"")
Loading required namespace: rJava
&gt; 
&gt; input &lt;- ""Esta mañana desperté feliz.""
&gt; 
&gt; Encoding(input)
[1] ""latin1""
&gt; 
&gt; input &lt;- iconv(input, ""latin1"", ""UTF-8"")
&gt; 
&gt; Encoding(input)
[1] ""UTF-8""
&gt; 
&gt; myannotation &lt;- cleanNLP::cnlp_annotate(input)
&gt; 
&gt; myannotation$token$word
[1] ""ROOT""    ""Esta""    ""ma""      ""ana""     ""despert"" ""feliz""   "".""
</code></pre>

<p>Session info:</p>

<pre class=""lang-r prettyprint-override""><code>&gt; sessionInfo()
R version 3.6.0 (2019-04-26)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 17134)

Matrix products: default

locale:
[1] LC_COLLATE=Spanish_Argentina.1252  LC_CTYPE=Spanish_Argentina.1252   
[3] LC_MONETARY=Spanish_Argentina.1252 LC_NUMERIC=C                      
[5] LC_TIME=Spanish_Argentina.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] cleanNLP_2.3.0

loaded via a namespace (and not attached):
[1] compiler_3.6.0    tools_3.6.0       textreadr_0.9.0   data.table_1.12.2
[5] knitr_1.22        xfun_0.6          rJava_0.9-11      XML_3.98-1.19    
&gt; 
</code></pre>
","r, stanford-nlp, rjava","<p>In <a href=""https://github.com/statsmaths/cleanNLP/issues/49"" rel=""nofollow noreferrer"">this</a> GitHub issue the package creator gave me the answer. The problem was my machine's default encoding. I just had to add <code>options(encoding = ""UTF-8"")</code> before annotating the string.</p>
",0,1,64,2019-06-15 18:23:29,https://stackoverflow.com/questions/56613057/encoding-issue-while-annotating-a-sentence-in-spanish-with-cleannlp-and-stanford
"when calculating the cooccurance of two words, do we sepate the sentences or linking all sentences?","<p>For example, I get I document that contains 2 sentences: I am a person. He also likes apples. 
Do we need to count the cooccurrence of ""person"" and ""He"" ?</p>
","nlp, glove","<p>Each document is separated with a line break. Context windows of cooccurrences are limited to each document. </p>

<p>Based on <a href=""https://github.com/stanfordnlp/GloVe/blob/master/src/cooccur.c#L127"" rel=""nofollow noreferrer"">the implementation here</a>.</p>

<blockquote>
  <p>A newline is taken as indicating a new document (contexts won't cross newline).</p>
</blockquote>

<p>So, depending on how you prepare sentences, you may get different results:</p>

<p>Setting 1: <code>('He', 'person')</code> cooccurred</p>

<pre><code>...
I am a person. He also likes apples.
...
</code></pre>

<p>Setting 2: <code>('He', 'person')</code> not cooccurred</p>

<pre><code>...
I am a person. 
He also likes apples.
...
</code></pre>
",1,1,42,2019-06-23 22:13:05,https://stackoverflow.com/questions/56728093/when-calculating-the-cooccurance-of-two-words-do-we-sepate-the-sentences-or-lin
Java Native Interface - C++ is not waiting for java function completion,"<p>I want the functionality of the Stanford Core NLP, written in java, to be available in C++. To do this I am making use of the Java Native Interface. I have a Java object that wraps multiple functions in a way that's easier to call from C++. However when I do call those functions, the C++ doesn't wait for the functions to complete before moving onto the next one.</p>

<p>The Java object has a Main function I use for testing, that calls all the appropriate functions for testing purposes. When running just the Java, it works perfectly. The annotation waits for the setup to complete (which does take a while), and the function that gets the dependencies waits for the annotation function to complete. Perfectly expected and correct behavior.
The problem comes when I start calling the java functions from C++. Part of the java function will run, but it will quit out and go back to the C++ at certain points, specified below. I would like for the C++ to wait for the java methods to finish.</p>

<p>If it matters, I'm using Stanford Core NLP 3.9.2.</p>

<p>I used the code in StanfordCoreNlpDemo.java that comes with the NLP .jar files as a starting point.</p>

<pre class=""lang-java prettyprint-override""><code>import java.io.*;
import java.util.*;

// Stanford Core NLP imports

public class StanfordCoreNLPInterface {

    Annotation annotation;
    StanfordCoreNLP pipeline;

    public StanfordCoreNLPInterface() {}

    /** setup the NLP pipeline */
    public void setup() {
        // Add in sentiment
        System.out.println(""creating properties"");
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment"");
        System.out.println(""starting the parser pipeline"");
        //&lt;---- doesn't get past this point
        pipeline = new StanfordCoreNLP(props);
        System.out.println(""started the parser pipeline"");
    }

    /** annotate the text */
    public void annotateText(String text) {
        // Initialize an Annotation with some text to be annotated. The text is the argument to the constructor.
        System.out.println(""text"");
        System.out.println(text);
        //&lt;---- doesn't get past this point
        annotation = new Annotation(text);
        System.out.println(""annotation set"");
        // run all the selected annotators on this text
        pipeline.annotate(annotation);
        System.out.println(""annotated"");
    }

    /** print the dependencies */
    public void dependencies() {
        // An Annotation is a Map with Class keys for the linguistic analysis types.
        // You can get and use the various analyses individually.
        // For instance, this gets the parse tree of the first sentence in the text.
        List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
        if (sentences != null &amp;&amp; ! sentences.isEmpty()) {
            CoreMap sentence = sentences.get(0);
            System.out.println(""The first sentence dependencies are:"");
            SemanticGraph graph = sentence.get(SemanticGraphCoreAnnotations.EnhancedPlusPlusDependenciesAnnotation.class);
            System.out.println(graph.toString(SemanticGraph.OutputFormat.LIST));
        }
    }

    /** Compile: javac -classpath stanford-corenlp-3.9.2.jar -Xlint:deprecation StanfordCoreNLPInterface.java*/
    /** Usage: java -cp .:""*"" StanfordCoreNLPInterface*/
    public static void main(String[] args) throws IOException {
        System.out.println(""starting main function"");
        StanfordCoreNLPInterface NLPInterface = new StanfordCoreNLPInterface();
        System.out.println(""new object"");
        NLPInterface.setup();
        System.out.println(""setup done"");

        NLPInterface.annotateText(""Here is some text to annotate"");
        NLPInterface.dependencies();
    }
}
</code></pre>

<p>I used the code in this tutorial
<a href=""http://tlab.hatenablog.com/entry/2013/01/12/125702"" rel=""nofollow noreferrer"">http://tlab.hatenablog.com/entry/2013/01/12/125702</a>
as a starting point.</p>

<pre><code>#include &lt;jni.h&gt;

#include &lt;cassert&gt;
#include &lt;iostream&gt;


/** Build:  g++ -Wall main.cpp -I/usr/lib/jvm/java-8-openjdk/include -I/usr/lib/jvm/java-8-openjdk/include/linux -L${LIBPATH} -ljvm*/
int main(int argc, char** argv) {
    // Establish the JVM variables
    const int kNumOptions = 3;
    JavaVMOption options[kNumOptions] = {
        { const_cast&lt;char*&gt;(""-Xmx128m""), NULL },
        { const_cast&lt;char*&gt;(""-verbose:gc""), NULL },
        { const_cast&lt;char*&gt;(""-Djava.class.path=stanford-corenlp""), NULL },
        { const_cast&lt;char*&gt;(""-cp stanford-corenlp/.:stanford-corenlp/*""), NULL }
    };

    // JVM setup before this point.
    // java object is created using env-&gt;AllocObject();
    // get the class methods
    jmethodID mid =
        env-&gt;GetStaticMethodID(cls, kMethodName, ""([Ljava/lang/String;)V"");
    jmethodID midSetup =
        env-&gt;GetMethodID(cls, kMethodNameSetup, ""()V"");
    jmethodID midAnnotate =
        env-&gt;GetMethodID(cls, kMethodNameAnnotate, ""(Ljava/lang/String;)V"");
    jmethodID midDependencies =
        env-&gt;GetMethodID(cls, kMethodNameDependencies, ""()V"");
    if (mid == NULL) {
        std::cerr &lt;&lt; ""FAILED: GetStaticMethodID"" &lt;&lt; std::endl;
        return -1;
    }
    if (midSetup == NULL) {
        std::cerr &lt;&lt; ""FAILED: GetStaticMethodID Setup"" &lt;&lt; std::endl;
        return -1;
    }
    if (midAnnotate == NULL) {
        std::cerr &lt;&lt; ""FAILED: GetStaticMethodID Annotate"" &lt;&lt; std::endl;
        return -1;
    }
    if (midDependencies == NULL) {
        std::cerr &lt;&lt; ""FAILED: GetStaticMethodID Dependencies"" &lt;&lt; std::endl;
        return -1;
    }
    std::cout &lt;&lt; ""Got all the methods"" &lt;&lt; std::endl;

    const jsize kNumArgs = 1;
    jclass string_cls = env-&gt;FindClass(""java/lang/String"");
    jobject initial_element = NULL;
    jobjectArray method_args = env-&gt;NewObjectArray(kNumArgs, string_cls, initial_element);

    // prepare the arguments
    jstring method_args_0 = env-&gt;NewStringUTF(""Get the flask in the room."");
    env-&gt;SetObjectArrayElement(method_args, 0, method_args_0);
    std::cout &lt;&lt; ""Finished preparations"" &lt;&lt; std::endl;

    // run the function
    //env-&gt;CallStaticVoidMethod(cls, mid, method_args);
    //std::cout &lt;&lt; ""main"" &lt;&lt; std::endl;
    env-&gt;CallVoidMethod(jobj, midSetup);
    std::cout &lt;&lt; ""setup"" &lt;&lt; std::endl;
    env-&gt;CallVoidMethod(jobj, midAnnotate, method_args_0);
    std::cout &lt;&lt; ""annotate"" &lt;&lt; std::endl;
    env-&gt;CallVoidMethod(jobj, midDependencies);
    std::cout &lt;&lt; ""dependencies"" &lt;&lt; std::endl;
    jvm-&gt;DestroyJavaVM();
    std::cout &lt;&lt; ""destroyed JVM"" &lt;&lt; std::endl;

    return 0;
}
</code></pre>

<p>Compiling the C++ with g++ and -Wall gives no warnings or errors, and neither does compiling the Java with javac. When I run the C++ code I get the following output.</p>

<pre><code>Got all the methods
Finished preparations
creating properties
starting the parser pipeline
setup
text
Get the flask in the room.
annotate
dependencies
destroyed JVM
</code></pre>

<p>Following the couts and printlines starting the the C++, you can see how the C++ is able to successfully get the methods and finish JVM and method preparations, before calling the setup method in java. That setup method starts and calls the first printline, creates the properties and assigned the values, then quits before it can start the parser pipeline and goes back to the C++.
It's basically the same story moving forward, the annotate text function is called and successfully receives the text from the C++ method call, but quits before it creates the annotation object. I don't have as many debug printlns in dependencies because at that point it doesn't matter, but needless to say none of the existing printlns are called.
At the very end the JVM is destroyed and the program ends.</p>

<p>Thank you for any help or insight you can provide.</p>
","java, c++, java-native-interface, stanford-nlp","<p>JNI method calls are always synchronous. When they return before they have reached the end of the method, it's because the code encountered an exception. This doesn't propagate to C++ exceptions automatically. You always have to check for exceptions after every call.</p>

<p>A common problem for code that runs fine when called from other Java code but not when called with JNI is the VM's classpath. While <code>java.exe</code> will resolve <code>*</code> and add every matching JAR to the classpath, programs using the invocation interface have to do that themselves. The <code>-Djava.class.path</code> in <code>JavaVMOption</code> works with real files only. Also you can only use actual VM options and not arguments like <code>-cp</code>, because they too are only resolved by <code>java.exe</code> and not part of the invocation interface.</p>
",1,4,480,2019-06-24 02:19:39,https://stackoverflow.com/questions/56729163/java-native-interface-c-is-not-waiting-for-java-function-completion
Stanford NLP Java Method Translation Issue,"<p>I'm using the Stanford NLP tools in C#, via the IKVM Java interface.  Also getting ideas from <a href=""https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html"" rel=""nofollow noreferrer"">https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html</a></p>

<pre><code>String text = ""This is a test sentence."";
var props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, parse"");
var curDir = Environment.CurrentDirectory;
Directory.SetCurrentDirectory(modelsDirectory);
var pipeline = new StanfordCoreNLP(props);
Directory.SetCurrentDirectory(curDir);
var annotation = new Annotation(text);
pipeline.annotate(annotation);
</code></pre>

<p>This code works fine to get my <code>Annotation</code>.  However, when I try to access the annotation to extract various entities in the annotation, I run into trouble. Using code from this: <a href=""https://stackoverflow.com/questions/9492707/how-can-i-split-a-text-into-sentences-using-the-stanford-parser"">How can I split a text into sentences using the Stanford parser?</a>      </p>

<pre><code>List&lt;CoreMap&gt; sentences = annotation.get(SentencesAnnotation.class);
</code></pre>

<p>Not clear how to translate <code>SentencesAnnotation.class</code> into something that C# will accept. </p>
","java, c#, stanford-nlp, translate","<p>In general, Java Foo.class translates to C# typeof(Foo), so C# should accept the following:</p>

<pre><code>IList&lt;CoreMap&gt; sentences = annotation.get(typeof(SentencesAnnotation));
</code></pre>
",2,0,75,2019-06-26 21:11:31,https://stackoverflow.com/questions/56780948/stanford-nlp-java-method-translation-issue
&quot;main&quot; java.lang.OutOfMemoryError: Java heap space Error in Stanford Custom Entity Recognition Model training,"<p>I'm trying to train a custom NER model to recognize 41 entities(the training set has around 6000 lines) </p>

<p>When I try to run the training command provided in the <a href=""https://nlp.stanford.edu/software/crf-faq.html"" rel=""nofollow noreferrer"">nlp site</a> :</p>

<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop 
</code></pre>

<p>This is the error I'm facing :</p>

<pre><code>Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at edu.stanford.nlp.optimization.AbstractCachingDiffFunction.ensure(AbstractCachingDiffFunction.java:136)
        at edu.stanford.nlp.optimization.AbstractCachingDiffFunction.derivativeAt(AbstractCachingDiffFunction.java:151)
        at edu.stanford.nlp.optimization.QNMinimizer.evaluateFunction(QNMinimizer.java:1150)
        at edu.stanford.nlp.optimization.QNMinimizer.minimize(QNMinimizer.java:898)
        at edu.stanford.nlp.optimization.QNMinimizer.minimize(QNMinimizer.java:856)
        at edu.stanford.nlp.optimization.QNMinimizer.minimize(QNMinimizer.java:850)
        at edu.stanford.nlp.optimization.QNMinimizer.minimize(QNMinimizer.java:93)
        at edu.stanford.nlp.ie.crf.CRFClassifier.trainWeights(CRFClassifier.java:1935)
        at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1742)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:785)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:756)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3011)

</code></pre>

<p>I tried adding -Xmx4096m to my java command to specify the max heap space as  4GB( that is the maximum available space in my machine) but still no luck.</p>

<p>I also tried adding -Xms1024m to specify minimum heap space and yet no different result.</p>

<p>This same command worked flawless without any heap space errors when I tried it to train a model for 20 entities(1500 lines)</p>

<p>Is this heap space related to RAM or the available space?</p>

<p>Should I try training in a machine with more Ram or storage?</p>
","java, linux, stanford-nlp, heap-memory","<p>If you think you end up with memory-availability issue, here is the guidelines from stanford (Refer back if you can).</p>

<ol>
<li>Ultimately, if you have tons of features and lots of classes, you need to have lots of memory to train a CRFClassifier. We frequently train models that require several gigabytes of RAM and are used to typing <code>java -mx4g.</code></li>
<li>You can decrease the memory of the <code>limited-memory quasi-Newton optimizer (L-BFGS)</code>. The optimizer maintains a number of past guesses which are used to approximate the Hessian. Having more guesses makes the estimate more accurate, and optimization is faster, but the memory used by the system during optimization is linear in the number of guesses. This is specified by the parameter <code>qnSize</code>. The default is 25. Using 10 is perfectly adequate. If you're short of memory, things will still work with much smaller values, even just a value of 2.</li>
<li>Decrease the order of the CRF. We usually use just first order CRFs (<code>maxLeft=1</code> and no features that refer to the answer class more than one away - it's okay to refer to word features any distance away). While the code supports arbitrary order CRFs, building second, third, or fourth order CRFs will greatly increase memory usage and normally isn't necessary. Remember: maxLeft refers to the size of the class contexts that your features use (that is, it is one smaller than the clique size). A first order CRF can still look arbitrarily far to the left or right to get information about the observed data context.</li>
<li>Decrease the number of features generated. To see all the features generated, you can set the property printFeatures to true. CRFClassifier will then write (potentially huge) files in the current directory listing the features generated for each token position. Options that generate huge numbers of features include useWordPairs and useNGrams when maxNGramLeng is a large number.</li>
<li>Decrease the number of classes in your model. This may or may not be possible, depending on what your modeling requirements are. But time complexity is proportional to the number of classes raised to the clique size.</li>
<li>Use the flag useObservedSequencesOnly=true. This makes it so that you can only label adjacent words with label sequences that were seen next to each other in the training data. For some kinds of data this actually gives better accuracy, for other kinds it is worse. But unless the label sequence patterns are dense, it will reduce your memory usage.</li>
<li>Of course, shrinking the amount of training data will also reduce the memory needed, but isn't very desirable if you're trying to train the best classifier. You might consider throwing out sentences with no entities in them, though.</li>
<li>If you're concerned about runtime memory usage, some of the above items still apply (number of features and classes, useObservedSequencesOnly, and order of the CRF), but in addition, you can use the flag featureDiffThresh, for example featureDiffThresh=0.05. In training, CRFClassifier will train one model, drop all the features with weight (absolute value) beneath the given threshold, and then train a second model. Training thus takes longer, but the resulting model is smaller and faster at runtime, and usually has very similar performance for a reasonable threshold such as 0.05.</li>
</ol>
",1,1,433,2019-07-08 08:18:40,https://stackoverflow.com/questions/56930903/main-java-lang-outofmemoryerror-java-heap-space-error-in-stanford-custom-enti
GloVe Word Vectors Cosine Similarity - &quot;Ally&quot; closer to &quot;powerful&quot; than &#39;friend&quot;,"<p>When I tested the two words ""ally"" and ""friend"" for cosine similarity (using a function verified to  be a correct implementation) in python 3.6 with GloVe word vectors, the cosine similarity was <code>
0.6274969008615137</code>. When I tested ""ally"" and ""friend"" however, the result was <code>0.4700224263147646</code>. </p>

<p>It seems that ""ally"" and ""friend"", two nouns given as synonyms, should have a larger cosine similarity than ""ally"" and ""powerful"", a noun and a barely related word. </p>

<p>Am I misunderstanding the idea of word vectors or cosine similarity?</p>
","nlp, cosine-similarity, glove","<p>Welcome to the wonderful word of learned embeddings. And to its pitfalls. <br/>
I am trying to explain this on a higher level, but feel free to read up on this topic, as there <a href=""https://www.google.com/search?client=ubuntu&amp;channel=fs&amp;q=interpretability+of+word+embeddings&amp;ie=utf-8&amp;oe=utf-8"" rel=""nofollow noreferrer"">seems to be</a> quite a bit of literature regarding the problem.
Neural networks in general suffer from the problem that the results are not naturally intuitive to humans - they often simply find statistically significant similarities in your training data, whether they are wanted <a href=""https://www.gwern.net/Tanks#versions-of-the-story"" rel=""nofollow noreferrer"">or not</a></p>

<p>To take your specific example (GloVe) and analyze some of the problems, let us cite its <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">official documentation</a>:</p>

<blockquote>
  <p>GloVe is an unsupervised learning algorithm for obtaining vector
  representations for words. Training is performed on aggregated global
  word-word co-occurrence statistics from a corpus, and the resulting
  representations showcase interesting linear substructures of the word
  vector space.</p>
</blockquote>

<p>What this tells us is that the learned representations are in general depending <em>on the context</em> of a specific word. Imagine if we (for example) have a training set that consists of a number of news articles, it is more likely to encounter articles that talk about ""allies""/""ally"" and ""powerful"" in the same context (think of political news articles), compared to articles that mention both ""ally"" and ""friend"" in a synonymous context.</p>

<p>Unless you actually encounter plenty of examples in which the context for both words is very similar (and thus the learned expression is similar), it is unlikely that your learned representation is close/similar.</p>

<p>The thing about embeddings is, that, while we can certainly find such counter examples in our data, overall they provide a <em>really good</em> numerical interpretation of our vocabulary, at least for the most common languages in research (English, Spanish, French being probably the most popular ones).<br/>
So the question becomes whether you want to spend the time to manually annotate a whole number of words, probably forgetting about associations in their respective context (for example, Apple might be a good example for both the fruit and company, but not everyone who hears of Toyota also thinks of it as a very common Japanese last name). </p>

<p>This, plus the obvious automated processing of word embeddings make them so atractive in the current time. I'm sure I potentially missed a few obvious point, and I want to add that the acceptance of embeddings widely ranges between different research areas, so please take this with a grain of salt.</p>
",2,2,1503,2019-07-11 11:17:13,https://stackoverflow.com/questions/56987977/glove-word-vectors-cosine-similarity-ally-closer-to-powerful-than-friend
How to extract Wikipedia entity matched to CoreEntityMention (WikiDictAnnotator),"<p>I am running CoreNLP over some text, and matching the entities found to Wikipedia entities. I want to reconstruct the sentence providing the link and other useful information for the entities found.</p>

<p>The CoreEntityMention has an <code>entity()</code> method, but it just returns a String.</p>

<pre class=""lang-java prettyprint-override""><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitylink"");

// set up pipeline
pipeline = new StanfordCoreNLP(props);
String doc = ""text goes here"";
pipeline.annotate(doc);

// Iterate the sentences
for (CoreSentence sentence : doc.sentences()) {
      Go through all mentions
      for (CoreEntityMention em : sentence.entityMentions()) {
          System.out.println(em.sentence());
          // Here I would like to extract the Wikipedia entity information
          System.out.println(em.entity());
      }
    }
</code></pre>
","java, stanford-nlp","<p>You just need to add the wikipedia page url.</p>

<p>So <code>Neil_Armstrong</code> maps to <code>https://en.wikipedia.org/wiki/Neil_Armstrong</code>.</p>
",0,0,86,2019-07-17 09:55:35,https://stackoverflow.com/questions/57073176/how-to-extract-wikipedia-entity-matched-to-coreentitymention-wikidictannotator
OutOfMemoryError while reproducing BioGrakn Text Mining example with client Java,"<p>I'm trying to reproduce the BioGrakn example from the <a href=""https://grakn.ai/src/pages/landingpages/downloads/Text-Mining.pdf"" rel=""nofollow noreferrer"">White Paper ""Text Mined Knowledge Graphs""</a> with the aim of building a text mined knowledge graph out of my (non-biomedical) document collection later on. Therefore, I buildt a Maven project out of the classes and the data from the textmining use case in the <a href=""https://github.com/graknlabs/biograkn"" rel=""nofollow noreferrer"">biograkn repo</a>. My pom.xml looks like that:</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;TextMining-BioGrakn&lt;/groupId&gt;
  &lt;artifactId&gt;TextMining-BioGrakn&lt;/artifactId&gt;
  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
  &lt;name&gt;TextMining-BioGrakn&lt;/name&gt;
  &lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;repo.grakn.ai&lt;/id&gt;
        &lt;url&gt;https://repo.grakn.ai/repository/maven/&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
        &lt;groupId&gt;io.grakn.client&lt;/groupId&gt;
        &lt;artifactId&gt;api&lt;/artifactId&gt;
        &lt;version&gt;1.5.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.grakn.core&lt;/groupId&gt;
        &lt;artifactId&gt;concept&lt;/artifactId&gt;
        &lt;version&gt;1.5.3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.graql&lt;/groupId&gt;
        &lt;artifactId&gt;lang&lt;/artifactId&gt;
        &lt;version&gt;1.0.1&lt;/version&gt;
    &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.9.2&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
            &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
            &lt;version&gt;3.9.2&lt;/version&gt;
            &lt;classifier&gt;models&lt;/classifier&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;  
</code></pre>

<p>Migrating the schema, inserting the pubmed articles and training the model works perfectly, but then I got an <code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>, which is thrown in the <code>mineText()</code> method in the CoreNLP class. This is how the main method in the Migrator class looks like: </p>

<pre><code>public class Migrator {

    public static void main(String[] args) {

        GraknClient graknClient = new GraknClient(""localhost:48555"");

        GraknClient.Session session = graknClient.session(""text_mining"");

        try {
            loadSchema(""schema/text-mining-schema.gql"", session);
            PubmedArticle.migrate(session);
            CoreNLP.migrate(session);
        } catch (Exception e) {
            e.printStackTrace();
            session.close();
        }

        session.close();
        graknClient.close();
    }
}
</code></pre>

<p>Do you have any idea on what could cause this error? Am I missing something fundamental here? Any help is highly appreciated.</p>
","java, stanford-nlp, heap-memory, vaticle-typedb","<p>It may be you need to <a href=""https://docs.oracle.com/cd/E15523_01/web.1111/e13814/jvm_tuning.htm#PERFM150"" rel=""nofollow noreferrer"">allocate more memory for your program</a>.</p>

<p>If there is some bug that is causing this issue then <a href=""https://docs.oracle.com/javase/7/docs/webnotes/tsg/TSG-VM/html/clopts.html"" rel=""nofollow noreferrer"">capture a heap dump (hprof) using the HeapDumpOnOutOfMemoryError flag</a>. (Make sure you put the command line flags in the right order: <a href=""https://stackoverflow.com/questions/4935520/generate-java-dump-when-outofmemory"">Generate java dump when OutOfMemory</a>)</p>

<p>Once you have the hprof you can analyze it using <a href=""http://www.eclipse.org/mat/"" rel=""nofollow noreferrer"">Eclipse Memory Analyzer Tool</a> 
It has a very nice ""Leak Suspects Report"" you can run at startup that will help you see what is causing the excessive memory usage. Use 'Path to GC root' on any very large objects that look like leaks to see what is keeping them alive on the heap.</p>

<p>If you need a second opinion on what is causing the leak check out the <a href=""https://www.ibm.com/developerworks/community/groups/service/html/communityview?communityUuid=4544bafe-c7a2-455f-9d43-eb866ea60091"" rel=""nofollow noreferrer"">IBM Heap Analyzer Tool</a>, it works very well also.</p>

<p>Good luck!</p>
",1,0,178,2019-07-23 13:00:47,https://stackoverflow.com/questions/57164755/outofmemoryerror-while-reproducing-biograkn-text-mining-example-with-client-java
Iterate through tokens and find the entity for a token,"<p><strong>Problem</strong></p>

<p>After running CoreNLP over some text, I want to reconstruct a sentence adding the POS-tag for each Token and grouping the tokens that form an entity.</p>

<p>This could be easily done if there was a way to see which entity a Token belongs to.</p>

<p><strong>Aproach</strong></p>

<p>One option I was considering now was going through <code>sentence.tokens()</code> and finding the index in a list containing only the Tokens from all the CoreEntityMentions for that sentence. Then I could see which CoreEntityMention that Token belongs to, so I can group them.</p>

<p>Another option could be to look the offsets of each Token in the sentence and compare it to the offset of each CoreEntityMention.</p>

<p>I think the question is similar to what was asked <a href=""https://stackoverflow.com/questions/14689717/is-it-possible-to-get-a-set-of-a-specific-named-entity-tokens-that-comprise-a-ph"">here</a>, but since it was a while ago, maybe the API has changed since.</p>

<p>This is the setup:</p>

<pre class=""lang-java prettyprint-override""><code>    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");

    pipeline = new StanfordCoreNLP(props);
    String text = ""Some text with entities goes here"";
    CoreDocument coreDoc = new CoreDocument(text);
    // annotate the document
    pipeline.annotate(coreDoc);
    for (CoreSentence sentence : coreDoc.sentences()) {
      // Code goes here
      List&lt;CoreEntityMention&gt; em : sentence.entityMentions();
    }

</code></pre>
",stanford-nlp,"<p>Each token in an entity mention contains an index to which entity mention in the document it corresponds to.</p>

<pre><code>cl.get(CoreAnnotations.EntityMentionIndexAnnotation.class);
</code></pre>

<p>I'll make a note to add a convenience method for this future versions.</p>
",1,1,173,2019-07-24 12:23:56,https://stackoverflow.com/questions/57182870/iterate-through-tokens-and-find-the-entity-for-a-token
Entities containing underscore character are split into multiple entities by TokensAnnotation in CoreNLP,"<p>I am observing that coreNLP 3.9.2 has started splitting enti_ties into multiple ones like 'enti' , '_', 'ties' while tokenizing</p>

<p>I have tried to use the tokenize.whitespace which solves this problem. But I think this will stop splitting tokens for ""cant't"" and ""dont't""</p>
","stanford-nlp, tokenize, penn-treebank","<p>One thing you can do is replace the underscores (_) with a period (.) and the parser (and tokenizer, I believe) will interpret it as one entity. </p>

<p>E.g. <code>enti_ties</code> > <code>enti.ties</code> where the latter is retained as one entity</p>

<p>This doesn't entirely resolve the problem, but serves as a workaround in a pinch. </p>
",1,0,123,2019-07-25 13:33:35,https://stackoverflow.com/questions/57203214/entities-containing-underscore-character-are-split-into-multiple-entities-by-tok
CoreNLP TrueCaseAnnotator returns uppercased text in some cases,"<p>Pretty often I got uppercased results. In some case model works good, but in some worse. Is any chance to fix this?</p>
<p>Some example of bad cases:</p>
<ul>
<li><p>World's Smallest Flower Vase! -&gt; WORLD 'S SMALLEST FLOWER VASE !</p>
</li>
<li><p>Swarna Chaturvedy likes. Plants and few clicks away to win his Free terrace garden! -&gt; SWARNA chaturvedy likes . Plants and few clicks away to WIN HIS FREE TERRACE GARDEN !</p>
</li>
<li><p>Thanos! Wins Infinity Gauntlet Fortnite: Battle Royale LIVE -&gt; Thanos ! Wins Infinity Gauntlet FORTNITE : Battle Royale Live</p>
</li>
<li><p>DIY Static Orbit Sander With Hard Disk -&gt; DIY STATIC ORBIT SANDER WITH HARD DISK</p>
</li>
<li><p>COOL CHRISTMAS CARDS -&gt; COOL CHRISTMAS CARDS</p>
</li>
<li><p>This futuristic 3D printer uses light to print -&gt; This futuristic 3D PRINTER USES LIGHT TO PRINT</p>
</li>
<li><p>Maia zooming for dinner -&gt; MAIA ZOOMING FOR DINNER</p>
</li>
<li><p>Cosmetic surgeons use lasers to remove moles -&gt; COSMETIC SURGEONS USE LASERS TO REMOVE MOLES</p>
<p>@anelkasam</p>
</li>
</ul>
<p>I tried to tune bias parameter but the issue is still there</p>
",stanford-nlp,"<p>Your best bet would be to train your own model.  We may look into training a new model and distributing that at some point.</p>

<p>You can look over the props files we used to train the model by extracting this file from the main models jar:</p>

<pre><code>edu/stanford/nlp/models/truecase/truecasing.fast.caseless.prop
</code></pre>

<p>The training data is just space separated tokens, one sentence per line with the correct case.  We can't distribute the training data we used for the model we distribute.  Whatever text is your typical domain, you can just feed millions of sentences from that into the training process and train a new model which may perform better on your dataset.</p>

<p>The training data we used has 1,301,730 sentences.</p>

<p>There is a GitHub thread here about this: <a href=""https://github.com/stanfordnlp/CoreNLP/issues/336"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/issues/336</a></p>

<p>The training command should be:</p>

<pre><code>java -Xmx100g edu.stanford.nlp.ie.crf.CRFClassifier -prop custom.prop
</code></pre>

<p>For reference this is what the extracted properties file looks like:</p>

<pre><code>serializeTo=truecasing.fast.caseless.qn.ser.gz
trainFileList=/scr/nlp/data/gale/NIST09/truecaser/crf/noUN.input
testFile=/scr/nlp/data/gale/AE-MT-eval-data/mt06/cased/ref0

map=word=0,answer=1

wordFunction = edu.stanford.nlp.process.LowercaseFunction

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useLongSequences=true
useSequences=true
usePrevSequences=true
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
useOccurrencePatterns=true
useLastRealWord=true
useNextRealWord=true
useDisjunctive=true
disjunctionWidth=5
wordShape=chris2useLC
usePosition=true
useBeginSent=true
useTitle=true

useObservedSequencesOnly=true
saveFeatureIndexToDisk=true
normalize=true

useQN=false
QNSize=25

maxLeft=1
l1reg=1.0

readerAndWriter=edu.stanford.nlp.sequences.TrueCasingForNISTDocumentReaderAndWriter
featureFactory=edu.stanford.nlp.ie.NERFeatureFactory

featureDiffThresh=0.02
</code></pre>
",0,0,103,2019-07-26 08:26:48,https://stackoverflow.com/questions/57216020/corenlp-truecaseannotator-returns-uppercased-text-in-some-cases
How to test Stanford Sentiment model?,"<p>I know the following command can be used to train a Stanford sentiment model</p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz
</code></pre>

<p>Now I want to how to test the model with the testing dataset.
I tried using <code>-trainPath</code> option, it didn't seem to work. I didn't find anything neither on the official documentation nor on the web.</p>
","java, machine-learning, nlp, stanford-nlp, sentiment-analysis","<p>You want to use the <code>Evaluate</code> class.</p>

<pre><code>java -Xmx5g edu.stanford.nlp.sentiment.Evaluate -model &lt;model&gt; -treebank &lt;treebank&gt;
</code></pre>
",2,-2,45,2019-07-29 05:19:46,https://stackoverflow.com/questions/57247567/how-to-test-stanford-sentiment-model
Getting character positions in outputs of stanfordNLP in coreference resolution,"<p>I'm trying to use the stanfordNLP for coreference resolution as it is explained <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">here</a>. I'm running the code of above (provided <a href=""https://stackoverflow.com/questions/39410282/coreference-resolution-in-python-nltk-using-stanford-corenlp"">here</a>):</p>

<pre class=""lang-py prettyprint-override""><code>from stanfordnlp.server import CoreNLPClient

text = 'Barack was born in Hawaii. His wife Michelle was born in Milan. He says that she is very smart.'
print(f""Input text: {text}"")

# set up the client
client = CoreNLPClient(properties={'annotators': 'coref', 'coref.algorithm' : 'statistical'}, timeout=60000, memory='16G')

# submit the request to the server
ann = client.annotate(text)    

mychains = list()
chains = ann.corefChain
for chain in chains:
    mychain = list()
    # Loop through every mention of this chain
    for mention in chain.mention:
        # Get the sentence in which this mention is located, and get the words which are part of this mention
        # (we can have more than one word, for example, a mention can be a pronoun like ""he"", but also a compound noun like ""His wife Michelle"")
        words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]
        #build a string out of the words of this mention
        ment_word = ' '.join([x.word for x in words_list])
        mychain.append(ment_word)
    mychains.append(mychain)

for chain in mychains:
    print(' &lt;-&gt; '.join(chain))

</code></pre>

<p>After installing the library:</p>

<pre><code>pip3 install stanfordcorenlp
</code></pre>

<p>downloading the models,</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip
</code></pre>

<p>and setting the $CORENLP_HOME variable,</p>

<pre class=""lang-py prettyprint-override""><code>os.environ['CORENLP_HOME'] = ""path/to/stanford-corenlp-full-2018-10-05""
</code></pre>

<p>This code works pretty well for me, however, the output only contains information by tokens instead of characters. For example, for the above code, the output is:</p>

<pre><code>Barack &lt;-&gt; His &lt;-&gt; He
His wife Michelle &lt;-&gt; she
</code></pre>

<p>printing the variable <em>mention</em> inside the buckle is:</p>

<pre><code>mentionID: 0
mentionType: ""PROPER""
number: ""SINGULAR""
gender: ""MALE""
animacy: ""ANIMATE""
beginIndex: 0
endIndex: 1
headIndex: 0
sentenceIndex: 0
position: 1

mentionID: 4
mentionType: ""PRONOMINAL""
number: ""SINGULAR""
gender: ""MALE""
animacy: ""ANIMATE""
beginIndex: 0
endIndex: 1
headIndex: 0
sentenceIndex: 1
position: 3

mentionID: 5
mentionType: ""PRONOMINAL""
number: ""SINGULAR""
gender: ""MALE""
animacy: ""ANIMATE""
beginIndex: 0
endIndex: 1
headIndex: 0
sentenceIndex: 2
position: 1

mentionID: 3
mentionType: ""PROPER""
number: ""SINGULAR""
gender: ""FEMALE""
animacy: ""ANIMATE""
beginIndex: 0
endIndex: 3
headIndex: 2
sentenceIndex: 1
position: 2

mentionID: 6
mentionType: ""PRONOMINAL""
number: ""SINGULAR""
gender: ""FEMALE""
animacy: ""ANIMATE""
beginIndex: 3
endIndex: 4
headIndex: 3
sentenceIndex: 2
position: 2

</code></pre>

<p>I was searching for other attributes, for example, printing ann.mentionsForCoref,</p>

<pre><code>mentionType: ""PROPER""
number: ""SINGULAR""
gender: ""MALE""
animacy: ""ANIMATE""
person: ""UNKNOWN""
startIndex: 0
endIndex: 1
headIndex: 0
headString: ""barack""
nerString: ""PERSON""
originalRef: 4294967295
goldCorefClusterID: -1
corefClusterID: 5
mentionNum: 0
sentNum: 0
utter: 0
paragraph: 1
isSubject: false
isDirectObject: true
isIndirectObject: false
isPrepositionObject: false
hasTwin: false
generic: false
isSingleton: false
hasBasicDependency: true
hasEnhancedDepenedncy: true
hasContextParseTree: true
</code></pre>

<p>Despite the great information provided by this attribute, there is no information about the character position of the words. I could split the sentences by spaces, but it is not general, I think that could be cases that it can fail. Can anyone help me with that??</p>
","python, stanford-nlp","<p>Try adding <code>output_format='json'</code> when you build the client.  The JSON data should have the character offset info of each token.</p>

<p>There is info here about using the client:</p>

<p><a href=""https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html</a></p>
",1,0,284,2019-08-02 05:39:14,https://stackoverflow.com/questions/57320524/getting-character-positions-in-outputs-of-stanfordnlp-in-coreference-resolution
CoreNLP constituency parsing,"<p>How would you describe the status of constituency parsing in CoreNLP? Is it maintained-yet-not-being-improved as the package has moved on (<a href=""https://stanfordnlp.github.io/CoreNLP/history.html"" rel=""nofollow noreferrer"">as of 3.5.3?</a>) to dependency parsing, thusly aligning with the recent decade's computational linguistics key fashion of research? </p>

<p>I wonder whether the <a href=""https://stanfordnlp.github.io/CoreNLP/contact.html"" rel=""nofollow noreferrer"">java-nlp-user mailing list</a> is not the more appropriate place for this discussion, but a short authoritative answer would be much appreciated, if there is one.</p>

<p>Since dependency parsing probably reaches very good accuracy using neural state-of-the-art, would you recommend any package for converting from dependency to constituency parses?</p>

<p>Is there any form of (noisy) conversion code provided in CoreNLP, for converting from its dependency parses to constituency parses? Only a rule-based conversion in the <em>opposite</em> direction <a href=""https://stanfordnlp.github.io/CoreNLP/parse.html"" rel=""nofollow noreferrer"">appears to be provided for some languages</a>.</p>
",stanford-nlp,"<p>We are not actively developing constituency parsing in the Java Stanford CoreNLP package any more.  I think any future improved constituency parsers will be in Python and neural based.  I believe AllenNLP has such an implementation, and it's possible in the future we will add a neural model to our <a href=""https://stanfordnlp.github.io/stanfordnlp/index.html"" rel=""nofollow noreferrer"">Python StanfordNLP</a> package.  </p>

<p>We do not offer any type of dependency to constituency conversion to the best of my knowledge.</p>
",2,3,1074,2019-08-02 19:50:29,https://stackoverflow.com/questions/57332664/corenlp-constituency-parsing
Stanford pos-tagger incremental training,"<p>We've been using Stanford CoreNLP for a while and most of the time it delivered correct results.</p>

<p>But for certain sentences the dependency parsing results mess up. As we observed, some of these errors are caused by POS tagging issue, e.g. the word <code>like</code> in <code>I really like this restaurant.</code>, or the word <code>ambient</code> in <code>Very affordable and excellent ambient!</code></p>

<p>Yes we are dealing with user reviews which might have slightly different wording with the training corpus in Stanford CoreNLP, so we are thinking of annotating some text ourselves and mix with the existing model. For NER we already had our own model for special NEs but for POS-tagging and dependency parsing we have no clue. </p>

<p>Could anyone provide any suggestions?</p>
","stanford-nlp, pos-tagger, dependency-parsing","<p>The best thing to do is use CoNLL-U data.</p>

<p>There are English treebanks available here: <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">https://universaldependencies.org/</a></p>

<p>There are examples of properties files for various part-of-speech models we've trained here (also in the models jars): </p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/tree/master/scripts/pos-tagger"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/tree/master/scripts/pos-tagger</a></p>

<p>Here is an example part-of-speech training command:</p>

<pre><code>java -Xmx10g edu.stanford.nlp.tagger.maxent.MaxentTagger -props custom.props
</code></pre>

<p>Note that you want to use this format to specify which files to use for training and evaluating on CoNLL-U files:</p>

<pre><code>trainFile = format=TSV,wordColumn=1,tagColumn=3,/path/to/train.conllu
</code></pre>

<p>Here you are specifying you use a tab separated file (which has one token per line, empty line for sentence breaks), and you are saying which columns represent the word and the tag respectively.</p>

<p>Here is an example command for training the dependency parser:</p>

<pre><code>java edu.stanford.nlp.parser.nndep.DependencyParser -Xmx10g -trainFile &lt;trainPath&gt; -devFile &lt;devPath&gt; -embedFile &lt;wordEmbeddingFile&gt; -embeddingSize &lt;wordEmbeddingDimensionality&gt; -model nndep.model.txt.gz
</code></pre>

<p>One thing to be aware of is the notion of a <code>UPOS</code> tag and an <code>XPOS</code> tag.  The <code>UPOS</code> tags are expected to be in column 3, whereas the <code>XPOS</code> are in column 4.  The <code>UPOS</code> are universal for all languages, the <code>XPOS</code> are fine-grained and language specific.</p>

<p>The <code>-cPOS</code> flag will tell the training process to use the <code>UPOS</code> tags which are in column index 3.  If you don't add this flag it will use column index 4 by default as in the example command.</p>

<p>This command should work and train a model properly with CoNLL-U data if you use the latest code for Stanford CoreNLP from GitHub.  If you are using code from 3.9.2, you will need to make sure to translate your data from CoNLL-U to CoNLL-X.  CoNLL-X is an older style that doesn't include info about multi-word tokens.</p>

<p>Also, for your model to perform optimally, you need to make sure you are using tokenization in your overall application that is consistent with the training data.</p>
",1,0,321,2019-08-05 08:24:04,https://stackoverflow.com/questions/57354728/stanford-pos-tagger-incremental-training
Getting word embeddings using XLNet?,"<p>Hello I have been trying to contextual extract word embedding using the novel XLNet but without luck.</p>

<p>Running on Google Colab with TPU</p>

<p>I would like to note that I get this error when I use TPU so thus I switch to GPU to avoid the error</p>

<pre><code>xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path)
</code></pre>

<blockquote>
  <p>AttributeError: module 'xlnet' has no attribute 'XLNetConfig'</p>
</blockquote>

<p>However I get another error when I use GPU</p>

<pre><code>run_config = xlnet.create_run_config(is_training=True, is_finetune=True, FLAGS=FLAGS)
</code></pre>

<blockquote>
  <p>AttributeError: use_tpu</p>
</blockquote>

<p>I will post the whole code below: I am using a small sentence as an input till it work and I switch to big data then</p>

<p><strong>Main Code:</strong></p>

<pre><code>import sentencepiece as spm
import numpy as np
import tensorflow as tf
from prepro_utils import preprocess_text, encode_ids
import xlnet
import sentencepiece as spm

text = ""The metamorphic rocks of western Crete form a series some 9000 to 10,000 ft.""
sp_model = spm.SentencePieceProcessor()
sp_model.Load(""/content/xlnet_cased_L-24_H-1024_A-16/spiece.model"")

text = preprocess_text(text) 
ids = encode_ids(sp_model, text)

#print('ids',ids)

# some code omitted here...
# initialize FLAGS
# initialize instances of tf.Tensor, including input_ids, seg_ids, and input_mask

# XLNetConfig contains hyperparameters that are specific to a model checkpoint.
xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path) **ERROR 1 HERE**
from absl import flags
import sys

FLAGS = flags.FLAGS
# RunConfig contains hyperparameters that could be different between pretraining and finetuning.
run_config = xlnet.create_run_config(is_training=True, is_finetune=True, FLAGS=FLAGS) **ERROR 2 HERE**
xp = []
xp.append(ids)
input_ids = np.asarray(xp)
xlnet_model = xlnet.XLNetModel(
    xlnet_config=xlnet_config,
    run_config=run_config,
    input_ids=input_ids,
    seg_ids=None,
    input_mask=None)
embed1=tf.train.load_variable('../data/xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt','model/transformer/word_embedding/lookup_table:0')`
</code></pre>

<p><strong>Before the main code I'm cloning Xlnet from GitHub and so on (I will also post it)</strong></p>

<pre><code>! pip install sentencepiece
#Download the pretrained XLNet model and unzip only needs to be done once
! wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip
! unzip cased_L-24_H-1024_A-16.zip
! git clone https://github.com/zihangdai/xlnet.git

SCRIPTS_DIR = 'xlnet' #@param {type:""string""}
DATA_DIR = 'aclImdb' #@param {type:""string""}
OUTPUT_DIR = 'proc_data/imdb' #@param {type:""string""}
PRETRAINED_MODEL_DIR = 'xlnet_cased_L-24_H-1024_A-16' #@param {type:""string""}
CHECKPOINT_DIR = 'exp/imdb' #@param {type:""string""}

train_command = ""python xlnet/run_classifier.py \
  --do_train=True \
  --do_eval=True \
  --eval_all_ckpt=True \
  --task_name=imdb \
  --data_dir=""+DATA_DIR+"" \
  --output_dir=""+OUTPUT_DIR+"" \
  --model_dir=""+CHECKPOINT_DIR+"" \
  --uncased=False \
  --spiece_model_file=""+PRETRAINED_MODEL_DIR+""/spiece.model \
  --model_config_path=""+PRETRAINED_MODEL_DIR+""/xlnet_config.json \
  --init_checkpoint=""+PRETRAINED_MODEL_DIR+""/xlnet_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=8 \
  --eval_batch_size=8 \
  --num_hosts=1 \
  --num_core_per_host=1 \
  --learning_rate=2e-5 \
  --train_steps=4000 \
  --warmup_steps=500 \
  --save_steps=500 \
  --iterations=500""

! {train_command}
</code></pre>
","python, nlp, data-mining, word2vec, glove","<p>Check this <a href=""https://colab.research.google.com/gist/amansrivastava17/c7b7c5e9902133174b4ef6d8587b34b1/embedding-as-service.ipynb#scrollTo=NQEqcqa5v7hg"" rel=""nofollow noreferrer"">gist</a> out.</p>

<p>We have made it really easy to get token level embeddings from <code>XLNet</code></p>

<p>Update: <a href=""https://colab.research.google.com/gist/ashutoshsingh0223/d6d673a942dd15546fc28e9fce875b51/embedding-as-service.ipynb"" rel=""nofollow noreferrer"">Updated gist</a> . </p>

<p>For detailed documentation and more examples check <a href=""https://github.com/amansrivastava17/embedding-as-service"" rel=""nofollow noreferrer"">Github</a></p>
",2,2,3000,2019-08-05 20:14:57,https://stackoverflow.com/questions/57365531/getting-word-embeddings-using-xlnet
How can I extract phrases from CoreNLPParser?,"<p><a href=""https://i.sstatic.net/lSJb8.png"" rel=""nofollow noreferrer"">See the screenshot</a></p>

<p>As you can see from the image parser returns NP, VP, PP, NP. I want to be able to access all phrases on different depth. For instance, in depth=1 there are two phrases NP and VP, in depth=2 there are some other phrases, in depth=3 there are some other. How can I access phrases that belongs to depth = n with python?</p>
","python-3.x, nlp, stanford-nlp, pycorenlp","<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;

import java.util.*;
import java.util.stream.*;

public class ConstituencyParserExample {

    public static void main(String[] args) {
        String text = ""The little lamb climbed the big mountain."";
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,parse"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = new CoreDocument(text);
        // annnotate the document
        pipeline.annotate(document);
        int maxDepth = 5;
        for (CoreSentence sentence : document.sentences()) {
            Set&lt;Constituent&gt; constituents = sentence.constituencyParse().constituents(
                    new LabeledScoredConstituentFactory(), maxDepth).stream().filter(
                            x -&gt; x.label().value().equals(""NP"")).collect(Collectors.toSet());
            for (Constituent constituent : constituents) {
                System.out.println(""---"");
                System.out.println(""label: ""+constituent.label().value());
                System.out.println(sentence.tokens().subList(constituent.start(), constituent.end()+1));
            }
        }
    }
}
</code></pre>
",0,1,136,2019-08-06 21:26:25,https://stackoverflow.com/questions/57384213/how-can-i-extract-phrases-from-corenlpparser
Textual Entailment on large data corpus,"<p>I'm working on textual entailment recently and I wanted to know the current research projects pertaining to Textual entailment on large datasets.</p>

<p>I have read research papers for the same and wanted to explore if there are any possibility of applying data augmentation to make the limited dataset robust or even applying Hierarchical attention mechanism? </p>

<p>Your valuable thoughts and references are very much appreciable</p>
","nlp, stanford-nlp","<p>you could have a look at <a href=""https://nlpprogress.com/english/semantic_textual_similarity.html"" rel=""nofollow noreferrer"">https://nlpprogress.com/english/semantic_textual_similarity.html</a> for state of the art models</p>

<p>Transfer Learning and pre-trained Language Models is a big theme since last year so you can have a look at <a href=""https://github.com/huggingface/pytorch-transformers"" rel=""nofollow noreferrer"">https://github.com/huggingface/pytorch-transformers</a> instead of data augmentation</p>
",1,0,73,2019-08-08 15:30:42,https://stackoverflow.com/questions/57416112/textual-entailment-on-large-data-corpus
How to speedup Stanford NLP in Python?,"<pre><code>import numpy as np
from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize
    #english.all.3class.distsim.crf.ser.gz
st = StanfordNERTagger('/media/sf_codebase/modules/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',
                           '/media/sf_codebase/modules/stanford-ner-2018-10-16/stanford-ner.jar',
                           encoding='utf-8')
</code></pre>

<p>After initializing above code Stanford NLP following code takes 10 second to tag the text as shown below. How to speed up?</p>

<pre><code>%%time
text=""My name is John Doe""
tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)
print (classified_text)
</code></pre>

<p>Output</p>

<pre><code>[('My', 'O'), ('name', 'O'), ('is', 'O'), ('John', 'PERSON'), ('Doe', 'PERSON')]
CPU times: user 4 ms, sys: 20 ms, total: 24 ms
Wall time: 10.9 s
</code></pre>
",stanford-nlp,"<p>Another solution within NLTK is to <em>not use</em> the old <code>nltk.tag.StanfordNERTagger</code> but instead to use the newer <code>nltk.parse.CoreNLPParser</code> . See, e.g., <a href=""https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK</a> .</p>

<p>More generally the secret to good performance is indeed to use a server on the Java side, which you can repeatedly call without having to start new subprocesses for each sentence processed. You can either use the <code>NERServer</code> if you just need NER or the <code>StanfordCoreNLPServer</code> for all CoreNLP functionality. There are a number of Python interfaces to it, see: <a href=""https://stanfordnlp.github.io/CoreNLP/other-languages.html#python"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/other-languages.html#python</a></p>
",4,0,939,2019-08-09 06:58:46,https://stackoverflow.com/questions/57424885/how-to-speedup-stanford-nlp-in-python
Can we compare word vectors from different models using transfer learning?,"<p>I want to train two word2vec/GLoVe models on different corpora and then compare the vectors of a single word. I know that it makes no sense to do so as different models start at different random states, but what if we use pre-trained word vectors as the starting point. Can we assume that the two models will continue to build upon the pre-trained vectors by incorporating the respective domain-specific knowledge, and not go into completely different states?</p>

<p>Tried to find some research papers which discuss this problem, but couldn't find any.</p>
","nlp, word2vec, glove","<p>Simply starting your models with pre-trained bectors would eliminate some of the randomness, but with each training epoch on your new corpora:</p>

<ul>
<li>there's still randomness introduced by negative-sampling (if using that default mode), by frequent-word downsampling (if using default values of the <code>sample</code> parameter in word2vec), and by the interplay of different threads</li>
<li>each epoch with your new corpora will be pulling the word-vectors for present words to new, better positions <strong>for that corpora</strong>, but leaving original words unmoved. The net movements over many epochs could move words arbitrarily far from where they started, in response to the whole-corpus-effects on all words.</li>
</ul>

<p>So, doing so wouldn't necessarily achieve your goal in a reliable (or theoretically-defensible) way, though it might kinda-work – at least better than starting from purely random initialization – especially if your corpora are small and you do few training epochs. (That's usually a bad idea – you want big varied training data and enough passes for extra passes to make little incremental difference. But doing those things ""wrong"" could make your results look ""better"" in this scenario, where you don't want your training to change the original coordinate-space ""too much"". I wouldn't rely on such an approach.)</p>

<p>Especially if the words you need to compare are a small subset of the total vocabulary, a couple things you could consider:</p>

<ul>
<li><p>combine the corpora into one training corpus, shuffled together, but for those words you need to compare, replace them with corpora-specific tokens. For example, replace <code>'sugar'</code> with <code>'sugar_c1'</code> and <code>'sugar_c2'</code> – leaving the vast majority of surrounding words to be the same tokens (and thus learn a single vector across the  whole corpus). Then, the two variant tokens for the ""same word"" will learn different vectors, based on their differing contexts that still share many of the same tokens. </p></li>
<li><p>using some ""anchor set"" of words that you know (or confidently conjecture) either do mean the same across both contexts, or <em>should</em> mean the same, train two models but learn a transformation between the two space based on those guide words. Then, when you apply that transformation to other words, that weren't used to learn the transformation, they'll land in contrasting positions in each others' spaces, <strong>maybe</strong> achieving the comparison you need. This is a technique that's been used for language-to-language translation, and there's a <a href=""https://radimrehurek.com/gensim/models/translation_matrix.html"" rel=""nofollow noreferrer"">helper class</a> and <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">example notebook</a> included with the Python <code>gensim</code> library. </p></li>
</ul>

<p>There may be other better approaches, these are just two quick ideas that might work without much change to existing libraries. A project like '<a href=""https://nlp.stanford.edu/projects/histwords/"" rel=""nofollow noreferrer"">HistWords</a>', which used word-vector training to try to track evolving changes in word-meaning over time, might also have ideas for usable techniques. </p>
",2,0,536,2019-08-12 21:53:40,https://stackoverflow.com/questions/57468725/can-we-compare-word-vectors-from-different-models-using-transfer-learning
Using Stanford Dependencies in python stanfordnlp (rather than Universal Dependencies),"<p>I'm trying to reproduce a study into sentiment analysis which uses dependency structures which were generated using the Stanford NLP library, the issue is that the study is from 2011 and I've found that than the Standford library used <em>Stanford Dependencies</em> but it now uses <em>Universal Dependencies</em> which gives different results (see <a href=""https://nlp.stanford.edu/software/stanford-dependencies.shtml#English"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/stanford-dependencies.shtml#English</a>). My query is whether one can still use Stanford dependencies in the stanfordnlp library in Python?</p>
","python, nlp, stanford-nlp","<p>There isn't a native Python model, but you could launch a Java Stanford CoreNLP server and get the older dependencies.</p>

<p>This model should use Stanford Dependencies for instance:</p>

<pre><code>edu/stanford/nlp/models/parser/nndep/wsj_SD.gz
</code></pre>
",0,1,102,2019-08-13 13:31:48,https://stackoverflow.com/questions/57478871/using-stanford-dependencies-in-python-stanfordnlp-rather-than-universal-depende
How to interpret tregex for clauses,"<p>I am looking at the source code for <a href=""http://www.personal.psu.edu/xxl13/downloads/l2sca.html"" rel=""nofollow noreferrer"">L2 Syntactic Complexity Analyzer</a></p>

<p>and it had a tregex expression for clause as:</p>

<blockquote>
  <p>S|SINV|SQ [> ROOT &lt;, (VP &lt;# VB) | &lt;# MD|VBZ|VBP|VBD | &lt; (VP [&lt;#
  MD|VBP|VBZ|VBD | &lt; CC &lt; (VP &lt;# MD|VBP|VBZ|VBD)])]</p>
</blockquote>

<p>I am reading tregex syntax from this <a href=""https://nlp.stanford.edu/~manning/courses/ling289/Tregex.html"" rel=""nofollow noreferrer"">link</a> but am not confident that I understood the <code>Boolean relational operators</code> correctly, specifically does the second part of this tregex:</p>

<blockquote>
  <p><strong>VP</strong> [&lt;# MD|VBP|VBZ|VBD | <strong>&lt; CC &lt; (VP &lt;# MD|VBP|VBZ|VBD)</strong>]</p>
</blockquote>

<p>mean </p>

<blockquote>
  <p>(VP &lt;# MD|VBP|VBZ|VBD) OR <strong>((VP &lt; CC) AND (VP &lt; (VP &lt;# MD|VBP|VBZ|VBD))</strong>)</p>
</blockquote>

<p>verb phrase that contains both a cc and vp (with md vbp vbz vbd)</p>

<p>Or</p>

<blockquote>
  <p>(VP &lt;# MD|VBP|VBZ|VBD) OR <strong>(VP &lt; (CC &lt; (VP &lt;# MD|VBP|VBZ|VBD))</strong>)</p>
</blockquote>

<p>verb phrase that contains a cc that contains a vp </p>
","parsing, pattern-matching, stanford-nlp","<p>In tregex (following earlier tgrep languages) a clause <code>A op B op C op D</code> always means <code>A op B AND A op C AND A op D</code>. If you want the opposite, you need to use parentheses as in Example 1: <code>A op (B op (C op D))</code>. So in the second disjunct of the original message, the VP has to contain a CC and another VP headed by a word of the indicated part-of-speech (<code>&lt;#</code> is the ""headed by"" relation). So the answer is basically the former interpretation, with the one added constraint that the first VP in each conjunct of the second disjunct has to be the same VP node.</p>
",1,0,59,2019-08-18 19:02:28,https://stackoverflow.com/questions/57547534/how-to-interpret-tregex-for-clauses
Getting full names from NER,"<p>From reading through the docs and playing with the API, it looks like CoreNLP will tell me the NER tags per token, but it won't help me extract out full names from a sentence. For example:</p>

<pre><code>Input: John Wayne and Mary have coffee
CoreNLP Output: (John,PERSON) (Wayne,PERSON) (and,O) (Mary,PERSON) (have,O) (coffee,O)
Desired Result: list of PERSON ==&gt; [John Wayne, Mary]
</code></pre>

<p>Unless there is some flag I missed, I believe to do this I will need to parse the tokens and glue together successive tokens tagged PERSON.</p>

<p>Can someone confirm that this is indeed what I need to do? I mostly want to know if there is some flag or utility in CoreNLP that does something like this for me. Bonus points if someone has a utility (ideally Java, since I'm using the Java API) that does this and wants to share :)</p>

<p>Thanks!</p>

<p>PS: There was a very similar question <a href=""https://stackoverflow.com/questions/25842982/tagging-full-name-in-stanford-ner"">here</a>, which seems to suggest the answer is ""roll your own"", but it was never confirmed by anyone else.</p>
","java, nlp, stanford-nlp, named-entity-recognition","<p>Your are probably looking for <a href=""https://stanfordnlp.github.io/CoreNLP/ner.html#entity-mention-detection"" rel=""nofollow noreferrer"">entity mentions</a> instead of or as well as NER tags. For example with the <a href=""https://stanfordnlp.github.io/CoreNLP/simple.html"" rel=""nofollow noreferrer"">Simple API</a>:</p>

<pre><code>new Sentence(""Jimi Hendrix was the greatest"").nerTags()
[PERSON, PERSON, O, O, O]

new Sentence(""Jimi Hendrix was the greatest"").mentions()
[Jimi Hendrix]
</code></pre>

<p>The link above has an example with the traditional non-simple API using a good old <code>StanfordCoreNLP</code> pipeline</p>
",2,1,825,2019-08-20 15:09:37,https://stackoverflow.com/questions/57576640/getting-full-names-from-ner
Troubles using Stanford Arabic Segmenter,"<p>I'm having troubles running the Stanford Arabic segmenter in Windows 10, as whenever I try to process the command as mentioned in the readme file, it fails to load the segmenter data/arabic-segmenter-atb+bn+arztrain.ser.gz</p>

<p>I'm not very familiar with Java so I don't even know whether I understood the classpath matter correctly. Guess, I didn't. Also, I find the readme instructions slightly confusing.</p>

<pre><code>Loaded ArabicTokenizer with options: null
loadClassifier=data/arabic-segmenter-atb+bn+arztrain.ser.gz
textFile=C:\Users\vmumm\OneDrive\Ulmo\Nizar\OLD\complete_NQ_new_April2019.txt
featureFactory=edu.stanford.nlp.international.arabic.process.StartAndEndArabicSegmenterFeatureFactory
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Failed to load segmenter data/arabic-segmenter-atb+bn+arztrain.ser.gz
        at edu.stanford.nlp.international.arabic.process.ArabicSegmenter.loadSegmenter(ArabicSegmenter.java:466)
        at edu.stanford.nlp.international.arabic.process.ArabicSegmenter.getSegmenter(ArabicSegmenter.java:629)
        at edu.stanford.nlp.international.arabic.process.ArabicSegmenter.main(ArabicSegmenter.java:532)
Caused by: java.io.IOException: Unable to open ""data/arabic-segmenter-atb+bn+arztrain.ser.gz"" as class path, filename or URL
        at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1503)
        at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2939)
        at edu.stanford.nlp.international.arabic.process.ArabicSegmenter.loadSegmenter(ArabicSegmenter.java:464)
</code></pre>

<p>I guess, I just need a simple guide how to run the segmenter - assuming that I don't usually work with Java.</p>
","stanford-nlp, arabic","<p>I'd recommend downloading the full Stanford CoreNLP package.</p>

<ol>
<li><p>Download Stanford CoreNLP from here: <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a></p></li>
<li><p>This should end up in a directory like:</p>

<pre><code>C:\Users\myusername\stanford-corenlp-full-2018-10-05
</code></pre></li>
<li><p>Download the Arabic models jar from that same link, and move it to the Stanford CoreNLP directory <code>C:\Users\myusername\stanford-corenlp-full-2018-10-05</code></p></li>
<li><p>Set CLASSPATH to include that directory which has all *.jar files you need.</p>

<pre><code>set CLASSPATH=C:\Users\myusername\stanford-corenlp-full-2018-10-05\*;
</code></pre></li>
<li><p>Run a pipeline on example text (make sure to be in the directory with the example file when you run this command)</p>

<pre><code>java -Xmx5g edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-arabic.properties -file example.txt -outputFormat text
</code></pre></li>
</ol>

<p>You should get segmented output in <code>example.txt.out</code> when this command finishes.</p>

<p>I don't have access to a Windows machine, so if my answer doesn't work please let me know, and I'll fix it.  I'll try to put up some documentation on our site about working with Windows.</p>
",0,0,254,2019-08-25 09:12:33,https://stackoverflow.com/questions/57644717/troubles-using-stanford-arabic-segmenter
Should the vocabulary be restricted to the training-set vocabulary when training an NN model with pretrained word2vec like GLOVE?,"<p>I wanted to use word embeddings for the embedding Layer in my neural network using pre-trained vectors from GLOVE. Do I need to restrict the vocabulary to the training-set when constructing the word2index dictionary? 
Wouldn't that lead to a limited non-generalizable model? 
Is considering all the vocabulary of GLOVE a recommended practice?</p>
","keras, neural-network, word-embedding, glove","<p>Yes, it is better to restrict your vocab size. Because pre-trained embeddings (like GLOVE) have many words in them that are not very useful (and so Word2Vec) and the bigger vocab size the more RAM you need and other problems.</p>

<p>Select your tokens from all of your data. it won't lead to a limited non-generalizable model if your data is big enough. if you think that your data does not have as many tokens as are needed, then you should know 2 things:</p>

<ol>
<li>Your data is not good enough and you have to gather more.</li>
<li>Your model can't generate well on the tokens that it hasn't seen at training! so it has no point to having many unused words on your embedding and better to gather more data to cover those words.</li>
</ol>

<p>I have an answer to show how you can select a minor set of word vectors from a pre-trained model <a href=""https://stackoverflow.com/a/55725093/7339624"">in here</a></p>
",1,0,322,2019-08-28 05:33:07,https://stackoverflow.com/questions/57685633/should-the-vocabulary-be-restricted-to-the-training-set-vocabulary-when-training
GC overhead limit exceeded when adding properties,"<p>I just started using the StanfordCoreNlp library for java and I keep on getting the GC overhead limit error when adding the coref or dcoref annotation properties. Any idea how to solve this?</p>

<p>I changed the JVM maxHeap memory up to 8GB of ram gradually for test purposes and this is definately not the issue.I tried removing several tags from the properties and this is the only one that seem to solve the overhead error, also it only gives the error on StanfordCoreNlp, the simpleCore api works without issue, but is not as effective. The snippet of code is the same as in the provided example on the official documentation from Stanford.</p>

<pre><code>public static void main(String[] args) {

            // run all Annotators on this text
            Properties props = new Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma,ner, parse, coref"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// read some text in the text variable
            String text = ""who is here?""; // Add your text here!
            Annotation document = new Annotation(text);

// run all Annotators on this text
            pipeline.annotate(document);
</code></pre>

<p>This is the exact error message:</p>

<pre><code>Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
    at java.util.Arrays.copyOfRange(Arrays.java:3664)
    at java.lang.String.&lt;init&gt;(String.java:207)
    at edu.stanford.nlp.util.StringUtils.splitOnChar(StringUtils.java:537)
    at edu.stanford.nlp.coref.data.Dictionaries.loadGenderNumber(Dictionaries.java:406)
    at edu.stanford.nlp.coref.data.Dictionaries.&lt;init&gt;(Dictionaries.java:676)
    at edu.stanford.nlp.coref.data.Dictionaries.&lt;init&gt;(Dictionaries.java:576)
    at edu.stanford.nlp.coref.CorefSystem.&lt;init&gt;(CorefSystem.java:32)
    at edu.stanford.nlp.pipeline.CorefAnnotator.&lt;init&gt;(CorefAnnotator.java:67)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.coref(AnnotatorImplementations.java:196)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$14(StanfordCoreNLP.java:532)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$25/2137589296.apply(Unknown Source)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$38/1798286609.get(Unknown Source)
    at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
    at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:251)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:192)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:188)
    at StanfordCoreNLPtest.main(StanfordCoreNLPtest.java:31)
</code></pre>

<p>Process finished with exit code 1</p>
",stanford-nlp,"<p>As far as I can see, this can only be caused by Java running out of memory — here it is just loading dictionaries, it isn’t in any infinite loop. Are you sure that you are specifying how to set the memory available correctly? E.g., if running under Eclipse, you need to be setting the application’s memory not the memory given to eclipse. 8GB should be sufficient to run CoreNLP with any options (unless a document is huge, but document loading happens after loading coref). But you could try with 12GB just in case to see what happens....</p>
",0,0,97,2019-08-28 07:27:19,https://stackoverflow.com/questions/57687040/gc-overhead-limit-exceeded-when-adding-properties
How to install correctly Stanford Parser in Ubuntu? (without nltk),"<p>I'm trying to install <strong>Stanford Parser</strong> on Ubuntu 18.04.3, purely without installing the NLTK package, but I am not sure how it's installed to be able to use it with Java or Python. </p>

<p>I have already downloaded the complete package and the languages I need from the official website  [Stanford Parser] [<a href=""https://nlp.stanford.edu/software/lex-parser.shtml#Download"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/lex-parser.shtml#Download</a> ]. 
So, I also have Java JDK and Java JRE installed, but I can't get Stanford Parser paks detected.</p>

<p>I'm new at this. :-|</p>
","java, python, ubuntu, stanford-nlp, linguistics","<ol>
<li>Download and unzip the directory</li>
<li><code>cd /path/to/stanford-parser-full-2018-10-17</code></li>
<li><code>export CLASSPATH=/path/to/stanford-parser-full-2018-10-17/*:</code></li>
<li><code>java -Xmx5g edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat ""penn,typedDependencies"" edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz data/english-onesent.txt</code> </li>
</ol>
",2,0,815,2019-09-02 17:39:10,https://stackoverflow.com/questions/57760919/how-to-install-correctly-stanford-parser-in-ubuntu-without-nltk
Patterns do not behave as expected,"<p>The actual patterns are not in English, so I created this simplified example to reproduce the problem: there are 3 levels of annotations (required for real application) and the 3rd level pattern does not work as expected. 
The phrase to be recognized is:
a b c</p>

<p>What I expect:</p>

<ul>
<li>1st level: ""a"" is annotated as A, ""b"" is annotated as ""B""</li>
<li>2nd: if there are annotations A and B, annotate them all together as AB</li>
<li>3rd: if at least one annotation AB is present and there is word ""c"", annotate them all together as C
Patterns are shown below.</li>
</ul>

<pre><code># 1.
{  pattern: (/a/), action: (Annotate($0, name, ""A"")) }
{  pattern: (/b/), action: (Annotate($0, name, ""B"")) }
# 2.
{  pattern: (([name:A]) ([name:B])), action: (Annotate($0, name, ""AB"")) }
# 3.
{  pattern: (([name:AB]+) /c/), action: (Annotate($0, name, ""C"")) }
</code></pre>

<p>#1 and #2 works and ""a b"" are annotated:
matched token: NamedEntitiesToken{word='a' name='AB' beginPosition=0 endPosition=1}
matched token: NamedEntitiesToken{word='b' name='AB' beginPosition=2 endPosition=3}
But the #3 pattern doesn't work even though one can see that we have 2 ""AB"" annotated tokens and it is exactly what is expected by #3 pattern.
Even more if I change #1 to be </p>

<pre><code>{  pattern: (/a/), action: (Annotate($0, name, ""AB"")) }
{  pattern: (/b/), action: (Annotate($0, name, ""AB"")) }
</code></pre>

<p>pattern #3 works correctly:
matched token: NamedEntitiesToken{word='a' name='C' beginPosition=0 endPosition=1}
matched token: NamedEntitiesToken{word='b' name='C' beginPosition=2 endPosition=3}
matched token: NamedEntitiesToken{word='c' name='C' beginPosition=4 endPosition=5}</p>

<p>I can't find any difference between matched tokens when I use</p>

<pre><code># In this case #3 pattern works
{  pattern: (/a/), action: (Annotate($0, name, ""AB"")) }
{  pattern: (/b/), action: (Annotate($0, name, ""AB"")) }
</code></pre>

<p>or when I use </p>

<pre><code># In this case #3 pattern doesn't work
# 1.
{  pattern: (/a/), action: (Annotate($0, name, ""A"")) }
{  pattern: (/b/), action: (Annotate($0, name, ""B"")) }
# 2.
{  pattern: (([name:A]) ([name:B])), action: (Annotate($0, name, ""AB"")) }
</code></pre>

<p>In both cases I get the same annotation, but first scenario works and the second doesn't.
What am I doing wrong?</p>
","stanford-nlp, tokenize","<p>This works for me:</p>

<pre><code># these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }

ENV.defaults[""stage""] = 1

{ ruleType: ""tokens"", pattern: (/a/), action: Annotate($0, ner, ""A"") }
{ ruleType: ""tokens"", pattern: (/b/), action: Annotate($0, ner, ""B"") }

ENV.defaults[""stage""] = 2

{ ruleType: ""tokens"", pattern: ([{ner: ""A""}] [{ner: ""B""}]), action: Annotate($0, ner, ""AB"") }

ENV.defaults[""stage""] = 3

{ ruleType: ""tokens"", pattern: ([{ner: ""AB""}]+ /c/), action: Annotate($0, ner, ""ABC"") }
</code></pre>

<p>There is a write up about TokensRegex here: </p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokensregex.html</a></p>
",0,0,45,2019-09-08 16:57:48,https://stackoverflow.com/questions/57843962/patterns-do-not-behave-as-expected
Text Classification - what can you do vs. what are your capabilities?,"<p>Text Classification basically works on the input training sentences. Little or less number of variations of in the sentences do work. But when there is a scenario like</p>

<blockquote>
  <p>What can you do &lt;&lt;==>> What are your capabilities</p>
</blockquote>

<p>This scenario does not work well with the regular classification or bot building platforms.</p>

<p>Are there any approaches for classification that would help me achieve this ?</p>
","nlp, classification, stanford-nlp, text-classification, azure-language-understanding","<p>What you are trying to solve is called Semantic Textual Similarity and is a known and well studied field.</p>

<p>There are many different ways to solve this even if your data is tagged or not.
For example, Google has published the <a href=""https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html"" rel=""nofollow noreferrer"">Universal Sentence Encoder</a> (<a href=""https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb"" rel=""nofollow noreferrer"">code example</a>) which is intended to tell if two sentences are similar like in your case.</p>

<p>Another example would be any solution you can find in <a href=""https://www.kaggle.com/c/quora-question-pairs"" rel=""nofollow noreferrer"">Quora Question Pairs Kaggle competition</a>.</p>

<p>There are also datasets for this problem, for example you can look for SemEval STS (STS for Semantic Textual Similarity), or the <a href=""https://github.com/google-research-datasets/paws"" rel=""nofollow noreferrer"">PAWS dataset</a> </p>
",3,0,69,2019-09-09 05:44:49,https://stackoverflow.com/questions/57848435/text-classification-what-can-you-do-vs-what-are-your-capabilities
SpaCy NER: Can a same word be part of two different entities?,"<p>For example:</p>
<p>Sentence: The best product in the world is Nestle Cookies.</p>
<blockquote>
<p>Entities:</p>
<p>BRAND: Nestle</p>
<p>PRODUCT: Nestle Cookie</p>
</blockquote>
<p>Are the above entities valid, or should I tag them as:</p>
<blockquote>
<p>Entities:</p>
<p>BRAND: Nestle</p>
<p>PRODUCT: Cookie</p>
</blockquote>
<p>And will it affect model performance?</p>
","nlp, stanford-nlp, spacy, feature-extraction, named-entity-recognition","<p>From the <a href=""https://support.prodi.gy/t/what-happens-if-your-annotation-has-overlapping-entity-spans/363"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>The entity recognizer is constrained to predict only non-overlapping, non-nested spans. The training data should obey the same constraint. If you like, you could have two sentences with the different annotations in your data. I’m not sure whether this would hurt or help your performance, though.</p>
<p>If you want spaCy to learn to recover both annotations, you could have two EntityRecognizer instances in the pipeline. You would need to move the entity annotations into an extension attribute, because you don’t want the second entity recogniser to overwrite the entities set by the first one.</p>
</blockquote>
<p>Consequence:</p>
<p>If you want to have a single NER tagger you must label as follows:<br />
Entities: BRAND: Nestle PRODUCT: Cookie</p>
<p>If you want to train two separate NER taggers (one for BRAND and one for PRODUCT)   then you can do:<br />
Entities: BRAND: Nestle PRODUCT: Nestle Cookie</p>
",4,3,2203,2019-09-11 09:27:13,https://stackoverflow.com/questions/57886043/spacy-ner-can-a-same-word-be-part-of-two-different-entities
TrueCaseAnnotator overwriteText option,"<p>I'm attempting to write a sentiment predictor for reviews. The Stanford docs say that poorly written inputs, e.g. capitalization, can throw off their tools, like sentiment detection. This is the hole I'm in right now.</p>

<p>I have the following:</p>

<pre><code>Properties prop = new Properties();
prop.setProperty( ""annotators"", ""tokenize, ssplit, truecase, pos, parse, sentiment"" );
StanfordCoreNLP pipeline = new StanfordCoreNLP( prop );
Annotation doc = new Annotation( ""I LOVE Target products. I love myself, too."" );
List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
for(CoreMap sentence : sentences)
{
  for(CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)
  {
    System.out.println(token + "": "" + token.get(SentimentCoreAnnotations.SentimentClass.class));
  }
  System.out.println();
}
</code></pre>

<p>This outputs:</p>

<pre><code>I-1: Neutral
LOVE-2: Neutral
Target-3: Neutral
products-4: Neutral
.-5: Neutral

I-1: Neutral
love-2: Very positive
myself-3: Neutral
,-4: Neutral
too-5: Neutral
.-6: Neutral
</code></pre>

<p>If the ""LOVE"" in the first sentence is <code>truecase</code>-d as ""love"", the sentiment comes out as ""Very positive"". From any perspective, ""LOVE"" should also be very positive. As this is tripping up sentiment detection, I wanted to apply <code>truecase-ing</code> in the pipeline before sentiment detection, the docs <a href=""https://stanfordnlp.github.io/CoreNLP/truecase.html"" rel=""nofollow noreferrer"">here</a> mention the <code>truecase.overwriteText</code> configuration for the <code>TrueCaseAnnotator</code>, but that appears to only be for the commandline.</p>

<p>Questions:</p>

<ol>
<li>How do I configure the <code>truecase</code>-ing stage in the pipeline to carry out the <code>overwriteText</code> step programmatically, via the API?</li>
<li>Generally, how does one configure Annotators in the pipeline?</li>
</ol>
",stanford-nlp,"<p>Based on the packaged properties files and the docs, I took a shot in the dark and did:</p>

<p><code>prop.setProperty(""truecase.overwriteText"", ""true"");</code></p>

<p>And that worked! This is the way to configure annotators in the pipeline.</p>
",0,0,40,2019-09-13 03:56:56,https://stackoverflow.com/questions/57917103/truecaseannotator-overwritetext-option
NLP Negation detection and Stop Words,"<p>I'd like to improve my sentiment analysis via negation detection. I'm implementing sentiment analysis using a bag of words approach highlight by professor, hence why I'm not using CoreNLP's sentiment annotator, yet. However, I noticed it has a problem.</p>

<p>Given the sentence, ""I'm not disappointed in them"", I would expect, at worst, a neutral sentiment, or weakly positive sentiment, both from the sentiment annotator, and my own bag of words implementation. The sentiment annotator reports this sentence as negative.</p>

<pre><code>I: PRP  Neutral
'm: VBP Neutral
not: RB Negative
disappointed: VBN   Negative
in: IN  Neutral
them: PRP   Neutral
.: .    Neutral
Negative
1
</code></pre>

<p>The last two lines show the sentence's sentiment label and the numerical sentiment score.</p>

<p>How do I improve the sentiment annotator's chances of getting this right, and how can I use CoreNLP to detect negation like what's shown, negation across sentences, and references to an entity across multiple sentences (which seems like the coref and dcoref annotators)?</p>

<p>Also, potentially useful would be getting rid of stop words. The lemma annotator seems to take care of stemming, but which annotator does stop words?</p>
","nlp, stanford-nlp","<p>If you use the <code>natlog</code> annotator, every token will be marked with a <code>NaturalLogicAnnotations.PolarityAnnotation</code>.  So negated words will have a polarity of <code>down</code>.</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.naturalli.*;
import edu.stanford.nlp.pipeline.*;

import java.util.*;


public class NaturalLogicExample {

  public static String text = ""I'm not disappointed in them."";

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,natlog"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    for (CoreLabel token : document.tokens()) {
      System.out.println(String.format(""%s\t%s"", token.word(),
          token.get(NaturalLogicAnnotations.PolarityAnnotation.class)));
    }
  }
}
</code></pre>
",1,0,1709,2019-09-14 23:31:08,https://stackoverflow.com/questions/57940020/nlp-negation-detection-and-stop-words
Integrate custom trained NER model with existing default model in Stanford CoreNLP,"<p>I have trained corpus by following below link. </p>

<p><a href=""https://www.sicara.ai/blog/2018-04-25-python-train-model-NTLK-stanford-ner-tagger"" rel=""nofollow noreferrer"">https://www.sicara.ai/blog/2018-04-25-python-train-model-NTLK-stanford-ner-tagger</a></p>

<p>Dataset is of some health blog (in English language) on which I've trained. I am successfully able to run this model on my new unseen text. </p>

<p>Problem: The problem I am facing is that I want to run my custom English NER model along with default English model in Stanford CoreNLP. </p>

<p>Desired Outcome: I want Stanford default model to run in sequential manner just after my own custom model NER model to handle those entities in English that are missed by my own model. </p>
","python, stanford-nlp, named-entity-recognition","<pre><code> ner.model = /path/to/custom-model.ser.gz,edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz
</code></pre>

<p>There is more info here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/api.html</a></p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/cmdline.html</a></p>
",0,1,392,2019-09-24 05:18:20,https://stackoverflow.com/questions/58073494/integrate-custom-trained-ner-model-with-existing-default-model-in-stanford-coren
Python NLP differentiation of British English and American English,"<p>Currently i am working on a project using nlp and python. i have content and need to find the language. I am using spacy to detect the language. The libraries are providing only language as English language. i need to find whether it is British or American English? Any suggestions?</p>

<p>I tried with Spacy, NLTK, lang-detect. but this libraries provide only English. but i need to display as en-GB for British and en-US for american.</p>
","python, nlp, stanford-nlp, spacy","<p>You can train your own model. Many geographically specific data on English were <a href=""http://wortschatz.uni-leipzig.de/en/download/"" rel=""nofollow noreferrer"">collected by University of Leipzig</a>, but it does not include US English. <a href=""http://www.anc.org"" rel=""nofollow noreferrer"">American National Corpus</a> should a free subset that you can use.</p>

<p>A popular library for language <em>langid.py</em> allows training your own model. They have a nice <a href=""https://github.com/saffsd/langid.py#training-a-model"" rel=""nofollow noreferrer"">tutorial on github</a>. Their model is based on character tri-gram frequencies, which might not be sufficiently distinctive statistics in this case.</p>

<p>Another option is to train a classifier on top of BERT using e.g., Pytorch and the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">transormers</a> library. This will surely get very good results, but if you are not experienced with deep learning, it might be actually a lot of work for you.</p>
",1,0,892,2019-10-01 09:24:52,https://stackoverflow.com/questions/58181798/python-nlp-differentiation-of-british-english-and-american-english
How to detect sentence stress by python NLP packages (spaCy or NLTK)?,"<p>Can we detect the <a href=""https://www.englishclub.com/pronunciation/sentence-stress.htm"" rel=""nofollow noreferrer"">sentence stress</a> (the stress on some words or pauses between words in a sentence) using common NLP packages such as spaCy or NLTK?</p>

<p><a href=""https://i.sstatic.net/d3aKy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/d3aKy.png"" alt=""enter image description here""></a></p>

<p>How can we tell content words from structure words using spaCy or NLTK?</p>

<p>Since all NLP programs detect the dependencies, there should be a possibility to identify which words are stressed in natural speech.</p>
","nlp, nltk, stanford-nlp, spacy","<p>I don't think that NLTK or spacy support this directly. You can find content words with either tool, sure, but that's only part of the picture. You want to look for software related to prosody or intonation, which you might find as a component of a text-to-speech system.</p>

<p>Here's a very recently published research paper with code that might be a good place to start: <a href=""https://github.com/Helsinki-NLP/prosody/"" rel=""nofollow noreferrer"">https://github.com/Helsinki-NLP/prosody/</a> . The annotated data and the references could be useful even if the code might not be exactly the kind of approach you're looking for.</p>
",2,0,1141,2019-10-05 18:56:20,https://stackoverflow.com/questions/58251398/how-to-detect-sentence-stress-by-python-nlp-packages-spacy-or-nltk
Hebrew Stanford NLP tag set,"<p>I am trying to find the <strong>exact</strong> list of tag set used in the Hebrew treebank used by Stanford NLP. Finding this tag set seems to be harder than finding a POS tagger :)</p>

<p>Are there any tools for reading the tag set used for training a (Penn?) tree bank?</p>
","nlp, stanford-nlp, hebrew, pos-tagger, penn-treebank","<p>For the stanfordnlp python package, for all languages, the POS tag set used is the <a href=""https://universaldependencies.org/u/pos/all.html"" rel=""nofollow noreferrer"">Universal Dependencies (UD) v2 tag set</a>. Some UD corpora also include an original POS tag set, which is often more fine-grained. But while the Hebrew Treebank was originally built with its own POS tag set, and was then coverted to UD, it seems like the supplied version in the UD repository comes only with the UD tag set. Individual languages may use only a subset of the UD POS tag set. You can find details of that on the <a href=""https://universaldependencies.org/treebanks/he_htb/index.html"" rel=""nofollow noreferrer"">Treebank hub page for the Hebrew TreeBank</a>. You'll see there that 15 of the 17 UD POS tags are used.</p>
",1,0,183,2019-10-08 18:32:50,https://stackoverflow.com/questions/58292167/hebrew-stanford-nlp-tag-set
What is the meaning of labels on the arrows of dependency parser graph?,"<p>I am using coreNLP module demo by Stanford online here: <a href=""https://corenlp.run"" rel=""nofollow noreferrer"">https://corenlp.run</a> .
So, I am trying out a few sentences to see their syntactic structure using dependency parser available here. 
One such example is this sentence:</p>

<p>""documents related to new york industry that is export oriented and documents related to Indian history where Akbar is not fighting""</p>

<p>Its result is:
<a href=""https://i.sstatic.net/iUwsK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iUwsK.png"" alt=""enter image description here""></a></p>

<p>I am not able to understand the meaning of labels written on the arrows. I have got the list of labels to all the words here: <a href=""https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels"">Java Stanford NLP: Part of Speech labels?</a></p>

<p>But where can I get a list of the labels on arrows/dependencies with their explanations?</p>
","label, stanford-nlp, dependency-parsing","<p>Here are the definitions of dependency relations and their labels as used for English. When you click on a relation label, you'll get full explanation with examples.</p>

<p><a href=""https://universaldependencies.org/en/dep/index.html"" rel=""nofollow noreferrer"">https://universaldependencies.org/en/dep/index.html</a></p>
",2,1,940,2019-10-10 16:59:29,https://stackoverflow.com/questions/58327780/what-is-the-meaning-of-labels-on-the-arrows-of-dependency-parser-graph
Python: Convert Dataframe into a natural language text,"<p>We're implementing NLP solution, where we have a bunch of paragraphs text and tables. We've used google's burt for NLP, and it works great on text. However, if we ask a question whose answer lies in a table value then our nlp solution wouldn't work. Because it only works on natural language text (sentence, paragraph etc).</p>

<p>So, in order to get the answer from a table (dataframe) we're thinking to convert the whole dataframe into a natural language text which perserve the relation of each cell with its corresponding column name and row. For example:</p>

<pre><code>+------------+-----------+--------+--+
| First Name | Last Name | Gender |  |
+------------+-----------+--------+--+
| Ali        | Asad      | Male   |  |
| Sara       | Dell      | Female |  |
+------------+-----------+--------+--+
</code></pre>

<p>Will become: </p>

<ul>
<li>First Name is Ali, Last Name is Asad, and Gender is Male </li>
<li>First Name is Sara, Last Name is Dell, and Gender is Female</li>
</ul>

<p>This will help us to find the right answer, for example, if I ask 'What's the Gender of 'Ali', then our NLP solution will give us the answer 'Male'.</p>

<p>I'm wondering is there any library available in python that converts a dataframe into a natural language text. Or shall I have to do it manually?</p>

<p>Many thanks</p>
","python, numpy, dataframe, nlp, stanford-nlp","<p>If you want to store it in a list you can easily do</p>

<pre><code>text=[]
for index,rows in df.iterrows():
  a='First Name is {0}, Last Name is {1} and Gender is {2}'.format(df['First Name'] 
  [index],df['Last Name'][index],df['Gender'][index])
  text.append(a)
print(text)
</code></pre>

<p>You can then convert this list in natural language so that model can understand.</p>
",1,-1,1069,2019-10-17 05:22:48,https://stackoverflow.com/questions/58425409/python-convert-dataframe-into-a-natural-language-text
Why am I getting String where should get a dict when using pycorenlp.StanfordCoreNLP.annotate?,"<p>I'm running this <a href=""https://stackabuse.com/python-for-nlp-getting-started-with-the-stanfordcorenlp-library/"" rel=""nofollow noreferrer"">example</a> using pycorenlp Stanford Core NLP python wrapper, but the annotate function returns a string instead of a dict, so, when I iterate over it to get each sentence sentiment value I get the following error: ""string indices must be integers"".</p>

<p>What could I do to get over it? Anyone could help me? Thanks in advance.
The code is below:</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp_wrapper = StanfordCoreNLP('http://localhost:9000')
doc = ""I like this chocolate. This chocolate is not good. The chocolate is delicious. Its a very 
    tasty chocolate. This is so bad""
annot_doc = nlp_wrapper.annotate(doc,
                                 properties={
                                            'annotators': 'sentiment',
                                            'outputFormat': 'json',
                                            'timeout': 100000,
                                 })
for sentence in annot_doc[""sentences""]:
      print("" "".join([word[""word""] for word in sentence[""tokens""]]) + "" =&gt; ""\
            + str(sentence[""sentimentValue""]) + "" = ""+ sentence[""sentiment""])
</code></pre>
","stanford-nlp, sentiment-analysis, pycorenlp","<p>You should just use the official stanfordnlp package! (note: the name is going to be changed to stanza at some point)</p>

<p>Here are all the details, and you can get various output formats from the server including JSON.</p>

<p><a href=""https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html</a></p>

<pre><code>from stanfordnlp.server import CoreNLPClient
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=30000, memory='16G') as client:
    # submit the request to the server
    ann = client.annotate(text)
</code></pre>
",0,1,248,2019-10-23 14:59:26,https://stackoverflow.com/questions/58525763/why-am-i-getting-string-where-should-get-a-dict-when-using-pycorenlp-stanfordcor
Starting a server instance in separate thread is not passing server object to method,"<p>I am trying to start Stanford NLP server in one separate thread and pass the server instance as a parameter to method <code>run_nlp</code> defined in class <code>Grid</code>, which is a layout class for my python application.</p>

<pre><code>from readUseCase import parser
...
...
class Grid(Widget):
    text_from_file = ObjectProperty(None)
    client = ObjectProperty(None)

    def start_server(self):
        with CoreNLPClient(properties='./server.props') as self.client:
            pass

    def run_nlp(self):
        parser(self.client, self.text_from_file.text)

class Main(App):
    def build(self):
        thread1 = threading.Thread(target=Grid().start_server)
        thread1.start()
        return Grid()

if __name__ == '__main__':
    Main().run()
</code></pre>

<p>So what happens is, I start app, <code>thread</code> starts, console prints out that the server is running, my app is also running and waiting for some user input. But when I input some text for NLP and I hit a button to start method <code>run_nlp()</code>, function <code>parser()</code> needs 2 input parameters, first is the server object and second is input text.</p>

<p>Function parser:</p>

<pre><code>def parser(client, text):

    # submit the request to the server
    ann = client.annotate(text)      //AttributeError(""NoneType"" object has no attribute ""annotate"")
</code></pre>

<p>I am getting correct value in <code>self.text_from_file.text</code> parameter but I get <code>NoneType</code> for <code>self.client</code> parameter, and that is what I do not know why. I have tried to assign it to some other variable, or I tried to <code>return self.client</code> but none of this works.</p>

<p>Am I calling the <code>start_server</code> right? Or where could be the problem?</p>
","python, multithreading, stanford-nlp","<p>Maybe it is because you create two objects of type <code>Grid</code> in the <code>build</code> method. One is used to start the thread but another one is created and returned. I think you should change your code to something like this: </p>

<pre><code>class Main(App):
    def build(self):
        grid = Grid()
        thread1 = threading.Thread(target=grid.start_server)
        thread1.start()
        return grid
</code></pre>

<p>Another problem could be, that client is at no point set. I don't know this library, but make sure you setup everything correctly.</p>
",0,0,36,2019-10-27 20:42:16,https://stackoverflow.com/questions/58583185/starting-a-server-instance-in-separate-thread-is-not-passing-server-object-to-me
Using torch.nn.Embedding for GloVe: should we fine-tune the embeddings or just use them as they are?,"<p>while transfer learning / fine-tuning recent language models, such as BERT and XLNET, is by far a very common practice, how is this for GloVe? </p>

<p>Basically, I see two options when using GloVe to get dense vector representations that can be used by downstream NNs.</p>

<p>1) Fine-tune GloVe embeddings (in pytorch terms, gradient enabled)</p>

<p>2) Just use the embeddings without gradient.</p>

<p>For instance, given GloVe's embeddings matrix, I do</p>

<pre><code>embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
...
dense = nn.Linear(...)
</code></pre>

<p>Is it best practice to solely use GloVe to get vector representation (and only train the dense layer and potentially other layers) or would one fine-tune the embeddings matrix, too? </p>
","pytorch, glove","<p>You should absolutely fine-tune your word embedding matrix. Here is the thing, when you initialize the word embedding matrix with the GloVe word embeddings, your word embeddings will already capture most of the semantic properties of the data. However, you want your word embeddings to be tailored to the task your solving i.e task specific (Check <a href=""https://arxiv.org/abs/1505.07931"" rel=""noreferrer"">Yang</a>). Now, assuming that you don't have enough data in your dataset, you can't learn the word embedding matrix on your own (If you initialize the word embedding matrix with random vectors). Because of that, you want to initialize it with vectors that have been trained on huge datasets and are general.</p>
<p>One really important thing to keep in mind → Because the rest of your model is going to be initialized randomly, when you start training your word embedding matrix may suffer from catastrophic forgetting (Check the work of <a href=""https://arxiv.org/abs/1801.06146"" rel=""noreferrer"">Howard and Ruder</a> and <a href=""https://arxiv.org/abs/1612.00796"" rel=""noreferrer"">Kirkpatrick et al.</a>), i.e., the gradients will be huge because your model will drastically underfit the data for the first few batches, and you will lose the initial vectors completely. You can overcome this by:</p>
<ol>
<li><p>For the first several epochs don't fine-tune the word embedding matrix, just keep it as it is: <code>embeddings = nn.Embedding.from_pretrained(glove_vectors, freeze=True)</code>.</p>
</li>
<li><p>After the rest of the model has learned to fit your training data, decrease the learning rate, unfreeze the your embedding module <code>embeddings.weight.requires_grad = True</code>, and continue training.</p>
</li>
</ol>
<p>By following the above mentioned steps, you will get the best of both worlds. In other words, your word embeddings will still capture semantic properties while being tailored for your own downstream task. Finally, there are works (Check <a href=""https://arxiv.org/abs/1510.03820"" rel=""noreferrer"">Ye Zhang</a> for example) showing that it is fine to fine-tune immediately, but I would opt for the safer option.</p>
",14,4,5407,2019-10-30 16:42:49,https://stackoverflow.com/questions/58630101/using-torch-nn-embedding-for-glove-should-we-fine-tune-the-embeddings-or-just-u
How to get training data and models of Stanford CoreNLP?,"<p>I downloaded Stanford CoreNLP from the <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">official website</a> and <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">GitHub</a>.</p>
<p>In the <a href=""https://nlp.stanford.edu/software/nndep.html"" rel=""nofollow noreferrer"">guides</a>, it is stated</p>
<blockquote>
<p>On the Stanford NLP machines, training data is available in
/u/nlp/data/depparser/nn/data</p>
</blockquote>
<p>or
<a href=""https://nlp.stanford.edu/static/software/nndep.shtml"" rel=""nofollow noreferrer"">HERE</a></p>
<blockquote>
<p>The list of models currently distributed is:</p>
<p>edu/stanford/nlp/models/parser/nndep/english_UD.gz (default, English,
Universal Dependencies)</p>
</blockquote>
<p>It may sound a silly question, but I cannot find such files and folders in any distribution.</p>
<p>Where can I find the source data and models officially distributed with Stanford CoreNLP?</p>
",stanford-nlp,"<p>We don't distribute most of the CoreNLP training data. Quite a lot of it is non-free, licensed data produced by other people (such as LDC <a href=""https://www.ldc.upenn.edu/"" rel=""nofollow noreferrer"">https://www.ldc.upenn.edu/</a>). </p>

<p>However, a huge number of free dependency treebanks are available through the Universal Dependencies project: <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">https://universaldependencies.org/</a>.</p>

<p>All the Stanford CoreNLP models are available in the ""models"" jar files. edu/stanford/nlp/models/parser/nndep/english_UD.gz is in this one: <code>stanford-corenlp-3.9.2-models.jar</code>, which is both in the zip file download <a href=""http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip</a> or can be found on Maven here: <a href=""http://central.maven.org/maven2/edu/stanford/nlp/stanford-parser/3.9.2/"" rel=""nofollow noreferrer"">http://central.maven.org/maven2/edu/stanford/nlp/stanford-parser/3.9.2/</a>.</p>
",1,0,500,2019-11-02 01:21:06,https://stackoverflow.com/questions/58667512/how-to-get-training-data-and-models-of-stanford-corenlp
How to train Stanford NLP NER Extraction model to skip the repeating words?,"<p>I am trying to extract the NER from the text using <strong>.NET Framework</strong> and <strong>StanFord NER Model</strong>.
I have a text like </p>

<p>Hello, I am John Doe. Body Mass index is 27. And Body Surface Area is 2.3m.</p>

<p>For this i did create tsv file to train the model. Which is as under:</p>

<pre><code>Hello   O
,   O
I   O
am  O
John    PERSON
Doe.    PERSON
Body    BMI
Mass    BMI
index   BMI
is  O
27. O
And O
Body    O
Surface O
Area    O
is  O
2.3m.   O
</code></pre>

<p>prop file is as under</p>

<pre><code>trainFileList = train/standford_train.tsv
serializeTo = dummy-ner-model-eng.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
</code></pre>

<p>and using below java command</p>

<pre><code>java -mx1g -cp stanford-ner.jar;lib/* edu.stanford.nlp.ie.crf.CRFClassifier -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -prop train/prop.txt
</code></pre>

<p>So, the problem i am facing is Body with tagging BMI is coming two times because of repetition in <strong>Body Mass Index</strong> and <strong>Body Surface Area</strong>.</p>

<p>Is there any way that i can omit this second body tag?</p>
","nlp, stanford-nlp, named-entity-recognition","<p>You'll need to produce more training data that has examples with <code>Body</code> not labeled as <code>BMI</code>.  If you are only looking for specific patterns, you might get better results with a rule-based approach.  There are tools for rule based NER building in Stanford CoreNLP.  </p>

<p>More info: <a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokensregex.html</a></p>
",0,0,272,2019-11-08 09:22:41,https://stackoverflow.com/questions/58763703/how-to-train-stanford-nlp-ner-extraction-model-to-skip-the-repeating-words
Error while trying to do POStagging: Error while loading a tagger model (probably missing model file),"<p>I am trying to use StanfordNLP for croatian using windows command prompt. I have downloaded the specific model for this language (hr_set_models) with .pt files.</p>

<p>I have created the .properties file but I get the following message:</p>

<p>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)</p>

<p>There is no problem for the tokenizer model and the file hr_set_tagger.pt is in the folder.</p>

<p>I see that in the model folder there is also a file named hr_set.pretrain.pt, I do not know if I should use it in the .properties file.</p>

<p>Thanks in advance! </p>

<p>Bellow is the .properties file I have created. </p>

<pre><code>annotators = tokenize, ssplit, pos, lemma, depparse
# tokenize
tokenize.model = hr_set_models/hr_set_tokenizer.pt
# pos
pos.model = hr_set_models/hr_set_tagger.pt
# lemma
lemma.model = hr_set_models/hr_set_lemmatizer.pt
#depparse
depparse.model = hr_set_models/hr_set_parser.pt
</code></pre>
",stanford-nlp,"<p>You need to use the full Python system.  There are no Java models for Croatian, so you shouldn't be using the Stanford CoreNLP server.  </p>

<p>There is more documentation here: <a href=""https://stanfordnlp.github.io/stanfordnlp/pipeline.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanfordnlp/pipeline.html</a></p>
",0,0,275,2019-11-11 21:30:05,https://stackoverflow.com/questions/58808950/error-while-trying-to-do-postagging-error-while-loading-a-tagger-model-probabl
Missing StanfordNLP Universal Dependency features in Java CoreNLP,"<p>Using the latest CoreNLP 3.9.2 Java API, I wish to extract the new Universal Dependencies features as they appear in the <a href=""https://stanfordnlp.github.io/stanfordnlp/processors.html"" rel=""nofollow noreferrer"">StanfordNLP Python library</a>, and as defined here - <a href=""https://universaldependencies.org/guidelines.html"" rel=""nofollow noreferrer"">universaldependencies.org/guidelines.html</a>. Specifically:</p>

<ol>
<li>Multiword tokens</li>
<li>POS tags in Universal Dependencies format (UPOS)</li>
<li>Grammatical dependencies in UD format (using UPOS tags)</li>
</ol>

<p>The current CoreNLP produces Penn Tree POS tags and dependencies as described <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow noreferrer"">here</a> and <a href=""https://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow noreferrer"">here</a>, respectively.</p>

<p>Pipeline config:</p>

<pre><code>    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote"");
    props.setProperty(""coref.algorithm"", ""neural"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    CoreDocument document = new CoreDocument(text);
    pipeline.annotate(document);

    CoreSentence sentence = document.sentences().get(0);
    sentence.posTags() // get pos tags
    sentence.dependencyParse() // dependency graph
</code></pre>

<p>Any help and clarification of my misunderstandings is much obliged.</p>
",stanford-nlp,"<p>The GitHub version of the code and models for French, German, and Spanish were trained on the CoNLL 2018 UD data, and support multi-word tokens.</p>

<p>We may or may not train and English UD part-of-speech model.</p>

<p>I believe the constituency parser data is using English-specific part-of-speech tags.</p>

<p>These changes will be put into the 4.0.0 release which will hopefully be done before the end of the year.</p>
",1,1,155,2019-11-15 21:16:29,https://stackoverflow.com/questions/58884788/missing-stanfordnlp-universal-dependency-features-in-java-corenlp
Stanfordnlp python - sentence split and other simple functionality,"<p>I'm trying to split a string into sentences using Stanford NLP parser, I used the sample code provided by Stanford NLP but it gave me words instead of sentences.</p>

<p>here's the sample input: </p>

<pre><code>""this is sample input. I want to split this text into a list of sentences. Please help""
</code></pre>

<p>here's my desired output: </p>

<pre><code>[""this is sample input."", ""I want to split this text into a list of sentences."", ""Please help""]
</code></pre>

<p>What I've done:</p>

<ul>
<li>NLTK sent_tokenizer(); does not split newlines, and seems less accurate than stanfordnlp</li>
<li>stanfordnlp split is great but the sample output isn't in the list of sentences</li>
</ul>

<p>I heard there's a nltk parser that uses stanfordnlp library, but I was unable to get any sample guide for it. </p>

<p>At this point, I'm quite confused as there's almost no exhaustive python guide for Stanford NLP. It's mandatory to use python for this task, as other components in my research use python to process the data. Please help! thank you.</p>

<p>sample code:</p>

<pre><code>import stanfordnlp

nlp = stanfordnlp.Pipeline(processors='tokenize', lang='en')
doc = nlp(a)
for i, sentence in enumerate(doc.sentences):
    print(f""====== Sentence {i+1} tokens ======="")
    print(*[f""index: {token.index.rjust(3)}\ttoken: {token.text}"" for token in sentence.tokens], sep='\n')
print(doc.sentences.tokens.text[2])
</code></pre>

<p>output: </p>

<pre><code>====== Sentence 84 tokens =======
index:   1  token: Retweet
index:   2  token: 10
index:   3  token: Like
index:   4  token: 83
index:   5  token: End
index:   6  token: of
index:   7  token: conversation
index:   8  token: ©
index:   9  token: 2019
index:  10  token: Twitter
index:  11  token: About
index:  12  token: Help
index:  13  token: Center
index:  14  token: Terms
index:  15  token: Privacy
index:  16  token: policy
====== Sentence 85 tokens =======
index:   1  token: Cookies
index:   2  token: Ads
index:   3  token: info
</code></pre>

<p>source : <a href=""https://stanfordnlp.github.io/stanfordnlp/pipeline.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanfordnlp/pipeline.html</a></p>
","python, stanford-nlp","<p>I would use normal <code>split('.')</code> but it will not work if sentence ends on <code>?</code> or <code>!</code>, etc.
It would need <code>regex</code> but it still may treats <code>...</code> inside sentence as ends of three sentences.</p>
<hr />
<p>With <code>stanfordnlp</code> I can only concatenate words in sentence so it gives sentence as one strings but this simple method adds spaces before <code>,.?!</code>, etc.</p>
<pre><code>import stanfordnlp

text = &quot;this is ... sample input. I want to split this text into a list of sentences. Can you? Please help&quot;

nlp = stanfordnlp.Pipeline(processors='tokenize', lang='en')
doc = nlp(text)

for i, sentence in enumerate(doc.sentences):
    sent = ' '.join(word.text for word in sentence.words)
    print(sent)
</code></pre>
<p>Result</p>
<pre><code>this is ... sample input .
I want to split this text into a list of sentences .
Can you ?
Please help
</code></pre>
<p>Maybe in source code it could find how it splits text to sentences and use it.</p>
",1,1,1530,2019-12-09 03:18:55,https://stackoverflow.com/questions/59242102/stanfordnlp-python-sentence-split-and-other-simple-functionality
Keras word embedding matrix has first row of zeros,"<p>I am looking at the Keras Glove word embedding example and it is not clear why the first row of the embedding matrix is populated with zeros.</p>

<p>First, the embedding index is created where words are associated with arrays.</p>

<pre><code>embeddings_index = {}
with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, 'f', sep=' ')
        embeddings_index[word] = coefs
</code></pre>

<p>Then the embedding matrix is created by looking at words from the index created by tokenizer. </p>

<pre><code># prepare embedding matrix
num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if i &gt;= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
</code></pre>

<p>Since the loop will start with <code>i=1</code>, then the first row will contain only zeros and random numbers if the matrix is initialized differently. Is there a reason for skipping the first row?</p>
","keras, word-embedding, glove","<p>The whole started from the fact that the <code>Tokenizer</code>'s programmers reserved the index <code>0</code> for some reason, maybe for some compatibility (some other languages use indexing from <code>1</code>) or coding technic reasons.</p>

<p>However they use numpy, where they want to indexing with the simple:</p>

<pre><code>embedding_matrix[i] = embedding_vector
</code></pre>

<p>indexing, so the <code>[0]</code> indexed row stays full of zeros and there is no case where, as wrote <em>""random numbers if the matrix is initialized differently""</em>, because this array has been initialized with <strong>zeros</strong>. 
So from this line we don't need the first row at all, but you can't delete it as the numpy array would lost the aligning its indexing with the tokenizer's indexing.</p>
",1,1,443,2019-12-30 15:43:43,https://stackoverflow.com/questions/59533346/keras-word-embedding-matrix-has-first-row-of-zeros
Customized StanfordNER,"<p>I am trying to build a customized StanfordNer model, training data and properties file are ready.<br>
But when I am trying to run the following code :</p>

<pre><code>java -cp ""stanford-ner.jar:lib/*"" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop download.txt
</code></pre>

<p>This error is popping out :</p>

<blockquote>
  <p>Error: Could not find or load main class
  edu.stanford.nlp.ie.crf.CRFClassifier</p>
</blockquote>

<p><em>Steps followed:</em></p>

<ol>
<li>Downloaded and extracted stanford-ner-2018-10-16.zip file.<br></li>
<li>Java 8 installed and $JAVA_HOME has been set.<br></li>
<li>The properties file (download.txt) has been placed in the folder where stanford-ner-2018-10-16.zip is extracted.</li>
</ol>
","java, machine-learning, nlp, stanford-nlp, named-entity-recognition","<p>If you are seeing errors like that it means your CLASSPATH is not properly configured.</p>

<p>You need to run that command in the same folder as the NER download or it won't find the needed jars.  That command should be run in whatever directory has <code>stanford-ner.jar</code> and <code>lib</code> in it.  Alternatively you can just set the <code>CLASSPATH</code> environment variable and remove the <code>-cp</code> option from the command.</p>

<p>More info on Java <code>CLASSPATH</code> here: <a href=""https://docs.oracle.com/javase/tutorial/essential/environment/paths.html"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/tutorial/essential/environment/paths.html</a></p>
",1,2,99,2020-01-10 12:37:27,https://stackoverflow.com/questions/59681871/customized-stanfordner
StanfordNLP to detect compound entities with prepositions,"<p>Basically, in the sentence:</p>

<pre><code>&lt;Lord of the bracelets&gt; is a fantasy movie.
</code></pre>

<p>I would like to detect the compound <code>Lord of the bracelets</code> as one entity (that could be linked in the entitylink annotator as well). This means detecting structures with POS tags of a form like <code>NNP</code> <code>DT</code> <code>NNP</code> or <code>NN</code> <code>IN</code> <code>DT</code> <code>NNP</code>.</p>

<p>Is this possible with CoreNLP?</p>

<p>My current setup doesn't detect them, and I couldn't find a way to do it.</p>

<pre class=""lang-java prettyprint-override""><code>
  public NamedEntityRecognition() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitylink"");
    props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");

    pipeline = new StanfordCoreNLP(props);
  }


  public CoreDocument recogniseEntities(String text) {
    CoreDocument doc = new CoreDocument(text);
    pipeline.annotate(doc);
    return doc;
  }

</code></pre>

<p>Thanks!</p>
","java, stanford-nlp, named-entity-recognition","<p>While @StanfordNLPHelp's answer was helpful, I thought I would add some more details into what my final solution was.</p>

<h1>Option 1:</h1>

<p>Add a <a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer""><em>TokensRegex</em> annotator</a> as pointed out by the previous answer. This adds a more customisable annotator to the pipeline, and you can specify your own rules in a text file.</p>

<p>This is what my rules file (extended_ner.rules) looks like:</p>

<pre><code># these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

# rule for recognizing compound names
{ ruleType: ""tokens"", pattern: ([{tag:""NN""}] [{tag:""IN""}] [{tag:""DT""}] [{tag:""NNP""}]), action: Annotate($0, ner, ""COMPOUND""), result: ""COMPOUND_RESULT"" }
</code></pre>

<p>You can see a breakdown of the rules sintax <a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html#tokens-rules-patterns"" rel=""nofollow noreferrer"">here</a>.</p>

<p><strong>Note:</strong> The TokensRegex annotator <em>must</em> be added after the ner annotator. Otherwise, the results will be overwritten.</p>

<p>This is what the Java code would look like:</p>

<pre class=""lang-java prettyprint-override""><code> public NamedEntityRecognition() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,tokensregex,entitylink"");
    props.setProperty(""tokensregex.rules"", ""extended_ner.rules"");
    props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");

    pipeline = new StanfordCoreNLP(props);
  }
</code></pre>

<h1>Option 2 (Chosen one)</h1>

<p>Instead of adding another annotator, the rules file can be sent to the ner annotator via de <code>""ner.additional.tokensregex.rules""</code> property. <a href=""https://stanfordnlp.github.io/CoreNLP/ner.html#additional-tokensregex-rules"" rel=""nofollow noreferrer"">Here</a> are the docs.</p>

<p>I chose this option because it seems simpler, and adding another annotator to the pipeline seemed a bit overdone for my case.</p>

<p>The rules file is exactly the same as in option 1, the java code now is:</p>

<pre class=""lang-java prettyprint-override""><code> public NamedEntityRecognition() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitylink"");
    props.setProperty(""ner.additional.tokensregex.rules"", ""extended_ner.rules"");

    props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");

    pipeline = new StanfordCoreNLP(props);
  }
</code></pre>

<p><strong>Note:</strong> For this to work, the property <code>""ner.applyFineGrained""</code> must be true (default value).</p>
",0,3,209,2020-01-16 10:14:10,https://stackoverflow.com/questions/59767325/stanfordnlp-to-detect-compound-entities-with-prepositions
Adding Stanford CoreNLP 3.9.2 as Dependency via Maven,"<p>I'm trying to add <a href=""https://mvnrepository.com/artifact/edu.stanford.nlp/stanford-corenlp/3.9.2"" rel=""nofollow noreferrer"">Stanford CoreNLP 3.9.2</a> as dependency to my Eclipse/Maven project:</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
   &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
   &lt;version&gt;3.9.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Next to my POM.xml file I see a little red x icon. When I open POM.xml there is no additional information regarding the error.</p>

<p>When I click on Java -> Properties -> Java Build Path -> Maven Dependencies I see that the Jars that were expected to be added to Maven via this dependency are missing. This is odd because I regularly add dependencies this way without any error.</p>

<p>Apparently, something is preventing Maven from downloading the dependencies. What could it be? </p>

<p><strong>Update:</strong> </p>

<p>I changed POM file to version 3.5.2 (instead of 3.9.2) and now all errors are gone.</p>

<p>If anyone can explain WHY this solved my problem (and how to make things work with version 3.9.2) I will accept it as the answer.</p>

<p><strong>Update:</strong></p>

<p>When I go to my Maven repository I see that most of the required Jars have been downloaded by Maven. For example, Maven repository will contain the folders: <code>\\maven\.m2\repository\edu\stanford\nlp\stanford-corenlp\3.9.2</code> However the folder will not contain the Jar: <code>stanford-corenlp-3.9.2</code> - but it will contain every other Jar such as <code>stanford-corenlp-3.9.2-models</code> and <code>stanford-corenlp-3.9.2-sources</code> etc. </p>

<p>This makes the whole situation even more confusing. If Maven is downloading the Jars why is it skipping just one Jar? I looked in several other folders (dependencies of corenlp) and I see similar phenomenon - it's always the main Jar of that folder that is missing. </p>

<p>What's worse, when I download and add the missing Jars manually to Maven folder, the (missing) text next to Jar goes away but there's still a little red x icon next to POM file. I have no idea what is going on.</p>

<p>Any insights?</p>

<p>Thanks!</p>
","java, eclipse, maven, dependencies, stanford-nlp","<p>I have no idea why this fixed the problem but in my POM file I had an entry:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;
    &lt;artifactId&gt;org.eclipse.debug.core&lt;/artifactId&gt;
    &lt;version&gt;3.13.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>I update this dependency to:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;
    &lt;artifactId&gt;org.eclipse.debug.core&lt;/artifactId&gt;
    &lt;version&gt;3.14.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Now all errors have disappeared.</p>
",1,2,783,2020-01-29 16:51:22,https://stackoverflow.com/questions/59971947/adding-stanford-corenlp-3-9-2-as-dependency-via-maven
Is there a listener in the Stanford CoreNLP pipeline checking for abort?,"<p>I am working on an <a href=""https://github.com/lcahlander/exist-stanford-nlp"" rel=""nofollow noreferrer"">XQuery implementation of the Stanford CoreNLP pipeline for eXist-db</a>. <a href=""http://exist-db.org"" rel=""nofollow noreferrer"">eXist-db</a> is an open source XML database.  </p>

<p>I have written a function module for eXist that is written in Java that wraps around the CoreNLP pipeline.</p>

<p>Here is an example of a call:</p>

<pre><code>xquery version ""3.1"";

import module namespace nlp=""http://exist-db.org/xquery/stanford-nlp"";

let $text := ""The fate of Lehman Brothers, the beleaguered investment bank, "" ||
             ""hung in the balance on Sunday as Federal Reserve officials and "" ||
             ""the leaders of major financial institutions continued to gather in "" ||
             ""emergency meetings trying to complete a plan to rescue the stricken "" ||
             ""bank.  Several possible plans emerged from the talks, held at the "" ||
             ""Federal Reserve Bank of New York and led by Timothy R. Geithner, "" ||
             ""the president of the New York Fed, and Treasury Secretary Henry M. Paulson Jr.""

let $properties := map { 
                     ""annotators"" : ""tokenize, ssplit, pos, lemma, ner, depparse, coref"",
                     ""tokenize.language"" : ""en"" 
                   }

return nlp:parse($text, $properties)
</code></pre>

<p>The function needs to be able to respond to a call to kill the running query.  The call is <code>system:kill-running-xquery()</code></p>

<p>Is there a listener or a callback feature in StanfordCoreNLP that will allow for the pipeline to terminate cleanly?</p>
","java, callback, listener, stanford-nlp, exist-db","<p>If you use the Stanford CoreNLP Server then you can send a shutdown command.  In Java code you can make a pipeline that is actually just backed by a server.</p>

<pre><code>// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);
pipeline.shutdown();
</code></pre>

<p>More info here: <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/corenlp-server.html</a></p>

<p>The pipeline class <code>StanfordCoreNLP</code> (as opposed to <code>StanfordCoreNLPClient</code>) doesn't have any ability to be shutdown in this manner.</p>
",2,1,106,2020-02-04 17:59:46,https://stackoverflow.com/questions/60063115/is-there-a-listener-in-the-stanford-corenlp-pipeline-checking-for-abort
i want to use stanford-parser-full-2015-01-30 in java command line,"<p><a href=""https://i.sstatic.net/BbzWY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BbzWY.jpg"" alt=""java -mx100m edu.stanford.nlp.trees.EnglishGrammaticalStructure -sentFile input.txt -collapsedTree -CCprocessed -parseTree -parserFile englishPCFG.ser.gz""></a></p>

<p>when i use this command </p>

<blockquote>
  <p>java -mx100m edu.stanford.nlp.trees.EnglishGrammaticalStructure
  -sentFile input.txt -collapsedTree -CCprocessed -parseTree -parserFile englishPCFG.ser.gz it return this Error: unable to find or load main
  class edu.stanford.nlp.trees.EnglishGrammaticalStructure</p>
</blockquote>

<p>and when i use this command </p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.trees.EnglishGrammaticalStructure
  -sentFile input.txt -collapsedTree -CCprocessed -parseTree -parserFile englishPCFG.ser.gz it retruns</p>
</blockquote>

<pre><code>Loading parser from serialized file englishPCFG.ser.gz ...
java.io.IOException: Unable to resolve ""englishPCFG.ser.gz"" as either class path, filename or URL
        at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:463)
        at edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:396)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:599)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:394)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:181)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at edu.stanford.nlp.trees.GrammaticalStructure.loadParser(GrammaticalStructure.java:1394)
        at edu.stanford.nlp.trees.GrammaticalStructure.main(GrammaticalStructure.java:1645)
Loading parser from text file englishPCFG.ser.gz java.io.IOException: Unable to resolve ""englishPCFG.ser.gz"" as either class path, filename or URL
        at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:463)
        at edu.stanford.nlp.io.IOUtils.readerFromString(IOUtils.java:591)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTextFile(LexicalizedParser.java:533)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:396)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:181)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at edu.stanford.nlp.trees.GrammaticalStructure.loadParser(GrammaticalStructure.java:1394)
        at edu.stanford.nlp.trees.GrammaticalStructure.main(GrammaticalStructure.java:1645)
Exception in thread ""main"" java.lang.RuntimeException: java.lang.NullPointerException
        at edu.stanford.nlp.trees.GrammaticalStructure.main(GrammaticalStructure.java:1655)
Caused by: java.lang.NullPointerException
        at edu.stanford.nlp.trees.GrammaticalStructure.main(GrammaticalStructure.java:1652)
</code></pre>

<p>i don't know where is the problem and how to use this library</p>
","java, nlp, stanford-nlp","<p>You have to supply the resource's full path in the command.</p>

<pre><code>edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz
</code></pre>

<p>When Java tries to find something, it looks for that path in all of the jar files in the CLASSPATH.  Also the <code>-cp ""*""</code> will only work if you are executing the command in the directory with all of the <code>.jar</code> files.</p>
",1,0,103,2020-02-07 11:12:56,https://stackoverflow.com/questions/60112324/i-want-to-use-stanford-parser-full-2015-01-30-in-java-command-line
ClassNotFoundException: edu.stanford.nlp.tagger.maxent.ExtractorNonAlphanumeric,"<p>I try to compile and run a Stanford NLP java example on this page: <a href=""https://stanfordnlp.github.io/CoreNLP/api.html#quickstart-with-convenience-wrappers"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/api.html#quickstart-with-convenience-wrappers</a> (the first example BasicPipelineExample)</p>

<p>It is said there that the example is developed for 3.9.0 but I could not get that anywhere, so I'm using 3.9.2.</p>

<p>I run the code under simple scala build tool because the further work will be written in scala.</p>

<p>My build.sbt is here:</p>

<pre><code>name := ""nlpexp""

version := ""1.0""

scalaVersion := ""2.12.10""

resolvers += ""Typesafe Repository"" at ""http://repo.typesafe.com/typesafe/releases/""

val stanford_corenlpV = ""3.9.2""
val AppleJavaExtensionsV = ""1.4""
val jollydayV = ""0.4.9""
val commons_lang3V = ""3.3.1""
val lucene_queryparserV = ""4.10.3""
val lucene_analyzers_commonV = ""4.10.3""
val lucene_queriesV = ""4.10.3""
val lucene_coreV = ""4.10.3""
val javax_servlet_apiV = ""3.0.1""
val xomV = ""1.2.10""
val joda_timeV = ""2.9.4""
val ejmlV = ""0.23""
val javax_jsonV = ""1.0.4""
val slf4j_apiV = ""1.7.12""
val protobuf_javaV = ""3.2.0""
val junitV = ""4.12""
val junit_quickcheck_coreV = ""0.5""
val junit_quickcheck_generatorsV = ""0.5""
val javax_activation_apiV = ""1.2.0""
val jaxb_apiV = ""2.4.0-b180830.0359""
val jaxb_coreV = ""2.3.0.1""
val jaxb_implV = ""2.4.0-b180830.0438""

val logbackVersion = ""1.2.3""

libraryDependencies ++= Seq(
  ""ch.qos.logback"" % ""logback-classic"" % logbackVersion withSources() withJavadoc(), //
  ""ch.qos.logback"" % ""logback-core"" % logbackVersion withSources() withJavadoc(), //
  ""ch.qos.logback"" % ""logback-access"" % logbackVersion withSources() withJavadoc(), //

  ""com.apple"" % ""AppleJavaExtensions"" % AppleJavaExtensionsV withJavadoc(),
  ""de.jollyday"" % ""jollyday"" % jollydayV withSources() withJavadoc(),
  ""org.apache.commons"" % ""commons-lang3"" % commons_lang3V withSources() withJavadoc(),
  ""org.apache.lucene"" % ""lucene-queryparser"" % lucene_queryparserV withSources() withJavadoc(),
  ""org.apache.lucene"" % ""lucene-analyzers-common"" % lucene_analyzers_commonV withSources() withJavadoc(),
  ""org.apache.lucene"" % ""lucene-queries"" % lucene_queriesV withSources() withJavadoc(),
  ""org.apache.lucene"" % ""lucene-core"" % lucene_coreV withSources() withJavadoc(),
  ""javax.servlet"" % ""javax.servlet-api"" % javax_servlet_apiV withSources() withJavadoc(),
  ""com.io7m.xom"" % ""xom"" % xomV withSources() withJavadoc(),
  ""joda-time"" % ""joda-time"" % joda_timeV withSources() withJavadoc(),
  ""com.googlecode.efficient-java-matrix-library"" % ""ejml"" % ejmlV withSources() withJavadoc(),
  ""org.glassfish"" % ""javax.json"" % javax_jsonV withSources() withJavadoc(),
  ""org.slf4j"" % ""slf4j-api"" % slf4j_apiV withSources() withJavadoc(),
  ""com.google.protobuf"" % ""protobuf-java"" % protobuf_javaV withSources() withJavadoc(),
  ""junit"" % ""junit"" % junitV  % ""test"" withSources() withJavadoc(),
  ""com.pholser"" % ""junit-quickcheck-core"" % junit_quickcheck_coreV % ""test"" withSources() withJavadoc(),
  ""com.pholser"" % ""junit-quickcheck-generators"" % junit_quickcheck_generatorsV % ""test"" withSources() withJavadoc(),
  ""javax.activation"" % ""javax.activation-api"" % javax_activation_apiV withSources() withJavadoc(),
  ""javax.xml.bind"" % ""jaxb-api"" % jaxb_apiV withSources() withJavadoc(),
  ""com.sun.xml.bind"" % ""jaxb-core"" % jaxb_coreV withSources() withJavadoc(),
  ""com.sun.xml.bind"" % ""jaxb-impl"" % jaxb_implV withSources() withJavadoc(),
""edu.stanford.nlp"" % ""stanford-corenlp"" % stanford_corenlpV withSources() withJavadoc()
)

scalacOptions += ""-deprecation""
</code></pre>

<p>I downloaded also following jars:</p>

<pre><code>-rw-rw-r-- 1 jk jk  455928708 stanford-corenlp-models-current.jar
-rw-rw-r-- 1 jk jk 1238400073 stanford-english-corenlp-models-current.jar
-rw-rw-r-- 1 jk jk  473999694 stanford-english-kbp-corenlp-models-current.jar
</code></pre>

<p>Which are in the dir of unmanaged libraries so that build can find them.</p>

<p>Example code compiles OK and starts to run but fails when trying to load pos annotator.</p>

<p>Output is this:</p>

<pre><code>[info] Running nlpexp.BasicPipelineExample 
Starting
Create pipeline
23:30:09.569 [run-main-0] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
23:30:09.587 [run-main-0] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
23:30:09.592 [run-main-0] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[error] (run-main-0) edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
[error] edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:925)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:823)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:797)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:320)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:273)
[error]     at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:85)
[error]     at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:73)
[error]     at edu.stanford.nlp.pipeline.AnnotatorImplementations.posTagger(AnnotatorImplementations.java:53)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$3(StanfordCoreNLP.java:521)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
[error]     at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
[error]     at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
[error]     at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:251)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:192)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:188)
[error]     at nlpexp.BasicPipelineExample.main(BasicPipelineExample.java:31)
[error]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[error]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[error]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[error]     at java.lang.reflect.Method.invoke(Method.java:498)
[error] Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.tagger.maxent.ExtractorNonAlphanumeric
[error]     at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
[error]     at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
[error]     at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
[error]     at java.lang.Class.forName0(Native Method)
[error]     at java.lang.Class.forName(Class.java:348)
[error]     at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:720)
[error]     at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1923)
[error]     at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1806)
[error]     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2097)
[error]     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)
[error]     at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)
[error]     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)
[error]     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)
[error]     at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)
[error]     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)
[error]     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)
[error]     at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)
[error]     at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.readExtractors(MaxentTagger.java:628)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:874)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:823)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:797)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:320)
[error]     at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:273)
[error]     at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:85)
[error]     at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:73)
[error]     at edu.stanford.nlp.pipeline.AnnotatorImplementations.posTagger(AnnotatorImplementations.java:53)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$3(StanfordCoreNLP.java:521)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
[error]     at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
[error]     at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
[error]     at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:251)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:192)
[error]     at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:188)
[error]     at nlpexp.BasicPipelineExample.main(BasicPipelineExample.java:31)
[error]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[error]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[error]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[error]     at java.lang.reflect.Method.invoke(Method.java:498)
[error] Nonzero exit code: 1
[error] (Compile / run) Nonzero exit code: 1
[error] Total time: 41 s, completed Feb 22, 2020 11:30:10 PM
sbt:nlpexp&gt; 
</code></pre>

<p>When I remove all but tokenize and ssplit annotators and run the shortened example:</p>

<pre><code>  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = new Properties();
    // set the list of annotators to run
    props.setProperty(""annotators"", ""tokenize,ssplit"");
    // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
    props.setProperty(""coref.algorithm"", ""neural"");
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = new CoreDocument(text);
    // annnotate the document
    pipeline.annotate(document);
    // examples

    // 10th token of the document
    CoreLabel token = document.tokens().get(10);
    System.out.println(""Example: token"");
    System.out.println(token);
    System.out.println();

    // text of the first sentence
    String sentenceText = document.sentences().get(0).text();
    System.out.println(""Example: sentence"");
    System.out.println(sentenceText);
    System.out.println();

    // second sentence
    CoreSentence sentence = document.sentences().get(1);
  }
</code></pre>

<p>output is:</p>

<pre><code>sbt:nlpexp&gt; run
Starting
Create pipeline
23:45:47.255 [run-main-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
23:45:47.271 [run-main-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
Create doc
Annotate doc
Example: token
he-4

Example: sentence
Joe Smith was born in California.
</code></pre>

<p>So it seems that tokenize and ssplit annotators load and run OK, but maybe pos fail to load.</p>

<p>Do I still miss one or more jars or what is the reason for this error here?</p>

<p>Thank you for your support!</p>
","java, scala, nlp, stanford-nlp","<p>I faced the same problem, just figured it out finally.
The models with links on the main repo page at: <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP</a>, e.g., this one: <a href=""http://nlp.stanford.edu/software/stanford-corenlp-models-current.jar"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/stanford-corenlp-models-current.jar</a>,
and the neighboring links are matched with the latest current code (i.e., the HEAD of the Git repo), and not any specific release like 3.9.2. </p>

<p>To get the models for the 3.9.2 version, you have to get it from the corresponding models.jar:</p>

<pre><code>&lt;dependency&gt;
&lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
&lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
&lt;version&gt;3.9.2&lt;/version&gt;
&lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
</code></pre>

<p>If you don't want to include this (really fat) jar in your build, one option is to first build with this dependency included, then find where the jar gets installed in your local ~/.m2 repo (~/.m2/repositories/edu/stanford/nlp/stanford-corenlp/3.9.2), extract (via jar xvf) the models you want from this jar, include the models in your build (e.g., by placing them under main/resources), comment out or remove the above dependency from your pom, and rebuild.</p>
",0,0,669,2020-02-23 07:57:38,https://stackoverflow.com/questions/60360209/classnotfoundexception-edu-stanford-nlp-tagger-maxent-extractornonalphanumeric
Unigram tagging in NLTK,"<p>Using <code>NLTK</code> Unigram Tagger, I am training sentences in <code>Brown Corpus</code></p>

<p>I try different <code>categories</code> and I get about the same value. The value is around <code>0.9328</code>... for each <code>categories</code> such as <code>fiction</code>, <code>romance</code> or <code>humor</code></p>

<pre><code>from nltk.corpus import brown


# Fiction    
brown_tagged_sents = brown.tagged_sents(categories='fiction')
brown_sents = brown.sents(categories='fiction')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown_tagged_sents)
&gt;&gt;&gt; 0.9415956079897209

# Romance
brown_tagged_sents = brown.tagged_sents(categories='romance')
brown_sents = brown.sents(categories='romance')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown_tagged_sents)
&gt;&gt;&gt; 0.9348490474422324
</code></pre>

<p>Why is it that the case? is it because they are from the same <code>corpus</code>? or are their <code>part-of-speech</code> tagging is the same?</p>
","nlp, nltk, stanford-nlp, allennlp","<p>It looks like you are training and then evaluating the trained <code>UnigramTagger</code> on the same training data.  Take a look at the documentation of <a href=""https://www.nltk.org/api/nltk.tag.html"" rel=""nofollow noreferrer"">nltk.tag</a> and specifically the <a href=""https://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.NgramTagger"" rel=""nofollow noreferrer"">part</a> about evaluation.</p>
<p>With your code, you will get a high score which is quite obvious because your training data and evaluation/testing data is the same. If you were to change that where the testing data is different from the training data, you will get different results. My examples are below:</p>
<p><strong>Category: Fiction</strong></p>
<p>Here I have used the training set as <code>brown.tagged_sents(categories='fiction')[:500]</code> and the test/evaluation set as <code>brown.tagged_sents(categories='fiction')[501:600]</code></p>
<pre><code>from nltk.corpus import brown
import nltk

# Fiction    
brown_tagged_sents = brown.tagged_sents(categories='fiction')[:500]
brown_sents = brown.sents(categories='fiction') # not sure what this line is doing here
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown.tagged_sents(categories='fiction')[501:600])
</code></pre>
<p>This gives you a score of ~ 0.7474610697359513</p>
<p><strong>Category: Romance</strong></p>
<p>Here I have used the training set as <code>brown.tagged_sents(categories='romance')[:500]</code> and the test/evaluation set as <code>brown.tagged_sents(categories='romance')[501:600]</code></p>
<pre><code>from nltk.corpus import brown
import nltk

# Romance
brown_tagged_sents = brown.tagged_sents(categories='romance')[:500]
brown_sents = brown.sents(categories='romance') # not sure what this line is doing here
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown.tagged_sents(categories='romance')[501:600])
</code></pre>
<p>This gives you a score of ~ 0.7046799354491662</p>
<p>I hope this helps and answers your question.</p>
",1,1,690,2020-03-03 03:46:55,https://stackoverflow.com/questions/60499791/unigram-tagging-in-nltk
Can Stanford CoreNLP lemmatise a word given a custom POS?,"<p>I would like to lemmatise a given word multiple times, with different POS supplied.</p>

<p>For example, the lemma of ""met"" is ""meet"" (POS: verb), while the lemma of ""meeting"" is ""meeting"" (POS: noun).</p>

<p>But if ""meeting"" is a verb, the lemma is ""meet"". I would like then to lemmatise ""meeting"" with a given verb POS, in an effort to find such similarities.</p>

<p>Is this possible?</p>

<p>Using latest Java CoreNLP 3.9.2</p>
",stanford-nlp,"<p>Try the method <code>String lemma(String word, String tag)</code> in <code>edu.stanford.nlp.process.Morphology</code>.</p>

<pre><code>Morphology morphology = new Morphology();

String word = ""meeting"";
String tag = ""VB"";
String lemma = morphology.lemma(word, tag);
System.out.println(String.format(""%s_%s   %s"", word, tag, lemma));
</code></pre>
",0,0,52,2020-03-03 09:15:15,https://stackoverflow.com/questions/60503776/can-stanford-corenlp-lemmatise-a-word-given-a-custom-pos
AttributeError: module &#39;sst&#39; has no attribute &#39;train_reader&#39;,"<p>I am very new to sentiment analysis. Trying to use Stanford Sentiment Treebank(sst) and ran into an error.</p>

<pre class=""lang-py prettyprint-override""><code>from nltk.tree import Tree
import os
import sst
trees = ""C:\\Users\m\data\trees""
tree, score = next(sst.train_reader(trees))
</code></pre>

<p>[Output]:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-19-4101f90b0b16&gt; in &lt;module&gt;()
----&gt; 1 tree, score = next(sst.train_reader(trees))

AttributeError: module 'sst' has no attribute 'train_reader'
</code></pre>
","python, nlp, stanford-nlp, sentiment-analysis, sst","<p>I think you're looking for <a href=""https://github.com/JonathanRaiman/pytreebank"" rel=""nofollow noreferrer"">https://github.com/JonathanRaiman/pytreebank</a>, not <a href=""https://pypi.org/project/sst/"" rel=""nofollow noreferrer"">https://pypi.org/project/sst/</a>.  </p>

<p>On the python side, that error is pretty clear. Once you import the right package, though, I'm not sure I saw <code>train_reader</code> but I could be wrong.</p>

<p>UPDATE:
I'm not entirely sure why you're running into the 'sst' not having the attribute train_reader. Make sure you didn't accidentally install the 'sst' package if you're using conda. It looks like the 'sst' is referring to a privately created module and that one <strong>should work.</strong></p>

<p>I got your import working but what I did was I:</p>

<ol>
<li>Installed everything specified in the <code>requirements.txt</code> file.</li>
<li><code>import sst</code> was still giving me an error so I installed nltk and sklearn to resolve that issue. (fyi, im not using conda. im just using pip and virtualenv for my own private package settings. i ran <code>pip install nltk</code> and <code>pip install sklearn</code>)</li>
<li>At this point, <code>import sst</code> worked for me. </li>
</ol>
",1,0,280,2020-03-03 16:33:34,https://stackoverflow.com/questions/60511708/attributeerror-module-sst-has-no-attribute-train-reader
How to get the index of value with list comprehension,"<p>I am working in Brown Corpus using NLTK. I want to separate out the <code>tokens</code> that has <code>tokens</code> tagged with <code>DT</code></p>

<p>My code:</p>

<pre><code>import nltk
from nltk.corpus import brown
brown_tag = brown.tagged_words()
brownDT = [(a,b) for (a,b) in brown_tag if b == 'DT']
</code></pre>

<p>The above code returns the <code>value</code> tagged with <code>DT</code> but I need the <code>index</code> too. I am trying to get the <code>value</code> and the <code>index</code> of the <code>value</code> in return. For example the output should be:</p>

<pre><code>[index, (token, 'DT')]
</code></pre>

<p>This code does not work:</p>

<pre><code>brownDT = [((a,b),brown_tag.index((a,b))) for (a,b) in brown_tag if b == 'DT']
</code></pre>
","python, nlp, nltk, stanford-nlp, rasa-nlu","<pre><code>brownDT = [(i,(a,b)) for (i, (a,b)) in enumerate(brown_tag) if b == 'DT']
</code></pre>
",1,0,39,2020-03-07 21:12:20,https://stackoverflow.com/questions/60582239/how-to-get-the-index-of-value-with-list-comprehension
How to run Stanford CoreNLP for lemmatization on Google Colab?,"<p>There is a similar question made, however google colab change a lot since that time, I was wondering how to use Stanford CoreNLP on Google Colab, specially for lemmatization.</p>

<p><strong>Expected answer:</strong></p>

<ul>
<li>import the module</li>
<li>lemmatize with a sample code</li>
</ul>

<p><strong>Using the code:</strong></p>

<pre class=""lang-py prettyprint-override""><code>!pip install stanfordnlp
import stanfordnlp
stanfordnlp.download(""es"")
nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')
doc = nlp(""Barack Obama was born in Hawaii."")
print(*[f'word: {word.text+"" ""}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')

%tb

------------
Loading: tokenize
With settings: 
{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
Cannot load model from /root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt
An exception has occurred, use %tb to see the full traceback.

SystemExit: 1

/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
</code></pre>

<p><em>any advice to improve the question will be considered</em></p>
","python, stanford-nlp, google-colaboratory","<p>Maybe it's better to use the new <code>StanfordNLP</code> instead of their old <code>CoreNLP</code>.</p>

<pre class=""lang-py prettyprint-override""><code>!pip install stanfordnlp
import stanfordnlp
stanfordnlp.download(""en"")
nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')
doc = nlp(""Barack Obama was born in Hawaii."")
print(*[f'word: {word.text+"" ""}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')
</code></pre>

<p>You will get this output</p>

<pre><code>word: Barack    lemma: Barack
word: Obama     lemma: Obama
word: was   lemma: be
word: born  lemma: bear
word: in    lemma: in
word: Hawaii    lemma: Hawaii
word: .     lemma: .
</code></pre>

<p>Here's an <a href=""https://colab.research.google.com/drive/1ArL5x_XTCfRr6tSG3A-SodQUoZqp1n8e"" rel=""nofollow noreferrer"">example notebook</a>.</p>
",2,0,1688,2020-03-09 03:44:20,https://stackoverflow.com/questions/60594461/how-to-run-stanford-corenlp-for-lemmatization-on-google-colab
Containerization of a python code with stanfordnlp that uses gpu,"<p>I would like to docker containerize my python script that uses gpu. The reason of containerization is that i am trying to run this code in a server that has python3.5 installed but stanfordnlp needs python3.6+. </p>

<p>So my approach is to create a container that uses the latest python image, run the python script that will use the nvidia gpu.</p>

<p>Below is a snippet of the python code where it imports the stanfordnlp for lemmatization technique</p>

<pre><code>import stanfordnlp
import pandas as pd
import string

stanfordnlp.download('en')
nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,lemma,pos', use_gpu=True)
</code></pre>

<p>As you can see I have explicitly typed use_gpu=True although that is True by default according to this <a href=""http://%20https://stanfordnlp.github.io/stanfordnlp/pipeline.html#usage"" rel=""nofollow noreferrer"">link</a></p>

<p>Next I have written a Dockerfile</p>

<pre><code>FROM python:3
WORKDIR /usr/src/app
COPY lemmatizer.py ./
COPY eng_wikipedia_2016_1M-sentences.txt ./
RUN pip install stanfordnlp pandas
CMD [ ""python"" , ""./lemmatizer.py"" ]
</code></pre>

<p>When I build and then run the Dockerfile using the following commands according to the guides in this <a href=""https://github.com/NVIDIA/nvidia-docker"" rel=""nofollow noreferrer"">link</a> to use the gpu services when creating and deploying the container.</p>

<pre><code>nvidia-docker build -t pylemm-gpu .
docker run -it --gpus all pylemm-gpu
</code></pre>

<p>After performing all the steps discussed above, I am still getting the message that my container is using cpu instead of gpu. I would really appreciate a guide and/or links to setup my container properly so that it can use the gpu.</p>

<p>p.s. I have tried the Usage part in the <a href=""https://github.com/NVIDIA/nvidia-docker"" rel=""nofollow noreferrer"">link</a> and checked that my server has 3 gpus.</p>
","python, docker, stanford-nlp","<p>Cuda-drivers were installed as it was showing with the command</p>

<pre><code>nvidia-smi
</code></pre>

<p>stanfordnlp uses pytorch therefore I pulled pytorch docker image and ran interactively. I imported torch and checked whether cuda was installed using </p>

<pre><code>torch.cuda.is_available()
</code></pre>

<p>which was returning False.</p>

<p>Using the command below</p>

<pre><code>torch.version.cuda
</code></pre>

<p>I checked the version of Cuda used in that image</p>

<p>Then I understood that the latest version of Cuda was being used in pytorch/pytorch:latest according to the <a href=""https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver"" rel=""nofollow noreferrer"">link</a>. In my server it was using Cuda version 10.0, therefore I searched for an older version of pytorch using Cuda version 10.0 (matches with my Cuda version) from this <a href=""https://hub.docker.com/r/pytorch/pytorch/tags"" rel=""nofollow noreferrer"">link</a>. </p>

<p>This solved my problem, stanfordnlp and stanza are both using GPU now.</p>
",0,0,468,2020-03-17 08:17:47,https://stackoverflow.com/questions/60718574/containerization-of-a-python-code-with-stanfordnlp-that-uses-gpu
Python glove missing module &#39;glove&#39; &#39;Glove&#39;,"<p>Here is what I performed:</p>

<p>Installed pip3 install glove_py ok.
In Jupyter Python, import glove works ok.</p>

<pre><code>from glove import *
</code></pre>

<p>Problem:</p>

<p>When I try a basic setup code to ensure all the modules are loaded and working. I have this code, which errors on message: ""NameError: name 'glove' is not defined"". Now since module glove import works ok, I have tried function 'glove' and 'Glove', both with NameError not defined. </p>

<p>I did find libraries like 'git clone <a href=""http://github.com/stanfordnlp/glove"" rel=""nofollow noreferrer"">http://github.com/stanfordnlp/glove</a>' and did download and build the code with make. This code ran ok in the console for a sample. </p>

<pre><code>pip3 install glove_py
</code></pre>

<p>Pip install for glove_py installed ok. </p>

<pre><code>pip3 install glove_python
</code></pre>

<p>But pip install for glove_python failed to install with ""Error Command errored out with exit status 1:"". </p>

<pre><code>glove &amp;&amp; make
mkdir -p build
</code></pre>

<p>glove 'git clone <a href=""http://github.com/stanfordnlp/glove"" rel=""nofollow noreferrer"">http://github.com/stanfordnlp/glove</a>' download ok and build with make ok. But even with this make'd version, I was not able to get the Python import glove to find this c code make realized inside the Jupyter Python environment.</p>

<p>I suspect that I am missing something simple, I would appreciate any insight.</p>

<p>Python code, test run. Here is my Python code test run which failed on module not found.</p>

<pre><code>model = glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)
model.train(df)
model.to_txt()
words = model.most_similary(""one"", 10)
</code></pre>

<pre><code>NameError                                 Traceback (most recent call last)
&lt;ipython-input-11-517b339bba36&gt; in &lt;module&gt;
----&gt; 1 model = glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)
      2 model.train(df)
      3 model.to_txt()
      4 words = model.most_similary(""one"", 10)
      5 print(words)

NameError: name 'glove' is not defined
</code></pre>

<p>Directory function to see the functions inside 'gl' module, imported from glove package, no module function names showed. So this clearly shows that the import of glove as gl, had some issues.</p>

<pre><code>dir(gl)
</code></pre>

<pre><code>['__doc__',
 '__file__',
 '__loader__',
 '__name__',
 '__package__',
 '__path__',
 '__spec__']
</code></pre>
","python-3.x, nlp, text-mining, glove","<p>What you want is the <code>Glove</code> class inside the module; note the capital letter.</p>

<p>I think this line</p>

<p><code>glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)</code></p>

<p>should be</p>

<p><code>Glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)</code></p>
",0,0,2942,2020-03-21 22:51:53,https://stackoverflow.com/questions/60794145/python-glove-missing-module-glove-glove
How can I output left-to-right NLP dependency graphs?,"<p>How do I output a diagram similar to this from depparse output (in JSON or CONLLU)?</p>

<p>Is there a specific tool or specific settings a template in Graphviz?</p>

<p><a href=""https://i.sstatic.net/bVZzY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bVZzY.png"" alt=""enter image description here""></a></p>

<p>I can output something like this in Graphviz (not the same sentence):</p>

<p><a href=""https://i.sstatic.net/fkUYF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fkUYF.png"" alt=""enter image description here""></a></p>

<p>but the flat, left-to-right ordered view is much more readable.</p>
","nlp, stanford-nlp, graphviz","<p>Look at the <strong>rank</strong> &amp; <strong>rankdir</strong> attributes.<br>
This input:</p>

<pre><code>digraph {
 {
   rank=same
   rankdir=LR
   a-&gt;b
   a-&gt;c
   a-&gt;d
   b-&gt;c
   b-&gt;e
   f-&gt;g
 }
}
</code></pre>

<p>Produces this:
<a href=""https://i.sstatic.net/VIgCE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VIgCE.png"" alt=""enter image description here""></a></p>
",0,0,75,2020-03-23 09:03:12,https://stackoverflow.com/questions/60810489/how-can-i-output-left-to-right-nlp-dependency-graphs
Stanford Java NLP Constituency labels abbreviations,"<p>Using the Stanford Java CoreNLP library, I have this:</p>

<pre><code>            String text = ""My name is Anthony"";
            CoreDocument doc = new CoreDocument(text);
            pipeline.annotate(doc);
            for(Tree t : doc.sentences().get(0).constituencyParse()) {
                String tmp = """";
                for(Word w : t.yieldWords()) {
                    tmp = tmp + "" "" + w.word();
                }
                System.out.println(t.label().toString() + "" - "" + WordParts.getValue(t.label().toString()) + "" - "" + tmp);
</code></pre>

<p>Right now, the program outputs this:</p>

<pre><code>ROOT - INVALID -  My name is Anthony
S - INVALID -  My name is Anthony
NP - INVALID -  My name
PRP$ - Possessive pronoun -  My
My-1 - INVALID -  My
NN - Singular noun -  name
name-2 - INVALID -  name
VP - INVALID -  is Anthony
VBZ - 3rd person singular present verb -  is
Subject:  Anthony
is-3 - INVALID -  is
NP - INVALID -  Anthony
NNP - Proper singular noun -  Anthony
Anthony-4 - INVALID -  Anthony
</code></pre>

<p>The <code>WordParts.java</code> abbreviations come from this post (<a href=""https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels"">Java Stanford NLP: Part of Speech labels?</a>) and the class file can be found here: (<a href=""https://github.com/AJ4real/References/blob/master/WordParts.java"" rel=""nofollow noreferrer"">https://github.com/AJ4real/References/blob/master/WordParts.java</a>)
I know that the labels are not <code>Parts of Speech</code> because some of the values return <code>INVALID</code>, so how can I find the full terms for the abbreviations that come from <code>t.label().toString()</code>?</p>
","java, nlp, stanford-nlp","<p>The rest are Penn Treebank phrase categories. E.g., see here:</p>

<p><a href=""https://gist.github.com/nlothian/9240750"" rel=""nofollow noreferrer"">https://gist.github.com/nlothian/9240750</a></p>
",0,0,139,2020-03-28 13:06:54,https://stackoverflow.com/questions/60901549/stanford-java-nlp-constituency-labels-abbreviations
How to solve &#39;lengths&#39; argument should be a 1D CPU int64?,"<p>I'm working on multi-text-classification LSTM model, but in the preprocessing of the text I'm getting an error that before I wasn't getting. I think it was an update on: <code>standfordnlp</code>.</p>

<hr>

<p><strong>Code</strong> were I'm getting the error:</p>

<p><strong>modules:</strong></p>

<pre class=""lang-py prettyprint-override""><code># StanfordNLP
!pip install stanfordnlp
import stanfordnlp
stanfordnlp.download('es', confirm_if_exists = True, version = 'latest')
stNLP = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', treebank = 'es_ancora', use_gpu=True)

# SpaCy
!spacy download es_core_news_sm # sm md
import spacy
spNLP = spacy.load('es_core_news_sm') #sm md
activated = spacy.prefer_gpu()
spacy.require_gpu()

import pandas as pd
import numpy as np
</code></pre>

<p><strong>Getting rid of the stopwords:</strong></p>

<pre class=""lang-py prettyprint-override""><code>def get_stop_words(): 
  # Getting in a list all the stopwords of the dataframe with is_stop() from SpaCy
  spacy_stop_words = list(dict.fromkeys([str(i) for i in spNLP(' '.join([elem for elem in new_df['descripcion']])) if i.is_stop == True]))

  stop_words = stopwords.words('spanish') # defining the language
  stop_words.extend(spec_stopwords) # extending the specific stopwords
  stop_words.extend(spacy_stop_words) # extending the spacy stopwords
  stop_words = set(stop_words)
  return stop_words

stop_words = get_stop_words() # defining the stop_words set in a variable to better understanding whe applying on the dataframe

# Applying stopwords on the dataframe
new_df['descripcion'] = new_df['descripcion'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
</code></pre>

<p><strong>Lemmatizing:</strong></p>

<pre class=""lang-py prettyprint-override""><code>def stanford_lemma(text):
  doc = stNLP(text)
  return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])

# Lemmatization of dataframe
new_df['descripcion'] = new_df['descripcion'].apply(lambda x: stanford_lemma(x))

# Getting new stop_words after lemmatization
get_stop_words()

# applying new stop_words on the dataframe
new_df['descripcion'] = new_df['descripcion'].apply(lambda x: ' '.join(
    [word for word in x.split() if word not in stop_words]))
</code></pre>

<hr>

<p><strong>Traceback</strong>:</p>

<pre class=""lang-py prettyprint-override""><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-18-60972fc225b2&gt; in &lt;module&gt;()
----&gt; 1 new_df['descripcion'] = new_df['descripcion'].apply(lambda x: stanford_lemma(x))
      2 
      3 # Getting new stop_words after lemmatization (Lemmatizing: personalidades, personlidad = stopword)
      4 get_stop_words()
      5 

9 frames
pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()

/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py in pack_padded_sequence(input, lengths, batch_first, enforce_sorted)
    231 
    232     data, batch_sizes = \
--&gt; 233         _VF._pack_padded_sequence(input, lengths, batch_first)
    234     return PackedSequence(data, batch_sizes, sorted_indices, None)
    235 

RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor
</code></pre>

<hr>

<p><em>Update:</em> using the new library <code>Stanza</code>, I'm getting the same issue. An the issue will persist even if I try the <em>Lemma Example Usage</em>:</p>

<pre class=""lang-py prettyprint-override""><code>!pip install stanza
import stanza

stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=True)
stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma',
                        lang='es',
                        use_gpu=True)

doc = nlp('Barack Obama nació en Hawaii.')
print(*[f'word: {word.text+"" ""}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')
</code></pre>

<hr>

<p><em>Requests:</em>
<strong>Dataset (<code>@Crystina</code>)</strong>: <a href=""https://github.com/GUNTERMAXIMUS/mbti/blob/master/mbti_new.csv"" rel=""nofollow noreferrer"">new_df</a></p>

<hr>

<p><em>Any suggetion to improve the question, will be considered.</em></p>

<hr>
","python-3.x, tensorflow, stanford-nlp","<p>It will only work if you use Stanza without loading spacy.
This is the error raised when you load <code>spacy.prefer_gpu()</code>
So don't load both the libraries in single GPU processing. Use them seperately.</p>
",0,0,2081,2020-04-01 19:37:53,https://stackoverflow.com/questions/60978997/how-to-solve-lengths-argument-should-be-a-1d-cpu-int64
NLP - linguistic consistency analysis,"<p>I hope you can help me :).</p>

<p>I am working for a translation company. </p>

<p>As you know, every translation consists in splitting the original text into small segments and then re-joining them into the final product.</p>

<p>In other words, the segments are considered as ""<strong>translation units</strong>"".</p>

<p>Often, especially for large documents, the translators make some linguistic consistency errors, I try to explain it with an example.</p>

<p>In Spanish, you can use ""tu"" or ""usted"", depending on the context, and this determines the formality-informality tone of the sentence.</p>

<p>So, if you consider these two sentences of a document:</p>

<pre><code>Lara, te has lavado las manos? (TU)

Lara usted se lavò las manos? (USTED)
</code></pre>

<p>They are BOTH correct, but if you consider the whole document, there is a linguistic inconsistency. </p>

<p>I am studying NLP basic in my spare time, and I am figuring out how to create a tool to perform a linguistic consistency analysis on a set of sentences.  </p>

<p>I am looking in particular at <strong>Standford CoreNLP</strong> (I prefer Java to Python). 
I guess that I need some linguistic tools to perform verb analysis first of all. And naturally, the tool would be able to work with different languages (EN, IT, ES, FR, PT).</p>

<p>Anyone can help me to figure out how to start this? </p>

<p>Any help would be appreciated,</p>

<p>thanks in advance!</p>
","nlp, stanford-nlp, data-analysis, linguistics","<p>Im not sure about Stanford CoreNLP, but if you're considering this an option, <strong>you could make your own tagger and use modifiers at pos tagging</strong>. Then, use this as a translation feature. </p>

<p>In other words, instead of just tagging a word to be a verb, you could tag it ""a verb in the infinitive second person"".</p>

<p>There are already <strong>good pre-tagged corpora out there for spanish</strong> that can help you do exactly that. For example, if you look at <a href=""https://universaldependencies.org/treebanks/es_ancora/"" rel=""nofollow noreferrer"">Universal Dependencies Ankora Corpus</a>, you can find that <a href=""https://universaldependencies.org/treebanks/es_ancora/es_ancora-feat-Person.html"" rel=""nofollow noreferrer"">there are annotations referring to the Person of a verb</a>. </p>

<p>With a little tweaking, you could make a compose PoS that takes in ""Verb-1st-Person"" or something like that and <strong>train a Tagger</strong>.</p>

<p>I've made an article about how to do it in Python, but I bet that you can do it in Java using Weka. <a href=""https://medium.com/analytics-vidhya/part-of-speech-tagging-what-when-why-and-how-9d250e634df6"" rel=""nofollow noreferrer"">You can read the article here</a>.</p>

<p>After this, I guess that the next step is that you ensure to match the person of one ""translation unit"" to the other, or make something in a pipeline fashion.</p>
",2,0,372,2020-04-03 10:14:32,https://stackoverflow.com/questions/61009579/nlp-linguistic-consistency-analysis
How to Extract subject Verb Object using NLP Java? for every sentence,"<p>I want to find a <strong>subject, verb, and object</strong> for each sentence and then it will be passed to natural language generation library <strong>simpleNLG</strong> to form a sentence.</p>

<p>I tried multiple libraries like <strong>Cornlp, opennlp, Standford parsers</strong>. But I can not find them accurately.</p>

<p>Now in the worst case, I will have to write a long set of if-else to find subject, verb, and object form each sentence which is not always accurate for simpleNLG</p>

<p>like, </p>

<ul>
<li>NN, nsub etc goes to subject, VB, VBZ goes to verb.</li>
</ul>

<p><strong>I tried lexical parser</strong>,</p>

<pre><code>LexicalizedParser lp = **new LexicalizedParser(""englishPCFG.ser.gz"");**
String[] sent = { ""This"", ""is"", ""an"", ""easy"", ""sentence"", ""."" };
Tree parse = (Tree) lp.apply(Arrays.asList(sent));
parse.pennPrint();
System.out.println();
TreePrint tp = new TreePrint(""penn,typedDependenciesCollapsed"");
tp.print(parse);
</code></pre>

<p><strong>which gives this output,</strong></p>

<pre><code>nsubj(use-2, I-1)
root(ROOT-0, use-2)
det(parser-4, a-3)
dobj(use-2, parser-4)
</code></pre>

<p><strong>And I want something like this</strong></p>

<pre><code>subject = I
verb = use
det = a
object = parser
</code></pre>

<p><strong><em>Is there a simpler way to find this in JAVA or should I go with if-else? please help me with it.</em></strong></p>
","stanford-nlp, opennlp, pos-tagger, part-of-speech, simplenlg","<p>You can use the <code>openie</code> annotator to get triples.  You can run this at the command line or build a pipeline with these annotators.</p>

<p>command:</p>

<pre><code>java -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,natlog,openie -file example.txt
</code></pre>

<p>Java:</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse,natlog,openie"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation result = pipeline.process(""...""); 
</code></pre>

<p>input:</p>

<pre><code>Joe ate some pizza.
</code></pre>

<p>output:</p>

<pre><code>Extracted the following Open IE triples:
1.0     Joe     ate     pizza
</code></pre>

<p>More details here: <a href=""https://stanfordnlp.github.io/CoreNLP/openie.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/openie.html</a></p>
",1,1,1292,2020-04-09 12:32:15,https://stackoverflow.com/questions/61121239/how-to-extract-subject-verb-object-using-nlp-java-for-every-sentence
CoreNLP Sentiment Analysis Python Loop through dataframe,"<p>How can I make this code loop(run) through all the sentences in my dataframe?</p>

<pre><code>def get_sentiment(review):
    for text in review:
        senti = nlp.annotate(text,
                       properties={
                           'annotators': 'sentiment',
                           'outputFormat': 'json',
                           'timeout': 40000,
                       })

    #for i in senti[""sentences""]:
        return (""{}: '{}': {} (Sentiment Value) {} (Sentiment)"".format(
        s[""index""],
        "" "".join([t[""word""] for t in s[""tokens""]]),
        s[""sentimentValue""], s[""sentiment""]))
</code></pre>

<p>The above when executed returns only the first row sentence: Below...</p>

<pre><code>""0: 'you can see everything from thousands of years in human history it was an unforgettable and wonderful trip to paris for me': 3 (Sentiment Value) Positive (Sentiment)""
</code></pre>

<p>I have tried several variations for the get_sentiment function but the best result I get is the one shown. </p>

<p>My dataframe is called 'reviews' and has one column (Review). This is the content:</p>

<pre><code>                                                                                                 Review
0   you can see everything from thousands of years in human history it was an unforgettable and wonderful trip to paris for me
1   buy your tickets in advance and consider investing in one of many reputable tour guides that you can find online for at least part of your visit to the louvre these 2 arrangements will absolutely maximize your time and enjoyment of th...
2   quite an interesting place and a must see for art lovers the museum is larger than i expected and has so many exhibition areas that a full day trip might be needed if one wants to visit the whole place
3   simply incredible do not forget to get a three day pass if you love architecture art and history it is a must
4   we got here about 45 minutes before opening time and we were very first in line to get into the museum make sure to buy tickets ahead of time to help get in faster this museum is massive and can easily take your entire day an incredi...
</code></pre>
","python-3.x, function, jupyter-notebook, stanford-nlp, pycorenlp","<p>Define your method <code>get_sentiment</code> as the following:</p>

<pre><code>def get_sentiment(row):

    s = nlp.annotate(
        row.Review,
        properties={
            ""annotators"": ""sentiment"",
            ""outputFormat"": ""json"",
            ""timeout"": 40000,
        },
    )

    print(
        ""{}: '{}': {} (Sentiment Value) {} (Sentiment)"".format(
            row.index.iloc[0],
            "" "".join([t[""word""] for t in s[""tokens""]]),
            s[""sentimentValue""],
            s[""sentiment""],
        )
    )
</code></pre>

<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer"">pandas.DataFrame.apply()</a> and run:</p>

<pre><code>&gt;&gt;&gt; reviews.apply(get_sentiment, axis=1)
</code></pre>
",0,0,478,2020-04-09 16:57:56,https://stackoverflow.com/questions/61126316/corenlp-sentiment-analysis-python-loop-through-dataframe
"tf.matmul(X,weight) vs tf.matmul(X,tf.traspose(weight)) in tensorflow","<p>In standard ANN for fully connected layers we are using the following formula: <code>tf.matmul(X,weight) + bias</code>. Which is clear to me, as we use matrix multiplication in order to connect input with th hidden layer.</p>

<p>But in GloVe implementation(<a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a>) we are using the following formula for embeddings multiplication: <code>tf.matmul(W, tf.transpose(U))</code> what confuses me is <code>tf.transpose(U)</code>part.
Why do we use <code>tf.matmul(W, tf.transpose(U))</code> instead of <code>tf.matmul(W, U)</code>?</p>
","python, tensorflow, deep-learning, neural-network, glove","<p>It has to do with the choice of column vs row orientation for the vectors.</p>

<p>Note that <code>weight</code> is the second parameter here:</p>

<pre><code>tf.matmul(X, weight)
</code></pre>

<p>But the first parameter, <code>W</code>, here:</p>

<pre><code>tf.matmul(W, tf.transpose(U))
</code></pre>

<p>So what you are seeing is a practical application of the following matrix transpose identity:</p>

<p><a href=""https://i.sstatic.net/DvLri.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DvLri.png"" alt=""matrix multiplication transpose identity""></a></p>

<hr>

<p>To bring it back to your example, let's assume 10 inputs and 20 outputs.</p>

<p>The first approach uses row vectors. A single input <code>X</code> would be a <code>1x10</code> matrix, called a row vector because it has a single row. To match, the <code>weight</code> matrix needs to be <code>10x20</code> to produce an output of size <code>20</code>.</p>

<p>But in the second approach the multiplication is reversed. That is a hint that everything is using column vectors. If the multiplication is reversed, then everything gets a transpose. So this example is using column vectors, so named because they have a single column.</p>

<p>That's why the transpose is there. The way they GLoVe authors have done their notation, with the multiplication reversed, the weight matrix <code>W</code> must already be transposed to <code>20x10</code> instead of <code>10x20</code>. And they must be expecting a <code>20x1</code> column vector for the output.</p>

<p>So if the input vector <code>U</code> is naturally a <code>1x10</code> row vector, it also has to be transposed, to a <code>10x1</code> column vector, to fit in with everything else.</p>

<hr>

<p>Basically you should pick row vectors or column vectors, all the time, and then the order of multiplications and the transposition of the weights is determined for you.</p>

<p>Personally I think that column vectors, as used by GloVe, are awkward and unnatural compared to row vectors. It's better to have the multiplication ordering follow the data flow ordering.</p>
",2,0,613,2020-04-19 20:53:42,https://stackoverflow.com/questions/61311505/tf-matmulx-weight-vs-tf-matmulx-tf-trasposeweight-in-tensorflow
NLTK unable to find java.exe (spontaneous path reduction),"<p>Similar questions were posted <a href=""https://stackoverflow.com/questions/52866988/python-nltk-stanford-ner-tagger-error-message-nltk-was-unable-to-find-the-java"">here</a> and <a href=""https://stackoverflow.com/questions/46202097/nltk-was-unable-to-find-the-java-file-for-stanford-pos-tagger"">here</a>, and my question is actually based on what was suggested in answers to those questions.</p>

<p>I try to parse some German texts using Stanford Parser and NLTK. </p>

<pre><code>from nltk.parse.stanford import StanfordParser
import os
os.environ['STANFORD_PARSER'] ='C:\PretestKorpus\stanford-parser-full-2018-10-17'
os.environ['STANFORD_MODELS'] = 'C:\PretestKorpus\stanford-parser-full-2018-10-17'
parser=StanfordParser(model_path=""C:\PretestKorpus\germanPCFG.ser.gz"")
new=list(parser.raw_parse(""Es war einmal ein Bauer""))
</code></pre>

<p>Then, of course, I get <code>NLTK was unable to find the java file!</code> error:</p>

<p>So I set configurations like this:</p>

<pre><code>nltk.internals.config_java('C:\Program Files (x86)\Java\jre1.8.0_251\bin\java.exe')
</code></pre>

<p>but it returns</p>

<pre><code>NLTK was unable to find the C:\Program Files (x86)\Java\jre1.8.0_251in\java.exe file!
Use software specific configuration paramaters or set the JAVAHOME environment variable.
</code></pre>

<p>So, somehow Python reduces the path <code>\\jre1.8.0_251\bin\java.exe</code> to <code>\\jre1.8.0_251in\java.exe</code></p>

<p>Looks like this:</p>

<p><a href=""https://i.sstatic.net/2A6Ya.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2A6Ya.png"" alt=""enter image description here""></a></p>

<p>Setting environment variable does not help either (returns <code>NLTK was unable to find the java file!</code>error). Obviously, python does not read the path correctly. But for what reason and how to fix that? Any help will be appreciated.</p>
","java, python, stanford-nlp, file-not-found","<p>In python <code>\b</code> inside a String is resolved to a backspace character. Therefore you see the white <em>BS</em> in the picture, becuase the console tries to represent this special character (BS for backspace).
<br/>
What you need to do is to escape the \ inside your String like so</p>

<pre><code>nltk.internals.config_java('C:\\Program Files (x86)\\Java\\jre1.8.0_251\\bin\\java.exe')
</code></pre>

<p>It is a good practice to alway escape all backslash characters, so you can be sure that problems like this one never occur.</p>
",2,0,126,2020-04-22 08:32:39,https://stackoverflow.com/questions/61360645/nltk-unable-to-find-java-exe-spontaneous-path-reduction
What is the difference between parsing and Part Of Speech Tagging?,"<p>I know that POS tagging labels each and every word in a sentence with its appropriate Part Of Speech , But isn't that what a Parser does too ? i.e, break a sentence into its component parts? 
I've looked this up on the internet but couldn't find any satisfactory explanation . 
Please clear my doubt.
Thanks in advance </p>
","parsing, nlp, stanford-nlp, part-of-speech","<p>They are two distinct procedures:</p>

<ul>
<li><p>POS Tagging: each <em>token</em> gets assigned a label which reflects its word class.</p></li>
<li><p>Parsing: each <em>sentence</em> gets assigned a structure (often a tree) which reflects how its components are related to each other.</p></li>
</ul>

<p>POS Tagging takes a tokenised sequence of words, and returns a list of annotated tokens, where each token has a word class label. This is often disambiguated by looking at the context surrounding the token.</p>

<p>There is also <a href=""https://en.wikipedia.org/wiki/Shallow_parsing"" rel=""nofollow noreferrer""><em>chunking</em></a>, which groups tokens into related groups (such as noun phrases). Chunks are non-overlapping sequences.</p>

<p>Parsing commonly results in a <a href=""https://en.wikipedia.org/wiki/Parse_tree"" rel=""nofollow noreferrer"">parse tree</a> for a sentence; often there can be many possible trees in case of ambiguous sentences.</p>

<p>POS tagging is usually a preparatory step in parsing, as a parser typically operates on word classes (though there are some parsing algorithms that work with tokens directly, or a mixture of tags and tokens).</p>
",4,5,1556,2020-04-26 18:38:23,https://stackoverflow.com/questions/61446106/what-is-the-difference-between-parsing-and-part-of-speech-tagging
Stanford NLP core 4.0.0 no longer splitting verbs and pronouns in Spanish,"<p>Very helpfully Stanford NLP core 3.9.2 used to split rolled together Spanish verbs and pronouns</p>

<p><a href=""https://i.sstatic.net/NYaSU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NYaSU.png"" alt=""enter image description here""></a></p>

<p>This is the 4.0.0 output:</p>

<p><a href=""https://i.sstatic.net/gE2q2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gE2q2.png"" alt=""enter image description here""></a></p>

<p>The previous version had more .tagger files.  These have not been included with the 4.0.0 distribution.</p>

<p><a href=""https://i.sstatic.net/3WYeL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3WYeL.png"" alt=""enter image description here""></a></p>

<p>Is that the cause.  Will be they added back?</p>
","windows, stanford-nlp","<p>There are some documentation updates that still need to be made for Stanford CoreNLP 4.0.0.</p>

<p>A major change is that a new multi-word-token annotator has been added, that makes tokenization conform with the UD standard. So the new default Spanish pipeline should run <code>tokenize,ssplit,mwt,pos,depparse,ner</code>.  It may not be possible to run such a pipeline from  the server demo at this time, as some modifications will need to be made.  I can try to send you what such modifications would be soon.  We will try to make a new release in early summer to handle issues like this that we missed.</p>

<p>It won't split the word in your example unfortunately, but I think in many cases it will do the correct thing. The Spanish <code>mwt</code> model is just based off of a large dictionary of terms, and was tuned to optimize performance on the Spanish training data.</p>
",0,0,92,2020-05-01 10:31:25,https://stackoverflow.com/questions/61540771/stanford-nlp-core-4-0-0-no-longer-splitting-verbs-and-pronouns-in-spanish
How to find all Wikipedia pages related to a named entity?,"<p>Given a text, I am looking to find links to all Wikipedia pages related to named entities mentioned in the text. Is there a reliable way to do this?  </p>

<p>For example, consider the text,</p>

<blockquote>
  <p>Mark Elliot Zuckerberg is an American internet entrepreneur and
  philanthropist.</p>
</blockquote>

<p>"" Given this, I am looking at output with the following links:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Mark_Zuckerberg"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Mark_Zuckerberg</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Americans"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Americans</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Internet"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Internet</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Entrepreneurship"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Entrepreneurship</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Philanthropy"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Philanthropy</a></li>
</ul>

<p>Is this possible at all given the current state of NLP? 
Many thanks!</p>
","nlp, mediawiki, stanford-nlp, wikipedia, named-entity-recognition","<p>The problem you are trying to solve is called <a href=""https://en.wikipedia.org/wiki/Entity_linking"" rel=""nofollow noreferrer"">Entity Linking</a>. There are many academic papers discussing solutions to this problem, but only few of them provide an implementation.</p>

<p><a href=""https://arxiv.org/abs/1904.09131"" rel=""nofollow noreferrer"">OpenTapioka</a> from Oxford has an <a href=""https://github.com/wetneb/opentapioca"" rel=""nofollow noreferrer"">open source implementation</a> and an <a href=""https://opentapioca.org"" rel=""nofollow noreferrer"">online demo</a>.</p>

<p><a href=""https://arxiv.org/pdf/1804.03580.pdf"" rel=""nofollow noreferrer"">SWAT</a> from the University of Pisa has a  <a href=""https://sobigdata.d4science.org/web/tagme/swat-api"" rel=""nofollow noreferrer"">publically available API</a>.</p>
",3,1,1361,2020-05-04 20:17:56,https://stackoverflow.com/questions/61600865/how-to-find-all-wikipedia-pages-related-to-a-named-entity
Extract Noun Phrases with Stanza and CoreNLPClient,"<p>I am trying to extract noun phrases from sentences using Stanza(with Stanford CoreNLP). This can only be done with the CoreNLPClient module in Stanza. </p>

<pre><code># Import client module
from stanza.server import CoreNLPClient
# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001
client = CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse'], memory='4G', endpoint='http://localhost:9001')
</code></pre>

<p>Here is an example of a sentence, and I am using the <code>tregrex</code> function in client to get all the noun phrases. <code>Tregex</code> function returns a <code>dict of dicts</code> in python. Thus I needed to process the output of the <code>tregrex</code> before passing it to the <code>Tree.fromstring</code> function in NLTK to correctly extract the Noun phrases as strings. </p>

<pre><code>pattern = 'NP'
text = ""Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.""
matches = client.tregrex(text, pattern) ``
</code></pre>

<p>Hence, I came up with the method <code>stanza_phrases</code> which has to loop through the <code>dict of dicts</code> which is the output of <code>tregrex</code> and correctly format for <code>Tree.fromstring</code> in NLTK.</p>

<pre><code>def stanza_phrases(matches):
  Nps = []
  for match in matches:
    for items in matches['sentences']:
      for keys,values in items.items():
        s = '(ROOT\n'+ values['match']+')'
        Nps.extend(extract_phrase(s, pattern))
  return set(Nps)
</code></pre>

<p>generates a tree to be used by NLTK </p>

<pre><code>from nltk.tree import Tree
def extract_phrase(tree_str, label):
    phrases = []
    trees = Tree.fromstring(tree_str)
    for tree in trees:
        for subtree in tree.subtrees():
            if subtree.label() == label:
                t = subtree
                t = ' '.join(t.leaves())
                phrases.append(t)

    return phrases
</code></pre>

<p>Here is my output:</p>

<pre><code>{'Albert Einstein', 'He', 'a German-born theoretical physicist', 'relativity',  'the theory', 'the theory of relativity'}
</code></pre>

<p>Is there a way I can make this more code efficient with less number of lines (especially, <code>stanza_phrases</code> and <code>extract_phrase</code> methods)</p>
","python, nlp, stanford-nlp, stanford-stanza","<pre><code>from stanza.server import CoreNLPClient

# get noun phrases with tregex
def noun_phrases(_client, _text, _annotators=None):
    pattern = 'NP'
    matches = _client.tregex(_text,pattern,annotators=_annotators)
    print(""\n"".join([""\t""+sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence]))

# English example
with CoreNLPClient(timeout=30000, memory='16G') as client:
    englishText = ""Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.""
    print('---')
    print(englishText)
    noun_phrases(client,englishText,_annotators=""tokenize,ssplit,pos,lemma,parse"")

# French example
with CoreNLPClient(properties='french', timeout=30000, memory='16G') as client:
    frenchText = ""Je suis John.""
    print('---')
    print(frenchText)
    noun_phrases(client,frenchText,_annotators=""tokenize,ssplit,mwt,pos,lemma,parse"")
</code></pre>
",5,4,2895,2020-05-06 11:00:29,https://stackoverflow.com/questions/61633485/extract-noun-phrases-with-stanza-and-corenlpclient
What does &#39;theta&#39; mean in a language model?,"<p>I know that if X denotes a text , p(X) denotes the language model of the text. And most often , we use maximum likelihood estimation to estimate the language model. 
But in many cases , I find a parameter $\theta$ used to represent a language model. I don't understand the meaning of this $\theta$ . 
For Example , for a document d in a collection what purpose does $\theta$ serve in ' p(d|$\theta$) ' ? </p>

<p>Does $\theta$ represent a maximum likelihood estimator or a language model ? </p>

<p>Can someone please explain this difference between a language model and $\theta$ in depth ? </p>

<p>Thanks in advance ! </p>
","nlp, stanford-nlp, information-retrieval, n-gram, language-model","<p><code>\theta</code> is a conventional/standard machine learning notation indicating (strictly speaking) a set of parameter (values), often more commonly known as the parameter vector. </p>

<p>The notation <code>P(Y|X;\theta)</code> is to read as the y-values (e.g. MNIST digit labels) are predicted from the x-values (e.g. input images of MNIST digits) with the help of a trained model that is trained on annotated (X,Y) pairs. This model <strong>is parameterized by</strong> <code>\theta</code>. Obviously, if the training algorithm changes, so will the parameter vector <code>\theta</code>.</p>

<p>The structure of these parameter vectors are usually interpreted from the model they are associated with, e.g. for multi-layered neural networks they indicate real-valued vectors initially randomly assigned and then updated by gradient descent at each iteration.</p>

<p>For word generation based language models, they refer to the probability of a word <code>v</code> following a word <code>u</code>, meaning that each element is an entry in a hash-table of the form <code>(u, v) --&gt; count(u.v)/count(u)</code>.
These probabilities are learned from a <strong>training</strong> collection, <code>C</code> of documents, as a result of which they essentially become a <strong>function of the training set</strong>. For a different collection, these probability values will be different.</p>

<p>Hence, the usual convention is to write <code>P(w_n|P_w_{n-1};\theta)</code>, which basically indicates that these word succession probabilities are <strong>parameterized</strong> by <code>\theta</code>.</p>

<p>A similar argument applies for document-level language models in information retrieval, where the weights essentially indicate probabilities of sampling terms from documents.</p>
",2,1,1958,2020-05-09 16:58:19,https://stackoverflow.com/questions/61700506/what-does-theta-mean-in-a-language-model
How to compare cosine similarities across three pretrained models?,"<p>I have two corpora - one with all women leader speeches and the other with men leader speeches. I would like to test the hypothesis that cosine similarity between two words in the one corpus is significantly different than cosine similarity between the same two words in another corpus. Is such a t-test (or equivalent) logical and possible?</p>

<p>Further, if the cosine similarities are different across the two corpora, how could I examine if cosine similarity between the same two words in a third corpus is more similar to the first or the second corpus?</p>
","nlp, gensim, word2vec, word-embedding, glove","<p>It's certainly <em>possible</em>. Whether it's meaningful, given a certain amount of data, is harder to answer. </p>

<p>Note that in separate training sessions, a given word <em>A</em> won't necessarily wind up in the same coordinates, due to inherent randomness used by the algorithm. That's even the case when training on the <em>exact same data</em>. </p>

<p>It's just the case that in general, the distances/directions to other words <em>B</em>, <em>C</em>, etc should be of similar overall usefulness, when there's sufficient data/training and well-chosen parameters. So <em>A</em>, <em>B</em>, <em>C</em>, etc may be in different places, with slightly-different distances/directions – but the relative relationships are still similar, in terms of neighborhoods-of-words, or the <em>(A-B)</em> direction still be predictive of certain human-perceptible meaning-differences if applied to other words <em>C</em> etc. </p>

<p>So, you should avoid making direct cosine-similarity comparisons between words from different training-runs or corpuses, but you may find meaning in differences in similarities ( <em>A-B</em> vs <em>A' - B'</em> ) or top-N lists or relative-rankings. (This could also be how to compare against 3rd corpora: to what extent is there variance or correlation in certain pairwise-similarities, or top-N lists, or ordinal ranks of relevant words in each other words' 'most similar' results.)</p>

<p>You might want to perform a sanity check on your measures, by seeing to what extent they imply meaningful differences in comparisons where they logically ""shouldn't"". For example, multiple runs against the exact same corpus that's just bee reshuffled, or against random subsets of the exact same corpus. (I'm not aware of anything as formal as a 't-test' in checking the significance of differences between word2vec models, but checking whether some differences are enough to distinguish a truly-different corpus, from just a 1/Nth random subset of the same corpus, to a certain confidence level might be a grounded way to assert meaningful differences.)</p>

<p>To the extent such ""oughtta be very similar"" runs instead show end vector results that are tangibly different, it could be suggestive that either:</p>

<ul>
<li><p>the corpus is too small, with too few varied usage examples per word - word2vec benefits from lots of data, and political speech collections may be quite small compared to the sometimes hundreds-of-billions training words used for large word2vec models</p></li>
<li><p>the model is mis-parameterized - an oversized (and thus prone to overfitting) model, or insufficient training passes, or other suboptimal parameters may yield models that vary more for the same training data</p></li>
</ul>

<p>You'd also want to watch out for mismatches in training-corpus size. A corpus that's 10x as large means many more words would pass a fixed <code>min_count</code> threshold, and any chosen <em>N</em> <code>epochs</code> of training will involve 10x as many examples of common-words, and support stable results in a larger (vector-size) model - whereas the same model parameters with a smaller corpus would give more volatile results. </p>

<p>Another technique you could consider would be combining corpuses into one training set, but munging the tokens of key words-of-interest to be different depending on the relevant speaker. For example, you'd replace the word <code>'family'</code> with <code>'f-family'</code> or <code>'m-family'</code>, depending on the gender of the speaker. (You might do this for every occurrence, or some fraction of the occurrences. You might also enter each speech into your corpus more than once, sometimes with the actual words and sometimes with some-or-all replaced with the context-labeled alternates.)</p>

<p>In that case, you'd wind up with one final model, and all words/context-tokens in the 'same' coordinate space for direct comparison. But, the pseudowords <code>'f-family'</code> and <code>'m-family'</code> would have been more influenced by their context-specific usages - and thus their vectors might vary from each other, and from the original <code>'family'</code> (if you've also retained unmunged instances of its use) in interestingly suggestive ways.</p>

<p>Also note: if using the 'analogy-solving' methods of the original Google word2vec code release, or other libraries that have followed its example (like <code>gensim</code>), note that it specifically <em>won't</em> return as an answer any of the words supplied as input. So when solving the gender-fraught analogy <code>'man' : 'doctor' :: 'woman' : _?_</code>, via the call <code>model.most_similar(positive=['doctor', 'woman'], negative=['man'])</code>, even if the underlying model <em>still</em> has <code>'doctor'</code> as the closest word to the target-coordinates, it is automatically skipped as one of the input words, yielding the second-closest word instead. </p>

<p>Some early ""bias-in-word-vectors"" write-ups ignored this detail and thus tended to imply larger biases, due to this implementation artifact, even where such biases small-to-nonexistent. (You can supply raw vectors, instead of string-tokens, to <code>most_similar()</code> - and then get full results, without any filtering of input-tokens.)</p>
",4,1,1397,2020-05-11 18:38:20,https://stackoverflow.com/questions/61736874/how-to-compare-cosine-similarities-across-three-pretrained-models
Stanford CoreNLP Train custom NER model,"<p>I was making some tests by training custom models with crf, and since i don't have a proper training file i would like to make by myself a list of 5 tags and maybe 10 words only to start with and the plan is to keep improving the model with more incoming data in the future. but the results i get are plenty of false positives (it tags many words which have nothing to do with the original one in the training file) i imagine since the models created are probabilistic and take into considerarion more than just separate words</p>

<p>Let's say i want to train corenlp to detect a small list of words without caring about the context are there some special settings for that? if not, is there a way to calculate how much data is needed to get an accurate model?</p>
","nlp, stanford-nlp, crf","<p>After some tests and research find out a really good option for my case is RegexNER which works in a deterministic way and can also be combined with NER. So far tried with smaller set of rules and does the job pretty well. Next step is to determine how scalable and usable is in a high traffic stress scenario (the one i'm interested of) and compare with other solutions based in python</p>
",0,0,214,2020-05-13 19:12:24,https://stackoverflow.com/questions/61783003/stanford-corenlp-train-custom-ner-model
Transforms of NLP dependency trees into binary trees?,"<p>Spacy (and Core NLP, and other parsers) output dependency trees that can contain varying numbers of children. In spacy for example,  each node has a <code>.lefts</code> and <code>.rights</code> relations (multiple left branches and multiple right branches):</p>

<p><a href=""https://i.sstatic.net/3yIW2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3yIW2.png"" alt=""![enter image description here""></a></p>

<p>Pattern pattern matching algorithms of are considerably simpler (and more efficient) when they  worked over predicates trees who's node have a fixed set of arities.</p>

<p>Is there any standard transformation from these multi-trees into from binary trees?</p>

<p>For example, in this example, we have ""publish"" with two <code>.lefts=[just, journal]</code> and, one <code>.right=[piece]</code>.  Can sentences such (generally) be tranformed into a strict binary tree notation (where each node has 0 or 1 left, and 0 or 1 right branch) without much loss of information, or are multi-trees essential to correctly carrying information?</p>
","nlp, nltk, stanford-nlp, spacy","<p>There are different types of trees in language analysis, <a href=""https://en.wikipedia.org/wiki/Immediate_constituent_analysis"" rel=""nofollow noreferrer"">immediate constituents</a> and <a href=""https://en.wikipedia.org/wiki/Dependency_grammar"" rel=""nofollow noreferrer"">dependency trees</a> (though you wouldn't normally talk of <em>trees</em> in dependency grammar). The former are usually binary (though there is no real reason why they have to be), as each category gets split into two subcategories, eg</p>

<blockquote>
  <p>S -> NP VP<br>
  NP -> det N1<br>
  N1 -> adj N1 | noun  </p>
</blockquote>

<p>Dependencies are not normally binary in nature, so there is no simple way to transform them into binary structures. The only fixed convention is that each word will be dependent on exactly one other word, but it might itself have multiple words depending on it.</p>

<p>So, the answer is basically ""no"".</p>
",2,2,258,2020-05-17 15:39:11,https://stackoverflow.com/questions/61854424/transforms-of-nlp-dependency-trees-into-binary-trees
Add custom rules for parsing quarters to SUTime,"<p>I'm following <a href=""https://stanfordnlp.github.io/CoreNLP/sutime.html#example-1-fiscal-year"" rel=""nofollow noreferrer"">the official instructions</a> for adding custom SUTime rules for fiscal year quarters (stuff like Q1, Q2, Q3 and Q4).</p>

<p>I used the default <code>defs.sutime.txt</code> and <code>english.sutime.txt</code> as templates for my own rule files.</p>

<p>After appending the following code to my <code>defs.sutime.txt</code></p>

<pre><code>  // Financial Quarters
  FYQ1 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ1"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,10,1), IsoDate(ANY,12,31), QUARTER))
  }
  FYQ2 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ2"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,1,1), IsoDate(ANY,3,31), QUARTER))
  }
  FYQ3 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ3"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,4,1), IsoDate(ANY,6,30), QUARTER))
  }
  FYQ4 = {
      type: QUARTER_OF_YEAR,
      label: ""FYQ4"",
      value: TimeWithRange(TimeRange(IsoDate(ANY,7,1), IsoDate(ANY,9,30), QUARTER))
  }
</code></pre>

<p>and appending the following code to my <code>english.sutime.txt</code></p>

<pre><code>  # Financial Quarters
  FISCAL_YEAR_QUARTER_MAP = {
    ""Q1"": FYQ1,
    ""Q2"": FYQ2,
    ""Q3"": FYQ3,
    ""Q4"": FYQ4
  }
  FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP = {
    ""Q1"": 1,
    ""Q2"": 0,
    ""Q3"": 0,
    ""Q4"": 0
  }
  $FiscalYearQuarterTerm = CreateRegex(Keys(FISCAL_YEAR_QUARTER_MAP))

  {
    matchWithResults: TRUE,
    pattern: ((/$FiscalYearQuarterTerm/) (FY)? (/(FY)?([0-9]{4})/)),
    result:  TemporalCompose(INTERSECT, IsoDate(Subtract({type: ""NUMBER"", value: $$3.matchResults[0].word.group(2)}, FISCAL_YEAR_QUARTER_YEAR_OFFSETS_MAP[$1[0].word]), ANY, ANY), FISCAL_YEAR_QUARTER_MAP[$1[0].word])
  }

  {
    pattern: ((/$FiscalYearQuarterTerm/)),
    result: FISCAL_YEAR_QUARTER_MAP[$1[0].word]
  }
</code></pre>

<p>I'm still unable to correctly parse stuff like ""Q1 2020"".</p>

<p>How can I properly add rules for parsing fiscal year quarters (e.g. ""Q1"")?</p>

<p>Here's my full code:</p>

<pre><code>import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.time.*;
import edu.stanford.nlp.util.CoreMap;

public class SUTimeSoExample {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.setProperty(""sutime.includeRange"", ""true"");
        props.setProperty(""sutime.markTimeRanges"", ""true"");
        props.setProperty(""sutime.rules"", ""./defs.sutime.txt,./english.sutime.txt"");

        AnnotationPipeline pipeline = new AnnotationPipeline();
        pipeline.addAnnotator(new TokenizerAnnotator(false));
        pipeline.addAnnotator(new WordsToSentencesAnnotator(false));
        pipeline.addAnnotator(new POSTaggerAnnotator(false));
        pipeline.addAnnotator(new TimeAnnotator(""sutime"", props));

        String input = ""Stuff for Q1 2020"";

        Annotation annotation = new Annotation(input);
        annotation.set(CoreAnnotations.DocDateAnnotation.class, ""2020-06-01"");
        pipeline.annotate(annotation);
        System.out.println(annotation.get(CoreAnnotations.TextAnnotation.class));
        List&lt;CoreMap&gt; timexAnnsAll = annotation.get(TimeAnnotations.TimexAnnotations.class);
        for (CoreMap cm : timexAnnsAll) {
            System.out.println(cm // match
                    + "" --&gt; "" + cm.get(TimeExpression.Annotation.class).getTemporal() // parsed value
            );
        }
    }
}
</code></pre>

<p>Note that I deleted the deafult <code>defs.sutime.txt</code> and <code>english.sutime.txt</code> files from the stanford corenlp models JAR in order to avoid <a href=""https://stackoverflow.com/questions/31970286/configuring-sutime-to-use-custom-rule-files"">this issue</a>.</p>
","java, nlp, stanford-nlp, sutime","<p>There is a Java code example here: </p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/sutime.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/sutime.html</a></p>

<p>It should work if you follow that example, mainly building your pipeline in this manner:</p>

<pre><code>props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
props.setProperty(""ner.docDate.usePresent"", ""true"");
// this will shut off the statistical models if you only want to run SUTime only
props.setProperty(""ner.rulesOnly"", ""true"");
// add your sutime properties as in your example
...
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>and make sure to use version 4.0.0.  </p>

<p>You can set <code>ner.rulesOnly</code> to true if you just want to run SUTime without running the statistical models.</p>

<p>You can use one of several properties for <code>ner.docDate</code> or just set the document date in the annotation before running.</p>
",1,1,231,2020-06-01 11:25:27,https://stackoverflow.com/questions/62131089/add-custom-rules-for-parsing-quarters-to-sutime
How do I host CoreNLP server with caseless models?,"<p>I'm trying to host a CoreNLP server but with the caseless models but I don't think I was successful and the official site doesn't have example hosting such model.</p>

<p>I'm currently hosting with:</p>

<pre><code>java -mx4g \
           -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
           -port 9000 \
           -timeout 15000
</code></pre>

<p>but this is the default way of hosting which doesn't use the caseless models. I checked the app log and it was loading the standard models instead of caseless models:</p>

<pre><code>[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
</code></pre>

<p>According to <a href=""https://stanfordnlp.github.io/CoreNLP/caseless.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/caseless.html</a>, I have downloaded the english models jar file and put it under the corenlp module folder, but I don't know exactly how to specify and use those for server hosting.</p>

<p>In the client side, I'm doing the following:</p>

<pre><code>import requests

r = requests.post('http://[::]:9000/?properties={""annotators"":""tokenize,ssplit,truecase,pos,ner"",""outputFormat"":""json""}', 
                  data=""show me hotels in toronto for next weekend"")
print(r.text)
</code></pre>

<p>The truecase is working, but I don't see the caseless models being used.</p>

<p>Any help would be appreciated.</p>
","python, nlp, stanford-nlp, named-entity-recognition","<p>You need to pass the property <code>""ner.model"": ""edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz""</code></p>

<p>Also you may want to use Stanza for accessing the Stanford CoreNLP server.</p>

<p>Details here: <a href=""https://stanfordnlp.github.io/stanza/corenlp_client.html#overview"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/corenlp_client.html#overview</a></p>
",1,1,198,2020-06-02 14:24:24,https://stackoverflow.com/questions/62154244/how-do-i-host-corenlp-server-with-caseless-models
StanfordNLP custom model in java,"<p>I am using Stanford NLP for the first time.<br>
Here is my code as of now:  </p>

<pre><code>Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
    props.setProperty(""ner.additional.regexner.mapping"", ""additional.rules"");
    //props.setProperty(""ner.applyFineGrained"", ""false"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    String content = ""request count for www.abcd.com"";
    CoreDocument doc = new CoreDocument(content);
    // annotate the document
    pipeline.annotate(doc);
    // view results
    System.out.println(""---"");
    System.out.println(""entities found"");
    for (CoreEntityMention em : doc.entityMentions())
      System.out.println(""\tdetected entity: \t"" + em.text() + ""\t"" + em.entityType());
    System.out.println(""---"");
    System.out.println(""tokens and ner tags"");
    String tokensAndNERTags =
        doc.tokens().stream().map(token -&gt; ""("" + token.word() + "","" + token.ner() + "")"")
            .collect(Collectors.joining("" ""));
    System.out.println(tokensAndNERTags);

</code></pre>

<p>I have set property <code>ner.additional.regexner.mapping</code> to include my own rules.</p>

<p>Rule File(additional.rules) looks somewhat like this:</p>

<pre><code>request count   getReq
requestcount    getReq
server details  getSer
serverdetails   getSer
</code></pre>

<p>where getReq and getSer are tags for the corresponding words.  </p>

<p>When I am running my code, I am not getting the required output.</p>

<p>Required for the sample line - (request count for www.abcd.com):</p>

<pre><code>request count  -&gt;  getReq
</code></pre>

<p>Output I am getting : </p>

<pre><code>---
entities found
    detected entity:    count   TITLE
    detected entity:    www.abcd.com    URL
---
tokens and ner tags
(request,O) (count,TITLE) (for,O) (www.abcd.com,URL)
</code></pre>

<p>What am I doing wrong?<br>
Please Help.</p>
","java, stanford-nlp","<p>Ok So the problem was in this line :</p>

<pre><code>props.setProperty(""ner.additional.regexner.mapping"", ""additional.rules"");
</code></pre>

<p>I removed it and added the following lines :</p>

<pre><code>pipeline.addAnnotator(new TokensRegexNERAnnotator(""additional.rules"", true));

</code></pre>

<p>Now I am getting the required output</p>
",1,1,144,2020-06-04 08:16:33,https://stackoverflow.com/questions/62189911/stanfordnlp-custom-model-in-java
Name of task - splitting up complex sentences?,"<p>I've go a question about the name of an NLP task - Splitting up a complex sentence into simple ones.
For example, if I have this sentence:</p>

<p>""Input t on the username input box and password input box.""</p>

<p>I'd like to split this sentence into simpler sentences:</p>

<p>""Input t on the username input box""
""Input t on the password input box""</p>

<p>What would this problem be called? I've tried clause extraction <a href=""https://stackoverflow.com/questions/39320015/how-to-split-an-nlp-parse-tree-to-clauses-independent-and-subordinate"">here</a> but I don't want clauses, but rather, fully formed sentences. I've also tried 'sentence simplification' but it exceeds what I'm trying to do, with its lexical simplification and all. </p>

<p>Thanks </p>
","nlp, nltk, stanford-nlp, spacy","<p>I don't think there is the name used by everyone but, for example, in this paper <a href=""https://arxiv.org/pdf/1805.01035"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1805.01035</a> they call it split-and-rephrase (in several other papers this term is used too). </p>
",2,0,608,2020-06-08 03:57:35,https://stackoverflow.com/questions/62254778/name-of-task-splitting-up-complex-sentences
how to calculate mean of words&#39; glove embedding in a sentence,"<p>I have downloaded the glove trained matrix and used it in a Keras layer. however, I need the sentence embedding for another task.</p>

<p>I want to calculate the mean of all the word embeddings that are in that sentence.</p>

<p>what is the most efficient way to do that since there are about 25000 sentences?</p>

<p>also, I don't want to use a Lambda layer in Keras to get the mean of them.</p>
","python, word-embedding, glove","<p>the best way to do this is to use a GlobalAveragePooling1D layer. it receives the embeddings of tokens inside the sentences from the Embedding layer with the shapes (n_sentence, n_token, emb_dim) and computes the average of each token present in the sentence. the result has shape (n_sentence, emb_dim)</p>

<p>here a code example</p>

<pre><code>embedding_dim = 128
vocab_size = 100
sentence_len = 20

embedding_matrix = np.random.uniform(-1,1, (vocab_size,embedding_dim))
test_sentences = np.random.randint(0,vocab_size, (3,sentence_len))

inp = Input((sentence_len))
embedder = Embedding(vocab_size, embedding_dim,
                     trainable=False, weights=[embedding_matrix])(inp)
avg = GlobalAveragePooling1D()(embedder)

model = Model(inp, avg)
model.summary()

model(test_sentences) # the mean of all the word embeddings inside sentences 
</code></pre>
",1,0,1629,2020-06-09 16:41:58,https://stackoverflow.com/questions/62287631/how-to-calculate-mean-of-words-glove-embedding-in-a-sentence
Category detection,"<p>i have used this code for category detection..</p>

<pre><code>import numpy as np

# Words -&gt; category
categories = {word: key for key, words in data.items() for word in words}

# Load the whole embedding matrix
embeddings_index = {}
with open('glove.6B.100d.txt', encoding=""utf8"") as f:
  for line in f:
    values = line.split()
    word = values[0]
    embed = np.array(values[1:], dtype=np.float32)
    embeddings_index[word] = embed
print('Loaded %s word vectors.' % len(embeddings_index))
# Embeddings for available words
data_embeddings = {key: value for key, value in embeddings_index.items() if key in categories.keys()}

# Processing the query
def process(query):
  query_embed = embeddings_index[query]
  scores = {}
  for word, embed in data_embeddings.items():
    category = categories[word]
    dist = query_embed.dot(embed)
    dist /= len(data[category])
    scores[category] = scores.get(category, 0) + dist
  return scores


# Testing
print(process('pizza'))
</code></pre>

<h1>OUTPUT</h1>

<pre><code>{'service': 6.385544379552205, 'ambiance': 3.5752111077308655, 'Food': 12.912149047851562}
</code></pre>

<p>is there a way I only get the highest accuracy category like Food??</p>
","python, glove","<pre><code>def process(query):
  query_embed = embeddings_index[query]
  scores = {}
  for word, embed in data_embeddings.items():
    category = categories[word]
    dist = query_embed.dot(embed)
    dist /= len(data[category])
    scores[category] = scores.get(category, 0) + dist
  return max(scores, key=scores.get)
</code></pre>

<p>You can use <code>max()</code> for this. This will return the key name of maximum value.</p>
",0,1,63,2020-06-09 20:03:51,https://stackoverflow.com/questions/62290956/category-detection
Does Stanford Core NLP support Russian sentence and word tokenization?,"<p>I could not see any Russian pre-trained tokenizer in Sandford-NLP and stanfordCoreNLP. Are there any models for Russian yet?</p>
",stanford-nlp,"<p>Unfortunately I don't know of any extensions that handle that for Stanford CoreNLP.</p>

<p>You can use Stanza (<a href=""https://stanfordnlp.github.io/stanza/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/</a>) which is our Python package to get Russian tokenization and sentence splitting.</p>

<p>You could theoretically tokenize and sentence split with Stanza, and then use the Stanford CoreNLP Server (which you can also use via Stanza) if you had any CoreNLP specific components you wanted to work with.</p>

<p>A group a while back submitted some models for Russian, but I don't see anything for tokenization.</p>

<p>The link to their resources is here: <a href=""https://stanfordnlp.github.io/CoreNLP/model-zoo.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/model-zoo.html</a></p>
",0,0,537,2020-06-17 07:46:44,https://stackoverflow.com/questions/62423948/does-stanford-core-nlp-support-russian-sentence-and-word-tokenization
Using glove.6B.100d.txt embedding in spacy getting zero lex.rank,"<p>I am trying to load glove 100d emebddings in spacy nlp pipeline.  </p>

<p>I create the vocabulary in spacy format as follows: </p>

<pre><code>python -m spacy init-model en spacy.glove.model --vectors-loc glove.6B.100d.txt
</code></pre>

<p>glove.6B.100d.txt is converted to word2vec format by adding ""400000 100"" in the first line. </p>

<p>Now </p>

<pre><code>spacy.glove.model/vocab has following files: 
5468549  key2row
38430528  lexemes.bin
5485216  strings.json
160000128  vectors
</code></pre>

<p>In the code: </p>

<pre><code>import spacy 
nlp = spacy.load(""en_core_web_md"")

from spacy.vocab import Vocab
vocab = Vocab().from_disk('./spacy.glove.model/vocab')

nlp.vocab = vocab

print(len(nlp.vocab.strings)) 
print(nlp.vocab.vectors.shape) gives 
</code></pre>

<p>gives 
407174
(400000, 100)</p>

<p>However the problem is that: </p>

<pre><code>V=nlp.vocab
max_rank = max(lex.rank for lex in V if lex.has_vector)
print(max_rank) 
</code></pre>

<p>gives 0 </p>

<p>I just want to use the 100d glove embeddings within spacy in combination with ""tagger"", ""parser"", ""ner"" models from en_core_web_md. </p>

<p>Does anyone know how to go about doing this correctly (is this possible)? </p>
","nlp, spacy, glove","<p>The tagger/parser/ner models are trained with the included word vectors as features, so if you replace them with different vectors you are going to break all those components.</p>

<p>You can use new vectors to train a new model, but replacing the vectors in a model with trained components is not going to work well. The tagger/parser/ner components will most likely provide nonsense results.</p>

<p>If you want 100d vectors instead of 300d vectors to save space, you can resize the vectors, which will truncate each entry to first 100 dimensions. The performance will go down a bit as a result.</p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_md"")
assert nlp.vocab.vectors.shape == (20000, 300)
nlp.vocab.vectors.resize((20000, 100))
</code></pre>
",1,0,1273,2020-06-17 17:34:30,https://stackoverflow.com/questions/62435042/using-glove-6b-100d-txt-embedding-in-spacy-getting-zero-lex-rank
Extract POS tag for a word coming before a given word,"<p>I am new in python and I am trying to extract Part of speech (Stanford CoreNLP) for a word coming before a given word. for the text = &quot;انسان يحضر طعامه باستخدام الخبز الابيض وبجانبه قطة سوداء؟&quot;</p>
<p>here is my code</p>
<pre><code>for i in nouns:             
    pattren =&quot;\w+(?=\s*&quot;+i+&quot;[^/])&quot;
    re1 = re.search(pattren , text)
    if(re1):
        for tag in tagger.tag(text.split()):       #POS tag extractor
            if re1[0] in tag[1]:
                for specific in tag[1].split():
                    if re1[0] in specific:
                        print(&quot;The Noun &quot; + i + &quot;:-&quot;)
                        print(specific)
</code></pre>
<p>where <strong>nouns</strong> is an array contains all the NN in the text ['انسان', 'طعام', 'استخدام', 'جانب', 'قطة']
I tried to use regular expression to extract word before .</p>
<p>the output was:</p>
<pre><code>The Noun طعام:-
يحضر/VBP
The Noun استخدام:-
ب/IN
The Noun استخدام:-
الخبز/DTNN
The Noun استخدام:-
الابيض/DTJJ
The Noun استخدام:-
ب/IN
The Noun استخدام:-
جانب/NN
The Noun جانب:-
ب/IN
The Noun جانب:-
الخبز/DTNN
The Noun جانب:-
الابيض/DTJJ
The Noun جانب:-
ب/IN
The Noun جانب:-
جانب/NN
The Noun قطة:-
ه/PRP$
The Noun قطة:-
ه/PRP$
</code></pre>
<p>there are repeated words ,and I really could not conduct the issue.</p>
","python, stanford-nlp, part-of-speech","<p>The issue was in the line</p>
<p><code>if re1[0] in tag[1]:</code></p>
<p>this gets all the words within tag[1] string matches with re1[0] whether it is a word or a char.</p>
<p>solution, I tried using regular expression to get the exact words in tag[1].</p>
<p><code>if re.match(r'\b'+ re1[0]+'(?!\.?\d)', tag[1]):</code></p>
",0,0,137,2020-07-04 18:10:42,https://stackoverflow.com/questions/62732740/extract-pos-tag-for-a-word-coming-before-a-given-word
How can I download a stanza&#39;s model via command line?,"<p>One can download a stanza's model via Python <a href=""https://pypi.org/project/stanza/"" rel=""nofollow noreferrer"">as follows</a> (<a href=""https://web.archive.org/web/20200708034746/https://pypi.org/project/stanza/"" rel=""nofollow noreferrer"">mirror</a>):</p>
<pre><code>import stanza
stanza.download('en')       # This downloads the English models for the neural pipeline
</code></pre>
<p>How can I download a stanza's model via command line?</p>
<p>E.g. with spaCy one can use:</p>
<pre><code>python -m spacy download en
</code></pre>
<p>I unsuccessfully tried:</p>
<pre><code>python -m stanza download en
</code></pre>
<p>I use <code>stanza==1.0.1</code>.</p>
","python, nlp, stanford-stanza","<p>Instead of using to <code>-m</code> argument for the Python interpreter, you can use <code>-c</code> (command). To replicate your download call would look like this:</p>
<pre><code>python -c 'import stanza; stanza.download(&quot;en&quot;)'
</code></pre>
",5,3,1078,2020-07-08 03:49:29,https://stackoverflow.com/questions/62787240/how-can-i-download-a-stanzas-model-via-command-line
How can I download a stanza&#39;s model manually?,"<p>One can theoretically download a stanza's model via Python <a href=""https://pypi.org/project/stanza/"" rel=""nofollow noreferrer"">as follows</a> (<a href=""https://web.archive.org/web/20200708034746/https://pypi.org/project/stanza/"" rel=""nofollow noreferrer"">mirror</a>):</p>
<pre><code>import stanza
stanza.download('en')       # This downloads the English models for the neural pipeline
</code></pre>
<p>However, the Stanford server is inaccessible from my computer:</p>
<pre><code>(neural-parser2) dernoncourt@ilcompn0:~/temp/stanza$ python
Python 3.6.7 (default, Oct 25 2018, 09:16:13)
[GCC 5.4.0 20160609] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import stanza
&gt;&gt;&gt; stanza.download('en')
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 24.5MB/s]
2020-07-07 21:08:34 INFO: Downloading default packages for language: en (English)...
Traceback (most recent call last):
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/connection.py&quot;, line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/util/connection.py&quot;, line 84, in create_connection
    raise err
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/util/connection.py&quot;, line 74, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 677, in urlopen
    chunked=chunked,
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 392, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 1026, in _send_output
    self.send(msg)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 964, in send
    self.connect()
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/connection.py&quot;, line 187, in connect
    conn = self._new_conn()
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/connection.py&quot;, line 172, in _new_conn
    self, &quot;Failed to establish a new connection: %s&quot; % e
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7fa28f172438&gt;: Failed to establish a new connection: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/requests/adapters.py&quot;, line 449, in send
    timeout=timeout
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;, line 725, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/urllib3/util/retry.py&quot;, line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='nlp.stanford.edu', port=80): Max retries exceeded with url: /software/stanza/1.0.0/en/default.zip (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fa28f172438&gt;: Failed to establish a new connection: [Errno 110] Connection timed out',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/stanza/utils/resources.py&quot;, line 236, in download
    request_file(f'{url}/{__resources_version__}/{lang}/default.zip', os.path.join(dir, lang, f'default.zip'), md5=resources[lang]['default_md5'])
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/stanza/utils/resources.py&quot;, line 83, in request_file
    download_file(url, path)
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/stanza/utils/resources.py&quot;, line 66, in download_file
    r = requests.get(url, stream=True)
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/requests/api.py&quot;, line 76, in get
    return request('get', url, params=params, **kwargs)
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/requests/api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/requests/sessions.py&quot;, line 530, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/requests/sessions.py&quot;, line 643, in send
    r = adapter.send(request, **kwargs)
  File &quot;/mnt/ilcodisk1/user/dernoncourt/pyenv/neural-parser2/lib/python3.6/site-packages/requests/adapters.py&quot;, line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='nlp.stanford.edu', port=80): Max retries exceeded with url: /software/stanza/1.0.0/en/default.zip (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fa28f172438&gt;: Failed to establish a new connection: [Errno 110] Connection timed out',))
</code></pre>
<p>How can I download a stanza's model manually? I'm hoping the stanza's models are stored in more than one servers.</p>
<p>I use <code>stanza==1.0.1</code>.</p>
","python, nlp, stanford-stanza","<p>I've checked with the Stanford NLP Group on Twitter and they've confirmed that the Standford NLP site (<a href=""https://nlp.stanford.edu/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/</a>) is down today (8 July) due to restructuring the Stanford-level data center. They'll be back tomorrow (9 July) and then your script should work again.</p>
<p>They've also now posted this on Twitter:</p>
<p><a href=""https://twitter.com/stanfordnlp/status/1280917602718461952?s=20"" rel=""nofollow noreferrer"">https://twitter.com/stanfordnlp/status/1280917602718461952?s=20</a></p>
",2,2,2712,2020-07-08 04:12:24,https://stackoverflow.com/questions/62787428/how-can-i-download-a-stanzas-model-manually
NLP problems to handle sentence with conjunctions,"<h1>What I would like to do</h1>
<p>I would like to preprocess sentences include conjunctions like below.
I don’t care the tense of verb and transformation following the subject.
What I want to is to hold new two sentences that have subjects and verbs individually.</p>
<pre><code>**Pattern1**
They entered the house and she glanced at the dark fireplace.
[&quot;They entered the house &quot;, &quot;she glanced at the dark fireplace&quot;]

**Pattern2** 
Felipa and Alondra sing a song.
[&quot;Felipa sing a song”, &quot;Alondra sing a song&quot;]

**Pattern3**
“Jessica watches TV and eats dinner.
[&quot;Jessica watch TV, “Jessica eat dinner”]
</code></pre>
<h1>Problem</h1>
<p>I was able to solve the sentence of Pattern1 with the below code, but I'm stack with thinking the solutions for Pattern2 and 3 with the below code no.2.</p>
<p>With using <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">the NLP library spaCy</a>, I was able to figure out conjunctions is recognized as <code>CCONJ</code>.
However, there is no clues to realize what I want to do like the above.</p>
<p>Please give me your advice!</p>
<h1>Current Code</h1>
<p>Pattern1</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;They entered the house and she glanced at the dark fireplace.&quot;
if 'and' in text:
    text = text.replace('and',',')
    l = [x.strip() for x in text.split(',') if not x.strip() == '']
l

#output
['They entered the house', 'she glanced at the dark fireplace.']
</code></pre>
<p>working code</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;Felipa and Alondra sing a song.&quot;
doc_dep = nlp(text)
for k in range(len(doc_dep)):
    token = doc_dep[k]
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_) 
    if token.pos_ == 'CCONJ':
        print(token.text)

#output
Felipa felipa NOUN NN nsubj
     SPACE _SP 
and and CCONJ CC cc
and
     SPACE _SP 
Alondra Alondra PROPN NNP nsubj
sing sing VERB VBP ROOT
a a DET DT det
song song NOUN NN dobj
. . PUNCT . punct
</code></pre>
<pre class=""lang-py prettyprint-override""><code>text = &quot;Jessica watches TV and eats dinner.&quot;
doc_dep = nlp(text)
for k in range(len(doc_dep)):
    token = doc_dep[k]
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_) 
    if token.pos_ == 'CCONJ':
        print(token.text)
#output
Jessica Jessica PROPN NNP nsubj
watches watch VERB VBZ ROOT
TV tv NOUN NN dobj
and and CCONJ CC cc
and
eats eat VERB VBZ conj
dinner dinner NOUN NN dobj
. . PUNCT . punct
</code></pre>
<h1>Development Environment</h1>
<p>python 3.7.4</p>
<p>spaCy version    2.3.1</p>
<p>jupyter-notebook : 6.0.3</p>
","python, python-3.x, nlp, stanford-nlp, spacy","<p>There's no reason to think that the same code should be able to handle all of these situations, as the function of the word &quot;and&quot; is very different in each case. In Pattern 1, it is connecting two independent clauses. In Pattern 2, it is creating a compound subject. In Pattern 3, it is coordinating verb phrases.</p>
<p>I would caution you that if your ultimate aim is to 'split' all sentences that contain the word 'and' (or any other coordinating conjunction) in this way, you have a very challenging job ahead of you. Coordinating conjunctions function in many different ways in English. There are many common patterns different from those you list here, such as nonconstituent coordination (&quot;Bill went to Chicago on Wednesday and New York on Thursday&quot;, which you'd presumably want to turn into [&quot;Bill went to Chicago on Wednesday&quot;, &quot;Bill went to New York on Thursday&quot;]) -- note the subtle but critical difference from &quot;Bill went to Chicago and New York on Thursday&quot;, which would need to become [&quot;Bill went to Chicago on Thursday&quot;, &quot;Bill went to New York on Thursday&quot;]; coordinated verbs (&quot;Mary saw and heard him walk up the steps&quot;), among others. And of course more than two constituents can be coordinated (&quot;Sarah, John, and Marcia...&quot;), and many patterns can all be combined in the same sentence.</p>
<p>English is complicated and handling this would be a huge job, even for a linguist with a strong command of what is going on syntactically in all the cases to be covered. Even just characterizing how English coordinations behave is tough, as <a href=""https://www.aclweb.org/anthology/C88-1061.pdf"" rel=""nofollow noreferrer"">this paper that considers just a handful of patterns</a> illustrates. If you consider that your code would have to handle real-world sentences with multiple 'and's doing different things (e.g., &quot;Autonomous cars shift insurance liability and moral responsibility toward manufacturers, and it doesn't look like this will change anytime soon&quot;), the complexity of the task becomes clearer.</p>
<p>That said, if you are only interested in handling the most common and simple cases, you might be able to make at least some headway by processing the results of a constituency parser like <a href=""https://www.nltk.org/book/ch08.html"" rel=""nofollow noreferrer"">the one built into NLTK</a>, or a SpaCy plugin like <a href=""https://spacy.io/universe/project/self-attentive-parser"" rel=""nofollow noreferrer"">benepar</a>. That at least would clearly show you what elements of the sentence are being coordinated by the conjunction.</p>
<p>I don't know what your ultimate task is so I can't say this with confidence, but I'm skeptical that the gains you get by preprocessing in this way will be worth the effort. You might consider stepping back and thinking about the ultimate task you are trying to achieve, and researching (and/or asking StackOverflow) whether there are any preprocessing steps that are known to generally improve performance.</p>
",1,3,2099,2020-07-16 15:44:39,https://stackoverflow.com/questions/62938465/nlp-problems-to-handle-sentence-with-conjunctions
How can I transform verbs from present tense to past tense with using NLP library?,"<h1>What I would like to do</h1>
<p>I would like to transform verbs from  present tense to past tense with using NLP library like below.</p>
<pre><code>As she leaves the kitchen, his voice follows her.

#output
As she left the kitchen, his voice followed her.
</code></pre>
<h1>Problem</h1>
<p>There is no way to transform from present tense to past tense.</p>
<p>I've checked the following similar question, but they only introduced the way to transform from
past tense to present tense.</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/3753021/using-nltk-and-wordnet-how-do-i-convert-simple-tense-verb-into-its-present-pas"">Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?</a></li>
</ul>
<h1>What I tried to do</h1>
<p>I was able to transform verbs from past tense to present tense using <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">spaCy</a>.
However, there is no way to do the same thing from present tense to past tense.</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;As she left the kitchen, his voice followed her.&quot;
doc_dep = nlp(text)
for i in range(len(doc_dep)):
    token = doc_dep[i]
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_) 
    if token.pos_== 'VERB':
        print(token.text)
        print(token.lemma_)
        text = text.replace(token.text, token.lemma_)
print(text)

#output
'As she leave the kitchen, his voice follow her.'
</code></pre>
<h1>Development Environment</h1>
<p>Python 3.7.0</p>
<p>spaCy version 2.3.1</p>
","python, python-3.x, nlp, stanford-nlp, spacy","<p>As far as I know spaCy does not have any built-in function for this type of transformation, but you can use an extension where you map present/past tense pairs, and where you don't have the appropriate pairs 'ed' suffix for the past participle of weak verbs as below:</p>
<pre><code>verb_map = {'leave': 'left'}

def make_past(token):
    return verb_map.get(token.text, token.lemma_ + 'ed')

spacy.tokens.Token.set_extension('make_past', getter=make_past, force=True)

text = &quot;As she leave the kitchen, his voice follows her.&quot;
doc_dep = nlp(text)
for i in range(len(doc_dep)):
    token = doc_dep[i]
    if token.tag_ in ['VBP', 'VBZ']:
        print(token.text, token.lemma_, token.pos_, token.tag_) 
        text = text.replace(token.text, token._.make_past)
print(text)
</code></pre>
<p>Output:</p>
<pre><code>leave leave VERB VBP
follows follow VERB VBZ
As she left the kitchen, his voice followed her.
</code></pre>
",3,8,5941,2020-07-17 01:02:51,https://stackoverflow.com/questions/62945590/how-can-i-transform-verbs-from-present-tense-to-past-tense-with-using-nlp-librar
Scaled Co-occurrence matrix with window size calculation in python,"<p>Say, I've a dataset in CSV format, which contains sentences/paragraphs in rows.
Suppose, it looks like this:</p>
<p>df = ['A B X B', 'X B B']</p>
<p>Now, I can generate co-occurrence matrix that looks like this</p>
<pre><code>  A B X
A 0 2 1
B 2 0 4
X 1 4 0
</code></pre>
<p>Here, (A,B,X) are words. It says B appeared where X is present = 4 times
Code that I used for it</p>
<pre><code>def co_occurrence(sentences, window_size):
    d = defaultdict(int)
    vocab = set()
    for text in sentences:
        # preprocessing (use tokenizer instead)
        text = text.lower().split()
        # iterate over sentences
        for i in range(len(text)):
            token = text[i]
            vocab.add(token)  # add to vocab
            next_token = text[i+1 : i+1+window_size]
            for t in next_token:
                key = tuple( sorted([t, token]) )
                d[key] += 1

    # formulate the dictionary into dataframe
    vocab = sorted(vocab) # sort vocab
    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),
                      index=vocab,
                      columns=vocab)
    for key, value in d.items():
        df.at[key[0], key[1]] = value
        df.at[key[1], key[0]] = value
    return df
</code></pre>
<p>The beauty of this code segment is that it allows me to choose windows size. That means if a particular word doesn't appear with a fixed amount of range from total sentence size then it gets ignored. But I would like to scale it.</p>
<p><a href=""https://i.sstatic.net/Zj4ZZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Zj4ZZ.png"" alt=""Scaled co-occurence"" /></a></p>
<p>So this means if a word is far from the target word &quot;to&quot; then it will be assigned lesser values. Unfortunately, I couldn't find a suitable solution for it. Is it possible with a package such as scikit-learn? Or is there any other way to do it except raw coding?</p>
","python, matrix, nlp, stanford-nlp, find-occurrences","<p><a href=""https://gist.github.com/zyocum/2ba0457246a4d0075149aa7d607432c1"" rel=""nofollow noreferrer"">Here</a>’s an implementation that can optionally scale the accumulated co-occurence values based on the distance between word tokens in the input sentences:</p>
<pre><code>In [11]: sentences = ['from swerve of shore to bend of bay , brings'.split()]                                    

In [12]: index, matrix = co_occurence_matrix(sentences, window=3, scale=True)                                    

In [13]: cell = index['bend'], index['of']                                                                       

In [14]: matrix[cell]                                                                                            
Out[14]: 1.3333333333333333

In [15]: index, matrix = co_occurence_matrix(sentences, window=3, scale=False)                                   

In [16]: matrix[cell]                                                                                            
Out[16]: 2.0

In [17]: {w: matrix[index['to']][i] for w, i in index.items()}                                                   
Out[17]: 
{',': 0.0,
 'bend': 1.0,
 'of': 1.0,
 'bay': 0.3333333333333333,
 'brings': 0.0,
 'to': 0.0,
 'from': 0.0,
 'shore': 1.0,
 'swerve': 0.3333333333333333}

</code></pre>
",1,1,1744,2020-07-17 08:15:27,https://stackoverflow.com/questions/62949869/scaled-co-occurrence-matrix-with-window-size-calculation-in-python
How to concatenate Glove 100d embedding and 1d array which contains additional signal?,"<p>I new to NLP and trying out some text classification algorithms. I have 100d GloVe vector representing each entry as a list of embeddings. Also, I have NER feature of shape (2234,) which shows if there is named entity or not. Array with GloVe embeddings is of shape (2234, 100).</p>
<p>How to correctly concatenate these array so each row represents its word?</p>
<p>Sorry for not including reproducible example. Please, use variables of your choice to explain the concatenation procedure.</p>
<p>Using <code>np.concatenate</code> did not work as I have expected but i don't know how to deal with dimensionality of embeddings.</p>
","numpy, nlp, concatenation, stanford-nlp, word-embedding","<p>Just in case someone accidentally gets here. Use:</p>
<pre><code>my_arr.reshape(2234,1)
</code></pre>
<p>Don't be me:)</p>
",0,0,209,2020-07-21 14:34:39,https://stackoverflow.com/questions/63016888/how-to-concatenate-glove-100d-embedding-and-1d-array-which-contains-additional-s
Stanford Parser - amod and nsubj dependencies - combinations of predicative and attributive adjectives,"<p>Having looked at the &quot;Stanford typed dependencies manual&quot;, I understand to some extent how the amod and nsubj dependencies function from the following examples:</p>
<p>&quot;Sam eats read meat&quot;        amod(meat, red)</p>
<p>“The baby is cute”      nsubj(cute, baby)</p>
<p>I am assuming then that it would treat slightly more complex examples as follows:</p>
<p>&quot;Sam eats meat that is red and tasty&quot;   nsubj(meat, red), nsubj(meat, tasty)</p>
<p>&quot;Red meat is tasty&quot;                 amod(meat, red), nsubj(tasty, meat)</p>
<p>Have I understood this correctly?</p>
<p>If so, I wonder if I have also correctly understood how the parser would treat these examples from my own research:</p>
<p>&quot;Vous ferez un vers baroque&quot; amod(baroque, vers)</p>
<p>&quot;Vous trouvez des vers plus baroques, plus durs, plus rocailleux que ceux-ci&quot;   amod(baroque, vers), amod(dur, vers), amod(rocailleux, vers)</p>
<p>&quot;Les beaux vers de Monsieur Racine sont durs et baroques&quot; nsubj(baroque, vers), nsubj(dur, vers), amod(beau,vers)</p>
<p>I apologize if these are naive questions. I am an art historian attempting to use computational linguistics tools for my research on eighteenth-century art criticism. I also apologize if this is the wrong forum for this type of question.</p>
<p>Thank you in advance for any assistance you may be able to offer.</p>
","parsing, stanford-nlp","<p>I am not a linguistics expert, but I should note that the parser is now creating Universal Dependencies, so you should consult this guide:</p>
<p><a href=""https://universaldependencies.org/u/overview/syntax.html"" rel=""nofollow noreferrer"">https://universaldependencies.org/u/overview/syntax.html</a></p>
<p>Furthermore, given that the parser is statistical, it may make errors from time to time.</p>
<p>Also, you can search through the French treebank (available at the universal dependencies web site) to see specific examples of French dependency trees used to train the parser. You could review some specific examples.</p>
",0,-1,192,2020-07-22 15:53:56,https://stackoverflow.com/questions/63038303/stanford-parser-amod-and-nsubj-dependencies-combinations-of-predicative-and
Stanford Core NLP use tokensRegex,"<p>Let's take the following&quot;</p>
<pre><code>client = CoreNLPClient(memory='1G', threads=1, annotators=['tokenize','ssplit','pos','lemma','ner','depparse'], timeout=1000)
ann = client.annotate('Wow a nice sentence here')
sentence = ann.sentence[0]
</code></pre>
<p>Then I process the tags, dependencies, etc. but I also want to use <code>TokensRegex</code> to extract specific words. I saw the answer using <code>requests</code> (<a href=""https://stackoverflow.com/questions/42802406/using-stanford-tregex-in-python"">here</a>) however it seems odd to have to send another request (and do the tagging again) in order to use <code>TokensRegex</code>. Can we just use the already annotated <code>sentence</code> with TokensRegex?</p>
<p><strong>Edit</strong><br>
I see that we can use <code>client.tokensregex('Wow a nice sentence here', &lt;pattern&gt;</code>) however this still has to send a request again I guess</p>
",stanford-nlp,"<p>There is a <code>tokensregex</code> annotator that you can place at the end of your pipeline that will run rules.</p>
<p>See here: <a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokensregex.html</a></p>
",0,0,168,2020-07-25 16:11:23,https://stackoverflow.com/questions/63090573/stanford-core-nlp-use-tokensregex
Sentiment results are different between stanford nlp python package and the live demo,"<p>I try sentiment analysis of tweet text by both stanford nlp python package and the live demo, but the results are different. The result of the python package is positive while the result of the live demo is negative.</p>
<ul>
<li>For python package, I download <strong>stanford-corenlp-4.0.0</strong> and install <strong>py-corenlp</strong>, basically follow the instruction in this answer: <a href=""https://stackoverflow.com/questions/32879532/stanford-nlp-for-python"">Stanford nlp for python</a>, the code is shown below:</li>
</ul>
<pre><code>import pycorenlp
from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP(&quot;http://localhost:9000&quot;)
text=&quot;noted former cocaine user carrie fisher says donald trump was absolutely on coke makes sense&quot;
res = nlp.annotate(text,properties={'annotators': 'sentiment','outputFormat': 'json','timeout': 1000})
for s in res[&quot;sentences&quot;]:
    print(s[&quot;sentimentValue&quot;], s[&quot;sentiment&quot;])
</code></pre>
<p>and the result is:</p>
<pre><code>3 Positive
</code></pre>
<ul>
<li>For the live demo:</li>
</ul>
<p><a href=""https://i.sstatic.net/jjLc4.png"" rel=""nofollow noreferrer"">screenshot of the live demo result</a></p>
","python, stanford-nlp, sentiment-analysis","<p>The old sentiment demo is probably running older code/older models, so that is why the results would be different. CoreNLP 4.0.0 should return POSITIVE for the entire sentence.</p>
",1,1,147,2020-07-26 03:10:40,https://stackoverflow.com/questions/63095707/sentiment-results-are-different-between-stanford-nlp-python-package-and-the-live
Is there any way to give an input file to Stanza (stanford corenlp client) rather then one piece of text while calling server?,"<p>I have a .csv file consists of Imdb sentiment analysis data-set. Each instance is a paragraph. I am using Stanza <a href=""https://stanfordnlp.github.io/stanza/client_usage.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/client_usage.html</a> for getting parse tree for each instance.</p>
<pre><code>text = &quot;Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.&quot;

with CoreNLPClient(
    annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
    timeout=30000,
    memory='16G') as client:
ann = client.annotate(text)
</code></pre>
<p>Right now, I have to re-run server for every instance and it is taking a lot of time since I have 50k instances.</p>
<pre><code>1
Starting server with command: java -Xmx16G -cp /home/wahab/treeattention/stanford-corenlp- 
4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 1200000 -threads 
5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-a74576b3341f4cac.props 
-preload parse
2
Starting server with command: java -Xmx16G -cp /home/wahab/treeattention/stanford-corenlp- 
4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 1200000 -threads 
5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-d09e0e04e2534ae6.props 
-preload parse
</code></pre>
<p>Is there any way to pass a file or do batching?</p>
","parsing, nlp, stanford-nlp","<p>You should only start the server once. It'd be easiest to load the file in Python, extract each paragraph, and submit the paragraphs. You should pass each paragraph from your IMDB to the <code>annotate()</code> method. The server will handle sentence splitting.</p>
",1,1,913,2020-07-28 13:45:36,https://stackoverflow.com/questions/63135603/is-there-any-way-to-give-an-input-file-to-stanza-stanford-corenlp-client-rathe
"In Stanford CoreNlp, why are not all proper nouns (NNP) also named entities","<p>I use Stanford CoreNlp for Names Entity Recognition (NER). I've noticed that in some cases that it's not 100% which is fine and not surprising. However, even if a, say, single-word named entity is not recognized (i.e., the label is <code>O</code>), it has the tag <code>NNP</code> (proper noun).</p>
<p>For example, given the example sentence &quot;The RestautantName in New York is the best outlet.&quot;, <code>nerTags()</code> yields <code>[O, O, O, LOCATION, LOCATION, O, O, O, O, O]</code> only correctly recognizing &quot;New York&quot;. The parse tree for this sentence looks like</p>
<pre><code>(ROOT
  (S
    (NP
      (NP (DT The) (NNP RestautantName))
      (PP (IN in)
        (NP (NNP New) (NNP York))))
    (VP (VBZ is)
      (NP (DT the) (JJS best) (NN outlet)))
    (. .)))
</code></pre>
<p>so &quot;RestaurantName&quot; is a proper noun (<code>NNP</code>)</p>
<p>When I look up the definition of a proper noun, it sounds very close to a named entity. What's the difference?</p>
","nlp, stanford-nlp, named-entity-recognition","<p>The parser is trained on parse treebank data and the named entity recognizer is trained on separate named entity data for PERSON, LOCATION, ORGANIZATION, MISC.</p>
<p>I would've thought that RestaurantName might get marked as MISC, but if it's not getting tagged it means that there are not really examples like that in the training data for named entities. The key point here is that the parse decisions and named entity decisions are made completely independently of each other by separate models trained on separate data.</p>
",2,1,1043,2020-07-31 04:44:05,https://stackoverflow.com/questions/63185929/in-stanford-corenlp-why-are-not-all-proper-nouns-nnp-also-named-entities
Tensorflow 2 Glove could not broadcast input array Can&#39;t prepare the embedding matrix but not +1,"<p>I get a <code>ValueError: could not broadcast input array from shape (50) into shape (100)</code> preparing embedding matrix I have loaded glove and made the word to vec Found 400000 word vectors.</p>
<p>I did look at a bunch of similar questions but <strong>they all seem to deal with forgetting to add the +1 in the max number words</strong>, I think I have that covered but still have the issue. Any help deeply appreciated.</p>
<pre><code>num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)
</code></pre>
<p>I also tried</p>
<pre><code>num_words = min(MAX_NUM_WORDS, len(word2idx_inputs)) + 1
</code></pre>
<p><a href=""https://stackoverflow.com/questions/56880252/using-pre-trained-word-embeddings-in-a-keras-model"">Using pre-trained word embeddings in a keras model?</a></p>
<p>I also tried this one as well</p>
<p><a href=""https://stackoverflow.com/questions/47215195/keras-word-embeddings-glove-cant-prepare-the-embedding-matrix"">Keras word embeddings Glove: can&#39;t prepare the embedding matrix</a></p>
<p>but also was the +1 issue</p>
<p>FYI: Extreme newbie at this 1st time doing Seq to seq to due to the translating Tagalog into English</p>
<p><strong>The Error that is received</strong></p>
<pre><code>
Filling pre-trained embeddings...

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-16-acf0d8a4c4ca&gt; in &lt;module&gt;
     8     if embedding_vector is not None:
     9       # words not found in embedding index will be all zeros.
---&gt; 10       embedding_matrix[i] = embedding_vector
    11 
    12 # create embedding layer

ValueError: could not broadcast input array from shape (50) into shape (100)

</code></pre>
<p><strong>Code</strong></p>
<pre><code>
# prepare embedding matrix
print('Filling pre-trained embeddings...')
num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word2idx_inputs.items():
 if i &lt; MAX_NUM_WORDS:
   embedding_vector = word2vec.get(word)
   if embedding_vector is not None:
     # words not found in embedding index will be all zeros.
     embedding_matrix[i] = embedding_vector

# create embedding layer
embedding_layer = Embedding(
 num_words,
 EMBEDDING_DIM,
 weights=[embedding_matrix],
 input_length=max_len_input,
 # trainable=True
)

# create targets, since we cannot use sparse
# categorical cross entropy when we have sequences
decoder_targets_one_hot = np.zeros(
 (
   len(input_texts),
   max_len_target,
   num_words_output
 ),
 dtype='float32'
)

# assign the values
for i, d in enumerate(decoder_targets):
 for t, word in enumerate(d):
   if word != 0:
     decoder_targets_one_hot[i, t, word] = 1


</code></pre>
","python, tensorflow, keras, stanford-nlp, word-embedding","<p>check the EMBEDDING_DIM value ,probably pre-trained data have less limit,
as error shows shape(50) into shape(100).
So set EMBEDDING_DIM  =50.</p>
",1,0,581,2020-07-31 14:26:34,https://stackoverflow.com/questions/63193730/tensorflow-2-glove-could-not-broadcast-input-array-cant-prepare-the-embedding-m
Stanford NLP: dependency tree results different between online and offline versions,"<p>I wanted to parse the following example using the Stanford Core NLP suite using the dependency parser:</p>
<pre><code>Call a yellow cab for James on Piccadilly Street in 5 minutes
</code></pre>
<p>I have parsed this sentence using the:</p>
<ul>
<li><a href=""https://nlp.stanford.edu/software/stanford-corenlp-latest.zip"" rel=""nofollow noreferrer"">suite downloaded package</a> from https://stanfordnlp.github.io/CoreNLP/(so i ran it offline, locally, on my computer):</li>
</ul>
<blockquote>
<pre><code>Dependency Parse (enhanced plus plus dependencies):
root(ROOT-0, Call-1)
det(cab-4, a-2)
amod(cab-4, yellow-3)
obj(Call-1, cab-4)
case(James-6, for-5)
nmod:for(cab-4, James-6)
case(Street-9, on-7)
compound(Street-9, Piccadilly-8)
nmod:on(cab-4, Street-9)
case(minutes-12, in-10)
nummod(minutes-12, 5-11)
nmod:in(cab-4, minutes-12)
</code></pre>
</blockquote>
<ul>
<li><a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">online website</a>(so I ran it on the provided website, online):</li>
</ul>
<blockquote>
<pre><code>Universal dependencies, enhanced
root(ROOT-0, Call-1)
det(cab-4, a-2)
amod(cab-4, yellow-3)
obj(Call-1, cab-4)
case(James-6, for-5)
obl:for(Call-1, James-6)
case(Street-9, on-7)
compound(Street-9, Piccadilly-8)
obl:on(Call-1, Street-9)
case(minutes-12, in-10)
nummod(minutes-12, 5-11)
nmod:in(Street-9, minutes-12)
</code></pre>
</blockquote>
<p>The online generated result is correct while the other one is not.</p>
<p>Can anybody help me understand why are the results different considering also that the online version is as old as 2016 and the downloaded version is since 2020? I would like to have the same results using the downloaded version as with the online version.</p>
<p>Can anybody help me understand the difference?</p>
<p><strong>P.S.</strong></p>
<p>I have also tried using the <code>stanford-corenlp-full-2016-10-31</code> version of core nlp - received the same result.</p>
<p>I have also copied the <a href=""http://nlp.stanford.edu/software/stanford-corenlp-4.1.0-models-english.jar"" rel=""nofollow noreferrer"">English model</a> inside the <code>stanford-corenlp-4.1.0</code> folder with no difference.</p>
",stanford-nlp,"<p>I think the online version is first constituency parsing the sentence and then converting to a dependency parse. The other example might be from the neural dependency parser.</p>
<p>So if you try just using the <code>parse</code> annotator (and don't use the <code>depparse</code> annotator), you should get the results you want.</p>
",1,0,517,2020-08-03 16:01:05,https://stackoverflow.com/questions/63232886/stanford-nlp-dependency-tree-results-different-between-online-and-offline-versi
Best way to detect person name / entity name from French text,"<p>I am trying to detect person name or company/institute name from French texts. I have tried the following and the results are not good at all.</p>
<pre><code>import spacy

# or any of the other two models fr_core_news_sm, fr_core_news_lg
nlp = spacy.load(&quot;fr_core_news_md&quot;)  

text =&quot;&quot;&quot;Tous les vents,
Balayent les mots de coeur
Moi, j'suis comme le vent:
L'esprit à mille à l'heure,
Je juge sans doute trop vite
C'est ok, tant pis
C'est juste là, je m'agite
Je grandis, l'amour aussi
C'est au gré du vent
Que j'aime vagabonder,
Moi, je suis comme le vent
J'embrasse toute une armée
De rêves et de bleuets,
Me plonger dedans
Je sais ce que je sais 
Rapport de: Andre STE-GERMAINE,
&quot;&quot;&quot;

doc_fr = nlp_fr(text_fr)

</code></pre>
<p>Well, it marked</p>
<ol>
<li>&quot;Balayent as PERSON which should not,</li>
<li>Rapport de: Andre STE-GERMAINE', 'MISC' which should be PERSON</li>
</ol>
<p>I am not sure if NLTK can help on this.</p>
<p>Is there another better tool to help with this task?</p>
<p>Thanks.</p>
","python-3.x, nlp, nltk, stanford-nlp, spacy","<p>The model you have chosen is trained on selected data that might not represent your own dataset. You could finetune the model by supplying more annotated data to create a better overlap.</p>
<p>Reformatting the text data should also increase the performance of the model as the text supplied is a little bit hard to understand by a Spacy model. Sentences are broken into pieces instead of full readable text, words are capitalized even though they shouldn't be.</p>
<p>I don't speak French, but have reformatted your text a little bit and seem already to get better results.</p>
<pre><code>import spacy
nlp = spacy.load(&quot;fr_core_news_md&quot;)
text =&quot;&quot;&quot;Tous les vents, balayent les mots de coeur.
Moi, j'suis comme le vent: L'esprit à mille à l'heure, je juge sans doute trop vite.
C'est ok, tant pis.
C'est juste là, je m'agite.
Je grandis, l'amour aussi.
C'est au gré du vent.
Que j'aime vagabonder, moi, je suis comme le vent.
J'embrasse toute une armée de rêves et de bleuets,
Me plonger dedans je sais ce que je sais.
Rapport de: Andre STE-GERMAINE,
&quot;&quot;&quot;
doc = nlp(text)
for ent in doc.ents:
    print(ent.text, '-',ent.label_)
</code></pre>
<p>This results in:</p>
<pre><code>Andre STE - PER
</code></pre>
<p>I explain in the comments to this post why the Person isn't fully tagged, but let's do it here again. The last sentence of this text isn't really a sentence on which the Spacy NER can really perform accurate as it isn't a real sentence which has been trained on. The name is also fully capitalized, which is not normal for anybody's surname which also confused the algorithm even more.</p>
<p>In conclusion, supply it with data that it is trained on, reformat your text, this would increase the performance significantly.</p>
",2,2,2365,2020-08-03 21:17:18,https://stackoverflow.com/questions/63237076/best-way-to-detect-person-name-entity-name-from-french-text
RuntimeWarning: Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned,"<p>I am parsing (getting constituency tree) IMDB dataset using CoreNLP server with Shift-Reduce parser. For few sentences it gives a warning which is:</p>
<p><code>RuntimeWarning: Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned. warnings.warn(&quot;Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned.&quot;, \</code></p>
<p>I really don't have any idea why it is giving this error.</p>
","parsing, nlp, stanford-nlp","<p>Update stanford-corenlp-4.0.0 to 4.1.0 (which is latest at the time of posting) and it worked.</p>
",1,0,47,2020-08-07 13:08:28,https://stackoverflow.com/questions/63302527/runtimewarning-failed-to-decode-a-serialized-output-from-corenlp-server-an-inc
How can I tag or give a document of text a topic?,"<p>I have set of documents and corresponding set of tags for those documents</p>
<p>ex.</p>
<p>Document-&quot;Learned Counsel appearing for the Appellants however points out that in the..etc etc&quot;</p>
<p>Tags - &quot;Compensation, Fundamental Right&quot;</p>
<p>Now I have multiple documents with their corresponding tags and I another test set of data without any tags what NLP techniques do I use to give these documents tag? Do I use text classification or topic modeling can someone please guide or suggest some ideas.</p>
","python, nlp, stanford-nlp","<p>you can use two approaches:</p>
<p>1- rule based (extract common words in each tag and classify documents with them)</p>
<p>2- machine learning</p>
<p>if you have a large scale training data you can use machine learning to classify documents:</p>
<p>you can use this approaches:</p>
<p><a href=""https://arxiv.org/abs/1904.08398"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1904.08398</a></p>
<p><a href=""https://medium.com/@armandj.olivares/using-bert-for-classifying-documents-with-long-texts-5c3e7b04573d"" rel=""nofollow noreferrer"">https://medium.com/@armandj.olivares/using-bert-for-classifying-documents-with-long-texts-5c3e7b04573d</a></p>
",2,0,746,2020-08-07 13:38:03,https://stackoverflow.com/questions/63303007/how-can-i-tag-or-give-a-document-of-text-a-topic
Resolving &#39;Resource stopwords not found&#39; in non-downloadable environment,"<p>I work at an environment where I cannot download modules. I need to work with existing modules on my computer.</p>
<p>I can use <code>nltk</code> module, but the installed version does not contain <code>stopwords</code>, so I cannot process the  corpus in my skip-gram. So, I invariably encounter this error message:</p>
<pre><code>Resource stopwords not found.
</code></pre>
<p>Is there any way to clone, copy &amp; paste, or create a <code>stopwords</code> myself  on jupyter notebook?</p>
","python, nlp, nltk, stanford-nlp, word2vec","<p>You can make a file or create a <code>stopwords</code> variable by copying the contents of <code>nltk.corpus.stopwords</code> which is simply a set of words.</p>
<pre><code>from nltk.corpus import stopwords

words = set(stopwords.words('english'))
print(words)
</code></pre>
<p>Output:</p>
<pre><code>{'does', &quot;it's&quot;, 'having', 've', &quot;hadn't&quot;, &quot;isn't&quot;, 'this', 'him', 'ours', &quot;mustn't&quot;, 'are', 'if', 'we', 'myself', 'these', 'a', 'not', 'what', 'weren', 'down', 'have', &quot;couldn't&quot;, 'after', 'again', 'most', 'at', 'such', 'by', 'just', 'd', 'i', &quot;you'll&quot;, 'should', &quot;haven't&quot;, 'as', 'do', 'from', 'other', 'than', 'which', 'were', 'mustn', 'yours', &quot;aren't&quot;, 'now', 'didn', &quot;won't&quot;, 'be', 'itself', 'in', 'all', 'once', 'few', 'through', 'its', &quot;didn't&quot;, 'needn', 'being', 'them', &quot;you'd&quot;, 'or', 'it', 'to', 'your', &quot;shan't&quot;, 'too', 'our', 'ourselves', 'his', 'am', 'below', 'isn', 'ma', 'further', 'yourself', 'out', 'up', &quot;don't&quot;, 'with', 'but', 'where', 'then', 'whom', 'each', 'hasn', 'very', 'more', 'he', 'won', 't', 'doing', 'until', 'doesn', 'herself', 'who', 'own', &quot;wasn't&quot;, 'those', 'nor', &quot;you're&quot;, 'shan', 'himself', 'll', 'that', 'both', 'shouldn', &quot;you've&quot;, 'over', 'an', 'when', 'because', 'ain', 'had', 'haven', 'themselves', 'same', 'under', 'no', &quot;mightn't&quot;, 'couldn', 'you', 'while', 'and', 'during', 'yourselves', 'my', &quot;shouldn't&quot;, &quot;wouldn't&quot;, 'off', 'she', 'me', 'wasn', 'above', 'y', 'will', 'been', 'mightn', 'was', 'before', &quot;needn't&quot;, 'so', 'on', 's', 'some', 'their', 'can', 'how', 'is', 'hadn', 'wouldn', 'here', 'why', 'her', 'only', 'the', 'o', &quot;should've&quot;, 'hers', &quot;doesn't&quot;, 'don', 'against', &quot;weren't&quot;, 'about', 'm', &quot;she's&quot;, 'of', 'into', 're', 'they', &quot;that'll&quot;, 'aren', &quot;hasn't&quot;, 'theirs', 'between', 'there', 'did', 'has', 'for', 'any'}
</code></pre>
",1,1,363,2020-08-12 05:05:28,https://stackoverflow.com/questions/63370108/resolving-resource-stopwords-not-found-in-non-downloadable-environment
How to get feature names for a glove vectors,"<p>Countvectorizer has feature names, like this.</p>
<pre><code>vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=15000)
vectorizer.fit(X_train['essay'].values) # fit has to happen only on train data

X_train_essay_bow = vectorizer.transform(X_train['essay'].values)
feature_names= vectorizer.get_feature_names()
</code></pre>
<p>What would be the feature names for a glove vector?</p>
<p>How to get those feature names?</p>
<pre><code>with open('glove_vectors', 'rb') as f:
    model = pickle.load(f)
    glove_words =  set(model.keys())
</code></pre>
<p>I have the glove vector file of 300 dimensions like the above shown.</p>
<p>What would be the name of the 300 dimensions of the glove vectors?</p>
","python, vector, nlp, stanford-nlp, word-embedding","<p>There is no name for the Glove features. The countvectorizer counts the occurrences of each token in each sentence. So the features have easily understandable names. The feature &quot;cat&quot; is the count in each sentence of the token &quot;cat&quot;.</p>
<p>For Glove Vectors, the strategy is totally different and there is no equivalent representation of the features. Glove vectors are embeddings of words in an abstract N-dimensional space.</p>
<p>The Glove vector for a token comes from passing the token as an input into a trained neural network, and taking the activations of an auto-encoding layer in the middle.</p>
<p>If you've ever trained a deep neural network, imagine choosing some hidden layer within. What is the feature_name for each node in that hidden layer? It's a meaningless question because the nodes aren't features; they exist to pass the activation to the next layer. The same is true of Glove vector features; they are the activation values of a hidden layer in a network.</p>
",2,2,422,2020-08-21 10:13:04,https://stackoverflow.com/questions/63520893/how-to-get-feature-names-for-a-glove-vectors
How to optimize memory footprint of Stanza models,"<p>I'm using Stanza to get tokens, lemmas and tags from documents in multiple languages for the purposes of a language learning app. This means that I need to store and load many Stanza (default) models for different languages.</p>
<p>My main problem right now is that if I want to load all those models the memory requirement is too much for my resources. I currently deploy a web API running Stanza NLP on AWS. <em>I want to keep my infrastructure costs at a minimum.</em></p>
<p>One possible solution is to load one model at a time when I need to run my script. I guess that means there will be some extra overhead each time in order to load the model in memory.</p>
<p>Another thing I tried is just to use the processors that I really need which decreases the memory footprint but not by that much.</p>
<p>I tried looking at open and closed issues on Github and Google but didn't find much.</p>
<p>What other possible solutions are out there?</p>
","nlp, stanford-nlp, stanza","<p>The bottom line is a model for a language has to be in memory during execution, so by some means or another you need to make the model smaller or tolerate storing models on disk.  I can offer some suggestions to make the models smaller, though be warned that making your model smaller will probably result in poorer accuracy.</p>
<p>You could examine the percentage breakdown of language requests, and store commonly requested languages in memory and only go to disk for rarer language requests.</p>
<p>The most immediate impact strategy for reducing model size is to shrink the vocabulary size. It is possible you could cut the vocabulary even smaller and still get similar accuracy. We have done some optimization on this front, but there may be more opportunity to cut model size.</p>
<p>You could experiment with smaller model size and word embeddings and may only get a small accuracy drop, we haven't really aggressively experimented with different model sizes to see how much accuracy you lose. This would mean retraining the model and just setting the embedding size and model size parameters smaller.</p>
<p>I don't know a lot about this, but there is a strategy of tagging a bunch of data with your big accurate model, and then training a smaller model to mimic the big model. I believe this is called &quot;knowledge distillation&quot;.</p>
<p>In a similar direction, you could tag a bunch of data with Stanza, and then train a CoreNLP model (which I think would have a smaller memory footprint).</p>
<p>In summary, I think the easiest thing to do would be to retrain a model with a smaller vocabulary size. We I think it currently has 250,000 words, and cutting to 10,000 or 50,000 will reduce model size, but may not affect accuracy too badly.</p>
<p>Unfortunately I don't think there is a magical option you can select that will just solve this issue, you will have to retrain models and see what kind of accuracy you are willing to sacrifice for a lower memory footprint.</p>
",1,1,885,2020-08-28 14:18:00,https://stackoverflow.com/questions/63635616/how-to-optimize-memory-footprint-of-stanza-models
How do I pretty print or visualise an object of class &#39;CoreNLP_pb2.ParseTree&#39; in Python/Jupyter Notebook?,"<p>I'm using Stanza's CoreNLP client in a Jupyter notebook to do constituency parsing on a string. The final output came in the form of an object of class 'CoreNLP_pb2.ParseTree'.</p>
<pre><code>&gt;&gt;&gt; print type(result)
&lt;class 'CoreNLP_pb2.ParseTree'&gt;
</code></pre>
<p>How should I print this in a visible way? When I directly call <code>print(result)</code>, there is no output.</p>
","jupyter-notebook, stanford-nlp","<p>You can conver <code>CoreNLP_pb2.ParseTree</code> into <code>nltk.tree.Tree</code> and the call <code>pretty_print()</code> to print the parse tree in a visible way.</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.tree import Tree
def convert_parse_tree_to_nltk_tree(parse_tree):
    return Tree(parse_tree.value, [get_nltk_tree(child) for child in parse_tree.child]) if parse_tree.child else parse_tree.value

convert_parse_tree_to_nltk_tree(constituency_parse).pretty_print()
</code></pre>
<p>The result is as follows:</p>
<pre><code>                      ROOT                    
                       |                       
                       S                      
        _______________|____________________   
       |                    VP              | 
       |            ________|___            |  
       NP          |            NP          | 
   ____|_____      |    ________|_____      |  
 NNP        NNP   VBZ  DT       JJ    NN    .
  |          |     |   |        |     |     |  
Chris     Manning  is  a       nice person  . 
</code></pre>
",2,1,712,2020-08-30 08:39:36,https://stackoverflow.com/questions/63655575/how-do-i-pretty-print-or-visualise-an-object-of-class-corenlp-pb2-parsetree-in
Loading pretrained glove on production with flask and Gunicorn,"<p>I have a model that requires some preprocessing using Glove from Stanford. From my experience it takes the at least 20-30 seconds until the Glove is loaded by this code:</p>
<pre><code>glove_pd = pd.read_csv(embed_path+'/glove.6B.300d.txt', sep=&quot; &quot;, quoting=3, header=None, index_col=0)
glove = {key: val.values for key, val in glove_pd.T.items()}
</code></pre>
<p>My question is what is the best practice to handle this in a production app? As far as I can understand is that everytime that I restart the server I need to wait 30 seconds until the endpoint is ready.</p>
<p>Also, <a href=""https://medium.com/faun/deploy-flask-app-with-nginx-using-gunicorn-7fda4f50066a"" rel=""nofollow noreferrer"">I have read</a> that when using Gunicorn, it is recommended to run with <code>workers&gt;1</code>, something like this:</p>
<pre><code>ExecStart=/path/to/gunicorn --workers 3 --bind unix:app.sock -m 007 wsgi:app
</code></pre>
<p>Does it mean that each instance of gunicorn requires to load the same glove to memory? This means that the server resources will be quite large, let me know if I am correct here.</p>
<p>Bottom line my question is what are the recommended methods for hosting a model that requires an pretrained embedding (glove/word2vec/fasttext) on a production server</p>
","python, stanford-nlp, word2vec, fasttext","<p>At one level, if you need it in memory, and that's how long it takes to read the gigabyte-plus from disk into useful RAM structures, then yes - that's how long it takes before a process is ready to use that data. But there's room for optimizations!</p>
<p>For example, reading this as 1st a Pandas dataframe, then converting it to a Python dict, involves both more steps &amp; more RAM than other options. (At the momentary peak, when both <code>glove_pd</code> and <code>glove</code> are fully constructed &amp; referenced, you'll have two full copies in memory – and neither is as compact as would be ideal, which could trigger other slowdowns, especially if the bloat triggers using any virtual-memory at all.)</p>
<p>And as you fear, if 3 <code>gunicorn</code> workers each run the same loading code, 3 separate copies of the same data will be loaded – but there's a way to avoid this, below.</p>
<p>I'd suggest 1st loading the vectors into a utility class for accessing word-vectors, like the <code>KeyedVectors</code> interface in the Gensim library. It'll store all the vectors in one compact <code>numpy</code> matrix, with a dict-like interface that still returns one <code>numpy</code> <code>ndarray</code> for as each individual vector.</p>
<p>For example, you can convert GLoVe text-format vectors to a slightly-different interchange format (with an extra header line, that Gensim calls <code>word2vec_format</code> after its use by the original Google <code>word2vec.c</code> code). In <code>gensim-3.8.3</code> (current release as of August 2020) you can do:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.scripts.glove2word2vec import glove2word2vec
glove2word2vec('glove.6B.300d.txt', 'glove.6B.300d.w2vtxt')
</code></pre>
<p>Then, the utility-class <code>KeyedVectors</code> can load them like so:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models import KeyedVectors
glove_kv = KeyedVectors.load_word2vec_format('glove.6B.300d.w2vtxt', binary=False)
</code></pre>
<p>(Starting in the future <code>gensim-4.0.0</code> release, it should be possible to skip conversion &amp; just use the new <code>no_header</code> argument to read a GLoVe text file directly: <code>glove_kv = KeyedVectors.load_word2vec_format('glove.6B.300d.w2vtxt', binary=False, no_header=True)</code>. But this headerless-format will be a little slower, as it requires two passes over the file - the 1st to learn the full size.)</p>
<p>Loading just once into <code>KeyedVectors</code> should already be faster &amp; more-compact than your original generic two-step process. And, lookups that are analogous to what you were doing on the prior dict will be available on the <code>glove_kv</code> instance. (Also, there are many other convenience operations, like ranked <code>.most_similar()</code> lookup, that utilize efficient array library functions for speed.)</p>
<p>You can take another step, though, to minimize the parsing-on-load, and to defer loading unneeded ranges of the full set of vectors, and automatically reuse raw array data between processes.</p>
<p>That extra step is to re-save the vectors using the Gensim instance's <code>.save()</code> function, which will dump the raw vectors into a separate dense file that's suitable for memory-mapping upon the next load. So first:</p>
<pre class=""lang-py prettyprint-override""><code>glove_kv.save('glove.6B.300d.gs')
</code></pre>
<p>This will create <strong>more than one file</strong> which must be kept together if relocated – but the <code>.npy</code> file(s) saved will be the exact minimal format ready for memory-mapping .</p>
<p>Then, when needed later, load as:</p>
<pre class=""lang-py prettyprint-override""><code>glove_kv = KeyedVectors.load('glove.6B.300d.gs', mmap='r')
</code></pre>
<p>The <code>mmap</code> argument uses underlying OS mechanisms to simply map the relevant matrix address-space to the (read-only) file(s) on disk, so that the initial 'load' is effectively instant, but any attempt to access ranges of the matrix will use virtual-memory to page-in the right ranges of the file. It thus eliminates any scanning-for-delimiters &amp; defers IO until absolutely needed. (And if there are any ranges you never access? They'll never be loaded.)</p>
<p>The other big benefit of memory-mapping is that if multiple processes each read-only memory-map the same on-disk files, the OS is smart enough to let them share any common paged-in ranges. So with, say, 3 totally-separate OS processes that each mmap the same file, you get 3X RAM savings.</p>
<p>(If after all these changes, the lag upon restarting server processes is still an issue – perhaps because the server processes crash or otherwise need restarting often – you could even consider using some <em>other</em> long-lived, stable process to initially mmap the vectors. Then, even the crash of all server processes wouldn't cause the OS to lose any paged-in ranges of the file, and the restart of the server processes might find some or all of the relevant data already in RAM. But the complication of this extra role may be superfluous once the other optimizations are in place.)</p>
<p>One extra caveat: if you start using <code>KeyedVectors</code> methods like <code>.most_similar()</code> that can (up through <code>gensim-3.8.3</code>) trigger the creation of a full-size cache of the unit-length-normalized word-vectors, you could lose the mmap benefits unless you take some extra steps to short-circuit that process. See more details in prior answer: <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">How to speed up Gensim Word2vec model load time?</a></p>
",1,1,672,2020-08-31 07:50:56,https://stackoverflow.com/questions/63666823/loading-pretrained-glove-on-production-with-flask-and-gunicorn
How can I use StanfordNLP tools (POSTagger and Parser) with an already Tokenized file?,"<p>I have a tokenized file and I would like to use StanfordNLP to annotate it with POS and dependency parsing tags.
I am using a Python script with the following configuration:</p>
<pre><code>config = {
'processors': 'pos,lemma,depparse',
'lang': 'de',
'pos_model_path': './de_gsd_models/de_gsd_tagger.pt',
'pos_pretrain_path': './de_gsd_models/de_gsd.pretrain.pt',
'lemma_model_path': './de_gsd_models/de_gsd_lemmatizer.pt',
'depparse_model_path': './de_gsd_models/de_gsd_parser.pt',
'depparse_pretrain_path': './de_gsd_models/de_gsd.pretrain.pt}'
</code></pre>
<p><code>nlp = stanfordnlp.Pipeline(**config)</code></p>
<p><code>doc = nlp(text)</code></p>
<p>However, I receive the following message:</p>
<ul>
<li>missing: {'tokenize'}
The processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.</li>
</ul>
<p>Is it possible to skip the tokenization step using a Python script?</p>
<p>Thanks in advance!</p>
","python, stanford-nlp, pipeline","<p>You need to include the <code>tokenize</code> processor and include the property <code>tokenize_pretokenized</code> set to <code>True</code>. This will assume the text is tokenized on whitespace and sentence split by newline. You can also past a list of lists of strings, each list representing a sentence, and the entries being the tokens.</p>
<p>This is explained here:</p>
<p><a href=""https://stanfordnlp.github.io/stanza/tokenize.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/tokenize.html</a></p>
",0,0,266,2020-08-31 14:47:20,https://stackoverflow.com/questions/63673038/how-can-i-use-stanfordnlp-tools-postagger-and-parser-with-an-already-tokenized
IllegalArgumentException: PTBLexer: Invalid options key in constructor: asciiQuotes Stanford NLP,"<p>I'm trying to test the Hello word of <a href=""https://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow noreferrer"">Stanford POS tagger</a> API in Java (I used the same .jar in python and it worked well) on french sentences.
Here is my code</p>
<pre><code>public class TextPreprocessor {

    private static MaxentTagger tagger=new MaxentTagger(&quot;../stanford-tagger-4.1.0/stanford-postagger-full-2020-08-06/models/french-ud.tagger&quot;);

    public static void main(String[] args) {
        
        String taggedString = tagger.tagString(&quot;Salut à tous, je suis coincé&quot;);
        System.out.println(taggedString);
    }
}

</code></pre>
<p>But I get the following exception:</p>
<pre><code>Loading POS tagger from C:/Users/_Nprime496_/Downloads/Compressed/stanford-tagger-4.1.0/stanford-postagger-full-2020-08-06/models/french-ud.tagger ... done [0.3 sec].
Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: PTBLexer: Invalid options key in constructor: asciiQuotes
    at edu.stanford.nlp.process.PTBLexer.&lt;init&gt;(PTBLexer.java)
    at edu.stanford.nlp.process.PTBTokenizer.&lt;init&gt;(PTBTokenizer.java:285)
    at edu.stanford.nlp.process.PTBTokenizer$PTBTokenizerFactory.getTokenizer(PTBTokenizer.java:698)
    at edu.stanford.nlp.process.DocumentPreprocessor$PlainTextIterator.&lt;init&gt;(DocumentPreprocessor.java:271)
    at edu.stanford.nlp.process.DocumentPreprocessor.iterator(DocumentPreprocessor.java:226)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.tokenizeText(MaxentTagger.java:1148)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger$TaggerWrapper.apply(MaxentTagger.java:1332)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.tagString(MaxentTagger.java:999)
    at modules.generation.preprocessing.TextPreprocessor.main(TextPreprocessor.java:19)
</code></pre>
<p>Can you help me?</p>
","java, stanford-nlp, pos-tagger, french","<p>You can use this code and the full CoreNLP package:</p>
<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;


public class PipelineExample {

  public static String text = &quot;Paris est la capitale de la France.&quot;;

  public static void main(String[] args) {
    // set up pipeline properties
    Properties props = StringUtils.argsToProperties(&quot;-props&quot;, &quot;french&quot;);
    // set the list of annotators to run
    props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,mwt,pos&quot;);
    // build pipeline
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // create a document object
    CoreDocument document = pipeline.processToCoreDocument(text);
    // display tokens
    for (CoreLabel tok : document.tokens()) {
      System.out.println(String.format(&quot;%s\t%s&quot;, tok.word(), tok.tag()));
    }
  }

}
</code></pre>
<p>You can download CoreNLP here: <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/</a></p>
<p>Make sure to download the latest French models.</p>
<p>I am not sure why your example with the standalone tagger does not work. What jars were you using?</p>
",1,0,322,2020-09-14 16:44:08,https://stackoverflow.com/questions/63888551/illegalargumentexception-ptblexer-invalid-options-key-in-constructor-asciiquo
Extracting multi-word named entities using NLTK Stanford NER in Python,"<p>I am trying to extract named entities from text using Stanford-NER. I have read all related threads regarding chunking and did not find anything to solve the problem I am having.</p>
<p>Input:</p>
<blockquote>
<p>The united nations is holding a meeting in the united states of America.</p>
</blockquote>
<p>Expected Output:</p>
<blockquote>
<p>united nations/organization</p>
</blockquote>
<blockquote>
<p>united states of America/location</p>
</blockquote>
<p>I was able to get this output, but it doesn't combine tokens for multi-work named entities:</p>
<pre><code>[('The', 'O'), ('united', 'ORGANIZATION'), ('nations', 'ORGANIZATION'), ('is', 'O'), ('holding', 'O'), ('a', 'O'), ('meeting', 'O'), ('in', 'O'), ('the', 'O'), ('united', 'LOCATION'), ('states', 'LOCATION'), ('of', 'LOCATION'), ('America', 'LOCATION'), ('.', 'O')]
</code></pre>
<p>or in a tree format:</p>
<pre><code>(S
  The/O
  united/ORGANIZATION
  nations/ORGANIZATION
  is/O
  holding/O
  a/O
  meeting/O
  in/O
  the/O
  united/LOCATION
  states/LOCATION
  of/LOCATION
  America/LOCATION
  ./O)
</code></pre>
<p>I am looking for this output:</p>
<pre><code>[('The', 'O'), ('united nations', 'ORGANIZATION'), ('is', 'O'), ('holding', 'O'), ('a', 'O'), ('meeting', 'O'), ('in', 'O'), ('the', 'O'), ('united states of America', 'LOCATION'), ('.', 'O')]
</code></pre>
<p>When I tried some of the code I found in other threads to join named entities in the tree format, it returned an empty list.</p>
<pre><code>import nltk
from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize
import os
java_path = &quot;C:\Program Files (x86)\Java\jre1.8.0_251/java.exe&quot;
os.environ['JAVAHOME'] = java_path

st = StanfordNERTagger(r'stanford-ner-4.0.0/stanford-ner-4.0.0/classifiers/english.all.3class.distsim.crf.ser.gz',
                       r'stanford-ner-4.0.0/stanford-ner-4.0.0/stanford-ner.jar',
                       encoding='utf-8')

text = 'The united nations is holding a meeting in the united states of America.'
tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)
namedEnt = nltk.ne_chunk(classified_text, binary = True)

#this line makes the tree return an empty list
np = [' '.join([y[0] for y in x.leaves()]) for x in namedEnt.subtrees() if x.label() == &quot;NE&quot;]

print(np)

print(classified_text)
</code></pre>
","python-3.x, nltk, stanford-nlp, named-entity-recognition","<p>The StanfordNERTagger in nltk doesn't retain information on the boundaries of named entities. If you try to parse the output of the tagger, there is no way to tell whether two consecutive nouns with the same tag are part of the same entity or whether they are distinct.</p>
<p>Alternatively, <a href=""https://stanfordnlp.github.io/CoreNLP/other-languages.html#python"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/other-languages.html#python</a> indicates that the Stanford team is actively developing a python package called Stanza which uses the Stanford CoreNLP. It is slow, but really easy to use.</p>
<p>$ pip3 install stanza</p>
<pre><code>&gt;&gt;&gt; import stanza
&gt;&gt;&gt; stanza.download ('en')
&gt;&gt;&gt; nlp = stanza.Pipeline ('en')
&gt;&gt;&gt; results = nlp (&lt;insert your text string here&gt;)
</code></pre>
<p>The chunked entities are in <code>results.ents</code>.</p>
",1,0,1750,2020-09-19 06:31:52,https://stackoverflow.com/questions/63965957/extracting-multi-word-named-entities-using-nltk-stanford-ner-in-python
Pattern matching with tregex in Stanzas Corenlp implementation doesn&#39;t seem to finde the right subtrees,"<p>I am relatively new to NLP and at the moment I'm trying to extract different phrase scructures in german texts. For that I'm using the Stanford corenlp implementation of stanza with the tregex feature for pattern machting in trees.</p>
<p>So far I didn't have any problem an I was able to match simple patterns like &quot;NPs&quot; or &quot;S &gt; CS&quot;.
No I'm trying to match S nodes that are immediately dominated either by ROOT or by a CS node that is immediately dominated by ROOT. For that im using the pattern &quot;S &gt; (CS &gt; TOP) | &gt; TOP&quot;. But it seems that it doesn't work properly. I'm using the following code:</p>
<pre><code>text = &quot;Peter kommt und Paul geht.&quot;    
def linguistic_units(_client, _text, _pattern):
        matches = _client.tregex(_text,_pattern)
        list = matches['sentences']
        print('+++++Tree++++') 
        print(list[0])
        for sentence in matches['sentences']:
            for match_id in sentence:
                print(sentence[match_id]['spanString'])
        return count_units



with CoreNLPClient(properties='./corenlp/StanfordCoreNLP-german.properties', 
                   annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'parse', 'depparse', 'coref'],
                   timeout=300000, 
                   be_quiet=True,
                   endpoint='http://localhost:9001', 
                   memory='16G') as client:

      result = linguistic_units(client, text, 'S &gt; (CS &gt; ROOT) | &gt; ROOT'
      print(result)
</code></pre>
<p>In the example with the text &quot;Peter kommt und Paul geht&quot; the pattern I'm using should match the two phrases &quot;Peter kommt&quot; and &quot;Paul geht&quot;, but it doesn't work.
Afterwards I had a look at the tree itselfe and the output of the parser was the following:</p>
<pre><code>constituency parse of first sentence
child {
  child {
    child {
      child {
        child {
          value: &quot;Peter&quot;
        }
        value: &quot;PROPN&quot;
      }
      child {
        child {
          value: &quot;kommt&quot;
        }
        value: &quot;VERB&quot;
      }
      value: &quot;S&quot;
    }
    child {
      child {
        value: &quot;und&quot;
      }
      value: &quot;CCONJ&quot;
    }
    child {
      child {
        child {
          value: &quot;Paul&quot;
        }
        value: &quot;PROPN&quot;
      }
      child {
        child {
          value: &quot;geht&quot;
        }
        value: &quot;VERB&quot;
      }
      value: &quot;S&quot;
    }
    value: &quot;CS&quot;
  }
  child {
    child {
      value: &quot;.&quot;
    }
    value: &quot;PUNCT&quot;
  }
  value: &quot;NUR&quot;
}
value: &quot;ROOT&quot;
score: 5466.83349609375
</code></pre>
<p>I now suspect that this is due to the ROOT node, since it is the last node of the tree.  Should the ROOT node not be at the beginning of the tree?
Does anyone know what I am doing wrong?</p>
",stanford-nlp,"<p>A few comments:</p>
<p>1.) Assuming you are using a recent version of CoreNLP (4.0.0+), you need to use the mwt annotator with German. So your annotators list should be <code>tokenize,ssplit,mwt,pos,parse</code></p>
<p>2.) Here is your sentence in PTB for clarity:</p>
<pre><code>(ROOT
  (NUR
    (CS
      (S (PROPN Peter) (VERB kommt))
      (CCONJ und)
      (S (PROPN Paul) (VERB geht)))))
</code></pre>
<p>As you can see the ROOT is the root node of the tree, so your pattern would not match in this sentence. I personally find the PTB format easier to see the tree structure and for writing Tregex patterns off of. You can get that via the json or text output formats (instead of the serialized object). In the client request set <code>output_format=&quot;text&quot;</code></p>
<p>3.) Here is the latest documentation on using the Stanza client: <a href=""https://stanfordnlp.github.io/stanza/client_properties.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/client_properties.html</a></p>
",0,0,524,2020-09-22 19:23:52,https://stackoverflow.com/questions/64016461/pattern-matching-with-tregex-in-stanzas-corenlp-implementation-doesnt-seem-to-f
How is the output of glove2word2vec() different from keyed_vectors.save(),"<p>I am new to NLP and I am running into this issue that I do not understand at all:</p>
<p>I have a text file with gloVe vectors.
I converted it to Word2Vec using</p>
<pre><code>glove2word2vec(TXT_FILE_PATH, KV_FILE_PATH)
</code></pre>
<p>this creates a KV file in my path which can then be loaded using</p>
<pre><code>word_vectors = KeyedVectors.load_word2vec_format(KV_FILE_PATH, binary=False)
</code></pre>
<p>I then save it using</p>
<pre><code>word_vectors.save(KV_FILE_PATH)
</code></pre>
<p>But when I now try to use the new KV file in intersect_word2vec_format it gives me an encoding error</p>
<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-11-d975bb14af37&gt; in &lt;module&gt;
      6 
      7 print(&quot;Intersect with pre-trained model...&quot;)
----&gt; 8 model.intersect_word2vec_format(KV_FILE_PATH, binary=False)
      9 
     10 print(&quot;Train custom word2vec model...&quot;)

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/word2vec.py in intersect_word2vec_format(self, fname, lockf, binary, encoding, unicode_errors)
    890         logger.info(&quot;loading projection weights from %s&quot;, fname)
    891         with utils.open(fname, 'rb') as fin:
--&gt; 892             header = utils.to_unicode(fin.readline(), encoding=encoding)
    893             vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    894             if not vector_size == self.wv.vector_size:

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/utils.py in any2unicode(text, encoding, errors)
    366     if isinstance(text, unicode):
    367         return text
--&gt; 368     return unicode(text, encoding, errors=errors)
    369 
    370 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
","python, nlp, stanford-nlp, gensim, word2vec","<p>The <code>.save()</code> method saves a model in Gensim's native format - which is primarily Python pickling, with large arrays as separate files (which must be kept alongside the main save file).</p>
<p>That format is not the same as the <code>word2vec_format</code> that can be loaded by <code>load_word2vec_format()</code> or <code>intersect_word2vec_format()</code>.</p>
<p>If you want to save a set of vectors into the <code>word2vec_format</code>, use the method <code>.save_word2vec_format()</code>, not plain <code>.save()</code>.</p>
",1,0,289,2020-09-24 04:15:04,https://stackoverflow.com/questions/64039454/how-is-the-output-of-glove2word2vec-different-from-keyed-vectors-save
how to use Nlp Pos Tagging in other langauge like sindhi / urdu,"<p>I am working on a research paper on pos tagging in NLP but my question is that how to implement the pos tagging in another local language plz help me thank you.</p>
","python, nlp, python-requests, stanford-nlp","<p>It depends on the POS-Tagger you are using. Usually a (probabilistic) tagger has two language-specific components: a language model and a dictionary.</p>
<p>The dictionary contains all words with their possible tags, annotated by frequency. This can be created and edited manually, or derived from training data. If your language has a rich morphology, you might want to use a morphological analyser to support this, or you could simply have all inflected forms as dictionary entries in their own right.</p>
<p>The language model contains sequences of tags and their frequencies, usually trigrams (sequences of three items). It is extracted from training data, and reflects grammatical constraints on word class distribution.</p>
<p>So in order to adapt an existing tagger for a new language there are two main steps:</p>
<ol>
<li><p>create a tag set for your language. While there is some overlap between tag sets for different languages (they usually all have nouns or verbs), you might want specific markers for cases or tenses, as they can help in disambiguation.</p>
</li>
<li><p>annotate training data. You need some texts to generate the language model (and possibly also the dictionary). This data you feed into the training algorithm to produce the language-specific resource files.</p>
</li>
</ol>
<p>Annotating by hand is fairly tedious, but you can use an iterative process: annotate a smallish text, run it through the training mechanism, and use the tagger to annotate a longer text. This will have many errors, but it's easier to correct the errors than it is to annotate a text from scratch. Then add this text to your training data and repeat. You will find that the tagger's performance will gradually get better as you build up more training data,</p>
",0,-1,143,2020-09-25 07:28:17,https://stackoverflow.com/questions/64059721/how-to-use-nlp-pos-tagging-in-other-langauge-like-sindhi-urdu
GloVe Import error - Corpus - Unable to import,"<p>I have tried everything possible while importing Corpus from Glove - I tried Pip Install and the Pip3 Install from the Zip file. Nothing seems to work. Please do help.</p>
","nlp, stanford-nlp","<p>You can use Glove from mittens as well. Mittens use the same algorithm as GloVe and vectorizes the objective function.</p>
<p>Install:</p>
<pre><code>pip install -U mittens
</code></pre>
<p>Import:</p>
<pre><code>from mittens import GloVe
</code></pre>
<p>For Details - <a href=""https://github.com/roamanalytics/mittens#mittens"" rel=""nofollow noreferrer"">https://github.com/roamanalytics/mittens#mittens</a></p>
",2,0,1363,2020-09-30 11:39:55,https://stackoverflow.com/questions/64136814/glove-import-error-corpus-unable-to-import
GloVe Import error - Corpus - Unable to import,"<p>I have tried everything possible while importing Corpus from Glove - I tried Pip Install and the Pip3 Install from the Zip file. Nothing seems to work. Please do help.</p>
","nlp, stanford-nlp","<p>You can use Glove from mittens as well. Mittens use the same algorithm as GloVe and vectorizes the objective function.</p>
<p>Install:</p>
<pre><code>pip install -U mittens
</code></pre>
<p>Import:</p>
<pre><code>from mittens import GloVe
</code></pre>
<p>For Details - <a href=""https://github.com/roamanalytics/mittens#mittens"" rel=""nofollow noreferrer"">https://github.com/roamanalytics/mittens#mittens</a></p>
",2,0,1363,2020-09-30 11:39:55,https://stackoverflow.com/questions/64136814/glove-import-error-corpus-unable-to-import
How may I resolve &quot;Tree not correctly binarized&quot; error in StanfordCoreNLP?,"<p><strong>I'm getting an error when run stanfordCoreNLP to get sentiment as below: I'm on version 4.0.0 currently, would you pls advise?</strong></p>
<p>SentimentCostAndGradient: warning: Tree not correctly binarized: (ROOT (: --) (S (VB let) (SBARQ (NP 's) (@SBARQ (ADVP just) (@SBARQ (VBP say) (SQ (S (VBG assuming) (SBAR (NP you) (VP (@VP (@VP (VBP get) (NP (JJ supportive) (NN legislation))) (NP (DT this) (NN year))) (PP (IN in) (NP (DT the) (@NP (NNP Texas) (NN legislature))))))) (@SQ (, ,) (@SQ (SBARQ (WHNP what) (SQ (VBZ 's) (NP (NP (DT the) (@NP (NN time) (NN line))) (PP (IN to) (S (VBG getting) (S (NP (DT an) (@NP (NNP AMI) (NN program))) (VP deployed))))))) (@SQ (, ,) (@SQ (@SQ (MD shall) (NP we)) (VP (@VP (@VP (@VP (VB say) (, ,)) (INTJ right)) (, ,)) (S (ADVP so) (VP (@VP (@VP (VBG starting) (PP (IN from) (NP midyear))) (NP (DT this) (NN year))) (ADVP onwards))))))))))))) (. ?))
Exception in thread &quot;main&quot; java.lang.AssertionError: Tree not correctly binarized
at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:532)
at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:512)
at edu.stanford.nlp.pipeline.SentimentAnnotator.doOneSentence(SentimentAnnotator.java:115)
at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:102)
at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:640)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:650)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1245)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1079)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1362)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1408)</p>
<p><strong>The text I input to StanfordCoreNLP module:</strong></p>
<p>Can you hear me? So a few follow-up questions if you can. First, let me just hit some of the details here on the credit side and the equity raise. Just how do you think about your [sort of] debt metrics, where they are today? Obviously, there's been some commentary already from the agencies. But -- and where do you need to go? I know you said you're not going to be specific today about the total quantum of equity needed, but just want to make sure we understand where you are today and where you need to go from a credit perspective. And I'm assuming that you want to sustain your current respective ratings. Got it, okay. And then let me come back to this twist, as far as I perceive it, with respect to the 50 to 150 megawatts -- or rather the upside outside of the procurement rather, is maybe the better way to say this, right? So you ran the RFP. You announced those results already. But then in the slides here, you talk about purchasing, it seems like, an additional quantum, if I -- if you will. Can you talk about that a little bit? And then just to be clear here, obviously, you awarded -- or you are proposing to award PPAs here. Would this be a potential build-own-transfer situation? And actually maybe even -- let me broaden this out, is the initially awarded RFP capacity potentially a build-own-transfer situation at all? And what's the time line for that, just if you don't mind reminding me? Just in terms of those -- I know you have this little description broadly this year about when you're getting these contracts done. But the time line specifically around the build-own-transfer, BOT, piece of that maybe. Got it, excellent. And then sorry, one last one just with respect to the AMI effort. The time line there, how are you thinking about -- let's just say assuming you get supportive legislation this year in the Texas legislature, what's the time line to getting an AMI program deployed, shall we say, right, so starting from midyear this year onwards? In the 2019 guidance, what level of rate relief is assumed with regards to both the -- and with regards to Texas as well, both the transmission as well as the distribution? Okay. Because looks like based on the way you file these things, it would seem like the transmission one should be available for the summer peak, while the distribution increase may not be there for the summer peak. No, I certainly appreciate that and understand that. But out of the $16 million request, under the assumption that those dollars in aggregate are approved, based on the time line, how much realistically falls into '19 versus '20? Okay. Also, in terms of equity, what is the -- in terms of your guidance, what's the share count assumption in terms of incremental equity for your 2019 guidance? Also, you've filed and -- waiting on the request for the approval. It was up to $200 million. I guess, can you help us in terms of -- timing-wise in terms of how the -- over what years or how you're thinking about that? Does that flexibility also contemplate the ability for funding the equity if you're able to reach an agreement with some of the RFP winners to build to buy, like Julien was describing? Can you give us a little bit of color on what you've seen in first quarter weather so far? Okay. And can you give us any sense of what you've seen out of the Texas legislature so far in terms of AMI? Okay. And as far as -- you gave us some idea of what the DCRF might look like this year. Are those numbers, you think, sort of what baseline annual numbers might look like? Or are you that far behind that these might be a little bit above normal sort of going forward? So maybe like half? Assuming your customer and load growth continues at similar rates, that -- that would maybe suggest about half would be what the going-forward rate might be. Yes. My understanding is that there's an effort in the -- with the legislature to empower the PUCT to be able to use forecasted test years for rate cases to help mitigate regulatory lag. I'm simply wondering kind of what you're -- what you're aware about that effort and the status and what you think the prospects are. Yes. I think I was kind of referring in -- to generation rider, probably more in a broader context. But did you -- if it's right now focused particularly on a generation rider, I guess what do you -- how do you see that right now? Sorry, guys, to follow up here. But I just wanted to clarify this, how do you think about your earned ROE baked into the '19 guidance here? Again, I know you talk about normalized weather, but I just want to understand earned ROE expectations and how those are established. And I'm really principally obviously asking more on the distribution side than the transmission side, if we can kind of dig into that. Got it. But the average reflected on the distribution side between the 2 jurisdictions is slightly below 9% on the distribution side.</p>
<p>Here is how I ran the model, I am on 4.1.0 version:</p>
<pre><code>java -mx3g -cp &quot;/home/cxiao/release_date/stanford-corenlp-4.1.0/*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLP -props.q1
</code></pre>
<p>For props.q1:</p>
<pre><code>annotators = tokenize, ssplit, pos, parse, sentiment
outputFormat = JSON 
outputDirectory = /path_to/standard_input/fail/
timeout = 500000
output.includeText = False
parse.model = edu/stanford/nlp/models/srparser/englishSR.ser.gz
continueOnAnnotateError = True
filelist = /path_to/stanford-corenlp-4.1.0/q/flist1.lst
</code></pre>
<p>For filelist:</p>
<pre><code>/pathto/failed_file.txt
</code></pre>
","machine-learning, nlp, stanford-nlp","<p>This is a really obscure bug in the sr parser, caused by a small handful of training trees in one of the recently added training sets breaking the assumption that all trees end with S, FRAG, etc and then go to ROOT as a unary transition.  The error produced here is only part of the sentence causing the problem, and the sr parser was trying to split that sentence into two pieces with two separate ROOTs.  In fact, the problem is even weirder than that, since this only caused a bug downstream when one of those ROOTs then had 3 or more pieces.</p>
<p>Anyway, the bug can be fixed by enforcing that ROOT nodes only occur at the very top of the tree.  That fix is in this branch of the source tree:</p>
<p><a href=""https://github.com/stanfordnlp/CoreNLP/tree/fix_sr_binary"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/tree/fix_sr_binary</a></p>
<p>If you don't see a branch there, that means it's already been merged into</p>
<p><a href=""https://github.com/stanfordnlp/CoreNLP/tree/dev"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/tree/dev</a></p>
<p>The plan is to post a new release in the next few weeks, at which point you can simply use the new release rather than build from source in order to get this bugfix.</p>
<p>Edit: You specifically want this change</p>
<p><a href=""https://github.com/stanfordnlp/CoreNLP/commit/10881ca4727b6d114e5157d2aa547bceac18d294"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/commit/10881ca4727b6d114e5157d2aa547bceac18d294</a></p>
<p>Anything past that will not be able to load old models, since there's a hack in readObject required to make it load the old models and we're going to redo the models with the updated transition rules</p>
",1,-1,80,2020-10-04 23:36:49,https://stackoverflow.com/questions/64200872/how-may-i-resolve-tree-not-correctly-binarized-error-in-stanfordcorenlp
Word2Vec- does the word embedding change?,"<p>just wanted to know if there are 2 sentences-</p>
<ol>
<li>The <em><strong>bank</strong></em> remains closed on public holidays</li>
<li>Don't go near the river <em><strong>bank</strong></em></li>
</ol>
<p>The word 'bank' will have different word embeddings or same? If we use word2vec or glove?</p>
","nlp, stanford-nlp, word2vec, word-embedding","<p>You can't meaningfully train a dense word embedding on just 2 texts. You'd need these, and dozens (or ideally hundreds) more examples of the use of <code>'bank'</code> in subtly-varying contexts to get a good word-vector for <code>'bank'</code>. (And that word-vector would only have meaning in comparison to other word-vectors for other well-sampled words in the same trained model.)</p>
<p>Let's assume you do have a large, diverse training corpus with many examples of <code>'bank'</code> in contexts. And you've trained a model, either word2vec or GLoVe on that corpus.</p>
<p>Then, imagine that corpus was changed so that there were relatively more contexts that included the 'river' sense. (Perhaps, a bunch of new texts are added that talk about nature, parks, boating, &amp; irrigation.) Then, you retrain your model, from scratch, on the new corpus.</p>
<p>In the new model, <code>'bank'</code> (and related words) will typically have been nudged to have more 'river bank'-like neighbors.</p>
<p>These words may be in totally different coordinates, overall, as each run includes enough randomness to change words' ending positions a lot. But their <em>relative neighborhoods</em> &amp; <em>relative directions</em> will tend to be of similar value from subsequent runs, and changes in the mix of examples will tend to nudge results in one direction or another.</p>
<p>This is the case for both GLoVe and word2vec: their end results will both be influenced by the relative preponderance of alternate word senses.</p>
<p>(That words have multiple contrasting meanings is generally referred to in the relevant literature as 'polysemy', so searches like [polysemy word-vectors] should turn up a lot more work related to your question.)</p>
",1,0,680,2020-10-08 11:25:20,https://stackoverflow.com/questions/64261521/word2vec-does-the-word-embedding-change
I don&#39;t want to remove stop words by splitting words into letters,"<p>I am writing this piece of code to remove stop words from my text.</p>
<p><strong>Problem - This code works perfectly for removing stopwords but the problem arises when words like ant, ide is present in my text as it removes both words ant and ide because ant is present in important, want and ide is present in side.</strong> But I don't want to split words into a letter to remove stopwords.</p>
<pre><code>            String sCurrentLine;
            List&lt;String&gt; stopWordsofwordnet=new ArrayList&lt;&gt;();
            FileReader fr=new FileReader(&quot;G:\\stopwords.txt&quot;);
            BufferedReader br= new BufferedReader(fr);
                while ((sCurrentLine = br.readLine()) != null)
                {
                    stopWordsofwordnet.add(sCurrentLine);
                }
                //out.println(&quot;&lt;br&gt;&quot;+stopWordsofwordnet);
            List&lt;String&gt; wordsList = new ArrayList&lt;&gt;();
            
            String text = request.getParameter(&quot;textblock&quot;);
            text=text.trim().replaceAll(&quot;[\\s,;]+&quot;, &quot; &quot;);
            String[] words = text.split(&quot; &quot;);

//            wordsList.addAll(Arrays.asList(words));
                for (String word : words) {
                wordsList.add(word);
                }
            out.println(&quot;&lt;br&gt;&quot;);

            //remove stop words here from the temp list
            for (int i = 0; i &lt; wordsList.size(); i++) 
            {
            // get the item as string
            for (int j = 0; j &lt; stopWordsofwordnet.size(); j++) 
            {
            if (stopWordsofwordnet.get(j).contains(wordsList.get(i).toLowerCase())) 
            {
                out.println(wordsList.get(i)+&quot;&amp;nbsp;&quot;);
                wordsList.remove(i);
                i--;
                break;
            }
            }
            }
            out.println(&quot;&lt;br&gt;&quot;);
            for (String str : wordsList) {
            out.print(str+&quot; &quot;);
            }
</code></pre>
","java, servlets, nlp, stanford-nlp","<p>Your code is overly complicated, and can be reduced to this:</p>
<pre class=""lang-java prettyprint-override""><code>// Load stop words from file
Set&lt;String&gt; stopWords = new TreeSet&lt;&gt;(String.CASE_INSENSITIVE_ORDER);
stopWords.addAll(Files.readAllLines(Paths.get(&quot;G:\\stopwords.txt&quot;)));

// Get text and split into words
String text = request.getParameter(&quot;textblock&quot;);
List&lt;String&gt; wordsList = new ArrayList&lt;&gt;(Arrays.asList(
        text.replaceAll(&quot;[\\s,;]+&quot;, &quot; &quot;).trim().split(&quot; &quot;)));

// Remove stop words from list of words
wordsList.removeAll(stopWords);
</code></pre>
",0,-1,238,2020-10-13 15:01:35,https://stackoverflow.com/questions/64337797/i-dont-want-to-remove-stop-words-by-splitting-words-into-letters
Stanfordnlp.download() fails: [Errno -2] Name or service not known&#39;)),"<p>I simply tried to run the example given by stanfordnlp themselves:</p>
<pre><code>&gt;&gt;&gt; import stanfordnlp
&gt;&gt;&gt; stanfordnlp.download('en')   # This downloads the English models for the neural pipeline
&gt;&gt;&gt; nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English
&gt;&gt;&gt; doc = nlp(&quot;Barack Obama was born in Hawaii.  He was elected president in 2008.&quot;)
&gt;&gt;&gt; doc.sentences[0].print_dependencies()
</code></pre>
<p>However, I was unable to do so, receiving the following error:</p>
<pre><code>ConnectionError: HTTPSConnectionPool(host='nlp.stanford.edu', port=443): Max retries exceeded with url: /software/stanfordnlp_models/latest/en_ewt_models.zip (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7f8f5dba7f10&gt;: Failed to establish a new connection: [Errno -2] Name or service not known'))
</code></pre>
<p>Why is this the case? I saw that this was an issue <a href=""https://github.com/stanfordnlp/stanza/issues/426"" rel=""nofollow noreferrer"">on their github</a>, but they stated that it was due to server issues which have sense been resolved. How can I solve this error? Thanks.</p>
","nlp, data-science, stanford-nlp","<p>The stanfordnlp package is now deprecated. We renamed it to Stanza for more recent releases. You should follow the instructions here: <a href=""https://stanfordnlp.github.io/stanza/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/</a> . Following the corresponding steps there worked fine just now for me:</p>
<pre><code>&gt;&gt;&gt; import stanza
&gt;&gt;&gt; stanza.download('en') # download English model
&gt;&gt;&gt; nlp = stanza.Pipeline('en') # initialize English neural pipeline
&gt;&gt;&gt; doc = nlp(&quot;Barack Obama was born in Hawaii.&quot;) # run annotation over a sentence
&gt;&gt;&gt; print(doc.entities)
</code></pre>
<p>That said, more details are:</p>
<ul>
<li>This error comes from failing to be able to download model data files from our lab machines. They are sometimes down. Try again the next day. Doing it right now, the model download succeeded fine (if a little slowly).</li>
<li><code>stanfordnlp</code> is incompatible with recent versions of PyTorch. If you see the error <code>RuntimeError: Integer division of tensors using div or / is no longer supported</code>, then you either need to switch to <code>stanza</code> or to downgrade your version of PyTorch to 1.5 or earlier</li>
<li>Stanza downloads large model data files from GitHub rather than from our lab machines, and so Stanza model data file download should be more reliable. But if you have trouble accessing GitHub, see <a href=""https://stanfordnlp.github.io/stanza/faq.html#getting-requestsexceptionsconnectionerror-when-downloading-models"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/faq.html#getting-requestsexceptionsconnectionerror-when-downloading-models</a></li>
</ul>
",2,0,664,2020-10-18 03:27:32,https://stackoverflow.com/questions/64409435/stanfordnlp-download-fails-errno-2-name-or-service-not-known
How does Stanford NLP identify &quot;it&quot; is being referred to whom?,"<p>Let's say we have two sentences:</p>
<ol>
<li>Jacob is going to watch a movie with Justin.</li>
<li>He will be back by 10 pm.</li>
</ol>
<p>How does Stanford NLP identify &quot;he&quot; refers to Jacob and not Justin?</p>
","nlp, stanford-nlp, text-classification","<p>This is called <code>coreference resolution</code> and is a well-studied problem in NLP. As such, there are many possible ways to do it. Stackoverflow is not the right venue for a literature review, but here are a few links to get you started</p>
<ul>
<li><a href=""http://www-labs.iro.umontreal.ca/%7Efelipe/IFT6010-Hiver2015/Presentations/Abbas-Coreference.pdf"" rel=""nofollow noreferrer"">http://www-labs.iro.umontreal.ca/~felipe/IFT6010-Hiver2015/Presentations/Abbas-Coreference.pdf</a></li>
<li><a href=""https://nlp.stanford.edu/projects/coref.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/coref.shtml</a> and links therein</li>
</ul>
",0,0,40,2020-10-27 03:16:49,https://stackoverflow.com/questions/64547843/how-does-stanford-nlp-identify-it-is-being-referred-to-whom
Need advice on Negation Handling while doing Aspect Based Sentiment Analysis in Python,"<p>I'm trying to write a Python code that does <strong>Aspect Based Sentiment Analysis</strong> of product reviews using Dependency Parser. I created an example review:</p>
<p>&quot;The Sound Quality is great but the battery life is bad.&quot;</p>
<p>The output is : <strong>[['soundquality', ['great']], ['batterylife', ['bad']]]</strong></p>
<p>I can properly get the aspect and it's adjective with this sentence but when I change the text to:</p>
<p>&quot;The Sound Quality is not great but the battery life is not bad.&quot;</p>
<p>The output still stays the same. How can I add a negation handling to my code? And are there ways to improve what I currently have?</p>
<pre><code>import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
import stanfordnlp

stanfordnlp.download('en')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

txt = &quot;The Sound Quality is not great but the battery life is not bad.&quot;

txt = txt.lower()
sentList = nltk.sent_tokenize(txt)

taggedList = []
for line in sentList:
    txt_list = nltk.word_tokenize(line) # tokenize sentence
    taggedList = taggedList + nltk.pos_tag(txt_list) # perform POS-Tagging
print(taggedList)

newwordList = []
flag = 0
for i in range(0,len(taggedList)-1):
    if(taggedList[i][1]=='NN' and taggedList[i+1][1]=='NN'):
        newwordList.append(taggedList[i][0]+taggedList[i+1][0])
        flag=1
    else:
        if(flag == 1):
            flag=0
            continue
        newwordList.append(taggedList[i][0])
        if(i==len(taggedList)-2):
            newwordList.append(taggedList[i+1][0])
finaltxt = ' '.join(word for word in newwordList)
print(finaltxt)

stop_words = set(stopwords.words('english'))
new_txt_list = nltk.word_tokenize(finaltxt)
wordsList = [w for w in new_txt_list if not w in stop_words]
taggedList = nltk.pos_tag(wordsList)

nlp = stanfordnlp.Pipeline()
doc = nlp(finaltxt)
dep_node = []
for dep_edge in doc.sentences[0].dependencies:
    dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])
for i in range(0, len(dep_node)):
    if(int(dep_node[i][1]) != 0):
        dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]
print(dep_node)

featureList = []
categories = []
totalfeatureList = []
for i in taggedList:
    if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):
        featureList.append(list(i))
        totalfeatureList.append(list(i)) # stores all the features for every sentence
        categories.append(i[0])
print(featureList)
print(categories)

fcluster = []
for i in featureList:
    filist = []
    for j in dep_node:
        if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [&quot;nsubj&quot;, &quot;acl:relcl&quot;, &quot;obj&quot;, &quot;dobj&quot;, &quot;agent&quot;, &quot;advmod&quot;, &quot;amod&quot;, &quot;neg&quot;, &quot;prep_of&quot;, &quot;acomp&quot;, &quot;xcomp&quot;, &quot;compound&quot;])):
            if(j[0]==i[0]):
                filist.append(j[1])
            else:
                filist.append(j[0])
    fcluster.append([i[0], filist])
print(fcluster)

finalcluster = []
dic = {}
for i in featureList:
    dic[i[0]] = i[1]
for i in fcluster:
    if(dic[i[0]]=='NN'):
        finalcluster.append(i)
print(finalcluster)
</code></pre>
","python, nlp, nltk, stanford-nlp, spacy","<p>You may wish to try <code>spacy</code>. The following pattern will catch:</p>
<ul>
<li>a noun phrase</li>
<li>followed by <code>is</code> or <code>are</code></li>
<li>optionally followed by <code>not</code></li>
<li>followed by an adjective</li>
</ul>
<pre><code>import spacy
from spacy.matcher import Matcher
nlp = spacy.load('en_core_web_sm')

output = []
doc = nlp('The product is very good')
matcher = Matcher(nlp.vocab)
matcher.add(&quot;mood&quot;,None,[{&quot;LOWER&quot;:{&quot;IN&quot;:[&quot;is&quot;,&quot;are&quot;]}},{&quot;LOWER&quot;:{&quot;IN&quot;:[&quot;no&quot;,&quot;not&quot;]},&quot;OP&quot;:&quot;?&quot;},{&quot;LOWER&quot;:&quot;very&quot;,&quot;OP&quot;:&quot;?&quot;},{&quot;POS&quot;:&quot;ADJ&quot;}])
for nc in doc.noun_chunks:
    d = doc[nc.root.right_edge.i+1:nc.root.right_edge.i+1+3]
    matches = matcher(d)
    if matches:
        _, start, end = matches[0]
        output.append((nc.text, d[start+1:end].text))
    
print(output)
[('The product', 'very good')]
</code></pre>
<p>Alternatively, you may broaden matching pattern with info from dependency parser that would add definition of adjectival phrase:</p>
<pre class=""lang-py prettyprint-override""><code>output = []
matcher = Matcher(nlp.vocab, validate=True)
matcher.add(&quot;mood&quot;,None,[{&quot;LOWER&quot;:{&quot;IN&quot;:[&quot;is&quot;,&quot;are&quot;]}},{&quot;LOWER&quot;:{&quot;IN&quot;:[&quot;no&quot;,&quot;not&quot;]},&quot;OP&quot;:&quot;?&quot;},{&quot;DEP&quot;:&quot;advmod&quot;,&quot;OP&quot;:&quot;?&quot;},{&quot;DEP&quot;:&quot;acomp&quot;}])
for nc in doc.noun_chunks:
    d = doc[nc.root.right_edge.i+1:nc.root.right_edge.i+1+3]
    matches = matcher(d)
    if matches:
        _, start, end = matches[0]
        output.append((nc.text, d[start+1:end].text))
    
print(output)
[('The product', 'very good')]
</code></pre>
",1,1,832,2020-10-30 11:48:15,https://stackoverflow.com/questions/64607800/need-advice-on-negation-handling-while-doing-aspect-based-sentiment-analysis-in
Finding the Antonym of a Word,"<p>I'm working on an aspect-based sentiment analysis model using spaCy. I managed to extract aspects and adjectives as pairs in a list. I also included &quot;not&quot; before any adjective to handle any negations. I want to swap the adjective with its antonym if there is &quot;not&quot; before the adjective. I know spaCy has some similarity detection tools but I couldn't find anything about antonyms. Is it possible to do this with spaCy? If not how can I do it or is there a better way to handle the negations?</p>
<pre><code>import spacy
from spacy.matcher import Matcher
nlp = spacy.load('en_core_web_sm')

txt = &quot;The performance of the product is not great but The price is fair.&quot;
txt = txt.lower()

output = []
doc = nlp(txt)

matcher = Matcher(nlp.vocab, validate=True)
matcher.add(&quot;mood&quot;,None,[{&quot;LOWER&quot;:{&quot;IN&quot;:[&quot;is&quot;,&quot;are&quot;]}},{&quot;LOWER&quot;:{&quot;IN&quot;:[&quot;no&quot;,&quot;not&quot;]},&quot;OP&quot;:&quot;?&quot;},{&quot;DEP&quot;:&quot;advmod&quot;,&quot;OP&quot;:&quot;?&quot;},{&quot;DEP&quot;:&quot;acomp&quot;}])
for nc in doc.noun_chunks:
    d = doc[nc.root.right_edge.i+1:nc.root.right_edge.i+1+3]
    matches = matcher(d)
    if matches:
        _, start, end = matches[0]
        output.append((nc.text, d[start+1:end].text))
    
print(output)
</code></pre>
<p>Expected output:</p>
<pre><code>[('the performance', 'not great'), ('the product', 'not great'), ('the price', 'fair')]
</code></pre>
","python, nlp, nltk, stanford-nlp, spacy","<p>This task seems to best addressed with <a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a> to provide you the antonym. You can then potentially use either WordNet or some spellchecking library to list synonyms and find antonyms for these (they are likely to be not <em>exact</em> antonyms then). Good python libraries for this are: <a href=""http://pyenchant.github.io"" rel=""nofollow noreferrer"">pyenchant</a> or <a href=""https://pypi.org/project/hunspell/"" rel=""nofollow noreferrer"">hunspell</a>.</p>
<p>WordNet (using API provided by NLTK - an 'older sister' NLP library to spaCy): see <a href=""https://stackoverflow.com/a/64320638/6573902"">this answer</a> or <a href=""https://stackoverflow.com/questions/47441518/how-to-get-a-list-of-antonyms-lemmas-using-python-nltk-and-wordnet"">another one</a>.</p>
",1,0,1315,2020-11-01 22:08:36,https://stackoverflow.com/questions/64637505/finding-the-antonym-of-a-word
Python: Guess gender from the input,"<p>So, I used python package &quot;gender-guesser&quot; to detect the gender of the person based on their names. However, I want to identify the gender from a sentence that does not have the person name.</p>
<p>Suppose I have the below sentence:</p>
<p>&quot;Prior to you with a 14 year old male, who got out of bed and had some sort of syncopal episode.&quot;</p>
<p>The sentence is just an example and only has the word male and not the person's name. But, the input can contain may contain other words like boy, girl, lady, transgender, guy, woman, man, unknown, etc.</p>
<p>This is what I am currently trying to do, but may not be correct for what I want the end result:</p>
<pre><code>#original string
wordlist=tokens
# using split() function

# total no of words
male_count=0
female_count=0

for i in range(len(wordlist)):
  if wordlist[i]==('male' or 'boy' or 'guy' or 'man'):
    print(i)
    male_count= male_count+1
  
  else: 
    if wordlist[i]==('female' or 'girl' or 'lady' or 'woman'):
      female_count= female_count+1
</code></pre>
<p>Is there a better way to identify the gender?</p>
","python, nlp, stanford-nlp","<p>A few ways to improve:</p>
<ol>
<li>instead of <code>if wordlist[i]==('male' or 'boy' or 'guy' or 'man')</code>, you can check
<code>if wordlist[i] in ['male', 'boy', 'guy', 'man']</code>. Same is valid for females.</li>
<li>Not a big deal but instead of <code>list</code> (i.e., ['male', 'boy', 'guy', 'man']), you can create a <code>set</code> as <code>set(['male', 'boy', 'guy', 'man'])</code>, same for females.</li>
<li>No need for the <code>else</code>.</li>
<li>You can use <code>a += 1</code> instead of <code>a = a + 1</code> which does the same job.</li>
<li>You don't need to iterate over <code>range(len(wordlist))</code>. You can just iterate over <code>word_list</code></li>
</ol>
<p>So, your code can be cleaned up a little as follows:</p>
<pre><code>male_count = 0
female_count = 0

male_categories = set(['male', 'boy', 'guy', 'man'])
female_categories = set(['female', 'girl', 'lady', 'woman'])
for word in wordlist:
    if word in male_categories:
        male_count += 1
    if word in female_categories:
        female_count += 1
</code></pre>
<p>There are different ways to do this as well, such as <code>counting males + boys + guy + man</code> in the list which would be one or two lines. But I think this is a better start and easier to understand.</p>
",2,0,1842,2020-11-15 02:30:35,https://stackoverflow.com/questions/64840675/python-guess-gender-from-the-input
How can I Convert a dataset to glove or word2vec format?,"<p>I have my twitter archive downloaded and wanted to run word2vec to experiment most similar words, analogies etc on it.</p>
<p>But I am stuck at first step - how to convert a given dataset / csv / document so that it can be input to word2vec? i.e. what is the process to convert data to glove/word2vec format?</p>
","python, nlp, stanford-nlp, word2vec","<p>Typically implementations of the word2vec &amp; GLoVe algorithms do one or both of:</p>
<ul>
<li><p>accept a plain text file, where tokens are delimited by (one or more) spaces, and text is considered each newline-delimited line at a time (with lines that aren't &quot;too long&quot; - usually, short-article or paragraph or sentence per line)</p>
</li>
<li><p>have some language/library-specific interface for feeding texts (lists-of-tokens) to the algorithm as a stream/iterable</p>
</li>
</ul>
<p>The Python Gensim library offers both options for its <code>Word2Vec</code> class.</p>
<p>You should generally try working through one or more tutorials to get a working overview of the steps involved, from raw data to interesting results, before applying such libraries to your own data. And, by examining the formats used by those tutorials – and the extra steps they perform to massage the data into the formats needed by exactly the libraries you're using – you'll also see ideas for how your data needs to be prepared.</p>
",1,0,176,2020-12-11 16:24:54,https://stackoverflow.com/questions/65255029/how-can-i-convert-a-dataset-to-glove-or-word2vec-format
Issue in creating Semgrex patterns with relation names containing &quot;:&quot; colon,"<p>I am trying to perform Semgrex in <a href=""https://corenlp.run/"" rel=""nofollow noreferrer"">https://corenlp.run/</a> on the below sentence to extract the transition event. Since the dependency relation <strong>&quot;obl:from&quot;</strong> has a colon in it, I get an error. But instead, if I used nsubj, I get the desired result. Can someone tell me how to work around this?</p>
<p><em>My text: The automobile shall change states from OFF to ON when the driver is in control.</em></p>
<pre><code>{} &lt;&lt;nsubj {}
{} &lt;&lt;obl:from  {}    
</code></pre>
<p><a href=""https://i.sstatic.net/1JyVK.png"" rel=""nofollow noreferrer"">working scenario screenshot</a>
<a href=""https://i.sstatic.net/Y3gxP.png"" rel=""nofollow noreferrer"">Issue scenario screenshot</a></p>
","parsing, nlp, constants, stanford-nlp, dependency-parsing","<p>Found the answer! Just wrap the expression within <code>/ /</code> and it works!
For eg.</p>
<pre><code>{} &lt;&lt;  /obl:from/  {} 
</code></pre>
",0,0,168,2020-12-13 07:39:34,https://stackoverflow.com/questions/65273410/issue-in-creating-semgrex-patterns-with-relation-names-containing-colon
Deal with Out of vocabulary word with Gensim pretrained GloVe,"<p>I am working on an NLP assignment and loaded the GloVe vectors provided by Gensim:</p>
<pre><code>import gensim.downloader
glove_vectors = gensim.downloader.load('glove-twitter-25')
</code></pre>
<p>I am trying to get the word embedding for each word in a sentence, but some of them are not in the vocabulary.</p>
<p>What is the best way to deal with it working with the Gensim API?</p>
<p>Thanks!</p>
","nlp, stanford-nlp, gensim, word-embedding","<p>Load the <a href=""https://radimrehurek.com/gensim/downloader.html"" rel=""nofollow noreferrer"">model</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import gensim.downloader as api
model = api.load(&quot;glove-twitter-25&quot;)  # load glove vectors
# model.most_similar(&quot;cat&quot;)  # show words that similar to word 'cat'
</code></pre>
<p>There is a very simple way to find out if the words exist in the model's vocabulary.</p>
<pre class=""lang-py prettyprint-override""><code>result = print('Word exists') if word in model.wv.vocab else print('Word does not exist&quot;)
</code></pre>
<p>Apart from that, I had used the following logic to create sentence embedding (25 dim) with <strong>N</strong> tokens:</p>
<pre class=""lang-py prettyprint-override""><code>from __future__ import print_function, division
import os
import re
import sys
import regex
import numpy as np
from functools import partial

from fuzzywuzzy import process
from Levenshtein import ratio as lev_ratio

import gensim
import tempfile


def vocab_check(model, word):
    similar_words = model.most_similar(word)
    match_ratio = 0.
    match_word = ''
    for sim_word, sim_score in similar_words:
        ratio = lev_ratio(word, sim_word)
        if ratio &gt; match_ratio:
            match_word = sim_word
    if match_word == '':
        return similar_words[0][1]
    return model.similarity(word, match_word)


def sentence2vector(model, sent, dim=25):
    words = sent.split(' ')
    emb = [model[w.strip()] for w in words]
    weights = [1. if w in model.wv.vocab else vocab_check(model, w) for w in words]
    
    if len(emb) == 0:
        sent_vec = np.zeros(dim, dtype=np.float16)
    else:
        sent_vec = np.dot(weights, emb)

    sent_vec = sent_vec.astype(&quot;float16&quot;)
    return sent_vec   
</code></pre>
",2,2,2998,2020-12-19 16:35:40,https://stackoverflow.com/questions/65372032/deal-with-out-of-vocabulary-word-with-gensim-pretrained-glove
POS Tag Set for Stanford NLP 4 Spanish,"<p>I asked a <a href=""https://stackoverflow.com/questions/61540771/stanford-nlp-core-4-0-0-no-longer-splitting-verbs-and-pronouns-in-spanish"">question before about tokenization</a> (getting the wrong end of the stick I believe.)  I see now that Stanford NLP 4 does the job of splitting Contractions &amp; Clitic pronouns - very well - much better than the 3.x version.</p>
<p>However I'm still short on how to get the new 4.x version to return the bigger tag set mentioned in Question 6 on the FAQ <a href=""https://nlp.stanford.edu/software/spanish-faq.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/spanish-faq.html</a></p>
<p>E.g. To differentiate between nc0s000 &amp; nc0p000  (Singular and Pural Nouns)</p>
<p>Is the .ud tagger going to be made available in future releases?</p>
",stanford-nlp,"<p>As of 4.2, only UD models are available.</p>
",1,0,106,2020-12-26 16:25:09,https://stackoverflow.com/questions/65458392/pos-tag-set-for-stanford-nlp-4-spanish
How can I iterate token attributes with coreference results in CoreNLP?,"<p>I am looking for a way to extract and merge annotation results from CoreNLP. To specify,</p>
<pre><code>import stanza
import os
from stanza.server import CoreNLPClient
corenlp_dir = '/Users/fatih/stanford-corenlp-4.2.0/'
os.environ['CORENLP_HOME'] = corenlp_dir

client = CoreNLPClient(
    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'coref'], 
    memory='4G', 
    endpoint='http://localhost:9001',
    be_quiet=True)

text = &quot;Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.&quot;

doc = client.annotate(text)

for x in doc.corefChain:
    for y in x.mention:
        print(y.animacy)
        
ANIMATE
ANIMATE
ANIMATE
</code></pre>
<p>I'd like to merge these results with the ones from following code:</p>
<pre><code>for i, sent in enumerate(document.sentence):
    print(&quot;[Sentence {}]&quot;.format(i+1))
    for t in sent.token:
        print(&quot;{:12s}\t{:12s}\t{:6s}\t{}&quot;.format(t.word, t.lemma, t.pos, t.ner))
    print(&quot;&quot;)

Barack          Barack          NNP     PERSON
Obama           Obama           NNP     PERSON
was             be              VBD     O
born            bear            VBN     O
in              in              IN      O
Hawaii          Hawaii          NNP     STATE_OR_PROVINCE
.               .               .       O

[Sentence 2]
He              he              PRP     O
is              be              VBZ     O
the             the             DT      O
president       president       NN      TITLE
.               .               .       O

[Sentence 3]
Obama           Obama           NNP     PERSON
was             be              VBD     O
elected         elect           VBN     O
in              in              IN      O
2008            2008            CD      DATE
.               .               .       O
</code></pre>
<p>Since annotations are stored in different object, I cannot iterate over the two different object and get the results for related items.</p>
<p>Is there a way out?</p>
<p>Thanks.</p>
","nlp, stanford-nlp","<p>The coref chains have a sentenceIndex and a beginIndex which should correlate to the position in the sentence.  You can use this to correlate the two.</p>
<p><a href=""https://github.com/stanfordnlp/stanza/blob/f0338f891a03e242c7e11e440dec6e191d54ab77/doc/CoreNLP.proto#L319"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/stanza/blob/f0338f891a03e242c7e11e440dec6e191d54ab77/doc/CoreNLP.proto#L319</a></p>
<p>Edit: quick and dirty change to your example code:</p>
<pre><code>from collections import defaultdict
from stanza.server import CoreNLPClient

client = CoreNLPClient(
    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'coref'],
    be_quiet=False)

text = &quot;Barack Obama was born in Hawaii.  In 2008 he became the president.&quot;

doc = client.annotate(text)

animacy = defaultdict(dict)
for x in doc.corefChain:
    for y in x.mention:
        print(y.animacy)
        for i in range(y.beginIndex, y.endIndex):
            animacy[y.sentenceIndex][i] = True
            print(y.sentenceIndex, i)

for sent_idx, sent in enumerate(doc.sentence):
    print(&quot;[Sentence {}]&quot;.format(sent_idx+1))
    for t_idx, token in enumerate(sent.token):
        animate = animacy[sent_idx].get(t_idx, False)
        print(&quot;{:12s}\t{:12s}\t{:6s}\t{:20s}\t{}&quot;.format(token.word, token.lemma, token.pos, token.ner, animate))
    print(&quot;&quot;)
</code></pre>
",1,0,296,2021-01-02 18:45:24,https://stackoverflow.com/questions/65542790/how-can-i-iterate-token-attributes-with-coreference-results-in-corenlp
Stanford CoreNLP: Java can&#39;t find or load main class / java.ClassNotFoundException,"<p>To be honest, Java is a mystery for me. I just started a few days ago to learn Python, but now I need to use <em>Stanford CoreNLP</em> which needs Java (GOD!!!!!)</p>
<p>When I import Stanford CoreNlp in CMD, it always shows &quot;Error: can't find or load main class ... Reason: java.lang.ClassNotFoundException: ...&quot;
<a href=""https://i.sstatic.net/9r4rq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9r4rq.png"" alt=""like what the picture says"" /></a></p>
<p>But in fact, I have already made some changes in the environment (though they may not be correct).
<a href=""https://i.sstatic.net/nP3Ej.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nP3Ej.png"" alt=""environment path"" /></a></p>
<p>It may be an error of the setting of environement path, but I really don't know how to solve it...</p>
","java, stanford-nlp","<p>You are facing with classpath issue</p>
<p>From your screenshot, current working directory is C:\Users(Name) which does not contains code of the SCNLP.</p>
<p>From <a href=""https://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow noreferrer"">Command Line Usage page</a>, the minimal command to run Stanford CoreNLP from the command line is:</p>
<pre><code>java -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLP -file input.txt
</code></pre>
<p>As you can see, you missed the <code>-cp</code> arg which does specify classpath.</p>
<p>You should <code>cd</code> to the code directory and use <code>-cp &quot;*&quot;</code> or pass the directory of the source code of Stanford CoreNLP as the value of <code>-cp</code> argument</p>
",1,0,427,2021-01-06 09:28:57,https://stackoverflow.com/questions/65593234/stanford-corenlp-java-cant-find-or-load-main-class-java-classnotfoundexcepti
twitter_samples in both corpus and download,"<p>I am trying to implement a basic twitter sentiment analysis project. For this, I import from a corpus twitter_samples as following</p>
<pre><code>from nltk.corpus import twitter_samples
</code></pre>
<p>but it also needs to use the following command;</p>
<pre><code>nltk.download('twitter_samples')
</code></pre>
<p>to work properly.</p>
<p>My intuition about this is that, the <code>twitter_samples</code> from <code>from nltk.corpus import twitter_samples</code> is a function and the <code>twitter_samples</code> in <code>nltk.download('twitter_samples')</code> is the dataset. Am I correct or is it something else? Please explain. Also is there any specific reason as to why the name <code>twitter_samples</code> is same in both?</p>
","python, python-3.x, nlp, nltk, stanford-nlp","<p>The reason for using same name is both the cases is that you have to download it only once. Post that you can directly use it with import statement.</p>
<p>nltk has huge number of offerings, all of them may not be required for everyone, hence you will have to download the stuff you want (only once per env), then use it using import statement.</p>
",1,0,277,2021-01-14 11:51:19,https://stackoverflow.com/questions/65718443/twitter-samples-in-both-corpus-and-download
How to load pre-trained glove model with gensim load_word2vec_format?,"<p>I am trying to load a pre-trained glove as a word2vec model in gensim. I have downloaded the glove file from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">here</a>. I am using the following script:</p>
<pre><code>from gensim import models
model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)
</code></pre>
<p>but get the following error</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-38-e0b48b51f433&gt; in &lt;module&gt;()
      1 from gensim import models
----&gt; 2 model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py in &lt;genexpr&gt;(.0)
    171     with utils.smart_open(fname) as fin:
    172         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 173         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    174         if limit:
    175             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: 'the'
</code></pre>
<p>What is the underlying problem? Does gensim need a specific format to be able to load it?</p>
","stanford-nlp, gensim, word2vec, word-embedding","<p>The GLoVe format is slightly different – missing a 1st-line declaration of vector-count &amp; dimensions – than the format that <code>load_word2vec_format()</code> supports.</p>
<p>There's a <code>glove2word2vec</code> utility script included you can run once to convert the file:</p>
<p><a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/scripts/glove2word2vec.html</a></p>
<p>Also, starting in Gensim 4.0.0 (currentlyu in prerelease testing), the <code>load_word2vec_format()</code> method gets a new optional <code>no_header</code> parameter:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=load_word2vec_format#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=load_word2vec_format#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</a></p>
<p>If set as <code>no_header=True</code>, the method will deduce the count/dimensions from a preliminary scan of the file - so it can read a GLoVe file with that option – but at the cost of two full-file reads instead of one. (So, you may still want to re-save the object with <code>.save_word2vec_format()</code>, or use the <code>glove2word2vec</code> script, to make future loads faster.)</p>
",7,4,8907,2021-02-03 04:08:58,https://stackoverflow.com/questions/66021131/how-to-load-pre-trained-glove-model-with-gensim-load-word2vec-format
Stanford Core NLP Tree Parser Sentence Limits wrong - suggestions?,"<p>I'm dealing with <em>german</em> law documents and would like to generate parse trees for sentences. I could find and use Standford <a href=""https://nlp.stanford.edu/software/lex-parser.html"" rel=""nofollow noreferrer"">CoreNLP Parser</a>. However, it does not recognize sentence limits as good as other tools (e.g. spaCy) when parsing the sentences of a document. For example, it would break sentences at every single '.'-character, incl. the dot at the end of abbreviations such as &quot;incl.&quot;)
Since it is crucial to cover the whole sentence for creating syntax trees, this does not really work out for me.</p>
<p>I would appreciate any suggestions to tackle this problem, espacially pointers to other software that might be better suited for my problem. If I overlooked the possibility to tweak the Stanford parser, I would be very grateful for any hints on how to make it better detect sentence limits.</p>
","nlp, stanford-nlp, parse-tree","<p>A quick glance into the docs did the trick: You can run your pipeline, which might include the <a href=""https://stanfordnlp.github.io/corenlp-docs-dev/ssplit.html"" rel=""nofollow noreferrer"">sentence splitter</a>, with the attribute
<code>ssplit.isOneSentence = true</code> to basically disable it. This means you can split the sentences beforehand, e.g. using spaCy, and then feed single sentences into the pipeline.</p>
",0,0,95,2021-02-16 14:38:43,https://stackoverflow.com/questions/66226460/stanford-core-nlp-tree-parser-sentence-limits-wrong-suggestions
Google colab Glove_Python pip install not working,"<p>I am using</p>
<pre><code>! pip install glove_python
</code></pre>
<p>I'm getting this error message:</p>
<pre><code>Collecting glove_python
  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)
     |████████████████████████████████| 266kB 16.9MB/s 
Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python) (1.19.5)
Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python) (1.4.1)
Building wheels for collected packages: glove-python
  Building wheel for glove-python (setup.py) ... error
  **ERROR: Failed building wheel for glove-python**
  Running setup.py clean for glove-python
  **ERROR: Failed cleaning build dir for glove-python**
Failed to build glove-python
Installing collected packages: glove-python
    Running setup.py install for glove-python ... error
**ERROR: Command errored out with exit status 1**: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-nypxp28t/glove-python/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-nypxp28t/glove-python/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-cnn32mbr/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.
</code></pre>
<p>As suggested below I tried</p>
<pre><code>! python -m pip install glove_python --verbose
</code></pre>
<p>Which outputs the following recurring error with different members:</p>
<pre><code>has no member named ‘exc_{member}’; did you mean ‘curexc_value’?
</code></pre>
<p>And ends with:</p>
<pre><code>error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
    Running setup.py install for glove-python ... error
Cleaning up...
  Removing source in /tmp/pip-install-ru3hxbde/glove-python
Removed build tracker '/tmp/pip-req-tracker-ps3qzi71'
ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-ru3hxbde/glove-python/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-ru3hxbde/glove-python/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-ywzvlm5m/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.
Exception information:
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py&quot;, line 153, in _main
    status = self.run(options, args)
  File &quot;/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py&quot;, line 455, in run
    use_user_site=options.use_user_site,
  File &quot;/usr/local/lib/python3.7/dist-packages/pip/_internal/req/__init__.py&quot;, line 62, in install_given_reqs
    **kwargs
  File &quot;/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py&quot;, line 888, in install
    cwd=self.unpacked_source_directory,
  File &quot;/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py&quot;, line 275, in runner
    spinner=spinner,
  File &quot;/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py&quot;, line 242, in call_subprocess
    raise InstallationError(exc_msg)
pip._internal.exceptions.InstallationError: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-ru3hxbde/glove-python/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-ru3hxbde/glove-python/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-ywzvlm5m/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.
</code></pre>
<p>Trying pip install glove-python-binary is successful but when I import it I get the following error:</p>
<pre><code>    import glove-python-binary
                ^
SyntaxError: invalid syntax
</code></pre>
","python, pip, google-colaboratory, stanford-nlp","<p>Seems like <code>glove_python</code> package is very old, last relese of it on PIP was in 2016. And it has only sources there, so should be compiled by C/C++ compiler, which is usually problematic everywhere (needs manual installing correct compiler and all dependencies).</p>
<p>Looks like updated version is <code>glove-python-binary</code> it dates to 2020. Try installing it through <code>! pip install glove-python-binary</code>.</p>
<p>According to <a href=""https://pypi.org/project/glove-python-binary/"" rel=""noreferrer"">pypi site</a> <code>glove-python-binary</code> needs Python of versions 3.6 or 3.7 or 3.8, smaller or bigger version of Python will not probably work.</p>
<p>When you have any problem or error in pip try adding <code>--verbose</code> to pip command line, this will print you a lot of details about reasons of pip failure.</p>
<p>Also as @GameDev <a href=""https://stackoverflow.com/questions/66396100/google-colab-glove-python-pip-install-not-working/66397661#comment117381046_66396100"">said</a> sometimes you have to also try running command <code>! python -m pip install PACKAGE</code> instead of <code>! pip install PACKAGE</code>, to install any Python pip PACKAGE, because first command uses exactly python that you use to run the script, while second may use <code>pip</code> from other Python installation, not from Python installation that is used to run actual script later.</p>
",8,9,9117,2021-02-27 06:11:15,https://stackoverflow.com/questions/66396100/google-colab-glove-python-pip-install-not-working
"Word Embeddings on Mobiles (Android, iOS)","<p>I would like to use Word Embeddings (aka GloVe) in my Mobile (Android, iOS) app. Nevertheless, the pre-trained weights file is large (starting from 100MB and up to 800MB). what are the generally accepted approaches for storing and using large pre-trained models on Mobiles?</p>
","android, ios, machine-learning, nlp, stanford-nlp","<p>Pretrained embeddings have massive vocabularies, and while you don't need to store each word's vector you do need the actual string to determine the input to the model. You'll either need to restrict the vocabulary or allow users to query your model over the web. If you need the model on-device make sure you load it asynchronously, since it will hang your threads.</p>
",1,-2,461,2021-03-03 11:02:35,https://stackoverflow.com/questions/66455708/word-embeddings-on-mobiles-android-ios
Key error while comparing the similarity between two statements using glove vectors,"<p>I am novice to NLP to be honest and I am trying to use GLOVE vectors for finding the similarity between two statements and I am getting a key error. Please let me know where I am wrong.
Thanks in advance for your help and if there are other better ways of measuring the similarity between the statements,please let me know.</p>
<pre><code>gloveFile = &quot;/content/glove.6B.50d.txt&quot;
import numpy as np
def loadGloveModel(gloveFile):
    print (&quot;Loading Glove Model&quot;)
    with open(gloveFile, encoding=&quot;utf8&quot; ) as f:
        content = f.readlines()
        print(content)
    model = {}
    for line in content:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print (&quot;Done.&quot;,len(model),&quot; words loaded!&quot;)
    return model

import re
from nltk.corpus import stopwords
import pandas as pd

def preprocess(raw_text):

    # keep only words
    letters_only_text = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, raw_text)

    # convert to lower case and split 
    words = letters_only_text.lower().split()

    # remove stopwords
    stopword_set = set(stopwords.words(&quot;english&quot;))
    cleaned_words = list(set([w for w in words if w not in stopword_set]))

    return cleaned_words

def cosine_distance_wordembedding_method(s1, s2):
    import scipy
    vector_1 = np.mean([model[word] for word in preprocess(s1)],axis=0)
    vector_2 = np.mean([model[word] for word in preprocess(s2)],axis=0)
    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)
    print('Word Embedding method with a cosine distance asses that our two sentences are similar to',round((1-cosine)*100,2),'%')

model = loadGloveModel(gloveFile)
for i in list121:
  cosine_distance_wordembedding_method(str4,i)
</code></pre>
<p>And then I got the error like:</p>
<pre><code>&lt;ipython-input-54-d463b41223c3&gt; in cosine_distance_wordembedding_method(s1, s2)
     36     import scipy
     37     vector_1 = np.mean([model[word] for word in preprocess(s1)],axis=0)
---&gt; 38     vector_2 = np.mean([model[word] for word in preprocess(s2)],axis=0)
     39     cosine = scipy.spatial.distance.cosine(vector_1, vector_2)
     40     print('Word Embedding method with a cosine distance asses that our two sentences are similar to',round((1-cosine)*100,2),'%')

&lt;ipython-input-54-d463b41223c3&gt; in &lt;listcomp&gt;(.0)
     36     import scipy
     37     vector_1 = np.mean([model[word] for word in preprocess(s1)],axis=0)
---&gt; 38     vector_2 = np.mean([model[word] for word in preprocess(s2)],axis=0)
     39     cosine = scipy.spatial.distance.cosine(vector_1, vector_2)
     40     print('Word Embedding method with a cosine distance asses that our two sentences are similar to',round((1-cosine)*100,2),'%')

KeyError: 'vehcile'
</code></pre>
","nlp, stanford-nlp, similarity","<p>I have found my mistake and I am just keeping this question so that somebody may get help.
The mistake I did is I have typed a wrong spelling like &quot;Vehcile&quot; instead of &quot;vehicle&quot;.</p>
",0,0,394,2021-04-24 15:31:32,https://stackoverflow.com/questions/67244484/key-error-while-comparing-the-similarity-between-two-statements-using-glove-vect
Adding metadata into Stanford coreNLP input,"<p>I have a large corpus of sentences (~ 1.1M) to parse through Stanford Core NLP but in the output I get more sentences than in the input, probably the system segments some sentences beyond the given segmentation into lines.</p>
<p>To control what happens I would like to include &quot;tags&quot; into the input. These tags should be recognizable in the output and should not affect parsing.</p>
<p>Something like</p>
<pre><code>&lt;0001&gt;
I saw a man with a telescope .
&lt;/0001&gt;
</code></pre>
<p>or</p>
<pre><code>#0001#
I saw a man with a telescope .
#/0001#
</code></pre>
<p>I have tried many formats, in all cases the &quot;tag&quot; has been parsed as if it were part of the text.</p>
<p>Is there some way to tell the parser &quot;do not parse this, just keep it <em>as is</em> in the output&quot;?</p>
<p>===A few hours later===</p>
<p>As I'm getting no answer, here is an example: I would like to process the sentence “Manon espérait secrètement y revoir un garçon qui l'avait marquée autrefois.” that carries tag <code>151_0_4</code>. I imagined to write the tag between two rows of equal signs on a separate line, followed by a period, to be sure that the tag will, at worst, be processed as a separate sentence:</p>
<pre><code>=====151_0_4======.
Manon espérait secrètement y revoir un garçon qui l'avait marquée autrefois.
=====151_0_4======.
</code></pre>
<p>And here is what this produced:</p>
<pre><code>(ROOT (SENT (NP (SYM =)) (NP (SYM =) (PP (SYM =) (NP (SYM =) (PP (SYM =) (NP (NUM 151_0_4) (SYM =) (SYM =) (NP (SYM =) (PP (SYM =) (NP (SYM =) (SYM =))))))))) (PUNCT .)))
</code></pre>
<p>As you see the tags are definitely considered as being part of the sentence, no way to separate them from it.</p>
<p>Same thing happened with XML-like tags <code>&lt;x151_0_4&gt;</code> or tags using the hash character...</p>
",stanford-nlp,"<p>If your current data is strictly one sentence per line, then by far the easiest thing to do is to just leave it like that and to give the option <code>-ssplit.eolonly=true</code>.</p>
<p>There unfortunately isn't an option to pass through certain kinds of meta-data or delimiters without attempting to parse or process them. However, you can indicate that they should not be made part of other sentences by means of the <code>ssplit.boundaryTokenRegex</code> or <code>ssplit.boundaryMultiTokenRegex</code> properties. However, your choices are then either to just delete them (see <code>ssplit.tokenPatternsToDiscard</code>) or else to process them as weird sentences, which you'd then need to clean up.</p>
",1,0,51,2021-05-05 08:33:40,https://stackoverflow.com/questions/67397774/adding-metadata-into-stanford-corenlp-input
Stanford CoreNLP output is very slow in Python,"<p>I'm using NLTK's StanfordDependencyParser to generate dependency trees. Here is the code</p>
<pre class=""lang-py prettyprint-override""><code>cpath = &quot;path to stanford-corenlp-4.2.0-models-english.jar&quot; + os.pathsep + &quot;path to stanford-parser.jar&quot;

if cpath not in os.environ['CLASSPATH']:
     os.environ['CLASSPATH'] = cpath + os.pathsep + os.environ['CLASSPATH']

# TODO: DEPRECATED
# self.dependency_parser_instance_corenlp = StanfordDependencyParser(path_to_models_jar=&quot;path to stanford-corenlp-4.2.0-models-english.jar&quot;, encoding='utf8')

dependencies = [list(parse.triples()) for parse in self.dependency_parser_instance_corenlp.raw_parse(query)]

# Encode every string in tree to utf8 so string matching will work
for dependency in dependencies[0]:
    dependency[0][0].encode('utf-8')
    dependency[0][1].encode('utf-8')
    dependency[1].encode('utf-8')
    dependency[2][0].encode('utf-8')
    dependency[2][1].encode('utf-8')
</code></pre>
<p>For a sentence with 10 words, it takes around 1.5 seconds to generate the output. Is this expected? Can you please steps to improve speed?
I've already tried using SR parser and removing all the extra folders(like coref, lexparser, ner, tagger) from the models JAR.</p>
","python, nltk, stanford-nlp","<p>Yes, the (old NLTK implementation) <code>StanfordDependencyParser</code> is very slow. You should not use it. You should use the classes and methods in the <a href=""http://www.nltk.org/api/nltk.parse.html#module-nltk.parse.corenlp"" rel=""nofollow noreferrer"">nltk.parse.corenlp</a> module, which are <em>much</em> faster.</p>
<p>For more info, see the notes in the <a href=""https://stanfordnlp.github.io/CoreNLP/other-languages.html#python"" rel=""nofollow noreferrer"">CoreNLP documentation</a> or <a href=""https://www.districtdatalabs.com/syntax-parsing-with-corenlp-and-nltk"" rel=""nofollow noreferrer"">this tutorial</a>.</p>
",3,4,337,2021-06-14 12:12:28,https://stackoverflow.com/questions/67969998/stanford-corenlp-output-is-very-slow-in-python
Extracting human names from text data using python stanza,"<p>I have a dataset containing the string value of book title pages (e.g. all words on the title page, each line of my txt file is a different book). From this I am trying to retrieve the author's name as the human name which appears on the title page, and store each name on a separate line in a csv file. When I type the following code I get a &quot;no author&quot; value for every entry, which is not plausible based on the input data. Can someone help me figure out what is going wrong? Thanks, I have been racking my head on this for the past few days with no results.</p>
<pre><code>import stanza 
import csv
stanza.download('en') 

nlp = stanza.Pipeline('en')

def get_human_names(text,output):
    with open(text, 'r', encoding = &quot;ISO-8859-1&quot;) as txt_file:
        Lines=txt_file.readlines()
        person_list=[]
        for line in Lines:
            doc=nlp(str(line))
            for sent in doc.sentences:
                for token in sent.tokens:
                    if {token.ner}=='B-PERSON' or {token.ner}=='E-PERSON':
                        person_list.append({token.text})
                if(len(person_list)==0): ## avoid skipping entries in the output file
                    person_list=[&quot;no author&quot;]
            with open(output, 'a') as csv_output:
                writer=csv.writer(csv_output)
                writer.writerow(person_list)

get_human_names('/Users/tancredirapone/Desktop/LoC_Project/titles.txt','/Users/tancredirapone/Desktop/LoC_Project/titles_author_stanza.csv')
</code></pre>
","python, stanford-nlp","<p>In case anyone has a similar issue... This seems to work, but the results are not altogether satisfactory (i.e. several names missed). I don't know if this is because of the code I wrote or just stanza missing names once in a while, but I suspect it's the latter.</p>
<pre><code>import csv
import stanza
stanza.download('en')
nlp=stanza.Pipeline('en')


with open('/Users/tancredirapone/Desktop/LoC_Project/titles.csv', 'r', encoding = &quot;ISO-8859-1&quot;) as txt_file:
        reader=csv.reader(txt_file)
        for row in reader:
            person_list=[]
            doc=nlp(str(row))
            for i, sentence in enumerate(doc.sentences):
                for token in sentence.tokens:
                    if &quot;PERSON&quot; in str({token.ner}):
                        person_list.append({token.text})
            if len(person_list)==0:
                person_list=[&quot;no author&quot;]
            with open('/Users/tancredirapone/Desktop/LoC_Project/author_names.csv', 'a') as csv_output:
                writer=csv.writer(csv_output)
                writer.writerow(person_list)
            person_list=[]   

</code></pre>
<p>a possibility is that perhaps stanza misses foreign names, but as far as I know it's not possible to create a pipeline with multiple languages (nlp=stanza.Pipeline('en', 'de', 'fr' ...).</p>
",0,2,1486,2021-06-30 13:13:59,https://stackoverflow.com/questions/68195506/extracting-human-names-from-text-data-using-python-stanza
How can you ensure a viable endpoint for a stanza CoreNLPClient?,"<p>I would like to use the stanza CoreNLPClient to extract noun phrases, similar to <a href=""https://stackoverflow.com/questions/61633485/extract-noun-phrases-with-stanza-and-corenlpclient"">this method</a>.</p>
<p>However, I cannot seem to find a good port to start the server on. The default is 9000, but this is often occupied, as indicated by the error message:</p>
<blockquote>
<p>PermanentlyFailedException: Error: unable to start the CoreNLP server
on port 9000 (possibly something is already running there)</p>
</blockquote>
<p>EDIT: Port 9000 is in use by python.exe, which is why I can't just shut the process down to make space for the CoreNLPClient.</p>
<p>Then, when I select other ports such as 7999, 8000, or 8080, the server keeps listening indefinetely, not executing the consecutive code lines, showing only the following:</p>
<blockquote>
<p>2021-07-19 12:05:55 INFO: Starting server with command: java -Xmx8G -cp C:\Users\timjo\stanza_corenlp* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 7998 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-2e15724b8064491b.props -preload -outputFormat serialized</p>
</blockquote>
<p>I have the latest version of stanza installed, and am running the following code from an .ipynb file in VS Code:</p>
<pre><code># sample sentence
sentence = &quot;Albert Einstein was a German-born theoretical physicist.&quot; 

# start the client as indicated in the docs
with CoreNLPClient(properties='corenlp_server-2e15724b8064491b.props', endpoint='https://localhost:7998', memory='8G', be_quiet=True) as client:
     matches = client.tregex(text=sentence, pattern = 'NP')

# extract the noun phrases and their indices
noun_phrases = [[text, begin, end] for text, begin, end in
     zip([sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence],
         [sentence[match_id]['characterOffsetBegin'] for sentence in matches['sentences'] for match_id in sentence],
         [sentence[match_id]['characterOffsetEnd'] for sentence in matches['sentences'] for match_id in sentence])]
</code></pre>
<p>Main question: <strong>How can I ensure that the server starts on an open port, and closes afterwards?</strong>   I would prefer having a semi-automatic way to finding open / shutting down occupied ports for the client to run on.</p>
","nlp, python-3.7, stanford-nlp, stanza","<p>In general it is sufficient to choose another number that nothing else is using – maybe 9017? There are lots of numbers to choose from! But the more careful choice would be to create the CoreNLPClient in a while loop with a try/catch and to increment the port number till you found one that was open.</p>
",1,1,194,2021-07-19 10:10:00,https://stackoverflow.com/questions/68438711/how-can-you-ensure-a-viable-endpoint-for-a-stanza-corenlpclient
Meaning of output/training status of 256 in Stanford NLP NER?,"<p>I have a Python program where I am using os.sys to train the Stanford NER from the command line. This returns an output/training status which I save in the variable &quot;status&quot;, and it is usually 0. However, I just ran it and got an output of 256, as well as not creating a file for the trained model. This error is only occurring for larger sets of training data. I searched through the documentation on the Stanford NLP website and there doesn't seem to be info on the meanings of the outputs or why increasing training data might affect the training. Thanks in advance for any help and problem code is below.</p>
<pre><code>cmdToSys = &quot;java -mx20g -cp stanford-corenlp-4.2.2.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop &quot; + self.trainPropFileName + &quot; -ner.useSUTime false test -ner.applyNumericClassifiers false test &quot;

status = os.system(cmdToSys)
</code></pre>
<p>note: self.trainPropFileName is just the property file</p>
","python, nlp, stanford-nlp, named-entity-recognition","<p>Status is an exit code, and non-zero exit codes mean your program failed. This is not a Stanford NLP convention, it's how all programs work on Unix/Linux.</p>
<p>There should be an error somewhere, maybe you ran out of memory? You'll have to track that down to find out what's wrong.</p>
",1,2,77,2021-07-27 14:34:44,https://stackoverflow.com/questions/68546867/meaning-of-output-training-status-of-256-in-stanford-nlp-ner
Stanford Classifier producing wrong results,"<p>I'm trying to perform a basic text classification with the <a href=""https://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">Stanford Classifier</a>. My example data set is based on <a href=""https://raw.githubusercontent.com/bigmlcom/python/master/data/spam.csv"" rel=""nofollow noreferrer"">Ham or Spam</a>.</p>
<p>This is my code:</p>
<pre><code>Properties props = new Properties();
ColumnDataClassifier cdc = new ColumnDataClassifier(props);

Classifier&lt;String, String&gt; cl = cdc.makeClassifier(cdc.readTrainingExamples(&quot;data.train&quot;));

for (String line : ObjectBank.getLineIterator(&quot;data.test&quot;, &quot;utf-8&quot;)) {
    Datum&lt;String, String&gt; d = cdc.makeDatumFromLine(line);
    System.out.println(line + &quot;  ==&gt;  &quot; + cl.classOf(d));
}
</code></pre>
<p>However, whatever text I try to classify, it always classifies it as Ham. The following sentence is clearly Spam, still it is classified as Ham:</p>
<blockquote>
<p>FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For terms &amp; conditions, visit <a href=""http://www.example.com"" rel=""nofollow noreferrer"">www.example.com</a></p>
</blockquote>
<p>Where is my mistake?</p>
","java, machine-learning, stanford-nlp, text-classification","<p>My mistake was that I didn't provide any properties:</p>
<pre><code>Properties props = new Properties();
ColumnDataClassifier cdc = new ColumnDataClassifier(props);
</code></pre>
<p>You can either specify the properties directly in the code:</p>
<pre><code>// set up pipeline properties
Properties props = new Properties();
// set the list of annotators to run
props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote&quot;);
// set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
props.setProperty(&quot;coref.algorithm&quot;, &quot;neural&quot;);
</code></pre>
<p>Or you can provide a properties file:</p>
<pre><code>ColumnDataClassifier cdc = new ColumnDataClassifier(propFile);
</code></pre>
",1,1,117,2021-07-28 17:01:51,https://stackoverflow.com/questions/68564577/stanford-classifier-producing-wrong-results
How to run &#39;run_squad.py&#39; on google colab? It gives &#39;invalid syntax&#39; error,"<p>I downloaded the file first using:</p>
<pre><code>!curl -L -O https://github.com/huggingface/transformers/blob/master/examples/legacy/question-answering/run_squad.py
</code></pre>
<p>Then used following code:</p>
<pre><code>!python run_squad.py  \
    --model_type bert   \
    --model_name_or_path bert-base-uncased  \
    --output_dir models/bert/ \
    --data_dir data/squad   \
    --overwrite_output_dir \
    --overwrite_cache \
    --do_train  \
    --train_file /content/train.json   \
    --version_2_with_negative \
    --do_lower_case  \
    --do_eval   \
    --predict_file /content/val.json   \
    --per_gpu_train_batch_size 2   \
    --learning_rate 3e-5   \
    --num_train_epochs 2.0   \
    --max_seq_length 384   \
    --doc_stride 128   \
    --threads 10   \
    --save_steps 5000 
</code></pre>
<p>Also tried following:</p>
<pre><code>!python run_squad.py \
  --model_type bert \
  --model_name_or_path bert-base-cased \
  --do_train \
  --do_eval \
  --do_lower_case \
  --train_file /content/train.json \
  --predict_file /content/val.json \
  --per_gpu_train_batch_size 12 \
  --learning_rate 3e-5 \
  --num_train_epochs 2.0 \
  --max_seq_length 584 \
  --doc_stride 128 \
  --output_dir /content/
</code></pre>
<p>The error says in both the codes:</p>
<blockquote>
<p>File &quot;run_squad.py&quot;, line 7

^ SyntaxError: invalid syntax</p>
</blockquote>
<p>What exactly is the issue? How can I run the <code>.py</code> file?</p>
","google-colaboratory, stanford-nlp, bert-language-model, huggingface-transformers, nlp-question-answering","<p>SOLVED: It was giving error because I was downloading the github link rather than the script in github. Once I copied and used 'Raw' link to download the script, the code ran.</p>
",2,0,507,2021-07-30 10:12:43,https://stackoverflow.com/questions/68589222/how-to-run-run-squad-py-on-google-colab-it-gives-invalid-syntax-error
How to configure and train the model using Glove and CNN for text classification?,"<p>I have worked with text classification using Glove and CNN and found the problem below:</p>
<pre><code>File &quot;c:\programfiles_anaconda\anaconda3\envs\math_stat_class\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 1657, in _create_c_op
    raise ValueError(str(e))

ValueError: Negative dimension size caused by subtracting 5 from 1 for '{{node max_pooling1d_9/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=&quot;NHWC&quot;, ksize=[1, 5, 1, 1], padding=&quot;VALID&quot;, strides=[1, 5, 1, 1]](max_pooling1d_9/ExpandDims)' with input shapes: [?,1,1,128].
</code></pre>
<h3>Glove input</h3>
<pre class=""lang-py prettyprint-override""><code>EMBEDDING_DIM = 100
    
embeddings_index = {}
    
f = open(glove_path, encoding='utf-8')  
for line in f:    
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
    f.close()
    
print('Found %s word vectors.' % len(embeddings_index))
    
embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
    
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
</code></pre>
<h3>Layer input of CNN</h3>
<pre class=""lang-py prettyprint-override""><code># apply embedding matrix into an Embedding layer
# trainable=False to prevent the weights from being updated during training
embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)
</code></pre>
<h3>Training 1D CNN</h3>
<pre class=""lang-py prettyprint-override""><code>sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

x = Conv1D(128, 5, activation='relu')(embedded_sequences)   
print(&quot;x shape = &quot;, x)

x = MaxPooling1D(5)(x)  
print(&quot;x shape = &quot;, x)
        
x = Conv1D(128, 5, activation='relu')(x)
print(&quot;x shape = &quot;, x)
    
#-----This line below produced error-----
x = MaxPooling1D(5)(x) #Error this line
#-----This line above produced error-----
        
print(&quot;x shape = &quot;, x)

x = Conv1D(128, 5, activation='relu')(x)
print(&quot;x shape = &quot;, x)
    
x = MaxPooling1D(35)(x)  # global max pooling
print(&quot;x shape = &quot;, x)
    
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
    
preds = Dense(len(labels_index), activation='softmax')(x)
model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])
    
# Learning
model.fit(X_train, y_train, validation_data=(X_val, y_val),
          epochs=2, batch_size=128)
</code></pre>
<h3>My ideas</h3>
<p><strong>1) Are there some issues/problems with Glove input?</strong></p>
<p><strong>2) Conv1D:</strong></p>
<ul>
<li>Change the &quot;kernel_size&quot; from 5 to a New Value.</li>
</ul>
<p><strong>3) MaxPooling1D:</strong></p>
<ul>
<li>Change pool_size from 5 to a New Value.</li>
<li>Specify other parameters: strides, padding and so on.</li>
</ul>
<p><strong>4) I currently use keras on tensorflow 2.20 and python 3.6</strong></p>
<ul>
<li>Do I need to upgrade tensorflow and python?</li>
</ul>
<p>However, I could not figure out any better way to do. May I have your suggestions?</p>
","python, tensorflow, conv-neural-network, stanford-nlp, text-classification","<p>Two things that come to my mind: your max-pooling layers are reducing the size of the input to the next convolutional layers every time and eventually the size is too small to run another max-pooling operation. Try running</p>
<pre><code> tf.print(model.summary) 
</code></pre>
<p>after each max-pooling operation and you will quickly find out that your tensor cannot be further reduced. You can then consider using a different <code>pool_size</code> in your max-pooling layers.</p>
<p>The second thing I notice (I am not sure if it is intentional), but <em>MaxPooling1D != Global Max Pooling</em>. Keras supports both <a href=""https://keras.io/api/layers/pooling_layers/"" rel=""nofollow noreferrer"">operations</a>. Take a look at the documentation.</p>
<p>On a side note, sentence classification with CNNs was widely popularized by the <a href=""https://arxiv.org/pdf/1408.5882.pdf"" rel=""nofollow noreferrer"">work</a> of Yoon Kim. In his work, he shows that global max-pooling operations perform much better than striding max-pooling operations in sentence classification (when using word embeddings, as you are doing).</p>
",0,0,257,2021-10-12 01:01:05,https://stackoverflow.com/questions/69533961/how-to-configure-and-train-the-model-using-glove-and-cnn-for-text-classification
Stanford CoreNLP - Unknown variable WORKDAY,"<p>I am processing some documents and I am getting many WORKDAY messages as seen below.
There's a similar issue posted <a href=""https://stackoverflow.com/questions/36449826/stanford-corenlp-unknown-variable-weekday"">here</a> for WEEKDAY. Does anyone know how to deal with this message. I am running corenlp in a Java server on Windows and accessing it using Juypyter Notebook  and Python code.</p>
<pre><code>[pool-2-thread-2] INFO edu.stanford.nlp.ling.tokensregex.types.Expressions - Unknown variable: WORKDAY
[pool-2-thread-2] INFO edu.stanford.nlp.ling.tokensregex.types.Expressions - Unknown variable: WORKDAY
[pool-2-thread-2] INFO edu.stanford.nlp.ling.tokensregex.types.Expressions - Unknown variable: WORKDAY
[pool-1-thread-7] WARN CoreNLP - java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error making document
</code></pre>
",stanford-nlp,"<p>This is an error in the current SUTime rules file (and it's actually been there for quite a few versions). If you want to fix it immediately, you can do the following. Or we'll fix it in the next release. These are Unix commands, but the same thing will work elsewhere except for how you refer to and create folders.</p>
<p>Find this line in <code>sutime/english.sutime.txt</code> and delete it. Save the file.</p>
<p><code>  { (/workday|work day|business hours/) =&gt; WORKDAY }</code></p>
<p>Then move the file to the right location for replacing in the jar file, and then replace it in the jar file. In the root directory of the CoreNLP distribution do the following (assuming you don't already have an <code>edu</code> file/folder in that directory):</p>
<pre><code>mkdir -p edu/stanford/nlp/models/sutime
cp sutime/english.sutime.txt edu/stanford/nlp/models/sutime
jar -uf stanford-corenlp-4.2.0-models.jar edu/stanford/nlp/models/sutime/english.sutime.txt
rm -rf edu
</code></pre>
",4,0,303,2021-11-13 14:38:52,https://stackoverflow.com/questions/69955279/stanford-corenlp-unknown-variable-workday
next release of Stanza,"<p>I'm interested in the Stanza constituency parser for Italian.
In <a href=""https://stanfordnlp.github.io/stanza/constituency.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/constituency.html</a> it is said that a new release with updated models (including an Italian model trained on the Turin treebank) should have been available in mid-November.
Any idea about when the next release of Stanza will appear?
Thanks
alberto</p>
",stanford-nlp,"<p>Technically you can already get it!  If you install the dev branch of stanza, you should be able to download an IT parser.</p>
<pre><code>pip install git+git://github.com/stanfordnlp/stanza.git@704d90df2418ee199d83c92c16de180aacccf5c0


stanza.download(&quot;it&quot;)
</code></pre>
<p>It's trained on the Turin treebank, which has about 4000 trees.  If you download the Bert version of the model, it gets over 91 F1 on the Evalita test set (but has a length limit of about 200 words per sentence).</p>
<p>We might splurge on getting the VIT treebank or something.  I've been agitating that we use that budget on Danish or PT or some other language where we have very few users, but it's a hard sell...</p>
<p>Edit: there's also some scripts included for converting the publicly available Turin trees into brackets.  Their MWT annotation style was to repeat the MWT twice in a row, which doesn't doesn't work too well for a task like parsing raw text.</p>
",0,0,160,2021-12-14 15:03:37,https://stackoverflow.com/questions/70351073/next-release-of-stanza
How to load pre trained FastText Word Embeddings using Gensim?,"<p>I downloaded word embedding <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">from this link</a>. I want to load it in <code>Gensim</code> to do some work but I am not able to load it. I have found many resources and none of it is working. I am using <code>Gensim</code> version <code>4.1</code>.</p>
<p>I have tried</p>
<pre><code>gensim.models.fasttext.load_facebook_model('/home/admin1/embeddings/crawl-300d-2M.vec')
gensim.models.fasttext.load_facebook_vectors('/home/admin1/embeddings/crawl-300d-2M.vec')
</code></pre>
<p>and it is showing me</p>
<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>
<p>I went to try to load it using using <code>FastText.load('/home/admin1/embeddings/crawl-300d-2M.vec',)</code> but then it showed <code>UnpicklingError: could not find MARK</code>.</p>
<p>Also, using</p>
","nlp, stanford-nlp, gensim, fasttext","<p>Per the <code>NotImplementedError</code>, those are the one kind of full Facebook FastText model, <code>-supervised</code> mode, that Gensim does not support.</p>
<p>So sadly, the answer to &quot;How do you load these?&quot; is &quot;you don't&quot;.</p>
<p>The <code>.vec</code> files contain just the full-word vectors in a plain-text format – no subword info for synthesizing OOV vectors, or supervised-classification output features. Those can be loaded into a <code>KeyedVectors</code> model:</p>
<pre class=""lang-py prettyprint-override""><code>kv_model = KeyedVectors.load_word2vec_format('crawl-300d-2M.vec')
</code></pre>
",3,1,1126,2021-12-29 16:20:14,https://stackoverflow.com/questions/70522109/how-to-load-pre-trained-fasttext-word-embeddings-using-gensim
Gensim train not updating weights,"<p>I have a domain specific corpus for which I am trying to train embeddings. Since I want to be comprehensive in vocabulary, I am adding word vectors from <code>glove.6B.50d.txt</code>. Post adding vectors from here, I am training the model using the corpus I have.</p>
<p>I am trying the solutions from <a href=""https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights"">here</a> but the word embeddings don't seem to update.</p>
<p>This is the solution I have so far.</p>
<pre><code>#read glove embeddings
glove_wv = KeyedVectors.load_word2vec_format(GLOVE_PATH, binary=False)

#initialize w2v model
model =  Word2Vec(vector_size=50, min_count=0, window=20, epochs=10, sg=1, workers=10, 
                      hs=1, ns_exponent=0.5, seed=42, sample=10**-2, shrink_windows=True)
model.build_vocab(sentences_tokenized)
training_examples_count = model.corpus_count

# add vocab from glove
model.build_vocab([list(glove_wv.key_to_index.keys())], update=True)
model.wv.vectors_lockf = np.zeros(len(model.wv)) # ALLOW UPDATE OF WEIGHTS FROM BACK PROP; 0 WILL SUPPRESS

# add glove embeddings
model.wv.intersect_word2vec_format(GLOVE_PATH,binary=False, lockf=1.0)
</code></pre>
<p>Below I am training the model and checking word embedding of a particular word explicitly present in training</p>
<pre><code># train model
model.train(sentences_tokenized,total_examples=training_examples_count, epochs=model.epochs)

#CHECK IF EMBEDDING CHANGES FOR 'oyo'
print(model.wv.get_vector('oyo'))
print(glove_wv.get_vector('oyo'))
</code></pre>
<p>The word embeddings of the word <code>oyo</code> comes out to be same before and after the training. Where am I going wrong?</p>
<p>The input corpus- <code>sentences_tokenized</code> contains few sentences that contains the word <code>oyo</code>. One of such sentences-</p>
<pre><code>'oyo global platform empowers entrepreneur small business hotel home providing full stack technology increase earnings eas operation bringing affordable trusted accommodation guest book instantly india largest budget hotel chain oyo room one preferred hotel booking destination vast majority student country hotel chain offer many benefit include early check in couple room id card flexibility oyo basically network budget hotel completely different famous hotel aggregator like goibibo yatra makemytrip partner zero two star hotel give makeover room bring customer hotel website mobile app'
</code></pre>
","python, stanford-nlp, gensim, word2vec, word-embedding","<p>You're improvising a lot here with a bunch of potential errors or suboptimalities. Note especially that:</p>
<ul>
<li>While (because it's Python) you can always mutate the models however you want for interesting effects, seeding a model with outside word-vectors then continuing training isn't formally- or well-supported by Gensim. As far as I can tell – &amp; I wrote a bunch of this code! – there aren't any good docs/examples of doing it well, or doing the necessary tuning/validation of results, or demonstrating a reliable advantage of this technique. Most examples online are of eager people plowing ahead unaware of the tradeoffs, seeing a trivial indicator of completion or a tiny bit of encouraging results, and then overconfidently showing their work as if this were a well-grounded technique or best-practice. It isn't. Without a deep understanding of the model, &amp; review of the source code, &amp; regular re-checking of your results for sanity/improvement, there will be hidden gotchas. It is especially the case that fresh training on just a subset of all words could pull those words out of compatible coordinate alignment with other words not receiving training.</li>
<li>The <code>intersect_word2vec_format()</code> feature, and especially the <code>lockf</code> function, are also experimental - one stab at maybe offering a way to mix in other word-vectors, but without any theoretical support. (I also believe <code>intersect_word2vec_format()</code> remains slightly broken in recent (circa 4.1.2) Gensim versions, though there may be a <a href=""https://github.com/RaRe-Technologies/gensim/issues/3094"" rel=""nofollow noreferrer"">simple workaround</a>.) Still, the <code>lockf</code> functionality may require tricky manual initialization &amp; adaptation to other non-standard steps. To use it, it'd be best to read &amp; understand the Gensim source code where related variables appear.</li>
</ul>
<p>So, if you really need a larger vocabulary than your initial domain-specific corpus, the safest approach is probably to extend your training corpus with more texts that feature the desired words, as used in similar language contexts. (For example, if you rdomain is scientific discourse, you'd want to extend your corpus with more similar scientific text to learn compatible words – not, say, classic fiction.) Then all words go through the well-characterized simultaneous training process.</p>
<p>That said, if you really want to continue experimenting with this potentially complicated and error-prone improvised approach, your main problems might be:</p>
<ul>
<li>using strings as your sentences instead of lists-of-tokens (so the training 'words' wind up actually just being single-characters)</li>
<li>something related to the <code>intersect_word2vec_format</code> bug; check if <code>.vectors_lockf</code> is the right length, with <code>1.0</code> in the all the right slots for word-updates, before training</li>
</ul>
<p>Separately, other observations:</p>
<ul>
<li><code>min_count=0</code> is usually a bad idea: these models improve when you discard rare words entirely. (Though, when doing a <code>.build_vocab(…, update=True)</code> vocab-expansion, a bunch of things with the usual neat handling of low-frequency words and frequency-sorted vocabularies become screwy.)</li>
<li><code>hs=1</code> should generally not be set without also disabling the usually-preferred default negative-sampling with <code>negative=0</code>. (Otherwise, you're creating a hybrid franken-model, using both modes on one side of the internal neural network, that share the same input word-vectors: a much slower approach not especially likely to be better than either alone.)</li>
<li><code>ns_exponent=0.5</code> is non-standard, and using non-standard values for the parameter is most-likely to offer benefit in peculiar situations (like training texts that aren't true natural language sentences), and should only be tweaked within a harness for comparing results with alternate values.</li>
<li><code>sample=10**-2</code> is also non-standard, and such a large value might be nearly the same as turning off <code>sample</code> (say with a <code>0</code> value) entirely. It's more common to want to make this parameter more-aggressive (smaller than the default), if you have plentiful training data.</li>
</ul>
<p>In general, while the defaults aren't sacred, you generally should avoid tinkering with them until you have both (a) a good idea of why your corpus/goals might benefit from a different value; &amp; (b) a system for verifying which alterations are helping or hurting, such as a grid-search over many parameter combinations that scores the fitness of resulting models on (some proxy for) your true end task.</p>
",1,1,392,2021-12-30 14:48:04,https://stackoverflow.com/questions/70533179/gensim-train-not-updating-weights
Error while loading vector from Glove in Spacy,"<p>I am facing the following attribute error when loading glove model:</p>
<p>Code used to load model:</p>
<pre><code>nlp = spacy.load('en_core_web_sm')
tokenizer = spacy.load('en_core_web_sm', disable=['tagger','parser', 'ner', 'textcat'])
nlp.vocab.vectors.from_glove('../models/GloVe')
</code></pre>
<p>Getting the following atribute error when trying to load the glove model:</p>
<pre><code>AttributeError: 'spacy.vectors.Vectors' object has no attribute 'from_glove'
</code></pre>
<p>Have tried to search on StackOverflow and elsewhere but can't seem to find the solution. Thanks!</p>
<p>From pip list:</p>
<ul>
<li>spacy version: 3.1.4</li>
<li>spacy-legacy 3.0.8</li>
<li>en-core-web-sm 3.1.0</li>
</ul>
","python, python-3.x, nlp, spacy, stanford-nlp","<p>Use <code>spacy init vectors</code> to load vectors from word2vec/glove text format into a new pipeline: <a href=""https://spacy.io/api/cli#init-vectors"" rel=""nofollow noreferrer"">https://spacy.io/api/cli#init-vectors</a></p>
",2,2,498,2022-03-17 12:10:32,https://stackoverflow.com/questions/71512064/error-while-loading-vector-from-glove-in-spacy
Convert constituent string to Tree object (Stanza),"<p>I am doing some experiments in <code>Python</code> with <code>Stanza</code> and I have converted a lot of sentences into <a href=""https://stanfordnlp.github.io/stanza/data_objects.html#parsetree"" rel=""nofollow noreferrer"">ParseTree</a> objects and saved them into a file like this:</p>
<pre><code># sent_id = Not relevant to the question
# text = Might be ok for some , but not for me
(S (NONE Might be) (OPINION_1_Negative_Standard (POLAR_EXPRESSION ok)) (NONE for some , but) (OPINION_0_Negative_Standard (POLAR_EXPRESSION not)) (NONE for) (ALL_01 (SOURCE me)))
</code></pre>
<p>And now, I want to read that file, get the strings of the <em>Tree</em> and convert them again into a <code>ParseTree</code> object, but I am not able to find how it should be done properly.</p>
<p>I checked <a href=""https://stanfordnlp.github.io/stanza/data_conversion.html"" rel=""nofollow noreferrer"">Data Conversion</a>, but I can not find what I am trying to do, and the <a href=""https://stanfordnlp.github.io/stanza/constituency.html"" rel=""nofollow noreferrer"">Constituency Parser</a> examples show how to get the string representation but not how to convert that string into a Tree.</p>
<p>Regards.</p>
","python, stanford-nlp, parse-tree","<p>I have found the solution reading the source code in their GitHub.</p>
<p>Inside the <a href=""https://github.com/stanfordnlp/stanza/tree/main/stanza/models/constituency"" rel=""nofollow noreferrer"">constituency</a> module, in the <a href=""https://github.com/stanfordnlp/stanza/blob/main/stanza/models/constituency/tree_reader.py"" rel=""nofollow noreferrer"">tree_reader</a> file there is a method called <code>read_trees(text)</code> that it does exactly what I wanted.</p>
<p>Regards.</p>
",1,1,747,2022-03-31 12:53:43,https://stackoverflow.com/questions/71692836/convert-constituent-string-to-tree-object-stanza
Extract meaningful words from spaceless texts,"<p>I have not done much NLP, but have a need. For example for the string 'australiafreedomrally', I need to automatically extract meaningful words i.e., 'australia', 'freedom' and 'rally'.</p>
<p>Is there any python package that can do it? Thanks</p>
","nlp, nltk, stanford-nlp, spacy, gensim","<p>Check out <a href=""https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words"">this thread</a>, where among other things a package is mentioned which does this. Generally an approach with a predefined list of common words can get you far. Your question has an overlap with the task of <a href=""https://sites.google.com/view/icdar2019-postcorrectionocr"" rel=""nofollow noreferrer"">Optical Character Recognition (OCR) Post Correction</a> which you can find some pretrained models for, although the problem being that strongly shifted towards one issue (missing whitespace character) probably leads to it not performing too great.</p>
<p>If you want to really get into this topic you could try to train a new model on this task, I can imagine that recent popular transformer models which use subtoken-level embeddings for unknown words could be trained to bring a decent performance on this task since there are models which go into a similar direction as <a href=""https://huggingface.co/vennify/t5-base-grammar-correction"" rel=""nofollow noreferrer"">grammar correction</a> and <a href=""https://huggingface.co/flexudy/t5-base-multi-sentence-doctor"" rel=""nofollow noreferrer"">sentence boundary correction</a>. There are also some older, rule-based approach papers which call this problem &quot;word boundary detection&quot; or more specifcally &quot;agglutination&quot;, check out e.g. <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351975/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351975/</a>, but generally the amount of off-the-shelf solutions you find for that problems is quite low.</p>
",2,0,676,2022-04-08 18:24:55,https://stackoverflow.com/questions/71801656/extract-meaningful-words-from-spaceless-texts
How to build a normalized tf dataframe?,"<p>I want to apply this into my tf function. <a href=""https://i.sstatic.net/ttWLp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ttWLp.png"" alt=""enter image description here"" /></a> But unable to build the function.</p>
<hr />
<p>My dataset looks like this
<a href=""https://i.sstatic.net/unVrX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/unVrX.png"" alt=""enter image description here"" /></a></p>
<p>I have tried to buield the function like this</p>
<pre><code>def term_document_matrix(data, vocab_list = None, doc_index= 'ID', text= 'text'):
      tf_matirx = pd.DataFrame(columns=df[document_index], index= vocab).fillna(0)
    a = int(input(&quot;enter the value&quot;))
    for word in tf_matrix.index:
    
    for doc in data[document_index]:
        
        result = a + (1-a)*[data[data[document_index] == doc][text].values[0].count(word)/X]
        X = ????????
        tf_matrix.loc[word,doc] = result
return tf_matrix
</code></pre>
<p>But unable to build this completely.</p>
<p>Here parameters are described as below</p>
<blockquote>
<pre><code>parameter: 
</code></pre>
</blockquote>
<pre><code>    data: DataFrame. 
    Frequency of word calculated against the data.
    
    vocab_list: list of strings.
    Vocabulary of the documents    
    
    doc_index: str.
    Column name for document index in DataFrame passed.
    
    text: str
    Column name containing text for all documents in DataFrame,
    
returns:
    tf_matrix: DataFrame.
    DataFrame containing term document matrix.
    &quot;&quot;&quot;
</code></pre>
<p><strong>My goal is to get a dataframe like this</strong>
<a href=""https://i.sstatic.net/wXpQZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wXpQZ.png"" alt=""enter image description here"" /></a></p>
","python, nlp, stanford-nlp, information-retrieval","<p>You can determine tf dataframe by using CountVectorizer. Then divide each value by max value of it's column and repeat this process for every column in your dataframe</p>
<pre><code> df_1st = df.apply(lambda col: col / col.max())
</code></pre>
<p>and then just multiply and add a scaler for each element in your dataframe.</p>
<pre><code>df_2nd = df_1st.apply(lambda col: lambda + col*(1-lambda))
tf_matrix = df_2nd
</code></pre>
",1,2,90,2022-04-14 18:16:31,https://stackoverflow.com/questions/71876033/how-to-build-a-normalized-tf-dataframe
Can we deduce the relationship b/w a dimension of a word vector with the linguistic characteristic it represents?,"<p>Let's imagine we generated a 200 dimension word vector using any pre-trained model of the word ('hello') as shown in the below image.</p>
<p><a href=""https://i.sstatic.net/5TfJp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5TfJp.png"" alt=""Word_Vector"" /></a></p>
<p>So, by any means can we tell which linguistic feature is represented by each d_i of this vector?</p>
<p>For example, d1 might be looking at whether the word is a noun; d2 might tell whether the word is a named entity or not and so on.</p>
","nlp, stanford-nlp, word2vec, word-embedding","<p>Because these word vectors are dense distributional representations, it is often difficult / impossible to interpret individual neurons, and such models often do not localize interpretable features to a single neuron (though this is an active area of research). For example, see <a href=""https://arxiv.org/abs/2010.02695"" rel=""nofollow noreferrer"">Analyzing Individual Neurons in Pre-trained Language Models
</a> for a discussion of this with respect to pre-trained language models).</p>
<p>A common method for studying how individual dimensions contribute to a particular phenomenon / task of interest is to train a linear model (i.e., logistic regression if the task is classification) to perform the task from fixed vectors, and then analyze the weights of the trained linear model.</p>
<p>For example, if you're interested in part of speech, you can train a linear model to map from the word vector to the POS [1]. Then, the weights of the linear model represent a linear combination of the dimensions that are predictive of the feature. For example, if the weight on the 5th neuron has large magnitude (very positive or very negative), you might expect that neuron to be somewhat correlated with the phenomenon of interest.</p>
<p>[1]: Note that defining a POS for a particular word is nontrivial, since the POS often depends on context. For example, &quot;play&quot; can be a noun (&quot;he saw a play&quot;) or a verb (&quot;I will play in the grass&quot;).</p>
",2,0,54,2022-05-03 05:25:50,https://stackoverflow.com/questions/72095099/can-we-deduce-the-relationship-b-w-a-dimension-of-a-word-vector-with-the-linguis
Custom Model In Stanford NLP (CoreNLP) For Persian,"<p>I use Stanford NLP to extract data from text , knowing some code and work with this excellent library but some how , need to create my project in java that I know better than python , so found this code for use custom model :</p>
<pre><code>import stanfordnlp

config = {
    'processors': 'tokenize,mwt,pos,lemma,depparse', # Comma-separated list of processors to use
    'lang': 'fa', # Language code for the language to build the Pipeline in
    'tokenize_model_path': './PersianPT/fa_seraji_models/fa_seraji_tokenizer.pt', # Processor-specific arguments are set with keys &quot;{processor_name}_{argument_name}&quot;
    'mwt_model_path': './PersianPT/fa_seraji_models/fa_seraji_mwt_expander.pt',
    'pos_model_path': './PersianPT/fa_seraji_models/fa_seraji_tagger.pt',
    'pos_pretrain_path': './PersianPT/fa_seraji_models/fa_seraji.pretrain.pt',
    'lemma_model_path': './PersianPT/fa_seraji_models/fa_seraji_lemmatizer.pt',
    'depparse_model_path': './PersianPT/fa_seraji_models/fa_seraji_parser.pt',
    'depparse_pretrain_path': './PersianPT/fa_seraji_models/fa_seraji.pretrain.pt'
}
nlp = stanfordnlp.Pipeline(**config) # Initialize the pipeline using a configuration dict
doc = nlp(&quot;من عاشقت هستم&quot;) # Run the pipeline on input text

with open('./Desktop/NLP/out.txt', &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
    for sen in doc.sentences[0]._tokens :
        f.write(sen.words[0].text + '---Upos : ' +sen.words[0].upos + '---Xpos : ' +sen.words[0].xpos + '\n')
doc.sentences[0].print_tokens()

</code></pre>
<p>which work fine in python but when using java to Implement the code , don't know why the output not the same!</p>
<p>Java Code :</p>
<pre><code>public class TextAnalyzer {
    public static String text = &quot;&quot;&quot;
            در ابتدا، زندگی‌نامه‌ها به عنوان یک بخش از تاریخ با تمرکز بر یک فرد خاص، با اهمیت تاریخی در نظر گرفته شد. انواع مستقل زندگی‌نامه‌نویسی با تمایز از تاریخ عمومی از قرن ۱۸ ام شروع شده و فرم‌های معاصر آن به قرن بیستم می‌رسد.
            &quot;&quot;&quot;;

      public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        //props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote&quot;);
        //props.setProperty(&quot;coref.algorithm&quot;, &quot;neural&quot;);
        //props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote&quot;);
        //props.setProperty(&quot;processors&quot;, &quot;tokenize,mwt,pos,lemma,depparse&quot;);
        
        //props.setProperty(&quot;processors&quot;, &quot;tokenize, mwt, lemma, pos, depparse&quot;);
        props.setProperty(&quot;annotators&quot;, &quot;tokenize, ssplit, parse&quot;);
        
        //props.setProperty(&quot;lang&quot;, &quot;fa&quot;);
        //props.setProperty(&quot;use_gpu&quot;, &quot;true&quot;);
        props.setProperty(&quot;tokenize_model_path&quot;, BasicLocation.getBaseFileDirForNLPPersian() + File.separatorChar + &quot;Seraji&quot; + File.separatorChar +&quot;fa_seraji_tokenizer.pt&quot;);
        props.setProperty(&quot;mwt_model_path&quot;, BasicLocation.getBaseFileDirForNLPPersian() + File.separatorChar + &quot;Seraji&quot; + File.separatorChar +&quot;fa_seraji_mwt_expander.pt&quot;);
        props.setProperty(&quot;pos_model_path&quot;, BasicLocation.getBaseFileDirForNLPPersian() + File.separatorChar + &quot;Seraji&quot; + File.separatorChar +&quot;fa_seraji_tagger.pt&quot;);
        props.setProperty(&quot;pos_pretrain_path&quot;, BasicLocation.getBaseFileDirForNLPPersian() + File.separatorChar + &quot;Seraji&quot; + File.separatorChar +&quot;fa_seraji.pretrain.pt&quot;);
        props.setProperty(&quot;lemma_model_path&quot;, BasicLocation.getBaseFileDirForNLPPersian() + File.separatorChar + &quot;Seraji&quot; + File.separatorChar +&quot;fa_seraji_lemmatizer.pt&quot;);
        props.setProperty(&quot;depparse_model_path&quot;, BasicLocation.getBaseFileDirForNLPPersian() + File.separatorChar + &quot;Seraji&quot; + File.separatorChar +&quot;fa_seraji_parser.pt&quot;);
        props.setProperty(&quot;depparse_pretrain_path&quot;, BasicLocation.getBaseFileDirForNLPPersian() + File.separatorChar + &quot;Seraji&quot; + File.separatorChar +&quot;fa_seraji.pretrain.pt&quot;);
        
        
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
       // CoreDocument document = new CoreDocument(text);
        
        Annotation document = new Annotation(text);
        // run all Annotators on this text
        pipeline.annotate(document);
        
        // annnotate the document
        //pipeline.annotate(document);
        // examples

        // 10th token of the document
        List&lt;CoreMap&gt; sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
        for (CoreMap sentence : sentences) {
          // Get the parse tree for each sentence
          Tree parseTree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);
          // Do something interesting with the parse tree!
          System.out.println(parseTree);
        }
       




      }

    }

</code></pre>
<p>I think It's maybe about processors or arg (.model_path) that not exist in java library so if some one know about it , Please share
Thanks</p>
","python, java, stanford-nlp","<p>unfortunately doing this isn't possible. While both CoreNLP (Java) and Stanza (formerly stanfordnlp, Python) do considerably overlapping things (part of speech, named entity recognition, parsing), their internals are completely different, dating from different decades. You cannot load Stanza models into CoreNLP, and there is at present no support for Persian in CoreNLP.</p>
",2,1,438,2022-05-03 15:31:15,https://stackoverflow.com/questions/72101823/custom-model-in-stanford-nlp-corenlp-for-persian
How to convert small dataset into word embeddings instead of one-hot encoding?,"<p>I have a dataset of 33 words that are a mix of verbs and nouns, for eg. father, sing, etc. I have tried converting them to 1-hot encoding but for my use case, it has been suggested to look into word2vec embedding. I have looked in gensim and glove but struggling to make it work.</p>
<p><strong>How could I convert my data into an embedding?</strong> Such that two words that may be semantically closer may have a lesser distance between their respective vectors. How may this be achieved or any helpful material on the same?</p>
<p>Such as this<img src=""https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png"" alt=""embedding"" /></p>
","nlp, stanford-nlp, gensim, word2vec","<p>Since your dataset is quite small, and I'm assuming it doesn't contain any jargon, it's best to use a pre-trained model in order to save up on training time.</p>
<p>With gensim, it's as simple as:</p>
<pre><code>import gensim.downloader as api
wv = api.load('word2vec-google-news-300')
</code></pre>
<p>The 'word2vec-google-news-300' model has been pre-trained on a part of the Google News Dataset and generalizes well enough to most tasks. Following this, you can create word embeddings/vectors like so:</p>
<pre><code>vec = wv['father']
</code></pre>
<p>And, finally, for computing word similarity:</p>
<pre><code>similarity_score = wv.similarity('father', 'sing')
</code></pre>
<p>Lastly, one major limitation of Word2Vec is it's inability to deal with words that are OOV(out of vocabulary). For such cases, it's best to train a custom model for your corpus.</p>
",3,1,1014,2022-06-08 12:30:53,https://stackoverflow.com/questions/72545744/how-to-convert-small-dataset-into-word-embeddings-instead-of-one-hot-encoding
How does one extract the verb phrase in Spacy?,"<p>For example:</p>
<blockquote>
<p>Ultimate Swirly Ice Cream Scoopers are usually overrated when one considers all of the scoopers one could buy.</p>
</blockquote>
<p>Here I'd like to pluck:</p>
<ul>
<li>Subject: &quot;Ultimate Swirly Ice Cream Scoopers&quot;</li>
<li>Adverbial Clause: &quot;When one considers all of the scoopers one could buy&quot;</li>
<li>Verb Phrase: &quot;are usually overrated&quot;</li>
</ul>
<hr />
<p>I have the following functions for <code>subject</code>, <code>object</code>, and <code>adverbial clause</code>:</p>
<pre><code>def get_subj(decomp):
    for token in decomp:
        if (&quot;subj&quot; in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

def get_obj(decomp):
    for token in decomp:
        if (&quot;dobj&quot; in token.dep_ or &quot;pobr&quot; in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

def get_advcl(decomp):
    for token in decomp:
        # print(f&quot;pos: {token.pos_}; lemma: {token.lemma_}; dep: {token.dep_}&quot;)
        if (&quot;advcl&quot; in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

phrase = &quot;Ultimate Swirly Ice Cream Scoopers are usually overrated when one considers all of the scoopers one could buy.&quot;

nlp = spacy.load(&quot;en_core_web_sm&quot;)
decomp = nlp(phrase)

subj = get_subj(decomp)
obj = get_obj(decomp)
advcl = get_advcl(decomp)

print(&quot;subj: &quot;, subj)
print(&quot;obj: &quot;, obj)
print(&quot;advcl: &quot;, advcl)
</code></pre>
<p>Output:</p>
<pre><code>subj:  Ultimate Swirly Ice Cream Scoopers
obj:  all of the scoopers
advcl:  when one considers all of the scoopers one could buy
</code></pre>
<hr />
<p>However, the actual <code>depenency</code> type <code>.dep_</code> for the final word of the VP, &quot;are usually overrated&quot;, is &quot;ROOT&quot;.</p>
<p>So, the subtree technique fails, as the subtree of <code>ROOT</code> returns the entire sentence.</p>
","nlp, nltk, stanford-nlp, spacy","<p>You are wanting to construct something more like a “verb group” where you keep with the root verb only certain close dependents like <code>aux</code>, <code>cop</code>, and <code>advmod</code> but not ones like <code>nsubj</code>, <code>obj</code>, or <code>advcl</code>.</p>
",3,1,693,2022-06-14 15:06:01,https://stackoverflow.com/questions/72619329/how-does-one-extract-the-verb-phrase-in-spacy
How to run NegAIT with Stanford core nlp in Java in terminal in Mac?,"<p>I am not familiar with Java (only python). But I'd like to run <a href=""https://github.com/gondyleroy/NegAIT"" rel=""nofollow noreferrer"">NegAIT.</a></p>
<p>I have stanford core nlp 3.7 installed &amp; downloaded the <a href=""https://github.com/gondyleroy/NegAIT"" rel=""nofollow noreferrer"">NegAIT repository</a>. They're both in my home directory. I also have JDK SE 1.8 set as my home path.</p>
<p>When I ran the command on my Macbook terminal inside the NegAIT-master folder:</p>
<pre><code>java -cp negate.jar:/Users/username/NegAIT-master/stanford-corenlp-3.7.0/stanford-corenlp-3.7.0.jar:/Users/username/NegAIT-master/stanford-corenlp-3.7.0/stanford-corenlp-3.7.0-models.jar negate.NegationParser AagenaesSyndrome.txt AagenaesSyndromeNegation
</code></pre>
<p>I get the following error:</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: edu/stanford/nlp/pipeline/StanfordCoreNLP
    at negate.TextRead.&lt;init&gt;(TextRead.java:48)
    at negate.NegationParser.main(NegationParser.java:21)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.pipeline.StanfordCoreNLP
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 2 more
</code></pre>
<p>As I said, I am familiar with Python but not with Java, if someone could help me out or give me a tip on how to run this tool I would highly appreciate it.</p>
","java, nlp, stanford-nlp, negation","<p>Stanford CoreNLP is not being found on your CLASSPATH. It looks like that is because you haven't substituted your login name for <code>username</code> in the paths in the <code>-cp</code>.</p>
<p>Or, if you are in running the commend inside the <code>NegAIT-master</code> folder, and the CoreNLP jar files are in a subdirectory <code>stanford-corenlp-3.7.0</code> from there, you could simplify the CLASSPATH to:</p>
<pre><code>java -cp negate.jar:stanford-corenlp-3.7.0/stanford-corenlp-3.7.0.jar:stanford-corenlp-3.7.0/stanford-corenlp-3.7.0-models.jar negate.NegationParser AagenaesSyndrome.txt AagenaesSyndromeNegation
</code></pre>
",1,0,50,2022-06-15 19:37:37,https://stackoverflow.com/questions/72636973/how-to-run-negait-with-stanford-core-nlp-in-java-in-terminal-in-mac
Cannot download GloVe embeddings. Have they been moved or is downloads.cs.stanford.edu down temporarily?,"<p>I am attempting to download glove.840B.300d.zip. I used the link at <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a> and also ran <code>wget https://nlp.stanford.edu/data/glove.840B.300d.zip</code>. The output from wget looks as follows:</p>
<pre><code>--2022-06-23 15:50:30--  (try: 2)  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip
Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... failed: Connection timed out.
Retrying.
</code></pre>
<p>Does anyone know if this is a temporary issue? Thank you!</p>
","nlp, stanford-nlp, word-embedding","<p>Just found that someone opened an issue for this: <a href=""https://github.com/stanfordnlp/GloVe/issues/206"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/issues/206</a></p>
<p>Downloading from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a> is not currently possible.</p>
<p>However, Huggingface has mirrors for all of the GloVe sets that can be downloaded. Links to these are provided by a comment made on the GitHub issue by joelsewhere on June 22nd.</p>
",1,2,630,2022-06-23 20:07:26,https://stackoverflow.com/questions/72736034/cannot-download-glove-embeddings-have-they-been-moved-or-is-downloads-cs-stanfo
Extracting country name from an address,"<p>I've a large dataset with an address column. I would like to extract the countries from the address. In many cases, the address column contains states, cities, and zip code, but the country names.  You can see samples of my data</p>
<p><img src=""https://www.linkpicture.com/q/Screen-Shot-2022-06-26-at-12.56.41-PM.png"" alt=""Text"" /></p>
<p>I'm using python, How I can extract the country name in all these cases.</p>
","python, nlp, stanford-nlp","<p>There are two ways according which I would go ahead and try to find the country name.</p>
<ol>
<li>Have a NER model trained to identify the city name or state. So the ML model will extract you the city/state from the big address and then use the Google Geocoding API that will return you all the details if you pass the city/state name.</li>
<li>Write a heuristic way to identify the city/state name and then use fuzzy match and look up into a database where you can maintain the known cities/state names against a country.</li>
</ol>
<p>You can refer to Google Geocoding API <a href=""https://developers.google.com/maps/documentation/geocoding/?hl=el#ReverseGeocoding"" rel=""nofollow noreferrer"">here</a>.
And use the python geocoder library to find the details. (Install it via <code>pip install geocoder</code>)</p>
<pre><code>import geocoder
results = geocoder.google(&quot;Delhi&quot;)
print(results.current_result)
</code></pre>
<p>P.S You might have to set your API key first.</p>
",1,1,1884,2022-06-26 19:09:52,https://stackoverflow.com/questions/72764444/extracting-country-name-from-an-address
"How can I pass training, validation and test data into Standfort NER CRF?","<p>I would like to train my own Stanford NER CRF model. I have a train, validation and test dataset. <a href=""https://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/crf-faq.shtml#a</a></p>
<p>Inside the properties file I can specify the path for my training and test dataset. How is it possible to use the validation set within the training and later evaluate only on the test dataset? How do I use the train, test and validation data set correctly?</p>
<p>Thank you for your help!</p>
",stanford-nlp,"<p>Stanford NLP CRF does not use validation data for choosing the best model.  Accordingly, you can use your dev set however you like.  One possibility is to train several different models with different hyperparameters, choosing the best model by comparing scores on the dev set.  Another possibility is to add the dev set to the training data.</p>
<p>The testFile flag controls which dataset you get scores for.  If you decide to use your dev set for choosing the best hyperparameters, you would set testFile to the dev set path for the initial models.  You can then set testFile to the test set for the final score once you have chosen a model structure.</p>
",1,-1,69,2022-07-10 12:11:37,https://stackoverflow.com/questions/72928380/how-can-i-pass-training-validation-and-test-data-into-standfort-ner-crf
Import text documents to graph database through RDF triples,"<p>I have a list of documents (summary text and full document link). I am using <a href=""https://nlp.stanford.edu/software/openie.html"" rel=""nofollow noreferrer""><code>OpenIE</code></a> on the summary text to get RDF triples.</p>
<p>How can I import the RDF triples and associated full document links in <a href=""https://www.ontotext.com/products/graphdb/"" rel=""nofollow noreferrer""><code>GraphDB</code></a> for querying using SparkQL?</p>
","python, rdf, stanford-nlp, graphdb","<p>As OpenIE seems to &quot;produce relation triples in a tab separated format&quot;, you could save it as a <a href=""https://en.wikipedia.org/wiki/Tab-separated_values"" rel=""nofollow noreferrer"">TSV</a> file, which can be imported into GraphDB: <a href=""https://i.sstatic.net/kztm6.png"" rel=""nofollow noreferrer"">&quot;Import tabular data with OntoRefine&quot;</a></p>
<p>After importing the file, you can click on &quot;RDF Mapping&quot; in the top right corner.</p>
<p>GraphDB <a href=""https://www.ontotext.com/knowledgehub/fundamentals/ontorefine-2/"" rel=""nofollow noreferrer"">offers</a> a video tutorial showing how to use SPARQL for that:</p>
<p><a href=""https://www.youtube.com/watch?v=BpWM1WBq2gk"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=BpWM1WBq2gk</a></p>
",0,-1,314,2022-07-11 16:40:25,https://stackoverflow.com/questions/72941880/import-text-documents-to-graph-database-through-rdf-triples
How do I get word indexes for Glove embeddings in pytorch,"<p>I am trying to use glove embeddings in pytorch to use in a model. I have the following code:</p>
<pre><code>from torchtext.vocab import GloVe
import torch.nn
glove= GloVe()
my_embeddings = torch.nn.Embedding.from_pretrained(glove.vectors,freeze=True) 
</code></pre>
<p>However, I don't understand how I can get the embeddings for a specific word from this. <code>my_embeddings</code> only take a pytorch index rather than text. I can just use:</p>
<pre><code>from torchtext.data import get_tokenizer
tokenizer = get_tokenizer(&quot;basic_english&quot;)
glove.get_vecs_by_tokens(tokenizer(&quot;Hello, How are you?&quot;))
</code></pre>
<p>But then I am confused why I need to use <code>torch.nn.Embedding</code> at all as most tutorials suggest I do?</p>
","pytorch, stanford-nlp","<p>So I believe this is done using <code>glove.stoi</code>:</p>
<pre><code>sentence = &quot;Hello, How are you?&quot;
tokenized_sentence = tokenizer(sentence)
torch_tensor_first_word = torch.tensor(glove.stoi[tokenized_sentence[0]], dtype=torch.long)
embeddings_for_first_word = my_embeddings(torch_tensor_first_word)
</code></pre>
",1,2,1580,2022-08-18 12:56:53,https://stackoverflow.com/questions/73403476/how-do-i-get-word-indexes-for-glove-embeddings-in-pytorch
How to get the training metrics in a file?,"<p>I have trained my own NER model. I would be interested to know if I can retrieve the metrics somewhere in a file after the training. Only as output from the console, they are unfortunately not usable for me.</p>
<p>I used the following command:</p>
<pre><code>java -cp /content/stanford-ner-tagger/stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier /content/ner-model1.ser.gz -testFile /content/test1.tsv
</code></pre>
<p><a href=""https://i.sstatic.net/pERi9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pERi9.png"" alt=""output:"" /></a></p>
<p>Does anyone have an idea how I can get the output as a file?</p>
",stanford-nlp,"<p>You can keep all the output at training time by redirecting it to a file, <code>&gt; asdf.txt</code> or <code>&gt; asdf.txt 2&gt;&amp;1</code></p>
<p>You can recreate the confusion matrix with</p>
<p><code>java edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier your_classifier.ser.gz -testFile your_test_file.txt</code></p>
",1,0,45,2022-08-26 17:53:11,https://stackoverflow.com/questions/73504840/how-to-get-the-training-metrics-in-a-file
Converting word to vector using GloVe,"<p>I loaded my glove package as follows:</p>
<pre><code>import gensim.downloader as api
model = api.load(&quot;glove-wiki-gigaword-100&quot;)
</code></pre>
<p>and would want to create a function where I pass in a word and the GloVe model, and it will return the corresponding vector, for instance,</p>
<pre><code>def convert_word_to_vec(word, model):
</code></pre>
<p>and when I pass in <code>convert_word_to_vec(lol, model)</code> it will return the vectors for the word <code>lol</code></p>
<p>Is there a way around this? Thank you!</p>
","python, deep-learning, stanford-nlp, gensim","<p>Usage:</p>
<pre><code>import gensim.downloader as api
model = api.load(&quot;glove-wiki-gigaword-100&quot;)

vector = model['lol']
print(vector)  # array with shape (100,)
</code></pre>
<p>Please check the documentations for more options:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html</a></p>
",0,0,605,2022-09-30 18:28:51,https://stackoverflow.com/questions/73912673/converting-word-to-vector-using-glove
Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded using Stanford CoreNLP,"<p>While setting up <code>Stanford CoreNLP</code>, I am running into following exception since a long time:</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded
    at java.lang.StringBuilder.toString(StringBuilder.java:407)
    at java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3388)
    at java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3183)
    at java.io.ObjectInputStream.readString(ObjectInputStream.java:1863)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1526)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
    at java.util.HashMap.readObject(HashMap.java:1402)
    at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:310)
    at edu.stanford.nlp.coref.statistical.FeatureExtractor.loadVocabulary(FeatureExtractor.java:90)
    at edu.stanford.nlp.coref.statistical.FeatureExtractor.&lt;init&gt;(FeatureExtractor.java:75)
    at edu.stanford.nlp.coref.statistical.StatisticalCorefAlgorithm.&lt;init&gt;(StatisticalCorefAlgorithm.java:63)
    at edu.stanford.nlp.coref.statistical.StatisticalCorefAlgorithm.&lt;init&gt;(StatisticalCorefAlgorithm.java:44)
    at edu.stanford.nlp.coref.CorefAlgorithm.fromProps(CorefAlgorithm.java:30)
    at edu.stanford.nlp.coref.CorefSystem.&lt;init&gt;(CorefSystem.java:40)
    at edu.stanford.nlp.pipeline.CorefAnnotator.&lt;init&gt;(CorefAnnotator.java:69)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.coref(AnnotatorImplementations.java:218)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$17(StanfordCoreNLP.java:641)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$27/1579572132.apply(Unknown Source)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$33(StanfordCoreNLP.java:711)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$40/2104457164.get(Unknown Source)
</code></pre>
<p>System Details:</p>
<pre><code>MacOS
java version &quot;1.8.0_131&quot;
Java(TM) SE Runtime Environment (build 1.8.0_131-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)
</code></pre>
<p>Command for which the exception occurs:</p>
<pre><code>java edu.stanford.nlp.pipeline.StanfordCoreNLP -file input.txt
</code></pre>
<p>I think I also have setup the right <code>CLASSPATH</code>. See the result for <code>echo $CLASSPATH</code></p>
<pre><code>:/Users/krishna/Downloads/NLP/stanford-corenlp-4.5.1/*
</code></pre>
","java, nlp, stanford-nlp","<p>Fixed using the flag <code>-Xms3056m</code> which increases the default memory to <code>3GB</code> to be used by <code>CoreNLP</code>.</p>
<pre><code>java -Xms3056m -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLP -file input.txt
</code></pre>
",1,0,157,2022-11-07 23:54:09,https://stackoverflow.com/questions/74354282/exception-in-thread-main-java-lang-outofmemoryerror-gc-overhead-limit-exceede
Converting a dataset to CoNLL format. Label remaining tokens with O,"<p>I have a manually annotated dataset that contains records in the following format:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;id&quot;: 1,
    &quot;text&quot;: &quot;At the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the Third Amendment and until December 30, 1996, the Company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0.&quot;,
    &quot;label&quot;: [
        [
            209,
            230,
            &quot;COV_3&quot;
        ],
        [
            379,
            390,
            &quot;VAL_3&quot;
        ]
    ],
}
</code></pre>
<p>In the example above, <code>&quot;label&quot;</code> represents the custom entities I have in my dataset. In the example shown above, the phrase <code>fixed charge coverage</code> is located at position <code>[309, 336]</code> and is given the label <code>COV_3</code>. Likewise, the phrase <code>1.25 to 1.0</code> is located at <code>[379, 390]</code> and is given the label <code>VAL_3</code>.</p>
<p>Now, I would like to fine-tune some transformer model like BERT on this dataset, however, I realised that the dataset must be in CoNLL format. Or at least, all the tokens of each datapoint must be labelled. Is there any way I can easily label the remaining tokens with label <code>&quot;O&quot;</code> or I can transform this dataset in the CoNLL format?</p>
","nlp, stanford-nlp, huggingface-transformers, named-entity-recognition","<p>You use spacy to tokenize and convert character offset annotation to IOB tags with built-in utility methods. Note that this will skip any spans that don't align to the token boundaries, so you may need to customize the tokenizer or provide the tokenization from another source when creating a <code>Doc</code>.</p>
<p>The character offsets in the question don't line up with the text and are modified below.</p>
<pre class=""lang-py prettyprint-override""><code># tested with spacy v3.4.3, should work with spacy v3.x
import spacy
from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags

data = {
    &quot;id&quot;: 1,
    &quot;text&quot;: &quot;At the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the Third Amendment and until December 30, 1996, the Company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0.&quot;,
    &quot;label&quot;: [[209, 230, &quot;COV_3&quot;], [254, 265, &quot;VAL_3&quot;]],
}

nlp = spacy.blank(&quot;en&quot;)

# tokenize the text to create a doc
doc = nlp(data[&quot;text&quot;])

# convert annotation to entity spans and add them to the doc
ents = []
for start, end, label in data[&quot;label&quot;]:
    span = doc.char_span(start, end, label=label)
    if span is not None:
        ents.append(span)
    else:
        print(
            &quot;Skipping span (does not align to tokens):&quot;,
            start,
            end,
            label,
            doc.text[start:end],
        )
doc.ents = ents

# convert doc annotation to IOB tags
for token, iob_tag in zip(doc, biluo_to_iob(doc_to_biluo_tags(doc))):
    print(token.text + &quot; &quot; + iob_tag)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>At O
the O
end O
of O
each O
fiscal O
quarter O
, O
for O
the O
four O
consecutive O
fiscal O
quarters O
ending O
as O
of O
such O
fiscal O
quarter O
end O
, O
from O
the O
date O
of O
the O
Third O
Amendment O
and O
until O
December O
30 O
, O
1996 O
, O
the O
Company O
shall O
maintain O
a O
fixed B-COV_3
charge I-COV_3
coverage I-COV_3
ratio O
of O
not O
less O
than O
1.25 B-VAL_3
to I-VAL_3
1.0 I-VAL_3
. O
</code></pre>
<p>These are the 1st and 4th columns from the 4-column CoNLL 2003 format. You may want to insert blank lines for sentence boundaries or add the special document boundary lines, and you may need some real or placeholder values for the 2nd/3rd tag and chunk columns for use with other tools.</p>
",3,4,1032,2022-12-03 05:41:44,https://stackoverflow.com/questions/74664286/converting-a-dataset-to-conll-format-label-remaining-tokens-with-o
Obtaining data from both token and word objects in a Stanza Document / Sentence,"<p>I am using a Stanford STANZA pipeline on some (italian) text.</p>
<p>Problem I'm grappling with is that I need data from BOTH the Token and Word objects.</p>
<p>While I'm able to access one or the other separately I'm not wrapping my head on how to get data from both in a single loop over the Document -&gt; Sentence</p>
<p>Specifically I need both some Word data (such as lemma, upos and head) but I also need to know the corresponding start and end position, which in my understanding I can find in the token.start_char and token.end_char.</p>
<p>Here's my code to test what I've achieved:</p>
<pre><code>import stanza
IN_TXT = '''Il paziente Rossi e' stato ricoverato presso il nostro reparto a seguito di accesso
  al pronto soccorso con diagnosi sospetta di aneurisma aorta
  addominale sottorenale. In data 12/11/2022 e' stato sottoposto ad asportazione dell'aneurisma
  con anastomosi aorto aortica con protesi in dacron da 20mm. Paziente dimesso in data odierna in 
  condizioni stabili.'''
stanza.download('it', verbose=False)
it_nlp = stanza.Pipeline('it', processors='tokenize,lemma,pos,depparse,ner',
                         verbose=False, use_gpu=False)
it_doc = it_nlp(IN_TXT)
# iterate through the Token objects
T = 0
for token in it_doc.iter_tokens():
    T += 1
    token_id = 'T' + str((T))
    token_start = token.start_char
    token_end = token.end_char
    token_text = token.text
    print(f&quot;{token_id}\t{token_start} {token_end} {token_text}&quot;)
# iterate through Word objects
print(*[f'word: {word.text}\t\t\tupos: {word.upos}\txpos: {word.xpos}\tfeats: {word.feats if word.feats else &quot;_&quot;}' for sent in it_doc.sentences for word in sent.words], sep='\n')
</code></pre>
<p>Here is the documentation of these objects: <a href=""https://stanfordnlp.github.io/stanza/data_objects.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/data_objects.html</a></p>
","python, nlp, stanford-nlp","<p>I just discovered the zip function which returns an iterator of tuples in Python 3.</p>
<p>Therefore to iterate in parallel through the Words and Tokens of a sentence you can code:</p>
<pre><code>for sentence in it_doc.sentences:
    for t, w in zip(sentence.tokens, sentence.words):
        print(f&quot;Text-&gt;{w.text}\tLemma-&gt;{w.lemma}\tStart-&gt;{t.start_char}\tStop-&gt;{t.end_char}&quot;)
</code></pre>
",1,1,490,2022-12-03 15:39:27,https://stackoverflow.com/questions/74668152/obtaining-data-from-both-token-and-word-objects-in-a-stanza-document-sentence
Stanford&#39;s Stanza NLP: find all words ids for a given span,"<p>I am using a Stanza pipeline that extracts both words and named entities.</p>
<p>The sentence.entities gives me a list of recognized named entities with their start and end characters. Here is an example:</p>
<pre><code>{
  &quot;text&quot;: &quot;Dante Alighieri&quot;,
  &quot;type&quot;: &quot;PER&quot;,
  &quot;start_char&quot;: 1,
  &quot;end_char&quot;: 16
}
</code></pre>
<p>The sentence.words gives a list of all tokenized words also with their start and end characters: Here is a fragment of the corresponding example:</p>
<pre><code>{
  &quot;id&quot;: 1,
  &quot;text&quot;: &quot;Dante&quot;,
  &quot;lemma&quot;: &quot;Dante&quot;,
  &quot;upos&quot;: &quot;PROPN&quot;,
  &quot;xpos&quot;: &quot;SP&quot;,
  &quot;head&quot;: 3,
  &quot;deprel&quot;: &quot;nsubj&quot;,
  &quot;start_char&quot;: 1,
  &quot;end_char&quot;: 6
}
{
  &quot;id&quot;: 2,
  &quot;text&quot;: &quot;Alighieri&quot;,
  &quot;lemma&quot;: &quot;Alighieri&quot;,
  &quot;upos&quot;: &quot;PROPN&quot;,
  &quot;xpos&quot;: &quot;SP&quot;,
  &quot;head&quot;: 1,
  &quot;deprel&quot;: &quot;flat:name&quot;,
  &quot;start_char&quot;: 7,
  &quot;end_char&quot;: 16
}
{
  &quot;id&quot;: 3,
  &quot;text&quot;: &quot;scrisse&quot;,
  &quot;lemma&quot;: &quot;scrivere&quot;,
  &quot;upos&quot;: &quot;VERB&quot;,
  &quot;xpos&quot;: &quot;V&quot;,
  &quot;feats&quot;: &quot;Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin&quot;,
  &quot;head&quot;: 0,
  &quot;deprel&quot;: &quot;root&quot;,
  &quot;start_char&quot;: 17,
  &quot;end_char&quot;: 24
}
</code></pre>
<p>I need to generate a list of all words that are included in the named entity span. Using the above example those would be the words with Id 1 and 2 but not 3</p>
","python, nlp, stanford-nlp","<p>You can build a mapping using first and last character to get the word ids.
For instance here is a quick function I built to extract NER words text in a sentence:</p>
<pre><code>def find_entity_words(ent, sent):
    id_tree = {word.id:word for word in sent.words}
    start_tree = {word.start_char:word for word in sent.words}
    end_tree = {word.end_char:word for word in sent.words}

    firstword = start_tree[ent.start_char]
    lastword = end_tree[ent.end_char]

    word_ids = range(firstword.id, lastword.id+1)
    words = [id_tree[i] for i in word_ids]

    return [word.text for word in words]
</code></pre>
<p>With an example:</p>
<pre><code>&gt;&gt;&gt; doc = nlp_model('Jean-Claude Van Damme est un acteur.')
&gt;&gt;&gt; doc.ents
[{
   &quot;text&quot;: &quot;Jean-Claude Van Damme&quot;,
   &quot;type&quot;: &quot;PER&quot;,
   &quot;start_char&quot;: 0,
   &quot;end_char&quot;: 21
 }]

&gt;&gt;&gt; find_entity_words(doc.ents[0],doc.sentences[0])
['Jean-Claude', 'Van', 'Damme']



</code></pre>
",1,0,245,2022-12-18 17:37:20,https://stackoverflow.com/questions/74843260/stanfords-stanza-nlp-find-all-words-ids-for-a-given-span
NoneType erorr when calling .lower() method on annotated text,"<p>I have annotated articles in a list (len=488), and I want to apply the <code>.lower()</code> method on the lemmas. I get the following error message <code>AttributeError: 'NoneType' object has no attribute 'lower'</code>. Here's the code:</p>
<pre><code>file = open(&quot;Guardian_Syria_text.csv&quot;, mode=&quot;r&quot;, encoding='utf-8-sig')
data = list(csv.reader(file, delimiter=&quot;,&quot;))
file.close

pickle.dump(data, open('List.p', 'wb'))

stanza.download('en')
nlp = stanza.Pipeline(lang='en',
                      processors='tokenize,lemma,POS',
                      use_gpu=True)

data_list = pickle.load(open('List.p', 'rb'))
new_list = []

for article in data_list:
  a = nlp(str(article))
  new_list.append(a)
pickle.dump(new_list, open('Annotated.p', 'wb'))

annot_data = pickle.load(open('Annotated.p', 'rb'))
pos_tags = {'NOUN', 'VERB', 'ADJ', 'ADV', 'X'}
lemmas = []

for article in annot_data:
  art_tokens = [w.text for s in article.sentences for w in s.words]
  art_lemmas = [w.lemma.lower() for s in article.sentences for w in s.words
                if w.upos in pos_tags]
  lemmas.append(art_lemmas)
</code></pre>
<p>I searched the variable <code>annot_data</code> for <code>None</code> (<code>print(annot_data is None)</code>), but it returned <code>False</code>.
I tried cleaning the variable like so <code>clean = [x for x in annot_data if x != None]</code>, but the length of the variable <code>clean</code> is the same as the old one (488), and the code gives me same error message using the new <code>clean</code> variable instead of the old <code>annot_data</code> one.</p>
<p>Where's the supposed NoneType and how can I avoid it?</p>
","python, stanford-nlp, attributeerror, nonetype, stanza","<p>The error refers to <code>w.lemma.lower()</code>, so the problem is that <code>w.lemma</code> is <code>None</code>, not that <code>article</code> is <code>None</code>.</p>
<p>You can check for this in the list comprehension.</p>
<pre><code>  art_lemmas = [w.lemma.lower() for s in article.sentences for w in s.words
                if w.lemma is not None and w.upos in pos_tags]
</code></pre>
",3,1,54,2023-01-24 16:50:19,https://stackoverflow.com/questions/75224676/nonetype-erorr-when-calling-lower-method-on-annotated-text
How to store Stanza Span in MongoDB collection?,"<p>I am trying to add a list of dictionaries (whose name is <code>stanzanerlist</code>) like the following:</p>
<pre class=""lang-python prettyprint-override""><code>stanzanerlist = [{
  &quot;text&quot;: &quot;Harry Potter&quot;,
  &quot;type&quot;: &quot;PER&quot;,
  &quot;start_char&quot;: 141,
  &quot;end_char&quot;: 153
}, {
  &quot;text&quot;: &quot;Hogwarts&quot;,
  &quot;type&quot;: &quot;LOC&quot;,
  &quot;start_char&quot;: 405,
  &quot;end_char&quot;: 413
}, {
  &quot;text&quot;: &quot;JK Rowling&quot;,
  &quot;type&quot;: &quot;PER&quot;,
  &quot;start_char&quot;: 505,
  &quot;end_char&quot;: 515
}]
</code></pre>
<p>as a field in a MongoDB document in a collection.</p>
<p>I am inserting the whole document as follows with <code>stanzanerlist</code> as the last item in <code>mongodocument</code>:</p>
<pre class=""lang-python prettyprint-override""><code>mongodocument = {
        &quot;_id&quot;: urlid,
        &quot;source&quot;: sourcename,
        &quot;stanzadoc&quot;: stanzadoc.to_serialized(),
        &quot;stanzaver&quot;: stanzaver,
        # &quot;timestamp&quot;: datetime.now(tzinfo),
        &quot;timestamp&quot;: datetime.now(
            tz=pytz.timezone(cfgdata[&quot;timezone&quot;][&quot;name&quot;])
        ),
        &quot;stanzanerlist&quot;: stanzanerlist,
    }
try:
        mdbrc = mdbcoll.insert_one(
            mongodocument
        )  # insert fails if URL/_ID already exists
        return mdbrc
except pymongo.errors.DuplicateKeyError:
        # manage the record update
        print(f&quot;Article {urlid} already exists!&quot;)
</code></pre>
<p>but while all other fields work well, the addition of <code>stanzanerlist</code> gives the following error:</p>
<pre class=""lang-python prettyprint-override""><code>cannot encode object: {
  &quot;text&quot;: &quot;Harry Potter&quot;,
  &quot;type&quot;: &quot;PER&quot;,
  &quot;start_char&quot;: 141,
  &quot;end_char&quot;: 153
}, of type: &lt;class 'stanza.models.common.doc.Span'&gt;
</code></pre>
<p>and I'm not able to understand if and how  I could achieve that addition.</p>
","python, mongodb, pymongo, stanford-nlp","<p><code>pymongo</code> doesn't natively know how to convert <code>&lt;class 'stanza.models.common.doc.Span'&gt;</code> types to an acceptable BSON data type.</p>
<p>You could <em>&quot;teach&quot;</em> <code>pymongo</code> how to do the proper conversion/encoding using a custom <a href=""https://pymongo.readthedocs.io/en/stable/api/bson/codec_options.html#bson.codec_options.TypeEncoder"" rel=""nofollow noreferrer""><code>bson.codec_options.TypeEncoder</code></a> and then <code>pymongo</code> would automatically perform type conversions as it does for other types.  Or, you could do the conversion/encoding each time yourself before storing the <code>Span</code> in your MongoDB collection.</p>
<p>Fortunately, Stanford NLP Stanza has convenience methods for type conversions.  <a href=""https://stanfordnlp.github.io/stanza/data_objects.html#span"" rel=""nofollow noreferrer""><code>&lt;class 'stanza.models.common.doc.Span'&gt;</code></a> has a <code>to_dict</code> method that will convert the type to type <code>Dict</code>, which <code>pymongo</code> does know how to encode.</p>
<p>So, in your code snippet, you could change the <code>mongodocument</code> assignment of <code>&quot;stanzanerlist&quot;</code> to:</p>
<pre class=""lang-python prettyprint-override""><code>&quot;stanzanerlist&quot;: [stan.to_dict() for stan in stanzanerlist]
</code></pre>
<p>... and then each <code>&lt;class 'stanza.models.common.doc.Span'&gt;</code> will be converted to a <code>Dict</code> and <code>pymongo</code> should be able to store it.</p>
",1,0,95,2023-02-21 18:49:25,https://stackoverflow.com/questions/75524716/how-to-store-stanza-span-in-mongodb-collection
How to see if one Nokogiri::XML::Node contains parts of another Nokogiri::XML::Node?,"<p>Using ruby Nokogiri I have a <code>Nokogiri::XML::Node</code> object (ConstituencyXMLNode inherits from <code>Nokogiri::XML::Node</code>) that is a tree object like such:</p>
<pre><code>#(ConstituencyXMLNode:0xc3c8 {
  name = &quot;PP&quot;,
  children = [
    #(ConstituencyXMLNode:0xc3dc { name = &quot;IN&quot;, children = [ #(Text &quot;out&quot;)] }),
    #(ConstituencyXMLNode:0xc3f0 {
      name = &quot;PP&quot;,
      children = [
        #(ConstituencyXMLNode:0xc404 { name = &quot;IN&quot;, children = [ #(Text &quot;of&quot;)] }),
        #(ConstituencyXMLNode:0xc418 {
          name = &quot;NP&quot;,
          children = [
            #(ConstituencyXMLNode:0xc42c { name = &quot;JJ&quot;, children = [ #(Text &quot;non-living&quot;)] }),
            #(ConstituencyXMLNode:0xc440 { name = &quot;NNS&quot;, children = [ #(Text &quot;resources&quot;)] })]
          })]
      })]
  })
</code></pre>
<p>We will call it <code>pp_leaf</code></p>
<p>and I want to compare and see if portions of this node are included within this other partial Nokogiri::XML::Node:</p>
<pre><code>#(ConstituencyXMLNode:0xc3f0 {
  name = &quot;PP&quot;,
  children = [
    #(ConstituencyXMLNode:0xc404 { name = &quot;IN&quot;, children = [ #(Text &quot;of&quot;)] }),
    #(ConstituencyXMLNode:0xc418 {
      name = &quot;NP&quot;,
      children = [
        #(ConstituencyXMLNode:0xc42c { name = &quot;JJ&quot;, children = [ #(Text &quot;non-living&quot;)] }),
        #(ConstituencyXMLNode:0xc440 { name = &quot;NNS&quot;, children = [ #(Text &quot;resources&quot;)] })]
      })]
  })
</code></pre>
<p>We will call this node <code>current_leaf</code>.</p>
<p>It seems that <a href=""https://nokogiri.org/rdoc/Nokogiri/XML/NodeSet.html#method-i-at_xpath:%7E:text=include%3F(node),-click%20to%20toggle"" rel=""nofollow noreferrer""><code>Nokogiri::XML::NodeSet</code> has the comparison tools I am looking for</a>, but I am unsure of how to convert a <code>Nokogiri::XML::Node</code> to a <code>Nokogiri::XML::NodeSet</code>.  None of the operators on <code>Nokogiri::XML::Node</code> appear to have the comparison operators I need to match if one set contains the other.</p>
<p>Any ideas?</p>
","ruby, xml, xpath, nokogiri, stanford-nlp","<p>I found a way to do it with:</p>
<pre class=""lang-rb prettyprint-override""><code>pp_leaf.xpath('.//*').include?(current_leaf)` 
</code></pre>
<p>thanks to <a href=""https://stackoverflow.com/a/10077311/3448554"">this answer</a>.  It breaks the node tree down into an array which lets you check if the node is a member.</p>
",0,1,103,2023-02-25 05:38:46,https://stackoverflow.com/questions/75563512/how-to-see-if-one-nokogirixmlnode-contains-parts-of-another-nokogirixmln
Can someone explain how to create a PTB Dataset And/Or Train my own model using StanfordNLP?,"<p>I'm learning about sentiment analysis and I can't seem to find anything online that outlines how to create a PTB Dataset. I'm using StanfordNLP with Java. I've downloaded the test, dev and validate data that they used and I can't get my head around how these have been outlined:</p>
<p>test.txt:</p>
<pre><code>(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 Century) (2 's)) (2 (3 new) (2 (2 ``) (2 Conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 Arnold) (2 Schwarzenegger)) (2 ,)) (2 (2 Jean-Claud) (2 (2 Van) (2 Damme)))) (2 or)) (2 (2 Steven) (2 Segal))))))))))))) (2 .)))
</code></pre>
<p>I figure that numbers are aligned to sentiment value but I'm still not sure how it works.</p>
<p>TLDR; I'm trying to develop my own model for news analysis and have seen that the StanfordNLP model has been trained on movie reviews which is leading to poor sentiment analysis so, I thought to attempt to develop my own but I can't find anything online that teaches what each element is or how to even do this.</p>
<p>At best; outlined on this page: <a href=""https://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/sentiment/code.html</a></p>
<p>Is the dataset available and the code to train.</p>
<pre><code>Models can be retrained using the following command using the PTB format dataset:

java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz
</code></pre>
<p>I have the data that I need to parse ready.</p>
","java, machine-learning, stanford-nlp","<p>Okay.. So I've done some more digging and have started to finally understand (some what) as how to create a Dataset Tree and will try to break it down for anyone who stumbles upon this post with the same troubles as I've been having.</p>
<p>Step 1.</p>
<ul>
<li>Find your data. (In my case it's news articles about the UK housing
market)</li>
</ul>
<pre><code>UK renters: are you living with someone you’ve fallen out with?
UK property asking prices stagnating, lifting hopes of softer landing for housing market
</code></pre>
<p>Step 2.</p>
<ul>
<li>Annotate your data</li>
</ul>
<pre><code>2 UK renters: are you living with someone you’ve fallen out with?
1 fallen out with
1 fallen out
2 UK renters
2 living with someone
3 fallen
2 :
2 ?
2 living with
2 someone

3 UK property asking prices stagnating, lifting hopes of softer landing for housing market
2 UK property
3 asking prices stagnating
2 asking prices
4 lifting hopes
2 hopes
4 lifting hopes of softer landing
3 softer landing for housing market
2 housing market
2 lifting
2 landing
2 , 
</code></pre>
<p><em>Annotation Meanings</em></p>
<pre><code>Very Positive= 4
Positive = 3
Neutral = 2
Negative = 1
Very Negative = 0
</code></pre>
<p><em>Structure</em></p>
<pre><code>2 UK renters: are you living with someone you’ve fallen out with?
   //Overall sentiment

1 fallen out with
   // Negative

1 fallen out
   // Negative

2 UK renters
   // Neutral

...etc..
</code></pre>
<ul>
<li>Save the annotated data to a .txt (sample.txt)</li>
</ul>
<p>Step 3:</p>
<ul>
<li><p>Locate your <code>stanford-corenlp-4.5.2.jar</code></p>
<ul>
<li><em>example</em> <code> ~/.m2/repository/edu/stanford/nlp/stanford-corenlp/4.5.2</code></li>
</ul>
</li>
</ul>
<p>Step 4:</p>
<ul>
<li>Open Bash and run
<ul>
<li><code>java -cp &quot;*&quot; -mx5g edu.stanford.nlp.sentiment.BuildBinarizedDataset -input /c/Users/rusku/Desktop/StanfordNPL/rusSample/sample.txt</code></li>
<li><em>replace the above data location</em></li>
</ul>
</li>
</ul>
<p>Step 5:</p>
<ul>
<li>Result</li>
</ul>
<pre><code>(2 (2 (2 (2 UK) (2 renters)) (2 :)) (2 (2 (2 (2 are) (2 you)) (2 (2 living) (2 (2 with) (2 (2 someone) (2 (2 you) (2 (2 ▒ve) (1 (1 (3 fallen) (2 out)) (2 with)))))))) (2 ?)))
(3 (3 (2 (3 UK) (3 property)) (2 (3 asking) (3 prices))) (3 (3 (3 stagnating) (3 (2 ,) (4 (2 lifting) (2 hopes)))) (3 (3 of) (3 (3 (3 softer) (2 landing)) (3 (3 for) (2 (3 housing) (3 market)))))))
</code></pre>
<p>Resource: <a href=""https://blogs.oracle.com/javamagazine/post/java-sentiment-analysis-domain-specific-phrases"" rel=""nofollow noreferrer"">Train Stanford CoreNLP about the sentiment of domain-specific phrases</a></p>
<p>This is as far as I've currently gotten.</p>
<p>Hope this helps.</p>
",0,0,157,2023-03-15 11:58:45,https://stackoverflow.com/questions/75744401/can-someone-explain-how-to-create-a-ptb-dataset-and-or-train-my-own-model-using
What is Stanford CoreNLP&#39;s recipe for tokenization?,"<p>Whether you're using Stanza or Corenlp (now deprecated) python wrappers, or the original Java implementation, the tokenization rules that StanfordCoreNLP follows is super hard for me to figure out from the code in the original codebases.</p>
<p>The implementation is very verbose and the tokenization approach is not really documented. Do they consider this proprietary? On their website, they say that &quot;CoreNLP splits texts into tokens with an elaborate collection of rules, designed to follow UD 2.0 specifications.&quot;</p>
<p>I'm looking for where to find those rules, and ideally, to replace CoreNLP (a massive codebase!) with just a regex or something much simpler to mimic their tokenization strategy. Please assume in your responses that Stanford's tokenization approach is the goal. I am not looking for alternative tokenization solutions, but I also very much do not want to include and ship a code base that requires a massive java library as a dependency.</p>
<p>The answer should address the following behavior:</p>
<ul>
<li>Word hyphenation should be disabled (someone with a hyphenated last name should not be split, e.g., Marie Illonig-Alberts should tokenize as [&quot;Marie&quot;, &quot;Illonig-Alberts&quot;]. Similarly, compound words like &quot;well-intentioned&quot; should not be split.</li>
<li>Plural apostrophes should be tokenized (e.g., all boys' shoes are red to [&quot;all&quot;, &quot;boys&quot;, &quot;'&quot;, &quot;shoes&quot;, &quot;are&quot;, red&quot;])</li>
<li>Apostrophes for single ownership (e.g., my aunt's favorite to [&quot;my&quot;, &quot;aunt&quot;, &quot;'s&quot;, &quot;favorite&quot;]</li>
<li>Mr./Mrs. should not be [&quot;Mr&quot;, &quot;.&quot;] / [&quot;Mrs&quot;, &quot;.&quot;]</li>
<li>Normal punctuation should be their own tokens (end of sentence periods, commas, quotes for direct quotes or to denote sarcasm, question marks, semicolon and colons, and dashes). Double dashes should not be separated (e.g., -- is [&quot;--&quot;] NOT [&quot;-&quot;, &quot;-&quot;]</li>
<li>Wouldn't should tokenize to [&quot;would&quot;, &quot;n't&quot;]</li>
<li>&quot;and/or&quot; should not tokenize</li>
<li>Contractions should tokenize (e.g., I'm to [&quot;I&quot;, &quot;'m&quot;]</li>
<li>I also see weird tokens that correspond to POS tags sometimes like &quot;-LRB-&quot; and &quot;:-RRB-&quot;, which I do not understand.</li>
</ul>
","python, nlp, stanford-nlp, tokenize","<p>Here are a few notes from one of main authors of it. What you write in your answer is all basically correct, but there are many nuances. 😊</p>
<ul>
<li>Yes, the CoreNLP tokenizer was written to follow the Linguistic Data Consortium (LDC)'s English tokenization, but there are actually two versions of it: old treebank tokenization (&quot;Penn Treebank 3&quot; <a href=""https://catalog.ldc.upenn.edu/LDC99T42"" rel=""nofollow noreferrer"">https://catalog.ldc.upenn.edu/LDC99T42</a>, 20th century) and new treebank tokenization (&quot;OntoNotes&quot; <a href=""https://catalog.ldc.upenn.edu/LDC2013T19"" rel=""nofollow noreferrer"">https://catalog.ldc.upenn.edu/LDC2013T19</a>, 21st century). <code>PTBTokenizer</code> supports both by specifying options. The biggest difference is that the new tokenization splits on most hyphens (except common prefixes, suffixes), which seems to not be what you want.</li>
<li>LDC tokenization, especially for the old tokenization, was unspecified for many things and we make our own pragmatic decisions (emoji, URLs, etc.)</li>
<li>UD v2 tokenization, our current default, basically follows new treebank tokenization, but it does not escape brackets (i.e., <code>( ) { }</code> become <code>-LRB- -RRB- -LCB- -RCB-</code> in LDC tokenization, something you appear not to want.</li>
<li>The NLTK tokenizer roughly does old treebank tokenization, but it differs (i.e., is worse) on many details. That's why the CoreNLP tokenizer is complex! To mention just two, URLs get broken up and it doesn't handle double contractions:</li>
</ul>
<pre><code>t.tokenize(&quot;Independent Living http://www.inlv.demon.nl/.&quot;)
['Independent', 'Living', 'http', ':', '//www.inlv.demon.nl/', '.']
# versus CoreNLP output is: 
# { &quot;Independent&quot;, &quot;Living&quot;, &quot;http://www.inlv.demon.nl/&quot;, &quot;.&quot; }
t.tokenize(&quot;I'd've thought that they'd've liked it.&quot;)
[&quot;I'd&quot;, &quot;'ve&quot;, 'thought', 'that', &quot;they'd&quot;, &quot;'ve&quot;, 'liked', 'it', '.']
# versus CoreNLP output is: 
# { &quot;I&quot;, &quot;'d&quot;, &quot;'ve&quot;, &quot;thought&quot;, &quot;that&quot;, &quot;they&quot;, &quot;'d&quot;, &quot;'ve&quot;, &quot;liked&quot;, &quot;it&quot;, &quot;.&quot; }

</code></pre>
",1,0,256,2023-04-11 20:16:51,https://stackoverflow.com/questions/75989822/what-is-stanford-corenlps-recipe-for-tokenization
How can I find the cosine similarity between two song lyrics represented as strings?,"<p>My friends and I are doing an NLP project on song recommendation.</p>
<p>Context: We originally planned on giving the model a recommended song playlist that has the most similar lyrics based on the random input corpus(from the literature etc), however we didn't really have a concrete idea of its implementation.</p>
<p>Currently our task is to find similar lyrics to a random lyric fed as a string input. We are using sentence BERT model(sbert) and cosine similarity to find the similarity between the songs and it seems like the output numbers are meaningful enough to find the most similar song lyrics.</p>
<p>Is there any other way that we can improve this approach?</p>
<p>We'd like to use BERT model and are open to suggestions that can be used on top of BERT if possible, but if there is any other models that should be used instead of BERT, we'd be happy to learn. Thanks.</p>
","nlp, stanford-nlp, bert-language-model, cosine-similarity, nlp-question-answering","<p><strong>Computing cosine similarity</strong></p>
<p>You can use the <code>util.cos_sim(embeddings1, embeddings2)</code> from the <code>sentence-transformers</code> package to compute the cosine similarity of two embeddings.</p>
<p>Alternatively, you can also use <code>sklearn.metrics.pairwise.cosine_similarity(X, Y, dense_output=True)</code> from the <code>scikit-learn</code> package.</p>
<p><strong>Improvements for representation and models</strong></p>
<p>Since you want recommendations just on top of BERT, you can consider RoBERTa as well with Byte-pair encoding for tokenizer over BERT's Wordpeice tokenizers.  Consider the <a href=""https://huggingface.co/roberta-base"" rel=""nofollow noreferrer"">roberta-base</a> model as a feature extractor from the HuggingFace<code>transformers</code> package.</p>
<pre class=""lang-python prettyprint-override""><code>from transformers import RobertaTokenizer, RobertaModel
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')
text = &quot;song lyrics in text.&quot;
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
</code></pre>
<p>Tokenizers work at various text granularity level of syntax &amp; semantics. They help generate quality vectors/embeddings. Each can yield different and better results if fine-tunned for the correct task and model.</p>
<p>Some other tokenizers you can consider are:
Character Level BPE, Byte-Level BPE, WordPiece (BERT uses this), SentencePiece, and Unigram tokenizer with LM Character.</p>
<p>Also consider exploring the HuggingFace official Tokenizer Library guide <a href=""https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt"" rel=""nofollow noreferrer"">here</a>.</p>
",0,3,710,2023-05-06 13:43:56,https://stackoverflow.com/questions/76189213/how-can-i-find-the-cosine-similarity-between-two-song-lyrics-represented-as-stri
Calculating similarity score in contexto.me clone,"<p>I am currently trying to clone the popular browser game contexto.me and I am having trouble with as to how to calculate the similarity score between two words (the target word and the user inputted guess word). I am able to get the cosine similarity between the two words, but as to how to properly quantify the score into a clean integer like in the game, I am confused as to how it is done.</p>
<p>For example, if the target word is 'helicopter' and I guess the word plane, contexto will return something like a similarity score of 13, but if I guess a word like 'king' contexto will return a score of '2000' for instance.</p>
<pre><code>target_word = &quot;helicopter&quot;
glove = torchtext.vocab.GloVe(name=&quot;6B&quot;, dim=100)


@app.route('/', methods=[&quot;GET&quot;, &quot;POST&quot;])
def getSimScore():
    if request.method == &quot;POST&quot;:
        text = request.form.get(&quot;word&quot;)
        new_text = singularize(text)
        sim_score = ((torch.cosine_similarity(glove[target_word].unsqueeze(0), glove[new_text].unsqueeze(0))).numpy()[0])
        print(sim_score)
    return render_template('homepage.html', messageText='sample text', gameNum=1, guessNum=1, wordAccuracy=999)
</code></pre>
<p>This is my code so far with sim_score printing to be ~0.77 for the input 'truck' and ~0.29 for the input 'king' (closer to 1 the more similar the word is to the target word).</p>
","python, python-3.x, nlp, stanford-nlp, torch","<blockquote>
<p>For example, if the target word is 'helicopter' and I guess the word plane, contexto will return something like a similarity score of 13, but if I guess a word like 'king' contexto will return a score of '2000' for instance.</p>
</blockquote>
<p>This metric is typically called &quot;rank,&quot; and you can calculate it with the following algorithm.</p>
<ol>
<li>Compute the similarity score of every word the user can enter.</li>
<li>Sort this list.</li>
<li>Given a specific score, find what position it appears on the list. If the score appears at index 0, then it is rank 1. If it appears at index 4, then  it is rank 5, and so on.</li>
</ol>
<p>For speed, steps 1 and 2 can be computed ahead of time, if you want.</p>
",0,0,425,2023-07-03 00:44:32,https://stackoverflow.com/questions/76601293/calculating-similarity-score-in-contexto-me-clone
How to get Enhanced++ dependency labels with a java command line in the terminal?,"<p>I don't really know java, but I was just trying to use the documentation of the Stanford NLP parser to get the Enhanced++ dependency labels. This is the line I ran:</p>
<pre><code>java -cp &quot;*&quot; -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators &quot;tokenize,ssplit,pos,lemma,depparse&quot; -file input.txt
</code></pre>
<p>And indeed I get an output. But I don't get the labels that I expect. For example, in the input.txt file there is a sentence &quot;The older couple is picnicking with wine&quot;, and the dependency between picnicking and wine should be nmod, but instead it is obl:with.
Another sentence is &quot;What do you call it?&quot;, where I expect a dobj relationship between &quot;call&quot; and &quot;it&quot;, but instead I get &quot;obj&quot;.</p>
<p>What should I fix to get the labels of the enhanced universal dependencies?</p>
<p>(Also, do I really need to specify the options &quot;tokenize,ssplit,pos,lemma&quot; if I am interested only in &quot;depparse&quot;?)</p>
<p>Thank you.</p>
","java, nlp, stanford-nlp","<p>You actually are getting enhanced++ dependency labels. However, it looks like you are looking for something else or an older version.</p>
<p>UD was somewhat revised between UDv1 and UDv2. One of the changes was to make oblique modifiers (PPs in English) of predicates into the relation <code>obl</code> rather than <code>nmod</code>, restricting <code>nmod</code> to modifiers of nominals. Hence, <code>obl</code> not <code>nmod</code>. And then part of being enhanced dependencies rather than basic dependencies is getting incorporation of the case or preposition in the label, so you get <code>obl:with</code>. Similarly, in UDv2, the label <code>dobj</code> was changed to simply <code>obj</code>.</p>
<p>(And, yes, you do need to use all the annotators &quot;tokenize,ssplit,pos,lemma&quot;, because they are needed preprocessing steps before dependency parsing.)</p>
",1,0,82,2023-09-13 00:12:52,https://stackoverflow.com/questions/77093345/how-to-get-enhanced-dependency-labels-with-a-java-command-line-in-the-terminal
How to make stanza lemmatizer to return just the lemma instead of a dictionary?,"<p>I'm implementing stanza's lemmatizer because it works well with spanish texts but the lemmatizer retuns a whole dictionary with ID and other characteristics I don't care about for the time being. I checked the &quot;processors&quot; in the pipeline but I don't seem to find and example where I just get the sence with the lemmatized text instead of the dictionary.</p>
<p>This is what I have:</p>
<pre><code>stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=False)
stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True)
stNLP('me hubiera gustado mas “sincronia” con la primaria')
</code></pre>
<p>Output:</p>
<pre><code>[
  [
    {
      &quot;id&quot;: 1,
      &quot;text&quot;: &quot;me&quot;,
      &quot;lemma&quot;: &quot;yo&quot;,
      &quot;upos&quot;: &quot;PRON&quot;,
      &quot;xpos&quot;: &quot;pp1cs000&quot;,
      &quot;feats&quot;: &quot;Case=Dat|Number=Sing|Person=1|PrepCase=Npr|PronType=Prs&quot;,
      &quot;start_char&quot;: 0,
      &quot;end_char&quot;: 2
    },
....
</code></pre>
<p>Of course when I try to lemmatize my document it returns a lot of text I don't need at the moment, how can I just obtain the lemma? I'm aware I could possibly extract the word from the dictionary but it takes a lot of time as it is, what I want to avoid is giving the fuction extra work.</p>
<p>Thank you in advance.</p>
","python, nlp, stanford-nlp, lemmatization","<p>I'm not entirely sure yet, but from what I've seen, it appears that Stanza's pipeline generates a nested structure in which each sentence is a list of tokens, and each token is akin to a dictionary containing various attributes such as ID, text, lemma, and so on.</p>
<p>It is easy to extract the lemmas by navigating this nested structure. Here's how I've done it.</p>
<pre><code>stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=False)
stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True)
doc = stNLP('me hubiera gustado mas “sincronia” con la primaria')
lemmas = [word.lemma for t in doc.iter_tokens() for word in t.words]
</code></pre>
<p>Note: As of the time of writing, the version of Stanza being used is stanza==1.7.0</p>
",0,0,415,2023-12-05 23:30:30,https://stackoverflow.com/questions/77609784/how-to-make-stanza-lemmatizer-to-return-just-the-lemma-instead-of-a-dictionary
How to use local files in an Azure Function hosted on the Linux Consumption plan?,"<p>I have an event grid triggered function on Azure under the Linux Consumption plan. The code requires finding / loading some local model files. How do I get these files deployed to the Azure function and specify the path where the files are located? Any help is appreciated. Thanks!</p>
","azure, azure-functions, stanford-nlp, ml.net","<p>To deploy the local file to Azure you need to Add Item in your <code>.csproj</code> file.</p>
<pre class=""lang-xml prettyprint-override""><code>  &lt;ItemGroup&gt;
    &lt;None Include=&quot;models\**\*&quot;&gt;
    &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/None&gt;
  &lt;/ItemGroup&gt;
</code></pre>
<blockquote>
<p><strong>Note :</strong> here <code>models\**\*</code> is used to add all subdirectories and all its files</p>
</blockquote>
<p>to get the path of the file use</p>
<pre class=""lang-cs prettyprint-override""><code>var path = Path.GetFullPath(Path.GetDirectoryName(Assembly.GetExecutingAssembly().Location));
</code></pre>
<p>Thanks to @benspeth and @Louie Almeda
For reference check this <a href=""https://github.com/Azure/azure-functions-dotnet-extensions/issues/17#issuecomment-498596856"" rel=""nofollow noreferrer"">GitHub</a> and <a href=""https://stackoverflow.com/questions/49597721/is-it-possible-to-read-file-from-same-folder-where-azure-function-exists/49597847#49597847"">SO Link</a> respectively.</p>
<p><strong>#My Directory</strong></p>
<p><img src=""https://i.imgur.com/rKUbFJ0.png"" alt=""enter image description here"" /></p>
<p>In <code>Isolated</code> model we need to define custom type for Event Properties as mentioned in <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-grid-trigger?tabs=python-v2,isolated-process,nodejs-v4,extensionv3&amp;pivots=programming-language-csharp"" rel=""nofollow noreferrer"">document</a>.</p>
<p><strong><code>EventGridTrigger.cs</code>:</strong></p>
<pre class=""lang-cs prettyprint-override""><code>using System;
using System.Text.Json;
using Azure.Messaging;
using System.Reflection;
using Microsoft.Azure.Functions.Worker;
using Microsoft.Extensions.Logging;

namespace Company.Function
{
    public class EventGridTrigger
    {
        private readonly ILogger&lt;EventGridTrigger&gt; _logger;

        public EventGridTrigger(ILogger&lt;EventGridTrigger&gt; logger)
        {
            _logger = logger;
        }

        [Function(nameof(EventGridTrigger))]
        public void Run([EventGridTrigger] MyEventType myEvent, FunctionContext context)
        {
            Console.WriteLine($&quot;Event type: {myEvent.EventType}, Event subject: {myEvent.Subject}&quot;);

            try
            {
                var path = Path.GetFullPath(Path.GetDirectoryName(Assembly.GetExecutingAssembly().Location));
                Console.WriteLine(path);
                string modelContent = File.ReadAllText(path+&quot;/models/model1.json&quot;);

                Console.Write($&quot;Data of the file: {modelContent.ToString()}&quot;);
            }
            catch (Exception ex)
            {
                _logger.LogError($&quot;Error processing model1.json: {ex.Message}&quot;);
            }
        }
    }
    public class MyEventType
    {
        public string Id { get; set; }

        public string Subject { get; set; }

        public string Topic { get; set; }

        public string EventType { get; set; }

        public DateTime EventTime { get; set; }

        public IDictionary&lt;string, object&gt; Data { get; set; }
    }
}
</code></pre>
<p><strong><code>Csharp.csproj</code>:</strong></p>
<pre class=""lang-xml prettyprint-override""><code>&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;net8.0&lt;/TargetFramework&gt;
    &lt;AzureFunctionsVersion&gt;v4&lt;/AzureFunctionsVersion&gt;
    &lt;OutputType&gt;Exe&lt;/OutputType&gt;
    &lt;ImplicitUsings&gt;enable&lt;/ImplicitUsings&gt;
    &lt;Nullable&gt;enable&lt;/Nullable&gt;
  &lt;/PropertyGroup&gt;
  &lt;ItemGroup&gt;
    &lt;PackageReference Include=&quot;Microsoft.Azure.Functions.Worker&quot; Version=&quot;1.20.0&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.Azure.Functions.Worker.Extensions.EventGrid&quot; Version=&quot;3.3.0&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.Azure.Functions.Worker.Extensions.Http&quot; Version=&quot;3.1.0&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.Azure.Functions.Worker.Sdk&quot; Version=&quot;1.16.2&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.ApplicationInsights.WorkerService&quot; Version=&quot;2.21.0&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.Azure.Functions.Worker.ApplicationInsights&quot; Version=&quot;1.0.0&quot; /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;None Update=&quot;host.json&quot;&gt;
      &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/None&gt;
    &lt;None Update=&quot;local.settings.json&quot;&gt;
      &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
      &lt;CopyToPublishDirectory&gt;Never&lt;/CopyToPublishDirectory&gt;
    &lt;/None&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;Using Include=&quot;System.Threading.ExecutionContext&quot; Alias=&quot;ExecutionContext&quot; /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;None Include=&quot;models\**\*&quot;&gt;
    &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/None&gt;
  &lt;/ItemGroup&gt;
&lt;/Project&gt;
</code></pre>
<p><strong><code>model.json</code>:</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;model_name&quot;: &quot;Model1&quot;,
  &quot;description&quot;: &quot;This is a sample model file.&quot;,
  &quot;value&quot;:10,
  &quot;version&quot;: &quot;1.0&quot;
}
</code></pre>
<p><strong><code>OUTPUT</code>:</strong>
<img src=""https://i.imgur.com/UxoB7Nq.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/oGotHee.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/FGUmcXD.png"" alt=""enter image description here"" /></p>
",2,0,807,2024-01-06 01:24:52,https://stackoverflow.com/questions/77767946/how-to-use-local-files-in-an-azure-function-hosted-on-the-linux-consumption-plan
