Detecting random keyboard hits considering QWERTY keyboard layout,"<p>The <a href=""http://arxiv.org/abs/1210.5560"" rel=""nofollow"">winner of a recent Wikipedia vandalism detection competition</a> suggests that detection could be improved by ""<i>detecting random keyboard hits considering QWERTY
keyboard layout</i>"".</p>

<p>Example: <code>woijf qoeoifwjf oiiwjf oiwj pfowjfoiwjfo oiwjfoewoh</code></p>

<p>Is there any software that <b>does this already</b> (preferably <a href=""http://en.wikipedia.org/wiki/FOSS"" rel=""nofollow"">free and open source</a>) ?</p>

<p><b>If not</b>, is there an active FOSS project whose <b>goal</b> is to achieve this?</p>

<p><b>If not</b>, how would you suggest to <b>implement</b> such a software?</p>
","algorithm, n-gram, qwerty, text-classification","<p>If two <a href=""http://en.wikipedia.org/wiki/Bigram"" rel=""noreferrer"">bigrams</a> in analyzed text are close in QWERTY terms but have near zero statistical frequency in English language (like pairs ""fg"" or ""cd"") then there is chance that random keyboard hits are involved. If more such pairs are found then chance increases greatly.</p>

<p>If you want to take into account the use of both hands for bashing then test letters that are separated with another letter for QWERTY closeness, but two bigrams (or even trigrams) for bigram frequency. For example in text ""flsjf"" you would check F and S for QWERTY distance, but bigrams FL and LS (or trigram FLS) for frequency.</p>
",7,8,2380,2010-09-27 08:41:23,https://stackoverflow.com/questions/3802232/detecting-random-keyboard-hits-considering-qwerty-keyboard-layout
MAXent classifier NLTK output understand,"<p>I am trying to understand the <code>classifier.show_most_informative_features(10)</code> for MAXent classfier. I don't understand what the columns indicate, for example in the following output:</p>

<pre><code>train on 460 instances, test on 154 instances accuracy: 0.61038961039 
pos precision: 0.432989690722 
pos recall: 0.893617021277 
neg precision: 0.912280701754 
neg recall: 0.485981308411    
-4.141 need==True and label is 'REL'    
3.395 approves==True and label is 'IRREL'   -
3.308 took==True and label is 'IRREL' 
-1.766 treat==True and label is 'REL' 
-1.488 tired==True and label is 'IRREL' 
-1.295 gave==True and label is 'IRREL' 
0.879 need==True and label is 'IRREL' 
</code></pre>
","python, machine-learning, nltk, text-classification","<p>It seems that you have two labels, <code>""RELEVANT""</code> and <code>""IRRELEVANT""</code>. When there are two labels, one is normally named ""1"" or positive and the other ""-1"" or negative. </p>

<p>During the training process, the classifier analysed the features of the 460 training instances and weighted them according to their ability to distinguish well between the two labels. The details of the weighting process depend on the algorithm you chose. </p>

<p><strong>Poitive precision:</strong> 43 % of the 154 testing instances that were classified as label 1 during the testing really have the label 1. </p>

<p><strong>Positive recall:</strong> 89 % of the label 1 instances in the testing set were found, i.e. classified as label 1.  </p>

<p><strong>Negative precision / Negative recall</strong> is the same, but for label -1. </p>

<p><strong>Accuracy:</strong> 61 % of the 154 testing instances were labeled correctly. </p>

<p>The features are sorted according to their absolute value which corresponds to their  relevance for the classification. The most ""helpful"" feature in this case was <em>need</em>, and if it is true, this is a very good hint that the label of the instance should be ""RELEVANT"". </p>
",2,2,1150,2013-04-28 19:26:23,https://stackoverflow.com/questions/16266842/maxent-classifier-nltk-output-understand
How can i classify text documents with using SVM and KNN,"<p>Almost all of the examples are based on numbers. In text documents i have words instead of numbers.</p>

<p>So can you show me simple examples of how to use these algorithms for text documents classification.</p>

<p>I don't need code example but just logic</p>

<p>Pseudocode would help greatly</p>
","svm, knn, document-classification, text-classification","<p>The common approach is to use a bag of words model (<a href=""http://en.wikipedia.org/wiki/Bag_of_words_model"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Bag_of_words_model</a>) where the classifier would learn the presence of words in a text, it is simple but works surprisingly well.</p>

<p>Also, here there is a similar question: <a href=""https://stackoverflow.com/questions/13942744/prepare-data-for-text-classification-using-scikit-learn-svm"">Prepare data for text classification using Scikit Learn SVM</a> </p>
",9,4,10703,2013-05-22 14:12:38,https://stackoverflow.com/questions/16694088/how-can-i-classify-text-documents-with-using-svm-and-knn
Natural Language Processing - Converting Text Features Into Feature Vectors,"<p>So I've been working on a natural language processing project in which I need to classify different styles of writing. Assuming that semantic features from texts have already been extracted for me, I plan to use Weka in Java to train SVM classifiers using these features that can be used to classify other different texts. </p>

<p>The part I'm having trouble on is that to train an SVM, the features must be converted into a feature vector. I'm not sure how you would be able to represent features such as vocabulary richness, n-grams, punctuation, number of paragraphs, and paragraph length as numbers in a vector. If somebody could point in the right direction, that would be greatly appreciated.</p>
","java, nlp, svm, text-classification","<p>I'm not sure what values your attributes can take on, but perhaps this example will help you:</p>

<p>Suppose we are conducting a supervised learning experiment to try to determine if a period marks the end of a sentence or not, <code>EOS</code> and <code>NEOS</code> respectively.  The training data came from normal sentences in a paragraph style format, but were transformed to the following vector model:</p>

<ul>
<li>Column 1: Class: End-of-Sentence or Not-End-of-Sentence</li>
<li>Columns 2-8: The +/- 3 words surrounding the period in question</li>
<li>Columns 9,10: The number of words to the left/right, respectively, of the period before the next reliable sentence delimiter (e.g. ?, ! or a paragraph marker).</li>
<li>Column 11: The number of spaces following the period.</li>
</ul>

<p>Of course, this is not a very complicated problem to solve, but it's a nice little introduction to Weka.  We can't just use the words as features (really high dimensional space), but we can take their POS (part of speech) tags.  We can also extract the length of words, whether or not the word was capitalized, etc.  </p>

<p>So, you could feed anything as testing data, so long as you're able to transform it into the vector model above and extract the features used in the .arff.</p>

<p>The following (very small portion of) .arff file was used for determining whether a period in a sentence marked the end of or not:  </p>

<pre><code>@relation period

@attribute minus_three {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute minus_three_length real
@attribute minus_three_case {'UC','LC','NA'}
@attribute minus_two {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute minus_two_length real
@attribute minus_two_case {'UC','LC','NA'}
@attribute minus_one {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute minus_one_length real
@attribute minus_one_case {'UC','LC','NA'}
@attribute plus_one {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute plus_one_length real
@attribute plus_one_case {'UC','LC','NA'}
@attribute plus_two {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute plus_two_length real
@attribute plus_two_case {'UC','LC','NA'}
@attribute plus_three {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute plus_three_length real
@attribute plus_three_case {'UC','LC','NA'}
@attribute left_before_reliable real
@attribute right_before_reliable real
@attribute spaces_follow_period real
@attribute class  {'EOS','NEOS'}

@data

VBP, 2, LC,NP, 4, UC,NN, 1, UC,NP, 6, UC,NEND, 1, NA,NN, 7, LC,31,47,1,NEOS
NNS, 10, LC,RBR, 4, LC,VBN, 5, LC,?, 3, NA,NP, 6, UC,NP, 6, UC,93,0,0,EOS
VBD, 4, LC,RB, 2, LC,RP, 4, LC,CC, 3, UC,UH, 5, LC,VBP, 2, LC,19,17,2,EOS
</code></pre>

<p>As you can see, each attribute can take on whatever you want it to:</p>

<ul>
<li><code>real</code> denotes a real number  </li>
<li>I made up <code>LC</code> and <code>UC</code> to denote upper case and lower case,  respectively</li>
<li>Most of the other values are <code>POS</code> tags</li>
</ul>

<p>You need to figure out exactly what your features are, and what values you'll use to represent/classify them. Then, you need to transform your data into the format defined by your .arff. </p>

<p>To touch on your punctuation question, let's suppose that we have sentences that all end in <code>.</code> or <code>?</code>. You can have an attribute called punc, which takes two values:</p>

<pre><code>@attribute punc {'p','q'}
</code></pre>

<p>I didn't use <code>?</code> because that is what is (conventionally) assigned when a data point is missing. Our you could have boolean attributes that indicates whether a character or what have you was present (with 0, 1 or false, true).  Another example,  but for quality:</p>

<pre><code>@attribute quality {'great','good', 'poor'}
</code></pre>

<p>How you determine said classification is up to you,  but the above should get you started.  Good luck. </p>
",5,6,1573,2013-05-29 20:54:40,https://stackoverflow.com/questions/16823609/natural-language-processing-converting-text-features-into-feature-vectors
"learning, validation, and testing classifier","<p>I'm working on Sentiment Analysis for text classification and I want to classify tweets from Twitter to 3 categories, positive, negative, or neutral. I have 210 training data, and I'm using <strong>Naive Bayes as classifier</strong>. I'm implementing using PHP and MySQL as my database for training data.
What I've done is in sequence :</p>

<ol>
<li>I split my training data based on <strong>10-fold Cross Validation</strong> to 189 training data and 21 testing data.</li>
<li>I insert my training data into database, so my classifier can classify based on training data</li>
<li>Then I classify my testing data using my classifier. I got 21 prediction results.</li>
<li>Repeat step 2 and 3 for 10 times based on <strong>10-fold Cross Validation</strong></li>
<li>I evaluate the accuracy of the classifier one by one, so I got 10 accuracy results. Then I take the average of the results.</li>
</ol>

<p>What i want to know is :</p>

<ol>
<li>Which is the learning process ? What is the input, process, and output ?</li>
<li>Which is the validation process ? What is the input, process, and output ?</li>
<li>Which is the testing process ? What is the input, process, and output ?</li>
</ol>

<p>I just want to make sure that my comprehension about these 3 process (learning, validation, and testing) is the right one.</p>
","machine-learning, text-classification","<p>In your example, I don't think there is a meaningful distinction between validation and testing.</p>

<ul>
<li><p><strong>Learning</strong> is when you train the model, which means that your outputs are, in general, parameters, such as coefficients in a regression model or weights for connections in a neural network. In your case, the outputs are estimated probabilities for the probability of seeing a word w in a tweet given the tweet positive P(w|+), seeing a word given negative P(w|-), and seeing a word given neutral P(w|*). Also the probabilities of not seeing words in the tweet given positive, negative, neutral, P(~w|+), etc. The inputs are the training data, and the process is simply estimating probabilities by measuring the frequencies that words occur (or don't occur) in each of your classes, i.e just counting!</p></li>
<li><p><strong>Testing</strong> is where you see how well your trained model does on data you haven't seen before. Training tends to produce outputs that <em>overfit</em> the training data, i.e. the coefficients or probabilities are ""tuned"" to noise in the training data, so you need to see how well your model does on data it hasn't been trained on. In your case, the inputs are the test examples, the process is applying Bayes theorem, and the outputs are classifications for the test examples (you classify based on which probability is highest).</p></li>
</ul>

<p>I have come across <strong>cross-validation</strong> -- in addition to testing -- in situations where you don't know what model to use (or where there are additional, ""extrinsic"", parameters to estimate that can't be done in the training phase). You split the data into 3 sets.</p>

<p>So, for example, in linear regression you might want to fit a straight line model, i.e. estimate <code>p</code> and <code>c</code> in <code>y = px + c</code>, or you might want to fit a quadratic model, i.e. estimate <code>p</code>, <code>c</code>, and <code>q</code> in <code>y = px + qx^2 + c</code>. What you do here is split your data into three. You train the straight line and quadratic models using part 1 of the data (the training examples). Then you see which model is better by using part 2 of the data (the cross-validation examples). Finally, once you've chosen your model, you use part 3 of the data (the test set) to determine how good your model is. Regression is a nice example because a quadratic model will always fit the training data better than the straight line model, so can't just look at the errors on the training data alone to decide what to do.</p>

<p>In the case of Naive Bayes, it <em>might</em> make sense to explore different prior probabilities, i.e. P(+), P(-), P(*), using a cross-validation set, and then use the test set to see how well you've done with the priors chosen using cross-validation and the conditional probabilities estimated using the training data.</p>

<hr>

<p>As an example of how to calculate the conditional probabilities, consider 4 tweets, which have been classified as ""+"" or ""-"" by a human</p>

<ul>
<li>T1, -, contains ""hate"", ""anger""</li>
<li>T2, +, contains ""don't"", ""hate""</li>
<li>T3, +, contains ""love"", ""friend""</li>
<li>T4, -, contains ""anger""</li>
</ul>

<p>So for P(hate|-) you add up the number of times hate appears in negative tweets. It appears   in T1 but not in T4, so P(hate|-) = 1/2. For P(~hate|-) you do the opposite, hate doesn't appear in 1 out of 2 of the negative tweets, so P(~hate|-) = 1/2.</p>

<p>Similar calculations give P(anger|-) = 1, and P(love|+) = 1/2.</p>

<p>A fly in the ointment is that any probability that is 0 will mess things up in the calculation phase, so you instead of using a zero probability you use a very low number, like 1/n or 1/n^2, where n is the number of training examples. So you might put P(~anger|-) = 1/4 or 1/16.</p>

<p>(The maths of the calculation I put in this <a href=""https://stackoverflow.com/questions/17030793/is-taking-logs-to-vectorize-repeated-multiplication-the-right-approach"">answer</a>).</p>
",1,0,710,2013-07-05 13:44:13,https://stackoverflow.com/questions/17490361/learning-validation-and-testing-classifier
Scalable or online out-of-core multi-label classifiers,"<p>I have been blowing my brains out over the past 2-3 weeks on this problem.
I have a multi-label (not multi-class) problem where each sample can belong to several of the labels.</p>

<p>I have around 4.5 million text documents as training data and around 1 million as test data. The labels are around 35K.</p>

<p>I am using <strong>scikit-learn</strong>. For feature extraction I was previously using TfidfVectorizer which didn't scale at all, now I am using HashVectorizer which is better but not that scalable given the number of documents that I have.</p>

<pre><code>vect = HashingVectorizer(strip_accents='ascii', analyzer='word', stop_words='english', n_features=(2 ** 10))
</code></pre>

<p>SKlearn provides a OneVsRestClassifier into which I can feed any estimator. For multi-label I found LinearSVC &amp; SGDClassifier only to be working correctly. Acc to my benchmarks SGD outperforms LinearSVC both in memory &amp; time. So, I have something like this</p>

<pre><code>clf = OneVsRestClassifier(SGDClassifier(loss='log', penalty='l2', n_jobs=-1), n_jobs=-1)
</code></pre>

<p>But this suffers from some serious issues:</p>

<ol>
<li>OneVsRest does not have a partial_fit method which makes it impossible for out-of-core learning. Are there any alternatives for that?</li>
<li>HashingVectorizer/Tfidf both work on a single core and don't have any n_jobs parameter. It's taking too much time to hash the documents. Any alternatives/suggestions? Also is the value of n_features correct?</li>
<li>I tested on 1 million documents. The Hashing takes 15 minutes and when it comes to clf.fit(X, y), I receive a MemoryError because OvR internally uses LabelBinarizer and it tries to allocate a matrix of dimensions (y x classes) which is fairly impossible to allocate. What should I do?</li>
<li>Any other libraries out there which have reliable &amp; scalable multi-label algorithms? I know of genism &amp; mahout but both of them don't have anything for multi-label situations?</li>
</ol>
","machine-learning, classification, scikit-learn, document-classification, text-classification","<p>I would do the multi-label part by hand. The  OneVsRestClassifier treats them as independent problems anyhow. You can just create the n_labels many classifiers and then call partial_fit on them. You can't use a pipeline if you only want to hash once (which I would advise), though.
Not sure about speeding up hashing vectorizer. You gotta ask @Larsmans and @ogrisel for that ;)</p>

<p>Having <code>partial_fit</code> on OneVsRestClassifier would be a nice addition, and I don't see a particular problem with it, actually. You could also try to implement that yourself and send a PR.</p>
",8,13,2618,2013-09-08 14:43:17,https://stackoverflow.com/questions/18684990/scalable-or-online-out-of-core-multi-label-classifiers
Convert Web page to ARFF File for Weka classification,"<p>I want to classify 10 webpages using weka.
How to convert web pages in to Weka's ARFF file format?
Do i need to convert all the 10 page in to one ARFF file or 
Do i need to convert  ARFF files for each web page i.e 10 ARFF files.</p>
","classification, weka, arff, text-classification","<p>Assuming that you want to keep your HTML formatting, this is relatively easy. Just put your HTML files in separate folders/directories (each directory a class), then apply the <code>TextDirectoryLoader</code> converter, as explained in the <a href=""http://weka.wikispaces.com/Text+categorization+with+WEKA"" rel=""nofollow"">Text categorization with WEKA</a> tutorial.</p>

<p>Assuming that e.g. you have two classes, what you should do (and get with this procedure) is a single ARFF file with one instance per file, and the text of each file into a single field (attribute value) for a text attribute, along with the class (directory name). Then you can follow up with the StringToWordVector filter to transform documents into term vectors and perform classification.</p>
",2,0,1977,2013-10-23 10:17:12,https://stackoverflow.com/questions/19538933/convert-web-page-to-arff-file-for-weka-classification
Which algorithms to use for one class classification?,"<p>I have over 15000 text docs of a specific topic. I would like to build a language model based on the former so that I can present to this model new random text documents of various topics and the algorithms tells if the new doc is of the same topic.</p>

<p>I tried out <code>sklearn.naive_bayes.MultinomialNB</code>, <code>sklearn.svm.classes.LinearSVC</code> and others, however I have the following problem:</p>

<p>These algorithms require training data with more than one label or category and I only have web pages of covering a specific topic. The other docs are not labeled and of many different topics.</p>

<p>I would appreciate any guidance on how to train a model with only one label or how to proceed in general. What I have so far is:</p>

<pre><code>c = MultinomialNB()
c.fit(X_train, y_train)
c.predict(X_test)
</code></pre>

<p>Thank you very much.</p>
","scikit-learn, text-classification","<p>What you're looking for is the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM"">OneClassSvm</a>. For more information you might want to check out the corresponding documentation <a href=""http://scikit-learn.org/stable/modules/outlier_detection.html"">at this link</a>.</p>
",12,10,14710,2013-10-23 20:40:24,https://stackoverflow.com/questions/19551858/which-algorithms-to-use-for-one-class-classification
N-grams vs other classifiers in text categorization,"<p>I'm new to text categorization techniques, I want to know the difference between the N-gram approach for text categorization and other classifier (decision tree, KNN, SVM) based text categorization.</p>

<p>i want to know which one is better, does n-grams comes under classifers ?. Does n-grams overcome any demerits in classifier techniques ?</p>

<p>where can i get comparative information regarding all this techniques.</p>

<p>thanks in advance.</p>
","machine-learning, data-mining, classification, n-gram, text-classification","<p>I'll actually post a full answer to this, since I think it's worth it being obvious that you can use n-gram models as classifiers (in much the same way as you can use any probability model of your features as one).</p>

<p>Generative classifiers approximate the posterior of interest, p(class | test doc) as:</p>

<p>p(c|t) \propto p(c) p(t|c)</p>

<p>where p(c) is the prior probability of c and p(t|c) is the likelihood. Classification picks the arg-max over all c. An n-gram language model, just like Naive Bayes or LDA or whatever generative model you like, can be construed as a probability model p(t|c) if you estimate a separate model for each class. As such, it can provide all the information required to do classification.</p>

<p>The question is whether the model is any use, of course. The major issue is that n-gram models tend to be built over billions of words of text, where classifiers are often trained on a few thousand. You can do complicated stuff like putting joint priors on the parameters of all the class' models, clamping hyperparameters to be equal (what these parameters are depends on how you do smoothing)... but it's still tricky.</p>

<p>An alternative is to build an n-gram model of characters (including spaces/punctuation if it turns out to be useful). This can be estimated much more reliably (26^3 parameters for tri-gram model instead of ~20000^3), and can be very useful for author identification/genre classification/other forms of classification that have stylistic elements.</p>
",11,6,6183,2013-12-01 18:54:14,https://stackoverflow.com/questions/20315897/n-grams-vs-other-classifiers-in-text-categorization
Text Classification - how to find the features that most affected the decision,"<p>When using <code>SVMlight</code> or <code>LIBSVM</code> in order to classify phrases as positive or negative (Sentiment Analysis), is there a way to determine which are the most influential words that affected the algorithms decision? For example, finding that the word <code>""good""</code> helped determine a phrase as positive, etc. </p>
","machine-learning, nlp, svm, sentiment-analysis, text-classification","<p>If you use the linear kernel then yes - simply compute the weights vector:</p>

<pre><code>w = SUM_i y_i alpha_i sv_i
</code></pre>

<p>Where:</p>

<ul>
<li><code>sv</code> - support vector</li>
<li><code>alpha</code> - coefficient found with SVMlight</li>
<li><code>y</code> - corresponding class (+1 or -1)</li>
</ul>

<p>(in some implementations <code>alpha</code>'s are already multiplied by <code>y_i</code> and so they are positive/negative)</p>

<p>Once you have <code>w</code>, which is of dimensions <code>1 x d</code> where <code>d</code> is your data dimension (number of words in the bag of words/tfidf representation) simply select the dimensions with high absolute value (no matter positive or negative) in order to find the most important features (words).</p>

<p>If you use some kernel (like RBF) then the answer is no, there is no direct method of taking out the most important features, as the classification process is performed in completely different way.</p>
",5,2,776,2013-12-29 22:47:34,https://stackoverflow.com/questions/20830964/text-classification-how-to-find-the-features-that-most-affected-the-decision
How can i apply feature reduction methods in Weka?,"<ol>
<li><p>How can i apply feature reduction methods like LSI etc in weka for text classification?</p>
</li>
<li><p>Can feature reduction methods like LSI etc improve the accuracy of classification?</p>
</li>
</ol>
","machine-learning, weka, text-classification, feature-selection","<ol>
<li><p>Take a look at <a href=""http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/FilteredClassifier.html"" rel=""nofollow"">FilteredClassifier</a> class or at <a href=""http://weka.wikispaces.com/Performing+attribute+selection"" rel=""nofollow"">AttributeSelectedClassifier</a>. With FilteredClassifier you can use such features reduction method as Principal Component Analysis (PCA). Here is a <a href=""http://www.youtube.com/watch?v=PY20auk5F_E"" rel=""nofollow"">video</a> how to filter your dataset using PCA, so that you could try different classifiers on reduced dataset.</p></li>
<li><p>It can help, but there is no guarantee about that. If you remove redundant features, or transform features in some way (like SVM or PCA do) classification task can become simpler. Anyway big number of features usually lead to <a href=""http://en.wikipedia.org/wiki/Curse_of_dimensionality"" rel=""nofollow"">curse of dimensionality</a> and attribute selection is a way to avoid it.</p></li>
</ol>
",2,-1,4496,2014-01-02 10:00:30,https://stackoverflow.com/questions/20880365/how-can-i-apply-feature-reduction-methods-in-weka
What is the impact of number of training documents on classification time?,"<p>Is there any impact of number of training documents on classification time ?? I know for K-nn that all of computations in K-nn is carried out  in classification while no or minimum work is done in training. Is same is the case with SVM, Naive Bayes, Decision Trees etc ?</p>
","performance, machine-learning, text-classification","<p>Only <strong>lazy</strong> classifiers have such a characteristics, one of which is KNN.</p>

<ul>
<li>SVM - classification time depends on the number of support vectors, which may, but not have to be - dependent on the number of training documents (they are the upper bound of the number of SVs)</li>
<li>Naive Bayes - there is no impact, unless these new documents carry many new words, as the NB classification time is O( number of features ), so if you do not enlarge the vocablurary (in case of BOW model) you are safe to use many training data</li>
<li>Decision Tree - the same as for NB, it depends only on the number of features (and the complexity of the problem, which do not change with number of instances)</li>
<li>Neural Network - here classification time only depends on the number of neurons</li>
</ul>
",2,1,60,2014-01-13 09:09:35,https://stackoverflow.com/questions/21087349/what-is-the-impact-of-number-of-training-documents-on-classification-time
How to rank features by their importance in a Weka classifier?,"<p>I use Weka to successfully build a classifier. I would now like to evaluate how effective or important my features are. Fot this I use AttributeSelection. But I don't know how to ouput the different features with their corresponding importance. I want simply list the features in decreasing order of their information gain scores! </p>
","machine-learning, nlp, weka, feature-selection, text-classification","<p>There are many ways of scoring the features, which are called <em>attributes</em>, in Weka. These methods are available as subclasses of <a href=""http://weka.sourceforge.net/doc.dev/weka/attributeSelection/ASEvaluation.html"" rel=""noreferrer"">weka.attributeSelection.ASEvaluation</a>.</p>

<p>Any of these evaluation classes will give you a score for each attribute. If you use information gain for scoring, for example, you will be using it the class <code>InfoGainAttributeEval</code>. The helpful methods are </p>

<ul>
<li><code>InfoGainAttributeEval.html#buildEvaluator()</code>, and</li>
<li><code>InfoGainAttributeEval.html#evaluateAttribute()</code></li>
</ul>

<p>The other types of feature scoring (gain ratio, correlation, etc.) have the same methods for scoring. Using any of these, you can rank all your features.</p>

<p>The ranking itself is independent of Weka. Of the many ways of doing it, this is one:</p>

<pre><code>Map&lt;Attribute, Double&gt; infogainscores = new HashMap&lt;Attribute, Double&gt;();
for (int i = 0; i &lt; instances.numAttributes(); i++) {
    Attribute t_attr = instaces.attribute(i);
    double infogain  = evaluation.evaluateAttribute(i);
    infogainscores.put(t_attr, infogain);
}
</code></pre>

<p>Now you have a map which needs to be sorted by value. Here's a generic code to do that:</p>

<pre><code> /**
  * Provides a {@code SortedSet} of {@code Map.Entry} objects. The sorting is in ascending order if {@param order} &gt; 0
  * and descending order if {@param order} &lt;= 0.
  * @param map   The map to be sorted.
  * @param order The sorting order (positive means ascending, non-positive means descending).
  * @param &lt;K&gt;   Keys.
  * @param &lt;V&gt;   Values need to be {@code Comparable}.
  * @return      A sorted set of {@code Map.Entry} objects.
  */
 static &lt;K,V extends Comparable&lt;? super V&gt;&gt; SortedSet&lt;Map.Entry&lt;K,V&gt;&gt;
 entriesSortedByValues(Map&lt;K,V&gt; map, final int order) {
     SortedSet&lt;Map.Entry&lt;K,V&gt;&gt; sortedEntries = new TreeSet&lt;&gt;(
         new Comparator&lt;Map.Entry&lt;K,V&gt;&gt;() {
             public int compare(Map.Entry&lt;K,V&gt; e1, Map.Entry&lt;K,V&gt; e2) {
                 return (order &gt; 0) ? compareToRetainDuplicates(e1.getValue(), e2.getValue()) : compareToRetainDuplicates(e2.getValue(), e1.getValue());
         }
     }
    );
    sortedEntries.addAll(map.entrySet());
    return sortedEntries;
}
</code></pre>

<p>and finally,</p>

<pre><code>private static &lt;V extends Comparable&lt;? super V&gt;&gt; int compareToRetainDuplicates(V v1, V v2) {
    return (v1.compareTo(v2) == -1) ? -1 : 1;
}
</code></pre>

<p>Now you have a list of entries sorted by values (in ascending or descending order, as you wish). Go crazy with it!</p>

<p>Please note that <strong>you should handle the case where more than one attribute has the same information gain</strong>. That is why I went through the process of sorting by values while retaining duplicates.</p>
",12,6,12647,2014-01-21 20:05:17,https://stackoverflow.com/questions/21267988/how-to-rank-features-by-their-importance-in-a-weka-classifier
Text Classification - using stemmer degrades results?,"<p>There's <a href=""http://www.cs.indiana.edu/~mkorayem/paper/survey_Arabic.pdf"" rel=""nofollow"">this</a> article about sentiment analysis of Arabic. </p>

<p>In the beginning of page 5 it says that:</p>

<blockquote>
  <p>""Experiments also show that stemming words before feature extraction and classification nearly always degrades the results"".</p>
</blockquote>

<p>Later on in the same page, they state that:</p>

<blockquote>
  <p>""...and an Arabic light stemmer is used for stemming the words""</p>
</blockquote>

<p>Um I thought that a stemmer/lemmatizer was <em>always</em> used before text classifications, why does he say that it degrades the results?</p>

<p>Thanks :)</p>
","nlp, sentiment-analysis, stemming, text-classification","<p>I do not know the arabic language, it may be specific in many aspects, my answer regards english.</p>

<blockquote>
  <p>Um I thought that a stemmer/lemmatizer was always used before text classifications, why does he say that it degrades the results?</p>
</blockquote>

<p>No it is not, in entirely depends on the <strong>task</strong>. If you want to extract some general concept of the text, then stemming/lematization is a good step. But in analysis of short chunks, where each word is valuable, stemming simply destroys its meaning. In particular - in sentiment analysis stemming may destroy the sentiment of the word. </p>
",5,3,822,2014-01-22 21:47:38,https://stackoverflow.com/questions/21294694/text-classification-using-stemmer-degrades-results
How to prune low frequency and high frequency words from a dataset?,"<p>Is there any tool available with which i can prune high frequency and low frequency terms from my dataset ?</p>
","machine-learning, text-classification, pruning","<p>A commonly used algorithm for this would be <a href=""http://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers"" rel=""nofollow"">Grubbs' test</a>. I don't really know an implementation in Java but if you would be willing to do the preprocessing in a different language, then there is the <a href=""http://cran.r-project.org/web/packages/outliers/outliers.pdf"" rel=""nofollow"">outliers package</a> in R containing amongst others the Grubbs' test. To eliminate multiple outliers you can just repeatedly apply Grubbs' test.</p>

<p>Edit:</p>

<p>I just saw that I missed the text classification tag. If you just want to keep too frequent terms from skewing your results, maybe <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">TF-IDF</a> could be interesting to you. This of course does not reduce dimensionality.</p>
",0,0,863,2014-02-01 15:12:58,https://stackoverflow.com/questions/21499768/how-to-prune-low-frequency-and-high-frequency-words-from-a-dataset
Why KNN has low accuracy but high precision?,"<p>I classified 20NG dataset with k-nn with 200 instance in each category with 80-20 train-test split where i found the following results </p>

<p><img src=""https://i.sstatic.net/uULWG.png"" alt=""enter image description here""></p>

<p>Here accuracy is quite low but how precision is high when accuracy is that low ? isn't precision formulae TP/(TP + FP) ? If yes than high accurate classifier needs to generate high true positive which will result in high precision but how K-nn is generating high precision with too less true positive rate ?</p>
","machine-learning, weka, nearest-neighbor, text-classification","<p>Recall is equivalent to the True Positive rate. Text classification tasks (especially Information Retrieval, but Text Categorization as well) <em>show a trade-off between recall and precision</em>. When precision is very high, recall tends to be low, and the opposite. This is due to the fact that you can tune the classifier to classify more or less instances as positive. The less instances you classify as positive, the higher the precision and the lower the recall.</p>

<p>To ensure that the effectiveness measure correlates with accuracy, you shoud focus on the F-measure, which averages recall and precision (F-measure = 2*r*p / (r+p)).</p>

<p>Non-lazy classifiers follow a training process in which they try to optimize accuracy or error. K-NN, being lazy, has not a training process, and in consequence, it does not try to optimize any effectiveness measure. You can play with different values of K, and intuitively, the bigger the K the higher the recall and the lower the precision, and the opposite.</p>
",5,2,5154,2014-02-04 08:30:50,https://stackoverflow.com/questions/21547423/why-knn-has-low-accuracy-but-high-precision
How can be estimate the total number of features?,"<p>If i have 1000 tokens(i assume tokens are features after preprocessing dataset), then how many bigram features would be generated from 1000 tokens(words) ? is it each token would have a bigram combination with all other tokens in vocabulary ? </p>

<p>i am asking this question as i have to pre-fill the number of words to keep in vocabulary in weka </p>
","machine-learning, weka, text-classification","<p>You cannot precompute this based just on the number of tokens. Bigrams are pairs of tokens which occur side-by-side (it is a term from n-gram models, where you have a notion of sequence). So in order to compute number of bigrams you have to slide a 2-token window through your data and check how many different pairs you find. </p>

<p>If you have N tokens coming from some data X, you can only say, that number of bigrams B is bounded as follows: <code>N &lt;= B &lt;= N^2</code>, but the exact number requires the procedure outlined above.</p>
",0,0,82,2014-02-04 15:59:35,https://stackoverflow.com/questions/21557249/how-can-be-estimate-the-total-number-of-features
How can i give output exampleset of &quot;Process Documents From Files&quot; to Multiple Classifiers in rapid miner?,"<p>I am working on parallel classifier combination and have a requirement in rapid miner to give output of ""Process Documents from Files"" operator to more than one classifier (L1, L2, L3). One way of doing it is to create three different processes and give each of them documents separately but that could be a performance bottleneck and i want to avoid it.</p>

<p>Is there any way to provide example set copies to classifiers (L1, L2, L3) ??</p>
","machine-learning, rapidminer, text-classification","<p>Use the <code>Multiply</code> operator to makes copies of example sets.</p>
",0,0,57,2014-02-16 15:43:42,https://stackoverflow.com/questions/21813396/how-can-i-give-output-exampleset-of-process-documents-from-files-to-multiple-c
Is TF-IDF necessary when using SVM?,"<p>I'm using Support Vector Machines to classify phrases. Before using the SVM, I understand I should do some kind of normalization on the phrase-vectors. One popular method is TF-IDF.</p>

<p>The terms with the highest TF-IDF score are often the terms that best characterize the topic of the document.</p>

<p>But isn't that exactly what SVM does anyway? Giving the highest weight to the terms that best characterize the document?</p>

<p>Thanks in advance :-)</p>
","nlp, normalization, svm, tf-idf, text-classification","<p>The weight of a term (as assigned by an SVM classifier) may or may not be directly proportional to the relevance of that term to a particular class. This depends on the kernel of the classifier as well as the regularization used. SVM does <em>NOT</em> assign weights to terms that best characterize a single document.</p>

<p>Term-frequency (tf) and inverse document frequency (idf) are used to encode the value of a term in a document vector. This is independent of the SVM classifier. </p>
",2,2,2752,2014-02-16 18:23:25,https://stackoverflow.com/questions/21815475/is-tf-idf-necessary-when-using-svm
How to apply InformationGain in rapidminer with seperate test set ?,"<p>I am dealing with text classification in rapidminer. I have seperate test and training splits. I applied Information Gain to a dataset using n-fold cross validation but i am confused on how to apply it on seperate test set ? Below is attached image <img src=""https://i.sstatic.net/RETm7.png"" alt=""enter image description here""></p>

<p>In figure i have connected the word list output from first ""Process Documents From Files"" which is used for training to second ""Processed Documents From Files"" which is used for testing but i want to apply the reduced feature to the second ""Process Documents From Files"" which perhaps should be the one returned from ""Select By Weight"" (reduced dimensions) operator but it returns weights which i cannot provide to second ""Process Documents From Files"". I searched alot but did'nt managed to find anything which can satisfy my need ?</p>

<p>Is it really possible for Rapidminer to have seperate test/train splits and apply feature selection ?</p>

<p>Is there any way to convert these weights into word list ? Please don't say write in repository (i can't do this) ?</p>

<p>In such scenario when i have different test/train splits and needs to apply feature selection, how would i make sure that test/train splits have same dimension vectors ?</p>

<p>I am really trapped out at it, kindly help ...</p>
","machine-learning, rapidminer, text-classification","<p>Immediately after the lower <code>Process Documents</code> operator insert a new <code>Select By Weight</code> operator before the <code>Apply Model</code>. Use a <code>Multiply</code> operator to copy the weights from the <code>Weight By Information Gain</code> operator and connect this to the input of the new <code>Select By Weight</code> operator.</p>
",1,0,650,2014-02-18 12:33:16,https://stackoverflow.com/questions/21853989/how-to-apply-informationgain-in-rapidminer-with-seperate-test-set
How can i convert from Rapid Miner exampleset into weka instances?,"<p>I need some functionality to be used from weka and some functionality to be used from rapidminer. How can i convert into rapid miner exampleset into weka instances perform some operations and then convert back into weka instances ?</p>

<p>I found a class on internet mentioned as ""WekaTools""  but it does not exist in rapidminer(5.3.013)</p>

<p>Can any one tell me where this functionality is moved in rapid miner 5.3.013? or how can i convert from Rapid miner example set into weka Instances ?</p>
","machine-learning, rapidminer, text-classification","<p>You have to download and install the Weka extension which is available from the RapidMiner Marketplace.</p>

<p>From the GUI, Help, Updates and extensions and then search for Weka.</p>
",0,0,104,2014-02-19 14:00:51,https://stackoverflow.com/questions/21882735/how-can-i-convert-from-rapid-miner-exampleset-into-weka-instances
Scikit learn - fit_transform on the test set,"<p>I am struggling to use Random Forest in Python with Scikit learn. My problem is that I use it for text classification (in 3 classes - positive/negative/neutral) and the features that I extract are mainly words/unigrams, so I need to convert these to numerical features. I found a way to do it with <code>DictVectorizer</code>'s <code>fit_transform</code>:</p>

<pre><code>from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.feature_extraction import DictVectorizer

vec = DictVectorizer(sparse=False)
rf = RandomForestClassifier(n_estimators = 100)
trainFeatures1 = vec.fit_transform(trainFeatures)

# Fit the training data to the training output and create the decision trees
rf = rf.fit(trainFeatures1.toarray(), LabelEncoder().fit_transform(trainLabels))

testFeatures1 = vec.fit_transform(testFeatures)
# Take the same decision trees and run on the test data
Output = rf.score(testFeatures1.toarray(), LabelEncoder().fit_transform(testLabels))

print ""accuracy: "" + str(Output)
</code></pre>

<p>My problem is that the <code>fit_transform</code> method is working on the train dataset, which contains around 8000 instances, but when I try to convert my test set to numerical features too, which is around 80000 instances, I get a memory error saying that:</p>

<pre><code>testFeatures1 = vec.fit_transform(testFeatures)
File ""C:\Python27\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 143, in fit_transform
return self.transform(X)
File ""C:\Python27\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 251, in transform
Xa = np.zeros((len(X), len(vocab)), dtype=dtype)
MemoryError
</code></pre>

<p>What could possibly cause this and is there any workaround? Many thanks!</p>
","machine-learning, classification, scikit-learn, random-forest, text-classification","<p>You are not supposed to do <code>fit_transform</code> on your test data, but only <code>transform</code>. Otherwise, you will get different vectorization than the one used during training.</p>

<p>For the memory issue, I recommend <code>TfIdfVectorizer</code>, which has numerous options of reducing the dimensionality (by removing rare unigrams etc.).</p>

<p><strong>UPDATE</strong></p>

<p>If the only problem is fitting <strong>test</strong> data, simply split it to small chunks. Instead of something like</p>

<pre><code>x=vect.transform(test)
eval(x)
</code></pre>

<p>you can do</p>

<pre><code>K=10
for i in range(K):
    size=len(test)/K
    x=vect.transform(test[ i*size : (i+1)*size ])
    eval(x)
</code></pre>

<p>and record results/stats and analyze them afterwards.</p>

<p>in particular</p>

<pre><code>predictions = []

K=10
for i in range(K):
    size=len(test)/K
    x=vect.transform(test[ i*size : (i+1)*size ])
    predictions += rf.predict(x) # assuming it retuns a list of labels, otherwise - convert it to list

print accuracy_score( predictions, true_labels )
</code></pre>
",16,14,10880,2014-02-24 20:13:46,https://stackoverflow.com/questions/21998008/scikit-learn-fit-transform-on-the-test-set
Using LibShortText with files in LibSVM format,"<p>I'm trying to use <code>LibShortText</code> but I don't entirely understand how it works.</p>

<p>From the <code>README</code>, it looks like it's functions are for text-files. However, I need to classify files that are <em>already</em> in LibSVM format, so I suppose functions like <code>text-train.py</code> and <code>text-predict.py</code> won't do...?</p>

<p>The <code>README</code> also states that:</p>

<pre><code>If a preprocessor directory is given instead, then it is assumed that the 
training data is already in LIBSVM format.
</code></pre>

<p>Anybody know what a <code>preprocessor directory</code> is...?</p>

<p>From the <a href=""http://www.csie.ntu.edu.tw/~cjlin/libshorttext/doc/classifier.html"" rel=""nofollow"">LibShortText documentations</a>, I see that there is a ""Middle-level Classification Modeul - learner"" that works on files like mine. However, I don't understand how it works! It doesn't have all the paramters that LIBSVM has, for example... And I haven't succeeded in finding how to save, or understand the results (where are the weights and predictions?).</p>

<p>If someone could explain how this thing words I'd <em>really</em> appreciate it (I've been testing it for quite a few hours now)... Thanks!</p>
","machine-learning, svm, text-classification, libshorttext","<p>Per the docs (<a href=""http://www.csie.ntu.edu.tw/~cjlin/libshorttext/doc/libshorttext.html#quick-start"" rel=""nofollow"">http://www.csie.ntu.edu.tw/~cjlin/libshorttext/doc/libshorttext.html#quick-start</a>) you can append .svm files using the -A option.</p>

<p>But if you you've already got your data in libsvm format you can use liblinear (the library underlying libshorttext) directly.</p>

<p>If you've got a distribution of libshorttext you already have liblinear.  You can compile and run by cd'ing (from wherever you've got libshorttext unpacked) like so:</p>

<p><code>
    $ cd libshorttext/classifier/learner/liblinear
    $ make
    $ ./train train_file.svm
    $ ./predict test_file.svm model_file output_file
</code></p>

<p>For reference here is the liblinear README: <a href=""https://github.com/ninjin/liblinear/blob/master/README"" rel=""nofollow"">https://github.com/ninjin/liblinear/blob/master/README</a> which I've found very handy.</p>
",1,1,780,2014-03-02 23:41:48,https://stackoverflow.com/questions/22135259/using-libshorttext-with-files-in-libsvm-format
Handling same words but from different documents,"<p>I'm making a python class which calculates the tfidf weight of each word in a document. Now in my dataset I have 50 documents. In these documents many words intersect, thus having multiple same word features but with different tfidf weight. So the question is how do I sum up all the weights into one singular weight?</p>
","python, machine-learning, text-classification, tf-idf","<p>First, let's get some terminology clear.  A term is a word-like unit in a corpus.  A token is a term at a particular location in a particular document.  There can be multiple tokens that use the same term.  For example, in my answer, there are many tokens that use the term ""the"".  But there is only one term for ""the"".</p>

<p>I think you are a little bit confused.  TF-IDF style weighting functions specify how to make a per term score out of the term's token frequency in a document and the background token document frequency in the corpus for each term in a document.  TF-IDF converts a document into a mapping of terms to weights.  So more tokens sharing the same term in a document will increase the corresponding weight for the term, but there will only be one weight per term.  There is no separate score for tokens sharing a term inside the doc.</p>
",2,0,794,2014-03-03 22:58:00,https://stackoverflow.com/questions/22159351/handling-same-words-but-from-different-documents
Theano Classification Task always gives 50% validation error and test error?,"<p>I am doing a text classification experiment with Theano's DBN (Deep Belief Network) and SDA (Stacked Denoising Autoencoder) examples.
I have produced a feature/label dataset just as Theano's MINST dataset is produced and changed the feature length and output values of those examples to adopt to my dataset (2 outputs instead of 10 outputs, and the number of features is adopted to my dataset).
Every time i run the experiments (both DBN and SDA) i get an exact 50% validation error and test error.
Do you have any ideas what i'm doing wrong? because i have just produced a dataset out of Movie Review Dataset as MINST dataset format and pickled it.</p>
<p>my code is the same code you can find in <a href=""http://www.deeplearning.net/tutorial/DBN.html"" rel=""nofollow noreferrer"">http://www.deeplearning.net/tutorial/DBN.html</a>
and my SDA code is the same code you can find in
<a href=""http://www.deeplearning.net/tutorial/SdA.html"" rel=""nofollow noreferrer"">http://www.deeplearning.net/tutorial/SdA.html</a></p>
<p>The only difference is that i have made my own dataset instead of MINST digit recognition dataset. My dataset is Bag of Words features from Movie Review Dataset which of course has different number of features and output classes so i just have made tiny modifications in function parameters number of inputs and output classes.
The code runs beautifully but the results are always 50%.
This is a sample output:</p>
<pre><code>Pre-training layer 2, epoch 77, cost  -11.8415031463
Pre-training layer 2, epoch 78, cost  -11.8225591118
Pre-training layer 2, epoch 79, cost  -11.8309999005
Pre-training layer 2, epoch 80, cost  -11.8362189546
Pre-training layer 2, epoch 81, cost  -11.8251214285
Pre-training layer 2, epoch 82, cost  -11.8333494168
Pre-training layer 2, epoch 83, cost  -11.8564580976
Pre-training layer 2, epoch 84, cost  -11.8243052414
Pre-training layer 2, epoch 85, cost  -11.8373403275
Pre-training layer 2, epoch 86, cost  -11.8341470443
Pre-training layer 2, epoch 87, cost  -11.8272021013
Pre-training layer 2, epoch 88, cost  -11.8403720434
Pre-training layer 2, epoch 89, cost  -11.8393612003
Pre-training layer 2, epoch 90, cost  -11.828745041
Pre-training layer 2, epoch 91, cost  -11.8300890796
Pre-training layer 2, epoch 92, cost  -11.8209189065
Pre-training layer 2, epoch 93, cost  -11.8263340225
Pre-training layer 2, epoch 94, cost  -11.8348454378
Pre-training layer 2, epoch 95, cost  -11.8288419285
Pre-training layer 2, epoch 96, cost  -11.8366522357
Pre-training layer 2, epoch 97, cost  -11.840142131
Pre-training layer 2, epoch 98, cost  -11.8334445128
Pre-training layer 2, epoch 99, cost  -11.8523094141
</code></pre>
<p>The pretraining code for file DBN_MovieReview.py ran for 430.33m</p>
<pre><code>... getting the finetuning functions
... finetuning the model
epoch 1, minibatch 140/140, validation error 50.000000 %
     epoch 1, minibatch 140/140, test error of best model 50.000000 %
epoch 2, minibatch 140/140, validation error 50.000000 %
epoch 3, minibatch 140/140, validation error 50.000000 %
epoch 4, minibatch 140/140, validation error 50.000000 %
Optimization complete with best validation score of 50.000000 %,with test performance 50.000000 %
</code></pre>
<p>The fine tuning code for file DBN_MovieReview.py ran for 5.48m</p>
<p>I ran both SDA and DBN with two different feature sets. So i got this exact 50% accuracy on all these 4 experiments.</p>
","sentiment-analysis, text-classification, theano, deep-learning","<p>I asked the same question in Theano's user groups and they answered that feature values should be between 0 and 1.</p>

<p>So i used a normalizer to normalize feature values and it solved the problem.</p>
",4,3,1240,2014-03-24 15:15:03,https://stackoverflow.com/questions/22613364/theano-classification-task-always-gives-50-validation-error-and-test-error
incremental training SGD Classifier of Sklearn with sentences,"<p>How to incrementally train SGDClassifier available in Sklearn linear models for sentences.
It is usually trains with docs.But i want to train it with sentences one by one.I want to incrementally train it for tweets for sentiment analysis.Any explanation with example in python will be great help.</p>

<p>Task: Sentiment Analysis of Tweets
Doubt: Incrementally  training on labelled Tweets</p>

<p>Thanks in advance.</p>
","python, machine-learning, scikit-learn, sentiment-analysis, text-classification","<p>Just make list of sentences 
for example </p>

<pre><code>sents=[""I am x"",""I am Y""]
</code></pre>

<p>then transform it using HashingVectorizer and then use partial_fit to train it incrementally.
This worked for me.</p>

<p>Thanks</p>
",1,1,875,2014-03-27 13:11:55,https://stackoverflow.com/questions/22688728/incremental-training-sgd-classifier-of-sklearn-with-sentences
"Can you recommend a package in R that can be used to count precision, recall and F1-score for multi class classification tasks","<p>Is there any package that you would recommend which can be used to calculate the precision, F1, recall for multi class classification task in R. I tried to use ROCR but it states that: </p>

<pre><code>ROCR currently supports only evaluation of binary classification tasks
</code></pre>
","r, text-classification, precision-recall","<p>I know that you were looking for a solution in R. That said, this is a link to a nice solution  library in <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html"" rel=""nofollow"">Python, using scikit-learn version 0.14</a>. Python is very similar to R in a lot of respects (if you haven't used it before), and this could be a good place to start.</p>

<p>Another place you might want to look, if you are focused on R, is the the <a href=""http://cran.r-project.org/web/packages/PerfMeas/PerfMeas.pdf"" rel=""nofollow"">PerfMeas</a> package. As I quote, this ""Package implements different performance measures for
classiﬁcation and ranking tasks. AUC, precision at a given recall, F-score for single and multiple classes are available.""</p>
",1,5,1480,2014-04-08 07:39:54,https://stackoverflow.com/questions/22930583/can-you-recommend-a-package-in-r-that-can-be-used-to-count-precision-recall-and
dimension reduction in spam filtering,"<p>I'm performing an experiment in which I need to compare classification performance of several classification algorithms for spam filtering, viz. Naive Bayes, SVM, J48, k-NN, RandomForests, etc. I'm using the WEKA data mining tool. While going through the literature I came to know about various dimension reduction methods which can be broadly classified into two types-</p>

<ol>
<li>Feature Reduction: Principal Component Analysis, Latent Semantic Analysis, etc.</li>
<li>Feature Selection: Chi-Square, InfoGain, GainRatio, etc.</li>
</ol>

<p>I have also read this tutorial of WEKA by Jose Maria in his blog: <a href=""http://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html"" rel=""nofollow"">http://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html</a></p>

<p>In this blog he writes, ""A typical text classification problem in which dimensionality reduction can be a big mistake is spam filtering"". So, now I'm confused whether dimensionality reduction is of any use in case of spam filtering or not?</p>

<p>Further, I have also read in the literature about Document Frequency and TF-IDF as being one of feature reduction techniques. But I'm not sure how does it work and come into play during classification.</p>

<p>I know how to use weka, chain filters and classifiers, etc. The problem I'm facing is since I don't have enough idea about feature selection/reduction (including TF-IDF) I am unable to decide how and what feature selection techniques and classification algorithms I should combine to make my study meaningful. I also have no idea about optimal threshold value that I should use with chi-square, info gain, etc.</p>

<p>In StringToWordVector class, I have an option of IDFTransform, so does it makes sence to set it to TRUE and also use a feature selection technique, say InfoGain?</p>

<p>Please guide me and if possible please provide links to resources where I can learn about dimension reduction in detail and can plan my experiment meaningfully!</p>
","data-mining, weka, text-mining, spam-prevention, text-classification","<p>Well, Naive Bayes seems to work best for spam filtering, and it doesn't play nicely with dimensionality reduction.</p>

<p>Many dimensionality reduction methods try to identify the features of the highest variance. This of course won't help a lot with spam detection, you want discriminative features.</p>

<p>Plus, there is not only one type of spam, but many. Which is likely why naive Bayes works better than many other methods that assume there is only one type of spam.</p>
",0,0,465,2014-04-09 10:38:38,https://stackoverflow.com/questions/22960024/dimension-reduction-in-spam-filtering
How to output resultant documents from Weka text-classification,"<p>So we are running a multinomial naive bayes classification algorithm on a set of 15k tweets. We first break up each tweet into a vector of word features based on Weka's StringToWordVector function. We then save the results to a new arff file to user as our training set. We repeat this process with another set of 5k tweets and re-evaluate the test set using the same model derived from our training set. </p>

<p>What we would like to do is to output each sentence that weka classified in the test set along with its classification... We can see the general information (Precision, recall, f-score) of the performance and accuracy of the algorithm but we cannot see the individual sentences that were classified by weka, based on our classifier... Is there anyway to do this?</p>

<p>Another problem is that ultimately our professor will give us 20k more tweets and expect us to classify this new document. We are not sure how to do this however as:</p>

<pre><code>All of the data we have been working with has been classified manually, both the training and test sets...
however the data we will be getting from the professor will be UNclassified... How can we 
reevaluate our model on the unclassified data if Weka requires that the attribute information must
be the same as the set used to form the model and the test set we are evaluating against?
</code></pre>

<p>Thanks for any help!</p>
","machine-learning, weka, sentiment-analysis, text-classification","<p>The easiest way to acomplish these tasks is using a <code>FilteredClassifier</code>. This kind of classifier integrates a <code>Filter</code> and a <code>Classifier</code>, so you can connect a <code>StringToWordVector</code> filter with the classifier you prefer (<code>J48</code>, <code>NaiveBayes</code>, whatever), and you will be always keeping the original training set (unprocessed text), and applying the classifier to new tweets (unprocessed) by using the vocabular derived by the <code>StringToWordVector</code> filter.</p>

<p>You can see how to do this in the command line in ""<a href=""http://jmgomezhidalgo.blogspot.com.es/2013/04/command-line-functions-for-text-mining.html"" rel=""nofollow"">Command Line Functions for Text Mining in WEKA</a>"" and via a program in ""<a href=""http://jmgomezhidalgo.blogspot.com.es/2013/04/a-simple-text-classifier-in-java-with.html"" rel=""nofollow"">A Simple Text Classifier in Java with WEKA</a>"".</p>
",1,0,485,2014-04-22 00:01:52,https://stackoverflow.com/questions/23208044/how-to-output-resultant-documents-from-weka-text-classification
Batch Filtering with Multi-Filter throws a &#39;Class attribute not set&#39; exception,"<p>We have a data set of 15k classified tweets with which we need to perform sentiment analysis. I would like to test against a test set of 5k classified tweets. Due to Weka needing the same attributes within the header of the test set as exist in the header of training set, I will have to use batch filtering if I want to be able to run my classifier against this 5k test set.</p>

<p>However, there are several filters that I need to run my training set through, so I figured the running a multifilter against the training set would be a good idea. The multifilter works fine when not running the batch argument, but when I try to batch filter I get an error from the CLI as it tried to execute the first filter within the multi-filter:</p>

<p><strong>CLI multiFilter command w/batch argument:</strong></p>

<pre><code>java weka.filters.MultiFilter -F ""weka.filters.supervised.instance.Resample -B 1.0 -S 1 -Z 15.0 -no-replacement"" \
-F ""weka.filters.unsupervised.attribute.StringToWordVector -R first-last -W 100000 -prune-rate -1.0 -N 0 -S -stemmer weka.core.stemmers.NullStemmer -M 2 -tokenizer weka.core.tokenizers.AlphabeticTokenizer"" \
-F ""weka.filters.unsupervised.attribute.Reorder -R 2-last,first""\
-F ""weka.filters.supervised.attribute.AttributeSelection -E \""weka.attributeSelection.InfoGainAttributeEval \"" -S \""weka.attributeSelection.Ranker -T 0.0 -N -1\"""" \
-F weka.filters.AllFilter \
-b -i input\Train.arff -o output\Train_b_out.arff -r input\Test.arff -s output\Test_b_out.arff
</code></pre>

<p>Here is <strong>the resultant error from the CLI:</strong></p>

<pre><code>weka.core.UnassignedClassException: weka.filters.supervised.instance.Resample: Class attribute not set!
at weka.core.Capabilities.test(Capabilities.java:1091)
at weka.core.Capabilities.test(Capabilities.java:1023)
at weka.core.Capabilities.testWithFail(Capabilities.java:1302)
at weka.filters.Filter.testInputFormat(Filter.java:434)
at weka.filters.Filter.setInputFormat(Filter.java:452)
at weka.filters.SimpleFilter.setInputFormat(SimpleFilter.java:195)
at weka.filters.Filter.batchFilterFile(Filter.java:1243)
at weka.filters.Filter.runFilter(Filter.java:1319)
at weka.filters.MultiFilter.main(MultiFilter.java:425)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at weka.gui.SimpleCLIPanel$ClassRunner.run(SimpleCLIPanel.java:265)
</code></pre>

<p>And here are the headers with a portion of data for both <strong>the training and test input arffs:</strong></p>

<p><strong>Training:</strong></p>

<pre><code>@RELATION classifiedTweets
@ATTRIBUTE @@sentence@@ string
@ATTRIBUTE @@class@@ {1,-1,0}
@DATA
""Conditioning be very important for curly dry hair"",0
""Combine with Sunday paper coupon and"",0
""Price may vary by store"",0
""Oil be not really moisturizers"",-1
</code></pre>

<p><strong>Testing:</strong></p>

<pre><code>@RELATION classifiedTweets
@ATTRIBUTE @@sentence@@ string
@ATTRIBUTE @@class@@ {1,-1,0}
@DATA
""5"",0
""I give the curl a good form and discipline"",1
""I have be cowashing every day"",0
""LOL"",0
""TITLETITLE Walgreens Weekly and Midweek Deal"",0
""And then they walk away"",0
</code></pre>

<p>Am I doing something wrong here? I know that supervised resampling requires the class attribute to be on the bottom of the attribute list within the header, and it is... within both the test and training input files.</p>

<p><strong>EDIT:</strong></p>

<p>Further testing reveals that this error does not occur with relationship to the batch filtering, it occurs whenever I run the supervised resample filter from the CLI... The data that I use works on every other filter I've tried within the CLI, so I don't understand why this filter is any different... resampling the data in the GUI works fine as well...</p>

<p><strong>Update:</strong></p>

<p>This also happens with the SMOTE filter instead of the resample filter</p>
","command-line, weka, text-classification","<p>Could not get the batch filter to work with any resampling filter. However, our workaround was to simply resample (and then randomize) the training data as step 1. From this reduced set we ran batch filters for everything else we wanted on the test set. This seemed to work fine.</p>
",0,1,1023,2014-04-27 20:44:18,https://stackoverflow.com/questions/23328769/batch-filtering-with-multi-filter-throws-a-class-attribute-not-set-exception
"How do I tell if a noun is a person, place, or a thing?","<p>I am trying to classify text and then map the nouns on to a person, place, or a thing.  Is there a way or dictionary to do that?</p>
","nlp, text-classification","<p>Since you are dealing with classification, it might be interesting for you to have a look at AlchemyAPI, <code>http://www.alchemyapi.com/products/features/</code>. You have a free api key where you can try things. </p>

<p>But this doesn't stops here, if you want to do it manually, as your can see in @tripleee answer, WordNet is mentioned, this is also something interesting, and right now you have API where you can use like Java and interact with WordNet.</p>

<p>More, you have ontologies and many are free, these ontologies are normally in OWL or RDF. You can query these ontologies and find the relevant information. For OWL ontologies, you have the OWLAPI which you can use to interact, for RDF ontologies, you can use Apache Jena and write SPARQL statements.</p>

<p>Also, you have dbpedia, which is very very interesting and I believe that this might solve your problem to a big extent. <a href=""http://en.wikipedia.org/wiki/DBpedia"" rel=""nofollow"">dbpedia</a> is Wikipedia in machine-readable format.</p>

<p>For example, you can write SPARQL (much like SQL Statements), for e.g. suppose I want to check the relationship between London and UK,</p>

<pre><code>SELECT ?property
WHERE {
:London ?property :United_Kingdom
} 
</code></pre>

<p>OR suppose that I want to get all countries and cities,</p>

<pre><code>SELECT DISTINCT ?city ?country
WHERE { ?city rdf:type dbpedia-owl:City ;
rdfs:label ?label ;
dbpedia-owl:country ?country
}
</code></pre>
",2,0,962,2014-04-29 03:45:53,https://stackoverflow.com/questions/23355082/how-do-i-tell-if-a-noun-is-a-person-place-or-a-thing
Lexicon dictionary for synonym words,"<p>There are few dictionaries available for natural language processing. Like positive, negative words dictionaries etc. </p>

<p>Is there any dictionary available which contains list of synonym for all dictionary words? </p>

<p>Like for <code>nice</code> </p>

<pre><code>synonyms: enjoyable, pleasant, pleasurable, agreeable, delightful, satisfying, gratifying, acceptable, to one's liking, entertaining, amusing, diverting, marvellous, good; 
</code></pre>
","dictionary, nlp, stanford-nlp, data-processing, text-classification","<p>Although WordNet is a good resource to start for finding synonym, one must note its limitations, here's an example with python API in <code>NLTK</code> library:</p>

<p>Firstly, words have multiple meanings (i.e. senses):</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; wn.synsets('nice')
[Synset('nice.n.01'), Synset('nice.a.01'), Synset('decent.s.01'), Synset('nice.s.03'), Synset('dainty.s.04'), Synset('courteous.s.01')]
</code></pre>

<p>And to access the correct sense of a word, you will need to know the correct sense of a word given a context. </p>

<pre><code>&gt;&gt;&gt; wn.synset('nice.a.01').definition()
u'pleasant or pleasing or agreeable in nature or appearance'
</code></pre>

<p>You can try Word Sense Disambiguation software but they are not perfect (see <a href=""https://stackoverflow.com/questions/4613773/anyone-know-of-some-good-word-sense-disambiguation-software/8808962#8808962"">Anyone know of some good Word Sense Disambiguation software?</a>). Even if you know the sense of the word, the entries of wordnet are limited. You cannot expect much:</p>

<pre><code>&gt;&gt;&gt; wn.synset('nice.a.01').lemma_names()
[u'nice']
&gt;&gt;&gt; wn.synset('nice.a.01').similar_tos()
[Synset('good.s.06'), Synset('pleasant.s.02')]
&gt;&gt;&gt; [i.lemma_names() for i in wn.synset('nice.a.01').similar_tos()]
[[u'good'], [u'pleasant']]
</code></pre>
",10,5,4519,2014-05-17 10:27:34,https://stackoverflow.com/questions/23710214/lexicon-dictionary-for-synonym-words
Hierarchical prediction using R,"<p>I'm pretty new in R, and I couldn't find any information about a package who can do the following: supposing that I have a set of data (for instance, different text documents), which can have several classes. </p>

<p>For example, a datum could be a Sport, a Sport with Ball, a Sport without Ball and a Car. I'd like to be able to predict to which category the data belongs, considering that I may not hit that the datum is a Sport with Ball, but I'd be happy if I correctly predict that it's a Sport.</p>

<p>Which package can provide this kind of stuff? Some examples would be useful, if possible.</p>

<p>Thanks in advance</p>
","r, machine-learning, classification, document-classification, text-classification","<p>I am not aware of any specific packages in R that can do hierarchical classification. So there are two options:</p>

<ul>
<li>Use the C API SVMstruct <a href=""http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html"" rel=""nofollow"">http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html</a>. Programming this in R from scratch will be quite some work.</li>
<li>Build your own hierarchical classifier system. In the top-down case you will have a    multi-class classifier for each level, e.g. rec vs sci and motorcycles vs sport etc. You will use the top classifier and use its prediction to choose next classifier. The data you feed in to train a classifier with a node is the union of all data in the subtree rooted at that node.</li>
</ul>

<p>For details read e.g. <a href=""http://jmlr.org/papers/v6/tsochantaridis05a.html"" rel=""nofollow"">http://jmlr.org/papers/v6/tsochantaridis05a.html</a></p>
",0,1,1151,2014-05-17 20:30:16,https://stackoverflow.com/questions/23715803/hierarchical-prediction-using-r
How to encode different size of feature vectors in SVM,"<p>I work on classifying some reviews (paragraphs) consists of multiple sentences. I classified them with bag-of-word features in Weka via libSVM. However, I had another idea which I don't know how to implement :</p>

<p>I thought creating syntactical and shallow-semantics based features per sentence in the reviews is worth to try. However, I couldn't find any way to encode those features sequentially, since a paragraph's sentence size varies. The reason that I wanted to keep those features in an order is that the order of sentence features may give a better clue for classification. For example, if I have two instances P1 (with 3 sentences) and P2 (2 sentences), I would have a space like that (assume each sentence has one binary feature as a or b):</p>

<p>P1 -> a b b /classX
P2 -> b a /classY</p>

<p>So, my question is that whether I can implement that classification of different feature sizes in feature space or not? If yes, is there any kind of classifier that I can use in Weka, scikit-learn or Mallet? I would appreciate any responses.</p>

<p>Thanks</p>
","machine-learning, scikit-learn, weka, text-classification","<p>Regardless of the implementation, an SVM with the standard kernels (linear, poly, RBF) requires fixed-length feature vectors. You can encode any information in those feature vectors by encoding as booleans; e.g. collect all syntactical/semantic features that occur in your corpus, then introduce booleans that represent that ""feature such and such occurred in this document"". If it's important to capture the fact that these features occur in multiple sentences, count them and use put the frequency in the feature vector (but be sure to normalize your frequencies by document length, as SVMs are not scale-invariant).</p>
",6,2,3118,2014-05-31 05:17:21,https://stackoverflow.com/questions/23966648/how-to-encode-different-size-of-feature-vectors-in-svm
Text classification scheme for a classification task with 120 classes,"<p>I have about 15k text instances which are hand categorized into one of the 120 categories. The text pieces are emails from customers. The frequency of classes varies from 1 to 2000. I would like to train a classifier on this data so that subsequent emails can be classified automatically.</p>

<p>I have tried Naive Bayes' and SVM both but report percentage of correctly classified instances as 51% and 57% respectively. I have used stemmer, removed stop words and changed text to lower case.</p>

<p>I am sure such a text classification task with large number of categories and uneven distribution, has to be approached differently but I could not find any reference for such a case...Any recommendations?</p>

<p>Thanks in advance! </p>
","weka, svm, text-classification","<p>I assume that classes are not overlapping (that is, exactly one class per message).</p>

<p>A useful approach in the case of imbalanced classes is <strong>using asymetric miss-classification costs</strong> in order to enforce the classifier to focus on the less represented class, as its cost is assigned much bigger figure than other classes.</p>

<p>This is relatively easy to do in WEKA (see e.g. <a href=""http://jmgomezhidalgo.blogspot.com.es/2008/03/class-imbalanced-distribution-and-weka.html"" rel=""nofollow"">Class imbalanced distribution and WEKA cost sensitive learning</a>) in the case of binary classifiers, but it is much harder to setup in the case of 120 classes. In consequence, one approach would be to turn this problem into 120 binary problems (one-against-the-rest) and setting up the appropriate cost matrixes for each problem.</p>

<p>A more viable alternative in my experience, and given the high number of classes, is <strong>to collapse the unfrequent classes into a bigger <code>other</code> class</strong>. This seems more useful for a practical setting; there is a ""other"" folder to check by a human expert while most of the time the classifier is correctly assigning the emails to the rest of --well populated-- classes.</p>

<p>As a final note, an accuracy of about 50-60% may be not to bad after all, depending on the distribution of classes. For instance, the majority classifier (the one that assigns every instance to the most populated class) in a 99%-1% would be 99% accurate; however it is absolutely useless, because it misses the interesting examples. In the real life this happens in email spam filtering, fraud spotting, and quite many other domains.</p>
",0,0,158,2014-06-04 06:18:57,https://stackoverflow.com/questions/24030354/text-classification-scheme-for-a-classification-task-with-120-classes
Naive Bayes: Imbalanced Test Dataset,"<p>I am using scikit-learn Multinomial Naive Bayes classifier for binary text classification (classifier tells me whether the document belongs to the category X or not). I use a balanced dataset to train my model and a balanced test set to test it and the results are very promising.</p>

<p>This classifer needs to run in real time and constantly analyze documents thrown at it randomly.</p>

<p>However, when I run my classifier in production, the number of false positives is very high and therefore I end up with a very low precision. The reason is simple: there are many more negative samples that the classifer encounters in the real-time scenario (around 90 % of the time) and this does not correspond to the ideal balanced dataset I used for testing and training.</p>

<p>Is there a way I can simulate this real-time case during training or are there any tricks that I can use (including pre-processing on the documents to see if they are suitable for the classifer)? </p>

<p>I was planning to train my classifier using an imbalanced dataset with the same proportions as I have in real-time case but I am afraid that might bias Naive Bayes towards the negative class and lose the recall I have on the positive class.</p>

<p>Any advice is appreciated. </p>
","python, machine-learning, classification, scikit-learn, text-classification","<p>You have encountered one of the problems with classification with a highly imbalanced class distribution. I have to disagree with those that state the problem is with the Naive Bayes method, and I'll provide an explanation which should hopefully illustrate what the problem is.</p>

<p>Imagine your false positive rate is 0.01, and your true positive rate is 0.9. This means your false negative rate is 0.1 and your true negative rate is 0.99.</p>

<p>Imagine an idealised test scenario where you have 100 test cases from each class. You'll get (in expectation) 1 false positive and 90 true positives. Great! Precision is 90 / (90+1) on your positive class!</p>

<p>Now imagine there are 1000 times more negative examples than positive. Same 100 positive examples at test, but now there are 1000000 negative examples. You now get the same 90 true positives, but (0.01 * 1000000) = 10000 false positives. Disaster! Your precision is now almost zero (90 / (90+10000)).</p>

<p>The point here is that the performance of the classifier <em>hasn't changed</em>; false positive and true positive rates remained constant, but the balance changed and your precision figures dived as a result.</p>

<p>What to do about it is harder. If your scores are separable but the threshold is wrong, you should look at the ROC curve for thresholds based on the posterior probability and look to see if there's somewhere where you get the kind of performance you want. If your scores are not separable, try a bunch of different classifiers and see if you can get one where they are (logistic regression is pretty much a drop-in replacement for Naive Bayes; you might want to experiment with some non-linear classifiers, however, like a neural net or non-linear SVM, as you can often end up with non-linear boundaries delineating the space of a very small class).</p>

<p>To simulate this effect from a balanced test set, you can simply multiply instance counts by an appropriate multiplier in the contingency table (for instance, if your negative class is 10x the size of the positive, make every negative instance in testing add 10 counts to the contingency table instead of 1).</p>

<p>I hope that's of some help at least understanding the problem you're facing.</p>
",11,17,11397,2014-06-23 13:25:26,https://stackoverflow.com/questions/24367141/naive-bayes-imbalanced-test-dataset
Wrong classification to multiple classes with different fraction of classes,"<p>I've encouraged with next problem:
I'm trying to classify a lot of text documents.</p>

<p>There are 20 classes: 1 normal, 19 - abnormal.
When I use Naïve bayes classification I have the following result: classification works well for 19 classes, but for ""normal"" class I got many misclassification errors: almost all cases in ""normal"" category were classified as other (non-normal) category.</p>

<p>There are my questions:</p>

<ul>
<li>How should I select training set for ""normal"" class?  (Now, I just fit
to classifier set of text with ""normal"" category, with 1/20
proportion).</li>
<li>Can classifier be specified this way: if probability of belonging to 
some class less then certain threshold then classifier must set up<br>
category for this sample (e.g. normal)?</li>
</ul>
","machine-learning, nlp, scikit-learn, text-classification","<p>I'm not sure to have the full picture but It seems like you have in fact only 2 classes ""normal"" and ""abnormal"" which are unbalanced in volume and thus prior.</p>

<p>To answer your first question, in that situation, I would try to over-sampling your normal class for training (passing same ""normal"" instances multiple times to  ""fake"" bigger volume) and see if it improves your performances.</p>

<p>I don't get your second question.</p>
",1,0,495,2014-06-24 09:12:02,https://stackoverflow.com/questions/24382672/wrong-classification-to-multiple-classes-with-different-fraction-of-classes
Tokenize and stopword don&#39;t work in Tweets DB using RapidMiner,"<p>I would like tokenize and apply stop word filter on Twitter comments contained in a database, but Process Document does nothing. What am I doing wrong?</p>

<p>My goal is to apply these filters but keep the comments in rows instead of a single word vector.</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;process version=""5.3.015""&gt;
  &lt;context&gt;
    &lt;input/&gt;
    &lt;output/&gt;
    &lt;macros/&gt;
  &lt;/context&gt;
  &lt;operator activated=""true"" class=""process"" compatibility=""5.3.015"" expanded=""true"" name=""Process""&gt;
    &lt;process expanded=""true""&gt;
      &lt;operator activated=""true"" class=""read_database"" compatibility=""5.3.015"" expanded=""true"" height=""60"" name=""Server Connection (2)"" width=""90"" x=""45"" y=""30""&gt;
        &lt;parameter key=""connection"" value=""sqlserver2014""/&gt;
        &lt;parameter key=""query"" value=""select top 60 tweetid,content from [Tweets General]""/&gt;
        &lt;enumeration key=""parameters""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:data_to_documents"" compatibility=""5.3.002"" expanded=""true"" height=""60"" name=""Data to Documents"" width=""90"" x=""246"" y=""30""&gt;
        &lt;parameter key=""select_attributes_and_weights"" value=""true""/&gt;
        &lt;list key=""specify_weights""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:process_documents"" compatibility=""5.3.002"" expanded=""true"" height=""94"" name=""Process Documents"" width=""90"" x=""447"" y=""30""&gt;
        &lt;process expanded=""true""&gt;
          &lt;operator activated=""true"" class=""text:tokenize"" compatibility=""5.3.002"" expanded=""true"" height=""60"" name=""Tokenize (3)"" width=""90"" x=""246"" y=""75""/&gt;
          &lt;connect from_port=""document"" to_op=""Tokenize (3)"" to_port=""document""/&gt;
          &lt;connect from_op=""Tokenize (3)"" from_port=""document"" to_port=""document 1""/&gt;
          &lt;portSpacing port=""source_document"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 1"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 2"" spacing=""0""/&gt;
        &lt;/process&gt;
      &lt;/operator&gt;
      &lt;connect from_op=""Server Connection (2)"" from_port=""output"" to_op=""Data to Documents"" to_port=""example set""/&gt;
      &lt;connect from_op=""Data to Documents"" from_port=""documents"" to_op=""Process Documents"" to_port=""documents 1""/&gt;
      &lt;connect from_op=""Process Documents"" from_port=""example set"" to_port=""result 1""/&gt;
      &lt;portSpacing port=""source_input 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 2"" spacing=""0""/&gt;
    &lt;/process&gt;
  &lt;/operator&gt;
&lt;/process&gt;
</code></pre>
","twitter, tokenize, stop-words, rapidminer, text-classification","<p>You need to convert any attributes of type nominal to be of type text before the <code>Data to Documents</code> operator. The operator <code>Nominal to Text</code> will do this. You also need to set the option <code>select attributes and weights</code> to false in <code>Data to Documents</code> because I think the setting you have will deselect everything.</p>
",0,0,276,2014-07-04 20:05:03,https://stackoverflow.com/questions/24580260/tokenize-and-stopword-dont-work-in-tweets-db-using-rapidminer
How to classify text properly in weka given preprocessing is needed,"<p>I need to classify some text using weka programmatically, but I am having trouble as the training data and the to-be-classified data need to be filtered (the same way) before being used with the classifier.</p>

<p>My approach to the problem is currently:
Create an arff with training data with a string attribute and a class.
Use StringToWordVector over the data set and save the filter for future use.
Use Attributeselection filter over the resulting data and save filter for future use.
Train the classifier with that data and save the classifier.
Create a ""Instances"" with the same attributes as the arff and populate it with the Instance I want to classify with the value of class attribute missing.
Load the StringToWordVector filter and use it to filter Instances.
Load AttributeSlection filter and use it to filter the result.
Load the classifier and classify the result.</p>

<p>It seems that StringToWordVector is working as I expected and using the same set of words with the new data as with the old. The problem is with AttributeSelection that tries, it seems, to run again not knowing that I just want it to use the attributes it already filtered before.</p>
","java, classification, weka, text-classification","<ol>
<li><p>Re-using same attribute selection setup:
Attribute selection is a filter, you should use batch filtering method to be able to re-use it and get compatible data (<a href=""http://weka.wikispaces.com/Use+Weka+in+your+Java+code#Batch%20filtering"" rel=""nofollow"">http://weka.wikispaces.com/Use+Weka+in+your+Java+code#Batch%20filtering</a>) => after declaring your filter &amp; setup, you should call setInputFormat (ie. myfilter.setInputFormat(train)), use it on training data (Filter.useFilter(train, myfilter)), serialize the data if you want to use it later on test data. The setInputFormat(Instances) method always has to be the last call before the filter is applied.</p></li>
<li><p><strong>Not re-running the attribute selection:</strong> use reduceDimensionality method of your AttributeSelection object (ie. myfilter.reduceDimensionality() would reduce the dimensionality to include only those attributes ""chosen by the last run of attribute selection""). I think it is your main problem now.</p></li>
<li><p>If you want to re-use multiple filters (ie. StringToWordVector, standardization, selection), you should test a multi-filter solution.</p>

<p>StringToWordVector swv = new StringToWordVector();
AttributeSelection as = new AttributeSelection();
Standardize st = new Standardize();
MultiFilter mf = new MultiFilter(); 
Filter[] filters = {swv, st, as};
mf.setFilters(filters);</p></li>
</ol>

<p>Xavier</p>
",1,2,1156,2014-07-12 18:50:21,https://stackoverflow.com/questions/24716221/how-to-classify-text-properly-in-weka-given-preprocessing-is-needed
What features are good for Sentence Classification apart from using vector representation like Bag-of-words?,"<p>I am trying to find whether a given sentence is a ""question request"", ""call for action"", etc. I am using supervised multilabel classification for that.</p>

<p>What will be a good set of features to use? I am currently using Bag-of-words with trigrams, modal verbs, question words, etc. but the result is not that good.</p>

<p>Input example: ""Can you get this today? I need following items.""  </p>
","python, nlp, text-classification, text-analysis","<p><a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">https://code.google.com/p/word2vec/</a> is probably a good feature. </p>

<p>Illinois Wilkifier can also be very helpful: <a href=""http://cogcomp.cs.illinois.edu/demo/wikify/?id=25"" rel=""nofollow"">http://cogcomp.cs.illinois.edu/demo/wikify/?id=25</a></p>

<p>Also take a look at features used for Dataless classification: <a href=""http://cogcomp.cs.illinois.edu/page/project_view/6"" rel=""nofollow"">http://cogcomp.cs.illinois.edu/page/project_view/6</a> </p>
",1,1,835,2014-07-16 19:18:04,https://stackoverflow.com/questions/24788916/what-features-are-good-for-sentence-classification-apart-from-using-vector-repre
"Scikit-learn: don&#39;t use some words as one word feature, but use in collocations","<p>I'm doing text classification with using Python and scikit-learn.</p>

<p>Now, I use TfidfVectorizer as vectorizer (for transform raw text to a feature vector) and MultinomialNB as a classifier.
I use parameter ngram_range = (1,2) (see <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a> ), e.g. I use one word and bigrams.</p>

<p>After classification and test my algorithm in test set and CV set, I'd like to improve accuracy. 
I see the most informative features ( due to question <a href=""https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"">How to get most informative features for scikit-learn classifiers?</a> ). And I see, that in the set most informative features I have words ( ngram=1), that don't have impact to classification, but in bigram (words collocations) they will have great impact.</p>

<p>So, I can't use stop_words, because Tfidfvectorizer will not consider this words in collocations and I can't use preprocessor at the same reason.
Question: How can I exclude some words in tfidfvectorizer, but save this words in different collocations?</p>
","python, scikit-learn, text-classification","<p>I think there are a few possible ways of doing it:</p>

<ol>
<li><p>Construct two <code>TfidfVectorizer</code> twice both with <code>ngram_range=(1,2)</code>. Extract the feature names after fitting the first vectorizer, filter out unwanted the unigram features, and feed this list of features as the <code>vocabulary</code> argument of the second vectorizer. Use the second vectorizer for transformation.</p></li>
<li><p>Supply the <code>analyzer</code> argument of <code>TfidfVectorizer</code> as a function which performs customized extraction of features from each raw document, e.g. avoid spitting out some useless unigram as feature (but this means you need to do the work of generating words combinations yourself).</p></li>
<li><p>Fit a <code>TfidfVectorizer</code> as usual, which might contain some unwanted unigrams. Use <code>get_feature_names()</code> to get the column indices corresponding to the features you want. When you do <code>transform()</code> using the vectorizer, do an extra step of slicing the columns of the resulting sparse matrix, based on the indices of interest.</p></li>
</ol>
",2,1,728,2014-07-22 12:44:02,https://stackoverflow.com/questions/24887775/scikit-learn-dont-use-some-words-as-one-word-feature-but-use-in-collocations
Machine-Learning - Concept / Recommendations,"<p>Hi I'm new at machine learning and therefore looking for a text classification solution. Could one recommend me a nice framework written in java? I thought about using WEKA, but also heard about MALLET. What's better, where are the main differences?</p>

<p>My target is to classify unlabeled text. Therefore I prepared about 18 topics and 100 text for each topic for learning.</p>

<p>What would you recommend to do? Would also appreciate a nice little example or hint of how to proceed.</p>
","machine-learning, classification, text-classification","<p>You have a very minimal text data set, you could use any library - it wouldn't really matter. More advanced options would require more data than you have to be meaningful, so its not an issue worth considering. The simple way text classifications problems are handled is to use a <a href=""http://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow"">Bag of Words</a> model and a linear classifier. Both Weka and MALLET support this. </p>

<p>Personally, I find Weka to be a pain and MALLET to be poorly documented / out of date when it is, so I use <a href=""https://code.google.com/p/java-statistical-analysis-tool/"" rel=""nofollow"">JSAT</a>. There is an example on doing spam classification <a href=""https://code.google.com/p/java-statistical-analysis-tool/wiki/LoadingTextDataSpam"" rel=""nofollow"">here</a>. </p>

<p>(bias warning, I'm the author of JSAT). </p>
",2,-2,630,2014-07-22 19:54:18,https://stackoverflow.com/questions/24896644/machine-learning-concept-recommendations
Document Clustering and Classification in Solr?,"<p>I'm building an index of documents in Solr. 
Documents are non-scientific. </p>

<p>I have a category linked to each document, they can be used for teaching. I would like to assign category for new document upon addition. Documents are added all the time without rebuilding all index.</p>

<p>Also documents can be about same thing, but from different sources, so I'd like to make document clustering. So when document is added - I can search whether I already have such topic in the last N days, if yes - then save cluster ID.</p>

<p>Index size is about 500k documents and rising, so speed is important. </p>

<p>So I want to calculate for each new document: Category ID (based on training with pre-defined documents), Cluster ID (matched only for N days, not the whole index).</p>

<p>Is that possible to make with SOLR? Or it is better to make separate solution (if yes then which one?)</p>
","solr, document-classification, text-classification","<p>solr 6.1 and lucene 6.1 has this capability now. It offers knn and naive bayes off the shelves. this is a great post about how to use it in solr: <a href=""http://alexbenedetti.blogspot.com/2015/07/solr-document-classification-part-1.html"" rel=""nofollow"">solr based text classification</a></p>
",1,1,1825,2014-08-20 07:41:44,https://stackoverflow.com/questions/25399291/document-clustering-and-classification-in-solr
How to split data (raw text) into test/train sets with scikit crossvalidation module?,"<p>I have a large corpus of opinions (2500) in raw text. I would like to use scikit-learn library to split them into test/train sets. What could be the best aproach to solve this task with scikit-learn?. Could anybody provide me an example of spliting raw text in test/train sets (probably i´ll use tf-idf representation).</p>
","machine-learning, scikit-learn, classification, cross-validation, text-classification","<p>Suppose your data is a list of strings, i.e. </p>

<pre><code>data = [""...."", ""..."", ]
</code></pre>

<p>Then you can split it into training (80%) and test (20%) sets using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer"">train_test_split</a> e.g. by doing:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.model_selection import train_test_split
train, test = train_test_split(data, test_size = 0.2)
</code></pre>

<p>Before you rush doing it, though, read <a href=""http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection"" rel=""nofollow noreferrer"">those docs</a> through. 2500 is not a ""large corpus"" and you probably want to do something like a k-fold cross-validation rather than a single holdout split.</p>
",22,10,21106,2014-09-11 17:44:05,https://stackoverflow.com/questions/25793887/how-to-split-data-raw-text-into-test-train-sets-with-scikit-crossvalidation-mo
Storing XValidation (Cross Validation) Folds in Rapidminer?,"<p>I have tried a lot via code to save the test/train split samples for each fold in 10-fold cross validation(stratified) but couldn't manage to do that...</p>

<p>Is there any way to save the test/train splits samples (not model) in rapid miner ? </p>
","machine-learning, classification, rapidminer, cross-validation, text-classification","<p>I was about to suggest this for your previous post.</p>

<p>The <code>Store</code> operator that lets you store an example set in the repository. To allow the names of the different folds to be distinguished, you would need to have a macro that changes each time the operator executes. You then use the macro name when configuring the name in the <code>Store</code> operator. To create a macro use the <code>Generate Macro</code> operator which you can also use to increment it within the cross validation operator.</p>
",0,0,656,2014-09-14 08:58:22,https://stackoverflow.com/questions/25831673/storing-xvalidation-cross-validation-folds-in-rapidminer
weka 3.7 explorer cannot classify text,"<p>I am trying to do text classification using weka 3.7 explorer. I converted 2 text files( separated into two dir class1 and class2) into arff using text loader. Before doing so, I standardized the case to lower. Now when I load the file into weka and apply filter stringtowordvector (such as stopwords,usewordcount, usestoplist, stemmer - snowballstemmer) I do not see any change in my list of variables . All the variables (words ) are given as 1 or 0 against each class. </p>

<p>Please help me.</p>

<p>Here is my filter command</p>

<p>weka.filters.unsupervised.attribute.StringToWordVector -R first-last -W 1000 -prune-rate -1.0 -C -N 0 -S -stemmer weka.core.stemmers.SnowballStemmer -M 1 -tokenizer ""weka.core.tokenizers.WordTokenizer -delimiters \"" \r\n\t.,;:\\'\\""()?!\""""</p>
","machine-learning, classification, weka, text-analysis, text-classification","<p>That happend to me when I wanted to read from .csv and use StringToWord vector.</p>

<p>My problem was, that the text attribute was of type nominal and not String. I used the class ""NominalToString"", used it to changed values to String, and then it worked. </p>
",0,0,157,2014-09-14 22:15:19,https://stackoverflow.com/questions/25838537/weka-3-7-explorer-cannot-classify-text
How to use pickled classifier with countVectorizer.fit_transform() for labeling data,"<p>I trained a classifier on a set of short documents and pickled it after getting the reasonable f1 and accuracy scores for a binary classification task. </p>

<p>While training, I reduced the number of features using a sciki-learn <code>countVectorizer</code> cv:</p>

<pre><code>    cv = CountVectorizer(min_df=1, ngram_range=(1, 3), max_features = 15000) 
</code></pre>

<p>and then used the <code>fit_transform()</code> and <code>transform()</code> methods to obtain the transformed train and test sets:</p>

<pre><code>    transformed_feat_train = numpy.zeros((0,0,))
    transformed_feat_test = numpy.zeros((0,0,))

    transformed_feat_train = cv.fit_transform(trainingTextFeat).toarray()
    transformed_feat_test = cv.transform(testingTextFeat).toarray()
</code></pre>

<p>This all worked fine for training and testing the classifier. However, I am not sure how to use <code>fit_transform()</code> and <code>transform()</code> with a pickled version of the trained classifier for predicting the label of unseen, unlabeled data.</p>

<p>I am extracting the features on the unlabeled data exactly the same way I was doing while training/testing the classifier:</p>

<pre><code>## load the pickled classifier for labeling
pickledClassifier = joblib.load(pickledClassifierFile)

## transform data
cv = CountVectorizer(min_df=1, ngram_range=(1, 3), max_features = 15000)
cv.fit_transform(NOT_SURE)

transformed_Feat_unlabeled = numpy.zeros((0,0,))
transformed_Feat_unlabeled = cv.transform(unlabeled_text_feat).toarray()

## predict label on unseen, unlabeled data
l_predLabel = pickledClassifier.predict(transformed_feat_unlabeled)
</code></pre>

<p>Error message:</p>

<pre><code>    Traceback (most recent call last):
      File ""../clf.py"", line 615, in &lt;module&gt;
        if __name__==""__main__"": main()
      File ""../clf.py"", line 579, in main
        cv.fit_transform(pickledClassifierFile)
      File ""../sklearn/feature_extraction/text.py"", line 780, in fit_transform
        vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary)
      File ""../sklearn/feature_extraction/text.py"", line 727, in _count_vocab
        raise ValueError(""empty vocabulary; perhaps the documents only""
    ValueError: empty vocabulary; perhaps the documents only contain stop words
</code></pre>
","python, scikit-learn, text-classification","<p>You should use the same vectorizer instance for transforming the training and test data. You can do that by creating a pipeline with the vectorizer + classifier, training the pipeline on the training set, pickling the whole pipeline. Later load the pickled pipeline and call predict on it.</p>

<p>See this related question: <a href=""https://stackoverflow.com/questions/25788151/bringing-a-classifier-to-production"">Bringing a classifier to production</a>.</p>
",4,5,5448,2014-09-23 21:02:23,https://stackoverflow.com/questions/26004670/how-to-use-pickled-classifier-with-countvectorizer-fit-transform-for-labeling
Unknown words in Naive Bayes classification,"<p>How do I test a text classification problem with unknown words? In training a model, we can use smoothing technique (Laplace add-1) to make sure any word will receive at least 1 count for each class.</p>

<p>Then, what about at testing stage? If a word doesn't occur in the training data, what's the best way to deal with it? Simply skip it, or also give an occurrence of 1 to it?</p>

<p>Thanks, for any suggestions or opinions. Specifically, I am using a Naive Bayes classifier. </p>
","machine-learning, smoothing, text-classification","<p>When you come to classify an instance, think about whats going on, if you do the add-1 smoothing for an unseen feature, then you'd simply multiply a very small probability (1 / vocabSize) (or add the log of a very small probability) to your accumulated scores. If you are skipping the unseen feature then nothing happens to the scores.</p>

<p>So, generally speaking an unseen feature in your test data shouldn't make a difference to your classification decision - you know nothing about it as you haven't seen it in training, so in the case of smoothing you'd be multiplying (or adding) the same small (log-)probability to all your scores per class or you'd simply ignore it for all of your class scores. </p>

<p>If you're not convinced, simply try both and see if it makes any difference.</p>
",3,2,3296,2014-10-09 21:10:49,https://stackoverflow.com/questions/26287971/unknown-words-in-naive-bayes-classification
How to lemmatize spanish words with Pattern?,"<p>I would like to lemmatize a bunch of opinions. As I know, nltk cannot lemmatize words in languages different from English. Researching a little, I found <a href=""http://www.clips.ua.ac.be/pattern"" rel=""nofollow"">pattern</a>, which can lemmatize words in several languages. How can I lemmatize some text with pattern?</p>

<p>This is my test corpus:</p>

<pre><code># -- coding: utf-8 --

from pattern.es import lemma #unresolved reference

opinions = [""Este es un post de juguetes de aprendizaje \
automático. En realidad, contiene no mucho \
material interesante."",
""Las bases de datos de imágenes proporcionan \
capacidades de almacenamiento."",
""La mayoría de las bases de datos de imágenes \
imágenes seguras de forma permanente."",
""Los datos de imagen de tienda bases de datos."",
""Imagina almacenar bases de datos de bases de \
datos de imágenes. Almacenar datos. Bases de datos \
de imágenes de datos de la tienda.""]

print lemma(opiniones)
</code></pre>

<p>output:</p>

<pre><code>  File ""/Users/user/PycharmProjects/Pruebas/Lemmatizacion.py"", line 18, in &lt;module&gt;
    print lemma(opiniones)
  File ""/usr/local/lib/python2.7/site-packages/pattern/text/__init__.py"", line 1591, in lemma
    if verb.lower() in self._inverse:
AttributeError: 'list' object has no attribute 'lower'
</code></pre>

<p>How can I lemmatize <code>opinions</code>?</p>
","python, machine-learning, nlp, nltk, text-classification","<p>You can lemmatize text using pattern's parsetree function with the lemmata parameter set to True.</p>

<pre><code>from pattern.es import parsetree
parsetree(""buena"", lemmata=True)
# Returns [Sentence('buena/JJ/B-ADJP/O/bueno')]
</code></pre>
",4,2,4718,2014-10-09 23:48:05,https://stackoverflow.com/questions/26289759/how-to-lemmatize-spanish-words-with-pattern
CountVectorizer: AttributeError: &#39;numpy.ndarray&#39; object has no attribute &#39;lower&#39;,"<p>I have a one-dimensional array with large strings in each of the elements. I am trying to use a <code>CountVectorizer</code> to convert text data into numerical vectors. However, I am getting an error saying:</p>

<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'lower'
</code></pre>

<p><code>mealarray</code> contains large strings in each of the elements. There are 5000 such samples. I am trying to vectorize this as given below:</p>

<pre><code>vectorizer = CountVectorizer(
    stop_words='english',
    ngram_range=(1, 1),  #ngram_range=(1, 1) is the default
    dtype='double',
)
data = vectorizer.fit_transform(mealarray)
</code></pre>

<p>The full stacktrace :</p>

<pre><code>File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py"", line 817, in fit_transform
    self.fixed_vocabulary_)
  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py"", line 748, in _count_vocab
    for feature in analyze(doc):
  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py"", line 234, in &lt;lambda&gt;
    tokenize(preprocess(self.decode(doc))), stop_words)
  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py"", line 200, in &lt;lambda&gt;
    return lambda x: strip_accents(x.lower())
AttributeError: 'numpy.ndarray' object has no attribute 'lower'
</code></pre>
","python, numpy, scikit-learn, text-classification","<p>Got the answer to my question.
Basically, CountVectorizer is taking lists (with string contents) as an argument rather than array. That solved my problem.</p>
",9,18,87639,2014-10-14 17:48:39,https://stackoverflow.com/questions/26367075/countvectorizer-attributeerror-numpy-ndarray-object-has-no-attribute-lower
groupingBy operation in Java-8,"<p>I'm trying to re-write famous example of Spark's text classification (<a href=""http://chimpler.wordpress.com/2014/06/11/classifiying-documents-using-naive-bayes-on-apache-spark-mllib/"" rel=""nofollow"">http://chimpler.wordpress.com/2014/06/11/classifiying-documents-using-naive-bayes-on-apache-spark-mllib/</a>) on Java 8.</p>

<p>I have a problem - in this code I'm making some data preparations for getting idfs of all words in all files:</p>

<pre><code>    termDocsRdd.collect().stream().flatMap(doc -&gt; doc.getTerms().stream()
                                .map(term -&gt; new ImmutableMap.Builder&lt;String, String&gt;()
                                .put(doc.getName(),term)
                                .build())).distinct()        
</code></pre>

<p>And I'm stuck on the groupBy operation. (I need to group this by term, so each term must be a key and the value must be a sequence of documents).
In Scala this operation looks very simple - <strong>.groupBy(_._2)</strong>.
But how can I do this in Java?</p>

<p>I tried to write something like:</p>

<pre><code>    .groupingBy(term -&gt; term, mapping((Document) d -&gt; d.getDocNameContainsTerm(term), toList()));
</code></pre>

<p>but it's incorrect...</p>

<p>Somebody knows how to write it in Java?</p>

<p>Thank You very much.</p>
","java-8, apache-spark, tf-idf, text-classification","<p>If I understand you correctly, you want to do something like this:</p>

<p>(<code>import static java.util.stream.Collectors.*;</code>)</p>

<pre><code>Map&lt;Term, Set&lt;Document&gt;&gt; collect = termDocsRdd.collect().stream().flatMap(
 doc -&gt; doc.getTerms().stream().map(term -&gt; new AbstractMap.SimpleEntry&lt;&gt;(doc, term)))
.collect(groupingBy(Map.Entry::getValue, mapping(Map.Entry::getKey, toSet())));
</code></pre>

<p>The use of <code>Map.Entry</code>/ <code>AbstractMap.SimpleEntry</code> is due to the absence of a standard <code>Pair&lt;K,V&gt;</code> class in Java-8. <code>Map.Entry</code> implementations can fulfill this role but at the cost of having unintuitive and verbose type and method names (regarding the task of serving as <code>Pair</code> implementation).</p>

<hr>

<p>If you are using the current Eclipse version (I tested with LunaSR1 20140925) with its limited type inference, you have to help the compiler a little bit:</p>

<pre><code>Map&lt;Term, Set&lt;Document&gt;&gt; collect = termDocsRdd.collect().stream().flatMap(
 doc -&gt; doc.getTerms().stream().&lt;Map.Entry&lt;Document,Term&gt;&gt;map(term -&gt; new AbstractMap.SimpleEntry&lt;&gt;(doc, term)))
.collect(groupingBy(Map.Entry::getValue, mapping(Map.Entry::getKey, toSet())));
</code></pre>
",2,1,875,2014-10-15 09:36:09,https://stackoverflow.com/questions/26379148/groupingby-operation-in-java-8
Detecting random keyboard hits considering QWERTY keyboard layout,"<p>The <a href=""http://arxiv.org/abs/1210.5560"" rel=""nofollow"">winner of a recent Wikipedia vandalism detection competition</a> suggests that detection could be improved by ""<i>detecting random keyboard hits considering QWERTY
keyboard layout</i>"".</p>

<p>Example: <code>woijf qoeoifwjf oiiwjf oiwj pfowjfoiwjfo oiwjfoewoh</code></p>

<p>Is there any software that <b>does this already</b> (preferably <a href=""http://en.wikipedia.org/wiki/FOSS"" rel=""nofollow"">free and open source</a>) ?</p>

<p><b>If not</b>, is there an active FOSS project whose <b>goal</b> is to achieve this?</p>

<p><b>If not</b>, how would you suggest to <b>implement</b> such a software?</p>
","algorithm, n-gram, qwerty, text-classification","<p>If two <a href=""http://en.wikipedia.org/wiki/Bigram"" rel=""noreferrer"">bigrams</a> in analyzed text are close in QWERTY terms but have near zero statistical frequency in English language (like pairs ""fg"" or ""cd"") then there is chance that random keyboard hits are involved. If more such pairs are found then chance increases greatly.</p>

<p>If you want to take into account the use of both hands for bashing then test letters that are separated with another letter for QWERTY closeness, but two bigrams (or even trigrams) for bigram frequency. For example in text ""flsjf"" you would check F and S for QWERTY distance, but bigrams FL and LS (or trigram FLS) for frequency.</p>
",7,8,2380,2010-09-27 08:41:23,https://stackoverflow.com/questions/3802232/detecting-random-keyboard-hits-considering-qwerty-keyboard-layout
MAXent classifier NLTK output understand,"<p>I am trying to understand the <code>classifier.show_most_informative_features(10)</code> for MAXent classfier. I don't understand what the columns indicate, for example in the following output:</p>

<pre><code>train on 460 instances, test on 154 instances accuracy: 0.61038961039 
pos precision: 0.432989690722 
pos recall: 0.893617021277 
neg precision: 0.912280701754 
neg recall: 0.485981308411    
-4.141 need==True and label is 'REL'    
3.395 approves==True and label is 'IRREL'   -
3.308 took==True and label is 'IRREL' 
-1.766 treat==True and label is 'REL' 
-1.488 tired==True and label is 'IRREL' 
-1.295 gave==True and label is 'IRREL' 
0.879 need==True and label is 'IRREL' 
</code></pre>
","python, machine-learning, nltk, text-classification","<p>It seems that you have two labels, <code>""RELEVANT""</code> and <code>""IRRELEVANT""</code>. When there are two labels, one is normally named ""1"" or positive and the other ""-1"" or negative. </p>

<p>During the training process, the classifier analysed the features of the 460 training instances and weighted them according to their ability to distinguish well between the two labels. The details of the weighting process depend on the algorithm you chose. </p>

<p><strong>Poitive precision:</strong> 43 % of the 154 testing instances that were classified as label 1 during the testing really have the label 1. </p>

<p><strong>Positive recall:</strong> 89 % of the label 1 instances in the testing set were found, i.e. classified as label 1.  </p>

<p><strong>Negative precision / Negative recall</strong> is the same, but for label -1. </p>

<p><strong>Accuracy:</strong> 61 % of the 154 testing instances were labeled correctly. </p>

<p>The features are sorted according to their absolute value which corresponds to their  relevance for the classification. The most ""helpful"" feature in this case was <em>need</em>, and if it is true, this is a very good hint that the label of the instance should be ""RELEVANT"". </p>
",2,2,1150,2013-04-28 19:26:23,https://stackoverflow.com/questions/16266842/maxent-classifier-nltk-output-understand
How can i classify text documents with using SVM and KNN,"<p>Almost all of the examples are based on numbers. In text documents i have words instead of numbers.</p>

<p>So can you show me simple examples of how to use these algorithms for text documents classification.</p>

<p>I don't need code example but just logic</p>

<p>Pseudocode would help greatly</p>
","svm, knn, document-classification, text-classification","<p>The common approach is to use a bag of words model (<a href=""http://en.wikipedia.org/wiki/Bag_of_words_model"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Bag_of_words_model</a>) where the classifier would learn the presence of words in a text, it is simple but works surprisingly well.</p>

<p>Also, here there is a similar question: <a href=""https://stackoverflow.com/questions/13942744/prepare-data-for-text-classification-using-scikit-learn-svm"">Prepare data for text classification using Scikit Learn SVM</a> </p>
",9,4,10703,2013-05-22 14:12:38,https://stackoverflow.com/questions/16694088/how-can-i-classify-text-documents-with-using-svm-and-knn
Natural Language Processing - Converting Text Features Into Feature Vectors,"<p>So I've been working on a natural language processing project in which I need to classify different styles of writing. Assuming that semantic features from texts have already been extracted for me, I plan to use Weka in Java to train SVM classifiers using these features that can be used to classify other different texts. </p>

<p>The part I'm having trouble on is that to train an SVM, the features must be converted into a feature vector. I'm not sure how you would be able to represent features such as vocabulary richness, n-grams, punctuation, number of paragraphs, and paragraph length as numbers in a vector. If somebody could point in the right direction, that would be greatly appreciated.</p>
","java, nlp, svm, text-classification","<p>I'm not sure what values your attributes can take on, but perhaps this example will help you:</p>

<p>Suppose we are conducting a supervised learning experiment to try to determine if a period marks the end of a sentence or not, <code>EOS</code> and <code>NEOS</code> respectively.  The training data came from normal sentences in a paragraph style format, but were transformed to the following vector model:</p>

<ul>
<li>Column 1: Class: End-of-Sentence or Not-End-of-Sentence</li>
<li>Columns 2-8: The +/- 3 words surrounding the period in question</li>
<li>Columns 9,10: The number of words to the left/right, respectively, of the period before the next reliable sentence delimiter (e.g. ?, ! or a paragraph marker).</li>
<li>Column 11: The number of spaces following the period.</li>
</ul>

<p>Of course, this is not a very complicated problem to solve, but it's a nice little introduction to Weka.  We can't just use the words as features (really high dimensional space), but we can take their POS (part of speech) tags.  We can also extract the length of words, whether or not the word was capitalized, etc.  </p>

<p>So, you could feed anything as testing data, so long as you're able to transform it into the vector model above and extract the features used in the .arff.</p>

<p>The following (very small portion of) .arff file was used for determining whether a period in a sentence marked the end of or not:  </p>

<pre><code>@relation period

@attribute minus_three {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute minus_three_length real
@attribute minus_three_case {'UC','LC','NA'}
@attribute minus_two {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute minus_two_length real
@attribute minus_two_case {'UC','LC','NA'}
@attribute minus_one {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute minus_one_length real
@attribute minus_one_case {'UC','LC','NA'}
@attribute plus_one {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute plus_one_length real
@attribute plus_one_case {'UC','LC','NA'}
@attribute plus_two {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute plus_two_length real
@attribute plus_two_case {'UC','LC','NA'}
@attribute plus_three {'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNPS', 'NNS', 'NP', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP','WRB', 'NUM', 'PUNC', 'NEND', 'RAND'}
@attribute plus_three_length real
@attribute plus_three_case {'UC','LC','NA'}
@attribute left_before_reliable real
@attribute right_before_reliable real
@attribute spaces_follow_period real
@attribute class  {'EOS','NEOS'}

@data

VBP, 2, LC,NP, 4, UC,NN, 1, UC,NP, 6, UC,NEND, 1, NA,NN, 7, LC,31,47,1,NEOS
NNS, 10, LC,RBR, 4, LC,VBN, 5, LC,?, 3, NA,NP, 6, UC,NP, 6, UC,93,0,0,EOS
VBD, 4, LC,RB, 2, LC,RP, 4, LC,CC, 3, UC,UH, 5, LC,VBP, 2, LC,19,17,2,EOS
</code></pre>

<p>As you can see, each attribute can take on whatever you want it to:</p>

<ul>
<li><code>real</code> denotes a real number  </li>
<li>I made up <code>LC</code> and <code>UC</code> to denote upper case and lower case,  respectively</li>
<li>Most of the other values are <code>POS</code> tags</li>
</ul>

<p>You need to figure out exactly what your features are, and what values you'll use to represent/classify them. Then, you need to transform your data into the format defined by your .arff. </p>

<p>To touch on your punctuation question, let's suppose that we have sentences that all end in <code>.</code> or <code>?</code>. You can have an attribute called punc, which takes two values:</p>

<pre><code>@attribute punc {'p','q'}
</code></pre>

<p>I didn't use <code>?</code> because that is what is (conventionally) assigned when a data point is missing. Our you could have boolean attributes that indicates whether a character or what have you was present (with 0, 1 or false, true).  Another example,  but for quality:</p>

<pre><code>@attribute quality {'great','good', 'poor'}
</code></pre>

<p>How you determine said classification is up to you,  but the above should get you started.  Good luck. </p>
",5,6,1573,2013-05-29 20:54:40,https://stackoverflow.com/questions/16823609/natural-language-processing-converting-text-features-into-feature-vectors
"learning, validation, and testing classifier","<p>I'm working on Sentiment Analysis for text classification and I want to classify tweets from Twitter to 3 categories, positive, negative, or neutral. I have 210 training data, and I'm using <strong>Naive Bayes as classifier</strong>. I'm implementing using PHP and MySQL as my database for training data.
What I've done is in sequence :</p>

<ol>
<li>I split my training data based on <strong>10-fold Cross Validation</strong> to 189 training data and 21 testing data.</li>
<li>I insert my training data into database, so my classifier can classify based on training data</li>
<li>Then I classify my testing data using my classifier. I got 21 prediction results.</li>
<li>Repeat step 2 and 3 for 10 times based on <strong>10-fold Cross Validation</strong></li>
<li>I evaluate the accuracy of the classifier one by one, so I got 10 accuracy results. Then I take the average of the results.</li>
</ol>

<p>What i want to know is :</p>

<ol>
<li>Which is the learning process ? What is the input, process, and output ?</li>
<li>Which is the validation process ? What is the input, process, and output ?</li>
<li>Which is the testing process ? What is the input, process, and output ?</li>
</ol>

<p>I just want to make sure that my comprehension about these 3 process (learning, validation, and testing) is the right one.</p>
","machine-learning, text-classification","<p>In your example, I don't think there is a meaningful distinction between validation and testing.</p>

<ul>
<li><p><strong>Learning</strong> is when you train the model, which means that your outputs are, in general, parameters, such as coefficients in a regression model or weights for connections in a neural network. In your case, the outputs are estimated probabilities for the probability of seeing a word w in a tweet given the tweet positive P(w|+), seeing a word given negative P(w|-), and seeing a word given neutral P(w|*). Also the probabilities of not seeing words in the tweet given positive, negative, neutral, P(~w|+), etc. The inputs are the training data, and the process is simply estimating probabilities by measuring the frequencies that words occur (or don't occur) in each of your classes, i.e just counting!</p></li>
<li><p><strong>Testing</strong> is where you see how well your trained model does on data you haven't seen before. Training tends to produce outputs that <em>overfit</em> the training data, i.e. the coefficients or probabilities are ""tuned"" to noise in the training data, so you need to see how well your model does on data it hasn't been trained on. In your case, the inputs are the test examples, the process is applying Bayes theorem, and the outputs are classifications for the test examples (you classify based on which probability is highest).</p></li>
</ul>

<p>I have come across <strong>cross-validation</strong> -- in addition to testing -- in situations where you don't know what model to use (or where there are additional, ""extrinsic"", parameters to estimate that can't be done in the training phase). You split the data into 3 sets.</p>

<p>So, for example, in linear regression you might want to fit a straight line model, i.e. estimate <code>p</code> and <code>c</code> in <code>y = px + c</code>, or you might want to fit a quadratic model, i.e. estimate <code>p</code>, <code>c</code>, and <code>q</code> in <code>y = px + qx^2 + c</code>. What you do here is split your data into three. You train the straight line and quadratic models using part 1 of the data (the training examples). Then you see which model is better by using part 2 of the data (the cross-validation examples). Finally, once you've chosen your model, you use part 3 of the data (the test set) to determine how good your model is. Regression is a nice example because a quadratic model will always fit the training data better than the straight line model, so can't just look at the errors on the training data alone to decide what to do.</p>

<p>In the case of Naive Bayes, it <em>might</em> make sense to explore different prior probabilities, i.e. P(+), P(-), P(*), using a cross-validation set, and then use the test set to see how well you've done with the priors chosen using cross-validation and the conditional probabilities estimated using the training data.</p>

<hr>

<p>As an example of how to calculate the conditional probabilities, consider 4 tweets, which have been classified as ""+"" or ""-"" by a human</p>

<ul>
<li>T1, -, contains ""hate"", ""anger""</li>
<li>T2, +, contains ""don't"", ""hate""</li>
<li>T3, +, contains ""love"", ""friend""</li>
<li>T4, -, contains ""anger""</li>
</ul>

<p>So for P(hate|-) you add up the number of times hate appears in negative tweets. It appears   in T1 but not in T4, so P(hate|-) = 1/2. For P(~hate|-) you do the opposite, hate doesn't appear in 1 out of 2 of the negative tweets, so P(~hate|-) = 1/2.</p>

<p>Similar calculations give P(anger|-) = 1, and P(love|+) = 1/2.</p>

<p>A fly in the ointment is that any probability that is 0 will mess things up in the calculation phase, so you instead of using a zero probability you use a very low number, like 1/n or 1/n^2, where n is the number of training examples. So you might put P(~anger|-) = 1/4 or 1/16.</p>

<p>(The maths of the calculation I put in this <a href=""https://stackoverflow.com/questions/17030793/is-taking-logs-to-vectorize-repeated-multiplication-the-right-approach"">answer</a>).</p>
",1,0,710,2013-07-05 13:44:13,https://stackoverflow.com/questions/17490361/learning-validation-and-testing-classifier
Scalable or online out-of-core multi-label classifiers,"<p>I have been blowing my brains out over the past 2-3 weeks on this problem.
I have a multi-label (not multi-class) problem where each sample can belong to several of the labels.</p>

<p>I have around 4.5 million text documents as training data and around 1 million as test data. The labels are around 35K.</p>

<p>I am using <strong>scikit-learn</strong>. For feature extraction I was previously using TfidfVectorizer which didn't scale at all, now I am using HashVectorizer which is better but not that scalable given the number of documents that I have.</p>

<pre><code>vect = HashingVectorizer(strip_accents='ascii', analyzer='word', stop_words='english', n_features=(2 ** 10))
</code></pre>

<p>SKlearn provides a OneVsRestClassifier into which I can feed any estimator. For multi-label I found LinearSVC &amp; SGDClassifier only to be working correctly. Acc to my benchmarks SGD outperforms LinearSVC both in memory &amp; time. So, I have something like this</p>

<pre><code>clf = OneVsRestClassifier(SGDClassifier(loss='log', penalty='l2', n_jobs=-1), n_jobs=-1)
</code></pre>

<p>But this suffers from some serious issues:</p>

<ol>
<li>OneVsRest does not have a partial_fit method which makes it impossible for out-of-core learning. Are there any alternatives for that?</li>
<li>HashingVectorizer/Tfidf both work on a single core and don't have any n_jobs parameter. It's taking too much time to hash the documents. Any alternatives/suggestions? Also is the value of n_features correct?</li>
<li>I tested on 1 million documents. The Hashing takes 15 minutes and when it comes to clf.fit(X, y), I receive a MemoryError because OvR internally uses LabelBinarizer and it tries to allocate a matrix of dimensions (y x classes) which is fairly impossible to allocate. What should I do?</li>
<li>Any other libraries out there which have reliable &amp; scalable multi-label algorithms? I know of genism &amp; mahout but both of them don't have anything for multi-label situations?</li>
</ol>
","machine-learning, classification, scikit-learn, document-classification, text-classification","<p>I would do the multi-label part by hand. The  OneVsRestClassifier treats them as independent problems anyhow. You can just create the n_labels many classifiers and then call partial_fit on them. You can't use a pipeline if you only want to hash once (which I would advise), though.
Not sure about speeding up hashing vectorizer. You gotta ask @Larsmans and @ogrisel for that ;)</p>

<p>Having <code>partial_fit</code> on OneVsRestClassifier would be a nice addition, and I don't see a particular problem with it, actually. You could also try to implement that yourself and send a PR.</p>
",8,13,2618,2013-09-08 14:43:17,https://stackoverflow.com/questions/18684990/scalable-or-online-out-of-core-multi-label-classifiers
Convert Web page to ARFF File for Weka classification,"<p>I want to classify 10 webpages using weka.
How to convert web pages in to Weka's ARFF file format?
Do i need to convert all the 10 page in to one ARFF file or 
Do i need to convert  ARFF files for each web page i.e 10 ARFF files.</p>
","classification, weka, arff, text-classification","<p>Assuming that you want to keep your HTML formatting, this is relatively easy. Just put your HTML files in separate folders/directories (each directory a class), then apply the <code>TextDirectoryLoader</code> converter, as explained in the <a href=""http://weka.wikispaces.com/Text+categorization+with+WEKA"" rel=""nofollow"">Text categorization with WEKA</a> tutorial.</p>

<p>Assuming that e.g. you have two classes, what you should do (and get with this procedure) is a single ARFF file with one instance per file, and the text of each file into a single field (attribute value) for a text attribute, along with the class (directory name). Then you can follow up with the StringToWordVector filter to transform documents into term vectors and perform classification.</p>
",2,0,1977,2013-10-23 10:17:12,https://stackoverflow.com/questions/19538933/convert-web-page-to-arff-file-for-weka-classification
Which algorithms to use for one class classification?,"<p>I have over 15000 text docs of a specific topic. I would like to build a language model based on the former so that I can present to this model new random text documents of various topics and the algorithms tells if the new doc is of the same topic.</p>

<p>I tried out <code>sklearn.naive_bayes.MultinomialNB</code>, <code>sklearn.svm.classes.LinearSVC</code> and others, however I have the following problem:</p>

<p>These algorithms require training data with more than one label or category and I only have web pages of covering a specific topic. The other docs are not labeled and of many different topics.</p>

<p>I would appreciate any guidance on how to train a model with only one label or how to proceed in general. What I have so far is:</p>

<pre><code>c = MultinomialNB()
c.fit(X_train, y_train)
c.predict(X_test)
</code></pre>

<p>Thank you very much.</p>
","scikit-learn, text-classification","<p>What you're looking for is the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM"">OneClassSvm</a>. For more information you might want to check out the corresponding documentation <a href=""http://scikit-learn.org/stable/modules/outlier_detection.html"">at this link</a>.</p>
",12,10,14710,2013-10-23 20:40:24,https://stackoverflow.com/questions/19551858/which-algorithms-to-use-for-one-class-classification
N-grams vs other classifiers in text categorization,"<p>I'm new to text categorization techniques, I want to know the difference between the N-gram approach for text categorization and other classifier (decision tree, KNN, SVM) based text categorization.</p>

<p>i want to know which one is better, does n-grams comes under classifers ?. Does n-grams overcome any demerits in classifier techniques ?</p>

<p>where can i get comparative information regarding all this techniques.</p>

<p>thanks in advance.</p>
","machine-learning, data-mining, classification, n-gram, text-classification","<p>I'll actually post a full answer to this, since I think it's worth it being obvious that you can use n-gram models as classifiers (in much the same way as you can use any probability model of your features as one).</p>

<p>Generative classifiers approximate the posterior of interest, p(class | test doc) as:</p>

<p>p(c|t) \propto p(c) p(t|c)</p>

<p>where p(c) is the prior probability of c and p(t|c) is the likelihood. Classification picks the arg-max over all c. An n-gram language model, just like Naive Bayes or LDA or whatever generative model you like, can be construed as a probability model p(t|c) if you estimate a separate model for each class. As such, it can provide all the information required to do classification.</p>

<p>The question is whether the model is any use, of course. The major issue is that n-gram models tend to be built over billions of words of text, where classifiers are often trained on a few thousand. You can do complicated stuff like putting joint priors on the parameters of all the class' models, clamping hyperparameters to be equal (what these parameters are depends on how you do smoothing)... but it's still tricky.</p>

<p>An alternative is to build an n-gram model of characters (including spaces/punctuation if it turns out to be useful). This can be estimated much more reliably (26^3 parameters for tri-gram model instead of ~20000^3), and can be very useful for author identification/genre classification/other forms of classification that have stylistic elements.</p>
",11,6,6183,2013-12-01 18:54:14,https://stackoverflow.com/questions/20315897/n-grams-vs-other-classifiers-in-text-categorization
Text Classification - how to find the features that most affected the decision,"<p>When using <code>SVMlight</code> or <code>LIBSVM</code> in order to classify phrases as positive or negative (Sentiment Analysis), is there a way to determine which are the most influential words that affected the algorithms decision? For example, finding that the word <code>""good""</code> helped determine a phrase as positive, etc. </p>
","machine-learning, nlp, svm, sentiment-analysis, text-classification","<p>If you use the linear kernel then yes - simply compute the weights vector:</p>

<pre><code>w = SUM_i y_i alpha_i sv_i
</code></pre>

<p>Where:</p>

<ul>
<li><code>sv</code> - support vector</li>
<li><code>alpha</code> - coefficient found with SVMlight</li>
<li><code>y</code> - corresponding class (+1 or -1)</li>
</ul>

<p>(in some implementations <code>alpha</code>'s are already multiplied by <code>y_i</code> and so they are positive/negative)</p>

<p>Once you have <code>w</code>, which is of dimensions <code>1 x d</code> where <code>d</code> is your data dimension (number of words in the bag of words/tfidf representation) simply select the dimensions with high absolute value (no matter positive or negative) in order to find the most important features (words).</p>

<p>If you use some kernel (like RBF) then the answer is no, there is no direct method of taking out the most important features, as the classification process is performed in completely different way.</p>
",5,2,776,2013-12-29 22:47:34,https://stackoverflow.com/questions/20830964/text-classification-how-to-find-the-features-that-most-affected-the-decision
How can i apply feature reduction methods in Weka?,"<ol>
<li><p>How can i apply feature reduction methods like LSI etc in weka for text classification?</p>
</li>
<li><p>Can feature reduction methods like LSI etc improve the accuracy of classification?</p>
</li>
</ol>
","machine-learning, weka, text-classification, feature-selection","<ol>
<li><p>Take a look at <a href=""http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/FilteredClassifier.html"" rel=""nofollow"">FilteredClassifier</a> class or at <a href=""http://weka.wikispaces.com/Performing+attribute+selection"" rel=""nofollow"">AttributeSelectedClassifier</a>. With FilteredClassifier you can use such features reduction method as Principal Component Analysis (PCA). Here is a <a href=""http://www.youtube.com/watch?v=PY20auk5F_E"" rel=""nofollow"">video</a> how to filter your dataset using PCA, so that you could try different classifiers on reduced dataset.</p></li>
<li><p>It can help, but there is no guarantee about that. If you remove redundant features, or transform features in some way (like SVM or PCA do) classification task can become simpler. Anyway big number of features usually lead to <a href=""http://en.wikipedia.org/wiki/Curse_of_dimensionality"" rel=""nofollow"">curse of dimensionality</a> and attribute selection is a way to avoid it.</p></li>
</ol>
",2,-1,4496,2014-01-02 10:00:30,https://stackoverflow.com/questions/20880365/how-can-i-apply-feature-reduction-methods-in-weka
What is the impact of number of training documents on classification time?,"<p>Is there any impact of number of training documents on classification time ?? I know for K-nn that all of computations in K-nn is carried out  in classification while no or minimum work is done in training. Is same is the case with SVM, Naive Bayes, Decision Trees etc ?</p>
","performance, machine-learning, text-classification","<p>Only <strong>lazy</strong> classifiers have such a characteristics, one of which is KNN.</p>

<ul>
<li>SVM - classification time depends on the number of support vectors, which may, but not have to be - dependent on the number of training documents (they are the upper bound of the number of SVs)</li>
<li>Naive Bayes - there is no impact, unless these new documents carry many new words, as the NB classification time is O( number of features ), so if you do not enlarge the vocablurary (in case of BOW model) you are safe to use many training data</li>
<li>Decision Tree - the same as for NB, it depends only on the number of features (and the complexity of the problem, which do not change with number of instances)</li>
<li>Neural Network - here classification time only depends on the number of neurons</li>
</ul>
",2,1,60,2014-01-13 09:09:35,https://stackoverflow.com/questions/21087349/what-is-the-impact-of-number-of-training-documents-on-classification-time
How to rank features by their importance in a Weka classifier?,"<p>I use Weka to successfully build a classifier. I would now like to evaluate how effective or important my features are. Fot this I use AttributeSelection. But I don't know how to ouput the different features with their corresponding importance. I want simply list the features in decreasing order of their information gain scores! </p>
","machine-learning, nlp, weka, feature-selection, text-classification","<p>There are many ways of scoring the features, which are called <em>attributes</em>, in Weka. These methods are available as subclasses of <a href=""http://weka.sourceforge.net/doc.dev/weka/attributeSelection/ASEvaluation.html"" rel=""noreferrer"">weka.attributeSelection.ASEvaluation</a>.</p>

<p>Any of these evaluation classes will give you a score for each attribute. If you use information gain for scoring, for example, you will be using it the class <code>InfoGainAttributeEval</code>. The helpful methods are </p>

<ul>
<li><code>InfoGainAttributeEval.html#buildEvaluator()</code>, and</li>
<li><code>InfoGainAttributeEval.html#evaluateAttribute()</code></li>
</ul>

<p>The other types of feature scoring (gain ratio, correlation, etc.) have the same methods for scoring. Using any of these, you can rank all your features.</p>

<p>The ranking itself is independent of Weka. Of the many ways of doing it, this is one:</p>

<pre><code>Map&lt;Attribute, Double&gt; infogainscores = new HashMap&lt;Attribute, Double&gt;();
for (int i = 0; i &lt; instances.numAttributes(); i++) {
    Attribute t_attr = instaces.attribute(i);
    double infogain  = evaluation.evaluateAttribute(i);
    infogainscores.put(t_attr, infogain);
}
</code></pre>

<p>Now you have a map which needs to be sorted by value. Here's a generic code to do that:</p>

<pre><code> /**
  * Provides a {@code SortedSet} of {@code Map.Entry} objects. The sorting is in ascending order if {@param order} &gt; 0
  * and descending order if {@param order} &lt;= 0.
  * @param map   The map to be sorted.
  * @param order The sorting order (positive means ascending, non-positive means descending).
  * @param &lt;K&gt;   Keys.
  * @param &lt;V&gt;   Values need to be {@code Comparable}.
  * @return      A sorted set of {@code Map.Entry} objects.
  */
 static &lt;K,V extends Comparable&lt;? super V&gt;&gt; SortedSet&lt;Map.Entry&lt;K,V&gt;&gt;
 entriesSortedByValues(Map&lt;K,V&gt; map, final int order) {
     SortedSet&lt;Map.Entry&lt;K,V&gt;&gt; sortedEntries = new TreeSet&lt;&gt;(
         new Comparator&lt;Map.Entry&lt;K,V&gt;&gt;() {
             public int compare(Map.Entry&lt;K,V&gt; e1, Map.Entry&lt;K,V&gt; e2) {
                 return (order &gt; 0) ? compareToRetainDuplicates(e1.getValue(), e2.getValue()) : compareToRetainDuplicates(e2.getValue(), e1.getValue());
         }
     }
    );
    sortedEntries.addAll(map.entrySet());
    return sortedEntries;
}
</code></pre>

<p>and finally,</p>

<pre><code>private static &lt;V extends Comparable&lt;? super V&gt;&gt; int compareToRetainDuplicates(V v1, V v2) {
    return (v1.compareTo(v2) == -1) ? -1 : 1;
}
</code></pre>

<p>Now you have a list of entries sorted by values (in ascending or descending order, as you wish). Go crazy with it!</p>

<p>Please note that <strong>you should handle the case where more than one attribute has the same information gain</strong>. That is why I went through the process of sorting by values while retaining duplicates.</p>
",12,6,12647,2014-01-21 20:05:17,https://stackoverflow.com/questions/21267988/how-to-rank-features-by-their-importance-in-a-weka-classifier
Text Classification - using stemmer degrades results?,"<p>There's <a href=""http://www.cs.indiana.edu/~mkorayem/paper/survey_Arabic.pdf"" rel=""nofollow"">this</a> article about sentiment analysis of Arabic. </p>

<p>In the beginning of page 5 it says that:</p>

<blockquote>
  <p>""Experiments also show that stemming words before feature extraction and classification nearly always degrades the results"".</p>
</blockquote>

<p>Later on in the same page, they state that:</p>

<blockquote>
  <p>""...and an Arabic light stemmer is used for stemming the words""</p>
</blockquote>

<p>Um I thought that a stemmer/lemmatizer was <em>always</em> used before text classifications, why does he say that it degrades the results?</p>

<p>Thanks :)</p>
","nlp, sentiment-analysis, stemming, text-classification","<p>I do not know the arabic language, it may be specific in many aspects, my answer regards english.</p>

<blockquote>
  <p>Um I thought that a stemmer/lemmatizer was always used before text classifications, why does he say that it degrades the results?</p>
</blockquote>

<p>No it is not, in entirely depends on the <strong>task</strong>. If you want to extract some general concept of the text, then stemming/lematization is a good step. But in analysis of short chunks, where each word is valuable, stemming simply destroys its meaning. In particular - in sentiment analysis stemming may destroy the sentiment of the word. </p>
",5,3,822,2014-01-22 21:47:38,https://stackoverflow.com/questions/21294694/text-classification-using-stemmer-degrades-results
How to prune low frequency and high frequency words from a dataset?,"<p>Is there any tool available with which i can prune high frequency and low frequency terms from my dataset ?</p>
","machine-learning, text-classification, pruning","<p>A commonly used algorithm for this would be <a href=""http://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers"" rel=""nofollow"">Grubbs' test</a>. I don't really know an implementation in Java but if you would be willing to do the preprocessing in a different language, then there is the <a href=""http://cran.r-project.org/web/packages/outliers/outliers.pdf"" rel=""nofollow"">outliers package</a> in R containing amongst others the Grubbs' test. To eliminate multiple outliers you can just repeatedly apply Grubbs' test.</p>

<p>Edit:</p>

<p>I just saw that I missed the text classification tag. If you just want to keep too frequent terms from skewing your results, maybe <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">TF-IDF</a> could be interesting to you. This of course does not reduce dimensionality.</p>
",0,0,863,2014-02-01 15:12:58,https://stackoverflow.com/questions/21499768/how-to-prune-low-frequency-and-high-frequency-words-from-a-dataset
Why KNN has low accuracy but high precision?,"<p>I classified 20NG dataset with k-nn with 200 instance in each category with 80-20 train-test split where i found the following results </p>

<p><img src=""https://i.sstatic.net/uULWG.png"" alt=""enter image description here""></p>

<p>Here accuracy is quite low but how precision is high when accuracy is that low ? isn't precision formulae TP/(TP + FP) ? If yes than high accurate classifier needs to generate high true positive which will result in high precision but how K-nn is generating high precision with too less true positive rate ?</p>
","machine-learning, weka, nearest-neighbor, text-classification","<p>Recall is equivalent to the True Positive rate. Text classification tasks (especially Information Retrieval, but Text Categorization as well) <em>show a trade-off between recall and precision</em>. When precision is very high, recall tends to be low, and the opposite. This is due to the fact that you can tune the classifier to classify more or less instances as positive. The less instances you classify as positive, the higher the precision and the lower the recall.</p>

<p>To ensure that the effectiveness measure correlates with accuracy, you shoud focus on the F-measure, which averages recall and precision (F-measure = 2*r*p / (r+p)).</p>

<p>Non-lazy classifiers follow a training process in which they try to optimize accuracy or error. K-NN, being lazy, has not a training process, and in consequence, it does not try to optimize any effectiveness measure. You can play with different values of K, and intuitively, the bigger the K the higher the recall and the lower the precision, and the opposite.</p>
",5,2,5154,2014-02-04 08:30:50,https://stackoverflow.com/questions/21547423/why-knn-has-low-accuracy-but-high-precision
How can be estimate the total number of features?,"<p>If i have 1000 tokens(i assume tokens are features after preprocessing dataset), then how many bigram features would be generated from 1000 tokens(words) ? is it each token would have a bigram combination with all other tokens in vocabulary ? </p>

<p>i am asking this question as i have to pre-fill the number of words to keep in vocabulary in weka </p>
","machine-learning, weka, text-classification","<p>You cannot precompute this based just on the number of tokens. Bigrams are pairs of tokens which occur side-by-side (it is a term from n-gram models, where you have a notion of sequence). So in order to compute number of bigrams you have to slide a 2-token window through your data and check how many different pairs you find. </p>

<p>If you have N tokens coming from some data X, you can only say, that number of bigrams B is bounded as follows: <code>N &lt;= B &lt;= N^2</code>, but the exact number requires the procedure outlined above.</p>
",0,0,82,2014-02-04 15:59:35,https://stackoverflow.com/questions/21557249/how-can-be-estimate-the-total-number-of-features
How can i give output exampleset of &quot;Process Documents From Files&quot; to Multiple Classifiers in rapid miner?,"<p>I am working on parallel classifier combination and have a requirement in rapid miner to give output of ""Process Documents from Files"" operator to more than one classifier (L1, L2, L3). One way of doing it is to create three different processes and give each of them documents separately but that could be a performance bottleneck and i want to avoid it.</p>

<p>Is there any way to provide example set copies to classifiers (L1, L2, L3) ??</p>
","machine-learning, rapidminer, text-classification","<p>Use the <code>Multiply</code> operator to makes copies of example sets.</p>
",0,0,57,2014-02-16 15:43:42,https://stackoverflow.com/questions/21813396/how-can-i-give-output-exampleset-of-process-documents-from-files-to-multiple-c
Is TF-IDF necessary when using SVM?,"<p>I'm using Support Vector Machines to classify phrases. Before using the SVM, I understand I should do some kind of normalization on the phrase-vectors. One popular method is TF-IDF.</p>

<p>The terms with the highest TF-IDF score are often the terms that best characterize the topic of the document.</p>

<p>But isn't that exactly what SVM does anyway? Giving the highest weight to the terms that best characterize the document?</p>

<p>Thanks in advance :-)</p>
","nlp, normalization, svm, tf-idf, text-classification","<p>The weight of a term (as assigned by an SVM classifier) may or may not be directly proportional to the relevance of that term to a particular class. This depends on the kernel of the classifier as well as the regularization used. SVM does <em>NOT</em> assign weights to terms that best characterize a single document.</p>

<p>Term-frequency (tf) and inverse document frequency (idf) are used to encode the value of a term in a document vector. This is independent of the SVM classifier. </p>
",2,2,2752,2014-02-16 18:23:25,https://stackoverflow.com/questions/21815475/is-tf-idf-necessary-when-using-svm
How to apply InformationGain in rapidminer with seperate test set ?,"<p>I am dealing with text classification in rapidminer. I have seperate test and training splits. I applied Information Gain to a dataset using n-fold cross validation but i am confused on how to apply it on seperate test set ? Below is attached image <img src=""https://i.sstatic.net/RETm7.png"" alt=""enter image description here""></p>

<p>In figure i have connected the word list output from first ""Process Documents From Files"" which is used for training to second ""Processed Documents From Files"" which is used for testing but i want to apply the reduced feature to the second ""Process Documents From Files"" which perhaps should be the one returned from ""Select By Weight"" (reduced dimensions) operator but it returns weights which i cannot provide to second ""Process Documents From Files"". I searched alot but did'nt managed to find anything which can satisfy my need ?</p>

<p>Is it really possible for Rapidminer to have seperate test/train splits and apply feature selection ?</p>

<p>Is there any way to convert these weights into word list ? Please don't say write in repository (i can't do this) ?</p>

<p>In such scenario when i have different test/train splits and needs to apply feature selection, how would i make sure that test/train splits have same dimension vectors ?</p>

<p>I am really trapped out at it, kindly help ...</p>
","machine-learning, rapidminer, text-classification","<p>Immediately after the lower <code>Process Documents</code> operator insert a new <code>Select By Weight</code> operator before the <code>Apply Model</code>. Use a <code>Multiply</code> operator to copy the weights from the <code>Weight By Information Gain</code> operator and connect this to the input of the new <code>Select By Weight</code> operator.</p>
",1,0,650,2014-02-18 12:33:16,https://stackoverflow.com/questions/21853989/how-to-apply-informationgain-in-rapidminer-with-seperate-test-set
How can i convert from Rapid Miner exampleset into weka instances?,"<p>I need some functionality to be used from weka and some functionality to be used from rapidminer. How can i convert into rapid miner exampleset into weka instances perform some operations and then convert back into weka instances ?</p>

<p>I found a class on internet mentioned as ""WekaTools""  but it does not exist in rapidminer(5.3.013)</p>

<p>Can any one tell me where this functionality is moved in rapid miner 5.3.013? or how can i convert from Rapid miner example set into weka Instances ?</p>
","machine-learning, rapidminer, text-classification","<p>You have to download and install the Weka extension which is available from the RapidMiner Marketplace.</p>

<p>From the GUI, Help, Updates and extensions and then search for Weka.</p>
",0,0,104,2014-02-19 14:00:51,https://stackoverflow.com/questions/21882735/how-can-i-convert-from-rapid-miner-exampleset-into-weka-instances
Scikit learn - fit_transform on the test set,"<p>I am struggling to use Random Forest in Python with Scikit learn. My problem is that I use it for text classification (in 3 classes - positive/negative/neutral) and the features that I extract are mainly words/unigrams, so I need to convert these to numerical features. I found a way to do it with <code>DictVectorizer</code>'s <code>fit_transform</code>:</p>

<pre><code>from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.feature_extraction import DictVectorizer

vec = DictVectorizer(sparse=False)
rf = RandomForestClassifier(n_estimators = 100)
trainFeatures1 = vec.fit_transform(trainFeatures)

# Fit the training data to the training output and create the decision trees
rf = rf.fit(trainFeatures1.toarray(), LabelEncoder().fit_transform(trainLabels))

testFeatures1 = vec.fit_transform(testFeatures)
# Take the same decision trees and run on the test data
Output = rf.score(testFeatures1.toarray(), LabelEncoder().fit_transform(testLabels))

print ""accuracy: "" + str(Output)
</code></pre>

<p>My problem is that the <code>fit_transform</code> method is working on the train dataset, which contains around 8000 instances, but when I try to convert my test set to numerical features too, which is around 80000 instances, I get a memory error saying that:</p>

<pre><code>testFeatures1 = vec.fit_transform(testFeatures)
File ""C:\Python27\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 143, in fit_transform
return self.transform(X)
File ""C:\Python27\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 251, in transform
Xa = np.zeros((len(X), len(vocab)), dtype=dtype)
MemoryError
</code></pre>

<p>What could possibly cause this and is there any workaround? Many thanks!</p>
","machine-learning, classification, scikit-learn, random-forest, text-classification","<p>You are not supposed to do <code>fit_transform</code> on your test data, but only <code>transform</code>. Otherwise, you will get different vectorization than the one used during training.</p>

<p>For the memory issue, I recommend <code>TfIdfVectorizer</code>, which has numerous options of reducing the dimensionality (by removing rare unigrams etc.).</p>

<p><strong>UPDATE</strong></p>

<p>If the only problem is fitting <strong>test</strong> data, simply split it to small chunks. Instead of something like</p>

<pre><code>x=vect.transform(test)
eval(x)
</code></pre>

<p>you can do</p>

<pre><code>K=10
for i in range(K):
    size=len(test)/K
    x=vect.transform(test[ i*size : (i+1)*size ])
    eval(x)
</code></pre>

<p>and record results/stats and analyze them afterwards.</p>

<p>in particular</p>

<pre><code>predictions = []

K=10
for i in range(K):
    size=len(test)/K
    x=vect.transform(test[ i*size : (i+1)*size ])
    predictions += rf.predict(x) # assuming it retuns a list of labels, otherwise - convert it to list

print accuracy_score( predictions, true_labels )
</code></pre>
",16,14,10880,2014-02-24 20:13:46,https://stackoverflow.com/questions/21998008/scikit-learn-fit-transform-on-the-test-set
Using LibShortText with files in LibSVM format,"<p>I'm trying to use <code>LibShortText</code> but I don't entirely understand how it works.</p>

<p>From the <code>README</code>, it looks like it's functions are for text-files. However, I need to classify files that are <em>already</em> in LibSVM format, so I suppose functions like <code>text-train.py</code> and <code>text-predict.py</code> won't do...?</p>

<p>The <code>README</code> also states that:</p>

<pre><code>If a preprocessor directory is given instead, then it is assumed that the 
training data is already in LIBSVM format.
</code></pre>

<p>Anybody know what a <code>preprocessor directory</code> is...?</p>

<p>From the <a href=""http://www.csie.ntu.edu.tw/~cjlin/libshorttext/doc/classifier.html"" rel=""nofollow"">LibShortText documentations</a>, I see that there is a ""Middle-level Classification Modeul - learner"" that works on files like mine. However, I don't understand how it works! It doesn't have all the paramters that LIBSVM has, for example... And I haven't succeeded in finding how to save, or understand the results (where are the weights and predictions?).</p>

<p>If someone could explain how this thing words I'd <em>really</em> appreciate it (I've been testing it for quite a few hours now)... Thanks!</p>
","machine-learning, svm, text-classification, libshorttext","<p>Per the docs (<a href=""http://www.csie.ntu.edu.tw/~cjlin/libshorttext/doc/libshorttext.html#quick-start"" rel=""nofollow"">http://www.csie.ntu.edu.tw/~cjlin/libshorttext/doc/libshorttext.html#quick-start</a>) you can append .svm files using the -A option.</p>

<p>But if you you've already got your data in libsvm format you can use liblinear (the library underlying libshorttext) directly.</p>

<p>If you've got a distribution of libshorttext you already have liblinear.  You can compile and run by cd'ing (from wherever you've got libshorttext unpacked) like so:</p>

<p><code>
    $ cd libshorttext/classifier/learner/liblinear
    $ make
    $ ./train train_file.svm
    $ ./predict test_file.svm model_file output_file
</code></p>

<p>For reference here is the liblinear README: <a href=""https://github.com/ninjin/liblinear/blob/master/README"" rel=""nofollow"">https://github.com/ninjin/liblinear/blob/master/README</a> which I've found very handy.</p>
",1,1,780,2014-03-02 23:41:48,https://stackoverflow.com/questions/22135259/using-libshorttext-with-files-in-libsvm-format
Handling same words but from different documents,"<p>I'm making a python class which calculates the tfidf weight of each word in a document. Now in my dataset I have 50 documents. In these documents many words intersect, thus having multiple same word features but with different tfidf weight. So the question is how do I sum up all the weights into one singular weight?</p>
","python, machine-learning, text-classification, tf-idf","<p>First, let's get some terminology clear.  A term is a word-like unit in a corpus.  A token is a term at a particular location in a particular document.  There can be multiple tokens that use the same term.  For example, in my answer, there are many tokens that use the term ""the"".  But there is only one term for ""the"".</p>

<p>I think you are a little bit confused.  TF-IDF style weighting functions specify how to make a per term score out of the term's token frequency in a document and the background token document frequency in the corpus for each term in a document.  TF-IDF converts a document into a mapping of terms to weights.  So more tokens sharing the same term in a document will increase the corresponding weight for the term, but there will only be one weight per term.  There is no separate score for tokens sharing a term inside the doc.</p>
",2,0,794,2014-03-03 22:58:00,https://stackoverflow.com/questions/22159351/handling-same-words-but-from-different-documents
Theano Classification Task always gives 50% validation error and test error?,"<p>I am doing a text classification experiment with Theano's DBN (Deep Belief Network) and SDA (Stacked Denoising Autoencoder) examples.
I have produced a feature/label dataset just as Theano's MINST dataset is produced and changed the feature length and output values of those examples to adopt to my dataset (2 outputs instead of 10 outputs, and the number of features is adopted to my dataset).
Every time i run the experiments (both DBN and SDA) i get an exact 50% validation error and test error.
Do you have any ideas what i'm doing wrong? because i have just produced a dataset out of Movie Review Dataset as MINST dataset format and pickled it.</p>
<p>my code is the same code you can find in <a href=""http://www.deeplearning.net/tutorial/DBN.html"" rel=""nofollow noreferrer"">http://www.deeplearning.net/tutorial/DBN.html</a>
and my SDA code is the same code you can find in
<a href=""http://www.deeplearning.net/tutorial/SdA.html"" rel=""nofollow noreferrer"">http://www.deeplearning.net/tutorial/SdA.html</a></p>
<p>The only difference is that i have made my own dataset instead of MINST digit recognition dataset. My dataset is Bag of Words features from Movie Review Dataset which of course has different number of features and output classes so i just have made tiny modifications in function parameters number of inputs and output classes.
The code runs beautifully but the results are always 50%.
This is a sample output:</p>
<pre><code>Pre-training layer 2, epoch 77, cost  -11.8415031463
Pre-training layer 2, epoch 78, cost  -11.8225591118
Pre-training layer 2, epoch 79, cost  -11.8309999005
Pre-training layer 2, epoch 80, cost  -11.8362189546
Pre-training layer 2, epoch 81, cost  -11.8251214285
Pre-training layer 2, epoch 82, cost  -11.8333494168
Pre-training layer 2, epoch 83, cost  -11.8564580976
Pre-training layer 2, epoch 84, cost  -11.8243052414
Pre-training layer 2, epoch 85, cost  -11.8373403275
Pre-training layer 2, epoch 86, cost  -11.8341470443
Pre-training layer 2, epoch 87, cost  -11.8272021013
Pre-training layer 2, epoch 88, cost  -11.8403720434
Pre-training layer 2, epoch 89, cost  -11.8393612003
Pre-training layer 2, epoch 90, cost  -11.828745041
Pre-training layer 2, epoch 91, cost  -11.8300890796
Pre-training layer 2, epoch 92, cost  -11.8209189065
Pre-training layer 2, epoch 93, cost  -11.8263340225
Pre-training layer 2, epoch 94, cost  -11.8348454378
Pre-training layer 2, epoch 95, cost  -11.8288419285
Pre-training layer 2, epoch 96, cost  -11.8366522357
Pre-training layer 2, epoch 97, cost  -11.840142131
Pre-training layer 2, epoch 98, cost  -11.8334445128
Pre-training layer 2, epoch 99, cost  -11.8523094141
</code></pre>
<p>The pretraining code for file DBN_MovieReview.py ran for 430.33m</p>
<pre><code>... getting the finetuning functions
... finetuning the model
epoch 1, minibatch 140/140, validation error 50.000000 %
     epoch 1, minibatch 140/140, test error of best model 50.000000 %
epoch 2, minibatch 140/140, validation error 50.000000 %
epoch 3, minibatch 140/140, validation error 50.000000 %
epoch 4, minibatch 140/140, validation error 50.000000 %
Optimization complete with best validation score of 50.000000 %,with test performance 50.000000 %
</code></pre>
<p>The fine tuning code for file DBN_MovieReview.py ran for 5.48m</p>
<p>I ran both SDA and DBN with two different feature sets. So i got this exact 50% accuracy on all these 4 experiments.</p>
","sentiment-analysis, text-classification, theano, deep-learning","<p>I asked the same question in Theano's user groups and they answered that feature values should be between 0 and 1.</p>

<p>So i used a normalizer to normalize feature values and it solved the problem.</p>
",4,3,1240,2014-03-24 15:15:03,https://stackoverflow.com/questions/22613364/theano-classification-task-always-gives-50-validation-error-and-test-error
incremental training SGD Classifier of Sklearn with sentences,"<p>How to incrementally train SGDClassifier available in Sklearn linear models for sentences.
It is usually trains with docs.But i want to train it with sentences one by one.I want to incrementally train it for tweets for sentiment analysis.Any explanation with example in python will be great help.</p>

<p>Task: Sentiment Analysis of Tweets
Doubt: Incrementally  training on labelled Tweets</p>

<p>Thanks in advance.</p>
","python, machine-learning, scikit-learn, sentiment-analysis, text-classification","<p>Just make list of sentences 
for example </p>

<pre><code>sents=[""I am x"",""I am Y""]
</code></pre>

<p>then transform it using HashingVectorizer and then use partial_fit to train it incrementally.
This worked for me.</p>

<p>Thanks</p>
",1,1,875,2014-03-27 13:11:55,https://stackoverflow.com/questions/22688728/incremental-training-sgd-classifier-of-sklearn-with-sentences
"Can you recommend a package in R that can be used to count precision, recall and F1-score for multi class classification tasks","<p>Is there any package that you would recommend which can be used to calculate the precision, F1, recall for multi class classification task in R. I tried to use ROCR but it states that: </p>

<pre><code>ROCR currently supports only evaluation of binary classification tasks
</code></pre>
","r, text-classification, precision-recall","<p>I know that you were looking for a solution in R. That said, this is a link to a nice solution  library in <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html"" rel=""nofollow"">Python, using scikit-learn version 0.14</a>. Python is very similar to R in a lot of respects (if you haven't used it before), and this could be a good place to start.</p>

<p>Another place you might want to look, if you are focused on R, is the the <a href=""http://cran.r-project.org/web/packages/PerfMeas/PerfMeas.pdf"" rel=""nofollow"">PerfMeas</a> package. As I quote, this ""Package implements different performance measures for
classiﬁcation and ranking tasks. AUC, precision at a given recall, F-score for single and multiple classes are available.""</p>
",1,5,1480,2014-04-08 07:39:54,https://stackoverflow.com/questions/22930583/can-you-recommend-a-package-in-r-that-can-be-used-to-count-precision-recall-and
dimension reduction in spam filtering,"<p>I'm performing an experiment in which I need to compare classification performance of several classification algorithms for spam filtering, viz. Naive Bayes, SVM, J48, k-NN, RandomForests, etc. I'm using the WEKA data mining tool. While going through the literature I came to know about various dimension reduction methods which can be broadly classified into two types-</p>

<ol>
<li>Feature Reduction: Principal Component Analysis, Latent Semantic Analysis, etc.</li>
<li>Feature Selection: Chi-Square, InfoGain, GainRatio, etc.</li>
</ol>

<p>I have also read this tutorial of WEKA by Jose Maria in his blog: <a href=""http://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html"" rel=""nofollow"">http://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html</a></p>

<p>In this blog he writes, ""A typical text classification problem in which dimensionality reduction can be a big mistake is spam filtering"". So, now I'm confused whether dimensionality reduction is of any use in case of spam filtering or not?</p>

<p>Further, I have also read in the literature about Document Frequency and TF-IDF as being one of feature reduction techniques. But I'm not sure how does it work and come into play during classification.</p>

<p>I know how to use weka, chain filters and classifiers, etc. The problem I'm facing is since I don't have enough idea about feature selection/reduction (including TF-IDF) I am unable to decide how and what feature selection techniques and classification algorithms I should combine to make my study meaningful. I also have no idea about optimal threshold value that I should use with chi-square, info gain, etc.</p>

<p>In StringToWordVector class, I have an option of IDFTransform, so does it makes sence to set it to TRUE and also use a feature selection technique, say InfoGain?</p>

<p>Please guide me and if possible please provide links to resources where I can learn about dimension reduction in detail and can plan my experiment meaningfully!</p>
","data-mining, weka, text-mining, spam-prevention, text-classification","<p>Well, Naive Bayes seems to work best for spam filtering, and it doesn't play nicely with dimensionality reduction.</p>

<p>Many dimensionality reduction methods try to identify the features of the highest variance. This of course won't help a lot with spam detection, you want discriminative features.</p>

<p>Plus, there is not only one type of spam, but many. Which is likely why naive Bayes works better than many other methods that assume there is only one type of spam.</p>
",0,0,465,2014-04-09 10:38:38,https://stackoverflow.com/questions/22960024/dimension-reduction-in-spam-filtering
How to output resultant documents from Weka text-classification,"<p>So we are running a multinomial naive bayes classification algorithm on a set of 15k tweets. We first break up each tweet into a vector of word features based on Weka's StringToWordVector function. We then save the results to a new arff file to user as our training set. We repeat this process with another set of 5k tweets and re-evaluate the test set using the same model derived from our training set. </p>

<p>What we would like to do is to output each sentence that weka classified in the test set along with its classification... We can see the general information (Precision, recall, f-score) of the performance and accuracy of the algorithm but we cannot see the individual sentences that were classified by weka, based on our classifier... Is there anyway to do this?</p>

<p>Another problem is that ultimately our professor will give us 20k more tweets and expect us to classify this new document. We are not sure how to do this however as:</p>

<pre><code>All of the data we have been working with has been classified manually, both the training and test sets...
however the data we will be getting from the professor will be UNclassified... How can we 
reevaluate our model on the unclassified data if Weka requires that the attribute information must
be the same as the set used to form the model and the test set we are evaluating against?
</code></pre>

<p>Thanks for any help!</p>
","machine-learning, weka, sentiment-analysis, text-classification","<p>The easiest way to acomplish these tasks is using a <code>FilteredClassifier</code>. This kind of classifier integrates a <code>Filter</code> and a <code>Classifier</code>, so you can connect a <code>StringToWordVector</code> filter with the classifier you prefer (<code>J48</code>, <code>NaiveBayes</code>, whatever), and you will be always keeping the original training set (unprocessed text), and applying the classifier to new tweets (unprocessed) by using the vocabular derived by the <code>StringToWordVector</code> filter.</p>

<p>You can see how to do this in the command line in ""<a href=""http://jmgomezhidalgo.blogspot.com.es/2013/04/command-line-functions-for-text-mining.html"" rel=""nofollow"">Command Line Functions for Text Mining in WEKA</a>"" and via a program in ""<a href=""http://jmgomezhidalgo.blogspot.com.es/2013/04/a-simple-text-classifier-in-java-with.html"" rel=""nofollow"">A Simple Text Classifier in Java with WEKA</a>"".</p>
",1,0,485,2014-04-22 00:01:52,https://stackoverflow.com/questions/23208044/how-to-output-resultant-documents-from-weka-text-classification
Batch Filtering with Multi-Filter throws a &#39;Class attribute not set&#39; exception,"<p>We have a data set of 15k classified tweets with which we need to perform sentiment analysis. I would like to test against a test set of 5k classified tweets. Due to Weka needing the same attributes within the header of the test set as exist in the header of training set, I will have to use batch filtering if I want to be able to run my classifier against this 5k test set.</p>

<p>However, there are several filters that I need to run my training set through, so I figured the running a multifilter against the training set would be a good idea. The multifilter works fine when not running the batch argument, but when I try to batch filter I get an error from the CLI as it tried to execute the first filter within the multi-filter:</p>

<p><strong>CLI multiFilter command w/batch argument:</strong></p>

<pre><code>java weka.filters.MultiFilter -F ""weka.filters.supervised.instance.Resample -B 1.0 -S 1 -Z 15.0 -no-replacement"" \
-F ""weka.filters.unsupervised.attribute.StringToWordVector -R first-last -W 100000 -prune-rate -1.0 -N 0 -S -stemmer weka.core.stemmers.NullStemmer -M 2 -tokenizer weka.core.tokenizers.AlphabeticTokenizer"" \
-F ""weka.filters.unsupervised.attribute.Reorder -R 2-last,first""\
-F ""weka.filters.supervised.attribute.AttributeSelection -E \""weka.attributeSelection.InfoGainAttributeEval \"" -S \""weka.attributeSelection.Ranker -T 0.0 -N -1\"""" \
-F weka.filters.AllFilter \
-b -i input\Train.arff -o output\Train_b_out.arff -r input\Test.arff -s output\Test_b_out.arff
</code></pre>

<p>Here is <strong>the resultant error from the CLI:</strong></p>

<pre><code>weka.core.UnassignedClassException: weka.filters.supervised.instance.Resample: Class attribute not set!
at weka.core.Capabilities.test(Capabilities.java:1091)
at weka.core.Capabilities.test(Capabilities.java:1023)
at weka.core.Capabilities.testWithFail(Capabilities.java:1302)
at weka.filters.Filter.testInputFormat(Filter.java:434)
at weka.filters.Filter.setInputFormat(Filter.java:452)
at weka.filters.SimpleFilter.setInputFormat(SimpleFilter.java:195)
at weka.filters.Filter.batchFilterFile(Filter.java:1243)
at weka.filters.Filter.runFilter(Filter.java:1319)
at weka.filters.MultiFilter.main(MultiFilter.java:425)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at weka.gui.SimpleCLIPanel$ClassRunner.run(SimpleCLIPanel.java:265)
</code></pre>

<p>And here are the headers with a portion of data for both <strong>the training and test input arffs:</strong></p>

<p><strong>Training:</strong></p>

<pre><code>@RELATION classifiedTweets
@ATTRIBUTE @@sentence@@ string
@ATTRIBUTE @@class@@ {1,-1,0}
@DATA
""Conditioning be very important for curly dry hair"",0
""Combine with Sunday paper coupon and"",0
""Price may vary by store"",0
""Oil be not really moisturizers"",-1
</code></pre>

<p><strong>Testing:</strong></p>

<pre><code>@RELATION classifiedTweets
@ATTRIBUTE @@sentence@@ string
@ATTRIBUTE @@class@@ {1,-1,0}
@DATA
""5"",0
""I give the curl a good form and discipline"",1
""I have be cowashing every day"",0
""LOL"",0
""TITLETITLE Walgreens Weekly and Midweek Deal"",0
""And then they walk away"",0
</code></pre>

<p>Am I doing something wrong here? I know that supervised resampling requires the class attribute to be on the bottom of the attribute list within the header, and it is... within both the test and training input files.</p>

<p><strong>EDIT:</strong></p>

<p>Further testing reveals that this error does not occur with relationship to the batch filtering, it occurs whenever I run the supervised resample filter from the CLI... The data that I use works on every other filter I've tried within the CLI, so I don't understand why this filter is any different... resampling the data in the GUI works fine as well...</p>

<p><strong>Update:</strong></p>

<p>This also happens with the SMOTE filter instead of the resample filter</p>
","command-line, weka, text-classification","<p>Could not get the batch filter to work with any resampling filter. However, our workaround was to simply resample (and then randomize) the training data as step 1. From this reduced set we ran batch filters for everything else we wanted on the test set. This seemed to work fine.</p>
",0,1,1023,2014-04-27 20:44:18,https://stackoverflow.com/questions/23328769/batch-filtering-with-multi-filter-throws-a-class-attribute-not-set-exception
"How do I tell if a noun is a person, place, or a thing?","<p>I am trying to classify text and then map the nouns on to a person, place, or a thing.  Is there a way or dictionary to do that?</p>
","nlp, text-classification","<p>Since you are dealing with classification, it might be interesting for you to have a look at AlchemyAPI, <code>http://www.alchemyapi.com/products/features/</code>. You have a free api key where you can try things. </p>

<p>But this doesn't stops here, if you want to do it manually, as your can see in @tripleee answer, WordNet is mentioned, this is also something interesting, and right now you have API where you can use like Java and interact with WordNet.</p>

<p>More, you have ontologies and many are free, these ontologies are normally in OWL or RDF. You can query these ontologies and find the relevant information. For OWL ontologies, you have the OWLAPI which you can use to interact, for RDF ontologies, you can use Apache Jena and write SPARQL statements.</p>

<p>Also, you have dbpedia, which is very very interesting and I believe that this might solve your problem to a big extent. <a href=""http://en.wikipedia.org/wiki/DBpedia"" rel=""nofollow"">dbpedia</a> is Wikipedia in machine-readable format.</p>

<p>For example, you can write SPARQL (much like SQL Statements), for e.g. suppose I want to check the relationship between London and UK,</p>

<pre><code>SELECT ?property
WHERE {
:London ?property :United_Kingdom
} 
</code></pre>

<p>OR suppose that I want to get all countries and cities,</p>

<pre><code>SELECT DISTINCT ?city ?country
WHERE { ?city rdf:type dbpedia-owl:City ;
rdfs:label ?label ;
dbpedia-owl:country ?country
}
</code></pre>
",2,0,962,2014-04-29 03:45:53,https://stackoverflow.com/questions/23355082/how-do-i-tell-if-a-noun-is-a-person-place-or-a-thing
Lexicon dictionary for synonym words,"<p>There are few dictionaries available for natural language processing. Like positive, negative words dictionaries etc. </p>

<p>Is there any dictionary available which contains list of synonym for all dictionary words? </p>

<p>Like for <code>nice</code> </p>

<pre><code>synonyms: enjoyable, pleasant, pleasurable, agreeable, delightful, satisfying, gratifying, acceptable, to one's liking, entertaining, amusing, diverting, marvellous, good; 
</code></pre>
","dictionary, nlp, stanford-nlp, data-processing, text-classification","<p>Although WordNet is a good resource to start for finding synonym, one must note its limitations, here's an example with python API in <code>NLTK</code> library:</p>

<p>Firstly, words have multiple meanings (i.e. senses):</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; wn.synsets('nice')
[Synset('nice.n.01'), Synset('nice.a.01'), Synset('decent.s.01'), Synset('nice.s.03'), Synset('dainty.s.04'), Synset('courteous.s.01')]
</code></pre>

<p>And to access the correct sense of a word, you will need to know the correct sense of a word given a context. </p>

<pre><code>&gt;&gt;&gt; wn.synset('nice.a.01').definition()
u'pleasant or pleasing or agreeable in nature or appearance'
</code></pre>

<p>You can try Word Sense Disambiguation software but they are not perfect (see <a href=""https://stackoverflow.com/questions/4613773/anyone-know-of-some-good-word-sense-disambiguation-software/8808962#8808962"">Anyone know of some good Word Sense Disambiguation software?</a>). Even if you know the sense of the word, the entries of wordnet are limited. You cannot expect much:</p>

<pre><code>&gt;&gt;&gt; wn.synset('nice.a.01').lemma_names()
[u'nice']
&gt;&gt;&gt; wn.synset('nice.a.01').similar_tos()
[Synset('good.s.06'), Synset('pleasant.s.02')]
&gt;&gt;&gt; [i.lemma_names() for i in wn.synset('nice.a.01').similar_tos()]
[[u'good'], [u'pleasant']]
</code></pre>
",10,5,4519,2014-05-17 10:27:34,https://stackoverflow.com/questions/23710214/lexicon-dictionary-for-synonym-words
Hierarchical prediction using R,"<p>I'm pretty new in R, and I couldn't find any information about a package who can do the following: supposing that I have a set of data (for instance, different text documents), which can have several classes. </p>

<p>For example, a datum could be a Sport, a Sport with Ball, a Sport without Ball and a Car. I'd like to be able to predict to which category the data belongs, considering that I may not hit that the datum is a Sport with Ball, but I'd be happy if I correctly predict that it's a Sport.</p>

<p>Which package can provide this kind of stuff? Some examples would be useful, if possible.</p>

<p>Thanks in advance</p>
","r, machine-learning, classification, document-classification, text-classification","<p>I am not aware of any specific packages in R that can do hierarchical classification. So there are two options:</p>

<ul>
<li>Use the C API SVMstruct <a href=""http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html"" rel=""nofollow"">http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html</a>. Programming this in R from scratch will be quite some work.</li>
<li>Build your own hierarchical classifier system. In the top-down case you will have a    multi-class classifier for each level, e.g. rec vs sci and motorcycles vs sport etc. You will use the top classifier and use its prediction to choose next classifier. The data you feed in to train a classifier with a node is the union of all data in the subtree rooted at that node.</li>
</ul>

<p>For details read e.g. <a href=""http://jmlr.org/papers/v6/tsochantaridis05a.html"" rel=""nofollow"">http://jmlr.org/papers/v6/tsochantaridis05a.html</a></p>
",0,1,1151,2014-05-17 20:30:16,https://stackoverflow.com/questions/23715803/hierarchical-prediction-using-r
How to encode different size of feature vectors in SVM,"<p>I work on classifying some reviews (paragraphs) consists of multiple sentences. I classified them with bag-of-word features in Weka via libSVM. However, I had another idea which I don't know how to implement :</p>

<p>I thought creating syntactical and shallow-semantics based features per sentence in the reviews is worth to try. However, I couldn't find any way to encode those features sequentially, since a paragraph's sentence size varies. The reason that I wanted to keep those features in an order is that the order of sentence features may give a better clue for classification. For example, if I have two instances P1 (with 3 sentences) and P2 (2 sentences), I would have a space like that (assume each sentence has one binary feature as a or b):</p>

<p>P1 -> a b b /classX
P2 -> b a /classY</p>

<p>So, my question is that whether I can implement that classification of different feature sizes in feature space or not? If yes, is there any kind of classifier that I can use in Weka, scikit-learn or Mallet? I would appreciate any responses.</p>

<p>Thanks</p>
","machine-learning, scikit-learn, weka, text-classification","<p>Regardless of the implementation, an SVM with the standard kernels (linear, poly, RBF) requires fixed-length feature vectors. You can encode any information in those feature vectors by encoding as booleans; e.g. collect all syntactical/semantic features that occur in your corpus, then introduce booleans that represent that ""feature such and such occurred in this document"". If it's important to capture the fact that these features occur in multiple sentences, count them and use put the frequency in the feature vector (but be sure to normalize your frequencies by document length, as SVMs are not scale-invariant).</p>
",6,2,3118,2014-05-31 05:17:21,https://stackoverflow.com/questions/23966648/how-to-encode-different-size-of-feature-vectors-in-svm
Text classification scheme for a classification task with 120 classes,"<p>I have about 15k text instances which are hand categorized into one of the 120 categories. The text pieces are emails from customers. The frequency of classes varies from 1 to 2000. I would like to train a classifier on this data so that subsequent emails can be classified automatically.</p>

<p>I have tried Naive Bayes' and SVM both but report percentage of correctly classified instances as 51% and 57% respectively. I have used stemmer, removed stop words and changed text to lower case.</p>

<p>I am sure such a text classification task with large number of categories and uneven distribution, has to be approached differently but I could not find any reference for such a case...Any recommendations?</p>

<p>Thanks in advance! </p>
","weka, svm, text-classification","<p>I assume that classes are not overlapping (that is, exactly one class per message).</p>

<p>A useful approach in the case of imbalanced classes is <strong>using asymetric miss-classification costs</strong> in order to enforce the classifier to focus on the less represented class, as its cost is assigned much bigger figure than other classes.</p>

<p>This is relatively easy to do in WEKA (see e.g. <a href=""http://jmgomezhidalgo.blogspot.com.es/2008/03/class-imbalanced-distribution-and-weka.html"" rel=""nofollow"">Class imbalanced distribution and WEKA cost sensitive learning</a>) in the case of binary classifiers, but it is much harder to setup in the case of 120 classes. In consequence, one approach would be to turn this problem into 120 binary problems (one-against-the-rest) and setting up the appropriate cost matrixes for each problem.</p>

<p>A more viable alternative in my experience, and given the high number of classes, is <strong>to collapse the unfrequent classes into a bigger <code>other</code> class</strong>. This seems more useful for a practical setting; there is a ""other"" folder to check by a human expert while most of the time the classifier is correctly assigning the emails to the rest of --well populated-- classes.</p>

<p>As a final note, an accuracy of about 50-60% may be not to bad after all, depending on the distribution of classes. For instance, the majority classifier (the one that assigns every instance to the most populated class) in a 99%-1% would be 99% accurate; however it is absolutely useless, because it misses the interesting examples. In the real life this happens in email spam filtering, fraud spotting, and quite many other domains.</p>
",0,0,158,2014-06-04 06:18:57,https://stackoverflow.com/questions/24030354/text-classification-scheme-for-a-classification-task-with-120-classes
Naive Bayes: Imbalanced Test Dataset,"<p>I am using scikit-learn Multinomial Naive Bayes classifier for binary text classification (classifier tells me whether the document belongs to the category X or not). I use a balanced dataset to train my model and a balanced test set to test it and the results are very promising.</p>

<p>This classifer needs to run in real time and constantly analyze documents thrown at it randomly.</p>

<p>However, when I run my classifier in production, the number of false positives is very high and therefore I end up with a very low precision. The reason is simple: there are many more negative samples that the classifer encounters in the real-time scenario (around 90 % of the time) and this does not correspond to the ideal balanced dataset I used for testing and training.</p>

<p>Is there a way I can simulate this real-time case during training or are there any tricks that I can use (including pre-processing on the documents to see if they are suitable for the classifer)? </p>

<p>I was planning to train my classifier using an imbalanced dataset with the same proportions as I have in real-time case but I am afraid that might bias Naive Bayes towards the negative class and lose the recall I have on the positive class.</p>

<p>Any advice is appreciated. </p>
","python, machine-learning, classification, scikit-learn, text-classification","<p>You have encountered one of the problems with classification with a highly imbalanced class distribution. I have to disagree with those that state the problem is with the Naive Bayes method, and I'll provide an explanation which should hopefully illustrate what the problem is.</p>

<p>Imagine your false positive rate is 0.01, and your true positive rate is 0.9. This means your false negative rate is 0.1 and your true negative rate is 0.99.</p>

<p>Imagine an idealised test scenario where you have 100 test cases from each class. You'll get (in expectation) 1 false positive and 90 true positives. Great! Precision is 90 / (90+1) on your positive class!</p>

<p>Now imagine there are 1000 times more negative examples than positive. Same 100 positive examples at test, but now there are 1000000 negative examples. You now get the same 90 true positives, but (0.01 * 1000000) = 10000 false positives. Disaster! Your precision is now almost zero (90 / (90+10000)).</p>

<p>The point here is that the performance of the classifier <em>hasn't changed</em>; false positive and true positive rates remained constant, but the balance changed and your precision figures dived as a result.</p>

<p>What to do about it is harder. If your scores are separable but the threshold is wrong, you should look at the ROC curve for thresholds based on the posterior probability and look to see if there's somewhere where you get the kind of performance you want. If your scores are not separable, try a bunch of different classifiers and see if you can get one where they are (logistic regression is pretty much a drop-in replacement for Naive Bayes; you might want to experiment with some non-linear classifiers, however, like a neural net or non-linear SVM, as you can often end up with non-linear boundaries delineating the space of a very small class).</p>

<p>To simulate this effect from a balanced test set, you can simply multiply instance counts by an appropriate multiplier in the contingency table (for instance, if your negative class is 10x the size of the positive, make every negative instance in testing add 10 counts to the contingency table instead of 1).</p>

<p>I hope that's of some help at least understanding the problem you're facing.</p>
",11,17,11397,2014-06-23 13:25:26,https://stackoverflow.com/questions/24367141/naive-bayes-imbalanced-test-dataset
Wrong classification to multiple classes with different fraction of classes,"<p>I've encouraged with next problem:
I'm trying to classify a lot of text documents.</p>

<p>There are 20 classes: 1 normal, 19 - abnormal.
When I use Naïve bayes classification I have the following result: classification works well for 19 classes, but for ""normal"" class I got many misclassification errors: almost all cases in ""normal"" category were classified as other (non-normal) category.</p>

<p>There are my questions:</p>

<ul>
<li>How should I select training set for ""normal"" class?  (Now, I just fit
to classifier set of text with ""normal"" category, with 1/20
proportion).</li>
<li>Can classifier be specified this way: if probability of belonging to 
some class less then certain threshold then classifier must set up<br>
category for this sample (e.g. normal)?</li>
</ul>
","machine-learning, nlp, scikit-learn, text-classification","<p>I'm not sure to have the full picture but It seems like you have in fact only 2 classes ""normal"" and ""abnormal"" which are unbalanced in volume and thus prior.</p>

<p>To answer your first question, in that situation, I would try to over-sampling your normal class for training (passing same ""normal"" instances multiple times to  ""fake"" bigger volume) and see if it improves your performances.</p>

<p>I don't get your second question.</p>
",1,0,495,2014-06-24 09:12:02,https://stackoverflow.com/questions/24382672/wrong-classification-to-multiple-classes-with-different-fraction-of-classes
Tokenize and stopword don&#39;t work in Tweets DB using RapidMiner,"<p>I would like tokenize and apply stop word filter on Twitter comments contained in a database, but Process Document does nothing. What am I doing wrong?</p>

<p>My goal is to apply these filters but keep the comments in rows instead of a single word vector.</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;process version=""5.3.015""&gt;
  &lt;context&gt;
    &lt;input/&gt;
    &lt;output/&gt;
    &lt;macros/&gt;
  &lt;/context&gt;
  &lt;operator activated=""true"" class=""process"" compatibility=""5.3.015"" expanded=""true"" name=""Process""&gt;
    &lt;process expanded=""true""&gt;
      &lt;operator activated=""true"" class=""read_database"" compatibility=""5.3.015"" expanded=""true"" height=""60"" name=""Server Connection (2)"" width=""90"" x=""45"" y=""30""&gt;
        &lt;parameter key=""connection"" value=""sqlserver2014""/&gt;
        &lt;parameter key=""query"" value=""select top 60 tweetid,content from [Tweets General]""/&gt;
        &lt;enumeration key=""parameters""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:data_to_documents"" compatibility=""5.3.002"" expanded=""true"" height=""60"" name=""Data to Documents"" width=""90"" x=""246"" y=""30""&gt;
        &lt;parameter key=""select_attributes_and_weights"" value=""true""/&gt;
        &lt;list key=""specify_weights""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:process_documents"" compatibility=""5.3.002"" expanded=""true"" height=""94"" name=""Process Documents"" width=""90"" x=""447"" y=""30""&gt;
        &lt;process expanded=""true""&gt;
          &lt;operator activated=""true"" class=""text:tokenize"" compatibility=""5.3.002"" expanded=""true"" height=""60"" name=""Tokenize (3)"" width=""90"" x=""246"" y=""75""/&gt;
          &lt;connect from_port=""document"" to_op=""Tokenize (3)"" to_port=""document""/&gt;
          &lt;connect from_op=""Tokenize (3)"" from_port=""document"" to_port=""document 1""/&gt;
          &lt;portSpacing port=""source_document"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 1"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 2"" spacing=""0""/&gt;
        &lt;/process&gt;
      &lt;/operator&gt;
      &lt;connect from_op=""Server Connection (2)"" from_port=""output"" to_op=""Data to Documents"" to_port=""example set""/&gt;
      &lt;connect from_op=""Data to Documents"" from_port=""documents"" to_op=""Process Documents"" to_port=""documents 1""/&gt;
      &lt;connect from_op=""Process Documents"" from_port=""example set"" to_port=""result 1""/&gt;
      &lt;portSpacing port=""source_input 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 2"" spacing=""0""/&gt;
    &lt;/process&gt;
  &lt;/operator&gt;
&lt;/process&gt;
</code></pre>
","twitter, tokenize, stop-words, rapidminer, text-classification","<p>You need to convert any attributes of type nominal to be of type text before the <code>Data to Documents</code> operator. The operator <code>Nominal to Text</code> will do this. You also need to set the option <code>select attributes and weights</code> to false in <code>Data to Documents</code> because I think the setting you have will deselect everything.</p>
",0,0,276,2014-07-04 20:05:03,https://stackoverflow.com/questions/24580260/tokenize-and-stopword-dont-work-in-tweets-db-using-rapidminer
How to classify text properly in weka given preprocessing is needed,"<p>I need to classify some text using weka programmatically, but I am having trouble as the training data and the to-be-classified data need to be filtered (the same way) before being used with the classifier.</p>

<p>My approach to the problem is currently:
Create an arff with training data with a string attribute and a class.
Use StringToWordVector over the data set and save the filter for future use.
Use Attributeselection filter over the resulting data and save filter for future use.
Train the classifier with that data and save the classifier.
Create a ""Instances"" with the same attributes as the arff and populate it with the Instance I want to classify with the value of class attribute missing.
Load the StringToWordVector filter and use it to filter Instances.
Load AttributeSlection filter and use it to filter the result.
Load the classifier and classify the result.</p>

<p>It seems that StringToWordVector is working as I expected and using the same set of words with the new data as with the old. The problem is with AttributeSelection that tries, it seems, to run again not knowing that I just want it to use the attributes it already filtered before.</p>
","java, classification, weka, text-classification","<ol>
<li><p>Re-using same attribute selection setup:
Attribute selection is a filter, you should use batch filtering method to be able to re-use it and get compatible data (<a href=""http://weka.wikispaces.com/Use+Weka+in+your+Java+code#Batch%20filtering"" rel=""nofollow"">http://weka.wikispaces.com/Use+Weka+in+your+Java+code#Batch%20filtering</a>) => after declaring your filter &amp; setup, you should call setInputFormat (ie. myfilter.setInputFormat(train)), use it on training data (Filter.useFilter(train, myfilter)), serialize the data if you want to use it later on test data. The setInputFormat(Instances) method always has to be the last call before the filter is applied.</p></li>
<li><p><strong>Not re-running the attribute selection:</strong> use reduceDimensionality method of your AttributeSelection object (ie. myfilter.reduceDimensionality() would reduce the dimensionality to include only those attributes ""chosen by the last run of attribute selection""). I think it is your main problem now.</p></li>
<li><p>If you want to re-use multiple filters (ie. StringToWordVector, standardization, selection), you should test a multi-filter solution.</p>

<p>StringToWordVector swv = new StringToWordVector();
AttributeSelection as = new AttributeSelection();
Standardize st = new Standardize();
MultiFilter mf = new MultiFilter(); 
Filter[] filters = {swv, st, as};
mf.setFilters(filters);</p></li>
</ol>

<p>Xavier</p>
",1,2,1156,2014-07-12 18:50:21,https://stackoverflow.com/questions/24716221/how-to-classify-text-properly-in-weka-given-preprocessing-is-needed
What features are good for Sentence Classification apart from using vector representation like Bag-of-words?,"<p>I am trying to find whether a given sentence is a ""question request"", ""call for action"", etc. I am using supervised multilabel classification for that.</p>

<p>What will be a good set of features to use? I am currently using Bag-of-words with trigrams, modal verbs, question words, etc. but the result is not that good.</p>

<p>Input example: ""Can you get this today? I need following items.""  </p>
","python, nlp, text-classification, text-analysis","<p><a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">https://code.google.com/p/word2vec/</a> is probably a good feature. </p>

<p>Illinois Wilkifier can also be very helpful: <a href=""http://cogcomp.cs.illinois.edu/demo/wikify/?id=25"" rel=""nofollow"">http://cogcomp.cs.illinois.edu/demo/wikify/?id=25</a></p>

<p>Also take a look at features used for Dataless classification: <a href=""http://cogcomp.cs.illinois.edu/page/project_view/6"" rel=""nofollow"">http://cogcomp.cs.illinois.edu/page/project_view/6</a> </p>
",1,1,835,2014-07-16 19:18:04,https://stackoverflow.com/questions/24788916/what-features-are-good-for-sentence-classification-apart-from-using-vector-repre
"Scikit-learn: don&#39;t use some words as one word feature, but use in collocations","<p>I'm doing text classification with using Python and scikit-learn.</p>

<p>Now, I use TfidfVectorizer as vectorizer (for transform raw text to a feature vector) and MultinomialNB as a classifier.
I use parameter ngram_range = (1,2) (see <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a> ), e.g. I use one word and bigrams.</p>

<p>After classification and test my algorithm in test set and CV set, I'd like to improve accuracy. 
I see the most informative features ( due to question <a href=""https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"">How to get most informative features for scikit-learn classifiers?</a> ). And I see, that in the set most informative features I have words ( ngram=1), that don't have impact to classification, but in bigram (words collocations) they will have great impact.</p>

<p>So, I can't use stop_words, because Tfidfvectorizer will not consider this words in collocations and I can't use preprocessor at the same reason.
Question: How can I exclude some words in tfidfvectorizer, but save this words in different collocations?</p>
","python, scikit-learn, text-classification","<p>I think there are a few possible ways of doing it:</p>

<ol>
<li><p>Construct two <code>TfidfVectorizer</code> twice both with <code>ngram_range=(1,2)</code>. Extract the feature names after fitting the first vectorizer, filter out unwanted the unigram features, and feed this list of features as the <code>vocabulary</code> argument of the second vectorizer. Use the second vectorizer for transformation.</p></li>
<li><p>Supply the <code>analyzer</code> argument of <code>TfidfVectorizer</code> as a function which performs customized extraction of features from each raw document, e.g. avoid spitting out some useless unigram as feature (but this means you need to do the work of generating words combinations yourself).</p></li>
<li><p>Fit a <code>TfidfVectorizer</code> as usual, which might contain some unwanted unigrams. Use <code>get_feature_names()</code> to get the column indices corresponding to the features you want. When you do <code>transform()</code> using the vectorizer, do an extra step of slicing the columns of the resulting sparse matrix, based on the indices of interest.</p></li>
</ol>
",2,1,728,2014-07-22 12:44:02,https://stackoverflow.com/questions/24887775/scikit-learn-dont-use-some-words-as-one-word-feature-but-use-in-collocations
Machine-Learning - Concept / Recommendations,"<p>Hi I'm new at machine learning and therefore looking for a text classification solution. Could one recommend me a nice framework written in java? I thought about using WEKA, but also heard about MALLET. What's better, where are the main differences?</p>

<p>My target is to classify unlabeled text. Therefore I prepared about 18 topics and 100 text for each topic for learning.</p>

<p>What would you recommend to do? Would also appreciate a nice little example or hint of how to proceed.</p>
","machine-learning, classification, text-classification","<p>You have a very minimal text data set, you could use any library - it wouldn't really matter. More advanced options would require more data than you have to be meaningful, so its not an issue worth considering. The simple way text classifications problems are handled is to use a <a href=""http://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow"">Bag of Words</a> model and a linear classifier. Both Weka and MALLET support this. </p>

<p>Personally, I find Weka to be a pain and MALLET to be poorly documented / out of date when it is, so I use <a href=""https://code.google.com/p/java-statistical-analysis-tool/"" rel=""nofollow"">JSAT</a>. There is an example on doing spam classification <a href=""https://code.google.com/p/java-statistical-analysis-tool/wiki/LoadingTextDataSpam"" rel=""nofollow"">here</a>. </p>

<p>(bias warning, I'm the author of JSAT). </p>
",2,-2,630,2014-07-22 19:54:18,https://stackoverflow.com/questions/24896644/machine-learning-concept-recommendations
Document Clustering and Classification in Solr?,"<p>I'm building an index of documents in Solr. 
Documents are non-scientific. </p>

<p>I have a category linked to each document, they can be used for teaching. I would like to assign category for new document upon addition. Documents are added all the time without rebuilding all index.</p>

<p>Also documents can be about same thing, but from different sources, so I'd like to make document clustering. So when document is added - I can search whether I already have such topic in the last N days, if yes - then save cluster ID.</p>

<p>Index size is about 500k documents and rising, so speed is important. </p>

<p>So I want to calculate for each new document: Category ID (based on training with pre-defined documents), Cluster ID (matched only for N days, not the whole index).</p>

<p>Is that possible to make with SOLR? Or it is better to make separate solution (if yes then which one?)</p>
","solr, document-classification, text-classification","<p>solr 6.1 and lucene 6.1 has this capability now. It offers knn and naive bayes off the shelves. this is a great post about how to use it in solr: <a href=""http://alexbenedetti.blogspot.com/2015/07/solr-document-classification-part-1.html"" rel=""nofollow"">solr based text classification</a></p>
",1,1,1825,2014-08-20 07:41:44,https://stackoverflow.com/questions/25399291/document-clustering-and-classification-in-solr
How to split data (raw text) into test/train sets with scikit crossvalidation module?,"<p>I have a large corpus of opinions (2500) in raw text. I would like to use scikit-learn library to split them into test/train sets. What could be the best aproach to solve this task with scikit-learn?. Could anybody provide me an example of spliting raw text in test/train sets (probably i´ll use tf-idf representation).</p>
","machine-learning, scikit-learn, classification, cross-validation, text-classification","<p>Suppose your data is a list of strings, i.e. </p>

<pre><code>data = [""...."", ""..."", ]
</code></pre>

<p>Then you can split it into training (80%) and test (20%) sets using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer"">train_test_split</a> e.g. by doing:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.model_selection import train_test_split
train, test = train_test_split(data, test_size = 0.2)
</code></pre>

<p>Before you rush doing it, though, read <a href=""http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection"" rel=""nofollow noreferrer"">those docs</a> through. 2500 is not a ""large corpus"" and you probably want to do something like a k-fold cross-validation rather than a single holdout split.</p>
",22,10,21106,2014-09-11 17:44:05,https://stackoverflow.com/questions/25793887/how-to-split-data-raw-text-into-test-train-sets-with-scikit-crossvalidation-mo
Storing XValidation (Cross Validation) Folds in Rapidminer?,"<p>I have tried a lot via code to save the test/train split samples for each fold in 10-fold cross validation(stratified) but couldn't manage to do that...</p>

<p>Is there any way to save the test/train splits samples (not model) in rapid miner ? </p>
","machine-learning, classification, rapidminer, cross-validation, text-classification","<p>I was about to suggest this for your previous post.</p>

<p>The <code>Store</code> operator that lets you store an example set in the repository. To allow the names of the different folds to be distinguished, you would need to have a macro that changes each time the operator executes. You then use the macro name when configuring the name in the <code>Store</code> operator. To create a macro use the <code>Generate Macro</code> operator which you can also use to increment it within the cross validation operator.</p>
",0,0,656,2014-09-14 08:58:22,https://stackoverflow.com/questions/25831673/storing-xvalidation-cross-validation-folds-in-rapidminer
weka 3.7 explorer cannot classify text,"<p>I am trying to do text classification using weka 3.7 explorer. I converted 2 text files( separated into two dir class1 and class2) into arff using text loader. Before doing so, I standardized the case to lower. Now when I load the file into weka and apply filter stringtowordvector (such as stopwords,usewordcount, usestoplist, stemmer - snowballstemmer) I do not see any change in my list of variables . All the variables (words ) are given as 1 or 0 against each class. </p>

<p>Please help me.</p>

<p>Here is my filter command</p>

<p>weka.filters.unsupervised.attribute.StringToWordVector -R first-last -W 1000 -prune-rate -1.0 -C -N 0 -S -stemmer weka.core.stemmers.SnowballStemmer -M 1 -tokenizer ""weka.core.tokenizers.WordTokenizer -delimiters \"" \r\n\t.,;:\\'\\""()?!\""""</p>
","machine-learning, classification, weka, text-analysis, text-classification","<p>That happend to me when I wanted to read from .csv and use StringToWord vector.</p>

<p>My problem was, that the text attribute was of type nominal and not String. I used the class ""NominalToString"", used it to changed values to String, and then it worked. </p>
",0,0,157,2014-09-14 22:15:19,https://stackoverflow.com/questions/25838537/weka-3-7-explorer-cannot-classify-text
How to use pickled classifier with countVectorizer.fit_transform() for labeling data,"<p>I trained a classifier on a set of short documents and pickled it after getting the reasonable f1 and accuracy scores for a binary classification task. </p>

<p>While training, I reduced the number of features using a sciki-learn <code>countVectorizer</code> cv:</p>

<pre><code>    cv = CountVectorizer(min_df=1, ngram_range=(1, 3), max_features = 15000) 
</code></pre>

<p>and then used the <code>fit_transform()</code> and <code>transform()</code> methods to obtain the transformed train and test sets:</p>

<pre><code>    transformed_feat_train = numpy.zeros((0,0,))
    transformed_feat_test = numpy.zeros((0,0,))

    transformed_feat_train = cv.fit_transform(trainingTextFeat).toarray()
    transformed_feat_test = cv.transform(testingTextFeat).toarray()
</code></pre>

<p>This all worked fine for training and testing the classifier. However, I am not sure how to use <code>fit_transform()</code> and <code>transform()</code> with a pickled version of the trained classifier for predicting the label of unseen, unlabeled data.</p>

<p>I am extracting the features on the unlabeled data exactly the same way I was doing while training/testing the classifier:</p>

<pre><code>## load the pickled classifier for labeling
pickledClassifier = joblib.load(pickledClassifierFile)

## transform data
cv = CountVectorizer(min_df=1, ngram_range=(1, 3), max_features = 15000)
cv.fit_transform(NOT_SURE)

transformed_Feat_unlabeled = numpy.zeros((0,0,))
transformed_Feat_unlabeled = cv.transform(unlabeled_text_feat).toarray()

## predict label on unseen, unlabeled data
l_predLabel = pickledClassifier.predict(transformed_feat_unlabeled)
</code></pre>

<p>Error message:</p>

<pre><code>    Traceback (most recent call last):
      File ""../clf.py"", line 615, in &lt;module&gt;
        if __name__==""__main__"": main()
      File ""../clf.py"", line 579, in main
        cv.fit_transform(pickledClassifierFile)
      File ""../sklearn/feature_extraction/text.py"", line 780, in fit_transform
        vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary)
      File ""../sklearn/feature_extraction/text.py"", line 727, in _count_vocab
        raise ValueError(""empty vocabulary; perhaps the documents only""
    ValueError: empty vocabulary; perhaps the documents only contain stop words
</code></pre>
","python, scikit-learn, text-classification","<p>You should use the same vectorizer instance for transforming the training and test data. You can do that by creating a pipeline with the vectorizer + classifier, training the pipeline on the training set, pickling the whole pipeline. Later load the pickled pipeline and call predict on it.</p>

<p>See this related question: <a href=""https://stackoverflow.com/questions/25788151/bringing-a-classifier-to-production"">Bringing a classifier to production</a>.</p>
",4,5,5448,2014-09-23 21:02:23,https://stackoverflow.com/questions/26004670/how-to-use-pickled-classifier-with-countvectorizer-fit-transform-for-labeling
Unknown words in Naive Bayes classification,"<p>How do I test a text classification problem with unknown words? In training a model, we can use smoothing technique (Laplace add-1) to make sure any word will receive at least 1 count for each class.</p>

<p>Then, what about at testing stage? If a word doesn't occur in the training data, what's the best way to deal with it? Simply skip it, or also give an occurrence of 1 to it?</p>

<p>Thanks, for any suggestions or opinions. Specifically, I am using a Naive Bayes classifier. </p>
","machine-learning, smoothing, text-classification","<p>When you come to classify an instance, think about whats going on, if you do the add-1 smoothing for an unseen feature, then you'd simply multiply a very small probability (1 / vocabSize) (or add the log of a very small probability) to your accumulated scores. If you are skipping the unseen feature then nothing happens to the scores.</p>

<p>So, generally speaking an unseen feature in your test data shouldn't make a difference to your classification decision - you know nothing about it as you haven't seen it in training, so in the case of smoothing you'd be multiplying (or adding) the same small (log-)probability to all your scores per class or you'd simply ignore it for all of your class scores. </p>

<p>If you're not convinced, simply try both and see if it makes any difference.</p>
",3,2,3296,2014-10-09 21:10:49,https://stackoverflow.com/questions/26287971/unknown-words-in-naive-bayes-classification
How to lemmatize spanish words with Pattern?,"<p>I would like to lemmatize a bunch of opinions. As I know, nltk cannot lemmatize words in languages different from English. Researching a little, I found <a href=""http://www.clips.ua.ac.be/pattern"" rel=""nofollow"">pattern</a>, which can lemmatize words in several languages. How can I lemmatize some text with pattern?</p>

<p>This is my test corpus:</p>

<pre><code># -- coding: utf-8 --

from pattern.es import lemma #unresolved reference

opinions = [""Este es un post de juguetes de aprendizaje \
automático. En realidad, contiene no mucho \
material interesante."",
""Las bases de datos de imágenes proporcionan \
capacidades de almacenamiento."",
""La mayoría de las bases de datos de imágenes \
imágenes seguras de forma permanente."",
""Los datos de imagen de tienda bases de datos."",
""Imagina almacenar bases de datos de bases de \
datos de imágenes. Almacenar datos. Bases de datos \
de imágenes de datos de la tienda.""]

print lemma(opiniones)
</code></pre>

<p>output:</p>

<pre><code>  File ""/Users/user/PycharmProjects/Pruebas/Lemmatizacion.py"", line 18, in &lt;module&gt;
    print lemma(opiniones)
  File ""/usr/local/lib/python2.7/site-packages/pattern/text/__init__.py"", line 1591, in lemma
    if verb.lower() in self._inverse:
AttributeError: 'list' object has no attribute 'lower'
</code></pre>

<p>How can I lemmatize <code>opinions</code>?</p>
","python, machine-learning, nlp, nltk, text-classification","<p>You can lemmatize text using pattern's parsetree function with the lemmata parameter set to True.</p>

<pre><code>from pattern.es import parsetree
parsetree(""buena"", lemmata=True)
# Returns [Sentence('buena/JJ/B-ADJP/O/bueno')]
</code></pre>
",4,2,4718,2014-10-09 23:48:05,https://stackoverflow.com/questions/26289759/how-to-lemmatize-spanish-words-with-pattern
CountVectorizer: AttributeError: &#39;numpy.ndarray&#39; object has no attribute &#39;lower&#39;,"<p>I have a one-dimensional array with large strings in each of the elements. I am trying to use a <code>CountVectorizer</code> to convert text data into numerical vectors. However, I am getting an error saying:</p>

<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'lower'
</code></pre>

<p><code>mealarray</code> contains large strings in each of the elements. There are 5000 such samples. I am trying to vectorize this as given below:</p>

<pre><code>vectorizer = CountVectorizer(
    stop_words='english',
    ngram_range=(1, 1),  #ngram_range=(1, 1) is the default
    dtype='double',
)
data = vectorizer.fit_transform(mealarray)
</code></pre>

<p>The full stacktrace :</p>

<pre><code>File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py"", line 817, in fit_transform
    self.fixed_vocabulary_)
  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py"", line 748, in _count_vocab
    for feature in analyze(doc):
  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py"", line 234, in &lt;lambda&gt;
    tokenize(preprocess(self.decode(doc))), stop_words)
  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py"", line 200, in &lt;lambda&gt;
    return lambda x: strip_accents(x.lower())
AttributeError: 'numpy.ndarray' object has no attribute 'lower'
</code></pre>
","python, numpy, scikit-learn, text-classification","<p>Got the answer to my question.
Basically, CountVectorizer is taking lists (with string contents) as an argument rather than array. That solved my problem.</p>
",9,18,87639,2014-10-14 17:48:39,https://stackoverflow.com/questions/26367075/countvectorizer-attributeerror-numpy-ndarray-object-has-no-attribute-lower
groupingBy operation in Java-8,"<p>I'm trying to re-write famous example of Spark's text classification (<a href=""http://chimpler.wordpress.com/2014/06/11/classifiying-documents-using-naive-bayes-on-apache-spark-mllib/"" rel=""nofollow"">http://chimpler.wordpress.com/2014/06/11/classifiying-documents-using-naive-bayes-on-apache-spark-mllib/</a>) on Java 8.</p>

<p>I have a problem - in this code I'm making some data preparations for getting idfs of all words in all files:</p>

<pre><code>    termDocsRdd.collect().stream().flatMap(doc -&gt; doc.getTerms().stream()
                                .map(term -&gt; new ImmutableMap.Builder&lt;String, String&gt;()
                                .put(doc.getName(),term)
                                .build())).distinct()        
</code></pre>

<p>And I'm stuck on the groupBy operation. (I need to group this by term, so each term must be a key and the value must be a sequence of documents).
In Scala this operation looks very simple - <strong>.groupBy(_._2)</strong>.
But how can I do this in Java?</p>

<p>I tried to write something like:</p>

<pre><code>    .groupingBy(term -&gt; term, mapping((Document) d -&gt; d.getDocNameContainsTerm(term), toList()));
</code></pre>

<p>but it's incorrect...</p>

<p>Somebody knows how to write it in Java?</p>

<p>Thank You very much.</p>
","java-8, apache-spark, tf-idf, text-classification","<p>If I understand you correctly, you want to do something like this:</p>

<p>(<code>import static java.util.stream.Collectors.*;</code>)</p>

<pre><code>Map&lt;Term, Set&lt;Document&gt;&gt; collect = termDocsRdd.collect().stream().flatMap(
 doc -&gt; doc.getTerms().stream().map(term -&gt; new AbstractMap.SimpleEntry&lt;&gt;(doc, term)))
.collect(groupingBy(Map.Entry::getValue, mapping(Map.Entry::getKey, toSet())));
</code></pre>

<p>The use of <code>Map.Entry</code>/ <code>AbstractMap.SimpleEntry</code> is due to the absence of a standard <code>Pair&lt;K,V&gt;</code> class in Java-8. <code>Map.Entry</code> implementations can fulfill this role but at the cost of having unintuitive and verbose type and method names (regarding the task of serving as <code>Pair</code> implementation).</p>

<hr>

<p>If you are using the current Eclipse version (I tested with LunaSR1 20140925) with its limited type inference, you have to help the compiler a little bit:</p>

<pre><code>Map&lt;Term, Set&lt;Document&gt;&gt; collect = termDocsRdd.collect().stream().flatMap(
 doc -&gt; doc.getTerms().stream().&lt;Map.Entry&lt;Document,Term&gt;&gt;map(term -&gt; new AbstractMap.SimpleEntry&lt;&gt;(doc, term)))
.collect(groupingBy(Map.Entry::getValue, mapping(Map.Entry::getKey, toSet())));
</code></pre>
",2,1,875,2014-10-15 09:36:09,https://stackoverflow.com/questions/26379148/groupingby-operation-in-java-8
Feature hashing in R for Text classification,"<p>I'm trying to implement feature hashing in R to help me with a text classification problem, but i'm not sure if i'm doing it the way it should be. Part of my code is based on this post: <a href=""https://stackoverflow.com/questions/14365911/hashing-function-for-mapping-integers-to-a-given-range"">Hashing function for mapping integers to a given range?</a>. </p>

<p>My code:</p>

<pre><code>random.data = function(n = 200, wlen = 40, ncol = 10){

  random.word = function(n){
    paste0(sample(c(letters, 0:9), n, TRUE), collapse = '')
  } 
  matrix(replicate(n, random.word(wlen)), ncol = ncol)   
}

feature_hash = function(doc, N){

  doc = as.matrix(doc)
  library(digest)

  idx = matrix(strtoi(substr(sapply(doc, digest), 28, 32), 16L) %% (N + 1), ncol = ncol(doc))
  sapply(1:N, function(r)apply(idx, 1, function(v)sum(v == r)))  
}

set.seed(1)
doc = random.data(50, 16, 5)
feature_hash(doc, 3)

       [,1] [,2] [,3]
 [1,]    2    0    1
 [2,]    2    1    1
 [3,]    2    0    1
 [4,]    0    2    1
 [5,]    1    1    1
 [6,]    1    0    1
 [7,]    1    2    0
 [8,]    2    0    0
 [9,]    3    1    0
[10,]    2    1    0
</code></pre>

<p>So, i'm basically converting the strings to integers using the last 5 hex digits of the md5 hash returned by <code>digest</code>. Questions:</p>

<p>1 - Is there any package that can do this for me? I haven't found any.
2 - Is it a good idea do use <code>digest</code> as hash function? If not, what can i do?</p>

<p>PS: I should test if it works before posting, but my files are quite big and take a lot of processing time, so i think it's more clever to someone point me in the right direction, because i'm sure i'm doing it wrong!</p>

<p>Thanks for nay help on this!</p>
","r, hash, hashcode, feature-extraction, text-classification","<p>I don't know any existed CRAN package for this.</p>

<p>However, I wrote a package for myself to do feature hashing. The source code is here: <a href=""https://github.com/wush978/FeatureHashing"" rel=""nofollow"">https://github.com/wush978/FeatureHashing</a>, but the API is different.</p>

<p>In my case, I use it to convert a data.frame to <code>CSRMatrix</code>, a customized sparse matrix in the package. I also implemented a helper function to convert the <code>CSRMatrix</code> to <code>Matrix::dgCMatrix</code>. For text classification, I guess the sparse matrix will be more suitable.</p>

<p>If you want to try it, please check the test script here: <a href=""https://github.com/wush978/FeatureHashing/blob/master/tests/test-conver-to-dgCMatrix.R"" rel=""nofollow"">https://github.com/wush978/FeatureHashing/blob/master/tests/test-conver-to-dgCMatrix.R</a></p>

<p>Note that I only used it in Ubuntu, so I don't know if it works for windows or macs or not. Please feel free to ask me any question of the package on <a href=""https://github.com/wush978/FeatureHashing/issues"" rel=""nofollow"">https://github.com/wush978/FeatureHashing/issues</a>.</p>
",2,3,1003,2014-10-19 02:37:35,https://stackoverflow.com/questions/26446728/feature-hashing-in-r-for-text-classification
How to classify URLs? what are URLs features? How to select and Extract features from URL,"<p>I have just started to work on a Classification problem. Its a two class problem, My Trained model(Machine Learning) will have to decide/predict either to allow a URL or Block it.</p>

<p>My Question is very specific. </p>

<ol>
<li>How to Classify URLs? Should i use normal text analysis methods?</li>
<li>What are URLs Features?</li>
<li>How to Select and Extract Features from URL?</li>
</ol>
","url, machine-learning, classification, feature-extraction, text-classification","<p>I assume you do not have access to the content of the URL thus you can only extract features from the url string itself. Otherwise it makes more sense to use the content of the URL.</p>

<p>Here are some features I will try. See <a href=""https://www.comp.nus.edu.sg/~kanmy/papers/cp689-kan.pdf"" rel=""noreferrer"">this</a> paper for more ideas:</p>

<ol>
<li><p>All url components. For example, this page has the below url:</p>

<p><code>http://stackoverflow.com/questions/26456904/how-to-classify-urls-what-are-urls-features-how-to-select-and-extract-features</code></p></li>
</ol>

<p>All tokens that occurs in different parts of URLs should have variable value to the classification. In this case, the last part after tokenization contributes great features for this page. (e.g., <strong>classify, urls, select, extract, features</strong>)</p>

<pre><code> * stackoverflow
 * com
 * questions
 * 26456904
 * how to classify urls what are urls features how to select and extract features
</code></pre>

<ol start=""2"">
<li>The length of a url;</li>
<li>n-grams (2-grams as examples below)
<ul>
<li>stackoverflow-com</li>
<li>com-questions  </li>
<li>questions-26456904</li>
<li>26456904-how</li>
<li>how-to</li>
<li>....</li>
</ul></li>
</ol>
",8,7,7217,2014-10-20 00:22:19,https://stackoverflow.com/questions/26456904/how-to-classify-urls-what-are-urls-features-how-to-select-and-extract-features
"How to use vector representation of words (as obtained from Word2Vec,etc) as features for a classifier?","<p>I am familiar with using BOW features for text classification, wherein we first find the size of the vocabulary for the corpus which becomes the size of our feature vector. For each sentence/document, and for all its constituent words, we then put 0/1 depending on the absence/presence of that word in that sentence/document. </p>

<p>However, now that I am trying to use vector representation of each word, is creating a global vocabulary essential? </p>
","text, vector, nlp, text-classification, word2vec","<p>Suppose the size of the vectors is N (usually between 50 or 500). The naive way of generalizing the traditional of generalizing BOW is just replacing 0 bit (in BOW) with N zeros, and replacing 1 bit (in BOW) with the the real vector (say from Word2Vec). Then the size of the features would be N * |V| (Compared to |V| feature vectors in the BOW, where |V| is the size of the vocabs). This simple generalization should work fine for decent number of training instances.</p>

<p>To make the feature vectors smaller, people use various techniques like using recursive combination of vectors with various operations. (See Recursive/Recurrent Neural Network and similar tricks, for example: <a href=""http://web.engr.illinois.edu/~khashab2/files/2013_RNN.pdf"">http://web.engr.illinois.edu/~khashab2/files/2013_RNN.pdf</a> or <a href=""http://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf"">http://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf</a> ) </p>
",7,15,3620,2014-10-26 03:45:28,https://stackoverflow.com/questions/26569592/how-to-use-vector-representation-of-words-as-obtained-from-word2vec-etc-as-fea
SciKit-Learn Text Classification from ODBC,"<p>I'm trying to adapt <a href=""http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html"" rel=""nofollow"">this example</a> to some social media data I have in a SQL server database.</p>

<p>I've intentionally forced both the training and test sets to only have social media posts that contain the word 'bunches'. Therefore I would expect an extremely high f-score for this word when I run it through all the algorithms. Instead I'm getting f-scores of around 2-4%. I have a feeling that I'm not feeding the data to the algorithms correctly.</p>

<pre><code>from __future__ import print_function
import numpy as np
from time import time
from sklearn.feature_extraction.text import TfidfVectorizer
import pyodbc
import pprint

#local windows connection
train = []
db = pyodbc.connect('DRIVER={SQL Server Native Client 11.0};SERVER=SERVER_IP;DATABASE=DB_NAME;Trusted_Connection=Yes;')
cursor = db.cursor()
training_query = ""SELECT top 2 percent postTitle FROM dbo.All_CH_Posts where monitorID ='1168136050' and postTitle like '%bunches%' ORDER BY NEWID()""
trainquery = cursor.execute(training_query)
traindata = cursor.fetchall()
for row in traindata:
    train.extend(row)

test = []
test_query = ""SELECT top 1 percent postTitle FROM dbo.All_CH_Posts where monitorID ='1168136050' and postTitle like '%bunches%' ORDER BY NEWID()""
testquery = cursor.execute(test_query)
testdata = cursor.fetchall()
for row in testdata:
    test.extend(row)
print('traindata')
pp.pprint(traindata)
print('testdata')
pp.pprint(testdata)
print('data loaded')

# split a training set and a test set
y_train = train
y_test =test


print(""Extracting features from the training dataset using a sparse vectorizer"")
t0 = time()
vectorizer = TfidfVectorizer(decode_error='ignore',sublinear_tf=True,
                             stop_words='english', lowercase=True, min_df=20)
X_train = vectorizer.fit_transform(train)
duration = time() - t0

print(""Extracting features from the test dataset using the same vectorizer"")
t0 = time()
X_test = vectorizer.transform(test)
duration = time() - t0
print(""n_samples: %d, n_features: %d"" % X_test.shape)

feature_names = np.asarray(vectorizer.get_feature_names())
print(feature_names)
</code></pre>

<p>I intentionally set the min_df high to get a look at what words are in my sparse matrix:</p>

<pre><code>n_samples: 237, n_features: 26
['almonds' 'amp' 'best' 'bowl' 'box' 'bunches' 'cereal' 'cheerios' 'crunch'
 'day' 'don' 'eat' 'eating' 'good' 'gt' 'honey' 'http' 'just' 'like' 'lol'
 'love' 'miss' 'morning' 'oats' 'rt' 'want']
</code></pre>

<p>So what am I doing wrong? Or am I thinking about this problem the wrong way/have a misconception of text classification?</p>

<p><a href=""https://drive.google.com/file/d/0ByAFICf5wlFqX00tMWpTem9oalE/view?usp=sharing"" rel=""nofollow"">Here is my training set.</a></p>

<p><a href=""https://drive.google.com/file/d/0ByAFICf5wlFqOWp4YWFpc3RBN28/view?usp=sharing"" rel=""nofollow"">Here is my test set.</a></p>
","python-3.x, scikit-learn, pyodbc, text-classification","<p>Thanks to @AndreasMueller and @Guru. The problem was in my labels.</p>

<p>The solution is to create labels for each row.</p>

<pre><code>training_query = ""SELECT top 2 percent monitorID, postTitle FROM dbo.All_CH_Posts where monitorID in ('1168136050','469407080') and postTitle &lt;&gt;'' ORDER BY NEWID()""
trainquery = cursor.execute(training_query)
traindata = cursor.fetchall()
for row in traindata:
   train_data.append(row.postTitle)
   train_target.append(row.monitorID)

test_data = []
test_target = []
test_query = ""SELECT top 2 percent monitorID, postTitle FROM dbo.All_CH_Posts where monitorID in ('1168136050','469407080') and postTitle &lt;&gt;'' ORDER BY NEWID()""
testquery = cursor.execute(test_query)
testdata = cursor.fetchall()
for row in testdata:
   test_data.append(row.postTitle)
   test_target.append(row.monitorID)

print(""data loaded"")


#assigning labels
train_le = preprocessing.LabelEncoder()
y_train = train_le.fit_transform(train_target)

test_le = preprocessing.LabelEncoder()
y_test = test_le.fit_transform(test_target)
</code></pre>
",2,0,575,2014-11-04 21:51:34,https://stackoverflow.com/questions/26745718/scikit-learn-text-classification-from-odbc
Converting Multilabel dataset into Single Label?,"<p>i am working on single label text categrorization with a dataset of reuter-21578 however the dataset is multi-label by default. Many researchers removed multilabel instances from thi dataset and their number of instances in reuters categories is quite different than mine. How can i remove all the instance that belongs to more than one category in a dataset ? Can i use weka or Rapidminer for this purpose to identify multilabel instances in a dataset ?</p>

<p><strong>Example:</strong> </p>

<pre>

    Input Dataset = {x1, x2, x3, x4, x5, x6, x7, x8, x9, x10}
    Labels = {acq, earn, grain , corn}


    Classification Results = 

    x1, x2, x3 = acq
    x4, x5 = earn
    x6, x7, x8 = grain
    x9 = grain, corn
    x10 = grain, acq

    Output Dataset (what i want) = 
    output dataset = {x1, x2, x3, x4, x5, x6, x7, x8}
    output labels = {acq, earn, grain, corn}

    Classification Results = 

    x1, x2, x3 = acq
    x4, x5 = earn
    x6, x7, x8 = grain

    **OR**
    {This is what i assume i have achieved with PolynomiaByBinomial Operator }
    output dataset = {x1, x2, x3, x4, x5, x6, x7, x8, x9, x10}
    output labels = {acq, earn, grain, corn}
    Classification Results = 

    x1, x2, x3 = acq
    x4, x5 = earn
    x6, x7, x8, x9, x10 = grain
    x9 = grain
    x10 = grain

</pre>

<p>Thanks in advance</p>
","machine-learning, weka, data-mining, rapidminer, text-classification","<p>The simplest way is to break the dataset into binary problems. If for example you have the datasets</p>

<pre><code>text1: science
text2: sports, politics
</code></pre>

<p>Break the dataset into 3 datasets:</p>

<pre><code>dataset1 (science): text1:true, text2:false
dataset2 (sports): text2:false, text2:true
dataset3 (science): text1:false, text2:true
</code></pre>

<p>Create 3 binary classifiers, one for each class, use the corresponding datasets to train them, and combine the results.</p>
",0,0,1265,2014-11-26 08:55:20,https://stackoverflow.com/questions/27144844/converting-multilabel-dataset-into-single-label
"trouble with nltk python NaiveBayesClassifier, I keep getting same probabilities inputs correct?","<p>so I'm working on a project its for class ""homework"" if you will, but what it does is it takes in anime names and genres and if they are relevant or irrelevant I am trying to build a NaiveBayesClassifier with that and then I want to pass in genres and for it to tell me if it is relevant or irrelevant I currently have the following:</p>

<pre><code>import nltk
trainingdata =[({'drama': True, 'mystery': True, 'horror': True, 'psychological': True}, 'relevant'), ({'drama': True, 'fantasy': True, 'romance': True, 'adventure': True, 'science fiction': True}, 'unrelevant')]
classifier = nltk.classify.naivebayes.NaiveBayesClassifier.train(trainingdata)
classifier.classify({'Fantasy': True, 'Comedy': True, 'Supernatural': True})
prob_dist = classifier.prob_classify(anime)
print ""relevant "" + str(prob_dist.prob(""relevant""))
print ""unrelevant "" + str(prob_dist.prob(""unrelevant""))
</code></pre>

<p>I currently have :</p>

<pre><code>size of training array:110
the relevant length 57
the unrelevant length 53
</code></pre>

<p>Some results I receive :</p>

<pre><code>relevant Tantei Opera Milky Holmes TD
input data passed to classify: {'Mystery': True, 'Comedy': True, 'Super': True, 'Power': True}
relevant 0.518018018018
unrelevant 0.481981981982

relevant Juuou Mujin no Fafnir
input data passed to classify :{'Romance': True, 'Fantasy': True, 'School': True}
relevant 0.518018018018
unrelevant 0.481981981982
</code></pre>

<p>So it looks like it's not reading my data correctly as 57/110 = .518018
But Im not sure what I am doing wrong...</p>

<p>I looked at this 
<a href=""https://stackoverflow.com/questions/20827741/nltk-naivebayesclassifier-training-for-sentiment-analysis/20833372#20833372"">nltk NaiveBayesClassifier training for sentiment analysis</a></p>

<p>and i feel like I am doing it correctly.. The only thing I am not doing is specifying every specific key that isn't found in keys. Does that matter?</p>

<p>Thanks!</p>
","python, classification, nltk, text-classification","<p>Some background, the OP purpose is to build a classifier for this purpose: <a href=""https://github.com/alejandrovega44/CSCE-470-Anime-Recommender"" rel=""nofollow"">https://github.com/alejandrovega44/CSCE-470-Anime-Recommender</a></p>

<p>Firstly, there are several methodological issues, i terms of what you're calling things. </p>

<p>You <strong>training data</strong> should be the raw data you're using for your task, i.e. the json file at: <a href=""https://raw.githubusercontent.com/alejandrovega44/CSCE-470-Anime-Recommender/naive2/py/UserAnime2"" rel=""nofollow"">https://raw.githubusercontent.com/alejandrovega44/CSCE-470-Anime-Recommender/naive2/py/UserAnime2</a></p>

<p>And the data structure that you've in your question should be called a <strong>feature vector</strong>, i.e. :</p>

<pre><code>({'drama': True, 'mystery': True, 'horror': True, 'psychological': True}, 'relevant')
({'drama': True, 'fantasy': True, 'romance': True, 'adventure': True, 'science fiction': True}, 'unrelevant')
</code></pre>

<p>The features in the training set in your sample code:</p>

<pre><code>'drama'
'mystery'
'horror'
'psychological'
'fantasy',
'romance', 
'adventure',
'science fiction'
</code></pre>

<p>But the features in your test set in your sample code are:</p>

<pre><code>'Fantasy'
'Comedy'
'Supernatural'
'Mystery'
'Comedy'
'Super'
'Power'
'Romance'
'Fantasy'
'School'
</code></pre>

<p>Because strings are case sensitive, none of your feature in the test data occurs in your training data. Hence the default probability assigned would be 50%-50% for a binary class, i.e.:</p>

<pre><code>import nltk
feature_vectors =[
({'drama': True, 'mystery': True, 'horror': True, 'psychological': True}, 'relevant'), 
({'drama': True, 'fantasy': True, 'romance': True, 'adventure': True, 'science fiction': True}, 'unrelevant')]
classifier = nltk.classify.naivebayes.NaiveBayesClassifier.train(feature_vectors)
prob_dist = classifier.prob_classify({'Fantasy': True, 'Comedy': True, 'Supernatural': True})
print ""relevant "" + str(prob_dist.prob(""relevant""))
print ""unrelevant "" + str(prob_dist.prob(""unrelevant""))
</code></pre>

<p>[out]:</p>

<pre><code>relevant 0.5
unrelevant 0.5
</code></pre>

<p>Even if you give the same documents but with capitalized features, the classifier won't know, e.g.:</p>

<pre><code>import nltk
feature_vectors =[
({'drama': True, 'mystery': True, 'horror': True, 'psychological': True}, 'relevant'), 
({'drama': True, 'fantasy': True, 'romance': True, 'adventure': True, 'science fiction': True}, 'unrelevant')]
classifier = nltk.classify.naivebayes.NaiveBayesClassifier.train(feature_vectors)

doc1 = {'drama': True, 'mystery': True, 'horror': True, 'psychological': True}
prob_dist = classifier.prob_classify(doc1)
print ""relevant "" + str(prob_dist.prob(""relevant""))
print ""unrelevant "" + str(prob_dist.prob(""unrelevant""))
print '----'
caps_doc1 = {'Drama': True, 'Mystery': True, 'Horror': True, 'Psychological':True }
prob_dist = classifier.prob_classify(caps_doc1)
print ""relevant "" + str(prob_dist.prob(""relevant""))
print ""unrelevant "" + str(prob_dist.prob(""unrelevant""))
print '----'
</code></pre>

<p>[out]:</p>

<pre><code>relevant 0.964285714286
unrelevant 0.0357142857143
----
relevant 0.5
unrelevant 0.5
----
</code></pre>

<p>Without giving more details and a better sample code to debug, this is all we can help on the question. =(</p>
",2,0,947,2014-12-06 18:20:12,https://stackoverflow.com/questions/27334874/trouble-with-nltk-python-naivebayesclassifier-i-keep-getting-same-probabilities
Implementation of text classification in MATLAB with naive bayes,"<p>I want to implement text classification with Naive Bayes algorithm in MATLAB. 
I have for now 3 matrices:</p>

<ol>
<li>Class priors (8*2 cell - 8 class names, for each class its % from the training) </li>
<li>Training Data: word count matrices - (15000*9 cell- for each class, counting of every feature (word) . the last column is each word count for all the documents. </li>
<li>Test Data: a matrices with (2000*1) cell - and for each cell a list of words which represent the document. </li>
</ol>

<p>What should I do now? I want to calculate recall and precision for the test set. I took a look in the matlab naive bayes functions, and it suppose to be simple , but I'm not sure how and where to start. </p>

<p>Thanks </p>
","matlab, classification, text-classification","<p>Here is an example of Naive Bayes classification,</p>

<pre><code>x1 = 5 * rand(100,1);
y1 = 5 * rand(100,1);
data1 = [x1,y1];
x2 = -5 * rand(100,1);
y2 =  5 * rand(100,1);
data2 = [x2,y2];
x3 = -5 * rand(100,1);
y3 = -5 * rand(100,1);
data3 = [x3,y3];
traindata = [data1(1:50,:);data2(1:50,:);data3(1:50,:)];
testdata = [data1(51:100,:);data2(51:100,:);data3(51:100,:)];
label = [repmat('x+y+',50,1);repmat('x-y+',50,1);repmat('x-y-',50,1)];
</code></pre>

<p>That was my data, three classes. Now the classification,</p>

<pre><code>nb = NaiveBayes.fit(traindata, label);
ClassifierOut = predict(nb,testdata);
</code></pre>

<p>I think you should change your data to matrix instead of cell, but the labels are okey.</p>

<p>Here are the results, <code>blue</code> is the training data and the rest is the classifier output for three classes.</p>

<p><img src=""https://i.sstatic.net/CNxfm.png"" alt=""enter image description here""></p>

<p>You can also see <a href=""https://stats.stackexchange.com/questions/51296/how-to-calculate-precision-and-recall-for-multiclass-classification-using-confus"">here</a> for calculation of recall and precision for multi-class data. </p>
",3,2,3583,2014-12-19 09:08:14,https://stackoverflow.com/questions/27562711/implementation-of-text-classification-in-matlab-with-naive-bayes
how can I complete the text classification task using less memory,"<p>(1)My goal: 
I am trying to use SVM to classify 10000 documents(each with 400 words) into 10 classes(evenly distributed). The features explored in my work include word n-gram (n=1~4),character n-gram(n=1~6). </p>

<p>(2)My approach:  I am representing each document using vectors of frequency values for each feature in the document. And using TF-IDF to formalize the vectors. parts of my code are below:</p>

<pre><code>def commonVec(dicts,count1,count2):
    ''' put features with frequency between count1 and count2 into a common vector used for SVM training''' 
    global_vector = []
    master = {}
    for i, d in enumerate(dicts):
        for k in d:
            master.setdefault(k, []).append(i)
    for key in master:
        if (len(master[key])&gt;=count1 and len(master[key])&lt;=count2):  
            global_vector.append(key)
    global_vector1 = sorted(global_vector)
    return global_vector1 
def featureComb(mix,count1,count2,res1):
    '''combine word n-gram and character n-gram into a vector'''
    if mix[0]:
        common_vector1 = []
        for i in mix[0]:
            dicts1 = []
            for res in res1: #I stored all documents into database. res1 is the document result set and res is each document. 
                dicts1.append(ngram.characterNgrams(res[1], i)) # characterNgrams()will return a dictionary with feature name as the key, frequency as the value.
            common_vector1.extend(commonVec(dicts1, count1, count2))
    else:
        common_vector1 = []
    if mix[1]:
        common_vector2 = []
        for j in mix[1]:
            dicts2 = []
            for res in res1:
                dicts2.append(ngram.wordNgrams(res[1], j))        
            common_vector2.extend(commonVec(dicts2, count1, count2))
    else:
        common_vector2 = []
    return common_vector1+common_vector2

def svmCombineVector(mix,global_combine,label,X,y,res1):
    '''Construct X vector that can be used to train SVM'''
    lstm = []
    for res in res1:            
        y.append(label[res[0]]) # insert class label into y

        dici1 = {}
        dici2 = {}
        freq_term_vector = []
        for i in mix[0]:             
            dici1.update(ngram.characterNgrams(res[1], i))
        freq_term_vector.extend(dici1[gram] if gram in dici1 else 0 for gram in global_combine)    
        for j in mix[1]:
            dici2.update(ngram.wordNgrams(res[1], j))
        freq_term_vector.extend(dici2[gram] if gram in dici2 else 0 for gram in global_combine)
        lstm.append(freq_term_vector)
    freq_term_matrix = np.matrix(lstm)
    transformer = TfidfTransformer(norm=""l2"")
    tfidf = transformer.fit_transform(freq_term_matrix)
    X.extend(tfidf.toarray())

X = []
y = []
character = [1,2,3,4,5,6]
word = [1,2,3,4]
mix = [character,word]
global_vector_combine = featureComb(mix, 2, 5000, res1)
print len(global_vector_combine) # 542401
svmCombineVector(mix,global_vector_combine,label,X,y,res1)
clf1 = svm.LinearSVC()
clf1.fit(X, y)
</code></pre>

<p>(3)My problem:  However, when I ran the source code, a memory error occurred.</p>

<pre><code>Traceback (most recent call last):
      File ""svm.py"", line 110, in &lt;module&gt;
        functions.svmCombineVector(mix,global_vector_combine,label,X,y,res1)
      File ""/home/work/functions.py"", line 201, in svmCombineVector
        X.extend(tfidf.toarray())
      File ""/home/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.py"", line 901, in toarray
        return self.tocoo(copy=False).toarray(order=order, out=out)
      File ""/home/anaconda/lib/python2.7/site-packages/scipy/sparse/coo.py"", line 269, in toarray
        B = self._process_toarray_args(order, out)
      File ""/home/anaconda/lib/python2.7/site-packages/scipy/sparse/base.py"", line 789, in _process_toarray
    _args
        return np.zeros(self.shape, dtype=self.dtype, order=order)
    MemoryError
</code></pre>

<p>I really have a hard time with it and need help from <strong>stackoverflow</strong>.</p>

<ol>
<li>Could anyone explain some details and give me some idea how to solve it?</li>
<li>could anyone check my source code and show me some other methods to make use of memory more effectively?</li>
</ol>
","python, memory, numpy, scikit-learn, text-classification","<p>The main problem you're facing is that you're using <em>far</em> too many features. It's actually quite extraordinary that you've managed to generate 542401 features from documents that contain just 400 words! I've seen SVM classifiers separate spam from non-spam with high accuracy using just 150 features -- word counts of selected words that say a lot about whether the document is spam. These use stemming and other normalization tricks to make the features more effective. </p>

<p>You need to spend some time thinning out your features. Think about <em>which</em> features are most likely to contain information useful for this task. Experiment with different features. As long as you keep throwing everything but the kitchen sink in, you'll get memory errors. Right now you're trying to pass 10000 data points with 542401 dimensions each to your SVM. That's 542401 * 10000 * 4 = <em>21 gigabytes</em> (conservatively) of data. My computer only has 4 gigabytes of RAM. You've got to pare this way down.<sup>1</sup> </p>

<p>A first step towards doing so would be to think about how big your total vocabulary size is. Each document has only 400 words, but let's say those 400 words are taken from a vocabulary of 5000 words. That means there will be 5000 ** 4 = 6.25 * 10 ** 14 possible 4-grams. That's <em>half a quadrillion possible 4-grams</em>. Of course not all those 4-grams will appear in your documents, but this goes a long way towards explaining why you're running out of memory. Do you really need these 4-grams? Could you get away with 2-grams only? There are a measly 5000 ** 2 = 25 million possible 2-grams. That will fit much more easily in memory, even if all possible 2-grams appear (unlikely).</p>

<p>Also keep in mind that even if the SVM could handle quadrillions of datapoints, it would probably give bad results, because when you give any learning algorithm too many features, it will tend to overfit, picking up on irrelevant patterns and overgeneralizing from them. There are ways of dealing with this, but it's best <em>not</em> to deal with it at all if you can help it.</p>

<p>I will also mention that these are not ""newbie"" problems. These are problems that machine learning specialists with PhDs have to deal with. They come up with lots of clever solutions, but we're not so clever that way, so we have to be clever a different way. </p>

<p>Although I can't offer you specific suggestions for cleverness without knowing more, I would say that, first, stemming is a good idea in at least some cases. Stemming simply removes grammatical inflection, so that different forms of the same word (""swim"" and ""swimming"") are treated as identical. This will probably reduce your vocabulary size significantly, at least if you're dealing with English text. A common choice is the <a href=""http://tartarus.org/martin/PorterStemmer/"" rel=""nofollow noreferrer"">porter stemmer</a>, which is included in <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer""><code>nltk</code></a>, as well as in a number of <a href=""https://pypi.python.org/pypi/snowballstemmer"" rel=""nofollow noreferrer"">other</a> <a href=""https://stackoverflow.com/q/10369393/577088"">packages</a>. Also, if you aren't already, you should probably strip punctuation and reduce all words to lower-case. From there, it <em>really</em> depends. Stylometry (identifying authors) sometimes requires only particles (""a"", ""an"", ""the""), conjunctions (""and"", ""but"") and other very common words; spam, on the other hand, has its own oddball vocabularies of interest. At this level, it is very difficult to say in advance what will work; you'll almost certainly need to try different approaches to see which is most effective. As always, testing is crucial!</p>

<p><sub>1. Well, possibly you have a huge amount of RAM at your disposal. For example, I have access to a machine with 48G of RAM at my current workplace. But I doubt it could handle this either, because the SVM will have its own internal representation of the data, which means there will be at least one copy at some point; if a second copy is needed at any point -- kaboom.</sub></p>
",2,3,1458,2014-12-29 13:35:59,https://stackoverflow.com/questions/27689953/how-can-i-complete-the-text-classification-task-using-less-memory
text classifier with weka: how to correctly train a classifier issue,"<p>I'm trying to build a text classifier using Weka, but the probabilities with <code>distributionForInstance</code> of the classes are <code>1.0</code> in one and <code>0.0</code> in all other cases, so <code>classifyInstance</code> always returns the same class as prediction. Something in the training doesn't work correctly.</p>

<h1>ARFF training</h1>

<pre><code>@relation test1

@attribute tweetmsg    String
@attribute classValues {politica,sport,musicatvcinema,infogeneriche,fattidelgiorno,statopersonale,checkin,conversazione}

@DATA

""Renzi Berlusconi Salvini Bersani"",politica
""Allegri insulta la terna arbitrale"",sport
""Bravo Garcia"",sport
</code></pre>

<h1>Training methods</h1>

<pre><code>public void trainClassifier(final String INPUT_FILENAME) throws Exception
{
    getTrainingDataset(INPUT_FILENAME);

    //trainingInstances consists of feature vector of every input

    for(Instance currentInstance : inputDataset)
    {           
        Instance currentFeatureVector = extractFeature(currentInstance);

        currentFeatureVector.setDataset(trainingInstances);
        trainingInstances.add(currentFeatureVector);                
    }

    classifier = new NaiveBayes();

    try {
        //classifier training code
        classifier.buildClassifier(trainingInstances);

        //storing the trained classifier to a file for future use
        weka.core.SerializationHelper.write(""NaiveBayes.model"",classifier);
    } catch (Exception ex) {
        System.out.println(""Exception in training the classifier.""+ex);
    }
}

private Instance extractFeature(Instance inputInstance) throws Exception
{       
    String tweet = inputInstance.stringValue(0);
    StringTokenizer defaultTokenizer = new StringTokenizer(tweet);
    List&lt;String&gt; tokens=new ArrayList&lt;String&gt;();
    while (defaultTokenizer.hasMoreTokens())
    {
        String t= defaultTokenizer.nextToken();
        tokens.add(t);
    }

    Iterator&lt;String&gt; a = tokens.iterator();
    while(a.hasNext())
    {
                String token=(String) a.next();
                String word = token.replaceAll(""#"","""");
                if(featureWords.contains(word))
                {                                              
                    double cont=featureMap.get(featureWords.indexOf(word))+1;
                    featureMap.put(featureWords.indexOf(word),cont);
                }
                else{
                    featureWords.add(word);
                    featureMap.put(featureWords.indexOf(word), 1.0);
                }

    }
    attributeList.clear();
    for(String featureWord : featureWords)
    {
        attributeList.add(new Attribute(featureWord));   
    }
    attributeList.add(new Attribute(""Class"", classValues));
    int indices[] = new int[featureMap.size()+1];
    double values[] = new double[featureMap.size()+1];
    int i=0;
    for(Map.Entry&lt;Integer,Double&gt; entry : featureMap.entrySet())
    {
        indices[i] = entry.getKey();
        values[i] = entry.getValue();
        i++;
    }
    indices[i] = featureWords.size();
    values[i] = (double)classValues.indexOf(inputInstance.stringValue(1));
    trainingInstances = createInstances(""TRAINING_INSTANCES"");

    return new SparseInstance(1.0,values,indices,1000000);
}


private void getTrainingDataset(final String INPUT_FILENAME)
{
    try{
        ArffLoader trainingLoader = new ArffLoader();
        trainingLoader.setSource(new File(INPUT_FILENAME));
        inputDataset = trainingLoader.getDataSet();
    }catch(IOException ex)
    {
        System.out.println(""Exception in getTrainingDataset Method"");
    }
    System.out.println(""dataset ""+inputDataset.numAttributes());
}

private Instances createInstances(final String INSTANCES_NAME)
{
    //create an Instances object with initial capacity as zero 
    Instances instances = new Instances(INSTANCES_NAME,attributeList,0);
    //sets the class index as the last attribute
    instances.setClassIndex(instances.numAttributes()-1);

    return instances;
}

public static void main(String[] args) throws Exception
{
      Classificatore wekaTutorial = new Classificatore();
      wekaTutorial.trainClassifier(""training_set_prova_tent.arff"");
      wekaTutorial.testClassifier(""testing.arff"");
}

public Classificatore()
{
    attributeList = new ArrayList&lt;Attribute&gt;();
    initialize();
}    

private void initialize()
{

    featureWords= new ArrayList&lt;String&gt;(); 

    featureMap = new TreeMap&lt;&gt;();

    classValues= new ArrayList&lt;String&gt;();
    classValues.add(""politica"");
    classValues.add(""sport"");
    classValues.add(""musicatvcinema"");
    classValues.add(""infogeneriche"");
    classValues.add(""fattidelgiorno"");
    classValues.add(""statopersonale"");
    classValues.add(""checkin"");
    classValues.add(""conversazione"");
}
</code></pre>

<p>TESTING METHODS</p>

<pre><code>public void testClassifier(final String INPUT_FILENAME) throws Exception
{
    getTrainingDataset(INPUT_FILENAME);

    //trainingInstances consists of feature vector of every input
    Instances testingInstances = createInstances(""TESTING_INSTANCES"");

    for(Instance currentInstance : inputDataset)
    {

        //extractFeature method returns the feature vector for the current input
        Instance currentFeatureVector = extractFeature(currentInstance);
        //Make the currentFeatureVector to be added to the trainingInstances
        currentFeatureVector.setDataset(testingInstances);
        testingInstances.add(currentFeatureVector);

    }


    try {
        //Classifier deserialization
        classifier = (Classifier) weka.core.SerializationHelper.read(""NaiveBayes.model"");

        //classifier testing code
        for(Instance testInstance : testingInstances)
        {

            double score = classifier.classifyInstance(testInstance);
            double[] vv= classifier.distributionForInstance(testInstance);
            for(int k=0;k&lt;vv.length;k++){
            System.out.println(""distribution ""+vv[k]); //this are the probabilities of the classes and as result i get 1.0 in one and 0.0 in all the others
            }
            System.out.println(testingInstances.attribute(""Class"").value((int)score));
        }
    } catch (Exception ex) {
        System.out.println(""Exception in testing the classifier.""+ex);
    }
}
</code></pre>

<p>I want to create a text classifier for short messages, this code is based on this tutorial <a href=""http://preciselyconcise.com/apis_and_installations/training_a_weka_classifier_in_java.php"" rel=""nofollow"">http://preciselyconcise.com/apis_and_installations/training_a_weka_classifier_in_java.php</a> . The problem is that the classifier predict the wrong class for almost every message in the testing.arff because the probabilities of the classes are not correct. The training_set_prova_tent.arff has the same number of messages per class.
The example i'm following use a featureWords.dat and associate 1.0 to the word if it is present in a message instead I want to create my own dictionary with the words present in the training_set_prova_tent plus the words present in testing and associate to every word the number of occurrences .</p>

<p>P.S
I know that this is exactly what can i do with the filter StringToWordVector but I haven't found any example that exaplain how to use this filter with two file: one for the training set and one for the test set. So it seems easier to adapt the code I found. </p>

<p>Thank you very much</p>
","java, weka, text-classification, categorization","<p>It seems like you changed the code from the <a href=""http://preciselyconcise.com/apis_and_installations/training_a_weka_classifier_in_java.php"" rel=""nofollow"">website you referenced</a> in some crucial points, but not in a good way. I'll try to draft what you're trying to do and what mistakes I've found.</p>

<p>What you (probably) wanted to do in <code>extractFeature</code> is</p>

<ul>
<li>Split each tweet into words (tokenize)</li>
<li>Count the number of occurrences of these words</li>
<li>Create a feature vector representing these word counts plus the class</li>
</ul>

<p>What you've overlooked in that method is</p>

<ol>
<li><p>You never reset your <code>featureMap</code>. The line</p>

<blockquote>
<pre><code>Map&lt;Integer,Double&gt; featureMap = new TreeMap&lt;&gt;();
</code></pre>
</blockquote>

<p>originally was at the beginning <code>extractFeatures</code>, but you moved it to <code>initialize</code>. That means that you always add up the word counts, but never reset them. For each new tweet, your word count also includes the word count of all previous tweets. I'm sure that is not what you wanted.</p></li>
<li><p>You don't initialize <code>featureWords</code> with the words you want as features. Yes, you create an empty list, but you fill it iteratively with each tweet.  The original code initialized it once in the <code>initialize</code> method and it never changed after that. There are two problems with that:</p>

<ul>
<li>With each new tweet, new features (words) get added, so your feature vector grows with each tweet. That wouldn't be such a big problem (SparseInstance), but that means that</li>
<li>Your <code>class</code> attribute is always in another place. These two lines work for the original code, because <code>featureWords.size()</code> is basically a constant, but in your code the class label will be at index 5, then 8, then 12, and so on, but it <strong>must</strong> be the same for every instance.</li>
</ul>

<blockquote>
<pre><code>indices[i] = featureWords.size();
values[i] = (double) classValues.indexOf(inputInstance.stringValue(1));
</code></pre>
</blockquote></li>
<li><p>This also manifests itself in the fact that you build a new <code>attributeList</code> with each new tweet, instead of only once in <code>initialize</code>, which is bad for already explained reasons.</p></li>
</ol>

<p>There may be more stuff, but - as it is - your code is rather unfixable. What you want is much closer to the tutorial source code which you modified than your version.</p>

<p>Also, you should look into <a href=""http://weka.sourceforge.net/doc.dev/weka/filters/unsupervised/attribute/StringToWordVector.html"" rel=""nofollow"">StringToWordVector</a> because it seems like this is exactly what you want to do:</p>

<blockquote>
  <p>Converts String attributes into a set of attributes representing word occurrence (depending on the tokenizer) information from the text contained in the strings. The set of words (attributes) is determined by the first batch filtered (typically training data).</p>
</blockquote>
",1,1,1884,2014-12-30 20:24:40,https://stackoverflow.com/questions/27712040/text-classifier-with-weka-how-to-correctly-train-a-classifier-issue
Parameters grid search with different text sets for dictionary creation and cross validation,"<p>I have to train a classifier for spam detecting.</p>

<p><strong>Dataset that I have.</strong></p>

<p>At hand I have one labeled dataset of emails with <code>[text, class]</code>.
And I also have a lot of emails without class labels.</p>

<p><strong>What I want to do.</strong></p>

<p>I want to use <code>gridsearchcv()</code> function to estimate the best hyperparameters for my model. And one of the parameters is related to dictionary creation (like 1-gram or 2-gram, min frequency etc.). What I want <code>gridsearchcv()</code> function to do is to use the whole emails dataset (emails with labels + emails without labels) for <code>CountVectorizer</code> in my pipeline to create dictionary. But I want it to test the result only on the labeled emails. So, basically I want to use the whole dataset for creation of dictionary and I want to estimate parameters using cross validation only on labeled dataset.</p>

<p>Any help will be apprectiated :)</p>

<p><strong>Update:</strong></p>

<p><strong>important:</strong> To address @AndreasMueller answer: the results will be different because I also tune parameters of CountVectorizer and I use inverse document frequency. So, I am searching for a way to make my classifier more general by also including the unlabeled data.</p>

<p>This is what I have by now:</p>

<pre><code>pipeline = Pipeline([
('features', FeatureUnion([
    ('words', Pipeline([
        ('vect',  CountVectorizer()),
        ('frequency_transform',  TfidfTransformer())
    ])),            
    ('url_feature',  Contains_URL_Transformer()),
    ('html_feature', Contains_HTML_Transformer()),
    ('length_feature', Text_Length_Transformer()),
    ('response_feature', Contains_Re_Transformer())
    ])),
('clf',  SVC())
])

parameters = {
'features__words__vect__min_df': (1, 3, 5),
'features__words__vect__token_pattern': (r""\b[^\W\d_]+\b"",),
'features__words__vect__binary': (False,),
'features__words__frequency_transform__use_idf' : (True,),
#'vect__max_features': (None, 5000, 10000, 50000),
'features__words__vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
'clf__C': (1, 5, 10),
'clf__kernel': ('linear', 'rbf')
#'tfidf__use_idf': (True, False)
#'tfidf__norm': ('l1', 'l2'),
#'clf__alpha': (0.00001, 0.000001),
#'clf__penalty': ('l2', 'elasticnet'),
#'clf__n_iter': (10, 50, 80),
}

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)

data_column = numpy.asarray(data['text'])

data_column = numpy.append(data_column, ['test'])

grid_search.fit(data_column, numpy.asarray(data['class']))

best_parameters = grid_search.best_estimator_.get_params()

for param_name in sorted(parameters.keys()):
    print(""\t%s: %r"" % (param_name, best_parameters[param_name]))
</code></pre>

<p>But I also have <code>unlabled_data['text']</code>. How can I add the mix of <code>data['text']</code> and <code>unlabled_data['text']</code> to the pipeline in order to create the dictionary (and estimate the parameters) from that mix but to test it on the labeled data. The problem is that when I do <code>grid_search.fit()</code> it uses the provided data set to create dictionary and I see no way to put there all the emails.</p>
","python, email, pandas, scikit-learn, text-classification","<p>A simple solution solution forces the fit data to be constant irrespective of the cross-validation data:</p>

<pre><code>X_all = full dataset

class MyVectorizer(sklearn.feature_extraction.text.TfidfVectorizer):
    def fit(self, X, y=None):
        return super(MyVectorizer, self).fit(X_all)
    def fit_transform(self, X, y=None):
        return super(MyVectorizer, self).fit(X_all).transform(X)
</code></pre>

<p>Use this in place of the <code>'words'</code> sub-pipeline above.</p>

<hr>

<p>An arguably less hacky, but much more complicated, solution is along the lines of:</p>

<ul>
<li>Concatenate the labelled and unlabelled data, setting the label of
the latter's instances to <code>-1</code></li>
<li>Use a custom cross validation generator that always keeps the unlabeled instances in the training set.</li>
<li>Use a wrapper around the post-feature-extraction part of the pipeline (here SVC) to remove the unlabelled data (note you cannot just
implement this as a <code>Transformer</code>). (Perhaps extending from SVC is simpler, a bit like what <code>MyVectorizer</code> does above, but without using the global data hack.)</li>
</ul>

<p>An advantage of this approach is that it applies regardless of the <code>GridSearchCV</code> input (as opposed to the hack of injecting the full data via a global variable).</p>

<p>Example code:</p>

<pre><code>def semisupervised_stratified_kfold(y, *args, **kwargs):
    labeled_idx = np.flatnonzero(y != -1)
    unlabeled_idx = np.flatnonzero(y == -1)
    for train, test in StratifiedKFold(y[labelled_idx], *args, **kwargs):
        train = np.concatenate([unlabeled_idx, labeled_idx.take(train)])
        test = labeled_idx.take(test)
        yield train, test

from sklearn.utils.metaestimators import if_delegate_has_method
class StripUnlabelled(sklearn.base.BaseEstimator):
    def __init__(self, estimator):
        self.estimator = sklearn.base.clone(estimator)
    def fit(self, X, y, **kwargs):
        return self.estimator.fit()
    @if_delegate_has_method(delegate='estimator')
    def predict(self, X):
        return self.estimator.predict(X)
    # and similar for decision_function, predict_proba, score, etc.
</code></pre>

<p>Then set <code>GridSearchCV</code>'s <code>cv</code> parameter to the custom generator, wrap <code>StripUnlabeled</code> around the <code>SVC</code> instance, and prefix SVC parameter names with <code>estimator__</code></p>

<p>This will actually not build the TFIDF model on all the data, but will use all the unlabeled data plus all the training folds of the labeled data.</p>

<p>Also, note that all similar solutions using <code>Pipeline</code> will be quite inefficient, given the repeated work is not cached when parameters are changed downstream, although there are generic solutions that have been proposed for caching parts of pipelines.</p>
",1,1,1477,2015-02-20 09:22:42,https://stackoverflow.com/questions/28625456/parameters-grid-search-with-different-text-sets-for-dictionary-creation-and-cros
CountVectorizer deleting features that only appear once,"<p>I'm using the sklearn python package, and I am having trouble creating a <code>CountVectorizer</code> with a pre-created dictionary, where the <code>CountVectorizer</code> doesn't delete features that only appear once or don't appear at all.</p>

<p>Here is the sample code that I have:</p>

<pre><code>train_count_vect, training_matrix, train_labels = setup_data(train_corpus, query, vocabulary=None)
test_count_vect, test_matrix, test_labels = setup_data(test_corpus, query, vocabulary=train_count_vect.get_feature_names())

print(len(train_count_vect.get_feature_names()))
print(len(test_count_vect.get_feature_names()))
</code></pre>

<p><code>len(train_count_vect.get_feature_names())</code> outputs <code>89967</code>
<code>len(test_count_vect.get_feature_names())</code> outputs <code>9833</code></p>

<p>Inside the <code>setup_data()</code> function, I am just initializing <code>CountVectorizer</code>. For training data, I'm initializing it without a preset vocabulary. Then, for test data, I'm initializing CountVectorizer with the vocabulary I retrieved from my training data.</p>

<p>How do I get the vocabularies to be the same lengths? I think sklearn is deleting features because they only appear once or don't appear at all in my test corpus. I need to have the same vocabulary because otherwise, my classifier will be of a different length from my test data points.</p>
","python, machine-learning, scikit-learn, text-classification","<p>So, it's impossible to say without actually seeing the source code of <code>setup_data</code>, but I have a pretty decent guess as to what is going on here. <code>sklearn</code> follows the <code>fit_transform</code> format, meaning there are two stages, specifically <code>fit</code>, and <code>transform</code>.</p>

<p>In the example of the <code>CountVectorizer</code> the <code>fit</code> stage effectively creates the vocabulary, and the <code>transform</code> step transforms your input text into that vocabulary space.</p>

<p>My guess is that you're calling <code>fit</code> on both datasets instead of just one, you need to be using the same ""fitted"" version of <code>CountVectorizer</code> on both if you want the results to line up. e.g.:</p>

<pre><code>model = CountVectorizer()
transformed_train = model.fit_transform(train_corpus)
transformed_test = model.transform(test_corpus)
</code></pre>

<p>Again, this can only be a guess until you post the <code>setup_data</code> function, but having seen this before I would guess you're doing something more like this:</p>

<pre><code>model = CountVectorizer()
transformed_train = model.fit_transform(train_corpus)
transformed_test = model.fit_transform(test_corpus)
</code></pre>

<p>which will effectively make a new vocabulary for the <code>test_corpus</code>, which unsurprisingly won't give you the same vocabulary length in both cases.</p>
",4,2,1280,2015-02-21 09:26:14,https://stackoverflow.com/questions/28644177/countvectorizer-deleting-features-that-only-appear-once
"Using Topic Model, how should we set up a &quot;stop words&quot; list?","<p>There are some standard stop lists, giving words like ""a the of not"" to be removed from corpus. However, I'm wondering, should the stop list change case by case?</p>

<p>For example, I have 10K of articles from a journal, then because of the structure of an article, basically you will see words like ""introduction, review, conclusion, page"" in every article. My concern is: should we remove these words from our corpus? (the words that every document has?) Thanks to every comment and suggestion.</p>
","stop-words, lda, topic-modeling, text-classification","<p>I am working on a similar problem, but of text categorization. From my experience, it is good to have a domain specific set of stop word list along with the standard .
list. Otherwise, these words like ""introduction"",""review"" etc. will come up in the term frequency matrix, if you have tried out analysing it. It can mislead your models by giving more weights to these domain specific keywords.</p>
",5,5,8341,2015-02-24 18:09:05,https://stackoverflow.com/questions/28703120/using-topic-model-how-should-we-set-up-a-stop-words-list
How to train a naive bayes classifier with pos-tag sequence as a feature?,"<p>I have two classes of sentences. Each has reasonably distinct pos-tag sequence. How can I train a Naive-Bayes classifier with POS-Tag sequence as a feature? Does Stanford CoreNLP/NLTK (Java or Python) provide any method for building a classifier with pos-tag as a feature?  I know in python <code>NaiveBayesClassifier</code> allows for building a NB classifier but it uses <code>contains-a-word</code> as feature but can it be extended to use pos-tag-sequence as a feature ?</p>
","machine-learning, nltk, stanford-nlp, text-classification, naivebayes","<p>If you know how to train and predict texts (or sentences in your case) using nltk's naive bayes classifier and words as features, than you can easily extend this approach in order to classify texts by pos-tags. This is because the classifier don't care about whether your feature-strings are words or tags. So you can simply replace the words of your sentences by pos-tags using for example nltk's standard pos tagger:</p>

<pre class=""lang-py prettyprint-override""><code>sent = ['So', 'they', 'have', 'internet', 'on', 'computers' , 'now']
tags = [t for w, t in nltk.pos_tag(sent)]
print tags
</code></pre>

<p>['IN', 'PRP', 'VBP', 'JJ', 'IN', 'NNS', 'RB']</p>

<p>As from now you can proceed with the ""contains-a-word"" approach.</p>
",6,6,3649,2015-02-27 11:50:31,https://stackoverflow.com/questions/28764459/how-to-train-a-naive-bayes-classifier-with-pos-tag-sequence-as-a-feature
Generate an Arff File for Weka,"<p>Hye there I am new to this work and I am getting confused after searching about how to get through it!
Actually i want to create a sparse ARFF file for weka for text classification! I have been searching online how to get start with it. My requirement is to generate a sparse arff file that should be compatible with the weka!
The outline for the arff should be like:</p>

<pre><code> @relation myrelation
 @attribute att0 numeric
 @attribute att1 numeric
 @data
 {0,1,4,5 , A}
 {0,5,2,,1 B}
</code></pre>

<p>Such that I have some strings and then a class
suppose my data set is as follow:</p>

<pre><code> string is a string A
 Hello a string B
 Another is string C
 .
 .
 .
</code></pre>

<p>first comes the string and then the class as A,B or C...
So what i want is to convert my dataset into above mentioned sparse arff format.
Can somebody give me a direction how can i do it? please
I want to do it in java</p>
","java, weka, text-classification, arff","<p>You can use Weka's StringToWordVector filter to convert the text into a word vector (but not necessarily a sparse matrix). Take a look at my <a href=""http://youtu.be/jSZ9jQy1sfE"" rel=""nofollow"">tutorial</a> on this. </p>
",2,2,2446,2015-02-27 16:09:45,https://stackoverflow.com/questions/28769427/generate-an-arff-file-for-weka
"Trying to run sklearn text classification on Apache Spark..GETTING Expected sequence or array-like, got PythonRDD[1] at RDD at PythonRDD.scala:43","<p>I am trying to run sklearn SDG classifier on twitter data which is manually labelled into two classes 0 and 1. </p>

<p>I am pretty new to spark and would like your help on this. </p>

<p>I saw some code online and tried to simulate for my example but unfortunately it doesnt seem to work and I dont know why.</p>

<p>Your help would be greatly appreciated.</p>

<pre><code>import sys
sys.path.append('/home/userName/Downloads/spark-1.2.1/python')

from pyspark import SparkContext

import numpy as np

from sklearn.cross_validation import train_test_split, Bootstrap
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier


import numpy as np
from sklearn.metrics import hamming_loss
from sklearn import cross_validation
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn import preprocessing
import pandas as pd;
from sklearn import metrics
from sklearn.utils import shuffle
from sklearn.linear_model import RidgeClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from time import time
from sklearn.externals import joblib
import re
from HTMLParser import HTMLParser
from sklearn.grid_search import GridSearchCV
import pickle
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
%matplotlib inline


def run(sc):
  u_cols = ['CLASS','USER_RATING', 'REVIEW_TEXT']
df =
pd.read_csv('/home/userName/Desktop/input_file.csv',header=1,names=u_cols)

#Cleaning the data
lenn = len(df['REVIEW_TEXT'])
tag_remove = re.compile(r'&lt;[^&gt;]+&gt;')
for i in range(0,lenn):
    #Removing code block
    df['REVIEW_TEXT'][i] = re.sub('&lt;code&gt;.*?&lt;/code&gt;', '', df['REVIEW_TEXT'][i])
    #Removeing html tags
    df['REVIEW_TEXT'][i] = tag_remove.sub('', df['REVIEW_TEXT'][i])



X_train = df['REVIEW_TEXT']
y_train = df['CLASS']


X_train_final = X_train
y_train_final = y_train

#Validation Set Approach
X_train_final, X_test_final, y_train_final, y_test_final =     cross_validation.train_test_split(
X_train_final, y_train_final, test_size=0.05, random_state=15)

vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 20, 
                           non_negative=True, stop_words = 'english', ngram_range = (1,2))


X_train_final = vectorizer.transform(X_train_final)
X_test_final = vectorizer.transform(X_test_final)


model = (SGDClassifier(alpha=1e-05, class_weight=None, epsilon=0.1, eta0=0.0,fit_intercept=True, 
                                           l1_ratio=0.15, learning_rate='optimal',loss='hinge', n_iter=5, n_jobs=1, 
                                           penalty='l1', power_t=0.5,random_state=None, shuffle=False, verbose=0,
                                           warm_start=False))

samples = sc.parallelize(Bootstrap(y_train_final.shape[0]))

vote_tally = samples.map(lambda (index, _):
    model.fit(X[index], y[index]).predict(X_test)
)

return accuracy_score(y_test_final, vote_tally)


if __name__ == '__main__':
    print run(SparkContext(""local"", ""Boost""))
</code></pre>

<p>getting the following ERROR:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-be25c966218e&gt; in &lt;module&gt;()
    107 
    108 if __name__ == '__main__':
--&gt; 109     print run(SparkContext(""local"", ""Boost""))
    110 

&lt;ipython-input-1-be25c966218e&gt; in run(sc)
    102     )
    103 
--&gt; 104     return accuracy_score(y_test_final, vote_tally)
    105     #print vote_tally.count()
    106     #return vote_tally

/usr/local/lib/python2.7/dist-packages/sklearn/metrics/metrics.pyc in accuracy_score(y_true, y_pred, normalize, sample_weight)
   1295 
   1296     # Compute accuracy for each possible representation
-&gt; 1297     y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)
   1298     if y_type == 'multilabel-indicator':
   1299         score = (y_pred != y_true).sum(axis=1) == 0

/usr/local/lib/python2.7/dist-packages/sklearn/metrics/metrics.pyc in _check_clf_targets(y_true, y_pred)
    107     y_pred : array or indicator matrix
    108     """"""
--&gt; 109     y_true, y_pred = check_arrays(y_true, y_pred, allow_lists=True)
    110     type_true = type_of_target(y_true)
    111     type_pred = type_of_target(y_pred)

/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc in check_arrays(*arrays, **options)
    248             checked_arrays.append(array)
    249             continue
--&gt; 250         size = _num_samples(array)
    251 
    252         if size != n_samples:

/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc in _num_samples(x)
    172             x = np.asarray(x)
    173         else:
--&gt; 174             raise TypeError(""Expected sequence or array-like, got %r"" % x)
    175     return x.shape[0] if hasattr(x, 'shape') else len(x)
    176 

**TypeError: Expected sequence or array-like, got PythonRDD[1] at RDD at PythonRDD.scala:43**
</code></pre>
","apache-spark, scikit-learn, text-classification","<p>The problem is that sklearn components expects sequences/array-like/sparse/etc. data to work on, but you work with RDDs in pyspark.<br>
We have a library which can help you solve your problem. It's called <a href=""https://github.com/lensacom/sparkit-learn"" rel=""nofollow"">sparkit-learn</a>.<br>
Give it a try.</p>
",1,0,1893,2015-03-01 02:10:34,https://stackoverflow.com/questions/28789318/trying-to-run-sklearn-text-classification-on-apache-spark-getting-expected-sequ
"With TfidfVectorizer, is it possible to use one corpus for idf information, and another one for the actual index?","<p>using sklearn.feature_extraction.text.TfidfVectorizer</p>

<p>I want to train a classifier with a Bag of Words tf-idf data.</p>

<p>I have a large untagged corpus, and a smaller tagged corpus.</p>

<p>I plan to use the tagged corpus to build a classifier, based on a bag of words with tf-idf model.
However, I prefer to use the complete corpus (including the untagged data) to compute the idf statistics.</p>

<p>Is this possible when using sklearn? </p>

<p>One solution I thought of was to build a model of all the corpus, and later remove the rows belonging to untagged data. however, the corpus might be to large to store in the ram.</p>
","scikit-learn, tf-idf, text-classification","<p>if I understand you correctly. You can fit your TFIDF model to all of your data and then call <code>transform</code> on the smaller tagged corpus:</p>

<pre><code>vec =TfidfVectorizer()
model = vec.fit(alldata)
tagged_data_tfidf = vec.transform(tagged_data)
</code></pre>
",1,1,1124,2015-03-05 12:04:30,https://stackoverflow.com/questions/28877481/with-tfidfvectorizer-is-it-possible-to-use-one-corpus-for-idf-information-and
How can I use my text classifier in practice? As of getting the tf-idf values of new comments,"<p>Let say I am building classifier in java which would classify comments to spam or not. The data set is simple it have two attributes the string comment and the nominal class.</p>

<p>Now I need to filter my training data set using StringToWordVector filter. My first problem is with test data set if it is filtered it will be different than the training set attributes. I researched and found that I can use batch filtering like:</p>

<pre><code>    StringToWordVector filter = new StringToWordVector();
    //Here I will set the options, I would be using tf-idf and someothers
    filter.setInputFormat(TrainingData);
</code></pre>

<p>Now is this approach correct? So If use this filter both the data sets should be compatible but well they be filtered in correct way? I am afraid the tf-idf values of the testing would be affected in a way that would reduce the accuracy.</p>

<p>Now to my main question how can I use my classifier in practice? in practice I am going to get a single comment which would be string I figured I would make it an instance but how would I filter it to get the tf-idf values to classify it?!! I figured maybe I could add the comment to the original training data set and recalculate the tf-idf everytime, but is that how it is done in practice?</p>
","weka, text-classification","<p>I am attempting to answer the question using a different text classification task than spam classification. </p>

<p>Say, I have the following training data:</p>

<pre><code>""The US government had imposed extra taxes on crude oil"", petrolium
""The German manufacturers are observing different genes of Canola oil"", non-petrolium
</code></pre>

<p>And the following test data:</p>

<pre><code>""Canada is famous for producing quality corn oil"", ?
</code></pre>

<p>Now, consider you are going to use <code>Naive Bayes</code> and use <code>StringToWordVector</code> filter. If you apply the filter on training and test data separately, you will have two very different word vectors. Each term in the training and test data will become a feature and therefore you will get an error like <strong>""Training and test data are not compatible""</strong>. So, the solution is to use <code>FilteredClassifier</code> that takes both the choice of classifier (in our case <code>Naive Bayes</code>) and the filter (in our case <code>StringToWordVector</code>). You will need something similar to what follows:</p>

<pre><code>private NaiveBayes nb;
private FilteredClassifier fc;
private StringToWordVector filter;
private double[] clsLabel;

// Set the filter---&gt;
filter = new StringToWordVector();
filter.setTokenizer(tokenizer); 
filter.setWordsToKeep(1000000); 
filter.setDoNotOperateOnPerClassBasis(true); 
filter.setLowerCaseTokens(true);
filter.setTFTransform(true);
filter.setIDFTransform(true);
filter.setStopwords(stopwords);

filter.setInputFormat(trainingData);    
//&lt;---setting of filter ends

//setting the classifier---&gt;
fc = new FilteredClassifier();
nb = new NaiveBayes();      
fc.setFilter(filter);
fc.setClassifier(nb);
//&lt;---setting of the classifier ends

fc.buildClassifier(trainingData);

//Classification---&gt;
        clsLabel = new double[testData.numInstances()]; //holds class label of the test documents
        //for each test document---&gt;
        for (int i = 0; i &lt; testData.numInstances(); i ++){
            try {
                clsLabel[i] = fc.classifyInstance(testData.instance(i));
            } catch (Exception e) {
                System.out.println(""Error from Classification.classify(). Cannot classify instance"");
            }
            testData.instance(i).setClassValue(clsLabel[i]);
        }//end for
        //&lt;---classification ends
</code></pre>

<p>NB. the TF-IDF calculations of training and test data will be done separately.</p>
",2,1,330,2015-03-09 10:47:27,https://stackoverflow.com/questions/28940110/how-can-i-use-my-text-classifier-in-practice-as-of-getting-the-tf-idf-values-of
Testing the NLTK classifier on specific file,"<p>The following code run <strong>Naive Bayes movie review classifier</strong>. 
The code generate a list of the most informative features. </p>

<p><em>Note:</em> <code>**movie review**</code> folder is in the <code>nltk</code>.</p>

<pre><code>from itertools import chain
from nltk.corpus import stopwords
from nltk.probability import FreqDist
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews
stop = stopwords.words('english')

documents = [([w for w in movie_reviews.words(i) if w.lower() not in stop and w.lower() not in string.punctuation], i.split('/')[0]) for i in movie_reviews.fileids()]


word_features = FreqDist(chain(*[i for i,j in documents]))
word_features = word_features.keys()[:100]

numtrain = int(len(documents) * 90 / 100)
train_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag in documents[:numtrain]]
test_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag  in documents[numtrain:]]

classifier = NaiveBayesClassifier.train(train_set)
print nltk.classify.accuracy(classifier, test_set)
classifier.show_most_informative_features(5)
</code></pre>

<p><a href=""https://stackoverflow.com/questions/21107075/classification-using-movie-review-corpus-in-nltk-python"">link of code</a> from <a href=""https://stackoverflow.com/users/610569/alvas"">alvas</a></p>

<p>how can I <strong>test</strong> the classifier on <strong>specific file</strong>?</p>

<p>Please let me know if my question is ambiguous or wrong.</p>
","python-2.7, nlp, classification, nltk, text-classification","<p>First, read these answers carefully, they contain parts of the answers you require and also briefly explains what the classifier does and how it works in NLTK:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/20827741/nltk-naivebayesclassifier-training-for-sentiment-analysis/20833372#20833372"">nltk NaiveBayesClassifier training for sentiment analysis</a></li>
<li><a href=""https://stackoverflow.com/questions/29275614/using-my-own-corpus-instead-of-movie-reviews-corpus-for-classification-in-nltk"">Using my own corpus instead of movie_reviews corpus for Classification in NLTK</a></li>
<li><a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">http://www.nltk.org/book/ch06.html</a></li>
</ul>

<hr>

<p><strong>Testing classifier on annotated data</strong></p>

<p>Now to answer your question. We assume that your question is a follow-up of this question: <a href=""https://stackoverflow.com/questions/29275614/using-my-own-corpus-instead-of-movie-reviews-corpus-for-classification-in-nltk"">Using my own corpus instead of movie_reviews corpus for Classification in NLTK</a></p>

<p>If your test text is structured the same way as the <code>movie_review</code> corpus, then you can simply read the test data as you would for the training data:</p>

<p>Just in case the explanation of the code is unclear, here's a walkthrough:</p>

<pre><code>traindir = '/home/alvas/my_movie_reviews'
mr = CategorizedPlaintextCorpusReader(traindir, r'(?!\.).*\.txt', cat_pattern=r'(neg|pos)/.*', encoding='ascii')
</code></pre>

<p>The two lines above is to read a directory <code>my_movie_reviews</code> with such a structure:</p>

<pre><code>\my_movie_reviews
    \pos
        123.txt
        234.txt
    \neg
        456.txt
        789.txt
    README
</code></pre>

<p>Then the next line extracts documents with its <code>pos/neg</code> tag that's part of the directory structure.</p>

<pre><code>documents = [([w for w in mr.words(i) if w.lower() not in stop and w not in string.punctuation], i.split('/')[0]) for i in mr.fileids()]
</code></pre>

<p>Here's the explanation for the above line:</p>

<pre><code># This extracts the pos/neg tag
labels = [i for i.split('/')[0]) for i in mr.fileids()]
# Reads the words from the corpus through the CategorizedPlaintextCorpusReader object
words = [w for w in mr.words(i)]
# Removes the stopwords
words = [w for w in mr.words(i) if w.lower() not in stop]
# Removes the punctuation
words = [w for w in mr.words(i) w not in string.punctuation]
# Removes the stopwords and punctuations
words = [w for w in mr.words(i) if w.lower() not in stop and w not in string.punctuation]
# Removes the stopwords and punctuations and put them in a tuple with the pos/neg labels
documents = [([w for w in mr.words(i) if w.lower() not in stop and w not in string.punctuation], i.split('/')[0]) for i in mr.fileids()]
</code></pre>

<p><strong>The SAME process should be applied when you read the test data!!!</strong></p>

<p>Now to the feature processing:</p>

<p>The following lines extra top 100 features for the classifier:</p>

<pre><code># Extract the words features and put them into FreqDist
# object which records the no. of times each unique word occurs
word_features = FreqDist(chain(*[i for i,j in documents]))
# Cuts the FreqDist to the top 100 words in terms of their counts.
word_features = word_features.keys()[:100]
</code></pre>

<p>Next to processing the documents into classify-able format:</p>

<pre><code># Splits the training data into training size and testing size
numtrain = int(len(documents) * 90 / 100)
# Process the documents for training data
train_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag in documents[:numtrain]]
# Process the documents for testing data
test_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag  in documents[numtrain:]]
</code></pre>

<p>Now to explain that long list comprehension for <code>train_set</code> and `test_set:</p>

<pre><code># Take the first `numtrain` no. of documents
# as training documents
train_docs = documents[:numtrain]
# Takes the rest of the documents as test documents.
test_docs = documents[numtrain:]
# These extract the feature sets for the classifier
# please look at the full explanation on https://stackoverflow.com/questions/20827741/nltk-naivebayesclassifier-training-for-sentiment-analysis/
train_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag  in train_docs]
</code></pre>

<p><strong>You need to process the documents as above for the feature extractions in the test documents too!!!</strong></p>

<p>So here's how you can read the test data:</p>

<pre><code>stop = stopwords.words('english')

# Reads the training data.
traindir = '/home/alvas/my_movie_reviews'
mr = CategorizedPlaintextCorpusReader(traindir, r'(?!\.).*\.txt', cat_pattern=r'(neg|pos)/.*', encoding='ascii')

# Converts training data into tuples of [(words,label), ...]
documents = [([w for w in mr.words(i) if w.lower() not in stop and w not in string.punctuation], i.split('/')[0]) for i in mr.fileids()]

# Now do the same for the testing data.
testdir = '/home/alvas/test_reviews'
mr_test = CategorizedPlaintextCorpusReader(testdir, r'(?!\.).*\.txt', cat_pattern=r'(neg|pos)/.*', encoding='ascii')
# Converts testing data into tuples of [(words,label), ...]
test_documents = [([w for w in mr_test.words(i) if w.lower() not in stop and w not in string.punctuation], i.split('/')[0]) for i in mr_test.fileids()]
</code></pre>

<p>Then continue with the processing steps described above, and simply do this to get the label for the test document as @yvespeirsman answered:</p>

<pre><code>#### FOR TRAINING DATA ####
stop = stopwords.words('english')

# Reads the training data.
traindir = '/home/alvas/my_movie_reviews'
mr = CategorizedPlaintextCorpusReader(traindir, r'(?!\.).*\.txt', cat_pattern=r'(neg|pos)/.*', encoding='ascii')

# Converts training data into tuples of [(words,label), ...]
documents = [([w for w in mr.words(i) if w.lower() not in stop and w not in string.punctuation], i.split('/')[0]) for i in mr.fileids()]
# Extract training features.
word_features = FreqDist(chain(*[i for i,j in documents]))
word_features = word_features.keys()[:100]
# Assuming that you're using full data set
# since your test set is different.
train_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag  in documents]

#### TRAINS THE TAGGER ####
# Train the tagger
classifier = NaiveBayesClassifier.train(train_set)

#### FOR TESTING DATA ####
# Now do the same reading and processing for the testing data.
testdir = '/home/alvas/test_reviews'
mr_test = CategorizedPlaintextCorpusReader(testdir, r'(?!\.).*\.txt', cat_pattern=r'(neg|pos)/.*', encoding='ascii')
# Converts testing data into tuples of [(words,label), ...]
test_documents = [([w for w in mr_test.words(i) if w.lower() not in stop and w not in string.punctuation], i.split('/')[0]) for i in mr_test.fileids()]
# Reads test data into features:
test_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag  in test_documents]

#### Evaluate the classifier ####
for doc, gold_label in test_set:
    tagged_label = classifier.classify(doc)
    if tagged_label == gold_label:
        print(""Woohoo, correct"")
    else:
        print(""Boohoo, wrong"")
</code></pre>

<p>If the above code and explanation makes no sense to you, then you <strong>MUST</strong> read this tutorial before proceeding: <a href=""http://www.nltk.org/howto/classify.html"" rel=""nofollow noreferrer"">http://www.nltk.org/howto/classify.html</a></p>

<hr>

<p>Now let's say you have no annotation in your test data, i.e. your <code>test.txt</code> is not in the directory structure like the <code>movie_review</code> and just a plain textfile:</p>

<pre><code>\test_movie_reviews
    \1.txt
    \2.txt
</code></pre>

<p>Then there's no point in reading it into a categorized corpus, you can simply do read and tag the documents, i.e.:</p>

<pre><code>for infile in os.listdir(`test_movie_reviews): 
  for line in open(infile, 'r'):
       tagged_label = classifier.classify(doc)
</code></pre>

<p>BUT <strong>you CANNOT evaluate the results without annotation</strong>, so you can't check the tag if the <code>if-else</code>, also <strong>you need to tokenize your text</strong> if you're not using the CategorizedPlaintextCorpusReader.</p>

<p>If you just want to tag a plaintext file <code>test.txt</code>:</p>

<pre><code>import string
from itertools import chain
from nltk.corpus import stopwords
from nltk.probability import FreqDist
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews
from nltk import word_tokenize

stop = stopwords.words('english')

# Extracts the documents.
documents = [([w for w in movie_reviews.words(i) if w.lower() not in stop and w.lower() not in string.punctuation], i.split('/')[0]) for i in movie_reviews.fileids()]
# Extract the features.
word_features = FreqDist(chain(*[i for i,j in documents]))
word_features = word_features.keys()[:100]
# Converts documents to features.
train_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag in documents]
# Train the classifier.
classifier = NaiveBayesClassifier.train(train_set)

# Tag the test file.
with open('test.txt', 'r') as fin:
    for test_sentence in fin:
        # Tokenize the line.
        doc = word_tokenize(test_sentence.lower())
        featurized_doc = {i:(i in doc) for i in word_features}
        tagged_label = classifier.classify(featurized_doc)
        print(tagged_label)
</code></pre>

<p>Once again, please don't just copy and paste the solution and try to understand why and how it works.</p>
",8,8,3284,2015-03-27 13:34:43,https://stackoverflow.com/questions/29301952/testing-the-nltk-classifier-on-specific-file
How to identify the ID / name / title of the misclassified text file with sci-kit learn,"<p>I am buidling my own classifier for text classification but at the moment I am playing with sci-kit learn in order to figure out few things. I classified few of my text files using NB classifier. I am using 26 text files manually categorised into 2 categories, with each of the files being numbered between 01 - 26 (i.e. '01.txt' and so forth).   </p>

<p>My code and results:</p>

<pre><code>import sklearn
from sklearn.datasets import load_files
import numpy as np
bunch = load_files('corpus')

split_pcnt = 0.75 
split_size = int(len(bunch.data) * split_pcnt)
X_train = bunch.data[:split_size]
X_test = bunch.data[split_size:]
y_train = bunch.target[:split_size]
y_test = bunch.target[split_size:]

from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer

clf_1 = Pipeline([('vect', CountVectorizer()),
                      ('clf', MultinomialNB()),
    ])

from sklearn.cross_validation import cross_val_score, KFold
from scipy.stats import sem

def evaluate_cross_validation(clf, X, y, K):
    # create a k-fold croos validation iterator of k=5 folds
    cv = KFold(len(y), K, shuffle=True, random_state=0)
    # by default the score used is the one returned by score &gt;&gt;&gt; method of the estimator (accuracy)
    scores = cross_val_score(clf, X, y, cv=cv)
    print scores
    print (""Mean score: {0:.3f} (+/-{1:.3f})"").format(np.mean(scores), sem(scores))

clfs = [clf_1]

for clf in clfs:
    evaluate_cross_validation(clf, bunch.data, bunch.target, 5)

[ 0.5  0.4  0.4  0.4  0.6]
Mean score: 0.460 (+/-0.040)

from sklearn import metrics

def train_and_evaluate(clf, X_train, X_test, y_train, y_test):

    clf.fit(X_train, y_train)

    print ""Accuracy on training set:""
    print clf.score(X_train, y_train)
    print ""Accuracy on testing set:""
    print clf.score(X_test, y_test)
    y_pred = clf.predict(X_test)

    print ""Classification Report:""
    print metrics.classification_report(y_test, y_pred)
    print ""Confusion Matrix:""
    print metrics.confusion_matrix(y_test, y_pred)


train_and_evaluate(clf_1, X_train, X_test, y_train, y_test)

Accuracy on training set:
1.0

Accuracy on testing set:
0.714285714286

    Classification Report:
                 precision    recall  f1-score   support

              0       0.67      0.67      0.67         3
              1       0.75      0.75      0.75         4

    avg / total       0.71      0.71      0.71         7

    Confusion Matrix:
    [[2 1]
     [1 3]]
</code></pre>

<p>What I cannot figure out is how to identify the IDs of the misclassified files, to see which exact files are misclassified (e.g. '05.txt' and '23.txt'). Is this possible with sci-kit learn at all? </p>

<p>best,</p>

<p>guzdeh</p>
","python-2.7, scikit-learn, text-classification, naivebayes","<p>Yes, you have to use the attribute <code>filenames</code> of the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow"">load_files</a> result.</p>

<p>However you have two model training and evaluation cycles in your example code: one using CV and another using simple train-test split.</p>

<p>In the train-test split:</p>

<pre><code>test_filenames = bunch.filenames[split_size:]
misclassified = (y_pred != y_test)
print test_filenames[misscalssified]
</code></pre>

<p>This answer does not assume that the text files are in alphabetical order or that all numbers are present.</p>
",0,0,1545,2015-04-06 13:34:21,https://stackoverflow.com/questions/29472347/how-to-identify-the-id-name-title-of-the-misclassified-text-file-with-sci-ki
Assign a short text to one of two categories according to previous assignments (votes),"<p>There is a stream of short texts. Each one has the size of a tweet, or let us just assume they are all tweets.</p>

<p>The user can vote on any tweet. So, each tweet has one of the following three states:</p>

<p>relevant (positive vote)</p>

<p>default (neutral i.e. no vote)</p>

<p>irrelevant (negative vote)</p>

<p>Whenever a new set of tweets come, they will be displayed in a specific order. This order is determined by the votes of the user on all previous tweets. The aim is to assign a score to each new tweet. This score is calculated based on the word similarity or match between the text of this tweet and all the previous tweets voted by the user. In other words, the tweet with the highest score is going to be the one which contains the maximum number of words voted previously positive and the minimum of words voted previously as negative. Also, the new tweets having a high score will trigger a notification to the user as they are considered very relevant.</p>

<p>One last thing, a minimum of semantic consideration (natural language processing) would be great. </p>

<p>I have read about <strong>Term Frequency–Inverse Document Frequency</strong> and come up with this very simple and basic solution:</p>

<p>Reminder: a high weight in tf–idf is reached by a high word frequency and a low total frequency of the word in the whole collection.</p>

<p>If the user votes positive on a Tweet, all the words of this tweet will receive a positive point (same thing for the negative case). This means that we will have a large set of words where each word has the total number of positive points and negative points.    </p>

<p>If (Tweet score > 0) then this tweet will trigger a notification.</p>

<p>Tweet score = sum of all individual words’ scores of this tweet</p>

<p>word score = word frequency * inverse total frequency</p>

<p>word frequency in all previous votes = ( total <strong>positive</strong> votes for this word - total <strong>negative</strong> votes for this word) / <strong>total</strong> votes for this word</p>

<p>Inverse total frequency = log ( total votes of <strong>all</strong> words / total votes for <strong>this</strong> word)</p>

<p>Is this method enough? I am open to any better methods and any ready API or algorithm.</p>
","twitter, nlp, tf-idf, text-classification, document-classification","<p>One possible solution would be to train a classifier such as <a href=""http://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow"">Naive Bayes</a> on the tweets that a user has voted on. You can take a look at the documentation of <a href=""http://scikit-learn.org/"" rel=""nofollow"">scikit-learn</a>, a Python library, which explains how you can easily <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">preprocess your text and train such a classifier</a>.  </p>
",1,0,130,2015-04-10 13:31:59,https://stackoverflow.com/questions/29562798/assign-a-short-text-to-one-of-two-categories-according-to-previous-assignments
SVM for text classification in R,"<p>I am using SVM to classify my text where in i don't actually get the result instead get with numerical probabilities.</p>

<p><strong>Dataframe (1:20 trained set, 21:50 test set)</strong></p>

<p><strong>Updated:</strong></p>

<pre><code>     ou &lt;- structure(list(text = structure(c(1L, 6L, 1L, 1L, 8L, 13L, 24L, 
5L, 11L, 12L, 33L, 36L, 20L, 25L, 4L, 19L, 9L, 29L, 22L, 3L, 
8L, 8L, 8L, 2L, 8L, 27L, 30L, 3L, 14L, 35L, 3L, 34L, 23L, 31L, 
22L, 6L, 6L, 7L, 17L, 3L, 8L, 32L, 18L, 15L, 21L, 26L, 3L, 16L, 
10L, 28L), .Label = c(""access, access, access, access"", ""character(0)"", 
""report"", ""report, access"", ""report, access, access"", ""report, access, access, access"", 
""report, access, access, access, access, access, access"", ""report, access, access, access, access, access, access, access"", 
""report, access, access, access, access, access, access, report"", 
""report, access, access, access, access, access, report"", ""report, access, access, access, report"", 
""report, access, access, access, report, access"", ""report, access, access, report, access, access, access, access, access, access"", 
""report, data"", ""report, data, data"", ""report, data, data, data"", 
""report, data, data, data, data"", ""report, data, data, data, data, data"", 
""report, data, data, data, report, report, data, access,access"", 
""report, data, data, report"", ""report, data, report"", ""report, report"", 
""report, report, access, access, access"", ""report, report, access, access, report, report, report, report, report, report, data, data, report, access, report, report"", 
""report, report, access, report, report, report, report, report, data, data, report, access, report, report"", 
""report, report, access, report, report, report, report, report, report, data, data, report, access, report, report"", 
""report, report, data"", ""report, report, data, report"", ""report, report, report, data, report, report, data, data, report, data, data"", 
""report, report, report, report"", ""report, report, report, report, data, report, report, data, report, data, report"", 
""report, report, report, report, report, data, report, data, data"", 
""report, report, report, report, report, report, report"", ""report, report, report, report, report, report, report, access, access, access"", 
""report, report, report, report, report, report, report, report, data, data, report, access, report, report"", 
""report, report, report, report, report, report, report, report, report, report, data, report, report, report, report, report, report, report,report""
), class = ""factor""), value = structure(c(2L, 2L, 2L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 3L, 2L, 3L, 3L, 3L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("""", 
""Access"", ""Report/Data""), class = ""factor"")), .Names = c(""text"", 
""value""), class = ""data.frame"", row.names = c(NA, -50L))
</code></pre>

<h2><strong>Code used:</strong></h2>

<pre><code>        library(RTextTools)

        doc_matrix &lt;- create_matrix(ou$text, language=""english"", removeNumbers=TRUE, stemWords=TRUE, removeSparseTerms=.998)

        #container &lt;- create_container(doc_matrix, ou$text, trainSize=1:20, testSize=21:50, virgin=FALSE)
        container &lt;- create_container(doc_matrix, as.numeric(factor(ou$text)), trainSize=1:20, testSize=21:50, virgin=FALSE)

        #Training models
        SVM &lt;- train_model(container,""SVM"")
        MAXENT &lt;- train_model(container,""MAXENT"")
        BAGGING &lt;- train_model(container,""BAGGING"")
        TREE &lt;- train_model(container,""TREE"")

        #Classify data using trained models
        SVM_CLASSIFY &lt;- classify_model(container, SVM)
        MAXENT_CLASSIFY &lt;- classify_model(container, MAXENT)
        BAGGING_CLASSIFY &lt;- classify_model(container, BAGGING)

        #Analytics

        analytics &lt;- create_analytics(container,SVM_CLASSIFY)

        models &lt;- train_models(container, algorithms=c(""MAXENT"",""SVM""))
        results &lt;- classify_models(container, models)
        analytics &lt;- create_analytics(container, results)
        summary(analytics)
        SVM &lt;- cross_validate(container, 5, ""SVM"")
        write.csv(analytics@document_summary, ""DocumentSummary.csv"")
</code></pre>

<h2><strong>expected result:</strong></h2>

<pre><code>          text                                                          value
     21 report, access, access, access, access, access, access, access       Access
     22 report, access, access, access, access, access, access, access       Access
     23 report, access, access, access, access, access, access, access       Access
     24 character(0)                                                          NA
     25 report, access, access, access, access, access, access, access       Access
     26 report, report, data                                             Report/Data
     27 report, report, report, report                                   Report/Data
     28 report                                                          Report/Data
     29 report, data                                                    Report/Data
     30 report, report, report, report, report, report, report, report,
         data, data, report, access, report, report                      Report/Data
</code></pre>

<p>the result where probabilities are :</p>

<pre><code>&gt;   MAXENTROPY_LABEL    MAXENTROPY_PROB SVM_LABEL   SVM_PROB    MANUAL_CODE CONSENSUS_CODE  CONSENSUS_AGREE CONSENSUS_INCORRECT PROBABILITY_CODE    PROBABILITY_INCORRECT
&gt; 1 8   0.999999066 22  0.070090645 8   8   1   0   8   0
&gt; 2 8   0.999999066 22  0.070090645 8   8   1   0   8   0
&gt; 3 8   0.999999066 22  0.070090645 8   8   1   0   8   0
&gt; 4 1   0.055555556 12  0.071384112 2   12  1   1   12  1
&gt; 5 8   0.999999066 22  0.070090645 8   8   1   0   8   0
&gt; 6 25  1   12  0.074126949 27  25  1   1   25  1
&gt; 7 33  0.627904676 13  0.068572857 30  33  1   1   33  1
&gt; 8 33  0.406792176 12  0.074592181 3   33  1   1   33  1
&gt; 9 20  1   12  0.074507793 14  20  1   1   20  1
</code></pre>

<p>EDIT 1:
How can i achieve the <strong>label names</strong> instead of SVM label numbers. </p>
","r, svm, text-classification, data-science","<p>What I usually do is</p>

<pre><code>ou &lt;- cbind(ou$text, results)
</code></pre>

<p>And to have the labels printed:</p>

<pre><code>ou$value &lt;- ""NONE""
ou$value[results$SVM_LABEL==""1""]  &lt;- ""Access""
ou$value[results$SVM_LABEL==""-1""] &lt;- ""Report/Data""
ou 
</code></pre>

<p><em>(assuming you used 1 and -1 when training the model)</em></p>

<p>I know it's a little bit primitive but it's clear and works fine</p>
",0,2,2757,2015-04-17 07:11:30,https://stackoverflow.com/questions/29692571/svm-for-text-classification-in-r
How to correctly override and call super-method in Python,"<p>First, the problem at hand. I am writing a wrapper for a <code>scikit-learn</code> class, and am having problems with the right syntax. What I am trying to achieve is an override of the <code>fit_transform</code> function, which alters the input only slightly, and then calls its <code>super</code>-method with the new parameters:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

class TidfVectorizerWrapper(TfidfVectorizer):
    def __init__(self):
        TfidfVectorizer.__init__(self)  # is this even necessary?

    def fit_transform(self, x, y=None, **fit_params):
        x = [content.split('\t')[0] for content in x]  # filtering the input
        return TfidfVectorizer.fit_transform(self, x, y, fit_params)  
                            # this is the critical part, my IDE tells me for
                            # fit_params: 'unexpected arguments'
</code></pre>

<p>The Program crashes all over the place, starting with a <code>Multiprocessing exception</code>, not really telling me anything usefull. How do I correctly do this?</p>

<p>Additional info: The reason why I need to wrap it this way is because I use <code>sklearn.pipeline.FeatureUnion</code> to collect my feature extractors before putting them into a <code>sklearn.pipeline.Pipeline</code>. A consequence of doing it this way is, that I can only feed a single data set across all feature extractors -- but different extractors need different data. My solution was to feed the data in an easily separable format and filtering different parts in different extractors. If there is a better solution to this problem, I'd also be happy to hear it.</p>

<p>Edit 1:
Adding <code>**</code> to unpack the dict seems to not change anything: 
<img src=""https://i.sstatic.net/Fo7JN.png"" alt=""Screenshot""></p>

<p>Edit 2:
I just solved the remaining problem -- I needed to remove the constructor overload. Apparently, by trying to call the parent constructor, wishing to have all instance variables initiated correctly, I did the exact opposite. My wrapper had no idea what kind of parameters it can expect. Once I removed the superfluous call, everything worked out perfectly.</p>
","python, scikit-learn, classification, text-classification","<p>You forget to unpack <code>fit_params</code> which is passed as a <code>dict</code> and you want to pass it through as a <code>keyword arguments</code> which require unpacking operator <code>**</code>. </p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

class TidfVectorizerWrapper(TfidfVectorizer):

    def fit_transform(self, x, y=None, **fit_params):
        x = [content.split('\t')[0] for content in x]  # filtering the input
        return TfidfVectorizer.fit_transform(self, x, y, **fit_params)  
</code></pre>

<p>one other thing that instaed of calling the <code>TfidfVectorizer</code>'s <code>fit_transform</code> directly you can call the overloaded version through <code>super</code> method</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

class TidfVectorizerWrapper(TfidfVectorizer):

    def fit_transform(self, x, y=None, **fit_params):
        x = [content.split('\t')[0] for content in x]  # filtering the input
        return super(TidfVectorizerWrapper, self).fit_transform(x, y, **fit_params)  
</code></pre>

<p>To understand it check the following example</p>

<pre><code>def foo1(**kargs):
    print kargs

def foo2(**kargs):
    foo1(**kargs)
    print 'foo2'

def foo3(**kargs):
    foo1(kargs)
    print 'foo3'

foo1(a=1, b=2)
</code></pre>

<p>it prints the dictionary <code>{'a': 1, 'b': 2}</code></p>

<pre><code>foo2(a=1, b=2)
</code></pre>

<p>prints both dictionary and <code>foo2</code>, but</p>

<pre><code>foo3(a=1, b=2)
</code></pre>

<p>raises error as we sent an <strong>positional argument</strong> equal to our dictionary to <code>foo1</code>, which does not accept such a thing. We could however do</p>

<pre><code>def foo4(**kargs):
    foo1(x=kargs)
    print 'foo4'
</code></pre>

<p>which works fine, but prints a new dictionary <code>{'x': {'a': 1, 'b': 2}}</code></p>
",6,4,1872,2015-04-22 15:34:46,https://stackoverflow.com/questions/29802368/how-to-correctly-override-and-call-super-method-in-python
In what order does .find() return MongoDB documents?,"<p>I was wondering if there is a certain ""order"" of some sort in which MongoDB documents are returned when querying like this:</p>

<pre><code>collection.find()
</code></pre>

<p>Is it always the same, given that <em>collection</em> does not change? Does MongoDB have some kind of order? There is an empty query, so it will just retrieve any document.
I'm asking because this is for classification. I want to retrieve a bunch of documents to train a model on. The test set cannot include documents from that same set, so I do this:</p>

<pre><code>trainset = collection.find().limit(train_set_size)
testset = collection.find().skip(train_set_size).limit(test_set_size)
</code></pre>

<p>So both sets will have absolutely no overlap.</p>

<p>Any thoughts?</p>

<p>Thanks!</p>
","python, mongodb, document, text-classification","<p>I think <code>collection.find()</code> return based on <code>id</code>. In <code>mongodb</code> <code>ObjectId</code> is based on time.So it should be <code>LIFO</code>(Last in First Out).So sorting is based on <code>time</code>.</p>

<p>In your set, there should be no overlap</p>
",1,1,1029,2015-04-27 12:56:28,https://stackoverflow.com/questions/29896459/in-what-order-does-find-return-mongodb-documents
Authorship Attribution using Machine Learning,"<p>I am working on a practical machine learning problem as an exercise. I just need help formulating my problem.</p>

<p>I have text from 20 books of a famous old Author. there are 5 more books that has been debated throughout history if the belong to the same author or not. </p>

<p>I am thinking about the best way to represent this problem. I am thinking of using a bag-of-words appoach to find the most significant words used by the author. </p>

<p>Should I treat it as a Naive Bayes (Spam/Ham) problem, or should I use KNN classification (Author/non-author) to detect the class of each document. Is there another way of doing it?</p>
","machine-learning, classification, text-mining, text-classification, document-classification","<p>I think Naive Bayes can give you insights. One more way can be , find out features which separate such books ex<br>
1. Complexity of words , some writers are easy to understand and use common words , i am hinting towards IDF (Inverse document frequency)<br>
2. Some words may not not even exist at his time like ""selfie"" , ""mobile"" etc.  </p>

<p>Try to find a lot of features like that and can also train a discriminative classifier. </p>
",0,-2,119,2015-04-28 15:47:09,https://stackoverflow.com/questions/29923983/authorship-attribution-using-machine-learning
"Weka ,Text Classification on an arff file","<p>.This is a basic question .I am trying to classify text files into 20 different classes.</p>

<p>Therefore I have a project structure with a folder called train,test.
In the train folder I have 20 different folders ,each folder again has many files related to that particular class.ex:weather, atheism...etc</p>

<p>I have now created a train.arff file for the entire train folder.When the data is visualized through I can see only two attributes .
Have provided a link below:</p>

<p><a href=""https://www.dropbox.com/s/7tgnhou1gba9yjn/doubt.jpg?dl=0"" rel=""nofollow"">Screen in weka</a></p>

<p>My Doubt is how can i view the various files under these folders and remove the stopwords,punctuation,stemmin.How do I go about preprocessing.If some links to good resources are available please suggest and provide the necessary links</p>
","weka, text-classification","<p>I found the videos below quite helpful when I first got my hands on text classification using Weka. You might want to take a look.</p>

<ul>
<li><a href=""https://www.youtube.com/watch?v=jSZ9jQy1sfE"" rel=""nofollow"">Weka Tutorial 31: Document Classification 1 (Application)</a></li>
<li><a href=""https://www.youtube.com/watch?v=zlVJ2_N_Olo"" rel=""nofollow"">Weka Tutorial 32: Document classification 2 (Application)</a></li>
<li><a href=""https://www.youtube.com/watch?v=IY29uC4uem8"" rel=""nofollow"">WEKA Text Classification for First Time &amp; Beginner Users</a></li>
</ul>

<p>You might want to use StringToWordVector filter to see the effect of each word as an attribute, which is indeed described in detail in the first and last video . Within the filter settings you can give a stopwords list and choose in each run to use it or not. Same with the stemming you can change it as well. This <a href=""https://weka.wikispaces.com/Text+categorization+with+WEKA"" rel=""nofollow"">documentation</a> and videos will get you to understand it easily.</p>
",0,0,1918,2015-04-29 06:59:59,https://stackoverflow.com/questions/29936450/weka-text-classification-on-an-arff-file
How to perform text classification with naive bayes using sklearn library?,"<p>I am trying text classification using naive bayes text classifier.
My data is in the below format and based on the question and excerpt i have to decide the topic of the question. The training data is having more than 20K records. I know SVM would be a better option here but i want to go with <a href=""http://scikit-learn.org/stable/modules/naive_bayes.html"" rel=""nofollow"">Naive Bayes using sklearn library</a>.</p>

<pre><code>{[{""topic"":""electronics"",""question"":""What is the effective differencial effective of this circuit"",""excerpt"":""I'm trying to work out, in general terms, the effective capacitance of this circuit (see diagram: https://i.sstatic.net/BS85b.png).  \n\nWhat is the effective capacitance of this circuit and will the ...\r\n        ""},
{""topic"":""electronics"",""question"":""Outlet Installation--more wires than my new outlet can use [on hold]"",""excerpt"":""I am replacing a wall outlet with a Cooper Wiring USB outlet (TR7745).  The new outlet has 3 wires coming out of it--a black, a white, and a green.  Each one needs to be attached with a wire nut to ...\r\n        ""}]}
</code></pre>

<p>This is what i have tried so far,</p>

<pre><code>import numpy as np
import json
from sklearn.naive_bayes import *

topic = []
question = []
excerpt = []

with open('training.json') as f:
    for line in f:
        data = json.loads(line)
        topic.append(data[""topic""])
        question.append(data[""question""])
        excerpt.append(data[""excerpt""])

unique_topics = list(set(topic))
new_topic = [x.encode('UTF8') for x in topic]
numeric_topics = [name.replace('gis', '1').replace('security', '2').replace('photo', '3').replace('mathematica', '4').replace('unix', '5').replace('wordpress', '6').replace('scifi', '7').replace('electronics', '8').replace('android', '9').replace('apple', '10') for name in new_topic]
numeric_topics = [float(i) for i in numeric_topics]

x1 = np.array(question)
x2 = np.array(excerpt)
X = zip(*[x1,x2])
Y = np.array(numeric_topics)
print X[0]
clf = BernoulliNB()
clf.fit(X, Y)
print ""Prediction:"", clf.predict( ['hello'] )
</code></pre>

<p>But as expected i am getting ValueError: could not convert string to float. My question is how can i create a simple classifier to classify the question and excerpt into related topic ?</p>
","python, machine-learning, scikit-learn, text-classification, naivebayes","<p>All classifiers in sklearn require input to be represented as vectors of some fixed dimensionality. For text there are <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""noreferrer""><code>CountVectorizer</code></a>, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html"" rel=""noreferrer""><code>HashingVectorizer</code></a> and <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""noreferrer""><code>TfidfVectorizer</code></a> which can transform your strings into vectors of floating numbers.</p>

<pre><code>vect = TfidfVectorizer()
X = vect.fit_transform(X)
</code></pre>

<p>Obviously, you'll need to vectorize your test set in the same way</p>

<pre><code>clf.predict( vect.transform(['hello']) )
</code></pre>

<p>See a <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""noreferrer"">tutorial on using sklearn with textual data</a>.</p>
",5,1,3182,2015-05-05 11:39:50,https://stackoverflow.com/questions/30051977/how-to-perform-text-classification-with-naive-bayes-using-sklearn-library
Python NLTK named entity recognition depends by the (upper)case of first letter?,"<p>I am planning on using Python NLTK for academic research. In particular, I need a way of screening the Twitter users and tease out the ones who do not seem to be using a ""real name"" in their profile.</p>

<p>I am thinking about using the default NLTK's name-entity recognition to separate the Twitter users who use seemingly real name from those who aren't. Do you think it's worth the try? Or should I train the classifier by myself?</p>

<pre><code>import nltk
import re
import time

##contentArray0 =['Health Alerts', ""Kenna Hill""]

contentArray =['ICU nurse toronto']

##let the fun begin!##
def processLanguage():
    try:
        for item in contentArray:
            tokenized = nltk.word_tokenize(item)
            tagged = nltk.pos_tag(tokenized)
            print tagged

            namedEnt = nltk.ne_chunk(tagged)
            ##namedEnt.draw()

            time.sleep(1)

    except Exception, e:
        print str(e)


processLanguage()
</code></pre>

<p>Edit: I have done a bit of testing. It seems nltk recognizes a name entity primarily by whether or not the first letter of the word is capital? For example, ""ICU Nurse Toronto"" will be recognized with NNP while ""ICU nurse toronto"" will not. It seems overly-simplistic and not very useful for my purpose (twitter) since many Twitter users using real name could be using lower case while some commercial organization will be using capital first letter.</p>
","python-2.7, classification, nltk, text-classification","<p>Definitely train one yourself. The NLTK's NE recognizer is trained to recognize named entities embedded in full sentences. But don't just retrain the nltk's NE recognizer on new data; it is a ""sequential classifier"", meaning it takes into account the surrounding words and POS tags and the named-entity classification of the preceding words. Since you already have the usernames, these will not be useful or relevant for your purposes. </p>

<p>I suggest you train a regular classifier (e.g., Naive Bayes), feed it whatever custom features you think might be relevant, and ask it to decide ""is this a real name"". To train, you <em>must</em> have a training corpus that contains examples of names and examples of non-names. Ideally the corpus should consist of what you're trying to classify: twitter handles. </p>

<p>Re the question in your comment, don't use entire words as features: your classifier can only reason with features it knows about, so census names can't help you with novel names unless your features are about <em>parts</em> of the name. Usually the features represent the endings (last letter, final bigram, final trigram), but you can try other things too like length and of course capitalization. The NLTK chapter discusses the task of recognizing the gender of names, and gives many examples of suffix features.</p>

<p>The catch, in your case, is that you have multiple words. So your classifier needs to be told somehow if some words are recognized as names and some are not. Somehow you must define your features in a way that preserves this information. E.g., you could set the feature ""known names"" to have the values ""None"", ""One"", ""Several"", ""All"". (Note that the NLTK's implementation treats feature values as ""categories"": They are simply distinct values. You can use 3 and 4 as feature values, but as far as the classifier is concerned you might as well have used ""green"" and ""elevator"".)</p>

<p>And don't forget to add a ""bias"" feature with constant value (see the NLTK chapter).</p>
",4,3,2316,2015-05-08 21:21:01,https://stackoverflow.com/questions/30133043/python-nltk-named-entity-recognition-depends-by-the-uppercase-of-first-letter
I&#39;m not sure how to interpret accuracy of this classification with Scikit Learn,"<p>I am trying to classify text data, with Scikit Learn, with the method shown here. (<a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a>) except I am loading my own dataset.</p>

<p>I'm getting results, but I want to find the accuracy of the classification results. </p>

<pre><code>    from sklearn.datasets import load_files

    text_data = load_files(""C:/Users/USERNAME/projects/machine_learning/my_project/train"", description=None, categories=None, load_content=True, shuffle=True, encoding='latin-1', decode_error='ignore', random_state=0)

    from sklearn.pipeline import Pipeline
    from sklearn.linear_model import SGDClassifier
    text_clf = Pipeline([('vect', CountVectorizer()),
                        ('tfidf', TfidfTransformer()),
                        ('clf', LinearSVC(loss='hinge', penalty='l2',
                                                random_state=42)),
    ])

    _ = text_clf.fit(text_data.data, text_data.target)

    docs_new = [""Some test sentence here."",]

    predicted = text_clf.predict(docs_new)
    print np.mean(predicted == text_data.target) 

    for doc, category in zip(docs_new, predicted):
        print('%r =&gt; %s' % (doc, text_data.target_names[predicted]))
</code></pre>

<p>Here, I get the np.mean prediction as 0.566.</p>

<p>If I try:</p>

<pre><code>twenty_test = load_files(""C:/Users/USERNAME/projects/machine_learning/my_project/testing"", description=None, categories=None, load_content=True, shuffle=True, encoding='latin-1', decode_error='ignore', random_state=0)
docs_test = twenty_test.data
predicted = text_clf.predict(docs_test)
np.mean(predicted == twenty_test.target)
</code></pre>

<p>Now it prints it out as 1.</p>

<p>I don't understand how this works, and what exactly np.mean is, and why it's showing different results when it's trained on the same data.</p>

<p>The ""train"" folder has approx 15 documents, and the text folder also has approx 15 documents, in case that matters. I'm very new to Scikit Learn and machine learning in general, so any help greatly appreciated. Thanks!</p>
","python, machine-learning, scikit-learn, classification, text-classification","<pre><code>text_data = load_files(""C:/Users/USERNAME/projects/machine_learning/my_project/train"", ...)
</code></pre>

<p>According to <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow"">the documentation</a>, that line loads your file's contents from <code>C:/Users/USERNAME/projects/machine_learning/my_project/train</code> into <code>text_data.data</code>. It will also load target labels (represented by their integer indexes) for each document into <code>text_data.target</code>. So <code>text_data.data</code> should be a list of strings and <code>text_data.target</code> a list of integers. The labels are derived from the folders in which the files are. Your explanation sounds like you don't have any subfolders in <code>C:/.../train/</code> and <code>C:/.../test/</code>, which will probably create problems (e. g. all labels being identical).</p>

<pre><code>from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
text_clf = Pipeline([('vect', CountVectorizer()),
                    ('tfidf', TfidfTransformer()),
                    ('clf', LinearSVC(loss='hinge', penalty='l2',
                                            random_state=42)),
])

_ = text_clf.fit(text_data.data, text_data.target)
</code></pre>

<p>The above lines are training (in <code>.fit()</code>) a classifier on your example documents. Very roughly speaking, you are telling the classifier (<code>LinearSVC</code>) how often which words appear in which documents (<code>CountVectorizer</code>, <code>TfidfTransformer</code>) and which label each of these documents has (<code>text_data.target</code>). Your classifier then tries to learn a rule that basically maps those word frequencies (TF-IDF values) to labels (e. g. <code>dog</code> and <code>cat</code> strongly indicating the label <code>animal</code>).</p>

<pre><code>docs_new = [""Some test sentence here."",]
predicted = text_clf.predict(docs_new)
</code></pre>

<p>After training your classifier on example data, you provide one completely new document and let your classifier predict the most appropriate label for that document based on what it has learned. <code>predicted</code> should then be a list of (indexes of) labels with just one element (because you had one document), e. g. <code>[5]</code>.</p>

<pre><code>print np.mean(predicted == text_data.target)
</code></pre>

<p>Here you are comparing the list of predictions (1 element) to the list of labels from your training data (15 elements) and then take the mean of the result. That doesn't make a whole lot of sense, because of the different list sizes and because your new example document doesn't really have anything to do with the training labels. Numpy will probably resort to comparing your predicted label (e. g. <code>5</code>) to every single element in <code>text_data.target</code>. That will create a list like <code>[False, False, False, True, False, True, ...]</code>, which will be interpreted by <code>np.mean</code> as <code>[0, 0, 0, 1, 0, 1, ...</code>], resulting in the mean being <code>1/15 * (0+0+0+1+0+1+...)</code>.</p>

<p>What you should be doing is e. g. something like:</p>

<pre><code>docs_new = [""Some test sentence here.""]
docs_new_labels = [1] # correct label index of the document

predicted = text_clf.predict(docs_new)
print np.mean(predicted == docs_new_labels) 
</code></pre>

<p>At least you shouldn't compare to your training labels.
Notice that if <code>np.mean</code> returns <code>1</code> then all documents where correctly classified. In the case of your test dataset the seems to happen. Make sure that your test and training data files are actually different, as 100% accuracy isn't very common (might however be an artifact of your low amount of training files). On a sidenote, notice that are currently not using tokenization, so for your classifier <code>here</code> and <code>here.</code> will be completely different words.</p>
",0,1,1880,2015-05-19 11:14:23,https://stackoverflow.com/questions/30324045/im-not-sure-how-to-interpret-accuracy-of-this-classification-with-scikit-learn
Implementing Naive Bayes text categorization but I keep getting zeros,"<p>I am using Naive Bayes for text categorization this is how I created the initial weights for each term in the specified category:</p>

<ul>
<li>term1:number of times term 1 exists/number of documents in categoryA</li>
<li>term2:number of times term 2 exists/number of documents in categoryA</li>
<li><p>term3:number of times term 3 exists/number of documents in categoryA</p></li>
<li><p>term1:number of times term 1 exists/number of documents in categoryB</p></li>
<li>term2:number of times term 2 exists/number of documents in categoryB</li>
<li>term3:number of times term 3 exists/number of documents in categoryB</li>
</ul>

<p>with the new test document I adjust the weights based on whether the term exists in the test document or not:</p>

<ul>
<li>term1: exists in the test document so I use the same weight for categoryA_term1 as above</li>
<li>term2: does NOT exist in the test document so I use the 1-weight for categoryA_term2</li>
<li><p>term3: does NOT exist in the test document so I use the 1-weight for categoryA_term3</p></li>
<li><p>term1: exists in the test document so I use the same weight for categoryB_term1 as above</p></li>
<li>term2: does NOT exist in the test document so I use the 1-weight for categoryB_term2</li>
<li>term3: exists in the test document so I use the same weight for categoryB_term2 as above</li>
</ul>

<p>Then I multiply the weights for each category. 
This works when I create dummy train/test documents of one sentence each but when I implement real documents for train/test documents I keep getting zero when I multiple it all together. Is this because the probabilities are so small that after multiplying so many small numbers, python just converges to zero??
I am so stuck and I just keep running into the same zero issue :( I would really appreciate your help!</p>
","python, algorithm, nlp, text-classification, naivebayes","<p>As Ed Cottrell commented, you need to consider what happens if you encounter a word that is not in the documents in a category. You can avoid multiplying by 0 by using <a href=""http://en.wikipedia.org/wiki/Additive_smoothing"" rel=""nofollow"">Laplace smoothing</a>. If you see a word in k out of n documents in a category, you assign the conditional probability (k+1)/(n+2) or (k+a)/(n+2a) to that word given the category. </p>

<p>Instead of taking a product of many small numbers, it is standard to compute the logarithm of the product. </p>

<pre><code>log x*y = log x + log y
log(P(a0|c) * P(a1|c) * ... * P(ak|c))
    = log P(a0|c) + log P(a1|c) + ... + log P(ak|c)
</code></pre>

<p>Then you have a sum of numbers that are not so small. Avoid using log 0. You can exponentiate afterwards if necessary, but usually you just translate your decision threshold into a condition on the logarithm.</p>
",1,1,425,2015-05-23 17:28:44,https://stackoverflow.com/questions/30415636/implementing-naive-bayes-text-categorization-but-i-keep-getting-zeros
classify keywords into fields,"<p>Is there an existing resource (dictionary/database or something else) that can help classify keywords into different categories of fields and can be integrated in programming? For example, when seeing ""arson"", classify it as ""Crime""; when seeing ""injection"", classify it as ""Health Care"", something like this.</p>
","keyword, text-classification","<p>This is not a so trivial problem. In fact, it's a standard question in Machine Learning, so there isn't a dirty easy solution.</p>
",0,0,73,2015-05-27 21:56:42,https://stackoverflow.com/questions/30493833/classify-keywords-into-fields
NaiveBayes Classifier: Do I have to concatenate all files of one class?,"<p>I am implementing a simple Naive Bayes classifier but I did not understand how to properly calculate the <strong>class conditional probability (P(d|c))</strong>. Just for completeness I shortly would like to explain the used terminology. Naive Bayes probabilities are computed by: </p>

<p><img src=""https://i.sstatic.net/mAJZ8.gif"" alt=""enter image description here""></p>

<p>c denotes an arbitrary class while d is a document. Let x = {x1,x2,...,xn} be a list of n features  e.g. 50 most frequent bigrams).</p>

<p>In my training set there are i classes (represented by a folder called c_i) and each of them has k documents (represented by normal text files).</p>

<p>The <strong>a-priori</strong> probability P(c) can be calculated easily: </p>

<p><img src=""https://i.sstatic.net/W8IXP.gif"" alt=""enter image description here""></p>

<p>Now I want to calculate <strong>P(d|c)</strong>. This should be done by</p>

<p><img src=""https://i.sstatic.net/vRumx.gif"" alt=""enter image description here""></p>

<p>Now I don't understand well how to compute P(x_i|c). I take feature x_i (let's say bigram ""th"") and now check how often it appears in class c. But how do I do it? Each class is represented by k documents. Do I have to concatenate all those files? Later I certaintly have to divide by ""total  count of all features"". Would this be the frequency of bigram ""th"" in all (concatenated) documents?</p>
","machine-learning, classification, text-classification, naivebayes","<p>The Bayes approach makes the assumption that a document is a set of words that were independently drawn from some probability distribution. Based on this independence assumption, you can indeed concatenate all the documents in a class and use the word frequencies of the class documents union as your estimate of the class probability distribution.</p>
",0,1,100,2015-06-14 17:17:56,https://stackoverflow.com/questions/30832292/naivebayes-classifier-do-i-have-to-concatenate-all-files-of-one-class
How to identifying the exact instances that are wrongly classified in weka,"<p>Here is my code, I'm using weka API. I want to printout wrongly classified instances and instances that are classified accurately. please help me, or tell me about any other text classification java API which is capable of doing what I want.</p>

<pre><code>    public void evaluation() throws Exception{
    BufferedReader reader=null;
    reader= new BufferedReader(new FileReader(""SparseDTM.arff""));

    Instances train= new Instances(reader);
    train.setClassIndex(0);
    train.toSummaryString();
    reader.close();
    SMO svm=new SMO();
    svm.buildClassifier(train);

    NaiveBayes nB = new NaiveBayes();
    nB.buildClassifier(train);

    weka.classifiers.Evaluation eval= new weka.classifiers.Evaluation(train);
    eval.crossValidateModel(nB, train,10,new Random(1));
    //eval.crossValidateModel(nB, train,10,new Random(1), new Object[] { });

    System.out.println(""\n\t************Results by Naive Bayes Classifier************\n"");
    System.out.println(eval.toSummaryString("""", true));
    System.out.println(eval.toClassDetailsString());
//  System.out.println(""F Measure: ""+eval.fMeasure(1) + "" "" + ""Precision: ""+eval.precision(1) + "" "" + ""Precision: ""+eval.recall(1));
//  System.out.println(""Correct :"" + eval.correct());
//  System.out.println(""Weighted True Negative Rate: "" + eval.weightedTrueNegativeRate());
//  System.out.println(""Weighted False Positive Rate:"" + eval.weightedFalsePositiveRate());
//  System.out.println(""Weighted False Negative Rate:"" + eval.weightedFalseNegativeRate());
//  System.out.println(""Weighted True Positive Rate:"" + eval.weightedTruePositiveRate());
    System.out.println(eval.toMatrixString());
    }
</code></pre>
","weka, text-classification","<p>Below is a method that will help you to solve your problem. So, You can edit it to reach your aim. </p>

<pre><code>public void showPredictions(  ){    

    BufferedReader reader=null;
    reader= new BufferedReader(new FileReader(""SparseDTM.arff""));

    Instances data = new Instances(reader);

    double[] predictions;
    try {

        NaiveBayes classifier = new NaiveBayes();
        classifier.buildClassifier(data);

        predictions = eval.evaluateModel(classifier, data );

        int classIndex = data.numAttributes()-1;
        // getting the array of predictions for each instance
        System.out.println(""predictions: "");
        for (int i=0; i &lt; data.numInstances(); i++ ) {
            double realValue = testData.instance(i).classValue(); // test or train data.
            System.out.print(""Real Value: "" + testData.instance(i).stringValue( classIndex ));
            System.out.println(""\tClassification predicted value: "" + predictions[i]);

            if( realValue != predictions[i] ) {
                System.out.println(""misclassified instance: "" + testData.instance(i).toString());
            }
        }       
    } catch (Exception e) {
        e.printStackTrace();
    }
}
</code></pre>

<p>Replace ""testData"" by ""data"" if you can observe misclassified instances related to trainning set. Otherwise, you must provide a test set.</p>
",0,0,334,2015-06-19 18:20:48,https://stackoverflow.com/questions/30944848/how-to-identifying-the-exact-instances-that-are-wrongly-classified-in-weka
Creating a variable directly after rails server loads,"<p>I am trying to utilize the <a href=""https://github.com/cardmagic/classifier"" rel=""nofollow"">classifier gem</a> to classify quotes as either ""happy"" or ""sad"". Right now, I have the app configured such that any new POST requests train the classifier by using the following command <code>bayes.train_happy ""I am very happy""</code> in <code>app/controllers/quotes_controller.rb</code></p>

<p>This works whenever adding new quotes while the server is running. However, whenever the server either shuts down or restarts (constantly during development), the variables <code>bayes</code> gets deleted, such that when I restart the server the quotes are saved in the database, but <code>bayes</code> becomes empty.</p>

<p>Essentially, I want to be able to call the following block of code directly after the server starts, such that a user can call the classifier that has already been built directly after the server starts.</p>

<pre><code>bayes = Classifier::Bayes.new 'happy', 'sad'

Quote.all.each do |quote|
    eval(""bayes.train_#{quote.classification} quote.body.upcase"")
    puts ""wrote {#{quote.body}} to bayes""
end
</code></pre>

<p>where <code>Quote</code> is defined as the following in <code>db/schema.rb</code></p>

<pre><code>create_table ""quotes"", force: true do |t|
    t.string   ""body""
    t.string   ""classification""
    t.datetime ""created_at""
    t.datetime ""updated_at""
end
</code></pre>
","ruby-on-rails, ruby, initialization, text-classification","<p>Create any file in <code>initializers</code> folder and put your code there. Make sure you have all dependencies included if any. Rails will load and execute it during server loading</p>
",1,0,33,2015-06-22 15:45:17,https://stackoverflow.com/questions/30984296/creating-a-variable-directly-after-rails-server-loads
PredictionIO train error tokens must not be empty,"<p>I am tinkering with predictioIO to build a custom classification engine. I have done this before without issues. But for current dataset <code>pio train</code> is giving me an error <code>tokens must not be empty.</code>I have edited Datasource.scala to mention fields in dataset to engine. A line from my dataset is as below</p>

<pre><code>{""event"": ""ticket"", ""eventTime"": ""2015-02-16T05:22:13.477+0000"", ""entityType"": ""content"",""entityId"": 365,""properties"":{""text"": ""Request to reset svn credentials"",""label"": ""Linux/Admin Task"" }}
</code></pre>

<p>I can import data and build engine without any issues. I am getting a set of observations too. The error is pasted below  </p>

<pre><code>[INFO] [Remoting] Starting remoting
[INFO] [Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.61.44:50713]
[INFO] [Engine$] EngineWorkflow.train
[INFO] [Engine$] DataSource: org.template.textclassification.DataSource@4fb64e14
[INFO] [Engine$] Preparator: org.template.textclassification.Preparator@5c4cc644
[INFO] [Engine$] AlgorithmList: List(org.template.textclassification.NBAlgorithm@62b6c045)
[INFO] [Engine$] Data sanity check is off.
[ERROR] [Executor] Exception in task 0.0 in stage 2.0 (TID 2)
[WARN] [TaskSetManager] Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.IllegalArgumentException: tokens must not be empty
at opennlp.tools.util.StringList.&lt;init&gt;(StringList.java:61)
at org.template.textclassification.PreparedData.org$template$textclassification$PreparedData$$hash(Preparator.scala:71)
at org.template.textclassification.PreparedData$$anonfun$2.apply(Preparator.scala:113)
at org.template.textclassification.PreparedData$$anonfun$2.apply(Preparator.scala:113)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:64)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

[ERROR] [TaskSetManager] Task 0 in stage 2.0 failed 1 times; aborting job
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted   due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent  failure: Lost task 0.0 in stage 2.0 (TID 2, localhost):    java.lang.IllegalArgumentException: tokens must not be empty
at opennlp.tools.util.StringList.&lt;init&gt;(StringList.java:61)
at    org.template.textclassification.PreparedData.org$template$textclassification$PreparedData$$hash(Preparator.scala:71)
at  org.template.textclassification.PreparedData$$anonfun$2.apply(Preparator.scala:113)
at  org.template.textclassification.PreparedData$$anonfun$2.apply(Preparator.scala:113)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at  org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
at  org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)
at  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
at  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:64)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
at  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
</code></pre>

<p>The problem is with dataset. I did splitting up dataset into parts and trained. Training was completed for that dataset and no errors were reported. How can I know which line in the dataset produce error? It should be very very helpful if this feature is in PredictionIO . </p>
","token, text-classification, training-data, predictionio","<p>So this is something that happens when you feed in an empty <code>Array[String]</code> to OpenNLP's <code>StringList</code> constructor. Try modifying the function hash in Prepared Data as follows:</p>

<pre><code>private def hash (tokenList : Array[String]): HashMap[String, Double] = {
// Initialize an NGramModel from OpenNLP tools library,
// and add the list of allowable tokens to the n-gram model.
try {
  val model : NGramModel = new NGramModel()
  model.add(new StringList(tokenList: _*), nMin, nMax)

  val map : HashMap[String, Double] = HashMap(
    model.iterator.map(
      x =&gt; (x.toString, model.getCount(x).toDouble)
    ).toSeq : _*
  )

  val mapSum = map.values.sum

  // Divide by the total number of n-grams in the document
  // to obtain n-gram frequency.
  map.map(e =&gt; (e._1, e._2 / mapSum))
} catch {
  case (e : IllegalArgumentException) =&gt; HashMap("""" -&gt; 0.0)
}
</code></pre>

<p>I've only encountered this issue in the prediction stage, and so you can see this is actually implemented in the models' predict methods. I'll update this right now, and put it in a new version release. Thank you for the catch and feedback!</p>
",1,0,296,2015-06-23 10:19:11,https://stackoverflow.com/questions/31000098/predictionio-train-error-tokens-must-not-be-empty
Scikit-learn&#39;s Pipeline: Error with multilabel classification. A sparse matrix was passed,"<p>I am implementing different classifiers using different machine learning algorithms.</p>

<p>I'm sorting text files, and do as follows:</p>

<pre><code>classifier = Pipeline([
('vectorizer', CountVectorizer ()),
('TFIDF', TfidfTransformer ()),
('clf', OneVsRestClassifier (GaussianNB()))])
classifier.fit(X_train,Y)
predicted = classifier.predict(X_test)
</code></pre>

<p>When I use the algorithm GaussianNB the following error occurs:</p>

<blockquote>
  <p>TypeError: A sparse matrix was passed, but dense data is required. 
  Use X.toarray () to convert to a dense numpy array.</p>
</blockquote>

<p>I saw the following post <a href=""https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required"">here</a></p>

<p>In this post a class is created to perform the transformation of the data.
It is possible to adapt my code with TfidfTransformer.
How I can fix this?</p>
","python, scikit-learn, gaussian, text-classification","<p>You can do the following:</p>

<pre><code>class DenseTransformer(TransformerMixin):
    def transform(self, X, y=None, **fit_params):
        return X.todense()

    def fit_transform(self, X, y=None, **fit_params):
        self.fit(X, y, **fit_params)
        return self.transform(X)

    def fit(self, X, y=None, **fit_params):
        return self

classifier = Pipeline([
('vectorizer', CountVectorizer ()),
('TFIDF', TfidfTransformer ()),
('to_dense', DenseTransformer()), 
('clf', OneVsRestClassifier (GaussianNB()))])
classifier.fit(X_train,Y)
predicted = classifier.predict(X_test)
</code></pre>

<p>Now, as a part of your pipeline, the data will be transform to dense representation.</p>

<p>BTW, I don't know your constraints, but maybe you can use another classifier, such as <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow"">RandomForestClassifier</a> or <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"" rel=""nofollow"">SVM</a> that DO accept data in sparse representation.</p>
",3,2,1201,2015-07-05 08:01:17,https://stackoverflow.com/questions/31228303/scikit-learns-pipeline-error-with-multilabel-classification-a-sparse-matrix-w
sklearn classifier get ValueError: bad input shape,"<p>I have a csv, struct is
<code>CAT1,CAT2,TITLE,URL,CONTENT</code>, CAT1, CAT2, TITLE ,CONTENT are in chinese.</p>

<p>I want train <code>LinearSVC</code> or <code>MultinomialNB</code> with X(TITLE) and   feature(CAT1,CAT2), both get this error. below is my code:<br></p>

<p>PS: I write below code through this example <a href=""https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py"" rel=""noreferrer"">scikit-learn text_analytics</a></p>

<pre><code>import numpy as np
import csv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

label_list = []

def label_map_target(label):
    ''' map chinese feature name to integer  '''
    try:
        idx = label_list.index(label)
    except ValueError:
        idx = len(label_list)
        label_list.append(label)

    return idx


c1_list = []
c2_list = []
title_list = []
with open(csv_file, 'r') as f:
    # row_from_csv is for shorting this example
    for row in row_from_csv(f):
        c1_list.append(label_map_target(row[0])
        c2_list.append(label_map_target(row[1])
        title_list.append(row[2])

data = np.array(title_list)
target = np.array([c1_list, c2_list])
print target.shape
# (2, 4405)
target = target.reshape(4405,2)
print target.shape
# (4405, 2)

docs_train, docs_test, y_train, y_test = train_test_split(
   data, target, test_size=0.25, random_state=None)

# vect = TfidfVectorizer(tokenizer=jieba_tokenizer, min_df=3, max_df=0.95)
# use custom chinese tokenizer get same error
vect = TfidfVectorizer(min_df=3, max_df=0.95)
docs_train= vect.fit_transform(docs_train)

clf = LinearSVC()
clf.fit(docs_train, y_train)
</code></pre>

<p>error:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-24-904eb9af02cd&gt; in &lt;module&gt;()
      1 clf = LinearSVC()
----&gt; 2 clf.fit(docs_train, y_train)

C:\Python27\lib\site-packages\sklearn\svm\classes.pyc in fit(self, X, y)
    198 
    199         X, y = check_X_y(X, y, accept_sparse='csr',
--&gt; 200                          dtype=np.float64, order=""C"")
    201         self.classes_ = np.unique(y)
    202 

C:\Python27\lib\site-packages\sklearn\utils\validation.pyc in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric)
    447                         dtype=None)
    448     else:
--&gt; 449         y = column_or_1d(y, warn=True)
    450         _assert_all_finite(y)
    451     if y_numeric and y.dtype.kind == 'O':

C:\Python27\lib\site-packages\sklearn\utils\validation.pyc in column_or_1d(y, warn)
    483         return np.ravel(y)
    484 
--&gt; 485     raise ValueError(""bad input shape {0}"".format(shape))
    486 
    487 

ValueError: bad input shape (3303, 2)
</code></pre>
","python, scikit-learn, classification, text-classification","<p>Thanks to @meelo, I solved this problem.
As he said: in my code, <code>data</code> is a feature vector, <code>target</code> is target value. I mixed up two things.</p>

<p>I learned that <code>TfidfVectorizer</code> processes data to [data, feature], and each data should map to just one target.</p>

<p>If I want to predict two type targets, I need two distinct targets:</p>

<ol>
<li><code>target_C1</code> with all C1 value</li>
<li><code>target_C2</code> with all C2 value.</li>
</ol>

<p>Then use the two targets and original data to train two classifier for each target. </p>
",9,15,108986,2015-07-09 00:59:27,https://stackoverflow.com/questions/31306390/sklearn-classifier-get-valueerror-bad-input-shape
Why should my training set also be skewed in terms of number of class distribution just because my test set is skewed,"<p>My question is why should my training set also be skewed (number of instances of positive class much fewer compared to negative class) when my test set is also skewed. I read that it is important to maintain the distribution between the classes the same in both training and test set to get the most realistic performance. For example, if my test set has 90%-10% distribution of class instances, should my training set also have the same proportions? </p>

<p>I am finding it difficult to understand why is it important to maintain the proportions of class instances in the training set as present in the test set. </p>

<p>The reason why I find it difficult to understand is don't we want a classifier to just learn the patterns in both the classes? So, should it matter to maintain skewness in the training set just because the test set is skewed?</p>

<p>Any thoughts will be helpful</p>
","machine-learning, neural-network, classification, deep-learning, text-classification","<p>IIUC, you're asking about the rationale for using <a href=""https://en.wikipedia.org/wiki/Stratified_sampling#Advantages"" rel=""nofollow"">Stratified Sampling</a> (e.g., as used in <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html"" rel=""nofollow"">Scikit's <code>StratifiedKFold</code></a>.</p>

<p>Once you've divided your data into train and test sets, you have <em>three</em> datasets to consider:</p>

<ol>
<li>the ""real world"" set, on which your classifier will really run</li>
<li>the train set, on which you'll learn patterns</li>
<li>the test set, which you'll use to evaluate the performance of the classifier</li>
</ol>

<p>(So the uses of 2. + 3. are really just for estimating how things will run on 1, including possibly tuning parameters.)</p>

<p>Suppose your data has some class represented far from uniform - say it appears only 5% of the times it would appear if classes would be generated uniformly. Moreover, you believe that this is not a GIGO case - in the real world, the probability of this class would be about 5%.</p>

<p>When you divide into 2. + 3., you run the chance that things will be skewed relative to 1.:</p>

<ul>
<li><p>It's very possible that the class won't appear 5% of the times (in the train or test set), but rather more or less.</p></li>
<li><p>It's very possible that some of the feature instances of the class will be skewed in the train or test set, relative to 1.</p></li>
</ul>

<p>In these cases, when you make decisions based on the 2. + 3. combination, it's probable that it won't indicate well the effect on 1., which is what you're really after. </p>

<p>Incidentally, I don't think the emphasis is on skewing the train to fit the test, but rather on making the train and test <em>each</em> fit the entire sampled data. </p>
",2,3,1462,2015-07-14 08:50:12,https://stackoverflow.com/questions/31402011/why-should-my-training-set-also-be-skewed-in-terms-of-number-of-class-distributi
Get corresponding classes to predict_proba (GridSearchCV sklearn),"<p>I'm using GridSearchCV and a pipeline to classify some text documents. A code snippet:</p>

<pre><code>clf = Pipeline([('vect', TfidfVectorizer()), ('clf', SVC())])
parameters = {'vect__ngram_range' : [(1,2)], 'vect__min_df' : [2], 'vect__stop_words' : ['english'],
                  'vect__lowercase' : [True], 'vect__norm' : ['l2'], 'vect__analyzer' : ['word'], 'vect__binary' : [True], 
                  'clf__kernel' : ['rbf'], 'clf__C' : [100], 'clf__gamma' : [0.01], 'clf__probability' : [True]} 
grid_search = GridSearchCV(clf, parameters, n_jobs = -2, refit = True, cv = 10)
grid_search.fit(corpus, labels)
</code></pre>

<p>My problem is that when using <code>grid_serach.predict_proba(new_doc)</code> and then wanting to find out what classes the probabilities corresponds to with <code>grid_search.classes_</code>, I get the following error:</p>

<blockquote>
  <p>AttributeError: 'GridSearchCV' object has no attribute 'classes_'</p>
</blockquote>

<p>What have I missed? I thought that if the last ""step"" in the pipeline was a classifier, then the return of GridSearchCV is also a classifier. Hence one can use the attributes of that classifier, e.g. classes_.</p>
","python, scikit-learn, text-classification","<p>As mentioned in the comments above, the <code>grid_search.best_estimator_.classes_</code> returned an error message since it returns a pipeline with no attribute <code>.classes_</code>. However, by first calling the step classifier of the pipeline I was able to use the classes attribute. Here is the solution</p>

<pre><code>grid_search.best_estimator_.named_steps['clf'].classes_
</code></pre>
",10,5,3257,2015-07-20 08:39:30,https://stackoverflow.com/questions/31512076/get-corresponding-classes-to-predict-proba-gridsearchcv-sklearn
Using libsvm in Java for String classification,"<p>Looking around I was not able to find a good way to use libsvm with Java and I still have some open questions:</p>

<p>1) It is possible to use only libsvm or I have to use also weka? If any, what's the difference?</p>

<p>2) When using String type data how can I pass the training set as Strings? I was using matlab for a similar problem for proteins classification and there I just gave the strings to the machine without problem. Is there a way to do this in Java?</p>

<p>Here is an incomplete example of what I did in matlab (it works):</p>

<pre><code>[~,posTrain] = fastaread('dataset/1.25.1.3_d1ilk__.pos-train.seq');
[~,posTest] = fastaread('dataset/1.25.1.3_d1ilk__.pos-test.seq');
trainKernel = spectrumKernel(trainData,k);
testKernel =  spectrumKernel(testData,k);
trainKf =[(1:length(trainData))', trainKernel];
testKf = [(1:length(testData))', testKernel];
disp('custom');
model = libsvmtrain(trainLabel,trainKf,'-t 4');
[~, accuracy, ~] = libsvmpredict(testLabel,testKf,model)
</code></pre>

<p>As you can see I read the file in fasta format and feed them to libsvm but libsvm for java look like it wants something called Node that is made of double. What I did is to take byte[] from the String and then transform them into Double. Is it correct?</p>

<p>3) How to use a custom kernel? I've found this line of code </p>

<pre><code> KernelManager.setCustomKernel(custom_kernel);      
</code></pre>

<p>but with my libsvm.jar I don't find. Which lib do I have to use? </p>

<p>Sorry for the multiple questions, I hope you will give me a brief overview of what is going on here.
Thanks.</p>
","java, weka, libsvm, text-classification","<p>Please note that I've used LIBSVM for MATLAB, but not for Java. I can only really answer question 1, but hopefully this still helps:</p>

<ol>
<li>It definitely is possible to use libsvm only, and the code is located here: <a href=""https://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""nofollow"">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</a>. Note that jlibsvm is a port of libsvm, and it seems to be easier to use and more optimized for Java. As far as I can tell, weka just has a wrapper class that runs libsvm anyways (it even requires the libsvm.jar), though I mainly based it off of this: <a href=""https://weka.wikispaces.com/LibSVM"" rel=""nofollow"">https://weka.wikispaces.com/LibSVM</a>.</li>
</ol>
",2,2,611,2015-08-05 09:18:26,https://stackoverflow.com/questions/31828295/using-libsvm-in-java-for-string-classification
NLTK accuracy: &quot;ValueError: too many values to unpack&quot;,"<p>I'm trying to do some sentiment analysis of a new movie from Twitter using the NLTK toolkit. I've followed the NLTK 'movie_reviews' example and I've built my own CategorizedPlaintextCorpusReader object. The problem arises when I call <code>nltk.classify.util.accuracy(classifier, testfeats)</code>. Here is the code:</p>

<pre><code>import os
import glob
import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews

def word_feats(words):
        return dict([(word, True) for word in words])

negids = movie_reviews.fileids('neg')
posids = movie_reviews.fileids('pos')

negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]

trainfeats = negfeats + posfeats

# Building a custom Corpus Reader
tweets = nltk.corpus.reader.CategorizedPlaintextCorpusReader('./tweets', r'.*\.txt', cat_pattern=r'(.*)\.txt')
tweetsids = tweets.fileids()
testfeats = [(word_feats(tweets.words(fileids=[f]))) for f in tweetsids]

print 'Training the classifier'
classifier = NaiveBayesClassifier.train(trainfeats)

for tweet in tweetsids:
        print tweet + ' : ' + classifier.classify(word_feats(tweets.words(tweetsids)))

classifier.show_most_informative_features()

print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)
</code></pre>

<p>It all seems to work fine until it gets to the last line. That's when I get the error:</p>

<pre><code>&gt;&gt;&gt; nltk.classify.util.accuracy(classifier, testfeats)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python2.7/dist-packages/nltk/classify/util.py"", line 87, in accuracy
    results = classifier.classify_many([fs for (fs,l) in gold])
ValueError: too many values to unpack
</code></pre>

<p>Does anybody see anything wrong within the code?</p>

<p>Thanks.</p>
","python, nltk, text-classification","<p>The error message</p>

<pre><code>File ""/usr/lib/python2.7/dist-packages/nltk/classify/util.py"", line 87, in accuracy
  results = classifier.classify_many([fs for (fs,l) in gold])
ValueError: too many values to unpack
</code></pre>

<p>arises because items in <code>gold</code> can not be unpacked into a 2-tuple, <code>(fs,l)</code>:</p>

<pre><code>[fs for (fs,l) in gold]  # &lt;-- The ValueError is raised here
</code></pre>

<p>It is the same error you would get if <code>gold</code> equals <code>[(1,2,3)]</code>, since the 3-tuple <code>(1,2,3)</code> can not be unpacked into a 2-tuple <code>(fs,l)</code>:</p>

<pre><code>In [74]: [fs for (fs,l) in [(1,2)]]
Out[74]: [1]
In [73]: [fs for (fs,l) in [(1,2,3)]]
ValueError: too many values to unpack
</code></pre>

<p><code>gold</code> might be buried inside the implementation of <code>nltk.classify.util.accuracy</code>, but this hints that your inputs, <code>classifier</code> or <code>testfeats</code> are of the wrong ""shape"".</p>

<p>There is no problem with classifer, since calling <code>accuracy(classifier, trainfeats)</code> 
works:</p>

<pre><code>In [61]: print 'accuracy:', nltk.classify.util.accuracy(classifier, trainfeats)
accuracy: 0.9675
</code></pre>

<p>The problem must be in <code>testfeats</code>.</p>

<hr>

<p>Compare <code>trainfeats</code> with <code>testfeats</code>. 
<code>trainfeats[0]</code> is a 2-tuple containing a dict and a classification:</p>

<pre><code>In [63]: trainfeats[0]
Out[63]: 
({u'!': True,
  u'""': True,
  u'&amp;': True,
  ...
  u'years': True,
  u'you': True,
  u'your': True},
 'neg')           # &lt;---  Notice the classification, 'neg'
</code></pre>

<p>but <code>testfeats[0]</code> is just a dict, <code>word_feats(tweets.words(fileids=[f]))</code>:</p>

<pre><code>testfeats = [(word_feats(tweets.words(fileids=[f]))) for f in tweetsids]
</code></pre>

<p>So to fix this you would need to define <code>testfeats</code> to look more like <code>trainfeats</code> -- each dict returned by <code>word_feats</code> must be paired with a classification.</p>
",2,1,4456,2015-08-10 12:56:14,https://stackoverflow.com/questions/31920199/nltk-accuracy-valueerror-too-many-values-to-unpack
SciKit-learn&#39;s &#39;predict&#39; function giving output in wrong format,"<p>I am new to scikit and so playing around with it.</p>

<p>Background about the problem:
I am trying to play with 'Byte the correct apple' competition on hackerRank.
In which we are given two files one containing the text of apple the company and one for apple the fruit. Now we must learn from it and then make prediction on a new text.</p>

<p>Though the code runs but my problems are:
- As 'line' (in the code below) is a single input I should get single digit output either zero or one. But I am getting an array as an output.
- Am I even close to learning anything using the code below?</p>

<pre><code>import numpy as np

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer


from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn import svm
from sklearn.svm import LinearSVC

from sklearn.pipeline import Pipeline

appleComputers = [];
appleFruits = [];
labels = [];

with open('apple-computers.txt','r') as f:
    for line in f:
        appleComputers.append(line)
        labels.append(1);

with open('apple-fruit.txt','r') as f:
    for line in f:
        appleFruits.append(line)
        labels.append(0);

text = appleComputers + appleFruits;
labels = np.asarray(labels)

#text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])
text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', LinearSVC(loss='hinge', penalty='l2')),])

text_clf = text_clf.fit(text, labels)


line = 'I am talking about apple the fruit we eat.'
line = 'I am talking about the product apple computer by Steve Jobs'
predicted = text_clf.predict(line);
print predicted
</code></pre>
","python, machine-learning, scikit-learn, text-classification","<p>I found the answer by myself.</p>

<p>For</p>

<pre><code>predicted = text_clf.predict(line);
</code></pre>

<p>'line' should be a list and not a string as it was for the 'fit' function.</p>

<p>i.e. Replace</p>

<pre><code>line = 'I am talking about the product apple computer by Steve Jobs'
</code></pre>

<p>by</p>

<pre><code>line = [];    
line.append('I am talking about apple the fruit we eat.');
</code></pre>

<p>or @jme suggested we can use</p>

<pre><code>text_clf.predict([line]) 
</code></pre>
",2,1,3046,2015-08-10 15:41:30,https://stackoverflow.com/questions/31923773/scikit-learns-predict-function-giving-output-in-wrong-format
NaiveBayes Classifer in R predicting only one Class,"<p>I am working on classifying arduino posts into hardware and software categories. I have manually prepared the train set. 
But, while entering the test set, all posts are predicted as ""hardware"".
Is there some mistake in the train set format. Does NaiveBayes fail to identify sentences as input to perform prediction?
The train-set format is: class ""\t"" pred ""\t"" set
The classifier will take set column to identify labels and pred column as the predicator. Class column is used only to create set column. </p>

<pre><code>//programmed in R
library(e1071)
train = read.table(""train_set.csv"", sep=""\t"", header=T)
test = read.table(""test_one.csv"", sep=""\t"", header=T)
train$set = ""Hardware""
train[train$class==0,]$set = ""Software""
train$set = as.factor(train$set)
model &lt;- naiveBayes(set ~ pred, data = train)
pred &lt;- predict(model, train[495:510,]) //displays train set prediction
pred1 &lt;- predict(model, test[1:10,]) //displays incorrect prediction for test set
</code></pre>

<p>Train data set (delimiter = \t, attaching only 4 rows of 1000 rows )</p>

<p>1 represents hardware
0 represents software
In the program, another column called ""set"" is appended to store ""hardware"" or ""software"" corresponding to 1 and 0.</p>

<pre><code>class   pred
1    Im making a simple Arduino web server and I want to keep it turned on all the time. So it must endure to stay working continuously. Im using an Arduino Uno with a Ethernet Shield.Its powered with a simple outlet power supply 5V @ 1A. My Questions: Will I have any problems leaving the Arduino turned on all the time? Is there some other Arduino board better recommended for this? Are there any precautions that I need to heed regarding this? 
1    Put plainly: is there a way to get an HTTPS connection on the Arduino? I have been looking in to it and I have found it is impossible with the standard library and the Ethernet shield but is there a custom library that can do it? What about a coprocessor i.e. like the WiFi shield has? Anyone know if the Arduino yn has ssl? 
0    The use of malloc and free seems pretty rare in the Arduino world. It is used in pure AVR C much more often but still with caution. Is it a really bad idea to use malloc and free with Arduino? 
0    What do I need to build a shield capable of receiving 1080p video from USB camera timestamp each frame and send the frame to memory card? 
</code></pre>

<p>Test data set</p>

<pre><code> pred
arduino-uno web-server ethernet i'm making a simple arduino web server and i want to keep it turned on all the time. so it must endure to stay working continuously. i'm using an arduino uno with a ethernet shield.it's powered with a simple outlet power supply 5v @ 1a. my questions: will i have any problems leaving the arduino turned on all the time? is there some other arduino board better recommended for this? are there any precautions that i need to heed regarding this?    
I made a circuit which in my intentions would allow me to toggle a LED dimming loop. Problem is that once I push the button the first time pushing it a second time doesnt toggle the LED loop off. Here is the code: const int LED = 9; // the pin for the LEDconst int BUTTON = 7;int val = LOW;int old_val = LOW;int state = 0;int i = 0;void setup{ pinModeLED OUTPUT; pinModeBUTTON INPUT;}void loop{ val = digitalReadBUTTON; if val == HIGH &amp;amp;&amp;amp; old_val==LOW { state = 1 - state; delay10; } old_val = val; if state == 1 { for i = 0; i &amp;lt; 255; i++ // loop from 0 to 254 fade in { analogWriteLED i; // set the LED brightness delay10; // Wait 10ms because analogWrite // is instantaneous and we would // not see any change } for i = 255; i &amp;gt; 0; i-- // loop from 255 to 1 fade out { analogWriteLED i; // set the LED brightness delay10; // Wait 10m
</code></pre>

<p>Expected Output:
Hardware Software</p>
","r, classification, text-classification, naivebayes","<pre><code>library(e1071)
library(tm)
library(MASS)
library(SnowballC)

train = read.table(""train_set.csv"", sep=""\t"", header=T)
test = read.table(""test_set.csv"", sep=""\t"", header=T)

#stopwords
mystopwords &lt;- c(stopwords(""english""),""week"",""arduino"",""words"",""need"",""get"",""will"",""want"",""know"",""work"",""also"")

#corpus for train set
train.corpus &lt;- Corpus(VectorSource(train$pred))
train.corpus &lt;- tm_map(train.corpus, content_transformer(tolower))
train.corpus &lt;- tm_map(train.corpus, removePunctuation)
train.corpus &lt;- tm_map(train.corpus, stripWhitespace)
train.corpus &lt;- tm_map(train.corpus, removeNumbers)
train.corpus &lt;- tm_map(train.corpus, removeWords, mystopwords)
train.corpus &lt;- tm_map(train.corpus, stemDocument)
train.corpus &lt;- tm_map(train.corpus, removeWords, ""(http)\\w+"")
train.corpus &lt;- tm_map(train.corpus, removeWords, ""\\b[a-zA-Z0-9]{10,100}\\b"")
train.corpus.dtm &lt;- DocumentTermMatrix(train.corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE), stopwords = TRUE, removePunctuation=TRUE))
train.corpus.dtms &lt;- removeSparseTerms(train.corpus.dtm, 0.98)

#Debugging
#TermDocumentMatrix(train.corpus)
#inspect(train.corpus.dtm)
#findFreqTerms(train.corpus.dtm, N)   #N &lt;- freq

#corpus for test set
test.corpus &lt;- Corpus(VectorSource(test$pred))
test.corpus &lt;- tm_map(test.corpus, content_transformer(tolower))
test.corpus &lt;- tm_map(test.corpus, removePunctuation)
test.corpus &lt;- tm_map(test.corpus, stripWhitespace)
test.corpus &lt;- tm_map(test.corpus, removeNumbers)
test.corpus &lt;- tm_map(test.corpus, removeWords, mystopwords)
test.corpus &lt;- tm_map(test.corpus, stemDocument)
test.corpus &lt;- tm_map(test.corpus, removeWords, ""(http)\\w+"")
test.corpus &lt;- tm_map(test.corpus, removeWords, ""\\b[a-zA-Z0-9]{10,100}\\b"")
test.corpus.dtm &lt;- DocumentTermMatrix(test.corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE), stopwords = TRUE, removePunctuation=TRUE))
test.corpus.dtms &lt;- removeSparseTerms(test.corpus.dtm, 0.98) 


m &lt;- as.matrix(train.corpus.dtms)
n &lt;- as.matrix(test.corpus.dtms)

#Train model
model &lt;- naiveBayes(m,as.factor(train$class));

#Prediction
results &lt;- predict(model,n[1:10,])
</code></pre>

<p>The next step is to include 10-fold cross validation into this classifier for performance check; where I am stuck right now.</p>
",0,1,1630,2015-08-25 18:12:18,https://stackoverflow.com/questions/32211157/naivebayes-classifer-in-r-predicting-only-one-class
How to use spark Naive Bayes classifier for text classification with IDF?,"<p>I want to convert text documents into feature vectors using tf-idf, and then train a naive bayes algorithm to classify them.</p>

<p>I can easily load my text files without the labels and use HashingTF() to convert it into a vector, and then use IDF() to weight the words according to how important they are. But if I do that I get rid of the labels and it seems to be impossible to recombine the label with the vector even though the order is the same.</p>

<p>On the other hand, I can call HashingTF() on each individual document and keep the labels, but then I can't call IDF() on it since it requires the whole corpus of documents (and the labels would get in the way).</p>

<p>The spark documentation for naive bayes only has one example where the points are already labeled and vectorized so that isn't much help.</p>

<p>I also had a look at this guide: <a href=""http://help.mortardata.com/technologies/spark/train_a_machine_learning_model"" rel=""nofollow"">http://help.mortardata.com/technologies/spark/train_a_machine_learning_model</a>
but here he only applies the hashing function on each document without idf.</p>

<p>So my question is whether there is a way to not only vectorize but also weight the words using idf for the naive bayes classifier? The main problem seems to be sparks's insistence on only accepting rdds of labeledPoints as input to NaiveBayes.</p>

<pre><code>def parseLine(line):
    label = row[1] # the label is the 2nd element of each row
    features = row[3] # the text is the 4th element of each row
    features = tokenize(features)
    features = hashingTF.transform(features)
    return LabeledPoint(label, features)
labeledData = data1.map(parseLine)
</code></pre>
","python, apache-spark, tf-idf, text-classification, apache-spark-mllib","<p>Standard PySpark approach (split -> transform -> zip) seems to work just fine:</p>

<pre><code>from pyspark.mllib.feature import HashingTF, IDF
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.classification import NaiveBayes   

training_raw = sc.parallelize([
    {""text"": ""foo foo foo bar bar protein"", ""label"": 1.0},
    {""text"": ""foo bar dna for bar"", ""label"": 0.0},
    {""text"": ""foo bar foo dna foo"", ""label"": 0.0},
    {""text"": ""bar foo protein foo "", ""label"": 1.0}])


# Split data into labels and features, transform
# preservesPartitioning is not really required
# since map without partitioner shouldn't trigger repartitiong
labels = training_raw.map(
    lambda doc: doc[""label""],  # Standard Python dict access 
    preservesPartitioning=True # This is obsolete.
)

tf = HashingTF(numFeatures=100).transform( ## Use much larger number in practice
    training_raw.map(lambda doc: doc[""text""].split(), 
    preservesPartitioning=True))

idf = IDF().fit(tf)
tfidf = idf.transform(tf)

# Combine using zip
training = labels.zip(tfidf).map(lambda x: LabeledPoint(x[0], x[1]))

# Train and check
model = NaiveBayes.train(training)
labels_and_preds = labels.zip(model.predict(tfidf)).map(
    lambda x: {""actual"": x[0], ""predicted"": float(x[1])})
</code></pre>

<p>To get some statistics you can use <code>MulticlassMetrics</code>:</p>

<pre><code>from pyspark.mllib.evaluation import MulticlassMetrics
from operator import itemgetter

metrics = MulticlassMetrics(
    labels_and_preds.map(itemgetter(""actual"", ""predicted"")))

metrics.confusionMatrix().toArray()
## array([[ 2.,  0.],
##        [ 0.,  2.]])
</code></pre>

<p><strong>Related</strong></p>

<ul>
<li><a href=""https://stackoverflow.com/q/45626754/6910411"">Handling continuous data in Spark NaiveBayes</a></li>
</ul>
",10,5,6352,2015-08-26 15:43:01,https://stackoverflow.com/questions/32231049/how-to-use-spark-naive-bayes-classifier-for-text-classification-with-idf
StackOverflow Tags Predictor…Suggest an Machine Learning Approach please?,"<p>I am trying to predict tags for stackoverflow questions and I am not able to decide which Machine Learning algorithm will be a correct approach for this.</p>

<p><strong>Input:</strong> As a dataset I have mined stackoverflow questions, I have tokenized the data set and removed stopwords and punctuation from this data.</p>

<p><strong>Things i have tried:</strong></p>

<ol>
<li>TF-IDF</li>
<li>Trained Naive Bayes on the dataset and then gave user defined input to predict tags, but its not working correctly</li>
<li>Linear SVM</li>
</ol>

<p>Which ML algorithm I should use Supervised or Unsupervised? If possible please, suggest a correct ML approach from the scratch. PS: I have the list of all tags present on StackOverflow so, will this help in anyway? Thanks</p>
","machine-learning, prediction, text-classification","<p>I would try MLP. In order to begin I would choose a reasonably small set of keywords for input and encode them [1..100 for example] and train for a reasonably small set of output tags. </p>

<p>PS: Unsupervised learning for this task is unfavorable in general because many questions that refer to different tags have very similar content and are very likely to get clustered together. </p>
",1,2,584,2015-09-01 06:21:22,https://stackoverflow.com/questions/32324813/stackoverflow-tags-predictor-suggest-an-machine-learning-approach-please
How to create a word map for custom text for text classification in R?,"<p>I am trying to implement a text classification program in R that classifies input text (args) into 3 different classes. I have successfully tested the sample program by dividing the input data into training and test data. </p>

<p>I would now like to build something that would allow me to classify custom text.
My input data has following structure:</p>

<p><img src=""https://i.sstatic.net/qcXbi.png"" alt=""""></p>

<p>So if I enter a custom text : ""games studies time"", I would like to get a matrix that looks like following:</p>

<p><img src=""https://i.sstatic.net/qS7Mh.png"" alt=""""></p>

<p>Please tell me what is the best way to do the same.</p>
","r, tm, knn, text-classification","<p>This sounds a lot like the application of a ""dictionary"" to text following the tokenization of that text.  What you have as the matrix result in your question, however, makes no use of the categories in the input data.  </p>

<p>So here are two solutions: one, for producing the matrix you state that you want, and two, for producing a matrix that counts the input text according to the counts of the categories to which your input data maps the text.</p>

<p>This uses the <em>quanteda</em> package in R.</p>

<pre><code>require(quanteda)
mymap &lt;- dictionary(list(school = c(""time"", ""games"", ""studies""),
                         college = c(""time"", ""games""),
                         office = c(""work"")))
dfm(""games studies time"", verbose = FALSE)
## Document-feature matrix of: 1 document, 3 features.
## 1 x 3 sparse Matrix of class ""dfmSparse""
##        features
## docs    games studies time
##   text1     1       1    1
dfm(""games studies time"", dictionary = mymap, verbose = FALSE)
## Document-feature matrix of: 1 document, 3 features.
## 1 x 3 sparse Matrix of class ""dfmSparse""
##        features
## docs    school college office
##   text1      3       2      0
</code></pre>
",0,0,433,2015-09-02 20:22:31,https://stackoverflow.com/questions/32362424/how-to-create-a-word-map-for-custom-text-for-text-classification-in-r
"Document Tagging with Named Topics, relevant literature? (Also asked on Quora)","<p>I am working on what is to me a very new domain in data science and would like to know if anyone can suggest any existing academic literature that has relevant approaches that address my problem.</p>

<p>The problem setting is as follows:
I have a set of named topics (about 100 topics). We have a document tagging engine that tags documents (news articles in our case) based on their text with up to 5 of these 100 topics. </p>

<p>All this is done using fairly rudimentary similarity metrics (each topic is a text vector and so is each document and we do a similarity between these vectors and assign the 5 most similar topics to each document).</p>

<p>We are looking to improve the quality of this process but the constraint is we have to maintain the set of 100 named topics which are vital for other purposes so unsupervised topic models like LDA are out because:
1. They don't provide named topics
2. Even if we are able to somehow map distributions of topics output by LDA to existing topics, these distributions will not remain constant and vary with the underlying corpus.</p>

<p>So could anyone point me towards papers that have worked with document tagging using a finite set of named topics?</p>

<p>There are 2 challenges here:
1. Given a finite set of named topics , how to tag new documents with them? (this is the bigger more obvious challenge)
2. How do we keep the topics updated with the changing document universe?
Any work that addresses one or both of these challenges would be a great help.</p>

<p>P.S. I've also asked this question on Quora if anyone else is looking for answers and would like to read both posts. I'm duplicating this question as I feel it is interesting and I'd like to get as many people talking about this problem as possible and as many literature suggestions as possible.</p>

<p><a href=""https://www.quora.com/Document-Tagging-with-Named-Topics-relevant-literature"" rel=""nofollow"">Same Question on Quora</a></p>
","machine-learning, nlp, classification, tagging, text-classification","<p>Have you tried <strong>classification</strong>?</p>

<p>Train a classifier for each topic.</p>

<p>Tag with the 5 most likely classes.</p>
",1,0,191,2015-09-13 17:05:29,https://stackoverflow.com/questions/32552283/document-tagging-with-named-topics-relevant-literature-also-asked-on-quora
"If my entire training set of documents is class A, how can I use TF-IDF to find other documents of class A?","<p>I have a collection X of documents, all of which are of class A (the only class in which I'm interested or know anything about). I also have a much larger collection Y of documents that I know nothing about. The documents in X and Y come from the same source and have similar formats and somewhat similar subject matters. I'd like to use the TF-IDF feature vectors of the documents in X to find the documents in Y that are most likely to be of class A.</p>

<p>In the past, I've used TF-IDF feature vectors to build naive Bayes classifiers, but in these situations, my training set X consisted of documents of many classes, and my objective was to classify each document in Y as one of the classes seen in X.</p>

<p>This seems like a different situation. Here, my entire training set has the same class (I have no documents that I know are not of class A), and I'm only interested in determining if documents in Y are or are not of that class.</p>

<p>A classifier seems like the wrong route, but I'm not sure what the best next step is. Is there a different algorithm that can use that TF-IDF matrix to determine the likelihood that a document is of the same class?</p>

<p>FYI, I'm using scikit-learn in Python 2.7, which obviously made computing the TF-IDF matrix of X (and Y) simple.</p>
","python, machine-learning, tf-idf, text-classification","<p>What I think you have is an unsupervised learning application.  Clustering.  Using the combined X &amp; Y dataset, generate clusters.  Then overlay the X boundary; the boundary that contains all X samples.  All items from Y in the X boundary can be considered X.  And the X-ness of a given sample from Y is the distance from the X cluster centroid.  Something like that.  </p>
",2,1,307,2015-09-13 19:39:43,https://stackoverflow.com/questions/32553806/if-my-entire-training-set-of-documents-is-class-a-how-can-i-use-tf-idf-to-find
How to combine multiple feature sets in bag of words,"<p>I have text classification data with predictions depending on categories, 'descriptions' and 'components'.  I could do the classification using bag of words in python with scikit on 'descriptions'. But  I want to get predictions using both categories in bag of words with weights to individual feature sets
x = descriptions + 2* components
How should  I proceed?</p>
","python-2.7, machine-learning, scikit-learn, text-mining, text-classification","<p>You can train individual classifiers for descriptions and merchants, and obtain a final score using <code>score = w1 * predictions + w2 * components.</code></p>

<p>The values of <code>w1</code> and <code>w2</code> should be obtained using cross validation.</p>

<p>Alternatively, you can train a single multiclass classifier by combining the training dataset. </p>

<p>You will now have 4 classes: </p>

<ol>
<li>Neither 'predictions' nor 'components'</li>
<li>'predictions' but not 'components'</li>
<li>not 'predictions' but 'components'</li>
<li>'predictions' and 'components'</li>
</ol>

<p>And you can go ahead and train as usual.</p>
",0,0,807,2015-09-30 06:41:41,https://stackoverflow.com/questions/32859460/how-to-combine-multiple-feature-sets-in-bag-of-words
Why won&#39;t my neural network train?,"<p>I have gathered over 20,000 legal pleadings in PDF format. I am an attorney, but also I write computer programs to help with my practice in MFC/VC++. I'd like to learn to use neural networks (unfortunately my math skills are limited to college algebra) to classify documents filed in lawsuits.</p>

<p>My first goal is to train a three layer feed forward neural network to recognize whether a document is a small claims document (with the letters ""SP"" in the case number), or whether it is a regular document (with the letters ""CC"" in the case number). Every attorney puts some variant of the word ""Case:"" or ""Case No"" or ""Case Number"" or one of an infinite variations of that. So I've taken the first 600 characters (all attorneys will put the case number within the first 600 chars), and made a CSV database with each row being one document, with 600 columns containing the ASCII codes of the first 600 characters, and the 601st character is either a ""1"" for regular cases, or a ""0"" for small claims.</p>

<p>I then run it through the neural network program coded here:
<a href=""https://takinginitiative.wordpress.com/2008/04/23/basic-neural-network-tutorial-c-implementation-and-source-code/"" rel=""nofollow"">https://takinginitiative.wordpress.com/2008/04/23/basic-neural-network-tutorial-c-implementation-and-source-code/</a>
(Naturally I update the program to handle 600 neurons, with one output), but when I run through the accuracy is horrible - something like 2% on the training data, and 0% on the general set. 1/8 of documents are for non-small claims cases. </p>

<p>Is this the sort of problem a Neural Net can handle? What am I doing wrong?</p>
","neural-network, text-classification","<blockquote>
  <p>So I've taken the first 600 characters (all attorneys will put the case number within the first 600 chars), and made a CSV database with each row being one document, with 600 columns containing the ASCII codes of the first 600 characters, and the 601st character is either a ""1"" for regular cases, or a ""0"" for small claims.</p>
</blockquote>

<p>Looking at each and every character at the beginning of the document independently will be very inaccurate.  Rather than consider the <em>characters</em> independently, first tokenize the first 600 characters into words.  Use those words as input to your neural net, rather than individual characters.</p>

<p>Note that once you have tokenized the first 600 characters, you may will find a distinctly finite list of tokens that mean ""case number"", removing the need for a neural net.</p>

<p>The <a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow"">Standford Natural Language Processor</a> provides this functionality.  You can find a .NET compatible implementation <a href=""https://www.nuget.org/packages/Stanford.NLP.Parser/3.5.2.1"" rel=""nofollow"">available in NuGet</a>.</p>
",0,0,197,2015-10-11 23:52:14,https://stackoverflow.com/questions/33071375/why-wont-my-neural-network-train
How to select best parameters for SVM linear kernel type,"<p>I perform a classification of two labels using libsvm. But I don't get good results for the default parameters of SVM kernel type = linear. Can any one please tell me a way to find best parameters for SVM linear kernel type </p>
","weka, svm, libsvm, text-classification","<p>The <code>linear kernel</code> depends on the <code>C</code> parameter. </p>

<p>You could perform a <code>grid-search</code> for this parameter in order to find the 'best' matching one for your given dataset.</p>

<p>For weka the procedure is described <a href=""https://weka.wikispaces.com/Optimizing+parameters"" rel=""nofollow"">here</a>.</p>
",1,0,1999,2015-10-15 08:02:41,https://stackoverflow.com/questions/33143060/how-to-select-best-parameters-for-svm-linear-kernel-type
Na&#239;ve Bayes Classifier Bernoulli model,"<p>I'm working on classifying invoices and receipt and I will be working with Bernoulli model.</p>

<p>This is the naive Bayes classifier :</p>

<p>P(c|x) = P(x|c) x P(c) / P(x)</p>

<p>I know how to compute P(c) class prior probability and since we assume that all the words are independent we don't need the P(x).</p>

<p>Now formula will be like this : P(c|x) = P(x|c) x P(c) and to compute P(x|c) we do the liklihood method which is calculating all the words probability P(c|X) = P(x1|c)P(x2|c)*P(x3|c).... </p>

<p>My question  is after calculating the liklihood do I need to multiply it with P(c) or not, P(c|X) = P(x1|c)P(x2|c)*P(x3|c)...*P(c)?  </p>
","machine-learning, probability, text-classification, naivebayes, bernoulli-probability","<p><code>P(c|x)</code> <strong>is not</strong> equal to  <code>P(x|c) P(c)</code>. It is <strong>proportional</strong>, as during classification you do </p>

<pre><code>cl(x) = arg max_c P(c|x) = arg max_c P(x|c) P(c) / P(x) = arg max_c P(x|c) P(c)
</code></pre>

<p>and this holds for <strong>every probability distribution</strong>, where <code>P(x)&gt;0</code>, no need to any Bayes assumptions at this point. It is just a simple Bayes theorem + noticing that <code>P(x)</code> is just a positive constant in this equation.</p>

<p>Thus you <strong>never</strong> actually compute <code>P(c|x)</code>, you just compute <code>P(x|c) P(c)</code> which will give you <strong>the same classification</strong>. I hope this shows that your classification <strong>has to be</strong> based on product of <code>P(x|c)</code> and <code>P(c)</code>, where as you pointed out <code>P(x|c) = PROD_i P(x_i|c)</code> (here we use Naive Bayes assumption regarding independence, not before).</p>
",1,1,565,2015-11-12 20:02:58,https://stackoverflow.com/questions/33680322/na%c3%afve-bayes-classifier-bernoulli-model
How to create a matrix for input string in SciKitLearn module of Python?,"<p>I have used <a href=""https://github.com/yassersouri/classify-text"" rel=""nofollow"">https://github.com/yassersouri/classify-text</a> to check the efficiency of various algorithms available in Scikitlearn. Now, I know there are following steps in classification</p>

<blockquote>
  <ol>
  <li>Create a matrix of training data and corresponding classes</li>
  <li>Train a classifier on the basis of these matrices</li>
  <li>Divide the training set into training/test</li>
  </ol>
</blockquote>

<p>Now, I would like to skip the step 3 (which I have already done) and take the input from user and test it against the trained variable.</p>

<p>Now I am not sure what is the next step after taking input from console, do I need to build BOW matrix for this string as well ?</p>
","python, matrix, scikit-learn, classification, text-classification","<p>Short answer: you need to run your new string through the <em>same</em> bag-of-words transformer that you used for your training data.</p>

<p>If you are using scikit-learn, I'd recommend the scikit-learn feature extraction tools directly rather than the utility routines at the link you gave. If you <a href=""https://github.com/yassersouri/classify-text/blob/master/util.py#L108"" rel=""nofollow"">click through that repository</a>, you'll find it uses scikit-learn's <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow""><code>CountVectorizer</code></a> to compute the bag-of-words representation.</p>

<p>The canonical way to use such a vectorizer is within a pipeline, and at that point the prediction on new data is straightforward. Here's a simple example, where I have left-out the cross-validation aspect for simpliticy:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.naive_bayes import MultinomialNB

corpus = ['this document is about cheese',
          'this is about snakes',
          'another one about cheese',
          'and another one about snakes']
label = ['cheese', 'snakes', 'cheese', 'snakes']

clf = make_pipeline(CountVectorizer(), MultinomialNB())
clf.fit(corpus, label)
predicted = clf.predict(['one about cheese'])
print(predicted)
# ['cheese']
</code></pre>
",0,1,396,2015-11-14 16:27:49,https://stackoverflow.com/questions/33710603/how-to-create-a-matrix-for-input-string-in-scikitlearn-module-of-python
python textblob and text classification,"<p>I'm trying do build a text classification model with python and <a href=""https://textblob.readthedocs.org/en/dev/index.html"" rel=""nofollow noreferrer"">textblob</a>, the script is runing on my server and in the future the idea is that users will be able to submit their text and it will be classified.
i'm loading the training set from csv :</p>
<pre><code># -*- coding: utf-8 -*-
import sys
import codecs
sys.stdout = open('yyyyyyyyy.txt',&quot;w&quot;);
from nltk.tokenize import word_tokenize
from textblob.classifiers import NaiveBayesClassifier
with open('file.csv', 'r', encoding='latin-1') as fp:
    cl = NaiveBayesClassifier(fp, format=&quot;csv&quot;)  

print(cl.classify(&quot;some text&quot;))
</code></pre>
<p>csv is about 500 lines long (with string between 10 and 100 chars), and NaiveBayesclassifier needs about 2 minutes for training and then be able to classify my text(not sure if is normal that it need so much time, maybe is my server slow with only 512mb ram).</p>
<p>example of csv line :</p>
<pre><code>&quot;Oggi alla Camera con la Fondazione Italia-Usa abbiamo consegnato a 140 studenti laureati con 110 e 110 lode i diplomi del Master in Marketing Comunicazione e Made in Italy.&quot;,FI-PDL
</code></pre>
<p>what is not clear to me, and i cant find an answer on textblob documentation, is if there is a way to 'save' my trained classifier (so save a lot of time), because by now everytime i run the script it will train again the classifier.
I'm new to text classification and machine learing so my apologize if it is a dumb question.</p>
<p>Thanks in advance.</p>
","python, nlp, nltk, text-classification, textblob","<p>Ok found that pickle module is what i need :)</p>

<p>Training:</p>

<pre><code># -*- coding: utf-8 -*-
import pickle
from nltk.tokenize import word_tokenize
from textblob.classifiers import NaiveBayesClassifier
with open('file.csv', 'r', encoding='latin-1') as fp:
    cl = NaiveBayesClassifier(fp, format=""csv"")  

object = cl
file = open('classifier.pickle','wb') 
pickle.dump(object,file)
</code></pre>

<p>extracting:</p>

<pre><code>import pickle
sys.stdout = open('demo.txt',""w"");
from nltk.tokenize import word_tokenize
from textblob.classifiers import NaiveBayesClassifier
cl = pickle.load( open( ""classifier.pickle"", ""rb"" ) )
print(cl.classify(""text to classify""))
</code></pre>
",6,2,2242,2015-11-24 01:35:30,https://stackoverflow.com/questions/33883976/python-textblob-and-text-classification
Using language models for term weighting,"<p>I understand that scikit supports n-grams using a Vectorizer. But those are only strings. I would like to use a statistical language model (<a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow"">https://en.wikipedia.org/wiki/Language_model</a>) like this one: <a href=""http://www.nltk.org/_modules/nltk/model/ngram.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/model/ngram.html</a>.</p>

<p>So, what I want is a Vectorizer using the probability as term weight instead of let's say tf-idf or simply a token count. Is there a reason why this is not supported by scikit? I'm relatively inexperienced with language modeling, so I'm not sure if this approach is a good idea for text classification.</p>
","python, machine-learning, scikit-learn, n-gram, text-classification","<p>It depends what do you mean by <strong>term</strong>. If - as usual - term is just a word, then a probability model will work the same as... simple tf weighting (even without idf!). Why? Beacause empirical estimator of <code>P(word)</code> is just <code># word / # all_words</code>, and as <code># all_words</code> is constant, then the weight becomes just <code>#word</code>, which is simple <strong>term frequency</strong>. So in this sense, scikit does what you need.</p>

<p>Ok, so maybe you want to consider context? Then what kind of context? Do you want to analyze independently <code>P(pre-word1, word)</code> and use it as a weighted sum for <code>word</code>? Then why not <code>P(word, post-word1)</code>? Why not <code>P(pre-word2, pre-word1, word, post-word1, post-word2)</code> etc.? Why not to include some reweighting based on unigrams when bigrams are not available? The answer is quite simple, once you go into using language models as a weighting schemes, amount of possible introductions grows exponentialy, and there is no ""typical"" approach, which is worth implementing as a ""standard"" for a library which is <strong>not a NLP library</strong>.</p>
",1,1,301,2015-11-24 17:15:32,https://stackoverflow.com/questions/33899867/using-language-models-for-term-weighting
Sklearn other inputs in addition to text for text classification,"<p>I am trying to do a text classifier using ""Sci kit"" learn bag of words. Vectorization into a classifier. However, I was wondering how would i add another variable to the input apart from the text itself. Say I want to add a number of words in the text in addition to text (because I think it may affect the result). How should I go about doing so?<br>
Do I have to add another classifier on top of that one? Or is there a way to add that input to vectorized text?  </p>
","python, scikit-learn, classification, words, text-classification","<p>Scikit learn classifiers works with numpy arrays. 
This means that after your vectorization of text, you can add your new features to this array easily (I am taking this sentence back, not very easily but doable).
Problem is in text categorization, your features will be sparse therefore normal numpy column additions does not work.</p>

<p>Code modified from <a href=""https://github.com/jakevdp/sklearn_scipy2013/blob/master/notebooks/05.2_application_to_text_mining.ipynb"" rel=""nofollow"">text mining example from scikit learn scipy 2013 tutorial</a>.</p>

<pre><code>from sklearn.datasets import load_files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import numpy as np
import scipy

# Load the text data

twenty_train_subset = load_files('datasets/20news-bydate-train/',
    categories=categories, encoding='latin-1')

# Turn the text documents into vectors of word frequencies
vectorizer = TfidfVectorizer(min_df=2)
X_train_only_text_features = vectorizer.fit_transform(twenty_train_subset.data)


print type(X_train_only_text_features)
print ""X_train_only_text_features"",X_train_only_text_features.shape

size = X_train_only_text_features.shape[0]
print ""size"",size

ones_column = np.ones(size).reshape(size,1)
print ""ones_column"",ones_column.shape


new_column = scipy.sparse.csr.csr_matrix(ones_column )
print type(new_column)
print ""new_column"",new_column.shape

X_train= scipy.sparse.hstack([new_column,X_train_only_text_features])

print ""X_train"",X_train.shape
</code></pre>

<p>output is following:</p>

<pre><code>&lt;class 'scipy.sparse.csr.csr_matrix'&gt;
X_train_only_text_features (2034, 17566)
size 2034
ones_column (2034L, 1L)
&lt;class 'scipy.sparse.csr.csr_matrix'&gt;
new_column (2034, 1)
X_train (2034, 17567)
</code></pre>
",2,4,1329,2015-12-08 17:23:19,https://stackoverflow.com/questions/34162154/sklearn-other-inputs-in-addition-to-text-for-text-classification
Error using &quot;TermDocumentMatrix&quot; and &quot;Dist&quot; functions in R,"<p>I have been trying to replicate the example <a href=""http://www.rexamine.com/2014/06/text-mining-in-r-automatic-categorization-of-wikipedia-articles/"" rel=""nofollow"">here</a>: but I have had some problems along the way.</p>

<p>Everything worked fine until here:</p>

<pre><code>docsTDM &lt;- TermDocumentMatrix(docs8)
</code></pre>

<blockquote>
  <p>Error in UseMethod(""meta"", x) : 
            no applicable method for 'meta' applied to an object of class ""character""<br>
            In addition: Warning message:<br>
            In mclapply(unname(content(x)), termFreq, control) :<br>
              all scheduled cores encountered errors in user code</p>
</blockquote>

<p>So I was able to fix that error modifying this previous step by changing this:</p>

<pre><code>docs8 &lt;- tm_map(docs7, tolower)
</code></pre>

<p>To this:</p>

<pre><code>docs8 &lt;- tm_map(docs7, content_transformer(tolower))
</code></pre>

<p>But then I got in trouble again with:</p>

<pre><code>docsdissim &lt;- dissimilarity(docsTDM, method = ""cosine"")
</code></pre>

<blockquote>
  <p>Error: could not find function ""dissimilarity""</p>
</blockquote>

<p>Then I learned that the ""dissimilarity"" function was replaced by the <code>dist</code> function, so I did:</p>

<pre><code>docsdissim &lt;- dist(docsTDM, method = ""cosine"")
</code></pre>

<blockquote>
  <p>Error in crossprod(x, y)/sqrt(crossprod(x) * crossprod(y)) : 
            non-conformable arrays</p>
</blockquote>

<p>And there is where I'm stuck.</p>

<p>By the way, my R version is : </p>

<blockquote>
  <p>R version 3.2.2 (2015-08-14) running on CentOS 7</p>
</blockquote>
","r, text-mining, text-classification, text-analysis","<p>change </p>

<pre><code>docsdissim &lt;- proxy::dist(docsTDM, method = ""cosine"")
</code></pre>

<p>to </p>

<pre><code>docsdissim &lt;- dist(as.matrix(docsTDM), method = ""cosine"")
</code></pre>

<p><code>dist</code> requires as input a numeric matrix, data frame or ""dist"" object and event though a termdocumentmatrix is a matrix, it needs to be transformed here. </p>
",3,3,1629,2015-12-19 15:08:55,https://stackoverflow.com/questions/34372166/error-using-termdocumentmatrix-and-dist-functions-in-r
Adding Special Case Idioms to Python Vader Sentiment,"<p>I've been using Vader Sentiment to do some text sentiment analysis and I noticed that my data has a lot of ""way to go"" phrases that were incorrectly being classified as neutral:</p>

<pre><code>In[11]: sentiment('way to go John')
Out[11]: {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}
</code></pre>

<p>After digging into the Vader Source Code, I found the following dictionary:</p>

<pre><code># check for special case idioms using a sentiment-laden keyword known to SAGE
SPECIAL_CASE_IDIOMS = {""the shit"": 3, ""the bomb"": 3, ""bad ass"": 1.5, ""yeah right"": -2,
                       ""cut the mustard"": 2, ""kiss of death"": -1.5, ""hand to mouth"": -2,
                       ""way to go"": 3}
</code></pre>

<p>As you can see, I added the ""Way to go"" entry manually. However, it seems to have no effect:</p>

<pre><code>In [12]: sentiment('way to go John')
Out[12]: {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}
</code></pre>

<p>Any idea what I am missing? Or more specifically, what do I need to do to make adding custom idioms work? Here is the Vader Sentiment source code:</p>

<pre><code>#######################################################################################################################
# SENTIMENT SCORING SCRIPT
#######################################################################################################################
'''
Created on July 04, 2013
@author: C.J. Hutto
  Hutto, C.J. &amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for 
  Sentiment Analysis of Social Media Text. Eighth International Conference on 
  Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.
'''

import os, math, re, sys, fnmatch, string 
reload(sys)

f = 'C:\\Users\\jamacwan\\Code\\Python\\Twitter API\\Sentiment Analysis\\vader_sentiment_lexicon.txt' 

def make_lex_dict(f):
    return dict(map(lambda (w, m): (w, float(m)), [wmsr.strip().split('\t')[0:2] for wmsr in open(f) ]))

WORD_VALENCE_DICT = make_lex_dict(f)

# empirically derived valence ratings for words, emoticons, slang, swear words, acronyms/initialisms 


##CONSTANTS#####

#(empirically derived mean sentiment intensity rating increase for booster words)
B_INCR = 0.293
B_DECR = -0.293

#(empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)
c_INCR = 0.733

# for removing punctuation
REGEX_REMOVE_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))

PUNC_LIST = [""."", ""!"", ""?"", "","", "";"", "":"", ""-"", ""'"", ""\"""",
                ""!!"", ""!!!"", ""??"", ""???"", ""?!?"", ""!?!"", ""?!?!"", ""!?!?""]

NEGATE = [""aint"", ""arent"", ""cannot"", ""cant"", ""couldnt"", ""darent"", ""didnt"", ""doesnt"",
              ""ain't"", ""aren't"", ""can't"", ""couldn't"", ""daren't"", ""didn't"", ""doesn't"",
              ""dont"", ""hadnt"", ""hasnt"", ""havent"", ""isnt"", ""mightnt"", ""mustnt"", ""neither"",
              ""don't"", ""hadn't"", ""hasn't"", ""haven't"", ""isn't"", ""mightn't"", ""mustn't"",
              ""neednt"", ""needn't"", ""never"", ""none"", ""nope"", ""nor"", ""not"", ""nothing"", ""nowhere"",
              ""oughtnt"", ""shant"", ""shouldnt"", ""uhuh"", ""wasnt"", ""werent"",
              ""oughtn't"", ""shan't"", ""shouldn't"", ""uh-uh"", ""wasn't"", ""weren't"",
              ""without"", ""wont"", ""wouldnt"", ""won't"", ""wouldn't"", ""rarely"", ""seldom"", ""despite""]

# booster/dampener 'intensifiers' or 'degree adverbs' http://en.wiktionary.org/wiki/Category:English_degree_adverbs

BOOSTER_DICT = {""absolutely"": B_INCR, ""amazingly"": B_INCR, ""awfully"": B_INCR, ""completely"": B_INCR, ""considerably"": B_INCR,
                ""decidedly"": B_INCR, ""deeply"": B_INCR, ""effing"": B_INCR, ""enormously"": B_INCR,
                ""entirely"": B_INCR, ""especially"": B_INCR, ""exceptionally"": B_INCR, ""extremely"": B_INCR,
                ""fabulously"": B_INCR, ""flipping"": B_INCR, ""flippin"": B_INCR,
                ""fricking"": B_INCR, ""frickin"": B_INCR, ""frigging"": B_INCR, ""friggin"": B_INCR, ""fully"": B_INCR, ""fucking"": B_INCR,
                ""greatly"": B_INCR, ""hella"": B_INCR, ""highly"": B_INCR, ""hugely"": B_INCR, ""incredibly"": B_INCR,
                ""intensely"": B_INCR, ""majorly"": B_INCR, ""more"": B_INCR, ""most"": B_INCR, ""particularly"": B_INCR,
                ""purely"": B_INCR, ""quite"": B_INCR, ""really"": B_INCR, ""remarkably"": B_INCR,
                ""so"": B_INCR,  ""substantially"": B_INCR,
                ""thoroughly"": B_INCR, ""totally"": B_INCR, ""tremendously"": B_INCR,
                ""uber"": B_INCR, ""unbelievably"": B_INCR, ""unusually"": B_INCR, ""utterly"": B_INCR,
                ""very"": B_INCR,
                ""almost"": B_DECR, ""barely"": B_DECR, ""hardly"": B_DECR, ""just enough"": B_DECR,
                ""kind of"": B_DECR, ""kinda"": B_DECR, ""kindof"": B_DECR, ""kind-of"": B_DECR,
                ""less"": B_DECR, ""little"": B_DECR, ""marginally"": B_DECR, ""occasionally"": B_DECR, ""partly"": B_DECR,
                ""scarcely"": B_DECR, ""slightly"": B_DECR, ""somewhat"": B_DECR,
                ""sort of"": B_DECR, ""sorta"": B_DECR, ""sortof"": B_DECR, ""sort-of"": B_DECR}

# check for special case idioms using a sentiment-laden keyword known to SAGE
SPECIAL_CASE_IDIOMS = {""the shit"": 3, ""the bomb"": 3, ""bad ass"": 1.5, ""yeah right"": -2,
                       ""cut the mustard"": 2, ""kiss of death"": -1.5, ""hand to mouth"": -2,
                       ""way to go"": 6}

def negated(list, nWords=[], includeNT=True):
    nWords.extend(NEGATE)
    for word in nWords:
        if word in list:
            return True
    if includeNT:
        for word in list:
            if ""n't"" in word:
                return True
    if ""least"" in list:
        i = list.index(""least"")
        if i &gt; 0 and list[i-1] != ""at"":
            return True
    return False

def normalize(score, alpha=15):
    # normalize the score to be between -1 and 1 using an alpha that approximates the max expected value
    normScore = score/math.sqrt( ((score*score) + alpha) )
    return normScore

def wildCardMatch(patternWithWildcard, listOfStringsToMatchAgainst):
    listOfMatches = fnmatch.filter(listOfStringsToMatchAgainst, patternWithWildcard)
    return listOfMatches


def isALLCAP_differential(wordList):
    countALLCAPS= 0
    for w in wordList:
        if w.isupper():
            countALLCAPS += 1
    cap_differential = len(wordList) - countALLCAPS
    if cap_differential &gt; 0 and cap_differential &lt; len(wordList):
        isDiff = True
    else: isDiff = False
    return isDiff

#check if the preceding words increase, decrease, or negate/nullify the valence
def scalar_inc_dec(word, valence, isCap_diff):
    scalar = 0.0
    word_lower = word.lower()
    if word_lower in BOOSTER_DICT:
        scalar = BOOSTER_DICT[word_lower]
        if valence &lt; 0: scalar *= -1
        #check if booster/dampener word is in ALLCAPS (while others aren't)
        if word.isupper() and isCap_diff:
            if valence &gt; 0: scalar += c_INCR
            else:  scalar -= c_INCR
    return scalar

def sentiment(text):
    """"""
    Returns a float for sentiment strength based on the input text.
    Positive values are positive valence, negative value are negative valence.
    """"""
    if not isinstance(text, unicode) and not isinstance(text, str):
        text = str(text)

    wordsAndEmoticons = text.split() #doesn't separate words from adjacent punctuation (keeps emoticons &amp; contractions)
    text_mod = REGEX_REMOVE_PUNCTUATION.sub('', text) # removes punctuation (but loses emoticons &amp; contractions)
    wordsOnly = text_mod.split()
    # get rid of empty items or single letter ""words"" like 'a' and 'I' from wordsOnly
    for word in wordsOnly:
        if len(word) &lt;= 1:
            wordsOnly.remove(word)    
    # now remove adjacent &amp; redundant punctuation from [wordsAndEmoticons] while keeping emoticons and contractions

    for word in wordsOnly:
        for p in PUNC_LIST:
            pword = p + word
            x1 = wordsAndEmoticons.count(pword)
            while x1 &gt; 0:
                i = wordsAndEmoticons.index(pword)
                wordsAndEmoticons.remove(pword)
                wordsAndEmoticons.insert(i, word)
                x1 = wordsAndEmoticons.count(pword)

            wordp = word + p
            x2 = wordsAndEmoticons.count(wordp)
            while x2 &gt; 0:
                i = wordsAndEmoticons.index(wordp)
                wordsAndEmoticons.remove(wordp)
                wordsAndEmoticons.insert(i, word)
                x2 = wordsAndEmoticons.count(wordp)

    # get rid of residual empty items or single letter ""words"" like 'a' and 'I' from wordsAndEmoticons
    for word in wordsAndEmoticons:
        if len(word) &lt;= 1:
            wordsAndEmoticons.remove(word)

    # remove stopwords from [wordsAndEmoticons]
    #stopwords = [str(word).strip() for word in open('stopwords.txt')]
    #for word in wordsAndEmoticons:
    #    if word in stopwords:
    #        wordsAndEmoticons.remove(word)

    # check for negation

    isCap_diff = isALLCAP_differential(wordsAndEmoticons)

    sentiments = []
    for item in wordsAndEmoticons:
        v = 0
        i = wordsAndEmoticons.index(item)
        if (i &lt; len(wordsAndEmoticons)-1 and item.lower() == ""kind"" and \
           wordsAndEmoticons[i+1].lower() == ""of"") or item.lower() in BOOSTER_DICT:
            sentiments.append(v)
            continue
        item_lowercase = item.lower()
        if item_lowercase in WORD_VALENCE_DICT:
            #get the sentiment valence
            v = float(WORD_VALENCE_DICT[item_lowercase])

            #check if sentiment laden word is in ALLCAPS (while others aren't)

            if item.isupper() and isCap_diff:
                if v &gt; 0: v += c_INCR
                else: v -= c_INCR


            n_scalar = -0.74
            if i &gt; 0 and wordsAndEmoticons[i-1].lower() not in WORD_VALENCE_DICT:
                s1 = scalar_inc_dec(wordsAndEmoticons[i-1], v,isCap_diff)
                v = v+s1
                if negated([wordsAndEmoticons[i-1]]): v = v*n_scalar
            if i &gt; 1 and wordsAndEmoticons[i-2].lower() not in WORD_VALENCE_DICT:
                s2 = scalar_inc_dec(wordsAndEmoticons[i-2], v,isCap_diff)
                if s2 != 0: s2 = s2*0.95
                v = v+s2
                # check for special use of 'never' as valence modifier instead of negation
                if wordsAndEmoticons[i-2] == ""never"" and (wordsAndEmoticons[i-1] == ""so"" or wordsAndEmoticons[i-1] == ""this""): 
                    v = v*1.5                    
                # otherwise, check for negation/nullification
                elif negated([wordsAndEmoticons[i-2]]): v = v*n_scalar
            if i &gt; 2 and wordsAndEmoticons[i-3].lower() not in WORD_VALENCE_DICT:
                s3 = scalar_inc_dec(wordsAndEmoticons[i-3], v,isCap_diff)
                if s3 != 0: s3 = s3*0.9
                v = v+s3
                # check for special use of 'never' as valence modifier instead of negation
                if wordsAndEmoticons[i-3] == ""never"" and \
                   (wordsAndEmoticons[i-2] == ""so"" or wordsAndEmoticons[i-2] == ""this"") or \
                   (wordsAndEmoticons[i-1] == ""so"" or wordsAndEmoticons[i-1] == ""this""):
                    v = v*1.25
                # otherwise, check for negation/nullification
                elif negated([wordsAndEmoticons[i-3]]): v = v*n_scalar


                # future work: consider other sentiment-laden idioms
                #other_idioms = {""back handed"": -2, ""blow smoke"": -2, ""blowing smoke"": -2, ""upper hand"": 1, ""break a leg"": 2, 
                #                ""cooking with gas"": 2, ""in the black"": 2, ""in the red"": -2, ""on the ball"": 2,""under the weather"": -2}

                onezero = u""{} {}"".format(wordsAndEmoticons[i-1], wordsAndEmoticons[i])
                twoonezero = u""{} {} {}"".format(wordsAndEmoticons[i-2], wordsAndEmoticons[i-1], wordsAndEmoticons[i])
                twoone = u""{} {}"".format(wordsAndEmoticons[i-2], wordsAndEmoticons[i-1])
                threetwoone = u""{} {} {}"".format(wordsAndEmoticons[i-3], wordsAndEmoticons[i-2], wordsAndEmoticons[i-1])
                threetwo = u""{} {}"".format(wordsAndEmoticons[i-3], wordsAndEmoticons[i-2])
                if onezero in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[onezero]
                elif twoonezero in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[twoonezero]
                elif twoone in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[twoone]
                elif threetwoone in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[threetwoone]
                elif threetwo in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[threetwo]
                if len(wordsAndEmoticons)-1 &gt; i:
                    zeroone = u""{} {}"".format(wordsAndEmoticons[i], wordsAndEmoticons[i+1])
                    if zeroone in SPECIAL_CASE_IDIOMS:
                        v = SPECIAL_CASE_IDIOMS[zeroone]
                if len(wordsAndEmoticons)-1 &gt; i+1:
                    zeroonetwo = u""{} {}"".format(wordsAndEmoticons[i], wordsAndEmoticons[i+1], wordsAndEmoticons[i+2])
                    if zeroonetwo in SPECIAL_CASE_IDIOMS:
                        v = SPECIAL_CASE_IDIOMS[zeroonetwo]

                # check for booster/dampener bi-grams such as 'sort of' or 'kind of'
                if threetwo in BOOSTER_DICT or twoone in BOOSTER_DICT:
                    v = v+B_DECR

            # check for negation case using ""least""
            if i &gt; 1 and wordsAndEmoticons[i-1].lower() not in WORD_VALENCE_DICT \
                and wordsAndEmoticons[i-1].lower() == ""least"":
                if (wordsAndEmoticons[i-2].lower() != ""at"" and wordsAndEmoticons[i-2].lower() != ""very""):
                    v = v*n_scalar
            elif i &gt; 0 and wordsAndEmoticons[i-1].lower() not in WORD_VALENCE_DICT \
                and wordsAndEmoticons[i-1].lower() == ""least"":
                v = v*n_scalar
        sentiments.append(v) 

    # check for modification in sentiment due to contrastive conjunction 'but'
    if 'but' in wordsAndEmoticons or 'BUT' in wordsAndEmoticons:
        try: bi = wordsAndEmoticons.index('but')
        except: bi = wordsAndEmoticons.index('BUT')
        for s in sentiments:
            si = sentiments.index(s)
            if si &lt; bi: 
                sentiments.pop(si)
                sentiments.insert(si, s*0.5)
            elif si &gt; bi: 
                sentiments.pop(si)
                sentiments.insert(si, s*1.5) 

    if sentiments:                      
        sum_s = float(sum(sentiments))
        #print sentiments, sum_s

        # check for added emphasis resulting from exclamation points (up to 4 of them)
        ep_count = text.count(""!"")
        if ep_count &gt; 4: ep_count = 4
        ep_amplifier = ep_count*0.292 #(empirically derived mean sentiment intensity rating increase for exclamation points)
        if sum_s &gt; 0:  sum_s += ep_amplifier
        elif  sum_s &lt; 0: sum_s -= ep_amplifier

        # check for added emphasis resulting from question marks (2 or 3+)
        qm_count = text.count(""?"")
        qm_amplifier = 0
        if qm_count &gt; 1:
            if qm_count &lt;= 3: qm_amplifier = qm_count*0.18
            else: qm_amplifier = 0.96
            if sum_s &gt; 0:  sum_s += qm_amplifier
            elif  sum_s &lt; 0: sum_s -= qm_amplifier

        compound = normalize(sum_s)

        # want separate positive versus negative sentiment scores
        pos_sum = 0.0
        neg_sum = 0.0
        neu_count = 0
        for sentiment_score in sentiments:
            if sentiment_score &gt; 0:
                pos_sum += (float(sentiment_score) +1) # compensates for neutral words that are counted as 1
            if sentiment_score &lt; 0:
                neg_sum += (float(sentiment_score) -1) # when used with math.fabs(), compensates for neutrals
            if sentiment_score == 0:
                neu_count += 1

        if pos_sum &gt; math.fabs(neg_sum): pos_sum += (ep_amplifier+qm_amplifier)
        elif pos_sum &lt; math.fabs(neg_sum): neg_sum -= (ep_amplifier+qm_amplifier)

        total = pos_sum + math.fabs(neg_sum) + neu_count
        pos = math.fabs(pos_sum / total)
        neg = math.fabs(neg_sum / total)
        neu = math.fabs(neu_count / total)

    else:
        compound = 0.0; pos = 0.0; neg = 0.0; neu = 0.0

    s = {""neg"" : round(neg, 3), 
         ""neu"" : round(neu, 3),
         ""pos"" : round(pos, 3),
         ""compound"" : round(compound, 4)}
    return s


if __name__ == '__main__':
    # --- examples -------
    sentences = [
                u""VADER is smart, handsome, and funny."",       # positive sentence example
                u""VADER is smart, handsome, and funny!"",       # punctuation emphasis handled correctly (sentiment intensity adjusted)
                u""VADER is very smart, handsome, and funny."",  # booster words handled correctly (sentiment intensity adjusted)
                u""VADER is VERY SMART, handsome, and FUNNY."",  # emphasis for ALLCAPS handled
                u""VADER is VERY SMART, handsome, and FUNNY!!!"",# combination of signals - VADER appropriately adjusts intensity
                u""VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!"",# booster words &amp; punctuation make this close to ceiling for score
                u""The book was good."",         # positive sentence
                u""The book was kind of good."", # qualified positive sentence is handled correctly (intensity adjusted)
                u""The plot was good, but the characters are uncompelling and the dialog is not great."", # mixed negation sentence
                u""A really bad, horrible book."",       # negative sentence with booster words
                u""At least it isn't a horrible book."", # negated negative sentence with contraction
                u"":) and :D"",     # emoticons handled
                u"""",              # an empty string is correctly handled
                u""Today sux"",     #  negative slang handled
                u""Today sux!"",    #  negative slang with punctuation emphasis handled
                u""Today SUX!"",    #  negative slang with capitalization emphasis
                u""Today kinda sux! But I'll get by, lol"" # mixed sentiment example with slang and constrastive conjunction ""but""
                 ]
    paragraph = ""It was one of the worst movies I've seen, despite good reviews. \
    Unbelievably bad acting!! Poor direction. VERY poor production. \
    The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!""

    from nltk import tokenize
    lines_list = tokenize.sent_tokenize(paragraph)
    sentences.extend(lines_list)

    tricky_sentences = [
                        ""Most automated sentiment analysis tools are shit."",
                        ""VADER sentiment analysis is the shit."",
                        ""Sentiment analysis has never been good."",
                        ""Sentiment analysis with VADER has never been this good."",
                        ""Warren Beatty has never been so entertaining."",
                        ""I won't say that the movie is astounding and I wouldn't claim that the movie is too banal either."",
                        ""I like to hate Michael Bay films, but I couldn't fault this one"",
                        ""It's one thing to watch an Uwe Boll film, but another thing entirely to pay for it"",
                        ""The movie was too good"",
                        ""This movie was actually neither that funny, nor super witty."",
                        ""This movie doesn't care about cleverness, wit or any other kind of intelligent humor."",
                        ""Those who find ugly meanings in beautiful things are corrupt without being charming."",
                        ""There are slow and repetitive parts, BUT it has just enough spice to keep it interesting."",
                        ""The script is not fantastic, but the acting is decent and the cinematography is EXCELLENT!"", 
                        ""Roger Dodger is one of the most compelling variations on this theme."",
                        ""Roger Dodger is one of the least compelling variations on this theme."",
                        ""Roger Dodger is at least compelling as a variation on the theme."",
                        ""they fall in love with the product"",
                        ""but then it breaks"",
                        ""usually around the time the 90 day warranty expires"",
                        ""the twin towers collapsed today"",
                        ""However, Mr. Carter solemnly argues, his client carried out the kidnapping under orders and in the ''least offensive way possible.''""
                        ]
    sentences.extend(tricky_sentences)
    for sentence in sentences:
        print sentence
        ss = sentiment(sentence)
        print ""\t"" + str(ss)

    print ""\n\n Done!""
</code></pre>
","python, sentiment-analysis, text-classification","<p>The code has several problems:</p>

<ol>
<li><p>Special cases works only for words in <code>vader_sentiment_lexicon.txt</code> because:</p>

<pre><code>if item_lowercase in WORD_VALENCE_DICT:
    #get the sentiment valence
    ...
    if onezero in SPECIAL_CASE_IDIOMS:
        v = SPECIAL_CASE_IDIOMS[onezero]
...
</code></pre>

<p>If you change you phrase to contain such a word, for example 'abandon', then this passes ok.
How to fix: </p>

<pre><code>if item_lowercase in WORD_VALENCE_DICT:
    #get the sentiment valence
    v = float(WORD_VALENCE_DICT[item_lowercase])
else:
    v = 0
#move next statements out of if
#check if sentiment laden word is in ALLCAPS (while others aren't)
if item.isupper() and isCap_diff:
        if v &gt; 0: v += c_INCR
        else: v -= c_INCR
</code></pre>

<p>plus some fixes inside.</p></li>
<li><p>Special cases are checked only if special word has at least 3rd position (index > 2).</p>

<pre><code>if i &gt; 0 and wordsAndEmoticons[i-1].lower() not in WORD_VALENCE_DICT:
    ... # no SPECIAL_CASE_IDIOMS
if i &gt; 1 and wordsAndEmoticons[i-2].lower() not in WORD_VALENCE_DICT:
    ... # no SPECIAL_CASE_IDIOMS
if i &gt; 2 and wordsAndEmoticons[i-3].lower() not in WORD_VALENCE_DICT:
    ...
    twoonezero = u""{} {} {}"".format(wordsAndEmoticons[i-2], wordsAndEmoticons[i-1], wordsAndEmoticons[i])
    ...
    elif twoonezero in SPECIAL_CASE_IDIOMS: ...
</code></pre>

<p>Here for phrase <code>way to abandon John</code> word <code>abandon</code> has index 2, but there's no such case. If we change phrase to <code>you way to abandon John</code>, then it starts working.
How to fix: move SPECIAL cases up one branch. Or better use real length of special case, than try to hardcode.</p></li>
</ol>

<p>Resume: code will not be easy in support.</p>
",3,5,2849,2015-12-21 16:43:58,https://stackoverflow.com/questions/34400485/adding-special-case-idioms-to-python-vader-sentiment
R - Automatic categorization of Wikipedia articles,"<p>I have been trying to follow this <a href=""http://www.rexamine.com/2014/06/text-mining-in-r-automatic-categorization-of-wikipedia-articles/"" rel=""nofollow"">example</a> by Norbert Ryciak, whom I havent been able to get in touch with.</p>

<p>Since this article was written in 2014, some things in R have changed so I have been able to update some of those things in the code, but I got stuck in the last part.</p>

<p>Here is my Working code so far:</p>

<pre><code> library(tm)
 library(stringi)
 library(proxy)

 wiki &lt;- ""https://en.wikipedia.org/wiki/""

 titles &lt;- c(""Integral"", ""Riemann_integral"", ""Riemann-Stieltjes_integral"",  ""Derivative"",
  ""Limit_of_a_sequence"", ""Edvard_Munch"", ""Vincent_van_Gogh"", ""Jan_Matejko"",
  ""Lev_Tolstoj"", ""Franz_Kafka"", ""J._R._R._Tolkien"")

 articles &lt;- character(length(titles))

 for (i in 1:length(titles)) {
   articles[i] &lt;- stri_flatten(readLines(stri_paste(wiki, titles[i])), col = "" "")
  }

 docs &lt;- Corpus(VectorSource(articles))

 docs[[1]]
 docs2 &lt;- tm_map(docs, function(x) stri_replace_all_regex(x, ""&lt;.+?&gt;"", "" ""))
 docs3 &lt;- tm_map(docs2, function(x) stri_replace_all_fixed(x, ""\t"", "" ""))
 docs4 &lt;- tm_map(docs3, PlainTextDocument)
 docs5 &lt;- tm_map(docs4, stripWhitespace)
 docs6 &lt;- tm_map(docs5, removeWords, stopwords(""english""))
 docs7 &lt;- tm_map(docs6, removePunctuation)
 docs8 &lt;- tm_map(docs7, content_transformer(tolower))
 docs8[[1]]

 docsTDM &lt;- TermDocumentMatrix(docs8)
 docsTDM2 &lt;- as.matrix(docsTDM)
 docsdissim &lt;- dist(docsTDM2, method = ""cosine"")
</code></pre>

<p>But I havent been able to get pass this part:</p>

<pre><code> docsdissim2 &lt;- as.matrix(docsdissim)
 rownames(docsdissim2) &lt;- titles
 colnames(docsdissim2) &lt;- titles
 docsdissim2
 h &lt;- hclust(docsdissim, method = ""ward.D"")
 plot(h, labels = titles, sub = """")
</code></pre>

<p>I tried to run the ""hclust"" directly, and then I was able to Plot, but nothing readable came out of it.</p>

<p>This are the errors Im getting:</p>

<pre><code> rownames(docsdissim2) &lt;- titles
 Error in `rownames&lt;-`(`*tmp*`, value = c(""Integral"", ""Riemann_integral"",  : 
   length of 'dimnames' [1] not equal to array extent
</code></pre>

<p>Another:</p>

<pre><code> plot(h, labels = titles, sub = """")
 Error in graphics:::plotHclust(n1, merge, height, order(x$order), hang,  : 
   invalid dendrogram input
</code></pre>

<p>Is there anyone that could give me a hand to finish this example?</p>

<p>Best Regards,</p>
","r, text-classification","<p>I was able to solve this problem thanks to Norbert Ryciak (the author of the tutorial).</p>

<p>Since he used an older version of ""tm"" (which was probably the latest at the time) it was not compatible with the one I used.</p>

<p>The solution was to replace ""docsTDM &lt;- TermDocumentMatrix(docs8)"" with ""docsTDM &lt;- DocumentTermMatrix(docs8)"".</p>

<p>So the final code:</p>

<pre><code> library(tm)
 library(stringi)
 library(proxy)

 wiki &lt;- ""https://en.wikipedia.org/wiki/""

 titles &lt;- c(""Integral"", ""Riemann_integral"", ""Riemann-Stieltjes_integral"",  ""Derivative"",
  ""Limit_of_a_sequence"", ""Edvard_Munch"", ""Vincent_van_Gogh"", ""Jan_Matejko"",
  ""Lev_Tolstoj"", ""Franz_Kafka"", ""J._R._R._Tolkien"")

 articles &lt;- character(length(titles))

 for (i in 1:length(titles)) {
   articles[i] &lt;- stri_flatten(readLines(stri_paste(wiki, titles[i])), col =     "" "")
  }

 docs &lt;- Corpus(VectorSource(articles))

 docs[[1]]
 docs2 &lt;- tm_map(docs, function(x) stri_replace_all_regex(x, ""&lt;.+?&gt;"", "" ""))
 docs3 &lt;- tm_map(docs2, function(x) stri_replace_all_fixed(x, ""\t"", "" ""))
 docs4 &lt;- tm_map(docs3, PlainTextDocument)
 docs5 &lt;- tm_map(docs4, stripWhitespace)
 docs6 &lt;- tm_map(docs5, removeWords, stopwords(""english""))
 docs7 &lt;- tm_map(docs6, removePunctuation)
 docs8 &lt;- tm_map(docs7, content_transformer(tolower))
 docs8[[1]]

 docsTDM &lt;- DocumentTermMatrix(docs8)
 docsTDM2 &lt;- as.matrix(docsTDM)
 docsdissim &lt;- dist(docsTDM2, method = ""cosine"")

 docsdissim2 &lt;- as.matrix(docsdissim)
 rownames(docsdissim2) &lt;- titles
 colnames(docsdissim2) &lt;- titles
 docsdissim2
 h &lt;- hclust(docsdissim, method = ""ward"")
 plot(h, labels = titles, sub = """")
</code></pre>
",1,0,567,2015-12-22 20:18:32,https://stackoverflow.com/questions/34423823/r-automatic-categorization-of-wikipedia-articles
Dataset for training text classifier,"<p>I'm new to data mining and I am trying to build a classifier that is able to classify student theses abstracts into a predefined set of categories under the area of Computer Science, e.g. Machine learning, Image Processing...etc.
I do not have enough classified abstracts to be used as a training dataset so would you please direct me to a dataset that can be used for this particular purpose. </p>
","text-mining, text-classification","<p>You can use the DBLP data (downloadable from <a href=""http://dblp.uni-trier.de/xml/"" rel=""nofollow"">http://dblp.uni-trier.de/xml/</a>) to generate a list of publications. Based on conferences/journal you can generate your classes e.g. MLJR is alway Machine Learning.</p>

<p>The abstracts you can acquire using:
<a href=""https://github.com/arc12/Text-Mining-Weak-Signals/blob/master/Abstract%20Acquisition%20Scripts/DBLP%20XML%20fetch%20abstracts%20.pl"" rel=""nofollow"">https://github.com/arc12/Text-Mining-Weak-Signals/blob/master/Abstract%20Acquisition%20Scripts/DBLP%20XML%20fetch%20abstracts%20.pl</a></p>
",0,-3,82,2015-12-24 14:11:36,https://stackoverflow.com/questions/34454065/dataset-for-training-text-classifier
Java - Method for batch processing text files is much slower then the same action individually the same amount of times,"<p>I wrote a method <code>processTrainDirectory</code> which is supposed to import and process all the text files from a given directory. Individually processing the files takes about the same time for each file (90ms), but when I use the method for batch importing a given directory, the time per file increases incrementally (from 90ms to over 4000ms after 300 files). The batch importing method is as follows:</p>

<pre><code>public void processTrainDirectory(String folderPath, Category category) {
    File folder = new File(folderPath);
    File[] listOfFiles = folder.listFiles();
    if (listOfFiles != null) {
        for (File file : listOfFiles) {
            if (file.isFile()) {
                processTrainText(file.getPath(), category);
            }
        }
    }
    else {
        System.out.println(foo);
    }

}
</code></pre>

<p>As I said, the method <code>processTrainText</code> is called per text file in the directory. This method takes incrementally longer when used inside <code>processTrainDirectory</code>. The method <code>processTrainText</code> is as follows:</p>

<pre><code> public void processTrainText(String path, Category category){
    trainTextAmount++;
    Map&lt;String, Integer&gt; text = prepareText(path);
    update(text, category);

}
</code></pre>

<p>I called <code>processTrainText</code> 200 times on 200 different texts manual and the time that this took was 200 * 90ms. But when I have a directory of 200 files and use <code>processTrainDirectory</code> it takes 90-92-96-104....3897-3940-4002ms which is WAY longer.</p>

<p>The problem persists when I call <code>processTrainText</code> a second time; it does not reset. Do you have any idea why this is or what the cause it, and how I can solve it? </p>

<p>Any help is greatly appreciated!</p>

<p>EDIT: somebody asked what other called methods did so here are all the used methods from my class <code>BayesianClassifier</code> all others are deleted for clarification, underneath you can find the class <code>Category</code>: </p>

<pre><code>public class BayesianClassifier {
    private Map&lt;String, Integer&gt; vocabulary;
    private List&lt;Category&gt; categories;
    private int trainTextAmount;
    private int testTextAmount;
    private GUI gui;


    public Map&lt;String, Integer&gt; prepareText(String path) {
        String text = readText(path);
        String normalizedText = normalizeText(text);
        String[] tokenizedText = tokenizeText(normalizedText);
        return countText(tokenizedText);
    }

    public String readText(String path) {
        BufferedReader br;
        String result = """";
        try {

            br = new BufferedReader(new FileReader(path));
            StringBuilder sb = new StringBuilder();
            String line = br.readLine();

            while (line != null) {
                sb.append(line);
                sb.append(""\n"");
                line = br.readLine();
            }
            result = sb.toString();
            br.close();
        } catch (IOException e) {
            e.printStackTrace();

        }

        return result;
    }


    public Map&lt;String, Integer&gt; countText(String[] words){
        Map&lt;String, Integer&gt; result = new HashMap&lt;&gt;();
        for(int i=0; i &lt; words.length; i++){
            if (!result.containsKey(words[i])){
                result.put(words[i], 1);
            }
            else {
                result.put(words[i], result.get(words[i]) + 1);
            }
        }
          return result;
    }

    public void processTrainText(String path, Category category){
        trainTextAmount++;
        Map&lt;String, Integer&gt; text = prepareText(path);
        update(text, category);   
    }

    public void update(Map&lt;String, Integer&gt; text, Category category) {
        category.addText();
        for (Map.Entry&lt;String, Integer&gt; entry : text.entrySet()){
            if(!vocabulary.containsKey(entry.getKey())){
                vocabulary.put(entry.getKey(), entry.getValue());
                category.updateFrequency(entry);
                category.updateProbability(entry);
                category.updatePrior();
            }

            else {
                vocabulary.put(entry.getKey(), vocabulary.get(entry.getKey()) + entry.getValue());
                category.updateFrequency(entry);
                category.updateProbability(entry);
                category.updatePrior();
            }

            for(Category cat : categories){
                if (!cat.equals(category)){
                    cat.addWord(entry.getKey());
                    cat.updatePrior();
                }
            }
        }
    }

    public void processTrainDirectory(String folderPath, Category category) {
        File folder = new File(folderPath);
        File[] listOfFiles = folder.listFiles();
        if (listOfFiles != null) {
            for (File file : listOfFiles) {
                if (file.isFile()) {
                    processTrainText(file.getPath(), category);
                }
            }
        }
        else {
            System.out.println(foo);
        }

    }
</code></pre>

<p>This is my <code>Category</code> class (all the methods that are not needed are deleted for clarification:</p>

<pre><code>public class Category {
    private String categoryName;
    private double prior;
    private Map&lt;String, Integer&gt; frequencies;
    private Map&lt;String, Double&gt; probabilities;
    private int textAmount;
    private BayesianClassifier bc;

    public Category(String categoryName, BayesianClassifier bc){
        this.categoryName = categoryName;
        this.bc = bc;
        this.frequencies = new HashMap&lt;&gt;();
        this.probabilities = new HashMap&lt;&gt;();
        this.textAmount = 0;
        this.prior = 0.00;
    }

    public void addWord(String word){
        this.frequencies.put(word, 0);
        this.probabilities.put(word, 0.0);
    }

    public void updateFrequency(Map.Entry&lt;String, Integer&gt; entry){
        if(!this.frequencies.containsKey(entry.getKey())){
            this.frequencies.put(entry.getKey(), entry.getValue());
        }
        else {
            this.frequencies.put(entry.getKey(), this.frequencies.get(entry.getKey()) + entry.getValue());
        }
    }

    public void updateProbability(Map.Entry&lt;String, Integer&gt; entry){
        double chance = ((double) this.frequencies.get(entry.getKey()) + 1) / (sumFrequencies() + bc.getVocabulary().size());
        this.probabilities.put(entry.getKey(), chance);
    }

    public Integer sumFrequencies(){
        Integer sum = 0;
        for (Integer integer : this.frequencies.values()) {
            sum = sum + integer;
        }
        return sum;
    }  
}
</code></pre>
","java, text, methods, batch-processing, text-classification","<p>It looks like the times per file are growing linearly and the total time quadratically. This means that with each file you're processing the data of all previous files. Indeed, you are:</p>

<p><code>updateProbability</code> calls <code>sumFrequencies</code>, which runs through the entire <code>frequencies</code>, which grows with each file. That's the culprit. Simply create a field <code>int sumFrequencies</code> and update it in `updateFrequency.</p>

<p>As a further improvement, consider using Guava <a href=""http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/collect/Multiset.html"" rel=""nofollow noreferrer"">Multiset</a>, which does the counting in a simpler and more efficient way (no autoboxing). After fixing your code, consider letting it be reviewed on <a href=""https://codereview.stackexchange.com/questions/tagged/java"">CR</a>; there are quite a few minor problems with it.</p>
",2,-1,186,2015-12-29 15:53:07,https://stackoverflow.com/questions/34514462/java-method-for-batch-processing-text-files-is-much-slower-then-the-same-actio
"How can I compute F1 measure for each class, in Multiclass Classification?","<p>I am using <strong>SciKit</strong>, as a library to work with classification algorithms like : NB,SVM.</p>

<p>here is a very nice and fine <a href=""http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html"" rel=""nofollow"">binary classification implementation</a> for ""<strong>SPAM</strong> and <strong>HAM</strong>"" <em>Emails</em>:</p>

<pre><code>    confusion += confusion_matrix(test_y, predictions)
    score = f1_score(test_y, predictions, pos_label=SPAM)
   //note in my case 3-classes I do not need to set [pos_label]
</code></pre>

<p>If I have Three Classes like {SPAM, HAM, NORMAL} instead of two, then: how can I adapt that code to find <strong>F1-Score</strong> for each class and also for all classes as <em>average</em>. </p>
","machine-learning, nltk, computer-science, text-classification","<p>Use classification report in sklearn to compute F-score for multiple classes.</p>

<pre><code>from sklearn.metrics import classification_report as cr
gold = []
pred = []
# given a test set with annotated gold labels
for testinstance, goldlabel in testdata:
    gold.append(goldlabel)
    #clf is your classifier object with predict method
    predictedlabel = clf.predict(testinstance)
    pred.append(predictedlabel)
print cr(gold,pred, digits=4)
</code></pre>
",3,2,1002,2016-01-03 15:51:45,https://stackoverflow.com/questions/34578423/how-can-i-compute-f1-measure-for-each-class-in-multiclass-classification
Convert text dataset to .arff file,"<p>I have this data set <a href=""https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences"" rel=""nofollow"">https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences</a></p>

<p>and I need to convert it from .txt to .arff file to make classification with weka program</p>
","converters, weka, text-mining, text-classification, arff","<p>Use a programming language.</p>

<p>It's not hard to write a program that</p>

<ol>
<li>reads the input format line by line</li>
<li>outputs the arff header</li>
<li>outputs the data in arff sparse vector format</li>
</ol>
",1,-2,1050,2016-01-13 20:22:30,https://stackoverflow.com/questions/34776325/convert-text-dataset-to-arff-file
Scikit-learn: precision_recall_fscore_support returns strange results,"<p>I am doing some text minining/classification and attempt to evaluate performance with the <code>precision_recall_fscore_support</code> function from the <code>sklearn.metrics</code> module. I am not sure how I can create a really small example reproducing the problem, but maybe somebody can help even so because it is something obvious I am missing. </p>

<p>The aforementioned function returns among other things the support for each class. The documentation states</p>

<blockquote>
  <p><strong>support: int (if average is not None) or array of int, shape = [n_unique_labels]</strong> :
  The number of occurrences of each label in y_true.</p>
</blockquote>

<p>But in my case, the number of classes for which support is returned is not the same as the number of different classes in the testing data.     </p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

classifier = svm.SVC(kernel=""linear"")
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred)

print(len(classifier.classes_)) # prints 18
print(len(supp))                # prints 19
print(len(np.unique(y_test)))   # prints 18
</code></pre>

<p>How can this be? How can there be support for a class which is not in the data?</p>
","python, machine-learning, scikit-learn, classification, text-classification","<p>I am not sure what the problem is, but in my case there seems to be a mismatch between the classes learned by the classifier and the ones occurring in the test data. One can force the the function to compute the performance measures for the right classes by explicitly naming them.</p>

<pre><code>prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred, labels=classifier.classes_)
</code></pre>
",0,0,2083,2016-02-05 20:19:49,https://stackoverflow.com/questions/35232804/scikit-learn-precision-recall-fscore-support-returns-strange-results
text classifier with bag of words and additional sentiment feature in sklearn,"<p>I am trying to build a classifier that in addition to bag of words uses features like the sentiment or a topic (LDA result). I have a pandas DataFrame with the text and the label and would like to add a sentiment value (numerical between -5 and 5) and the result of LDA analysis (a string with the topic of the sentence).</p>

<p>I have a working bag of words classifier that uses CountVectorizer from sklearn and performs the classification with MultinomialNaiveBayes.</p>

<pre><code>df = pd.DataFrame.from_records(data=data, columns=names)
train, test = train_test_split(
    df,
    train_size=train_ratio,
    random_state=1337
)
train_df = pd.DataFrame(train, columns=names)
test_df = pd.DataFrame(test, columns=names)
vectorizer = CountVectorizer()
train_matrix = vectorizer.fit_transform(train_df['text'])
test_matrix = vectorizer.transform(test_df['text'])
positive_cases_train = (train_df['label'] == 'decision')
positive_cases_test = (test_df['label'] == 'decision')
classifier = MultinomialNB()
classifier.fit(train_matrix, positive_cases_train)
</code></pre>

<p>The question is now. How can I additionally to the bag of words technique introduce the other features to my classifier?</p>

<p>Thanks in advance and if you need more information I am glad to provide those.</p>

<p>Edit: After adding the rows like suggested by @Guiem a new question regarding weight of the new feature. This Edit adds to that new question:</p>

<p>The shape of my train matrix is <code>(2554, 5286)</code>. The weird thing though is that it is this shape with and without the sentiment column added (Maybe the row is not added properly?)</p>

<p>If I print the Matrix I get the following output:</p>

<pre><code>  (0, 322)  0.0917594575712
  (0, 544)  0.196910480455
  (0, 556)  0.235630958238
  (0, 706)  0.137241420774
  (0, 1080) 0.211125349374
  (0, 1404) 0.216326271935
  (0, 1412) 0.191757369869
  (0, 2175) 0.128800602511
  (0, 2176) 0.271268708356
  (0, 2371) 0.123979845513
  (0, 2523) 0.406583720526
  (0, 3328) 0.278476810585
  (0, 3752) 0.203741786877
  (0, 3847) 0.301505063552
  (0, 4098) 0.213653538407
  (0, 4664) 0.0753937554096
  (0, 4676) 0.164498844366
  (0, 4738) 0.0844966331512
  (0, 4814) 0.251572721805
  (0, 5013) 0.201686066537
  (0, 5128) 0.21174469759
  (0, 5135) 0.187485844479
  (1, 291)  0.227264696182
  (1, 322)  0.0718526940442
  (1, 398)  0.118905396285
  : :
  (2553, 3165)  0.0985290985889
  (2553, 3172)  0.134514497354
  (2553, 3217)  0.0716087169489
  (2553, 3241)  0.172404983302
  (2553, 3342)  0.145912701013
  (2553, 3498)  0.149172538211
  (2553, 3772)  0.140598133976
  (2553, 4308)  0.0704700896603
  (2553, 4323)  0.0800039075449
  (2553, 4505)  0.163830579067
  (2553, 4663)  0.0513678549359
  (2553, 4664)  0.0681930862174
  (2553, 4738)  0.114639856277
  (2553, 4855)  0.140598133976
  (2553, 4942)  0.138370066422
  (2553, 4967)  0.143088901589
  (2553, 5001)  0.185244190321
  (2553, 5008)  0.0876615764151
  (2553, 5010)  0.108531807984
  (2553, 5053)  0.136354534152
  (2553, 5104)  0.0928665728295
  (2553, 5148)  0.171292088292
  (2553, 5152)  0.172404983302
  (2553, 5191)  0.104762377866
  (2553, 5265)  0.123712025565
</code></pre>

<p>I hope that helps a little or did you want some other information?</p>
","python, scikit-learn, text-classification","<p>One option would be to just add these two new features to your CountVectorizer matrix as <strong>columns</strong>.</p>

<p>As you are not performing any tf-idf, your count matrix is going to be filled with integers so you could encode your new columns as int values.</p>

<p>You might have to try several encodings but you can start with something like:</p>

<ul>
<li>sentiment [-5,...,5] transformed to [0,...,10]</li>
<li>string with topic of sentence. Just assign integers to different topics (<code>{'unicorns':0, 'batman':1, ...}</code>), you can keep a dictionary structure to assign integers and avoid repeating topics. </li>
</ul>

<p>And just in case you don't know how to add columns to your train_matrix:</p>

<pre><code>dense_matrix = train_matrix.todense() # countvectorizer returns a sparse matrix
np.insert(dense_matrix,dense_matrix.shape[1],[val1,...,valN],axis=1)
</code></pre>

<p>note that the column <code>[val1,...,valN]</code> needs to have the same lenght as num. samples you are using </p>

<p>Even though it won't be strictly a Bag of Words anymore (because not all columns represent word frequency), just adding this two columns will add up the extra information you want to include. And naive Bayes classifier considers each of the features to contribute independently to the probability, so we are okay here.</p>

<blockquote>
  <p><strong>Update</strong>: better use a 'one hot' encoder to encode <strong>categorical</strong> features (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</a>). This way you prevent weird behavior by assigning integer values to your new features (maybe you can still do that with sentiment, because in a scale of sentiment from 0 to 10 you assume that a 9 sentiment is closer to a sample with sentiment 10 rather than another with sentiment 0). But with categorical features you better do the one-hot encoding.
  So let's say you have 3 topics, then <strong>you can use same technique of adding columns</strong> only now you have to add 3 instead of one [topic1,topic2,topic3]. This way if you have a sample that belongs to topic1, you'll encode this as [1 , 0 , 0], if that's topic3, your representation is [0, 0, 1] (you mark with 1 the column that corresponds to the topic)  </p>
</blockquote>
",3,3,2631,2016-02-07 14:11:41,https://stackoverflow.com/questions/35254526/text-classifier-with-bag-of-words-and-additional-sentiment-feature-in-sklearn
Getting AttributeError on nltk Textual entailment classifier,"<p>Im referring to the link in the section
<a href=""http://www.nltk.org/book/ch06.html#recognizing-textual-entailment"" rel=""nofollow"">http://www.nltk.org/book/ch06.html#recognizing-textual-entailment</a></p>

<pre><code>def rte_features(rtepair):
    extractor = nltk.RTEFeatureExtractor(rtepair)
    features = {}
    features['word_overlap'] = len(extractor.overlap('word'))
    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))
    features['ne_overlap'] = len(extractor.overlap('ne'))
    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))
    return features
rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])

extractor = nltk.RTEFeatureExtractor(rtepair)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-39-a7f96e33ba9e&gt; in &lt;module&gt;()
----&gt; 1 extractor = nltk.RTEFeatureExtractor(rtepair)

C:\Users\RAVINA\Anaconda2\lib\site-packages\nltk\classify\rte_classify.pyc in __init__(self, rtepair, stop, lemmatize)
     65 
     66         #Get the set of word types for text and hypothesis
---&gt; 67         self.text_tokens = tokenizer.tokenize(rtepair.text)
     68         self.hyp_tokens = tokenizer.tokenize(rtepair.hyp)
     69         self.text_words = set(self.text_tokens)

AttributeError: 'list' object has no attribute 'text'
</code></pre>

<p>Its the exact code as mentioned in the book, can anyone help me whats going wrong here.
Thanks
Ravina</p>
","python, nltk, text-classification","<p>Take a look at the type signatures. Type this into the python shell:</p>

<pre><code>import nltk
x = nltk.corpus.rte.pairs(['rte3_dev.xml'])
type(x)
</code></pre>

<p>tells you <code>x</code> is of type list.</p>

<p>Now, type:</p>

<pre><code>help(nltk.RTEFeatureExtractor)
</code></pre>

<p>which tells you:</p>

<blockquote>
  <p>:param rtepair: a <code>RTEPair</code> from which features should be extracted</p>
</blockquote>

<p>Clearly, <code>x</code> does not have the correct type for calling <code>nltk.RTEFeatureExtractor</code>. Instead:</p>

<pre><code>type(x[33])
&lt;class 'nltk.corpus.reader.rte.RTEPair'&gt;
</code></pre>

<p>A single item of the list does have the correct type.</p>

<hr>

<p><strong>Update:</strong>
As mentioned in the comment section, <code>extractor.text_words</code> shows only empty strings. This seems to be due to changes made in NLTK since the documentation was written. Long story short: You won't be able to fix this without downgrading to an older version of NLTK or fixing the problem in NLTK yourself.
Inside the file <code>nltk/classify/rte_classify.py</code>, you will find the following piece of code:</p>

<pre><code>class RTEFeatureExtractor(object):
    …
    import nltk
    from nltk.tokenize import RegexpTokenizer
    tokenizer = RegexpTokenizer('([A-Z]\.)+|\w+|\$[\d\.]+')
    self.text_tokens = tokenizer.tokenize(rtepair.text)
    self.text_words = set(self.text_tokens)
</code></pre>

<p>If you run the same <code>RegexpTokenizer</code> with the exact text from the extractor, it will produce only empty strings:</p>

<pre><code>import nltk
rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer('([A-Z]\.)+|\w+|\$[\d\.]+')
tokenizer.tokenize(rtepair.text)
</code></pre>

<p>Returns <code>['', '', …, '']</code> (i.e., a list of empty strings).</p>
",0,2,560,2016-02-08 09:10:40,https://stackoverflow.com/questions/35265843/getting-attributeerror-on-nltk-textual-entailment-classifier
Multilabel Text Classification using TensorFlow,"<p>The text data is organized as vector with 20,000 elements, like [2, 1, 0, 0, 5, ...., 0]. 
i-th element indicates the frequency of the i-th word in a text. </p>

<p>The ground truth label data is also represented as vector with 4,000 elements, like [0, 0, 1, 0, 1, ...., 0]. 
i-th element indicates whether the i-th label is a positive label for a text. 
The number of labels for a text differs depending on texts. </p>

<p>I have a code for single-label text classification. </p>

<p>How can I edit the following code for multilabel text classification?</p>

<p>Especially, I would like to know following points. </p>

<ul>
<li>How to compute accuracy using TensorFlow. </li>
<li>How to set a threshold which judges whether a label is positive or negative. For instance, if the output is [0.80, 0.43, 0.21, 0.01, 0.32] and the ground truth is [1, 1, 0, 0, 1], the labels with scores over 0.25 should be judged as positive. </li>
</ul>

<p>Thank you. </p>

<pre><code>import tensorflow as tf

# hidden Layer
class HiddenLayer(object):
    def __init__(self, input, n_in, n_out):
        self.input = input

        w_h = tf.Variable(tf.random_normal([n_in, n_out],mean = 0.0,stddev = 0.05))
        b_h = tf.Variable(tf.zeros([n_out]))

        self.w = w_h
        self.b = b_h
        self.params = [self.w, self.b]

    def output(self):
        linarg = tf.matmul(self.input, self.w) + self.b
        self.output = tf.nn.relu(linarg)

        return self.output

# output Layer
class OutputLayer(object):
    def __init__(self, input, n_in, n_out):
        self.input = input

        w_o = tf.Variable(tf.random_normal([n_in, n_out], mean = 0.0, stddev = 0.05))
        b_o = tf.Variable(tf.zeros([n_out]))

        self.w = w_o
        self.b = b_o
        self.params = [self.w, self.b]

    def output(self):
        linarg = tf.matmul(self.input, self.w) + self.b
        self.output = tf.nn.relu(linarg)

        return self.output

# model
def model():
    h_layer = HiddenLayer(input = x, n_in = 20000, n_out = 1000)
    o_layer = OutputLayer(input = h_layer.output(), n_in = 1000, n_out = 4000)

    # loss function
    out = o_layer.output()
    cross_entropy = -tf.reduce_sum(y_*tf.log(out + 1e-9), name='xentropy')    

    # regularization
    l2 = (tf.nn.l2_loss(h_layer.w) + tf.nn.l2_loss(o_layer.w))
    lambda_2 = 0.01

    # compute loss
    loss = cross_entropy + lambda_2 * l2

    # compute accuracy for single label classification task
    correct_pred = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, ""float""))

    return loss, accuracy
</code></pre>
","python, tensorflow, text-classification, multilabel-classification","<p>Change relu to sigmoid of output layer.
Modify cross entropy loss to explicit mathematical formula of sigmoid cross entropy loss (explicit loss was working in my case/version of tensorflow )</p>

<pre><code>import tensorflow as tf

# hidden Layer
class HiddenLayer(object):
    def __init__(self, input, n_in, n_out):
        self.input = input

        w_h = tf.Variable(tf.random_normal([n_in, n_out],mean = 0.0,stddev = 0.05))
        b_h = tf.Variable(tf.zeros([n_out]))

        self.w = w_h
        self.b = b_h
        self.params = [self.w, self.b]

    def output(self):
        linarg = tf.matmul(self.input, self.w) + self.b
        self.output = tf.nn.relu(linarg)

        return self.output

# output Layer
class OutputLayer(object):
    def __init__(self, input, n_in, n_out):
        self.input = input

        w_o = tf.Variable(tf.random_normal([n_in, n_out], mean = 0.0, stddev = 0.05))
        b_o = tf.Variable(tf.zeros([n_out]))

        self.w = w_o
        self.b = b_o
        self.params = [self.w, self.b]

    def output(self):
        linarg = tf.matmul(self.input, self.w) + self.b
        #changed relu to sigmoid
        self.output = tf.nn.sigmoid(linarg)

        return self.output

# model
def model():
    h_layer = HiddenLayer(input = x, n_in = 20000, n_out = 1000)
    o_layer = OutputLayer(input = h_layer.output(), n_in = 1000, n_out = 4000)

    # loss function
    out = o_layer.output()
    # modified cross entropy to explicit mathematical formula of sigmoid cross entropy loss
    cross_entropy = -tf.reduce_sum( (  (y_*tf.log(out + 1e-9)) + ((1-y_) * tf.log(1 - out + 1e-9)) )  , name='xentropy' )    

    # regularization
    l2 = (tf.nn.l2_loss(h_layer.w) + tf.nn.l2_loss(o_layer.w))
    lambda_2 = 0.01

    # compute loss
    loss = cross_entropy + lambda_2 * l2

    # compute accuracy for single label classification task
    correct_pred = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, ""float""))

    return loss, accuracy
</code></pre>
",20,35,25176,2016-02-15 01:10:07,https://stackoverflow.com/questions/35400065/multilabel-text-classification-using-tensorflow
Save progress between multiple instances of partial_fit in Python SGDClassifier,"<p>I've successfully followed <a href=""http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py"" rel=""nofollow"">this</a> example for my own text classification script.</p>

<p>The problem is I'm not looking to process pieces of a huge, but existing data set in a loop of partial_fit calls, like they do in the example. I want to be able to add data as it becomes available, even if I shut down my python script in the meantime. </p>

<p>Ideally I'd like to do something like this:</p>

<p>sometime in 2015:</p>

<p>model2015=partial_fit(dataset2015)</p>

<p>save_to_file(model2015)</p>

<p>shut down my python script</p>

<p>sometime in 2016:</p>

<p>open my python script again</p>

<p>load_from_file(model2015)</p>

<p>partial_fit(dataset2016 incorporating model2015)</p>

<p>save_to_file(model2016)</p>

<p>sometime in 2017:</p>

<p>open my python script again</p>

<p>etc...</p>

<p>Is there any way I can do this in scikit-learn? Or in some other package (Tensorflow perhaps)?</p>
","python-3.x, machine-learning, scikit-learn, text-classification","<p>Simply pickle your model and save it to disk. The other way is to dump .coef_ and .intercept_ fields (which is just two arrays) and use them as initializers when you call <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit"" rel=""nofollow"">.fit</a> </p>
",1,0,669,2016-02-26 22:05:01,https://stackoverflow.com/questions/35662635/save-progress-between-multiple-instances-of-partial-fit-in-python-sgdclassifier
how to store multiple values for one key in python,"<p>The parameter,allWords, contains two column and thousands of rows. The first column tweet. The second one contains a sentiment( 0 for negative and 4 for positive.</p>

<p>As the bottom code shows I have created two dictionaries(negative &amp; positive) to store the word in the dictionary with their frequency. </p>

<p>if you run the code it shows as it follows: </p>

<p>This is for negative dictionary {'transit': 1, 'infect': 4, 'spam': 6}</p>

<p>This is for positive dictionary {'transit': 3, 'infect': 5, 'spam': 2}</p>

<pre><code>   def vectorRepresentation(allWords):       
    negative = {}
    positive = {}

    for (t,s) in allWords:
        if(s=='0'):
            for w in t:
                if w in negative:
                    negative[w]+=1
                else:
                    negative[w]=1
        if(s=='4'):
            for w in t:
                if w in positive:
                    positive[w]+=1
                else:
                    positive[w]=1
     print(negative)
     print(positive)
</code></pre>

<p>However, I want to create one dictionary and store the two values for the same key. For example</p>

<p>newDictionary = {'transit': [1][3], 'infect': [4][5], 'spam': [6][2]}</p>

<p>The first value represents the negative. While, the second value is for positive. How can achieve that?</p>
","python, python-2.7, python-3.x, nltk, text-classification","<p>As I think the structure you want is weird and dont make sense , I put them both in one list :</p>

<pre><code>neg = {'transit': 1, 'infect': 4, 'spam': 6}
pos =  {'transit': 3, 'infect': 5, 'spam': 2}
result = {}
for k,v in neg.items():
    result[k] = [v,pos[k]]
result # {'spam': [6, 2], 'transit': [1, 3], 'infect': [4, 5]}
</code></pre>
",1,1,9274,2016-03-07 20:44:56,https://stackoverflow.com/questions/35853661/how-to-store-multiple-values-for-one-key-in-python
How to classify new sentences with unknown attributes?,"<p>I'm trying to classify 30000 unlabeled sentences into 2 labels(for example, pos and neg) by using machine learning algorithms. To do this, I have chosen 100 positive sentences and 100 negative sentences as training sets. Then, I used SVM to train the 200 chosen sentences to create a trained model. Finally, classified the remaining 29800 unlabeled sentences with the trained model.</p>

<p>However, I believe that when the trained model classifies the remaining 29800 unlabeled sentences there must have been some unknown words that was not trained to the model I created. There might be sentence that does not include any words that have been trained and how can it be classified. The following code has classified the sentences using scikit.</p>

<pre><code>import numpy as np
import data
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline

data = data.Data()

allSent = data.getPosSent() + data.getNegSent()
stopWords = data.getStopwords()

Dataset_X = []
Dataset_Y = []

for meta, label in allSent:
   Dataset_X.append(meta)
   Dataset_Y.append(label)
X_train = np.array(Dataset_X)
Y_train = np.array(Dataset_Y)

classifier_SVC = Pipeline([
   ('vectorizer', CountVectorizer()),
   ('classifier', SVC(kernel='rbf', C=10000000, gamma=1e-08))
   ])
classifier_SVC.fit(X_train, Y_train)

predSent = data.getPredSent()
predSentData = []
for i in range(len(predSent)):
   predSentData.append(predSent[i])
pred = np.array(predSentData)

for i in range(len(predSent)):
  print classifier_SVC.predict(pred)[i]
</code></pre>

<p>In conclusion, my question is</p>

<ul>
<li>How can a classifier classify a totally new sentence which means that the trained model have never seen any words contained in the new sentence.</li>
<li>How many sentences do I need to train a model to say that it is enough? In other words, I have trained 200 but I do not know that it needs more data or not.</li>
</ul>
","python, machine-learning, scikit-learn, classification, text-classification","<p>1) Let us consider <code>y_i</code>, <code>x_i</code>, and <code>c_i</code> is <code>y</code>, <code>x</code> and weight of <code>i</code>-th support vector, respectively. For a given input z, we calculate <code>predict = sgn(sum(c_i*y_i*K(x_i,z))+b)</code> where b is a bias and <code>K</code> is a kernel( rbf kernel in your code). If <code>z</code> is a totally new sentence we obtain <code>predict = sgn(sum(c_i*y_i*exp(-gamma*x_i**2))*exp(-gamma*z**2)+b)</code> </p>

<p>2) It depend on your data. How about you check how many sentences cover how much % of words? Or if you have more than 200 labelled data, how about you evaluate a relations between the number of trained sentences and predicted scores?</p>
",0,-1,620,2016-03-16 12:58:10,https://stackoverflow.com/questions/36036414/how-to-classify-new-sentences-with-unknown-attributes
Multiclass classification with Naive Bayes and R,"<p>So I am trying to classify documents bases on its texts with Naive Bayes. Each document might belong to 1 to n categories (think of it as tags in a blog post).</p>

<p>My current approach is to provide R with a csv looking like this</p>

<pre><code>+-------------------------+---------+-------+-------+
|    TEXT TO CLASSIFY     | Tag 1   | Tag 2 | Tag 3 |
+-------------------------+---------+-------+-------+
| Some text goes here     | Yes     | No    | No    |
+-------------------------+---------+-------+-------+
| Some other text here    | No      | Yes   | Yes   |
+-------------------------+---------+-------+-------+
| More text goes here     | Yes     | No    | Yes   |
+-------------------------+---------+-------+-------+
</code></pre>

<p>Of course the desired behaviour is to have an input looking like</p>

<pre><code>Some new text to classify
</code></pre>

<p>And an output like</p>

<pre><code>+------+------+-------+
| Tag 1| Tag 2| Tag 3 |
+------+------+-------+
| 0.12 | 0.75 | 0.65  |
+------+------+-------+
</code></pre>

<p>And then based on a certain threshold, determine whether or not the given text belongs to tags 1, 2, 3. </p>

<p>Now the question is, in the tutorials I have found, it looks like the input should be more like</p>

<pre><code>+--------------------------+---------+
|    TEXT TO CLASSIFY      | Class   |
+--------------------------+---------+
| Some other text here     | No      |
+--------------------------+---------+
| Some other text here     | Yes     |
+--------------------------+---------+
| Some other text here     | Yes     |
+--------------------------+---------+
</code></pre>

<p>That is, a ROW per text per class... Then using that yes, i can train naive bayes and then use one-vs-all in order to determine which texts belongs to which tags. Question is, can I do this in a more elegant way (that is, with the training data looking like the first example I mentioned)?</p>

<p>One of the examples I found is <a href=""http://blog.thedigitalgroup.com/rajendras/2015/05/28/supervised-learning-for-text-classification/"" rel=""nofollow"">http://blog.thedigitalgroup.com/rajendras/2015/05/28/supervised-learning-for-text-classification/</a></p>
","r, machine-learning, text-classification, naivebayes","<p>There are conceptually two approaches.</p>

<ol>
<li>You combine the tag into a combined tag. Then you would get the joint probability. The main drawback is the combinatorial explosion, which implies that you also need much more training data</li>
<li>You build a individual NB model for each tag.</li>
</ol>

<p>As always in probabilistic modelling is the question whether you assume that your tags are independent or not. In the spirit of Naive Bayes the independence assumption would be very natural. In that case 2. would be the way to go. If the independence assumption is not justified and you are afraid of the combinatorial explosion, you can use a standard Bayesian Network. If you keep certain assumptions your performance will not be impacted.</p>

<p>However, you could also assume a mixed a approach.</p>

<ol>
<li>You could use a Hierarchical Naive Bayes Model. If there is some logical structure in the Tags you can introduce a parent variable for the classes. Bascially you have a value tag1/tag2 if both tags occur together.</li>
<li>The basic idea can be extended towards a latent variable you do not observer. This can be trained using a EM scheme. This will slightly impact your training performance, as you need to run the training, multiple iteration, however, will probably give you the best results.</li>
</ol>

<p><a href=""http://link.springer.com/article/10.1007%2Fs10994-006-6136-2#/page-1"" rel=""nofollow"">http://link.springer.com/article/10.1007%2Fs10994-006-6136-2#/page-1</a></p>
",1,2,3571,2016-03-31 03:46:05,https://stackoverflow.com/questions/36323759/multiclass-classification-with-naive-bayes-and-r
Machine learning text classification where a text belongs to 1 to N classes,"<p>So I am trying to (just for fun) classify movies based on their description, the idea is to ""tag"" movies, so a given movie might be ""action"" and ""humor"" at the same time for example. </p>

<p>Normally when using a text classifier, what you get is the class to where a given text belongs, but in my case I want to assign a text to 1 to N tags.</p>

<p>Currently my training set would look like this</p>

<pre><code>+--------------------------+---------+
|        TEXT              |  TAG    |
+--------------------------+---------+
| Some text from a movie   |  action |
+--------------------------+---------+
| Some text from a movie   |  humor  |
+--------------------------+---------+
| Another text here        | romance |
+--------------------------+---------+
| Another text here        | cartoons|
+--------------------------+---------+
| And some text more       | humor   |
+--------------------------+---------+
</code></pre>

<p>What I am doing next is to train classifiers to tell me whether or not each tag belongs to a single text, so for example, if I want to figure out whether or not a text is classified as ""humor"" I would end up with the following training set</p>

<pre><code>+--------------------------+---------+
|        TEXT              |  TAG    |
+--------------------------+---------+
| Some text from a movie   |  humor  |
+--------------------------+---------+
| Another text here        |not humor|
+--------------------------+---------+
| And some text more       | humor   |
+--------------------------+---------+
</code></pre>

<p>Then I train a classifier that would learn whether or not a text is humor or not (the same approach is done with the rest of the tags). After that I end with a total of 4 classifiers that are</p>

<ul>
<li>action / no action</li>
<li>humor / no humor</li>
<li>romance / no romance</li>
<li>cartoons / no cartoons</li>
</ul>

<p>Finally when I get a new text, I apply it to each of the 4 classifiers, for each classifier that gives me a positive classification (that is, gives me X instead of no-X) if such classification is over a certain threshold (say 0.9), then I assume that the new text belongs to tag X, and then I repeat the same with each of the classifiers.</p>

<p>In particular I am using Naive Bayes as algorithm, but the same could be applied with any algorithm that outputs a probability. </p>

<p>Now the question is, is this approach correct? Am I doing something terribly wrong here? From the results I get things seems to make sense, but I would like a second opinion.</p>
","machine-learning, statistics, text-classification, naivebayes","<p>Yes, this makes sense. it is a well known, basic technique for multilabel/multiclass classification known as ""one vs all"" (or ""one vs all"") classifier. This is very old and widely used. On the other hand - it is also very naive as you do not consider any relations between your classes/tags. You might be interested in reading about structure learning, which covers topics where there is some structure over labels space that can be exploited (and usually there is).</p>
",0,0,452,2016-04-13 22:15:10,https://stackoverflow.com/questions/36610526/machine-learning-text-classification-where-a-text-belongs-to-1-to-n-classes
invalid &#39;type&#39; (character) of argument,"<p>It is the error message on the topic. I am getting this error when I try to run naive.bayes classifier. Here is the summary of my train data:</p>

<pre><code>'data.frame':   7269 obs. of  193 variables:
 $ pid       : int  2 4 5 7 10 11 14 18 25 31 ...
 $ acquir    : int  0 0 0 0 1 1 0 0 0 0 ...
 $ addit     : int  0 0 0 0 2 2 0 0 0 0 ...
 $ agre      : int  0 0 0 0 0 0 0 0 0 0 ...
 $ agreement : int  0 0 0 0 0 0 0 0 0 0 ...
 $ also      : int  1 0 0 0 2 2 0 0 0 0 ...
 $ american  : int  0 0 0 0 0 0 0 0 0 0 ...
 $ announc   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ annual    : int  0 0 0 0 0 0 0 0 2 0 ...
 $ approv    : int  0 3 0 0 0 0 0 0 0 0 ...
 $ april     : int  0 0 0 0 0 0 0 0 1 0 ...
 $ bank      : int  0 7 0 0 0 0 0 0 0 0 ...
 $ base      : int  0 0 0 0 0 0 0 0 0 0 ...
 .
 .
 $... all of them are integer, except the class column
 .
 .
 $ class     : Factor w/ 10 levels ""acq"",""corn"",""crude"",..: 1 1 4 4 9 1 4 3 1 4 ...
</code></pre>

<p>And this is the <code>naive.bayes()</code> line:</p>

<pre><code>model &lt;- naiveBayes(as.factor(class) ~ ., data = as.matrix(train), laplace = 3)
</code></pre>

<p>Can anyone tell me why it is happening?:</p>

<pre><code>Error in sum(x) : invalid 'type' (character) of argument
</code></pre>
","r, classification, text-classification, naivebayes, document-classification","<p>Eventually your data is converted to character because of <code>as.matrix(train)</code>. Try </p>

<pre><code>model &lt;- naiveBayes(class ~ ., data=train, laplace = 3)
</code></pre>

<p>or eventually </p>

<pre><code>model &lt;- naiveBayes(train$class ~ ., data=train[, -c(""class"")], laplace = 3)
</code></pre>

<p>The second variant is more or less the same as the first variant. The <code>.</code> in the RHS of the formula is expanded to 'all other variables'; so it excludes the column <code>class</code> mentioned on the LHS. (More information is in the documentation of <code>formula</code>)</p>
",1,1,8058,2016-04-15 07:12:59,https://stackoverflow.com/questions/36640529/invalid-type-character-of-argument
similarity measure scikit-learn document classification,"<p>I am doing some work in document classification with scikit-learn. For this purpose, I represent my documents in a tf-idf matrix and feed a Random Forest classifier with this information, works perfectly well. I was just wondering which similarity measure is used by the classifier (cosine, euclidean, etc.) and how I can change it. Haven't found any parameters or informatin in the documentation.</p>

<p>Thanks in advance!</p>
","python-2.7, scikit-learn, text-classification","<p>As with most supervised learning algorithms, Random Forest Classifiers do not use a similarity measure, they work directly on the feature supplied to them. So decision trees are built based on the terms in your tf-idf vectors.</p>

<p>If you want to use similarity then you will have to compute a similarity matrix for your documents and use this as your features. </p>
",0,1,440,2016-04-18 07:35:32,https://stackoverflow.com/questions/36687929/similarity-measure-scikit-learn-document-classification
SVM feature vector representation by using pre-made dictionary for text classification,"<p>I want to classify a collection of text into two class, let's say I would like to do a sentiment classification. I have two pre-made sentiment dictionaries, one contain only positive words and another contain only negative words. I would like to incorporate these dictionaries into feature vector for SVM classifier. My question is, is it possible to separate between positive and negative words dictionary to be represented as SVM feature vector, especially when I generate feature vector for the test set?</p>

<p>If my explanation is not clear enough, let me give the example. Let's say I have these two sentences as training data:</p>

<pre><code>Pos: The book is good
Neg: The book is bad
</code></pre>

<p>Word 'good' exists in positive dictionary and 'bad' exists in negative dictionary, while other words do not exist in neither dictionary. I want the words that exist in matching dictionary with the sentence's class have a big weight value, while other words have small value. So, the feature vectors will be like these:</p>

<pre><code>+1 1:0.1 2:0.1 3:0.1 4:0.9
-1 1:0.1 2:0.1 3:0.1 5:0.9
</code></pre>

<p>If I want to classify a test sentence ""<em>The food is bad</em>"", how should I generate a feature vector for the test set with weight that depend on existing dictionary when I cannot match test sentence's class with each of the dictionary? What I can think is, for test set, as long as the word exist in both dictionary, I will give the word a high weight value.</p>

<pre><code>0 1:0.1 3:0.1 5:0.9
</code></pre>

<p>I wonder if this is the right way for creating vector representation for both training set and test set.</p>

<p>--Edit--
I forgot to mention that these pre-made dictionaries was extracted using some kind of topic model. For example, the top 100 words from topic 1 are kinda represent positive class and words in topic 2 represent negative class. I want to use this kind of information to improve the classifier more than using only bag-of-words feature.</p>
","machine-learning, svm, sentiment-analysis, text-classification","<p>In short - <strong>this is not the way it works</strong>.</p>

<p>The whole point of <strong>learning</strong> is to give classifier ability to assign these weights on their own. You cannot ""force it"" to have a high value per class for a particular feature (I mean, you could on the optimization level, but this would require changing the whole svm structure).</p>

<p>So the right way is to simply create a ""normal"" representation. Without any additional specification. Let the model decide, they are better at statistical analysis than human intuition, really.</p>
",1,1,546,2016-04-24 10:14:31,https://stackoverflow.com/questions/36821818/svm-feature-vector-representation-by-using-pre-made-dictionary-for-text-classifi
Problems with Naive Bayes,"<p>I'm trying to run Naive Bayes in R for making predictions from textual data (by building a Document Term Matrix).</p>

<p>I read several posts warning about terms that could be missing in both the training and the testing set, so I decided to work with only one data frame and split it afterwards. The code I'm using is this:</p>

<pre><code>data &lt;- read.csv(file=""path"",header=TRUE)

########## NAIVE BAYES
library(e1071)
library(SparseM)
library(tm)

# CREATE DATA FRAME AND TRAINING AND
# TEST INCLUDING 'Text' AND 'InfoType' (columns 8 and 27)
traindata &lt;- as.data.frame(data[13000:13999,c(8,27)])
testdata &lt;- as.data.frame(data[14000:14999,c(8,27)])
complete &lt;- as.data.frame(data[13000:14999,c(8,27)])

# SEPARATE TEXT VECTOR TO CREATE Source(),
# Corpus() CONSTRUCTOR FOR DOCUMENT TERM
# MATRIX TAKES Source()
completevector &lt;- as.vector(complete$Text)

# CREATE SOURCE FOR VECTORS
completesource &lt;- VectorSource(completevector)

# CREATE CORPUS FOR DATA
completecorpus &lt;- Corpus(completesource)

# STEM WORDS, REMOVE STOPWORDS, TRIM WHITESPACE
completecorpus &lt;- tm_map(completecorpus,tolower)
        completecorpus &lt;- tm_map(completecorpus,PlainTextDocument)
        completecorpus &lt;- tm_map(completecorpus, stemDocument)
completecorpus &lt;- tm_map(completecorpus, removeWords,stopwords(""english""))
        completecorpus &lt;- tm_map(completecorpus,removePunctuation)
        completecorpus &lt;- tm_map(completecorpus,removeNumbers)
        completecorpus &lt;- tm_map(completecorpus,stripWhitespace)

# CREATE DOCUMENT TERM MATRIX
completematrix&lt;-DocumentTermMatrix(completecorpus)
trainmatrix &lt;- completematrix[1:1000,]
testmatrix &lt;- completematrix[1001:2000,]

# TRAIN NAIVE BAYES MODEL USING trainmatrix DATA AND traindata$InfoType CLASS VECTOR
model &lt;- naiveBayes(as.matrix(trainmatrix),as.factor(traindata$InfoType),laplace=1)

# PREDICTION
results &lt;- predict(model,as.matrix(testmatrix))
conf.matrix&lt;-table(results, testdata$InfoType,dnn=list('predicted','actual'))

conf.matrix
</code></pre>

<p>The problem is that I'm getting weird results like this:</p>

<pre><code>               actual
predicted    1   2   3
         1  60 833 107
         2   0   0   0
         3   0   0   0
</code></pre>

<p>Any idea of why is this happening?</p>

<p>The raw data looks like this:</p>

<pre><code>head(complete)

      Text
13000 Milkshakes, milkshakes, whats not to love? Really like the durability and weight of the cup. Something about it sure makes good milkshakes.Works beautifully with the Cuisinart smart stick.
13001 excellent. shipped on time, is excellent for protein shakes with a cuisine art mixer.  easy to clean and the mixer fits in perfectly
13002 Great cup. Simple and stainless steel great size cup for use with my cuisinart mixer.  I can do milkshakes really easy and fast. Recommended. No problems with the shipping.
13003 Wife Loves This. Stainless steel....attractive and the best part is---it won't break. We are considering purchasing another one because they are really nice.
13004 Great! Stainless steel cup is great for smoothies, milkshakes and even chopping small amounts of vegetables for salads!Wish it had a top but still love it!
13005 Great with my. Stick mixer...the plastic mixing container cracked and became unusable as a result....the only downside is you can't see if the stuff you are mixing is mixed well 

      InfoType
13000        2
13001        2
13002        2
13003        3
13004        2
13005        2
</code></pre>
","r, classification, text-classification, naivebayes","<p>Seemingly the problem is that the TDM needs to get rid of so much sparsity. So I added:</p>

<pre><code>completematrix&lt;-removeSparseTerms(completematrix, 0.95)
</code></pre>

<p>And it started working!!</p>

<pre><code>             actual
predicted   1   2   3
        1  60 511   6
        2   0  86   2
        3   0 236  99
</code></pre>

<p>Thank you all for your ideas (thank you Chelsey Hill!!)</p>
",0,0,391,2016-04-30 19:39:16,https://stackoverflow.com/questions/36959387/problems-with-naive-bayes
Text classification with R and SVM. Matrix features,"<p>I am playing a bit with text classification and SVM.</p>
<p>My understanding is that typically the way to pick up the features for the training matrix is essentially to use a &quot;bag of words&quot; where we essentially end up with a matrix with as many columns as different words are in our document and the values of such columns is the number of occurrences per word per document (of course each document is represented by a single row).</p>
<p>So that all works fine, I can train my algorithm and so on, but sometimes i get an error like</p>
<blockquote>
<p>Error during wrapup: test data does not match model !</p>
</blockquote>
<p>By digging it a bit, I found the answer in this question <a href=""https://stackoverflow.com/questions/26265305/error-in-predict-svm-test-data-does-not-match-model"">Error in predict.svm: test data does not match model</a> which essentially says that <strong>if your model has features A, B and C, then your new data to be classified should contain columns A, B and C</strong>. Of course <strong>with text this is a bit tricky, my new documents to classify might contain words that have never been seen by the classifier with the training set.</strong></p>
<p>More specifically I am using the RTextTools library whith uses SparseM and tm libraries internally, the object used to train the svm is of type &quot;matrix.csr&quot;.</p>
<p>Regardless of the specifics of the library my question is, is there any technique in document classification to ensure that the fact that training documents and new documents have different words will not prevent new data from being classified?</p>
<p><strong>UPDATE</strong> The solution suggested by @lejlot is very simple to achieve in RTextTools by simply making use of the <strong>originalMatrix</strong> optional parameter when using the <strong>create_matrix</strong> function. Essentially, originalMatrix should be the SAME matrix that one creates when one uses the create_matrix function for TRAINING the data. So after you have trained your data and have your models, keep also the original document matrix, when using new examples, make sure of using such object when creating the new matrix for your prediction set.</p>
","r, machine-learning, svm, text-classification","<blockquote>
  <p>Regardless of the specifics of the library my question is, is there any technique in document classification to ensure that the fact that training documents and new documents have different words will not prevent new data from being classified?</p>
</blockquote>

<p>Yes, and it is very trivial one. Before applying any training or classification you create a preprocessing object, which is supposed to map text to your vector representation. In particular - it <strong>stores whole vocabulary used for training</strong>. Later on you reuse the same preprocessing object on test documents, and you simply ignore words from outside of vocabulary stored before (OOV words, as they are often refered in the literature).</p>

<p>Obviously there are plenty other more ""heuristic"" approaches, where instead of discarding you try to map them to existing words (although it is less theoreticalyy justified). Rather - you should create intermediate representation, which will be your new ""preprocessing"" object which can handle OOV words (through some levenstein distance mapping etc.).</p>
",2,1,976,2016-05-07 08:42:47,https://stackoverflow.com/questions/37086414/text-classification-with-r-and-svm-matrix-features
Naive Bayes unseen features handling scikit learn,"<p>I am classifying small texts (tweets) using Naive Bayes (MultinominalNB) in scikit-learn.
My train data has 1000 features, and my test data has 1200 features.
Let's say 500 features are common for both train and test data.</p>

<p>I wonder why MultinominalNB in scikit learn does not handle unseen features, and gives me an error:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/osopova/Documents/00_KSU_Masters/01_2016_Spring/Twitter_project/mda_project_1/step_4.py"", line 60, in &lt;module&gt;
    predict_Y = classifiers[i].predict(test_X)
  File ""/Library/Python/2.7/site-packages/sklearn/naive_bayes.py"", line 65, in predict
    jll = self._joint_log_likelihood(X)
  File ""/Library/Python/2.7/site-packages/sklearn/naive_bayes.py"", line 672, in _joint_log_likelihood
    return (safe_sparse_dot(X, self.feature_log_prob_.T)
  File ""/Library/Python/2.7/site-packages/sklearn/utils/extmath.py"", line 184, in safe_sparse_dot
    return fast_dot(a, b)
ValueError: matrices are not aligned
</code></pre>
","machine-learning, scikit-learn, text-classification, naivebayes","<p>It does not handle unseen features because you do not pass any reference <strong>naming</strong> features. Why do you have 1200 features in one case and 1000 in another? Probably because there were objects in the test setting not present in the training - but how Naive Bayes is supposed to figure out which ones of these 1200 are missing in 1000? In this implementation (which is the only possible when you assume arrays as input) it is your duty to remove all columns, which do not correspond to the ones in the training set, add columns of zeros (in valid spots) if it is the other way around, and most importantly - make sure that ""ith"" column in one set is the same (captures occurence of the same word/object) as ""ith"" column in the second one. Consequently in your case there are just 500 columns which can actually be used, and Naive Bayes has no information how to find these. You have to provide, in test scenario, the same 1000 features which were used in train, thus in your case it means removing 700 columns not seen during train, and adding (in valid spots!) 500 columns of zeros.</p>

<p>In particular, scikit-learn gives you plenty of data preprocessing utilities, which do this for you (like CountVectorizer etc.).</p>
",2,0,543,2016-05-08 21:41:31,https://stackoverflow.com/questions/37105126/naive-bayes-unseen-features-handling-scikit-learn
scikit classifier with unknown prediction,"<p>I'm planning to use Python Scikit to do some text classification, and was planning to use using TfidfVectorizer and MultinomialNB.</p>

<p>but I realized that MultinomialNB will always predict my sample into an existing (known) category. </p>

<p>for example, if I have:</p>

<pre><code>category A: trained with sample ""this is green""
category B: trained with sample ""this is blue""
category C: trained with sample ""this is red""
</code></pre>

<p>and I try to predict: <code>""this is yellow""</code></p>

<p>it will give me <code>category A</code> (or any other, because the probablity is the same for all categories in this case).</p>

<p>my question is: is there a classifier that would give me ""unknown"" (or none, or false, or error) for the test case above?</p>

<p>I would like to know when my test case could not be predicted with the given training set.</p>

<p>I think I could check if <code>my_classifier.predict_proba(X_test))</code> returns an array with all equal or close values (in this example case: <code>[[ 0.33333333  0.33333333  0.33333333]]</code>). </p>

<p>actually, I would have to check if the values are close to their defaults, because the probabilities might not be the same for each category :)</p>

<p>so... any better approach or... is there a classifier with some confidence threshold I could use?</p>
","scikit-learn, text-classification, naivebayes","<p>You can look into doing <a href=""http://scikit-learn.org/stable/modules/outlier_detection.html"" rel=""nofollow"">novelty detection</a>. I would check out that link and the associated <a href=""http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#example-svm-plot-oneclass-py"" rel=""nofollow"">example</a>. The idea, in that example, is to use a:</p>

<blockquote>
  <p>One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: <strong>classifying new data as similar or different to the training set.</strong></p>
</blockquote>

<p>(Emphasis is mine.) I don't know how it would perform with the small amount of data in your example, I'd guess ""poorly"", but I believe that <a href=""https://en.wikipedia.org/wiki/Novelty_detection"" rel=""nofollow"">novelty detection</a> is the sort of thing you are looking for here.</p>
",0,2,745,2016-05-10 01:37:05,https://stackoverflow.com/questions/37127941/scikit-classifier-with-unknown-prediction
Scikit-learn: How to extract features from the text?,"<p>Assume I have an array of Strings:</p>

<pre><code>['Laptop Apple Macbook Air A1465, Core i7, 8Gb, 256Gb SSD, 15""Retina, MacOS' ... 'another device description']
</code></pre>

<p>I'd like to extract from this description features like:</p>

<pre><code>item=Laptop
brand=Apple
model=Macbook Air A1465
cpu=Core i7
...
</code></pre>

<p>Should I prepare the pre-defined known features first? Like</p>

<pre><code>brands = ['apple', 'dell', 'hp', 'asus', 'acer', 'lenovo']
cpu = ['core i3', 'core i5', 'core i7', 'intel pdc', 'core m', 'intel pentium', 'intel core duo']
</code></pre>

<p>I am not sure that I need to use <code>CountVectorizer</code> and <code>TfidfVectorizer</code> here, it's more appropriate to have <code>DictVictorizer</code>, but how can I make dicts with keys extracting values from the entire string?</p>

<p>is it possible with scikit-learn's Feature Extraction? Or should I make my own <code>.fit()</code>, and <code>.transform()</code> methods?</p>

<p>UPDATE:
@sergzach, please review if I understood you right:</p>

<pre><code>data = ['Laptop Apple Macbook..', 'Laptop Dell Latitude...'...]

for d in data:
    for brand in brands:
       if brand in d:
          # ok brand is found
for model in models:
       if model in d:
          # ok model is found
</code></pre>

<p>So creating N-loops per each feature? This might be working, but not sure if it is right and flexible.</p>
","machine-learning, scikit-learn, text-classification","<p>Yes, something like the next.</p>

<p>Excuse me, probably you should correct the code below.</p>

<pre><code>import re

data = ['Laptop Apple Macbook..', 'Laptop Dell Latitude...'...]

features = {
    'brand': [r'apple', r'dell', r'hp', r'asus', r'acer', r'lenovo'],
    'cpu': [r'core\s+i3', r'core\s+i5', r'core\s+i7', r'intel\s+pdc', r'core\s+m', r'intel\s+pentium', r'intel\s+core\s+duo']
    # and other features
}

cat_data = [] # your categories which you should convert into numbers

not_found_columns = []

for line in data:
    line_cats = {}

    for col, features in features.iteritems():
        for i, feature in enumerate(features):
            found = False

            if re.findall(feature, line.lower(), flags=re.UNICODE) != []:
                line_cats[col] = i + 1 # found numeric category in column. For ex., for dell it's 2, for acer it's 5.               
                found = True
                break # current category is determined by a first occurence

        # cycle has been end but feature had not been found. Make column value as default not existing feature
        if not found:       
            line_cats[col] = 0
            not_found_columns.append((col, line))

        cat_data.append(line_cats)

# now we have cat_data where each column is corresponding to a categorial (index+1) if a feature had been determined otherwise 0.
</code></pre>

<p>Now you have column names with lines (<code>not_found_columns</code>) which was not found. View them, probably you forgot some features.</p>

<p>We can also write strings (instead of numbers) as categories and then use <code>DV</code>. In result the approaches are equivalent.</p>
",0,0,541,2016-05-15 10:14:32,https://stackoverflow.com/questions/37237019/scikit-learn-how-to-extract-features-from-the-text
SVM results in Rapidminer much worse than in knime,"<p>I'm comparing various classification algorithms for a project using knime. I was very happy with the results I got for Support Vector Machines (LibSVM).
I then wanted to try hierarchical classification and installed the Rapidminer plugin for knime. To get things to work I first tested the SVM implementation without hierarchies.</p>

<p>Comparing the results of the knime LibSVM implementation and the rapidminer LibSVM implementation I noticed that the rapidminer implementation yielded worse results.
The knime implementation in fact produced an error rate of approximately 2.4% while the rapidminer one produced an error rate of approx. 61%.
Why is that? Am I doing something wrong?</p>

<p><a href=""https://i.sstatic.net/YBXZx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YBXZx.jpg"" alt=""Confusion Matrix Comparison""></a></p>

<p>I use C-SVC SVMs with linear kernel, 1.0 Cost, 0.001 epsilon and 80mb cache for both implementations. </p>

<p><a href=""https://i.sstatic.net/8Zxwp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8Zxwp.jpg"" alt=""Rapidminer Workflow and Options""></a> <a href=""https://i.sstatic.net/rn8rU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rn8rU.jpg"" alt=""Knime Options""></a></p>

<p>The documents are wikipedia article texts, preprocessed, transformed to a binary document vector and labeled with some kind of type.</p>

<p>I hope you can help me.</p>
","svm, libsvm, rapidminer, text-classification, knime","<p>You do not need to include the <code>Row ID</code>s in this case (<code>Row ID</code> tab, make to button show <code>Do not use</code> by clicking on it in case it is <code>Use</code> and the text field is not disabled), and you should not perform <code>Nominal to...</code> transformations on them. After that, you should get similar results in both cases.</p>
",2,1,533,2016-05-18 10:38:48,https://stackoverflow.com/questions/37297124/svm-results-in-rapidminer-much-worse-than-in-knime
Should I remove stopwords when feed sentence to RNN,"<p>In bag-of-words model, I know we should remove stopwords and punctuation before training. But in RNN model, if I want to do text classification, should I remove stopwords too ? </p>
","machine-learning, nlp, deep-learning, text-classification, recurrent-neural-network","<p>This depends on what your model classifies.  If you're doing something in which the classification is aided by stop words -- some level of syntax understanding, for instance -- then you need to either leave in the stop words or alter your stop list, such that you don't lose that information.  For instance, cutting out all verbs of being (is, are, should be, ...) can mess up a NN that depends somewhat on sentence structure.</p>

<p>However, if your classification is topic-based (as suggested by your bag-of-words reference), then treat the input the same way: remove those pesky stop words before they burn valuable training time.</p>
",2,0,4110,2016-05-19 14:13:28,https://stackoverflow.com/questions/37325914/should-i-remove-stopwords-when-feed-sentence-to-rnn
Complication using log-probabilities - Naive Bayes text classifier,"<p>I'm constructing a Naive Bayes text classifier from scratch in Python and I am aware that, upon encountering a product of very small probabilities, using a logarithm over the probabilities is a good choice.</p>

<p>The issue now, is that the mathematical function that I'm using has a summation OVER a product of these extremely small probabilities.</p>

<p>To be specific, I'm trying to calculate the total word probabilities given a mixture component (class) over all classes.</p>

<p>Just plainly adding up the logs of these total probabilities is incorrect, since the log of a sum is not equal to the sum of logs.</p>

<p>To give an example, lets say that I have 3 classes, 2000 words and 50 documents.
Then I have a word probability matrix called wordprob with 2000 rows and 3 columns.</p>

<p>The algorithm for the total word probability in this example would look like this:</p>

<pre><code>sum = 0
for j in range(0,3):
    prob_product = 1
    for i in words:  #just the index of words from my vocabulary in this document
        prob_product = prob_product*wordprob[i,j]
    sum = sum + prob_product
</code></pre>

<p>What ends up happening is that prob_product becomes 0 on many iterations due to many small probabilities multiplying with each other.</p>

<p>Since I can't easily solve this with logs (because of the summation in front) I'm totally clueless.</p>

<p>Any help will be much appreciated.</p>
","python, math, statistics, text-classification, naivebayes","<p>I think you may be best to keep everything in logs. The first part of this, to compute the log of the product is just adding up the log of the terms. The second bit, computing the log of the sum of the exponentials of the logs is a bit trickier. </p>

<p>One way would be to store each of the logs of the products in an array, and then you need a function that, given an array L with n elements, will compute </p>

<pre><code>S = log( sum { i=1..n | exp( L[i])})
</code></pre>

<p>One way to do this is to find the maximum, M say, of the L's; a little algebra shows </p>

<pre><code>S = M + log( sum { i=1..n | exp( L[i]-M)})
</code></pre>

<p>Each of the terms L[i]-M is non-positive so overflow can't occur. Underflow is not a problem as for them exp will return 0. At least one of them (the one where L[i] is M) will be zero so it's exp will be one and we'll end up with something we can pass to log. In other words the evaluation of the formula will be trouble free.</p>

<p>If you have the function log1p (log1p(x) = log(1+x)) then you could gain some accuracy by omitting the (just one!) i where L[i] == M from the sum, and passing the sum to log1p instead of log.</p>
",3,2,4048,2016-05-20 11:17:31,https://stackoverflow.com/questions/37345314/complication-using-log-probabilities-naive-bayes-text-classifier
Should I use word2vec to do word embedding including testing data?,"<p>I am a new people in NLP and I am try do the text classification job. Before doing the job, I know that we should do word embedding.
My question is should I do word embedding job only on training data <strong>(so that testing data get vector just from pre-trained vec-model of training data)</strong>, or both on training data &amp; testing data?</p>
","machine-learning, nlp, text-classification, word2vec, word-embedding","<p>This is a very important question. In NN community what typically people do is to use a threshold (i.e. frequency &lt; = 2) in the training set and replace all words which occur less than that threshold by UNK token. Then in the test time, if there is a word that doesn't match an actual training set word, UNK's representation will replace it.</p>
",-1,0,810,2016-05-22 03:43:42,https://stackoverflow.com/questions/37370299/should-i-use-word2vec-to-do-word-embedding-including-testing-data
Classification of sparse data,"<p>I am struggling with the best choice for a classification/prediction problem. Let me explain the task - I have a database of keywords from abstracts for different research papers, also I have a list of journals with specified impact factors. I want to build a model for article classification based on their keywords, the result is the possible impact factor (taken just as a number without any further journal description) with a given keywords. I removed the unique keyword tags as they do not have much statistical significance so I have only keywords that are repeated 2 and more times in my abstract list (6000 keyword total). I think about dummy coding - for each article I will create a binary feature vector 6000 attributes in length - each attribute refers to presence of the keyword in the abstract and classify the whole set by SVM. I am pretty sure that this solution is not very elegant and probably also not correct, do you have any suggestions for a better deal?</p>
","python, r, classification, data-mining, text-classification","<p>There is nothing wrong with using this coding strategy for text and support vector machines.</p>

<p>For your actual objective:</p>

<ul>
<li>support vector <em>regression</em> (SVR) may be more appropriate</li>
<li>beware of the journal impact factor. It is very crude. You need to take temporal aspects into account; and many very good work is not published in journals at all</li>
</ul>
",0,0,564,2016-05-28 10:19:04,https://stackoverflow.com/questions/37497795/classification-of-sparse-data
Classifying words inside a document,"<p>The problem that I'm facing is:
I want to read a document, get the raw string of this document, and classify the information.
For example, I want to identify when the string is a ""Name"", or a ""date"" ou some other useful information.</p>

<p>Is it possible to use machine learning to do that?
How may I approach the problem?</p>

<p>The most hard problem here is that I'm not trying to classify the document itself, but the String information inside the document.</p>
","machine-learning, text-classification","<p>So it's all about how you think about your problem. I think your problem can be formulated as an entity extraction/recognition problem, where you have a document and want to identify specific entities <em>within</em> the text (where an entity might be a person, date, etc). Take a look at Conditional Random Fields and their applications to named entity recognition (NER for short), as there are some libraries &amp; tools already implemented. </p>

<p>For example, check out <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">StanfordNER</a>.</p>
",2,0,88,2016-06-02 13:23:15,https://stackoverflow.com/questions/37593164/classifying-words-inside-a-document
Text Categorization Python with pre-trained data,"<p>how can i associate my tfidf matrix with a category ? for example i have the below data set</p>

<pre><code>**ID**        **Text**                                     **Category**
   1     jake loves me more than john loves me               Romance
   2     july likes me more than robert loves me             Friendship
   3     He likes videogames more than baseball              Interest 
</code></pre>

<p>once i calculate tfidf for each and every sentence by taking '<strong>Text</strong>' column as my input, how would i be able to train the system to categorize that row of the matrix to be associated with my category above so that i would be able to reuse for my test data ?</p>

<p>using the above train dataset , when i pass a new sentence 'julie is a lovely person', i would like that sentence to be categorized into single or multiple pre-defined categories as above.</p>

<p>I have used this link <a href=""https://stackoverflow.com/questions/29788047/keep-tfidf-result-for-predicting-new-content-using-scikit-for-python"">Keep TFIDF result for predicting new content using Scikit for Python</a> as my starting point to solve this issue but i was not able to understand on how to map tfidf matrix for a sentence to a category </p>
","python-3.x, scikit-learn, tf-idf, text-classification","<p>It looks like you already vectorised the text, i.e. already converted the text to numbers so that you can use scinkit-learns classifiers. Now the next step is to train a classifier. You can follow <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#training-a-classifier"" rel=""nofollow"">this link</a>. It looks like this:</p>

<p><strong>Vectorization</strong></p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
X_train = count_vect.fit_transform(your_text)
</code></pre>

<p><strong>Train classifier</strong></p>

<pre><code>from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB().fit(X_train, y_train)
</code></pre>

<p><strong>Predict on new docs</strong>:</p>

<pre><code>docs_new = ['God is love', 'OpenGL on the GPU is fast']
X_new = count_vect.transform(docs_new)
predicted = clf.predict(X_new)
</code></pre>
",1,0,808,2016-06-06 14:54:11,https://stackoverflow.com/questions/37660555/text-categorization-python-with-pre-trained-data
Show accuracy for each class in every given test data using sklearn,"<p>I have something to ask.</p>

<p>I've trained my sklearn Logistic Regression classifier with 10 thousand training data in Python. 
I have 2 thousand test data and I use accuracy score to show the accuracy and confusion matrix.. but both only show overall accuracy of all test data.</p>

<p>what I want is for example:</p>

<p>Test data 1: ""abc""</p>

<p>Accuracy of class A given test data: 80%</p>

<p>Accuracy of class B given test data: 10%</p>

<p>Accuracy of class C given test data: 10%</p>

<p>Test data 2: ""def""</p>

<p>Accuracy of class A given test data: 50%</p>

<p>Accuracy of class B given test data: 30%</p>

<p>Accuracy of class C given test data: 20%</p>

<p>and so on for the rest of all the test data.
and I want to show it in table like this.
<a href=""https://i.sstatic.net/jTVpP.png"" rel=""nofollow"">example</a></p>

<p>is it possible to that using sklearn?</p>
","python, machine-learning, scikit-learn, classification, text-classification","<p>Based on the example provided by you, I think what you are asking is probabilistic prediction for each of your test data points. You can do it easily by using the <code>predict_proba</code>  method of the LogisticRegression class (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba</a>). This will give you the probabilities of each of the classes. The returned matrix should have a size of <code>2000 x 3</code> in your case. You can multiply them by 100 to get the necessary percentages for each of your class.</p>

<p>Hope that helps.</p>
",0,1,1994,2016-06-16 01:17:44,https://stackoverflow.com/questions/37848349/show-accuracy-for-each-class-in-every-given-test-data-using-sklearn
Countif or counta google Spreadsheets to count word appearances,"<p>I want to count how many times specific words show up (per line) in my google spreadsheet.  <br/>
My spreadsheet looks like this (while ? is the area where i want to have my counted results): <br/>
table 1 <br/>
 <strong>|comments|mentions delivery|mentions price|mentions service|</strong> <br/>
|fast delivery, very good support|?|?|?| <br/>
|price for quality was too high. package was damaged|?|?|?| <br/>
|E-mail support was fast and helpful|0?|?|?| <br/></p>

<p>table 2 (all words that table 1 comments should be compared to): <br/>
<strong>|mentions delivery|mentions price|mentions service|</strong> <br/>
|delivery|price|support| <br/>
|shipping|pricey|service| <br/>
|box|-|call|</p>

<p>after counting how many of my words from table 2 show up in the comments of table 1 table 1 should look like this: <br/>
<strong>|comments|mentions delivery|mentions price|mentions service|</strong> <br/>
|fast delivery, very good support|1|0|1| <br/>
|price for quality was too high. package was damaged|1|1|0| <br/>
|E-mail support was fast and helpful|0|0|1| <br/>
 <br/>
I tried using the countif function which i could not get to work and are now using counta, which works like expected (without using table 2), but I want to have the formular adjust automatically when I add more words in table 2: <br/></p>

<p><code>=COUNTA(Filter(Split(D2,"" ""),""delivery"")) + COUNTA(Filter(Split(D2,"" ""),""shipping""))</code>
 <br/>
Any ideas for a solution?</p>
","google-sheets, countif, text-classification","<p>Let's say that F2 contains a comment while column A lists the words related to delivery. The following formula computes the number of words in column A that appear in the comment. It requires word boundaries, so that ""caprice"" is not mistaken for ""price"", but allows some plural forms, so that ""price"" is recognized in ""prices"" and ""box"" in ""boxes"".</p>

<pre><code>=sum(arrayformula(n(regexmatch(F2, ""\b"" &amp; filter(A2:A, len(A2:A)) &amp; ""e?s?\b""))))
</code></pre>

<h3>Explanation</h3>

<ol>
<li><code>filter(A2:A, len(A2:A))</code> prepares the list of words, omitting blanks. </li>
<li>""\b"" enforce word boundaries, while ""e?s?"" allows plural forms.</li>
<li><code>regexmatch</code> returns True or False, depending on whether the comment in F2 matches.</li>
<li><code>n</code> converts True to 1 and False to 0.</li>
<li><code>sum(arrayformula(...))</code> says: do the above for each word, and add the results.</li>
</ol>
",0,1,124,2016-06-16 12:25:59,https://stackoverflow.com/questions/37859198/countif-or-counta-google-spreadsheets-to-count-word-appearances
Tensorflow error using my own data for text classification,"<p>I've been playing with the Tensorflow library doing the tutorials.</p>

<p>I'm using this <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/text_classification_save_restore.py#L1"" rel=""nofollow noreferrer"">example</a>. And I changed the parameters in the example from this: <code>n_classes = 15</code> 
to this: <code>n_classes = 2</code> as I have only two classes to classify.</p>

<p>I read data like:</p>

<pre><code>train = pandas.read_csv('tensorflow_feed/test/train_with_abs.csv', header=None)
X_train, y_train = train[1], train[0]
test = pandas.read_csv('tensorflow_feed/test/test_with_abs.csv', header=None)
X_test, y_test = test[1], test[0]
</code></pre>

<p>But it gives following error:</p>

<pre><code>Total words: 35
Traceback (most recent call last):
  File ""/home/sumit/PycharmProjects/experiments/text_classification_save_restore.py"", line 94, in &lt;module&gt;
    classifier.fit(X_train, y_train)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py"", line 160, in fit
    monitors=monitors)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 449, in _train_model
    train_op, loss_op = self._get_train_ops(features, targets)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 673, in _get_train_ops
    _, loss, train_op = self._call_model_fn(features, targets, ModeKeys.TRAIN)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 656, in _call_model_fn
    features, targets, mode=mode)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py"", line 369, in _model_fn
    predictions, loss = model_fn(features, targets)
  File ""/home/sumit/PycharmProjects/experiments/text_classification_save_restore.py"", line 73, in rnn_model
    word_list = tf.unpack(word_vectors, axis=1)
TypeError: unpack() got an unexpected keyword argument 'axis'

Process finished with exit code 1
</code></pre>
","python, machine-learning, tensorflow, text-classification","<p>The ""axis"" parameter was just added to tf.unpack on June 23, and the example you're looking at was changed to use it:</p>

<p><a href=""https://github.com/tensorflow/tensorflow/commit/eff93149a6dc8e6826898fd9f9c28c81e21c9836"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/commit/eff93149a6dc8e6826898fd9f9c28c81e21c9836</a></p>

<p>So I suggest either:</p>

<ul>
<li><p>use an older version of the example from before that commit, e.g.:
<a href=""https://github.com/tensorflow/tensorflow/blob/892ca4ddc12852a7b4633fd08f163941356cb4e6/tensorflow/examples/skflow/text_classification_save_restore.py"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/blob/892ca4ddc12852a7b4633fd08f163941356cb4e6/tensorflow/examples/skflow/text_classification_save_restore.py</a></p></li>
<li><p>build a newer Tensorflow from github HEAD.</p></li>
</ul>

<p>I hope that helps!</p>
",2,1,427,2016-06-30 11:12:26,https://stackoverflow.com/questions/38121451/tensorflow-error-using-my-own-data-for-text-classification
Defining vocabulary size in text classification,"<p>I have a question regarding the defining of vocabulary set needed for feature extraction in text classification.
 In an experiment, there are two approaches I can think of:</p>

<p>1.Define vocabulary size using both training data and test data, so that no word from the test data would be treated as being 'unknown' during the testing.</p>

<p>2.Define vocabulary size according to data only from the training data, and treat every word in the testing data that does not also appear in the training data as 'unknown'.</p>

<p>At first glance the more scientific way is the second one. However it is worth noticing that although there is no way we can know about the true size of vocabulary in a practical system, there seems to be no problem to set the vocabulary size a little bit larger than the size appeared in the training data in order to cover potentially larger problems. This is helpful in that it actually treats different unknown words as being different, instead of summing them up as 'unknown'. Is there any reason why this is not practical?</p>

<p>New to machine learning. Help much appreciated. </p>
","machine-learning, nlp, text-classification","<p>If you include the test set words that don't occur in the training set into your model (e.g. a classification model) then because they have not occurred in the training set, their weight in the trained model will be zero and so they won't have any effect other than increasing the model size. So option 2 is better.</p>

<p>Having said that, to compensate for the changing nature of your test data, one solution is to re-train your model periodically, Another is to use word2vec to build representations and a K-Nearest Neighbour model that given each unseen word in the test set gives you the nearest word in the training set so that you can use that one instead of the unknown word.</p>
",9,6,9313,2016-07-02 02:44:24,https://stackoverflow.com/questions/38156017/defining-vocabulary-size-in-text-classification
One-to-one matching to labels for text classification,"<p>I am using <code>scikit-learn</code> for a text classification problem and I would like to know if there is a machine learning technique that uses a one-to-one, mutually exclusive mapping for labeling.</p>

<p>For example, say I want to label three documents based on what city they represent. My label choices are New York, Detroit and Los Angeles. My documents are ""The Big Apple,"" ""The Big City,"" and ""City of Angels."" Let's say just for this example that ""City of Angels"" most closely maps to Los Angeles, while both ""The Big Apple"" and ""The Big City"" should map most closely to New York. However, I want one to map to New York (""The Big Apple"" because let's say that has a better fit) and one to map to Detroit because New York has already been used, and Detroit is the only choice that's left and it still fits in some sense.</p>

<p>I want to tell the predictor that if it has used one label, it cannot use it again, so it needs to make the best guess for that label since it can only be used once.</p>

<p>Does <code>scikit-learn</code> or another library have a feature for handling this one-to-one (and only one) text classification like I would like to do?</p>
","machine-learning, scikit-learn, text-classification","<p>To achieve this kind of functionality, I'd suggest you do the following:</p>

<p>I'd assume that in your text classification algorithm, you obtain a probability score for each document for every label. </p>

<p>e.g.:</p>

<pre><code>  Documents  ""The Big Apple""  ""The Big City""  ""City of Angels""

  Label     

  ""New York""       0.45           0.45            0.1

  ""Detroit""        0.4            0.5             0.1                

  ""Los Angeles""    0.15           0.05            0.8
</code></pre>

<p>You might now be able to see where I am heading towards with this.</p>

<p>Use the argmax function (returns the label with the maximum probability for each document). </p>

<p>In this case, the argmax function would return the label ""New York"" for the documents ""The Big Apple"" and the ""The Big City"", the label ""The Big City"" for the document ""Detroit"" and the label ""Los Angeles"" for the document ""City of Angels"".</p>

<p>Since, in this case there is a conflict (I'd rather not call it conflict) in assigning a label ""New York"" for a document (since you require a one to one mapping), I'd say you go to the next label. The label ""The Big City"" can be clearly assigned to the document ""Detroit"" as it has the maximum probability (matching), and then you remove the label ""Detroit"" from the set of possible labels (remaining labels -> ""New York"" and ""Los Angeles""). You then move on to the next label ""Los Angeles"" and the argmax function tells you that the document ""City of Angels"" has the highest probability (maximum matching) of having the label ""Los Angeles"". You then remove the label ""Lost Angeles"" from the remaining labels. At this point, remaining labels -> ""New York"". You then go to the next label ""New York"" and see that the only document it can be assigned to is ""The Big Apple"" and you have a one-to-one mapping between the documents and the labels. </p>

<p>I have done this before in two ways, breaking a tie by assigning a label to a document randomly, or by breaking the tie by calculating the probability for the next label. This technique is also used in a decision tree algorithm to find the most suitable attribute at a given level in the tree. It is called as the entropy or the information gain of that attribute. This implementation is a simpler version of the information gain from the ID3 decision tree algorithm. </p>

<p>More about the ID3 decision tree algorithm <a href=""https://en.wikipedia.org/wiki/ID3_algorithm"" rel=""nofollow"">here</a>.</p>
",1,1,362,2016-07-07 22:36:30,https://stackoverflow.com/questions/38256348/one-to-one-matching-to-labels-for-text-classification
How to check an input string contains street address or not?,"<p>We want to identify the address fields from a document. For Identifying the address fields we converted the document to OCR files using Tesseract. From the tesseract output we want to check a string contains the address field or not . Which is the right strategy to resolve this problem ? </p>

<ol>
<li>Its not possible to solve this problem using the regex because address fields are different for various documents and countries</li>
<li>Tried NLTK for classifying the words but not works perfectly for address field.</li>
</ol>

<p><strong>Required output</strong> </p>

<pre><code>I am staying at 234 23 Philadelphia - Contains address files &lt;234 23 Philadelphia&gt;

I am looking for a place to stay - Not contains address 
</code></pre>

<p>Provide your suggestions to solve this problem .  </p>
","machine-learning, neural-network, nltk, street-address, text-classification","<p>As in many ML problems, there are mutiple posible solutions, and the important part(and the one commonly has greater impact) is not which algorithm or model you use, but feature engineering ,data preprocessing and standarization ,and things like that. The first solution comes to my mind(and its just an idea, i would test it and see how it performs) its:</p>

<ol>
<li>Get your training set examples  and list the ""N"" most commonly used words in all examples(thats your vocabulary), this list will contain every one of the ""N"" most used words , every word would be represented by a number(the list index)</li>
<li>Transform your training examples: read every training example and change its representation replacing every word by the number  of the word in the vocabolary.</li>
<li>Finally, for every training example create a feature vector of the same size as the vocabulary, and for every word in the vocabulary your feature vector will be 0(the corresponding word doesnt exists in your example) or 1(it exists) , or the count of how many times the word appears(again ,this is feature engineering)</li>
<li>Train multiple classifiers ,varing algorithms,parameters, training set sizes, etc, and do cross validation to choose your best model.</li>
</ol>

<p>And from there keep the standard ML workflow...</p>
",4,3,10586,2016-07-20 21:34:07,https://stackoverflow.com/questions/38491232/how-to-check-an-input-string-contains-street-address-or-not
Saving Word2Vec for CNN Text Classification,"<p>I want to train my own Word2Vec model for my text corpus. I can get the code from TensorFlow's tutorial. What I don't know is how to save this model to use for CNN text classification later? Should I use pickle to save it and then read it later?</p>
","tensorflow, deep-learning, text-classification, word2vec","<p>No pickling is not the way of saving the model in case of tensorflow.</p>

<p>Tensorflow provides with tensorflow serving for saving the models as proto bufs(for exporting the model). The way to save model would be to save the  tensorflow session as:
             <strong>saver.save(sess, 'my_test_model',global_step=1000)</strong></p>

<p>Heres the link for complete answer:
            <strong><a href=""https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model"">Tensorflow: how to save/restore a model?</a></strong></p>
",0,1,858,2016-07-24 18:06:16,https://stackoverflow.com/questions/38555148/saving-word2vec-for-cnn-text-classification
Classification using SVM,"<p>In an attempt to classify text I want to use SVM.
I want to classify test data into one of the labels(health/adult)
The training &amp; test data are text files</p>

<p>I am using python's scikit library.
While I was saving the text to txt files I encoded it in <code>utf-8</code>
that's why i am decoding them in the snippet.
Here's my attempted code</p>

<pre><code>String = String.decode('utf-8')
String2 = String2.decode('utf-8')
bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),
                                     token_pattern=r'\b\w+\b', min_df=1)

X_2 = bigram_vectorizer.fit_transform(String2).toarray()
X_1 = bigram_vectorizer.fit_transform(String).toarray()
X_train = np.array([X_1,X_2])
print type(X_train)
y = np.array([1, 2])
clf = SVC()
clf.fit(X_train, y)

#prepare test data
print(clf.predict(X))
</code></pre>

<p>This is the error I am getting</p>

<pre><code>  File ""/Users/guru/python_projects/implement_LDA/lda/apply.py"", line 107, in &lt;module&gt;
    clf.fit(X_train, y)
  File ""/Users/guru/python_projects/implement_LDA/lda/lib/python2.7/site-packages/sklearn/svm/base.py"", line 150, in fit
    X = check_array(X, accept_sparse='csr', dtype=np.float64, order='C')
  File ""/Users/guru/python_projects/implement_LDA/lda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 373, in check_array
    array = np.array(array, dtype=dtype, order=order, copy=copy)
ValueError: setting an array element with a sequence.
</code></pre>

<p>When I searched for the error, I found some results but they even didn't help. I think I am logically wrong here in applying SVM model. Can someone give me a hint on this?</p>

<p>Ref: <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"" rel=""nofollow"">[1]</a><a href=""http://scikit-learn.org/stable/modules/feature_extraction.html"" rel=""nofollow"">[2]</a></p>
","python, machine-learning, classification, svm, text-classification","<p>You have to combine your samples, vectorize them and then fit the classifier. Like this:</p>

<pre><code>String = String.decode('utf-8')
String2 = String2.decode('utf-8')
bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),
                                     token_pattern=r'\b\w+\b', min_df=1)

X_train = bigram_vectorizer.fit_transform(np.array([String, String2]))
print type(X_train)
y = np.array([1, 2])
clf = SVC()
clf.fit(X_train, y)

#prepare test data
print(clf.predict(bigram_vectorizer.transform(np.array([X1, X2, ...]))))
</code></pre>

<p>But 2 sample it's a very few amount of data so likely your prediction will not be accurate.</p>

<p><strong>EDITED:</strong></p>

<p>Also you can combine transformation and classification in one step using Pipeline.</p>

<pre><code>from sklearn.pipeline import Pipeline

print type(X_train) # Should be a list of texts length 100 in your case
y_train = ... # Should be also a list of length 100
clf = Pipeline([
    ('transformer', CountVectorizer(...)),
    ('estimator', SVC()),
])
clf.fit(X_train, y_train)

X_test = np.array([""sometext""]) # array of test texts length = 1
print(clf.predict(X_test))
</code></pre>
",2,0,284,2016-07-27 11:16:41,https://stackoverflow.com/questions/38611447/classification-using-svm
Encoding data&#39;s label for text classification,"<p>I am doing a project in clinical text classification. In my corpus ,data are already labelled by code (For examples: 768.2, V13.02, V13.09, 599.0 ...). I already separated text and labels then using word-embedded for text. I am going to feed them into convolution neural network. However, the labels are needs to encode, I read examples of sentiment text classification and mnist but they all used integers to classify their data, my label in text form that why I cannot use one-hot encoding like them. Could anyone suggest any way to do it ?
Thanks   </p>
","python, encoding, tensorflow, text-classification","<p>Discrete text label is easily convertible to discrete numeric data by creating an enumeration mapping. For example, assuming the labels ""Yes"", ""No"" and ""Maybe"":</p>

<pre><code>No    -&gt; 0
Yes   -&gt; 1
Maybe -&gt; 2
</code></pre>

<p>And now you have numeric data, which can later be converted back (as long as the algorithm treat those as discrete values and do not return 0.5 or something like that).</p>

<p>In the case each instance can have multiples labels, as you said in a comment, you can create the encoding by putting each label in a column (""one-hot encoding""). Even if some software do not implement that off-the-shelf, it is not hard to do by hand.</p>

<p>Here's a very simple (and not well-written to be honest) example using Panda's get_dummies function:</p>

<pre><code>import numpy as np
import pandas as pd
labels = np.array(['a', 'b', 'a', 'c', 'ab', 'a', 'ac'])
df = pd.DataFrame(labels, columns=['label'])
ndf = pd.get_dummies(df)
ndf.label_a = ndf.label_a + ndf.label_ab + ndf.label_ac
ndf.label_b = ndf.label_b + ndf.label_ab
ndf.label_c = ndf.label_c + ndf.label_ac
ndf = ndf.drop(['label_ab', 'label_ac'], axis=1)
ndf

    label_a label_b label_c
0   1.0     0.0     0.0
1   0.0     1.0     0.0
2   1.0     0.0     0.0
3   0.0     0.0     1.0
4   1.0     1.0     0.0
5   1.0     0.0     0.0
6   1.0     0.0     1.0
</code></pre>

<p>You can now train a multivariate model to output the values of <code>label_a</code>, <code>label_b</code> and <code>label_c</code> and then reconstruct the original labels like ""ab"". Just make sure the output is in the set [0, 1] (by applying softmax-layer or something like that).</p>
",1,0,1718,2016-08-02 02:41:50,https://stackoverflow.com/questions/38710993/encoding-datas-label-for-text-classification
Text2Vec classification with caret problems,"<p>Some context: <a href=""https://stackoverflow.com/questions/38755207/working-with-text-classification-and-big-sparse-matrices-in-r"">Working with text classification and big sparse matrices in R</a></p>

<p>I have been working on a text multi-class classification problem with the <code>text2vec</code> package and <code>caret</code>. The plan is to use <code>text2vec</code> for building the document-term matrix, prune vocabulary and all sorts of pre-processing stuff, and then try different models with <code>caret</code> but I can't get results as when training, caret throws some errors that look like the following:</p>

<pre><code>+ Fold02.Rep1: cost=0.25 
predictions failed for Fold01.Rep1: cost=0.25 Error in as.vector(data) : 
no method for coercing this S4 class to a vector
</code></pre>

<p>This happens for all the folds and repetitions. I suposse there is a problem when converting the document-term matrix that <code>text2vec</code> produces to a vector because caret needs to do some calculations, but I am honestly not sure and that is the main reason for this question.</p>

<p>The code used, with some skipped parts, looks as following. Note that I feed <code>caret</code> with the direct result of the document-term matrix that <code>text2vec</code> returns and I am not completely sure this is ok.</p>

<pre><code>library(text2vec)
library(caret)
data(""movie_review"")
train = movie_review[1:4000, ]
test = movie_review[4001:5000, ]

it &lt;- itoken(train$review, preprocess_function = tolower, tokenizer = word_tokenizer)
vocab &lt;- create_vocabulary(it, stopwords = tokenizers::stopwords())
pruned_vocab &lt;- prune_vocabulary(vocab, term_count_min = 10, doc_proportion_max = 0.5, doc_proportion_min = 0.001)

vectorizer &lt;- vocab_vectorizer(pruned_vocab)
it = itoken(train$review, tokenizer = word_tokenizer, ids = train$id)
dtm_train = create_dtm(it, vectorizer)
it = itoken(test$review, tokenizer = word_tokenizer, ids = test$id)
dtm_test = create_dtm(it, vectorizer)

ctrl.svm.1 &lt;- trainControl(method=""repeatedcv"",
                           number=10,
                           repeats=5,
                           summaryFunction = multiClassSummary,
                           verboseIter = TRUE)

fit.svm.1 &lt;- train(x = dtm_train, y= as.factor(train$sentiment), 
                   method=""svmLinear2"",  
                   metric=""Accuracy"", 
                   trControl = ctrl.svm.1, 
                   scale = FALSE, verbose = TRUE)
</code></pre>

<p>As I said, the problem appears when launching the train() function.
The dtm_train object is of class:</p>

<pre><code>[1] ""dgCMatrix""
attr(,""package"")
[1] ""Matrix""
</code></pre>

<p>And the structure looks like this:</p>

<pre><code>str(dtm_train)
&gt; Formal class 'dgCMatrix' [package ""Matrix""] with 6 slots
  ..@ i       : int [1:368047] 2582 2995 3879 3233 2118 2416 2468 2471 3044 3669 ...
  ..@ p       : int [1:6566] 0 0 3 4 4 10 10 14 14 22 ...
  ..@ Dim     : int [1:2] 4000 6565
  ..@ Dimnames:List of 2
  .. ..$ : chr [1:4000] ""5814_8"" ""2381_9"" ""7759_3"" ""3630_4"" ...
  .. ..$ : chr [1:6565] ""floriane"" ""lil"" ""elm"" ""kolchak"" ...
  ..@ x       : num [1:368047] 1 1 1 1 1 1 2 2 1 3 ...
  ..@ factors : list()
</code></pre>

<p>What am I doing wrong? Why is caret unable to work with this kind of data if in the documentation it implies that is able to?</p>
","r, svm, r-caret, text-classification, text2vec","<p>Íf you turn your S4 class dtm_train into a simple matrix the code will work.</p>

<pre><code>fit.svm.1 &lt;- train(x = as.matrix(dtm_train), y= as.factor(train$sentiment), 
                   method=""svmLinear2"",  
                   metric=""Accuracy"", 
                   trControl = ctrl.svm.1, 
                   scale = FALSE, verbose = TRUE)
</code></pre>

<p>Do not forget to do the same for your dtm_test otherwise the predict function will complain as well. </p>

<p><code>pred &lt;- predict(fit.svm.1, newdata = as.matrix(dtm_test)</code></p>
",4,4,1224,2016-08-04 13:19:10,https://stackoverflow.com/questions/38768499/text2vec-classification-with-caret-problems
sklearn - predict top 3-4 labels in multi-label classifications from text documents,"<p>I currently have a classifier <code>MultinomialNB()</code> set up using <code>CountVectorizer</code> for feature extraction from text documents, and whilst this works quite well, I want to use the same methodology to predict the top 3-4 labels, not just the top one.</p>

<p>The main reason is that there are c.90 labels and data input isn't great, resulting in a 35% accuracy for the top estimate. If I can offer the user the top 3-4 most likely labels as a suggestion, then I could significantly increase the accuracy coverage.</p>

<p>Any suggestions? Any pointers would be appreciated!</p>

<p>The current code looks like:</p>

<pre><code>import numpy
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.cross_validation import KFold
from sklearn.metrics import confusion_matrix, accuracy_score

df = pd.read_csv(""data/corpus.csv"", sep="","", encoding=""latin-1"")

df = df.set_index('id')
df.columns = ['class', 'text']

data = df.reindex(numpy.random.permutation(df.index))

pipeline = Pipeline([
    ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),
    ('classifier',         MultinomialNB())
])

k_fold = KFold(n=len(data), n_folds=6, shuffle=True)

for train_indices, test_indices in k_fold:
    train_text = data.iloc[train_indices]['text'].values
    train_y = data.iloc[train_indices]['class'].values.astype(str)

    test_text = data.iloc[test_indices]['text'].values
    test_y = data.iloc[test_indices]['class'].values.astype(str)

    pipeline.fit(train_text, train_y)
    predictions = pipeline.predict(test_text)
    confusion = confusion_matrix(test_y, predictions)

    accuracy = accuracy_score(test_y, predictions)
    print accuracy
</code></pre>
","python, scikit-learn, classification, text-classification, multilabel-classification","<p>Once you have done your predictions, you can get the probability of each labels with:</p>

<pre><code>labels_probability = pipeline.predict_proba(test_text)
</code></pre>

<p>You will get the probability for each label. see <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.predict_proba"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.predict_proba</a></p>
",1,3,2326,2016-08-04 15:25:26,https://stackoverflow.com/questions/38771478/sklearn-predict-top-3-4-labels-in-multi-label-classifications-from-text-docume
Using label encoder on a dictionary,"<p>I am using the sklearn <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow"">LabelEncoder</a>. I know how to use it for a 1D array, but my use case is as such:</p>

<p>I have multiple arrays of dicts like this (which is effectively the cost of me assigning each text label <code>u'a'</code>,<code>u'b'</code> etc in a classifier), all within a dict:</p>

<pre><code>{'open_model':    
[
    {u'a': 47502.125, u'c': 45.3, u'd': 2.3, u'e': 0.45},
    {u'b': 121, u'a': 1580.5625, u'c': 12, u'e': 62,u'd':0.343},
    {u'e': 12321, u'b': 4, u'a': 0.1112}
    ],
 'closed_model':
 [
    {u'a': 1231.22, u'c': 43.1},
    {u'b': 342.2, u'a': 121.1, u'c': 343},
    {u'b': 14.2, u'a': 53.2}
    ]
}
</code></pre>

<p>I need to be able to encode this into numerical labels and then decode all of them back, so for example:</p>

<pre><code>[
    {1: 47502.125, 3: 45.3, 4: 2.3, 5: 0.45},
    {2: 121, 1: 1580.5625, 3: 12, 5: 62, 4: 0.343},
    {5: 12321, 2: 4, 1: 0.1112}
    ]
</code></pre>

<p>Which I use effectively to generate predictions of the best label for each row, so:</p>

<pre><code>[5, 4, 1] perhaps in this case.
</code></pre>

<p>What I need to do is to be able to decode this back into:</p>

<pre><code>[u'e',u'd', u'a'] perhaps in this case.
</code></pre>

<p>How can I get the same <code>LabelEncoder</code> functionality but to <code>fit_transform</code> on an array of dicts where the dict keys are my labels?</p>

<p>Note, dict within the array of dicts is a different length, but I do have list of all the potential labels, i.e. for the open_model labels, <code>set([u'a',u'b',u'c',u'd',u'e'])</code> and for the closed_model labels: <code>set([u'a',u'b',u'c'])</code>.</p>
","python, dictionary, scikit-learn, text-classification, multilabel-classification","<p>Although it is a good practice to use already implemented functionality, you could easily achieve this with a couple of lines of code. Given your list input:</p>

<pre><code>dico = [
{u'a': 47502.125, u'b': 1580.5625, u'c': 45.3, u'd': 2.3, u'e': 0.45},
{u'b': 121, u'a': 1580.5625, u'c': 12, u'e': 62, u'd': 0.343},
{u'e': 12321, u'b': 4, u'd': 5434, u'c': 2.3, u'a': 0.1112}
]
</code></pre>

<p>you can get the set of labels by simply:</p>

<pre><code>keyset = set(dico[0].keys()) #Get the set of keys assuming they all appear in each list item. 
mapping = { val:key+1 for key,val in enumerate(list(keyset))} # Create a mapping from int -&gt; str
inv_mapping = { key+1:val for key,val in enumerate(list(keyset))} # Create a mapping from str:int. 
</code></pre>

<p>Having the <code>mapping</code> and <code>inv_mapping</code> you can change the representation of your data by:</p>

<pre><code>for inner_dict in dico:
    for key in inner_dict.keys():
        inner_dict[mapping[key]] = inner_dict.pop(key)
print dico
</code></pre>

<p>which will give you <code>[{1: 47502.125, ...}]</code> and then if needed:</p>

<pre><code>for inner_dict in dico:
    for key in inner_dict.keys():
        inner_dict[inv_mapping[key]] = inner_dict.pop(key)
print dico
</code></pre>

<p>to get the initial version.</p>

<p>Also, and maybe more closely related to your issue, having your output <code>[5, 4, 1]</code> you can easily transform it by:</p>

<pre><code>print [inv_mapping[i] for i in x]
</code></pre>
",2,1,5927,2016-08-08 22:04:38,https://stackoverflow.com/questions/38839211/using-label-encoder-on-a-dictionary
Reuse a logistic regression object for different fitted models,"<p>I have a <code>Pipeline</code> object that I want to fit on different combinations of training and test labels and thus using the <code>fit</code> objects, create different predictions. But I believe that <code>fit</code> using the same classifier object gets rid of previous <code>fit</code> objects.</p>

<p>An example of my code is:</p>

<pre><code>text_clf = Pipeline([('vect', CountVectorizer(analyzer=""word"",tokenizer=None,preprocessor=None,stop_words=None,max_features=5000)),
                          ('tfidf', TfidfTransformer(use_idf=True,norm='l2',sublinear_tf=True)),
                          ('clf',LogisticRegression(solver='newton-cg',class_weight='balanced', multi_class='multinomial',fit_intercept=True),
                          )])

    print ""Fitting the open multinomial BoW logistic regression model for probability models...\n""
    open_multi_logit_words = text_clf.fit(train_wordlist, train_property_labels)

    print ""Fitting the open multinomial BoW logistic regression model w/ "",threshold,"" MAPE threshold...\n""
    open_multi_logit_threshold_words = (text_clf.copy.deepcopy()).fit(train_wordlist, train_property_labels_threshold)
</code></pre>

<p>However, classifier objects do not have <code>deepcopy()</code> methods. How can I achieve what I need without having to define:</p>

<pre><code>text_clf_open_multi_logit = Pipeline([('vect', CountVectorizer(analyzer=""word"",tokenizer=None,preprocessor=None,stop_words=None,max_features=5000)),
                              ('tfidf', TfidfTransformer(use_idf=True,norm='l2',sublinear_tf=True)),
                              ('clf',LogisticRegression(solver='newton-cg',class_weight='balanced', multi_class='multinomial',fit_intercept=True),
                              )])
</code></pre>

<p>For all of my 16 classifier combinations?</p>
","python, scikit-learn, logistic-regression, text-classification","<p>I would try</p>

<pre><code>text_clf0=copy.deepcopy(text_clf)
open_multi_logit_threshold_words = text_clf0.fit(train_wordlist, train_property_labels_threshold)
</code></pre>

<p>EDIT: you can use a list</p>

<pre><code>text_clf_list=[copy.deepcopy(text_clf) for _ in range(16)]
</code></pre>

<p>or directly</p>

<pre><code>copy.deepcopy(text_c‌​lf).fit(train_wordlis‌​t, train_property_label‌​s_threshold) 
</code></pre>
",3,2,378,2016-08-17 13:09:24,https://stackoverflow.com/questions/38997591/reuse-a-logistic-regression-object-for-different-fitted-models
TypeError in Countvectorizer scikit-learn: Expected string or buffer,"<p>I am trying to solve a classification problem. when I feed the text to CountVectorizer it gives error: </p>

<blockquote>
  <p>expected string or buffer. </p>
</blockquote>

<p>Is anything wrong with my dataset as it contains message mixture of number and word even special character is also in message.</p>

<p>Sample how does message look like is following:</p>

<pre><code>0         I have not received my gifts which I ordered ok
1                 hth her wells idyll McGill kooky bbc.co
2                                   test test test 1 test
3                                                    test
4                         hello where is my reward points
5       hi, can you get koovs coupons or vouchers here...
</code></pre>

<p>Here is the code I used to do classification:</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
df = pd.read_excel('training_data.xlsx')
X_train = df.message
print X_train.shape
map_class_label = {'checkin':0, 'greeting':1,'more reward options':2,'noclass':3, 'other':4,'points':5,
                           'referral points':6,'snapbill':7, 'thanks':8,'voucher not working':9,'voucher':10}
df['label_num'] = df['Final Category'].map(map_class_label)
y_train = df.label_num
vectorizer = CountVectorizer(lowercase=False,decode_error='ignore')
X_train_dtm = vectorizer.fit_transform(X_train)
</code></pre>
","python-2.7, pandas, dataframe, scikit-learn, text-classification","<p>You need convert column <code>message</code> to <code>string</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""nofollow""><code>astype</code></a>, because in data are some numeric values:</p>

<pre><code>df = pd.read_excel('training_data.xlsx')
df['message'] = df['message'].values.astype('unicode')
...
...
</code></pre>
",1,1,2422,2016-08-24 06:39:54,https://stackoverflow.com/questions/39116088/typeerror-in-countvectorizer-scikit-learn-expected-string-or-buffer
"In general, when does TF-IDF reduce accuracy?","<p>I'm training a corpus consisting of 200000 reviews into positive and negative reviews using a Naive Bayes model, and I noticed that performing TF-IDF actually reduced the accuracy (while testing on test set of 50000 reviews) by about 2%. So I was wondering if TF-IDF has any underlying assumptions on the data or model that it works with, i.e. any cases where accuracy is reduced by the use of it?</p>
","sentiment-analysis, tf-idf, text-classification, naivebayes","<p>The IDF component of TF*IDF can harm your classification accuracy in some cases. </p>

<p>Let suppose the following artificial, easy classification task, made for the sake of illustration: </p>

<ul>
<li>Class A: texts containing the word 'corn' </li>
<li>Class B: texts not containing the word 'corn'</li>
</ul>

<p>Suppose now that in Class A, you have 100 000 examples and in class B, 1000 examples. </p>

<p>What will happen to TFIDF? The inverse document frequency of corn will be very low (because it is found in almost all documents), and the feature 'corn' will get a very small TFIDF, which is the weight of the feature used by the classifier. Obviously, 'corn' was THE best feature for this classification task. This is an example where TFIDF may reduce your classification accuracy. In more general terms:</p>

<ul>
<li>when there is class imbalance. If you have more instances in one class, the good word features of the frequent class risk having lower IDF, thus their best features will have a lower weight</li>
<li>when you have words with high frequency that are very predictive of one of the classes (words found in most documents of that class)</li>
</ul>
",11,2,4825,2016-08-25 18:18:28,https://stackoverflow.com/questions/39152229/in-general-when-does-tf-idf-reduce-accuracy
How can we classify a post on a blog as being inappropriate for underage readers say 16?,"<p>Update: How would one approach the task of classifying any text on public forums such as Games, or blogs such that derogatory comments/texts before bring posted are filtered.</p>
<p>Original: &quot;
I want to filter out adult content from tweets (or any text for that matter).</p>
<p>For spam detection, we have datasets that check whether a particular text is spam or ham.</p>
<p>For adult content, I found a dataset I want to use (extract below):</p>
<pre><code>arrBad = [
'acrotomophilia',
'anal',
'anilingus',
'anus',
.
. etc.
.
'zoophilia']
</code></pre>
<p><strong>Question</strong></p>
<p>How can I use that dataset to filter text instances?
&quot;</p>
","python, nlp, nltk, text-classification","<p>I would approach this as a Text Classification problem, because using blacklists of words typically does not work very well to classify full texts. The main reason why blacklists don't work is that you will have a lot of false positives (one example: your list contains the word 'sexy', which alone isn't enough to flag a document as being for adults). To do so you need a training set with documents tagged as being ""adult content"" and others ""safe for work"". So here is what I would do:</p>

<ol>
<li>check whether an existing labelled dataset can be used. You need
several thousands of documents of each class.  </li>
<li>If you don't find any, create one. For instance you can create a scraper and download Reddit content. Read for instance <a href=""http://www.cs.ubc.ca/~nando/540-2013/projects/p38.pdf"" rel=""noreferrer"">Text Classification of NSFW Reddit Posts</a> </li>
<li>Build a text classifier with NLTK. If you don't know how, read: <a href=""http://www.nltk.org/book/ch06.html"" rel=""noreferrer"">Learning to Classify Text</a></li>
</ol>
",8,-2,2066,2016-08-29 07:39:04,https://stackoverflow.com/questions/39200888/how-can-we-classify-a-post-on-a-blog-as-being-inappropriate-for-underage-readers
nltk naivebayes classifier for text classification,"<p>In following code, I know that my naivebayes classifier is working correctly because it is working correctly on trainset1 but why is it not working on trainset2? I even tried it on two classifiers, one from TextBlob and other directly from nltk.</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob
from nltk.tokenize import word_tokenize
import nltk

trainset1 = [('I love this sandwich.', 'pos'),
('This is an amazing place!', 'pos'),
('I feel very good about these beers.', 'pos'),
('This is my best work.', 'pos'),
(""What an awesome view"", 'pos'),
('I do not like this restaurant', 'neg'),
('I am tired of this stuff.', 'neg'),
(""I can't deal with this"", 'neg'),
('He is my sworn enemy!', 'neg'),
('My boss is horrible.', 'neg')]

trainset2 = [('hide all brazil and everything plan limps to anniversary inflation plan initiallyis limping its first anniversary amid soaring prices', 'class1'),
         ('hello i was there and no one came', 'class2'),
         ('all negative terms like sad angry etc', 'class2')]

def nltk_naivebayes(trainset, test_sentence):
    all_words = set(word.lower() for passage in trainset for word in word_tokenize(passage[0]))
    t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in trainset]
    classifier = nltk.NaiveBayesClassifier.train(t)
    test_sent_features = {word.lower(): (word in word_tokenize(test_sentence.lower())) for word in all_words}
    return classifier.classify(test_sent_features)

def textblob_naivebayes(trainset, test_sentence):
    cl = NaiveBayesClassifier(trainset)
    blob = TextBlob(test_sentence,classifier=cl)
    return blob.classify() 

test_sentence1 = ""he is my horrible enemy""
test_sentence2 = ""inflation soaring limps to anniversary""

print nltk_naivebayes(trainset1, test_sentence1)
print nltk_naivebayes(trainset2, test_sentence2)
print textblob_naivebayes(trainset1, test_sentence1)
print textblob_naivebayes(trainset2, test_sentence2)
</code></pre>

<p>Output:</p>

<pre><code>neg
class2
neg
class2
</code></pre>

<p>Although test_sentence2 clearly belongs to class1.</p>
","machine-learning, nlp, nltk, text-classification, document-classification","<p>I will assume your understand that you cannot expect a classifier to learn a good model with only 3 examples, and that your question is more to understand why it does that in this specific example. </p>

<p>The likely reason it does that is that naive bayes classifier uses a prior class probability. That is, the probability of neg vs pos, regardless of the text. In your case, 2/3 of the examples are negative, thus the prior is 66% for neg and 33% for pos. The positive words in your single positive instance are 'anniversary' and 'soaring', which are unlikely to be enough to compensate this prior class probability. </p>

<p>In particular, be aware that the calculation of word probabilities involve various 'smoothing' functions (for instance, it will be log10(Term Frequency + 1)  in each class, not log10(Term Frequency) to prevent low frequency words to impact too much the classification results, divisions by zero, etc. Thus the probabilities for ""anniversary"" and ""soaring"" are not 0.0 for neg and 1.0 for pos, unlike what you may have expected.</p>
",5,3,945,2016-09-06 14:38:22,https://stackoverflow.com/questions/39351735/nltk-naivebayes-classifier-for-text-classification
Why do Tensorflow tf.learn classification results vary a lot?,"<p>I use the TensorFlow high-level API <code>tf.learn</code> to train and evaluate a DNN classifier for a series of binary text classifications (actually I need multi-label classification but at the moment I check every label separately). My code is very similar to the <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/tflearn/index.html#tf-contrib-learn-quickstart"" rel=""nofollow"">tf.learn Tutorial</a></p>

<pre><code>classifier = tf.contrib.learn.DNNClassifier(
    hidden_units=[10],
    n_classes=2,
    dropout=0.1,
    feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(training_set.data))
classifier.fit(x=training_set.data, y=training_set.target, steps=100)
val_accuracy_score = classifier.evaluate(x=validation_set.data, y=validation_set.target)[""accuracy""]
</code></pre>

<p>Accuracy score varies roughly from 54% to 90%, with 21 documents in the validation (test) set which are always the same.</p>

<p>What does the very significant deviation mean? I understand there are some random factors (eg. dropout), but to my understanding the model should converge towards an optimum.</p>

<p>I use words (lemmas), bi- and trigrams, sentiment scores and <a href=""http://liwc.wpengine.com/"" rel=""nofollow"">LIWC scores</a> as features, so I do have a very high-dimensional feature space, with only 28 training and 21 validation documents. Can this cause problems? How can I consistently improve the results apart from collecting more training data?</p>

<p>Update: To clarify, I generate a dictionary of occurring words and n-grams and discard those that occur only 1 time, so I only use words (n-grams) that exist in the corpus.</p>
","machine-learning, tensorflow, classification, deep-learning, text-classification","<p>This has nothing to do with TensorFlow. This dataset is ridiculously small, thus you can obtain any results. You have 28 + 21 points, in a space which has ""infinite"" amount of dimensions (there are around 1,000,000 english words, thus 10^18 trigrams, however some of them do not exist, and for sure they do not exist in your 49 documents, but still you have at least 1,000,000 dimensions). For such problem, you have to expect <strong>huge</strong> variance of the results.</p>

<blockquote>
  <p>How can I consistently improve the results apart from collecting more training data?</p>
</blockquote>

<p>You pretty much cannot. This is simply way to small sample to do any statistical analysis. </p>

<p>Consequently the best you can do is <strong>change evaluation scheme</strong> instead of splitting data to 28/21 do 10-fold cross validation, with ~50 points this means that you will have to run 10 experiments, each with 45 training documents and 4 testing ones, and average the result. This is the only thing you can do to reduce the variance, however remember that even with CV, dataset so small gives you <strong>no guarantees</strong> how well your model will actualy behave ""in the wild"" (once applied to never seen before data). </p>
",2,0,1135,2016-09-10 18:31:30,https://stackoverflow.com/questions/39429313/why-do-tensorflow-tf-learn-classification-results-vary-a-lot
Text Classification for multiple label,"<p>I doing Text Classification by Convolution Neural Network. I used health documents (ICD-9-CM code) for my project and I used the same model as <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow"">dennybritz</a> used but my data has 36 labels. I used one_hot encoding to encode my label.</p>

<p>Here is my problem, when I run data which has one label for each document my code the accuracy is perfect from 0.8 to 1. If I run data which has more than one labels, the accuracy is significantly reduced. </p>

<p>For example: a document has single label as <code>""782.0""</code>: <code>[0 0 1 0 ... 0]</code>,<br>
a document has multiple label as <code>""782.0 V13.09 593.5""</code>: <code>[1 0 1 0 ... 1]</code>.</p>

<p>Could anyone suggest why this happen and how to improve it?</p>
","python, machine-learning, tensorflow, text-classification","<p>The label encoding seems correct. If you have multiple correct labels, <code>[1 0 1 0 ... 1]</code> looks totally fine. The loss function used in Denny's <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">post</a> is <code>tf.nn.softmax_cross_entropy_with_logits</code>, which is the loss function for a multi-class problem. </p>

<blockquote>
  <p>Computes softmax cross entropy between logits and labels.</p>
  
  <p>Measures the probability error in discrete classification tasks in
  which the classes are <strong>mutually exclusive</strong> (each entry is in exactly one class).</p>
</blockquote>

<p>In multi-label problem, you should use <code>tf.nn.sigmoid_cross_entropy_with_logits</code>:</p>

<blockquote>
  <p>Computes sigmoid cross entropy given logits.</p>
  
  <p>Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.</p>
</blockquote>

<p>The input to the loss function would be logits (<code>WX</code>) and targets (labels).</p>

<h3>Fix the accuracy measure</h3>

<p>In order to measure the accuracy correctly for a multi-label problem, the code below needs to be changed.</p>

<pre><code># Calculate Accuracy
with tf.name_scope(""accuracy""):
    correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))
    self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")
</code></pre>

<p>The logic of <code>correct_predictions</code> above is incorrect when you could have multiple correct labels. For example, say <code>num_classes=4</code>, and label 0 and 2 are correct. Thus your <code>input_y=[1, 0, 1, 0].</code> The <code>correct_predictions</code> would need to break tie between index 0 and index 2. I am not sure how <code>tf.argmax</code> breaks tie but if it breaks the tie by choosing the smaller index, a prediction of label 2 is always considered wrong, which definitely hurt your accuracy measure. </p>

<p>Actually in a multi-label problem, <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow noreferrer"">precision and recall</a> are better metrics than accuracy. Also you can consider using precision@k (<code>tf.nn.in_top_k</code>) to report classifier performance.</p>
",4,1,1423,2016-09-14 11:23:23,https://stackoverflow.com/questions/39489197/text-classification-for-multiple-label
Calculate cosine similarity from tf-idf,"<p>In a data frame <code>df</code> I have a following column <code>tf-idf</code>:</p>

<pre><code>       tf-idf
0      {u'selection': 3.83579393163, u'carltons': 7.0...
1      {u'precise': 6.43261849762, u'thomas': 3.31980...
2      {u'just': 2.70047792082, u'issued': 4.42829758...
3      {u'englishreading': 9.88788310056, u'all': 1.6...
4      {u'they': 1.89922701484, u'gangstergenka': 10....
5      {u'since': 1.45530416153, u'less': 3.956522477...
6      {u'exclusive': 10.4488880129, u'producer': 2.6...
7      {u'taxi': 6.04485296662, u'all': 1.64302370465...
8      {u'houston': 3.93463976627, u'frankie': 6.0306...
9      {u'phenomenon': 5.74474837417, u'deborash': 10...
10     {u'zwigoff': 19.7757662011, u'september': 1.90...
11     {u'gospels': 7.9419729515, u'theft': 6.0028887... `
</code></pre>

<p>I am struggling to find <code>cosine similarity</code> between two samples - for example between <code>df['tf-idf'][0]</code> and <code>df['tf-idf'][1]</code>.</p>
","python-2.7, pandas, scikit-learn, text-classification, cosine-similarity","<p>You could use scikit-learn:</p>

<pre><code>from sklearn.feature_extraction import DictVectorizer
from sklearn.metrics.pairwise import cosine_similarity

a = DictVectorizer().fit_transform(df['tf-idf'])
cosine_similarity(a[0], a[1])
</code></pre>
",2,2,466,2016-09-26 11:30:36,https://stackoverflow.com/questions/39701677/calculate-cosine-similarity-from-tf-idf
Python: classify text into the categories,"<p>I have a part of training set</p>

<pre><code>url  category
ebay.com/sch/Mens-Clothing-/1059/i.html?_from=R40&amp;LH_BIN=1&amp;Bottoms%2520Size%2520%2528Men%2527s%2529=33&amp;Size%2520Type=Regular&amp;_nkw=Джинсы&amp;_dcat=11483&amp;Inseam=33&amp;rt=nc&amp;_trksid=p2045573.m1684 Онлайн-магазин
google.ru/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%D0%BA%D0%BA%D1%83%D0%BF%D0%BE%D0%BD%D1%8B%20aliexpress%202016  Search
google.ru/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#newwindow=1&amp;q=%D0%BA%D1%83%D0%BF%D0%BE%D0%BD%D1%8B+aliexpress+2016    Search
google.ru/search?q=авито&amp;oq=авито&amp;aqs=chrome..69i57j0l5.1608j0j7&amp;sourceid=chrome&amp;es_sm=122&amp;ie=UTF-8 Search
irecommend.ru/content/kogda-somnenii-byt-ne-mozhet-tolko-klear-blyu-pomozhet    Форумы и отзывы
ebay.com/sch/Mens-Clothing-/1059/i.html?_from=R40&amp;LH_BIN=1&amp;Bottoms%2520Size%2520%2528Men%2527s%2529=33&amp;Size%2520Type=Regular&amp;_dcat=11483&amp;Inseam=33&amp;_nkw=Джинсы&amp;_sop=15  Онлайн-магазин
ebay.com/sch/Mens-Clothing-/1059/i.html?_from=R40&amp;LH_BIN=1&amp;Bottoms%2520Size%2520%2528Men%2527s%2529=33&amp;Size%2520Type=Regular&amp;_dcat=11483&amp;Inseam=33&amp;_nkw=Джинсы&amp;_sop=15  Онлайн-магазин
irecommend.ru/content/gramotnyi-razvod-na-dengi-bolshe-ne-kuplyu-vret   Форумы и отзывы
google.ru/search?q=яндекс&amp;oq=яндекс&amp;aqs=chrome..69i57j69i61l3j69i59l2.1383j0j1&amp;sourceid=chrome&amp;es_sm=93&amp;ie=UTF-8    Search
google.ru/search?q=авито&amp;oq=авито&amp;aqs=chrome..69i57j69i59j69i60.1095j0j1&amp;sourceid=chrome&amp;es_sm=93&amp;ie=UTF-8  Search
otzovik.com/review_1399716.html#debug   Форумы и отзывы
svyaznoy.ru Онлайн-магазин
mvideo.ru/smartfony-sotovye-telefony/apple-iphone-2927  Онлайн-магазин
mvideo.ru/promo/rassrochka-0-0-12-mark24197850/f/category=iphone-914?sort=priceLow&amp;_=1453896710474&amp;categoryId=10    Онлайн-магазин
svyaznoy.ru/catalog/phone/224/tag/windows-phone Онлайн-магазин
google.it/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%D0%B5%D0%B2%D1%80%D0%BE%D1%81%D0%B5%D1%82%D1%8C    Search
vk.com   Social network
</code></pre>

<p>it's a connection between <code>url</code> and <code>category</code>
And also I have test set and I need to get category to every url.</p>

<pre><code>url    
vk.com/topic-102849764_32295213
stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set
google.ru/search?q=learning+sample&amp;oq=learning+sample&amp;aqs=chrome..69i57.4063j0j1&amp;sourceid=chrome&amp;ie=UTF-8#newwindow=1&amp;q=machine+learning+test+and+learn
facebook.com
locals.ru
tvzvezda.ru/news/vstrane_i_mire/content/201609261038-k6n1.htm
</code></pre>

<p>I don't know, what algorithm should I use to solve this task.
I need the best way to get the most accuracy.
And I think it's a problem, that I have multiple categories.</p>

<p>I try first parse html tag <code>title</code>, because I think, that I can's determine category only with <code>url</code>.</p>
","python, url, machine-learning, scikit-learn, text-classification","<p>Basically you will classify strings into categories. Therefore you will to use a classifier. But you will not just use one classifier but rather test several and chose the most accurate. </p>

<p>Yet firstly, you will have to think about features of each url. I expect that you will not achieve great accuracy if you are simply feeding the url as a string and as the only feature.</p>

<p>Rather you will preprocess each url to extract features. The choice of relevant/useful features strongly depends on the domain. A feature could be:</p>

<p>simple features</p>

<ul>
<li><p>the first word until the dot such as: facebook for ""facebook.com"" </p></li>
<li><p>the length of the whole string</p></li>
</ul>

<p>complex features</p>

<p>imagine you define keywords for each cluster such as for ""online-shopping""-cluster you will define [promo, buy, shop, sell, price], then you can compute the number of keywords which occur in the string for each cluster as a feature</p>

<p>Therefore, you will have to continue firstly with <strong>feature-engineering</strong> and secondly with a comparing classifier performance. </p>

<p>Additional input: </p>

<p><a href=""https://stackoverflow.com/questions/26456904/how-to-classify-urls-what-are-urls-features-how-to-select-and-extract-features"">Similiar question on SO (regarding URL features)</a></p>

<p><a href=""http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/"" rel=""nofollow noreferrer"">Text feature extraction</a></p>

<p><a href=""https://www.comp.nus.edu.sg/~kanmy/papers/cp689-kan.pdf"" rel=""nofollow noreferrer"">Fast Webpage Classification Using URL Features</a></p>

<p>EDIT: An example</p>

<pre><code>url = ""irecommend.ru/content/kogda-somnenii-byt-ne-mozhet-tolko-klear-blyu-pomozhet""    

f1  = len(url) = 76
f2 = base = str(url).split(""/"",1)[0] = ""irecommend.ru""
f3 = segments = str(a).count(""/"") = 2
</code></pre>

<p>more solutions from <a href=""https://stackoverflow.com/questions/6969268/counting-letters-numbers-and-punctuation-in-a-string"">here</a> by <a href=""https://stackoverflow.com/users/997301/eiyrio%C3%BC-von-kauyf"">Eiyrioü von Kauyf</a></p>

<pre><code>import string
count = lambda l1,l2: sum([1 for x in l1 if x in l2])

f4 = count_punctuation = count(a,set(string.punctuation))
f5 = count_ascii = count(a,set(string.ascii_letters))
</code></pre>

<p>Yet all these examples are very simple features, which do not cover the semantic content of the URL. Depending on the depth/sophistication of your target variables (clusters), you might need to use features n-gram based features such as in <a href=""https://www.google.de/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwjgrsC2i7TPAhWKaRQKHUfLBD4QFggvMAA&amp;url=http%3A%2F%2Fwww.springer.com%2Fcda%2Fcontent%2Fdocument%2Fcda_downloaddocument%2F9783319258393-c2.pdf%3FSGWID%3D0-0-45-1533089-p177762447&amp;usg=AFQjCNHJeV8TIhj1ylnkG0aMWIZoNJK4bA&amp;cad=rja"" rel=""nofollow noreferrer"">here</a></p>
",2,1,2399,2016-09-28 10:53:14,https://stackoverflow.com/questions/39745431/python-classify-text-into-the-categories
Setting up a MLP for binary classification with tensorflow,"<p>I have some troubles trying to set up a multilayer perceptron for binary classification using tensorflow.</p>

<p>I have a very large dataset (about 1,5*10^6 examples) each with a binary (0/1) label and 100 features. What I need to do is to set up a simple MLP and then try to change the learning rate and the initialization pattern to document the results (it's an assignment). 
I am getting strange results, though, as my MLP seem to get stuck with a low-but-not-great cost early and never getting off of it. With fairly low values of learning rate the cost goes NAN almost immediately. I don't know if the problem lies in how I structured the MLP (I did a few tries, going to post the code for the last one) or if I am missing something with my tensorflow implementation.</p>

<h1>CODE</h1>

<pre><code>import tensorflow as tf
import numpy as np
import scipy.io

# Import and transform dataset
print(""Importing dataset."")
dataset = scipy.io.mmread('tfidf_tsvd.mtx')

with open('labels.txt') as f:
    all_labels = f.readlines()

all_labels = np.asarray(all_labels)
all_labels = all_labels.reshape((1498271,1))

# Split dataset into training (66%) and test (33%) set
training_set    = dataset[0:1000000]
training_labels = all_labels[0:1000000]
test_set        = dataset[1000000:1498272]
test_labels     = all_labels[1000000:1498272]

print(""Dataset ready."") 

# Parameters
learning_rate   = 0.01 #argv
mini_batch_size = 100
training_epochs = 10000
display_step    = 500

# Network Parameters
n_hidden_1  = 64    # 1st hidden layer of neurons
n_hidden_2  = 32    # 2nd hidden layer of neurons
n_hidden_3  = 16    # 3rd hidden layer of neurons
n_input     = 100   # number of features after LSA

# Tensorflow Graph input
x = tf.placeholder(tf.float64, shape=[None, n_input], name=""x-data"")
y = tf.placeholder(tf.float64, shape=[None, 1], name=""y-labels"")

print(""Creating model."")

# Create model
def multilayer_perceptron(x, weights):
    # First hidden layer with SIGMOID activation
    layer_1 = tf.matmul(x, weights['h1'])
    layer_1 = tf.nn.sigmoid(layer_1)
    # Second hidden layer with SIGMOID activation
    layer_2 = tf.matmul(layer_1, weights['h2'])
    layer_2 = tf.nn.sigmoid(layer_2)
    # Third hidden layer with SIGMOID activation
    layer_3 = tf.matmul(layer_2, weights['h3'])
    layer_3 = tf.nn.sigmoid(layer_3)
    # Output layer with SIGMOID activation
    out_layer = tf.matmul(layer_2, weights['out'])
    return out_layer

# Layer weights, should change them to see results
weights = {
    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], dtype=np.float64)),       
    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=np.float64)),
    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],dtype=np.float64)),
    'out': tf.Variable(tf.random_normal([n_hidden_2, 1], dtype=np.float64))
}

# Construct model
pred = multilayer_perceptron(x, weights)

# Define loss and optimizer
cost = tf.nn.l2_loss(pred-y,name=""squared_error_cost"")
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initializing the variables
init = tf.initialize_all_variables()

print(""Model ready."")

# Launch the graph
with tf.Session() as sess:
    sess.run(init)

    print(""Starting Training."")

    # Training cycle
    for epoch in range(training_epochs):
        #avg_cost = 0.
        # minibatch loading
        minibatch_x = training_set[mini_batch_size*epoch:mini_batch_size*(epoch+1)]
        minibatch_y = training_labels[mini_batch_size*epoch:mini_batch_size*(epoch+1)]
        # Run optimization op (backprop) and cost op
        _, c = sess.run([optimizer, cost], feed_dict={x: minibatch_x, y: minibatch_y})

        # Compute average loss
        avg_cost = c / (minibatch_x.shape[0])

        # Display logs per epoch
        if (epoch) % display_step == 0:
        print(""Epoch:"", '%05d' % (epoch), ""Training error="", ""{:.9f}"".format(avg_cost))

    print(""Optimization Finished!"")

    # Test model
    # Calculate accuracy
    test_error = tf.nn.l2_loss(pred-y,name=""squared_error_test_cost"")/test_set.shape[0]
    print(""Test Error:"", test_error.eval({x: test_set, y: test_labels}))
</code></pre>

<h1>OUTPUT</h1>

<pre><code>python nn.py
Importing dataset.
Dataset ready.
Creating model.
Model ready.
Starting Training.
Epoch: 00000 Training error= 0.331874878
Epoch: 00500 Training error= 0.121587482
Epoch: 01000 Training error= 0.112870921
Epoch: 01500 Training error= 0.110293652
Epoch: 02000 Training error= 0.122655269
Epoch: 02500 Training error= 0.124971940
Epoch: 03000 Training error= 0.125407845
Epoch: 03500 Training error= 0.131942481
Epoch: 04000 Training error= 0.121696954
Epoch: 04500 Training error= 0.116669835
Epoch: 05000 Training error= 0.129558477
Epoch: 05500 Training error= 0.122952110
Epoch: 06000 Training error= 0.124655344
Epoch: 06500 Training error= 0.119827300
Epoch: 07000 Training error= 0.125183779
Epoch: 07500 Training error= 0.156429254
Epoch: 08000 Training error= 0.085632880
Epoch: 08500 Training error= 0.133913128
Epoch: 09000 Training error= 0.114762624
Epoch: 09500 Training error= 0.115107805
Optimization Finished!
Test Error: 0.116647016708
</code></pre>

<p><strong>This is what MMN advised</strong></p>

<pre><code>weights = {
    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=0, dtype=np.float64)),     
    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=0.01, dtype=np.float64)),
    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],  stddev=0.01, dtype=np.float64)),
    'out': tf.Variable(tf.random_normal([n_hidden_2, 1], dtype=np.float64))
}
</code></pre>

<p><strong>This is the output</strong></p>

<pre><code>Epoch: 00000 Training error= 0.107566668
Epoch: 00500 Training error= 0.289380907
Epoch: 01000 Training error= 0.339091784
Epoch: 01500 Training error= 0.358559815
Epoch: 02000 Training error= 0.122639698
Epoch: 02500 Training error= 0.125160135
Epoch: 03000 Training error= 0.126219718
Epoch: 03500 Training error= 0.132500418
Epoch: 04000 Training error= 0.121795254
Epoch: 04500 Training error= 0.116499476
Epoch: 05000 Training error= 0.124532673
Epoch: 05500 Training error= 0.124484790
Epoch: 06000 Training error= 0.118491177
Epoch: 06500 Training error= 0.119977633
Epoch: 07000 Training error= 0.127532511
Epoch: 07500 Training error= 0.159053519
Epoch: 08000 Training error= 0.083876224
Epoch: 08500 Training error= 0.131488483
Epoch: 09000 Training error= 0.123161189
Epoch: 09500 Training error= 0.125011362
Optimization Finished!
Test Error: 0.129284643093
</code></pre>

<p><strong>Connected third hidden layer, thanks to MMN</strong></p>

<p>There was a mistake in my code and I had two hidden layers instead of three. I corrected doing:</p>

<pre><code>'out': tf.Variable(tf.random_normal([n_hidden_3, 1], dtype=np.float64))
</code></pre>

<p>and</p>

<pre><code>out_layer = tf.matmul(layer_3, weights['out'])
</code></pre>

<p>I returned to the old value for stddev though, as it seems to cause less fluctuation in the cost function.</p>

<p><strong>The output is still troubling</strong></p>

<pre><code>Epoch: 00000 Training error= 0.477673073
Epoch: 00500 Training error= 0.121848744
Epoch: 01000 Training error= 0.112854530
Epoch: 01500 Training error= 0.110597624
Epoch: 02000 Training error= 0.122603499
Epoch: 02500 Training error= 0.125051472
Epoch: 03000 Training error= 0.125400717
Epoch: 03500 Training error= 0.131999354
Epoch: 04000 Training error= 0.121850889
Epoch: 04500 Training error= 0.116551533
Epoch: 05000 Training error= 0.129749704
Epoch: 05500 Training error= 0.124600464
Epoch: 06000 Training error= 0.121600218
Epoch: 06500 Training error= 0.121249676
Epoch: 07000 Training error= 0.132656938
Epoch: 07500 Training error= 0.161801757
Epoch: 08000 Training error= 0.084197352
Epoch: 08500 Training error= 0.132197409
Epoch: 09000 Training error= 0.123249055
Epoch: 09500 Training error= 0.126602369
Optimization Finished!
Test Error: 0.129230736355
</code></pre>

<p><strong>Two more changes thanks to Steven</strong>
So Steven proposed to change Sigmoid activation function with ReLu, and so I tried. In the mean time, I noticed I didn't set an activation function for the output node, so I did that too (should be easy to see what I changed).</p>

<pre><code>Starting Training.
Epoch: 00000 Training error= 293.245977809
Epoch: 00500 Training error= 0.290000000
Epoch: 01000 Training error= 0.340000000
Epoch: 01500 Training error= 0.360000000
Epoch: 02000 Training error= 0.285000000
Epoch: 02500 Training error= 0.250000000
Epoch: 03000 Training error= 0.245000000
Epoch: 03500 Training error= 0.260000000
Epoch: 04000 Training error= 0.290000000
Epoch: 04500 Training error= 0.315000000
Epoch: 05000 Training error= 0.285000000
Epoch: 05500 Training error= 0.265000000
Epoch: 06000 Training error= 0.340000000
Epoch: 06500 Training error= 0.180000000
Epoch: 07000 Training error= 0.370000000
Epoch: 07500 Training error= 0.175000000
Epoch: 08000 Training error= 0.105000000
Epoch: 08500 Training error= 0.295000000
Epoch: 09000 Training error= 0.280000000
Epoch: 09500 Training error= 0.285000000
Optimization Finished!
Test Error: 0.220196439287
</code></pre>

<p><strong>This is what it does with the Sigmoid activation function on every node, output included</strong></p>

<pre><code>Epoch: 00000 Training error= 0.110878121
Epoch: 00500 Training error= 0.119393080
Epoch: 01000 Training error= 0.109229532
Epoch: 01500 Training error= 0.100436962
Epoch: 02000 Training error= 0.113160662
Epoch: 02500 Training error= 0.114200962
Epoch: 03000 Training error= 0.109777990
Epoch: 03500 Training error= 0.108218725
Epoch: 04000 Training error= 0.103001394
Epoch: 04500 Training error= 0.084145737
Epoch: 05000 Training error= 0.119173495
Epoch: 05500 Training error= 0.095796251
Epoch: 06000 Training error= 0.093336573
Epoch: 06500 Training error= 0.085062860
Epoch: 07000 Training error= 0.104251661
Epoch: 07500 Training error= 0.105910949
Epoch: 08000 Training error= 0.090347288
Epoch: 08500 Training error= 0.124480612
Epoch: 09000 Training error= 0.109250224
Epoch: 09500 Training error= 0.100245836
Optimization Finished!
Test Error: 0.110234139674
</code></pre>

<p>I found these numbers very strange, in the first case, it is stuck in a higher cost than sigmoid, even though sigmoid should saturate very early. In the second case, it starts with a training error which is almost the last one... so it basically converges with one mini-batch. I'm starting to think that I am not calculating the cost correctly, in this line:
    avg_cost = c / (minibatch_x.shape[0])</p>
","machine-learning, tensorflow, deep-learning, text-classification","<p>So it could be a couple of things:</p>

<ol>
<li>You could be saturating the sigmoid units (as MMN mentioned)  I would suggest trying relu units instead.</li>
</ol>

<p>replace:</p>

<pre><code>tf.nn.sigmoid(layer_n)
</code></pre>

<p>with:</p>

<pre><code>tf.nn.relu(layer_n)
</code></pre>

<ol start=""2"">
<li>Your model may not have the expressive power to actually learn your data. I.e. it would need to be deeper.</li>
<li>You can also try a different optimizer like Adam() as such</li>
</ol>

<p>replace:</p>

<pre><code>optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
</code></pre>

<p>with:</p>

<pre><code>optimizer = tf.train.AdamOptimizer().minimize(cost)
</code></pre>

<hr>

<p>A few other points:</p>

<ol start=""4"">
<li>You should add a bias term to your weights</li>
</ol>

<p>like so:</p>

<pre><code>biases = {
 'b1': tf.Variable(tf.random_normal([n_hidden_1],   dtype=np.float64)),       
 'b2': tf.Variable(tf.random_normal([n_hidden_2], dtype=np.float64)),
 'b3': tf.Variable(tf.random_normal([n_hidden_3],dtype=np.float64)),
 'bout': tf.Variable(tf.random_normal([1], dtype=np.float64))
 }

def multilayer_perceptron(x, weights):
    # First hidden layer with SIGMOID activation
    layer_1 = tf.matmul(x, weights['h1']) + biases['b1']
    layer_1 = tf.nn.sigmoid(layer_1)
    # Second hidden layer with SIGMOID activation
    layer_2 = tf.matmul(layer_1, weights['h2']) + biases['b2']
    layer_2 = tf.nn.sigmoid(layer_2)
    # Third hidden layer with SIGMOID activation
    layer_3 = tf.matmul(layer_2, weights['h3']) + biases['b3']
    layer_3 = tf.nn.sigmoid(layer_3)
    # Output layer with SIGMOID activation
    out_layer = tf.matmul(layer_2, weights['out']) + biases['bout']
    return out_layer
</code></pre>

<ol start=""5"">
<li>and you can update the learning rate over time</li>
</ol>

<p>like so:</p>

<pre><code>    learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE,
                                           global_step,
                                           decay_steps,
                                           LEARNING_RATE_DECAY_FACTOR,
                                           staircase=True)
</code></pre>

<p>You just need to define the decay steps i.e. when to decay and LEARNING_RATE_DECAY_FACTOR i.e. decay by how much.</p>
",2,4,2346,2016-10-02 14:23:09,https://stackoverflow.com/questions/39817949/setting-up-a-mlp-for-binary-classification-with-tensorflow
Vocabulary Processor function,"<p>I am researching about embedding input for Convolution Neural Network and I understand Word2vec. However, in <a href=""https://github.com/dennybritz/cnn-text-classification-tf/blob/master/train.py"" rel=""noreferrer"">CNN text classification</a>. dennybritz used function <code>learn.preprocessing.VocabularyProcessor</code>. In the <a href=""http://tflearn.org/data_utils/"" rel=""noreferrer"">document</a>. They said it Maps documents to sequences of word ids. I am not quite sure how this function work. Does it creates a list of Ids then maps the Ids with Words or It has an dictionary of words and their Ids, when run function it only give the ids ? </p>
","python, tensorflow, text-classification","<p>Lets say that you have just two documents <code>I like pizza</code> and <code>I like Pasta</code>. Your whole vocabulary consists of these words <code>(I, like, pizza, pasta)</code> For every word in the vocabulary, there is an index associated like so (1, 2, 3, 4). Now given a document like <code>I like pasta</code> it can be converted into a vector [1, 2, 4]. This is what the <code>learn.preprocessing.VocabularyProcessor</code> does. The parameter <code>max_document_length</code> makes sure that all the documents are represented by a vector of length <code>max_document_length</code> either by padding numbers if their length is shorter than <code>max_document_length</code> and clipping them if their length is greater than <code>max_document_length</code> Hope this helps you </p>
",20,8,7176,2016-10-03 05:24:53,https://stackoverflow.com/questions/39825043/vocabulary-processor-function
Text classification using e1071 (SVM),"<p>I have a dataframe having two columns. One Column contains text. Each row of that column one contains some type of data of three different classes(skill,qualification,experience) and other column is their respective class labels.</p>

<p>Snapshot of the dataframe:</p>

<p><img src=""https://i.sstatic.net/xZTJi.png"" alt=""snapshot of the dataframe""></p>

<p>How to apply svm from package e1071. How to Convert text data Column into some score. I thought of converting the textual column into document-term matrix. Is their any other way? How to make a d-t-matrix ?</p>
","r, svm, text-classification, multilabel-classification","<p>You can use <a href=""https://cran.r-project.org/web/packages/RTextTools/RTextTools.pdf"" rel=""noreferrer"">RTextTools</a> packages to create a document term matrix.
Use create_matrix function :</p>
<pre><code># Create the document term matrix. If column name is v1
dtMatrix &lt;- create_matrix(data[&quot;v1&quot;])
</code></pre>
<p>Then you can train your SVM model using this:</p>
<pre><code># Configure the training data
container &lt;- create_container(dtMatrix, data$label, trainSize=1:102, virgin=FALSE)
 
# train a SVM Model
model &lt;- train_model(container, &quot;SVM&quot;, kernel=&quot;linear&quot;, cost=1)
</code></pre>
<p>For information, <strong>RTextTools</strong> user <strong>e1071</strong> package internally to train the models.</p>
<p>For more details, please refer the RTextTools and e1071 documentation.</p>
",5,4,4738,2016-10-14 20:32:07,https://stackoverflow.com/questions/40051542/text-classification-using-e1071-svm
R: how to use random forests to predict binary outcome using string variables?,"<p>Consider the following dataframe</p>

<pre><code>outcome &lt;- c(1,0,0,1,1)
string &lt;- c('I love pasta','hello world', '1+1 = 2','pasta madness', 'pizza madness')

df = df=data.frame(outcome,string)


&gt; df
  outcome        string
1       1  I love pasta
2       0   hello world
3       0       1+1 = 2
4       1 pasta madness
5       1 pizza madness
</code></pre>

<p>Here I would like to use random forests to understand which words in the sentences contained in the <code>string</code> variable are <strong>strong predictors</strong> of the <code>outcome</code> variable.</p>

<p>Is there a (simple) way to do that in R?</p>
","r, machine-learning, classification, random-forest, text-classification","<p>What you want is the variable importance measures as produced by <code>randomForest</code>. This is obtained from the <code>importance</code> function. Here is some code that should get you started:</p>

<pre><code>outcome &lt;- c(1,0,0,1,1)
string &lt;- c('I love pasta','hello world', '1+1 = 2','pasta madness', 'pizza madness')
</code></pre>

<p><strong>Step 1:</strong> We want <code>outcome</code> to be a factor so that <code>randomForest</code> will do <strong>classification</strong> and <code>string</code> as character vectors.</p>

<pre><code>df &lt;- data.frame(outcome=factor(outcome,levels=c(0,1)),string, stringsAsFactors=FALSE)
</code></pre>

<p><strong>Step 2:</strong> Tokenize the <code>string</code> column into words. Here, I'm using <code>dplyr</code> and <code>tidyr</code> just for convenience. The key is to have just word tokens that you want as your predictor variable.</p>

<pre><code>library(dplyr)
library(tidyr)
inp &lt;- df %&gt;% mutate(string=strsplit(string,split="" "")) %&gt;% unnest(string)
##   outcome  string
##1        1       I
##2        1    love
##3        1   pasta
##4        0   hello
##5        0   world
##6        0     1+1
##7        0       =
##8        0       2
##9        1   pasta
##10       1 madness
##11       1   pizza
##12       1 madness
</code></pre>

<p><strong>Step 3:</strong> Construct a model matrix and feed it to <code>randomForest</code>:</p>

<pre><code>library(randomForest)
mm &lt;- model.matrix(outcome~string,inp)
rf &lt;- randomForest(mm, inp$outcome, importance=TRUE)
imp &lt;- importance(rf)
##                     0        1 MeanDecreaseAccuracy MeanDecreaseGini
##(Intercept)   0.000000 0.000000             0.000000        0.0000000
##string1+1     0.000000 0.000000             0.000000        0.3802400
##string2       0.000000 0.000000             0.000000        0.4514319
##stringhello   0.000000 0.000000             0.000000        0.4152465
##stringI       0.000000 0.000000             0.000000        0.2947108
##stringlove    0.000000 0.000000             0.000000        0.2944955
##stringmadness 4.811252 5.449195             5.610477        0.5733814
##stringpasta   4.759957 5.281133             5.368852        0.6651675
##stringpizza   0.000000 0.000000             0.000000        0.3025495
##stringworld   0.000000 0.000000             0.000000        0.4183821
</code></pre>

<p>As you can see, pasta and madness are key words to predict the <code>outcome</code>.</p>

<p><strong>Please Note:</strong> There are many parameters to <code>randomForest</code> that will be relevant for tackling the real-problem of scale. This is by no means a complete solution to your problem. It is only meant to illustrate the use of the <code>importance</code> function in answering your question. You may want to ask appropriate questions on <a href=""http://stats.stackexchange.com"">Cross Validated</a> concerning the details of using <code>randomForest</code>.</p>
",5,4,1710,2016-10-21 14:10:00,https://stackoverflow.com/questions/40178855/r-how-to-use-random-forests-to-predict-binary-outcome-using-string-variables
Use pos tagging in bag of words,"<p>I'm using the bag of words for text classification.
Results aren't good enough, test set accuracy is below 70%.</p>

<p>One of the things I'm considering is to use POS tagging to distinguish the function of words. How is the to go approach to doing it?</p>

<p>I'm thinking on append the tags to the words, for example the word ""love"", if it's used as a noun use:</p>

<pre><code>love_noun
</code></pre>

<p>and if it's a verb use:</p>

<pre><code>love_verb
</code></pre>
","machine-learning, text-classification","<p>Test set accuracy near 70% is not that bad if you have hundreds of categories. You might want to measure overall precision and recall instead of accuracy.</p>

<p>What you proposed sounds good, which is an approach to add feature conjunctions as additional features. Here are a few suggestions:</p>

<p><strong>Still keep your original features</strong>. That is to say, don't replace <code>love</code> with <code>love_noun</code> or <code>love_verb</code>. Instead, you have two features coming from <code>love</code>:</p>

<pre><code> love, love_noun (or)
 love, love_verb
</code></pre>

<p>If you need some sample code, you can start from <a href=""http://www.nltk.org/"" rel=""nofollow""><code>nltk</code></a> python package.</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag, word_tokenize
&gt;&gt;&gt; pos_tag(word_tokenize(""Love is a lovely thing""))
[('Love', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('lovely', 'JJ'), ('thing', 'NN')]
</code></pre>

<p><strong>Consider using n-grams</strong>, maybe starting from adding 2-grams. For example, you might have ""in"" and ""stock"" and you might just remove ""in"" because it is a stop-word. If you consider 2-grams, you will get a new feature:</p>

<pre><code>in-stock
</code></pre>

<p>which has a different meaning to ""stock"". It might help a lot in certain cases, for example, to distinguish from ""finance"" from ""shopping"". </p>
",1,0,1382,2016-10-25 20:04:51,https://stackoverflow.com/questions/40248764/use-pos-tagging-in-bag-of-words
Cheapest way to classify HTTP post objects,"<p>I can use SciPy to classify text on my machine, but I need to categorize string objects from HTTP POST requests at, or in near, real time. What algorithms should I research if my goals are high concurrency, near real-time output and small memory footprint? I figured I could get by with the Support Vector Machine (SVM) implementation in Go, but is that the best algorithm for my use case? </p>
","algorithm, go, machine-learning, svm, text-classification","<p>Yes, SVM (with a linear kernel) should be a good starting point. You can use <a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">scikit-learn</a> (it wraps <a href=""http://www.csie.ntu.edu.tw/~cjlin/liblinear/"" rel=""nofollow"">liblinear</a> I believe) to train your model. After the model is learned, the model is simply a list of <code>feature:weight</code> for each category you want to classifying into. Something like this (suppose you have only 3 classes):</p>

<pre><code>class1[feature1] = weight11
class1[feature2] = weight12
...
class1[featurek] = weight1k    ------- for class 1

... different &lt;feature, weight&gt; ------ for class 2
... different &lt;feature, weight&gt; ------ for class 3 , etc
</code></pre>

<p>At prediction time, you don't need scikit-learn at all, you can use whatever language you are using on the server backend to do a linear computation. Suppose a specific POST request contains features (feature3, feature5), what you need to do is like this:</p>

<pre><code>linear_score[class1] = 0
linear_score[class1] += lookup weight of feature3 in class1
linear_score[class1] += lookup weight of feature5 in class1

linear_score[class2] = 0
linear_score[class2] += lookup weight of feature3 in class2
linear_score[class2] += lookup weight of feature5 in class2

..... same thing for class3
pick class1, or class2 or class3 whichever has the highest linear_score
</code></pre>

<p><strong>One step further</strong>: If you could have some way to define the feature weight (e.g., using tf-idf score of tokens), then your prediction could become:</p>

<pre><code>linear_score[class1] += class1[feature3] x feature_weight[feature3]
so on and so forth.
</code></pre>

<p>Note <code>feature_weight[feature k]</code> is usually different for each request.
Since for each request, the total number of active features must be much smaller than the total number of considered features (consider 50 tokens or features vs your entire vocabulary of 1 MM tokens), the prediction should be very fast. I can imagine once your model is ready, an implementation of the prediction could be just written based on a <strong>key-value store</strong> (e.g., <a href=""http://redis.io/"" rel=""nofollow"">redis</a>).</p>
",1,0,73,2016-10-27 02:56:32,https://stackoverflow.com/questions/40275368/cheapest-way-to-classify-http-post-objects
"Stanford NLP Text Classifier, Custom Features and Confusion Matrix","<p>I using Stanford NLP Text Classifier (ColumnDataClassifier) from my Java code. I have two main questions.</p>

<p>1-) How do I print more detailed evaluation information such as a confusion matrix.</p>

<p>2-) My code already, does the pre-processing and extracts numeric features (vectors) for terms, such as binary features or TF-IDF values. How can I use those features to train and test the classifier.</p>
","stanford-nlp, text-classification","<ol>
<li><p>I asked a related question in <a href=""https://stackoverflow.com/questions/36361348/stanford-classifier-cross-validation-averaged-or-aggregate-metrics"">here</a>. <code>ColumnDataClassifier</code> does not have an option to output the metrics in a confusion matrix. However, if you look at the code in at <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/classify/ColumnDataClassifier.java"" rel=""nofollow noreferrer"">ColumnDataClassifier.java</a> you can see where the TP, FP, TN, FN are output to the stdin. This place has the raw values that you need. It could be used for a method that aggregates these into a confusion matrix and outputs it after the run, but you would have to write this code yourself.</p></li>
<li><p>The <a href=""http://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">wiki</a> has an example of how to use numerical features with the <code>ColumnDataClassifier</code>. If you use numerical features, take a look at these options from the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html"" rel=""nofollow noreferrer"">API</a> that allow you do apply some transformations:</p>

<pre><code>realValued  boolean false   Treat this column as real-valued and do not perform any transforms on the feature value.    Value
logTransform    boolean false   Treat this column as real-valued and use the log of the value as the feature value. Log
logitTransform  boolean false   Treat this column as real-valued and use the logit of the value as the feature value.   Logit
sqrtTransform   boolean false   Treat this column as real-valued and use the square root of the value as the feature value. Sqrt
</code></pre></li>
</ol>
",1,1,534,2016-11-02 21:03:28,https://stackoverflow.com/questions/40389751/stanford-nlp-text-classifier-custom-features-and-confusion-matrix
text classificacion: how many dimensions does my data have?,"<p>I am classifying text using the bag of words model. I read in 800 text files, each containing a sentence.</p>

<p>The sentences are then represented like this:</p>

<pre><code>[{""OneWord"":True,""AnotherWord"":True,""AndSoOn"":True},{""FirstWordNewSentence"":True,""AnSoOn"":True},...]
</code></pre>

<p>How many dimensions does my data have?</p>

<p>Is it the number of entries in the largest vector? Or is it the number of unique words? Or something else?</p>
",text-classification,"<p>For each doc, the bag of words model has a set of <strong>sparse</strong> features. For example (use your first sentence in your example):</p>

<pre><code>OneWord
AnotherWord
AndSoOn
</code></pre>

<p>The above three are the three <code>active</code> features for the document. It is sparse because we never list those <code>inactive</code> features explicitly AND we have a very large vocabulary (all possible unique words that you consider as features). In another words, we did not say:</p>

<pre><code>OneWord
AnotherWord
AndSoOn
FirstWordNewSentence: false
</code></pre>

<p>We only include those words that are ""true"". </p>

<blockquote>
  <blockquote>
    <p>How many dimensions does my data have?
    Is it the number of entries in the largest vector? Or is it the number of unique words? Or something else?</p>
  </blockquote>
</blockquote>

<p>If you stick with the <strong>sparse</strong> feature representation, you might want to estimate the average number of active features per document instead. That number is 2.5 in your example ((3+2)/2 = 2.5).</p>

<p>If you use a <strong>dense representation</strong> (e.g., <a href=""https://stackoverflow.com/questions/17469835/one-hot-encoding-for-machine-learning"">one-hot encoding</a>, it is not a good idea though if the vocabulary is large), the input dimension is equal to your vocabulary size.</p>

<p>If you use a <strong>word embedding</strong> that has 100-dimension and combine all words' embedding to form a new input vector to represent a document, your input dimension is 100 then. In this case, you convert your sparse features into <strong>dense</strong> features via the <strong>embedding</strong>. </p>
",1,0,142,2016-11-07 11:25:50,https://stackoverflow.com/questions/40464043/text-classificacion-how-many-dimensions-does-my-data-have
spark ml 2.0 - Naive Bayes - how to determine threshold values for each class,"<p>I am using NB for document classification and trying to understand threshold parameter to see how it can help to optimize algorithm.</p>

<p>Spark ML 2.0 <code>thresholds</code> doc says:</p>

<p><code>Param for Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt;= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold.</code></p>

<p>0) Can someone explain this better? What goal it can achieve? My general idea is if you have threshold 0.7 then at least one class prediction probability should be more then 0.7 if not then prediction should return empty. Means classify it as 'uncertain' or just leave empty for prediction column. How can p/t function going to achieve that when you still pick the category with max probability?</p>

<p>1) What probability it adjust? default column 'probability' is actually conditional probability and 'rawPrediction' is
confidence according to document. I believe threshold will adjust 'rawPrediction' not 'probability' column. Am I right? </p>

<p>2) Here's how some of my probability and rawPrediction vector look like. How do I set threshold values based on this so I can remove certain uncertain classification? <code>probability</code> is between 0 and 1 but <code>rawPrediction</code> seems to be on log scale here. </p>

<p>Probability:
<code>[2.233368649314982E-15,1.6429456680945863E-9,1.4377313514127723E-15,7.858651849363202E-15]</code></p>

<p>rawPrediction:
<code>[-496.9606736723107,-483.452183395287,-497.40111830218746]</code></p>

<p>Basically I want classifier to leave Prediction column empty if it doesn't have any probability that is more then 0.7 percent.</p>

<p>Also, how to classify something as uncertain when more then one category has very close scores e.g. 0.812, 0.800, 0.799 . Picking max is something I may not want here but instead classify as ""uncertain"" or leave empty and I can do further analysis and treatment for those documents or train another model for those docs.</p>
","apache-spark, machine-learning, text-classification, threshold, naivebayes","<p>I haven't played with it, but the intent is to supply <em>different</em> threshold values for each class. I've extracted this example from the docstring:</p>

<pre><code>model = nb.fit(df)
&gt;&gt;&gt; result.prediction
1.0
&gt;&gt;&gt; result.probability
DenseVector([0.42..., 0.57...])
&gt;&gt;&gt; result.rawPrediction
DenseVector([-1.60..., -1.32...])
&gt;&gt;&gt; nb = nb.setThresholds([0.01, 10.00])
&gt;&gt;&gt; model3 = nb.fit(df)
&gt;&gt;&gt; result = model3.transform(test0).head()
&gt;&gt;&gt; result.prediction
0.0
</code></pre>

<p>If I understand correctly, the effect was to transform [0.42, 0.58] into [<sup>.42</sup>/<sub>.01</sub>, <sup>.58</sup>/<sub>10</sub>] = [42, 5.8], switching the prediction (""largest p/t"") from column 1 (third row above) to column 0 (last row above).  However, I couldn't find the logic in the source. Anyone?</p>

<p>Stepping back: I do not see a built-in way to do what you want: be agnostic if no class dominates.  You will have to add that with something like:</p>

<pre class=""lang-py prettyprint-override""><code>def weak(probs, threshold=.7, epsilon=.01):
    return np.all(probs &lt; threshold) or np.max(np.diff(probs)) &lt; epsilon

&gt;&gt;&gt; cases = [[.5,.5],[.5,.7],[.7,.705],[.6,.1]]
&gt;&gt;&gt; for case in cases:
...    print '{:15s} - {}'.format(case, weak(case))

[0.5, 0.5]      - True
[0.5, 0.7]      - False
[0.7, 0.705]    - True
[0.6, 0.1]      - True
</code></pre>

<p>(Notice I haven't checked whether <code>probs</code> is a legal probability distribution.)  </p>

<p>Alternatively, if you are not actually making a hard decision, use the predicted probabilities and a metric like Brier score, log loss, or info gain that accounts for the calibration as well as the accuracy. </p>
",2,2,1822,2016-11-07 23:54:41,https://stackoverflow.com/questions/40476738/spark-ml-2-0-naive-bayes-how-to-determine-threshold-values-for-each-class
RNN for binary classification of sequence,"<p>I wondering if someone can suggest a good library or reference (tutorial or article) to implement a Recurrent Neural Network (RNN).</p>

<p>I tried to use the <a href=""https://sourceforge.net/p/rnnl/wiki/Home/"" rel=""noreferrer"">rnnlib</a> by Alex Graves, but I had some troubles in changing the architecture to adapt the network to my needs. </p>

<p>In particular, I'm trying to recognize strings coming from a regular language, using positive and negative samples. </p>

<p>This is an example of my training set:</p>

<p>str1: w1 w2 ... wn --> label 1 (it is a valid string for the language)</p>

<p>str2: w1 w2 ... wi --> label 0 (it does not belong to the language)</p>

<p>where wi are drawn from the alphabet of the unknown language.</p>

<p>Thanks in advance.</p>
","deep-learning, regular-language, text-classification, recurrent-neural-network","<p>For tools, I would highly recommend <a href=""https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html"" rel=""nofollow noreferrer"">TensforFlow</a>. </p>

<ul>
<li>Great intro to <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noreferrer"">RNN</a></li>
<li><a href=""http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/"" rel=""nofollow noreferrer"">RNN + TensorFlow for noobs</a></li>
<li><a href=""https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/"" rel=""nofollow noreferrer"">RNN + classification</a> : take a look at the <strong>sequence classification</strong> in this article, which is the case of yours.  </li>
</ul>
",2,5,8658,2016-11-09 18:11:50,https://stackoverflow.com/questions/40513413/rnn-for-binary-classification-of-sequence
GridSearchCV: How to specify test set?,"<p>I have a question regarding <code>GridSearchCV</code>:</p>

<p>by using this:</p>

<pre><code>gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1, cv=6, scoring=""f1"")
</code></pre>

<p>I specify that k-fold cross-validation should be used with 6 folds right?</p>

<p>So that means that my corpus is split into training set and tet set 6 times.</p>

<p>Doesn't that mean that for the <code>GridSearchCV</code> I need to use my entire corpus, like so:</p>

<pre><code>gs_clf = gs_clf.fit(corpus.data, corpus.target)
</code></pre>

<p>And if so, how would I then get my trainig set from there used for the predict method?</p>

<pre><code>predictions = gs_clf.predict(??)
</code></pre>

<p>I have seen code where the corpus is split into test set and training set using <code>train_test_split</code> and then <code>X_train</code> and <code>Y_train</code> are passed to <code>gs_clf.fit</code>.</p>

<p>But that doesn't make sense to me: If I split it the corpus beforehand, why use cross validation again in the <code>GridSearchCV</code>?</p>

<p>Thanks for some clarification!!</p>
","python, scikit-learn, cross-validation, text-classification","<ol>
<li><code>GridSearchCV</code> is not designed for measuring the performance of your model but to optimize the hyper-parameter of classifier while training. And when you write <code>gs_clf.fit</code> you are actually trying different models on your entire data (but different folds) in the pursuit of the best hyper-parameter. For example, if you have n different <code>c</code>'s and m different <code>gamma</code>'s for an SVM model, then you have n X m models and you are searching (grid-search) through them to see which one works best on your data.</li>
<li>When you found the best model using <code>gs_clf.best_params_</code>, then you can use your test data to get the actual performance (e.g., accuracy, precision, ...) of your model.</li>
<li>Of course, only then it is time for testing the model. Your test data must not have any overlap with the data you trained your model against. For instance, you should have something like <code>corpus.train</code> and <code>corpus.test</code>, and you should reserve <code>corpus.test</code> only for the last round when you are done with training and you only want to test the final model.</li>
</ol>

<p>As we all know, any use of test data in the process of training the model (where training data should be used) or tuning the hyper-parameters (where the validation data should be used) is considered cheating and results in unrealistic performance.</p>
",18,8,12465,2016-11-11 10:37:57,https://stackoverflow.com/questions/40546178/gridsearchcv-how-to-specify-test-set
Classifying text documents using nltk,"<p>I'm currently working on a project where I'm taking emails, stripping out the message bodies using the email package, then I want to categorize them using labels like sports, politics, technology, etc...</p>

<p>I've successfully stripped the message bodies out of my emails, now I'm looking to start classifying. I've done the classic example of sentiment-analysis classification using the move_reviews corpus separating documents into positive and negative reviews.</p>

<p>I'm just wondering how I could apply this approach to my project? Can I create multiple classes like sports, technology, politics, entertainment, etc.? I have hit a road block here and am looking for a push in the right direction.</p>

<p>If this isn't an appropriate question for SO I'll happily delete it.</p>

<p><strong>Edit</strong>: Hello everyone, I see that this post has gained a bit of popularity, I did end up successfully completing this project, here is a link to the code in the projects GitHub Repo:
<a href=""https://github.com/codyreandeau/Email-Categorizer/blob/master/Email_Categorizer.py"" rel=""nofollow noreferrer"">https://github.com/codyreandeau/Email-Categorizer/blob/master/Email_Categorizer.py</a></p>
","python, machine-learning, nltk, text-classification, document-classification","<p>To create a classifier, you need a training data set with the classes you are looking for. In your case, you may need to either:</p>

<ol>
<li>create your own data set</li>
<li>use a pre-existing dataset</li>
</ol>

<p>The <a href=""https://en.wikipedia.org/wiki/Brown_Corpus"" rel=""nofollow noreferrer"">brown corpus</a> is a seminal text with many of the categories you are speaking about. This could be a starting point to help classify your emails using some package like <code>gensim</code> to find semantically similar texts.</p>

<p>Once you classify your emails, you can then train a system to predict a label for each unseen email. </p>
",0,2,5806,2016-11-27 05:48:25,https://stackoverflow.com/questions/40826144/classifying-text-documents-using-nltk
StanfordCoreNLP object creation error,"<p>I am facing this issue:</p>

<pre><code>Exception in thread ""main"" java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)

Caused by: java.io.InvalidClassException: edu.stanford.nlp.tagger.maxent.ExtractorDistsim; local class incompatible: stream classdesc serialVersionUID = 1, local class serialVersionUID = 2
at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.readExtractors(MaxentTagger.java:622)
at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:868)
... 23 more
</code></pre>

<p>at code line</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>Note : I have put the stanford core nlp latest jar file but it didn't work and even tried explicitly adding stanford nlp pos tagger jar file but it didn't work and even tried adding the models jar file but didn't work.</p>

<p>Please help.</p>
","machine-learning, nlp, text-classification, stanford-nlp","<p>Whoever encounters this problem i would suggest them to visit <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP</a> and download the LATEST model files from there it will mostly solve the issue.</p>
",0,0,638,2016-12-05 20:21:55,https://stackoverflow.com/questions/40982653/stanfordcorenlp-object-creation-error
"Want to store variable names in list, not said variable&#39;s contents","<p>Sorry if the title is confusing; let me explain.</p>

<p>So, I've written a program that categorizes emails by topic using nltk and tools from sklearn. </p>

<p>Here is that code:</p>

<pre><code>#Extract Emails
tech = extract_message(""C:\\Users\\Cody\\Documents\\Emails\\tech.html"")
gary = extract_message(""C:\\Users\\Cody\\Documents\\Emails\\gary.html"")
gary2 = extract_message(""C:\\Users\\Cody\\Documents\\Emails\\gary2.html"")
jesus = extract_message(""C:\\Users\\Cody\\Documents\\Emails\\Jesus.html"")
jesus2 = extract_message(""C:\\Users\\Cody\\Documents\\Emails\\jesus2.html"")
hockey = extract_message(""C:\\Users\\Cody\\Documents\\Emails\\hockey.html"")
hockey2 = extract_message(""C:\\Users\\Cody\\Documents\\Emails\\hockey2.html"")
shop = extract_message(""C:\\Users\\Cody\\Documents\\Emails\\shop.html"")

#Build dictionary of features
count_vect = CountVectorizer()
x_train_counts = count_vect.fit_transform(news.data)

#Downscaling
tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)
tf_transformer = TfidfTransformer(use_idf=False).fit(x_train_counts)
x_train_tf = tf_transformer.transform(x_train_counts)

#Train classifier
clf = MultinomialNB().fit(x_train_tfidf, news.target)

#List of the extracted emails
docs_new = [gary, gary2, jesus, jesus2, shop, tech, hockey, hockey2]

#Extract feautures from emails
x_new_counts = count_vect.transform(docs_new)
x_new_tfidf = tfidf_transformer.transform(x_new_counts)

#Predict the categories for each email
predicted = clf.predict(x_new_tfidf)
</code></pre>

<p>Now I'm looking to store each variable in an appropriate list, based off of the predicted label. I figured I could do that doing this:</p>

<pre><code>#Store Files in a category
hockey_emails = []
computer_emails = []
politics_emails = []
tech_emails = []
religion_emails = []
forsale_emails = []

#Print out results and store each email in the appropritate category list
for doc, category in zip(docs_new, predicted):
  print('%r ---&gt; %s' % (doc, news.target_names[category]))
   if(news.target_names[category] == 'comp.sys.ibm.pc.hardware'):
        computer_emails.append(doc)
   if(news.target_names[category] == 'rec.sport.hockey'):
        hockey_emails.append(doc)
   if(news.target_names[category] == 'talk.politics.misc'):
       politics_emails.append(doc)
   if(news.target_names[category] == 'soc.religion.christian'):
       religion_emails.append(doc)
   if(news.target_names[category] == 'misc.forsale'):
       forsale_emails.append(doc)
   if(news.target_names[category] == 'comp.sys.ibm.pc.hardware'):
       computer_emails.append(doc)
</code></pre>

<p>My output if I were to print out one of these lists, let's say hockey for instance, displays the contents stored in the variable rather than the variable itself.</p>

<p>I want this:</p>

<pre><code>print(hockey_emails)

output: ['hockey', 'hockey2']
</code></pre>

<p>but instead I'm getting this:</p>

<pre><code> output: ['View View online click here Hi Thanks for signing up as a EA SPORTS NHL insider You ll now receive all of the latest and greatest news and info at this e mail address as you ve requested EA com If you need technical assistance please contact EA Help Privacy Policy Our Certified Online Privacy Policy gives you confidence whenever you play EA games To view our complete Privacy and Cookie Policy go to privacy ea com or write to Privacy Policy Administrator Electronic Arts Inc Redwood Shores Parkway Redwood City CA Electronic Arts Inc All Rights Reserved Privacy Policy User Agreement Legal ActionsMark as UnreadMark as ReadMark as SpamStarClear StarArchive Previous Next ', 'View News From The Hockey Writers The Editor s Choice stories from The Hockey Writers View this email in your browser edition Recap Stars Steamroll Predators By Matt Pryor on Dec am As the old Mary Chapin Carpenter song goes Sometimes you re the windshield Sometimes you re the bug It hasn t happened very often this season but the Dallas Stars had a windshield Continue Reading A Review of Years in Blue and White Damien Cox One on One By Anthony Fusco on Dec pm The Toronto Maple Leafs are one of the most storied and iconic franchises in the entire National Hockey League They have a century of history that spans all the way back to the early s When you have an Continue Reading Bruins Will Not Miss Beleskey By Kyle Benson on Dec am On Monday it was announced that Matt Beleskey will miss the next six weeks due to a knee injury he sustained over the weekend in a game against the Buffalo Sabres Six weeks is a long stint to be without a potential top Continue Reading Recent Articles Galchenyuk Injury Costly for CanadiensFacing Off Picking Team Canada for World JuniorsAre Johnson s Nomadic Days Over Share Tweet Forward Latest News Prospects Anaheim Ducks Arizona Coyotes Boston Bruins Buffalo Sabres Calgary Flames Carolina Hurricanes Chicago Blackhawks Colorado Avalanche Columbus Blue Jackets Dallas Stars Detroit Red Wings Edmonton Oilers Florida Panthers Los Angeles Kings Minnesota Wild Montreal Canadiens Nashville Predators New Jersey Devils New York Islanders New York Rangers Philadelphia Flyers Pittsburgh Penguins Ottawa Senators San Jose Sharks St Louis Blues Tampa Bay Lightning Toronto Maple Leafs Vancouver Canucks Washington Capitals Winnipeg Jets Copyright The Hockey Writers All rights reserved You are receiving this email because you opted in at The Hockey Writers or one of our Network Sites Our mailing address is The Hockey Writers Victoria Ave St Lambert QC J R R CanadaAdd us to your address book unsubscribe from this list update subscription preferences ActionsMark as UnreadMark as ReadMark as SpamStarClear StarArchive Previous Next ']
</code></pre>

<p>I figured this would be simple, but I'm sitting here scratching my head. Is this even possible? Should I use something else instead of a list? This is probably simple I'm just blanking.</p>
","python, machine-learning, scikit-learn, nltk, text-classification","<p>You have to keep track of the names yourself, Python won't do it for you.</p>

<pre><code>names = 'gary gary2 Jesus jesus2 shop tech hockey hockey2'.split()
docs_new = [extract_message(""C:\\Users\\Cody\\Documents\\Emails\\%s.html"" % name)
            for name in names]

for name, category in zip(names, predicted):
    print('%r ---&gt; %s' % (name, news.target_names[category]))
    if (news.target_names[category] == 'comp.sys.ibm.pc.hardware'):
        computer_emails.append(name)
</code></pre>
",1,1,1122,2016-12-11 07:47:07,https://stackoverflow.com/questions/41084045/want-to-store-variable-names-in-list-not-said-variables-contents
What is the difference between gensim LabeledSentence and TaggedDocument,"<p>Please help me in understanding the difference between how <code>TaggedDocument</code> and <code>LabeledSentence</code> of <code>gensim</code> works. My ultimate goal is Text Classification using <code>Doc2Vec</code> model and any classifier. I am following this <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""noreferrer"">blog</a>!</p>

<pre><code>class MyLabeledSentences(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(LabeledSentence([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList


class MyTaggedDocument(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(TaggedDocument([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList

sentences = MyLabeledSentences(some_dir_name)
model_l = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5,     workers=7)
sentences_l = sentences.ToArray()
model_l.build_vocab(sentences_l )
for epoch in range(15): # 
    random.shuffle(sentences_l )
    model.train(sentences_l )
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha 

sentences = MyTaggedDocument(some_dir_name)
model_t = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=7)
sentences_t = sentences.ToArray()
model_l.build_vocab(sentences_t)
for epoch in range(15): # 
    random.shuffle(sentences_t)
    model.train(sentences_t)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha
</code></pre>

<p>My question is <code>model_l.docvecs['some_word']</code> is same as <code>model_t.docvecs['some_word']</code>?
Can you provide me weblink of good sources to get a grasp on how <code>TaggedDocument</code> or <code>LabeledSentence</code> works.</p>
","gensim, text-classification, word2vec, doc2vec","<p><code>LabeledSentence</code> is an older, deprecated name for the same simple object-type to encapsulate a text-example that is now called <code>TaggedDocument</code>. Any objects that have <code>words</code> and <code>tags</code> properties, each a list, will do. (<code>words</code> is always a list of strings; <code>tags</code> can be a mix of integers and strings, but in the common and most-efficient case, is just a list with a single id integer, starting at 0.)</p>

<p><code>model_l</code> and <code>model_t</code> will serve the same purposes, having trained on the same data with the same parameters, using just different names for the objects. But the vectors they'll return for individual word-tokens (<code>model['some_word']</code>) or document-tags (<code>model.docvecs['somefilename_NN']</code>) will likely be different – there's randomness in Word2Vec/Doc2Vec initialization and training-sampling, and introduced by ordering-jitter from multithreaded training.  </p>
",7,8,4641,2016-12-16 10:33:16,https://stackoverflow.com/questions/41182372/what-is-the-difference-between-gensim-labeledsentence-and-taggeddocument
Text classification algorithms which are not Naive?,"<p>Naive Bayes Algorithm assumes independence among features. What are some text classification algorithms which are not <strong>Naive</strong> i.e. do not assume independence among it's features.</p>
","machine-learning, text-classification, data-science","<p>The answer will be very straight forward, since nearly <strong>every</strong> classifier (besides <strong>Naive</strong> Bayes) is not naive. Features independence is very rare assumption, and is not taken by (among huge list of others):</p>

<ul>
<li>logistic regression (in NLP community known as maximum entropy model)</li>
<li>linear discriminant analysis (fischer linear discriminant)</li>
<li>kNN</li>
<li>support vector machines</li>
<li>decision trees / random forests</li>
<li>neural nets</li>
<li>...</li>
</ul>

<p>You are asking about text classification, but there is nothing really special about text, and you can use any existing classifier for such data.</p>
",0,-1,93,2016-12-20 13:30:06,https://stackoverflow.com/questions/41243531/text-classification-algorithms-which-are-not-naive
Memory leak evaluating CNN model for text clasification,"<p>I've been doing some adaptation to code in this blog about CNN for text clasification: 
<a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a></p>

<p>Everything works fine! But when I try to use the model trained to predict new instances it consumes all memory available. It seems that it's not liberating any memory when evaluates and load all the model again and again. As far as I know memory should be liberated after every sess.run command.</p>

<p>Here is the part of the code I'm working with:</p>

<pre><code>with graph.as_default():

session_conf = tf.ConfigProto(
  allow_soft_placement=FLAGS.allow_soft_placement,
  log_device_placement=FLAGS.log_device_placement)
sess = tf.Session(config=session_conf)
with sess.as_default():

    # Load the saved meta graph and restore variables
    saver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))
    saver.restore(sess, checkpoint_file)

    # Get the placeholders from the graph by name
    input_x = graph.get_operation_by_name(""input_x"").outputs[0]
    # input_y = graph.get_operation_by_name(""input_y"").outputs[0]
    dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]

    # Tensors we want to evaluate
    predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]

    # Add a vector for probas
    probas =graph.get_operation_by_name(""output/scores"").outputs[0]

    # Generate batches for one epoch
    print(""\nGenerating Bathces...\n"")
    gc.collect()
    #mem0 = proc.get_memory_info().rss
    batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)
    #mem1 = proc.get_memory_info().rss

    print(""\nBatches done...\n"")
    #pd = lambda x2, x1: 100.0 * (x2 - x1) / mem0
    #print ""Allocation: %0.2f%%"" % pd(mem1, mem0)
    # Collect the predictions here
    all_predictions = []

    all_probas = []

    for x_test_batch in batches:
        #Calculate probability of prediction been good
        gc.collect()
        batch_probas = sess.run(tf.reduce_max(tf.nn.softmax(probas),1), {input_x: x_test_batch, dropout_keep_prob: 1.0})
        batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})
        all_predictions = np.concatenate([all_predictions, batch_predictions])
        all_probas = np.concatenate([all_probas, batch_probas])
        # Add summary ops to collect data
        with tf.name_scope(""eval"") as scope:
            p_h = tf.histogram_summary(""eval/probas"", batch_probas)
            summary= sess.run(p_h)
            eval_summary_writer.add_summary(summary)
</code></pre>

<p>Any help will be much appreciated</p>

<p>Cheers</p>
","memory-leaks, tensorflow, deep-learning, text-classification","<p>Your training loop creates new TensorFlow operations (<code>tf.reduce_max()</code>, <code>tf.nn.softmax()</code> and <code>tf.histogram_summary()</code>) in each iteration, which will lead to more memory being consumed over time. TensorFlow is most efficient when you run the same graph many times, because it can amortize the cost of optimizing the graph over multiple executions. Therefore,
to get the best performance, you should revise your program so that you create each of these operations <strong>once</strong>, before the <code>for x_test_batch in batches:</code> loop, and then re-use the same operations in each iteration.</p>
",2,1,474,2016-12-21 04:20:06,https://stackoverflow.com/questions/41255031/memory-leak-evaluating-cnn-model-for-text-clasification
Cross Validation classification error,"<p>I am using the following code to get the classification results: </p>

<pre><code> folds = 5 #number of folds for the cv

        #Logistic Regression--
        clf = linear_model.LogisticRegression(penalty='l1')
        kf = KFold

(len(clas), n_folds=folds)
    fold = 1
    cms = np.array([[0,0],[0,0]])
    accs = []
    aucs=[]
    for train_index, test_index in kf:
        X_train, X_test = docs[train_index], docs[test_index]
        y_train, y_test = clas2[train_index], clas2[test_index]
        clf.fit(X_train, y_train)
        prediction = clf.predict(X_test)
        acc = accuracy_score(prediction, y_test)
        cm = confusion_matrix(y_test,prediction)
        pred_probas = clf.predict_proba(X_test)[:,1]
        fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_probas)
        print('Test Accuracy for fold {}: {}\n{}'.format(fold,round((acc*100),2),cm))
        roc_auc = auc(fpr,tpr)
        print('AUC for fold {} : {}'.format(fold,round((roc_auc*100),2)))
        fold +=1
        cms += cm
        accs.append(acc)
        aucs.append(roc_auc)
    print('CV test accuracy: {}\n{}'.format(round((np.mean(accs)*100),2),cms))
    print('\nCV AUC: {}'.format(round(np.mean(aucs)*100),2))
    print('\nCV accuracy: %.3f +/- %.3f' % (round((np.mean(accs)*100),2),round((np.std(accs)*100),2)))
    print('\nCV ROC AUC: %.3f +/- %.3f' % (round((np.mean(aucs)*100),2),round((np.std(aucs)*100),2)))
    print('\nPeak accuracy: '+str(round((np.amax(accs)*100),2)))
    print('\nPeak ROC AUC: '+str(round((np.amax(aucs)*100),2)))
</code></pre>

<p>I am not sure if I am doing something wring but I have 2 classes Yes= 406
No= 139, and the code is giving me following result</p>

<pre><code>Test Accuracy for fold 1: 87.16
[[94  9]
 [ 5  1]]
AUC for fold 1 : 66.1
Test Accuracy for fold 2: 92.66
[[100   6]
 [  2   1]]
AUC for fold 2 : 62.42
Test Accuracy for fold 3: 90.83
[[99  7]
 [ 3  0]]
AUC for fold 3 : 43.08
Test Accuracy for fold 4: 88.07
[[83  8]
 [ 5 13]]
AUC for fold 4 : 85.5
Test Accuracy for fold 5: 53.21
[[ 0  0]
 [51 58]]
AUC for fold 5 : nan
CV test accuracy: 82.39
[[376  30]
 [ 66  73]]

CV AUC: nan

CV accuracy: 82.390 +/- 14.720

CV ROC AUC: nan +/- nan

Peak accuracy: 92.66

Peak ROC AUC: nan
C:\Users\kkothari\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\metrics\ranking.py:530: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless
  UndefinedMetricWarning)
C:\Users\kkothari\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\metrics\ranking.py:95: RuntimeWarning: invalid value encountered in less
  if np.any(dx &lt; 0):
</code></pre>

<p>Initially I just had 17 No docs but it was working fine.. Can someone point out some mistake or explain what is going on?</p>
","python, machine-learning, scikit-learn, cross-validation, text-classification","<p>Basically you have one very small class (something around 20-30 samples?) and in one of the splits you did not get any, thus leading to errors. You can use StratifiedKFold instead, which guarantees that in each split you have a constant amount of samples from each class.</p>
",2,0,1165,2016-12-21 19:23:11,https://stackoverflow.com/questions/41270436/cross-validation-classification-error
How to use bag of words or tf-idf to classify text,"<p>I have a general question regarding classifying using bag of words or similar methods.</p>

<p>I have text that I am trying to classify.The classes are known to me and I know that each sentence of the text belongs to one type of sentences.For example sentence 1 should be an order,Sentence 2 should be news, etc.</p>

<p>So what I was thinking is to use n-gram generation for feature extraction and my idea is that n-grams of words can be helpful for the machine to find the right category.But implementing the idea using Python is not easy for me.I can not connect concepts with impelementation. For example I am not sure if I have to supply all possible chunks of POS tags that can belong to each category or the machine can find them.Also, I feel that n-grams can be helpful in this kind of analysis.But I don't know how.</p>

<p>It would be great if can give me some ideas or tell me the steps I should take to do this kind of classification.</p>

<p>Best</p>
","python, machine-learning, nlp, text-classification","<p>To use ngrams in this type of analysis, you can extract all the ngrams that appear in the text. Then, you can calculate TF-IDF for each ngram in each sentence in the following way:</p>

<ul>
<li>TF: represents the number of times an ngram appears in the sentence. </li>
<li>IDF: represents the proportion of sentences that include that ngram. </li>
</ul>

<p>This will give you a TF-IDF metric that measures the 'value of each ngram to each sentence given all sentences'. Once you have the TF-IDF metrics, you can feed your sentences in a standard supervised method. </p>

<p>For each class, you can also build language models based on you ngrams, POS tags, and even dependency parsed sentences. Then, given a new sentence you can calculate the likelihood that the sentence can be generated from each of the language models. Then again, you can take advantage of these probability values in a supervised learning method.</p>

<p>I suggest you check out the following articles: </p>

<p>1 - Look at Section 5.1 here for the use of <a href=""https://pdfs.semanticscholar.org/9f3f/6f65344da1bd61f1311ab134c1a7bfcd0741.pdf"" rel=""nofollow noreferrer"">TF-IDF</a> </p>

<p>2- This document provides an example for the use of <a href=""https://www.aclweb.org/anthology/D/D10/D10-1121.pdf"" rel=""nofollow noreferrer"">language models</a> </p>

<p>Good luck ;)</p>
",3,2,1311,2016-12-22 12:19:17,https://stackoverflow.com/questions/41283047/how-to-use-bag-of-words-or-tf-idf-to-classify-text
Scikit learn-Classification,"<p>Is there a straightforward way to view the top features of each class? Based on tfidf?</p>

<p>I am using KNeighbors classifer, SVC-Linear, MultinomialNB.</p>

<p>Secondly, I have been searching for a way to view documents that have not been classified correctly? I can view the confusion matrix but I would like to see specific documents to see what features are causing the misclassification.</p>

<pre><code>classifier = SVC(kernel='linear')
counts = tfidf_vectorizer.fit_transform(data['text'].values).toarray()
targets = data['class'].values
classifier.fit(counts, targets)
counts = tfidf_vectorizer.fit_transform(test['text'].values).toarray()  
predictions = classifier.predict(counts)
</code></pre>

<p>EDIT: I have added the code snippet where I am only creating a tfidf vectorizer and using it to traing the classifier. </p>
","python, scikit-learn, text-classification","<p>Like the previous comments suggest, a more specific question would result in a better answer, but I use this package all the time so I will try and help.</p>

<p>I. Determining top features for classification classes in sklearn really depends on the individual tool you are using. For example, many ensemble methods (like <code>RandomForestClassifier</code> and <code>GradientBoostingClassifer</code>) come with the <code>.feature_importances_</code> attribute which will score each feature based on its importance. In contrast, most linear models (like <code>LogisticRegression</code> or <code>RidgeClassifier</code>) have a regularization penalty which penalizes for the size of coefficients, meaning that the coefficient sizes are somewhat a reflection of feature importance (although you need to keep in mind the numeric scales of individual features) which can be accessed using the <code>.coef_</code> attribute of the model class.</p>

<p>In summary, almost all sklearn models have some method to extract the feature importances but the methods are different from model to model. Luckily the sklearn documentation is FANTASTIC so I would read up on your specific model to determine your best approach. Also, make sure to read the <a href=""http://scikit-learn.org/stable/user_guide.html"" rel=""nofollow noreferrer"">User Guide</a> associated with your problem type in addition to the model specific API.</p>

<p>II. There is no out of the box sklearn method to provide the mis-classified records but if you are using a pandas DataFrame (which you should) to feed the model it can be accomplished in a few lines of code like this.</p>

<pre><code>import pandas as pd
from sklearn.linear_model import RandomForestClassifier

df = pd.DataFrame(data)
x = df[[&lt;list of feature columns&gt;]]
y = df[&lt;target column&gt;]

mod = RandomForestClassifier()
mod.fit(x.values, y.values)

df['predict'] = mod.predict(x.values)

incorrect = df[df['predict']!=df[&lt;target column&gt;]]
</code></pre>

<p>The resultant <code>incorrect</code> DataFrame will contain only records which are misclassified.</p>

<p>Hope this helps!</p>
",0,-1,753,2016-12-30 20:07:04,https://stackoverflow.com/questions/41402098/scikit-learn-classification
Predicting from SciKitLearn RandomForestClassification with Categorical Data,"<p>I created a RandomForestClassification model using SkLearn using 10 different text features and a training set of 10000.  Then, I pickled the model (76mb) in hopes of using it for prediction.</p>

<p>However, in order to produce the Random Forest, I used the LabelEncoder and OneHotEncoder for best results on the categorical/string data.</p>

<p>Now, I'd like to pull up the pickled model and get a classification prediction on 1 instance.  However, I'm not sure how to encode the text on the 1 instance without loading the entire training &amp; test dataset CSV
again and going through the entire encoding process.</p>

<p>It seems quite laborious to load the csv files every time.  I'd like this to run 1000x per hour so it doesn't seem right to me.</p>

<p>Is there a way to quickly encode 1 row of data given the pickle or other variable/setting?  Does encoding always require ALL the data?</p>

<p>If loading all the training data is required to encode a single row, would be advantageous to encode the text data myself in a database where each feature assigned to a table, auto-incremented with a numeric id and a UNIQUE key on the text/categorical field, then pass this id to the RandomForestClassification?  Obviously I would need to refit and pickle this new model, but then I would know exactly the (encoded) numeric representation of a new row and simply request a prediction on those values.</p>

<p>It's highly likely that I'm missing a feature or misunderstanding SkLearn or Python, I only started both a 3 days ago.  Please excuse my naivety.</p>
","python, machine-learning, scikit-learn, random-forest, text-classification","<p>Using <a href=""https://docs.python.org/2/library/pickle.html"" rel=""nofollow noreferrer"">Pickle</a> you should save your Label and One Hot Encoder. You can then read this each time and easily transform new instances. For example,</p>

<pre><code>import cPickle as pickle
from sklearn.externals import joblib
from sklearn import preprocessing

le = preprocessing.LabelEncoder()
train_x = [0,1,2,6,'true','false']
le.fit_transform(train_x)

# Save your encoding
joblib.dump(le, '/path/to/save/model')
# OR
pickle.dump(le, open( '/path/to/model', ""wb"" ) )

# Load those encodings
le = joblib.load('/path/to/save/model') 
# OR
le = pickle.load( open( '/path/to/model', ""rb"" ) )

# Then use as normal
new_x = [0,0,0,2,2,2,'false']
le.transform(new_x)
# array([0, 0, 0, 1, 1, 1, 3])
</code></pre>
",2,0,900,2017-01-05 23:01:04,https://stackoverflow.com/questions/41496248/predicting-from-scikitlearn-randomforestclassification-with-categorical-data
Text Classification - Label Pre Process,"<p>I have a data set of 1M+ observations of customer interactions with a call center. The text is free text written by the representative taking the call. The text is not well formatted nor is it close to being grammatically correct (a lot of short hand). None of the free text has a label on the data as I do not know what labels to provide. </p>

<p>Given the size of the data, would a random sample of the data  (to give a high level of confidence) be reasonable first step in determining what labels to create? Is it possible not to have to manually label 400+ random observations from the data, or is there no other method to pre-process the data in order to determine the a good set of labels to use for classification?</p>

<p>Appreciate any help on the issue.</p>
","python, r, nlp, preprocessor, text-classification","<p><strong>Text Pre-Processing:</strong> 
Convert all text to lower case, tokenize into unigrams, remove all stop words, use stemmer to normalize a token to it's base word.</p>

<p>There are 2 approaches I can think of for classifying the documents a.k.a. the free text you spoke about. Each free text is a document:</p>

<p>1) <strong>Supervised classification</strong> Take some time and randomly pick few samples of documents and assign them a category. Do this until you have multiple documents per category and all categories that you want to predict are covered. </p>

<p>Next, create a Tf-Idf matrix from this text. Select the top K features (tune value of K to get best results). Alternatively, you can use SVD to reduce the number of features by combining correlated features into one. Please bare in mind that you can use other features like the department of the customer service executive and many others also as predictors. Now train a machine learning model and test it out. </p>

<p>2) <strong>Unsupervised learning:</strong> If you know how many categories you have in your output variable, you can use that number as the number of clusters you want to create. Use the Tf-Idf vector from above technique and create k clusters. Randomly pick a few documents from each cluster and decide which category the documents belong to. Supposing you picked 5 documents and noticed that they belong to the category ""Wanting Refund"". Label all documents in this cluster to ""Wanting Refund"". Do this for all the remaining clusters. </p>

<p>The advantage of unsupervised learning is that it saves you the pain of pre-classification and data preparation, but beware of unsupervised learning. The accuracy might not be as good as supervised learning.</p>

<p>The 2 method explained are an abstract overview of what can be done. Now that you have an idea, read up more on the topics and use a tool like rapidminer to achieve your task much faster.</p>
",1,0,684,2017-02-05 05:01:47,https://stackoverflow.com/questions/42048725/text-classification-label-pre-process
Get accuracy of guess,"<p>I'm currently trying to find the pronounceability of a list of words using <a href=""https://stackoverflow.com/questions/40209592/arranging-letters-in-the-most-pronounceable-way"">this SO question</a></p>

<p>The following code is as follows:</p>

<pre><code>import random
def scramble(s):
    return """".join(random.sample(s, len(s)))

words = [w.strip() for w in open('/usr/share/dict/words') if w == w.lower()]
scrambled = [scramble(w) for w in words]

X = words+scrambled
y = ['word']*len(words) + ['unpronounceable']*len(scrambled)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

text_clf = Pipeline([
    ('vect', CountVectorizer(analyzer='char', ngram_range=(1, 3))),
    ('clf', MultinomialNB())
    ])

text_clf = text_clf.fit(X_train, y_train)
predicted = text_clf.predict(X_test)

from sklearn import metrics
print(metrics.classification_report(y_test, predicted))
</code></pre>

<p>This outputs with random words this</p>

<pre><code>&gt;&gt;&gt; text_clf.predict(""scaroly"".split())
['word']
</code></pre>

<p>I've been checking the <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"" rel=""nofollow noreferrer"">scikit documentation</a> but I still can't seem to find out how I would have it print the score of the input word.</p>
","python, scikit-learn, classification, text-classification","<p>Try <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.predict_proba"" rel=""nofollow noreferrer""><code>sklearn.pipeline.Pipeline.predict_proba</code></a>:</p>

<pre><code>&gt;&gt;&gt; text_clf.predict_proba([""scaroly""])
array([[  5.87363027e-04,   9.99412637e-01]])
</code></pre>

<p>It returns the likelihood a given input (in this case, <code>""scaroly""</code>) belongs to the classes upon which you trained the model. So there's a 99.94% chance <code>""scaroly""</code> is pronounceable.</p>

<p>Conversely, the Welsh word for ""new"" is likely unpronounceable:</p>

<pre><code>&gt;&gt;&gt; text_clf.predict_proba([""newydd""])
array([[ 0.99666533,  0.00333467]])
</code></pre>
",1,1,265,2017-02-08 01:17:39,https://stackoverflow.com/questions/42103002/get-accuracy-of-guess
Stanford Classifier with Real Valued Features,"<p>I'd like to use the <a href=""http://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">Stanford Classifier</a> for text classification. My features are mostly textual, but there are some numeric features as well (e.g. the length of a sentence). </p>

<p>I started off with the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/classify/ClassifierExample.java"" rel=""nofollow noreferrer"">ClassifierExample</a> and replaced the current features by a simple real valued feature <code>F</code> with value <code>100</code> if a stop light is <code>BROKEN</code> and <code>0.1</code> otherwise, which results in the following code (apart from the <code>makeStopLights()</code> function in line 10-16, this is just the code of the original ClassifierExample class):</p>

<pre><code>public class ClassifierExample {

    protected static final String GREEN = ""green"";
    protected static final String RED = ""red"";
    protected static final String WORKING = ""working"";
    protected static final String BROKEN = ""broken"";

    private ClassifierExample() {} // not instantiable

    // the definition of this function was changed!!
    protected static Datum&lt;String,String&gt; makeStopLights(String ns, String ew) {
        String label = (ns.equals(ew) ? BROKEN : WORKING);
        Counter&lt;String&gt; counter = new ClassicCounter&lt;&gt;();
        counter.setCount(""F"", (label.equals(BROKEN)) ? 100 : 0.1);
        return new RVFDatum&lt;&gt;(counter, label);
    }


    public static void main(String[] args) {
        // Create a training set
        List&lt;Datum&lt;String,String&gt;&gt; trainingData = new ArrayList&lt;&gt;();
        trainingData.add(makeStopLights(GREEN, RED));
        trainingData.add(makeStopLights(GREEN, RED));
        trainingData.add(makeStopLights(GREEN, RED));
        trainingData.add(makeStopLights(RED, GREEN));
        trainingData.add(makeStopLights(RED, GREEN));
        trainingData.add(makeStopLights(RED, GREEN));
        trainingData.add(makeStopLights(RED, RED));
        // Create a test set
        Datum&lt;String,String&gt; workingLights = makeStopLights(GREEN, RED);
        Datum&lt;String,String&gt; brokenLights = makeStopLights(RED, RED);
        // Build a classifier factory
        LinearClassifierFactory&lt;String,String&gt; factory = new LinearClassifierFactory&lt;&gt;();
        factory.useConjugateGradientAscent();
        // Turn on per-iteration convergence updates
        factory.setVerbose(true);
        //Small amount of smoothing
        factory.setSigma(10.0);
        // Build a classifier
        LinearClassifier&lt;String,String&gt; classifier = factory.trainClassifier(trainingData);
        // Check out the learned weights
        classifier.dump();
        // Test the classifier
        System.out.println(""Working instance got: "" + classifier.classOf(workingLights));
        classifier.justificationOf(workingLights);
        System.out.println(""Broken instance got: "" + classifier.classOf(brokenLights));
        classifier.justificationOf(brokenLights);
    }

}
</code></pre>

<p>In my understanding of linear classifiers, feature <code>F</code> should make the classification task pretty easy - after all, we just need to check whether the value of <code>F</code> is greater than some threshold. However, the classifier returns <code>WORKING</code> on every instance in the test set. </p>

<p>Now my question is: Have I made something wrong, do I need to change some other parts of the code as well for real-valued features to work or is there something wrong with my understanding of linear classifiers?</p>
","java, machine-learning, classification, stanford-nlp, text-classification","<p>Your code looks fine.  Note that typically with a Maximum Entropy classifier you provide binary valued features (1 or 0).</p>

<p>Here is some more reading on Maximum Entropy classifiers: <a href=""http://web.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers"" rel=""nofollow noreferrer"">http://web.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers</a></p>

<p>Look at slide titled: ""Feature-Based Linear Classifiers"" to see the specific probability calculation for Maximum Entropy classifiers.</p>

<p>Here is the formula for your example case with 1 feature and 2 classes (""works"", ""broken""):</p>

<p><code>probability(c1) = exp(w1 * f1) / total
probability(c2) = exp(w2 * f1) / total
total = exp(w1 * f1) + exp(w2 * f1)</code></p>

<p>w1 is the learned weight for ""works"" and w2 is the learned weight for ""broken""</p>

<p>The classifier selects the higher probability.  Note that f1 = (100 or 0.1) your feature value.</p>

<p>If you consider your specific example data, since you have (2 classes, 1 feature, feature is always positive), it is not possible to build a maximum entropy classifier that will separate that data, it will always guess all one way or the other.</p>

<p>For sake of argument say <code>w1 &gt; w2</code>.</p>

<p>Say <code>v &gt; 0</code> is your feature value (either 100 or 0.1).</p>

<p>Then <code>w1 * v &gt; w2 * v</code>, thus <code>exp(w1 * v) &gt; exp(w2 * v)</code>, so you'll always assign more probability to class1 regardless of what value v has.</p>
",1,1,212,2017-02-09 15:55:23,https://stackoverflow.com/questions/42141223/stanford-classifier-with-real-valued-features
MultinomialNB - Theory vs practice,"<p>OK so I'm just studying Andrew Ng's Machine Learning course. I'm currently reading <a href=""https://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf"" rel=""nofollow noreferrer"">this chapter</a> and want to try the Multinomial Naive Bayes (bottom of page 12) for myself using SKLearn and Python. So Andrew proposes a method, in which each email in this case is encoded so</p>

<blockquote>
  <p>We let <code>x_i</code> denote the identity of the <code>i</code>-th word in the email. Thus, <code>x_i</code> is now an integer taking values in <code>{1, . . . , |V|}</code>, where <code>|V|</code> is
  the size of our vocabulary (dictionary). An email of n words is now
  represented by a vector <code>(x1, x2, . . . , xn)</code> of length <code>n</code> <strong>note that n
  can vary for different documents</strong>. For instance, if an email starts
  with <code>“A NIPS . . . ,”</code> then <code>x_1 = 1</code> (<code>“a”</code> is the first word in the
  dictionary), and <code>x2 = 35000</code> (if <code>“nips”</code> is the 35000th word in the
  dictionary).</p>
</blockquote>

<p>See highlights. </p>

<p>So this is also what I did in Python. I have a <code>vocabulary</code>, which is a list of 502 words, and I encoded each ""email"" so that it's represented the same way as Andrew describes, for example the message ""this is sparta"" is represented by <code>[495, 296, 359]</code> and ""this is not sparta"" by <code>[495, 296, 415, 359]</code>.</p>

<p>So here comes the problem.</p>

<p>Apparently, SKLearn's <code>MultinomialNB</code> requires input with uniform shape (I'm not sure about this, but as of now, I'm getting <code>ValueError: setting an array element with a sequence.</code>, which I think is because the input vectors are not of same size). </p>

<p>So my question is, how can I use <code>MultinomialNB</code> for multiple length messages? Is it possible? What am I missing?</p>

<p>Here's some more of what I'm doing with code:</p>

<pre><code>X = posts['wordsencoded'].values
y = posts['highview'].values
clf = MultinomialNB()
clf.fit(X, y)
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
print(clf.predict())
</code></pre>

<p>What the input looks like:<a href=""https://i.sstatic.net/4D3Sd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4D3Sd.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/fM73L.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fM73L.png"" alt=""enter image description here""></a><a href=""https://i.sstatic.net/4y2AX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4y2AX.png"" alt=""enter image description here""></a></p>

<p>Stack trace:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-933-dea987cd8603&gt; in &lt;module&gt;()
      3 y = posts['highview'].values
      4 clf = MultinomialNB()
----&gt; 5 clf.fit(X, y)
      6 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
      7 print(clf.predict())

/usr/local/lib/python3.4/dist-packages/sklearn/naive_bayes.py in fit(self, X, y, sample_weight)
    525             Returns self.
    526         """"""
--&gt; 527         X, y = check_X_y(X, y, 'csr')
    528         _, n_features = X.shape
    529 

/usr/local/lib/python3.4/dist-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    508     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
    509                     ensure_2d, allow_nd, ensure_min_samples,
--&gt; 510                     ensure_min_features, warn_on_dtype, estimator)
    511     if multi_output:
    512         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,

/usr/local/lib/python3.4/dist-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    371                                       force_all_finite)
    372     else:
--&gt; 373         array = np.array(array, dtype=dtype, order=order, copy=copy)
    374 
    375         if ensure_2d:

ValueError: setting an array element with a sequence.
</code></pre>
","python, machine-learning, scikit-learn, text-classification, multinomial","<p>Yes you are thinking right. You have to encode each mail with fixed length vector. This vector is called word count vector of 502 dimensions (in your case) for each email of training set. </p>

<p>Each word count vector contains the frequency of 502 dictionary words in the training file. Of course you might have guessed by now that most of them will be zero. for example : ""this is not sparta not is this sparta"" will be encoded like below. 
[0,0,0,0,0,.......0,0,2,0,0,0,......,0,0,2,0,0,...0,0,2,0,0,......2,0,0,0,0,0,0]</p>

<p>Here, all the four 2's are placed at 296th, 359th, 415th, 495th index of 502 length word count vector.</p>

<p>So, a feature vector matrix will be generated whose rows denote number of files of training set and columns denote 502 words of dictionary. 
The value at index ‘ij’ will be the number of occurrences of jth word of dictionary in ith file.</p>

<p>This generated encoding of emails(feature vector matrix), can be given to the MultinomialNB for training.</p>

<p>You will also have to generate similar 502 length encoding for test email also before predicting the class.</p>

<p>You can easily build a spam filter classifier with multinomialNB on ling-spam dataset using the following blog. The blog-post uses sklearn and python for implementation.</p>

<p><a href=""https://appliedmachinelearning.wordpress.com/2017/01/23/nlp-blog-post/"" rel=""nofollow noreferrer"">https://appliedmachinelearning.wordpress.com/2017/01/23/nlp-blog-post/</a></p>
",2,4,7063,2017-02-09 16:48:48,https://stackoverflow.com/questions/42142366/multinomialnb-theory-vs-practice
Best machine learning approach to automate text/fuzzy matching,"<p>I'm reasonably new to machine learning, I've done a few projects in python. I'm looking for advice on how to approach the below problem which I believe could be automated.</p>

<p>A user in a data quality team in my organisation has a daily task of taking a list of company names (with addresses) that have been manually entered, he has to then search a database of companies to find the matching result, using his judgement - i.e. no hard and fast rule.</p>

<p>An example of the input would be:</p>

<p>Company Name, Address Line 1, Country</p>

<p>Of this, the user takes the company name and enters it into the search tool. Where he is presented with a list of results and he picks the best match but may choose not to pick any match. The search tool is built in house and talks to an external API, I have access to the source code so I can modify the search tool to capture the input, the list of results, and I could add a checkbox to see which result was used, and a check box to signify that none was chosen. Therefore this would become my labelled training data.</p>

<p>The columns used from the results to make the judgement are roughly the same:</p>

<p>Company Name, Address Line 1, Country</p>

<p>Given a company name like Stack Overflow, the results may return Stack Overflow Ltd., Stacking Overflowing Shelves Ltd. etc. The input data is reasonably good, so the results usually yield about 10 matches, and to a human, it's fairly obvious which one to pick.</p>

<p>My thought is that with enough training data I could call the API directly with the search term, and then choose the appropriate result from the list of results. </p>

<p>Is this something that could be achieved through ML? I'm struggling with the fact that the data will be different every time. Thoughts on the best way to achieve this are welcome, in particular how to structure the data for the model and what kind of classifier to use etc.</p>
","machine-learning, text-classification, fuzzy-comparison, record-linkage","<p>To frame it as a ML problem, you could learn a similarity function. </p>

<p>Instead of classifying ""Acme Corp"" as matching the target class ""Acme"" (classifier), you would instead learn a function that learns to tell that ""Acme Corp"" is similar to ""Acme"", but dissimilar to ""ABC Corp"". </p>

<p>This is usually called ""Similarity Learning"", in your case, maybe more specifically ""Ranking similarity learning"" since your goal is not to learn a function that will output a similarity value, but instead rank potential candidates. </p>

<p>But before using full ML algorithms, I would start first by using a string distance metric, for instance the Levenshtein distance metric (very common and easy to find). Transform your data in positive and negative examples (a positive example: Acme is a match to Acme Corp). The simplest learning function would be finding the Edit Distance threshold that maximizes your score. You can also add parameters like: ""remove Corp."", ""remove Ltd"", etc. and find what combination works best. </p>
",8,7,27521,2017-02-16 16:40:04,https://stackoverflow.com/questions/42279814/best-machine-learning-approach-to-automate-text-fuzzy-matching
Dealing with differences in feature space regarding text classification using SVM,"<p>I asked this questions on the R mailing list, but I think here is a better place to look for answers and tips.</p>

<p>I'm currently working on text classification of student's essays, trying 
to identify texts that fit to a certain class or not. I use texts from 
one semester (A) for training and texts from another semester (B) for 
testing the classifier. My workflow is like this:</p>

<ul>
<li>read all texts from A, build a DTM(A) with about 1387 terms (package tm)</li>
<li>read all texts from B, build a DTM(B) with about 626 terms</li>
<li>train the classifier with DTM(A), using a SVM (package e1071)</li>
</ul>

<p>Now I want to classify all texts in DTM(B) using the classifyer. But 
when I try to use predict(), I always get the error message: Error in 
eval(expr, envir, enclos) : object 'XY' not found. As I found out, the 
reason for this is that DTM(A) and DTM(B) have a different number of 
terms and consequently not every term used for training the model is 
available in DTM(B).</p>

<p>Sure it's problematic to do a classification with two different feature spaces, but I want to finde a solution for this ""real-world-problem"". The idea is to identify wether or not a text turned in by a student fits the other texts or not. So my naive idea is to develop a prediction model with texts from one semester [DTM(A)] and then use this model to evaluate a new text from another semester [DTM(B)]. As the new text isn't in the original DTM, the feature spaces differ. So far I only found code that uses a DTM created from all texts, but this would require to create a new DTM(A)` and re-train the SVM each and every time.</p>

<p>My question is: how should/do I deal with this? Should I match the terms 
used in DTM(A) and DTM(B), in order to get an identical feature space? 
This could be achieved either reducing the number of terms in DTM(A) or 
adding several empty/NA columns to DTM(B). Or is there another solution 
to my problem?</p>

<p>Kind regards</p>

<p>Björn</p>
","r, svm, text-classification","<p>After some more experiments and some research, I came across the RTextTools package and its function ""create_matrix()"". This function creates a new DTM and you can also adjust the matrix to the originalMatrix, which has been used to train the model. This was exactly what I was looking for. So I looked at the original code (<a href=""https://github.com/timjurka/RTextTools/blob/master/RTextTools/R/create_matrix.R"" rel=""nofollow noreferrer"">https://github.com/timjurka/RTextTools/blob/master/RTextTools/R/create_matrix.R</a>) and came up with this:</p>

<pre><code># get all the terms which are in the training df, but not in the test df
terms &lt;- colnames(train.df[,which(!colnames(train.df) %in% colnames(test.df))])
# weight is set, this is just in case that weightTfIdf was used, otherwise it should be 0
weight &lt;- 0.000000001
# now create a new matrix with the missing terms
amat &lt;- matrix(weight, nrow = nrow(test.df), ncol = length(terms))
colnames(amat) &lt;- terms
rownames(amat) &lt;- rownames(test.df)

# create a new test df with the original values plus the new matrix with the missing terms
test.df.fixed &lt;- cbind(test.df[,which(colnames(test.df) %in% colnames(train.df))],amat)
test.df.fixed &lt;- test.df.fixed[, sort(colnames(test.df.fixed))]
</code></pre>

<p>The result is a test data frame which has all features (columns) of the data frame that was used for training. So it's basically an ""up-filtering"" instead of a down-filtering. A quick test showed it works quite well (Accuracy: .91, Kappa: .88).</p>
",0,0,187,2017-02-20 08:49:44,https://stackoverflow.com/questions/42339991/dealing-with-differences-in-feature-space-regarding-text-classification-using-sv
Which is the efficient way to remove stop words in textblob for sentiment analysis of text?,"<p>I'm trying to implement Naive Bayes algorithm for sentiment analysis of News Paper headlines. I'm using TextBlob for this purpose and I'm finding it difficult to remove stop words such as 'a', 'the', 'in' etc. Below is the snippet of my code in python:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob

test = [
(""11 bonded labourers saved from shoe firm"", ""pos""),
(""Scientists greet Abdul Kalam after the successful launch of Agni on May 22, 1989"",""pos""),
(""Heavy Winter Snow Storm Lashes Out In Northeast US"", ""neg""),
(""Apparent Strike On Gaza Tunnels Kills 2 Palestinians"", ""neg"")
       ]

with open('input.json', 'r') as fp:
cl = NaiveBayesClassifier(fp, format=""json"")

print(cl.classify(""Oil ends year with biggest gain since 2009""))  # ""pos""
print(cl.classify(""25 dead in Baghdad blasts""))  # ""neg""
</code></pre>
","python, sentiment-analysis, text-classification, textblob","<p>You can first load the json and then create list of tuples(text, label) with the replacement.</p>

<p>Demonstration:</p>

<p>Suppose the input.json file is something like this:</p>

<pre><code>[
    {""text"": ""I love this sandwich."", ""label"": ""pos""},
    {""text"": ""This is an amazing place!"", ""label"": ""pos""},
    {""text"": ""I do not like this restaurant"", ""label"": ""neg""}
]
</code></pre>

<p>Then you can use:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
import json

train_list = []
with open('input.json', 'r') as fp:
    json_data = json.load(fp)
    for line in json_data:
        text = line['text']
        text = text.replace("" is "", "" "") # you can remove multiple stop words
        label = line['label']
        train_list.append((text, label))
    cl = NaiveBayesClassifier(train_list)

from pprint import pprint
pprint(train_list)
</code></pre>

<p>output:</p>

<pre><code>[(u'I love this sandwich.', u'pos'),
 (u'This an amazing place!', u'pos'),
 (u'I do not like this restaurant', u'neg')]
</code></pre>
",-1,1,1770,2017-02-20 14:02:48,https://stackoverflow.com/questions/42346637/which-is-the-efficient-way-to-remove-stop-words-in-textblob-for-sentiment-analys
How to link 10-fcv weka predicted result back to original comment for text classification,"<p>Is there anyway I can route back my predicted result to original comment after text classification using 10-fold cross validation?</p>

<p>From the result of 2000 comments of class non-sarc and sarc: </p>

<pre><code>inst#,actual,predicted,error,prediction
1,2:non-sarc,2:non-sarc,,1
2,2:non-sarc,1:sarc,+,1
3,2:non-sarc,2:non-sarc,,1
4,2:non-sarc,2:non-sarc,,1
5,2:non-sarc,2:non-sarc,,1
.
.
101,1:sarc,1:sarc,,1
102,1:sarc,2:non-sarc,+,1
103,1:sarc,1:sarc,,1
104,1:sarc,1:sarc,,1
105,1:sarc,1:sarc,,1
.
.
</code></pre>

<p>It looks like weka has re-arranged my comment to class split before hold out for training and testing. How can i refer back this result to original comments which are not in sequence order (not like after 10-fcv)? I've try re-arranged the comment to class sequence of non-sarc and sarc but I'm confuse which one test/training first, is it first fold test first, or last fold test first, or any other?</p>

<p>Thanks in advance. </p>
","weka, text-classification, rweka","<p>Since no one answered my question and I've figured myself, hope this will help others if facing the same issue.</p>

<ol>
<li>In Preprocess; Filter> unsupervised; AddID to the attributes, to the first position. This will give ID for each of original label [IDIndex: First]</li>
</ol>

<p><a href=""https://i.sstatic.net/UGb5X.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UGb5X.png"" alt=""1.Add IDIndex""></a> </p>

<ol start=""2"">
<li>In Classify; Choose classifier. For test option, set 10-fcv, and in more option, set attributes to 1. And choose for link and output format prediction result [attributes: 1]</li>
</ol>

<p><a href=""https://i.sstatic.net/3Ckvu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3Ckvu.png"" alt=""2.Attribute and Output""></a></p>

<ol start=""3"">
<li>Start/Run prediction. Output shows actual label and prediction. Error is mark with + and ID refers to original label before prediction.</li>
</ol>

<p><a href=""https://i.sstatic.net/wnFJs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wnFJs.png"" alt=""3.Output""></a></p>

<p>All the best!</p>
",0,0,172,2017-02-23 22:55:34,https://stackoverflow.com/questions/42427746/how-to-link-10-fcv-weka-predicted-result-back-to-original-comment-for-text-class
How can I split documents into training set and test set?,"<p>I am trying to build a classification model. I have 1000 text documents in local folder. I want to divide them into training set and test set with a split ratio of 70:30(70 → Training and 30 → Test). What is the better approach to do so? I am using Python.</p>
<hr />
<p>I wanted a approach programmatically to split the training set and test set. First to read the files in local directory. Second, to build a list of those files and shuffle them. Thirdly to split them into a training set and test set.</p>
<p>I tried a few ways by using built-in Python keywords and functions only to fail. Lastly, I got the idea of approaching it. Also <em>cross-validation</em> is a good option to be considered for the building general classification models.</p>
","machine-learning, scikit-learn, text-classification","<p>There will be a few steps:</p>
<ol>
<li>Get a list of the files</li>
<li>Randomize the files</li>
<li>Split files into training and testing sets</li>
<li>Do the thing</li>
</ol>
<h3>1. Get a list of the files</h3>
<p>Let's assume that your files all have the extension <code>.data</code> and they're all in the folder <code>/ml/data/</code>. We want to get a list of all of these files. This is done simply with the <code>os</code> module. I'm assuming you don't have any subdirectories; this would change if there were.</p>
<pre class=""lang-python prettyprint-override""><code>import os

def get_file_list_from_dir(datadir):
    all_files = os.listdir(os.path.abspath(datadir))
    data_files = list(filter(lambda file: file.endswith('.data'), all_files))
    return data_files
</code></pre>
<p>So if we were to call <code>get_file_list_from_dir('/ml/data')</code>, we would get back a list of all the <code>.data</code> files in that directory (equivalent in the shell to the glob <code>/ml/data/*.data</code>).</p>
<h3>2. Randomize the files</h3>
<p>We don't want the sampling to be predictable, as that is considered a poor way to train an ML classifier.</p>
<pre class=""lang-python prettyprint-override""><code>from random import shuffle

def randomize_files(file_list):
    shuffle(file_list)
</code></pre>
<p>Note that <code>random.shuffle</code> performs an <em>in-place</em> shuffling, so it modifies the existing list. (Of course this function is rather silly since you could just call <code>shuffle</code> instead of <code>randomize_files</code>; you can write this into another function to make it make more sense.)</p>
<h3>3. Split files into training and testing sets</h3>
<p>I'll assume a 70:30 ratio instead of any specific number of documents. So:</p>
<pre class=""lang-python prettyprint-override""><code>from math import floor

def get_training_and_testing_sets(file_list):
    split = 0.7
    split_index = floor(len(file_list) * split)
    training = file_list[:split_index]
    testing = file_list[split_index:]
    return training, testing
</code></pre>
<h3>4. Do the thing</h3>
<p>This is the step where you open each file and do your training and testing. I'll leave this to you!</p>
<hr />
<h3>Cross-Validation</h3>
<p>Out of curiosity, have you considered using <a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)"" rel=""nofollow noreferrer"">cross-validation</a>? This is a method of splitting your data so that you use every document for training and testing. You can customize how many documents are used for training in each &quot;fold&quot;. I could go more into depth on this if you like, but I won't if you don't want to do it.</p>
<p>All right, since you requested it, I will explain this a little bit more.</p>
<p>So we have a 1000-document set of data. The idea of cross-validation is that you can use <em>all</em> of it for both training and testing — just not at once. We split the dataset into what we call &quot;folds&quot;. The number of folds determines the size of the training and testing sets at any given point in time.</p>
<p>Let's say we want a 10-fold cross-validation system. This means that the training and testing algorithms will run ten times. The first time will train on documents 1-100 and test on 101-1000. The second fold will train on 101-200 and test on 1-100 and 201-1000.</p>
<p>If we did, say, a 40-fold CV system, the first fold would train on document 1-25 and test on 26-1000, the second fold would train on 26-40 and test on 1-25 and 51-1000, and on.</p>
<p>To implement such a system, we would still need to do steps (1) and (2) from above, but step (3) would be different. Instead of splitting into just two sets (one for training, one for testing), we could turn the function into a <a href=""https://wiki.python.org/moin/Generators"" rel=""nofollow noreferrer""><em>generator</em></a> — a function which we can iterate through like a list.</p>
<pre class=""lang-python prettyprint-override""><code>def cross_validate(data_files, folds):
    if len(data_files) % folds != 0:
        raise ValueError(
            &quot;invalid number of folds ({}) for the number of &quot;
            &quot;documents ({})&quot;.format(folds, len(data_files))
        )
    fold_size = len(data_files) // folds
    for split_index in range(0, len(data_files), fold_size):
        training = data_files[split_index:split_index + fold_size]
        testing = data_files[:split_index] + data_files[split_index + fold_size:]
        yield training, testing
</code></pre>
<p>That <code>yield</code> keyword at the end is what makes this a generator. To use it, you would use it like so:</p>
<pre class=""lang-none prettyprint-override""><code>def ml_function(datadir, num_folds):
    data_files = get_file_list_from_dir(datadir)
    randomize_files(data_files)
    for train_set, test_set in cross_validate(data_files, num_folds):
        do_ml_training(train_set)
        do_ml_testing(test_set)
</code></pre>
<p>Again, it's up to you to implement the actual functionality of your ML system.</p>
",21,4,17992,2017-02-26 17:08:29,https://stackoverflow.com/questions/42471570/how-can-i-split-documents-into-training-set-and-test-set
How many classes can CNN classify the short text?,"<p>I know that CNN(conv-neural-network) could classify more than 10 thousands of images of the ImageNet.</p>

<p>I find that CNN could only classify 10-20 text classes as <a href=""https://arxiv.org/pdf/1502.01710.pdf"" rel=""nofollow noreferrer"">this paper</a> write.</p>

<p>How many classes can CNN classify the short text? <strong>What is the high limit of the classes number?</strong> </p>
","machine-learning, nlp, deep-learning, conv-neural-network, text-classification","<p>The number of categories a classifier could classify with good precision/recall is decided by (but not limited to):</p>

<ul>
<li>how distinct each category is?</li>
<li>how many features you could derive from the content (short text definitely carries much less information here than images) -- since you are using CNN for text, I assume the features would be merely characters or words.</li>
<li>How these features work to differentiate between categories?</li>
<li>how many high-quality labeled examples you have? (We don't have a public labeled large multi-category dataset for short text)</li>
</ul>

<p>It's hard to just give you a number without knowing the answers to above questions.</p>
",3,-1,1633,2017-03-09 06:00:12,https://stackoverflow.com/questions/42687841/how-many-classes-can-cnn-classify-the-short-text
Low accuracy with Text Classification while trying to predict user&#39;s personality via Twitter,"<p>I am working on a project to predict the personality of a user using his tweets.</p>

<p>For training, I have a large corpus of 350000 tweets of users who have already taken the personality test, each tweet being linked to a specific personality type.
There are 16 different personality types(1-16)</p>

<p>I have performed pre-processing on this tweets to remove stop words, stemming and POS tagging.</p>

<p>I have a large dictionary of 500 most frequent words that I will be using as my features for training.
After this, I have performed tfidf vectorization on each tweet using the predefined dictionary of 500 words to create a word vector for each tweet.</p>

<pre><code>vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)
x=vectorizer.fit_transform(corpus).toarray()
</code></pre>

<p>Where corpus is a list of all the tweets.
I then combine this <code>x</code> and <code>y</code>(1-16 classes for each tweet) using:</p>

<pre><code>result=np.append(x,y,axis=1)
X=pandas.DataFrame(result)
X.to_csv('vectorized500.csv')
</code></pre>

<p>I am using this (350000*500) dataframe as <code>X</code> and my 1-16 numbered personality types as my <code>Y</code> dataframe(350000*1) which is divided into training and testing equally using: </p>

<pre><code>X=pd.read_csv('vectorized500.csv')
train = X.sample(frac=0.8, random_state=200)
test=X.drop(train.index)
y_train=train[""501""] #501 is the column name where Y is in the csv file
y_test=test[""501""]
xtrain=train.drop(""501"",axis=1)
xtest=test.drop(""501"",axis=1)
</code></pre>

<p>However, no matter what algorithm I run, I'm getting very poor results:</p>

<pre><code>model=RandomForestClassifier()
model.fit(xtrain,y_train)
pickle.dump(model, open('rf1000.sav', 'wb'))
print(model.score(xtest,y_test))
</code></pre>

<p>If i run <code>RandomForestClassifier</code>, i get 52% accuracy.</p>

<p>If I run Naive Bayes, Logistic Regression or Linear SVM, i get below 20% accuracy.</p>

<p>Is there any efficient way to run this kind of multiclass text classification or is there something I am doing wrong?
The accuracy is too low and I want to improve it. </p>
","python, machine-learning, svm, random-forest, text-classification","<p>The problem might be the imbalanced dataset you are using.</p>

<pre><code>0. 56887 INFP 1. 54607 INFJ 2. 52511 INTJ 3. 52028 ENFP 4. 24294 INTP 5. 19032 ENTJ 6. 14284 ENFJ 7. 12502 ISFJ 8. 12268 ISTP 9. 10713 ISTJ 10. 10523 ESFP 11. 8103 ESTP 12. 7436 ESFJ 13. 7016 ESTJ 14. 6725 ISFP
</code></pre>

<p>Imbalanced data, refers to a problem where the classes are not equally represented. There are many techniques that can be used for dealing with this phenomenon. </p>

<ol>
<li><p><strong>Collect more data</strong></p>

<p>Try if possible, to collect more data for the classes with few examples.</p></li>
<li><p><strong>Use other performance metrics</strong></p>

<p>Accuracy is not a metric that can be used when your dataset is imbalanced. Imagine that you have two classes (<code>0</code> and <code>1</code>) where 99 examples belong to <code>class 0</code> and just 1 example to <code>class 1</code>. If you build a model that always assigns <code>class 0</code> to every testing point you will end up with 99% accuracy but obviously this is not what you want. Some useful metrics other than accuracy are the following:</p>

<ul>
<li>Precision/Recall/F-score (Extracted from a Confusion Matrix)</li>
<li>ROC curves</li>
</ul></li>
<li><p><strong>Undersampling</strong></p>

<p>Try to discard examples from your most popular classes, so that all the classes have approximately the same amount of examples. Throwing data away might not be a good idea, so try to avoid undersampling. </p></li>
</ol>
",5,3,1684,2017-03-11 12:27:17,https://stackoverflow.com/questions/42735189/low-accuracy-with-text-classification-while-trying-to-predict-users-personality
Scikit-learn out-of-core text classification memory consumption,"<p>I am trying to use scikit-learn to classify a large number of text documents, although I'm using the out-of-core functionality (with <code>SGDClassifier</code> and <code>HashingVectorizer</code>) the program seems to be consuming a lot of RAM (>10GB). I performed lemmatization and removed stopwords from the text data prior to this. I feel like I am missing out something important here. Can you spot a mistake in my code?</p>

<p>Thank you very much for any suggestion!</p>

<p>This is my python code:</p>

<pre><code>import time
import numpy as np
import os
import re
import pyprind
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

directory = ""mydirectory""
batch_size = 1000
n_batches = 44
pbar = pyprind.ProgBar(n_batches)

class Doc_Iterable:
    def __init__(self, file):
        self.file = file
    def __iter__(self):
        for line in self.file:
            line = re.sub('[^\w\s]|(.\d{1,4}[\./]\d{1,2}[\./]\d{1,4})|(\s\d{1,})', '', line)
            yield line


def stream_docs(path, texts_file, labels_file):
    with open(path + texts_file, 'r') as fX, open(path + labels_file, 'r') as fy:
        for text in fX:
            label = next(fy)
            text = re.sub('[^\w\s]|(.\d{1,4}[\./]\d{1,2}[\./]\d{1,4})|(\s\d{1,})', '', text)
            yield text, label

def get_minibatch(doc_stream, size):
    X, y = [], []
    for _ in range(size):
        text, label = next(doc_stream)
        X.append(text)
        y.append(label)
    return X, y


classes = set()
for label in open(directory + 'y_train', 'r'):
    classes.add(label)
for label in open(directory + 'y_test', 'r'):
    classes.add(label)
classes = list(classes)

validation_scores = []
training_set_size = []

h_vectorizer = HashingVectorizer(lowercase=True, ngram_range=(1,1))
clf = SGDClassifier(loss='hinge', n_iter=5, alpha=1e-4, shuffle=True)

doc_stream = stream_docs(path=directory, texts_file='X_train', labels_file='y_train')
n_samples = 0
iteration = 0

for _ in range(n_batches):
    print(""Training with batch nr."", iteration)
    iteration += 1

    X_train, y_train = get_minibatch(doc_stream, size=batch_size)

    n_samples += len(X_train)

    X_train = h_vectorizer.transform(X_train)

    clf.partial_fit(X_train, y_train, classes=classes)

    pbar.update()


del X_train
del y_train
print(""Training complete. Classifier trained with "" + str(n_samples) + "" samples."")
print()
print(""Testing..."")
print()
X_test = h_vectorizer.transform(Doc_Iterable(open(directory + 'X_test')))
y_test = np.genfromtxt(directory + 'y_test', dtype=None, delimiter='|').astype(str)
prediction = clf.predict(X_test)
score = metrics.accuracy_score(y_test, prediction)
print(""Accuracy: "", score)
print()
</code></pre>
","python, scikit-learn, text-classification","<p>Try adjusting <code>n_features</code> in the <code>HashingVectorizer</code>, for example:</p>

<pre><code>h_vectorizer = HashingVectorizer(n_features=10000, lowercase=True, ngram_range=(1,1))
</code></pre>

<p>With the default parameters(<code>n_features=1048576</code>) you can expect your transformed matrix to have up to:</p>

<pre><code>1048576(features) x 1000(mini batch size) x 8 bytes = 8.4 GB
</code></pre>

<p>It will be less than that because of sparsity but the coefficients of the classifier will add up:</p>

<pre><code>1048576(features) x len(classes) * 8 bytes
</code></pre>

<p>so that might explain your current memory usage.</p>
",3,3,562,2017-03-14 15:33:47,https://stackoverflow.com/questions/42790232/scikit-learn-out-of-core-text-classification-memory-consumption
UserWarning: Label not :NUMBER: is present in all training examples,"<p>I am doing multilabel classification, where I try to predict correct labels for each document and here is my code:</p>

<pre><code>mlb = MultiLabelBinarizer()
X = dataframe['body'].values 
y = mlb.fit_transform(dataframe['tag'].values)

classifier = Pipeline([
    ('vectorizer', CountVectorizer(lowercase=True, 
                                   stop_words='english', 
                                   max_df = 0.8, 
                                   min_df = 10)),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])

predicted = cross_val_predict(classifier, X, y)
</code></pre>

<p>When running my code I get multiple warnings:</p>

<pre><code>UserWarning: Label not :NUMBER: is present in all training examples.
</code></pre>

<p>When I print out predicted and true labels, cca half of all documents has it's predictions for labels empty.</p>

<p>Why is this happening, is it related to warnings it prints out while training is running? How can I avoid those empty predictions?
<hr>
<strong>EDIT01:</strong>
This is also happening when using other estimators than <code>LinearSVC()</code>.</p>

<p>I've tried <code>RandomForestClassifier()</code> and it gives empty predictions as well. Strange thing is, when I use <code>cross_val_predict(classifier, X, y, method='predict_proba')</code> for predicting probabilities for each label, instead of binary decisions 0/1, there is always at least one label per predicted set with probability > 0 for given document. So I dont know why is this label not chosen with binary decisioning? Or is binary decisioning evaluated in different way than probabilities?</p>

<p><strong>EDIT02:</strong>
I have found an old <a href=""http://scikit-learn-general.narkive.com/lm8imv9z/linearsvc-somtimes-returns-no-label"" rel=""noreferrer"">post</a> where OP was dealing with similar problem. Is this the same case?</p>
","python, scikit-learn, classification, text-classification, multilabel-classification","<blockquote>
  <p>Why is this happening, is it related to warnings it prints out while training is running?</p>
</blockquote>

<p>The issue is likely to be that some tags occur just in a few documents (check out <a href=""https://stackoverflow.com/questions/34342122/python-sklearn-multilabel-classification-userwarning-label-not-226-is-present"">this thread</a> for details). When you split the dataset into train and test to validate your model, it may happen that some tags are missing from the training data. Let <code>train_indices</code> be an array with the indices of the training samples. If a particular tag (of index <code>k</code>) does not occur in the training sample, all the elements in the <code>k</code>-th column of the indicator matrix <code>y[train_indices]</code> are zeros. </p>

<blockquote>
  <p>How can I avoid those empty predictions?  </p>
</blockquote>

<p>In the scenario described above the classifier will not be able to reliably predict the <code>k</code>-th tag in the test documents (more on this in the next paragraph). Therefore you cannot trust the predictions made by <code>clf.predict</code> and you need to implement the prediction function on your own, for example by using the decision values returned by <code>clf.decision_function</code> as suggested in <a href=""https://stackoverflow.com/questions/34561554/scikit-learn-label-not-x-is-present-in-all-training-examples/34561760#34561760"">this answer</a>.</p>

<blockquote>
  <p>So I don't know why is this label not chosen with binary decisioning? Or is binary decisioning evaluated in different way than probabilities?</p>
</blockquote>

<p>In datasets containing many labels the occurrence frequency for most of them uses to be rather low. If these low values are fed to a binary classifier (i.e. a classifier that makes a 0-1 prediction) it is highly probable that the classifier would pick 0 for all tags on all documents.</p>

<blockquote>
  <p>I have found an old post where OP was dealing with similar problem. Is this the same case?</p>
</blockquote>

<p>Yes, absolutely. That guy is facing exactly the same problem as you and his code is pretty similar to yours.</p>

<hr>

<p><strong><em>Demo</em></strong></p>

<p>To further explain the issue I have elaborated a simple toy example using mock data.</p>

<pre><code>Q = {'What does the ""yield"" keyword do in Python?': ['python'],
     'What is a metaclass in Python?': ['oop'],
     'How do I check whether a file exists using Python?': ['python'],
     'How to make a chain of function decorators?': ['python', 'decorator'],
     'Using i and j as variables in Matlab': ['matlab', 'naming-conventions'],
     'MATLAB: get variable type': ['matlab'],
     'Why is MATLAB so fast in matrix multiplication?': ['performance'],
     'Is MATLAB OOP slow or am I doing something wrong?': ['matlab-oop'],
    }
dataframe = pd.DataFrame({'body': Q.keys(), 'tag': Q.values()})    

mlb = MultiLabelBinarizer()
X = dataframe['body'].values 
y = mlb.fit_transform(dataframe['tag'].values)

classifier = Pipeline([
    ('vectorizer', CountVectorizer(lowercase=True, 
                                   stop_words='english', 
                                   max_df=0.8, 
                                   min_df=1)),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])
</code></pre>

<p>Please, notice that I have set <code>min_df=1</code> since my dataset is much smaller than yours. When I run the following sentence:</p>

<pre><code>predicted = cross_val_predict(classifier, X, y)
</code></pre>

<p>I get a bunch of warnings</p>

<pre><code>C:\...\multiclass.py:76: UserWarning: Label not 4 is present in all training examples.
  str(classes[c]))
C:\\multiclass.py:76: UserWarning: Label not 0 is present in all training examples.
  str(classes[c]))
C:\...\multiclass.py:76: UserWarning: Label not 3 is present in all training examples.
  str(classes[c]))
C:\...\multiclass.py:76: UserWarning: Label not 5 is present in all training examples.
  str(classes[c]))
C:\...\multiclass.py:76: UserWarning: Label not 2 is present in all training examples.
  str(classes[c]))
</code></pre>

<p>and the following prediction:</p>

<pre><code>In [5]: np.set_printoptions(precision=2, threshold=1000)    

In [6]: predicted
Out[6]: 
array([[0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 1],
       [0, 0, 0, 0, 0, 0, 1],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 1, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0]])
</code></pre>

<p>Those rows whose entries are all <code>0</code> indicate that no tag is predicted for the corresponding document.</p>

<hr>

<p><strong><em>Workaround</em></strong></p>

<p>For the sake of the analysis, let us validate the model manually rather than through <code>cross_val_predict</code>. </p>

<pre><code>import warnings
from sklearn.model_selection import ShuffleSplit

rs = ShuffleSplit(n_splits=1, test_size=.5, random_state=0)
train_indices, test_indices = rs.split(X).next()

with warnings.catch_warnings(record=True) as received_warnings:
    warnings.simplefilter(""always"")
    X_train, y_train = X[train_indices], y[train_indices]
    X_test, y_test = X[test_indices], y[test_indices]
    classifier.fit(X_train, y_train)
    predicted_test = classifier.predict(X_test)
    for w in received_warnings:
        print w.message
</code></pre>

<p>When the snippet above is executed two warnings are issued (I used a context manager to make sure warnings are catched):</p>

<pre><code>Label not 2 is present in all training examples.
Label not 4 is present in all training examples.
</code></pre>

<p>This is consistent with the fact that tags of indices <code>2</code> and <code>4</code> are missing from the training samples:</p>

<pre><code>In [40]: y_train
Out[40]: 
array([[0, 0, 0, 0, 0, 1, 0],
       [0, 1, 0, 0, 0, 0, 0],
       [0, 1, 0, 1, 0, 0, 0],
       [1, 0, 0, 0, 0, 0, 1]])
</code></pre>

<p>For some documents, the prediction is empty (those documents corresponding to the rows with all zeros in <code>predicted_test</code>):</p>

<pre><code>In [42]: predicted_test
Out[42]: 
array([[0, 1, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 1, 0, 1, 0, 0, 0]])
</code></pre>

<p>To overcome that issue, you could implement your own prediction function like this:</p>

<pre><code>def get_best_tags(clf, X, lb, n_tags=3):
    decfun = clf.decision_function(X)
    best_tags = np.argsort(decfun)[:, :-(n_tags+1): -1]
    return lb.classes_[best_tags]
</code></pre>

<p>By doing so, each document is always assigned the <code>n_tag</code> tags with the highest confidence score:</p>

<pre><code>In [59]: mlb.inverse_transform(predicted_test)
Out[59]: [('matlab',), (), (), ('matlab', 'naming-conventions')]

In [60]: get_best_tags(classifier, X_test, mlb)
Out[60]: 
array([['matlab', 'oop', 'matlab-oop'],
       ['oop', 'matlab-oop', 'matlab'],
       ['oop', 'matlab-oop', 'matlab'],
       ['matlab', 'naming-conventions', 'oop']], dtype=object)
</code></pre>
",20,17,6749,2017-03-15 21:48:18,https://stackoverflow.com/questions/42821315/userwarning-label-not-number-is-present-in-all-training-examples
How to classify text pairs using scikit-learn?,"<p>I have read many different blogs on this topic, but haven't been able to find a clear solution. I have  the following scenario:</p>

<ol>
<li>I have a list of pairs of texts with labels 1, or -1. </li>
<li>For each text pair , I want the features to be a concatenation in the following fashion:   f () = tfidf(t1) ""concat"" tfidf(t2)</li>
</ol>

<p>Any suggestions on how to do the same ? I have the following code but it gives an error:</p>

<pre><code>    count_vect = TfidfVectorizer(analyzer=u'char', ngram_range=ngram_range)
    X0_train_counts = count_vect.fit_transform([x[0] for x in training_documents])
    X1_train_counts = count_vect.fit_transform([x[1] for x in training_documents])
    combined_features = FeatureUnion([(""x0"", X0_train_counts), (""x1"", X1_train_counts)])
    clf = LinearSVC().fit(combined_features, training_target)
    average_training_accuracy += clf.score(combined_features, training_target)
</code></pre>

<p>Here's the error I get:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
scoreEdgesUsingClassifier(None, pos, neg, 1,ngram_range=(2,5), max_size=1000000, test_size=100000)

 scoreEdgesUsingClassifier(unc, pos, neg, number_of_iterations, ngram_range, max_size, test_size)
 X0_train_counts = count_vect.fit_transform([x[0] for x in training_documents])
 X1_train_counts = count_vect.fit_transform([x[1] for x in training_documents])
 combined_features = FeatureUnion([(""x0"", X0_train_counts), (""x1"", X1_train_counts)])
 print ""Done transforming, now training classifier""

lib/python2.7/site-packages/sklearn/pipeline.pyc in __init__(self, transformer_list, n_jobs, transformer_weights)
616         self.n_jobs = n_jobs
617         self.transformer_weights = transformer_weights
--&gt; 618         self._validate_transformers()
619 
620     def get_params(self, deep=True):

lib/python2.7/site-packages/sklearn/pipeline.pyc in _validate_transformers(self)
660                 raise TypeError(""All estimators should implement fit and ""
661                                 ""transform. '%s' (type %s) doesn't"" %
--&gt; 662                                 (t, type(t)))
663 
664     def _iter(self):

TypeError: All estimators should implement fit and transform. '  (0, 49025) 0.0575144797079

 (254741, 38401)    0.184394443164
 (254741, 201747)   0.186080393768
 (254741, 179231)   0.195062580945
 (254741, 156925)   0.211367771299
 (254741, 90026)    0.202458920022' (type &lt;class 'scipy.sparse.csr.csr_matrix'&gt;) doesn't
</code></pre>

<p><strong>Update</strong></p>

<p>Here's the solution:</p>

<pre><code>    count_vect = TfidfVectorizer(analyzer=u'char', ngram_range=ngram_range)
    training_docs_combined = [x[0] for x in training_documents] + [x[1] for x in training_documents]        
    X_train_counts = count_vect.fit_transform(training_docs_combined)
    concat_features  = hstack((X_train_counts[0:len(training_docs_combined) / 2 ], X_train_counts[len (training_docs_combined) / 2:]))

    clf = LinearSVC().fit(concat_features, training_target)
    average_training_accuracy += clf.score(concat_features, training_target)
</code></pre>
","python, machine-learning, scikit-learn, tf-idf, text-classification","<p><code>FeatureUnion</code> from scikit-learn takes as input estimators, not data arrays. </p>

<p>You can either concatenate the resulting <code>X0_train_counts</code>, <code>X1_train_counts</code> arrays simply with <code>scipy.sparse.hstack</code>, or create two independent instances of <code>TfidfVectorizer</code>, apply <code>FeatureUnion</code> to them, and then call the <code>fit_transform</code> method.</p>
",2,3,580,2017-03-16 20:48:32,https://stackoverflow.com/questions/42844491/how-to-classify-text-pairs-using-scikit-learn
How to reduce compilation time using pickle by saving trained model?,"<p>This is a sentiment analysis code and every time I change my input, it takes 10-15 mins to compile. What are the ways in which I can reduce it? Using pickle by saving the classifier or any other method is preferable?
Other functions are not mentioned here.</p>

<pre><code>inpTweets = csv.reader(open('training_neatfile_4.csv', 'r' ,encoding='ISO-8859-1'), delimiter=',')
stopWords = getStopWordList('stopwords.txt')
count = 0;
featureList = []
tweets = []
for row in inpTweets:
    sentiment = row[0]
    tweet = row[1]
    processedTweet = processTweet(tweet)
    featureVector = getFeatureVector(processedTweet, stopWords)
    featureList.extend(featureVector)
    tweets.append((featureVector, sentiment));
#end loop

# Remove featureList duplicates
featureList = list(set(featureList))

# Generate the training set
training_set = nltk.classify.util.apply_features(extract_features, tweets)

# Train the Naive Bayes classifier
nb_classifier = nltk.NaiveBayesClassifier.train(training_set)

# Test the classifier
testTweet = 'He is a brainless kid'
processedTestTweet = processTweet(testTweet)
sentiment = nb_classifier.classify(extract_features(getFeatureVector(processedTestTweet, stopWords)))
print (""testTweet = %s, sentiment = %s\n"" % (testTweet, sentiment))
</code></pre>
","python, machine-learning, nltk, text-classification, naivebayes","<p>Training a NaiveBayesClassifier(or any) takes a lot of time(depends on the feeding of training data), it becomes easier if you save the object of classifier(NBClassifier) once you've trained it to save time by omitting re-training.</p>

<p>Following is the way to save objects using pickle, you may use it in your code to save train or load the Classifier.</p>

<pre><code>import pickle
pickle.dump(object, file)
</code></pre>

<p>You may save NaiveBayesClassifier by saving its object(nb_classifier) as following.</p>

<pre><code>with open('model.pkl', 'wb') as nb_classifier_model:
    pickle.dump(nb_classifier, nb_classifier_model)
</code></pre>

<p>Then, you can retrieve it as:</p>

<pre><code>with open('model.pkl', 'rb') as nb_classifier_model:
    nb_classifier = pickle.load(nb_classifier_model)
</code></pre>

<p>That is how you may achieve your goal by using accordingly.</p>

<p>Hope it helps!</p>
",4,2,518,2017-03-17 04:46:29,https://stackoverflow.com/questions/42849402/how-to-reduce-compilation-time-using-pickle-by-saving-trained-model
Saving and Loading Trained Stanford classifier in java,"<p>I have a dataset of 1 million labelled sentences and using it for finding sentiment through Maximum Entropy. I am using Stanford Classifier for the same:-</p>

<pre><code>public class MaximumEntropy {

static ColumnDataClassifier cdc;

public static float calMaxEntropySentiment(String text) {
    initializeProperties();
    float sentiment = (getMaxEntropySentiment(text));
    return sentiment;
}

public static void initializeProperties() {
    cdc = new ColumnDataClassifier(
            ""\\stanford-classifier-2016-10-31\\properties.prop"");
}

public static int getMaxEntropySentiment(String tweet) {

    String filteredTweet = TwitterUtils.filterTweet(tweet);
    System.out.println(""Reading training file"");
    Classifier&lt;String, String&gt; cl = cdc.makeClassifier(cdc.readTrainingExamples(
            ""\\stanford-classifier-2016-10-31\\labelled_sentences.txt""));

    Datum&lt;String, String&gt; d = cdc.makeDatumFromLine(filteredTweet);
    System.out.println(filteredTweet + ""  ==&gt;  "" + cl.classOf(d) + "" "" + cl.scoresOf(d));
    // System.out.println(""Class score is: "" +
    // cl.scoresOf(d).getCount(cl.classOf(d)));
    if (cl.classOf(d) == ""0"") {
        return 0;
    } else {
        return 4;
    }
}
}
</code></pre>

<p>My data is labelled 0 or 1. Now for each tweet the whole dataset is being read and it is taking a lot of time considering the size of dataset.
My query is that is there any way to first train the classifier and then load it when a tweet's sentiment is to be found. I think this approach will take less time. Correct me if I am wrong. 
The following link provides this but there is nothing for JAVA API.
<a href=""https://nlp.stanford.edu/wiki/Software/Classifier#Saving_and_loading_this_classifier"" rel=""nofollow noreferrer"">Saving and Loading Classifier</a>
Any help would be appreciated.</p>
","java, twitter, stanford-nlp, text-classification, maxent","<p>Yes; the easiest way to do this is using Java's default serialization mechanism to serialize a classifier. A useful helper here is the <code>IOUtils</code> class:</p>

<pre><code>IOUtils.writeObjectToFile(classifier, ""/path/to/file"");
</code></pre>

<p>To read the classifier:</p>

<pre><code>Classifier&lt;String, String&gt; cl = IOUtils.readObjectFromFile(new File(""/path/to/file"");
</code></pre>
",2,1,212,2017-03-28 06:22:25,https://stackoverflow.com/questions/43061992/saving-and-loading-trained-stanford-classifier-in-java
Can I retrain an old model with new data using TensorFlow?,"<p>I am new to TensorFlow and I am just trying to see if my idea is even possible.</p>

<p>I have trained a model with multi class classifier. Now I can classify a sentence in input, but I would like to change the result of CNN, for example, to improve the score of classification or change the classification.</p>

<p>I want to try to train just a single sentence with its class on a trained model, is this possible?</p>
","tensorflow, classification, text-classification, training-data","<p>If I understand your question correctly, you are trying to reload a previously trained model either to run it through further iterations, test it on a new sentence, or fine tune the model a bit. If this is the case, yes you can do this. Look into saving and restoring models (<a href=""https://www.tensorflow.org/api_guides/python/state_ops#Saving_and_Restoring_Variables"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_guides/python/state_ops#Saving_and_Restoring_Variables</a>).</p>

<p>To give you a rough outline, when you initially train your model, after setting up the network architecture, set up a saver:</p>

<pre><code>trainable_var = tf.trainable_variables()
sess = tf.Session()
saver = tf.train.Saver()
sess.run(tf.global_variables_initializer

# Run/train your model until some completion criteria is reached
#....
#....

saver.save(sess, 'model.ckpt')
</code></pre>

<p>Now, to reload your model:</p>

<pre><code>saver = tf.train.import_meta_graph('model.ckpt.meta')
saver.restore('model.ckpt')
#Note: if you have already defined all variables before restoring the model, import_meta_graph is not necessary
</code></pre>

<p>This will give you access to all the trained variables and you can now feed in whatever new sentence you have. Hope this helps.</p>
",0,4,2247,2017-03-29 09:15:25,https://stackoverflow.com/questions/43089267/can-i-retrain-an-old-model-with-new-data-using-tensorflow
Pandas dataframe indexes,"<p>Lets say I am working with a dataset which has 10 columns. 
Now, the Label column for my 'Y' is 1. 
How do I set my X and Y. 
This is what I have done so far. </p>

<pre><code>array = dataframe. values
X = array[:,0:2:32]   #I know this isn't the right way to do this.  
Y = array[:,1]
</code></pre>

<p>Yes, we can assign names to columns and omit the column with name=label, but I am trying to find a straight forward approach based on indexes. Any leads?</p>
","python, pandas, text-classification","<p>You can do this:</p>

<pre><code>Y = df.loc[:,1].values
cols = list(df.loc[:,:0]) + list(df.loc[:,2:32])
X = df[cols].values
</code></pre>
",0,0,55,2017-03-29 19:39:58,https://stackoverflow.com/questions/43102707/pandas-dataframe-indexes
StringToWordVectore error in java for text classification,"<p>1- I try to apply StringToWordVector filter into text by java coding, but it does not work. The output of the filter is incorrect.
the code that I used:</p>

<pre><code>Instances instances = source.getDataSet();
instances.setClassIndex(instances.numAttributes()-1);
StringToWordVector stwv = new StringToWordVector();
//Splits a string into an n-gram with min and max grams.
NGramTokenizer tokenizer = new NGramTokenizer();
tokenizer.setNGramMinSize(1);
tokenizer.setNGramMaxSize(1);
tokenizer.setDelimiters("" \r\n\t.,;:'\""()?!'"");
stwv.setTokenizer(tokenizer);

stwv.setDoNotOperateOnPerClassBasis(true);
stwv.setOutputWordCounts(true);
stwv.setDictionaryFileToSaveTo(new File(""/forEclips/RandomForset/DictionaryFile.txt""));
//------------------------
stwv.setInputFormat(instances);
// Apply the filter
Instances dataFiltered = weka.filters.Filter.useFilter(instances, stwv);
System.out.println(""\n\nFiltered data:\n\n"" + dataFiltered.toString() );
</code></pre>

<p>The Output looks like:</p>

<pre><code>@relation 'DIMS-weka.filters.unsupervised.attribute.StringToWordVector-R1-W10-prune-rate-1.0-C-N0-stemmerweka.core.stemmers.NullStemmer-stopwords-handlerweka.core.stopwords.Null-M1-O-tokenizerweka.core.tokenizers.NGramTokenizer -max 1 -min 1 -delimiters \"" \\r\\n\\t.,;:\\\'\\\""()?!\\\'\""-dictionary/forEclips/RandomForset/DictionaryFile.txt 
@attribute class {Di,MS}
@attribute اشبو numeric
@attribute اللي numeric
@attribute المويه numeric
@attribute النار numeric
@attribute تشوفوا numeric
@attribute تعرفون numeric
@attribute حبايبي numeric
@attribute حجازي numeric
@attribute خلال numeric
@attribute دي numeric
@attribute زي numeric
@attribute سيدي numeric
@attribute صور numeric
@attribute في numeric
@attribute كتير numeric
@attribute كتييير numeric
@attribute كتيييير numeric
@attribute كده numeric
@attribute مثل numeric
@attribute من numeric
@attribute مو numeric
@attribute هل numeric
@attribute وعيشوا numeric
@attribute وقدود، numeric
@attribute يا numeric
@attribute يده numeric

@data
{0 MS,9 1,13 3,20 2}
{0 MS,9 3,13 1,20 2}
{0 MS,6 1,22 1}
{5 1,16 1,17 1,23 1,24 1}
{2 2,3 1,4 1,8 1,11 1,14 2,19 1,21 1,26 2}
{1 1,7 1,10 1,12 1,15 1,18 1,20 1,25 1}`
</code></pre>

<p>We can see here it does not put the class at the end in the section @attribute.In addition, in section @data, the first three instances, the class in the first, while the last three, do not any class and class's id.
It should be at the end the class with it is id.</p>

<p>2- Also, I want to add an attribute (newattribut) with type numeric for all the instances that I have with the same weight(value =44).<br>
that mean the section @attribute will look like:</p>

<pre><code>   @attribute اشبو numeric
   @attribute اللي numeric
   @attribute المويه numeric
   @attribute النار numeric
   @attribute تشوفوا numeric
   @attribute تعرفون numeric
   @attribute حبايبي numeric
   @attribute حجازي numeric
   @attribute خلال numeric
   @attribute دي numeric
   @attribute زي numeric
   @attribute سيدي numeric
   @attribute صور numeric
   @attribute في numeric
   @attribute كتير numeric
   @attribute كتييير numeric
   @attribute كتيييير numeric
   @attribute كده numeric
   @attribute مثل numeric
   @attribute من numeric
   @attribute مو numeric
   @attribute هل numeric
   @attribute وعيشوا numeric
   @attribute وقدود، numeric
   @attribute يا numeric
   @attribute يده numeric
   @attribute newattribute numeric
   @attribute class {Di,MS}


   @data
   {8 1,12 3,19 2,26 44,27 MS}
   {8 3,12 1,19 2,26 44, 27 MS}
   {5 1,21 1,26 44,27 MS}
   {4 1,15 1,16 1,22 1,23 1,26 44,27 Di}
   {1 2,2 1,3 1,7 1,10 1,13 2,18 1,20 1,25 2,26 44,27 Di}
   {0 1,6 1,9 1,11 1,14 1,17 1,19 1,24 1,26 44,27 Di}
</code></pre>

<p>3- I want to use this training data to classify the text by Naive baise, Random Forest, and SVM. How to build Cross-validation for training and testing data by using weka library in java. I try to use SVM by adding Libsvm in java building path put it gives me an error.</p>

<p>Regards;</p>
","java, weka, random-forest, libsvm, text-classification","<p>I found these websites very useful to do text classification with filter StringToWordVector.
<a href=""http://www.uky.edu/~nyu222/tutorials/Weka.htm"" rel=""nofollow noreferrer"">http://www.uky.edu/~nyu222/tutorials/Weka.htm</a>
<a href=""https://www.youtube.com/watch?v=Tggs3Bd3ojQ&amp;list=PLm4W7_iX_v4OMSgc8xowC2h70s-unJKCp&amp;index=11"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Tggs3Bd3ojQ&amp;list=PLm4W7_iX_v4OMSgc8xowC2h70s-unJKCp&amp;index=11</a></p>
",0,0,194,2017-04-03 03:59:48,https://stackoverflow.com/questions/43176300/stringtowordvectore-error-in-java-for-text-classification
LSTM Error python keras,"<p>Good morning, I'm trying to train lstm to classify spam and not spam, I came across the following error:</p>

<p><code>ValueError: Input 0 is incompatible with layer lstm_1: expected ndim = 3, found ndim = 4</code></p>

<p>Can someone help me understand where the problem is?</p>

<p>my code:</p>

<pre><code>import sys
import pandas as pd
import numpy as np
import math
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.feature_extraction.text import CountVectorizer

if __name__ == ""__main__"":
    np.random.seed(7)

    with open('SMSSpamCollection') as file:
        dataset = [[x.split('\t')[0],x.split('\t')[1]] for x in [line.strip() for line in file]]

    data   = np.array([dat[1] for dat in dataset])
    labels = np.array([dat[0] for dat in dataset])

    dataVectorizer = CountVectorizer(analyzer = ""word"",  
                             tokenizer = None,   
                             preprocessor = None,
                             stop_words = None,  
                             max_features = 5000) 
    labelVectorizer = CountVectorizer(analyzer = ""word"",  
                             tokenizer = None,   
                             preprocessor = None,
                             stop_words = None,  
                             max_features = 5000) 

    data = dataVectorizer.fit_transform(data).toarray()
    labels = labelVectorizer.fit_transform(labels).toarray()
    vocab = labelVectorizer.get_feature_names()

    print(vocab)
    print(data)
    print(labels)

    data = np.reshape(data, (data.shape[0], 1, data.shape[1]))

    input_dim = data.shape
    tam = len(data[0])

    print(data.shape)
    print(tam)

    model = Sequential()
    model.add(LSTM(tam, input_shape=input_dim))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(data, labels, epochs=100, batch_size=1, verbose=2)
</code></pre>

<p>I tried adding another position in the data array but also with no result
my file <code>SMSSpamCollection</code></p>

<pre><code>ham Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...
ham Ok lar... Joking wif u oni...
spam    Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's
ham U dun say so early hor... U c already then say...
ham Nah I don't think he goes to usf, he lives around here though
spam    FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv
ham Even my brother is not like to speak with me. They treat me like aids patent.
...
</code></pre>

<p>thanks</p>
","python, classification, keras, lstm, text-classification","<p>The problem lies in fact that you are adding an additional dimension connected with samples. Try:</p>

<pre><code>input_dim = (data.shape[1], data.shape[2])
</code></pre>

<p>This should work.</p>
",1,1,230,2017-04-03 15:24:38,https://stackoverflow.com/questions/43188504/lstm-error-python-keras
How to assign an new observation to existing Kmeans clusters based on nearest cluster centriod logic in python?,"<p>I used the below code to create k-means clusters using Scikit learn.</p>

<pre><code>kmean = KMeans(n_clusters=nclusters,n_jobs=-1,random_state=2376,max_iter=1000,n_init=1000,algorithm='full',init='k-means++')

kmean_fit = kmean.fit(clus_data)
</code></pre>

<p>I also have saved the centroids using <code>kmean_fit.cluster_centers_</code> </p>

<p>I then pickled the K means object.</p>

<pre><code>filename = pickle_path+'\\'+'_kmean_fit.sav'
pickle.dump(kmean_fit, open(filename, 'wb'))
</code></pre>

<p>So that I can load the same kmeans pickle object and apply it to new data when it comes, using <code>kmean_fit.predict().</code> </p>

<p><strong>Questions :</strong> </p>

<ol>
<li><p>Will the approach of loading kmeans pickle object and applying
<code>kmean_fit.predict()</code> allow me to assign the <strong><em>new observation to
existing clusters</em></strong> based on centroid of the existing clusters? Does this approach just recluster from scratch on the new data?</p></li>
<li><p>If this method wont work how to assign the new observation to
    existing clusters given that I already have saved the cluster
    centriods using efficent python code?</p></li>
</ol>

<p>PS: I know building a classifer using existing clusters as dependent variable is another way but I dont want to do that because of time crunch.</p>
","python, scikit-learn, cluster-analysis, k-means, text-classification","<p>Yes. Whether the <code>sklearn.cluster.KMeans</code> object is pickled or not (<strong>if you un-pickle it correctly, you'll be dealing with the ""same"" original object</strong>) does not affect that you can use the <code>predict</code> method to cluster a new observation. </p>

<p>An example:</p>

<pre><code>from sklearn.cluster import KMeans
from sklearn.externals import joblib

model = KMeans(n_clusters = 2, random_state = 100)
X = [[0,0,1,0], [1,0,0,1], [0,0,0,1],[1,1,1,0],[0,0,0,0]]
model.fit(X)
</code></pre>

<p>Out:</p>

<pre><code>KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=2, n_init=10,
    n_jobs=1, precompute_distances='auto', random_state=100, tol=0.0001,
    verbose=0)
</code></pre>

<p>Continue:</p>

<pre><code>joblib.dump(model, 'model.pkl')  
model_loaded = joblib.load('model.pkl')

model_loaded
</code></pre>

<p>Out: </p>

<pre><code>KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=2, n_init=10,
    n_jobs=1, precompute_distances='auto', random_state=100, tol=0.0001,
    verbose=0)
</code></pre>

<p>See how the <code>n_clusters</code> and <code>random_state</code> parameters are the same between the <code>model</code> and <code>model_new</code> objects? You're good to go. </p>

<p>Predict with the ""new"" model:</p>

<pre><code>model_loaded.predict([0,0,0,0])

Out[64]: array([0])
</code></pre>
",6,2,10429,2017-04-06 14:22:39,https://stackoverflow.com/questions/43257975/how-to-assign-an-new-observation-to-existing-kmeans-clusters-based-on-nearest-cl
xml files from folder into list,"<p>I'm pretty new in programming and it is the first time I use xml, but for class I'm doing a gender classification project with a dataset of Blogs. 
I have a folder which consists of xml files. Now I need to make a list of names of the files there.
Then I should be able to run through the list with a loop and open each file containing XML and get out of it what I want (ex. Text and class) and then store that in another variable, like adding it to a list or dictionary.</p>

<p>I tried something, but it isn't right and I'm kind of stuck. Can someone help me? This is wat I have so far:</p>

<pre><code>path ='\\Users\\name\\directory\\folder'
dir = os.listdir( path )
def select_files_in_folder(dir, ext):
    for filename in os.listdir(path):
        fullname= os.path.join(path, filename)
        tree = ET.parse(fullname)
    for elem in doc.findall('gender'):
        print(elem.get('gender'), elem.text)
</code></pre>
","python, xml, nlp, text-classification","<p>If you want to build a list of all the xml files in a given directory you can do the following </p>

<pre><code>def get_xml_files(path):
    xml_list = []
    for filename in os.listdir(path):
        if filename.endswith("".xml""):
            xml_list.append(os.path.join(path, filename))
    return xml_list
</code></pre>

<p>just keep in mind that this is not recursive through the folders and it's just assuming that the xml files finish with .xml. </p>

<p>EDIT :</p>

<p>Parsing xml is highlly dependent of the library you'll be using. From your code I guess you're using xml.etree.ElementTree (keep in mind this lib is not safe against maliciously constructed data). </p>

<pre><code>def get_xml_data(list):
    data = []
    for filename in list :
        root = ET.parse(filename)
        data = [ text for text in root.findall(""whatever you want to get"") ]
    return data
</code></pre>
",0,1,4471,2017-04-10 14:50:06,https://stackoverflow.com/questions/43326360/xml-files-from-folder-into-list
"How to remove HTML, Urls from with Python","<p>I have this list of xml files. Now I have to filter some labels out of it. The problem is the text, there is a lot of html mark up and urls in it and I need plain text. I would like to remove this elements in a loop and then append the cleaned text to my new list. This is what I have so far.</p>

<pre><code>    data = []
    for conv in root.findall('./conversations/conversation'):
        pattern = re.compile( r'!\b(((ht|f)tp(s?))\://)?(www.|[a-z].)[a-z0-9\-\.]+\.)(\:[0-9]+)*(/($|[a-z0-9\.\,\;\?\\\\\\\+&amp;amp;%\$#\=~_\-]+))*\b!i')
        if pattern.search(conv.text):
           re.sub(pattern, ' ')
           data.append(conv.text)    
</code></pre>

<p>I can't find the right regex to remove things like this <code>br /&gt;;&lt;br /&gt;</code> and urls like this: <code>http://neocash43.blog.com/2011/07/26/psp-sport-assessment-neopets-the-wand-of-wishing/&lt;/a&gt;</code></p>

<p>Second problem is that with this xml root structure, I don't now how to append the cleaned conversation text to my new list.</p>
","python, html, regex, xml, text-classification","<p>You could try <a href=""http://pyparsing.wikispaces.com/file/view/htmlStripper.py/591745692/htmlStripper.py"" rel=""nofollow noreferrer"">http://pyparsing.wikispaces.com/file/view/htmlStripper.py/591745692/htmlStripper.py</a> which uses the pyparsing library. I just used this script on my machine with Python 3.4.</p>
",1,0,709,2017-04-12 15:02:01,https://stackoverflow.com/questions/43373207/how-to-remove-html-urls-from-with-python
Which decision_function_shape for sklearn.svm.SVC when using OneVsRestClassifier?,"<p>I am doing multi-label classification where I am trying to predict correct tags to questions:</p>

<p>(X = questions, y = list of tags for each question from X).</p>

<p>I am wondering, which <code>decision_function_shape</code> for <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"" rel=""nofollow noreferrer""><code>sklearn.svm.SVC</code></a> should be be used with <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html"" rel=""nofollow noreferrer""><code>OneVsRestClassifier</code></a>?</p>

<p>From docs we can read that <code>decision_function_shape</code> can have two values <code>'ovo'</code> and <code>'ovr'</code>:</p>

<blockquote>
  <p><strong>decision_function_shape</strong> : ‘ovo’, ‘ovr’ or None, default=None</p>
  
  <p>Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) as all other classifiers, or the original
  one-vs-one (‘ovo’) decision function of libsvm which has shape
  (n_samples, n_classes * (n_classes - 1) / 2). The default of None will
  currently behave as ‘ovo’ for backward compatibility and raise a
  deprecation warning, but will change ‘ovr’ in 0.19.</p>
</blockquote>

<p>But I still don't understand what is the difference between:</p>

<pre><code># First decision_function_shape set to 'ovo'
estim = OneVsRestClassifier(SVC(kernel='linear', decision_function_shape ='ovo'))

# Second decision_function_shape set to 'ovr'
estim = OneVsRestClassifier(SVC(kernel='linear', decision_function_shape ='ovr'))
</code></pre>

<p>Which <code>decision_function_shape</code> should be used for <a href=""http://scikit-learn.org/stable/modules/multiclass.html#multiclass-and-multilabel-algorithms"" rel=""nofollow noreferrer"">multi-label classification</a> problem?</p>

<p><strong>EDIT:</strong> <a href=""https://stackoverflow.com/questions/39604468/what-is-the-difference-between-onevsrestclassifier-with-svc-and-svc-with-decisio?rq=1"">Question</a> asking a similar thing with no answer.</p>
","python, scikit-learn, svm, text-classification, multilabel-classification","<p>I think the question of which should be used is best left up to a situational. That could easily be a part of your GridSearch. But just intuitively I would feel that as far as differences go you are going to be doing the same thing. Here is my reasoning:</p>

<p><code>OneVsRestClassifier</code> is designed to model each class against all of the other classes independently, and create a classifier for each situation. The way I understand this process is that <code>OneVsRestClassifier</code> grabs a class, and creates a binary label for whether a point is or isn't that class. Then this labelling gets fed into whatever estimator you have chosen to use. I believe the confusion comes in in that <code>SVC</code> also allows you to make this same choice, but in effect with this implementation the choice will not matter because you will always only be feeding two classes into the <code>SVC</code>.  </p>

<p>And here is an example:</p>

<pre><code>from sklearn.datasets import load_iris
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC

data = load_iris()

X, y = data.data, data.target
estim1 = OneVsRestClassifier(SVC(kernel='linear', decision_function_shape='ovo'))
estim1.fit(X,y)

estim2 = OneVsRestClassifier(SVC(kernel='linear', decision_function_shape='ovr'))
estim2.fit(X,y)

print(estim1.coef_ == estim2.coef_)
array([[ True,  True,  True,  True],
       [ True,  True,  True,  True],
       [ True,  True,  True,  True]], dtype=bool)
</code></pre>

<p>So you can see the coefficients are all equal for all three estimators built by the two models. Granted this dataset only has 150 samples and 3 classes so it is possible these results could be different for a more complex dataset, but it's a simple proof of concept. </p>
",4,6,16616,2017-04-19 20:26:07,https://stackoverflow.com/questions/43505451/which-decision-function-shape-for-sklearn-svm-svc-when-using-onevsrestclassifier
SVM value error text classification,"<p>I've gone through Scikit-SVM tutorial, and written the code to train and test. But I'm facing an issue with prediction, where it says, 'shape should be equal to training shape'. Here is the code below.</p>

<p>EDIT1: Sample Data</p>

<pre><code>ERROR_DESC  CLASSIFICATION_LABEL
ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: ORA-01017: invalid username/password; logon denied at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:389) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:382) at oracle.jdbc.driver.T4CTTIfun.processError(T4CTTIfun.java:675) at oracle.jdbc.driver.T4CTTIoauthenticate.processError(T4CTTIoauthenticate.java:448) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)  --  ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1095),INCORRECT_CREDENTIALS-Database-RAISE_SERVICENOW_DB_CREDENTIALS
A client error (ThrottlingException) occurred when calling the DescribeCluster operation: Rate exceeded   fetching DNS name  --  ERROR manager.SqlManager: Error executing statement: java.sql.SQLRecoverableException: IO Error: The Network Adapter could not establish the connection at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:489)  --  ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1095), NETWORK_ERROR-Database-RAISE_SERVICENOW_DB_CONNECTION
</code></pre>

<p>I also found a similar question on SO: <a href=""https://stackoverflow.com/questions/21173010/error-in-testing-svm-classifier-for-text-classification"">Link</a> I tried to use transform, but it throws a different error.</p>

<pre><code>import pandas as pd
​
# data paths
data_in = '../data/input/file.csv'
​
df_data = pd.read_csv(data_in)

# lower case all columns for uniformity
df_data.columns = map(str.lower, df_data.columns)
# lower case all data for uniformity
df_data = df_data.apply(lambda x: x.astype(str).str.lower())

labels = df_data['classification_label'].unique()

label_map = {}
i = 1
for label in labels:
    label_map[label] = i
    i += 1
​    

# apply map to classification_label column 
# df_data['classification_label'] = df_data['classification_label'].map(lambda s: label_map.get(s) if s in label_map else s)

# select features and labels
df_final = df_data[['error_desc', 'classification_label']]


from sklearn.feature_extraction.text import TfidfVectorizer
v = TfidfVectorizer()
X = v.fit_transform(df_final['error_desc'])
y = df_final['classification_label']


from sklearn.cross_validation import train_test_split
​
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)


from sklearn.svm import SVC
​
def train_svm(X, y):
    """"""
    Create and train the Support Vector Machine.
    """"""
    svm = SVC(C=1000000.0, gamma='auto', kernel='rbf')
    svm.fit(X, y)
    return svm



svm = train_svm(X_train, y_train)



from sklearn.metrics import confusion_matrix
​
# Make an array of predictions on the test set
pred = svm.predict(X_test)
​
# Output the hit-rate and the confusion matrix for each model
print(svm.score(X_test, y_test))
print(confusion_matrix(pred, y_test))



0.777777777778
[[0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 2 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 3 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0 0 0 0 0]
 [1 0 0 0 0 1 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 3 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 1]]



pred_x = """"""ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: ORA-01017: invalid username/password; logon denied at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:389) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:382) at oracle.jdbc.driver.T4CTTIfun.processError(T4CTTIfun.java:675) at oracle.jdbc.driver.T4CTTIoauthenticate.processError(T4CTTIoauthenticate.java:448) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)  --  ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1095)""""""
​


pred_x_vector = TfidfVectorizer().fit_transform([pred_x])


svm.predict(pred_x_vector)



---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-86-130bf7f79131&gt; in &lt;module&gt;()
----&gt; 1 svm.predict(pred_x_vector)

/Users/userOne/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc in predict(self, X)
    571             Class labels for samples in X.
    572         """"""
--&gt; 573         y = super(BaseSVC, self).predict(X)
    574         return self.classes_.take(np.asarray(y, dtype=np.intp))
    575 

/Users/userOne/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc in predict(self, X)
    308         y_pred : array, shape (n_samples,)
    309         """"""
--&gt; 310         X = self._validate_for_predict(X)
    311         predict = self._sparse_predict if self._sparse else self._dense_predict
    312         return predict(X)

/Users/userOne/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc in _validate_for_predict(self, X)
    477             raise ValueError(""X.shape[1] = %d should be equal to %d, ""
    478                              ""the number of features at training time"" %
--&gt; 479                              (n_features, self.shape_fit_[1]))
    480         return X
    481 

ValueError: X.shape[1] = 49 should be equal to 554, the number of features at training time
</code></pre>
","python, scikit-learn, text-classification","<pre><code>import pandas as pd

df_data = pd.DataFrame([['ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: ORA-01017: invalid username/password; logon denied at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:389) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:382) at oracle.jdbc.driver.T4CTTIfun.processError(T4CTTIfun.java:675) at oracle.jdbc.driver.T4CTTIoauthenticate.processError(T4CTTIoauthenticate.java:448) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)  --  ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1095)','INCORRECT_CREDENTIALS-Database-RAISE_SERVICENOW_DB_CREDENTIALS'],\
['A client error (ThrottlingException) occurred when calling the DescribeCluster operation: Rate exceeded   fetching DNS name  --  ERROR manager.SqlManager: Error executing statement: java.sql.SQLRecoverableException: IO Error: The Network Adapter could not establish the connection at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:489)  --  ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1095)', 'NETWORK_ERROR-Database-RAISE_SERVICENOW_DB_CONNECTION']])

df_data.columns = ['ERROR_DESC' , 'CLASSIFICATION_LABEL']

# lower case all columns for uniformity
df_data.columns = map(str.lower, df_data.columns)

# select features and labels
df_final = df_data[['error_desc', 'classification_label']]

from sklearn.feature_extraction.text import TfidfVectorizer
v = TfidfVectorizer()
X = v.fit_transform(df_final['error_desc'])
y = df_final['classification_label']
orig_vocab = v.get_feature_names() #save the orig vocabulary

from sklearn.svm import SVC

def train_svm(X, y):
    """"""
    Create and train the Support Vector Machine.
    """"""
    svm = SVC(C=1000000.0, gamma='auto', kernel='rbf')
    svm.fit(X, y.values)
    return svm

svm = train_svm(X, y)

pred_x = """"""ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: ORA-01017: invalid username/password; logon denied at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:389) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:382) at oracle.jdbc.driver.T4CTTIfun.processError(T4CTTIfun.java:675) at oracle.jdbc.driver.T4CTTIoauthenticate.processError(T4CTTIoauthenticate.java:448) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)  --  ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1095)""""""
pred_x_vector =   TfidfVectorizer(vocabulary=orig_vocab).fit_transform([pred_x]) #vectorize by original vocabulary

svm.predict(pred_x_vector)
</code></pre>

<p>Explanation:</p>

<p>A trained model can only predict on vectors the same size as the vectors it trained upon. Therefore in cases of vectorizing text by bag of words methods you must keep the original vocabulary of the train sample in order to create vectors according to the same vocabulary.</p>

<p>Remarks:</p>

<ol>
<li><p>Used only two samples, so no train test split (just trained on both) and therefore no cross validation</p></li>
<li><p>no need to lower case the data sklearn vectorizer does that for you.  </p></li>
</ol>
",0,0,361,2017-04-20 21:41:40,https://stackoverflow.com/questions/43530398/svm-value-error-text-classification
"Improving on the basic, existing GloVe model","<p>I am using GloVe as part of my research. I've downloaded the models from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">here</a>. I've been using GloVe for sentence classification. The sentences I'm classifying are specific to a particular domain, say some STEM subject. However, since the existing GloVe models are trained on a general corpus, they may not yield the best results for my particular task. </p>

<p>So my question is, how would I go about loading the retrained model and just retraining it a little more on my own corpus to learn the semantics of my corpus as well? There would be merit in doing this were it possible. </p>
","nlp, text-classification, glove","<p>After a little digging, I found <a href=""https://github.com/stanfordnlp/GloVe/issues/62"" rel=""nofollow noreferrer"">this issue</a> on the git repo. Someone suggested the following:</p>

<blockquote>
  <p>Yeah, this is not going to work well due to the optimization setup. But what you can do is train GloVe vectors on your own corpus and then concatenate those with the pretrained GloVe vectors for use in your end application.</p>
</blockquote>

<p>So that answers that.</p>
",2,7,2345,2017-04-25 18:15:28,https://stackoverflow.com/questions/43618145/improving-on-the-basic-existing-glove-model
compare text in object javascript,"<p>I want so set data in an object with 2 label positive and negative and I want to set word into the object. I tried this code:</p>

<pre><code>function cok(_class, doc) {
    var vocab = {
        po: {
            wd: ""good job""
        },
        ne: {
            wd: ""nice job""
        }
    }
    doc = doc.split(' ')
    for (var _word of doc) {
        console.log(vocab[_class])
    }
}
cok('po', 'baik good');
cok('ne', 'jelek nakal');
</code></pre>

<p>I have set the positive and negative label but I'm unsure of how to store the value and loop the word.</p>
","javascript, node.js, classification, text-classification, naivebayes","<p>loop your array like</p>

<pre><code>array.forEach(function(entry) {
    entry.po = ""positive"";
    entry.ne = ""negative"";
});
</code></pre>

<p>This will loop and add values into each object</p>
",0,0,86,2017-05-02 09:37:52,https://stackoverflow.com/questions/43734616/compare-text-in-object-javascript
classify text with object javascript naive bayes classifier,"<p>I will add a wordInDoc object (word: num) if the word is in the object vocab [positive], I try with equal to but fail. Why? </p>

<p>this is my code </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>var nbayes = function(_class, docs) {
  var vocab = {
    positif: {
      wd: ['baik', 'pintar']
    },
    negatif: {
      wd: ['buruk', 'jelek']
    }
  }
  var wordInDoc = {}
  var sumDocs = 0;
  docs = docs.split(' ')
  var wd = 'wd'
  for (var word of docs) {
    if (word in vocab[_class][wd]) {
      var delta = 1
      wordInDoc[word] = 0
      wordInDoc[word] += delta
      sumDocs++
    }
    console.log(wordInDoc, sumDocs)
  }

};

nbayes('positif', 'baik dan rajin')
nbayes('negatif', 'nakal dan bodoh')</code></pre>
</div>
</div>
</p>
","javascript, text-classification, naivebayes","<p>Is this a solution you were looking for?</p>

<p>Loop through the array 'docs' then check for the index of matching in 'vocab[_class][wd]'.</p>

<p>Some other validation should be done for non existent classes'_class'.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>var nbayes = function(_class, docs) {
  var wordInDoc = {};
  var sumDocs = 0;
  var wd = 'wd';
  var word;
  var vocab = {
    positif: {
      wd: ['baik', 'pintar']
    },
    negatif: {
      wd: ['buruk', 'jelek']
    }
  }
  
  docs = docs.split(' ');
  
  for (var i = 0;i &lt; docs.length; i++) {
    word = docs[i];
    if (vocab[_class] &amp;&amp; vocab[_class][wd].indexOf(word) !== -1) {
      var delta = 1;
      wordInDoc[word] = 0;
      wordInDoc[word] += delta;
      sumDocs++;
    }
    console.log(wordInDoc, sumDocs)
  }

};

nbayes('positif', 'baik dan rajin')
nbayes('negatif', 'nakal dan bodoh')</code></pre>
</div>
</div>
</p>
",1,1,431,2017-05-03 11:07:51,https://stackoverflow.com/questions/43758317/classify-text-with-object-javascript-naive-bayes-classifier
Word2vec classification and clustering tensorflow,"<p>I am trying to cluster some sentences using similarity (maybe cosine) and then maybe use a classifier to put text in predefined classes. </p>

<p>My idea is to use tensorflow to generate the word embedding then average them for each sentence. Next use a clustering/classification algorithm.</p>

<p>Does tensorflow provide ready to use word2vec generation algorithm?</p>

<p>Would a bag of words model generate a good output?</p>
","tensorflow, nlp, word2vec, text-classification","<ul>
<li><p><strong>No</strong>, tensorflow does not provide a <strong>ready-to-use word2vec</strong> but it does have <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">a tutorial on word2vec</a>.</p></li>
<li><p><strong>Yes</strong>, a bag of words can generate surprisingly good output (but not <em>state-of-the-art</em>), and has the benefit of being <strong>amazingly faster</strong>.  I have a small amount of data (tens of thousands of sentences) and have achieved F1 scores of >0.90 for classification.</p></li>
</ul>
",1,0,947,2017-05-04 14:17:45,https://stackoverflow.com/questions/43785438/word2vec-classification-and-clustering-tensorflow
iab.taxonome.org error code -5,"<p>i'm trying to use iab.taxonome.org service to classify texts, and get error response -5 (text too short)</p>

<p>Here is what i'm sending to the service:<br>
<a href=""https://rest.taxonome.org/v1/taxono?me=A"" rel=""nofollow noreferrer"">https://rest.taxonome.org/v1/taxono?me=A</a> college basketball game at Allen Fieldhouse, in Lawrence, Kansas, the home of the Kansas Jayhawks
The history of basketball is traced back to a YMCA International Training School, known today as Springfield College, located in Springfield, Massachusetts&amp;token=[...MyKey...]&amp;ver=1</p>
","classification, categories, taxonomy, text-classification, custom-taxonomy","<p>Indeed I had the same issue. After clearing this with taxonome support team I figure out there is a requirement for at least 500 words per classification.
I have asked to add it to the API reference page.</p>

<p>Double checking and editing my answer: It is depends which framework is being used to send this data. In case you are implementing the client and not encoding the URL string it won't work for you (e.g. space = %20).
Check the API example here:
<a href=""https://iab.taxonome.org/api"" rel=""nofollow noreferrer"">https://iab.taxonome.org/api</a></p>
",1,1,43,2017-05-09 07:43:56,https://stackoverflow.com/questions/43864026/iab-taxonome-org-error-code-5
iab taxonomy API order of responses at https://iab.taxonome.org,"<p>I'm sending series of texts to be classified by <a href=""https://iab.taxonome.org"" rel=""nofollow noreferrer"">https://iab.taxonome.org</a> classification API, but since I work in node JS enviroment in a proper, async way, the responses get's out of sync with the requests. I can use recorsive calls, but this might overflow my stack. Any idea how to sync calls with classification responses?</p>
","node.js, classification, taxonomy, text-classification","<p>You might use the optional private ID to pass a ""cookie"" to identify the session.
From <a href=""https://iab.taxonome.org/api"" rel=""nofollow noreferrer"">https://iab.taxonome.org/api</a>
<strong><em>id optional  String  Request user defined identifier or empty string.</em></strong>
curl -i <a href=""https://rest.taxonome.org/v1/taxono?me=this%20is%20a%20text%20to%20be%20classified&amp;token=123e4567-e89b-12d3-a456-426655440000&amp;ver=1&amp;"" rel=""nofollow noreferrer"">https://rest.taxonome.org/v1/taxono?me=this%20is%20a%20text%20to%20be%20classified&amp;token=123e4567-e89b-12d3-a456-426655440000&amp;ver=1&amp;</a><strong>id=MyUserDefinedId</strong></p>
",1,1,73,2017-05-10 07:25:50,https://stackoverflow.com/questions/43886012/iab-taxonomy-api-order-of-responses-at-https-iab-taxonome-org
SKlearn SGD Partial Fit error: Number of features 378 does not match previous data 4598,"<p>I have pkl my classifier and opened in another notebook and try to do partial_fit on the classifier but received error Number of features 378 does not match previous data 4598.</p>

<pre><code>with open(""models/count_vect_Item Group.pkl"", 'r') as f:
 global count_vect_item_group
 count_vect_item_group = joblib.load(f)

with open(""models/model_Item Group.pkl"", 'r') as f:
 global model_predicted_item_group
 model_predicted_item_group = joblib.load(f)

count_matrix_X_train = count_vect_item_group.fit_transform(X_test)
X_train_tf_idf = tf_idf(count_matrix_X_train)

model_predicted_item_group.partial_fit(X_train_tf_idf, labels_test )
</code></pre>

<p>not able to train my classifier using new dataset.</p>
","python, machine-learning, scikit-learn, text-classification","<p>This error is because before you pickled your classifier, you trained it with 4598 features (number of columns in X) which in now 378 only. It should be equal to older features. </p>

<p>How you can do that, by calling only <code>count_vect_item_group.transform()</code>. You are now again calling the fit_transform() on the <code>count_vect_item_group</code> which then forgets about the previously learned data, and fits on the new data, hence the number of features found are lesser than before.</p>

<p>Change your code to:</p>

<pre><code>count_matrix_X_train = count_vect_item_group.transform(X_test)
X_train_tf_idf = tf_idf(count_matrix_X_train)

model_predicted_item_group.partial_fit(X_train_tf_idf, labels_test)
</code></pre>
",3,1,950,2017-05-17 19:38:05,https://stackoverflow.com/questions/44033443/sklearn-sgd-partial-fit-error-number-of-features-378-does-not-match-previous-da
Split text files into two groups - unsupervised learning,"<p>Imagine, you are a librarian and during time you
have classified a bunch of text files (approx 100)
with a general ambiguous keyword.</p>

<p>Every text file is actually a topic of keyword_meaning1
or a topic of keyword_meaning2.</p>

<p>Which unsupervised learning approach would you use,
to split the text files into two groups?</p>

<p>What precision (in percentage) of correct classification
can be achieved according to a number of text files?</p>

<p>Or can be somehow indicated in one group, that there is
a need of a librarian to check certain files, because
they may be classifed incorrectly?</p>
","text-classification, unsupervised-learning","<p>The easiest starting point would be to use a naive Bayes classifier. It's hard to speculate about the expected precision. You have to test it yourself. Just get a program for e-mail spam detection and try it out. For example, SpamBayes (<a href=""http://spambayes.sourceforge.net/"" rel=""nofollow noreferrer"">http://spambayes.sourceforge.net/</a>) is a quite good starting point and easily hackable. SpamBayes has a nice feature that it will label messages as ""unsure"" when there is no clear separation between two classes.</p>

<p>Edit: When you really want unsupervised clustering method, then perhaps something like Carrot2 (<a href=""http://project.carrot2.org/"" rel=""nofollow noreferrer"">http://project.carrot2.org/</a>) is more appropriate.  </p>
",1,0,84,2017-05-18 18:46:14,https://stackoverflow.com/questions/44055661/split-text-files-into-two-groups-unsupervised-learning
Text analysis with a mix of text &amp; categorical columns in R,"<p>I have a dataset of IT operations tickets with fields like Ticket No, Description, Category,SubCategory,Priority etc.</p>

<p>What I need to do is to use available data(except ticket no) to predict the ticket priority. Sample data shown below.</p>

<pre><code>Number  Priority Created_on Description               Category     Sub Category
719515  MEDIUM  05-01-2016  MedWay 3rd Lucene.... Server       Change
720317  MEDIUM  07-01-2016  DI - Medway 13146409  Application  Incident
720447  MEDIUM  08-01-2016  DI QLD Chermside....  Application  Medway
</code></pre>

<p>Please guide me on this. </p>
","python, r, text-classification","<p>Answering without more is a bit tough, and this is more of a context questions than a code question. But here is the logic I would use to start to evaluate this problem Keep in mind it might involve writing a few separate scripts each performing part of the task.</p>

<p>Try breaking the problem up into smaller pieces.You cannot do an analysis without all the data so start by creating the data. </p>

<p>You have the category and sub category already make a list of all the unique factors in each list and create a set of weights for each based on your system and business needs. As you make subcategory weights, keep in mind how they will interact with categories (+/- as well as magnitude).</p>

<p>Write a script to read the description, count all the non-trivial words. Create some kind of classifications for words to help you build lists that will inform the model with categories and sub categories. 
Is the value an error message, or machine name, or some other code or type of problem you can extract using key words? </p>

<p>How are all the word groupings meaningful? 
How would the contribute to making a decision? </p>

<p>Think about the categories when you decide these things.</p>

<p>Then with all of the parts, decide on a model, build, test and refine. I know there is no code in this but the problem solving part of Data Science happens outside of code most of the time.</p>

<p>You need to come up with the code yourself. If you get stuck post an edit and we can help.</p>
",3,1,108,2017-05-26 11:29:52,https://stackoverflow.com/questions/44200490/text-analysis-with-a-mix-of-text-categorical-columns-in-r
Improve flow Python classifier and combine features,"<p>I am trying to create a classifier to categorize websites. I am doing this for the very first time so it's all quite new to me. Currently I am trying to do some Bag of Words on a couple of parts of the web page (e.g. title, text, headings). It looks like this:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
countvect_text = CountVectorizer(encoding=""cp1252"", stop_words=""english"")
countvect_title = CountVectorizer(encoding=""cp1252"", stop_words=""english"")
countvect_headings = CountVectorizer(encoding=""cp1252"", stop_words=""english"")

X_tr_text_counts = countvect_text.fit_transform(tr_data['text'])
X_tr_title_counts = countvect_title.fit_transform(tr_data['title'])
X_tr_headings_counts = countvect_headings.fit_transform(tr_data['headings'])

from sklearn.feature_extraction.text import TfidfTransformer

transformer_text = TfidfTransformer(use_idf=True)
transformer_title = TfidfTransformer(use_idf=True)
transformer_headings = TfidfTransformer(use_idf=True)

X_tr_text_tfidf = transformer_text.fit_transform(X_tr_text_counts)
X_tr_title_tfidf = transformer_title.fit_transform(X_tr_title_counts)
X_tr_headings_tfidf = transformer_headings.fit_transform(X_tr_headings_counts)

from sklearn.naive_bayes import MultinomialNB
text_nb = MultinomialNB().fit(X_tr_text_tfidf, tr_data['class'])
title_nb = MultinomialNB().fit(X_tr_title_tfidf, tr_data['class'])
headings_nb = MultinomialNB().fit(X_tr_headings_tfidf, tr_data['class'])

X_te_text_counts = countvect_text.transform(te_data['text'])
X_te_title_counts = countvect_title.transform(te_data['title'])
X_te_headings_counts = countvect_headings.transform(te_data['headings'])

X_te_text_tfidf = transformer_text.transform(X_te_text_counts)
X_te_title_tfidf = transformer_title.transform(X_te_title_counts)
X_te_headings_tfidf = transformer_headings.transform(X_te_headings_counts)

accuracy_text = text_nb.score(X_te_text_tfidf, te_data['class'])
accuracy_title = title_nb.score(X_te_title_tfidf, te_data['class'])
accuracy_headings = headings_nb.score(X_te_headings_tfidf, te_data['class'])
</code></pre>

<p>This works fine, and I get the accuracies as expected. However, as you might have guessed, this looks cluttered and is filled with duplication. My question then is, <strong>is there a way to write this more concisely?</strong></p>

<p>Additionally, I am not sure how I can combine these three features <strong>into a single multinomial classifier</strong>. I tried passing a list of tfidf values to <code>MultinomialNB().fit()</code>, but apparently that's not allowed. </p>

<p>Optionally, it would also be nice to add weights to the features, so that in the final classifier some vectors have a higher importance than others.</p>

<p>I'm guessing I need <code>pipeline</code> but I'm not at all sure how I should use it in this case.</p>
","python, scikit-learn, text-classification, supervised-learning, multinomial","<p>First, CountVectorizer and TfidfTransformer can be removed by using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a> (which is essentially combination of both).</p>

<p>Second, the TfidfVectorizer and MultinomialNB can be combined in a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"" rel=""nofollow noreferrer"">Pipeline</a>. 
A pipeline sequentially apply a list of transforms and a final estimator. When <code>fit()</code> is called on a <code>Pipeline</code>, it fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator. And when <code>score()</code> or <code>predict()</code> is called, it only call <code>transform()</code> on all transformers and <code>score()</code> or <code>predict()</code> on last one.</p>

<p>So the code will look like:</p>

<pre><code>from sklearn.pipeline import Pipeline
pipeline = Pipeline([('vectorizer', TfidfVectorizer(encoding=""cp1252"",
                                                    stop_words=""english"",
                                                    use_idf=True)), 
                     ('nb', MultinomialNB())])

accuracy={}
for item in ['text', 'title', 'headings']:

    # No need to save the return of fit(), it returns self
    pipeline.fit(tr_data[item], tr_data['class'])

    # Apply transforms, and score with the final estimator
    accuracy[item] = pipeline.score(te_data[item], te_data['class'])
</code></pre>

<p><strong>EDIT</strong>:
Edited to include the combining of all features to get single accuracy:</p>

<p>To combine the results, we can follow multiple approaches. One that is easily understandable (but a bit of again going to the cluttery side) is the following:</p>

<pre><code># Using scipy to concatenate, because tfidfvectorizer returns sparse matrices
from scipy.sparse import hstack

def get_tfidf(tr_data, te_data, columns):

    train = None
    test = None

    tfidfVectorizer = TfidfVectorizer(encoding=""cp1252"",
                                      stop_words=""english"",
                                      use_idf=True)
    for item in columns:
        temp_train = tfidfVectorizer.fit_transform(tr_data[item])
        train = hstack((train, temp_train)) if train is not None else temp_train

        temp_test = tfidfVectorizer.transform(te_data[item])
        test = hstack((test , temp_test)) if test is not None else temp_test

    return train, test

train_tfidf, test_tfidf = get_tfidf(tr_data, te_data, ['text', 'title', 'headings']) 

nb = MultinomialNB()
nb.fit(train_tfidf, tr_data['class'])
nb.score(test_tfidf, te_data['class'])
</code></pre>

<p>Second approach (and more preferable) will be to include all these in pipeline. But due to selecting the different columns ('text', 'title', 'headings') and concatenating the results, its not that straightforward. We need to use FeatureUnion for them. And specifically the following example:</p>

<ul>
<li><a href=""http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py</a></li>
</ul>

<p>Third, if you are open to use other libraries, then <a href=""https://github.com/pandas-dev/sklearn-pandas"" rel=""nofollow noreferrer""><code>DataFrameMapper</code></a> from <code>sklearn-pandas</code> can simplify the usage of FeatureUnions used in previous example.</p>

<p>If you do want to go the second or third way, please feel free to contact if having any difficulties.</p>

<p>NOTE: I have not checked the code, but it should work (less some syntax errors, if any). Will check as soon as on my pc.</p>
",2,1,798,2017-05-26 17:27:54,https://stackoverflow.com/questions/44207142/improve-flow-python-classifier-and-combine-features
how to do classification on the result of stanford-core nlp,"<p>I have a couple of questions about core nlp and doing classification,</p>

<p>firstly I should say that I have read this questions but still, I am confused:</p>

<p><a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool?noredirect=1&amp;lq=1"">link1</a></p>

<p><a href=""https://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">link2</a></p>

<p><a href=""https://stackoverflow.com/questions/32085525/example-for-stanford-nlp-classifier"">link3</a></p>

<p><a href=""https://stackoverflow.com/questions/33712795/questions-about-creating-stanford-corenlp-training-models?rq=1"">link4</a></p>

<p>and some others related to this,</p>

<p>but my confusion:
when I faced with these links, I was happy that I can do something on the result of the corenlp I got to classify them, then gain the accuracy,</p>

<p>my result like other results of corenlp is something like this:</p>

<p><a href=""https://i.sstatic.net/uK7DG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uK7DG.png"" alt=""enter image description here""></a></p>

<p>but now In these links they are talking about doing labling then using stanford-classifier.</p>

<p>so it seems stanford-classifier is something that do classification on the data like other classification methods, and so there is no way to do classification on the result we get of corenl,</p>

<p>may I ask you to critique me, and share your information regarding this</p>

<p>many thanks</p>
","stanford-nlp, sentiment-analysis, text-classification","<p>Actually the right answer is that, YES stanford classifier is a supervised algorithm,</p>

<p>so if anyone want to do classification on the result of corenlp, it needs some coding , like for example I firstly did the corenlp for very negative ones, then I made the document as the text for very negative text,</p>

<p>then I made another document for very positive, and so on,</p>

<p>finally I had for example two document of the result of corenlp for positive and negative,</p>

<p>Then I used those document for the stanford classifier</p>

<p>hope helps somebody which is new to this area</p>
",0,1,503,2017-05-27 03:57:37,https://stackoverflow.com/questions/44212814/how-to-do-classification-on-the-result-of-stanford-core-nlp
Searching for list of terms using Google in order to build a bag-of-words for a particular category,"<p>I am having a hard time understanding the process of building a bag-of-words. This will be a multiclass classfication supervised machine learning problem wherein a webpage or a piece of text is assigned to one category from multiple pre-defined categories. Now the method that I am familiar with when building a bag of words for a specific category (for example, 'Math') is to collect a lot of webpages that are related to Math. From there, I would perform some data processing (such as remove stop words and performing TF-IDF) to obtain the bag-of-words for the category 'Math'.</p>

<p>Question: Another method that I am thinking of is to instead search in google for something like 'List of terms related to Math' to build my bag-of-words. I would like to ask if this is method is okay?</p>

<p>Another question: In the context of this question, does bag-of-words and corpus mean the same thing?</p>

<p>Thank you in advance!</p>
","machine-learning, text-classification, supervised-learning, multiclass-classification","<p>This is not what bag of words is. Bag of words is the term to describe a specific way of representing a given <strong>document</strong>. Namely, a document (paragraph, sentence, webpage) is represented as a mapping of form </p>

<pre><code>word: how many times this word is present in a document
</code></pre>

<p>for example ""John likes cats and likes dogs"" would be represented as: {john: 1, likes: 2, cats: 1, and: 1, dogs: 1}. This kind of representation can be easily fed into typical ML methods (especially if one assumes that total vocabulary is finite so we end up with numeric vectors).</p>

<p>Note, that this is <strong>not</strong> about ""creating a bag of words for a category"". Category, in typical supervised learning would consist of multiple documents, and each of them <strong>independently</strong> is represented as a bag of words.</p>

<p>In particular this invalidates your final proposal of asking google for words that are related to category - this is not how typical ML methods work. You get a lot of documents, represent them as bag of words (or something else) and then perform statistical analysis (build a model) to figure out the best set of rules to discriminate between categories. These rules usually <strong>will not</strong> be simply ""if the word X is present, this is related to Y"".</p>
",0,0,73,2017-05-27 11:53:50,https://stackoverflow.com/questions/44216522/searching-for-list-of-terms-using-google-in-order-to-build-a-bag-of-words-for-a
Top m topics in a collection of comments,"<p>I have a collection of comments and each comment discusses a topic. I want to figure out the top m topics discussed in these comments. Also, I am receiving these comments in an online fashion(i.e. I don't get the entire comments in one go, instead I have to process these comments one-by-one). I thought of using Word2Vec for feature extraction and then applying some clustering algorithm like k-means(cluster would correspond to a topic) and then I can get the answer from the top m clusters(which have most number of points in them). But the problem is that I don't know the number of clusters and also at any point of time, the number of different topics(clusters) is not fixed because a new comment might discuss a new topic(so, this problem can't be solved by applying k-means with different values of k). So, should I use some other clustering algorithm(like DBSCAN) and what should be the approach in that case or should I use a totally different approach?</p>
","machine-learning, nlp, cluster-analysis, text-classification","<p>Why can't you try something simple LDA and start with a large number for topics and then narrow it down? <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a></p>

<p>On a similar note you can take a look at sense2vec where they used reddit comments to build a topic model <a href=""https://explosion.ai/blog/sense2vec-with-spacy"" rel=""nofollow noreferrer"">https://explosion.ai/blog/sense2vec-with-spacy</a></p>
",1,0,62,2017-05-30 14:58:43,https://stackoverflow.com/questions/44265674/top-m-topics-in-a-collection-of-comments
Hold out sample when loading data in Scikit-Learn with sklearn.datasets.load_files,"<p>I'm experimenting with a simple Naive Bayes with Scikit-learn. </p>

<p>Essentially, I've got two folders, respectively named Cat A and Cat B, each of which consisting of circa 1,500 text files. </p>

<p>I'm loading these files in order to train the classifier like so:</p>

<pre><code># Declare the categories
categories = ['CatA', 'CatB']

# Load the dataset
docs_to_train = sklearn.datasets.load_files(""/Users/dh/Documents/Development/Python/Test_Data"", description=None, categories=categories, load_content=True, shuffle=True, encoding='utf-8', decode_error='strict', random_state=0)
</code></pre>

<p>I'm testing the classifier with short strings of text, e.g. </p>

<pre><code>docs_new = ['This is test string 1.', 'This is test string 2.', 'This is test string 3.']
X_new_counts = count_vect.transform(docs_new)
X_new_tfidf = tfidf_transformer.transform(X_new_counts)

predicted = clf.predict(X_new_tfidf)

for doc, category in zip(docs_new, predicted):
    print('%r =&gt; %s' % (doc, docs_to_train.target_names[category]))
</code></pre>

<p>Everything works as it ought to, but what I'd really like to do is test the classifier on some data that closely resembles to training data. Ideally, I'd like to carve out a hold out sample within the data I'm using the train the classifier and then cross-validate with that. </p>

<p>I suppose I could just move 500-odd documents from each of the training datasets into a different folders, but I was wondering whether there's a better way to create the hold out sample?</p>

<p>The <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow noreferrer"">documentation</a> doesn't appear to offer on guidance with this.</p>

<p>The code in full follows.</p>

<pre><code>import sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import numpy as np
from sklearn import datasets
from pprint import pprint

# Declare the categories
categories = ['CatA', 'CatB']

# Load the dataset
docs_to_train = sklearn.datasets.load_files(""/Users/dh/Documents/Development/Python/Test_Data"", description=None, categories=categories, load_content=True, shuffle=True, encoding='utf-8', decode_error='strict', random_state=0)

print len(docs_to_train.data)

# Vectorise the dataset

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(docs_to_train.data)

# Fit the estimator and transform the vector to tf-idf

tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
X_train_tf = tf_transformer.transform(X_train_counts)
X_train_tf.shape

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
print X_train_tfidf.shape

clf = MultinomialNB().fit(X_train_tfidf, docs_to_train.target)

docs_new = ['I am test string 1.', 'I am test string 2', 'I am test string 3']
X_new_counts = count_vect.transform(docs_new)
X_new_tfidf = tfidf_transformer.transform(X_new_counts)

predicted = clf.predict(X_new_tfidf)

for doc, category in zip(docs_new, predicted):
    print('%r =&gt; %s' % (doc, docs_to_train.target_names[category])) 
</code></pre>
","python, scikit-learn, text-classification","<p>What you're looking for is referred to as a ""train-test split."" </p>

<p>Use  <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer""><code>sklearn.model_selection.train_test_split</code></a>:</p>

<pre><code>from sklearn.model_selection import train_test_split

train_X, test_X, train_y, test_y = train_test_split(docs_to_train.data, 
                               docs_to_train.target,
                               test_size = 500)
</code></pre>
",1,0,2251,2017-06-02 16:21:46,https://stackoverflow.com/questions/44333423/hold-out-sample-when-loading-data-in-scikit-learn-with-sklearn-datasets-load-fil
Data csv file into different text files with Python,"<p>I'm a beginner in programming, but for a Dutch text categorization experiment I want to turn every instance (row) of a csv file into separate .txt files, so that the texts can be analyzed by a NLP tool. My csv looks like this.</p>

<p><a href=""https://i.sstatic.net/21dIK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/21dIK.png"" alt=""enter image description here""></a></p>

<p>As you can see, each instance has text in the column 'Taaloefening1' or in the column 'Taaloefening2'. Now I need to save the text per instance in a .txt file and the name of the file needs to be the id and the label.
I was hoping I could to this automatically by programming a script in Python by using the csv module. I have an idea about how to save the text into a .txt file, but I have no idea how to take the id and label, which match the text, as the file name.
Any ideas?</p>
","python, csv, nlp, text-classification","<p>The <a href=""https://docs.python.org/3/library/csv.html#csv.DictReader"" rel=""nofollow noreferrer""><code>csv.DictReader</code></a> should be able to do what you need:</p>

<pre><code>from csv import DictReader

INPUT_FILE = 'data.csv'

with open(INPUT_FILE, 'rb') as csvfile:
    reader = DictReader(csvfile)
    for row in reader:
        file_name = ""{}_{}.txt"".format(row[""id""], row[""Label""])
        if row[""Taaloefening1""]:     # if this field is not empty
            line = row[""Taaloefening1""] + '\n'
        elif row[""Taaloefening2""]:
            line = row[""Taaloefening2""] + '\n'
        else:
            print(""Both 'Taaloefening2' and 'Taaloefening2' empty on {}_{}. Skipping."".format(row[""id""], row[""Label""]))
            continue
        with open(file_name, 'w') as output:
            output.write(line)
</code></pre>
",1,0,1485,2017-06-09 08:27:23,https://stackoverflow.com/questions/44452899/data-csv-file-into-different-text-files-with-python
Text Classification Using Python,"<p>I have list of words in text variable with their labels. I like to make a classifier that can predict the label of new input text. </p>

<p>I am thinking of using scikit-learn package in Python to use SVM model. </p>

<p>I realize that the text need to be corverted to vector form so I am trying TfidfVectorizer and CountVectorizer.</p>

<p>This is my code so far using TfidfVectorizer:</p>

<pre><code>from sklearn import svm
from sklearn.feature_extraction.text import TfidfVectorizer

label = ['organisasi','organisasi','organisasi','organisasi','organisasi','lokasi','lokasi','lokasi','lokasi','lokasi']
text = ['Partai Anamat Nasional','Persatuan Sepak Bola', 'Himpunan Mahasiswa','Organisasi Sosial','Masyarakat Peduli','Malioboro','Candi Borobudur','Taman Pintar','Museum Sejarah','Monumen Mandala']

vectorizer = TfidfVectorizer(min_df=1)

X = vectorizer.fit_transform(text)
y = label

klasifikasi = svm.SVC()

klasifikasi = klasifikasi.fit(X,y) #training

test_text = ['Partai Perjuangan']
test_vector = vectorizer.fit_transform(test_text)

prediksi = klasifikasi.predict([test_vector]) #test

print(prediksi)
</code></pre>

<p>I also try the CountVectorizer with same code above.
Both showing the same Error result:</p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>

<p>How to solve this problem? Thanks</p>
","python, scikit-learn, vectorization, text-classification, countvectorizer","<p>The error is due to this line:</p>

<pre><code>prediksi = klasifikasi.predict([test_vector])
</code></pre>

<p>Most scikit estimators require an array of shape <code>[n_samples, n_features]</code>. The <code>test_vector</code> output from TfidfVectorizer is already in that shape ready to use for estimators. You don't need to wrap it in square brackets (<code>[</code> and <code>]</code>). The wrapping makes it a list which is unsuitable. </p>

<p>Try using it like this:</p>

<pre><code>prediksi = klasifikasi.predict(test_vector)
</code></pre>

<p>But even then you will gt error. Because of this line:</p>

<pre><code>test_vector = vectorizer.fit_transform(test_text)
</code></pre>

<p>Here you are fitting the vectorizer in a different way than what was learned by the <code>klasifikasi</code> estimator. <code>fit_transform()</code> is just a shortcut for calling <code>fit()</code> (learning the data) and then <code>transform()</code> it. For test data, always use <code>transform()</code> method, never <code>fit()</code> or <code>fit_transform()</code> </p>

<p>So the correct code will be:</p>

<pre><code>test_vector = vectorizer.transform(test_text)
prediksi = klasifikasi.predict(test_vector)

#Output: array(['organisasi'],  dtype='|S10')
</code></pre>
",2,1,831,2017-06-14 06:49:56,https://stackoverflow.com/questions/44537453/text-classification-using-python
"inconsistent shape error MultiLabelBinarizer on y_test, sklearn multi-label classification","<pre><code>import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.svm import SVC

data = r'C:\Users\...\Downloads\news_v1.xlsx'

df = pd.read_excel(data)
df = pd.DataFrame(df.groupby([""id"", ""doc""]).label.apply(list)).reset_index()

X = np.array(df.doc)
y = np.array(df.label)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

mlb = preprocessing.MultiLabelBinarizer()
Y_train = mlb.fit_transform(y_train)

classifier = Pipeline([
('vectorizer', CountVectorizer()),
('tfidf', TfidfTransformer()),
('clf', OneVsRestClassifier(LinearSVC()))])

classifier.fit(X_train, Y_train)
predicted = classifier.predict(X_test)

Y_test = mlb.fit_transform(y_test)

print(""Y_train: "", Y_train.shape)
print(""Y_test: "", Y_test.shape)
print(""Predicted: "", predicted.shape)
print(""Accuracy Score: "", accuracy_score(Y_test, predicted))
</code></pre>

<p>I can't seems to do any measurements since Y_test gives a different matrix dimension after fit_transform with MultiLabelBinarizer.</p>

<p>Results and error: </p>

<pre><code>Y_train:  (1278, 49)
Y_test:  (630, 42)
Predicted:  (630, 49)
Traceback (most recent call last):
  File ""C:/Users/../PycharmProjects/MultiAutoTag/classifier.py"", line 41, in &lt;module&gt;
    print(""Accuracy Score: "", accuracy_score(Y_test, predicted))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\metrics\classification.py"", line 174, in accuracy_score
    differing_labels = count_nonzero(y_true - y_pred, axis=1)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\compressed.py"", line 361, in __sub__
    raise ValueError(""inconsistent shapes"")
ValueError: inconsistent shapes
</code></pre>

<p>Looking at the printed Y_test, the shape is different than the rest. What am i doing wrong and why does MultiLabelBinarizer return a different size for Y_test?
Thanks for the help in advance!</p>

<p><strong>Edit</strong> <strong>New error:</strong></p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/../PycharmProjects/MultiAutoTag/classifier.py"", line 47, in &lt;module&gt;
    Y_test = mlb.transform(y_test)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\label.py"", line 763, in transform
    yt = self._transform(y, class_to_index)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\label.py"", line 787, in _transform
    indices.extend(set(class_mapping[label] for label in labels))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\label.py"", line 787, in &lt;genexpr&gt;
    indices.extend(set(class_mapping[label] for label in labels))
KeyError: 'Sanction'
</code></pre>

<p>This is how y_test looks like:</p>

<pre><code>print(y_test)

[['App'] ['Contract'] ['Pay'] ['App'] 
 ['App'] ['App']
 ['Reports'] ['Reports'] ['Executive', 'Pay']
 ['Change'] ['Reports']
 ['Reports'] ['Issue']]
</code></pre>
","numpy, scikit-learn, text-classification, multilabel-classification","<p>You should only call <code>transform()</code> on test data. Never <code>fit()</code> or its variations like <code>fit_transform()</code> or <code>fit_predict()</code> etc. They should be used only on training data. </p>

<p>So change the line:</p>

<p><code>Y_test = mlb.fit_transform(y_test)</code> </p>

<p>to </p>

<p><code>Y_test = mlb.transform(y_test)</code></p>

<p><strong>Explanation</strong>:</p>

<p>When you call <code>fit()</code> or <code>fit_transform()</code>, the mlb forgets its previous learnt data and learn the new supplied data. This can be problematic when <code>Y_train</code> and <code>Y_test</code> may have difference in labels as your case have. </p>

<p>In your case, <code>Y_train</code> have 49 different kinds of labels, whereas <code>Y_test</code> have only 42 different labels. But this doesn't mean that Y_test is 7 labels short of <code>Y_train</code>. It can be possible that <code>Y_test</code> may have entirely different set of labels, which when binarized results in 42 columns, and that will affect the results.</p>
",2,2,2108,2017-06-21 08:51:27,https://stackoverflow.com/questions/44671194/inconsistent-shape-error-multilabelbinarizer-on-y-test-sklearn-multi-label-clas
Reloading Keras Tokenizer during Testing,"<p>I followed the tutorial here: (<a href=""https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"" rel=""noreferrer"">https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html</a>)</p>

<p>However, I modified the code to be able to save the generated model through <code>h5py</code>. Thus, after running the training script, I have a generated <code>model.h5</code> in my directory. </p>

<p>Now, when I want to load it, my problem is that I'm confused as to how to re-initiate the <code>Tokenizer</code>. The tutorial has the following line of code:</p>

<pre><code>tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
</code></pre>

<p>But hypothetically, if I reload the <code>model.h5</code> in a different module, I'll need to create another <code>Tokenizer</code> to tokenize the test set. But then, the new <code>Tokenizer</code> will be fit on the test data thus creating a completely different word table.</p>

<p>Therefore, my question is: How do I reload the <code>Tokenizer</code> that was trained on the training dataset? Am I in some way misunderstanding the functionality of the <code>Embedding</code> layer in Keras? Right now, I'm assuming that since we mapped certain word indices to their corresponding embedding vectors based on the pre-trained word embeddings, the word indices need to be consistent. However, this is not possible if we perform another <code>fit_on_texts</code> on the test dataset.</p>

<p>Thank you and looking forward to your answers!</p>
","tensorflow, keras, tokenize, text-classification, word-embedding","<p><a href=""https://stackoverflow.com/questions/44758405/keras-tokenizer-random-results-due-to-different-mapping-of-text-tokens-and-ids"">Check out this question</a> 
The commenter recommends using a pickle to save the object &amp; state, though the question still remains why this kind of functionality is not built into keras. </p>
",2,8,3587,2017-06-26 13:31:26,https://stackoverflow.com/questions/44760961/reloading-keras-tokenizer-during-testing
Identifying positivity and negativity separately from a negative dataset,"<p>First of all, i would like you to know that I am new to machine learning (ML). I am working on a project which detects how <strong>positive or negative</strong> a set of words can be, therefore i have created a database containing possible <strong>negative</strong> words. So that ML can take place and predict the overall score on how <strong>positive or negative</strong> the whole set of words. 
My questions are is it possible to classify <strong>positive</strong> words with only  <strong>negative</strong> words in the dataset? Does it affect the accuracy of predicting if it is possible?</p>
","machine-learning, dataset, classification, text-classification","<p>No, it's not generally possible.  The model will have no way to differentiate among (1) new negative phrases; (2) neutral phrases; (3) positive phrases.  In fact, with only negative phrases, the model will have a hard time learning that ""bad"" and ""not bad"" are opposites, as it has seen plenty of ""not"" references in the negative literature, such as ""not worth watching, even for free.""</p>
",0,0,49,2017-06-26 21:29:05,https://stackoverflow.com/questions/44768976/identifying-positivity-and-negativity-separately-from-a-negative-dataset
AttributeError: &#39;NoneType&#39; object has no attribute &#39;items&#39; for classifier = nltk.NaiveBayesClassifier.train(training_set),"<p>I am getting this error for AttributeError: 'NoneType' object has no attribute 'items. The code is as follows:</p>

<pre><code>import nltk
import random
from nltk.corpus import movie_reviews
from nltk.classify import NaiveBayesClassifier
from nltk.probability import FreqDist

documents = [ (list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
                for fileid in movie_reviews.fileids(category)]


random.shuffle(documents)

all_words = []
for w in movie_reviews.words():
all_words.append(w.lower())

all_words =nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3000]
print (word_features)

def find_features(documents):
    words = set(documents)
    features = {}
    for w in word_features:
        features[w] = (w in words)


featuresets = [(find_features(rev), category) for (rev, category) in 
documents]

training_set = featuresets[:1500]
testing_set = featuresets[1500:]


classifier = nltk.NaiveBayesClassifier.train(training_set)

print (""Naive Bayes Classifier Algo Accuracy: "",nltk.classify.accuracy(classifier, testing_set))*100)

classifier.show_most_informative_features(15)
</code></pre>

<p>I have been following a video tutorial and it is the same code there. There it runs fine but here it shows the following error:</p>

<pre><code>AttributeError: 'NoneType' object has no attribute 'items'
</code></pre>

<p>for the line</p>

<blockquote>
  <p>classifier = nltk.NaiveBayesClassifier.train(training_set)</p>
</blockquote>

<p>What is the reason and the solution?</p>
","machine-learning, nlp, nltk, text-classification, naivebayes","<p>You're not returning the list from the function. In your find_feature function use:</p>

<pre><code>def find_features(documents):
    words = set(documents)
    features = {}
    for w in word_features:
        features[w] = (w in words)
    return features #ADD THIS LINE
</code></pre>
",0,0,1391,2017-07-12 07:13:20,https://stackoverflow.com/questions/45050805/attributeerror-nonetype-object-has-no-attribute-items-for-classifier-nltk
Simple machine learning for website classification,"<p>I am trying to generate a Python program that determines if a website is harmful (porn etc.).</p>
<p>First, I made a Python web scraping program that counts the number of occurrences for each word.</p>
<p><a href=""https://i.sstatic.net/sE0xr.png"" rel=""nofollow noreferrer"">result for harmful websites</a></p>
<p>It's a key value dictionary like</p>
<p>{ word : [ # occurrences in harmful websites, # of websites that contain these words] }.</p>
<p>Now I want my program to analyze the words from any websites to check if the website is safe or not. But I don't know which methods will suit to my data.</p>
","machine-learning, artificial-intelligence, classification, cross-validation, text-classification","<p>The key thing here is your training data. You need some sort of <a href=""https://en.wikipedia.org/wiki/Supervised_learning"" rel=""nofollow noreferrer"">supervised learning</a> technique where your training data consists of website's data itself (text document) and its label (<code>harmful</code> or <code>safe</code>). </p>

<p>You can certainly use the <a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network"" rel=""nofollow noreferrer"">RNN</a> but there also other <a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow noreferrer"">natural language processing</a> techniques and much faster ones. </p>

<p>Typically, you should use a proper vectorizer on your training data (think of each site page as a text document), for example <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tf-idf</a> (but also other possibilities; if you use Python I would strongly suggest <a href=""http://scikit-learn.org/stable/"" rel=""nofollow noreferrer"">scikit</a> that provides lots of useful machine learning techniques and mentioned <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">sklearn.TfidfVectorizer</a> is already within). The point is to vectorize your text document in enhanced way. Imagine for example the English word <code>the</code> how many times it typically exists in text? You need to think of biases such as these.</p>

<p>Once your training data is vectorized you can use for example <a href=""http://scikit-learn.org/stable/modules/sgd.html"" rel=""nofollow noreferrer"">stochastic gradient descent</a> classifier and see how it performs on your test data (in machine learning terminology the test data means to simply take some new data example and test what your ML program outputs). </p>

<p>In either case you will need to experiment with above options. There are many nuances and you need to test your data and see where you achieve the best results (depending on ML algorithm settings, type of vectorizer, used ML technique itself and so on). For example <a href=""http://scikit-learn.org/stable/modules/svm.html"" rel=""nofollow noreferrer"">Support Vector Machines</a> are great choice when it comes to binary classifiers too. You may wanna play with that too and see if it performs better than SGD.</p>

<p>In any case, remember that you will need to obtain quality training data with labels (<code>harmful</code> vs. <code>safe</code>) and find the best fitting classifier. On your journey to find the best one you may also wanna use <a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)"" rel=""nofollow noreferrer"">cross validation</a> to determine how well your classifier behaves. Again, already contained in <a href=""http://scikit-learn.org/stable/modules/cross_validation.html"" rel=""nofollow noreferrer"">scikit-learn</a>. </p>

<p><em>N.B. Don't forget about valid cases. For example there may be a completely safe online magazine where it only mentions the harmful topic in some article; it doesn't mean the website itself is harmful though.</em></p>

<p>Edit: As I think of it, if you don't have any experience with ML at all it could be useful to take any <a href=""https://www.google.cz/search?q=machine%20learning%20course"" rel=""nofollow noreferrer"">online course</a> because despite the knowledge of API and libraries you will still need to know what it does and the math behind the curtain (at least roughly).</p>
",1,2,3755,2017-07-12 08:50:20,https://stackoverflow.com/questions/45052850/simple-machine-learning-for-website-classification
Machine learning to classify company names to their industries,"<p>What I'm trying to do is to ask the user to input a company name, for example Microsoft, and be able to predict that it is in the Computer Software industry. I have around 150 000 names and 60+ industries. Some of the names are not English company names.</p>

<p>I have tried training a Word2Vec model using Gensim based on company names only and averaged up the word vectors before feeding it into SKlearn's logistic regression but had terrible results. My questions are:</p>

<ol>
<li><p>Has anyone tried these kind of tasks? Googling on short text classification shows me results on classifying short sentences instead of pure names. If anyone had tried this before, mind sharing a few keywords or research papers regarding this task?</p></li>
<li><p>Would it be better if I have a brief description for each company instead of only using their names? How much would it help for my Word2Vec model rather than using only the company names?</p></li>
</ol>
","python, machine-learning, text-classification, multilabel-classification","<p>For your problem, This is nothing but Company-Industry Relationship so for that, you have to train your word2vec <a href=""https://github.com/llSourcell/word_vectors_game_of_thrones-LIVE"" rel=""nofollow noreferrer"">model</a> using company description data because the word2vec works on calculating the similar words related to the given word.So if you train, based on the company names that would give you bad results.If you train on the description then that would give you the similar words related to the particular industry.By using that you can get the industry it belongs to.</p>

<p>If you want to train based on company names NER(Named Entity Tagger) will be useful.But this will not be accurate.</p>
",3,2,3419,2017-07-13 01:49:54,https://stackoverflow.com/questions/45070186/machine-learning-to-classify-company-names-to-their-industries
Text labeling with machine learning,"<p>I want to label a bunch of bank transactions according to a set of predefined classes (example below, its a map in <code>clojure</code>). I tried a naive bayes approach but sometimes it totally gives me the wrong label.</p>

<p>According to my research, I should use a supervised ML algorithm, something like a linear SVM tuned for multiclass classification. Problem is I don't know anything about ML really. Second problem is that most clojure libs are outdated.</p>

<pre><code>{:label ""5339134-17-CPR-FARMODISSEIA LD"", :value -13271 :class :health}
{:label ""PAG.SERV. 10297 779747511"", :value -2889 :class :utilities}
{:label ""5339134-14-CPR-GREEN PEPER"", :value -1785 :class :restaurants}
{:label ""5339134-03-LEV-Av Alm Kings"", :value -4000 :class :atm}
{:label ""5339134-02-LEV-Big Field, 1"", :value -7000 :class :atm}
{:label ""IMPOSTO DE SELO"", :value -17 :class :banking}
</code></pre>

<p>So most of the similar transactions have like 90% similar text (see eg: <code>:atm</code>), I believe this should be an easy problem.</p>

<p>My questions:</p>

<ul>
<li>what algorithms can I use?</li>
<li>how should I prepare the data? I believe I only have two features, tx label and tx value. Some tutorials I see have a bunch of vectors, but I don't know if/how to convert the string data to the proper ML format.</li>
</ul>

<p>Any sample in either clj or java will be greatly appreciated.</p>
","java, machine-learning, clojure, text-classification","

<p>Since you said in your question that</p>

<blockquote>
  <p>most of the similar transactions have like 90% similar text</p>
</blockquote>

<p>I thought it would make sense to first figure out which transaction labels are similar to each other and group them together. Then you have a limited number of groups, and the group that each label falls into can be used as a nominal attribute in place of the text itself. If transactions in the same class have similar label text, then hopefully this should allow the classification algorithm to easily draw correlations between label and class.</p>

<p>I tried implementing a solution using these dependencies:</p>

<pre class=""lang-clj prettyprint-override""><code>[[org.clojure/clojure ""1.8.0""]
 [clj-fuzzy ""0.4.0""]
 [cc.artifice/clj-ml ""0.8.5""]
 [rm-hull/clustering ""0.1.3""]]
</code></pre>

<p>After clustering the labels, the naïve Bayes approach seemed to work fine for me:</p>

<pre class=""lang-clj prettyprint-override""><code>(require '[clj-fuzzy.metrics :as fm]
         '[clj-ml.classifiers :as classify]
         '[clj-ml.data :as data]
         '[clustering.core.qt :as qt])

(def data
  [{:label ""5339134-17-CPR-FARMODISSEIA LD"", :value -13271 :class :health}
   {:label ""PAG.SERV. 10297 779747511"", :value -2889 :class :utilities}
   {:label ""5339134-14-CPR-GREEN PEPER"", :value -1785 :class :restaurants}
   {:label ""5339134-03-LEV-Av Alm Kings"", :value -4000 :class :atm}
   {:label ""5339134-02-LEV-Big Field, 1"", :value -7000 :class :atm}
   {:label ""IMPOSTO DE SELO"", :value -17 :class :banking}])

(def clusters
  (into {}
        (for [cluster (qt/cluster fm/levenshtein (map :label data) 13 1)
              s cluster]
          [s (keyword (str ""cluster"" (hash cluster)))])))

(def dataset
  (-&gt; (data/make-dataset ""my-data""
                         [:value
                          {:label (seq (set (vals clusters)))}
                          {:class [:health :utilities :restaurants :atm :banking]}]
                         (map (juxt :value (comp clusters :label) :class) data))
      (data/dataset-set-class :class)))

(def data-map
  (let [m (into {} (map (juxt data/instance-to-map identity)
                        (data/dataset-seq dataset)))]
    (into {} (for [x data]
               [x (-&gt; x (update :label clusters) (update :value double) m)]))))

(def classifier
  (-&gt; (classify/make-classifier :bayes :naive)
      (classify/classifier-train dataset)))

(defn foo []
  (for [x data]
     (-&gt;&gt; x
          data-map
          data/instance-set-class-missing
          (classify/classifier-classify classifier)
          (assoc x :predicted))))

(run! prn (foo))
;; {:label ""5339134-17-CPR-FARMODISSEIA LD"", :value -13271, :class :health, :predicted :health}
;; {:label ""PAG.SERV. 10297 779747511"", :value -2889, :class :utilities, :predicted :utilities}
;; {:label ""5339134-14-CPR-GREEN PEPER"", :value -1785, :class :restaurants, :predicted :restaurants}
;; {:label ""5339134-03-LEV-Av Alm Kings"", :value -4000, :class :atm, :predicted :atm}
;; {:label ""5339134-02-LEV-Big Field, 1"", :value -7000, :class :atm, :predicted :atm}
;; {:label ""IMPOSTO DE SELO"", :value -17, :class :banking, :predicted :banking}
</code></pre>

<p>I'm quite new to ML though, so please let me know if there's something I've overlooked.</p>

<p>Also, in my implementation, I use QT clustering to do a one-time partition of the labels in the input dataset, but if the goal is to continue incorporating new data over time, it may be necessary to use a streaming clustering algorithm instead. It looks like this may be possible with k-means, but that would require implementation of a ""Levenshtein averaging"" function. In addition, I'm not sure if the clustering library I'm using supports iteration upon its initial result, so further implementation may be necessary.</p>
",1,2,928,2017-07-13 18:19:44,https://stackoverflow.com/questions/45088295/text-labeling-with-machine-learning
Machine Learning - Huge Only positive text dataset,"<p>I have a dataset with thousand of sentences belonging to a subject. I would like to know what would be best to create a classifier that will predict a text as ""True"" or ""False"" depending on whether they talk about that subject or not.</p>

<p>I've been using solutions with <strong>Weka</strong> (basic classifiers) and <strong>Tensorflow</strong> (neural network approaches).</p>

<p>I use string to word vector to preprocess the data.</p>

<p>Since there are no negative samples, I deal with a single class. I've tried one-class classifier (<em>libSVM</em> in Weka) but the number of false positives is so high I cannot use it.</p>

<p>I also tried adding negative samples but when the text to predict does not fall in the negative space, the classifiers I've tried (NB, CNN,...) tend to predict it as a false positive. I guess it's because of the sheer amount of positive samples</p>

<p>I'm open to discard ML as the tool to predict the new incoming data if necessary</p>

<p>Thanks for any help</p>
","machine-learning, tensorflow, weka, text-classification","<p>My answer is based on the assumption that that adding of at least 100 negative samples for author’s dataset with 1000 positive samples is acceptable for the author of the question, since I have no answer for my question about it to the author yet</p>

<p>Since this case with detecting of specific topic is looks like particular case of topics classification I would recommend using classification approach with the two simple classes 1 class – your topic and another – all other topics for beginning</p>

<p>I succeeded with the same approach for face recognition task – at the beginning I built model with one output neuron with high level of output for face detection and low if no face detected</p>

<p>Nevertheless such approach gave me too low accuracy – less than 80%
But when I tried using 2 output neurons – 1 class for face presence on image and another if no face detected on the image, then it gave me more than 90% accuracy for MLP, even without using of CNN</p>

<p>The key point here is using of SoftMax function for the output layer. It gives significant increase of accuracy. From my experience, it increased accuracy of the MNIST dataset even for MLP from 92% up to 97% for the same model</p>

<p>About dataset. Majority of classification algorithms with a trainer, at least from my experience are more efficient with equal quantity of samples for each class in a training data set. In fact, if I have for 1 class less than 10% of average quantity for other classes it makes model almost useless for the detection of this class. So if you have 1000 samples for your topic, then I suggest creating 1000 samples with as many different topics as possible</p>

<p>Alternatively, if you don’t want to create a such big set of negative samples for your dataset, you can create a smaller set of negative samples for your dataset and use batch training with a size of batch = 2x your negative sample quantity. In order to do so, split your positive samples in n chunks with the size of each chunk ~ negative samples quantity and when train your NN by N batches for each iteration of training process with chunk[i] of positive samples and all your negative samples for each batch. Just be aware, that lower accuracy will be the price for this trade-off</p>

<p>Also, you could consider creation of more generic detector of topics – figure out all possible topics which can present in texts which your model should analyze, for example – 10 topics and create a training dataset with 1000 samples per each topic. It also can give higher accuracy
One more point about the dataset. The best practice is to train your model only with part of a dataset, for example – 80% and use the rest 20% for cross-validation. This cross-validation of unknown previously data for model will give you a good estimation of your model accuracy in real life, not for the training data set and allows to avoid overfitting issues</p>

<p>About building of model. I like doing it by ""from simple to complex"" approach. So I would suggest starting from simple MLP with SoftMax output and dataset with 1000 positive and 1000 negative samples. After reaching 80%-90% accuracy you can consider using CNN for your model, and also I would suggest increasing training dataset quantity, because deep learning algorithms are more efficient with bigger dataset</p>
",0,2,1213,2017-07-15 08:12:15,https://stackoverflow.com/questions/45116034/machine-learning-huge-only-positive-text-dataset
How to show topics of reuters dataset in Keras?,"<p>I use reuters dataset in Keras.</p>

<p>And I want to know the 46 topics' names.</p>

<p>How can I show topics of reuters dataset in Keras?</p>

<p><a href=""https://keras.io/datasets/#reuters-newswire-topics-classification"" rel=""noreferrer"">https://keras.io/datasets/#reuters-newswire-topics-classification</a></p>
","deep-learning, keras, text-classification","<p>Associated mapping of topic labels as per original <a href=""https://martin-thoma.com/nlp-reuters/"" rel=""nofollow noreferrer"">Reuters Dataset</a> with the topic indexes in Keras version is:</p>

<pre><code>['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',
 'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',
 'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',
 'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',
 'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']
</code></pre>

<p>To understand the approach of extracting the above mapping, please follow <a href=""https://github.com/keras-team/keras/issues/12072#issuecomment-458154097"" rel=""nofollow noreferrer"">this thread comment</a>. </p>
",2,11,1107,2017-07-17 07:27:31,https://stackoverflow.com/questions/45138290/how-to-show-topics-of-reuters-dataset-in-keras
Number of features in text mining,"<p>I am trying to make a predictive model based on text mining. I am confused how many features should I set up in my model. I have 1000 document in my analysis (so corpus will take around 700). Number of terms in corpus is around 20 000, so it exceeds number of documents (P >> N). Having so much features has any sense?</p>

<p>Number of features in HashingTF method should be higher than total numbers of terms in the corpus? Or should I make it smaller (like 512 features?)</p>

<p>I am a little bit confused.</p>
","machine-learning, text-mining, text-classification","<p>Assuming you are talking about using just unigrams as features, you are right that we want p &lt; n. (Not citing sources here since you seem to know what this means.)</p>

<p>Finally, to achieve p &lt; n, you could either </p>

<ol>
<li><p>select features with count>=k. Measure performance for various k and select the best k, or-</p></li>
<li><p>use all features but with L1 regularization. </p></li>
</ol>

<p>If you use hashing like you mentioned, you should set number of features less than even 512 because -</p>

<ol>
<li>n=700 and p=512 is still too skewed. </li>
<li>Typically, there are a very small number of important words. It might even be less than 50 in your case. You could try number of hash buckets = {10, 20, 50, 100, 500, 1000} and pick the best one.</li>
</ol>

<p>Good luck! </p>
",0,0,477,2017-07-19 13:24:18,https://stackoverflow.com/questions/45191993/number-of-features-in-text-mining
How to explain to customer why classifier make such decision?,"<p>In our project we use Stochastic Gradient Descent (after CountVectorizer, TfidfTransformer) for classify messages The customer new in ML, so he want to understand (how) why model relates a message to some category. Could you please give suggestions for explanation, of course, without math?</p>
","machine-learning, text-classification","<p>first and foremost if your customer is not from technical background , then never use technical terms in front him. by using technical terms , you are making situation even more worse because he is into new thing and he is confused with your jargon. so never make someone freak out .!!</p>

<p>give him simple examples :</p>

<p>like "" Recommending music on gaana.com , saavn.com "" or "" Recommending Movie on NetFlix "" :</p>

<p>tell him/her "" how a machine will understand what is your like and dislike ? "". or even how you will get to know my music liking and disliking ? can you ?</p>

<p>by finding my favorite. right !! 
how you get to know this my favorite ? Either by asking me !! or looking into my most top rated or more liked or you heard i always listen some common songs !! </p>

<p>Exactly this above process you followed is called "" finding pattern in Machine Learning "" and using this method machine able to recommend new songs, movies, ..etc.</p>

<p>i'll also suggest don't explain above thing . because your customer is not that much dumb who is dealing with your work/product !! </p>

<p>give him real time use cases.. like recommendation process , or in social media recommending friends !!.. and many more...</p>
",0,-1,29,2017-07-20 09:44:45,https://stackoverflow.com/questions/45211095/how-to-explain-to-customer-why-classifier-make-such-decision
ROC for multiclass classification,"<p>I'm doing different text classification experiments. Now I need to calculate the AUC-ROC for each task. For the binary classifications, I already made it work with this code:</p>

<pre><code>scaler = StandardScaler(with_mean=False)

enc = LabelEncoder()
y = enc.fit_transform(labels)

feat_sel = SelectKBest(mutual_info_classif, k=200)

clf = linear_model.LogisticRegression()

pipe = Pipeline([('vectorizer', DictVectorizer()),
                 ('scaler', StandardScaler(with_mean=False)),
                 ('mutual_info', feat_sel),
                 ('logistregress', clf)])
y_pred = model_selection.cross_val_predict(pipe, instances, y, cv=10)
# instances is a list of dictionaries

#visualisation ROC-AUC

fpr, tpr, thresholds = roc_curve(y, y_pred)
auc = auc(fpr, tpr)
print('auc =', auc)

plt.figure()
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b',
label='AUC = %0.2f'% auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.2])
plt.ylim([-0.1,1.2])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()
</code></pre>

<p>But now I need to do it for the multiclass classification task. I read somewhere that I need to binarize the labels, but I really don't get how to calculate ROC for multiclass classification. Tips?</p>
","python, scikit-learn, text-classification, roc, multiclass-classification","<p>As people mentioned in comments you have to convert your problem into binary by using <code>OneVsAll</code> approach, so you'll have <code>n_class</code> number of ROC curves.</p>
<p>A simple example:</p>
<pre><code>from sklearn.metrics import roc_curve, auc
from sklearn import datasets
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

iris = datasets.load_iris()
X, y = iris.data, iris.target

y = label_binarize(y, classes=[0,1,2])
n_classes = 3

# shuffle and split training and test sets
X_train, X_test, y_train, y_test =\
    train_test_split(X, y, test_size=0.33, random_state=0)

# classifier
clf = OneVsRestClassifier(LinearSVC(random_state=0))
y_score = clf.fit(X_train, y_train).decision_function(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot of a ROC curve for a specific class
for i in range(n_classes):
    plt.figure()
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc=&quot;lower right&quot;)
    plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/ByrqW.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ByrqW.png"" alt=""enter image description here"" /></a><a href=""https://i.sstatic.net/pqC8U.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/pqC8U.png"" alt=""enter image description here"" /></a><a href=""https://i.sstatic.net/ydVNu.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ydVNu.png"" alt=""enter image description here"" /></a></p>
",49,42,105071,2017-07-26 16:16:09,https://stackoverflow.com/questions/45332410/roc-for-multiclass-classification
Sklearn - feature extraction from text - normalize text features by merging plural and singular forms,"<p>I am doing some text classification right now using sklearn.</p>

<p>As first step I obviously need to use vectorizer - either CountVectorizer or TfIdfVectorizer. The issue which I want to tackle is that in my documents often times I have singular and plural forms of same word. When performing vectorization I want to 'merge' singular and plural forms and treat them as a same text feature.</p>

<p>Obviously I can manually pre-process texts and just replace all plural word forms with singular word forms when I know which words have this issue. But maybe there is some way to do it in a more automated way, when words which are extremely similar to each other are merged into same feature?</p>

<p><strong>UPDATE.</strong></p>

<p>Based on the answer provided earlier, I needed to perform a stemming. Below is a sample code which stems all words in 'review' column of a dataframe DF, which I then use in vectorization and classification. Just in case anyone finds it useful.</p>

<pre><code>from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer(""english"")


df['review_token']=df['review'].apply(lambda x : filter(None,x.split("" "")))

df['review_stemmed']=df['review_token'].apply(lambda x : [stemmer.stem(y) for y in x])

df['review_stemmed_sentence']=df['review_stemmed'].apply(lambda x : "" "".join(x))
</code></pre>
","python, scikit-learn, text-mining, feature-extraction, text-classification","<p>I think what you need is stemming, namely removing the endings of words that have a common root, and it's one of the basic operations in preprocessing text data.</p>

<p>Here's some rules for stemming and lemmatization explained: <a href=""https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html</a></p>
",2,1,1180,2017-07-26 20:10:12,https://stackoverflow.com/questions/45336491/sklearn-feature-extraction-from-text-normalize-text-features-by-merging-plur
Found array with dim 3. Estimator expected &lt;= 2,"<p>I am using LDA over a simple collection of documents. my goal is to extract topics, then use the extracted topics as features to evaluate my model.</p>

<p>I decided to use multinomial SVM as the evaluater. not sure its good or not?</p>

<pre><code>import itertools
from gensim.models import ldamodel
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
from sklearn.naive_bayes import MultinomialNB

tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = {'a'}

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

# create sample documents
doc_a = ""Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.""
doc_b = ""My mother spends a lot of time driving my brother around to baseball practice.""
doc_c = ""Some health experts suggest that driving may cause increased tension and blood pressure.""
doc_d = ""I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.""
doc_e = ""Health professionals say that brocolli is good for your health.""

# compile sample documents into a list
doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]

# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:
    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]

    # add tokens to list
    texts.append(stemmed_tokens)

# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]


# generate LDA model
#ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20)

id2word = corpora.Dictionary(texts)
# Creates the Bag of Word corpus.
mm = [id2word.doc2bow(text) for text in texts]

# Trains the LDA models.
lda = ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=4,
                               update_every=1, chunksize=10000, passes=1)


# Assigns the topics to the documents in corpus
a=[]
lda_corpus = lda[mm]
for i in range(len(doc_set)):
    a.append(lda_corpus[i])
    print(lda_corpus[i])
merged_list = list(itertools.chain(*lda_corpus))
print(a)
    #my_list.append(my_list[i])


sv=MultinomialNB()

yvalues = [0,1,2,3]

sv.fit(a,yvalues)
predictclass = sv.predict(a)

testLables=[0,1,2,3]
from sklearn import metrics, tree
#yacc=metrics.accuracy_score(testLables,predictclass)
#print (yacc)
</code></pre>

<p>when I run this code it throws the error mentioned in the subject.</p>

<p>Also this is the output of LDA model(topic doc distribution) that I feed to SVM:</p>

<pre><code>[[(0, 0.95533888404477663), (1, 0.014775921798986477), (2, 0.015161897773308793), (3, 0.014723296382928375)], [(0, 0.019079556242721694), (1, 0.017932434792585779), (2, 0.94498655991579728), (3, 0.018001449048895311)], [(0, 0.017957955483631164), (1, 0.017900184473362918), (2, 0.018133572636989413), (3, 0.9460082874060165)], [(0, 0.96554611572184923), (1, 0.011407838337200715), (2, 0.011537900721487016), (3, 0.011508145219463113)], [(0, 0.023306931039431281), (1, 0.022823706054846005), (2, 0.93072240824085961), (3, 0.023146954664863096)]]
</code></pre>

<p>My labels here are 0,1,2,3 .</p>

<p>I found a response <a href=""https://stackoverflow.com/questions/34972142/sklearn-logistic-regression-valueerror-found-array-with-dim-3-estimator-expec"">here</a></p>

<p>but when I write down :</p>

<pre><code>nsamples, nx, ny = a.shape
d2_train_dataset = a.reshape((nsamples,nx*ny))
</code></pre>

<p>According to my case, it does not work. actually a does not have shape method.</p>

<p><strong>whole traceback error</strong></p>

<pre><code>Traceback (most recent call last):
  File ""/home/saria/PycharmProjects/TfidfLDA/test3.py"", line 87, in &lt;module&gt;
    sv.fit(a,yvalues)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/naive_bayes.py"", line 562, in fit
    X, y = check_X_y(X, y, 'csr')
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/utils/validation.py"", line 521, in check_X_y
    ensure_min_features, warn_on_dtype, estimator)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/utils/validation.py"", line 405, in check_array
    % (array.ndim, estimator_name))
ValueError: Found array with dim 3. Estimator expected &lt;= 2.
</code></pre>
","python, machine-learning, svm, text-classification, lda","<p>The error trying to call the <code>fit</code> on <code>MultinomialNB</code> is being raised because the data contained in <code>a</code> is in greater than 2-dimensions.  As constructed now <code>a</code> is feeding a list of tuples for each document, which is not allowed by the model.</p>

<p>Since the first part of the tuple is just the topic label you can remove that value from the tuple and reconstruct your data into a 2-dimensional matrix.  The code below will do that,</p>

<pre><code>new_a = []
new_y = []
for x in a:
    temp_a = []
    sorted_labels = sorted(x, key=lambda x: x[1], reverse=True)
    new_y.append(sorted_labels[0][0])
    for z in x:
        temp_a.append(z[1])
    new_a.append(temp_a)
</code></pre>

<p><code>new_a</code> will be the list of documents where each document will contain the scores for topics 0, 1, 2, and 3.  You can then call <code>sv.fit(new_a, yvalues)</code> to fit your model.</p>
",1,0,4637,2017-08-08 18:01:53,https://stackoverflow.com/questions/45575127/found-array-with-dim-3-estimator-expected-2
Using feature selection with LinearSVC in python,"<p>I have a task to create a multi class classifier for product titles to classify them into 11 categories. I'm using scikit's <code>LinearSVC</code> for classification. I preprocessed the product titles first by removing stopwords, using POS tags for lemmatization, and using bigrams with TFIDF vectorizer.</p>

<p>I now want to use the <code>chi2</code> method of feature selection to elimination not important features from these and then do the training. But how do I use <code>chi2</code> with my model. Below is the code:</p>

<pre><code>def identity(arg):
    """"""
    Simple identity function works as a passthrough.
    """"""
    return arg

class NLTKPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self, stopwords=None, punct=None,
                 lower=True, strip=True):

        self.lower      = lower
        self.strip      = strip
        self.stopwords  = stopwords or set(sw.words('english'))
        self.punct      = punct or set(string.punctuation)
        self.lemmatizer = WordNetLemmatizer()

    def fit(self, X, y=None):
        return self

    def inverse_transform(self, X):
        return ["" "".join(doc) for doc in X]

    def transform(self, X):
        return [
            list(self.tokenize(doc)) for doc in X
        ]

    def tokenize(self, document):

        # Break the document into sentences
        for sent in sent_tokenize(document):
            # Break the sentence into part of speech tagged tokens
            for token, tag in pos_tag(wordpunct_tokenize(sent)):
                # Apply preprocessing to the token
                token = token.lower() if self.lower else token
                token = token.strip() if self.strip else token
                token = token.strip('_') if self.strip else token
                token = token.strip('*') if self.strip else token

                # If stopword, ignore token and continue
                if token in self.stopwords or token.isdigit() == True:
                    continue

                # If punctuation, ignore token and continue
                if all(char in self.punct for char in token):
                    continue


                # Lemmatize the token and yield
                lemma = self.lemmatize(token, tag)


                yield lemma

    def lemmatize(self, token, tag):
        tag = {
            'N': wn.NOUN,
            'V': wn.VERB,
            'R': wn.ADV,
            'J': wn.ADJ
        }.get(tag[0], wn.NOUN)

        return self.lemmatizer.lemmatize(token, tag)

def build_and_evaluate(X, y,
    classifier=LinearSVC, outpath=None, verbose=True):

    def build(classifier, X, y=None):

        if isinstance(classifier, type):
            classifier = classifier()

        model = Pipeline([
            ('preprocessor', NLTKPreprocessor()),
            ('vectorizer', TfidfVectorizer(
                tokenizer=identity, preprocessor=None, ngram_range = (1,2), min_df = 4, lowercase=False
            )),
            ('classifier', classifier),
        ])

        model.fit(X, y)
        return model

    labels = LabelEncoder()
    y = labels.fit_transform(y)

    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2)
    model = build(classifier, X_train, y_train)

    y_pred = model.predict(X_test)
    print(clsr(y_test, y_pred, target_names=labels.classes_))

    return model


if __name__ == '__main__':
    df = pd.read_csv('file.txt', sep='\t', quoting=csv.QUOTE_NONE, usecols=[6, 12], skiprows=[0],
                           names=[""category"", ""product_title""])


    freq = df['category'].value_counts()[:10].to_dict()
    new_categories = []
    for i, category in enumerate(df['category']):
        if category in freq.keys():
            new_categories.append(category)
        else:
            new_categories.append('Other')

    df['new_categories'] = new_categories

    X = df['product_title'].tolist()
    X = [i.replace('""', '') for i in X]
    newlist=[]
    for i in X:
        i = i.decode('utf8')
        newlist.append(i)

    y = df['new_categories'].tolist()

    model = build_and_evaluate(newlist,y)
</code></pre>

<p>Can anyone help me with how to use <code>chi2</code> with the above code? Thanks!</p>
","python, scikit-learn, pipeline, text-classification, feature-selection","<p>Declare it the same way you did in for <code>NLTKPreprocessor</code> but just above the classifier inside the pipeline.</p>

<p>Declare your pipeline as below:</p>

<pre><code>model = Pipeline([
        ('preprocessor', NLTKPreprocessor()),
        ('vectorizer', TfidfVectorizer(
            tokenizer=identity, preprocessor=None, ngram_range = (1,2), min_df = 4, lowercase=False
        )),
        ('selector', SelectKBest(chi2, k=10)),
        ('classifier', classifier),
    ])
</code></pre>

<p>Experiment with the param <code>k</code> to set different number of selected features. I have used 10 here, but you need to tweak that. Maybe using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"" rel=""nofollow noreferrer"">GridSearchCV</a>.</p>
",2,0,603,2017-08-14 02:30:11,https://stackoverflow.com/questions/45666692/using-feature-selection-with-linearsvc-in-python
Reducing the output of Wordnet to one meaning,"<p>First off, let me introduce you to my problem: for a project I have to classify 8000 questions and put them into 7 categories (constitution, sports, geography, history, science, education and tech). Because the questions are very short SVM's don't make much sense, so I just created a list of words for every category. To improve accuracy I have to expand these lists, so unlabeled strings can be put into categories. On the internet I heard about WordNet to get synonyms of words (which makes sense for me, because I need as many synonyms for my words as possible).
But here comes the problem: WordNet shows under</p>

<pre><code>from nltk.corpus import wordnet as wn
for synset in wn.synsets(word):
    for lemma in synset.lemmas():
        print(lemma.name())
</code></pre>

<p>all the related words. An example is the word <strong>capital</strong>: I just mean capital in the sense of the capital city of a country, but WordNet returns the words <strong>capital, working, capital letter, upper case, upper-case, majuscule and Capital Washington</strong>.
Obviously, I don't need the word <em>upper-case</em> in a bag of words for geography.
So I ask you if there is any possibility to reduce WordNet to only one meaning or if there is any alternative that I can use.</p>

<p>Sincerely, James</p>
","python, python-3.x, wordnet, text-classification","<p>You need to find the synonyms for a specific lemma (canonical dictionary entry; a word with a single definition). I'll simply include the <a href=""https://stackoverflow.com/questions/42038337/what-is-the-connection-or-difference-between-lemma-and-synset-in-wordnet"">link</a> I posted in the comments, and wish you good luck.</p>
",1,1,362,2017-08-19 18:28:46,https://stackoverflow.com/questions/45775028/reducing-the-output-of-wordnet-to-one-meaning
how to classify input text under different categories,"<p>text= ""my dog is a rice eater"", ""I want to buy an a new"",""my cat prefers chocolate milk""</p>

<p>how could I extract keywords from these text (or text corpora) and classify them in different categories (i.e. dog, cat be categorized as Pet and rice, chocolate milk be categorized as Food)</p>
","r, text-classification","<p>You were down voted because the question does not provide enough detail as to what you mean by ""classify"" and because you did not show what target outcome you wish to achieve.</p>

<p>Here's a basic answer, however: You can create a dictionary and count the hits according to the dictionary.  In <strong>quanteda</strong>, it works like this:</p>

<pre><code>text &lt;- c(""my dog is a rice eater"", 
          ""I want to buy an a new"",
          ""my cat prefers chocolate milk"")

library(""quanteda"")

fooddict &lt;- dictionary(list(pet = c(""cat"", ""dog""),
                            food = c(""rice"", ""chocolate milk"")))

dfm(text, dictionary = fooddict)
# Document-feature matrix of: 3 documents, 2 features (33.3% sparse).
# 3 x 2 sparse Matrix of class ""dfmSparse""
#        features
# docs    pet food
#   text1   1    1
#   text2   0    0
#   text3   1    1
</code></pre>
",2,0,928,2017-08-22 11:27:28,https://stackoverflow.com/questions/45816374/how-to-classify-input-text-under-different-categories
Why are Logistic Regression and SVM predictions multiplied by constants at the end?,"<p>I'm currently trying to understand certain high-level classification problems and have come across some code from a Kaggle competition that ran in 2012. The competition discussion board are (<a href=""https://machinelearningmastery.com/machine-learning-in-python-step-by-step/"" rel=""nofollow noreferrer"">here</a>) and the winning code is (<a href=""https://kaggle2.blob.core.windows.net/forum-message-attachments/4809/model6.py"" rel=""nofollow noreferrer"">here</a>). At almost the end of the code at line 223 the predicted values in list of two arrays are multiplied by 0.4 and 0.6 respectively and then added together. This is the line <code>final_pred = preds[0] * 0.4 + preds[1] * 0.6</code>. My question is, why are the values multiplied before being returned as an array to the calling function? After the array is returned, its values are saved to CSV so no more ""processing"" is made. The models used are Logistic Regression and SVM.svc but this happens after all the models finish their business with the data and after the data is predicted using <code>pred = model.predict_proba(X_test)</code>.</p>

<p>Can anyone please give me some information as to why this happens?</p>

<p><strong>EDIT to add the function's code for completeness' sake</strong>
This code is part of a longer program that predicts (binary [0,1]) text as either an insult or non-insult. The links to the original code are included in my original post.</p>

<pre><code>def runClassifiers(X_train, y_train, X_test, y_test = None, verbose = True):

models = [  linear_model.LogisticRegression(C=3), 
            svm.SVC(C=0.3,kernel='linear', probability=True)]
# another two classifiers are commented out by the original author

dense = [False, False, True, True]    # if model needs dense matrix

X_train_dense = X_train.todense()
X_test_dense  = X_test.todense()

preds = []
for ndx, model in enumerate(models):
    t0 = time()
    print ""Training: "", model, 20 * '_'        
    if dense[ndx]:
        model.fit(X_train_dense, y_train)
        pred = model.predict_proba(X_test_dense)    
    else:
        model.fit(X_train, y_train)
        pred = model.predict_proba(X_test)    
    print ""Training time: %0.3fs"" % (time() - t0)
    preds.append(array(pred[:,1]))

final_pred = preds[0]*0.4 + preds[1]*0.6
return final_pred
</code></pre>
","python, text-classification, kaggle","<p>This is just a meta-predictor using two sub-predictors (LogReg and SVM).</p>

<p>There are tons of approaches of combining multiple prediction-models and this <a href=""https://en.wikipedia.org/wiki/Convex_combination"" rel=""nofollow noreferrer"">convex-combination</a> is one of the most simple ones.</p>

<p>The values are probably also trained with some cross-validation approach, leading to these numbers where the SVM-classifier is taken more seriously!</p>

<p>I'm not sure what exactly the task is, but i think the number of classes should be 2 (0 and 1 or -1 and 1; at least in this prediction-step; there might be some outer OvO or OvA scheme) to make sense here.</p>
",2,0,62,2017-08-25 13:19:23,https://stackoverflow.com/questions/45882389/why-are-logistic-regression-and-svm-predictions-multiplied-by-constants-at-the-e
How to test data with R e1071 SVM multiclass,"<p>First time I'm using R and e1071 package and SVM multiclass! I'm very confused, then. The goal is: if I have a sentence with sunny; it will be classified as ""yes"" sentence; if I have a sentence with cloud, it will be classified as ""maybe"", if I have a sentence with rainy; il will be classified ad ""no"". The true goal is to do some text classification for my research.</p>

<p>I have two files:</p>

<ul>
<li>train.csv: a file where there are two columns/Variables one is the
data, the other is the label.</li>
</ul>

<p>Example: </p>

<pre><code>                  V1    V2
1              sunny   yes
2        sunny sunny   yes
3  sunny rainy sunny   yes
4  sunny cloud sunny   yes
5              rainy    no
6        rainy rainy    no
7  rainy sunny rainy    no
8  rainy cloud rainy    no
9              cloud maybe
10       cloud cloud maybe
11 cloud rainy cloud maybe
12 cloud sunny cloud maybe
</code></pre>

<ul>
<li>test.csv: in this file there are the new data to be classified.</li>
</ul>

<p>Example:</p>

<pre><code>      V1
1  sunny
2  rainy
3  hello
4  cloud
5      a
6      b
7  cloud
8      d
9      e
10     f
11     g
12 hello
</code></pre>

<p>Following the examples for the iris dataset 
(<a href=""https://cran.r-project.org/web/packages/e1071/e1071.pdf"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/e1071/e1071.pdf</a> and <a href=""http://rischanlab.github.io/SVM.html"" rel=""nofollow noreferrer"">http://rischanlab.github.io/SVM.html</a>)
I created my model and then test the training data in this way:</p>

<pre><code>&gt; library(e1071)
&gt; train &lt;- read.csv(file=""C:/Users/Stef/Desktop/train.csv"", sep = "";"", header = FALSE)
&gt; test &lt;- read.csv(file=""C:/Users/Stef/Desktop/test.csv"", sep = "";"", header = FALSE)
&gt; attach(train)
&gt; x &lt;- subset(train, select=-V2)
&gt; y &lt;- V2
&gt; model &lt;- svm(V2 ~ ., data = train, probability=TRUE)
&gt; summary(model)

Call:
svm(formula = V2 ~ ., data = train, probability = TRUE)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.08333333 

Number of Support Vectors:  12

 ( 4 4 4 )


Number of Classes:  3 

Levels: 
 maybe no yes

&gt; pred &lt;- predict(model,x)
&gt; system.time(pred &lt;- predict(model,x))
   user  system elapsed 
      0       0       0 
&gt; table(pred,y)
       y
pred    maybe no yes
  maybe     4  0   0
  no        0  4   0
  yes       0  0   4
&gt; pred
    1     2     3     4     5     6     7     8     9    10    11    12 
  yes   yes   yes   yes    no    no    no    no maybe maybe maybe maybe 
Levels: maybe no yes
</code></pre>

<p>I think it's ok until now.
Now the question is: what about the test data? I didn't find anything for the test data. Then, I thought that maybe I should test the model with the test data. And I did this:</p>

<pre><code>&gt; test
      V1
1  sunny
2  rainy
3  hello
4  cloud
5      a
6      b
7  cloud
8      d
9      e
10     f
11     g
12 hello
&gt; z &lt;- subset(test, select=V1)
&gt; pred &lt;-predict(model,z)
Error in predict.svm(model, z) : test data does not match model !
</code></pre>

<p>What is wrong here? Can you please explain me how can I test new data using the old train model?
Thank you</p>

<p><strong>EDIT</strong></p>

<p>These are the first 5 rows for each file .csv</p>

<pre><code>&gt; head(train,5)
                 V1  V2
1             sunny yes
2       sunny sunny yes
3 sunny rainy sunny yes
4 sunny cloud sunny yes
5             rainy  no
&gt; head(test,5)
     V1
1 sunny
2 rainy
3 hello
4 cloud
5     a
</code></pre>
","r, classification, svm, text-classification","<p>Factors in train and test dataset are different here so you would need to fix it first.</p>

<pre><code>library(e1071)
#sample data
train_data &lt;- data.frame(V1 = c(""sunny"",""sunny sunny"",""rainy"",""rainy rainy"",""cloud"",""cloud cloud""),
                         V2= c(""yes"",""yes"",""no"",""no"",""maybe"",""maybe""))
test_data &lt;- data.frame(V1 = c(""sunny"",""rainy"",""hello"",""cloud""))

#fix levels in train_data &amp; test_data dataset before running model
train_data$ind &lt;- ""train""
test_data$ind &lt;- ""test""
merged_data &lt;- rbind(train_data[,-grep(""V2"", colnames(train_data))],test_data)
#train data
train &lt;- merged_data[merged_data$ind==""train"",]
train$V2 &lt;- train_data$V2
train &lt;- train[,-grep(""ind"", colnames(train))]
#test data
test &lt;- merged_data[merged_data$ind==""test"",]
test &lt;- data.frame(V1 = test[,-grep(""ind"", colnames(test))])

#svm model
svm_model &lt;- svm(V2 ~ ., data = train, probability=TRUE)
summary(svm_model)
train_pred &lt;- predict(svm_model,train[""V1""])
table(train_pred,train$V2)

#prediction on test data
test$test_pred &lt;- predict(svm_model,test)
test
</code></pre>

<p>Hope this helps!</p>
",1,1,1595,2017-09-01 18:14:40,https://stackoverflow.com/questions/46005892/how-to-test-data-with-r-e1071-svm-multiclass
Scikit Text Classification – Bad input shape error,"<p>I have modified this tutorial (<a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a>) to build a text classifier on the Reuters Corpus. However, I get a bad input shape error: </p>

<p>EDIT: Thanks to the help of @Vivek Kumar, I have solved the Bad input shape issue. However, now I get an AttributeError: lower not found. After some research I think that it might have something to do with the Reuters corpus not having the correct form. Is there any way I can fix this?</p>

<p>This is my Code:</p>

<pre><code>from sklearn.datasets import fetch_rcv1 #import reuters corpus
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

rcv1 = fetch_rcv1()


reuters_train = fetch_rcv1(subset='train', shuffle=True, random_state=42)
reuters_train.target_names

count_vect = CountVectorizer()

train_counts = count_vect.fit_transform(reuters_train.data)
train_counts.shape
count_vect.vocabulary_.get(u'alogrithm')

tf_transformer = TfidfTransformer(use_idf=False).fit(train_counts)
train_tf = tf_transformer.transform(train_counts)
train_tf.shape
tfidf_transformer = TfidfTransformer()
train_tfidf = tfidf_transformer.fit_transform(train_counts)
train_tfidf.shape

clf = MultinomialNB().fit(train_tfidf, reuters_train.target)

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB()),])

text_clf.fit(reuters_train.data, reuters_train.target)
Pipeline(...)

import numpy as np

reuters_testset = fetch_rcv1(subset='test', shuffle=True, random_state=42)

reuters_test = reuters_testset.data

predicted = text_clf.predict(reuters_test)

np.mean(predicted == reuters_test.target)
</code></pre>

<p>I'm a real beginner at programming and NLP, so I really don't know very much about all of that stuff (yet).
Thanks for any advice and help!</p>
","python, scikit-learn, text-classification, valueerror","<p>Thats because you are not using the actual data in the CountVectorizer. You are using <code>reuters_train</code> whereas you should be using <code>reuters_train.data</code>. </p>

<p>Change:</p>

<pre><code>train_counts = count_vect.fit_transform(reuters_train)
</code></pre>

<p>to:</p>

<pre><code>train_counts = count_vect.fit_transform(reuters_train.data)
</code></pre>

<p>Also CountVectorizer + TfidfTransformer = <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a>. So I would recommend using that inplace of two objects.</p>

<p>On further reading of the description of <a href=""http://scikit-learn.org/stable/datasets/rcv1.html#rcv1-dataset"" rel=""nofollow noreferrer"">RCV1 dataset here</a>, its given that the <code>.data</code> contains:</p>

<blockquote>
  <p>Non-zero values contains cosine-normalized, log TF-IDF vectors.</p>
</blockquote>

<p>So there is no need to actually do the CountVectorizer and TfidfTransformer on the data and you can directly use it like this:</p>

<pre><code>clf = MultinomialNB().fit(reuters_train.data, reuters_train.target)
</code></pre>

<p>But you will again encounter an error and this time due to the shape of target data. You see <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.fit"" rel=""nofollow noreferrer""><code>MultinomialNB().fit()</code></a> only works with single dimension targets (may be multi-class or binary) but not with multi-label or multi-output data.</p>

<p><strong>TLDR;</strong> So you need to remove CountVectorizer and TfidfTransformer from your code because its already done in the data and you need to change the classifier MultinomialNB to any other which supports 2-d in target <code>y</code> like maybe DecisionTreeClassifier or others. </p>
",1,2,917,2017-09-03 19:17:34,https://stackoverflow.com/questions/46027033/scikit-text-classification-bad-input-shape-error
Keras Text Classification Custom Dataset from csv,"<p>I'm trying to build a Keras model to classify different articles into topics. Each article only has one topic. I have a custom csv file with the following structure:</p>

<pre><code>""topic1"",""article1""
""topic2"",""article2""
</code></pre>

<p>I'm trying to train my model for this data set, but unfortunately I get an error because the data from the csv was not yet processed to be a vector. </p>

<p>This is my code:</p>

<pre><code>from __future__ import print_function
import csv
import numpy as np
import keras
import os
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.preprocessing.text import Tokenizer

max_words = 1000
batch_size = 32
epochs = 5
model_file_name = 'model.h5'


def load_data(word_max, test_split):
    xs = []
    labels = []
    counter = 0
    with open('data.csv', 'r') as f:
        reader = csv.reader(f)
        for line in reader:
            if counter &gt; word_max:
                break
            xs.append(line[1])
            labels.append(line[0])
            counter += 1
    idx = int(len(xs) * (1 - test_split))
    train_x, train_y = np.array(xs[:idx]), np.array(labels[:idx])
    test_x, test_y = np.array(xs[idx:]), np.array(labels[idx:])
    return (train_x, train_y), (test_x, test_y)


print('Loading data...')
(x_train, y_train), (x_test, y_test) = load_data(max_words, 0.3)

print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

num_classes = np.max(y_train) + 1
print(num_classes, 'classes')

print('Vectorizing sequence data...')
tokenizer = Tokenizer(num_words=max_words)
x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')
x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

print('Convert class vector to binary class matrix '
      '(for use with categorical_crossentropy)')
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)

if os.path.isfile(model_file_name):
    model = keras.models.load_model(model_file_name)
else:
    print('Building model...')
    model = Sequential()
    model.add(Dense(512, input_shape=(max_words,)))
    model.add(Activation('relu'))
    model.add(Dropout(0.35))
    model.add(Dense(num_classes))
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    history = model.fit(x_train, y_train,
                        batch_size=batch_size,
                        epochs=epochs,
                        verbose=1,
                        validation_split=0.1)
    score = model.evaluate(x_test, y_test,
                           batch_size=batch_size, verbose=1)
    print('Test score:', score[0])
    print('Test accuracy:', score[1])
    model.save(model_file_name)
</code></pre>

<p>How can I load my data correctly to train my model? And how would I predict a topic for a given text with <code>model.predict</code>?</p>

<p>EDIT:
I've got the training of the model working by changing the process of loading the training data like:</p>

<pre><code> print('Loading data...')
(x_train, y_train), (x_test, y_test) = load_data(max_words, 0.3)

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(x_train)
x_train = tokenizer.texts_to_sequences(x_train)
tokenizer.fit_on_texts(y_train)
y_train = tokenizer.texts_to_sequences(y_train)
tokenizer.fit_on_texts(x_test)
x_test = tokenizer.texts_to_sequences(x_test)
tokenizer.fit_on_texts(y_test)
y_test = tokenizer.texts_to_sequences(y_test)

print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')
</code></pre>

<p>So, how can I use this and predict a label from a given article?:</p>

<pre><code>model = keras.models.load_model(model_file_name)
to_predict = np.array(['The sun is shining.'])
# Predict label from example article
label = model.predict(...)
print(label)
</code></pre>
","python, csv, numpy, keras, text-classification","<p>You could not convert the article to the numpy array directly, you need to use the same tokenizer to convert the article to a numpy array.</p>

<pre><code>array = tokenizer.texts_to_sequences([title]) # the tokenizer must be same with the training tokenizer
array = np.asanyarray(array)
array = sequence.pad_sequences(array, maxlen=max_words, padding='post', truncating='post')
array = np.asarray(array)
result = model.predict(array)
</code></pre>

<p>Then, the result would be a vector of probability(because you use the softmax function), like [0.3,0.7], each element represents the probability to the corresponding topic. the topic which has the highest probability will be the prediction.</p>
",1,1,3022,2017-09-05 17:44:53,https://stackoverflow.com/questions/46060638/keras-text-classification-custom-dataset-from-csv
"CNN converges to same accuracy regardless of hyperparameters, what does this indicate?","<p>I have written tensorflow code based on:</p>

<p><a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a></p>

<p>but using precomputed word embeddings from the GoogleNews word2vec 300 dimension model.</p>

<p>I created my own data from the UCML News Aggregator Dataset in which I parsed the content of the news articles and have created my own labels.</p>

<p>Due to the size of the articles I use TF-IDF to filter out the top 120 words per article and embed those into 300 dimensions.</p>

<p>When I run the CNN I created regardless of the hyper parameters it converges to a small general accuracy, around 38%.</p>

<p>Hyper parameters changed: </p>

<p>Various filter sizes: </p>

<p>I've tried a single filter of 1,2,3
Combinations of filters [3,4,5], [1,3,4]</p>

<p>Learning Rate:</p>

<p>I've varied this from very low to very high, very low doesn't converge to 38% but anything between 0.0001 and 0.4 does.</p>

<p>Batch Size:</p>

<p>Tried many ranges between 5 and 100.</p>

<p>Weight and Bias Initialization:</p>

<p>Set stddev of weights between 0.4 and 0.01.
Set bias initial values between 0 and 0.1.
Tried using the xavier initializer for the conv2d weights. </p>

<p>Dataset Size:</p>

<p>I have only tried on two partial data sets, one with 15 000 training data, and the other on the 5000 test data. In total I have 263 000 data to train on. There is no accuracy difference whether trained and evaluated on the 15 000 training data or by using the 5000 test data as the training data (to save testing time).</p>

<p>I've run successful classifications on the 15 000 / 5000 split using a feed forward network with a BoW input (93% accurate), TF-IDF with SVM (92%), and TF-IDF with Native Bayes (91.5%). So I don't think it is the data.</p>

<p>What does this imply? Is the model just a poor model for this task? Is there an error in my work?</p>

<p>I feel like my do_eval function is incorrect to evaluate the accuracy / loss over an epoch of the data:</p>

<pre><code>        def do_eval(data_set,
                label_set,
                batch_size):
            """"""
            Runs one evaluation against the full epoch of data.
            data_set: The set of embeddings to eval
            label_set: the set of labels to eval
            """"""
            # And run one epoch of eval.

            true_count = 0  # Counts the number of correct predictions.
            steps_per_epoch = len(label_set) // batch_size
            num_examples = steps_per_epoch * batch_size
            totalLoss = 0
            # Need to compute eval accuracy
            for evalStep in xrange(steps_per_epoch):
                input_batch, label_batch = nextBatch(data_set, labels_set, batchSize)
                evalAcc, evalLoss = eval_step(input_batch, label_batch)
                true_count += evalAcc * batchSize
                totalLoss += evalLoss
            precision = float(true_count) / num_examples
            print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))
            print(""Eval Loss: "" + str(totalLoss))
</code></pre>

<p>The entire model is as follows:</p>

<pre><code>class TextCNN(object):
""""""
A CNN for text classification
Uses a convolutional, max-pooling and softmax layer.
""""""

    def __init__(
            self, batchSize, numWords, num_classes,
            embedding_size, filter_sizes, num_filters):

        # Set place holders
        self.input_placeholder = tf.placeholder(tf.float32,[batchSize,numWords,embedding_size,1])
        self.labels = tf.placeholder(tf.int32, [batchSize,num_classes])
        self.pKeep = tf.placeholder(tf.float32)

        # Inference
        '''
        Ready to build conv layers followed by max pooling layers
        Each conv layer produces a different shaped output so need to loop over
        them and create a layer for each and then merge the results
        '''
        pooled_outputs = []
        for i, filter_size in enumerate(filter_sizes):
            with tf.name_scope(""conv-maxpool-%s"" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, embedding_size, 1, num_filters]

                # W: Filter matrix
                W = tf.Variable(tf.truncated_normal(filter_shape,stddev=0.01), name='W')
                b = tf.Variable(tf.constant(0.0,shape=[num_filters]),name=""b"")


                # Valid padding: Narrow convolution (no edge padded so filter slides over everything)
                # Output size = (input_size (numWords in this case) + 2 * padding (0 in this case) - filter_size) + 1
                conv = tf.nn.conv2d(
                    self.input_placeholder,
                    W,
                    strides=[1, 1, 1, 1],
                    padding=""VALID"",
                    name=""conv"")

                # Apply nonlinearity i.e add the bias to Wx + b
                # Where Wx is the conv layer above
                # Then run it through the activation function
                h = tf.nn.relu(tf.nn.bias_add(conv, b),name='relu')

                # Max-pooling over the outputs
                # Max-pool to control the output size
                # By taking only the best features determined by the filter
                # Ksize is the size of the window of the input tensor
                pooled = tf.nn.max_pool(
                    h,
                    ksize=[1, numWords - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1],
                    padding='VALID',
                    name=""pool"")

                # Each pooled outputs a tensor of size
                # [batchSize, 1, 1, num_filters] where num_filters represents the
                # Number of features we wanted pooled
                pooled_outputs.append(pooled)

        # Combine all pooled features
        num_filters_total = num_filters * len(filter_sizes)
        # Concat the pool output along the 3rd (num_filters / feature size) dimension
        self.h_pool = tf.concat(pooled_outputs, 3)
        # Flatten
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])

        # Add drop out to regularize the learning curve / accuracy
        with tf.name_scope(""dropout""):
            self.h_drop = tf.nn.dropout(self.h_pool_flat,self.pKeep)

        # Fully connected output layer
        with tf.name_scope(""output""):
            W = tf.Variable(tf.truncated_normal([num_filters_total,num_classes],stddev=0.01),name=""W"")
            b = tf.Variable(tf.constant(0.0,shape=[num_classes]), name='b')
            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name='logits')
            self.predictions = tf.argmax(self.logits, 1, name='predictions')

        # Loss
        with tf.name_scope(""loss""):
            losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.labels,logits=self.logits, name=""xentropy"")
            self.loss = tf.reduce_mean(losses)

        # Accuracy
        with tf.name_scope(""accuracy""):
            correct_predictions = tf.equal(self.predictions, tf.argmax(self.labels,1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")

     ##################################################################################################################
# Running the training
# Define various parameters for network

batchSize = 100
numWords = 120
embedding_size = 300
num_classes = 4
filter_sizes = [3,4,5] # slide over a the number of words, i.e 3 words, 4     words etc...
num_filters = 126
maxSteps = 5000
initial_learning_rate = 0.001
dropoutRate = 1


data_set = np.load(""/home/kevin/Documents/NSERC_2017/articles/classifyDataSet/TestSmaller_CNN_inputMat_0.npy"")
labels_set = np.load(""Test_NN_target_smaller.npy"")


with tf.Graph().as_default():

    sess = tf.Session()

    with sess.as_default():
    cnn = TextCNN(batchSize=batchSize,
                  numWords=numWords,
                  num_classes=num_classes,
                  num_filters=num_filters,
                  embedding_size=embedding_size,
                  filter_sizes=filter_sizes)

        # Define training operation
        # Pick an optimizer, set it's learning rate, and tell it what to minimize

        global_step = tf.Variable(0,name='global_step', trainable=False)
        optimizer = tf.train.AdamOptimizer(initial_learning_rate)
        grads_and_vars = optimizer.compute_gradients(cnn.loss)
        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)

        # Summaries to save for tensor board

        # Set directory
        out_dir = ""/home/kevin/Documents/NSERC_2017/articles/classifyDataSet/tf_logs/CNN_Embedding/""

        # Loss and accuracy summaries
        loss_summary = tf.summary.scalar(""loss"",cnn.loss)
        acc_summary = tf.summary.scalar(""accuracy"", cnn.accuracy)

        # Train summaries
        train_summary_op = tf.summary.merge([loss_summary,acc_summary])
        train_summary_dir = out_dir + ""train/""
        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)

        # Test summaries
        test_summary_op = tf.summary.merge([loss_summary, acc_summary])
        test_summary_dir = out_dir + ""test/""
        test_summary_write = tf.summary.FileWriter(test_summary_dir, sess.graph)

        # Init all variables

        init = tf.global_variables_initializer()
        sess.run(init)

    ############################################################################################

        def train_step(input_data, labels_data):
            '''
            Single training step
            :param input_data: input
            :param labels_data: labels to train to
            '''
            feed_dict = {
                cnn.input_placeholder: input_data,
                cnn.labels: labels_data,
                cnn.pKeep: dropoutRate
            }
            _, step, summaries, loss, accuracy = sess.run(
                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],
            feed_dict=feed_dict)
            train_summary_writer.add_summary(summaries, step)


    ###############################################################################################

        def eval_step(input_data, labels_data, writer=None):
            """"""
            Evaluates model on a test set
            Single step
            """"""
            feed_dict = {
            cnn.input_placeholder: input_data,
            cnn.labels: labels_data,
            cnn.pKeep: 1.0
            }

            step, summaries, loss, accuracy = sess.run(
            [global_step, test_summary_op, cnn.loss, cnn.accuracy],
            feed_dict)
            if writer:
                writer.add_summary(summaries, step)
        return accuracy, loss

    ###############################################################################

        def nextBatch(data_set, labels_set, batchSize):
            '''
            Get the next batch of data
            :param data_set: entire training or test data set
            :param labels_set: entire training or test label set
            :param batchSize: batch size
            :return: a batch of the data and it's corresponding labels
            '''
            # Generate random row indices for the documents
            rand_index = np.random.choice(data_set.shape[0], size=batchSize)

            # Grab the data to give to the feed dicts
            data_batch, labels_batch = data_set[rand_index, :, :], labels_set[rand_index, :]

            # Resize for tensorflow
            data_batch = data_batch.reshape([data_batch.shape[0],data_batch.shape[1],data_batch.shape[2],1])
            return data_batch, labels_batch
 ################################################################################

        def do_eval(data_set,
                label_set,
                batch_size):
            """"""
            Runs one evaluation against the full epoch of data.
            data_set: The set of embeddings to eval
            label_set: the set of labels to eval
            """"""
            # And run one epoch of eval.

            true_count = 0  # Counts the number of correct predictions.
            steps_per_epoch = len(label_set) // batch_size
            num_examples = steps_per_epoch * batch_size
            totalLoss = 0
            # Need to compute eval accuracy
            for evalStep in xrange(steps_per_epoch):
                input_batch, label_batch = nextBatch(data_set, labels_set, batchSize)
                evalAcc, evalLoss = eval_step(input_batch, label_batch)
                true_count += evalAcc * batchSize
                totalLoss += evalLoss
            precision = float(true_count) / num_examples
            print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))
            print(""Eval Loss: "" + str(totalLoss))

    ######################################################################################################
        # Training Loop

        for step in range(maxSteps):
            input_batch, label_batch = nextBatch(data_set,labels_set,batchSize)
            train_step(input_batch,label_batch)

        # Evaluate over the entire data set on last eval
            if step  % 100 == 0:
                print ""On Step : "" + str(step) + "" of "" + str(maxSteps)
                do_eval(data_set, labels_set,batchSize)
</code></pre>

<p>The embedding is done before the model:</p>

<pre><code>def createInputEmbeddedMatrix(corpusPath, maxWords, svName):
    # Create a [docNum, Words per Art, Embedding Size] matrix to fill

    genDocsPath = ""gen_docs_classifyData_smallerTest_TFIDF.npy""
    # corpus = ""newsCorpus_word2vec_All_Corpus.mm""
    dictPath = 'news_word2vec_smallerDict.dict'
    tf_idf_path = ""news_tfIdf_word2vec_All.tfidf_model""

    gen_docs = np.load(genDocsPath)
    dictionary = gensim.corpora.dictionary.Dictionary.load(dictPath)
    tf_idf = gensim.models.tfidfmodel.TfidfModel.load(tf_idf_path)

    corpus = corpora.MmCorpus(corpusPath)
    numOfDocs = len(corpus)
    embedding_size = 300

    id2embedding = np.load(""smallerID2embedding.npy"").item()

    # Need to process in batches as takes up a ton of memory

    step = 5000
    totalSteps = int(np.ceil(numOfDocs / step))

    for i in range(totalSteps):
        # inputMatrix = scipy.sparse.csr_matrix([step,maxWords,embedding_size])
        inputMatrix = np.zeros([step, maxWords, embedding_size])
        start = i * step
        end = start + step
        for docNum in range(start, end):
            print ""On docNum "" + str(docNum) + "" of "" + str(numOfDocs)
            # Extract the top N words
            topWords, wordVal = tf_idfTopWords(docNum, gen_docs, dictionary, tf_idf, maxWords)
            # doc = corpus[docNum]
            # Need to track word dex and doc dex seperate
            # Doc dex because of the batch processing
            wordDex = 0
            docDex = 0
            for wordID in wordVal:
                inputMatrix[docDex, wordDex, :] = id2embedding[wordID]
                wordDex += 1
            docDex += 1

        # Save the batch of input data
        # scipy.sparse.save_npz(svName + ""_%d""  % i, inputMatrix)
        np.save(svName + ""_%d.npy"" % i, inputMatrix)


#####################################################################################
</code></pre>
","tensorflow, conv-neural-network, text-classification","<p>Turns out my error was in the creation of the input matrix.</p>

<pre><code>for i in range(totalSteps):
    # inputMatrix = scipy.sparse.csr_matrix([step,maxWords,embedding_size])
    inputMatrix = np.zeros([step, maxWords, embedding_size])
    start = i * step
    end = start + step
    for docNum in range(start, end):
        print ""On docNum "" + str(docNum) + "" of "" + str(numOfDocs)
        # Extract the top N words
        topWords, wordVal = tf_idfTopWords(docNum, gen_docs, dictionary, tf_idf, maxWords)
        # doc = corpus[docNum]
        # Need to track word dex and doc dex seperate
        # Doc dex because of the batch processing
        wordDex = 0
        docDex = 0
        for wordID in wordVal:
            inputMatrix[docDex, wordDex, :] = id2embedding[wordID]
            wordDex += 1
        docDex += 1
</code></pre>

<p>docDex should not have been reset to 0 on each iteration of the inner loop, I was effectively overwriting the first row of my input matrix and thus the rest were 0's. </p>
",0,0,451,2017-09-19 12:11:55,https://stackoverflow.com/questions/46300155/cnn-converges-to-same-accuracy-regardless-of-hyperparameters-what-does-this-ind
Optimizer and Estimator in Neural Networks,"<p>When I started with Neural it seemed I understood Optimizers and Estimators well.</p>

<p>Estimators: <code>Classifier</code> to classify the value based on sample set and <code>Regressor</code> to predict the value based on sample set.</p>

<p>Optimizer: Using different optimizers (Adam, GradientDescentOptimizer) to minimise the loss function, which could be complex. </p>

<p>I understand every estimators come up with an Default optimizer internally to perform minimising the loss. </p>

<p>Now my question is how do they fit in together and optimize the machine training?</p>
","tensorflow, neural-network, text-classification","<p>short answer: loss function link them together.</p>

<p>for example, if you are doing a classification, your <strong>classifier</strong> can take input and output a <strong>prediction</strong>. then you can calculate your <strong>loss</strong> by take <strong>predicted class</strong> and <strong>ground truth class</strong>. the task of your <strong>optimizer</strong> is to minimize the loss by modifying the parameter of your classifier.</p>
",0,1,291,2017-09-27 07:28:24,https://stackoverflow.com/questions/46441889/optimizer-and-estimator-in-neural-networks
What is the difference between CountVectorizer and CharNGramAnalyzer in scikit-learn?,"<p>I have a confusion between CountVectorizer and  CharNGramAnalyzer. According to my understanding, </p>

<ol>
<li>CountVectorizer builds a count matrix where rows are occurrences counts of different words taking into account the high-dimensional sparsity.</li>
<li>CharNGramAnalyzer builds a count matrix where rows are occurrences counts of different characters. Since it does not take into account words, its does not require to know separation between words and works well for languages other than English.</li>
</ol>

<p>Am I correct about my understanding? If not, I would like a detailed explanation or any source explaining it.</p>
","python, machine-learning, scikit-learn, text-classification","<p>First, check your sklearn version. I feel that you are using an old version of sklearn. The explanation that you gave for <code>CountVectorizer</code> is not right. It does not count the number of different words in the corpus, at least not the current version.</p>

<p>As per the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">docs of CountVectorizer</a>, you need to pass <code>analyzer='word'</code> to make the word count. In the latest version of sklearn, <code>CharNGramAnalyzer</code> is deprecated and now merged with <code>CountVectorizer</code>. Just do <code>analyzer='char'</code> to replicate <code>CharNGramAnalyzer</code>. To verify this check <a href=""http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text</a> has no entry for <code>CharNGramAnalyzer</code></p>
",4,-1,2991,2017-10-06 08:29:07,https://stackoverflow.com/questions/46601522/what-is-the-difference-between-countvectorizer-and-charngramanalyzer-in-scikit-l
Incremental/online learning using SGDClassifier partial_fit method,"<p>I have build a incremental learning model but not sure whether it is right or wrong i have 2 training data first consist 20000 rows and second consist 10000 rows both of them having two columns description and id...in case of offline learning my model is working fine it is classifying correct id for given description..
datafile_train is first training data
datafile_train1 is second training data
I am using SGDClassifier and partial_fit method for incremental </p>

<p>1) Countvectorizer,tfidf and partial_fit</p>

<pre><code>vectorizer = CountVectorizer()
tfidf_transformer = TfidfTransformer()
X_train = vectorizer.fit_transform(datafile_train.loc[:,'description'])
X_train_tfidf = tfidf_transformer.fit_transform(X_train)
clf = linear_model.SGDClassifier(penalty='l2',loss='hinge')
prd=clf.partial_fit(X_train_tfidf, datafile_train.loc[:,'taxonomy_id'],classes=np.unique(datafile_train.loc[:,'taxonomy_id']))
</code></pre>

<p>after this i pickled classifier and again unpickled to use in next partial_fit for incremental learning</p>

<p>2) pickling and unpickling of classifier</p>

<pre><code>def store(prd):
    import pickle
    filename = ""incremental""
    f = open(filename, 'wb')
    pickle.dump(prd, f)
    f.close()
store(prd)

def train_data():
    import pickle
    f = open('incremental', 'rb')
    classifier = pickle.load(f)
    f.close()
    return classifier
    clfp=train_data()
</code></pre>

<p>3) again Countvectorizer,tfidf and partial_fit for new data</p>

<pre><code>vectorizer = CountVectorizer()
tfidf_transformer = TfidfTransformer()
X_train1 = vectorizer.fit_transform(datafile_train1.loc[:,'description'])
X_train_tfidf1 = tfidf_transformer.fit_transform(X_train1)
prd1=clfp.partial_fit(X_train_tfidf1, datafile_train1.loc[:,'taxonomy_id'])
# here clfp is previously trained data which is unpickled
</code></pre>

<p>i have build model like this but when i checked size of pickle file(first trained data) it is 5 MB and when i used this model to trained new data as you can see in second partial fit i have used clfp(5 MB size) after training new data when i pickle train file for second partial_fit it also shows only 5 MB size it should get updated because i am training new data on previously trained data
Is this a correct way to achieve incremental/online learning??
please help i am new to machine learning so it will be good if you explain using code</p>

<p>And this error is thrown</p>

<pre><code>ValueError: Number of features 125897 does not match previous data 124454.
</code></pre>

<p>****Edit (using Hashingvectorizer)</p>

<pre><code>hashing = HashingVectorizer()
X_train_hashing=hashing.fit_transform(datafile_train.loc[:,'description'])
clf = linear_model.SGDClassifier(penalty='l2',loss='hinge')
prd=clf.partial_fit(X_train_hashing, datafile_train.loc[:,'taxonomy_id'],classes=np.unique(datafile_train.loc[:,'taxonomy_id']))
def store(prd):
    import pickle
    filename = ""inc""
    f = open(filename, 'wb')
    pickle.dump(prd, f)
    f.close()
store(prd)
def train_data():
    import pickle
    f = open('inc', 'rb')
    classifier = pickle.load(f)
    f.close()
    return classifier
 clfp=train_data()
</code></pre>

<p>now i am using clfp train model for next partial_fit</p>

<pre><code>X_train_hashing1=hashing.transform(datafile_train1.loc[:,'description'])
prd1=clfp.partial_fit(X_train_hashing1, datafile_train1.loc[:,'taxonomy_id'])
def store(prd1):
    import pickle
    timestr = time.strftime(""%Y%m%d-%H%M%S"")
    filename = ""Train-"" + timestr +"".pickle""
    f = open(filename, 'wb')
    pickle.dump(prd1, f)
    f.close()
store(prd1) 
</code></pre>

<p>In this EDIT it is not giving any error but both pickle file have same size 25.2 MB but second pickle size should be greater than first pickle size because i am using first trained model on new data</p>
","python, machine-learning, scikit-learn, classification, text-classification","<p>I dont think that the saved model size should increase much or maybe at all.</p>
<p>The model is not storing the whole new data sent to <code>partial_fit()</code>, only updating its attributes based on that data. Those attributes once assigned some storage space based on their type (<code>float32, float64</code> etc) will occupy that much space irrespective of their value.</p>
<p>The notable attributes which will change in SGDClassifier are:-</p>
<blockquote>
<p><strong>coef_</strong> : array, shape (1, n_features) if n_classes == 2 else (n_classes, n_features) Weights assigned to the features.</p>
<p><strong>intercept_</strong> : array, shape (1,) if n_classes == 2 else (n_classes,) Constants in decision function.</p>
</blockquote>
<p>So when you initialize the model, they are either not assigned or all initialized to 0. Once you pass your first data to <code>partial_fit()</code>, these values are updated according to the data trying to minimize the loss over the predictions.</p>
<p>When you pass the new data, these values again get updated but they still occupy the same storage space as designated to their type (<code>float32, float64</code> etc).</p>
<p>So thats the reason the saved model dont have their sizes changed.</p>
",1,0,2062,2017-10-10 11:32:46,https://stackoverflow.com/questions/46665694/incremental-online-learning-using-sgdclassifier-partial-fit-method
sentiment analysis and classification of the user based on this tweets?best approach for classifying the users(positive or negative) based on tweets?,"<p>i am working on the classification(positive/negative) of the followers of a twitter account based on the followers tweets ,</p>

<p><strong>collecting data</strong></p>

<ol>
<li>got all the followers and the their tweets from the respective account</li>
<li>sentiment analysis of each tweet and labelled as (positive/negative/neutral)</li>
</ol>

<p><strong>which one of these are right approach?if no... is there any better approach?</strong></p>

<p><strong>my approach 1 for user classification:</strong></p>

<ol>
<li>since polarity score of neutral is 0 ,delete all neutral labelled tweets</li>
<li>took the count of positive and negative tweets

<ol>
<li>there are only 17% of negative tweets out of the all tweets(pos+neg)</li>
</ol></li>
<li>i kept the threshold as 34% and grouped tweets based on the data user

<ol>
<li>out the total tweets(pos+neg) by the user,if the negative tweets account more than 34% i am classifying him as the negative user otherwise positive user</li>
</ol></li>
<li>Results:out of 300 followers i got 19 of them are negative rest of them are positive  </li>
</ol>

<p><strong>my code for approach 1</strong></p>

<pre><code>users=set(classify_followers['users'])
user_to_classify=[]
classify=[]
for user in users:
    user_to_classify.append(user)
    temp=classify_followers[(classify_followers['users']==user)]
    if(temp.shape[0]&gt;1):
        if(('positive' in set(temp['sentiment'])) 
           and ('negative' in set(temp['sentiment'])) ):
            positive_count=temp[(temp['sentiment']=='positive')]['sentiment'].count()
            negetive_count=temp[(temp['sentiment']=='negative')]['sentiment'].count()
            positive_percent=(positive_count/temp.shape[0])*100
            negetive_percent=(negetive_count/temp.shape[0])*100
            if(negetive_percent&gt;=34):
                classify.append('negative')
            else:
                classify.append('positive')
        else:
            if('positive' in set(temp['sentiment'])):
                classify.append('positive')
            else:
                classify.append('negative')
    else:
        if('positive' in set(temp['sentiment'])):
            classify.append('positive')
        else:
            classify.append('negative')
</code></pre>

<p><strong>my approach 2 for user classification:</strong></p>

<ol>
<li>since polarity score of neutral is 0 ,delete all neutral labelled tweets</li>
<li>clustering the tweets using k-means algorithm(from the prior knowledge it is known that there are more positive tweets,so the cluster with more tweets is a positive cluster)</li>
<li>grouped tweets based on the data user</li>
<li>predicted to which cluster each tweet of the user belong to and label the tweet with the cluster name</li>
<li>counted the cluster-labels ,and assign the user positive or negative</li>
<li>Results:out of 300 followers i got 19 of them are negative rest of them are positive</li>
</ol>

<p><strong>my code for approach 2</strong></p>

<pre><code>df=data[((data['sentiment']=='negative') | (data['sentiment']=='positive'))]
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['tweets'])

true_k = 2
model = KMeans(n_clusters=true_k, init='k-means++'
               , max_iter=10000, n_init=1)
model.fit(X)

print(""Top terms per cluster:"")
order_centroids = model.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(true_k):
    print(""Cluster %d:"" % i),
    for ind in order_centroids[i, :20]:
        print(' %s' % terms[ind]),
    print

labels=model.labels_
print( np.bincount(labels))
d=pd.DataFrame()
d['labels']=labels
d['senti']=df['sentiment']
d['labels'].value_counts()
#to know which label having more data(more data is positive less is negative)
a=[i for i in range(0,len(labels)) if((d['senti'][i]=='positive') 
                                      and d['labels'][i]==1)]
b=[i for i in range(0,len(labels)) if((d['senti'][i]=='positive') 
                                      and d['labels'][i]==0)]
c=[i for i in range(0,len(labels)) if((d['senti'][i]=='negative') 
                                      and d['labels'][i]==1)]
d=[i for i in range(0,len(labels)) if((d['senti'][i]=='negative') 
                                      and d['labels'][i]==0)]
print(len(a),len(b),len(c),len(d))
users=set(df['users'])
prediction=[]
for user in users:
    temp=df[(df['users']==user)]
    temp=temp['tweets']
    Y = vectorizer.transform(temp)
    tweet_predictions=model.predict(Y)
    no_one=np.count_nonzero(tweet_predictions==1)
    no_zero=np.count_nonzero(tweet_predictions==0)
    if(no_one&gt;no_zero):
        prediction.append('positive')
    else:
        prediction.append('negative')
</code></pre>

<p>is this the right approach for classifying the users based on the tweets?if no... is there any better approach?</p>
","python, machine-learning, classification, tweepy, text-classification","<p>The right approach is the third one:</p>

<p>Some judges evaluate a subset of your users and based on their tweets assign a sentiment-score to them. Then using some machine learning technique (I suggest SVM) you can train a model based on these labeled examples, providing as input their tweets' content, sentiment-score, absolute number of positives and negatives, percentage of negatives or other (aggregated) relevant features. Lastly you should apply the model to the unseen users to understand their polarity.</p>

<p>I don't want to introduce train-validation-test too, but this should be the way. Your approaches don't use machine learning to discriminate the users, since you are using it as a black box only to gather the tweets' polarity.</p>
",0,0,595,2017-10-14 06:43:30,https://stackoverflow.com/questions/46741701/sentiment-analysis-and-classification-of-the-user-based-on-this-tweetsbest-appr
Information Gain calculation with Scikit-learn,"<p>I am using Scikit-learn for text classification. I want to calculate the Information Gain for each attribute with respect to a class in a (sparse) document-term matrix.</p>
<ul>
<li>the <strong>Information Gain</strong> is defined as <code>H(Class) - H(Class | Attribute)</code>, where <code>H</code> is the entropy.</li>
<li>in weka, this would be calculated with <a href=""http://weka.sourceforge.net/doc.stable-3-8/weka/attributeSelection/InfoGainAttributeEval.html"" rel=""nofollow noreferrer""><code>InfoGainAttribute</code></a>.</li>
<li>But I haven't found this measure in scikit-learn.</li>
</ul>
<p>(It was <a href=""https://stats.stackexchange.com/questions/13389/information-gain-mutual-information-and-related-measures"">suggested</a> that the formula above for Information Gain is the same measure as mutual information. This matches also the definition in <a href=""https://en.wikipedia.org/wiki/Mutual_information"" rel=""nofollow noreferrer"">wikipedia</a>. Is it possible to use a specific setting for mutual information in scikit-learn to accomplish this task?)</p>
","python, machine-learning, scikit-learn, text-classification, feature-selection","<p>You can use scikit-learn's <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif"" rel=""noreferrer""><code>mutual_info_classif</code></a> 
here is an example</p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_extraction.text import CountVectorizer

categories = ['talk.religion.misc',
              'comp.graphics', 'sci.space']
newsgroups_train = fetch_20newsgroups(subset='train',
                                      categories=categories)

X, Y = newsgroups_train.data, newsgroups_train.target
cv = CountVectorizer(max_df=0.95, min_df=2,
                                     max_features=10000,
                                     stop_words='english')
X_vec = cv.fit_transform(X)

res = dict(zip(cv.get_feature_names(),
               mutual_info_classif(X_vec, Y, discrete_features=True)
               ))
print(res)
</code></pre>

<p>this will output a dictionary of each attribute, i.e. item in the vocabulary as keys and their information gain as values</p>

<p>here is a sample of the output</p>

<pre><code>{'bible': 0.072327479595571439,
 'christ': 0.057293733680219089,
 'christian': 0.12862867565281702,
 'christians': 0.068511328611810071,
 'file': 0.048056478042481157,
 'god': 0.12252523919766867,
 'gov': 0.053547274485785577,
 'graphics': 0.13044709565039875,
 'jesus': 0.09245436105573257,
 'launch': 0.059882179387444862,
 'moon': 0.064977781072557236,
 'morality': 0.050235104394123153,
 'nasa': 0.11146392824624819,
 'orbit': 0.087254803670582998,
 'people': 0.068118370234354936,
 'prb': 0.049176995204404481,
 'religion': 0.067695617096125316,
 'shuttle': 0.053440976618359261,
 'space': 0.20115901737978983,
 'thanks': 0.060202010019767334}
</code></pre>
",32,34,66470,2017-10-15 07:17:46,https://stackoverflow.com/questions/46752650/information-gain-calculation-with-scikit-learn
Data Preparation for training,"<p>I am trying to prepare the data file by creating one hot encoding of the text of characters using which I can later train my model for classification. I have a training data file which consists of lines of characters and I am doing initially the integer encoding of them and then the one hot encoding. </p>

<p>e.g. this is how the data file looks:</p>

<ol>
<li>afafalkjfalkfalfjalfjalfjafajfaflajflajflajfajflajflajfjaljfafj    </li>
<li>fgtfafadargggagagagagagavcacacacarewrtgwgjfjqiufqfjfqnmfhbqvcqvfqfqafaf   </li>
<li>fqiuhqqhfqfqfihhhhqeqrqtqpocckfmafaflkkljlfabadakdpodqpqrqjdmcoqeijfqfjqfjoqfjoqgtggsgsgqr    </li>
</ol>

<p>This is how I am approaching it:</p>

<pre><code>import pandas as pd
from sklearn import preprocessing

categorical_data = pd.read_csv('abc.txt', sep=""\n"", header=None)
labelEncoder = preprocessing.LabelEncoder()
X = categorical_data.apply(labelEncoder.fit_transform)
print(""Afer label encoder"")
print(X.head())

oneHotEncoder = preprocessing.OneHotEncoder()
oneHotEncoder.fit(X)

onehotlabels = oneHotEncoder.transform(X).toarray()
print(""Shape after one hot encoding:"", onehotlabels.shape)

print(onehotlabels)
</code></pre>

<p>I am getting the integer encoding for each line (0,1,2 in my case) and then the subsequent one hot encoded vector. </p>

<p>My question is that how do I do it for each character in an individual line as for prediction, the model should learn from the characters in one line( which corresponds to a certain label). Can someone give me some insight on how to proceed from there?</p>
","machine-learning, scikit-learn, text-classification, data-processing, one-hot-encoding","<p>Given your example I end up with a DataFrame like so:</p>

<pre><code>    0
0   0
1   1
2   2
</code></pre>

<p>From your description it sounds like you want each line to have its own independent one hot encoding. So lets first look at line 1.</p>

<pre><code>afafalkjfalkfalfjalfjalfjafajfaflajflajflajfajflajflajfjaljfafj
</code></pre>

<p>The reason you are getting the dataframe I included above is that this line is getting read into the DataFrame and then passed to the <code>labelEncoder</code> and <code>oneHotEncoder</code> as a single value, not an array of 63 values (the length of the string).</p>

<p>What you really want to do is pass the <code>labelEncoder</code> an array of size 63.</p>

<pre><code>data = np.array([let for let in categorical_data[0][0]])
X = labelEncoder.fit_transform(data)
oneHotEncoder.fit(X.reshape(-1,1))
row_1_labels = oneHotEncoder.transform(X.reshape(-1,1)).toarray()
row_1_labels

array([[ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  0.,  1.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  0.,  1.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.]])
</code></pre>

<p>You could repeat this for each row to get the independent one hot encodings. Like so:</p>

<pre><code>one_hot_encodings = categorical_data.apply(lambda x: [oneHotEncoder.fit_transform(labelEncoder.fit_transform(np.array([let for let in x[0]])).reshape(-1,1)).toarray()], axis=1)
one_hot_encodings

                                                    0
0   [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0....
1   [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...
2   [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...
</code></pre>

<p>If you wanted the rows to be one hot encoded based on the values found in all rows you would just first fit the <code>labelEncoder</code> to all of the unique letters and then do the transformations for each row. Like so:</p>

<pre><code>unique_letters = np.unique(np.array([let for row in categorical_data.values for let in row[0]]))
labelEncoder.fit(unique_letters)
unique_nums = labelEncoder.transform(unique_letters)
oneHotEncoder.fit(unique_nums.reshape(-1,1))
cat_dat = categorical_data.apply(lambda x: [np.array([let for let in x[0]])], axis=1)
one_hot_encoded = cat_dat.apply(lambda x: [oneHotEncoder.transform(labelEncoder.transform(x[0]).reshape(-1,1)).toarray()], axis=1)
one_hot_encoded

                                                    0
0   [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...
1   [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...
2   [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...
</code></pre>

<p>This will return you a DataFrame with each row containing the one hot encoded array of letters based on the letters from all rows.</p>
",1,1,164,2017-10-17 15:32:56,https://stackoverflow.com/questions/46793895/data-preparation-for-training
TensorFlow - Understanding filter and stride shapes for CNN text classification,"<p>I am reviewing Denny Britz's <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">tutorial</a> on <strong>text</strong> classification using <code>CNNs</code> in <code>TensorFlow</code>. Filter and stride shapes make perfect sense in the image domain. However, when it comes to text, I am confused on how to correctly define the stride and filter shapes. Consider the following two layers from Denny's code:</p>

<pre><code># Create a convolution + maxpool layer for each filter size
pooled_outputs = []
for i, filter_size in enumerate(filter_sizes):
    with tf.name_scope(""conv-maxpool-%s"" % filter_size):
        # Convolution Layer
        filter_shape = [filter_size, embedding_size, 1, num_filters]
        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")
        conv = tf.nn.conv2d(
            self.embedded_chars_expanded,
                W,
                strides=[1, 1, 1, 1],
                padding=""VALID"",
                name=""conv"")
        # Apply nonlinearity
        h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")
        # Maxpooling over the outputs
        pooled = tf.nn.max_pool(
            h,
            ksize=[1, sequence_length - filter_size + 1, 1, 1],
            strides=[1, 1, 1, 1],
            padding='VALID',
            name=""pool"")
        pooled_outputs.append(pooled)
</code></pre>

<p>The shape of <code>self.embedded_chars_expanded</code> is <code>[batch_size, max_sentence_length, embedding_size, 1]</code> which means each batch member is a single channel matrix of <code>max_sentence_length x embedding_size</code></p>

<p><strong>Filters</strong></p>

<p>Suppose my filter_shape is <code>[3, 50, 1, 16]</code>. I interpret this as the filter will slide over <code>3</code> word vectors at a time which has dimensionality <code>50</code>. This is text, so the <code>1</code> corresponds to a single channel of input (as opposed to <code>3</code> with <code>RGB</code>). Lastly, the <code>16</code> implies I will have <code>16</code> filters in the <code>conv layer</code>.</p>

<p>Have I interpreted this correctly?</p>

<p><strong>Strides</strong></p>

<p>Similarly, stride shapes, in both the conv and pooling layers are defined as <code>[1, 1, 1, 1]</code>. </p>

<p>Does this shape's dimensions correspond to the dimensions of the <code>filter_shape</code>? </p>

<p>If so, this is why I am confused. It would seem that the nature of word vector representations means that the stride length should be <code>[1, embedding_size, 1, 1]</code> meaning I want to move the window one full word at-a-time over one channel for each filter.</p>
","python, tensorflow, conv-neural-network, text-classification","<blockquote>
<p>Filters</p>
<p>Have I interpreted this correctly?</p>
</blockquote>
<p>Yes, exactly.</p>
<blockquote>
<p>Strides</p>
<p>Does this shape's dimensions correspond to the dimensions of the
filter_shape?</p>
</blockquote>
<p>Yes, it corresponds to the strides in which you convolve the filter on the input embedding.</p>
<blockquote>
<p>It would seem that the nature of word vector representations
means that the stride length should be [1, embedding_size, 1, 1]
meaning I want to move the window one full word at-a-time over one
channel for each filter.</p>
</blockquote>
<p>Pay attention to the padding strategy - the padding in <code>conv2d</code> is set to be <code>VALID</code>. This means there will be no padding. Since the filter size in the embedding dimension covers the input entirely, it can fit only once without any consideration of the stride along this dimension.</p>
<p>Put differently - you can convolve along the embedding dimension only once independently of the stride.</p>
",1,0,1328,2017-10-25 21:44:23,https://stackoverflow.com/questions/46942526/tensorflow-understanding-filter-and-stride-shapes-for-cnn-text-classification
NLTK title classifier,"<p>Apologies in advance if this has already been questioned/answered, but I couldn't find any answer close to my problem. I am also somewhat noob as to dealing with Python, so sorry too for the long post.</p>
<p>I am trying to build a Python script that, based on a user-given Pubmed query (i.e., &quot;cancer&quot;), retrieves a file with N article titles, and evaluates their relevance to the subject in question.</p>
<p>I have successfully built the &quot;pubmed search and save&quot; part, having it return a .txt file containing titles of articles (each line corresponds to a different article title), for instance:</p>
<blockquote>
<p>Feasibility of an ovarian cancer quality-of-life psychoeducational intervention.</p>
<p>A randomized trial to increase physical activity in breast cancer survivors.</p>
</blockquote>
<p>Having this file, the idea is to use it into a classifier and get it to answer if the titles in the .txt file are relevant to a subject, for which I have a &quot;gold standard&quot; of titles that I know are relevant (i.e., I want to know the precision and recall of the queried set of titles against my gold standard). For example: Title 1 has the word &quot;neoplasm&quot; X times and &quot;study&quot; N times, therefore it is considered as relevant to &quot;cancer&quot; (Y/N).</p>
<p>For this, I have been using NLTK to (try to) classify my text. I have pursued 2 different approaches, both unsuccessfully:</p>
<p><strong>Approach 1</strong></p>
<p>Loading the .txt file, preprocessing it (tokenization, lower-casing, removing stopwords), converting the text to NLTK text format, finding the N most-common words. All this runs without problems.</p>
<pre><code>f = open('SR_titles.txt')
raw = f.read() 
tokens = word_tokenize(raw)
words = [w.lower() for w in tokens]
words = [w for w in words if not w in stopwords.words(&quot;english&quot;)]
text = nltk.Text(words)
fdist = FreqDist(text)
&gt;&gt;&gt;&lt;FreqDist with 116 samples and 304 outcomes&gt;
</code></pre>
<p>I am also able to find colocations/bigrams in the text, which is something that might be important afterward.</p>
<pre><code>text.collocations()
&gt;&gt;&gt;randomized controlled; breast cancer; controlled trial; physical
&gt;&gt;&gt;activity; metastatic breast; prostate cancer; randomised study; early
&gt;&gt;&gt;breast; cancer patients; feasibility study; psychosocial support;
&gt;&gt;&gt;group psychosocial; group intervention; randomized trial
</code></pre>
<p>Following <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">NLTKs tutorial</a>, I built a feature extractor, so the classifier will know which aspects of the data it should pay attention to.</p>
<pre><code>def document_features(document):
  document_words = set(document)
  features = {}
  for word in word_features:
      features['contains({})'.format(word)] = (word in document_words)
  return features
</code></pre>
<p>This would, for instance, return something like this:</p>
<pre><code>{'contains(series)': False, 'contains(disorders)': False,
'contains(group)': True, 'contains(neurodegeneration)': False,
'contains(human)': False, 'contains(breast)': True}
</code></pre>
<p>The next thing would be to use the feature extractor to train a classifier to label new article titles, and following NLTKs example, I tried this:</p>
<pre><code>featuresets = [(document_features(d), c) for (d,c) in text]
</code></pre>
<p>Which gives me the error:</p>
<pre><code>ValueError: too many values to unpack
</code></pre>
<p>Quickly googled this and found that this has something to do with tuples, but did not get how can I solve it (like I said, I'm somewhat noob in this), unless by creating a categorized corpus (I would still like to understand how can I solve this tuple problem).</p>
<p>Therefore, I tried <strong>approach 2</strong>, following Jacob Perkings Text Processing with NLTK Cookbook:</p>
<p>Started by creating a corpus and attributing categories. This time I had 2 different .txt files, one for each subject of title articles.</p>
<pre><code>reader = CategorizedPlaintextCorpusReader('.', r'.*\,
    cat_map={'hd_titles.txt': ['HD'], 'SR_titles.txt': ['Cancer']})
</code></pre>
<p>With &quot;reader.raw()&quot; I get something like this:</p>
<blockquote>
<p>u&quot;A pilot investigation of a multidisciplinary quality of life intervention for men with biochemical recurrence of prostate cancer.\nA randomized controlled pilot feasibility study of the physical and psychological effects of an integrated support programme in breast cancer.\n&quot;</p>
</blockquote>
<p>The categories for the corpus seem to be right:</p>
<pre><code>reader.categories()
&gt;&gt;&gt;['Cancer', 'HD']
</code></pre>
<p>Then, I try to construct a list of documents, labeled with the appropriate categories:</p>
<pre><code>documents = [(list(reader.words(fileid)), category)
          for category in reader.categories()
          for fileid in reader.fileids(category)]
</code></pre>
<p>Which returns me something like this:</p>
<pre><code>[([u'A', u'pilot', u'investigation', u'of', u'a', u'multidisciplinary',
u'quality', u'of', u'life', u'intervention', u'for', u'men', u'with', 
u'biochemical', u'recurrence', u'of', u'prostate', u'cancer', u'.'], 
'Cancer'), 
 ([u'Trends', u'in', u'the', u'incidence', u'of', u'dementia', u':', 
u'design', u'and', u'methods', u'in', u'the', u'Alzheimer', u'Cohorts', 
u'Consortium', u'.'], 'HD')]
</code></pre>
<p>Next step would be creating a list of labeled feature sets, for which I used the next function, that takes a corpus and a feature_detector function  (that would be document_features referred above). It then constructs and returns a mapping of the form {label: [featureset]}.</p>
<pre><code>def label_feats_from_corpus(corp, feature_detector=document_features):
    label_feats = collections.defaultdict(list)
    for label in corp.categories():
        for fileid in corp.fileids(categories=[label]):
            feats = feature_detector(corp.words(fileids=[fileid]))
            label_feats[label].append(feats)
    return label_feats 

lfeats = label_feats_from_corpus(reader)
&gt;&gt;&gt;defaultdict(&lt;type 'list'&gt;, {'HD': [{'contains(series)': True, 
'contains(disorders)': True, 'contains(neurodegeneration)': True, 
'contains(anilinoquinazoline)': True}], 'Cancer': [{'contains(cancer)': 
True, 'contains(of)': True, 'contains(group)': True, 'contains(After)': 
True, 'contains(breast)': True}]})
</code></pre>
<p>(the list is a lot bigger and everything is set as True).</p>
<p>Then I want to construct a list of labeled training instances and testing instances.</p>
<p>The split_label_feats() function takes a mapping returned from
label_feats_from_corpus() and splits each list of feature sets
into labeled training and testing instances.</p>
<pre><code>def split_label_feats(lfeats, split=0.75):
    train_feats = []
    test_feats = []
    for label, feats in lfeats.items():
        cutoff = int(len(feats) * split)
        train_feats.extend([(feat, label) for feat in feats[:cutoff]])
        test_feats.extend([(feat, label) for feat in feats[cutoff:]])
    return train_feats, test_feats

train_feats, test_feats = split_label_feats(lfeats, split=0.75)
len(train_feats)
&gt;&gt;&gt;0
len(test_feats)
&gt;&gt;&gt;2
print(test_feats)
&gt;&gt;&gt;[({'contains(series)': True, 'contains(China)': True, 
'contains(disorders)': True, 'contains(neurodegeneration)': True}, 
'HD'), ({'contains(cancer)': True, 'contains(of)': True, 
'contains(group)': True, 'contains(After)': True, 'contains(breast)': 
True}, 'Cancer')]
</code></pre>
<p>I should've ended up with a lot more labeled training instances and labeled testing instances, I guess.</p>
<p><strong>This brings me to where I am now</strong>. I searched stackoverflow, biostars, etc and could not find how to deal with both problems, so any help would be deeply appreciated.</p>
<p><strong>TL;DR</strong>: Can't label a single .txt file to classify text, and can't get a corpus correctly labeled (again, to classify text).</p>
<p>If you've read this far, thank you as well.</p>
","python, nlp, nltk, text-mining, text-classification","<p>You're getting an error on the following line:</p>

<pre><code>featuresets = [(document_features(d), c) for (d,c) in text]
</code></pre>

<p>Here, you are supposed to convent each document (i.e. each title) to a dictionary of features. But to train with the results, the <code>train()</code> method needs both the feature dictionaries and the correct answer (""label""). So the normal workflow is to have a list of <code>(document, label)</code> pairs, which you transform to <code>(features, label)</code> pairs. It looks like your variable <code>documents</code> has the right structure, so if you just use it instead of <code>text</code>, this should work correctly:</p>

<pre><code>featuresets = [(document_features(d), c) for (d,c) in documents]
</code></pre>

<p>As you go forward, get in the habit of inspecting your data carefully and figuring out what will (and should) happen to them. If <code>text</code> is a list of titles, it makes no sense to unpack each title to a pair <code>(d, c)</code>. That should have pointed you in the right direction. </p>
",0,2,1847,2017-10-27 18:47:04,https://stackoverflow.com/questions/46981605/nltk-title-classifier
Globally register custom TextClassifier with TextClassificationManager (Android O),"<p>Starting with Android 8.0 (Oreo) / API level 26, Android shows contextual actions for highlighted text (e.g. ""Call"" action if a phone number is selected). Also, Android automatically selects multiple words that belong together (e.g. a whole street address). This is called ""Smart Text Selection"".</p>

<p>This is how you register a new <a href=""https://developer.android.com/reference/android/view/textclassifier/TextClassifier.html"" rel=""nofollow noreferrer"">TextClassifier</a>:</p>

<pre><code>TextClassificationManager manager = 
    (TextClassificationManager) getSystemService(Context.TEXT_CLASSIFICATION_SERVICE);
TextClassifier c = manager.getTextClassifier();
manager.setTextClassifier(new MyTextClassifier());
</code></pre>

<p><code>MyTextClassifier</code> implements the <code>TextClassifier</code> interface and implements <code>suggestSelection()</code>  and <code>classifyText()</code>.</p>

<p><code>classifyText()</code> returns a <a href=""https://developer.android.com/reference/android/view/textclassifier/TextClassification.html"" rel=""nofollow noreferrer"">TextClassification</a> object which has a title and an Intent which describes the contextual action to perform.</p>

<p><strong>Here is my question</strong> (finally): If I register my TextClassifier with the <code>TextClassificationManager</code> it works for my own app only.
Is there any way for defining a TextClassifier which can be used by other apps?</p>

<p>I would like to create an app which globally registers a new custom <code>TextClassifier</code>.</p>

<p>That way I could write an app that adds system-wide support for, let's say, song names (assuming I have a list of all song names I want to support). <code>EditText</code>s would automatically select ""Smells Like Teen Spirit"" even though the user just long-clicked ""Teen"". And a contextual action ""Search on Google Play Music"" could pop up next to ""Cut"", ""Copy"" etc.</p>
","android, text-classification, android-8.0-oreo","<p>Unfortunately, in Android O, you won't be able to make other apps use your custom TextClassifier.</p>

<p>But if you'd like to encourage other apps to take advantage of your custom TextClassifier, you can create a library with your custom TextClassifier that can be built into other apps and set as the TextClassifier.</p>
",1,3,851,2017-11-07 19:17:36,https://stackoverflow.com/questions/47165835/globally-register-custom-textclassifier-with-textclassificationmanager-android
Classify a sentence into multiple categories,"<p>A beginner in NLTK and Scikit-learn here. I want to be able to classify a given sentence(or even a paragraph) into a group of categories. By categories, I don't mean only two categories like spam and not spam or good sentiment and bad sentiment, meaning its multiple(more than two) categories to choose from. Please help me in choosing the simplest algorithm for solving this problem. Thanks in advance.</p>
","python-3.x, machine-learning, scikit-learn, nltk, text-classification","<p>By the tag you used in the post I see that you know about <code>machine learning</code>... This is a good way for doing this project.</p>

<p>What you are going to need is a decent amount of sample data, that is, a table of text (sample sentences, paragraphs, whatever...) and then a column that states the category it is in.</p>

<p>What you do is <code>train</code> the program, to look for patterns in the sample text, and if you have enough sample data, you can then <code>analyze</code> text, and have the program output what category it is.</p>

<p>You could use <a href=""https://www.tensorflow.org/install/"" rel=""nofollow noreferrer"">TensorFlow</a> as your machine learning framework.</p>

<p>I suggest you start with a few simpler projects to get an idea for how machine learning works and what works best.</p>
",-1,-2,1872,2017-11-13 16:06:25,https://stackoverflow.com/questions/47268610/classify-a-sentence-into-multiple-categories
How to interpret scored probabilities in machine learning classification algorithm?,"<p>I am using two Neural networks for two class text classification. I'm getting 90% accuracy on test data. Also using different performance metrics like precision, recall, f-score and confusion matrix to make sure that model is performing as expected.</p>

<p>In the predictive experiment using trained model, I'm fetching probabilities for each prediction.The output looks as follows (Couldn't provide codes it's implemented in Azure ML Studio )</p>

<p>ex:
class (probability) , class 2 (probability) -> predicted class</p>

<p>class 1 (0.99) , class 2 (0.01)     ->  class 1</p>

<p>class 1 (0.53) , class 2 (0.47)     ->  class 1</p>

<p>class 1 (0.2)  , class 2(0.8)       ->  class 2</p>

<p><a href=""https://i.sstatic.net/nzsGX.png"" rel=""nofollow noreferrer"">Example</a></p>

<p>As per my understanding so far, by looking at the probability we can tell, how confident is the model about its prediction.And 90% accuracy means out 100 records 10 predictions could go wrong.</p>

<p>Now my question is, by looking at probability (confidence) can we tell which bucket the current records falls into 90%(correct prediction) or 10% (wrong prediction)?</p>

<p>What I'm trying to achieve is, to give end your some metric to tell him/her that this prediction is probably wrong, they might want to change it to some other class before using these results.</p>
","machine-learning, neural-network, classification, probability, text-classification","<blockquote>
  <p>90% accuracy means out 100 records 10 predictions could go wrong.</p>
</blockquote>

<p>It is not exactly like that; accuracy is always (although implicitly) linked to the <em>specific</em> test set we have used to measure it: so, 90% means that out of 100 records our classifier indeed misclassified 10 (i.e. there is not ""could"").</p>

<p>What we <em>hope</em> for in machine learning is that the performance of our models in new, unseen data, will be comparable to that of our test set (which, regarding the training of our model, is also unseen). Roughly speaking, provided that our new data come from the same statistical distribution with our training &amp; test sets, it is not an unreasonable expectation.</p>

<blockquote>
  <p>What I'm trying to achieve is, to give end your some metric to tell him/her that this prediction is probably wrong, they might want to change it to some other class before using these results.</p>
</blockquote>

<p>Intuitively, you should already know the answer to this: interpreting the returned probabilities as confidence (which, at least in principle, is not an invalid interpretation), their values tell you something about how ""certain"" your model is about its answers: so, what you could do is provide the end users with these probability values; in your example, the case of ""Question"" with probability 0.97 is indeed qualitatively not the same with the case ""Question"" with probability ~ 0.50...</p>
",2,0,1875,2017-11-20 08:40:40,https://stackoverflow.com/questions/47387959/how-to-interpret-scored-probabilities-in-machine-learning-classification-algorit
Multiclass text classification with python and nltk,"<p>I am given a task of classifying a given news text data into one of the following 5 categories - Business, Sports, Entertainment, Tech and Politics
<br/><br/>
About the data I am using:<br/>
Consists of text data labeled as one of the 5 types of news statement (Bcc news data)<br/><br/> 
I am currently using NLP with nltk module to calculate the frequency distribution of every word in the training data with respect to each category(except the stopwords).<br/><br/>
Then I classify the new data by calculating the sum of weights of all the words with respect to each of those 5 categories. The class with the most weight is returned as the output.<br/><br/>
Heres the actual <a href=""https://github.com/sujaybr/NewsClassifier/blob/master/bccNewsReal.py"" rel=""nofollow noreferrer"">code</a>.<br/><br/>
This algorithm does predict new data accurately but I am interested to know about some other simple algorithms that I can implement to achieve better results. I have used Naive Bayes algorithm to classify data into two classes (spam or not spam etc) and would like to know how to implement it for multiclass classification if it is a feasible solution.<br/><br/>
Thank you.</p>
","nlp, nltk, text-classification, naivebayes, multiclass-classification","<p>In classification, and especially in text classification, choosing the right machine learning algorithm often comes after selecting the right features. Features are domain dependent, require knowledge about the data, but good quality leads to better systems quicker than tuning or selecting algorithms and parameters.</p>

<p>In your case you can either go to word embeddings as already said, but you can also design your own custom features that you think will help in discriminating classes (whatever the number of classes is). For instance, how do you think a spam e-mail is often presented ? A lot of mistakes, syntaxic inversion, bad traduction, punctuation, slang words... A lot of possibilities ! Try to think about your case with sport, business, news etc.</p>

<p>You should try some new ways of creating/combining features and then choose the best algorithm. Also, have a look at other weighting methods than term frequencies, like <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tf-idf</a>.</p>
",0,0,2222,2017-11-29 18:49:47,https://stackoverflow.com/questions/47559727/multiclass-text-classification-with-python-and-nltk
"For text classification with scikit-learn, do I have to use both, Countvectorizer and TFIDF?","<p>Looking through scikit-learn documentation code, it suggests to implement the Countvectorizer first and then on top TFIDF. Can I use only TFIDF?
<a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></p>

<p><a href=""https://i.sstatic.net/B9VEH.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>If I only use TFIDF and I give my preprocessed texts as input it won't take the data type (I tried as a list and a np array). Can someone help?</p>
","text, machine-learning, scikit-learn, text-classification","<ol>
<li>In the example they show, they use on top of <code>CountVectorizer</code> a <code>TfidfTransformer</code>. Using directly <code>TfidfVectorizer</code> produces the same result. Thus, it is up to you to chose which weighting scheme you want.</li>
<li>I don't understand really well your question. Scikit vectorizers can have different types of input, ranging from list/arrays of strings to file descriptor and others. To construct the ngrams, it uses the argument <code>tokenizer=</code> and <code>preprocessor=</code>. What is your issue here ?</li>
</ol>
",-1,0,194,2017-12-02 13:12:11,https://stackoverflow.com/questions/47608261/for-text-classification-with-scikit-learn-do-i-have-to-use-both-countvectorize
text classification of large dataset in python,"<p>I have 2.2 million data samples to classify into more than <strong>7500 categories</strong>. I am using pandas and sckit-learn of python to do so.</p>

<p>Below is the sample of my dataset</p>

<pre><code>itemid       description                                            category
11802974     SPRO VUH3C1 DIFFUSER VUH1 TRIPLE Space heaters    Architectural Diffusers
10688548     ANTIQUE BRONZE FINISH PUSHBUTTON  switch           Door Bell Pushbuttons
9836436     Descente pour Cable tray fitting and accessories    Tray Cable Drop Outs
</code></pre>

<p>Below are the steps I have followed:</p>

<ol>
<li>Pre-processing</li>
<li>Vector representation</li>
<li><p>Training </p>

<pre><code> dataset=pd.read_csv(""trainset.csv"",encoding = ""ISO-8859-1"",low_memory=False)
 dataset['description']=dataset['description'].str.replace('[^a-zA-Z]', ' ')
 dataset['description']=dataset['description'].str.replace('[\d]', ' ')
 dataset['description']=dataset['description'].str.lower()

 stop = stopwords.words('english')
 lemmatizer = WordNetLemmatizer()

  dataset['description']=dataset['description'].str.replace(r'\b(' + r'|'.join(stop) + r')\b\s*', ' ')
  dataset['description']=dataset['description'].str.replace('\s\s+',' ')
  dataset['description'] =dataset['description'].apply(word_tokenize)
  ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'
  POS_LIST = [NOUN, VERB, ADJ, ADV]
  for tag in POS_LIST:
  dataset['description'] = dataset['description'].apply(lambda x: 
  list(set([lemmatizer.lemmatize(item,tag) for item in x])))
  dataset['description']=dataset['description'].apply(lambda x : "" "".join(x))


 countvec = CountVectorizer(min_df=0.0005)
 documenttermmatrix=countvec.fit_transform(dataset['description'])
 column=countvec.get_feature_names()

 y_train=dataset['category']
 y_train=dataset['category'].tolist()

 del dataset
 del stop
 del tag
</code></pre></li>
</ol>

<p>The documenttermmatrix generated will be of type scipy csr matrix with <strong>12k</strong> features and 2.2 million samples.</p>

<p>For training I tried using xgboost of sckit learn</p>

<pre><code>model = XGBClassifier(silent=False,n_estimators=500,objective='multi:softmax',subsample=0.8)
model.fit(documenttermmatrix,y_train,verbose=True)
</code></pre>

<p>After 2-3 minutes execution of above code i got error</p>

<pre><code>OSError: [WinError 541541187] Windows Error 0x20474343
</code></pre>

<p>I also tried Naive Bayes of sckit learn for which i got memory error</p>

<p><strong>Question</strong></p>

<p>I have used Scipy matrix which consumes very less memory and also I am deleting all the unused objects before executing xgboost or Naive bayes, I am using system with <strong>128GB RAM</strong> but still getting memory issue while training.</p>

<p>I am new to python.Is there any thing wrong in my code?
can anyone tell how can I use  Memory efficiently and proceed further?</p>
","python, pandas, scikit-learn, large-data, text-classification","<p>I think I can explain the problem in your code.
The OS error appears to be:</p>

<p>""</p>

<pre><code>ERROR_DS_RIDMGR_DISABLED
8263 (0x2047)
</code></pre>

<p>The directory service detected the subsystem that allocates relative identifiers is disabled. This can occur as a protective mechanism when the system determines a significant portion of relative identifiers (RIDs) have been exhausted. </p>

<p>"" via <a href=""https://msdn.microsoft.com/en-us/library/windows/desktop/ms681390"" rel=""nofollow noreferrer"">https://msdn.microsoft.com/en-us/library/windows/desktop/ms681390</a></p>

<p>I think you exhausted a significant portion of the RIDs at this step in your code:</p>

<pre><code>dataset['description'] = dataset['description'].apply(lambda x: 
list(set([lemmatizer.lemmatize(item,tag) for item in x])))
</code></pre>

<p>You're passing a lemmatizer in your lambda, but lambdas are anonymous, so it looks like you might be making 2.2 million copies of that lemmatizer at runtime.</p>

<p>You should try changing the low_memory flag to true whenever you have a memory issue.</p>

<p>Response to comment-</p>

<p>I checked the Pandas documentation, and you can define a function outside of dataset['description'].apply(), and then reference that function in the call to dataset['description'].apply().  Here is how I would write said function.</p>

<pre><code>def lemmatize_descriptions(x):
return list(set([lemmatizer.lemmatize(item,tag) for item in x]))
</code></pre>

<p>Then, the call to apply() would be-</p>

<pre><code>dataset['description'] = dataset['description'].apply(lemmatize_descriptions)
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.18/generated/pandas.Series.apply.html"" rel=""nofollow noreferrer"">Here is the documentation.</a></p>
",6,4,788,2017-12-03 15:19:36,https://stackoverflow.com/questions/47619836/text-classification-of-large-dataset-in-python
FastText using pre-trained word vector for text classification,"<p>I am working on a text classification problem, that is, given some text, I need to assign to it certain given labels.</p>

<p>I have tried using fast-text library by Facebook, which has two utilities of interest to me:</p>

<p>A) Word Vectors with pre-trained models</p>

<p>B) Text Classification utilities</p>

<p>However, it seems that these are completely independent tools as I have been unable to find any tutorials that merge these two utilities.</p>

<p>What I want is to be able to classify some text, by taking advantage of the pre-trained models of the Word-Vectors. Is there any way to do this?</p>
","nlp, word2vec, text-classification, fasttext","<p>FastText's native classification mode depends on you training the word-vectors yourself, using texts with known classes. The word-vectors thus become optimized to be useful for the specific classifications observed during training. So that mode typically <em>wouldn't</em> be used with pre-trained vectors. </p>

<p>If using pre-trained word-vectors, you'd then somehow compose those into a text-vector yourself (for example, by averaging all the words of a text together), then training a separate classifier (such as one of the many options from scikit-learn) using those features. </p>
",4,13,14086,2017-12-07 10:28:32,https://stackoverflow.com/questions/47692906/fasttext-using-pre-trained-word-vector-for-text-classification
Tensorboard Embedding Visualization,"<p>I am working on a text classification problem. I have 3M+ rows which need to be classified into 20 categories.</p>

<p>Following are two code snippets from my entire code:</p>

<p>This is the code where my tf variables are defined.:</p>

<pre><code> class TextCNNRNN(object):
    def __init__(self, embedding_mat, non_static, hidden_unit, sequence_length, max_pool_size,
                 num_classes, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):

        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name='input_x')
        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')
        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')
        self.batch_size = tf.placeholder(tf.int32, [])
        self.pad = tf.placeholder(tf.float32, [None, 1, embedding_size, 1], name='pad')
        self.real_len = tf.placeholder(tf.int32, [None], name='real_len')

        l2_loss = tf.constant(0.0)

        with tf.device('/cpu:0'), tf.name_scope('embedding'):
            self.emb_var = tf.Variable(embedding_mat, name='emb_var')
            # if not non_static:
            #   self.emb_var = tf.constant(embedding_mat, name='enb_var')
            # else:
            #   self.emb_var = tf.Variable(embedding_mat, name='emb_var')
            self.embedded_chars = tf.nn.embedding_lookup(self.emb_var, self.input_x)
            self.emb = tf.expand_dims(self.embedded_chars, -1)
</code></pre>

<p>The tensor I would like to visualize is embedded_chars.</p>

<p>And this is the code where I am given input to the projector api:</p>

<pre><code>config = projector.ProjectorConfig()
config.model_checkpoint_path = checkpoint_prefix + str(best_at_step) +'.ckpt'
embedding = config.embeddings.add()
embedding.tensor_name = cnn_rnn.embedded_chars.name

#embedding.metadata_path = 'metadata.tsv'
emb_writer = tf.summary.FileWriter(metadata_path,sess.graph)
projector.visualize_embeddings(emb_writer, config)
</code></pre>

<p><strong>My expectation:</strong>
I want to see my trained input data and how its being classified.</p>

<p><strong>Actual result:</strong>
When I use embedded_chars tensor as input to projector, noting loads. however, when I use emb_var I see the embeddings loading. The problem is emb_var is just my vocabulary but I need to see my actual dataset.</p>
","tensorflow, embedding, tensorboard, text-classification, projector","<p>Figured it out.</p>

<pre><code>final_embed_matrix = sess.run(cnn_rnn.emb_var)
            embedding_var = tf.Variable(final_embed_matrix, name='embedding_viz' + str(i))
            saver_embed = tf.train.Saver([embedding_var, output_var])
            sess.run(embedding_var.initializer)
            config = projector.ProjectorConfig()
            config.model_checkpoint_path = emb_dir + '/' + foldername + str(best_at_step)+'viz' +'.ckpt'
            emb_writer = tf.summary.FileWriter(emb_dir, sess.graph)

            embedding = config.embeddings.add()
            embedding.metadata_path = foldername + '_metadata.tsv'
            embedding.tensor_name = output_var.name

            embedding = config.embeddings.add()
            embedding.metadata_path = 'metadata.tsv'
            embedding.tensor_name = embedding_var.name

            projector.visualize_embeddings(emb_writer, config)
            saver_embed.save(sess, checkpoint_viz_prefix + str(best_at_step)+'viz' +'.ckpt')
</code></pre>
",0,0,871,2017-12-11 20:40:48,https://stackoverflow.com/questions/47761069/tensorboard-embedding-visualization
multi-label text classification with feedback,"<p>I'm currently working on a multi-label multi-class text classification problem.
In some part of the problem I need to give feedback to user that based on which features, words or phrases the algorithm decide to classify an instance to a certain category?</p>

<p>For example for a text like <code>A B C D E F G H I J K L M N O P Q R</code> I need output like this:
<a href=""https://i.sstatic.net/fYhp1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fYhp1.png"" alt=""enter image description here""></a></p>

<p>How can I consider this problem? what is the name or keywords of this problems? Is there any ready python package for this type of problem?</p>
","python, deep-learning, classification, text-classification, multilabel-classification","<p>What you are looking for is <strong>interpretability</strong> in machine learning (ml) models. There is much <a href=""https://www.kdnuggets.com/2015/04/model-interpretability-neural-networks-deep-learning.html"" rel=""nofollow noreferrer"">discussion</a> on this topic and is steadily gaining more attention, as more real-world applications in important subjects such as <a href=""https://www.engineering.com/DesignerEdge/DesignerEdgeArticles/ArticleID/14426/Big-Data-Predicts-Terrorist-Attacks-with-More-Than-90-Accuracy.aspx"" rel=""nofollow noreferrer"">terrorism detection</a>, <a href=""http://www.healthcareitnews.com/news/google-makes-ai-tool-precision-medicine-open-source"" rel=""nofollow noreferrer"">precision-medicine</a> etc. incorporate ml components. </p>

<p>There is at least one python module already doing what you seem to ask for, named <a href=""https://github.com/marcotcr/lime"" rel=""nofollow noreferrer"">lime</a>.</p>

<p>If you are interested in the task in general, there are many other resources you can also check out (e.g. an <a href=""http://people.csail.mit.edu/beenkim/papers/BeenK_FinaleDV_ICML2017_tutorial.pdf"" rel=""nofollow noreferrer"">extended presentation</a> and <a href=""https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning"" rel=""nofollow noreferrer"">article</a>, <a href=""http://interpretable.ml/"" rel=""nofollow noreferrer"">conferences</a> etc.).</p>
",4,1,632,2017-12-30 13:05:41,https://stackoverflow.com/questions/48034205/multi-label-text-classification-with-feedback
Trouble implementing Bernoulli Naive Bayes Classifier,"<p>I am trying to implement a <code>Bernoulli Naive Bayes</code> Classifier from <code>scikit-learn</code> library for text classification. But I am stuck with this error </p>

<blockquote>
  <p>ValueError: Expected 2D array, got 1D array instead:</p>
  
  <p>Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.</p>
</blockquote>

<p>Error in detail</p>

<pre><code>Traceback (most recent call last):
  File ""BNB.py"", line 27, in &lt;module&gt;
    clf.fit(train_data, train_labels)
  File ""/home/atinesh/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py"", line 579, in fit
    X, y = check_X_y(X, y, 'csr')
  File ""/home/atinesh/.local/lib/python3.6/site-packages/sklearn/utils/validation.py"", line 573, in check_X_y
    ensure_min_features, warn_on_dtype, estimator)
  File ""/home/atinesh/.local/lib/python3.6/site-packages/sklearn/utils/validation.py"", line 441, in check_array
    ""if it contains a single sample."".format(array))
ValueError: Expected 2D array, got 1D array instead:
array=['Apple' 'Banana' 'Cherry' 'Grape' 'Guava' 'Lemon' 'Mangos' 'Orange'
 'Strawberry' 'Watermelon' 'Potato' 'Spinach' 'Carrot' 'Onion' 'Cabbage'
 'Barccoli' 'Tomatoe' 'Pea' 'Cucumber' 'Eggplant'].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>

<p>""BNB.py""</p>

<pre><code>from sklearn.naive_bayes import BernoulliNB

dataPos = ['Apple', 'Banana', 'Cherry', 'Grape', 'Guava', 'Lemon', 'Mangos',
            'Orange', 'Strawberry', 'Watermelon']

dataNeg = ['Potato', 'Spinach', 'Carrot', 'Onion', 'Cabbage', 'Barccoli', 
            'Tomatoe', 'Pea', 'Cucumber', 'Eggplant']

def get_data():
    examples = []
    labels   = []

    for item in dataPos:
        examples.append(item)
        labels.append('positive')

    for item in dataNeg:
        examples.append(item)
        labels.append('negative')

    return examples, labels

train_data, train_labels = get_data()

# Train
clf = BernoulliNB()
clf.fit(train_data, train_labels)

# Predict
print(clf.predict('Apple Banana'))
print(clf.predict_proba('Apple Banana'))
</code></pre>
","python, scikit-learn, text-classification, naivebayes","<p>I would recommend use the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer"" rel=""nofollow noreferrer"">LabelBinarizer</a> in sklearn</p>

<pre><code>from sklearn.naive_bayes import BernoulliNB
import numpy as np
from sklearn import preprocessing

dataPos = ['Apple', 'Banana', 'Cherry', 'Grape', 'Guava', 'Lemon', 'Mangos',
                       'Orange', 'Strawberry', 'Watermelon']

dataNeg = ['Potato', 'Spinach', 'Carrot', 'Onion', 'Cabbage', 'Barccoli',
                       'Tomatoe', 'Pea', 'Cucumber', 'Eggplant']

Y=[0]*10+[1]*10
Y=np.array(Y)

lb = preprocessing.LabelBinarizer()
X = lb.fit_transform(dataPos+dataNeg)
clf = BernoulliNB()
clf.fit(X, Y)

test_sample = lb.transform([['Apple'],['Banana'],['Spinach']])
print clf.predict(test_sample)
</code></pre>

<p>Your code errors out because when doing <code>clf.fit(X,Y)</code>, X needs to be 2d array. Each row corresponding to a feature vector.</p>
",3,0,841,2018-01-01 19:24:21,https://stackoverflow.com/questions/48052570/trouble-implementing-bernoulli-naive-bayes-classifier
Tensorflow: bag of words text classification: Cannot feed value of shape,"<p>I've nicked this code here: <a href=""https://sourcedexter.com/tensorflow-text-classification-python/"" rel=""nofollow noreferrer"">https://sourcedexter.com/tensorflow-text-classification-python/</a> to try to predict if a given question is either one of two categories.</p>

<p>However, I'm getting the following error:</p>

<blockquote>
  <p>Cannot feed value of shape (1, 1666) for Tensor 'TargetsData/Y:0', which has shape '(?, 2)'</p>
</blockquote>

<p>Relevant code below:</p>

<pre><code># train_x contains the Bag of words and train_y contains the label/ category
train_x = list(training[:,0])
train_y = list(training[:,1])

#reset underlying graph data
tf.reset_default_graph()
#Build neural network
net = tflearn.input_data(shape=[None,len(train_x[0])])
#layer?
net = tflearn.fully_connected(net,8)
#layer?
net = tflearn.fully_connected(net,8)
#output layer
net = tflearn.fully_connected(net, len(train_y[0]),activation='softmax')
net = tflearn.regression(net)


#define model and set up tensorboard
model = tflearn.DNN(net, tensorboard_dir = 'tflearn_logs')
#start training (grad descent algo)
model.fit(train_x, train_x, n_epoch = 1000, batch_size=1, show_metric = True)
model.save('model.tflearn')
</code></pre>

<p>How do I fix it?</p>
","python, tensorflow, text-classification","<ul>
<li>This is common shape mismatch error</li>
<li>Error is self-explanatory</li>
<li>Your target tensor is of shape [None, 2]</li>
<li>You are feeding you target tensor an array of (1, 1666)</li>
<li><p>Your <strong>model.fit()</strong> should be:</p>

<p><code>model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)</code> </p></li>
<li>See the second parameter which is the target <code>train_y</code> but you have given the second parameter as <code>train_x</code></li>
<li>which means you are saying that your inputs are you labels which is not true </li>
<li>May be that's why its throwing you the error</li>
</ul>
",0,0,463,2018-01-11 10:09:42,https://stackoverflow.com/questions/48204445/tensorflow-bag-of-words-text-classification-cannot-feed-value-of-shape
Number of training samples for text classification tas,"<p>Suppose you have a set of transcribed customer service calls between customers and human agents, where on average each call's length is 7 minutes. Customers will mostly call because of issues they have with the product. Let's assume that a human can assign one label per axis per call:</p>

<ul>
<li>Axis 1: What was the problem from the customer's perspective?</li>
<li>Axis 2: What was the problem from the agent's perspective?</li>
<li>Axis 3: Could the agent resolve the customer's issue?</li>
</ul>

<p>Based on the manually labeled texts you want to train a text classifier that shall predict a label for each call for each of the three axes. But the labeling of recordings takes time and costs money. On the other hand you need a certain amount of training data to get good prediction results.</p>

<p>Given the above assumptions, how many manually labeled training texts would you start with? And how do you know that you need more labeled training texts?</p>

<p>Maybe you've worked on a similar task before and can give some advice.</p>

<p>UPDATE (2018-01-19): There's no right or wrong answer to my question. Ok, ideally, somebody worked on exactly the same task, but that's very unlikely. I'll leave the question open for one more week and then accept the best answer.</p>
","nlp, text-classification, spacy","<p>This would be tricky to answer but I will try my best based on my experience. </p>

<p>In the past, I have performed text classification on 3 datasets; the number in the bracket indicates how big my dataset was: <strong><em>restaurant reviews (50K sentences), reddit comments (250k sentences)</em></strong> and <strong><em>developer comments from issue tracking systems (10k sentences)</em></strong>. Each of them had multiple labels as well. </p>

<p>In each of the three cases, including the one with 10k sentences, I achieved an F1 score of more than 80%. I am stressing on this dataset specifically because I was told by some that the size is less for this dataset. </p>

<p>So, in your case, assuming you have atleast 1000 instances (calls that include conversation between customer and agent) of average 7 minute calls, this should be a decent start. If the results are not satisfying, you have the following options:</p>

<p>1) Use different models (MNB, Random Forest, Decision Tree, and so on in addition to whatever you are using)</p>

<p>2) If point 1 gives more or less similar results, check the ratio of instances of all the classes you have (the 3 axis you are talking about here). If they do not share a good ratio, get more data or try out the different <a href=""https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/"" rel=""nofollow noreferrer"">balancing techniques</a> if you cannot get more data.</p>

<p>3) Another way would be to classify them at a sentence level than message or conversation level to generate more data and individual labels for sentences rather than message or the conversation itself.</p>
",1,0,799,2018-01-17 14:55:20,https://stackoverflow.com/questions/48303933/number-of-training-samples-for-text-classification-tas
AttributeError while designing Naive Bayes Classifier,"<p>I am trying to create a simple <code>Naive Bayes Classifier</code> for classifying data among two classes as mentioned in the code below. But I am stuck with the below error, Can anybody tell me what I am doing wrong.</p>

<pre><code>Traceback (most recent call last):
  File ""NBC.py"", line 33, in &lt;module&gt;
    test(['Apple', 'Banana'])
  File ""NBC.py"", line 16, in test
    prob_dist = classifier.prob_classify(lst)
  File ""/home/***/.local/lib/python3.6/site-packages/nltk/classify/naivebayes.py"", line 95, in prob_classify
    for fname in list(featureset.keys()):
AttributeError: 'list' object has no attribute 'keys'
</code></pre>

<p>""NBC.py""</p>

<pre><code>from nltk.classify import NaiveBayesClassifier

dataFruits = ['Apple', 'Banana', 'Cherry', 'Grape', 'Guava', 
              'Lemon', 'Mangos', 'Orange', 'Strawberry', 'Watermelon']

dataVeggies = ['Potato', 'Spinach', 'Carrot', 'Onion', 'Cabbage', 
               'Barccoli', 'Tomatoe', 'Pea', 'Cucumber', 'Eggplant']

def create_features(word):
    my_dict = dict([(word, True)])
    return my_dict

def test(words):
    lst = [create_features(wd) for wd in words]

    prob_dist = classifier.prob_classify(lst)
    print(prob_dist.prob('fruit'))

class1= [(create_features(item), 'fruit') for item in dataFruits]
#print(class1)

class2 = [(create_features(item), 'veggie') for item in dataVeggies]
#print(class2)

train_set = class1[:] + class2
print(train_set)

# Train
classifier = NaiveBayesClassifier.train(train_set)


# Predict
test(['Apple', 'Banana'])
</code></pre>
","python, text-classification, naivebayes","<p>What your code is trying to do is to build is a very simple classifier based on name features. Based on its name, an item will be classified as a <code>'fruit'</code> or as a <code>'veggie'</code>. The training set contains a few names with their respective classes.</p>

<p>The error you're getting is due to the wrong format of your training set and test set. The training set is a list of <em>featuresets</em> (one featureset for each training example) and should have a structure of the form:</p>

<pre><code>training_set = [featureset1, featureset2, ...]
</code></pre>

<p>Each featureset is a <em>pair</em> <code>(features, class)</code> where <code>features</code> is a dictionary</p>

<pre><code>{'f1': value1, 'f2': value2, ...}
</code></pre>

<p>and <code>class</code> is some value. For instance in your classifier the featureset for <code>'Apple'</code> is:</p>

<pre><code>({'Apple': True,
  'Banana': False,
  'Broccoli': False,
  'Cabbage': False,
  'Carrot': False,
  'Cherry': False,
  'Cucumber': False,
  'Eggplant': False,
  'Grape': False,
  'Guava': False,
  'Lemon': False,
  'Mangos': False,
  'Onion': False,
  'Orange': False,
  'Pea': False,
  'Potato': False,
  'Spinach': False,
  'Strawberry': False,
  'Tomato': False,
  'Watermelon': False},
 'fruit')
</code></pre>

<p>Here is the corrected code:</p>

<pre><code>from nltk.classify import NaiveBayesClassifier, accuracy

dataFruits = ['Apple', 'Banana', 'Cherry', 'Grape', 'Guava', 
              'Lemon', 'Mangos', 'Orange', 'Strawberry', 'Watermelon']

dataVeggies = ['Potato', 'Spinach', 'Carrot', 'Onion', 'Cabbage', 
               'Broccoli', 'Tomato', 'Pea', 'Cucumber', 'Eggplant']

def create_features(word, featureNames):
    my_dict = dict([(w, False) for w in featureNames])    
    my_dict[word] = True
    return my_dict

def test(word):
    lst = create_features(word, allFeatures)
    prob_dist = classifier.prob_classify(lst)
    print('{}'.format(word))
    print('Fruit probability: {:.2f}\tVeggie probability: {:.2f}'.format( prob_dist.prob('fruit'), prob_dist.prob('veggie')))
    return prob_dist

allFeatures = dataFruits + dataVeggies
class1= [(create_features(item, allFeatures), 'fruit') for item in dataFruits]

class2 = [(create_features(item, allFeatures), 'veggie') for item in dataVeggies]

train_set = class1[:] + class2
test_set = [(create_features(item, allFeatures), 'fruit') for item in ['Apple','Banana']]

# Train
classifier = NaiveBayesClassifier.train(train_set)


# Predict
test('Strawberry')
test('Strawby')

# Accuracy on test set
print('Accuracy on test set: {:.2f}'.format(accuracy(classifier, test_set)))  
</code></pre>

<p>A slightly better classifier, maybe this is what you were thinking of (along the lines of the example in <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">http://www.nltk.org/book/ch06.html</a> (Document Classification). Here the classifier simply predicts whether a basket contains more fruits or veggies. Based on this you can construct more complex classifiers (with better features and more training data).            </p>

<pre><code>from nltk.classify import NaiveBayesClassifier, accuracy

dataFruits = ['Apple', 'Banana', 'Cherry', 'Grape', 'Guava', 
              'Lemon', 'Mangos', 'Orange', 'Strawberry', 'Watermelon']

dataVeggies = ['Potato', 'Spinach', 'Carrot', 'Onion', 'Cabbage', 
               'Broccoli', 'Tomato', 'Pea', 'Cucumber', 'Eggplant']


def basket_features(basket): 
    basket_items = set(basket) 
    features = {}
    for item in allFeatures:
        features['contains({})'.format(item)] = (item in basket_items)
    return features

def test(basket):
    lst = basket_features(basket)
    prob_dist = classifier.prob_classify(lst)
    print('Basket: {}'.format(basket))
    print('Fruit probability: {:.2f}\tVeggie probability: {:.2f}'.format(prob_dist.prob('fruit'), prob_dist.prob('veggie')))
    return prob_dist

allFeatures = dataFruits + dataVeggies
class1= [(basket_features([item]), 'fruit') for item in dataFruits]

class2 = [(basket_features([item]), 'veggie') for item in dataVeggies]

train_set = class1[:] + class2

# Train
classifier = NaiveBayesClassifier.train(train_set)


# Predict
test(['Apple', 'Banana', 'Cherry', 'Carrot', 'Eggplant', 'Cabbage','Pea'])
test(['Apple', 'Banana',  'Mangos', 'Carrot', 'Eggplant', 'Cabbage','Pea', 'Cucumber'])
test(['Apple', 'Banana'])
test(['Apple', 'Banana', 'Grape'])

classifier.show_most_informative_features(5)          
</code></pre>
",1,1,769,2018-01-27 19:45:43,https://stackoverflow.com/questions/48479867/attributeerror-while-designing-naive-bayes-classifier
How to handle Naive Bayes Classifier when keywords are not present in training set,"<p>I am trying to implement a simple <code>Naive Bayes Classifier</code>, On training I observed that if keywords (prediction) belongs to both class equally then classifier assigns equal probability to both the classes and if the keywords (prediction) are not present in the training data then also it assigns same probability to both the class.</p>

<p>It is difficult to distinguish between these 2 scenarios. I believe this is happening because of Laplace smoothing of 1 and the probability 0.5 in case 3 is because of a probability of class but I am not sure. Can I do a certain trick to ensure that if keywords are not present in the training data then classifier assigns a probability of zero. As the training data is less is it possible or I should look for some another option for this scenario.</p>

<blockquote>
  <p>Fruit probability: 0.50 Veggie probability: 0.50</p>
  
  <p>Fruit probability: 0.50 Veggie probability: 0.50</p>
  
  <p>Fruit probability: 0.50 Veggie probability: 0.50</p>
</blockquote>

<p>Code</p>

<pre><code>from nltk.classify import NaiveBayesClassifier, accuracy

dataFruits = ['Apple', 'Banana', 'Cherry', 'Grape', 'Guava', 
              'Lemon', 'Mangos', 'Orange', 'Strawberry', 'Watermelon']

dataVeggies = ['Potato', 'Spinach', 'Carrot', 'Onion', 'Cabbage', 
               'Broccoli', 'Tomato', 'Pea', 'Cucumber', 'Eggplant']

def basket_features(basket): 
    basket_items = set(basket) 
    features = {}
    for item in allFeatures:
        features['contains({})'.format(item)] = (item in basket_items)
    return features

def test(basket):
    lst = basket_features(basket)
    prob_dist = classifier.prob_classify(lst)
    print('\nFruit probability: {:.2f}\tVeggie probability: {:.2f}'.format(prob_dist.prob('fruit'), prob_dist.prob('veggie')))

allFeatures = dataFruits + dataVeggies

class1= [(basket_features([item]), 'fruit') for item in dataFruits]
class2 = [(basket_features([item]), 'veggie') for item in dataVeggies]

train_set = class1[:] + class2

# Train
classifier = NaiveBayesClassifier.train(train_set)

# Predict
test(['Apple', 'Banana', 'Potato', 'Spinach'])
test(['Apple', 'Banana', 'Potato', 'Spinach', 'Strawberry', 'Pea'])
test(['Hello', 'World'])
</code></pre>
","python, nlp, nltk, text-classification, naivebayes","<p>It sounds like the Naive Bayes Classifier is doing the right thing, namely trying to estimate the (conditional) probability distribution of classes when given some input features. If there aren't any input features that match your training data (your case 3), then it's correct that the output (conditional) probability distribution is flat. In your case, that means that case 3 (no usable input features) is equivalent to case 1 (input features that are present, but don't discriminate at all between fruit and vegetables).
If you wanted to distinguish the two cases, it might be helpful to look at the prior probability of your input features - this would show that case 1 looks much more like your training data than case 3, which presumably has no features in common with your training data. Depending on how your NBC is constructed, that prior probability might be strictly zero, or assigned some small value to avoid the risk of calculating a log-probability that is singular.</p>
",1,0,694,2018-01-28 09:18:51,https://stackoverflow.com/questions/48484826/how-to-handle-naive-bayes-classifier-when-keywords-are-not-present-in-training-s
Is there anyway to extract Maximum A Posteriori in scikit-learn Multinomial Naive Bayes based on the Stanford NLP research paper?,"<p>I'm trying to replicate the results of the paper in the link </p>

<p><a href=""https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html</a></p>

<p>This link explains how Multinomial Naive Bayes works for text classification.</p>

<p>I've tried to reproduce the example using scikit learn.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import make_scorer
from sklearn.naive_bayes import MultinomialNB

#TRAINING SET
dftrain = pd.DataFrame(data=np.array([[""Chinese Beijing Chinese"", ""Chinese Chinese Shanghai"", ""Chinese Macao"", ""Tokyo Japan Chinese""], 
[""yes"", ""yes"", ""yes"", ""no""]]))

dftrain = dftrain.T
dftrain.columns = ['text', 'label']

#TEST SET
dftest = pd.DataFrame(data=np.array([[""Chinese Chinese Chinese Tokyo Japan""]]))
dftest.columns = ['text']

count_vectorizer = CountVectorizer(min_df=0, token_pattern=r""\b\w+\b"", stop_words = None)
count_train = count_vectorizer.fit_transform(dftrain['text'])
count_test = count_vectorizer.transform(dftest['text'])

clf = MultinomialNB()
clf.fit(count_train, df['label'])
clf.predict(count_test)
</code></pre>

<p>The output is correctly printed as:</p>

<pre><code>array(['yes'],
  dtype='&lt;U3')
</code></pre>

<p>Just like how its mentioned in the paper!
The paper predicts it as YES because </p>

<p><code>P(yes | test set) = 0.0003 &gt; P(no | test set) = 0.0001</code></p>

<p>I want to be able to see those two probabilities! </p>

<p>When I type:</p>

<pre><code>clf.predict_proba(count_test)
</code></pre>

<p>I get </p>

<pre><code>array([[ 0.31024139,  0.68975861]])
</code></pre>

<p>I think what this means is: </p>

<p><code>P(test belongs to label 'no') = 0.31024139</code>
and <code>P(test belongs to label 'yes') = 0.68975861</code></p>

<p>Therefore, <code>scikit-learn</code> predicts the text as belonging to the label <code>yes</code>, but </p>

<p>My question is: Why are the probabilities different? <code>P(yes | test set) = 0.0003 &gt; P(no | test set) = 0.0001</code>, I don't see the numbers <code>0.0003</code> and <code>0.0001</code> but instead see <code>0.31024139</code> and <code>0.68975861</code></p>

<p>Am I missing something here? Does this have something to do with <code>class_prior</code> parameter?</p>

<p>I did read the documentation! </p>

<p><a href=""http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes</a> </p>

<p>Apparently, the parameter is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting.</p>

<p>What I'm wondering is, is there anyway, I can replicated and see the results as the one in the research paper? </p>
","python, scikit-learn, text-classification, naivebayes","<p>This is more to do with the meaning of the probability <code>predict_proba</code> produces. the number .0003 and .0001 are not normalised i.e. they don't sum to one. if you normalise these values you'll get the same result </p>

<p>see the snippet below:</p>

<pre><code>clf.predict_proba(count_test)
Out[63]: array([[ 0.31024139,  0.68975861]])

In [64]: p = (3/4)*((3/7)**3)*(1/14)*(1/14)

In [65]: p
Out[65]: 0.00030121377997263036

In [66]: p0 = (1/4)*((2/9)**3)*(2/9)*(2/9)

In [67]: p0
Out[67]: 0.00013548070246744223

#normalised values
In [68]: p/(p0+p)
Out[68]: 0.6897586117634674

In [69]: p0/(p0+p)
Out[69]: 0.3102413882365326
</code></pre>
",1,1,1490,2018-02-01 19:33:46,https://stackoverflow.com/questions/48570417/is-there-anyway-to-extract-maximum-a-posteriori-in-scikit-learn-multinomial-naiv
Effective classification of natural text in Sci-kit learn/python,"<p>I want my classification algorithm to classify my natural language based raw data based on a set of category if and only if it is going to meet a certain threshold accuracy with respect to a category(say 80% of accuracy) else I want my classifier to classify that particular raw text to a 'unclassified' category. How do I do this?</p>

<p>My example data set:</p>

<pre><code>+----------------------+------------+
| Details              | Category   |
+----------------------+------------+
| Any raw text1        | cat1       |
+----------------------+------------+
| any raw text2        | cat1       |
+----------------------+------------+
| any raw text5        | cat2       |
+----------------------+------------+
| any raw text7        | cat1       |
+----------------------+------------+
| any raw text8        | cat2       |
+----------------------+------------+
| Any raw text4        | cat4       |
+----------------------+------------+
| any raw text5        | cat4       |
+----------------------+------------+
| any raw text6        | cat3       |
+----------------------+------------+
</code></pre>

<p>this would be my training data, I'll divide the same data as test set and train set</p>

<pre><code>import pandas as pd
import numpy as np
import scipy as sp
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt  
from sklearn.model_selection import train_test_split 
data= pd.read_csv('mydata.xls.gold', delimiter='\t',usecols=
['Details','Category'],encoding='utf-8')
target_one=data['Category']
target_list=data['Category'].unique()         
x_train, x_test, y_train, y_test = train_test_split(data.Details, 
data.NUM_CATEGORY, random_state=42)
vect = CountVectorizer(ngram_range=(1,2))
#converting traning features into numeric vector
X_train = vect.fit_transform(x_train.values.astype('U'))
#converting training labels into numeric vector
X_test = vect.transform(x_test.values.astype('U'))
start = time.clock()

mnb = MultinomialNB(alpha =0.13)

mnb.fit(X_train,y_train)

result= mnb.predict(X_test)

print (time.clock()-start)

# mnb.predict_proba(x_test)[0:10,1]
accuracy_score(result,y_test)
</code></pre>

<p>How do I proceed ? Is there any parameter that needs to  be  set for the classifier?
Thanks in advance.</p>
","python-3.x, machine-learning, scikit-learn, text-classification","<p>You can use <code>predict_proba</code> result and create a pandas data-frame with <code>columns =  target_list</code> then use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.max.html"" rel=""nofollow noreferrer""><code>max</code></a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html"" rel=""nofollow noreferrer""><code>idxmax</code></a> to find the category with the highest probability for each element in the test set. once that is done you can use boolean masking and broadcasting to set the categories that's below the threshold to ""unclassified""</p>

<pre><code>import pandas as pd

df = pd.DataFrame(clf.predict_proba(X_test), columns=target_list)
res_df = pd.DataFrame()

res_df['max_prob'] = df.max(axis=1)
res_df['max_prob_cat'] = df.idxmax(axis=1)

res_df.loc[res_df['max_prob'] &lt; .8, 'max_prob_cat'] = 'unclassified'
</code></pre>

<p>df will look like below</p>

<pre><code>              cat1          cat2          cat3          cat4
0     1.091685e-06  2.257549e-04  9.994661e-01  3.070665e-04
1     2.288312e-02  9.752170e-01  1.783878e-03  1.159706e-04
2     1.980685e-01  3.494765e-01  4.416871e-01  1.076788e-02
3     2.205478e-07  9.999601e-01  3.276864e-05  6.920325e-06
4     2.736805e-03  9.795997e-01  1.718200e-02  4.815429e-04
</code></pre>

<p>res_df will look like </p>

<pre><code>      max_prob  max_prob_cat
0     0.999466          cat3
1     0.975217          cat2
2     0.441687  unclassified
3     0.999960          cat2
4     0.979600          cat2
5     0.999956          cat2
6     0.998864          cat3
7     0.996888          cat3
8     0.999422          cat1
9     0.994412          cat3
10    0.954508          cat2
11    0.999999          cat2
</code></pre>
",1,1,164,2018-02-05 05:10:30,https://stackoverflow.com/questions/48616190/effective-classification-of-natural-text-in-sci-kit-learn-python
How to build a text classifier for words?,"<p>I am trying to classify a large number of words into 5 categories. Examples of classes and strings for each class include:</p>

<pre><code>invoice-Number : ""inv123"",""in12"",""123""
invoice-Date   : ""22/09/1994"",""22-Mon-16""
vendor-Name    : ""samplevendorname""
email          : ""abc@gmail.com""
net-amount     : ""1234.56""
</code></pre>

<p>Any pointers to achieve  this in python is very much appreciated.</p>

<p>EDIT 1: I'm looking for a machine learning approach as the number of classes will be more and the data in each class will be different so regex is not feasible.</p>
","python, machine-learning, nltk, text-classification, naivebayes","<p>You can start with a based idea of BoW <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow noreferrer"">(Bag of Word)</a> but modify to BoC (Bac of character) with a tokenizer that doesn't remove any character and build a dictionary of <a href=""https://en.wikipedia.org/wiki/N-gram"" rel=""nofollow noreferrer"">n-grams</a> for 1 to 4 characters.</p>

<p>After that you can represent any word as a vector, that can be counter the number of presences, yes or not presence or <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tfidf</a>.</p>

<p>Then build your model and pass the words-vector to it for learn. You can study the cross label of the n-grams to discard the ones that make noise in the dataset.</p>

<p>I hope this helps for a start point.</p>
",1,-2,973,2018-02-06 12:45:52,https://stackoverflow.com/questions/48643395/how-to-build-a-text-classifier-for-words
Finding correctly and incorrectly classified data,"<p>I want to find the raw data which are classified successfully and which are not classified after Multinomial Nieves Bayes Classification algorithm is applied.
For instance I got the accuracy as 88% after applying Multinomail Naives Bayes classification.
I want to know the 12% of data which are not classified and also 88% of the data that is classified.
 Thanks in advance</p>

<p>My data set:</p>

<pre><code>+----------------------+------------+
| Details              | Category   |
+----------------------+------------+
| Any raw text1        | cat1       |
+----------------------+------------+
| any raw text2        | cat1       |
+----------------------+------------+
| any raw text5        | cat2       |
+----------------------+------------+
| any raw text7        | cat1       |
+----------------------+------------+
| any raw text8        | cat2       |
+----------------------+------------+
| Any raw text4        | cat4       |
+----------------------+------------+
| any raw text5        | cat4       |
+----------------------+------------+
| any raw text6        | cat3       |
+----------------------+------------+
</code></pre>

<p>My code:</p>

<pre><code>import pandas as pd
import numpy as np
import scipy as sp
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt  
from sklearn.model_selection import train_test_split 
data= pd.read_csv('mydat.xls', delimiter='\t',usecols=
['Details','Category'],encoding='utf-8')
target_one=data['Category']
target_list=data['Category'].unique()         
x_train, x_test, y_train, y_test = train_test_split(data.Details, 
data.Category, random_state=42)
vect = CountVectorizer(ngram_range=(1,2))
#converting traning features into numeric vector
X_train = vect.fit_transform(x_train.values.astype('U'))
#converting training labels into numeric vector
X_test = vect.transform(x_test.values.astype('U'))
# start = time.clock()

mnb = MultinomialNB(alpha =0.13)

mnb.fit(X_train,y_train)

result= mnb.predict(X_test)


# mnb.predict_proba(x_test)[0:10,1]
accuracy_score(result,y_test)
</code></pre>
","python, machine-learning, scikit-learn, data-science, text-classification","<p>Just iterate over your Data:</p>

<pre><code>for i in range(len(y_test)):
    if result[i] == y_test[i]:
        print(""CORRECT: "", X_test[i])
    else
        print(""INCORRECT: "", X_test[i])
</code></pre>

<p>you can add them to two different lists or only print the id or do whatever you want instead.</p>
",0,-2,482,2018-02-07 10:34:56,https://stackoverflow.com/questions/48661602/finding-correctly-and-incorrectly-classified-data
Classification of Categories in Text Data,"<p>This may be an abstract question, but I always face difficulties with this kind of problem and it keeps on coming to me.</p>

<p>I crawled data (example: news articles about Tata Steel) extracted the content, manually read the content of each link and classified them as Finance, Operation, Sustainability and so on.</p>

<p>Then I made tf-idf data frame to be the features for classifier model. </p>

<p>I want to train the model to classify these articles. I am only left with either SVM or Logistic using the tf-idf features.</p>

<p>Is there a better approach to clssify text data? Can there be better approach rather then making tf-idf as we may loose information (contextual meaning of sentence) when breaking them into words and use as features.</p>

<p>Any algorithm which can help me to improve classification on text data? </p>
","machine-learning, nlp, text-classification","<p>There are several commercial APIs as well as frameworks for text classification task that improve upon SVM/logistic on tf-idf. They include the semantic/context/word order in sentences for classification. Deep Neural Nets have been quite useful in this task and you can research LSTM and RNN test classification if you want to build a neural net from scratch. For existing and easier to get started, you can look at Spacy and FastText. Both have examples of labeling and training data for classification models  </p>
",1,0,432,2018-02-15 07:07:32,https://stackoverflow.com/questions/48801637/classification-of-categories-in-text-data
"Multi-label text classification with scikit-learn, which classifiers to use?","<p>I have done text classification using scikit-learn Python library importing these classifiers:</p>

<pre><code>from sklearn.linear_model import RidgeClassifier
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
</code></pre>

<p>The input text was in the form:</p>

<pre><code>('some text 1', 'class1')
('some text 2', 'class2')
('some text 3', 'class3')
...
</code></pre>

<p>And everything was ok. But what I want to know is if I have multi-labeled text like:</p>

<pre><code>('some text 1', 'class1', 'class3')
('some text 2', 'class2', 'class1')
('some text 3', 'class3')
...
</code></pre>

<p>if that is possible to use these classifiers, or should I use some other classifiers?</p>
","python, scikit-learn, classification, text-classification","<p>All classifiers able to do Multi-class or Multi-Label are referred on <a href=""http://scikit-learn.org/stable/modules/multiclass.html"" rel=""nofollow noreferrer"">this page</a>.</p>

<p>Based on it, only 2 of your models can be used directly as multi-label:</p>

<ul>
<li>RandomForestClassifier</li>
<li>KNeighborsClassifier</li>
</ul>

<p>After what I've done (in an exercice), is to use a OneVsAll with another compatible classifier then extract the top N or all labels above X% (the more labels you have, the lower will be the threshold as the sum is equal to 1). It's not the cleanest thing you can do but it works (I compared it with multi-label classifier results and it was pretty close or identical)</p>

<p>I hope it helps,
Nicolas</p>
",1,2,3004,2018-02-16 20:56:23,https://stackoverflow.com/questions/48834722/multi-label-text-classification-with-scikit-learn-which-classifiers-to-use
Mutli-class classification in python,"<p>I am in the process of converting a binary classification problem to multi-label classification program. The code is written in python.</p>

<p>The below is the existing code:</p>

<pre><code>positive_labels = [[0, 1] for _ in positive_examples]
negative_labels = [[1, 0] for _ in negative_examples]
</code></pre>

<p>Now i would like to convert this into a multi-label like 3 classes - 0,1,2</p>

<pre><code>positive_labels = [[1,0,0] for _ in positive_examples]
neutral_labels = [[0,1,0] for _ in neutral_examples]
negative_labels = [[0,0,1] for _ in negative_examples]
</code></pre>

<p>Is this correct? If not could you please let me know how to do this?</p>

<p>Please help. </p>
","python, scikit-learn, classification, text-classification, multilabel-classification","<p>You could use MultiLabelBinarizer in scikit-learn for this</p>

<pre><code>from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
# to fit transform you pass the rows of labels
mlb.fit_transform([(0,), (1,),(1,2)])
</code></pre>

<p>You get a output like shown below</p>

<pre><code>array([[1, 0, 0],
       [0, 1, 0],
       [0, 1, 1]])
</code></pre>

<p>fit_transform method implements the TransformerMixin (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html</a>). It fits the learn and then transforms it. Once you have called fit_transform, there is no need to call fit again, you just call transform like shown below</p>

<pre><code>mlb.transform([(1,2),(0,1)]) 

array([[0, 1, 1],
       [1, 1, 0]])
</code></pre>
",2,0,111,2018-02-23 05:14:56,https://stackoverflow.com/questions/48941548/mutli-class-classification-in-python
how to solve this error with lambda and sorted method when i try to make sentiment analysis (POS or NEG text)?,"<p>Input code:   </p>

<p><code>best = sorted(word_scores.items(), key=lambda w, s: s, reverse=True)[:10000]</code></p>

<p>Result:  </p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\Sarah\Desktop\python\test.py"", line 78, in &lt;module&gt;
best = sorted(word_scores.items(), key=lambda w, s: s, reverse=True)[:10000]
TypeError: &lt;lambda&gt;() missing 1 required positional argument: 's'
</code></pre>

<hr>

<p>How do I solve it?</p>
","python-3.x, nltk, sentiment-analysis, feature-extraction, text-classification","<p>If I've understood the format of your <code>word_scores</code> dictionary correctly (that the keys are words and the values are integers representing scores), and you're simply looking to get an ordered list of words with the highest scores, it's as simple as this:</p>

<pre><code>best = sorted(word_scores, key=word_scores.get, reverse=True)[:10000]
</code></pre>

<p>If you want to use a lambda to get an ordered list of tuples, where each tuple is a word and a score, and they are ordered by score, you can do the following:</p>

<pre><code>best = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:10000]
</code></pre>

<p>The difference between this and your original attempt is that I have passed one argument (x) to the lambda, and x is a tuple of length 2 - x[0] is the word and x[1] is the score. Since we want to sort by score, we use x[1].</p>
",0,0,1271,2018-02-23 11:07:57,https://stackoverflow.com/questions/48946458/how-to-solve-this-error-with-lambda-and-sorted-method-when-i-try-to-make-sentime
Test Maximum Entropy classifier,"<p>Is it possible to classify new data trough the Stanford Maximum Entropy classifier WITHOUT creating an external file including all the features?</p>

<p>In other words i have a test file in the following format:</p>

<p><strong>token1 \t feature1_1 \t ... \t feature1_N \t goldLabel1</strong></p>

<p><strong>...</strong></p>

<p><strong>tokenM \t featureM_1 \t ... \t featureM_N \t goldLabelM</strong></p>

<p>I was wondering if it is possible to use a data structure to include test data
without creating an external file.</p>
","nlp, stanford-nlp, text-classification","<p>If you review this method (line 409 in ColumnDataClassifier)</p>

<pre><code>private Pair&lt;GeneralDataset&lt;String,String&gt;, List&lt;String[]&gt;&gt; readDataset(String filename, boolean inTestPhase) {
</code></pre>

<p>you can see how the code goes from a  file path to a <code>Pair&lt;GeneralDataset&lt;String,String&gt;, List&lt;String[]&gt;&gt;</code></p>

<p>That is the key data object needed for evaluation.</p>

<p>If you review this method (line 2158 in ColumnDataClassifier) you can see how the evaluation is done</p>

<p><code>public Pair&lt;Double, Double&gt; testClassifier(String testFile) {</code></p>

<p>If you review the <code>main()</code> method (line 2011) you will see an example of the <code>ColumnDataClassifier</code> being built.</p>

<p>By looking at these three methods you can write additional code to do what you want to do and avoid writing to disk.</p>
",0,0,134,2018-02-23 12:10:10,https://stackoverflow.com/questions/48947590/test-maximum-entropy-classifier
Taking columns of different type as training dataset,"<p>I earlier worked on taking just one column(string type data) as my train set, I would like to take another corresponding column(Amount column of float type) into consideration as a train set along with the Details column.
In the amount column negative value indicates debit and positive value indicates credit.
How do I proceed with this, I tried appending two columns together but I
had to convert the float type amount to  string type which doesn't make
any sense in my dataset.
I want to include the Amount column to check if the machine could learn the variations, which is quite important in this case.
Thanks in advance.</p>

<pre><code>Details                    |Amount               |Category
-------------------------------------------------------------                                
Tanishq Jwellery Bangalore |-990                 |jwellery
ODESK***BAL-28APR13        |240                  |Others
AEGON RELIGARE LIFE IN     |456                  |Others
INTERNET PAYMENT #999999   |-250                 |Transfer in for Card Payment
WWW.VISTAPRINT.IN          |245                  |Print
Khazana Jwellery           |-9000                |jwellery
INTERNET PAYMENT #999999   |785                  |Transfer in for Card Payment
Indian Oil                 |344                  |Fuel
Touch foot wear            |-782                 |Clothing
</code></pre>

<p>Part of my script:</p>

<pre><code>import pandas as pd
import numpy as np
import scipy as sp
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
import time
import matplotlib.pyplot as plt  
from sklearn.model_selection import train_test_split 

# TRAIN DATA
data= pd.read_csv('ds1.csv', delimiter=',',usecols=['Details','Amount','Category'],encoding='utf-8')
data=data[data.Category !=""Others""]

target_one=data['Category']
target_list=data['Category'].unique()

# TEST DATASET
test_data=pd.read_csv('ds2.csv', delimiter='\t',usecols=['Details','Amount','Category'],encoding='utf-8')

x_train, y_train = (data.Details, data.Category )
x_test, y_test = (test_data.Details, test_data.Category)

vect = CountVectorizer(ngram_range=(1,2))
X_train = vect.fit_transform(x_train)

X_test = vect.transform(x_test)
start = time.clock()

mnb = MultinomialNB(alpha =0.13)
mnb.fit(X_train,y_train)

result= mnb.predict(X_test)
print (time.clock()-start)

accuracy_score(result,y_test)
</code></pre>
","python, machine-learning, scikit-learn, data-science, text-classification","<p>If you just want to stack the ""amount"" column to the matrix of text fetaures obtained with the <code>CountVectorizer</code>, just do this before fitting the <code>MultinomialNB</code>:</p>

<pre><code>import numpy as np

X_amount = data[""Amount""].as_matrix().reshape(-1, 1)
X_train = X_train.toarray()
X_train = np.hstack((X_train, X_amount))
X_test_amount = test_data[""Amount""].as_matrix().reshape(-1, 1)
X_test = X_test.toarray()
X_test = np.hstack((X_test, X_test_amount)) 
</code></pre>

<p>OR if you want to keep dealing with sparse matrix for X_train : </p>

<pre><code>import scipy as sp

X_amount = data[""Amount""].as_matrix().reshape(-1, 1)
X_train = sp.sparse.hstack((X_train, X_amount))
X_test_amount = test_data[""Amount""].as_matrix().reshape(-1, 1)
X_test = sp.sparse.hstack((X_test, X_test_amount)) 
</code></pre>

<p>But then, I think you will end up with <code>ValueError: Input X must be non-negative</code>, because <code>MultinomialNB</code> is intended for use with non-negative feature values...</p>
",0,0,292,2018-02-26 12:59:15,https://stackoverflow.com/questions/48989120/taking-columns-of-different-type-as-training-dataset
Input parameter for model as string in Text classification,"<p>I am building document classification system using scikit-learn and it works fine. I am converting the model to Core ML model format. But the model format excepts the input parameter as multiArrayType. I want make it to excepts string or array of string so that I can easily predict from IOS application.I have tried following way:</p>

<pre><code>from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(X_train_dtm, y_train)

#testing a value
docs_new = ['get exclusive prize offer']
docs_pred_class = nb.predict(count_vect.transform(docs_new))

#Exporting to coremodel
import coremltools

coreml_model = coremltools.converters.sklearn.convert(logreg)
#print model
coreml_model
</code></pre>

<p>Printing the coreml model gives following output:</p>

<pre><code> input {
     name: ""input""
     type {
     multiArrayType {
      shape: 7505
      dataType: DOUBLE
    }
  }
}
output {
  name: ""classLabel""
  type {
    int64Type {
    }
  }
}
output {
  name: ""classProbability""
  type {
    dictionaryType {
      int64KeyType {
      }
    }
  }
  }
  predictedFeatureName: ""classLabel""
predictedProbabilitiesName: ""classProbability"" 
</code></pre>

<p>I checked the <a href=""https://github.com/toddkramer/DocumentClassifier/blob/master/Sources/DocumentClassification.mlmodel"" rel=""nofollow noreferrer"">Core ML model</a> in GitHub library, I can see there is different input and output. 
<a href=""https://i.sstatic.net/plPq9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/plPq9.png"" alt=""""></a></p>

<p>How can I achieve this, so that I can pass a simple parameter from IOS app to make prediction.</p>
","python, scikit-learn, text-classification, coreml, coremltools","<p>It sounds like that other mlmodel you found uses a <code>DictVectorizer</code> to turn the strings into indexes (possibly followed by a <code>OneHotEncoder</code>). </p>

<p>You can do this by making a pipeline in sklearn and converting that pipeline to Core ML.</p>
",1,1,857,2018-02-26 13:24:17,https://stackoverflow.com/questions/48989629/input-parameter-for-model-as-string-in-text-classification
Check if a string format a word,"<p>I am building a python text classification application. In the app the user provides a small sentence (or a single word) and we classify his sentence. The problem I'm facing is to find a way to check if his string format a word or a group of words.</p>

<p>Examples of users inputs:</p>

<p>1) ""asdfasdfa""</p>

<p>2) ""This is adsfgafdga""</p>

<p>The example 1 is not a word so I want to raise an Error, also the example 2 contains a non-word string in it so I want to raise an Error too.</p>

<p>Correct Examples:</p>

<p>1) ""Hello""</p>

<p>2) ""This is good""</p>

<p>Is there a way to do that without a list of words or someone know an API to do that?</p>
","python, machine-learning, text-classification","<p>One extensive method is to create a list and store the dictionary words in it. First perform a split on the user input to singularly extract each word off a phrase using a <code>phrase.split()</code>. </p>

<pre><code>words = phrase.split() 
// words : ['This', 'is', 'good'] 

len(words) 
// number of words : 3 
</code></pre>

<p>Run a loop according to the number of words in the phrase if the result is greater than 1.
And then its a mere matter of checking whether the word is present in the list using the following.</p>

<pre><code>if ""word"" in dictionary_words:
   print ""Word is available""
</code></pre>

<p>There's a neat <a href=""http://wordlist.aspell.net/"" rel=""nofollow noreferrer"">XML version of the dictionary words</a> you can use instead of the list.</p>

<p>For a more sophisticated solution, you can try incorporating an API like <a href=""https://github.com/rfk/pyenchant"" rel=""nofollow noreferrer"">PyEnchant</a> that provisions a spell checking library. For further details in this regard, you can check it out and do a <code>pip install pyenchant</code>and import it.</p>

<pre><code>&gt;&gt;&gt; import enchant
&gt;&gt;&gt; help(enchant)
</code></pre>
",2,-1,88,2018-03-07 09:33:08,https://stackoverflow.com/questions/49148447/check-if-a-string-format-a-word
serializeTo parameter in ColumnDataClassifier,"<p>I am currently performing a text classification using the ColumnDataClassifier by Stanford NLP group.
I would like to perform the training stage serializing the model through the serializeTo parameter included in the prop file.</p>

<p>Classification results obtained performing training and test stages through the same command line are different from those ones obtained applying the serialized classifier on a new test document. Why this happens?</p>

<p>Example:</p>

<p><strong>First classification</strong></p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -prop myfile.prop </p>

<p>where in myfile.prop i added values for trainFile and testFile.</p>

<p><a href=""https://i.sstatic.net/kMBQ2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kMBQ2.png"" alt=""enter image description here""></a></p>

<p><strong>Second classification</strong></p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -prop myfile2.prop</p>

<p>where in myfile2.prop i added values for trainFile and serializeTo. I am not including any testFile in myfile2.prop. Once i finish the training stage i want to classify new data using the classifier serialized during the training phase.</p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -loadClassifier MyClassifier -testFile myTestFile</p>

<p><a href=""https://i.sstatic.net/hAO7p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hAO7p.png"" alt=""enter image description here""></a></p>

<p>As you can see results are different. In particular, the serialized classifier associates almost all the instances to the class O (the default one).</p>
","nlp, stanford-nlp, text-classification","<p>Well, i resolved the problem by myself. Writing the following line of code leads to unexpected results:</p>

<p><code>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -loadClassifier MyClassifier -testFile myTestFile</code></p>

<p>This is the right command:</p>

<p><code>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -prop file.prop -loadClassifier MyClassifier -testFile myTestFile</code></p>

<p>In other words, the property file must be included even if it used only during the training stage.</p>
",0,-1,112,2018-03-07 12:07:48,https://stackoverflow.com/questions/49151576/serializeto-parameter-in-columndataclassifier
categorize large text using machine learning,"<p>I have a large xls document where each row contains a problem id , its description and its category for expl: category 1- A- a1</p>

<p>I am trying to build a machine learning model that helps me to classify this document according to the categories. Goal is for each new entry ( aka new problem description), the model we ll be able to define its respective category. </p>

<p>Constraints:
I have more than 10  Categories which are also hierarchical ( category 1 has different subcategories and each subcategory has different subsubcategories). I am thinking of hierarchical classification or multiclass classification But can't tell. </p>

<p>the description feature is a long text. I am thinking of multinomial logistic regression but I read that it needs numerical data. Do I have to make dictionary of all words used in the document to give it a numerical value? is this a correct choice ? </p>

<p>I also want to have scores for each new entry to classify the nearest classes ( for a new description X , the category 2-B-b1 gives an 80% score)</p>
","python, text, machine-learning, classification, text-classification","<p>One of the strategies you could use is a Bernoulli Naive Bayes (<a href=""https://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</a>).</p>

<p>This is a simple mathematical equation that allows you to reduce the problem to simple lists of word frequencies per category. </p>

<p>Once you have established a meaningful baseline of word frequencies for texts that are known to be in the appropriate categories, the formula will be able to return a probability of match in each category for a new text.</p>

<p>This can give a very large matrix of words x categories but the processing of each element is very simple.  Depending on your volumes and on the performance requirements, there is an optimization of the formula that can be done to limit calculations to the words that are actually present in the text to categorize and skip the factors that are linked to other words that have been seen before but are not present in the text (I could elaborate on that if the Bernoulli classifier is relevant to your solution).  Note that there could be existing implementations of the classifier in Python (I haven't checked).</p>
",1,1,1003,2018-03-07 12:20:14,https://stackoverflow.com/questions/49151825/categorize-large-text-using-machine-learning
Text Classification using Keras,"<p>I am starting off with Keras in R and want to build a model for text classification. However I am stuck with an error which most likely is due to my limited understanding of Deep Learning and Keras. Any help would be great. Sharing the code below. The data in the code snippet is limited, so that it may be as quickly reproducible by the gurus. </p>

<pre><code>library(keras)
library(tm)

data &lt;- data.frame(""Id"" = 1:10, ""Text"" = c(""the cat was mewing"",""the cat was black in color"",""the dog jumped over the wall"",""cat cat cat everywhere"",""dog dog cat play style"",""cat is white yet it is nice"",""dog is barking"",""cat sweet"",""angry dog"",""cat is nice nice nice""), ""Label"" = c(1,1,2,1,2,1,2,1,2,1))
corpus &lt;- VCorpus(VectorSource(data$Text))
tdm &lt;- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stopwords = TRUE,removeNumbers = TRUE))
data_t &lt;- as.matrix(tdm)
data &lt;- cbind(data_t,data$Label) 
dimnames(data) = NULL
#Normalize data
data[,1:(ncol(data)-1)] = normalize(data[,1:(ncol(data)-1)])
data[,ncol(data)] = as.numeric(data[,ncol(data)]) - 1
set.seed(123)
ind = sample(2,nrow(data),replace = T,prob = c(0.8,0.2))
training = data[ind==1,1:(ncol(data)-1)]
test = data[ind==2,1:(ncol(data)-1)]
traintarget = data[ind==1,ncol(data)]
testtarget = data[ind==2,ncol(data)]
# One hot encoding
trainLabels = to_categorical(traintarget)
testLabels = to_categorical(testtarget)
print(testLabels)
#Create sequential model
model = keras_model_sequential()
model %&gt;% 
  layer_dense(units=8,activation='relu',input_shape=c(16)) 
summary(model)
model %&gt;%
compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')
history = model %&gt;%
  fit(training,
      trainLabels,
      epoch=200,
      batch_size=2,
      validation_split=0.2)
</code></pre>

<p>In this example one hot encoding may be unnecessary. Along with that, there may be several areas where I have gone wrong. However, the last line of the code is throwing me an error with the shape. I have used shape as 16, due to 16 columns in my data.</p>

<p>The error that I am getting is </p>

<p>Error in py_call_impl(callable, dots$args, dots$keywords) : 
  ValueError: Error when checking target: expected dense_32 to have shape (None, 8) but got array with shape (7, 2)</p>

<p>Any guidance in this would be really helpful</p>
","r, keras, text-classification","<p>It is due to the fact that your first layer is also your output layer. Your output layer should have the same amount of units as the number of classes that you are trying to predict. Here, it has 8 neurons while you have only 2 classes (<code>trainLabels</code> has two columns). In your case you could edit your model like this:</p>

<pre><code>model %&gt;% 
  layer_dense(units = 8, activation = 'relu', input_shape = 16) %&gt;%
  layer_dense(units = 2, activation = 'softmax')
</code></pre>
",1,1,426,2018-03-07 15:23:06,https://stackoverflow.com/questions/49155477/text-classification-using-keras
Processing Text for Classification with Keras,"<p>I'm trying to train a basic text classification NN using Keras. I download 12,500 pos and 12,500 negative movie reviews from a website. I'm having trouble processing the data into something Keras can use however. </p>

<p>First, I open the 25000 text files and store each file into an array. I then run each array (one positive and one negative) through this function:</p>

<pre><code>def process_for_model(textArray):
    '''
     Given a 2D array of the form:
     [[fileLines1],[fileLines2]...[fileLinesN]]
     converts the text into integers
    '''
    result = []
    for file_ in textArray:
        inner = []
        for line in file_:
            length = len(set(text_to_word_sequence(line)))
            inner.append(hashing_trick(line,round(length*1.3),hash_function='md5'))
        result.append(inner)

    return result
</code></pre>

<p>With the purpose of converting the words into numbers to get them close to something a Keras model can use.</p>

<p>I then append the converted numbers into a single array, along with appending a 0 or 1 to another array as labels:</p>

<pre><code>training_labels = []
train_batches = []
for i in range(len(positive_encoded)):
    train_batches.append(positive_encoded[i])
    training_labels.append([0])
for i in range(len(negative_encoded)):
    train_batches.append(negative_encoded[i])
    training_labels.append([1])
</code></pre>

<p>And finally I convert each array to a np array:</p>

<pre><code>train_batches = array(train_batches)
training_labels = array(training_labels)
</code></pre>

<p>However, I'm not really sure where to go from here. Each review is, I believe, 168 words. I don't know how to create an appropriate model for this data or how to properly scale all the numbers to be between 0 and 1 using sklearn.</p>

<p>The things I am most confused on are: how many layers should I have, how many neurons each layer should have, and how many input dimensions should I have for the first layer.</p>

<p>Should I be taking another approach entirely?</p>
","python, machine-learning, keras, text-classification","<p>Here is quite a good tutotial with Keras and this dataset: <a href=""https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/</a></p>
",1,1,361,2018-03-12 02:11:14,https://stackoverflow.com/questions/49227237/processing-text-for-classification-with-keras
Neural Networks in WEKA,"<p>I have a textual data set which has been already classified. I have 7 available classes. </p>

<p>I used (Waikato Environment for Knowledge Analysis) WEKA in building the model. Also with this, I have trained and tested 3 different algorithms to determine which algorithm works best for my data set. </p>

<p>I tried Naive Bayes, J48 and Neural Networks (SMO) which are all available in WEKA's machine learning environment. </p>

<p>During training and testing, found out the ranking of three algorithms in terms of accuracy with the following:</p>

<ol>
<li>Neural Networks - 98%</li>
<li>Naive Bayes - 90%</li>
<li>J48 - 85%</li>
</ol>

<p>With the results above, I decided to use the Neural Networks and build the model. I created an application in JAVA and loaded the Neural Networks model built from WEKA.</p>

<p>However, my problem is, the model cannot predict the new data correctly. I am a bit confused as during training and testing I obtained a high accuracy but during deployment the accuracy rate is somewhat 40% only. </p>

<p>I tried to do this in C# and obtained the same results.
Below is a sample code I used. </p>

<pre><code>   Instances test = null; 
   DataSource source = new DataSource(""C:\\Users\\Ian\\Desktop\\FINAL\\testdataset.arff"");
   test = source.getDataSet();
   test.setClassIndex(1); 

   FilteredClassifier cl1 = (FilteredClassifier) weka.core.SerializationHelper.read(""C:\\FINAL\\NeuralNetworks.model"");        
   Evaluation evaluation = new Evaluation(test);      
   evaluation.evaluateModel(cl1,test);
   System.out.println(""Results:"" + evaluation.toSummaryString());   


    for (int i = 0; i &lt; test.numInstances(); i++) {
        String trueClassLabel = test.instance(i).toString(test.classIndex());
        double predictionIndex =cl1.classifyInstance(test.instance(i));

        String predictedClassLabel;
        predictedClassLabel = test.classAttribute().value((int) predictionIndex );
        System.out.println((i+1) + ""\t"" + trueClassLabel + ""\t"" + predictedClassLabel);
    }
</code></pre>

<p>Any advise where do you think I did wrong?</p>
","machine-learning, neural-network, weka, text-classification","<p>After our short chat in the comments, it seems obvious to me that you are overfitting on your training data. This is most likely caused by a neural network architecture which is too overpowered for the problem you are trying to solve. It can be shown that any function can be represented by a neural network with just enough degrees of freedom. Instead of finding a well generalizing solution, the NN memorized the training data during training, leading to a nearly perfect accuracy. But as soon as it has to deal with new data, it can not do it very well, as it didn’t find a proper generalization rule.
In order to solve this problem, you have to reduce the degrees of freedom of your NN. This can be achieved by reducing the number of layers and nodes in each layer. Try to start as simple as only 1 or 2 hidden layers with very few nodes. Then, keep increasing both, nodes and layers until you reach your best performance.</p>

<p>Important: Always measure performance with independent test set, rather than with the same data you trained the model with.</p>

<p>You can find some further hints on this issue <a href=""https://de.mathworks.com/help/nnet/ug/improve-neural-network-generalization-and-avoid-overfitting.html"" rel=""nofollow noreferrer"">here</a></p>
",0,0,3907,2018-03-13 06:43:59,https://stackoverflow.com/questions/49249828/neural-networks-in-weka
Is there method .predict in official python bindings for fastText,"<p>I know there are unofficial bindings with .predict method in python(fasttext, pyfasttext) but they do not work with recent models trained on on official FastText bash tool or do not have all the options. Official python bindings have only load_model(path)and tokenize(text) methods described , which sounds strange as with this you can not do any predictions. Am I missing something here? </p>
","machine-learning, nlp, text-classification, fasttext","<p>I use the Python package built and installed according to this link <a href=""https://github.com/facebookresearch/fastText/blob/master/README.md#building-fasttext-for-python"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/blob/master/README.md#building-fasttext-for-python</a>. I consider it official.
The model object loaded via load_model has the requested predict method.</p>
",2,0,316,2018-03-13 21:34:07,https://stackoverflow.com/questions/49266209/is-there-method-predict-in-official-python-bindings-for-fasttext
random forest text classification giving extra rows in prediction,"<p>I am using random forest for text classification. My input data is having 17197 rows.</p>

<pre><code>&gt; nrow(sparse_4testing)
[1] 17197
</code></pre>

<p>I am using </p>

<pre><code>set.seed(123)
tweetRand = randomForest(label ~ ., data = train_sparse, importance=TRUE, nTree=500)

predicrRand_test=predict(tweetRand, data=sparse_4testing)
q1=data.frame(ifelse(predicrRand_test&gt;0.5,1,0))
</code></pre>

<p>The issue is when I am doing a sanity check I am getting extra rows in q1</p>

<pre><code>&gt; nrow(q1)
[1] 22373  
</code></pre>

<p>I do not understand the issue. I am new to machine learning. Please help me out.
I have run the model multiple time. Still getting the same issue. </p>

<pre><code>&gt; nrow(predicrRand_test)

NULL
&gt; head(predicrRand_test)
            1             3             6             7             9            10 
 1.858321e-01 -8.326673e-17  1.321640e-01  2.222222e-04  2.345304e-02  1.651133e-01 
&gt; head(q1)
   ifelse.predicrRand_test...0.05..1..0.
1                                      1
3                                      0
6                                      1
7                                      0
9                                      0
10                                     1

&gt; length(predicrRand_test)
[1] 22373
</code></pre>
","r, machine-learning, random-forest, text-classification","<p>The issue is due to wrong argument name in <code>predict</code> - it should be <code>newdata</code>, not <code>data</code> (<a href=""http://ugrad.stat.ubc.ca/R/library/randomForest/html/predict.randomForest.html"" rel=""nofollow noreferrer"">docs</a>):</p>

<pre><code>predicrRand_test=predict(tweetRand, newdata=sparse_4testing)
</code></pre>

<p>As it is now, your code ignores the <code>data</code> argument, and simply returns the predictions on the training set in the <code>predicrRand_test</code> dataframe.</p>
",0,0,164,2018-03-16 14:41:52,https://stackoverflow.com/questions/49323408/random-forest-text-classification-giving-extra-rows-in-prediction
How to maximize recall in multilabel setting?,"<p>I have a text classification problem in which I want to assign one of three labels (-1, 0, 1) to text documents. The most important metric is recall: I care that all texts that should be labelled ""-1"" are indeed labelled ""-1"". The precision, that everything that is labelled ""-1"" is indeed labelled ""-1"", is less important. </p>

<p>So far, I am using a pipeline in scikit-learn with logistic regression. The hyperparameters are tuned in GridSearchCV, but so far, accuracy is maximized.  </p>

<pre><code>steps = [('vect', CountVectorizer()),
      ('tfidf', TfidfTransformer()), 
      ('clf', LogisticRegression())]

parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],
           'tfidf__use_idf': (True, False),
           'clf__C': [0.001, 0.01, 0.1, 1, 10],}

pipeline = Pipeline(steps)
text_clf = GridSearchCV(pipeline, parameters, cv = 5)

text_clf.fit(X_train, y_train)
y_pred = text_clf.predict(X_test)

scores = cross_val_score(text_clf, X_test, y_test, cv = 5)
</code></pre>

<p>Changing</p>

<pre><code>text_clf = GridSearchCV(pipeline, parameters, scoring = 'recall', cv = 5)
</code></pre>

<p>does not work because it is a multiclass setting. Does anybody have an idea how I could reformulate this in order to maximise recall? </p>
","python, scikit-learn, classification, text-classification","<p>The grid-search can work if only a single number is presented by the metric as score which GridSearchCV will use to order the results.</p>

<p>In case of multi-label setting, you need to decide which type of averaging you want there for different labels. You can use the following alternatives:</p>

<pre><code>scoring = 'recall_micro'
scoring = 'recall_macro'
scoring = 'recall_weighted'
scoring = 'recall_samples'
</code></pre>

<p>For the description of these, please refer to <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score"" rel=""nofollow noreferrer"">documentation of recall_score</a>:</p>

<blockquote>
<pre><code>average : string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]

    This parameter is required for multiclass/multilabel targets. 
    If None, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    'binary':
        Only report results for the class specified by pos_label. 
        This is applicable only if targets (y_{true,pred}) are binary.

    'micro':
        Calculate metrics globally by counting the total true positives, 
        false negatives and false positives.

    'macro':
        Calculate metrics for each label, and find their unweighted mean. 
        This does not take label imbalance into account.

    'weighted':
        Calculate metrics for each label, and find their average, weighted 
        by support (the number of true instances for each label).
        This alters ‘macro’ to account for label imbalance; it can result in
        an F-score that is not between precision and recall.

    'samples':
        Calculate metrics for each instance, and find their average 
        (only meaningful for multilabel classification where this
        differs from accuracy_score).
</code></pre>
</blockquote>
",1,1,899,2018-03-18 20:15:16,https://stackoverflow.com/questions/49352416/how-to-maximize-recall-in-multilabel-setting
Sentiment classifier training with Keras,"<p>I am using keras (backend tensorflow) to classify sentiments from Amazon review.</p>

<p>It starts with an embedding layer (which uses GloVe), then LSTM layer and finally a Dense layer as output. Model summary below:</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 100)         2258700   
_________________________________________________________________
lstm_1 (LSTM)                (None, 16)                7488      
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 2,266,273
Trainable params: 2,266,273
Non-trainable params: 0
_________________________________________________________________
Train on 454728 samples, validate on 113683 samples
</code></pre>

<p>When training the train and eval accuracy is about 74% and loss (train and eval) around 0.6.</p>

<p>I've tried with changing amount of elements in LSTM layer, as well as including dropout, recurrent dropout, regularizer, and with GRU (instead of LSTM). Then the accuracy increased a bit (~76%).</p>

<p><a href=""https://i.sstatic.net/1HBeU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1HBeU.png"" alt=""enter image description here""></a></p>

<p>What else could I try in order to improve my results?</p>
","tensorflow, keras, text-classification","<p>I have had a great a better success with sentiment analysis using Bidirectional LSTM also stacking two layers vertically i.e 2 LSTMS together forming a deep network also helped and try to increase the number of lstm elements to be around 128.</p>
",0,0,154,2018-03-20 00:45:42,https://stackoverflow.com/questions/49374691/sentiment-classifier-training-with-keras
Improving the prediction score by use of confidence level of classifiers on instances,"<p>I am using three classifiers (<code>RandomForestClassifier</code>, <code>KNearestNeighborClassifier</code>, and <code>SVM Classifier</code>)  which you can see below:</p>

<pre><code>&gt;&gt; svm_clf_sl_GS
SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=True, random_state=41, shrinking=True,
  tol=0.001, verbose=False)

&gt;&gt; knn_clf_sl_GS
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=3, p=2,
           weights='distance')

&gt;&gt; for_clf_sl_GS
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
</code></pre>

<p>During training, <code>RandomForestClassifer</code> gives the best <code>f1_score</code> followed by <code>KNearestNeighborClassifier</code>, and then <code>SVMClassifier</code> on the predictions from the data. Here is my X_train (standard scaled values, if needed you can ask how I got this) &amp; y_train:</p>

<pre><code>&gt;&gt; X_train
array([[-0.11034393, -0.72380296,  0.15254572, ...,  0.4166148 ,
        -0.91095473, -0.91095295],
       [ 1.6817184 ,  0.40040944, -0.6770607 , ..., -0.2403781 ,
         0.02962478,  0.02962424],
       [ 1.01128052, -0.21062032, -0.2460462 , ..., -0.04817728,
        -0.15848331, -0.15847739],
       ..., 
       [-1.18666853,  0.87297522,  0.47136779, ..., -0.19599824,
         0.72417473,  0.72416714],
       [ 1.6835304 ,  0.40605067, -0.63383059, ..., -0.37094083,
         0.09505496,  0.09505389],
       [ 0.19950709, -1.04624152, -0.18351693, ...,  0.4362658 ,
        -0.77994791, -0.77994176]])

&gt;&gt; y_train_sl
874     0
1863    0
1493    0
288     1
260     0
495     0
1529    0
1704    1
75      1
1792    0
626     0
99      1
222     0
774     0
52      1
1688    1
1770    0
53      1
1814    0
488     0
230     0
481     0
132     1
831     0
1166    1
1593    0
771     0
1785    0
616     0
207     0
       ..
155     1
1506    0
719     0
547     0
613     0
652     0
1351    0
304     0
1689    1
1693    1
1128    0
1323    0
763     0
701     0
467     0
917     0
329     0
375     0
1721    0
928     0
1784    0
1200    0
832     0
986     0
1687    1
643     0
802     0
280     1
1864    0
1045    0
Name: Type of Formation_shaly limestone, Length: 1390, dtype: uint8
</code></pre>

<p>As you can see my y_train is in Boolean form (i.e. where the instances are <code>True</code> and where <code>False</code>.</p>

<p>I want to improve the accuracy of the predictions further by use of <code>predict_proba</code> in such a way that when I see that predictions from the classifier (let's say <code>RandomForestClassifier</code> first) has a low confidence level (&lt;60%) about particular instances it predicted (which is what I am supposed to find first), it moves to the next classifier (let's say <code>KNearestNeighborClassifier</code>) and check the confidence level of those instances by the next classifier on those instances, if it has a high confidence level compared to the previous classifier (>60%) accept the solution from that classifier instead, similarly if this classifier has a lower confidence level on the same instances still(&lt;60%), move to the next classifier and do the same thing for the third classifier.</p>

<p>Finally, if the third classifier has a lower confidence level (&lt;60%) too, I need to accept the solution from the classifier which has the highest confidence level among all three classifiers. </p>

<p>Since, I am new to Machine Learning I might be confusing you with some of the statements for which I apologize so just correct me where I am wrong. </p>

<p><strong>EDIT:</strong>
X_test and y_test are shown below. I need to predict on the X_test_prepared and evaluate the predictions and y_test_sl using <code>f1_score</code>. The predicted y must have passed through all three classifiers and has the best confidence levels for all the instances. </p>

<pre><code>&gt;&gt; X_test_prepared
array([[ 0.69961751, -0.11156033, -0.43852312, ..., -0.40967982,
         0.32099948,  0.32099952],
       [ 0.90256086, -0.54532856, -0.46399801, ..., -0.05752097,
        -0.54261829, -0.54261947],
       [ 1.67447042,  0.24530384, -1.0113221 , ..., -0.54844942,
        -0.26066608, -0.26066032],
       ...,
       [ 0.28104683,  1.52670909,  0.62653301, ..., -1.15596295,
         2.05859487,  2.05859247],
       [ 1.50595496,  0.84507934, -0.44109634, ..., -0.71277072,
         0.14474518,  0.14474398],
       [-1.63423112, -0.12690448,  0.48577783, ..., -0.36025459,
         0.29137477,  0.29137047]])

&gt;&gt; y_test_sl
1321    0
1433    0
1859    0
1496    0
492     0
736     0
996     0
1001    0
634     0
1486    0
910     0
1579    0
373     0
1750    0
1563    0
1584    0
51      1
349     0
1162    1
594     0
1121    0
1637    0
1116    0
106     1
1533    0
993     0
960     0
277     0
142     1
1010    0
       ..
1104    1
1404    0
1646    0
1009    0
61      1
444     0
10      1
704     0
744     0
418     0
998     0
740     0
465     0
97      1
1550    1
1738    0
978     0
690     0
1071    0
1228    1
1539    0
145     1
1015    0
1371    0
1758    0
315     0
71      1
1090    0
1766    0
33      1
Name: Type of Formation_shaly limestone, Length: 515, dtype: uint8
</code></pre>
","python, machine-learning, boolean, text-classification","<p>The goal here turned out to create an ensemble of classifiers and take the most ""confident"" (highest probability class) predictions of all classifiers. The code is below: </p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import numpy as np
from sklearn.datasets import make_classification

X_train, y_train = make_classification(n_features=4) # Put your training data here instead

# parameters for random forest
rfclf_params = {
    'bootstrap': True, 
    'class_weight':None, 
    'criterion':'entropy',
    'max_depth':None, 
    'max_features':'auto', 
    # ... fill in the rest you want here
}

# Fill in svm params here
svm_params = {
    'probability':True
}

# KNeighbors params go here
kneighbors_params = {

}

params = [rfclf_params, svm_params, kneighbors_params]
classifiers = [RandomForestClassifier, SVC, KNeighborsClassifier]

def ensemble(classifiers, params, X_train, y_train, X_test):
    best_preds = np.zeros((len(X_test), 2))
    classes = np.unique(y_train)

    for i in range(len(classifiers)):
        # Construct the classifier by unpacking params 
        # store classifier instance
        clf = classifiers[i](**params[i])
        # Fit the classifier as usual and call predict_proba
        clf.fit(X_train, y_train)
        y_preds = clf.predict_proba(X_test)
        # Take maximum probability for each class on each classifier 
        # This is done for every instance in X_test
        # see the docs of np.maximum here: 
        # https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.maximum.html
        best_preds = np.maximum(best_preds, y_preds)

    # map the maximum probability for each instance back to its corresponding class
    preds = np.array([classes[np.argmax(pred)] for pred in best_preds])
    return preds

# Test your predictions  
from sklearn.metrics import accuracy_score, f1_score
y_preds = ensemble(classifiers, params, X_train, y_train, X_train)
print(accuracy_score(y_train, y_preds), f1_score(y_train, y_preds))
</code></pre>

<p>If you want the algorithm to return the highest probabilities instead of the predicted class, have <code>ensemble</code> return <code>[np.amax(pred_probs) for pred_probs in best_preds]</code> rather than preds. </p>
",0,0,1358,2018-03-21 02:00:43,https://stackoverflow.com/questions/49396961/improving-the-prediction-score-by-use-of-confidence-level-of-classifiers-on-inst
TypeError from MultinomialNB: float() argument must be a string or a number,"<p>I am trying to compare the performance of Multinomial, Binomial and Bernoulli classifiers but I am having an error:</p>

<blockquote>
  <p>TypeError: float() argument must be a string or a number, not 'set'</p>
</blockquote>

<p>The code below is til <code>MultinomialNB</code>.</p>

<pre><code>documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

#print(documents[1])

all_words = []

for w in movie_reviews.words():
    all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3000]

def look_for_features(document):
    words = set(document)
    features = {}
    for x in word_features:
        features[x] = {x in words}
    return features

#feature set will be finding features and category
featuresets = [(look_for_features(rev), category) for (rev, category) in documents]

training_set = featuresets[:1400]
testing_set = featuresets[1400:]

#Multinomial
MNB_classifier = SklearnClassifier(MultinomialNB())
MNB_classifier.train(training_set)
print (""Accuracy: "", (nltk.classify.accuracy(MNB_classifier,testing_set))*100)
</code></pre>

<p>The error seems to be in <code>MNB_classifier.train(training_set)</code>. 
The error in this code is similar to error <a href=""https://stackoverflow.com/questions/45814086/naive-base-classifier-of-nltk-giving-unhashable-type-error"">here</a>.</p>
","python-3.x, machine-learning, scikit-learn, text-classification, naivebayes","<p>Change...</p>

<pre><code>features[x] = {x in words}
</code></pre>

<p>to...</p>

<pre><code>features[x] = x in words
</code></pre>

<p>The first line creates a list <code>featuresets</code> of pairs <code>(word, {True})</code> or <code>(word, {False})</code>, i.e. the second element is a <code>set</code>. <code>SklearnClassifier</code> does not expect this as a label.</p>

<hr>

<p>The code looks very much like one from <a href=""https://pythonprogramming.net/sentiment-analysis-module-nltk-tutorial/"" rel=""nofollow noreferrer"">""Creating a module for Sentiment Analysis with NLTK""</a>. The author is using a tuple <code>(x in words)</code> there, but it's no different from just <code>x in words</code>.</p>
",1,1,358,2018-03-21 19:38:38,https://stackoverflow.com/questions/49415195/typeerror-from-multinomialnb-float-argument-must-be-a-string-or-a-number
maximum number of classes for ColumnDataClassifier,"<p>Is there a limit on the maximum number of classes i can have in using ColumnDataClassifier? I have about addresses that I want to assign to 10k orgs, but i kept running into memory issue even after I set the -xmx number to maximum. </p>
","classification, stanford-nlp, text-classification","<p>There isn't an explicit limit for the size of the label set, but 10k is an extremely large set, and I am not surprised you are having memory issues.  You should try some experiments with substantially smaller label sets (~ 100 labels) and see if your issues go away.  I don't know how many labels will practically work, but I doubt it's anywhere near 10,000.  I would try much smaller sets just to understand how the memory usage is growing at the label set size grows.</p>

<p>You may have to have a hierarchy of labels and different classifiers.  You could imagine the first label being ""California-organization"", and then having a second classifier to select the various California organizations, etc...</p>
",0,0,274,2018-03-27 18:10:10,https://stackoverflow.com/questions/49519757/maximum-number-of-classes-for-columndataclassifier
sklearn model data transform error: CountVectorizer - Vocabulary wasn&#39;t fitted,"<p>I already trained a model for topic classification. Then when I am going to  transform new data into vectors for prediction, it going wrong. It shows ""NotFittedError: CountVectorizer - Vocabulary wasn't fitted."" But when I did the prediction by splitting training data into test data in trained model, it works. Here are the code:</p>

<pre><code>from sklearn.externals import joblib
from sklearn.feature_extraction.text import CountVectorizer

import pandas as pd
import numpy as np

# read new dataset
testdf = pd.read_csv('C://Users/KW198/Documents/topic_model/training_data/testdata.csv', encoding='cp950')

testdf.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1800 entries, 0 to 1799
Data columns (total 2 columns):
keywords    1800 non-null object
topics      1800 non-null int64
dtypes: int64(1), object(1)
memory usage: 28.2+ KB

# read columns
kw = testdf['keywords']
label = testdf['topics']

# 將預測資料轉為向量
vectorizer = CountVectorizer(min_df=1, stop_words='english')
x_testkw_vec = vectorizer.transform(kw)
</code></pre>

<p>Here is an error</p>

<pre><code>---------------------------------------------------------------------------
NotFittedError                            Traceback (most recent call last)
&lt;ipython-input-93-cfcc7201e0f8&gt; in &lt;module&gt;()
      1 # 將預測資料轉為向量
      2 vectorizer = CountVectorizer(min_df=1, stop_words='english')
----&gt; 3 x_testkw_vec = vectorizer.transform(kw)

~\Anaconda3\envs\ztdl\lib\site-packages\sklearn\feature_extraction\text.py in transform(self, raw_documents)
    918             self._validate_vocabulary()
    919 
--&gt; 920         self._check_vocabulary()
    921 
    922         # use the same matrix-building strategy as fit_transform

~\Anaconda3\envs\ztdl\lib\site-packages\sklearn\feature_extraction\text.py in _check_vocabulary(self)
    301         """"""Check if vocabulary is empty or missing (not fit-ed)""""""
    302         msg = ""%(name)s - Vocabulary wasn't fitted.""
--&gt; 303         check_is_fitted(self, 'vocabulary_', msg=msg),
    304 
    305         if len(self.vocabulary_) == 0:

~\Anaconda3\envs\ztdl\lib\site-packages\sklearn\utils\validation.py in check_is_fitted(estimator, attributes, msg, all_or_any)
    766 
    767     if not all_or_any([hasattr(estimator, attr) for attr in attributes]):
--&gt; 768         raise NotFittedError(msg % {'name': type(estimator).__name__})
    769 
    770 

NotFittedError: CountVectorizer - Vocabulary wasn't fitted.
</code></pre>
","python, machine-learning, scikit-learn, text-classification, countvectorizer","<p>You need to call <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit"" rel=""noreferrer""><code>vectorizer.fit()</code></a> for the count vectorizer to build the dictionary of words before calling <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform"" rel=""noreferrer""><code>vectorizer.transform()</code></a>. You can also just call <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform"" rel=""noreferrer""><code>vectorizer.fit_transform()</code></a> that combines both. </p>

<p><strong>But you should not</strong> be using a new vectorizer for test or any kind of inference. You need to use the same one you used when training the model, or your results will be random since vocabularies are different (lacking some words, does not have the same alignment etc..)</p>

<p>For that, you can just <a href=""https://docs.python.org/3/library/pickle.html"" rel=""noreferrer"">pickle</a> the vectorizer used in the training and load it on inference/test time.</p>
",6,3,4463,2018-03-29 03:48:27,https://stackoverflow.com/questions/49547715/sklearn-model-data-transform-error-countvectorizer-vocabulary-wasnt-fitted
keras neural network giving float ouputs but i need boolean outputs,"<p>this is my code
loaded the pre-trained weights and embedding matrices</p>

<pre><code>from __future__ import print_function
import numpy as np
import pandas as pd
import csv, datetime, time, json
from zipfile import ZipFile
from os.path import expanduser, exists
from keras.preprocessing.text import Tokenizer
 from keras.preprocessing.sequence import pad_sequences
  from keras.models import Model
  from keras.layers import Input, TimeDistributed, Dense, Lambda, 
concatenate, Dropout, BatchNormalization
 from keras.layers.embeddings import Embedding
from keras.regularizers import l2
from keras.callbacks import Callback, ModelCheckpoint
from keras.utils.data_utils import get_file
from keras import backend as K
from sklearn.model_selection import train_test_split


**Initialize global variables**
 KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')
 QUESTION_PAIRS_FILE_URL = 
 'http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv'
 QUESTION_PAIRS_FILE = 'test.csv'
 GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'
 GLOVE_ZIP_FILE = 'glove.840B.300d.zip'
 GLOVE_FILE = 'glove.840B.300d.txt'
 Q1_TRAINING_DATA_FILE = 'q1_train.npy'
 Q2_TRAINING_DATA_FILE = 'q2_train.npy'
 LABEL_TRAINING_DATA_FILE = 'label_train.npy'
 WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'
 NB_WORDS_DATA_FILE = 'nb_words.json'
 MAX_NB_WORDS = 200000
 MAX_SEQUENCE_LENGTH = 25
 EMBEDDING_DIM = 300
 MODEL_WEIGHTS_FILE = 'question_pairs_weights.h5'
 VALIDATION_SPLIT = 0.1
 TEST_SPLIT = 0.1
 RNG_SEED = 13371447
 NB_EPOCHS = 1
 DROPOUT = 0.1
 BATCH_SIZE = 32
 OPTIMIZER = 'adam'



word_embedding_matrix = np.load(open(WORD_EMBEDDING_MATRIX_FILE, 'rb'))
with open(NB_WORDS_DATA_FILE, 'r') as f:
nb_words = json.load(f)['nb_words']

print(""Processing"", QUESTION_PAIRS_FILE)
question1 = []
question2 = []

with open(KERAS_DATASETS_DIR + QUESTION_PAIRS_FILE, encoding='utf-8') as 
csvfile:
reader = csv.DictReader(csvfile, delimiter=',')
for row in reader:
    question1.append(row['question1'])
    question2.append(row['question2'])

 print('Question pairs: %d' % len(question1))
 T1=len(question1)
 print(T1)
 **Build tokenized word index**
 questions = question1 + question2
 tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
 tokenizer.fit_on_texts(questions)
 question1_word_sequences = tokenizer.texts_to_sequences(question1)
 question2_word_sequences = tokenizer.texts_to_sequences(question2)
 word_index = tokenizer.word_index

 print(""Words in index: %d"" % len(word_index))

  Prepare word embedding matrix

 Prepare training data tensors
 q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)
 q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)

  print('Shape of question1 data tensor:', q1_data.shape)
  print('Shape of question2 data tensor:', q2_data.shape)




 **Define the model**
question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))
question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))


q1 = Embedding(nb_words + 1, 
             EMBEDDING_DIM, 
             weights=[word_embedding_matrix], 
             input_length=MAX_SEQUENCE_LENGTH, 
             trainable=False)(question1)
q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)
q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)

q2 = Embedding(nb_words + 1, 
             EMBEDDING_DIM, 
             weights=[word_embedding_matrix], 
             input_length=MAX_SEQUENCE_LENGTH, 
             trainable=False)(question2)
q2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)
q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)

merged = concatenate([q1,q2])
merged = Dense(200, activation='relu')(merged)
merged = Dropout(DROPOUT)(merged)
merged = BatchNormalization()(merged)
merged = Dense(200, activation='relu')(merged)
merged = Dropout(DROPOUT)(merged)
merged = BatchNormalization()(merged)
merged = Dense(200, activation='relu')(merged)
merged = Dropout(DROPOUT)(merged)
merged = BatchNormalization()(merged)
merged = Dense(200, activation='relu')(merged)
merged = Dropout(DROPOUT)(merged)
merged = BatchNormalization()(merged)

is_duplicate = Dense(1, activation='sigmoid')(merged)

model = Model(inputs=[question1,question2], outputs=is_duplicate)
 model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics= 
 ['accuracy'])
 model.load_weights(MODEL_WEIGHTS_FILE)

 temp= model.predict([q1_data, q2_data])

 df = pd.DataFrame(temp)
 df.to_csv(""hero.csv"",header=['is_duplicate'])
</code></pre>

<p><strong>it giving me output like this</strong></p>

<pre><code>test id is_duplicate
0   0.585984
1   0.13437697
2   0.7458109
3   0.6282846
4   0.44692168
</code></pre>

<p><strong>but i need boolean values</strong></p>

<pre><code> test id    is_duplicate
 0       1
 1       0
 2       1
 3       1
</code></pre>

<p>dont tell me to round up the values</p>

<p>I want the neural network itself gives me the boolean values</p>

<p>is it possible to train the network like that </p>

<p>if yes, please suggest the things i can include in code</p>

<p>thanks in advance</p>
","python, machine-learning, keras, classification, text-classification","<p>The network cannot output plain boolean values, you just have to threshold the output of the network as it has a sigmoid activation. Say you set a threshold of 0.5, any value over 0.5 will be class 1, and an output lower than 0.5 will be class zero.</p>

<p>Additionally the network output value gives you a ""confidence"" estimate of the network's uncertainty of the output.</p>
",1,-3,3258,2018-04-01 09:22:05,https://stackoverflow.com/questions/49596618/keras-neural-network-giving-float-ouputs-but-i-need-boolean-outputs
NLTK classifier for integer features?,"<p>I have integer-type features in my feature vector that NLTK’s <code>NaiveBayesClassifier</code> is treating as nominal values.</p>
<h2>Context</h2>
<p>I am trying to build a language classifier using n-grams. For instance, the bigram ‘th’ is more common in English than French.</p>
<p>For each sentence in my training set, I extract a feature as follows: <code>bigram(th): 5</code> where 5 (example) represents the number of times the bigram ‘th’ appeared in the sentence.</p>
<p>When I try building a classifier with features like this and I check the most informative features, I realize that the classifier does not realize that such features are linear. For example, it might consider <code>bigram(ea): 4</code> as French, <code>bigram(ea): 5</code> as English and <code>bigram(ea): 6</code> as French again. This is quite arbitrary and does not represent the logic that a bigram is either more common in English or in French. This is why I need the integers to be treated as such.</p>
<h2>More thoughts</h2>
<p>Of course, I could replace these features with features such as <code>has(th): True</code>. However, I believe this is a bad idea because both a French sentence with 1 instance of 'th' and an English sentence with 5 instances of 'th' will have the feature <code>has(th): True</code> which cannot differentiate them.</p>
<p>I also found <a href=""https://groups.google.com/forum/#!topic/nltk-users/PapQfN4t2nk"" rel=""nofollow noreferrer"">this relevant link</a> but it did not provide me with the answer.</p>
<h2>Feature Extractor</h2>
<p>My feature extractor looks like this:</p>
<pre><code>def get_ngrams(word, n):
    ngrams_list = []
    ngrams_list.append(list(ngrams(word, n, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')))
    ngrams_flat_tuples = [ngram for ngram_list in ngrams_list for ngram in ngram_list]
    format_string = ''
    for i in range(0, n):
        format_string += ('%s')
    ngrams_list_flat = [format_string % ngram_tuple for ngram_tuple in ngrams_flat_tuples]
    return ngrams_list_flat

# Feature extractor
def get_ngram_features(sentence_tokens):
    features = {}
    # Unigrams
    for word in sentence_tokens:
        ngrams = get_ngrams(word, 1)
        for ngram in ngrams:
            features[f'char({ngram})'] = features.get(f'char({ngram})', 0) + 1
    # Bigrams
    for word in sentence_tokens:
        ngrams = get_ngrams(word, 2)
        for ngram in ngrams:
            features[f'bigram({ngram})'] = features.get(f'bigram({ngram})', 0) + 1
    # Trigrams
    for word in sentence_tokens:
        ngrams = get_ngrams(word, 3)
        for ngram in ngrams:
            features[f'trigram({ngram})'] = features.get(f'trigram({ngram})', 0) + 1
    # Quadrigrams
    for word in sentence_tokens:
        ngrams = get_ngrams(word, 4)
        for ngram in ngrams:
            features[f'quadrigram({ngram})'] = features.get(f'quadrigram({ngram})', 0) + 1
    return features
</code></pre>
<h2>Feature Extraction Example</h2>
<pre><code>get_ngram_features(['test', 'sentence'])
</code></pre>
<p>Returns:</p>
<pre><code>{'char(c)': 1,
 'char(e)': 4,
 'char(n)': 2,
 'char(s)': 2,
 'char(t)': 3,
 'bigram(_s)': 1,
 'bigram(_t)': 1,
 'bigram(ce)': 1,
 'bigram(e_)': 1,
 'bigram(en)': 2,
 'bigram(es)': 1,
 'bigram(nc)': 1,
 'bigram(nt)': 1,
 'bigram(se)': 1,
 'bigram(st)': 1,
 'bigram(t_)': 1,
 'bigram(te)': 2,
 'quadrigram(_sen)': 1,
 'quadrigram(_tes)': 1,
 'quadrigram(ence)': 1,
 'quadrigram(ente)': 1,
 'quadrigram(est_)': 1,
 'quadrigram(nce_)': 1,
 'quadrigram(nten)': 1,
 'quadrigram(sent)': 1,
 'quadrigram(tenc)': 1,
 'quadrigram(test)': 1,
 'trigram(_se)': 1,
 'trigram(_te)': 1,
 'trigram(ce_)': 1,
 'trigram(enc)': 1,
 'trigram(ent)': 1,
 'trigram(est)': 1,
 'trigram(nce)': 1,
 'trigram(nte)': 1,
 'trigram(sen)': 1,
 'trigram(st_)': 1,
 'trigram(ten)': 1,
 'trigram(tes)': 1}
</code></pre>
","python, machine-learning, nltk, text-classification, n-gram","<h1>TL;DR</h1>

<p>It's easier to use other libraries for this purpose. It's easier to do something like this <a href=""https://www.kaggle.com/alvations/basic-nlp-with-nltk"" rel=""nofollow noreferrer"">https://www.kaggle.com/alvations/basic-nlp-with-nltk</a> with <code>sklearn</code> using a custom analyzer, e.g. <code>CountVectorizer(analyzer=preprocess_text)</code></p>

<p>For example:</p>

<pre><code>from io import StringIO
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from nltk import everygrams

def sent_process(sent):
    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) 
            if ' ' not in ng and '\n' not in ng and ng != ('_',)]

sent1 = ""The quick brown fox jumps over the lazy brown dog.""
sent2 = ""Mr brown jumps over the lazy fox.""
sent3 = 'Mr brown quickly jumps over the lazy dog.'
sent4 = 'The brown quickly jumps over the lazy fox.'

with StringIO('\n'.join([sent1, sent2])) as fin:
    # Override the analyzer totally with our preprocess text
    count_vect = CountVectorizer(analyzer=sent_process)
    count_vect.fit_transform(fin)
count_vect.vocabulary_ 


train_set = count_vect.fit_transform([sent1, sent2])

# To train the classifier
clf = MultinomialNB() 
clf.fit(train_set, ['pos', 'neg']) 

test_set = count_vect.transform([sent3, sent4])
clf.predict(test_set)
</code></pre>

<h1>Cut-away</h1>

<p>Firstly, there's really no need to explicitly label the <code>char(...)</code>, <code>unigram(...)</code>, <code>bigram(...)</code>, <code>trigram(...)</code> and <code>quadrigram(...)</code> part to the features. </p>

<p>The feature set are just dictionary keys and you can use the actual ngram tuple as the keys, e.g. </p>

<pre><code>from collections import Counter
from nltk import ngrams, word_tokenize

features = Counter(ngrams(word_tokenize('This is a something foo foo bar foo foo sentence'), 2))
</code></pre>

<p>[out]:</p>

<pre><code>&gt;&gt;&gt; features
Counter({('This', 'is'): 1,
         ('a', 'something'): 1,
         ('bar', 'foo'): 1,
         ('foo', 'bar'): 1,
         ('foo', 'foo'): 2,
         ('foo', 'sentence'): 1,
         ('is', 'a'): 1,
         ('something', 'foo'): 1})
</code></pre>

<p>As for ngrams of several orders, you can use <code>everygrams()</code>, e.g. </p>

<pre><code>from nltk import everygrams

sent = word_tokenize('This is a something foo foo bar foo foo sentence')
Counter(everygrams(sent, 1, 4))
</code></pre>

<p>[out]:</p>

<pre><code>Counter({('This',): 1,
         ('This', 'is'): 1,
         ('This', 'is', 'a'): 1,
         ('This', 'is', 'a', 'something'): 1,
         ('a',): 1,
         ('a', 'something'): 1,
         ('a', 'something', 'foo'): 1,
         ('a', 'something', 'foo', 'foo'): 1,
         ('bar',): 1,
         ('bar', 'foo'): 1,
         ('bar', 'foo', 'foo'): 1,
         ('bar', 'foo', 'foo', 'sentence'): 1,
         ('foo',): 4,
         ('foo', 'bar'): 1,
         ('foo', 'bar', 'foo'): 1,
         ('foo', 'bar', 'foo', 'foo'): 1,
         ('foo', 'foo'): 2,
         ('foo', 'foo', 'bar'): 1,
         ('foo', 'foo', 'bar', 'foo'): 1,
         ('foo', 'foo', 'sentence'): 1,
         ('foo', 'sentence'): 1,
         ('is',): 1,
         ('is', 'a'): 1,
         ('is', 'a', 'something'): 1,
         ('is', 'a', 'something', 'foo'): 1,
         ('sentence',): 1,
         ('something',): 1,
         ('something', 'foo'): 1,
         ('something', 'foo', 'foo'): 1,
         ('something', 'foo', 'foo', 'bar'): 1})
</code></pre>

<p>A clean way to extract the features you want:</p>

<pre><code>def sent_vectorizer(sent):
    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) 
            if ' ' not in ng and ng != ('_',)]
Counter(sent_vectorizer('This is a something foo foo bar foo foo sentence'))
</code></pre>

<p>[out]:</p>

<pre><code>Counter({'o': 9, 's': 4, 'e': 4, 'f': 4, '_f': 4, 'fo': 4, 'oo': 4, 'o_': 4, '_fo': 4, 'foo': 4, 'oo_': 4, '_foo': 4, 'foo_': 4, 'i': 3, 'n': 3, 'h': 2, 'a': 2, 't': 2, 'hi': 2, 'is': 2, 's_': 2, '_s': 2, 'en': 2, 'is_': 2, 'T': 1, 'm': 1, 'g': 1, 'b': 1, 'r': 1, 'c': 1, 'Th': 1, '_i': 1, '_a': 1, 'a_': 1, 'so': 1, 'om': 1, 'me': 1, 'et': 1, 'th': 1, 'in': 1, 'ng': 1, 'g_': 1, '_b': 1, 'ba': 1, 'ar': 1, 'r_': 1, 'se': 1, 'nt': 1, 'te': 1, 'nc': 1, 'ce': 1, 'Thi': 1, 'his': 1, '_is': 1, '_a_': 1, '_so': 1, 'som': 1, 'ome': 1, 'met': 1, 'eth': 1, 'thi': 1, 'hin': 1, 'ing': 1, 'ng_': 1, '_ba': 1, 'bar': 1, 'ar_': 1, '_se': 1, 'sen': 1, 'ent': 1, 'nte': 1, 'ten': 1, 'enc': 1, 'nce': 1, 'This': 1, 'his_': 1, '_is_': 1, '_som': 1, 'some': 1, 'omet': 1, 'meth': 1, 'ethi': 1, 'thin': 1, 'hing': 1, 'ing_': 1, '_bar': 1, 'bar_': 1, '_sen': 1, 'sent': 1, 'ente': 1, 'nten': 1, 'tenc': 1, 'ence': 1})
</code></pre>

<h1>In Long</h1>

<p>Unfortunately, there's no easy way to change the hardcoded manner of how the <code>NaiveBayesClassifier</code> in NLTK works.</p>

<p>If we look at <a href=""https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py#L185"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py#L185</a> , behind the scenes <strong>NLTK is already counting the number of occurrence in the features.</strong> </p>

<p>But note, it's counting the document frequency, not term frequency, i.e. <strong>in that case regardless of how many times an element appears in the document, it counts as one</strong>. There isn't a clean way without changing the NLTK code to add the value of each feature since it's hardcoded to do <code>+=1</code>, <a href=""https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py#L201"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py#L201</a> </p>
",1,1,397,2018-04-01 16:44:06,https://stackoverflow.com/questions/49600319/nltk-classifier-for-integer-features
Why Mallet text classification output the same value 1.0 for all test files?,"<p>I am learning Mallet text classification command lines. The output values for estimating differrent classes are all the same 1.0. I do not know where I am incorrect. Can you help?</p>

<p>mallet version: E:\Mallet\mallet-2.0.8RC3</p>

<pre><code>//there is a txt file about cat breed (catmaterial.txt) in cat dir.
//command 1
C:\Users\toshiba&gt;mallet import-dir --input E:\Mallet\testmaterial\cat --output E
:\Mallet\testmaterial\cat.mallet --remove-stopwords

//command 1 output
Labels =
   E:\Mallet\testmaterial\cat

//command 2, save classifier as catClass.classifier
C:\Users\toshiba&gt;mallet train-classifier --input E:\Mallet\testmaterial\cat.mall
et --trainer NaiveBayes --output-classifier E:\Mallet\testmaterial\catClass.clas
sifier

//command 2 output
Training portion = 1.0
Unlabeled training sub-portion = 0.0
Validation portion = 0.0
Testing portion = 0.0

-------------------- Trial 0  --------------------

Trial 0 Training NaiveBayesTrainer with 1 instances
Trial 0 Training NaiveBayesTrainer finished
No examples with predicted label !
No examples with true label !
No examples with predicted label !
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer training data accuracy = 1.0
Trial 0 Trainer NaiveBayesTrainer Test Data Confusion Matrix
No examples with predicted label !
Trial 0 Trainer NaiveBayesTrainer test data precision() = 1.0
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer test data recall() = 1.0
No examples with predicted label !
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer test data F1() = 1.0
Trial 0 Trainer NaiveBayesTrainer test data accuracy = NaN

NaiveBayesTrainer
Summary. train accuracy mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test accuracy mean = NaN stddev = NaN stderr = NaN
Summary. test precision() mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test recall() mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test f1() mean = 1.0 stddev = 0.0 stderr = 0.0

//command 3, estimate classes of the three files about cat, deer and dog. The cat file is the same as the one for cat.mallet
C:\Users\toshiba&gt;mallet classify-dir --input E:\Mallet\testmaterial\test_cat_dir
 --output - --classifier E:\Mallet\testmaterial\catClass.classifier


//command 3 output
file:/E:/Mallet/testmaterial/test_cat_dir/catmaterial.txt               1.0
file:/E:/Mallet/testmaterial/test_cat_dir/deertext.txt          1.0
file:/E:/Mallet/testmaterial/test_cat_dir/dogmaterial.txt               1.0

// why the three classes are all 1.0 ?

C:\Users\toshiba&gt;
</code></pre>

<p>Can you help? 
Thanks.</p>

<p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p>

<p><strong>Update:</strong> </p>

<p>Thank you for answer, but still output 1.0 for all files.</p>

<p>My idea was that I put some dog files in dog dir and treated these dog files as instances, trained model, then tested some files in test_dir to see the result.</p>

<p>I tried according to my understanding of your suggestion but still output all same 1.0.</p>

<p>Will you help me with my commandlines below?</p>

<p>In E:\Mallet\train_dir\dog,  there are  4 dog txt files(dog 2.txt, dog4.txt,dog5.txt, dogmaterial.txt).</p>

<p>In E:\Mallet\test_dir, there are 9 txt files (cat2.txt, catmaterial.txt, deermaterial.txt, dog3.txt, dog6.txt, dog 2.txt, dog4.txt, dog5.txt, dogmaterial.txt).</p>

<hr>

<pre><code>C:\Users\toshiba&gt;mallet import-dir --input E:\Mallet\train_dir\dog --output E:\M
allet\classifier_dir\3animal.mallet --remove-stopwords
Labels =
   E:\Mallet\train_dir\dog


C:\Users\toshiba&gt;mallet train-classifier --input E:\Mallet\classifier_dir\3anima
l.mallet --trainer NaiveBayes --output-classifier E:\Mallet\classifier_dir\3anim
alClass.classifier
Training portion = 1.0
Unlabeled training sub-portion = 0.0
Validation portion = 0.0
Testing portion = 0.0                          
-------------------- Trial 0  --------------------

Trial 0 Training NaiveBayesTrainer with 4 instances
Trial 0 Training NaiveBayesTrainer finished
No examples with predicted label !
No examples with true label !
No examples with predicted label !
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer training data accuracy = 1.0
Trial 0 Trainer NaiveBayesTrainer Test Data Confusion Matrix
No examples with predicted label !
Trial 0 Trainer NaiveBayesTrainer test data precision() = 1.0
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer test data recall() = 1.0
No examples with predicted label !
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer test data F1() = 1.0
Trial 0 Trainer NaiveBayesTrainer test data accuracy = NaN

NaiveBayesTrainer
Summary. train accuracy mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test accuracy mean = NaN stddev = NaN stderr = NaN
Summary. test precision() mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test recall() mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test f1() mean = 1.0 stddev = 0.0 stderr = 0.0


C:\Users\toshiba&gt;mallet classify-dir --input E:\Mallet\test_dir --output - --cla
ssifier E:\Mallet\classifier_dir\3animalClass.classifier

file:/E:/Mallet/test_dir/cat2.txt               1.0
file:/E:/Mallet/test_dir/catmaterial.txt                1.0
file:/E:/Mallet/test_dir/deertext.txt           1.0
file:/E:/Mallet/test_dir/dog%202.txt            1.0
file:/E:/Mallet/test_dir/dog3.txt               1.0
file:/E:/Mallet/test_dir/dog4.txt               1.0
file:/E:/Mallet/test_dir/dog5.txt               1.0
file:/E:/Mallet/test_dir/dog6.txt               1.0
file:/E:/Mallet/test_dir/dogmaterial.txt                1.0
C:\Users\toshiba&gt;
</code></pre>

<hr>

<p>Thank you.</p>
","machine-learning, nlp, classification, text-classification, mallet","<p>There are two input options. <code>input-dir</code> treats directories as classes and each file in each directory as an input instance. <code>input-file</code> reads the input file line by line and treats various fields within the line as label and instance data. You are using the files-in-directories input type, so you are creating a classifier with one class and one instance. I'm guessing you want the lines-in-file type.</p>
",1,0,168,2018-04-04 11:35:05,https://stackoverflow.com/questions/49649946/why-mallet-text-classification-output-the-same-value-1-0-for-all-test-files
Finding the top three relevant category and its corresponding probabilities,"<p>From the below script, I find the highest probability and its corresponding category in a multi class text classification problem. How do I find the highest top 3 predicted probability and its corresponding category in a best efficient way without using loops.</p>

<pre><code>probabilities = classifier.predict_proba(X_test)
max_probabilities = probabilities.max(axis=1)
order=np.argsort(probabilities, axis=1)
classification=(classifier.classes_[order[:, -1:]])
print(accuracy_score(classification,y_test))
</code></pre>

<p>Thanks in advance.
( I have around 50 categories, I want to extract the top 3 best relevant category among 50 categories for each of my narrations and display them in a dataframe)</p>
","python-3.x, pandas, machine-learning, scikit-learn, text-classification","<p>You've done most of the hard work here, just missing a bit of <code>numpy</code> foo to finish it off. Your line </p>

<pre><code>order = np.argsort(probabilities, axis=1)
</code></pre>

<p>Contains the indices of the sorted probabilities, so <code>[[lowest_prob_class_1, ..., highest_prob_class_1]...]</code> for each of your samples. Which you have used to give your classification with <code>order[:, -1:]</code>, i.e. the index of the highest probability class. So to get the top three classes we can just make a simple change </p>

<pre><code>top_3_classes = classifier.classes_[order[:, -3:]]
</code></pre>

<p>Then to get the corresponding probabilities we can use</p>

<pre><code>top_3_probabilities = probabilities[np.repeat(np.arange(order.shape[0]), 3),
                                    order[:, -3:].flatten()].reshape(order.shape[0], 3)
</code></pre>
",1,0,266,2018-04-06 07:10:21,https://stackoverflow.com/questions/49687304/finding-the-top-three-relevant-category-and-its-corresponding-probabilities
UnicodeDecodeError in Python Classification Arabic Datasets,"<p>I have Arabic datasets for classification using Python; two directories (negative and positive) in a Twitter directory.</p>

<p>I want to use Python classes to classify the data. When I run the attached code, this error occurs:</p>

<p>>
  File ""C:\Users\DEV2016\Anaconda2\lib\encodings\utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)</p>

<blockquote>
  <p>UnicodeDecodeError: 'utf8' codec can't decode byte 0xc7 in position 0: invalid continuation byte</p>
</blockquote>

<pre><code>import sklearn.datasets
import sklearn.metrics
import sklearn.cross_validation
import sklearn.svm
import sklearn.naive_bayes
import sklearn.neighbors

dir_path = ""E:\Twitter\Twitter""

# Loading files into memory
files = sklearn.datasets.load_files(dir_path)

# Calculating BOW
count_vector = sklearn.feature_extraction.text.CountVectorizer()
word_counts=count_vector.fit_transform(files.data)

# Calculating TFIDF
tf_transformer = sklearn.feature_extraction.text.TfidfTransformer(use_idf=True).fit(word_counts)
X = tf_transformer.transform(word_counts)

# Create classifier
# clf = sklearn.naive_bayes.MultinomialNB()
# clf = sklearn.svm.LinearSVC()
n_neighbors = 11
weights = 'distance'
clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors, weights=weights)

# Test the classifier
# Train-test split
test_size=0.4
X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X, files.target, test_size=test_size)

# Test classifier
clf.fit(X_train, y_train)
y_predicted = clf.predict(X_test)
print (sklearn.metrics.classification_report(y_test, y_predicted,
target_names=files.target_names))
print ('Confusion Matrix:')
print (sklearn.metrics.confusion_matrix(y_test, y_predicted))
</code></pre>

<p><strong>Traceback</strong></p>

<pre><code>File ""&lt;ipython-input-19-8ea269fd9c3d&gt;"", line 1, in &lt;module&gt;
runfile('C:/Users/DEV2016/.spyder/clf.py', wdir='C:/Users/DEV2016/.spyder')

File ""C:\Users\DEV2016\Anaconda2\lib\site-
packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
execfile(filename, namespace)

File ""C:\Users\DEV2016\Anaconda2\lib\site-
packages\spyder\utils\site\sitecustomize.py"", line 87, in execfile
exec(compile(scripttext, filename, 'exec'), glob, loc)

File ""C:/Users/DEV2016/.spyder/clf.py"", line 18, in &lt;module&gt;
word_counts=count_vector.fit_transform(files.data)

File ""C:\Users\DEV2016\Anaconda2\lib\site-
packages\sklearn\feature_extraction\text.py"", line 869, in fit_transform
self.fixed_vocabulary_)

File ""C:\Users\DEV2016\Anaconda2\lib\site-
packages\sklearn\feature_extraction\text.py"", line 792, in _count_vocab
for feature in analyze(doc):

File ""C:\Users\DEV2016\Anaconda2\lib\site-
packages\sklearn\feature_extraction\text.py"", line 266, in &lt;lambda&gt;
tokenize(preprocess(self.decode(doc))), stop_words)

File ""C:\Users\DEV2016\Anaconda2\lib\site-
packages\sklearn\feature_extraction\text.py"", line 116, in decode
doc = doc.decode(self.encoding, self.decode_error)

File ""C:\Users\DEV2016\Anaconda2\lib\encodings\utf_8.py"", line 16, in decode
return codecs.utf_8_decode(input, errors, True)

UnicodeDecodeError: 'utf8' codec can't decode byte 0xc7 in position 0:
invalid continuation byte
</code></pre>
","python-2.7, text-classification, nearest-neighbor, naivebayes, sklearn-pandas","<p>In the Twitter data you are trying to load, there are characters that are not recognized by utf-8. Try to load it with other encoding formats like</p>

<pre><code>files = sklearn.datasets.load_files(dir_path, encoding=""iso-8859-1"")
</code></pre>
",0,0,535,2018-04-08 21:57:17,https://stackoverflow.com/questions/49723152/unicodedecodeerror-in-python-classification-arabic-datasets
Renaming categories,"<pre><code>    text       category
----------------------------------------------- 
    nike    shoes from nike brought by ankit
    flour   grocery
    rice    grocery
    adidas  shoes from adidas are cool
</code></pre>

<p>The above is my data-set format. How exactly do I generalize the category while categorising.
Example I want the output as:-</p>

<pre><code>text       category
----------------------------------------------- 
    nike    shoes from brand
    flour   grocery
    rice    grocery
    adidas  shoes from brand
</code></pre>
","python, string, pandas, dataframe, text-classification","<p>One way is to use a custom function with <code>pd.DataFrame.apply</code>:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'text': ['nike', 'flour', 'rice', 'adidas'],
                   'category': ['shoes from nike bought by ankit', 'grocery', 'grocery',
                                'shoes from adidas are cool']})

def converter(row):
    if row['text'] in row['category']:
        return row['category'].split(' from ')[0] + ' from brand'
    else:
        return row['category']

df['category'] = df.apply(converter, axis=1)

#            category    text
# 0  shoes from brand    nike
# 1           grocery   flour
# 2           grocery    rice
# 3  shoes from brand  adidas
</code></pre>
",3,-1,223,2018-04-09 09:59:02,https://stackoverflow.com/questions/49730257/renaming-categories
Should I do loss masking while using variable sequence length in Dynamic_RNN,"<p>I am currently building a model for text classification using Dynamic RNN in tf. My inputs are of different lengths, so I padded the inputs to be of same (max) length. </p>

<p>I used the sequence_length argument in tf.nn.dynamic_rnn to feed in a list of each input length.</p>

<p>My question:</p>

<p>Should I do loss masking? and When to do loss masking?</p>
","python, tensorflow, neural-network, text-classification, recurrent-neural-network","<p>No, you need not do loss masking. I assume you want a fixed length vector output for each sequence. Since you are giving the sequence length as input, dynamic RNN automatically makes sure that the state is copied through for the sequence outside the sequence length.</p>

<p>Reference: <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a>. Look at the information about the sequence_length argument.</p>
",2,3,492,2018-04-15 00:28:02,https://stackoverflow.com/questions/49837426/should-i-do-loss-masking-while-using-variable-sequence-length-in-dynamic-rnn
How to view incorrectly recognized text,"<p>There is a neural network that classifies the sentiment of the reviews. The accuracy is not 100%, hence there are texts that are recognized by the network incorrectly. How can I see them? I tried my function, but it gives an error</p>

<pre><code>    data = pd.concat([positive_train_data,negative_train_data,positive_test_data,negative_test_data],ignore_index = True)
    data.reset_index(drop=True,inplace=True)
    x = data.Text
    y = data.Sentiment

    x_train, x_test, y_train1, y_test = train_test_split(x, y, test_size = 0.50, random_state = 2000)
    print( ""Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive"".format(len(x_train),
                                                                                 (len(x_train[y_train1 == 0]) / (len(x_train)*1.))*100,
                                                                                (len(x_train[y_train1 == 1]) / (len(x_train)*1.))*100))

    print (""Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive"".format(len(x_test),
                                                                                 (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,
                                                                                (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))

    tvec1 = TfidfVectorizer(max_features=10000,ngram_range=(1, 2),min_df=3,use_idf=1,smooth_idf=1,sublinear_tf=1,stop_words = 'english')
    tvec1.fit(x_train)
    x_train_tfidf = tvec1.transform(x_train)
    print(x_test.shape)
    x_test_tfidf = tvec1.transform(x_test).toarray()
model = Sequential()
model.add(Dense(100, activation='relu', input_dim=10000))
model.add(Dropout(0.25))
model.add(Dense(50,activation = 'relu'))
model.add(Dense(1, activation='sigmoid'))
optimiz = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss = 'binary_crossentropy',optimizer = optimiz ,metrics = ['accuracy'])
hist  = model.fit(x_train_tfidf,y_train1,validation_data = (x_test_tfidf,y_test ),epochs = 5,batch_size = 64)
</code></pre>

<p>And my function</p>

<pre><code>y_pred_vect = model.predict(x_test_tfidf)
# bolean mask
mask = (y_pred_vect != y_test).any(axis=1)
print(mask)
print(len(mask))
num_words=5000 # only use top 1000 words
INDEX_FROM=3   # word index offset
# этот шаг нужен чтобы получить `test_x` в изначальном виде (до токенизации):
(train_x, _), (test_x, _) = imdb.load_data(num_words=num_words, index_from=INDEX_FROM)
x_wrong = test_x[mask]

word_to_id = imdb.get_word_index()
word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}
word_to_id[""&lt;PAD&gt;""] = 0
word_to_id[""&lt;START&gt;""] = 1
word_to_id[""&lt;UNK&gt;""] = 2

id_to_word = {value:key for key,value in word_to_id.items()}
all_wrong_sents = [' '.join(id_to_word[id] for id in sent) for sent in x_wrong]
print(all_wrong_sents[:10])
</code></pre>

<p>Error on line - 
mask = (y_pred_vect != y_test).any(axis=1)</p>

<p>Data must be 1-dimensional</p>
","python, neural-network, keras, text-classification","<p>Try this...</p>

<pre><code>import numpy as np

mask = np.squeeze(y_pred_vect) != y_test
</code></pre>
",0,0,52,2018-04-16 21:14:49,https://stackoverflow.com/questions/49866343/how-to-view-incorrectly-recognized-text
How to add another text feature to current bag of words classification? In Scikit-learn,"<p>this is my input matrix <a href=""https://i.sstatic.net/Hdtfx.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>my sample Code: </p>

<pre><code>from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(data['Extract'], 
data['Expense Account code Description'], random_state = 0)

from sklearn.pipeline import Pipeline , FeatureUnion
text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))),
              ('tfidf', TfidfTransformer(use_idf = False)),
              ('clf', RandomForestClassifier(n_estimators =100, 
 max_features='log2',criterion = 'entropy')),
 ])
 text_clf = text_clf.fit(X_train, y_train)
</code></pre>

<p>here I am applying Bag of word model for 'Extract' column classifying 'Expense Account code Description' , Here i am getting an accuracy of around 92% , but if i want to include 'Vendor name' as the set of another input feature how can i do that.  Is there any way of doing it along with the bag of words ? , </p>
","python, machine-learning, scikit-learn, nlp, text-classification","<p>You can use FeatureUnion.
also you will need to create a new Transformer class with the necessary actions you need to take i.e. Include Vendor name , get dummies.</p>

<p>Feature Union will fit in your pipeline.</p>

<p>For reference.</p>

<pre><code>class get_Vendor(BaseEstimator,TransformerMixin):

    def transform(self, X,y):
        return 

lr_tfidf = Pipeline([('features',FeatureUnion([('other',get_vendor()),
        ('vect', tfidf)])),('clf', RandomForestClassifier())])
</code></pre>
",1,2,667,2018-05-03 22:39:35,https://stackoverflow.com/questions/50164737/how-to-add-another-text-feature-to-current-bag-of-words-classification-in-sciki
python sklearn pipiline fit: &quot;AttributeError: lower not found&quot;,"<p>I'm trying to classify sveveral text data into 3 categories using sklearn. But I'm getting </p>

<blockquote>
  <p>""AttributeError: lower not found""</p>
</blockquote>

<p>when running.</p>

<p><strong>Code:</strong></p>

<pre><code>train, test = train_test_split(df, random_state=42, test_size=0.3, shuffle=True)
X_train = train.contents
X_test = test.contents
Y_train = train.category
Y_test = test.category

clf_svc = Pipeline([('vect', CountVectorizer()),
                    ('tfidf', TfidfVectorizer(tokenizer=',', use_idf=True, stop_words=""english"")),
                    ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),
                    ])

clf_svc = clf_svc.fit(X_train, Y_train)
predicted_svc = clf_svc(X_test)
print(np.mean(predicted_svc == Y_test))
</code></pre>

<p>Dataframe (df) consists of 2 columns: contents (long text data) and categories (text 
data). contents are scraped texts thus contain tens or hundreds of words, and categories are single words such as ""A"", ""B"".</p>

<p>I've already checked past questions in stackoverflow but I could not solve this error occuring.<br>
I'd be very glad to know the solution, or problems in the code itself.<br>
Any advice and answers will be greatly appreciated.</p>

<p>Thanks in advance.</p>
","python, machine-learning, scikit-learn, svm, text-classification","<p>Either remove step <code>('vect', CountVectorizer())</code> or use <code>TfidfTransformer</code> instead of <code>TfidfVectorizer</code> as <code>TfidfVectorizer</code> expects array of strings as an input and  <code>CountVectorizer()</code> returns a matrix of occurances (i.e. numeric matrix).</p>

<p>Per default <code>TfidfVectorizer(..., lowercase=True)</code> will try to ""lowercase"" all strings, hence the <code>“AttributeError: lower not found”</code> error message.</p>

<p>Also parameter <code>tokenizer</code> expects either a callable (function) or <code>None</code>, so don't specify it.</p>
",10,1,7939,2018-05-05 18:29:33,https://stackoverflow.com/questions/50192763/python-sklearn-pipiline-fit-attributeerror-lower-not-found
How to import(restore) Neural network model built by tflearn from files,"<p>I am referring to <a href=""https://sourcedexter.com/tensorflow-text-classification-python/"" rel=""nofollow noreferrer"">this tutorial</a> on text classification and built a custom training set for a text classification.</p>

<p>I am saving the model with below code.</p>

<pre><code># Define model and setup tensorboard
model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')
# Start training (apply gradient descent algorithm)
model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)
model.save('model.tflearn')
</code></pre>

<p>This generates below files.</p>

<pre><code>model.tflearn.data-00000-of-00001
model.tflearn.index
model.tflearn.meta
tflearn_logs folder
</code></pre>

<p>I want to use the model built in different iteration for testing purpose.</p>

<p>I tried ,</p>

<pre><code>with tf.Session() as sess:
    saver = tf.train.import_meta_graph('model.tflearn.meta')
    saver.restore(sess, tf.train.latest_checkpoint('./'))
</code></pre>

<p><em>but I get;</em></p>

<blockquote>
  <p><strong>KeyError: ""The name 'adam' refers to an Operation not in the graph.""</strong> error</p>
</blockquote>

<p>I know <a href=""http://tflearn.org/getting_started/#weights-persistence"" rel=""nofollow noreferrer"">from documentation</a> that <code>tflearn.DNN(network).load('file_name')</code> loads a model , but we need to create and pass the network instance, to build a network we again go through same code from scratch which takes time since it will do training which I want to avoid.</p>

<p>Code for building network</p>

<pre><code>net = tflearn.input_data(shape=[None, len(train_x[0])])
net = tflearn.fully_connected(net, 8)
net = tflearn.fully_connected(net, 8)
net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')
net = tflearn.regression(net)
</code></pre>

<p><code>tflearn.input_data</code> has shape input as mandatory , so we would again need training data to be fed again.So it causes rebuilding model.
I checked the documentation , could not find what I need (2-3 lines of code which would import build neural network model to save retraining time.</p>

<p>Please let me know if you guys know solution for this.</p>

<p><a href=""https://stackoverflow.com/questions/45595646/in-tensorflow-how-to-freeze-saved-model/46637498?noredirect=1#comment87414509_46637498"">Similar question</a> but its not duplicate </p>

<ul>
<li>OP was facing issue while building neural net during building tree , while I am facing issue importing build model.  </li>
<li>Tutorial mentioned in the answer does not have tflearn NN model import </li>
</ul>
","python, tensorflow, neural-network, text-classification, tflearn","<p>I was able to restore the saved model with below code. </p>

<p><code>tflearn</code> can restore model from saved log and model files.</p>

<h2>Create dummy neural net of same size as saved model</h2>

<p>Note : You may need to keep track of previously saved model's weights (size of input training and corresponding classes)</p>

<pre><code>net = tflearn.input_data(shape=[None, train_x[0]])
net = tflearn.fully_connected(net, 8, restore=False)
net = tflearn.fully_connected(net, 8, restore=False)
net = tflearn.fully_connected(net, train_y[0], activation='softmax', restore=False)
dnn = tflearn.DNN(net, tensorboard_dir='tflearn_logs')
</code></pre>

<h2>Load the saved model on to DNN</h2>

<pre><code>model = dnn.load('./model.tflearn')
</code></pre>

Use the loaded model for predictions

<pre><code>test_data = ###converted data 
model.predict(test_data)
</code></pre>
",0,0,1380,2018-05-07 18:14:45,https://stackoverflow.com/questions/50220191/how-to-importrestore-neural-network-model-built-by-tflearn-from-files
Random forest classifier result from Predict_proba() does not match with predict()?,"<pre><code>from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

from sklearn.ensemble import RandomForestClassifier
pipeline = Pipeline([
('features', FeatureUnion([
    ('Comments',Pipeline([
        ('selector',ItemSelector(column = ""Comments"")),
        ('tfidf',TfidfVectorizer(use_idf=False,ngram_range=(1,2),max_df=0.95, min_df=0,sublinear_tf=True)),
    ])),
    ('Vendor', Pipeline([
        ('selector',ItemSelector(column = ""Vendor Name"")),
        ('tfidf',TfidfVectorizer(use_idf=False)),

    ]))
])),
('clf',RandomForestClassifier(n_estimators =200, max_features='log2',criterion = 'entropy',random_state = 45))
 #('clf',LogisticRegression())
 ])


X_train, X_test, y_train, y_test = train_test_split(X,
                                df['code Description'],
                                test_size = 0.3, 
                                train_size = 0.7,
                                random_state = 100)
model = pipeline.fit(X_train, y_train)
s = pipeline.score(X_test,y_test)
pred = model.predict(X_test)
predicted =model.predict_proba(X_test)
</code></pre>

<p>for some of classification my <code>predict</code> is matching with prediction score. but in some cases,</p>

<pre><code>proba_predict = [0.3,0.18,0.155]
</code></pre>

<p>but instead of classifying it as class A, it is classifying as Class B.</p>

<p>Predict class: B</p>

<p>Actual Class : A </p>

<p>Right side column is my labels and left side column is my input text data:</p>

<p><a href=""https://i.sstatic.net/slRNs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/slRNs.png"" alt=""enter image description here""></a></p>
","python, machine-learning, classification, random-forest, text-classification","<p>I think that you state the following situation: For a test vector <code>X_test</code> you find a predicted probability distribution y=[p1, p2, p3] from the <code>predict_proba()</code> method with p1>p2 and p1>p3 but the <code>predict()</code> method does not output class 0 for this vector.</p>

<p>If you inspect the <a href=""https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/forest.py#L518"" rel=""nofollow noreferrer"">source code</a> of the <code>predict</code> function of sklearn's <code>RandomForestClassifier</code>, you will see that the <code>predict_proba()</code> method of the RandomForest is called there:</p>

<pre><code>proba = self.predict_proba(X)
</code></pre>

<p>From these probabilities, the <code>argmax</code> is used to output the class.</p>

<p>Hence, the prediction step uses the <code>predict_proba</code> method for its output. For me it seems impossible that anything goes wrong there.</p>

<p>I would assume that you mixed up some class names in your routine and got confused there. But it is not possible to give a more detailed answer based on the information you provided.</p>
",1,0,7050,2018-05-08 19:44:30,https://stackoverflow.com/questions/50241216/random-forest-classifier-result-from-predict-proba-does-not-match-with-predict
How to classify multiple objects in an R data frame,"<p>This is just a small portion of the data frame I am working with:</p>

<pre><code>id       drug        start        stop          dose    unit    route   
2010003  Amlodipine  2009-02-04   2009-11-19    1.5     mg      Oral    
2010003  Amlodipine  2009-11-19   2010-01-11    1.5     mg      Oral      
2010004  Cefprozil   2004-03-12   2004-03-19    175     mg      Oral    
2010004  Clobazam    2002-12-30   2003-01-01    5       mg      Oral
</code></pre>

<p>I have a Stata <code>do</code> file, which shows what I am trying to do:</p>

<pre><code>replace class = ""ACE Inhibitor"" if strmatch(upper(drug), ""CAPTOPRIL*"")
replace class = ""ACE Inhibitor"" if strmatch(upper(drug), ""ENALAPRIL*"")
replace class = ""ACE Inhibitor"" if strmatch(upper(drug), ""ENALAPRILAT*"")
replace class = ""ACE Inhibitor"" if strmatch(upper(drug), ""FOSINOPRIL*"")
replace class = ""ACE Inhibitor"" if strmatch(upper(drug), ""LISINOPRIL*"")
replace class = ""ACE Inhibitor"" if strmatch(upper(drug), ""RAMIPRIL*"")
replace class = ""Acne Medication"" if strmatch(upper(drug), ""ADAPALENE*"")
replace class = ""Acne Medication"" if strmatch(upper(drug), ""ADAPALENE/BENZOYL PEROXIDE*"")
replace class = ""Acne Medication"" if strmatch(upper(drug), ""BENZOYL PEROXIDE*"")
replace class = ""Acne Medication"" if strmatch(upper(drug), ""BENZOYL PEROXIDE/CLINDAMYCIN*"")
replace class = ""Acne Medication"" if strmatch(upper(drug), ""ISOTRETINOIN*"")
replace class = ""Acne Medication"" if strmatch(upper(drug), ""ERYTHROMYCIN/TRETINOIN*"")
replace class = ""Acne Medication/Acute Promyelocytic Leukemia Medication"" if strmatch(upper(drug), ""TRETINOIN*"")
replace class = ""Alpha Agonist"" if strmatch(upper(drug), ""XYLOMETAZOLINE*"")
replace class = ""Alpha Blocker"" if strmatch(upper(drug), ""DOXAZOSIN*"")
replace class = ""Alpha Blocker"" if strmatch(upper(drug), ""PHENOXYBENZAMINE*"")
replace class = ""Alpha Blocker"" if strmatch(upper(drug), ""PHENTOLAMINE*"")
replace class = ""Alpha Blocker"" if strmatch(upper(drug), ""PRAZOSIN*"")
replace class = ""Alpha Blocker"" if strmatch(upper(drug), ""TAMSULOSIN*"")
replace class = ""Alpha Blocker"" if strmatch(upper(drug), ""TERAZOSIN*"")
replace class = ""Alpha/Beta Blocker"" if strmatch(upper(drug), ""CARVEDILOL*"")
replace class = ""Alpha/Beta Blocker"" if strmatch(upper(drug), ""LABETALOL*"")
replace class = ""Alpha-1 Agonist"" if strmatch(upper(drug), ""PHENYLEPHRINE*"")
replace class = ""Alpha-1 Agonist"" if strmatch(upper(drug), ""MIDODRINE*"")
replace class = ""Alpha-2 Agonist"" if strmatch(upper(drug), ""CLONIDINE*"")
replace class = ""Alpha-2 Agonist"" if strmatch(upper(drug), ""DEXMEDETOMIDINE*"")
replace class = ""Anaesthetic, general"" if strmatch(upper(drug), ""KETAMINE*"")
replace class = ""Anaesthetic, general"" if strmatch(upper(drug), ""THIOPENTAL*"")
replace class = ""Anaesthetic, local"" if strmatch(upper(drug), ""BENZOCAINE*"")
replace class = ""Anaesthetic, local"" if strmatch(upper(drug), ""BUPIVACAINE*"")
replace class = ""Anaesthetic, local"" if strmatch(upper(drug), ""BUPIVACAINE/FENTANYL*"")
replace class = ""Anaesthetic, local"" if strmatch(upper(drug), ""TETRACAINE*"")
replace class = ""Anaesthetic, local"" if strmatch(upper(drug), ""XYLOCAINE*"")
replace class = ""Anaesthetic, local/Antiarrythmic"" if strmatch(upper(drug), ""LIDOCAINE*"")
replace class = ""Anaesthetic, local/Antiseptic"" if strmatch(upper(drug), ""HEXYLRESORCINOL*"")
replace class = ""Anaesthetic, topical"" if strmatch(upper(drug), ""LIDOCAINE/PRILOCAINE*"")
replace class = ""Anaesthetic, topical"" if strmatch(upper(drug), ""PROPARACAINE*"")
replace class = ""Analgesic"" if strmatch(upper(drug), ""ACETAMINOPHEN*"")
replace class = ""Analgesic"" if strmatch(upper(drug), ""BELLADONNA &amp; OPIUM SUPPOSITORY*"")
</code></pre>

<p>I want to do the same classification in R but I do not know Stata. </p>

<p>Note that drugs can have more than one <code>class</code>. </p>

<p>Any advice and help would be greatly appreciated.</p>
","r, dataframe, stata, text-classification","<p>As a first step I would import all the drug data from your Stata script (assuming the data is not in a clean, usable format already):</p>

<pre><code>drug_class_data &lt;- read.table(""Desktop/stata_script"", header=FALSE, sep='""',stringsAsFactors = FALSE)  
drug_class_data &lt;-drug_class_data[,c(2,4)] 
colnames(drug_class_data) &lt;- c('Drug_class','Drug')
</code></pre>

<p>Remove the trailing * - used as a wildcard in the Stata script</p>

<pre><code>drug_class_data$Drug = gsub(""\\*"", """", drug_class_data$Drug)
</code></pre>

<p>This gives you a dataframe with 2 columns ('Drug_class' &amp; 'Drug') - the line extracts any data in quotes from each line of the Stata script (Highlighted in bold below):</p>

<blockquote>
  <p>replace class = ""<strong>ACE Inhibitor</strong>"" if strmatch(upper(drug), ""<strong>CAPTOPRIL*</strong>"")</p>
</blockquote>

<p>I would then save that as a file which you can then import as needed (I assume that this data isn't already available like this as you have hardcoded all these values in the Stata example).</p>

<pre><code>write.csv(drug_class_data, file = ""drug_class_data.csv"",row.names=FALSE)
</code></pre>

<p>From there it depends upon whether you want:</p>

<p>1) Multiple rows for each drug instance with a single text column where the drug class is explicitly specified. The number of rows per drug = number of drug classes it is a member of.  There are some advantages to this approach but it leads to a lot of duplicated data.</p>

<p>2) A single row for each drug and multiple boolean columns for each drug class - ""ACE Inhibitor"", ""Acne Medication"", etc -containing a binary TRUE or FALSE to indicate whether it is a member of that class.</p>

<p>Personly I would favour option 2 as a starting point for downstream analysis.  (As you mention drugs are likely to be categorised under multiple classes, also several drug classes appear hierarchical - 'Anaesthetic, local' could be a parent term for 'Anaesthetic, local/Antiarrythmic', 'Anaesthetic, local/Antiseptic' etc)</p>

<p>Extract all unique classes of drugs from your data frame into a list:</p>

<pre><code>drug_class_list &lt;- unique(drug_class_data[,1])
</code></pre>

<p>I would then use the ugly code below to create a new dataframe:</p>

<pre><code>create_flat_table &lt;- function(df_drugs, df_classes){   
# Extract list of drug classes present in df

class_list &lt;- unique(df_classes[,1])  
# Reiterate over this list creating a new column in the drug df and populating it with data   
drugs &lt;- as.list(drug_data['drug'])  
results &lt;- df_drugs   
for(class in class_list){   
class_drugs &lt;- df_classes[df_classes$Drug_class == class,]   
boolean_list &lt;- toupper(df_drugs[,2])%in%class_drugs[,2]
results &lt;- cbind(results, boolean_list )   }   
colnames(results) &lt;- c(colnames(df_drugs), class_list)   
return(results) }

combined_df &lt;- create_flat_table(drug_data, drug_class_data)
</code></pre>

<p>The resulting dataframe will look like:</p>

<p><a href=""https://i.sstatic.net/z4Oge.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/z4Oge.png"" alt=""Resulting Dataframe""></a></p>

<p>Note that in this example I have changed the data so that at least one drug in your toy dataset will match a class in your abbreviated list of drug classes.</p>
",0,0,594,2018-05-09 14:45:25,https://stackoverflow.com/questions/50256184/how-to-classify-multiple-objects-in-an-r-data-frame
R: LIME returns error on different feature numbers when it&#39;s not the case,"<p>I'm building a text classifier of Clinton &amp; Trump tweets (data can be found on <a href=""https://www.kaggle.com/benhamner/clinton-trump-tweets/data#"" rel=""nofollow noreferrer"">Kaggle</a> ).</p>

<p>I'm doing EDA and modelling using <code>quanteda</code> package: </p>

<pre><code>library(dplyr)
library(stringr)
library(quanteda)
library(lime)

#data prep
tweet_csv &lt;- read_csv(""tweets.csv"")
tweet_data &lt;- tweet_csv %&gt;% 
  select(author = handle,
     text,
     retweet_count,
     favorite_count,
     source_url,
     timestamp = time) %&gt;% 
mutate(date = as_date(str_sub(timestamp, 1, 10)),
     hour = hour(hms(str_sub(timestamp, 12, 19))),
     tweet_num = row_number()) %&gt;% 
select(-timestamp)

# creating corpus and dfm
tweet_corpus &lt;- corpus(tweet_data)

edited_dfm &lt;- dfm(tweet_corpus, remove_url = TRUE, remove_punct = TRUE,     remove = stopwords(""english""))

set.seed(32984)
trainIndex &lt;- sample.int(n = nrow(tweet_csv), size =     floor(.8*nrow(tweet_csv)), replace = F)

train_dfm &lt;- edited_dfm[as.vector(trainIndex), ]
train_raw &lt;- tweet_data[as.vector(trainIndex), ]
train_label &lt;- train_raw$author == ""realDonaldTrump""

test_dfm &lt;- edited_dfm[-as.vector(trainIndex), ]
test_raw &lt;- tweet_data[-as.vector(trainIndex), ]
test_label &lt;- test_raw$author == ""realDonaldTrump""

# making sure train and test sets have the same features
test_dfm &lt;- dfm_select(test_dfm, train_dfm)

# using quanteda's NB model
nb_model &lt;- quanteda::textmodel_nb(train_dfm, train_labels)
nb_preds &lt;- predict(nb_model, test_dfm) 


# defining textmodel_nb as classification model
class(nb_model)

model_type.textmodel_nb_fitted &lt;- function(x, ...) {
  return(""classification"")
}

# a wrapper-up function for data preprocessing

get_matrix &lt;- function(df){
  corpus &lt;- corpus(df)
  dfm &lt;- dfm(corpus, remove_url = TRUE, remove_punct = TRUE, remove = stopwords(""english""))
}
</code></pre>

<p>then I define the explainer - no problems here: </p>

<pre><code>explainer &lt;- lime(train_raw[1:5],
              model = nb_model,
              preprocess = get_matrix)
</code></pre>

<p>But when I run an explainer, even on exactly same dataset as in <code>explainer</code>, I get an error: </p>

<pre><code>explanation &lt;- lime::explain(train_raw[1:5], 
                              explainer, 
                              n_labels = 1,
                              n_features = 6,
                              cols = 2,
                              verbose = 0)
</code></pre>

<blockquote>
  <p>Error in predict.textmodel_nb_fitted(x, newdata = newdata, type = type,  : 
    feature set in newdata different from that in training set</p>
</blockquote>

<p>Does it have something to do with <code>quanteda</code> and dfms? I honestly don't see why this should happen. Any help will be great, thanks!</p>
","r, text-classification, quanteda, lime","<p>We can trace the error to <code>predict_model</code>, which calls <code>predict.textmodel_nb_fitted</code> (I used only the first 10 rows of <code>train_raw</code> to speed up computation):</p>

<pre><code>traceback()
# 7: stop(""feature set in newdata different from that in training set"")
# 6: predict.textmodel_nb_fitted(x, newdata = newdata, type = type, 
#        ...)
# 5: predict(x, newdata = newdata, type = type, ...)
# 4: predict_model.default(explainer$model, case_perm, type = o_type)
# 3: predict_model(explainer$model, case_perm, type = o_type)
# 2: explain.data.frame(train_raw[1:10, 1:5], explainer, n_labels = 1, 
#        n_features = 5, cols = 2, verbose = 0)
# 1: lime::explain(train_raw[1:10, 1:5], explainer, n_labels = 1, 
#        n_features = 5, cols = 2, verbose = 0)
</code></pre>

<p>The problem is that <code>predict.textmodel_nb_fitted</code> expects a dfm, not a data frame. For example, <code>predict(nb_model, test_raw[1:5])</code> gives you the same ""feature set in newdata different from that in training set"" error. However, <code>explain</code> takes a data frame as its <code>x</code> argument.</p>

<p>A solution is to write a custom <code>textmodel_nb_fitted</code> method for <code>predict_model</code> that does the necessary object conversions before calling <code>predict.textmodel_nb_fitted</code>:</p>

<pre><code>predict_model.textmodel_nb_fitted &lt;- function(x, newdata, type, ...) {
  X &lt;- corpus(newdata)
  X &lt;- dfm_select(dfm(X), x$data$x)   
  res &lt;- predict(x, newdata = X, ...)
  switch(
   type,
   raw = data.frame(Response = res$nb.predicted, stringsAsFactors = FALSE),
   prob = as.data.frame(res$posterior.prob, check.names = FALSE)
  )  
}
</code></pre>

<p>This gives us</p>

<pre><code>explanation &lt;- lime::explain(train_raw[1:10, 1:5], 
                              explainer,
                              n_labels = 1,
                              n_features = 5,
                              cols = 2,
                              verbose = 0)
explanation[1, 1:5]
#       model_type case label label_prob    model_r2
# 1 classification    1 FALSE  0.9999986 0.001693861
</code></pre>
",2,1,953,2018-05-10 13:20:34,https://stackoverflow.com/questions/50273919/r-lime-returns-error-on-different-feature-numbers-when-its-not-the-case
Visualize a SVM model having100 attributes in 2D plot python,"<p>I am using SVM for text classification (tf-idf score based classification).
Is it possible to plot SVM having more than 100 attributes and 10 labels. Is there any way to reduce the features and then plot the same multiclass SVM.</p>
","python, machine-learning, plot, svm, text-classification","<p>For input data of higher dimensionality, I think that there is not a direct way to render a SVM. You should apply a dimensionality reduction, in order to have something to plot in 2-d or 3-d. </p>
",1,1,193,2018-05-11 10:08:17,https://stackoverflow.com/questions/50289947/visualize-a-svm-model-having100-attributes-in-2d-plot-python
R: problems applying LIME to quanteda text model,"<p>it's a modified version of my <a href=""https://stackoverflow.com/q/50273919/6327771"">previous question</a>: I'm trying to run LIME on my <code>quanteda</code> text model that feeds off <a href=""https://www.kaggle.com/benhamner/clinton-trump-tweets/data#"" rel=""nofollow noreferrer"">Trump &amp; Clinton tweets data</a>. I run it following an example given by Thomas Pedersen in his <a href=""https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html"" rel=""nofollow noreferrer"">Understanding LIME</a> and useuful SO answer provided by <a href=""https://stackoverflow.com/a/50275665/6327771"">@Weihuang Wong</a>:</p>

<pre><code>library(dplyr)
library(stringr)
library(quanteda)
library(lime)

#data prep
tweet_csv &lt;- read_csv(""tweets.csv"")

# creating corpus and dfm for train and test sets

get_matrix &lt;- function(df){
  corpus &lt;- quanteda::corpus(df)
  dfm &lt;- quanteda::dfm(corpus, remove_url = TRUE, remove_punct = TRUE,     remove = stopwords(""english""))
}

set.seed(32984)
trainIndex &lt;- sample.int(n = nrow(tweet_csv), size =     floor(.8*nrow(tweet_csv)), replace = F)

train_dfm &lt;- get_matrix(tweet_csv$text[trainIndex])
train_raw &lt;- tweet_csv[, c(""text"", ""tweet_num"")][as.vector(trainIndex), ]
train_labels &lt;- tweet_csv$author[as.vector(trainIndex)] == ""realDonaldTrump""

test_dfm &lt;- get_matrix(tweet_csv$text[-trainIndex])
test_raw &lt;- tweet_csv[, c(""text"", ""tweet_num"")][-as.vector(trainIndex), ]
test_labels &lt;- tweet_csv$author[-as.vector(trainIndex)] == ""realDonaldTrump""

#### make sure that train &amp; test sets have exactly same features
test_dfm &lt;- dfm_select(test_dfm, train_dfm)

### Naive Bayes model using quanteda::textmodel_nb ####
nb_model &lt;- quanteda::textmodel_nb(train_dfm, train_labels)
nb_preds &lt;- predict(nb_model, test_dfm) #&gt; 0.5


# select only correct predictions
predictions_tbl &lt;- data.frame(predict_label = nb_preds$nb.predicted,
                          actual_label = test_labels,
                          tweet_name = rownames(nb_preds$posterior.prob)
) %&gt;%
  mutate(tweet_num = 
       as.integer(
         str_trim(
           str_replace_all(tweet_name, ""text"", """"))
     )) 


correct_pred &lt;- predictions_tbl %&gt;%
  filter(actual_label == predict_label) 

# pick a sample of tweets for explainer 
tweets_to_explain &lt;- test_raw %&gt;%
  filter(tweet_num %in% correct_pred$tweet_num) %&gt;% 
  head(4)



### set up correct model class and predict functions 
class(nb_model)

model_type.textmodel_nb_fitted &lt;- function(x, ...) {
  return(""classification"")
}


# have to modify the textmodel_nb_fitted so that 

predict_model.textmodel_nb_fitted &lt;- function(x, newdata, type, ...) {
  X &lt;- corpus(newdata)
  X &lt;- dfm_select(dfm(X), x$data$x)   
  res &lt;- predict(x, newdata = X, ...)
  switch(
    type,
    raw = data.frame(Response = res$nb.predicted, stringsAsFactors = FALSE),
    prob = as.data.frame(res$posterior.prob, check.names = FALSE)
  )  
}


### run the explainer - no problems here 
explainer &lt;- lime(tweets_to_explain$text, # lime returns error on different features in explainer and explanations, even if I use the same dataset in both. Raised an issue on Github and asked a question on SO
              model = nb_model,
              preprocess = get_matrix) 
</code></pre>

<p>But when I run the explainer...</p>

<pre><code>corr_explanation &lt;- lime::explain(tweets_to_explain$text, 
                              explainer, 
                              n_labels = 1,
                              n_features = 6,
                              cols = 2,
                              verbose = 0)
</code></pre>

<p>... I get the following error:</p>

<blockquote>
  <p>Error in UseMethod(""corpus"") : 
    no applicable method for 'corpus' applied to an object of class ""c('dfm', 'dgCMatrix', 'CsparseMatrix', 'dsparseMatrix', 'generalMatrix', 'dCsparseMatrix', 'dMatrix', 'sparseMatrix', 'compMatrix', 'Matrix', 'xMatrix', 'mMatrix', 'Mnumeric', 'replValueSp')""</p>
</blockquote>

<p>It goes back to applying <code>corpus()</code> to <code>newdata</code>:</p>

<pre><code>5.corpus(newdata) 
4.predict_model.textmodel_nb_fitted(x = explainer$model, newdata = permutations_tokenized, 
type = o_type) 
3.predict_model(x = explainer$model, newdata = permutations_tokenized, 
type = o_type) 
2.explain.character(tweets_to_explain$text, explainer, n_labels = 1, 
n_features = 6, cols = 2, verbose = 0) 
1.lime::explain(tweets_to_explain$text, explainer, n_labels = 1, 
n_features = 6, cols = 2, verbose = 0) 
</code></pre>

<p>But I don't understand why should this cause any issues as new data is a text vector?</p>

<p>Thanks for any hints</p>
","r, text, text-classification, quanteda, lime","<p><code>corpus</code> doesn't have to be run. Try redefining <code>predict_model.textmodel_nb_fitted</code> as follows, where the only modification is to add the <code>dfm_select</code> step:</p>

<pre><code>predict_model.textmodel_nb_fitted &lt;- function(x, newdata, type, ...) {
  X &lt;- dfm_select(dfm(newdata), x$data$x)   
  res &lt;- predict(x, newdata = X, ...)
  switch(
    type,
    raw = data.frame(Response = res$nb.predicted, stringsAsFactors = FALSE),
    prob = as.data.frame(res$posterior.prob, check.names = FALSE)
  )  
}
</code></pre>

<p>As your <code>traceback()</code> output shows, <code>corpus</code> throws an error. To debug, I inserted <code>print(str(newdata))</code> in the first line of the <code>predict_model.textmodel_nb_fitted</code> function. This shows that <code>newdata</code> is already a <code>dfm</code> object, so it can be passed directly into <code>predict.textmodel_nb_fitted</code> (after processing it with <code>dfm_select</code>).</p>

<hr>

<p>In more recent versions of <code>quanteda</code>, <code>textmodel_nb()</code> returns an object of classes <code>textmodel_nb</code>,<code>textmodel</code>, and <code>list</code>. This would first require a corresponding method for <code>model_type</code>:</p>

<pre><code>model_type.textmodel_nb &lt;- function(x, ...) {
  return(""classification"")
}
</code></pre>

<p>We then also have to write a <code>textmodel_nb</code> method for <code>predict_model</code>:</p>

<pre><code>predict_model.textmodel_nb &lt;- function(x, newdata, type, ...) {
  X &lt;- dfm_select(dfm(newdata), x$x)   
  res &lt;- predict(x, newdata = X, ...)
  switch(
    type,
    raw = data.frame(Response = res$nb.predicted, stringsAsFactors = FALSE),
    prob = as.data.frame(res$posterior.prob, check.names = FALSE)
  )  
}
</code></pre>

<p>Notice that the second argument to <code>dfm_select</code> is different from that in <code>predict_model.textmodel_nb_fitted</code> (from the original version of the answer). This is because the structure of the <code>x</code> object -- the output from <code>textmodel_nb()</code> -- has changed.</p>
",2,2,587,2018-05-11 10:57:20,https://stackoverflow.com/questions/50290782/r-problems-applying-lime-to-quanteda-text-model
How can I visualize border/decision function of two classes using scikit-learn,"<p>I am pretty new in machine learning, so I still don't understand how I can visualize the border between 2 classes in bag of words case.</p>

<p>I found the following exaplpe to plot data</p>

<p><a href=""https://stackoverflow.com/questions/28160335/plot-a-document-tfidf-2d-graph"">plot a document tfidf 2D graph</a></p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt

newsgroups_train = fetch_20newsgroups(subset='train', 
                                      categories=['alt.atheism', 'sci.space'])
pipeline = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
])        
X = pipeline.fit_transform(newsgroups_train.data).todense()

pca = PCA(n_components=2).fit(X)
data2D = pca.transform(X)
plt.scatter(data2D[:,0], data2D[:,1], c=newsgroups_train.target)
plt.show()
</code></pre>

<p><a href=""https://i.sstatic.net/FG6BU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FG6BU.jpg"" alt=""enter image description here""></a></p>

<p>In my project I use SVC estimator</p>

<pre><code>clf = SVC(random_state=241, kernel = 'linear')
clf.fit(X,newsgroups_train.target)
</code></pre>

<p>I have tried to use the example
<a href=""http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html</a>
but it didn't work in text clasification case</p>

<p>So how can I add the border of two classes to this plot?</p>

<p>Thank you!</p>
","python, machine-learning, scikit-learn, svm, text-classification","<p>The problem is that you need to select only 2 features in order to create the 2-dimensional decision surface plot. I will provide 2 examples. The first using <code>iris</code> data and the second using <code>your</code> data.</p>

<p>I have also written an article about this here: 
<a href=""https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=friends_link&amp;sk=80f72ab272550d76a0cc3730d7c8af35"" rel=""nofollow noreferrer"">https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=friends_link&amp;sk=80f72ab272550d76a0cc3730d7c8af35</a></p>

<p><strong>In both cases, I select only 2 features in order to create the plot.</strong></p>

<h2><strong>Example 1 using iris data:</strong></h2>

<pre><code>from sklearn.svm import SVC
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
y = iris.target

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

model = svm.SVC(kernel='linear')
clf = model.fit(X, y)

fig, ax = plt.subplots()
# title for the plots
title = ('Decision surface of linear SVC ')
# Set-up grid for plotting.
X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
ax.set_ylabel('y label here')
ax.set_xlabel('x label here')
ax.set_xticks(())
ax.set_yticks(())
ax.set_title(title)
ax.legend()
plt.show()
</code></pre>

<hr>

<p><strong>RESULTS</strong>
<a href=""https://i.sstatic.net/vCwr9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vCwr9.png"" alt=""See here""></a></p>

<h2><strong>Example 2 using your data:</strong></h2>

<pre><code>from sklearn.svm import SVC
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt

newsgroups_train = fetch_20newsgroups(subset='train', 
                                      categories=['alt.atheism', 'sci.space'])
pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer())])        
X = pipeline.fit_transform(newsgroups_train.data).todense()

# Select ONLY 2 features
X = np.array(X)
X = X[:, [0,1]]
y = newsgroups_train.target

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

model = svm.SVC(kernel='linear')
clf = model.fit(X, y)

fig, ax = plt.subplots()
# title for the plots
title = ('Decision surface of linear SVC ')
# Set-up grid for plotting.
X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
ax.set_ylabel('y label here')
ax.set_xlabel('x label here')
ax.set_xticks(())
ax.set_yticks(())
ax.set_title(title)
ax.legend()
plt.show()
</code></pre>

<p><strong>RESULTS</strong></p>

<p><a href=""https://i.sstatic.net/M8AOT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M8AOT.png"" alt=""Using your data""></a></p>

<h2><strong>Important note:</strong></h2>

<p>In the second case, the plot is not nice since we selected randomly only 2 features to create it. One way to make it nice is the following: You could use a <code>univariate ranking method</code> (e.g. ANOVA F-value test) and find the best <code>top-2</code> features from the <code>22464</code> that you initially have. Then using these <code>top-2</code> you could create a nice separating surface plot.</p>

<hr>
",2,5,3382,2018-05-12 10:25:49,https://stackoverflow.com/questions/50305223/how-can-i-visualize-border-decision-function-of-two-classes-using-scikit-learn
How can I process Persian texts using Rapid Miner?,"<p>I am working on a persian classification project. Persian texts is very similar to arabic texts. when I use Tokenize, it does not show any word in its wordlist page and in Example Set Page, The Image below will be shown:</p>

<p><a href=""https://i.sstatic.net/yNYXe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yNYXe.png"" alt=""""></a>
I need to classify persian text to some category, but I dont know How?.</p>

<p>I Follow some steps like this:</p>

<p>1- Read Excel(using Read Excel component) dataset with 2 column => col1:persian Text ,col2: Category</p>

<p>2- I use Set role component to labeling data</p>

<p>3- I use Process Documents from Data component containing :(Tokenize(with any mode not change anythings) and Filter Token(min:5,max:25) inside it)</p>

<p>4- Then I use Cross Validation Component to train with SVM or Basian and in test mode to get performance.</p>

<p>The program runs correctly and performance is not bad for e.g accuracy is 50% but I think my work is Wrong.</p>

<p>Any help would be appreciated.</p>
","classification, text-processing, text-classification, rapidminer","<p>first, make sure your text data have <strong>UTF-8 encoding</strong> 
and if u use filter tokens(by length) 5 is <strong>too much</strong> for minimum try <strong>2</strong> or at least 3 
also, I recommend using <strong>Filter Stopwords (Dictionary)</strong> operator and the dictionary should have Persian stopwords in each line 
hope it will help u</p>
",0,0,191,2018-05-16 11:13:31,https://stackoverflow.com/questions/50369400/how-can-i-process-persian-texts-using-rapid-miner
How to compare different models configurations,"<p>I am implementing a neural network model for text classification. I am trying different configurations on RNN and lstm neural network.</p>

<p>My question: How to compare these configuration, should I compare the models using the training set accuracy, validation accuracy or testing set accuracy?</p>
","neural-network, lstm, text-classification, recurrent-neural-network","<p>I will explain how I finally compared my different RNN models.</p>

<p>First of all, I used my CPU for model training. This will ensure that I get the same model parameters each run as GPU computations are known to be non-deterministic.</p>

<p>Secondly, I used the same tf seed for each run. To make sure that the random variables generated in each run is the same.</p>

<p>Finally, I used my validation accuracy to optimize my hyper-parameters. Each run I used a combination of different parameters until I choose the model with the highest validation accuracy to be my best model.</p>
",-1,-1,109,2018-05-24 19:19:56,https://stackoverflow.com/questions/50516703/how-to-compare-different-models-configurations
"Adding optimizations decrease the accuracy, precision, f1 of classifier algorithms","<p>I want to build an algorithm that classifies text: ham or spam; I have the train/test data for each category of text. (my train data has for each category <code>8000 sentences</code>, and for test each category contains <code>2000 sentences</code>)</p>

<p>X_train looks like this <code>['please, call me asap!', 'watch out the new sales!', 'hello jim can we talk?', 'only today you can buy this', 'don't miss our offer!']</code></p>

<p>y_train looks like this <code>[1 0 1 0 0]</code> where 1 = ham, 0 = spam</p>

<p>the same with X_test and y_test.</p>

<p>This is a snippet of my code:</p>

<pre><code># classifier can be LogisticRegression, MultinomialNB, RandomForest, DecisionTree
text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', classifier),
                    ])
model = text_clf.fit(X_train, y_train)
y_predict = model.predict(X_test)
</code></pre>

<p>And these are the parameters that I measure:</p>

<pre><code>print(accuracy_score(y_test, y_predict))
print(f1_score(y_test, y_predict, average=""weighted""))
print(recall_score(y_test, y_predict, pos_label=1, average=""binary""))
print(precision_score(y_test, y_predict, average=""weighted""))
</code></pre>

<p>If I don't use any optimization (<code>remove stop words, remove punctuation, stem words, lemmatize words</code>) I obtain results around 95% each parameter. If I use those optimizations, the accuracy, f1 score and precision decrease drastically to 50-60%. The recall function stays the same at 95%.</p>

<p>Why is this happening? Where am I mistaking? Did I calculate right those parameters? Or this is a normal behavior?</p>
","python, machine-learning, classification, text-classification","<p>I just figured out what is wrong: underfitting. I performed cross-validation</p>

<p><code>scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')</code></p>

<p>and now everything is fine, I obtain the results I was expecting.</p>
",0,1,185,2018-06-05 14:22:54,https://stackoverflow.com/questions/50702508/adding-optimizations-decrease-the-accuracy-precision-f1-of-classifier-algorith
Tensorflow model for text classification,"<p>I'm building an Android application with OCR and Tensorflow. It scans price tags in supermarkets and has to put the scanned data into different fields. I've done the OCR part, so the image -> text recognition works fine and Tensorflow is only required to work with text input.</p>

<p>I'm new to Tensorflow and machine learning in overall. Is it possible to do the following work using Tensorflow and if yes, could you share some ideas on how to do so?</p>

<p>The average input looks like this:</p>

<pre><code>CARLSBERG
EESTI
HELE OLU 5%
1.59 +0.10
500 ml pudel
3.18 /I
4740019113419
</code></pre>

<p>The goal is to sort this data as follows:</p>

<pre><code>Brand: CARLSBERG
Product name: HELE OLU 5%
Size: 500
Units: ml
</code></pre>

<p>The parameters that determine how a particular string will be classified are:</p>

<ul>
<li>Case </li>
<li>Line number</li>
<li>Supermarket (it's known by default)</li>
<li>Total number of lines</li>
<li>Letters/numbers ratio</li>
</ul>
","python, python-3.x, tensorflow, text-classification, tensorflow-lite","<p>I think the first step would be to get your hands on or to generate some labelled training data. You should look into at feature extraction; for example, if you notice that for a certain item, the second line is usually the price, you could represent that as a parameter. Or say if a number is followed by a unit like ml/l/oz, it's likely to be the volume. What you want to know is how confident you are that a specific line/string is say the price.</p>

<p>However, I think TensorFlow would be more suited for the OCR portion of the problem, which you have already solved. What you are asking is more towards text parsing, which could be better solved with an <a href=""https://www.nltk.org/book/ch07.html"" rel=""nofollow noreferrer"">NLP approach</a>.</p>
",1,3,1144,2018-06-20 13:23:23,https://stackoverflow.com/questions/50949318/tensorflow-model-for-text-classification
How to change the classification with &quot;yes&quot; or &quot;no&quot; to the score of &quot;yes&quot; or &quot;no&quot; by using tensorflow?,"<p>I'm novice to deep learning.I use tensorflow to construct my TextCNN model(two categories) referring this <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">tutorial</a>.<br>
This model can predict the categories of the text. But I want a score (continuous value in [0,1]) rather than the discrete value. For example, If the model give 0.77, the text is more likely one of the category; if it gives 1.0, the text is actually that category.<br>
This is the part of my code.  </p>

<pre><code>def cnn(self):
    # word embedding
    with tf.device('/cpu:0'):
        embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])
        embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)

    with tf.name_scope(""cnn""):
        # CNN layer
        conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv')
        # global max pooling layer
        gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')

    with tf.name_scope(""score""):
        # full connected layer
        fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')
        fc = tf.contrib.layers.dropout(fc, self.keep_prob)
        fc = tf.nn.relu(fc)

        # classification
        self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')
        self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别

    with tf.name_scope(""optimize""):
        # Loss function, cross entropy
        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)
        self.loss = tf.reduce_mean(cross_entropy)
        # optimizer
        self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)

    with tf.name_scope(""accuracy""):
        # accuracy
        correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)
        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
</code></pre>

<p>Thanks in advance.</p>
","python, tensorflow, conv-neural-network, text-classification","<p>Use <code>tf.nn.softmax(self.logits)</code> to get probabilistic scores. Also see this question: <a href=""https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits"">What is logits, softmax and softmax_cross_entropy_with_logits?</a></p>
",0,0,491,2018-07-15 05:06:55,https://stackoverflow.com/questions/51345440/how-to-change-the-classification-with-yes-or-no-to-the-score-of-yes-or-no
expected dense to have shape but got array with shape,"<p>I am getting the following error while calling the model.predict function when running a text classification model in keras. I searched the everywhere but it isn't working for me.</p>

<pre><code>ValueError: Error when checking input: expected dense_1_input to have shape (100,) but got array with shape (1,)
</code></pre>

<p>My data has 5 classes and has a total of 15 examples only. Below is the dataset</p>

<pre><code>             query        tags
0               hi       intro
1      how are you       wellb
2            hello       intro
3        what's up       wellb
4       how's life       wellb
5              bye          gb
6    see you later          gb
7         good bye          gb
8           thanks   gratitude
9        thank you   gratitude
10  that's helpful   gratitude
11      I am great  revertfine
12            fine  revertfine
13       I am fine  revertfine
14            good  revertfine
</code></pre>

<p>This is the code of my model</p>

<pre><code>from keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelBinarizer
from keras.models import Sequential
import pandas as pd
from keras.layers import Dense, Activation

data = pd.read_csv('text_class.csv')
train_text = data['query']
train_labels = data['tags']

tokenize = Tokenizer(num_words=100)
tokenize.fit_on_texts(train_text)

x_data = tokenize.texts_to_matrix(train_text)

encoder = LabelBinarizer()
encoder.fit(train_labels)
y_data = encoder.transform(train_labels)

model = Sequential()
model.add(Dense(512, input_shape=(100,)))
model.add(Activation('relu'))
model.add(Dense(5))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
model.fit(x_data, y_data, batch_size=8, epochs=10)

predictions = model.predict(x_data[0])
tag_labels = encoder.classes_
predicted_tags = tag_labels[np.argmax(predictions)]
print (predicted_tags)
</code></pre>

<p>I am not able to figure out where the problem lies and how to fix it.</p>
","python, keras, shapes, text-classification","<p><code>x_data</code> is 2-dimensional array with shape <code>(15, 100)</code></p>

<pre><code>  print(x_data.shape) 
</code></pre>

<p>but <code>x_data[0]</code> is 1-dimensional array with shape <code>(100, )</code></p>

<pre><code>  print(x_data[0].shape) 
</code></pre>

<p>and it makes problem.</p>

<p>Use slicing <code>x_data[0:1]</code> to get it as 2-dimensional array with shape <code>(1, 100)</code> </p>

<pre><code> print(x_data[0:1].shape) 
</code></pre>

<p>and it will work</p>

<pre><code> predictions = model.predict(x_data[0:1])
</code></pre>
",15,12,31602,2018-07-19 11:46:13,https://stackoverflow.com/questions/51421885/expected-dense-to-have-shape-but-got-array-with-shape
Cannot freeze Tensorflow models into frozen(.pb) file,"<p>I am referring  (<a href=""https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc"" rel=""nofollow noreferrer"">here</a>) to freeze models into .pb file. My model is CNN for text classification I am using (<a href=""https://github.com/dennybritz/cnn-text-classification-tf"" rel=""nofollow noreferrer"">Github</a>) link to train CNN for text classification and exporting in form of models. I have trained models to 4 epoch and My checkpoints folders look as follows: </p>

<p><a href=""https://i.sstatic.net/qk6QU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qk6QU.png"" alt=""enter image description here""></a></p>

<p>I want to freeze this model into (.pb file). For that I am using following script:</p>

<pre><code>import os, argparse

import tensorflow as tf

# The original freeze_graph function
# from tensorflow.python.tools.freeze_graph import freeze_graph 

dir = os.path.dirname(os.path.realpath(__file__))

def freeze_graph(model_dir, output_node_names):
    """"""Extract the sub graph defined by the output nodes and convert 
    all its variables into constant 
    Args:
        model_dir: the root folder containing the checkpoint state file
        output_node_names: a string, containing all the output node's names, 
                            comma separated
    """"""
    if not tf.gfile.Exists(model_dir):
        raise AssertionError(
            ""Export directory doesn't exists. Please specify an export ""
            ""directory: %s"" % model_dir)

    if not output_node_names:
        print(""You need to supply the name of a node to --output_node_names."")
        return -1

    # We retrieve our checkpoint fullpath
    checkpoint = tf.train.get_checkpoint_state(model_dir)
    input_checkpoint = checkpoint.model_checkpoint_path

    # We precise the file fullname of our freezed graph
    absolute_model_dir = ""/"".join(input_checkpoint.split('/')[:-1])
    output_graph = absolute_model_dir + ""/frozen_model.pb""

    # We clear devices to allow TensorFlow to control on which device it will load operations
    clear_devices = True

    # We start a session using a temporary fresh Graph
    with tf.Session(graph=tf.Graph()) as sess:
        # We import the meta graph in the current default Graph
        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)

        # We restore the weights
        saver.restore(sess, input_checkpoint)

        # We use a built-in TF helper to export variables to constants
        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess, # The session is used to retrieve the weights
            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes 
            output_node_names.split("","") # The output node names are used to select the usefull nodes
        ) 

        # Finally we serialize and dump the output graph to the filesystem
        with tf.gfile.GFile(output_graph, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())
        print(""%d ops in the final graph."" % len(output_graph_def.node))

    return output_graph_def

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(""--model_dir"", type=str, default="""", help=""Model folder to export"")
    parser.add_argument(""--output_node_names"", type=str, default="""", help=""The name of the output nodes, comma separated."")
    args = parser.parse_args()

    freeze_graph(args.model_dir, args.output_node_names)
</code></pre>

<p>I am using following argument parser to run the above code </p>

<pre><code>python3 freeze_graph.py --model_dir /Users/path_to_checkpoints/ --output_node_names softmax
</code></pre>

<p>It is giving error</p>

<pre><code>    assert d in name_to_node_map, ""%s is not in graph"" % d
AssertionError: softmax is not in graph
</code></pre>

<p>My model is CNN for text classification. What should I write in output_node_names ? to produce a successful .pb file in the output</p>
","python, python-3.x, tensorflow, text-classification, tensorflow-serving","<p>Use the below script to print the tensors... the last tensor would be the output tensor.
Original author: <a href=""https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc"" rel=""nofollow noreferrer"">https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc</a></p>

<pre><code>import argparse
import tensorflow as tf


def print_tensors(pb_file):
    print('Model File: {}\n'.format(pb_file))
    # read pb into graph_def
    with tf.gfile.GFile(pb_file, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # import graph_def
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(graph_def)

    # print operations
    for op in graph.get_operations():
        print(op.name + '\t' + str(op.values()))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(""--pb_file"", type=str, required=True, help=""Pb file"")
    args = parser.parse_args()
    print_tensors(args.pb_file)
</code></pre>
",1,4,2288,2018-07-27 01:23:44,https://stackoverflow.com/questions/51549549/cannot-freeze-tensorflow-models-into-frozen-pb-file
Getting same output using Naive Bayes Classifier Python for Text Classification,"<p>I am trying to do text classification in Python using Naive Bayes classifier, and it works well when there are two labels ""negative"" and ""Positive"" as the results. I have sample data of about 300 sentences, and I want to label them based on Moore Bygrave model, so essentially I want to map these 300 sentences to around 10 labels. So when I train the model and try to predict the answer is always just one label ""Commitment"". I am confused, can anybody please guide and give me some references as to how I handle complex text classification problems using Python. </p>

<p>The code has been shown below:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob


train = [
('Less chance of success','Achievement'),
('It comes with a lot of paperwork that can take up time and energy','Experience'),
('High competition in the market','Risk taking'),
('It can be lonely and scary to be completely responsible','Risk taking'),
('If business slows down, your personal income can be at risk','Risk taking'),
('it requires more work and longer hours than being an employee','Experience'),
('your college degrees won’t help you to succeed in business','Experience'),
('Not willing to take risks','Risk taking'),
('It does not guarantee percent success','Achievement'),
('Fear of failure','Risk taking'),
('Chances of low profit','Economy'),
('Hurdles in reaching potential customer','Achievement'),
('Profit maximization doesn’t guarantee pursuit of one’s interests','Personal Values'),
('Unwanted feedback from family','Family'),
('Not caring about customers in the long run is contrary to my ethical standards','Personal Values'),
('Not getting appropriate recognition soon','Achievement'),
('Lack of agility in personality','Commitment'),
('Difficult to save money efficiently','Economy'),
('Requires good money management skills which one doesn’t develop easily','Experience'),
('Hassle of legal work','Lawyers'),
('Fear of being sued','Lawyers'),
('Hate to delegate tasks','Manager'),
('Tendency to make same mistakes','Locus of Control'),
('Hate taking risks','Risk taking'),
('Not persistent enough to keep going despite failures initially','Commitment'),
('Sounds like a boring idea','Personal Values'),
('Too much hard work','Commitment'),
('Time management is difficult','Manager'),
('Lack of resources','Resources'),
('Raising capital is a difficult task','Resources'),
('Afraid of law suits','Lawyers'),
('No clear goal in mind','Locus of Controly'),
('Lack of accountability upon which is bad for self-assessment','Competitors'),
('Marketing is expensive','Economy'),
('Target customers are hard to reach','Customers'),
('Return is uncertain','Risk Taking'),
('Fear of rejection','Achievment'),
('Struggling in the market','Strategy'),
('Strategy formulation is a strenuous task','Negative'),
('Failing a business is ugly','Risk taking'),
('Fear of ending behind bars','Risk taking'),
('Feels secluded from the world','Personal Vlues'),
('Managing everything alone is a headache','Manager'),
('Supervision is a difficult job','Manager'),
('No more chit chatting with co employees','Team'),
('No flexible timings','Locus of Control'),
('Lack of motivation','Commitment'),
('Pricing is a headache','Economy'),
('High tax rates','Economy'),
('No pension funds','Economy'),
('Miss gratuity benefits','Economy'),
('No Provident fund is too disadvantageous','Economy'),
('I feel bored','Personal Values'),
('Too many ups and downs','Experience'),
('So stressful','Personal Values'),
('Uncertain income','Rist taking'),
('Way too much hard work and investment initially','Risk taking'),
('Might make one lose sight of his/her passions','Vision'),
('No time for family','Family'),
('Goodbye to hobbies','Resources'),
('Lack of structure','Structure'),
('Can’t find meaning in it','Vision'),
('Competition is terrific','Competitors'),
('Wouldn’t prefer to invest life long savings in something risky/','Risk taking'),
('Deal with unending collections every day','Resources'),
('Partners can back out','Team'),
('Promises get broken','Vision'),
('The sole purpose of businesses-profit maximization- doesn’t care about people','Personal Values'),
('Hours can be a killer','Commitment'),
('It is unethical','Personal Values'),
('Regulations are a headache','Government Policy'),
('Government is troublesome','Government Policy'),
('I require a peaceful life','Personal Values'),
('Unending paperwork','Resources'),
('Not creative enough','Creativity'),
('Responsibility is just too much','Leader'),
('Doesn’t make me happy','Job dissatisfaction'),
('Dislike power','Leader'),
('Less days off','Commitment'),
('Its shallow work','Personal Values'),
('Customers ultimately leave','Customers'),
('It fails oftentimes since innovation is not an easy task','Creativity'),
('Can’t deal with difficult clients','Customers'),
('Taking a giant financial risk','Risk taking'),
('Fear of ruining resume in case of failure','Risk taking'),
('Depressing work','Personal Values'),
('Uncertain environment','Culture'),
('Cruel competitors','Competitors'),
('Lack of trust on people','Team'),
('Fewer vacations','Commitment'),
('Legal actions can make one stay awake at night','Lawyers'),
('Can’t call in sick','Commitment'),
('Desire a steady income','Resouces'),
('Can’t follow ‘Customer is always right’','Customers'),
('Fear of failure remains there always','Achievement'),
('There is no sick pay','Commitment'),
('Think about it ','Vision'),
('Family tensions may arise in case of family business','Family'),
('Unneeded advices are a headache','Advisors'),
('Last to get paid','Economy'),
('risk of loss','Risk taking'),
('almost whole day work initially people usually give','Commitment'),
('motivating employees and getting new once can be hectic','Team'),
('lazy personality','Commitment'),
('if business doesnt go well there is loss of money and time','Achievement'),
('you need to learn first about business related tools like marketing and all','Negative'),
('you should have links which can help you out or else its v v difficult','Networks'),
('it takes time to establish business if you dont have time or smth then its useless','Commitment'),
('coming up with new and creative idea can be hectic and if a idea is not good business will be in loss','Creativity'),
('people can be difficult to deal with','Team'),
('Strenuous work','Commitment'),
('High risk','Risk taking'),
('Will need a large sum of money to invest','Economy'),
('If the business incurs loss I alone will have to make compensations','Economy'),
('Irregular salary because I ll be making payments to my employees first','Economy'),
('Involves responsibility','Leader'),
('I dont have any prior knowledge or experience about running a business','Experience'),
('Will have to face immense competition during initial years','Competitors'),
('Makes one less disciplined','Locus of Control'),
('Lack of structure in work','Structure'),
('Doesn’t suit my personality','Personal Values'),
('No fixed work timings','Commitment'),
('staying late night at office','Commitment'),
('Must prioritize it','Manager'),
('I can be lazy sometimes which will impact my business performance','Commitment'),
('In Pakistan business individuals are taxed higher than salaried individuals','Government Policy'),
('In Pakistan businessmen often find themselves in toght spots where they often mo option but to deal with corrupt individuals in order to continue their business','Teams'),
('You may not have the precise leadership skills that are required','Experience'),
('Startups often tend to ask for more time and attention than do jobs','Commitment'),
('Finances may be hard to come by','Resources'),
('You may not be able to give time to your family and friends','Family'),
('You may not have the relevant organization skills needed for business execution','Experience'),
('It may sound like a wonderful idea to put together your own team, but finding the right people may be difficult','Team'),
('Boring work involved','Personal Values'),
('The employees who suit you may not want to work for you due to the lower pay, fringe benefits, or due to your business having a lack of a brand name','Economy'),
('The stress is great, and you may not find all the stress and hard work worth it if you don’t generate enough profits or satisfaction from your organization','Job Dissatisfaction'),
('Sure/ Stable/ Risk less Income','Oppurtunity Recognition '),
('Higher tax bracket','Economy'),
('Promise to receive pension after retirement','Government Policy'),
('No ownership or responsibility of ownership ie no need to worry about performance of company','Entreprenuer'),
('Want a tension free life','Personal Values'),
('Free Medical Treatment offered by most companies','Resources'),
('It will stress me out','Commitment'),
('You take all the risks','Risk taking'),
('When there is a loss you have to bare all of it','Risk taking'),
('You need to be the most responsible person on the ground','Leader'),
('Your decisions are vital and can affect the whole business, so wrong decisions at times can affect alot','Leader'),
('You also need to think about all your subordinates so mostly your decisions are influencedd by others','Leader'),
('You need to pay for all the expenses and all the taxes','Economy'),
('You are answerable to the government for any misconduct or illegal activity of your business','Government Policy'),
('Never prefer taking risks','Risk taking'),
('Other opportunities','Oppurtunities'),
('Always need a certain lead to follow','Role Models'),
('Think i can capitalise on my capablities in services sector','Experience'),
('Can work from home -','Opportunities'),
('Need a hard separation between my private life and working life','Personal Values'),
('Think there are high tax rates in Pakistan','Government Policy'),
('Belong to family which is least interested in doing business','Family'),
('Dont have initial amount to invest','Resources'),
('Am never best suited for running my own business','Entreprenuer'),
('Think true competence is in the job market','Competitors'),
('Think a business startup requires 100 percent dedication','Commitment'),
('I Cant dedicate my all the time and effort to a single cause','Commitment'),
('salary is not fixed','Economy'),
('Chances of making losses','Risk taking'),
('Too much burden','Commitment'),
('Rivalry/competition','Competitors'),
('Fear of not being able to manage well','Manager'),
('Extortion money threats','Government Policy'),
('Want to give more time to family','Family'),
('Business requires startup capital','Resources'),
('Complications of documents and stuff','Resources'),
('I have other better options','Personal Values '),
('vast responsibility','Leader'),
('All the stake is yours, one wrong decision and you are gone','Leader'),
('Availability at all times','Commitment'),
('Fewer vacations','Personal Values'),
('I want fixed salary','Economy'),
('Hate working all the time','Commitment'),
('It is exciting, each day is filled with new opportunities','Oppurtunites'),
('I can schedule my work hours around other commitments','Commitment'),
('It allows one to set his/her own earnings','Economy'),
('You choose the work you like to do and that makes the most of your strengths and skills','Experience'),
('It gives a great amount of freedom','Entreprenuer'),
('There is room to implement one’s own ideas','Creativity'),
('high job satisfaction','Achievement'),
('You can pursue your passion','Vision'),
('You can Watch your organization grow from start to finish','Achievement'),
('You decide who to hire and bring into your company','Leader'),
('It gives the opportunity to network with other entrepreneurs and professionals','Networks'),
('Through business, one can help people by preparing products or giving services to improve their life','Products'),
('Sell how you want to sell','Vision'),
('It helps discover new perspective and approaches','Opportunities'),
('connection with clients','Networks'),
('development of skills','Experience'),
('Develop new skill set','Experience'),
('feeling of self-satisfaction','Achievement'),
('Control over destiny','Vision'),
('Easy work','Commitment'),
('Area for innovation','Creativiy'),
('People at work become family like','Culture'),
('No dress codes','Culture'),
('Opportunity to change lives','Oppurtunities'),
('Control over workspace','Structure'),
('Shot of adrenaline after reaching a goal','Achievement'),
('Becoming a role model for others through success in business ventures','Role Models'),
('No age barrier','Culture'),
('Freedom to travel','Resources'),
('Less boredom','Personal Values'),
('Good utility of intelligence','Creativity'),
('Going cubicle free','Leader'),
('Pride in calling oneself a business owner','Entreprenuer'),
('No feeling of worthlessness','Achievement'),
('It helps create something from nothing','Opportunity Recognition'),
('It makes things happen!','Opportunity Recognition'),
('Adjust schedules and spend more time with friends and family','Family'),
('Output aligns with input','Commitment'),
('Involving family in work is a lovely idea','Family'),
('Give back to society','Opportunity recognition'),
('It helps become healthy both mentally and physically','Achievement'),
('Gives Free time which can be invested in hobbies','Leader'),
('It doesn’t make you frantically check the time','Entreprenuer'),
('It fulfills thrill for challenges','Risk taking'),
('There is no reporting to a boss','Leader'),
('No repetition in work','Personal Values'),
('Business provides room to utilize your creative skills','Creativity'),
('Meeting brilliant minds','Role Models'),
('Chance to create a legacy','Opportunities'),
('Life requires flexibility','Vision'),
('It is fascinating','Opportunities'),
('It helps make people happy','Teams'),
('Unlimited room for growth','Structure'),
('Earn doing what you love','Personal Values'),
('Its great to feel appreciated','Opportunity Recognition'),
('Build own security','Leader'),
('Learn constantly','Education'),
('Create your own job','Entreprenuer'),
('No stress of being fired','Leader'),
('Bad days aren’t as bad as with jobs','Job Dissatisfaction'),
('No requirement of degree or qualifications','Education'),
('No boundaries','Opportunities'),
('Satisfy curiosity','Opportunity Recognition'),
('An end to boring meetings','Team'),
('Media acknowledgement/fame','Vision'),
('Feeling of becoming a provider','Vision'),
('The opportunity to create a corporate culture','Culture'),
('Experience personal growth','Leader'),
('Become expert in problem solving','Entreprenuer'),
('No more boring work feels','Personal Values'),
('Endless experiences','Opportunities'),
('Dream big, achieve big','Economy'),
('Compete with yourself','Competitors'),
('Bad luck is not a thing here','Commitment'),
('Gives a happy feeling','Achievement'),
('It is adventurous','Risk taking'),
('Respect in society','Family'),
('Name in market','Networks'),
('Develops interpersonal skills','Experience'),
('Recognition feels great','Networks'),
('Satisfies passion for learning','Education'),
('Appreciation for unconventional ideas','Creativity'),
('Can provide ways to change the world','Economy'),
('Unlimited earning possibilities','Opportunties'),
('Family pressure','Family'),
('Leave an impact for generations','Family'),
('Locate your office wherever you want','Structure'),
('No more bossy heads','Leader'),
('Thrill for risks!','Risk taking'),
('No other choice','Job Loss'),
('Admire this work','Personal Values'),
('It is cool','Personal Values'),
('Peer pressure','Networks'),
('Drive to do something different','Creativity'),
('Defining the work objectives','Structure'),
('Opportunity to implement creativity','Creativity'),
('Greater autonomy','Leader'),
('Getting interviewed sounds exciting','Opportunities'),
('Change the market','Vision'),
('Sense of accomplishment','Achievement'),
('Stress-free life after some years','Commitment'),
('The idea of freedom is appealing','Leader'),
('More spare time','Family'),
('Story to tell people','Networks'),
('Tax benefits through expenses','Economy'),
('Pride in creating something','Creativity')]

cl = NaiveBayesClassifier(train)


# Classify some text
print(cl.classify('Need no capital to implement ideas'))  #""Resources""
print(cl.classify(""Keep experiencing new things.""))   # ""Creativity""
print(cl.classify(""Money is good.""))   # ""Resources""
print(cl.classify(""Less strenuous""))   # ""Personal Values""
</code></pre>

<h2>OUTPUT</h2>

<pre><code>The output of the above code is coming out to be this:

Commitment
Commitment
Commitment
Commitment
Commitment
</code></pre>

<p>Where as the label for any of these is not Commitment, please advice as to how I should fix my code, so that proper text classification can be achieved, please help out. Thanks.</p>
","python-3.x, machine-learning, spyder, text-classification, naivebayes","<p>The problem is not in your code but in your data. You have a lot of classes (48) and only 285 training examples. Out of these, Commitment has 28 examples. That's why the model is assigning high scores to that class.</p>

<p>Add more examples, also try to achieve a data balance (roughly equal number of examples per label). You can also try removing stop words.</p>
",2,0,633,2018-07-31 05:18:03,https://stackoverflow.com/questions/51605889/getting-same-output-using-naive-bayes-classifier-python-for-text-classification
convolutional neural network (CNN) for text classification,"<p>I am using CNN for text classification, in my model after flatten layer I used output layer directly without using dense layer. is that correct or must use dense layer? </p>

<p>just as example:</p>

<pre><code>model.add(Conv1D(filters=100, kernel_size=2,,padding='same' ,activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(3, activation='softmax'))
</code></pre>
","text, keras, deep-learning, conv-neural-network, text-classification","<p>Generally, you do not have to use a Dense layer. As the outputs of your convolution do contain some spatial information, though, it might make sense to use one.</p>

<p>I am assuming that you already understood how a convolution works. In that case, imagine what the activations at a certain position in the sentence mean, with regards to the end result in classification.</p>

<p>There are even End-to-End-CNNs, as for example GoogleNet, which (as the name indicates), also does not have a fully connected layer.</p>
",1,-1,414,2018-08-02 05:25:50,https://stackoverflow.com/questions/51646074/convolutional-neural-network-cnn-for-text-classification
which class is positive and how to determine real alarm in preparing data for ML,"<p>I am preparing some data for Machine Learning.
This question would be very easy, but I am little bit confusing.</p>

<p>Let's suppose there are system that have about 100 alarm every 1 hour and only 1 or 2 alarm are real alarm of them. Fake alarm will be ignored by human. I've collected some datas that looks like feature and gave label 0 or 1 to fake or real alarm.</p>

<p>In this case, is real alarm 0 or 1? By this, TP, TN, FP, FN, chance level would be changed. What we have interest in is real alarm and we don't want to miss it even though all alarms are checked by human manually.</p>

<p>Almost alarm is fake, so the chance level would be over 95%. Then the major class  and positive class would be 1 and fake alarm? But our interest is not fake alarm.
How should I set the label in this case?</p>
","machine-learning, classification, text-classification, confusion-matrix","<p>Both ways are equally ok to the performance of the model since the loss function is not affected by the way you label 1 or 0. Personally, I think 0 should be used for faked ones and 1 for real ones.</p>
",1,0,33,2018-08-04 00:03:14,https://stackoverflow.com/questions/51681353/which-class-is-positive-and-how-to-determine-real-alarm-in-preparing-data-for-ml
Machine Learning/NLP text classification: training a model from corpus of text files - scikit learn,"<p>I am very new to machine learning and I was wondering if somebody could take me through this code and why it is not working. It is my own variation of the scikit-learn tutorial found at: <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a> which is basically what I am trying to do. I need to train a model with a labelled training set so that when I use my test set, it can predict the label of the test set. Also it would be really useful if somebody could show me how to save and load the model. Thank you very much. This is what I have so far:</p>

<pre><code>import codecs
import os

import numpy as np
import pandas as pd

from Text_Pre_Processing import Pre_Processing

filenames = os.listdir(
    ""...scikit-machine-learning/training_set"")
files = []
array_data = []
array_label = []
for file in filenames:
    with codecs.open(""...scikit-machine-learning/training_set/"" + file, ""r"",
                     encoding='utf-8', errors='ignore') as file_data:
        open_file = file_data.read()
        open_file = Pre_Processing.lower_case(open_file)
        open_file = Pre_Processing.remove_punctuation(open_file)
        open_file = Pre_Processing.clean_text(open_file)
        files.append(open_file)
# ----------------------------------------------------
# PUTTING LABELS INTO LIST
for file in files:
    if 'inheritance' in file:
        array_data.append(file)
        array_label.append('Inheritance (object-oriented programming)')
    elif 'pagerank' in file:
        array_data.append(file)
        array_label.append('PageRank')
    elif 'vector space model' in file:
        array_data.append(file)
        array_label.append('Vector Space Model')
    elif 'bayes' in file:
        array_data.append(file)
        array_label.append('Bayes' + ""'"" + ' Theorem')
    else:
        array_data.append(file)
        array_label.append('Dynamic programming')
#----------------------------------------------------------

csv_array = []
for i in range(0, len(array_data)):
    csv_array.append([array_data[i], array_label[i]])

# format of array [[string, label], [string, label], [string, label]]
import csv

with open('data.csv', 'w') as target:
    writer = csv.writer(target)
    writer.writerows(zip(test_array))

data = pd.read_csv('data.csv')
numpy_array = data.as_matrix()

X = numpy_array[:, 0]
Y = numpy_array[:, 1]

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.text import TfidfTransformer

from sklearn.naive_bayes import MultinomialNB

from sklearn.pipeline import Pipeline

text_clf = Pipeline(['vect', CountVectorizer(stop_words='english'), 'tfidf', TfidfTransformer(),
                     'clf', MultinomialNB()])

text_clf = text_clf.fit(X_train, Y_train)

predicted = text_clf.predict(X_test)
np.mean(predicted == Y_test)
</code></pre>

<p>I saw online people using csv files to input the data so I tried that too, i may not need it so i apologise if that is incorrect. </p>

<p>error being shown: </p>

<pre><code>C:.../scikit-machine-learning/train.py:63: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  numpy_array = data.as_matrix()
Traceback (most recent call last):
  File ""C:/...scikit-machine-learning/train.py"", line 66, in &lt;module&gt;
    Y = numpy_array[:,1]
IndexError: index 1 is out of bounds for axis 1 with size 1
</code></pre>

<p>Thank you very much for your help, please let me know if you need further explanation.</p>

<p>sample of two entries in csv:</p>

<pre><code>""['dynamic programming is an algorithmic technique used to solve certain optimization problems where the object is to find the best solution from a number of possibilities it uses a so called bottomup approach meaning that the problem is solved as a set of subproblems which in turn are made up of subsubproblemssubproblems are then selected and used to solve the overall problem these subproblems are only solved once and the solutions are saved so that they will not need to be recalculated again whilst calculated individually they may also overlap when any subproblem is met again it can be found and reused to solve another problem since it searches all possibilities it is also very accurate this method is far more efficient than recalculating and therefore considerably reduces computation it is widely used in computer science and can be applied for example to compress data in high density bar codes dynamic programming is most effective and therefore most often used on objects that are ordered from left to right and whose order cannot be rearranged this means it works well on character chains for example ', 'Dynamic programming']""

""['inheritance is one of the basic concepts of object oriented programming its objective is to add more detail to preexisting classes whilst still allowing the methods and variables of these classes to be reused the easiest way to look at inheritance is as an is a kind of relationship for example a guitar is a kind of string instrument electric acoustic and steel stringed are all types of guitar the further down an inheritance tree you get the more specific the classes become an example here would be books books generally fall into two categories fiction and nonfiction each of these can then be subdivided into more groups fiction for example can be split into fantasy horror romance and many more nonfiction splits the same way into other topics such as history geography cooking etc history of course can be subdivided into time periods like the romans the elizabethans the world wars and so on', 'Inheritance (object-oriented programming)']""
</code></pre>
","machine-learning, scikit-learn, nlp, text-classification","<p>You need to remove the characters [' and '] from the csv since read_csv is treating them as a string (one column) rather than a two column dataframe.
There is also a typo error on the line text_clf = Pipeline so I fixed it too. Goodluck!</p>

<pre><code>data = pd.read_csv('data.csv', header=None)
numpy_array = data.as_matrix()

strarr = numpy_array[:, 0]
X=[strarr[i].split("","")[0].replace(""["",'').replace(""'"",'') for i in range(len(strarr))]
Y=[strarr[i].split("","")[1].replace(""]"",'').replace(""'"",'') for i in range(len(strarr))]

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.text import TfidfTransformer

from sklearn.naive_bayes import MultinomialNB

from sklearn.pipeline import Pipeline

text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])

text_clf = text_clf.fit(X_train, Y_train)

predicted = text_clf.predict(X_test)
np.mean(predicted == Y_test)
</code></pre>
",2,1,315,2018-08-04 13:28:57,https://stackoverflow.com/questions/51686078/machine-learning-nlp-text-classification-training-a-model-from-corpus-of-text-f
Predict a text with bag of word approach,"<p>I am trying text classification using the bag of word model. Everything works fine till I use the test set for testing and evaluation of accuracy but how we can check the class of a single statement.</p>

<p>I have a data frame with 2 classes labels and body. </p>

<pre><code>cout_vect = CountVectorizer()
final_count = cout_vect.fit_transform(df['body'].values.astype('U'))

from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
X_train, X_test, y_train, y_test = train_test_split(final_count, df['label'], test_size = .3, random_state=25)

model = Sequential()
model.add(Dense(264, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
y_train = np_utils.to_categorical(y_train, num_classes=3)
y_test = np_utils.to_categorical(y_test, num_classes=3)

model.fit(X_train, y_train, epochs=50, batch_size=32)
model.evaluate(x=X_test, y=y_test, batch_size=None, verbose=1, sample_weight=None)
</code></pre>

<p>Now I want to predict this statement using my model. How to do this 
I tried converting my statement to vector using the count vectorizer but according to the bag of word approach, it is just an 8 dimension vector. </p>

<pre><code>x = ""Your account balance has been deducted for 4300""
model.predict(x, batch_size=None, verbose=0, steps=None)
</code></pre>
","python, machine-learning, scikit-learn, keras, text-classification","<p>You need to do this:</p>

<pre><code># First transform the sentence to bag-of-words according to the already learnt vocabulary
x = cout_vect.transform([x])

# Then send the feature vector to the predict
print(model.predict(x, batch_size=None, verbose=0, steps=None))
</code></pre>

<p>You have not shown how you ""<em>I tried converting my statement to vector using the count vectorizer but according to the bag of word approach, it is just an 8 dimension vector.</em>"", but I'm guessing you did this:</p>

<pre><code>cout_vect.fit_transform([x])
</code></pre>

<p>If you call <code>fit()</code> (or <code>fit_transform()</code>), the vectorizer will forget all the previous training and only remember the current vocab, hence you only got a feature vector of size 8, whereas your previous vector was of higher size.</p>
",3,2,1284,2018-08-07 07:01:51,https://stackoverflow.com/questions/51720720/predict-a-text-with-bag-of-word-approach
NLTK NaiveBayesClassifier classifier issues,"<p>I am experimenting with NaiveBayesClassifier and have following training data:</p>

<pre><code>positive_vocab = [ 'awesome' ]
negative_vocab = [ 'bad']
neutral_vocab = [ 'so-so' ]
...
classifier = NaiveBayesClassifier.train(train_set) 
</code></pre>

<p>I then classify following sentence: <strong>bad Awesome movie, I liked it</strong></p>

<p>Here is what I get for each word:</p>

<p>bad:neg
awesome:pos
movie,:pos
i:pos
liked:pos
it:pos</p>

<p>How/why decision is made to classify words not in the training set (such as <strong>I Liked It, Movie</strong>) as positive?</p>

<p>thanks</p>
","python, nltk, text-classification, naivebayes","<p>Training a sentiment model means that your model learns how words affect the sentiment. <strong>Thus it's not about specifying which words are positive and which are negative — it's about how to train your model to understand that from a text by itself</strong>.</p>

<p>The simplest implementation is called ""bag of words"" (which is usually used with TF-IDF normalization). Bag of words works this way: you split your text by words and count occurrences of each word within the given text block (or review). In this way rows correspond to different reviews, and columns correspond to the number of occurrences of the given word within the given review. This table becomes your <code>X</code> and the target sentiment to predict becomes your <code>Y</code> (say 0 for negative and 1 for positive) .</p>

<p>Then you train your classifier:
</p>

<pre><code>from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

reviews, Y = your_load_function()

vectorizer = TfidfVectorizer()  # or CountVectorizer()
X = vectorizer.fit_transform(reviews)  # convert text to words counts

model = MultinomialNB()
model.fit(X, Y)
</code></pre>

<p>After the model is trained you can make predictions:</p>



<pre><code>new_reviews = your_load_function2()
new_X = vectorizer.transform(new_reviews)
predicted_Y = model.predict(new_X)
</code></pre>

<p>Further reading:<br>
<a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Bag-of-words_model</a><br>
<a href=""https://en.wikipedia.org/wiki/Tf-idf"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Tf-idf</a><br>
<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html</a><br>
<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>
",1,0,327,2018-08-09 21:11:17,https://stackoverflow.com/questions/51775834/nltk-naivebayesclassifier-classifier-issues
Text classification with Naive Bayes,"<p>I am leaning NLP and noticed that TextBlob classification based in Naive Bayes (textblob is Build on top of NLTK) <a href=""https://textblob.readthedocs.io/en/dev/classifiers.html"" rel=""nofollow noreferrer"">https://textblob.readthedocs.io/en/dev/classifiers.html</a> works fine when training data is list of sentences and does not work at all when training data are individual words (where each word and assigned classification).</p>

<p>Why?</p>
","python, nlp, nltk, text-classification, textblob","<p>Because you don't have single words in the training data.</p>

<p>Usually the training and evaluation/testing data are supposed to be selected with identical distribution. Biases or skews are usually problematic. In very few cases you can train the model to do one thing and use it to do something else.</p>

<p>In your case, the model likely spreads the weights over the words in the sentence. So when you pick a single word, you only get a small portion of the weight represented.</p>

<p>To get it to work you should add single word examples to your training data.</p>
",1,0,190,2018-08-15 12:52:36,https://stackoverflow.com/questions/51859091/text-classification-with-naive-bayes
Keras LSTM predict two features from one input in Text classification?,"<p>I have X as text, with two different labels(columns) to train.</p>

<pre><code>--input.csv--
content, category, rate
text test, 1, 3
new test, 2, 2
</code></pre>

<p>Here my input X will be content. I have converted it to sequence matrix. I need both category and rate to be trained along with content. I couldn't figure out how to pass this inside the layers.</p>

<pre><code>def RNN():
    num_categories = 2
    num_rates = 3
    inputs = Input(name='inputs',shape=[max_len])

    layer = Embedding(max_words,150,input_length=max_len)(inputs)
    layer = LSTM(100)(layer)

    shared_layer = Dense(256, activation='relu', name='FC1')(layer)
    shared_layer = Dropout(0.5)(shared_layer)

    cat_out = Dense(num_categories, activation='softmax', name='cat_out')(shared_layer)
    rate_out = Dense(num_rates, activation='softmax', name='rate_out')(shared_layer)

    model = Model(inputs=inputs,outputs=[cat_out, rate_out])
    return model

model = RNN()
model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  
model.fit(sequences_matrix,[Y_train, Z_train])
</code></pre>

<p>Y_train contains only category. I want to add rate to the training. Does any one know?
I want two results. One should be about category and another is Rate. 
Currently its returning only the label. Not with the rate. I don't know the way to add a layer for the Rate column.</p>
","python, keras, text-classification","<p>You can achieve this with the functional API, just let the network have 2 outputs from the shared feature layer:</p>

<pre><code>shared_layer = Dense(256, activation='relu', name='FC1')(layer)
shared_layer = Dropout(0.5)(shared_layer)
cat_out = Dense(num_categories, activation='softmax', name='cat_out')(shared_layer)
rate_out = Dense(num_rates, activation='softmax', name='rate_out')(shared_layer)

model = Model(inputs=inputs,outputs=[cat_out, rate_out])
return model
</code></pre>

<p>You will now train with two targets, <code>y_train_cat</code> and <code>y_train_rate</code> and give them as a list to <code>model.fit(X_train, [y_train_cat, y_train_rate])</code> and the model will make two distinct predictions.</p>

<p>Have a look at the functional API <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">documentation</a> on how to handle multi-input / multi-output models.</p>
",1,0,725,2018-08-18 09:28:40,https://stackoverflow.com/questions/51907189/keras-lstm-predict-two-features-from-one-input-in-text-classification
NameError: name &#39;fit_classifier&#39; is not defined,"<p>I'm trying to make a text classifier</p>

<pre><code>import pandas as pd
import pandas
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.multiclass import OneVsOneClassifier
from sklearn.svm import SVC
from sklearn import cross_validation
from sklearn.metrics import confusion_matrix

dataset = pd.read_csv('data.csv', encoding = 'utf-8')
data = dataset['text']
labels = dataset['label']

X_train, X_test, y_train, y_test = train_test_split (data, labels, test_size = 0.2, random_state = 0)

count_vector = CountVectorizer()
tfidf = TfidfTransformer()

classifier = OneVsOneClassifier(SVC(kernel = 'linear', random_state = 84))

train_counts = count_vector.fit_transform(X_train)
train_tfidf = tfidf.fit_transform(train_counts)
classifier.fit(train_tfidf, y_train)

test_counts = count_vector.transform(X_test)
test_tfidf = tfidf.transform(test_counts)
classifier.predict(test_tfidf)

fit_classifier(X_train, y_train)
predicted = predict(X_test)

print(""confusion matrix"")
print(confusion_matrix(X_test, predicted, labels = labels))

print(""cross validation"")
test_counts = count_vector.fit_transform(data)
test_tfidf = tfidf.fit_transform(test_counts)

scores = cross_validation.cross_val_score(classifier, test_tfidf, labels, cv = 10)
print(scores)
print(""Accuracy: {} +/- {}"".format(scores.mean(), scores.std() * 2))
</code></pre>

<p>But I have the following error and I can not understand. </p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File ""classificacao.py"", line 37, in 
      fit_classifier(X_train, y_train)</p>
  
  <p>NameError: name 'fit_classifier' is not defined</p>
</blockquote>

<p>But <strong>fit</strong> is not always defined by default?</p>
","python, python-3.x, scikit-learn, classification, text-classification","<p>you are calling a non existing function:</p>

<blockquote>
  <p>fit_classifier(X_train, y_train)</p>
</blockquote>

<p>to fit your classifier you would use </p>

<blockquote>
  <p>classifier.fit(X_train, y_train) </p>
</blockquote>

<p>instead. 
You'll get the same error when trying to predict your test data.
You need to change </p>

<blockquote>
  <p>predicted = predict(X_test)</p>
</blockquote>

<p>to </p>

<blockquote>
  <p>predicted = classifier.predict(X_test)</p>
</blockquote>

<p>Your Confusionmatrix should get your labels, not your test data:</p>

<blockquote>
  <p>print(confusion_matrix(<strong>y_test</strong>, predicted, labels = labels))</p>
</blockquote>
",6,-2,7439,2018-08-20 06:26:47,https://stackoverflow.com/questions/51925123/nameerror-name-fit-classifier-is-not-defined
ValueError: cannot use sparse input in &#39;SVC&#39; trained on dense data,"<p>I'm trying to run my classifier but I get this error</p>

<pre><code>import pandas
import numpy as np
import pandas as pd
from sklearn import svm
from sklearn.svm import SVC
from sklearn import cross_validation
from sklearn.metrics import confusion_matrix
from sklearn.multiclass import OneVsOneClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

dataset = pd.read_csv('all_topics_limpo.csv', encoding = 'utf-8')
data = pandas.get_dummies(dataset['verbatim_corrige'])
labels = dataset['label']

X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state = 0)

count_vector = CountVectorizer()
tfidf = TfidfTransformer()

classifier = OneVsOneClassifier(SVC(kernel = 'linear', random_state = 100))
#classifier = LogisticRegression()

train_counts = count_vector.fit_transform(X_train)
train_tfidf = tfidf.fit_transform(train_counts)
classifier.fit(X_train, y_train)

test_counts = count_vector.transform(X_test)
test_tfidf = tfidf.transform(test_counts)
predicted = classifier.predict(test_tfidf)

predicted = classifier.predict(X_test)

print(""confusion matrix"")
print(confusion_matrix(y_test, predicted, labels = labels))

print(""F-score"")
print(f1_score(y_test, predicted))
print(precision_score(y_test, predicted))
print(recall_score(y_test, predicted)) 

print(""cross validation"")
test_counts = count_vector.fit_transform(data)
test_tfidf = tfidf.fit_transform(test_counts)

scores = cross_validation.cross_val_score(classifier, test_tfidf, labels, cv = 10)
print(scores)
print(""Accuracy: {} +/- {}"".format(scores.mean(), scores.std() * 2))
</code></pre>

<p>My output error:</p>

<blockquote>
  <p>ValueError: cannot use sparse input in 'SVC' trained on dense data</p>
</blockquote>

<p>I can not execute my code because of this problem and I am not understanding anything of what is happening.</p>

<p><strong>all output error</strong></p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File ""classification.py"", line 42, in 
      predicted = classifier.predict(test_tfidf)</p>
  
  <p>File ""/usr/lib/python3/dist-packages/sklearn/multiclass.py"", line 584, in predict
      Y = self.decision_function(X)</p>
  
  <p>File ""/usr/lib/python3/dist-packages/sklearn/multiclass.py"", line 614, in decision_function
      for est, Xi in zip(self.estimators_, Xs)]).T</p>
  
  <p>File ""/usr/lib/python3/dist-packages/sklearn/multiclass.py"", line 614, in 
      for est, Xi in zip(self.estimators_, Xs)]).T</p>
  
  <p>File ""/usr/lib/python3/dist-packages/sklearn/svm/base.py"", line 548, in predict
      y = super(BaseSVC, self).predict(X)</p>
  
  <p>File ""/usr/lib/python3/dist-packages/sklearn/svm/base.py"", line 308, in predict
      X = self._validate_for_predict(X)</p>
  
  <p>File ""/usr/lib/python3/dist-packages/sklearn/svm/base.py"", line 448, in _validate_for_predict
      % type(self).<strong>name</strong>)</p>
  
  <p>ValueError: cannot use sparse input in 'SVC' trained on dense data</p>
</blockquote>
","python, python-3.x, machine-learning, scikit-learn, text-classification","<p>You get this error because your training &amp; test data are not of the same kind: while you train in your initial <code>X_train</code> set:</p>

<pre><code>classifier.fit(X_train, y_train)
</code></pre>

<p>you are trying to get predictions from a dataset which has undergone count vectorization &amp; tf-idf transormations first:</p>

<pre><code>predicted = classifier.predict(test_tfidf)
</code></pre>

<p>It is puzzling why you choose to do so, why you nevertheless compute <code>train_counts</code> and <code>train_tfidf</code> (you don't seem to actually use them anywhere), and why you are also trying to redefine <code>predicted</code> as <code>classifier.predict(X_test)</code> immediately afterwards. Normally, changing your training line to</p>

<pre><code>classifier.fit(train_tfidf, y_train)
</code></pre>

<p>and getting rid of your second <code>predicted</code> definition should work OK...</p>
",2,0,3966,2018-08-20 10:32:37,https://stackoverflow.com/questions/51928856/valueerror-cannot-use-sparse-input-in-svc-trained-on-dense-data
Classification with one file with entirely the training and another file with entirely test,"<p>I am trying to make a classification in which one file is entirely the training and another file is entirely the test. It's possible? I tried:</p>

<pre><code>import pandas
import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn import cross_validation
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer, TfidfTransformer
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score

#csv file from train
df = pd.read_csv('data_train.csv', sep = ',')

#csv file from test
df_test = pd.read_csv('data_test.csv', sep = ',')

#Randomising the rows in the file
df = df.reindex(np.random.permutation(df.index))
df_test = df_test.reindex(np.random.permutation(df_test.index))

vect = CountVectorizer()

X = vect.fit_transform(df['data_train'])
y = df['label']

X_T = vect.fit_transform(df_test['data_test'])
y_t = df_test['label']

X_train, y_train = train_test_split(X, y, test_size = 0, random_state = 100)
X_test, y_test = train_test_split(X_T, y_t, test_size = 0, random_state = 100)

tf_transformer = TfidfTransformer(use_idf=False).fit(X) 
X_train_tf = tf_transformer.transform(X) 
X_train_tf.shape

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X) 
X_train_tfidf.shape

tf_transformer = TfidfTransformer(use_idf=False).fit(X_T) 
X_train_tf_teste = tf_transformer.transform(X_T) 
X_train_tf_teste.shape

tfidf_transformer = TfidfTransformer()
X_train_tfidf_teste = tfidf_transformer.fit_transform(X_T) 
X_train_tfidf_teste.shape

#RegLog
clf = LogisticRegression().fit(X_train, y_train)

y_pred = clf.predict(X_test)

print(""confusion matrix"")
print(confusion_matrix(y_test, y_pred, labels = y))

print(""F-score"")
print(f1_score(y_test, y_pred, average=None))
print(precision_score(y_test, y_pred, average=None))
print(recall_score(y_test, y_pred, average=None)) 

print(""cross validation"")

scores = cross_validation.cross_val_score(clf, X, y, cv = 10)
print(scores)
print(""Accuracy: {} +/- {}"".format(scores.mean(), scores.std() * 2))
</code></pre>

<p>I have set test_size to zero because I do not want to have a partition in those files. And I also applied Count and TFIDF in the training and test file.</p>

<p><strong>My output error:</strong></p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File ""classif.py"", line 34, in 
      X_train, y_train = train_test_split(X, y, test_size = 0, random_state = 100)</p>
  
  <p>ValueError: too many values to unpack (expected 2)</p>
</blockquote>
","python, python-3.x, machine-learning, scikit-learn, text-classification","<p>The error you are getting in train_test_split is clearly indicated and solved by @Alexis. And once again I also suggest to not use train_test_split as it will not do anything except shuffling, which you have already done.</p>

<p>But I want to highlight another important point, i.e., If you are keeping your train and test files separately, then just don't fit vectorizers separately. It will create different columns for train and test files. Example:</p>

<pre><code>cv = CountVectorizer()
train=['Hi this is stack overflow']
cv.fit(train)
cv.get_feature_names()
</code></pre>

<p>Output:
    <code>['hi', 'is', 'overflow', 'stack', 'this']</code></p>

<pre><code>test=['Hi that is not stack overflow']
cv.fit(test)
cv.get_feature_names()
</code></pre>

<p>Output:
    <code>['hi', 'is', 'not', 'overflow', 'stack', 'that']</code></p>

<p>Hence, fitting them separately will result in columns mismatch. So, you should merge train and test files firstly and then fit_transform vectorizer collectively, or if you don't have test data beforehand you could only transform the test data using vectorizer fitted on train data, which will ignore the words not present in train data.</p>
",5,0,1794,2018-08-21 13:22:50,https://stackoverflow.com/questions/51949736/classification-with-one-file-with-entirely-the-training-and-another-file-with-en
LSTM Text Classification Bad Accuracy Keras,"<p>I'm going crazy in this project. This is multi-label text-classification with lstm in keras. My model is this: </p>

<pre><code>model = Sequential()

model.add(Embedding(max_features, embeddings_dim, input_length=max_sent_len, mask_zero=True, weights=[embedding_weights] ))
model.add(Dropout(0.25))
model.add(LSTM(output_dim=embeddings_dim , activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True))
model.add(Dropout(0.25))
model.add(LSTM(activation='sigmoid', units=embeddings_dim, recurrent_activation='hard_sigmoid', return_sequences=False))
model.add(Dropout(0.25))
model.add(Dense(num_classes))
model.add(Activation('sigmoid'))

adam=keras.optimizers.Adam(lr=0.04)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
</code></pre>

<p>Only that I have too low an accuracy .. with the binary-crossentropy I get a good accuracy, but the results are wrong !!!!! changing to categorical-crossentropy, I get very low accuracy. Do you have any suggestions?</p>

<p>there is my code: <a href=""http://github.com/ancileddu/multi-label-text-classification"" rel=""nofollow noreferrer"">GitHubProject - Multi-Label-Text-Classification</a></p>
","keras, lstm, text-classification, recurrent-neural-network, multilabel-classification","<p>In last layer, the activation function you are using is <code>sigmoid</code>, so <code>binary_crossentropy</code> should be used. Incase you want to use <code>categorical_crossentropy</code> then use <code>softmax</code> as activation function in last layer.</p>

<p>Now, coming to the other part of your model, since you are working with text, i would tell you to go for <code>tanh</code> as activation function in LSTM layers.</p>

<p>And you can try using LSTM's dropouts as well like <code>dropout</code> and <code>recurrent dropout</code></p>

<pre><code>LSTM(units, dropout=0.2, recurrent_dropout=0.2,
                             activation='tanh')
</code></pre>

<p>You can define units as <code>64</code> or <code>128</code>. Start from small number and after testing you take them till <code>1024</code>. </p>

<p>You can try adding <code>convolution</code> layer as well for extracting features or use <code>Bidirectional LSTM</code> But models based <code>Bidirectional</code> takes time to train.</p>

<p>Moreover, since you are working on text, <code>pre-processing of text and size of training data</code> always play much bigger role than expected.</p>

<p><strong>Edited</strong></p>

<p>Add Class weights in fit parameter</p>

<pre><code>class_weights = class_weight.compute_class_weight('balanced',
                                                  np.unique(labels),
                                                  labels)
class_weights_dict = dict(zip(le.transform(list(le.classes_)),
                          class_weights))


model.fit(x_train, y_train, validation_split, class_weight=class_weights_dict)
</code></pre>
",8,5,2325,2018-08-22 07:49:22,https://stackoverflow.com/questions/51962128/lstm-text-classification-bad-accuracy-keras
How do I determine the binary class predicted by a convolutional neural network on Keras?,"<p>I'm building a CNN to perform sentiment analysis on Keras.
Everything is working perfectly, the model is trained and ready to be launched to production.</p>

<p>However, when I try to predict on new unlabelled data by using the method <code>model.predict()</code> it only outputs the associated probability. I tried to use the method <code>np.argmax()</code> but it <strong>always</strong> outputs 0 even when it should be 1 (on test set, my model achieved 80% of accuracy). </p>

<p>Here is my code to pre-process the data:</p>

<pre><code># Pre-processing data
x = df[df.Sentiment != 3].Headlines
y = df[df.Sentiment != 3].Sentiment

# Splitting training, validation, testing dataset
x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.3,
                                                                                      random_state=SEED)
x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test,
                                                                  test_size=.5, random_state=SEED)

tokenizer = Tokenizer(num_words=NUM_WORDS)
tokenizer.fit_on_texts(x_train)

sequences = tokenizer.texts_to_sequences(x_train)
x_train_seq = pad_sequences(sequences, maxlen=MAXLEN)

sequences_val = tokenizer.texts_to_sequences(x_validation)
x_val_seq = pad_sequences(sequences_val, maxlen=MAXLEN)

sequences_test = tokenizer.texts_to_sequences(x_test)
x_test_seq = pad_sequences(sequences_test, maxlen=MAXLEN)
</code></pre>

<p>And here is my model:</p>

<pre><code>MAXLEN = 25
NUM_WORDS = 5000
VECTOR_DIMENSION = 100

tweet_input = Input(shape=(MAXLEN,), dtype='int32')

tweet_encoder = Embedding(NUM_WORDS, VECTOR_DIMENSION, input_length=MAXLEN)(tweet_input)

# Combinating n-gram to optimize results
bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation=""relu"", strides=1)(tweet_encoder)
bigram_branch = GlobalMaxPooling1D()(bigram_branch)
trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation=""relu"", strides=1)(tweet_encoder)
trigram_branch = GlobalMaxPooling1D()(trigram_branch)
fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation=""relu"", strides=1)(tweet_encoder)
fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)
merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)

merged = Dense(256, activation=""relu"")(merged)
merged = Dropout(0.25)(merged)
output = Dense(1, activation=""sigmoid"")(merged)

optimizer = optimizers.adam(0.01)

model = Model(inputs=[tweet_input], outputs=[output])
model.compile(loss=""binary_crossentropy"", optimizer=optimizer, metrics=['accuracy'])
model.summary()

# Training the model
history = model.fit(x_train_seq, y_train, batch_size=32, epochs=5, validation_data=(x_val_seq, y_validation))
</code></pre>

<p>I also tried to change the number of activations on the final Dense layer from 1 to 2, but I get an error: </p>

<pre><code>Error when checking target: expected dense_12 to have shape (2,) but got array with shape (1,)
</code></pre>
","python, machine-learning, keras, deep-learning, text-classification","<p>You are doing binary classification. So you have a Dense layer consisting of one unit with an activation function of <code>sigmoid</code>. Sigmoid function outputs a value in range [0,1] which corresponds to the probability of the given sample belonging to positive class (i.e. class one). Everything below 0.5 is labeled with zero (i.e. negative class) and everything above 0.5 is labeled with one. So to find the predicted class you can do the following:</p>

<pre><code>preds = model.predict(data)
class_one = preds &gt; 0.5
</code></pre>

<p>The true elements of <code>class_one</code> correspond to samples labeled with one (i.e. positive class).</p>

<p><strong>Bonus:</strong> to find the accuracy of your predictions you can easily compare <code>class_one</code> with the true labels:</p>

<pre><code>acc = np.mean(class_one == true_labels)
</code></pre>

<p>Note that I have assumed that <code>true_labels</code> consists of zeros and ones.</p>

<hr>

<p>Further, if your model were defined using Sequential class, then you could easily use <code>predict_classes</code> method:</p>

<pre><code>pred_labels = model.predict_classes(data)
</code></pre>

<p>However, since you are using Keras functional API to construct your model (which is a very good thing to do so, in my opinion), you can't use <code>predict_classes</code> method since it is ill-defined for such models.</p>
",11,6,8321,2018-08-25 15:22:35,https://stackoverflow.com/questions/52018645/how-do-i-determine-the-binary-class-predicted-by-a-convolutional-neural-network
RNN: Get prediction from a text input after the model is trained,"<p>I am new to RNNs and I have been working on a small binary label classifier. I have been able to get a stable model with satisfactory results. </p>

<p>However, I am having a hard time using the model to classify new inputs and I was wondering if any of you could help me. Please see my code below for reference. </p>

<p>Thank you very much.</p>

<pre><code>from tensorflow.keras import preprocessing
from sklearn.utils import shuffle
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras import models
from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, 
Embedding
from tensorflow.keras.optimizers import RMSprop, Adam
from tensorflow.keras.preprocessing import sequence, text
from tensorflow.keras.callbacks import EarlyStopping
from matplotlib import pyplot

class tensor_rnn():
def __init__(self, hidden_layers=3):
    self.data_path = 'C:\\\\Users\\cmazz\\PycharmProjects\\InvestmentAnalysis_2.0\\Sentiment\\Finance_Articles\\'
    # self.corp_paths = corpora_paths
    self.h_layers = hidden_layers
    self.num_words = []
    good = pd.read_csv(self.data_path + 'GoodO.csv')
    good['Polarity'] = 'pos'
    for line in good['Head'].tolist():
        counter = len(line.split())
        self.num_words.append(counter)
    bad = pd.read_csv(self.data_path + 'BadO.csv')
    bad['Polarity'] = 'neg'
    for line in bad['Head'].tolist():
        counter = len(line.split())
        self.num_words.append(counter)
    self.features = pd.concat([good, bad]).reset_index(drop=True)
    self.features = shuffle(self.features)

    self.max_len = len(max(self.features['Head'].tolist()))
    # self.train, self.test = train_test_split(features, test_size=0.33, random_state=42)
    X = self.features['Head']
    Y = self.features['Polarity']
    le = LabelEncoder()
    Y = le.fit_transform(Y)
    Y = Y.reshape(-1, 1)
    self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, test_size=0.30)
    self.tok = preprocessing.text.Tokenizer(num_words=len(self.num_words))
    self.tok.fit_on_texts(self.X_train)
    sequences = self.tok.texts_to_sequences(self.X_train)
    self.sequences_matrix = preprocessing.sequence.pad_sequences(sequences, maxlen=self.max_len)

def RNN(self):
    inputs = Input(name='inputs', shape=[self.max_len])
    layer = Embedding(len(self.num_words), 30, input_length=self.max_len)(inputs)
    # layer = LSTM(64, return_sequences=True)(layer)
    layer = LSTM(32)(layer)
    layer = Dense(256, name='FC1')(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(0.5)(layer)
    layer = Dense(1, name='out_layer')(layer)
    layer = Activation('sigmoid')(layer)
    model = Model(inputs=inputs, outputs=layer)
    return model

def model_train(self):
    self.model = self.RNN()
    self.model.summary()
    self.model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])   # RMSprop()

def model_test(self):
    self.history = self.model.fit(self.sequences_matrix, self.Y_train, batch_size=100, epochs=3,
              validation_split=0.30, callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)])
    test_sequences = self.tok.texts_to_sequences(self.X_test)
    test_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen=self.max_len)
    accr = self.model.evaluate(test_sequences_matrix, self.Y_test)
    print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))


if __name__ == ""__main__"":
    a = tensor_rnn()
    a.model_train()
    a.model_test()
    a.model.save('C:\\\\Users\\cmazz\\PycharmProjects\\'
                              'InvestmentAnalysis_2.0\\RNN_Model.h5', 
    include_optimizer=True)
     b = models.load_model('C:\\\\Users\\cmazz\\PycharmProjects\\'
                              'InvestmentAnalysis_2.0\\RNN_Model.h5')
    stringy = ['Fund managers back away from Amazon as they cut FANG exposure']
    prediction = b.predict(np.array(stringy))
    print(prediction)
</code></pre>

<p>When I run my code I get the following error:</p>

<blockquote>
  <p>ValueError: Error when checking input: expected inputs to have shape
  (39,) but got array with shape (1,)</p>
</blockquote>
","python, tensorflow, machine-learning, keras, text-classification","<p>Based on the ValueError and <code>prediction = b.predict(np.array(stringy))</code>, I think you need to tokenize your input string.</p>
",3,1,1208,2018-08-25 17:49:51,https://stackoverflow.com/questions/52019808/rnn-get-prediction-from-a-text-input-after-the-model-is-trained
"Classify strings having centers already found, python","<p>I have a list of binary strings and two center strings which are not in the list.
I would like to classify that list around the center strings in order to create two clusters. A string of the list will be assigned to the cluster whose center is nearest to that string (hamming distance as metric).
I've seen that there are alghoritms like Neighbours Classifier, k-medoids, Affinity propagation, but all these procedure calculate centroids on their own; I have to use my center strings instead.</p>

<p>Any suggestion?</p>
","python, classification, cluster-analysis, text-classification","<p>Perform 1-nearest neighbor classification using your <em>centers</em> only.</p>

<p>No need for anything special here. Just assign to the nearest center.</p>
",0,-1,54,2018-08-31 18:08:29,https://stackoverflow.com/questions/52120756/classify-strings-having-centers-already-found-python
Sklearn Pipeline ValueError: could not convert string to float,"<p>I'm playing around with sklearn and NLP for the first time, and thought I understood everything I was doing up until I didn't know how to fix this error. Here is the relevant code (largely adapted from <a href=""http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html"" rel=""noreferrer"">http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html</a>):</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import TruncatedSVD
from sgboost import XGBClassifier
from pandas import DataFrame

def read_files(path):
    for article in os.listdir(path):
        with open(os.path.join(path, doc)) as f:
            text = f.read()
        yield os.path.join(path, article), text

def build_data_frame(path, classification)
    rows = []
    index = []
    for filename, text in read_files(path):
        rows.append({'text': text, 'class': classification})
        index.append(filename)
    df = DataFrame(rows, index=index)
    return df

data = DataFrame({'text': [], 'class': []})
for path, classification in SOURCES: # SOURCES is a list of tuples
    data = data.append(build_data_frame(path, classification))
data = data.reindex(np.random.permutation(data.index))

classifier = Pipeline([
    ('features', FeatureUnion([
        ('text', Pipeline([
            ('tfidf', TfidfVectorizer()),
            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)
            ])),
        ('words', Pipeline([('wscaler', StandardScaler())])),
    ])),
    ('clf, XGBClassifier(silent=False)),
])
classifier.fit(data['text'].values, data['class'].values)
</code></pre>

<p>The data loaded into the DataFrame is preprocessed text with all stopwords, punctuation, unicode, capitals, etc. taken care of. This is the error I'm getting once I call fit on the classifier where the ... represents one of the documents that should have been vecorized in the pipeline:</p>

<pre><code>ValueError: could not convert string to float: ...
</code></pre>

<p>I first thought the TfidfVectorizer() is not working, causing an error on the SVD algorithm, but after I extracted each step out of the pipeline and implemented them sequentially, the same error only came up on XGBClassifer.fit().</p>

<p>Even more confusing to me, I tried to piece this script apart step-by-step in the interpreter, but when I tried to import either read_files or build_data_frame, the same ValueError came up with one of my strings, but this was merely after:</p>

<pre><code>from classifier import read_files
</code></pre>

<p>I have no idea how that could be happening, if anyone has any idea what my glaring errors may be, I'd really appreciate it. Trying to wrap my head around these concepts on my own but coming across a problem likes this leaves me feeling pretty incapacitated.</p>
","python, scikit-learn, nlp, text-classification","<p>First part of your pipeline is a <code>FeatureUnion</code>. <code>FeatureUnion</code> will pass all the data it gets parallely to all internal parts. The second part of your <code>FeatureUnion</code> is a Pipeline containing single <code>StandardScaler</code>. Thats the source of error.</p>

<p>This is your data flow:</p>

<pre><code>X --&gt; classifier, Pipeline
            |
            |  &lt;== X is passed to FeatureUnion
            \/
      features, FeatureUnion
                      |
                      |  &lt;== X is duplicated and passed to both parts
        ______________|__________________
       |                                 |
       |  &lt;===   X contains text  ===&gt;   |                         
       \/                               \/
   text, Pipeline                   words, Pipeline
           |                                  |   
           |  &lt;===    Text is passed  ===&gt;    |
          \/                                 \/ 
       tfidf, TfidfVectorizer            wscaler, StandardScaler  &lt;== Error
                 |                                   |
                 | &lt;==Text converted to floats       |
                \/                                   |
              svd, TruncatedSVD                      |
                       |                             |
                       |                             |
                      \/____________________________\/
                                      |
                                      |
                                     \/
                                   clf, XGBClassifier
</code></pre>

<p>Since text is passed to <code>StandardScaler</code>, the error is thrown, <code>StandardScaler</code> can only work with numerical features.</p>

<p>Just as you are converting text to numbers using TfidfVectorizer, before sending that to TruncatedSVD, you need to do the same before <code>StandardScaler</code>, or else only provide numerical features to it.</p>

<p>Looking at the description in question, did you intend to keep StandardScaler after the results of TruncatedSVD?</p>
",3,7,4984,2018-08-31 21:59:32,https://stackoverflow.com/questions/52123026/sklearn-pipeline-valueerror-could-not-convert-string-to-float
Keras: How to display attention weights in LSTM model,"<p>I made a text classification model using an LSTM with attention layer. I did my model well, it works well, but I can't display the attention weights and the importance/attention of each word in a review (the input text).
The code used for this model is:</p>

<pre class=""lang-py prettyprint-override""><code>def dot_product(x, kernel):
   if K.backend() == 'tensorflow':
       return K.squeeze(K.dot(x, K.expand_dims(kernel)),axis=-1)
   else:
       return K.dot(x, kernel)

class AttentionWithContext(Layer):
    """"""
Attention operation, with a context/query vector, for temporal data.

""Hierarchical Attention Networks for Document Classification""
by using a context vector to assist the attention
# Input shape
    3D tensor with shape: (samples, steps, features).
# Output shape
    2D tensor with shape: (samples, features).
How to use:
Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.
The dimensions are inferred based on the output shape of the RNN.
Note: The layer has been tested with Keras 2.0.6
Example:
    model.add(LSTM(64, return_sequences=True))
    model.add(AttentionWithContext())
    # next add a Dense layer (for classification/regression) or whatever
     """"""

def __init__(self,
             W_regularizer=None, u_regularizer=None, b_regularizer=None,
             W_constraint=None, u_constraint=None, b_constraint=None,
             bias=True, **kwargs):

    self.supports_masking = True
    self.init = initializers.get('glorot_uniform')

    self.W_regularizer = regularizers.get(W_regularizer)
    self.u_regularizer = regularizers.get(u_regularizer)
    self.b_regularizer = regularizers.get(b_regularizer)

    self.W_constraint = constraints.get(W_constraint)
    self.u_constraint = constraints.get(u_constraint)
    self.b_constraint = constraints.get(b_constraint)

    self.bias = bias
    super(AttentionWithContext, self).__init__(**kwargs)

def build(self, input_shape):
    assert len(input_shape) == 3

    self.W = self.add_weight((input_shape[-1], input_shape[-1],),
                             initializer=self.init,
                             name='{}_W'.format(self.name),
                             regularizer=self.W_regularizer,
                             constraint=self.W_constraint)
    if self.bias:
        self.b = self.add_weight((input_shape[-1],),
                                 initializer='zero',
                                 name='{}_b'.format(self.name),
                                 regularizer=self.b_regularizer,
                                 constraint=self.b_constraint)

    self.u = self.add_weight((input_shape[-1],),
                             initializer=self.init,
                             name='{}_u'.format(self.name),
                             regularizer=self.u_regularizer,
                             constraint=self.u_constraint)

    super(AttentionWithContext, self).build(input_shape)

def compute_mask(self, input, input_mask=None):
    # do not pass the mask to the next layers
    return None

def call(self, x, mask=None):
    uit = dot_product(x, self.W)

    if self.bias:
        uit += self.b

    uit = K.tanh(uit)
    ait = dot_product(uit, self.u)

    a = K.exp(ait)

    # apply mask after the exp. will be re-normalized next
    if mask is not None:
        # Cast the mask to floatX to avoid float64 upcasting in theano
        a *= K.cast(mask, K.floatx())

    # in some cases especially in the early stages of training the sum may be almost zero
    # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.
    # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())
    a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

    a = K.expand_dims(a)
    weighted_input = x * a
    return K.sum(weighted_input, axis=1)

def compute_output_shape(self, input_shape):
    return input_shape[0], input_shape[-1]


EMBEDDING_DIM=100
max_seq_len=118
bach_size = 256
num_epochs=50
from keras.models import Model
from keras.layers import Dense, Embedding, Input
from keras.layers import LSTM, Bidirectional, Dropout


def BidLstm():
    #inp = Input(shape=(118,100))
    #x = Embedding(max_features, embed_size, weights=[embedding_matrix],
              #trainable=False)(inp)
     model1=Sequential()
     model1.add(Dense(512,input_shape=(118,100)))
    model1.add(Activation('relu'))
    #model1.add(Flatten()) 
    #model1.add(BatchNormalization(input_shape=(100,)))
    model1.add(Bidirectional(LSTM(100, activation=""relu"",return_sequences=True)))
    model1.add(Dropout(0.1))
    model1.add(TimeDistributed(Dense(200)))
    model1.add(AttentionWithContext())
    model1.add(Dropout(0.25))
    model1.add(Dense(4, activation=""softmax""))
    model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam',
              metrics=['accuracy'])
    model1.summary()
    return model1
</code></pre>
","python, keras, lstm, text-classification, attention-model","<p>Thank you for your edit.
Your solution return the weights of attention layers but I'm looking for the word weights.</p>

<p>I found other solution for this problem:</p>

<p>1.define function to compute attention weight:</p>

<pre><code>def cal_att_weights(output, att_w):
#if model_name == 'HAN':
eij = np.tanh(np.dot(output[0], att_w[0]) + att_w[1])
eij = np.dot(eij, att_w[2])
eij = eij.reshape((eij.shape[0], eij.shape[1]))
ai = np.exp(eij)
weights = ai / np.sum(ai)
return weights
from keras import backend as K
sent_before_att = K.function([model1.layers[0].input,K.learning_phase()],  [model1.layers[2].output])
sent_att_w = model1.layers[5].get_weights()
test_seq=np.array(userinp)
test_seq=np.array(test_seq).reshape(1,118,100)
out = sent_before_att([test_seq, 0])
</code></pre>
",0,2,6614,2018-09-03 14:50:36,https://stackoverflow.com/questions/52152054/keras-how-to-display-attention-weights-in-lstm-model
R - How to apply terms from training document-term-matrix (dtm) to test dtm (both unigrams and bigrams)?,"<p>I am training a simple text classification method on 1,000 training examples and would like to make predictions on unseen test data (about 500,000 observations).</p>

<p>The script is working fine, when I work only with unigrams. However, I am not sure how to use <code>control = list(dictionary=Terms(dtm_train_unigram))</code> when working with unigrams and bigrams as I have two separate document-term-matrices (one for unigrams, one for bigrams, see below):</p>

<pre><code>  UnigramTokenizer &lt;- function(x) unlist(lapply(NLP::ngrams(words(x), 1), paste, collapse = "" ""), use.names = FALSE)
  dtm_train_unigram &lt;- DocumentTermMatrix(processed_dataset, control = list(tokenize = UnigramTokenizer, wordLengths=c(3,20), bounds = list(global = c(4,Inf))))

  BigramTokenizer &lt;- function(x) unlist(lapply(NLP::ngrams(words(x), 2), paste, collapse = "" ""), use.names = FALSE)
  dtm_train_bigram &lt;- DocumentTermMatrix(processed_dataset, control = list(tokenize = BigramTokenizer, wordLengths=c(6,20), bounds = list(global = c(7,Inf))))
</code></pre>

<p>To ensure that the test set has the same terms as the training set, I use the following function:</p>

<pre><code>corpus_test &lt;- VCorpus(VectorSource(test_set))
dtm_test &lt;- DocumentTermMatrix(corpus_test, control = list(dictionary=Terms(dtm_train_unigram), wordLengths = c(3,20)))
</code></pre>

<p>How do I feed the terms of both the <code>dtm_train_unigram</code> and the <code>dtm_train_bigram</code> to the dtm_test?</p>

<ol>
<li>Can I combine <code>dtm_train_unigram</code> and <code>dtm_train_bigram</code> to a single dtm after creating them separately (as currently done)?  </li>
<li>Can I simplify my two-step Tokenizer function so I only create a single
dtm with unigrams and bigrams in the first place?</li>
</ol>

<p>Thank you!</p>
","r, text, nlp, tm, text-classification","<p>Answering your questions:</p>

<p>Official documentation of tm states the following for combining things.:</p>

<blockquote>
  <p>Combine several corpora into a single one, combine multiple documents
  into a corpus, combine multiple term-document matrices into a single
  one, or combine multiple term frequency vectors into a single
  term-document matrix.</p>
</blockquote>

<p>which in your case would be the answer to 1:</p>

<pre><code>my_dtms &lt;- c(dtm_train_unigram, dtm_train_bigram)
</code></pre>

<p>But it does result in doubling the number of documents which is actually not the case.</p>

<p>So we come to point 2, you can create a tokenizer from the NLP package that handles more than just 1 instance of n-gram:</p>

<pre><code>my_tokenizer &lt;- function(x) unlist(lapply(NLP::ngrams(words(x), 1:2), paste, collapse = "" ""), use.names = FALSE)
</code></pre>

<p>note the vector 1:2 ngram function. Change this to 1:3 for 1, 2, 3 grams or 2:3 for just 2 and 3 grams.</p>
",2,0,492,2018-09-06 15:00:26,https://stackoverflow.com/questions/52207021/r-how-to-apply-terms-from-training-document-term-matrix-dtm-to-test-dtm-bot
Classifying data into 3 classes with a binary classifier model,"<p>Hi I am new to Machine Learning. I have a task of classifying data into 3 classes. I have data for only 2 classes. I have used SVM Linear Classifier for the classes for which I have data (2 classes say A and B). Now there is another third class C for which I don't have any data. is there a way that I can use this binary classifier and predict if a sample doesn't belong to any of the 2 classes(A and B) then it must belong to C ?</p>
","python, machine-learning, svm, text-classification","<p>You can indeed eventually use your model by observing if a specific threshold (activation) is met or not but I am not sure what will be good in practice. I doubt you can find a stable threshold values that will successfully divide the dimension so you can classify class C.
I would rather try another approach. I would concat A and B and train an outlier detector such as <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html"" rel=""nofollow noreferrer"">One Class SVM</a>. Then you would be able to classify C i.e it is neither A nor B .</p>
",2,0,955,2018-09-12 12:00:09,https://stackoverflow.com/questions/52294663/classifying-data-into-3-classes-with-a-binary-classifier-model
Text classification - randomForest. variables in the training data missing in newdata,"<p>I'm completely new to statistical learning etc but have a particular interest in text classification. I was following a lab I found on the topic here: <a href=""https://cfss.uchicago.edu/text_classification.html#fnref1"" rel=""nofollow noreferrer"">https://cfss.uchicago.edu/text_classification.html#fnref1</a>. Unfortunately the lab ends before the trained model could be used on new data, so I tried to figure out how to complete it myself. </p>

<p>I have my model trained, Im using random forest. When I try to use <code>predict()</code> on new data it throws an error: <code>Error in predict.randomForest(modelFit, newdata) : 
  variables in the training data missing in newdata</code></p>

<p>Which in my mind doesn't make sense as the test data is literally a subset of the original data. I <em>assume</em> this error has something to do with how I built my model vs the data structure of the test data but I'm honestly not competent enough to figure out how to solve the error or where it is actually even stemming from (though I assume Im making some ridiculous error). </p>

<p>There are other posts with the same error but I think the source of their errors are different to mine, I've tried to find a fix for this all day! </p>

<p>Complete code I'm using below: </p>

<pre><code>library(tidyverse)
library(tidytext)
library(stringr)
library(caret)
library(tm)

data(USCongress, package = ""RTextTools"")
test &lt;- congress[1:100, ]
congress &lt;- congress[100:nrow(congress), ]

(congress &lt;- as_tibble(USCongress) %&gt;%
    mutate(text = as.character(text)))
(congress_tokens &lt;- congress %&gt;%
    unnest_tokens(output = word, input = text) %&gt;%
    # remove numbers
    filter(!str_detect(word, ""^[0-9]*$"")) %&gt;%
    # remove stop words
    anti_join(stop_words) %&gt;%
    # stem the words
    mutate(word = SnowballC::wordStem(word)))
(congress_dtm &lt;- congress_tokens %&gt;%
    # get count of each token in each document
    count(ID, word) %&gt;%
    # create a document-term matrix with all features and tf weighting
    cast_dtm(document = ID, term = word, value = n))
congress_dtm &lt;- removeSparseTerms(congress_dtm, sparse = .99)
congress_rf &lt;- train(x = as.matrix(congress_dtm),
                     y = factor(congress$major),
                     method = ""rf"",
                     ntree = 200,
                     trControl = trainControl(method = ""oob""))
final_predictions &lt;- predict(congress_rf, newdata = test)
</code></pre>

<p>The last line (<code>final_predictions &lt;- predict(congress_rf, newdata = test</code>) is where the error appears, no error messages occur before that. </p>
","r, r-caret, text-classification","<p>The problem is that <code>test</code> is not a subset of the data that you are fitting the model with (<code>congress_dtm</code>). If you create a subset of <code>congress_dtm</code>, it does work:</p>

<pre><code>#....
congress_dtm &lt;- removeSparseTerms(congress_dtm, sparse = .99)
test &lt;- congress_dtm[100, ]
congress_rf &lt;- train(x = as.matrix(congress_dtm),
                     y = factor(congress$major),
                     method = ""rf"",
                     ntree = 200,
                     trControl = trainControl(method = ""oob""))
final_predictions &lt;- predict(congress_rf, newdata = test)
final_predictions
#&gt; [1] 12
#&gt; Levels: 1 2 3 4 5 6 7 8 10 12 13 14 15 16 17 18 19 20 21 99
</code></pre>
",1,-1,822,2018-09-19 15:23:44,https://stackoverflow.com/questions/52409297/text-classification-randomforest-variables-in-the-training-data-missing-in-ne
Difference of Pre-Padding and Post-Padding text when preprossing different text sizes for tf.nn.embedding_lookup,"<p>I have seen two types of padding when feeding to embedding layers. </p>

<blockquote>
  <p><strong>eg:</strong></p>
  
  <p>considering two sentences:</p>
  
  <p>word1 = ""I am a dog person.""</p>
  
  <p>word2 = ""Krishni and Pradeepa both love cats.""</p>
  
  <p>word1_int = [1,2,3,4,5,6] </p>
  
  <p>word2_int = [7,8,9,10,11,12,13]</p>
  
  <p>padding both words to length = 8</p>
  
  <p><strong>padding method 1</strong>(putting 0s at the beginning)</p>
  
  <p>word1_int = [0,0,1,2,3,4,5,6] </p>
  
  <p>word2_int = [0,7,8,9,10,11,12,13]</p>
  
  <p><strong>padding method 2</strong>(putting 0s at the end)</p>
  
  <p>word1_int = [1,2,3,4,5,6,0,0] </p>
  
  <p>word2_int = [7,8,9,10,11,12,13,0]</p>
</blockquote>

<p>I am trying to do an <strong>online</strong> classification using the 20 news groups dataset. and I am currently using the 1st method to pad my text. </p>

<p><strong>Question</strong>: Is there any advantage of using the 1st method over the other one in my implementation?</p>

<p>Thank you in advance!</p>

<p>My code is shown below:</p>

<pre><code>from collections import Counter
import tensorflow as tf
from sklearn.datasets import fetch_20newsgroups
import matplotlib as mplt
mplt.use('agg') # Must be before importing matplotlib.pyplot or pylab!
import matplotlib.pyplot as plt
from string import punctuation
from sklearn.preprocessing import LabelBinarizer
import numpy as np
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')



def pre_process():
    newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))

    words = []
    temp_post_text = []
    print(len(newsgroups_data.data))

    for post in newsgroups_data.data:

        all_text = ''.join([text for text in post if text not in punctuation])
        all_text = all_text.split('\n')
        all_text = ''.join(all_text)
        temp_text = all_text.split("" "")

        for word in temp_text:
            if word.isalpha():
                temp_text[temp_text.index(word)] = word.lower()

        # temp_text = [word for word in temp_text if word not in stopwords.words('english')]
        temp_text = list(filter(None, temp_text))
        temp_text = ' '.join([i for i in temp_text if not i.isdigit()])
        words += temp_text.split("" "")
        temp_post_text.append(temp_text)

    # temp_post_text = list(filter(None, temp_post_text))

    dictionary = Counter(words)
    # deleting spaces
    # del dictionary[""""]
    sorted_split_words = sorted(dictionary, key=dictionary.get, reverse=True)
    vocab_to_int = {c: i for i, c in enumerate(sorted_split_words,1)}

    message_ints = []
    for message in temp_post_text:
        temp_message = message.split("" "")
        message_ints.append([vocab_to_int[i] for i in temp_message])


    # maximum message length = 6577

    # message_lens = Counter([len(x) for x in message_ints])AAA

    seq_length = 6577
    num_messages = len(temp_post_text)
    features = np.zeros([num_messages, seq_length], dtype=int)
    for i, row in enumerate(message_ints):
        print(features[i, -len(row):])
        features[i, -len(row):] = np.array(row)[:seq_length]
        print(features[i, -len(row):])

    lb = LabelBinarizer()
    lbl = newsgroups_data.target
    labels = np.reshape(lbl, [-1])
    labels = lb.fit_transform(labels)

    return features, labels, len(sorted_split_words)+1


def get_batches(x, y, batch_size=1):
    for ii in range(0, len(y), batch_size):
        yield x[ii:ii + batch_size], y[ii:ii + batch_size]


def plot(noOfWrongPred, dataPoints):
    font_size = 14
    fig = plt.figure(dpi=100,figsize=(10, 6))
    mplt.rcParams.update({'font.size': font_size})
    plt.title(""Distribution of wrong predictions"", fontsize=font_size)
    plt.ylabel('Error rate', fontsize=font_size)
    plt.xlabel('Number of data points', fontsize=font_size)

    plt.plot(dataPoints, noOfWrongPred, label='Prediction', color='blue', linewidth=1.8)
    # plt.legend(loc='upper right', fontsize=14)

    plt.savefig('distribution of wrong predictions.png')
    # plt.show()



def train_test():
    features, labels, n_words = pre_process()

    print(features.shape)
    print(labels.shape)

    # Defining Hyperparameters

    lstm_layers = 1
    batch_size = 1
    lstm_size = 200
    learning_rate = 0.01

    # --------------placeholders-------------------------------------

    # Create the graph object
    graph = tf.Graph()
    # Add nodes to the graph
    with graph.as_default():

        tf.set_random_seed(1)

        inputs_ = tf.placeholder(tf.int32, [None, None], name=""inputs"")
        # labels_ = tf.placeholder(dtype= tf.int32)
        labels_ = tf.placeholder(tf.float32, [None, None], name=""labels"")

        # output_keep_prob is the dropout added to the RNN's outputs, the dropout will have no effect on the calculation of the subsequent states.
        keep_prob = tf.placeholder(tf.float32, name=""keep_prob"")

        # Size of the embedding vectors (number of units in the embedding layer)
        embed_size = 300

        # generating random values from a uniform distribution (minval included and maxval excluded)
        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1),trainable=True)
        embed = tf.nn.embedding_lookup(embedding, inputs_)

        print(embedding.shape)
        print(embed.shape)
        print(embed[0])

        # Your basic LSTM cell
        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)


        # Add dropout to the cell
        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)

        # Stack up multiple LSTM layers, for deep learning
        cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)

        # Getting an initial state of all zeros
        initial_state = cell.zero_state(batch_size, tf.float32)

        outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)

        # hidden layer
        hidden = tf.layers.dense(outputs[:, -1], units=25, activation=tf.nn.relu)

        print(hidden.shape)

        logit = tf.contrib.layers.fully_connected(hidden, num_outputs=20, activation_fn=None)

        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=labels_))

        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

        saver = tf.train.Saver()

    # ----------------------------online training-----------------------------------------

    with tf.Session(graph=graph) as sess:
        tf.set_random_seed(1)
        sess.run(tf.global_variables_initializer())
        iteration = 1
        state = sess.run(initial_state)
        wrongPred = 0
        noOfWrongPreds = []
        dataPoints = []

        for ii, (x, y) in enumerate(get_batches(features, labels, batch_size), 1):

            feed = {inputs_: x,
                    labels_: y,
                    keep_prob: 0.5,
                    initial_state: state}

            embedzz = sess.run(embedding, feed_dict=feed)

            print(embedzz)


            predictions = tf.nn.softmax(logit).eval(feed_dict=feed)

            print(""----------------------------------------------------------"")
            print(""Iteration: {}"".format(iteration))

            isequal = np.equal(np.argmax(predictions[0], 0), np.argmax(y[0], 0))

            print(np.argmax(predictions[0], 0))
            print(np.argmax(y[0], 0))

            if not (isequal):
                wrongPred += 1

            print(""nummber of wrong preds: "",wrongPred)

            if iteration%50 == 0:
                noOfWrongPreds.append(wrongPred/iteration)
                dataPoints.append(iteration)

            loss, states, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)

            print(""Train loss: {:.3f}"".format(loss))
            iteration += 1

        saver.save(sess, ""checkpoints/sentiment.ckpt"")
        errorRate = wrongPred / len(labels)
        print(""ERRORS: "", wrongPred)
        print(""ERROR RATE: "", errorRate)
        plot(noOfWrongPreds, dataPoints)


if __name__ == '__main__':
    train_test()
</code></pre>

<p>This is the code sample that I am using to pad all the sentences.</p>

<pre><code>  seq_length = 6577
  num_messages = len(temp_post_text)
    features = np.zeros([num_messages, seq_length], dtype=int)
    for i, row in enumerate(message_ints):
        print(features[i, -len(row):])
        features[i, -len(row):] = np.array(row)[:seq_length]
        print(features[i, -len(row):])
</code></pre>
","python-3.x, tensorflow, machine-learning, text-classification, word-embedding","<p>Commonly, when we use LSTM or RNN's, we use the final output or the hidden state and pass it along to make predictions. You are also doing the same thing as seen in this line: </p>

<pre><code>logit = tf.contrib.layers.fully_connected(hidden, num_outputs=20, activation_fn=None)
</code></pre>

<p>Here the two methods of padding get differentiated. If you use the 2nd method of padding, post-padding, then the final hidden state would get flushed out as mostly it will be <code>0</code>, whereas by using the 1st method, we make sure that the hidden state output is correct.</p>
",1,1,4717,2018-09-20 10:36:00,https://stackoverflow.com/questions/52423147/difference-of-pre-padding-and-post-padding-text-when-preprossing-different-text
Prediction giving same value in every Iteration in an online multiclass classification using LSTM,"<p>I have developed a code to do <strong>online</strong> <strong>multi-class classification</strong> using the <strong>20 news groups data-set</strong>. In order to eliminate the effect of the padded 0s of the text fed into the <strong>LSTM</strong>, I added the <strong>'sequence_length'</strong> parameter to the <strong>dynamic_rnn</strong> passing the length of each text being processed. </p>

<p>After I added this attribute, the prediction (the code shown blow) gives the <strong>same prediction</strong> for all the iterations <strong>except the very 1st one</strong>.</p>

<pre><code>predictions = tf.nn.softmax(logit).eval(feed_dict=feed)
</code></pre>

<p>Shown below are the predictions I received for the 1st, 2nd, 3rd and 4th iterations :</p>

<blockquote>
  <p>1st: [[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05
  0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05]]</p>
  
  <p>2nd: [[0.04994956 0.04994956 0.04994956 0.04994956 0.04994956
  0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.0509586  0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.04994956 0.04994956]]</p>
  
  <p>3rd: [[0.0498649  0.0498649  0.0498649  0.05072384 0.0498649 
  0.0498649
    0.0498649  0.0498649  0.0498649  0.0498649  0.05170782 0.0498649
    0.0498649  0.0498649  0.0498649  0.0498649  0.0498649  0.0498649
    0.0498649  0.0498649 ]]</p>
  
  <p>4th: [[0.04974937 0.04974937 0.04974937 0.05137746 0.04974937
  0.04974937 0.04974937 0.04974937 0.04974937 0.04974937 0.05234195 0.04974937 0.04974937 0.04974937 0.04974937 0.04974937 0.04974937 0.05054148 0.04974937 0.04974937]]</p>
</blockquote>

<p>After the 2nd iteration the prediction doesn't change (<strong>the argmax of the prediction always comes as 10</strong>).</p>

<p><strong>Question</strong>: What am I doing wrong here? 
Thank you in advance!</p>

<p>Shown below is my complete code:</p>

<pre><code>from collections import Counter
import tensorflow as tf
from sklearn.datasets import fetch_20newsgroups
import matplotlib as mplt
mplt.use('agg') # Must be before importing matplotlib.pyplot or pylab!
import matplotlib.pyplot as plt
from string import punctuation
from sklearn.preprocessing import LabelBinarizer
import numpy as np
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')



def pre_process():
    newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))

    words = []
    temp_post_text = []
    print(len(newsgroups_data.data))

    for post in newsgroups_data.data:

        all_text = ''.join([text for text in post if text not in punctuation])
        all_text = all_text.split('\n')
        all_text = ''.join(all_text)
        temp_text = all_text.split("" "")

        for word in temp_text:
            if word.isalpha():
                temp_text[temp_text.index(word)] = word.lower()

        # temp_text = [word for word in temp_text if word not in stopwords.words('english')]
        temp_text = list(filter(None, temp_text))
        temp_text = ' '.join([i for i in temp_text if not i.isdigit()])
        words += temp_text.split("" "")
        temp_post_text.append(temp_text)

    # temp_post_text = list(filter(None, temp_post_text))

    dictionary = Counter(words)
    # deleting spaces
    # del dictionary[""""]
    sorted_split_words = sorted(dictionary, key=dictionary.get, reverse=True)
    vocab_to_int = {c: i for i, c in enumerate(sorted_split_words,1)}

    message_ints = []
    for message in temp_post_text:
        temp_message = message.split("" "")
        message_ints.append([vocab_to_int[i] for i in temp_message])


    # maximum message length = 6577

    # message_lens = Counter([len(x) for x in message_ints])AAA

    seq_length = 6577
    num_messages = len(temp_post_text)
    features = np.zeros([num_messages, seq_length], dtype=int)
    for i, row in enumerate(message_ints):
        # print(features[i, -len(row):])
        # features[i, -len(row):] = np.array(row)[:seq_length]
        features[i, :len(row)] = np.array(row)[:seq_length]
        # print(features[i])

    lb = LabelBinarizer()
    lbl = newsgroups_data.target
    labels = np.reshape(lbl, [-1])
    labels = lb.fit_transform(labels)

    sequence_lengths = [len(msg) for msg in message_ints]
    return features, labels, len(sorted_split_words)+1, sequence_lengths


def get_batches(x, y, sql, batch_size=1):
    for ii in range(0, len(y), batch_size):
        yield x[ii:ii + batch_size], y[ii:ii + batch_size], sql[ii:ii+batch_size]


def plot(noOfWrongPred, dataPoints):
    font_size = 14
    fig = plt.figure(dpi=100,figsize=(10, 6))
    mplt.rcParams.update({'font.size': font_size})
    plt.title(""Distribution of wrong predictions"", fontsize=font_size)
    plt.ylabel('Error rate', fontsize=font_size)
    plt.xlabel('Number of data points', fontsize=font_size)

    plt.plot(dataPoints, noOfWrongPred, label='Prediction', color='blue', linewidth=1.8)
    # plt.legend(loc='upper right', fontsize=14)

    plt.savefig('distribution of wrong predictions.png')
    # plt.show()



def train_test():
    features, labels, n_words, sequence_length = pre_process()

    print(features.shape)
    print(labels.shape)

    # Defining Hyperparameters

    lstm_layers = 1
    batch_size = 1
    lstm_size = 200
    learning_rate = 0.01

    # --------------placeholders-------------------------------------

    # Create the graph object
    graph = tf.Graph()
    # Add nodes to the graph
    with graph.as_default():

        tf.set_random_seed(1)

        inputs_ = tf.placeholder(tf.int32, [None, None], name=""inputs"")
        # labels_ = tf.placeholder(dtype= tf.int32)
        labels_ = tf.placeholder(tf.float32, [None, None], name=""labels"")
        sql_in = tf.placeholder(tf.int32, [None], name= 'sql_in')

        # output_keep_prob is the dropout added to the RNN's outputs, the dropout will have no effect on the calculation of the subsequent states.
        keep_prob = tf.placeholder(tf.float32, name=""keep_prob"")

        # Size of the embedding vectors (number of units in the embedding layer)
        embed_size = 300

        # generating random values from a uniform distribution (minval included and maxval excluded)
        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1),trainable=True)
        embed = tf.nn.embedding_lookup(embedding, inputs_)

        print(embedding.shape)
        print(embed.shape)
        print(embed[0])

        # Your basic LSTM cell
        lstm =  tf.contrib.rnn.BasicLSTMCell(lstm_size)


        # Add dropout to the cell
        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)

        # Stack up multiple LSTM layers, for deep learning
        cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)

        # Getting an initial state of all zeros
        initial_state = cell.zero_state(batch_size, tf.float32)

        outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state, sequence_length=sql_in)

        # hidden layer
        hidden = tf.layers.dense(outputs[:, -1], units=25, activation=tf.nn.relu)

        print(hidden.shape)

        logit = tf.contrib.layers.fully_connected(hidden, num_outputs=20, activation_fn=None)

        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=labels_))

        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

        saver = tf.train.Saver()

    # ----------------------------online training-----------------------------------------

    with tf.Session(graph=graph) as sess:
        tf.set_random_seed(1)
        sess.run(tf.global_variables_initializer())
        iteration = 1
        state = sess.run(initial_state)
        wrongPred = 0
        noOfWrongPreds = []
        dataPoints = []

        for ii, (x, y, sql) in enumerate(get_batches(features, labels, sequence_length, batch_size), 1):

            feed = {inputs_: x,
                    labels_: y,
                    sql_in : sql,
                    keep_prob: 0.5,
                    initial_state: state}

            predictions = tf.nn.softmax(logit).eval(feed_dict=feed)

            print(""----------------------------------------------------------"")
            print(""sez: "",sql)
            print(""Iteration: {}"".format(iteration))

            isequal = np.equal(np.argmax(predictions[0], 0), np.argmax(y[0], 0))

            print(np.argmax(predictions[0], 0))
            print(np.argmax(y[0], 0))

            if not (isequal):
                wrongPred += 1

            print(""nummber of wrong preds: "",wrongPred)

            if iteration%50 == 0:
                noOfWrongPreds.append(wrongPred/iteration)
                dataPoints.append(iteration)

            loss, states, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)

            print(""Train loss: {:.3f}"".format(loss))
            iteration += 1

        saver.save(sess, ""checkpoints/sentiment.ckpt"")
        errorRate = wrongPred / len(labels)
        print(""ERRORS: "", wrongPred)
        print(""ERROR RATE: "", errorRate)
        plot(noOfWrongPreds, dataPoints)


if __name__ == '__main__':
    train_test()
</code></pre>
","python-3.x, tensorflow, machine-learning, lstm, text-classification","<p>It seems that your model learns nothing and only do the random guessing. I have below provided few suggestions (however may not be the exact reason for the random guessing),</p>

<ol>
<li>Masking the Cost Function :</li>
</ol>

<p>As explained here: <a href=""https://danijar.com/variable-sequence-lengths-in-tensorflow/"" rel=""nofollow noreferrer"">https://danijar.com/variable-sequence-lengths-in-tensorflow/</a>,  it is a good practice to consider only the actual sequence length when you calculating the loss rather than averaging over the padded sequence length.</p>

<p>Following explanation is extracted from the above source :</p>

<p><strong>Note that our output will still be of size batch_size x max_length x out_size, but with the last being zero vectors for sequences shorter than the maximum length. When you use the outputs at each time step, as in sequence labeling, we don’t want to consider them in our cost function. We mask out the unused frames and compute the mean error over the sequence length by dividing by the actual length. Using tf.reduce_mean() does not work here because it would devide by the maximum sequence length.</strong></p>

<ol start=""2"">
<li>stacking multiple cells :</li>
</ol>

<p>Following code snippet  stacks the same copy of the lstm cell rather than different instances, </p>

<pre><code>    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)
</code></pre>

<p>More detail explation can be found here:<a href=""https://stackoverflow.com/questions/47371608/cannot-stack-lstm-with-multirnncell-and-dynamic-rnn/47376568#47376568"">Cannot stack LSTM with MultiRNNCell and dynamic_rnn</a></p>

<ol start=""3"">
<li>Batch Size:</li>
</ol>

<p>You are using batch size = 1, which is the stochastic gradient descent approach. Therefore, try to increase your batch size (mini batch gradient descent approach) that would be less noisy and have faster convergence properties.</p>

<ol start=""4"">
<li>Try few epochs and see how the loss and the accuracy changes:</li>
</ol>

<p>This would give you a good understanding of how your model behaves.</p>

<p>Hope this helps.</p>
",1,0,385,2018-09-21 06:55:22,https://stackoverflow.com/questions/52438193/prediction-giving-same-value-in-every-iteration-in-an-online-multiclass-classifi
Text Classification + Naive Bayes + Scikit learn,"<p>I am going to do Text classification first time with Naive Bayes.
This code I found on <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a> :</p>

<pre><code>&gt;&gt;&gt; from sklearn.naive_bayes import MultinomialNB
&gt;&gt;&gt; clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)
</code></pre>

<p>I want to resolve one doubt about the parameters <code>X_train_tfidf</code>, <code>twenty_train.target</code> passed to the function fit().</p>

<p>X_train_tfidf is the tfidf vector representation of all the documents in the train set.</p>

<p>twenty_train.target is the corresponding labels of documents <strong>in the exact order</strong> as they appear in the X_train_tfidf set.</p>

<p>Am I correct?</p>
","scikit-learn, text-classification, naivebayes","<p>Short answer: Yes</p>

<p>Long answer: This is true for every <strong>fit</strong> method you will find using the API. Given a <strong>matrix</strong> of documents <strong>X</strong> with dimensions [m, n], the target <strong>vector Y</strong> will have dimension [n, 1] and <strong>document X[:, j] matches target Y[j] for every j from 0 to n-1</strong>. </p>

<p>If documents and targets don't match you will probably get a very poor and unreasonable result from your training process.</p>
",1,1,136,2018-09-25 13:20:06,https://stackoverflow.com/questions/52499233/text-classification-naive-bayes-scikit-learn
"Text classification + Naive Bayes + Python : Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;)","<p>I am trying to do text classification with Naive Bayes. This is my code:</p>

<pre><code>#splitting Pandas dataframe into train set and test set

x_train, x_test, y_train, y_test = cross_validation.train_test_split(data['description'], data['category_id'], test_size=0.2, random_state=42)

#production of bag of words from x_train

count_vect = CountVectorizer()
x_train_counts = count_vect.fit_transform(x_train)
train_vocab = count_vect.get_feature_names()

#training the Naive Bayes classifier

clf = MultinomialNB().fit(x_train_counts, y_train)
</code></pre>

<p>Error:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-46-0cb3dc7193bf&gt; in &lt;module&gt;()
      1 #training the Naive Bayes classifier
      2 
----&gt; 3 clf = MultinomialNB().fit(x_train_counts, y_train)

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/naive_bayes.py in fit(self, X, y, sample_weight)
    577             Returns self.
    578         """"""
--&gt; 579         X, y = check_X_y(X, y, 'csr')
    580         _, n_features = X.shape
    581 

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    577     else:
    578         y = column_or_1d(y, warn=True)
--&gt; 579         _assert_all_finite(y)
    580     if y_numeric and y.dtype.kind == 'O':
    581         y = y.astype(np.float64)

~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py in _assert_all_finite(X)
     42             and not np.isfinite(X).all()):
     43         raise ValueError(""Input contains NaN, infinity""
---&gt; 44                          "" or a value too large for %r."" % X.dtype)
     45 
     46 

ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
</code></pre>

<p>The type of x_train_counts is scipy.sparse.csr.csr_matrix.</p>

<pre><code>print(type(x_train_counts))
&lt;class 'scipy.sparse.csr.csr_matrix'&gt;
</code></pre>

<p>The type of y_train is pandas.core.series.Series.</p>

<pre><code>print(type(y_train))
&lt;class 'pandas.core.series.Series'&gt;
</code></pre>
","python, numpy, scikit-learn, text-classification, naivebayes","<p>I suspect the issue is related to your <code>data['description']</code> and <code>data['category_id']</code>. Is the first one something like an array with <strong>n</strong> elements comprising of texts and the second another array like object also with <strong>n</strong> elements consisting of labels for for the first, e.g, <code>['0', '1', '3', ...]</code>?</p>

<p>As a test, only by replacing your data with some sklearn dataset would produce a correct run:</p>

<pre><code>from sklearn.datasets import fetch_20newsgroups

categories = ['alt.atheism', 'soc.religion.christian',
               'comp.graphics', 'sci.med']

dataset = fetch_20newsgroups(subset='train',
     categories=categories, shuffle=True, random_state=42)

x_train, x_test, y_train, y_test = cross_validation.train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=42)

#production of bag of words from x_train

count_vect = CountVectorizer()
x_train_counts = count_vect.fit_transform(x_train)
train_vocab = count_vect.get_feature_names()

#training the Naive Bayes classifier

clf = MultinomialNB().fit(x_train_counts, y_train)
</code></pre>

<p>Try to test that out and let me know if it helps.</p>
",1,0,1131,2018-09-27 21:47:38,https://stackoverflow.com/questions/52545600/text-classification-naive-bayes-python-input-contains-nan-infinity-or-a-v
How to classify text documents in legal domain,"<p>I've been working on a project which is about <strong>classifying text documents in the legal domain</strong> <em>(Legal Judgment Prediction class of problems)</em>.<br>
The given data set consists of 700 legal documents (well balanced in two classes). After the preprocessing, which consists in applying all the best practices (such as deleting stopwords,etc.), there are 3 paragraphs for each document, which I could consider all together or separately. On average, the text documents size is 2285 words.</p>

<p>I aim to use something different from the classical n-grams model (which doesn't take into account any words order or semantic) :</p>

<ul>
<li>Using a Neural Network (<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2Vec</a>) for transforming <strong>the text of each document into a vector</strong> in a continuous domain; in order to create a dataset with the vectors, representing the documents, and the corresponding labels (as I said there are 2 possible labels: 0 or 1);</li>
<li>Training a SVM for classifying the samples, I've been using a 10-fold cross-validation.</li>
</ul>

<p>I was wondering if there's someone who has some experience in this particular domain, who can suggest me other ways or how to improve the model since I'm not getting particularly good results: 74% accuracy. </p>

<p><strong>Is it correct using Doc2Vec for transforming text into vectors and using them for feeding a classifier?</strong></p>

<p>My model represantation:</p>

<p><a href=""https://i.sstatic.net/UKXE4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UKXE4.png"" alt=""enter image description here""></a></p>
","python, svm, text-classification, word-embedding, doc2vec","<p>Doc2Vec is a reasonable way to tranform a variable-length text into a summary-vector, and these vectors are often useful for classification – especially topical or sentiment classification (two applications highlighted in the original 'Paragraph Vector' paper). </p>

<p>However, 700 docs is extremely small as a training set. Published work has tended to use corpuses of tens-of-thousands to millions of documents. </p>

<p>Also, your specific classification target – predicting a legal judgment – strikes me as much harder than topical or sentiment classification. Knowing how a case will be decided depends on a large body of outside law/precedent (that's not in the training-set), and logical deductions, sometimes on individual fine points of a situation. Those are things the fuzzy-summary of a single-text-vector are unlikely to capture. </p>

<p>Against that, your reported 74% accuracy sounds downright impressive. (Would a lay person do as well, with just these summaries?) I wonder if there are certain 'tells' in the summaries – with word choices of the summarizer strongly hinting, or downright revealing, the actual judgment. If that's the strongest signal in the text (barring actual domain knowledge &amp; logical reasoning), you might get just-as-good results from a more simple n-grams/bag-of-words representation and classifier. </p>

<p>Meta-optimizing your training parameters might incrementally improve results, but I'd think you'd need a lot more data, and perhaps far more advanced learning techniques, to really approximate the kind of legally-competent human-level predictions you may be aiming for. </p>
",1,2,456,2018-10-01 12:49:39,https://stackoverflow.com/questions/52591572/how-to-classify-text-documents-in-legal-domain
Multiclass Text Classification in Python,"<p>I am trying to create a Multiclass Text Classifier as explained <a href=""https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5"" rel=""nofollow noreferrer"">here</a>. However, my code is breaking at line:</p>

<pre><code>NB_pipeline.fit(X_train, train[category])
</code></pre>

<p>Below is the error which I am getting:</p>

<pre><code>File ""pandas\hashtable.pyx"", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas\hashtable.c:12322)
</code></pre>

<p>I tried to find out what <code>train[category]</code> returns and I got same error. </p>

<p>1) <code>X_train</code> is a <code>dataframe</code> with one column and contains customer feedback. </p>

<p>2) <code>train</code> is a <code>dataframe</code> with two columns; first column contains customer review(same as <code>X_train</code>) and second column contains one of the 5 categories (<code>Systems Error, Proactive Communication, Staff Behaviour, Website Functionalities, Others</code>).</p>

<p>3) <code>category</code> is one of the above mentioned categories.</p>

<p>Below is the sample train <code>dataframe</code>:</p>

<pre><code>Index           Feedback                                    Category
  0           While making payment got system error.         System error
              Staff behaviour was good at hotel

  1           While making payment got system error.         Staff Behaviour
              Staff behaviour was good at hotel
</code></pre>
","python, text-classification, multiclass-classification","<p>This is one of the most over-looked issue. </p>

<p>The reason for this error is that the ""column"" script is looking for is not available in the dataframe. All the 5 categories you have, should be columns in the input dataframe and rows will take 1/0 if one of the categories is applicable for the feedback/comment. Ideally, Your input dataframe should look like this.</p>

<pre><code>Index           Feedback                                  System error    Staff Behaviour
  0           While making payment got system error.         1                  1
              Staff behaviour was good at hotel

  1           While making payment got system error.         1                  0

  2           Staff behaviour was good at hotel              0                  1
</code></pre>

<p>I have used same comment to show how input dataframe should look like.</p>
",2,1,310,2018-10-04 08:36:28,https://stackoverflow.com/questions/52642417/multiclass-text-classification-in-python
tf-idf sickitlearn separate &quot;word&quot; from word,"<p>I am working with a problem in text classification where If a word was found in this format <strong>""word""</strong> it will have a different importance from if found in this format <strong>word</strong>  so I tried this code </p>

<pre><code>    import re
    from sklearn.feature_extraction.text import CountVectorizer
    sent1 = ""The cat sat on my \""face\"" face""
    sent2 = ""The dog sat on my bed""
    content = [sent1,sent2]
    vectorizer = CountVectorizer(token_pattern=r""(?u)\b\w\w+\b|!|\?|\""|\'"")
    vectorizer.fit(content)
    print (vectorizer.get_feature_names()) 
</code></pre>

<p>The result was </p>

<pre><code>    ['""', 'bed', 'cat', 'dog', 'face', 'my', 'on', 'sat', 'the']
</code></pre>

<p>Where I wished it would be</p>

<pre><code>    ['bed', 'cat', 'dog', 'face','""face""' 'my', 'on', 'sat', 'the']
</code></pre>
","python, text-classification, tf-idf","<p>Your token pattern is</p>

<pre><code>token_pattern=r""(?u)\b\w\w+\b|!|\?|\""|\'""
</code></pre>

<p>which is looking for words (\b\w\w+\b) or exclamation marks, or question marks, or quotation marks. Try something like</p>

<pre><code>token_pattern=r""(?u)\b\w\w+\b|\""\b\w\w+\b\""|!|\?|\'""
</code></pre>

<p>Note the part</p>

<pre><code>\""\b\w\w+\b\""
</code></pre>

<p>which looks for a word surrounded by quotes. </p>
",2,2,149,2018-10-04 09:20:07,https://stackoverflow.com/questions/52643239/tf-idf-sickitlearn-separate-word-from-word
How to find similar noun phrases in NLP?,"<p>Is there a way to identify similar noun phrases. Some suggest use pattern-based approaches, for example <code>X as Y</code> expressions:</p>
<blockquote>
<p>Usain Bolt as Sprint King</p>
<p>Liverpool as Reds</p>
</blockquote>
","nlp, text-classification, synonym, named-entity-recognition, pattern-synonyms","<p>There are many techniques to find alternative names for a given entity,
using patterns such as: </p>

<ul>
<li><code>X also known as Y</code> </li>
<li><code>X also titled as Y</code></li>
</ul>

<p>and scanning large collections of documents (e.g., Wikipedia or news papers articles) is one way to do it. </p>

<p>There are also other alternatives, one I remember is using Wikipedia inter-links structure, for instance, by exploring the redirect links between articles. You can download a file with a list of redirects from here: <a href=""https://wiki.dbpedia.org/Downloads2015-04"" rel=""nofollow noreferrer"">https://wiki.dbpedia.org/Downloads2015-04</a> and exploring the file you can find alternative names/synonyms for entities, e.g.:</p>

<ul>
<li><code>Kennedy_Centre -&gt; John_F._Kennedy_Center_for_the_Performing_Arts&gt;</code></li>
<li><code>Lord_Alton_of_Liverpool -&gt; David_Alton,_Baron_Alton_of_Liverpool</code></li>
<li><code>Indiana_jones_2 -&gt; Indiana_Jones_and_the_Temple_of_Doom</code></li>
</ul>

<p>Another thing you can do is combine these two techniques, for instance, look for text segments where both <code>Indiana Jones</code> and <code>Indiana_Jones_and_the_Temple_of_Doom</code> occur and are not further apart more than, let's say, 4 or 5 tokens. You might find patterns like <code>also titled as</code>, then you can use these patterns to find more synonyms/alternative names.</p>
",2,3,354,2018-10-27 20:10:17,https://stackoverflow.com/questions/53025861/how-to-find-similar-noun-phrases-in-nlp
&quot;ValueError: The shape of the input to &quot;Flatten&quot; is not fully defined&quot; with variable length LSTM,"<p>Here's my code:</p>

<pre><code>    from keras.layers import LSTM, Bidirectional, Dense, Input, Flatten
    from keras.models import Model

    input = Input(shape=(None, 100))
    lstm_out = Bidirectional(LSTM(10, return_sequences=True))(input)
    something = Flatten()(lstm_out)
    output = Dense(22, activation='softmax')(something)

    model = Model(inputs=input, outputs=output)
    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])
</code></pre>

<p>I'm building an LSTM with variable input through 
<a href=""https://stackoverflow.com/questions/38189070/how-do-i-create-a-variable-length-input-lstm-in-keras"">this stackoverflow question</a>. But now my model is saying <code>ValueError: The shape of the input to ""Flatten"" is not fully defined (got (None, 20)</code>. How can I fix this?</p>

<p>Thanks in advance</p>
","python, keras, lstm, text-classification, variable-length","<p>You can't <em>fix</em> this particular problem because you can pass a variable size vector to a <code>Dense</code> layer. <em>Why?</em> Because it has a fixed size weights matrix, i.e. the kernel <code>W</code>.</p>

<p>You should instead look at layers that can handle variable length sequences such as RNNs. For example you can let the LSTM learn a representation over the entire sequence:</p>

<pre><code>input = Input(shape=(None, 100))
lstm_out = Bidirectional(LSTM(10))(input) # the LSTM produces a single fixed size vector
output = Dense(22, activation='softmax')(lstm_out) # Dense classifies it
</code></pre>

<p>If you want more capacity in your model you can chain RNN layers so long as the last one doesn't return sequences:</p>

<pre><code>lstm_out = Bidirectional(LSTM(10, return_sequences=True))(input)
lstm_out = Bidirectional(LSTM(10))(lstm_out) # this LSTM produces a single vector
</code></pre>
",1,2,871,2018-10-28 04:28:42,https://stackoverflow.com/questions/53028377/valueerror-the-shape-of-the-input-to-flatten-is-not-fully-defined-with-vari
What is the numpy way to conditionally merge arrays?,"<p>I have two numpy arrays <code>(1000,)</code> filled with predictions from two models:</p>

<pre><code>pred_1 = model_1.predict(x_test)
pred_2 = model_2.predict(x_test)
</code></pre>

<p><code>model_1</code> is attractive due to extremely low <code>FP</code>, but consequently high <code>FN</code>.</p>

<p><code>model_2</code> is attractive due to overall accuracy and recall.</p>

<p>How can I <em>conditionally</em> apply predictions to take advantage of these strengths and weaknesses?</p>

<p>I'd like to take all positive (<code>1</code>) predictions from the first model, and let the second model deal with the rest.</p>

<p>Essentially I'm looking for something like this:</p>

<pre><code>final_pred = model_1.predict() if model_1.predict() &gt; 0.5 else model_2.predict()
</code></pre>

<p>This fails: The truth value of an array with more than one element is ambiguous.</p>

<p>What is the numpy way to combine these arrays as above?</p>
","python, numpy, text-classification","<p>You're looking for <a href=""https://www.numpy.org/devdocs/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>numpy.where</code></a>:</p>

<pre><code>a = model_1.predict(x_test)
b = model_2.predict(x_test)

out = np.where(a &gt; 0.5, a, b)
</code></pre>
",3,1,1276,2018-11-03 23:49:31,https://stackoverflow.com/questions/53136542/what-is-the-numpy-way-to-conditionally-merge-arrays
How to use class Imbalance technique (SMOTE) with Java Weka API?,"<p>I am trying to build classification model using Java Weka API. My training data set have class imbalance problem. For this reasons, I want to use class imbalance techniques like SMOTE to reduce the class imbalance problem. </p>

<p>Source Code are below:</p>

<pre><code>package classification;
import java.util.Random;
import weka.classifiers.Classifier;
import weka.classifiers.bayes.NaiveBayesMultinomial;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.converters.ConverterUtils.DataSource;
import weka.filters.Filter;
import weka.filters.unsupervised.attribute.StringToWordVector;
public class questStackoverflow {
public static void main(String agrs[]) throws Exception{
String fileRootPath = ""../file.arff""; //Dataset
    Instances strdata = DataSource.read(fileRootPath); //Load Dataset
    StringToWordVector filter = new StringToWordVector(10000);
    filter.setInputFormat(strdata);
    String[] options = { ""-W"", ""10000"", ""-L"", ""-M"", ""1"",
            ""-stemmer"", ""weka.core.stemmers.IteratedLovinsStemmer"", 
            ""-stopwords-handler"", ""weka.core.stopwords.Rainbow"", 
            ""-tokenizer"", ""weka.core.tokenizers.AlphabeticTokenizer"" 
            };
    filter.setOptions(options);
    filter.setIDFTransform(true);
    Instances data = Filter.useFilter(strdata,filter); //Apply filter
    data.setClassIndex(0); //set class index        
    double recall=0.0;
    double precision=0.0;
    double fmeasure=0.0;
    double tp, fp, fn, tn;

    Classifier classifier = null;
    classifier = new NaiveBayesMultinomial(); //classifer

    int folds = 10;         
    Random random = new Random(1);
    data.randomize(random);
    data.stratify(folds);
    tp = fp = fn = tn = 0;
    for (int i = 0; i &lt; folds; i++) {
       Instances trains = data.trainCV(folds, i,random); //training dataset
       Instances tests = data.testCV(folds, i); //testing dataset
        classifier.buildClassifier(trains);    //build classifier           
        for (int j = 0; j &lt; tests.numInstances(); j++) {    
           Instance instance = tests.instance(j);
           double classValue = instance.classValue();                   
           double result = classifier.classifyInstance(instance);
            if (result == 0.0 &amp;&amp; classValue == 0.0) {
                    tp++;
                } else if (result == 0.0 &amp;&amp; classValue == 1.0) {
                    fp++;
                } else if (result == 1.0 &amp;&amp; classValue == 0.0) {
                    fn++;
                } else if (result == 1.0 &amp;&amp; classValue == 1.0) {
                    tn++;
                }
            }   
        }

        if (tn + fn &gt; 0)
            precision = tn / (tn + fn);
        if (tn + fp &gt; 0)
            recall = tn / (tn + fp);
        if (precision + recall &gt; 0)
            fmeasure = 2 * precision * recall / (precision + recall);
        System.out.println(""Precision: "" + precision);
        System.out.println(""Recall: "" + recall);
        System.out.println(""Fmeasure: "" + fmeasure);

    }

}
</code></pre>

<p>My code is work well without class imbalance techniques. But, I need to use class imbalance techniques to mitigate class imbalance problem. But, I do not know how to use it in Java Weka API.</p>
","java, weka, text-classification","<p>You can add the following lines of code with your code:</p>

<pre><code>weka.filters.supervised.instance.SMOTE


SMOTE smote=new SMOTE();
smote.setInputFormat(trains);       
Instances Trains_smote= Filter.useFilter(trains, smote);
</code></pre>

<p>Your code will be the following.</p>

<pre><code>package classification;
import java.util.Random;
import weka.classifiers.Classifier;
import weka.classifiers.bayes.NaiveBayesMultinomial;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.converters.ConverterUtils.DataSource;
import weka.filters.Filter;
import weka.filters.unsupervised.attribute.StringToWordVector;
weka.filters.supervised.instance.SMOTE
public class questStackoverflow {
public static void main(String agrs[]) throws Exception{
String fileRootPath = ""../file.arff""; //Dataset
Instances strdata = DataSource.read(fileRootPath); //Load Dataset
StringToWordVector filter = new StringToWordVector(10000);
filter.setInputFormat(strdata);
String[] options = { ""-W"", ""10000"", ""-L"", ""-M"", ""1"",
        ""-stemmer"", ""weka.core.stemmers.IteratedLovinsStemmer"", 
        ""-stopwords-handler"", ""weka.core.stopwords.Rainbow"", 
        ""-tokenizer"", ""weka.core.tokenizers.AlphabeticTokenizer"" 
        };
filter.setOptions(options);
filter.setIDFTransform(true);
Instances data = Filter.useFilter(strdata,filter); //Apply filter
data.setClassIndex(0); //set class index        
double recall=0.0;
double precision=0.0;
double fmeasure=0.0;
double tp, fp, fn, tn;

Classifier classifier = null;
classifier = new NaiveBayesMultinomial(); //classifer

int folds = 10;         
Random random = new Random(1);
data.randomize(random);
data.stratify(folds);
tp = fp = fn = tn = 0;
for (int i = 0; i &lt; folds; i++) {
   Instances trains = data.trainCV(folds, i,random); //training dataset
   Instances tests = data.testCV(folds, i); //testing dataset
   SMOTE smote=new SMOTE();
   smote.setInputFormat(trains);        
   Instances Trains_smote = Filter.useFilter(trains, smote);

    classifier.buildClassifier(Trains_smote);    //build classifier           
    for (int j = 0; j &lt; tests.numInstances(); j++) {    
       Instance instance = tests.instance(j);
       double classValue = instance.classValue();                   
       double result = classifier.classifyInstance(instance);
        if (result == 0.0 &amp;&amp; classValue == 0.0) {
                tp++;
            } else if (result == 0.0 &amp;&amp; classValue == 1.0) {
                fp++;
            } else if (result == 1.0 &amp;&amp; classValue == 0.0) {
                fn++;
            } else if (result == 1.0 &amp;&amp; classValue == 1.0) {
                tn++;
            }
        }   
    }

    if (tn + fn &gt; 0)
        precision = tn / (tn + fn);
    if (tn + fp &gt; 0)
        recall = tn / (tn + fp);
    if (precision + recall &gt; 0)
        fmeasure = 2 * precision * recall / (precision + recall);
    System.out.println(""Precision: "" + precision);
    System.out.println(""Recall: "" + recall);
    System.out.println(""Fmeasure: "" + fmeasure);

}
</code></pre>

<p>}</p>
",2,3,1371,2018-11-07 00:30:15,https://stackoverflow.com/questions/53182077/how-to-use-class-imbalance-technique-smote-with-java-weka-api
"Getting different accuracy on each run of Random Forest, Non-Linear SVC and Multinomial NB in python for text classification","<p>I am working on a binary text classification problem in python, and have developed models in Random Forest, Non-Linear SVC &amp; Multinomial NB.</p>

<p>But on each run, of these respective models, am getting different accuracy &amp; confusion matrix parameters on the test set. I have used random_state parameter in train_test_split and while initializing each of these models. Random.Seed is also added in the code.</p>

<p>Is there anything else I am missing? </p>

<p>Thanks.</p>

<p><strong>Code Sample:</strong></p>

<pre><code>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, stratify= Y, random_state = 42) 

tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words = 'english', max_df = 0.8, min_df = 0.05, ngram_range=(1,3)) 
tfidf_train = tfidf_vectorizer.fit_transform(X_train) 
tfidf_test = tfidf_vectorizer.transform(X_test) #Default Hyperparameters 

rfc = RandomForestClassifier(random_state = 42) 

rfc.fit(tfidf_train,Y_train) 
predictions = rfc.predict(tfidf_test) 

score = metrics.accuracy_score(Y_test, predictions) # get scores

print(""accuracy: %0.3f"" % score) #printing score
</code></pre>
","python, machine-learning, classification, random-forest, text-classification","<p>Some of the utility you used might be contain some hidden <strong>random action</strong>, uncertainty.</p>

<p>As some of the libraries use <a href=""https://docs.scipy.org/doc/numpy-1.15.1/reference/routines.random.html"" rel=""nofollow noreferrer"">numpy.random()</a> instead of <a href=""https://docs.python.org/3.6/library/random.html"" rel=""nofollow noreferrer"">random.random()</a> you should use <code>numpy.random.seed()</code>.</p>
",0,1,335,2018-11-14 14:57:44,https://stackoverflow.com/questions/53303064/getting-different-accuracy-on-each-run-of-random-forest-non-linear-svc-and-mult
Keras: tweets classification,"<p>Hello dear forum members,</p>

<p>I have a data set of 20 Million randomly collected individual tweets (no two tweets come from the same account). Let me refer to this data set as ""general"" data set. Also, I have another ""specific"" data set that includes 100,000 tweets collected from drug (opioid) abusers. Each tweet has at least one tag associated with it, e.g., opioids, addiction, overdose, hydrocodone, etc. (max 25 tags).</p>

<p>My goal is to use the ""specific"" data set to train the model using Keras and then use it to tag tweets in the ""general"" data set to identify tweets that might have been written by drug abusers.</p>

<p>Following examples in <a href=""https://cloud.google.com/blog/products/gcp/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts"" rel=""nofollow noreferrer"">source1</a> and <a href=""http://%20%20%20%20https://datascience.stackexchange.com/questions/38280/keras-multiple-text-features-input-and-single-text-label-output-classification"" rel=""nofollow noreferrer"">source2</a>, I managed to build a simple working version of such model:</p>

<pre><code>from tensorflow.python import keras
import pandas as pd
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.metrics import confusion_matrix
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.preprocessing import text, sequence
from keras import utils

# load opioid-specific data set, where post is a tweet and tags is a single tag associated with a tweet
# how would I include multiple tags to be used in training?
data = pd.read_csv(""filename.csv"")
train_size = int(len(data) * .8)
train_posts = data['post'][:train_size]
train_tags = data['tags'][:train_size]
test_posts = data['post'][train_size:]
test_tags = data['tags'][train_size:]

# tokenize tweets
vocab_size = 100000 # what does vocabulary size really mean?
tokenize = text.Tokenizer(num_words=vocab_size)
tokenize.fit_on_texts(train_posts)
x_train = tokenize.texts_to_matrix(train_posts)
x_test = tokenize.texts_to_matrix(test_posts)

# make sure columns are strings
data['post'] = data['post'].astype(str)
data['tags'] = data['tags'].astype(str)

# labeling
# is this where I add more columns with tags for training?
encoder = LabelBinarizer()
encoder.fit(train_tags)
y_train = encoder.transform(train_tags)
y_test = encoder.transform(test_tags)

# model building
batch_size = 32
model = Sequential()
model.add(Dense(512, input_shape=(vocab_size,)))
model.add(Activation('relu'))
num_labels = np.max(y_train) + 1 #what does this +1 really mean?
model.add(Dense(1865))
model.add(Activation('softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size = batch_size, epochs = 5, verbose = 1, validation_split = 0.1)

# test prediction accuracy
score = model.evaluate(x_test, y_test, 
                       batch_size=batch_size, verbose=1)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# make predictions using a test set
for i in range(1000):    
    prediction = model.predict(np.array([x_test[i]]))
text_labels = encoder.classes_ 
predicted_label = text_labels[np.argmax(prediction[0])]
print(test_posts.iloc[i][:50], ""..."")
print('Actual label:' + test_tags.iloc[i])
print(""Predicted label: "" + predicted_label)
</code></pre>

<p>In order to move forward, I would like to clarify a few things:</p>

<ol>
<li>Let's say all my training tweets have a single tag -- opioids. Then if I pass the non-tagged tweets through it, isn't it likely that the model simply tags all of them as opioids as it doesn't know anything else? Should I be using a variety of different tweets/tags then for the learning purpose? Perhaps, there are any general guidelines for the selection of the tweets/tags for the training purposes?</li>
<li>How can I add more columns with tags for training (not a single one like is used in the code)?</li>
<li>Once I train the model and achieve appropriate accuracy, how do I pass non-tagged tweets through it to make predictions?</li>
<li>How do I add a confusion matrix?</li>
</ol>

<p>Any other relevant feedback is also greatly appreciated.</p>

<p>Thanks!</p>

<p>Examples of ""general"" tweets:</p>

<pre><code>everybody messages me when im in class but never communicates on the weekends like this when im free. feels like that anyway lol.
i woke up late, and now i look like shit. im the type of person who will still be early to whatever, ill just look like i just woke up.
</code></pre>

<p>Examples of ""specific"" tweets:</p>

<pre><code>$2 million grant to educate clinicians who prescribe opioids
early and regular marijuana use is associated with use of other illicit drugs, including opioids
</code></pre>
","python, machine-learning, keras, text-classification, tweets","<p>My shot to this is:</p>

<ol>
<li><p>Create a new dataset with tweets from general + specific data. Let's say 200k-250K where 100K is you specific data set, rest is general</p></li>
<li><p>Take your 25 keywords/tags and write a rule if any one or more than one exists in a tweet it is DA (Drug Abuser) or NDA(Non Drug Abuser). This will be your dependent variable.</p></li>
<li><p>Your new dataset will be one column with all the tweets and another column with the dependent variable telling it is DA or NDA</p></li>
<li><p>Now divide into train/test and use keras or any other algo. so that it can learn. </p></li>
<li><p>Then test the model by plotting Confusion Matrix</p></li>
<li><p>Passs you other remaining data set from General to this model and check,</p></li>
</ol>

<p>If their are new words other than 25 which are not in the specific dataset, from the model you built it will still try to intelligently guess the right category by the group of words that come together, tone etc.</p>
",1,1,310,2018-11-14 18:29:08,https://stackoverflow.com/questions/53306630/keras-tweets-classification
Reusing an sklearn text classification model with tf-idf feature selection,"<p>I'm relatively new to sk-learn &amp; machine learning here so forgive any possible ignorance. I'm making a model to classify assets based on a text description (in python). There is only one predictor (the text) and one predicted (the category) variable. For the labels, I am factorizing the categories, there are about 30, so each is represented by a number from 0 to 29. For the features, I'm using a tf-idf score. The modeling and accuracy are fine and I'm saving the model using a pickle dump.</p>

<p>However, the model needs to be reusable, so it must be able to load again at some point in time in order to label a new set of data. Please see the code for the saving/loading of the model below.</p>

<pre><code>## Save the linear SVC model
filename = 'SVM_model.sav'
pickle.dump(model, open(filename, 'wb'))

## Load model and test prediction accuracy
tfidf = TfidfVectorizer(analyzer='word',sublinear_tf=True, min_df=3, 
norm='l2', ngram_range=(1, 2), encoding='latin-1', stop_words='english')
testdf=pd.read_excel('learning_df.xlsx').tail(54068)
testdf['input']=testdf['item_description'].astype(str)+'- 
'+testdf['category_name_client'].astype(str)
testdf=testdf[['input','category_id_D&amp;P']].dropna()
testdf['factor']=testdf['category_id_D&amp;P'].apply(lambda x: cat_dict[x])
loaded_model = pickle.load(open(filename, 'rb'))
X_test1=tfidf.fit_transform(testdf.input).toarray()
y_test1=testdf['factor']
result = loaded_model.score(X_test1, y_test1)
print(result)
</code></pre>

<p>Please note that the tfidf vectorizer settings are exactly the same as what the model training/validation was done with. The cat_dict is the initial factorization of the categories and here I'm just making sure that the text categories are converted to the same numbers as what the model was trained/validated on.</p>

<p>When I attempted this, I came to this error:</p>

<pre><code>ValueError: X has 24008 features per sample; expecting 20012
</code></pre>

<p>which is understandable because the tf-idf of the new dataset does not produce the same number of features as the training/validation dataset initially used.</p>

<p>So I was wondering if there a workaround for this? Should I not use tf-idf in the first place when training the model? If not, what are alternative ways for feature selection that would not lead to this problem later on?</p>

<p>Thanks in advance and again sorry if I'm missing something obvious.</p>
","python, machine-learning, text-classification, tf-idf, tfidfvectorizer","<p>You can't fit a new vectorizer because you won't be picking the same features as before. You need to stop using <code>.fit_transform()</code> and instead use <code>.fit()</code>, save the vectorizer, and then run the exact same fitted vectorizer on each data set with <code>.transform()</code></p>
",2,2,1073,2018-11-16 16:46:14,https://stackoverflow.com/questions/53342128/reusing-an-sklearn-text-classification-model-with-tf-idf-feature-selection
Making a prediction after training and fitting a RNN Sequential model,"<p>I am trying to get predictions from my sentiment analysis models that classify 500 worded News articles. The models validation loss and training loss is in are about the same and their scores are relatively high. However when I try to make predictions with them I get the same classification result in all of them regardless of the text input. 
I believe that the problem might be on the way I am trying to make a prediction (I pad my string with spaced characters). I was hoping that someone here could shed some light on this issue (my code below). Thank you for your help</p>

<pre><code>comment = 'SAMPLE TEXT STRING'
for i in range(300-len(comment.split(' '))):
    apad += ' A'
comment = comment + apad
tok.fit_on_texts([comment])
X = tokenizer.texts_to_sequences([comment])
X = preprocessing.sequence.pad_sequences(X)
yhat = b.predict_classes(X)
print(yhat)
prediction = b.predict(X, batch_size=None, verbose=0, steps=None)
print(prediction)
</code></pre>

<p>The output of this script is below. Both prediction and predicted classes, are regardless of the text input always 0 for some reason:</p>

<p>[[0]] [[0.00645966]]</p>
","python, tensorflow, keras, neural-network, text-classification","<p>The problem seems to be with the tokenizer. </p>

<p>You can't <code>fit</code> the tokenizer again, because you will have different tokens for each word. You should fit the tokenizer only once before training and then save the tokens to be used with all new text. </p>
",1,0,105,2018-11-29 04:26:08,https://stackoverflow.com/questions/53531824/making-a-prediction-after-training-and-fitting-a-rnn-sequential-model
MutiLabel classification,"<p>I have some 1000 news articles related to science and technology. I need to train a classifier which will predict say 3(computer science, electronics, electrical) confidence scores for each article. 
Each score represents how much the article belongs to each field.
The confidence score will be a value between zero and one.</p>

<p>But the data set doesn't have a training label.  </p>

<p>How do I proceed from here? What kind of data do I need?
How do I train such a model?</p>
","python, machine-learning, deep-learning, text-classification","<p>This is therefore not a classification problem.
It's unsupervised as long as you don't have any label.</p>

<p>What you can do is to look at K-Means (unsupervised Machine Leaning algorithm) that allows you to cluster you data into predefined number of cluster (here 3):</p>

<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans</a></p>

<p>But you won't have any measure to verify the ""ground truth""</p>

<p>If you really want to go further, you can try to label these articles yourself (with let's say computer science, electronics and electrical) and try some supervised algorithms (with <em>scikit-learn</em>) / neural net (with <em>tensorflow</em>).</p>

<p>The idea is to vectorize your input data (you can take a look at TF-IDF) and then try any supervised model.</p>

<p>This is called NLP. You also have libraries that can help you doing this.</p>

<p>NLTK &amp; Spacy are a good start :</p>

<p><a href=""https://www.nltk.org/"" rel=""nofollow noreferrer"">https://www.nltk.org/</a></p>

<p><a href=""https://spacy.io/"" rel=""nofollow noreferrer"">https://spacy.io/</a></p>
",2,2,75,2018-12-07 10:55:30,https://stackoverflow.com/questions/53668101/mutilabel-classification
How can I reduce the RAM utilization in my program that use deeplearning4j (Memory-mapped files and WorkspaceConfiguration)?,"<p>I'm using deeplearning4j but when i load pre-trained model for text-classification I don't have enough RAM on my pc.</p>

<p>I tried to change eclipse.ini file and add more memory changing Xms and Xmx. Unfortunately it doesn't work for me.</p>

<p><a href=""https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory"" rel=""nofollow noreferrer"">https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory</a></p>

<p>In this link seems there is a possible solution to use less RAM even though it cost more time of corse, but I don't care now.</p>

<p>From that link:</p>

<blockquote>
  <p>Memory-mapped files ND4J supports the use of a memory-mapped file
  instead of RAM when using the nd4j-native backend. On one hand, it’s
  slower then RAM, but on other hand, it allows you to allocate memory
  chunks in a manner impossible otherwise.</p>
</blockquote>

<p>Can I add this in a code like this (follow the link)?</p>

<p><a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.java</a></p>

<p>Of cours if there is another way (or a better way) write it. I'll appreciate any advice.</p>

<p>Thanks in advance.</p>
","java, neural-network, deep-learning, text-classification, deeplearning4j","<p>I'm from the deeplearning4j project.  Memory mapped workspaces are made for embeddings yes and should be considered a separate concept from our off heap memory.  The off heap memory is a conceptual rabbit hole I won't cover here (you have to have an understanding of the JVM and the topic isn't relevant here)</p>

<p>The way you would have to use memory mapped workspaces is by loading the word2vec inside a memory mapped scope. 
The first component is the configuration:</p>

<pre><code>import org.nd4j.linalg.api.memory.MemoryWorkspace;
import org.nd4j.linalg.api.memory.conf.WorkspaceConfiguration;
import org.nd4j.linalg.api.memory.enums.LocationPolicy;
WorkspaceConfiguration mmap = WorkspaceConfiguration.builder()
            .initialSize(initialSize)
            .policyLocation(LocationPolicy.MMAP)
            .build();

try (MemoryWorkspace ws =   
           Nd4j.getWorkspaceManager().getAndActivateWorkspace(mmap)) {
 //load your word2vec here            

} 
</code></pre>

<p>Of note with memory mapped workspaces is how it should be used. Mem map is intended only for accessing a large array and pulling subsets of it out from ram.
You should only use it to pull out a subset of the word vectors out you need for doing training.</p>

<p>When using word2vec (or any other embedding technique), the typical pattern is to lookup only the word vectors you want and merge them together in to a mini batch.
That minibatch (and the associated training) should happen in a separate workspace (or have it be unattached which is the default). The reason you can have it unattached is we already do workspaces and the other associated optimizations for  you inside of ComputationGraph and MultiLayerNetwork. Just make sure to pass in whatever you need to fit. </p>

<p>From there, use the INDArray get(..) and put(..) methods to copy the rows you need in to another array that you should use for training.
For more on that see: <a href=""https://deeplearning4j.org/docs/latest/nd4j-overview"" rel=""nofollow noreferrer"">https://deeplearning4j.org/docs/latest/nd4j-overview</a></p>

<p>For more information look at leverage, leverageTo, detach,..  in the INDArray javadoc:
<a href=""https://deeplearning4j.org/api/latest/org/nd4j/linalg/api/ndarray/INDArray.html"" rel=""nofollow noreferrer"">https://deeplearning4j.org/api/latest/org/nd4j/linalg/api/ndarray/INDArray.html</a></p>
",1,0,535,2018-12-07 14:34:57,https://stackoverflow.com/questions/53671623/how-can-i-reduce-the-ram-utilization-in-my-program-that-use-deeplearning4j-memo
"OpenNLP-Document Categorizer- how to classify documents based on status; language of docs not English, also default features?","<p>I want to classify my documents using OpenNLP's Document Categorizer, based on their status: pre-opened, opened, locked, closed etc.</p>

<p>I have 5 classes and I'm using the Naive Bayes algorithm, 60 documents in my training set, and trained my set on 1000 iterations with 1 cut off param.</p>

<p>But no success, when I test them I don't get good results. I was thinking maybe it is because of the language of the documents (is not in English) or maybe I should somehow add the statuses as features. I have set the default features in the categorizer, and also I'm not very familiar with them.</p>

<p>The result should be locked, but its categorized as opened.</p>

<pre><code>InputStreamFactory in=null;
try {
in= new MarkableFileInputStreamFactory(new 
File(""D:\\JavaNlp\\doccategorizer\\doccategorizer.txt""));
}
catch (FileNotFoundException e2) {
System.out.println(""Creating new input stream"");
e2.printStackTrace();
}

ObjectStream lineStream=null;
ObjectStream sampleStream=null;

try {
lineStream = new PlainTextByLineStream(in, ""UTF-8"");
sampleStream = new DocumentSampleStream(lineStream);            
}
catch (IOException e1) {
System.out.println(""Document Sample Stream"");
e1.printStackTrace();
}


TrainingParameters params = new TrainingParameters();
params.put(TrainingParameters.ITERATIONS_PARAM, 1000+"""");
params.put(TrainingParameters.CUTOFF_PARAM, 1+"""");
params.put(AbstractTrainer.ALGORITHM_PARAM, 
NaiveBayesTrainer.NAIVE_BAYES_VALUE);


DoccatModel model=null;
try {
model = DocumentCategorizerME.train(""en"", sampleStream, params, new 
DoccatFactory());
} 
catch (IOException e) 
{
System.out.println(""Training..."");
e.printStackTrace();
}


System.out.println(""\nModel is successfully trained."");


BufferedOutputStream modelOut=null;

try {
modelOut = new BufferedOutputStream(new 
FileOutputStream(""D:\\JavaNlp\\doccategorizer\\classifier-maxent.bin""));
} 
catch (FileNotFoundException e) {

System.out.println(""Creating output stream"");
e.printStackTrace();
}
try {
model.serialize(modelOut);
}
catch (IOException e) {

System.out.println(""Serialize..."");
e.printStackTrace();
}
System.out.println(""\nTrained model is kept in: 
""+""model""+File.separator+""en-cases-classifier-maxent.bin"");

DocumentCategorizer doccat = new DocumentCategorizerME(model);
String[] docWords = ""Some text here..."".replaceAll(""[^A-Za-z]"", "" "").split("" "");
double[] aProbs = doccat.categorize(docWords);


System.out.println(""\n---------------------------------\nCategory : 
Probability\n---------------------------------"");
for(int i=0;i&lt;doccat.getNumberOfCategories();i++){
System.out.println(doccat.getCategory(i)+"" : ""+aProbs[i]);
}
System.out.println(""---------------------------------"");

System.out.println(""\n""+doccat.getBestCategory(aProbs)+"" : is the category 
for the given sentence"");
</code></pre>

<p><img src=""https://photos.app.goo.gl/GEgrCSJjk8NqaSe7A"" alt=""results""> 
<img src=""https://photos.app.goo.gl/j2qDeLNm6wiw9nzx8"" alt=""results2""></p>

<p>Can someone make a suggestion for me how to categorize my documents well, like should I add a language detector first, or add new features?</p>

<p>Thanks in advance</p>
","java, nlp, text-classification, naivebayes, opennlp","<p>By default, the document classifier takes the document text and forms a bag of words. Each word in the bag becomes a feature. As long as the language can be tokenized by an English tokenizer (again by default a white space tokenizer), I would guess that the language is not your problem.  I would check the format of the data you are using for the training data.  It should be formatted like this:</p>

<pre><code>category&lt;tab&gt;document text
</code></pre>

<p>The text should fit be one line.  The opennlp documentation for the document classifier can be found at <a href=""http://opennlp.apache.org/docs/1.9.0/manual/opennlp.html#tools.doccat.training.tool"" rel=""nofollow noreferrer"">http://opennlp.apache.org/docs/1.9.0/manual/opennlp.html#tools.doccat.training.tool</a></p>

<p>It would be helpful if you could provide a line or two of training data to help examine the format.</p>

<p><em>Edit</em>:  Another potential issue.  60 documents may not be enough documents to train a good classifier, particularly if you have a large vocabulary.  Also, even though this is not English, please tell me it is not multiple languages. Finally, is the document text the best way to classify the document?  Would metadata from the document itself produce better features.</p>

<p>Hope it helps.</p>
",1,0,2493,2018-12-24 12:58:32,https://stackoverflow.com/questions/53913787/opennlp-document-categorizer-how-to-classify-documents-based-on-status-languag
How to handle text classification problems when multiple features are involved,"<p>I am working on a text classification problem where multiple text features and need to build a model to predict salary range. Please refer the <a href=""https://i.sstatic.net/MedzR.png"" rel=""noreferrer"">Sample dataset</a>
Most of the resources/tutorials deal with feature extraction on only one column and then predicting target. I am aware of the processes such as text pre-processing, feature extraction (CountVectorizer or TF-IDF) and then the applying algorithms. </p>

<p>In this problem, I have multiple input text features. <strong>How to handle text classification problems when multiple features are involved?</strong> These are the methods I have already tried but I am not sure if these are the right methods. Kindly provide your inputs/suggestion.</p>

<p>1) Applied data cleaning on each feature separately followed by TF-IDF and then logistic regression. Here I tried to see if I can use only one feature for classification.   </p>

<p>2) Applied Data cleaning on all the columns separately and then applied TF-IDF for each feature and then merged the all feature vectors to create only one feature vector. Finally logistic regression. </p>

<p>3) Applied Data cleaning on all the columns separately and merged all the cleaned columns to create one feature 'merged_text'. Then applied TF-IDF on this merged_text and followed by logistic regression.</p>

<p>All these 3 methods gave me around 35-40% accuracy on cross-validation &amp; test set. I am expecting at-least 60% accuracy on the test set which is not provided.</p>

<p>Also, I didn't understand how use to <strong>'company_name'</strong> &amp; <strong>'experience'</strong> with text data. there are about 2000+ unique values in company_name. Please provide input/pointer on how to handle numeric data in text classification problem.</p>
","python, nlp, feature-extraction, text-classification","<p>Try these things:</p>

<ol>
<li><p>Apply text preprocessing on 'job description', 'job designation' and 'key skills. Remove all stop words, separate each words removing punctuations, lowercase all words then apply TF-IDF or Count Vectorizer, don't forget to scale these features before training model.</p></li>
<li><p>Convert Experience to Minimum experience and Maximum experience 2 features and treat is as a discrete numeric feature.</p></li>
<li><p>Company and location can be treated as a categorical feature and create dummy variable/one hot encoding before training the model.</p></li>
<li><p>Try combining job type and key skills and then do vectorization, see how if it works better.</p></li>
<li><p>Use Random Forest Regressor, tune hyperparameters: n_estimators, max_depth, max_features using GridCV.</p></li>
</ol>

<p>Hopefully, these will increase the performance of the model.</p>

<p>Let me know how is it performing with these.</p>
",6,6,4190,2018-12-26 07:56:39,https://stackoverflow.com/questions/53929134/how-to-handle-text-classification-problems-when-multiple-features-are-involved
How can I get around Keras pad_sequences() rounding float values to zero?,"<p>So I have a text classification model built with Keras. I've been trying to pad my varying length sequences but the Keras function <code>pad_sequences()</code> has just returned zeros.</p>

<p>I've figured out that if you have a numpy array like the one below, it works just fine. But once the elements become floats or decimals like the second array it just turns to zeros.</p>

<pre><code>x = [[1, 2], [3,4,5], [4], [7,8,9,10]]
print pad_sequences(x, padding='post')
</code></pre>

<p>outputs:</p>

<pre><code>[[ 1  2  0  0]
 [ 3  4  5  0]
 [ 4  0  0  0]
 [ 7  8  9 10]]
</code></pre>

<p>But</p>

<pre><code>x = [[.1, .2], [.3,.4,.5], [.4], [.7,.8,.9,.010]]
print pad_sequences(x, padding='post')
</code></pre>

<p>outputs:</p>

<pre><code>[[ 0  0  0  0]
 [ 0  0  0  0]
 [ 0  0  0  0]
 [ 0  0  0  0]]
</code></pre>

<p>And this:</p>

<pre><code>x = [[.1, .2], [.3,.4,.5], [.4], [.7,.8,.9,.010]]
print pad_sequences(x, padding='post', value=99)
</code></pre>

<p>outputs:</p>

<pre><code>[[ 0  0 99 99]
 [ 0  0  0 99]
 [ 0 99 99 99]
 [ 0  0  0  0]]
</code></pre>

<p>So I guess this function just ignores floats/decimals. Is there a way I can get around this?</p>
","python, numpy, keras, lstm, text-classification","<p>It is caused by the fact that the default data type considered in the <code>pad_sequences</code> function is <code>int32</code>. Therefore, all the values will be casted to integer (and in this case become zero). To resolve this, pass <code>dtype='float32'</code> argument:</p>

<pre><code>pad_sequences(x, padding='post', value=99, dtype='float32')
</code></pre>
",14,9,4186,2019-01-03 23:21:24,https://stackoverflow.com/questions/54031161/how-can-i-get-around-keras-pad-sequences-rounding-float-values-to-zero
Feeding LSTMCell with whole sentences using embeddings gives dimensionality error,"<p>So currently i'm sitting on a text-classification problem, but i can't even set up my model in Tensorflow. I have a batch of sentences of length 70 (using padding) and i'm using a embedding_lookup with an embedding size of 300. Here the code for the embedding:</p>

<pre><code>embedding = tf.constant(embedding_matrix, name=""embedding"")
inputs = tf.nn.embedding_lookup(embedding, input_.input_data)
</code></pre>

<p>So now inputs should be of shape <strong>[batch_size, sentence_length, embedding_size]</strong> which is not surprising. Now sadly i'm getting a ValueError for my LSTMCell since it is expecting ndim=2 and obviously inputs is of ndim=3. I have not found a way to change the expected input shape of the LSTM Layer. Here is the code for my LSTMCell init:</p>

<pre><code>for i in range(num_layers):
    cells.append(LSTMCell(num_units, forget_bias, state_is_tuple, reuse=reuse, name='lstm_{}'.format(i))
cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)
</code></pre>

<p>The error is triggered in the call function of the cell, that looks like this:</p>

<pre><code>for step in range(self.num_steps):
    if step &gt; 0: tf.get_variable_scope().reuse_variables()
    (cell_output, state) = cell(inputs[:, step, :], state)
</code></pre>

<p>Similar question but not helping: <a href=""https://stackoverflow.com/questions/39324520/understanding-tensorflow-lstm-input-shape"">Understanding Tensorflow LSTM Input shape</a></p>
","python, tensorflow, machine-learning, lstm, text-classification","<p>I could solve the problem myself. As it seems, the LSTMCell implementation is more hands on and basic in relation to how a LSTM actually works. The Keras LSTM Layers took care of stuff i need to consider when i'm using TensorFlow. The example i'm using is from the following official TensorFlow example: </p>

<p><a href=""https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb</a></p>

<p>As we want to feed our LSTM Layers with a sequence, we need to feed the cells each word after another. As the call of the Cell creates two outputs (cell output and cell state), we use a loop for all words in all sentences to feed the cell and reuse our cell states. This way we create the output for our layers, which we can then use for further operations. The code for this looks like this:</p>

<pre><code>self._initial_state = cell.zero_state(config.batch_size, data_type())
state = self._initial_state
outputs = []
with tf.variable_scope(""RNN""):
  for time_step in range(self.num_steps):
    if time_step &gt; 0: tf.get_variable_scope().reuse_variables()
    (cell_output, state) = cell(inputs[:, time_step, :], state)
    outputs.append(cell_output)
output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])
</code></pre>

<p><strong>num_steps</strong> represents the amount of words in our sentence, that we are going to use.</p>
",0,0,123,2019-01-05 17:32:04,https://stackoverflow.com/questions/54054455/feeding-lstmcell-with-whole-sentences-using-embeddings-gives-dimensionality-erro
How to resample text (imbalanced groups) in a pipeline?,"<p>I'm trying to do some text classification using MultinomialNB, but I'm running into problems because my data is unbalanced. (Below is some sample data for simplicity. In actuality, mine is much larger.) I'm trying to resample my data using over-sampling, and I would ideally like to build it into this pipeline. </p>

<p>The pipeline below works fine without over-sampling, but again, in real life my data requires it. It's very imbalanced. </p>

<p>With this current code, I keep getting the error: ""TypeError: All intermediate steps should be transformers and implement fit and transform.""</p>

<p>How do I build RandomOverSampler into this pipeline? </p>

<pre><code>data = [['round red fruit that is sweet','apple'],['long yellow fruit with a peel','banana'],
    ['round green fruit that is soft and sweet','pear'], ['red fruit that is common', 'apple'],
    ['tiny fruits that grow in bunches','grapes'],['purple fruits', 'grapes'], ['yellow and long', 'banana'],
    ['round, small, green', 'grapes'], ['can be red, green, or purple', 'grapes'], ['tiny fruits', 'grapes'], 
    ['small fruits', 'grapes']]

df = pd.DataFrame(data,columns=['Description','Type'])  

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
text_clf = Pipeline([('vect', CountVectorizer()),
                    ('tfidf', TfidfTransformer()), 
                    ('RUS', RandomOverSampler()),
                    ('clf', MultinomialNB())])
text_clf = text_clf.fit(X_train, y_train)
y_pred = text_clf.predict(X_test)

print('Score:',text_clf.score(X_test, y_test))
</code></pre>
","python, pipeline, text-classification, resampling, oversampling","<p>You should use the Pipeline implemented in the <code>imblearn</code> package, not the one from <code>sklearn</code>. E.g., this code runs fine:</p>

<pre><code>import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB

from imblearn.over_sampling import RandomOverSampler
from imblearn.pipeline import Pipeline


data = [['round red fruit that is sweet','apple'],['long yellow fruit with a peel','banana'],
    ['round green fruit that is soft and sweet','pear'], ['red fruit that is common', 'apple'],
    ['tiny fruits that grow in bunches','grapes'],['purple fruits', 'grapes'], ['yellow and long', 'banana'],
    ['round, small, green', 'grapes'], ['can be red, green, or purple', 'grapes'], ['tiny fruits', 'grapes'],
    ['small fruits', 'grapes']]

df = pd.DataFrame(data, columns=['Description','Type'])

X_train, X_test, y_train, y_test = train_test_split(df['Description'],
    df['Type'], random_state=0)

text_clf = Pipeline([('vect', CountVectorizer()),
                    ('tfidf', TfidfTransformer()),
                    ('RUS', RandomOverSampler()),
                    ('clf', MultinomialNB())])
text_clf = text_clf.fit(X_train, y_train)
y_pred = text_clf.predict(X_test)

print('Score:',text_clf.score(X_test, y_test))
</code></pre>
",6,5,3450,2019-01-09 20:45:08,https://stackoverflow.com/questions/54118076/how-to-resample-text-imbalanced-groups-in-a-pipeline
Adding an extra dimension to text classification,"<p>I want to do text classification using a neural network in Keras. I have setup a simple test sample using the following network:</p>

<pre><code>model = Sequential()
model.add(Embedding(NUMVOCABOLARYWORDS, 5, input_length = sequenceDataPadded.shape[1]))
model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(128))
model.add(Dense(1))
</code></pre>

<p>This network accepts tokenized padded sequences of text. E.g. I tokenize the text ""hello world"" = [0,1,0,0,0..]. It train &amp; evaluates fine.</p>

<p>Now my issue is that I do not want to enter a single sequence into the network, but rather a collection (let's say 500) sequences into the network and get a category out. So instead of an input with shape (100) it's now (500, 100). I'm unsure how to best create the network architecture, ie:</p>

<p>1) Should I flatten the input or try to reduce the dimensions? What layers could I use for that job?</p>

<p>2) Should I just create one large sequence with all the text?</p>

<p>3) Does it even make sense to have a LSTM with 4 dimensions?</p>

<p>4) Does examples exist for classification with an array of array of tokens?</p>

<p>The text is collected text from different sources, so the different sequences in each batch is not necessarily related in relation to anything else than date.</p>
","tensorflow, neural-network, recurrent-neural-network, text-classification, tensor","<p>I don't think that merging all text together is the solution. The problem then is that if you feed it to the LSTM it that the hidden states of every text does not start initially. So you feed in the first text, and then the second and all other texts will have the current hidden state.</p>

<p>You could use the functional API and create different inputs and give each input its own LSTM. Then you can merge them and have the dense layers at the end.
Another thing that you could try is to use CNN. Again you'd either have to create multiple inputs or concatenate all the inputs and then use CNN layers. The advantage here could be the speed. Because depending on how many LSTMs you have and how big your input is training can take quite a while. Especially because the backpropagation also has to go through every timestep. So performance wise you may be better off with CNNs.</p>

<p>So what I would do is to keep the arrays separately with a max length. Then you pad every array to this length (if they are to short). Then you create multiple inputs with the <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">Functional API</a> and use Conv1D Layers behind it. You do some conv operations (maybe stack a few conv layers, maxpooling, etc.). Then you merge them with the <a href=""https://keras.io/layers/merge/"" rel=""nofollow noreferrer"">concatenate layer</a>. And then you have some more dense or CNN.</p>
",0,0,228,2019-01-24 12:58:34,https://stackoverflow.com/questions/54347230/adding-an-extra-dimension-to-text-classification
Spacy text categorization: getting the error massage &quot;&#39;float&#39; object is not iterable&quot;,"<p>I work on text categorization project using spaCy. I follow spaCy code example very closely. The only important difference is that I'm using two categories instead of one in the example. I don't understand what is wrong, as I checked and the data that I'm loading is in the same format as in the original example. Here is the relevant code (the full code attahced below):</p>

<pre><code>def load_data(limit=0, split=0.8):
    """"""Load the patents data.""""""
    # Partition off part of the train data for evaluation
    temp=pd.read_csv(excel + 'patents_text_class.csv',header = None)
    new_cols = ['id' , 'class' , 'patent_text']
    temp.columns = new_cols
    print(temp)
    train_data = list(zip(temp[""patent_text""], temp[""class""]))
    random.shuffle(train_data)
    train_data = train_data[-limit:]
    texts, labels = zip(*train_data)
    cats = [{""A01D"": bool(y) , ""A01B"": operator.not_(bool(y))} for y in labels]
    split = int(len(train_data) * split)
    return (texts[:split], cats[:split]), (texts[split:], cats[split:])
</code></pre>

<p>and this is the log:</p>

<pre><code>Loaded model 'en_core_web_lg'
Loading patents data...
            id  class                                        patent_text
0         1317      0  Improvement n revolving harrows &lt;div itemprop=...
1         2476      1  Machine for cutting meat and other substances ...
2         2650      0  Improvement in cultivators fob vines &lt;div item...
3         3311      0  Improvement in plows &lt;div itemprop=""content"" h...
4         4544      0  Improvement in plow-clevises &lt;div itemprop=""co...
5         7277      1  Improvement in machines for raking and loading...
6         8721      0  Improvement in shovel-plows &lt;div itemprop=""con...
7         8844      0  Improvement in gang-plows &lt;div itemprop=""conte...
8         9069      0  Improvement in potato-diggers and stone-gather...
9        10624      0  Improvement in rotary cultivators &lt;div itempro...
10       12057      0  Improvement in hoes &lt;div itemprop=""content"" ht...
[70000 rows x 3 columns]
Using 10000 examples (8000 training, 2000 evaluation)
Training the model...
LOSS      P       R       F  
Traceback (most recent call last):
  File ""process/task_classification.py"", line 150, in &lt;module&gt;
    plac.call(main)
  File ""/anaconda/lib/python3.6/site-packages/plac_core.py"", line 328, in call
    cmd, result = parser.consume(arglist)
  File ""/anaconda/lib/python3.6/site-packages/plac_core.py"", line 207, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File ""process/task_classification.py"", line 78, in main
    losses=losses)
  File ""/anaconda/lib/python3.6/site-packages/spacy/language.py"", line 405, in update
    gold = GoldParse(doc, **gold)
  File ""gold.pyx"", line 409, in spacy.gold.GoldParse.__init__
TypeError: 'float' object is not iterable
</code></pre>

<p>Any ideas why I'm getting this error? </p>

<p>The full code for reference:</p>

<pre><code>#!/usr/bin/env python
# coding: utf8
""""""Train a convolutional neural network text classifier on the
IMDB dataset, using the TextCategorizer component. The dataset will be loaded
automatically via Thinc's built-in dataset loader. The model is added to
spacy.pipeline, and predictions are available via `doc.cats`. For more details,
see the documentation:
* Training: https://spacy.io/usage/training
Compatible with: spaCy v2.0.0+
""""""
from __future__ import unicode_literals, print_function
import plac
import random
from pathlib import Path
import thinc.extra.datasets
import os
import pandas as pd
import operator
import spacy
from spacy.util import minibatch, compounding

root = 'path/to/folder'
output = root + 'output/'
process = root + 'process/'
excel = root + 'excel/'

@plac.annotations(
    model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
    output_dir=(""Optional output directory"", ""option"", ""o"", Path),
    n_texts=(""Number of texts to train from"", ""option"", ""t"", int),
    n_iter=(""Number of training iterations"", ""option"", ""n"", int))
def main(model='en_core_web_lg', output_dir=output, n_iter=5, n_texts=10000):
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()

    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print(""Created blank 'en' model"")

    # add the text classifier to the pipeline if it doesn't exist
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'textcat' not in nlp.pipe_names:
        textcat = nlp.create_pipe('textcat')
        nlp.add_pipe(textcat, last=True)
    # otherwise, get it, so we can add labels to it
    else:
        textcat = nlp.get_pipe('textcat')

    # add label to text classifier
    textcat.add_label(""A01B"")
    textcat.add_label(""A01D"")
    # load the patents dataset
    print(""Loading patents data..."")
    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)
    print(""Using {} examples ({} training, {} evaluation)""
          .format(n_texts, len(train_texts), len(dev_texts)))
    train_data = list(zip(train_texts,
                          [{'cats': cats} for cats in train_cats]))

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']
    with nlp.disable_pipes(*other_pipes):  # only train textcat
        optimizer = nlp.begin_training()
        print(""Training the model..."")
        print('{:^5}\t{:^5}\t{:^5}\t{:^5}'.format('LOSS', 'P', 'R', 'F'))
        for i in range(n_iter):
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(train_data, size=compounding(4., 32., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,
                           losses=losses)
            with textcat.model.use_params(optimizer.averages):
                # evaluate on the dev data split off in load_data()
                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)
            print('{0:.3f}\t{1:.3f}\t{2:.3f}\t{3:.3f}'  # print a simple table
                  .format(losses['textcat'], scores['textcat_p'],
                          scores['textcat_r'], scores['textcat_f']))

    # test the trained model
    test_text = ""Harvesting""
    doc = nlp(test_text)
    print(test_text, doc.cats)

    test_text = ""Plowing""
    doc = nlp(test_text)
    print(test_text, doc.cats)

    if output_dir is not None:
        with nlp.use_params(optimizer.averages):
            nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(output_dir)
        doc2 = nlp2(test_text)
        print(test_text, doc2.cats)


def load_data(limit=0, split=0.8):
    """"""Load the patents data.""""""
    # Partition off part of the train data for evaluation
    temp=pd.read_csv(excel + 'patents_text_class.csv',header = None)
    new_cols = ['id' , 'class' , 'patent_text']
    temp.columns = new_cols
    train_data = list(zip(temp[""patent_text""], temp[""class""]))
    random.shuffle(train_data)
    train_data = train_data[-limit:]
    texts, labels = zip(*train_data)
    cats = [{""A01D"": bool(y) , ""A01B"": operator.not_(bool(y))} for y in labels]
    split = int(len(train_data) * split)
    return (texts[:split], cats[:split]), (texts[split:], cats[split:])


def evaluate(tokenizer, textcat, texts, cats):
    docs = (tokenizer(text) for text in texts)
    tp = 0.0   # True positives
    fp = 1e-8  # False positives
    fn = 1e-8  # False negatives
    tn = 0.0   # True negatives
    for i, doc in enumerate(textcat.pipe(docs)):
        gold = cats[i]
        print(i)
        for label, score in doc.cats.items():
            if label not in gold:
                continue
            if score &gt;= 0.5 and gold[label] &gt;= 0.5:
                tp += 1.
            elif score &gt;= 0.5 and gold[label] &lt; 0.5:
                fp += 1.
            elif score &lt; 0.5 and gold[label] &lt; 0.5:
                tn += 1
            elif score &lt; 0.5 and gold[label] &gt;= 0.5:
                fn += 1
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f_score = 2 * (precision * recall) / (precision + recall)
    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}


if __name__ == '__main__':
    plac.call(main)
</code></pre>
","python, text-classification, spacy","<p>According to the documentation, first argument of <code>Language.update</code> accepts a batches of <code>unicode</code> or <code>Doc</code>'s. Probalby <code>texts</code> contatin some <code>NaN</code> values, which has a type <code>float</code>. Related code:</p>

<pre><code>batches = minibatch(train_data, size=compounding(4., 32., 1.001))
for batch in batches:
    texts, annotations = zip(*batch)  # check texts for NaN
    nlp.update(texts, annotations, sgd=optimizer, drop=0.2,
               losses=losses)
</code></pre>

<p><code>spacy</code> tries to iterate a <code>NaN</code> (float), and it causes an </p>

<pre><code>...
TypeError: 'float' object is not iterable
</code></pre>

<p>- so, you can drop all <code>NaN</code> values or replace them with empty string.</p>

<p>Also, this kind of error is very frequent for NLP (but not only NLP) tasks. Always check out text data for <code>NaN</code>'s and replace them, especially when you receive similar error message.</p>
",4,1,1212,2019-01-24 19:59:45,https://stackoverflow.com/questions/54354431/spacy-text-categorization-getting-the-error-massage-float-object-is-not-iter
Multiple input parameters during text classification - Scikit learn,"<p>I'm new to machine learning. I'm trying to do some text classification. 'CleanDesc' has the text sentence. And 'output' has the corresponding output. Initially i tried using one input parameter which is the string of texts(newMerged.cleanDesc) and one output parameter(newMerged.output)</p>

<pre><code>finaldata = newMerged[['id','CleanDesc','type','output']]

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(newMerged.CleanDesc)

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

clf = MultinomialNB().fit(X_train_tfidf, newMerged.output)    
testdata = newMerged.ix[1:200]
X_test_counts = count_vect.transform(testdata.CleanDesc)
X_test_tfidf = tfidf_transformer.transform(X_test_counts)

predicted = clf.predict(X_new_tfidf)
</code></pre>

<p>This works fine. But the accuracy is very low. I wanted to include one more parameter(newMerged.type) as the input, along with the text to try improving it. Can I do that? How do I do it. newMerged.type is not a text. It just a two character string like ""HT"". I tried doing it as follows, but it failed,</p>

<pre><code>finaldata = newMerged[['id','CleanDesc','type','output']]

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(newMerged.CleanDesc)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

clf = MultinomialNB().fit([[X_train_tfidf,newMerged.type]], 
newMerged.output)    
testdata = newMerged.ix[1:200]
X_test_counts = count_vect.transform(testdata.CleanDesc)
X_test_tfidf = tfidf_transformer.transform(X_test_counts)

predicted = clf.predict([[X_new_tfidf, testdata.type]])
</code></pre>
","machine-learning, scikit-learn, nlp, text-classification","<p>You have to use hstack from sicpy for appending arrays to sparse matrix. </p>

<p>Try this!</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelBinarizer
from scipy.sparse import hstack
corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?',
]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names())

print(X.shape)

#

['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
(4, 9)
</code></pre>

<p>You need to do encoding of your categorical variables. </p>

<pre><code>cat_varia= ['s','ut','ss','ss']
lb=LabelBinarizer()
feature2=lb.fit_transform(cat_varia)

appended_X = hstack((X, feature2))

import pandas as pd
pd.DataFrame(appended_X.toarray())

#

    0   1   2   3   4   5   6   7   8   9   10  11
0   0.000000    0.469791    0.580286    0.384085    0.000000    0.000000    0.384085    0.000000    0.384085    1.0 0.0 0.0
1   0.000000    0.687624    0.000000    0.281089    0.000000    0.538648    0.281089    0.000000    0.281089    0.0 0.0 1.0
2   0.511849    0.000000    0.000000    0.267104    0.511849    0.000000    0.267104    0.511849    0.267104    0.0 1.0 0.0
3   0.000000    0.469791    0.580286    0.384085    0.000000    0.000000    0.384085    0.000000    0.384085    0.0 1.0 0.0
</code></pre>
",2,2,1772,2019-01-25 01:57:48,https://stackoverflow.com/questions/54357984/multiple-input-parameters-during-text-classification-scikit-learn
Naive Bayes in Quanteda vs caret: wildly different results,"<p>I'm trying to use the packages <code>quanteda</code> and <code>caret</code> together to classify text based on a trained sample. As a test run, I wanted to compare the build-in naive bayes classifier of <code>quanteda</code> with the ones in <code>caret</code>. However, I can't seem to get <code>caret</code> to work right.</p>

<p>Here is some code for reproduction. First on the <code>quanteda</code> side:</p>

<pre><code>library(quanteda)
library(quanteda.corpora)
library(caret)
corp &lt;- data_corpus_movies
set.seed(300)
id_train &lt;- sample(docnames(corp), size = 1500, replace = FALSE)

# get training set
training_dfm &lt;- corpus_subset(corp, docnames(corp) %in% id_train) %&gt;%
  dfm(stem = TRUE)

# get test set (documents not in id_train, make features equal)
test_dfm &lt;- corpus_subset(corp, !docnames(corp) %in% id_train) %&gt;%
  dfm(stem = TRUE) %&gt;% 
  dfm_select(pattern = training_dfm, 
             selection = ""keep"")

# train model on sentiment
nb_quanteda &lt;- textmodel_nb(training_dfm, docvars(training_dfm, ""Sentiment""))

# predict and evaluate
actual_class &lt;- docvars(test_dfm, ""Sentiment"")
predicted_class &lt;- predict(nb_quanteda, newdata = test_dfm)
class_table_quanteda &lt;- table(actual_class, predicted_class)
class_table_quanteda
#&gt;             predicted_class
#&gt; actual_class neg pos
#&gt;          neg 202  47
#&gt;          pos  49 202
</code></pre>

<p>Not bad. The accuracy is 80.8% percent without tuning. Now the same (as far as I know) in <code>caret</code> </p>

<pre><code>training_m &lt;- convert(training_dfm, to = ""matrix"")
test_m &lt;- convert(test_dfm, to = ""matrix"")
nb_caret &lt;- train(x = training_m,
                  y = as.factor(docvars(training_dfm, ""Sentiment"")),
                  method = ""naive_bayes"",
                  trControl = trainControl(method = ""none""),
                  tuneGrid = data.frame(laplace = 1,
                                        usekernel = FALSE,
                                        adjust = FALSE),
                  verbose = TRUE)

predicted_class_caret &lt;- predict(nb_caret, newdata = test_m)
class_table_caret &lt;- table(actual_class, predicted_class_caret)
class_table_caret
#&gt;             predicted_class_caret
#&gt; actual_class neg pos
#&gt;          neg 246   3
#&gt;          pos 249   2
</code></pre>

<p>Not only is the accuracy abysmal here (49.6% - roughly chance), the pos class is hardly ever predicted at all! So I'm pretty sure I'm missing something crucial here, as I would assume the implementations should be fairly similar, but not sure what.</p>

<p>I already looked at the source code for the <code>quanteda</code> function (hoping that it might be built on <code>caret</code> or the underlying package anyway) and saw that there is some weighting and smoothing going on. If I apply the same to my dfm before training (setting <code>laplace = 0</code> later on), accuracy is a bit better. Yet also only 53%.</p>
","r, r-caret, text-classification, supervised-learning, quanteda","<p>The answer is that <strong>caret</strong> (which uses <code>naive_bayes</code> from the <strong>naivebayes</strong> package) assumes a Gaussian distribution, whereas <code>quanteda::textmodel_nb()</code> is based on a more text-appropriate multinomial distribution (with the option of a Bernoulli distribution as well). </p>

<p>The documentation for <code>textmodel_nb()</code> replicates the example from the <em>IIR</em> book (Manning, Raghavan, and Schütze 2008) and a further example from Jurafsky and Martin (2018) is also referenced.  See:</p>

<ul>
<li><p>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. 2008. An Introduction to Information Retrieval. Cambridge University Press (Chapter 13). <a href=""https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf"" rel=""noreferrer"">https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a></p></li>
<li><p>Jurafsky, Daniel, and James H. Martin. 2018. Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Draft of 3rd edition, September 23, 2018 (Chapter 4). <a href=""https://web.stanford.edu/~jurafsky/slp3/4.pdf"" rel=""noreferrer"">https://web.stanford.edu/~jurafsky/slp3/4.pdf</a></p></li>
</ul>

<p>Another package, <strong>e1071</strong>, produces the same results you found as it is also based on a Gaussian distribution.</p>

<pre><code>library(""e1071"")
nb_e1071 &lt;- naiveBayes(x = training_m,
                       y = as.factor(docvars(training_dfm, ""Sentiment"")))
nb_e1071_pred &lt;- predict(nb_e1071, newdata = test_m)
table(actual_class, nb_e1071_pred)
##             nb_e1071_pred
## actual_class neg pos
##          neg 246   3
##          pos 249   2
</code></pre>

<p>However both <strong>caret</strong> and <strong>e1071</strong> work on dense matrices, which is one reason they are so mind-numbingly slow compared to the <strong>quanteda</strong> approach which operates on the sparse dfm.  So from the standpoint of appropriateness,  efficiency, and (as per your results) the performance of the classifier, it should be pretty clear which one is preferred!</p>

<pre><code>library(""rbenchmark"")
benchmark(
    quanteda = { 
        nb_quanteda &lt;- textmodel_nb(training_dfm, docvars(training_dfm, ""Sentiment""))
        predicted_class &lt;- predict(nb_quanteda, newdata = test_dfm)
    },
    caret = {
        nb_caret &lt;- train(x = training_m,
                          y = as.factor(docvars(training_dfm, ""Sentiment"")),
                          method = ""naive_bayes"",
                          trControl = trainControl(method = ""none""),
                          tuneGrid = data.frame(laplace = 1,
                                                usekernel = FALSE,
                                                adjust = FALSE),
                          verbose = FALSE)
        predicted_class_caret &lt;- predict(nb_caret, newdata = test_m)
    },
    e1071 = {
        nb_e1071 &lt;- naiveBayes(x = training_m,
                       y = as.factor(docvars(training_dfm, ""Sentiment"")))
        nb_e1071_pred &lt;- predict(nb_e1071, newdata = test_m)
    },
    replications = 1
)
##       test replications elapsed relative user.self sys.self user.child sys.child
## 2    caret            1  29.042  123.583    25.896    3.095          0         0
## 3    e1071            1 217.177  924.157   215.587    1.169          0         0
## 1 quanteda            1   0.235    1.000     0.213    0.023          0         0
</code></pre>
",8,5,1284,2019-01-29 17:57:06,https://stackoverflow.com/questions/54427001/naive-bayes-in-quanteda-vs-caret-wildly-different-results
Can i use CountVectorizer on both test and train data at the same time or do I need to split it up?,"<p>I currently have an SVM model that classify text into two different classes. I'm currently using CountVectorizer and TfidfTransformer to create my ""word vector."" </p>

<p>The thing is that I think I maybe do it in the wrong order when I'm doing the conversion of all the text first and then split it up.</p>

<p>My question is, will there be any difference if I do train_test_split first and then do the fit_transform only on the train data and then transform on the test data? </p>

<p>What is the correct way to do it? </p>

<p>Big thanks in advance, happy coding! </p>

<pre><code>count_vect = CountVectorizer(stop_words='english')
X_counts = count_vect.fit_transform(textList)

tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_counts)

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, correctLabels, test_size=.33, random_state=17)
</code></pre>
","machine-learning, scikit-learn, text-classification, word-count","<p><strong>First split in train and test set, then fit only on the train set and transform the test set</strong></p>

<p>If you do it the other way around, you are <a href=""https://machinelearningmastery.com/data-leakage-machine-learning/)"" rel=""noreferrer"">leaking information</a> from the test set to the train set. This might cause overfitting, which will make your model not generalize well to new, unseen data.</p>

<p>The goal of a test set is to test how well your model performs on new data. In the case of Text Analytics, this may mean words it has never seen before and know nothing of the importances of, or new distributions of the occurrence of words. If you first use your <code>CountVectorizer</code> and <code>TfIdfTransformer</code>, you will have no idea of know how it responds to this: after all, all the data has been seen by the transformers. The problem: you think you have built a great model with great performance, but when it is put in production, the accuracy will be much lower.</p>
",6,2,3964,2019-02-02 10:03:57,https://stackoverflow.com/questions/54491953/can-i-use-countvectorizer-on-both-test-and-train-data-at-the-same-time-or-do-i-n
Retrain production model with labeled + predicted data?,"<p>Let's say that I'm currently doing text classifying with two different classes. The labeled data I have now is the one I have manually classified as either X or Y. The dataset is atm kind of large with a dataset with size 7000 (3500 X, 3500 Y).</p>

<p>The thing is that I have 2000 which are currently not labeled, but they belong to either X or Y (there is no other class). </p>

<p>My model's accuracy, recall, and f1-score is around 95-98 depending on the model I use.</p>

<p>The goal is not to be needing this manual categorization of either X or Y anymore, and just let the ML model do it for me (ofc it gets it wrong sometimes and its all okay).</p>

<p>The question is, can I use the model's predictions together with the manually categorized data on training and validation when I later retrain my model?</p>

<p>I know this is a kind of hard question, due to you don't have all the information, etc. But I guess I'm not the only one that wants to replace something that's currently done manually with an ML model.</p>
","machine-learning, text-classification, training-data","<p>I think it is definitely not a good idea. By doing so, you will basically just improve your model's ""confidence"" that the predictions are correct. What if you add documents that are very different from those in your training set? I would rather suggest one of the two things (although it seems like your model already has a very good performance):</p>

<ol>
<li><p>If you can manually label a couple of more documents, maybe you can come up with a rationale of which you would want to label. For instance, you could label manually those where the prediction probability is low (where the classifier you trained is not very ""confident"" about the accuracy of the prediction)</p></li>
<li><p>If you have a lot of unlabeled data and you expect them to behave differently from your training data, it might be worth checking out semi-supervised learning. This would take advantage of both the labeled data and the distribution of the unlabeled data.</p></li>
</ol>
",1,0,420,2019-02-03 18:38:45,https://stackoverflow.com/questions/54506219/retrain-production-model-with-labeled-predicted-data
Different accuracy for the same code in text classification in keras,"<p>I'm training a recurrent neural network based on LSTM for text classification and I have a strange behaviour. With the same code and same training set I obtain very different level of accuracy. </p>

<p>I know it's normal to have different value but sometimes i get value of 40% accuracy and others 90%. How is it possible?</p>

<p>Moreover sometimes I get ""stuck"" on the accuracy level, I mean that the loss and the accuracy doesn't change during the different epochs so both values remain the same. which is the  explanation for this?</p>

<p>Another aspect that I'm not sure that i truly understand is the fact of the padding for my input text (I'm using training in batch). I think, since I'm using a RNN, should be better use a left padding. Do you know how to pad the input? Is better left or right padding?</p>

<p>Last question is how to choose the number of layer and the number of node. I know that for someone the best approach is by experiment but some suggestion could be useful.</p>

<p>This is the implementation of my RNN:</p>

<pre><code>sentence_indices = Input(shape=input_shape, dtype=np.int32)
embedding_layer =  pretrained_embedding_layer(word_to_vec_map, word_to_index, emb_dim)

embeddings = embedding_layer(sentence_indices)   
X = LSTM(128, return_sequences=True)(embeddings)
X = Dropout(0.5)(X)
X = LSTM(128)(X)
X = Dropout(0.5)(X)
X = Dense(num_activation, activation='softmax')(X)
X =  Activation('softmax')(X)
model = Model(sentence_indices, X)
</code></pre>

<ul>
<li>the embeddings layer came from GloVe, a model pre-trained. </li>
<li>I'm using 5 categories and 300 samples more or less</li>
<li>My training set is not uniform</li>
</ul>

<p>I'll appreciate any hint of the question above and also some suggestion to improve my neural network.</p>
","python, tensorflow, keras, nlp, text-classification","<p>Are you training over the same amount of epochs? </p>

<p>One of the things that could happen is that you have exploding gradients in some of the runs. Maybe you could introduce gradient clipping (<a href=""https://keras.io/optimizers/"" rel=""nofollow noreferrer"">gradient clipping in keras</a>) to avoid that problem. You could also use regularization (<a href=""https://keras.io/regularizers/"" rel=""nofollow noreferrer"">keras ruglarizers</a>) to have another measurement in place.
When it comes to padding than right padding is the usual thing to do as far as I know. The rationale is that the initial hidden state is always 0 for the sequences that come in. Otherwise you start of with different hidden stated depending on how much padding you had to the left.</p>

<p>In your model there is one problem. You have twice a softmax layer. So it is sufficient to just have </p>

<pre><code>X = Dense(num_activations, activation='softmax')(X)
</code></pre>

<p>You don't need the Activation Layer next. </p>

<p>Do you just use 300 samples for the training or the test? For training this seems like very few samples, more would be better. You could try to make the LSTM bigger (e.g 512) if you have the computational resources for that. But if you just have 300 samples that won't make a difference in terms of performance I guess. Something else that you could try is to tweak hyperparameters like the optimizer and learning rate. And you could try to use CNN instead of LSTM, maybe that would also increase the performance a bit.</p>
",0,1,444,2019-02-06 16:55:29,https://stackoverflow.com/questions/54558760/different-accuracy-for-the-same-code-in-text-classification-in-keras
Predict multi class in svm,"<p>I have user review dataset like</p>

<pre><code>review-1, 0,1,1,0,0
</code></pre>

<p><code>review-1</code> is user review and <code>0,1,1,0,0</code> is review categories. one review can have multiple categories. I want to predict categories to reviews. so I implement the code that</p>

<pre><code>transformer = TfidfVectorizer(lowercase=True, stop_words=stop, max_features=500)
X = transformer.fit_transform(df.Review)

X_train, X_test, y_train, y_test = train_test_split(X, df.iloc[:, 1:6],
                                                test_size=0.25, random_state=42)

SVM = svm.SVC()
SVM.fit(X_train, y_train)
</code></pre>

<p>But I'm getting error like</p>

<pre><code>ValueError: bad input shape (75, 5)
</code></pre>

<p>Could anyone suggest any good solution to solve this?</p>
","python, machine-learning, scikit-learn, svm, text-classification","<p>You could use a binary classifier (like <code>svm.SVC()</code>) to solve the multi-label classification problem using <code>OneVsRestClassifier</code>.</p>

<p><strong>Example:</strong></p>

<pre><code>from sklearn.multiclass import OneVsRestClassifier

from sklearn.svm import SVC

cls = OneVsRestClassifier(estimator=SVC(gamma ='auto'))

import numpy as np
cls.fit(np.random.rand(20,10),np.random.binomial(1,0.2,size=(20,5)))
</code></pre>
",5,1,652,2019-02-13 07:48:58,https://stackoverflow.com/questions/54665028/predict-multi-class-in-svm
Is there a difference in computation according to input shape? (CNN in Python with Tensorflow),"<p>I am solving a text classification problem by reference to the paper(<a href=""https://www.aclweb.org/anthology/D14-1181"" rel=""nofollow noreferrer"">Kim, 2014</a>).
And then I found between below two models, the model on the left(Model 1) takes about 2.5 times more time than the model on the right(Model 2).
I think the number of weight parameters of the two models is the same.
Why is there the difference of time between the two models?<br>
*The input data's contents of the two models are the same. Simply changed the shape.  </p>

<p><a href=""https://i.sstatic.net/mKKpt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mKKpt.png"" alt=""model input data""></a>
I used <strong>tf.nn.conv2d</strong>. And the filter shapes and the stride are as following<br>
model 1 : 3x9x1xthe number of filters, stride 3<br>
model 2 : 1x9x3xthe number of filters, stride 1<br>
And the other things are the same<br>
*On above image, width means 'self.embedding_dim' and height means 'self.max_length'.  </p>

<pre><code>pooled_outputs = []
with tf.name_scope(""conv-maxpool-3""):
# Convolution Layer
filter_shape = [3, self.embedding_dim, 1, self.num_filters]
W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=""b"")
conv = tf.nn.conv2d(
    self.embedded_chars_expanded,
    W,
    strides=[1, 1, 3, 1],
    padding=""VALID"",
    name=""conv"")
# Apply nonlinearity
h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")
# Maxpooling over the outputs
pooled = tf.nn.max_pool(
    h,
    ksize=[1, self.max_length - 3 + 1, 1, 1],
    strides=[1, 1, 1, 1],
    padding='VALID',
    name=""pool"")
pooled_outputs.append(pooled)

----------------------------------------------------------------------

pooled_outputs = []
with tf.name_scope(""conv-maxpool-1""):
# Convolution Layer
filter_shape = [1, self.embedding_dim, 3, self.num_filters]
W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=""b"")
conv = tf.nn.conv2d(
    self.embedded_chars_expanded,
    W,
    strides=[1, 1, 1, 1],
    padding=""VALID"",
    name=""conv"")
# Apply nonlinearity
h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")
# Maxpooling over the outputs
pooled = tf.nn.max_pool(
    h,
    ksize=[1, self.max_length - 1 + 1, 1, 1],
    strides=[1, 1, 1, 1],
    padding='VALID',
    name=""pool"")
pooled_outputs.append(pooled)
</code></pre>
","python, tensorflow, classification, conv-neural-network, text-classification","<p>In the first model you set the stride to <code>[1, 1, 3, 1]</code> and you don't specify the data order, so it defaults to <code>NHWC</code>, i.e. (num_batches, height, width, channels) (check the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""nofollow noreferrer"">docu</a>). So the stride of 3 applies to the width, not the height, as your picture of model 1 indicates. Because you are using <code>VALID</code> padding, the stride of 3 on the width has no effect, by the way.</p>

<p>So basically, your depiction of model 1 is wrong: in step 2 it doesn't jump to the 4th row, but to the 2nd row. Meaning model 1 computes about 3 times as many convolutions as model 2.</p>

<p>There's other factors that could contribute to a difference in speed - may be model 2 can be better parallelized on the GPU, but that is hard to judge.</p>
",1,2,145,2019-02-13 10:19:51,https://stackoverflow.com/questions/54667727/is-there-a-difference-in-computation-according-to-input-shape-cnn-in-python-wi
Can I use a 3D input on a Keras Dense Layer?,"<p>As an exercise I need to use only dense layers to perform text classifications. I want to leverage words embeddings, the issue is that the dataset then is 3D (samples,words of sentence,embedding dimension). Can I input a 3D dataset into a dense layer?</p>

<p>Thanks</p>
","keras, text-classification, keras-layer, word-embedding, nlp","<p>As stated in <a href=""https://keras.io/layers/core/"" rel=""noreferrer"">the keras documentation</a> you can use 3D (or higher rank) data as input for a Dense layer but the input gets flattened first:</p>

<blockquote>
  <p>Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.</p>
</blockquote>

<p>This means that if your input has shape <code>(batch_size, sequence_length, dim)</code>, then the dense layer will first flatten your data to shape <code>(batch_size * sequence_length, dim)</code> and then apply a dense layer as usual. The output will have shape <code>(batch_size, sequence_length, hidden_units)</code>. This is actually the same as applying a Conv1D layer with kernel size 1, and it might be more explicit to use a Conv1D layer instead of a Dense layer.</p>
",12,2,5028,2019-02-21 10:52:50,https://stackoverflow.com/questions/54805345/can-i-use-a-3d-input-on-a-keras-dense-layer
Intent classification with large number of intent classes,"<p>I am working on a data set of approximately 3000 questions and I want to perform intent classification. <strong>The data set is not labelled yet</strong>, but from the business perspective, there's a requirement of identifying approximately <strong>80 various intent classes</strong>. Let's assume my training data has approximately equal number of each classes and is not majorly skewed towards some of the classes. I am intending to convert the text to word2vec or Glove and then feed into my classifier.</p>

<p>I am familiar with cases in which I have a smaller number of intent classes, such as 8 or 10 and the choice of machine learning classifiers such as SVM, naive bais or deeplearning (CNN or LSTM).</p>

<p>My question is that if you have had experience with such large number of intent classes before, and which of machine learning algorithm do you think will perform reasonably? do you think if i use deep learning frameworks, still large number of labels will cause poor performance given the above training data?</p>

<p>We need to start labelling the data and it is rather laborious to come up with 80 classes of labels and then realise that it is not performing well, so I want to ensure that I am making the right decision on <strong>how many classes of intent maximum</strong> I should consider and what machine learning algorithm do you suggest?</p>

<p>Thanks in advance...</p>
","python, tensorflow, nlp, text-classification","<p>First, word2vec and GloVe are, almost, dead. You should probably consider using more recent embeddings like BERT or ELMo (both of which are sensitive to the context; in other words, you get different embeddings for the same word in a different context). Currently, BERT is my own preference since it's completely open-source and available (gpt-2 was released a couple of days ago which is apparently a little bit better. But, it's not completely available to the public).</p>

<p>Second, when you use BERT's pre-trained embeddings, your model has the advantage of seeing a massive amount of text (Google massive) and thus can be trained on small amounts of data which will increase it's performance drastically.</p>

<p>Finally, if you could classify your intents into some coarse-grained classes, you could train a classifier to specify which of these coarse-grained classes your instance belongs to. Then, for each coarse-grained class train another classifier to specify the fine-grained one. This hierarchical structure will probably improve the results. Also for the type of classifier, I believe a simple fully connected layer on top of BERT would suffice. </p>
",6,5,2563,2019-02-24 09:50:31,https://stackoverflow.com/questions/54850657/intent-classification-with-large-number-of-intent-classes
Predict &quot;user-input&quot; reviews with Naive Bayes trained model,"<p>I am using a dataset with textual Yelp restaurant reviews and their ""star"" rating. 
My data is a df and looks like this:</p>

<pre><code>Textual Review           Numeric rating
""super cool restaurant""  5
""horrible experience""    1
</code></pre>

<p>I have built the MultinomialNB model which predicts the ""star"" (1-stands for negative, 5 stands for positive; using only these two categories) for the review.</p>

<pre><code>import pandas as pd
import numpy as np
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, classification_report
from nltk.corpus import stopwords
import string
import numpy

df = pd.read_csv('YELP_rev.csv')
#subsetting only the reviews on the extreme sides of the rating
df_class = df[(df['Numeric rating'] ==1) | (df['Numeric rating'] == 5)]

X = df_class['Textual review']
y = df_class['Numeric rating']
vectorizer=CountVectorizer()
X = vectorizer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

nb = MultinomialNB()
#fiting the model with X_train, y_train
nb.fit(X_train, y_train)
#doing preditions
pred = nb.predict(X_test)
print(confusion_matrix(y_test, pred))



precision    recall  f1-score   support

           1       0.43      0.33      0.38         9
           5       0.90      0.93      0.92        61

   micro avg       0.86      0.86      0.86        70
   macro avg       0.67      0.63      0.65        70
weighted avg       0.84      0.86      0.85        70
</code></pre>

<p><strong>What I'm trying to do is to predict ""star"" rating for the user provided restaurant review.</strong> Here are my attempts:</p>

<pre><code>test_review = input(""Enter a review:"")  

def input_process(text):
    nopunc = [char for char in text if char not in string.punctuation]
    nopunc = ''.join(nopunc)
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

new_x=vectorizer.transform(input_process(test_review))
test_review_rate = nb.predict(new_x)
print(test_review_rate)
</code></pre>

<p>I am not sure whether the output that I am getting is correct since I get an array of scores. <strong>Can someone help me interpret these scores?</strong> <strong>Do I just take the average and that will be my ""star""rating for the review?</strong></p>

<pre><code>&gt;&gt;Enter a review:We had dinner here for my birthday in Stockholm. The restaurant was very popular, so I would advise you book in advance.Blahblah
#my output
&gt;&gt;[5 5 5 5 5 5 5 5 5 1 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5
 5 5 5 5]
</code></pre>

<p>ps I realize that the sample data is poor and my model is biased towards positive ratings!
Thanks beforehand!</p>
","python, scikit-learn, user-input, text-classification, naivebayes","<p>You need to <code>join</code> your words back into a single string. Right now the output from your <code>input_process</code> function is a list of words, so your model is interpreting each word as a separate input sample, which is why you are getting a score for each <strong>word</strong> in your review, instead of one score for the whole text.</p>

<p>Some changes in your code:</p>

<pre><code>def input_process(text):
    # Something you can try for removing punctuations
    translator = str.maketrans('', '', string.punctuation)
    nopunc = text.translate(translator)
    words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
    # Join the words back and return as a string
    return ' '.join(words)

# vectorizer.transform takes a list as input
# You will have to pass your single string input as a list
new_x=vectorizer.transform([input_process(test_review)])
</code></pre>
",1,0,477,2019-02-28 18:35:34,https://stackoverflow.com/questions/54932155/predict-user-input-reviews-with-naive-bayes-trained-model
How to recognize entities in text that is the output of optical character recognition (OCR)?,"<p>I am trying to do multi-class classification with textual data. Problem I am facing that I have unstructured textual data. I'll explain the problem with an example.
consider this image for example:</p>

<p><a href=""https://i.sstatic.net/mS1fx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mS1fx.jpg"" alt=""example data""></a></p>

<p>I want to extract and classify text information given in image. Problem is when I extract information OCR engine will give output something like this:</p>

<pre><code>18
EURO 46
KEEP AWAY
FROM FIRE
MADE IN CHINA
2226249917581
7412501
DOROTHY
PERKINS
</code></pre>

<p>Now target classes here are:</p>

<pre><code>18 -&gt; size
EURO 46 -&gt; price
KEEP AWAY FROM FIRE -&gt; usage_instructions
MADE IN CHINA -&gt; manufacturing_location
2226249917581 -&gt; product_id
7412501 -&gt; style_id
DOROTHY PERKINS -&gt; brand_name
</code></pre>

<p>Problem I am facing is that input text is not separable, meaning ""multiple lines can belong to same class"" and there can be cases where ""single line can have multiple classes"".</p>

<p>So I don't know how I can split/merge lines before passing it to classification model.<br> Is there any way using NLP I can split paragraph based on target class. In other words given input paragraph split it based on target labels.</p>
","nlp, recurrent-neural-network, text-classification, named-entity-recognition, named-entity-extraction","<p>If you only consider the text, this is a Named Entity Recognition (NER) task.</p>

<p>What you can do is <a href=""https://spacy.io/usage/training#section-ner"" rel=""noreferrer""><strong>train</strong> a Spacy model to NER for your particular problem</a>.</p>

<p>Here is what you will need to do:</p>

<ol>
<li>First gather a list of training text data</li>
<li>Label that data with corresponding entity types</li>
<li>Split the data into training set and testing set</li>
<li>Train a model with Spacy NER using training set</li>
<li>Score the model using the testing set</li>
<li>...</li>
<li>Profit!</li>
</ol>

<p>See <a href=""https://spacy.io/usage/training#section-ner"" rel=""noreferrer"">Spacy documentation on training specific NER models</a></p>

<p>Good luck!</p>
",5,6,1436,2019-03-03 10:52:46,https://stackoverflow.com/questions/54968055/how-to-recognize-entities-in-text-that-is-the-output-of-optical-character-recogn
NLP data preparation and sorting for text-classification task,"<p>I read a lot of tutorials on the web and topics on stackoverflow but one question is still foggy for me. If consider just the stage of collecting data for multi-label training, what way (see below) are better and whether are both of them acceptable and effective? </p>

<ol>
<li>Try to find 'pure' one-labeled examples at any cost.</li>
<li>Every example can be multi labeled.</li>
</ol>

<p>For instance, I have articles about war, politics, economics, culture. Usually, politics tied to economics, war connected to politics, economics issues may appear in culture articles etc. I can assign strictly one main theme for each example and drop uncertain works or assign 2, 3 topics. </p>

<p>I'm going to train data using Spacy, volume of data will be about 5-10 thousand examples per topic.</p>

<p>I'd be grateful for any explanation and/or a link to some relevant discussion.</p>
","python, nlp, dataset, text-classification, spacy","<p>You can try <strong><em>OneVsAll / OneVsRest</em></strong> strategy. This will allow you to do both: predict exact one category without the need to strictly assign one label. </p>

<blockquote>
  <p>Also known as one-vs-all, this strategy consists in fitting one
  classifier per class. For each classifier, the class is fitted against
  all the other classes. In addition to its computational efficiency
  (only n_classes classifiers are needed), one advantage of this
  approach is its interpretability. Since each class is represented by
  one and one classifier only, it is possible to gain knowledge about
  the class by inspecting its corresponding classifier. This is the most
  commonly used strategy for multiclass classification and is a fair
  default choice.</p>
  
  <p>This strategy can also be used for multilabel learning, where a
  classifier is used to predict multiple labels for instance, by fitting
  on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and
  0 otherwise.</p>
</blockquote>

<p>Link to docs:
<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html</a></p>
",1,1,223,2019-03-04 17:10:22,https://stackoverflow.com/questions/54988214/nlp-data-preparation-and-sorting-for-text-classification-task
Text classification beyond the keyword dependency and inferring the actual meaning,"<p>I am trying to develop a text classifier that will classify a piece of text as <strong>Private</strong> or <strong>Public</strong>. Take medical or health information as an example domain. A typical classifier that I can think of considers keywords as the main distinguisher, right? What about a scenario like bellow? What if both of the pieces of text contains similar keywords but carry a different meaning. </p>

<p><strong>Following piece of text is revealing someone's private (health) situation (the patient has cancer):</strong>  </p>

<p>I've been to two <code>clinics</code> and my <code>pcp</code>. I've had an <code>ultrasound</code> only to be told it's a resolving <code>cyst</code> or a <code>hematoma</code>, but it's getting larger and starting to make my leg <code>ache</code>. The <code>PCP</code> said it can't be a <code>cyst</code> because it started out way too big and I swear I have NEVER <code>injured</code> my leg, not even a <code>bump</code>. I am now scared and afraid of <code>cancer</code>. I noticed a slightly uncomfortable sensation only when squatting down about 9 months ago. 3 months ago I went to squat down to put away laundry and it kinda <code>hurt</code>. The <code>pain</code> prompted me to examine my <code>leg</code> and that is when I noticed a <code>lump</code> at the bottom of my calf <code>muscle</code> and flexing only made it more noticeable. Eventually after four <code>clinic</code> visits, an <code>ultrasound</code> and one <code>pcp</code> the result seems to be positive and the mass is getting larger.<br>
<strong>[Private] (Correct Classification)</strong></p>

<p><strong>Following piece of text is a comment from a doctor which is definitely not revealing is health situation. It introduces the weaknesses of a typical classifier model:</strong>  </p>

<p>Don’t be scared and do not assume anything bad as <code>cancer</code>. I have gone through several cases in my <code>clinic</code> and it seems familiar to me. As you mentioned it might be a <code>cyst</code> or a <code>hematoma</code> and it's getting larger, it must need some additional <code>diagnosis</code> such as <code>biopsy</code>. Having an <code>ache</code> in that area or the size of the <code>lump</code> does not really tells anything <code>bad</code>. You should visit specialized <code>clinics</code> few more times and go under some specific tests such as <code>biopsy</code>, <code>CT scan</code>, <code>pcp</code> and <code>ultrasound</code> before that <code>lump</code> become more larger.<br>
<strong>[Private] (Which is the Wrong Classification. It should be [Public])</strong>  </p>

<p>The second paragraph was classified as private by all of my current classifiers, for obvious reason. Similar keywords, valid word sequences, the presence of subjects seemed to make the classifier very confused. Even, both of the content contains subjects like <code>I</code>, <code>You</code> (Noun, Pronouns) etc. I thought about from Word2Vec to Doc2Vec, from Inferring meaning to semantic embeddings but can't think about a solution approach that best suits this problem.</p>

<p>Any idea, which way I should handle the classification problem? Thanks in advance. </p>

<p><strong>Progress so Far:</strong><br>
The data, I have collected from a public source where patients/victims usually post their own situation and doctors/well-wishers reply to those. I assumed while crawling is that - posts belongs to my private class and comments belongs to public class. All to gether I started with 5K+5K posts/comments and got around 60% with a naive bayes classifier without any major preprocessing. I will try Neural Network soon. But before feeding into any classifier, I just want to know how I can preprocess better to put reasonable weights to either class for better distinction.</p>
","python, text-classification, nlp","<p>If the data you posted is representative of the classes you're trying to distinguish, keyword based features might not be the most effective. It looks like some terms that are sometimes treated as stop-words will be very good cues as to what is Private and what is Public. </p>

<p>You mention pronouns, I think that's likely still a good avenue forward. If you're using unigram/bag-of-words kinds of features, make sure your vectorizer is not removing them.</p>

<p>Doing a count of instances of first person pronouns  (<code>I</code>, <code>my</code>, <code>I've</code>, <code>mine</code>)  gives 13 for the Private case and 2 for the Public case. </p>

<p>The Public example has second person pronouns (e.g. <code>you</code>) where the first example doesn't. So maybe features about counts or smoothed ratios of first to second person pronouns would be effective.</p>

<p>If you have syntactic structure or are keeping track of positional information through n-grams or a similar representation, then features involving first-person pronouns and your keywords may be effective.</p>

<p>Also, verb-initial sentence structures (<code>Don't be ...</code>, <code>Having an...</code>) are characteristic of second-person directed language and may show up more in the public than the private text.</p>

<p>One last speculative thought: The sentiment of the two passages is pretty different, so if you have access to sentiment analysis, that might provide additional cues. I would expect the Public class would be more neutral that the Private class.</p>

<p>Plugging your Public example into the <a href=""https://www.ibm.com/watson/services/tone-analyzer/"" rel=""nofollow noreferrer"">Watson Tone Analyzer</a> demo gives this notable result:</p>

<pre><code>{
  ""sentence_id"": 3,
  ""text"": ""I am now scared and afraid of cancer."",
  ""tones"": [
    {
      ""score"": 0.991397,
      ""tone_id"": ""fear"",
      ""tone_name"": ""Fear""
    }
  ]
},
</code></pre>

<p>The Public statement also contains a fear-tagged sentence, but it's not scored as highly, is accompanied by other annotations, and contains an explicit negation in the sentence. So it might be worthwhile to leverage those as features as well.</p>

<pre><code>""sentences_tone"": [
    {
      ""sentence_id"": 0,
      ""text"": ""Don’t be scared and do not assume anything bad as cancer."",
      ""tones"": [
        {
          ""score"": 0.874498,
          ""tone_id"": ""fear"",
          ""tone_name"": ""Fear""
        },
        {
          ""score"": 0.786991,
          ""tone_id"": ""tentative"",
          ""tone_name"": ""Tentative""
        },
        {
          ""score"": 0.653099,
          ""tone_id"": ""analytical"",
          ""tone_name"": ""Analytical""
        }
      ]
    },
</code></pre>
",4,10,657,2019-03-04 22:00:24,https://stackoverflow.com/questions/54992220/text-classification-beyond-the-keyword-dependency-and-inferring-the-actual-meani
One Category Text Classification on imbalanced data-set,"<p>I am having <strong>imbalanced</strong> dataset scraped from web pages <strong>text data</strong> and have <strong>manually</strong> classified it into <strong>positive</strong> class, while the other negative class can have <strong>any type of text data</strong> which I have marked as <strong>negative</strong>.
Looking at the dataset it was then clear that negative samples are <strong>very less</strong> approx. <strong>1200</strong> out of 6000.</p>

<p>Negative = 1200</p>

<p>Positive = 4800</p>

<p>Initially with the imbalanced port stemmed dataset the model biased to majority class with high accuracy which was having worst performance in unseen data.</p>

<blockquote>
  <p>So I took <strong>1200 Negative and 1200 Positive and made it balanced</strong>.</p>
  
  <p>I implemented a Dense Model of 64 nodes in 4 layers with
  regularization of 0.5 using Keras and was able to achieve 60% accuracy
  in cross-validation while train accuracy goes as high as up to >95%.</p>
</blockquote>

<p>Looking at the <code>val_acc</code> and <code>acc</code> I feel that it is totally <strong>overfitting</strong> after around 20 epochs. In addition to that, it is also not able to generalize due to less number of data rows in the balanced dataset.</p>

<ul>
<li><strong>What are the ways to tackle such problems?</strong> </li>
<li><strong>Can One Class SVM help in single category text classification?</strong></li>
<li><strong>If One Class SVM can help then can anyone provide a basic example or resource for its implementation?</strong></li>
</ul>
","tensorflow, machine-learning, keras, svm, text-classification","<p>First of all, are you sure there are no positive classes in those 6000 you deemed negative? Rubbish in, rubbish out, make sure it's not the case here.</p>

<h1>What are the ways to tackle such problems</h1>

<p>In the order I would approach the problem.</p>

<ul>
<li><p>Make sure your data representation is good. If you are working with text data you should use word vectors like <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">pretrained word2vec</a>, also available in <a href=""https://www.tensorflow.org/guide/embedding"" rel=""nofollow noreferrer"">tensorflow</a> and <a href=""https://tfhub.dev/"" rel=""nofollow noreferrer"">tensorflow hub</a> (you can find a more advanced approach to word embeddings here like <a href=""https://tfhub.dev/google/elmo/2"" rel=""nofollow noreferrer"">ELMo</a>.</p></li>
<li><p>Getting more examples - this one should usually yield the best results (in case the step above is performed), but would take time.</p></li>
<li>Trying different algorithm - some algorithms don't really care about class imbalance. Decision trees and their variants being the most prominent I think. You should really check them out, starting at simple decision tree, than <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow noreferrer"">random forest</a> and boosted trees like <a href=""https://xgboost.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">xgboost</a>, <a href=""https://lightgbm.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">LightGBM</a> or <a href=""https://github.com/catboost/catboost"" rel=""nofollow noreferrer"">catboost</a>, the last three should perform quite similar I think, xgboost might be best choice due to abundance of materials on this topic. </li>
<li>Different metrics - accuracy is not the best one, as it's highly motivated by the negative class. Use other metrics like <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow noreferrer"">precision and recall</a> and focus on the latter (as your algorithm probably does not find enough positive classes).</li>
<li>Weighted loss - error made on positive examples would be weighted higher than the one on negative examples. I like it better than the next ones, as the model tries to accomodate to data. <a href=""https://stackoverflow.com/questions/35155655/loss-function-for-class-imbalanced-binary-classifier-in-tensor-flow"">Here</a> is an example of custom loss in Tensorflow.</li>
<li>Upsampling - reverse of what you did, giving your model same positive examples multiple times (each 5 times in this case, so there 6000 positive examples, as much as negatives). You do not lose information, but training takes longer (basically non-existent problem with your 7200 examples total).</li>
<li>Undersampling - what you did here, but you are losing a lot of information about negative class and it's traits. Better for bigger datasets, yours is small.</li>
<li>Creative approaches - it is harder with textual data, if this wasn't the case, you could try dimensionality reduction or other representation of data which could find an underlying cause of difference between positive and negative points. Hardest and probably would not help in your case.</li>
</ul>

<h1>Can One Class SVM help</h1>

<p>Doubt it, it's used for outlier detection. 1200 data points out of 7200 should not be considered an outlier. Furthermore it may share a lot of features with the negative class and you couldn't make use of the labeled data you currently have.</p>

<p>If you want to try it anyway, there is an implementation in sklearn <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html"" rel=""nofollow noreferrer"">here</a>.</p>
",4,2,802,2019-03-06 07:57:24,https://stackoverflow.com/questions/55018167/one-category-text-classification-on-imbalanced-data-set
Use word2vec word embeding as feature vector for text classification (simlar to count vectorizer/tfidf feature vector),"<p>I am trying to perform some text classification using machine learning and for that I have extracted feature vectors from the per-processed textual data using simple bag of words approach(count vectorizer) and tfidf vectorizer.  </p>

<p>Now I want to use word2vec i.e. word embedding as my feature vector similar as that of count vectorizer/tfidf vectorizer where I should be able to learn vocabulary from the train data and transform or fit the test data with the learned vocab but I can't find a way to implement that.  </p>

<pre><code>//I need something like this with word2vec

count = CountVectorizer()
train_feature_ vector =count.fit_transform(train_data)
test_feature_vector = count.fit(test_data)

//So I can train my model like this
mb = MultinomialNB()
mb.fit(train_feature_vector,y_train)
acc_score = mb.score(test_feature_vector,y_test)
print(""Accuracy ""+str(acc_score))
</code></pre>
","machine-learning, scikit-learn, word2vec, text-classification, word-embedding","<p>You first should understand what Word Embeddings are. When you apply a CountVectorizer or TfIdfVectorizer what you get is a <strong>sentence</strong> representation in a sparse way, commonly known as a One Hot encoding. The word embeddings representation are used to represent a <strong>word</strong> in a high dimensional space of real numbers.</p>

<p>Once you get your per word representation there are some ways to do this, check:<a href=""https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence"">How to get vector for a sentence from the word2vec of tokens in sentence</a></p>
",1,-1,2048,2019-03-11 07:06:19,https://stackoverflow.com/questions/55096725/use-word2vec-word-embeding-as-feature-vector-for-text-classification-simlar-to
Load a plain text file into PyTorch,"<p>I have two separate files, one is a text file, with each line being a single text. The other file contains the class label of that corresponding line. How do I load this into PyTorch and carry out further tokenization, embedding, etc? </p>
","python, nlp, pytorch, text-classification, torchtext","<p>What have you tried already? What you described is still not very PyTorch related, you can make a pre-processing script that loads all the sentences into single data structured, e.g.: a list of (text, label) tuple.You can also already split your data into training and hold-out set in this step. You can then dump all this into .csv files.</p>

<p>Then, one way to do it is in 3 steps:</p>

<ul>
<li>Implement the class <a href=""https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset"" rel=""nofollow noreferrer"">Dataset</a> - to load efficiently your data, reading the produced .csv files;</li>
<li>Have another like <strong>Vocabulary</strong> that keeps a mapping from tokens to ids and vice-verse;</li>
<li>Something like a <strong>Vectorizer</strong>, that converts your sentences into vectors, either one-hot-encondings or embeddings;</li>
</ul>

<p>Then you can use this to produce a vector representation of your sentences a pass it to a neural network.</p>

<p>Look into this notebook to understand all this in more detail:</p>

<ul>
<li><a href=""https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_3/3_5_Classifying_Yelp_Review_Sentiment.ipynb"" rel=""nofollow noreferrer"">Sentiment Classification</a></li>
</ul>
",0,0,2857,2019-03-18 07:25:21,https://stackoverflow.com/questions/55216339/load-a-plain-text-file-into-pytorch
Not able to load keras trained model,"<p>I am using following code to train HAN Network.
<a href=""https://www.kaggle.com/hsankesara/news-classification-using-han/notebook"" rel=""nofollow noreferrer"">Code Link</a></p>

<p>I have trained the model successfully but when I tried to load the model using keras load_model it gives me following error-
Unknown layer: AttentionWithContext</p>
","python, keras, deep-learning, text-classification","<p>Add the following function in the AttentionWithContext.py file:</p>

<pre><code>def create_custom_objects():
    instance_holder = {""instance"": None}

    class ClassWrapper(AttentionWithContext):
        def __init__(self, *args, **kwargs):
            instance_holder[""instance""] = self
            super(ClassWrapper, self).__init__(*args, **kwargs)

    def loss(*args):
        method = getattr(instance_holder[""instance""], ""loss_function"")
        return method(*args)

    def accuracy(*args):
        method = getattr(instance_holder[""instance""], ""accuracy"")
        return method(*args)
    return {""ClassWrapper"": ClassWrapper ,""AttentionWithContext"": ClassWrapper, ""loss"": loss,
            ""accuracy"":accuracy}
</code></pre>

<p>When loading the model:</p>

<pre><code>from AttentionWithContext import create_custom_objects

model = keras.models.load_model(model_path, custom_objects=create_custom_objects())

model.evaluate(X_test, y_test) # or model.predict
</code></pre>
",2,1,792,2019-03-18 09:31:50,https://stackoverflow.com/questions/55218234/not-able-to-load-keras-trained-model
NLP - which technique to use to classify labels of a paragraph?,"<p>I'm fairly new to NLP and trying to learn the techniques that can help me get my job done. </p>

<p>Here is my task: I have to classify stages of a drilling process based on text memos. </p>

<p><a href=""https://i.sstatic.net/j6BUS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/j6BUS.png"" alt=""enter image description here""></a></p>

<p>I have to classify labels for ""Activity"", ""Activity Detail"", ""Operation"" based on what's written in ""Com"" column. </p>

<p>I've been reading a lot of articles online and all the different kinds of techniques that I've read really confuses me. </p>

<p>The buzz words that I'm trying to understand are</p>

<ol>
<li>Skip-gram (prediction based method, Word2Vec)</li>
<li>TF-IDF (frequency based method)</li>
<li>Co-Occurrence Matrix (frequency based method)</li>
</ol>

<p>I am given about ~40,000 rows of data (pretty small, I know), and I came across an article that says neural-net based models like Skip-gram might not be a good choice if I have small number of training data. So I was also looking into frequency based methods too. Overall, I am unsure which technique is the best for me.</p>

<p>Here's what I understand:</p>

<ol>
<li>Skip-gram: technique used to represent words in a vector space. But I don't understand what to do next once I vectorized my corpus</li>
<li>TF-IDF: tells how important each word is in each sentence. But I still don't know how it can be applied on my problem</li>
<li>Co-Occurence Matrix: I don'y really understand what it is.</li>
<li>All the three techniques are to numerically represent texts. But I am unsure what step I should take next to actually classify labels.</li>
</ol>

<p>What approach &amp; sequence of techniques should I use to tackle my problem? If there's any open source Jupyter notebook project, or link to an article (hopefully with codes) that did the similar job done, please share it here.</p>
","python, machine-learning, text, nlp, text-classification","<p>Let's get things a bit clearer. You task is to create a system that will predict labels for given texts, right? And label prediction (classification) can't be done for unstructured data (texts). So you need to make your data structured, and then train and infer your classifier. Therefore, you need to induce two separate systems:</p>

<ol>
<li>Text vectorizer (as you said, it helps to numerically represent texts).</li>
<li>Classifier (to predict the labels for numerically represented texts).</li>
</ol>

<p>Skip-Gram and co-occurrence matrix are ways to vectorize your texts (here is a nice <a href=""http://veredshwartz.blogspot.com/2016/01/representing-words.html"" rel=""nofollow noreferrer"">article</a> that explains their difference). In case of skip-gram you could download and use a 3rd party model that already has mapping of vectors to most of the words; in case of co-occurrence matrix you need to build it on your texts (if you have specific lexis, it will be a better way). In this matrix you could use different measures to represent the degree of co-occurrence of words with words or documents with documents. TF-IDF is one of such measures (that gives a score for every word-document pair); there are a lot of others (PMI, BM25, etc). This <a href=""https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"" rel=""nofollow noreferrer"">article</a> should help to implement classification with co-occurrence matrix on your data. And this <a href=""https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"" rel=""nofollow noreferrer"">one</a> gives an idea how to do the same with Word2Vec. </p>

<p>Hope it helped! </p>
",3,1,2058,2019-03-20 07:46:55,https://stackoverflow.com/questions/55255780/nlp-which-technique-to-use-to-classify-labels-of-a-paragraph
"CountVectorizer values work alone in classifier, cannot get working when adding other features","<p>I have a CSV of twitter profile data, containing: name, description, followers count, following count, bot (class I want to predict)</p>

<p>I have successfully executed a classification model when using just the CountVectorizer values (xtrain) and Bot (ytrain). But have not been able to add this feature to my set of other features.</p>

<pre><code>vectorizer = CountVectorizer()
CountVecTest = vectorizer.fit_transform(training_data.description.values.astype('U'))
CountVecTest = CountVecTest.toarray()
arr = sparse.coo_matrix(CountVecTest)
training_data[""NewCol""] = arr.toarray().tolist()

rf = RandomForestClassifier(criterion='entropy', min_samples_leaf=10, min_samples_split=20)
rf = rf.fit(training_data[[""followers_count"",""friends_count"",""NewCol"",""bot""]], training_data.bot)
</code></pre>

<p>ERROR:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-54-7d67a6586592&gt; in &lt;module&gt;()
      1 rf = RandomForestClassifier(criterion='entropy', min_samples_leaf=10, min_samples_split=20)
----&gt; 2 rf = rf.fit(training_data[[""followers_count"",""friends_count"",""NewCol"",""bot""]], training_data.bot)

D:\0_MyFiles\0_Libraries\Documents\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py in fit(self, X, y, sample_weight)
    245         """"""
    246         # Validate or convert input data
--&gt; 247         X = check_array(X, accept_sparse=""csc"", dtype=DTYPE)
    248         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
    249         if sample_weight is not None:

D:\0_MyFiles\0_Libraries\Documents\Anaconda3\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    431                                       force_all_finite)
    432     else:
--&gt; 433         array = np.array(array, dtype=dtype, order=order, copy=copy)
    434 
    435         if ensure_2d:

ValueError: setting an array element with a sequence.
</code></pre>

<p>I did some debugging:</p>

<pre><code>print(type(training_data.NewCol))
print(type(training_data.NewCol[0]))
&gt;&gt;&gt; &lt;class 'pandas.core.series.Series'&gt;
&gt;&gt;&gt; &lt;class 'numpy.ndarray'&gt;
</code></pre>

<p>Any help would be appreciated. </p>
","python, scikit-learn, classification, text-classification, countvectorizer","<p>I would do this the other way around and add your features to your vectorization. Here is what I mean with a toy example:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np
from scipy.sparse import hstack, csr_matrix
</code></pre>

<p>Suppose now you have you features in a dataframe called <code>df</code> and your labels in <code>y_train</code>:</p>

<pre><code>df = pd.DataFrame({""a"":[1,2],""b"":[2,3],""c"":['we love cars', 'we love cakes']})
y_train = np.array([0,1])
</code></pre>

<p>You want to perform a text vectorization on column <code>c</code> and add the features <code>a</code> and <code>b</code> to your vectorization.</p>

<pre><code>vectorizer = CountVectorizer()
CountVecTest = vectorizer.fit_transform(df.c)

CountVecTest.toarray()
</code></pre>

<p>This will return:</p>

<pre><code>array([[0, 1, 1, 1],
       [1, 0, 1, 1]], dtype=int64)
</code></pre>

<p>But <code>CountVecTest</code> now is a scipy sparse matrix. So what you need to do is add your features to this matrix. Like this:</p>

<pre><code>X_train = hstack([CountVecTest, csr_matrix(df[['a','b']])])

X_train.toarray()
</code></pre>

<p>This will return, as expected:</p>

<pre><code>array([[0, 1, 1, 1, 1, 2],
       [1, 0, 1, 1, 2, 3]], dtype=int64)
</code></pre>

<p>Then you can train your random forest:</p>

<pre><code>rf = RandomForestClassifier(criterion='entropy', min_samples_leaf=10, min_samples_split=20)
rf.fit(X_train, y_train)
</code></pre>

<p>NB: In the code snippet you provided, you passed the label info (the ""bot"" column) to the training features, which you should obviously not do.</p>
",0,0,469,2019-03-20 20:56:55,https://stackoverflow.com/questions/55270053/countvectorizer-values-work-alone-in-classifier-cannot-get-working-when-adding
Doc2Vec &amp; classification - very poor results,"<p>I have a dataset of 6000 observations; a sample of it is the following:</p>

<pre><code>job_id      job_title                                           job_sector
30018141    Secondary Teaching Assistant                        Education
30006499    Legal Sales Assistant / Executive                   Sales
28661197    Private Client Practitioner                         Legal
28585608    Senior hydropower mechanical project manager        Engineering
28583146    Warehouse Stock Checker - Temp / Immediate Start    Transport &amp; Logistics
28542478    Security Architect Contract                         IT &amp; Telecoms
</code></pre>

<p>The goal is to predict the job sector of each row based on the job title.</p>

<p>Firstly, I apply some preprocessing on the <code>job_title</code> column:</p>

<pre><code>def preprocess(document):
    lemmatizer = WordNetLemmatizer()
    stemmer_1 = PorterStemmer()
    stemmer_2 = LancasterStemmer()
    stemmer_3 = SnowballStemmer(language='english')

    # Remove all the special characters
    document = re.sub(r'\W', ' ', document)

    # remove all single characters
    document = re.sub(r'\b[a-zA-Z]\b', ' ', document)

    # Substituting multiple spaces with single space
    document = re.sub(r' +', ' ', document, flags=re.I)

    # Converting to lowercase
    document = document.lower()

    # Tokenisation
    document = document.split()

    # Stemming
    document = [stemmer_3.stem(word) for word in document]

    document = ' '.join(document)

    return document

df_first = pd.read_csv('../data.csv', keep_default_na=True)

for index, row in df_first.iterrows():

    df_first.loc[index, 'job_title'] = preprocess(row['job_title'])
</code></pre>

<p>Then I do the following with <code>Gensim</code> and <code>Doc2Vec</code>:</p>

<pre><code>X = df_first.loc[:, 'job_title'].values
y = df_first.loc[:, 'job_sector'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)

tagged_train = TaggedDocument(words=X_train.tolist(), tags=y_train.tolist())
tagged_train = list(tagged_train)

tagged_test = TaggedDocument(words=X_test.tolist(), tags=y_test.tolist())
tagged_test = list(tagged_test)

model = Doc2Vec(vector_size=5, min_count=2, epochs=30)

training_set = [TaggedDocument(sentence, tag) for sentence, tag in zip(X_train.tolist(), y_train.tolist())]

model.build_vocab(training_set)

model.train(training_set, total_examples=model.corpus_count, epochs=model.epochs)   

test_set = [TaggedDocument(sentence, tag) for sentence, tag in zip(X_test.tolist(), y_test.tolist())]

predictors_train = []
for sentence in X_train.tolist():

    sentence = sentence.split()
    predictor = model.infer_vector(doc_words=sentence, steps=20, alpha=0.01)

    predictors_train.append(predictor.tolist())

predictors_test = []
for sentence in X_test.tolist():

    sentence = sentence.split()
    predictor = model.infer_vector(doc_words=sentence, steps=20, alpha=0.025)

    predictors_test.append(predictor.tolist())

sv_classifier = SVC(kernel='linear', class_weight='balanced', decision_function_shape='ovr', random_state=0)
sv_classifier.fit(predictors_train, y_train)

score = sv_classifier.score(predictors_test, y_test)
print('accuracy: {}%'.format(round(score*100, 1)))
</code></pre>

<p>However, the result which I am getting is 22% accuracy.</p>

<p>This makes me a lot suspicious especially because by using the <code>TfidfVectorizer</code> instead of the <code>Doc2Vec</code> (both with the same classifier) then I am getting 88% accuracy (!).</p>

<p>Therefore, I guess that I must be doing something wrong in how I apply the <code>Doc2Vec</code> of <code>Gensim</code>.</p>

<p>What is it and how can I fix it?</p>

<p>Or it it simply that my dataset is relatively small while more advanced methods such as word embeddings etc require way more data?</p>
","python, classification, gensim, text-classification, doc2vec","<p>You don't mention the size of your dataset - in rows, total words, unique words, or unique classes. Doc2Vec works best with lots of data. Most published work trains on tens-of-thousands to millions of documents, of dozens to thousands of words each. (Your data appears to only have 3-5 words per document.)</p>

<p>Also, published work tends to train on data where every document has a unique-ID. It can sometimes make sense to use known-labels as tags instead of, or in addition to, unique-IDs. But it isn't necessarily a better approach. By using known-labels as the only tags, you're effectively only training one doc-vector per label. (It's essentially similar to concatenating all rows with the same tag into one document.)</p>

<p>You're inexplicably using fewer <code>steps</code> in inference than <code>epochs</code> in training - when in fact these are analogous values. In recent versions of <code>gensim</code>, inference will by default use the same number of inference epochs as the model was configured to use for training. And, it's more common to use <em>more</em> epochs during inference than training. (Also, you're inexplicably using different starting <code>alpha</code> values for inference for both classifier-training and classifier-testing.)</p>

<p>But the main problem is likely your choice of tiny <code>size=5</code> doc vectors. Instead of the <code>TfidfVectorizer</code>, which will summarize each row as a vector of width equal to the unique-word count – perhaps hundreds or thousands of dimensions – your <code>Doc2Vec</code> model summarizes each document as just 5 values. You've essentially lobotomized <code>Doc2Vec</code>. Usual values here are 100-1000 – though if the dataset is tiny smaller sizes may be required.</p>

<p>Finally, the lemmatization/stemming may not be strictly necessary and may even be destructive. Lots of <code>Word2Vec</code>/<code>Doc2Vec</code> work doesn't bother to lemmatize/stem - often because there's plentiful data, with many appearances of all word forms. </p>

<p>These steps are most likely to help with smaller data, by making sure rarer word forms are combined with related longer forms to still get value from words that would otherwise be too rare to be retained (or get useful vectors). </p>

<p>But I can see many ways they might hurt for your domain. <code>Manager</code> and <code>Management</code> won't have exactly the same implications in this context, but could both be stemmed to <code>manag</code>. Similar for <code>Security</code> and <code>Securities</code> both becoming <code>secur</code>, and other words. I'd only perform these steps if you can prove through evaluation that they're helping. (Are the words passed to the <code>TfidfVectorizer</code> being lemmatized/stemmed?)</p>
",5,3,5094,2019-03-22 23:51:46,https://stackoverflow.com/questions/55309197/doc2vec-classification-very-poor-results
Error multiclass text classification with pre-trained BERT model,"<p>I am trying to classify text in 34 mutually exclusive classes using Google's BERT pre-trained model. After preparing the ""train"", ""dev"" and ""test"" TSV files which BERT expects as input, I try to execute the following command in my Colab (Jupyter) Notebook</p>

<pre><code>!python bert/run_classifier.py 
--task_name=cola
--do_train=true 
--do_eval=true 
--data_dir=./Bert_Input_Folder 
--vocab_file=./uncased_L-24_H-1024_A-16/vocab.txt 
--bert_config_file=./uncased_L-24_H-1024_A-16/bert_config.json 
--init_checkpoint=./uncased_L-24_H-1024_A-16/bert_model.ckpt 
--max_seq_length=512 
--train_batch_size=32 
--learning_rate=2e-5 
--num_train_epochs=3.0 
--output_dir=./Bert_Output_Folder
</code></pre>

<p>I get the following error</p>

<pre><code>WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:

https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.
WARNING:tensorflow:Estimator's model_fn (&lt;function model_fn_builder..model_fn at 0x7f4b945a01e0&gt;) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:Using config: {'_model_dir': './Bert_Output_Folder', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
rewrite_options {
meta_optimizer_iterations: ONE
}
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4b94f366a0&gt;, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}
INFO:tensorflow:_TPUContext: eval_on_tpu True
WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.
INFO:tensorflow:Writing example 0 of 23834
Traceback (most recent call last):
File ""bert/run_classifier.py"", line 981, in 
tf.app.run()
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
File ""bert/run_classifier.py"", line 870, in main
train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)
File ""bert/run_classifier.py"", line 490, in file_based_convert_examples_to_features
max_seq_length, tokenizer)
File ""bert/run_classifier.py"", line 459, in convert_single_example
label_id = label_map[example.label]
KeyError: '33'
</code></pre>

<p>In the ""run_classifier.py"" script, I have modified the ""get_labels()"" function, originally written for a binary classification task, to return all my 34 classes</p>

<pre><code>def get_labels(self): 
""""""See base class."""""" 
return [""0"", ""1"", ""2"", ..., ""33""]

</code></pre>

<p>Any idea what is wrong or if I am missing additional necessary modifications?</p>

<p>Thanks!</p>
","python, text-classification, multiclass-classification, transfer-learning","<p>Solved just by replacing <code>['0', '1', '2', ... '33']</code> with <code>[str(x) for x in range(34)]</code> in the get_label function (the two are actually equivalent, but for some unknown reason, this solved the issue).</p>
",2,2,1664,2019-04-07 16:12:06,https://stackoverflow.com/questions/55561060/error-multiclass-text-classification-with-pre-trained-bert-model
I want to predict the no.of updates for a new incident? how to do that in python?,"<p>I have a data set of n incidents which has some information to it. Information such as description (text is in either english or german) and no.of updates information(in intiger). I want to predict the no.of updates for a new incident regardless of the defined language(german or english).</p>

<p>I just started learnig python. Please suggest the action plan libraies/algortims used in python?</p>
","python, knn, text-classification, naivebayes, workload","<p>I'd suggest <a href=""https://pypi.org/project/pandas/"" rel=""nofollow noreferrer"">pandas</a> and <a href=""https://pypi.org/project/nltk/"" rel=""nofollow noreferrer"">nltk</a>.</p>

<p>However, @Emil is right. You should probably follow a course or tutorial with <a href=""https://www.anaconda.com/distribution/"" rel=""nofollow noreferrer"">Anaconda</a> and Jupyter Notebooks or just any other Data Science material.</p>
",0,0,61,2019-04-11 13:24:57,https://stackoverflow.com/questions/55633613/i-want-to-predict-the-no-of-updates-for-a-new-incident-how-to-do-that-in-python
&quot;Number of features of the model must match the input&quot; while trying to predict new unseen data,"<p>I trained a model on some Wikipedia articles divided by two categories (each category has 12 articles).</p>

<p>Below is how I created the model, trained it and pickled it:</p>

<pre><code>import numpy as np
import re
import nltk
from sklearn.datasets import load_files
import pickle
from nltk.corpus import stopwords
data = load_files(r'[...]review_polarity')
X, y = data.data, data.target
documents = []
from nltk.stem import WordNetLemmatizer
stemmer = WordNetLemmatizer()
for sen in range(0, len(X)):  
    # Remove all the special characters
    document = re.sub(r'\W', ' ', str(X[sen]))

    # remove all single characters
    document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)

    # Remove single characters from the start
    document = re.sub(r'\^[a-zA-Z]\s+', ' ', document) 

    # Substituting multiple spaces with single space
    document = re.sub(r'\s+', ' ', document, flags=re.I)

    # Removing prefixed 'b'
    document = re.sub(r'^b\s+', '', document)

    # Converting to Lowercase
    document = document.lower()

    # Lemmatization
    document = document.split()

    document = [stemmer.lemmatize(word) for word in document]
    document = ' '.join(document)

    documents.append(document)

from sklearn.feature_extraction.text import TfidfTransformer
tfidfconverter = TfidfTransformer()
X = tfidfconverter.fit_transform(X).toarray()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=1000,random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

with open('text_classifier', 'wb') as picklefile:
    pickle.dump(classifier, picklefile)
</code></pre>

<p>Then, I loaded the pickle file and tried to predict the classification for a new unseen article:</p>

<pre><code>import pickle
import sys, os
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

with open(os.path.join(sys.path[0], 'text_classifier'), 'rb') as training_model:
    model = pickle.load(training_model)

with open(os.path.join(sys.path[0], 'article.txt'), 'rb') as f:
    X = [f.read()]

documents = []
stemmer = WordNetLemmatizer()

for sen in range(0, len(X)):  
    # Remove all the special characters
    document = re.sub(r'\W', ' ', str(X[sen]))

    # remove all single characters
    document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)

    # Remove single characters from the start
    document = re.sub(r'\^[a-zA-Z]\s+', ' ', document) 

    # Substituting multiple spaces with single space
    document = re.sub(r'\s+', ' ', document, flags=re.I)

    # Removing prefixed 'b'
    document = re.sub(r'^b\s+', '', document)

    # Converting to Lowercase
    document = document.lower()

    # Lemmatization
    document = document.split()

    document = [stemmer.lemmatize(word) for word in document]
    document = ' '.join(document)

    documents.append(document)

tfidfconverter = TfidfVectorizer(max_features=1500, min_df=0, max_df=1.0, stop_words=stopwords.words('english'))
X = tfidfconverter.fit_transform(documents).toarray()

y_pred = model.predict(X)
print y_pred
</code></pre>

<p>I got the following error while calling the predict function:</p>

<p><em>Number of features of the model must match the input. Model n_features is 10 and input n_features is 47</em></p>

<p>It seems like the new article got a numpy array of 47 features, while the trained model works with arrays of 10 features. I'm not sure I understood this correctly, I'd be glad if you can help me understand better and make it work.</p>

<p>Thanks!</p>
","python, numpy, text-classification","<p>The answer is that I should have used the ""transform"" function instead of ""fit_transform"" for the new unseen data in order to keep the number of features the same.</p>
",0,-1,116,2019-04-12 16:05:09,https://stackoverflow.com/questions/55655621/number-of-features-of-the-model-must-match-the-input-while-trying-to-predict-n
How can I convert a Pandas DataFrame of vectors and labels into input for an RNN in TensorFlow,"<p>I'm working on a text classifier using an LSTM in TensorFlow and can't figure out the format of the input data.
My input data is a Pandas Dataframe with one feature column and one label column.</p>

<p>My feature column is a 2D array representing an array of vectors and my label column is a String, an example of my data input is below.</p>

<p>How do I convert this Dataframe into a dataset that can be used as input to be used in a Tensorflow.Keras model?</p>

<p>I've tried converting the Dataframe into a TensorFlow.Dataset dataset using tf.data.Dataset.from_tensor_slices but this produces a TypeError</p>

<pre class=""lang-py prettyprint-override""><code>##Building input data
test01 = ([[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]],'label1')
test02 = ([[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[0,0,0,0]],'label2')
test03 = ([[1,1,1,1],[1,1,1,1],[1,1,1,1],[0,0,0,0],[1,1,1,1]],'label3')
test04 = ([[1,1,1,1],[0,0,0,0],[1,1,1,1],[1,1,1,1],[1,1,1,1]],'label1')
test_data = [test01,test02,test03,test04]


##DataFrame from data
columns = ['feature','label']
t_df = pd.DataFrame(data = test_data, columns = columns)

</code></pre>

<pre class=""lang-py prettyprint-override""><code>##Convert to TensorFlow Dataset
dataset = tf.data.Dataset.from_tensor_slices((t_df['feature'], t_df['label']))
</code></pre>

<p>This produces the following error:</p>

<pre class=""lang-py prettyprint-override""><code>TypeError: Expected binary or unicode string, got [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]
</code></pre>

<pre class=""lang-py prettyprint-override""><code>##TensorFlow Model Example 
model = tf.keras.Sequential([,
    tf.keras.layers.LSTM(input_shape),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
</code></pre>
","python, tensorflow, machine-learning, text-classification","<p>In this case you just passed slightly wrong dimensions. <code>from_tensor_slices</code> expects a list of objects, not a nested list.</p>

<pre><code>dataset = tf.data.Dataset.from_tensor_slices(([i for i in t_df['feature']], t_df['label']))
</code></pre>
",0,0,782,2019-04-13 16:59:24,https://stackoverflow.com/questions/55667856/how-can-i-convert-a-pandas-dataframe-of-vectors-and-labels-into-input-for-an-rnn
How can I test the model I created with Keras,"<p>I was working on a text classification problem with Keras. But
I tried to test the model I created, but I cannot use the TfidfVectorizer to test the class.</p>

<pre><code>with open('model_architecture.json', 'r') as f:
model = model_from_json(f.read())

model.load_weights('model_weights.h5')
</code></pre>

<p>After installing the model I have prepared a test list to use.</p>

<pre><code>test_data=[""sentence1"",""sentence2"",""sentence3""]
</code></pre>

<p>No problem so far</p>

<p>But..</p>

<pre><code>tf=TfidfVectorizer(binary=True)
train=tf.fit_transform(test_data)
test=tf.transform(test_data)
print(model.predict_classes(test))

ValueError: Error when checking input: expected dense_1_input to have shape (11103,) but got array with shape (92,)
</code></pre>

<p>I get such an error</p>

<p>And I also tried</p>

<pre><code>tf=TfidfVectorizer(binary=True)
test=tf.transform(test_data)

sklearn.exceptions.NotFittedError: TfidfVectorizer - Vocabulary wasn't fitted.
</code></pre>

<p>but I have received such an error, I learned that the fit () method should come before this can not be used.</p>

<p>But I still can't test the model I'm training</p>
","python, keras, text-classification","<p>You need to encode your test data using the exact same TfIdfVectorizer object you fit and used to transform the original training data, way back when you originally trained the model. If you fit a different TfidfVectorizer to your test data then the encoding (including the vocab length) will be completely different and it will not work. It is this difference in vocab length that is the proximate cause of the error you're seeing. However, even if you do get the dimensions match purely by chance, it still won't work because the model was <em>trained</em> with an encoding that maps ""cat"" to 42, or whatever, while you're <em>testing</em> it with an encoding that maps ""cat"" to 13 or something. You'd basically be feeding it scrabbled nonsense. There really is no alternative but to go and get the original TfidfVectorizer, or at least to fit a TfidfVectorizer to the exact same documents with the exact same configuration. If this is not possible, then you'll simply have to train a new model and this time remember to save off the TfidfVectorizer as well.</p>

<p>Normally the fitted preprocessing are saved to a pickle file via <code>pickle.dump()</code> during the initial training, and loading with <code>pickle.load()</code> for testing and production, similar to what you did for <code>model_architecture.json</code> and <code>model_weights.hd5</code>. It is also convenient to put everything together into an <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"" rel=""nofollow noreferrer"">sklearn pipeline</a> so you only have to pickle one object, but I'm not sure how works together with the Keras model.</p>
",1,0,254,2019-04-26 23:56:44,https://stackoverflow.com/questions/55876116/how-can-i-test-the-model-i-created-with-keras
How to use text classification with dataframe in python,"<p>I'm using text classification to classify dialects. However, I noticed that I have to use countVectorizer like so:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer  
vectorizer = CountVectorizer(max_features=200, min_df=2, max_df=0.7, stop_words=stopwords.words('arabic'))  
X = vectorizer.fit_transform(X).toarray()
</code></pre>

<p>what happens is that I have make a new text file for every line in my csv file. I have collected 1000 tweets from twitter. and they're labeled. and I have them as csv in one file.  </p>

<p>I have 2 questions:</p>

<ol>
<li>Do I have to do this? separate every line in one text file? or I can use it as a dataframe </li>
<li>Do I have to use countVectorizer in text classification? is there another way?</li>
</ol>
","python, dataframe, machine-learning, text-classification, countvectorizer","<ol>
<li><p>No, you dont have to separate every line in a new text file. If you look at the official sklearn document example <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a> , you will see how to do it. If you want to follow that example, then you will have to convert your csv column of tweets from dataframe to a list and pass it to the function the same way they did it in the document example.</p></li>
<li><p>No, you dont have to use countvectorizer. there are several other ways to do this like Tf-IDF, Word2Vec, bag-of-words, etc. There are several method of converting text to vectors for classification. For your case, I believe TF-IDF or Word2Vec will work fine.</p></li>
</ol>
",1,1,976,2019-05-08 18:43:54,https://stackoverflow.com/questions/56047174/how-to-use-text-classification-with-dataframe-in-python
Text classification issue,"<p>I'm newbie in ML and try to classify text into two categories. My dataset is made with Tokenizer from medical texts, it's unbalanced and there are 572 records for training and 471 for testing.</p>

<p>It's really hard for me to make model with diverse predict output, almost all values are same. I've tired using models from examples like <a href=""https://www.tensorflow.org/alpha/tutorials/keras/basic_text_classification"" rel=""nofollow noreferrer"">this</a> and to tweak parameters myself but output is always without sense</p>

<p>Here are tokenized and prepared <a href=""https://drive.google.com/file/d/18tu1bxI_hZ7FDXOCTwVf2b20hH3CzByM/view?usp=sharing"" rel=""nofollow noreferrer"">data</a></p>

<p>Here is script: <a href=""https://gist.github.com/SYtor/702e5b4e41d27032d92b334b0d4cf296"" rel=""nofollow noreferrer"">Gist</a></p>

<p>Sample model that I used</p>

<pre><code>    sequential_model = keras.Sequential([
        layers.Dense(15, activation='tanh',input_dim=vocab_size),
        layers.BatchNormalization(),
        layers.Dense(8, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(1, activation='sigmoid')
    ])

    sequential_model.summary()
    sequential_model.compile(optimizer='adam',
                             loss='binary_crossentropy',
                             metrics=['acc'])

    train_history = sequential_model.fit(train_data,
                                         train_labels,
                                         epochs=15,
                                         batch_size=16,
                                         validation_data=(test_data, test_labels),
                                         class_weight={1: 1, 0: 0.2},
                                         verbose=1)
</code></pre>

<p>Unfortunately I can't share datasets. 
Also I've tired to use keras.utils.to_categorical with class labels but it didn't help</p>
","tensorflow, machine-learning, keras, text-classification","<p>Your loss curves makes sense as we see the network overfit to training set while we see the usual bowl-shaped validation curve. </p>

<p>To make your network perform better, you can always deepen it (more layers), widen it (more units per hidden layer) and/or add more nonlinear activation functions for your layers to be able to map to a wider range of values.</p>

<p>Also, I believe the reason why you originally got so many repeated values is due to the size of your network. Apparently, each of the data points has roughly 20,000 features (pretty large feature space); the size of your network is too small and the possible space of output values that can be mapped to is consequently smaller. I did some testing with some larger hidden unit layers (and bumped up the number of layers) and was able to see that the prediction values did vary: [0.519], [0.41], [0.37]...</p>

<p>It is also understandable that your network performance varies so because the number of features that you have is about 50 times the size of your training (usually you would like a smaller proportion). Keep in mind that training for too many epochs (like more than 10) for so small training and test dataset to see improvements in loss is not great practice as you can seriously overfit and is probably a sign that your network needs to be wider/deeper.</p>

<p>All of these factors, such as layer size, hidden unit size and even number of epochs can be treated as hyperparameters. In other words, hold out some percentage of your training data as part of your validation split, go one by one through the each category of factors and optimize to get the highest validation accuracy. To be fair, your training set is not too high, but I believe you should hold out some 10-20% of the training as a sort of validation set to tune these hyperparameters given that you have such a large number of features per data point. At the end of this process, you should be able to determine your true test accuracy. This is how I would optimize to get the best performance of this network. Hope this helps. </p>

<p><a href=""https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio"">More about training, test, val split</a></p>
",1,0,85,2019-05-09 13:01:07,https://stackoverflow.com/questions/56060044/text-classification-issue
Is Naive Bayes biased?,"<p>I have  a use case where in text needs to be classified into one of the three categories. I started with Naive Bayes [Apache OpenNLP, Java] but i was informed that the algorithm is biased, meaning if my training data has 60% of data as classA and 30% as classB and 10% as classC then the algorithm tends to biased towards ClassA and thus predicting the other class texts to be of classA. </p>

<p>If this is true is there a way to overcome this issue? </p>

<p>There are other algorithm that i came across like SVM Classifier or logistic regression (maximum entropy model), however I am not sure which will be more suitable for my use case. Please advise.</p>
","machine-learning, nlp, text-classification, naivebayes","<blockquote>
  <p><em>there a way to overcome this issue?</em></p>
</blockquote>

<p>Yes, there is.  But first you need to understand why it happens?</p>

<p>Basically your dataset is imbalanced.</p>

<p>An imbalanced dataset means instances of one of the two classes is higher than the other, in another way, the number of observations is not the same for all the classes in a classification dataset.</p>

<p>In this scenario, your model becomes bias towards the class with majority of samples as you have more training data for that class.</p>

<p><strong>Solutions</strong></p>

<ol>
<li><strong>Under sampling:</strong>
Randomly removing samples from majority class to make dataset balance.</li>
<li><strong>Over sampling:</strong> 
Adding more samples of minority classes to makes dataset balance.</li>
<li><strong>Change Performance Metrics</strong>
Use <code>F1-score</code>, 'recall<code>or</code>precision` to measure the performance of your model.</li>
</ol>

<p>There are few more solutions, if you want to know more refer this <a href=""https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"" rel=""nofollow noreferrer"">blog</a></p>

<blockquote>
  <p><em>There are other algorithm that i came across like SVM Classifier or logistic regression (maximum entropy model), however I am not sure
  which will be more suitable for my usecase</em></p>
</blockquote>

<p>You will never know unless you try, I would suggest you try 3-4 different algorithms on your data.</p>
",1,1,1040,2019-05-10 06:22:51,https://stackoverflow.com/questions/56071840/is-naive-bayes-biased
How to make text classification gives a None category,"<p>I'm doing text classification for dialects. After I trained it for 3 types of dialects, I tested it with the test data I have. However, now suppose I'm going to extract a tweet from twitter, and ask the classifier to output the corresponding dialect, but what if the tweet wasn't written in any of those 3 dialects? I assume that he will give a category regardless, but that would be false positive. Therefore, I want him to give a None category. How to do that? Should I also give training data with None labels?</p>
","python, machine-learning, text-classification, countvectorizer","<p>If you want to predict a new category (in this case None) with the same classifier, you have to provide  training data corresponding to this category.</p>

<p>Another idea (better discussed here: <a href=""https://stats.stackexchange.com/questions/174856/semi-supervised-classification-with-unseen-classes"">https://stats.stackexchange.com/questions/174856/semi-supervised-classification-with-unseen-classes</a>) is to train a multi-class classifier which assigns a sentence to one of the dialects; then train various one-class classifiers, one for each dialect, which can confirm or deny multi-class classifier predictions.</p>

<p><em>An example:<br>
Dialects A, B, C.<br><br>
Multi-class classifier assigns sentence to dialect A.<br>
One-class classifier for dialect A classifies sentence as dialect A.<br>
Sentence belongs to dialect A.<br><br>
Multi-class classifier assigns sentence to dialect A.<br>
One-class classifier for dialect A classifies sentence as not dialect A.<br>
Sentence belongs to unknown dialect (None).</em></p>
",1,0,1314,2019-05-10 09:26:11,https://stackoverflow.com/questions/56074591/how-to-make-text-classification-gives-a-none-category
Mapping doc2vec paragraph representation to its class tag post-training,"<p>I have trained Doc2Vec paragraph embeddings on text documents using the <code>Doc2Vec</code> module in Python's <code>gensim</code> package. Normally each document is tagged with a unique ID, yielding a unique output representation, as follows (see <a href=""https://fzr72725.github.io/2018/01/14/genism-guide.html"" rel=""nofollow noreferrer"">this link</a> for details):</p>

<pre><code>def tag_docs(docs, col):
    tagged = docs.apply(lambda r: TaggedDocument(words=simple_preprocess(r[col]), tags=[r.label]), axis=1)
    return tagged
</code></pre>

<p>However, you can also tag a group of documents with the same tag in order to train class representations, which is what I did here. You can query the number of output representations with the following command:</p>

<pre><code>print(model.docvecs.count)
</code></pre>

<p>My question is as follows: I trained the model of <code>n</code> classes of documents, yielding <code>n</code> document vectors in <code>model.docvecs</code>. Now I want to map each document vector to the corresponding class tag. How can I establish which vector is associated with which tag?</p>
","python, gensim, word2vec, text-classification, doc2vec","<p>If <code>classA</code> was one of the document-tags you provided during training, then <code>model.docvecs['classA']</code> will return the single doc-vector that was learned for that tag from training. </p>

<p>If you have another new vector – for example one inferred on new text via <code>model.infer_vector(words)</code>, then you can get a list of which learned doc-vectors in the model are closest via <code>model.docvecs.most_similar(positive=[new_vector])</code>.</p>

<p>If your true aim to classify new documents into one (or more) of these classes, then taking the top <code>most_similar()</code> result is one crude way to do that. </p>

<p>But  having reduced all classes to just a single summary vector (the one vector learned for that tag), then taking just the one nearest-neighbor of a new-document, may not perform well. It somewhat forces an assumption that that classes are very simple shapes in the n-dimensional space. </p>

<p>For classification, you may want to let all documents get individual vectors (not based on their known classes, or in addition to their known classes), then train a separate classifier on that set of (doc-vector, label) labeled-data. That could discover finer-grained, and oddly-shaped boundaries between the classes. </p>
",1,0,329,2019-05-10 10:59:45,https://stackoverflow.com/questions/56076298/mapping-doc2vec-paragraph-representation-to-its-class-tag-post-training
Why scikit learn confusion matrix is reversed?,"<p>I have 3 questions:</p>

<p>1)</p>

<p>The confusion matrix for sklearn is as follows:</p>

<pre><code>TN | FP
FN | TP
</code></pre>

<p>While when I'm looking at online resources, I find it like this:</p>

<pre><code>TP | FP
FN | TN
</code></pre>

<p>Which one should I consider? </p>

<p>2) </p>

<p>Since the above confusion matrix for scikit learn is different than the one I find in other rescources, in a multiclass confusion matrix, what's the structure will be? I'm looking at this post here:
<a href=""https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal"">Scikit-learn: How to obtain True Positive, True Negative, False Positive and False Negative</a>
In that post, @lucidv01d had posted a graph to understand the categories for multiclass. is that category the same in scikit learn? </p>

<p>3)</p>

<p>How do you calculate the accuracy of a multiclass? for example, I have this confusion matrix:</p>

<pre><code>[[27  6  0 16]
 [ 5 18  0 21]
 [ 1  3  6  9]
 [ 0  0  0 48]]
</code></pre>

<p>In that same post I referred to in question 2, he has written this equation:</p>

<h1>Overall accuracy</h1>

<p>ACC = (TP+TN)/(TP+FP+FN+TN)</p>

<p>but isn't that just for binary? I mean, for what class do I replace TP with? </p>
","scikit-learn, text-classification, confusion-matrix, performance-measuring","<p>As the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"" rel=""noreferrer"">sklearn guide</a> says: <em>""(Wikipedia and other references may use a different convention for axes)""</em></p>

<p>What does it mean? When building the confusion matrix, the first step is to decide where to put predictions and real values (true labels).  There are two possibilities:</p>

<ul>
<li>put predictions to the columns, and true labes to rows</li>
<li>put predictions to the rows, and true labes to columns</li>
</ul>

<p>It is totally <strong>subjective</strong> to decide which way you want to go. From this picture, <a href=""https://i.sstatic.net/kD0TO.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/kD0TO.png"" alt=""Sklearn&#39;s Confusion Matrix""></a> explained in <a href=""https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix"" rel=""noreferrer"">here</a>, it is clear that scikit-learn's convention is to put predictions to columns, and true labels to rows. </p>

<p>Thus, according to scikit-learns convention, it means:</p>

<ul>
<li>the first column contains, negative predictions (TN and FN)</li>
<li>the second column contains, positive predictions (TP and FP)</li>
<li>the first row contains negative labels (TN and FP)</li>
<li>the second row contains positive labels (TP and FN)</li>
<li>the <strong>diagonal</strong>  contains the number of correctly predicted labels.</li>
</ul>

<p>Based on this information I think you will be able to solve part 1 and part 2 of your questions.</p>

<p>For part 3, you just sum the values in the diagonal and divide by the sum of all elements, which will be </p>

<p>(27 + 18 + 6 + 48) / (27 + 18 + 6 + 48 + 6 + 16 + 5 + 21 + 1 + 3 + 9)</p>

<p>or you can just use score() function. </p>
",5,14,9399,2019-05-10 12:57:35,https://stackoverflow.com/questions/56078203/why-scikit-learn-confusion-matrix-is-reversed
How can I know the labels of my predicted classification?,"<p>I have trained my classifier on 3 dialects using text classification. And this was the confusion matrix and precision:</p>

<p>confusion matrix</p>

<pre><code>[[27  6  0 16]
 [ 5 18  0 21]
 [ 1  3  6  9]
 [ 0  0  0 48]]
</code></pre>

<p>The precision</p>

<pre><code>[0.81818182 0.66666667 1.         0.5106383 ]
</code></pre>

<p>How to know which row in the confusion matrix and which element in the precision belong to what dialect I have? I provided the training data to the classifier with the following labels :</p>

<pre><code>Egyptian
Sudan
Iraqi
Jordan
</code></pre>

<p>Here's the code, I used RandomForestClassifier:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=1000, random_state=0)  
classifier.fit(X, labels)  
test_pred = classifier.predict(y)
precision_score(labels_test,test_pred,average=None)
</code></pre>

<p>output:</p>

<pre><code>array([0.91024735, 0.94929397, 0.98622273, 0,95343322])
</code></pre>
","python, machine-learning, text-classification, multiclass-classification","<p><code>classifier.classes_</code> will give you the labels the classifier is scoring on in the order they are stored in the classifier object. That should be the same order as the outputs you've already got, though I would verify that with some spot-checking of your predictions to be sure</p>
",1,0,694,2019-05-10 15:35:48,https://stackoverflow.com/questions/56080782/how-can-i-know-the-labels-of-my-predicted-classification
How to use bigrams + trigrams + word-marks vocabulary in countVectorizer?,"<p>I'm using text classification with naive Bayes and countVectorizer to classify dialects. I read a research paper that the author has used a combination of :</p>

<pre><code>bigrams + trigrams + word-marks vocabulary 
</code></pre>

<p>He means by word-marks here, the words that are specific to a certain dialect.</p>

<p>How can I tweak those parameters in countVectorizer? </p>

<h3>word marks</h3>

<p>So those are examples of word marks, but it isn't what I have, because mine are arabic. So I translated them.</p>

<pre><code>word_marks=['love', 'funny', 'happy', 'amazing']
</code></pre>

<p>Those are used to classify a text.</p>

<p>Also, in the this post:
<a href=""https://stackoverflow.com/questions/24005762/understanding-the-ngram-range-argument-in-a-countvectorizer-in-sklearn"">Understanding the `ngram_range` argument in a CountVectorizer in sklearn</a></p>

<p>There was this answer :</p>

<pre><code>&gt;&gt;&gt; v = CountVectorizer(ngram_range=(1, 2), vocabulary={""keeps"", ""keeps the""})
&gt;&gt;&gt; v.fit_transform([""an apple a day keeps the doctor away""]).toarray()
array([[1, 1]])  # unigram and bigram found
</code></pre>

<p>I couldn't understand the output, what does [1,1] mean here? and how was he able to use ngram with vocabulary? aren't both of them mutually exclusive? </p>
","machine-learning, nlp, text-classification, countvectorizer","<p>You want to use the n_gram range argument to use bigrams and trigrams. In your case, it would be CountVectorizer(ngram_range=(1, 3)).</p>

<p>See the accepted answer <a href=""https://stackoverflow.com/questions/24005762/understanding-the-ngram-range-argument-in-a-countvectorizer-in-sklearn"">to this question</a> for more details.</p>

<p>Please provide example of ""word-marks"" for the other part of your question.</p>

<p>You may have to run CountVectorizer twice - once for n-grams and once for your custom word-mark vocabulary. You can then concatenate the two outputs from the two CountVectorizers to get a single feature set of n-gram counts and custom vocabulary counts. The answer to the above question also explains how to specify a custom vocabulary for this second use of CountVectorizer.</p>

<p>Here's a <a href=""https://stackoverflow.com/questions/9236926/concatenating-two-one-dimensional-numpy-arrays"">SO answer</a> on concatenating arrays</p>
",2,0,2252,2019-05-10 19:00:53,https://stackoverflow.com/questions/56083449/how-to-use-bigrams-trigrams-word-marks-vocabulary-in-countvectorizer
How to extract manually annotated tweets using Twitter API?,"<p>I'm using text classification to classify dialects. First I need a large manually annotated tweets, and I have read a research paper that says:</p>

<blockquote>
  <p>We have collected tweets that were published during June 2015. Arabic
  linguists manually annotated a small part of these tweets, so we got
  51,589 tweets with correct dialectal labels. These tweets were
  manually found in Twitter and annotated by the linguists.</p>
</blockquote>

<p>So this researcher was able to extract those tweets, I wanted to contact him but their emails weren't valid. He says those tweets were published during June 2015. How can I extract those tweets?</p>
","twitter, text-classification, corpus, tagged-corpus","<p>I would have to assume that the researcher did that in realtime during June 2015.</p>

<p>Today, the only way to do that would be to use the Full Archive Search API (a premium, paid offering from Twitter) to search for those Tweets. In terms of the annotations, those would have been part of their research; Twitter does not annotate Tweets with dialectal labels.</p>
",1,0,96,2019-05-11 08:47:23,https://stackoverflow.com/questions/56088680/how-to-extract-manually-annotated-tweets-using-twitter-api
What is the most efficient way to extract tweets that has certain dialect?,"<p>I'm doing text classification for Arabic dialects, and I need to collect data. So I'm using Twitter API to do that.</p>
<h3>However, the problem is:</h3>
<p>I need to find tweets that have the same dialect.</p>
<h3>One solution I have is:</h3>
<p>Is to collect tweets based on certain keywords only one dialect have</p>
<h3>one problem with that solution is:</h3>
<p>When I test the data, of course the accuracy will be high. Because the test data will contain those keywords that I used to collect the dataset.</p>
<h3>what I'm looking for</h3>
<p>Isn't there another way to circumvent this bias?</p>
","python, twitter, dataset, text-classification","<p>Note that this is a platform to get advice with particular code, not to discuss methodologies. </p>

<p>That said, you could manually collect data from this particular dialect and collect other tweets as well and then build a classifier that predicts to what group a tweet belongs. </p>
",1,-2,42,2019-05-13 18:39:03,https://stackoverflow.com/questions/56118110/what-is-the-most-efficient-way-to-extract-tweets-that-has-certain-dialect
How to train a classifier to detect vernacular from grammatical language?,"<p>I'm using text classification to classify Arabic dialects, so far I have 4 dialects. However, now I want the classifier to detect the formal(standard or grammatical) language of those dialects which is called MSA(Modern Standard Arabic). </p>

<p>Should I use grammatical analysis? build a language model? or I do the same as I did with the dialects by collecting MSA tweets and then train them?</p>
","python, machine-learning, nlp, text-classification","<p>You can train a language model for each dialects of the language. Then, given a sentence find the (log) probability returned by each language model and assign it to the language model which returns the high score. </p>

<pre><code>p* = argmax p_i p_i(sentence)
</code></pre>

<p>where <code>p_i</code> is the language model of the dialects i. </p>

<p>Language model is a probability distribution over sequences of words. Given a sentence, say of length <code>m</code>, it assigns a probability <code>P(w1, ... ,wm)</code> to the whole sequence. So the sentence will belong to the dialect whose <code>P_i(w)</code> is high, where <code>P_i</code> is the language model of dialect <code>i</code>.</p>
",1,2,76,2019-05-14 11:28:09,https://stackoverflow.com/questions/56129349/how-to-train-a-classifier-to-detect-vernacular-from-grammatical-language
I get isnan error when I merge two countvectorizers,"<p>I'm going dialect text classification and I have this code:</p>

<pre><code>from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

vectorizerN = CountVectorizer(analyzer='char',ngram_range=(3,4))
XN = vectorizerN.fit_transform(X_train)

vectorizerMX = CountVectorizer(vocabulary=a['vocabs'])
MX = vectorizerMX.fit_transform(X_train)

from sklearn.pipeline import FeatureUnion
combined_features = FeatureUnion([('CountVectorizer', MX),('CountVect', XN)])
combined_features.transform(test_data)
</code></pre>

<p>When I run this code I get this error:</p>

<pre><code>TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>

<p>I was following the code in this post:
<a href=""https://stackoverflow.com/questions/37081597/merging-countvectorizer-in-scikit-learn-feature-extraction?rq=1"">Merging CountVectorizer in Scikit-Learn feature extraction</a></p>

<p><strong>Also, how can I train and predict afterwards?</strong></p>
","python, scikit-learn, nlp, text-classification, countvectorizer","<p>You should union <code>vectorizerN</code> and <code>vectorizerMX</code>, not <code>MX</code> and <code>XN</code>.
Change the line to</p>

<pre><code>combined_features = FeatureUnion([('CountVectorizer', vectorizerMX), ('CountVect', vectorizerN)])
</code></pre>
",1,0,91,2019-05-19 07:41:45,https://stackoverflow.com/questions/56205728/i-get-isnan-error-when-i-merge-two-countvectorizers
How to reduce the number of features in text classification?,"<p>I'm doing dialect text classification and I'm using countVectorizer with naive bayes. The number of features are too many, I have collected 20k tweets with 4 dialects. every dialect have 5000 tweets. And the total number of features are 43K. I was thinking maybe that's why I could be having overfitting. Because the accuracy has dropped a lot when I tested on new data. So how can I fix the number of features to avoid overfitting the data?</p>
","python, nlp, text-classification, naivebayes, countvectorizer","<p>You can set the parameter <strong>max_features</strong> to 5000 for instance, It might help with overfitting. You could also tinker with <strong>max_df</strong> (for instance set it to 0.95)</p>
",2,0,778,2019-05-19 20:18:14,https://stackoverflow.com/questions/56211670/how-to-reduce-the-number-of-features-in-text-classification
How to compute the perplexity in text classification?,"<p>I'm doing dialect text classification with scikit learn, naive bayes and countvectorizer. So far I'm only doing 3 dialects text classification. I'm going to add a new dialect(or actually, the formal language for those dialects). The problem is, the new text that I'm going to add, shares a lot of words with the other 3 dialects. So I read the following in a research document:</p>

<blockquote>
  <p>We train an n-gram model for each dialect from the collected data. To
  train the MSA model, we select sentences from Arabic UN corpus and
  news collections. All the dialect and MSA models share the same
  vocabulary, thus perplexity can be compared properly. At
  classification time, given an input sentence, the classifier computes
  the perplexity for each dialect type and choose the one with minimum
  perplexity as the label.</p>
</blockquote>

<p>They mean by MSA(Modern Standard Arabic) which is the formal language for those dialects. How are they  calculating the perplexity? Are they just using naive bayes or there's more to it?</p>
","nlp, text-classification, naivebayes, countvectorizer, perplexity","<p>From what I see here, the quoted work is not using a Naive Bayes classifier at all; the approach is different to what you're suggesting.</p>

<p>The proposed approach there is to train individual n-gram based language models for <em>each</em> dialect to be classified. To classify which dialect a given input is in, the input text is scored with each language model. The lower the perplexity according to an LM, the higher the probability. Therefore, if the LM trained on dialect A assigns lower perplexity (i.e. higher probability) to an input than dialect B does, it is more likely that the input text is in dialect A.</p>

<blockquote>
  <p>Perplexity is the inverse probability of some text normalized by the number of words (<a href=""https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"" rel=""nofollow noreferrer"">source</a>).</p>
</blockquote>

<p>For a sentence W,<br>
<code>Perplexity(W) = P(W)^(-1/N)</code>, where <code>N</code> is the number of words in the sentence, and <code>P(W)</code> is the probability of W according to an LM. </p>

<p>Therefore, the probability, and hence the perplexity, of the input according to each language model is computed, and these are compared to choose the most likely dialect.</p>
",2,0,1608,2019-05-20 13:22:49,https://stackoverflow.com/questions/56221883/how-to-compute-the-perplexity-in-text-classification
How can I classify big text data with scikit-learn?,"<p>I have large database of 50GB in size, which consists of excerpts of 486,000 dissertations in 780 specialties.
For scientific purposes, it is necessary to conduct training on the basis of this data. But alas, resources are limited to a mobile processor, 16 GB of memory (+ 16Gb  SWAP) </p>

<p>The analysis was carried out using a set of 40,000 items (10% of the base) (4.5 GB) and the SGDClassifier classifier, and the memory consumption was around 16-17 GB.</p>

<p>Therefore, I ask the community for help on this. </p>

<p>currently my code is similar</p>

<pre><code>text_clf = Pipeline([
     ('count', CountVectorizer()),
     ('tfidf', TfidfTransformer()),
     ('clf', SGDClassifier(n_jobs=8),)
 ],
 )
texts_train, texts_test, cat_train, cat_test = train_test_split(texts, categories_ids, test_size=0.2)
text_clf.fit(texts_train, cat_train)
</code></pre>

<p>Therefore, I ask for advice on how to optimize this process so that I can process the entire database.</p>
","python, machine-learning, scikit-learn, text-classification","<p>You can utilize <code>warm_start=True</code> and call <code>.partial_fit()</code> (instead of <code>.fit()</code>).</p>

<p>See the documentation <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"" rel=""nofollow noreferrer"">here</a> for the model you are using where it describes that argument and function respectively.</p>

<p>Basically, you would load only a portion of the data at a time, run it through your pipeline and call partial_fit in a loop. This would keep the memory requirements down while also allowing you to train on all the data, regardless of the amount.</p>

<p><strong>EDIT</strong></p>

<p>As noted in the comments, the above mentioned loop will only work for the predictive model, so the data pre-processing will need to occur separately.</p>

<p><a href=""https://stackoverflow.com/questions/40054473/scikit-learn-vectorizing-with-big-dataset"">Here is a solution for training the CountVectorizer iteratively.</a></p>

<p><a href=""https://stackoverflow.com/questions/25145552/tfidf-for-large-dataset"">This question contains a TFIDF implementation that doesn't require all of the data to be loaded into memory.</a></p>

<p>So the final solution would be to preprocess the data in two stages. The first for the CountVectorizer and the second for the TFIDF weighting.</p>

<p>Then to train the model you follow the same process as originally proposed, except without a Pipeline because that is no longer needed.</p>
",1,0,203,2019-05-21 16:52:31,https://stackoverflow.com/questions/56243043/how-can-i-classify-big-text-data-with-scikit-learn
How to have multioutput in text classification?,"<p>I'm doing dialect text classification. The problem is some tweets, can be classified as both dialect A and B, how can I do that? I want to do it and then automatically calculate the accuracy, I don't want to do it manually. When I don't classify them as both A and B, it gives me many misclassified texts. </p>

<p>In the training though, they're not classified as both dialect A and B. but separately. </p>
","python, scikit-learn, nlp, text-classification","<p>Make use of <code>OneHotEncoding</code></p>

<pre><code>from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

# Your target will look similar to
target = ['A', 'A', 'B']

# After OneHotEncoding
[[1, 0],
 [1, 0],
 [0, 1]]
</code></pre>

<p>After training on this target, your model will predict the probability of the class. You can set a threshhold to classify the prediction to both the classes</p>

<pre><code># Sample output
[[1., 0.],
 [0.5, 0.5],
 [0.1, 0.9]]

predictions = ['A', 'A and B', 'B']
</code></pre>

<p><a href=""https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/"" rel=""nofollow noreferrer"">Example</a></p>
",1,0,373,2019-05-22 12:03:48,https://stackoverflow.com/questions/56256399/how-to-have-multioutput-in-text-classification
Google Cloud Natural Language API Classifying Plaintext vs Html,"<p>I want to use Google Natural Language API to classify query results: <a href=""https://cloud.google.com/natural-language/docs/classifying-text"" rel=""nofollow noreferrer"">Classifying content</a></p>

<p>The query results, which I want to classify, are available in HTML and plain text. The <a href=""https://cloud.google.com/natural-language/docs/reference/rest/v1/documents#Document"" rel=""nofollow noreferrer"">official documentation</a> says that the API accepts both types <code>Document.Type.PLAIN_TEXT</code> and <code>Document.Type.HTML</code>. </p>

<p>Because the HTML format has additional annotations like e.g. <code>&lt;b&gt;important text&lt;/b&gt;</code>, I am wondering which format is better to achieve the best classification result possible?</p>
","google-cloud-platform, text-classification, google-natural-language","<p>(not sure if this response is still useful or not.)
Sometimes html pages have a lot of unimportant pieces around the main center piece. Those could easily affect the classification of the content (e.g. ads around the main content). The html handling in the API basically tries to prune these sections and only deal with the main piece. If your html file needs this type of handling, it'd be better to use HTML type when calling the API.</p>
",0,0,116,2019-05-24 18:02:28,https://stackoverflow.com/questions/56297574/google-cloud-natural-language-api-classifying-plaintext-vs-html
Is there a fast way to train many models at the same time?,"<p>I want to train a 2-way classifier, that is, assume I have 4 classes that I want to classify a text to. I don't want to group all the training data in one training set and the labels then would be 4 labels. Rather, I want to make a binary labels. For example, I have to first make 4 copies of the dataset, and then, I make label A and the rest Not A, and then the second dataset would be B and Not B and so on..</p>

<p>After that, I have to make 4 models(naive bayes for example) and train every dataset I made. What I want is a method to do all of that without all of this work. Is that possible?</p>
","python, classification, text-classification, naivebayes, multiclass-classification","<p>Yes, this strategy where separate binary classifiers are fit for each of multiple classes present in a single dataset is called ""one versus all"" or <a href=""https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest"" rel=""nofollow noreferrer"">""one versus rest""</a>. Some sklearn models come with this available as a parameter, such as <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""nofollow noreferrer"">logistic regression</a> where you can set the <code>multi_class</code> parameter to <code>'ovr'</code> for one v. rest. </p>

<p>There's a nice sklearn object that makes it easy for other algorithms called <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier"" rel=""nofollow noreferrer"">OneVersusRestClassifier</a>. For your naive bayes example, it's as easy as:</p>

<pre><code>from sklearn.multiclass import OneVsRestClassifier
from sklearn.naive_bayes import GaussianNB

clf = OneVsRestClassifier(GaussianNB())
</code></pre>

<p>Then you can use your classifier as normal from there, e.g. <code>clf.fit(X,y)</code></p>

<p>(Interestingly, a one versus all naive bayes model is not simply equivalent to multinomial naive bayes when there are three or more classes, as I had initially assumed. There's a short example <a href=""http://qwone.com/~jason/writing/ova.pdf"" rel=""nofollow noreferrer"">here</a> which demonstrates this.)</p>
",1,1,331,2019-06-02 18:20:57,https://stackoverflow.com/questions/56417753/is-there-a-fast-way-to-train-many-models-at-the-same-time
"when checking target: expected dense_2 to have shape (1,) but got array with shape (2,)","<p>sentiment analyses with csv contains 45k with two cols[text,sentiment],trying to use sigmoid with binary_crossentropy but its return an error :</p>

<blockquote>
  <p>Error when checking target: expected dense_2 to have shape (1,) but
  got array with shape (2,)</p>
</blockquote>

<p>i have tried to use LabelEncoder , but its return, bad input shape, how do i let the encoding label acceptable for Sigmond 1 dense ?</p>

<pre><code>#I do aspire here to have balanced classes
num_of_categories = 45247
shuffled = data.reindex(np.random.permutation(data.index))
e = shuffled[shuffled['sentiment'] == 'POS'][:num_of_categories]
b = shuffled[shuffled['sentiment'] == 'NEG'][:num_of_categories]
concated = pd.concat([e,b], ignore_index=True)
for idx,row in data.iterrows():
    row[0] = row[0].replace('rt',' ')
#Shuffle the dataset
concated = concated.reindex(np.random.permutation(concated.index))
concated['LABEL'] = 0

#encode the lab
encoder = LabelEncoder()
concated.loc[concated['sentiment'] == 'POS', 'LABEL'] = 0
concated.loc[concated['sentiment'] == 'NEG', 'LABEL'] = 1
print(concated['LABEL'][:10])
labels = encoder.fit_transform(concated)
print(labels[:10])
if 'sentiment' in concated.keys():
    concated.drop(['sentiment'], axis=1)

n_most_common_words = 8000
max_len = 130
tokenizer = Tokenizer(num_words=n_most_common_words, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(concated['text'].values)
sequences = tokenizer.texts_to_sequences(concated['text'].values)
word_index = tokenizer.word_index
</code></pre>
","python-3.x, encoding, deep-learning, sentiment-analysis, text-classification","<p>The output of <code>LabelEncoder</code> if also 1 dim, I guess the output of your network have two dim. So you need to one-hot your y_true.</p>

<p>use </p>

<pre><code>labels = keras.utils.to_categorical(concated['LABEL'], num_classes=2)
</code></pre>

<p>instead</p>

<pre><code>labels = encoder.fit_transform(concated)
</code></pre>
",1,0,76,2019-06-13 20:11:28,https://stackoverflow.com/questions/56587933/when-checking-target-expected-dense-2-to-have-shape-1-but-got-array-with-sha
Keras Movie Review Sentiment Classifier: What is the role of GlobalAveragePooling1D layer?,"<p>I was reading some IMDB movie review sentiment classifier on top of Keras. Here is the model definition:</p>

<pre><code>model = keras.Sequential()
model.add(keras.layers.Embedding(vocab_size, 16))
model.add(keras.layers.GlobalAveragePooling1D())
model.add(keras.layers.Dense(16, activation=""relu""))
model.add(keras.layers.Dense(1, activation=""sigmoid""))
</code></pre>

<p>What I don't understand is the role of GlobalAveragePooling1D here. </p>
","keras, text-classification","<p>To answer specifically why it's there (and not how it works), <code>model.summary()</code> will reveal that it is providing dimensional reduction.</p>
",0,0,71,2019-06-17 19:36:46,https://stackoverflow.com/questions/56637851/keras-movie-review-sentiment-classifier-what-is-the-role-of-globalaveragepoolin
Encode sentence as sequence model with Spark,"<p>I am doing text classification and I use <code>pyspark.ml.feature.Tokenizer</code> to tokenize the text. However <code>CountVectorizer</code> transforms the tokenized list of words to bag of words model, not the sequence model.</p>

<p>Assume that we have the following DataFrame with columns id and texts:</p>

<pre><code> id | texts
----|----------
 0  | Array(""a"", ""b"", ""c"")
 1  | Array(""a"", ""b"", ""b"", ""c"", ""a"")
each row in texts is a document of type Array[String]. Invoking fit of CountVectorizer produces a CountVectorizerModel with vocabulary (a, b, c). Then the output column “vector” after transformation contains:

 id | texts                           | vector
----|---------------------------------|---------------
 0  | Array(""a"", ""b"", ""c"")            | (3,[0,1,2],[1.0,1.0,1.0])
 1  | Array(""a"", ""b"", ""b"", ""c"", ""a"")  | (3,[0,1,2],[2.0,2.0,1.0])
</code></pre>

<p>What I want here is (for the row 1)</p>

<pre><code>Array(""a"", ""b"", ""b"", ""c"", ""a"")  | [0, 1, 1, 2, 0]
</code></pre>

<p>So is there anyway that I can write custom function to run encoding in parallel? Or is there any other library that can do in parallel other than using spark?</p>
","apache-spark, parallel-processing, pyspark, text-classification","<p>You could use <code>StringIndexer</code> and <code>explode</code>:</p>

<pre><code>df = spark_session.createDataFrame([
    Row(id=0, texts=[""a"", ""b"", ""c""]),
    Row(id=1, texts=[""a"", ""b"", ""b"", ""c"", ""a""])
])

data = df.select(""id"", explode(""texts"").alias(""texts""))
indexer = StringIndexer(inputCol=""texts"", outputCol=""indexed"", stringOrderType=""alphabetAsc"")
indexer\
    .fit(data)\
    .transform(data)\
    .groupBy(""id"")\
    .agg(collect_list(""texts"").alias(""texts""), collect_list(""indexed"").alias(""vector""))\
    .show(20, False)
</code></pre>

<p>Output:</p>

<pre><code>+---+---------------+-------------------------+
|id |texts          |vector                   |
+---+---------------+-------------------------+
|0  |[a, b, c]      |[0.0, 1.0, 2.0]          |
|1  |[a, b, b, c, a]|[0.0, 1.0, 1.0, 2.0, 0.0]|
+---+---------------+-------------------------+
</code></pre>
",1,0,314,2019-06-17 19:39:44,https://stackoverflow.com/questions/56637881/encode-sentence-as-sequence-model-with-spark
What does a &quot;None&quot; mean after compilation of keras model?,"<p>I'm trying to implement a binary text classification model using keras layers. After compiling a model, in a summary, I am getting <strong>None</strong> at the bottom end and I don't exactly understand what does it mean?</p>

<p>here is the code which I am using.</p>

<pre><code>max_words = 10000
max_len = 500
tok = Tokenizer(num_words=max_words)
tok.fit_on_texts(X_train)
sequences = tok.texts_to_sequences(X_train)
sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)

model = Sequential()
model.add(Embedding(max_words, 50, input_length=max_len))
model.add(LSTM(64))
model.add(Dense(256,name='FC1',activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics= 
             ['acc'])
print(model.summary())
</code></pre>

<p>This is the model summary and at the bottom end It is showing <strong>None</strong>.</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 500, 50)           500000    
_________________________________________________________________
lstm_1 (LSTM)                (None, 64)                29440     
_________________________________________________________________
FC1 (Dense)                  (None, 256)               16640     
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 257       
=================================================================
Total params: 546,337
Trainable params: 546,337
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>
","python, keras, lstm, text-classification","<p><code>model.summary()</code> returns nothing (<code>None</code>), and you are printing the return value of it. <code>model.summary()</code> already does the printing internally, there is no need to get confused with printing it manually, so just do:</p>

<pre><code>model.summary()
</code></pre>
",5,2,1253,2019-06-22 15:15:37,https://stackoverflow.com/questions/56716637/what-does-a-none-mean-after-compilation-of-keras-model
No batch_size while making inference with BERT model,"<p>I am working on a binary classification problem with Tensorflow BERT language model. Here is the <a href=""https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"" rel=""noreferrer"">link</a> to google colab. After saving and loading the model is trained, I get error while doing the prediction.</p>

<p>Saving the Model</p>

<pre><code>def serving_input_receiver_fn():
  feature_spec = {
      ""input_ids"" : tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),
      ""input_mask"" : tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),
      ""segment_ids"" : tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),
      ""label_ids"" :  tf.FixedLenFeature([], tf.int64)
  }
  serialized_tf_example = tf.placeholder(dtype=tf.string,
                                         shape=[None],
                                         name='input_example_tensor')
  print(serialized_tf_example.shape)
  receiver_tensors = {'example': serialized_tf_example}
  features = tf.parse_example(serialized_tf_example, feature_spec)
  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)

export_path = '/content/drive/My Drive/binary_class/bert/'
estimator._export_to_tpu = False  # this is important
estimator.export_saved_model(export_dir_base=export_path,serving_input_receiver_fn=serving_input_receiver_fn)
</code></pre>

<p>Predicting on dummy text</p>

<pre><code>pred_sentences = [
  ""A novel, simple method to get insights from reviews""
]

def getPrediction1(in_sentences):
  labels = [""Irrelevant"", ""Relevant""]
  input_examples = [run_classifier.InputExample(guid="""", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, """" is just a dummy label
  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)
  predictions = est.predict(predict_input_fn)
  print(predictions)
  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]

est = tf.contrib.estimator.SavedModelEstimator(MODEL_FILE_PATH)
predictions = getPrediction1(pred_sentences[0])
predictions
</code></pre>

<p>Error </p>

<pre><code>W0702 05:44:17.551325 139812812932992 estimator.py:1811] Using temporary folder as model directory: /tmp/tmpzeiaa6q8
W0702 05:44:17.605536 139812812932992 saved_model_estimator.py:170] train mode not found in SavedModel.
W0702 05:44:17.608479 139812812932992 saved_model_estimator.py:170] eval mode not found in SavedModel.
&lt;generator object Estimator.predict at 0x7f27fa721eb8&gt;
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-28-56ea95428bf4&gt; in &lt;module&gt;()
     21 # Relevant ""Nanoparticulate drug delivery is a promising drug delivery system to a range of molecules to desired site specific action in the body. In this present work nanoparticles are prepared with positive group of amino group of chitosan with varying concentration based nanoparticles are loaded with anastrazole were prepared by with negative group of sodium tripolyphosphate by ionotropic gelation method. All these formulated nanoparticles are characterized for its particle size ,zeta potential ,drug entrapment efficacy and in-vitro release kinetics .The particle size of all these formulations were found to be 200,365,420,428 And 483.zeta potential of all formulations are-16.3±2.1 ,28.2±4.3,-10.38±3.6,-24.31±3.2 and 21.38±5.2.respectively. FT-IR studies indicated that there was no chemical interaction between drug and polymer and stability of drug. The in-vitro release behaviour from all the drug loaded batches was found to be zero order and provided sustained release over a period of 12 h by diffusion and swelling mechanism and The values of n and r 2 for coated batch was 0.731 and 0.979.Since the values of slope (n) lies in between 0.5 and 1 it was concluded that the mechanism by which drug is being released is a Non-Fickian anomalous solute diffusion mechanism, ""
     22 
---&gt; 23 predictions = getPrediction1(pred_sentences[0:2])
     24 predictions
     25 

5 frames
&lt;ipython-input-28-56ea95428bf4&gt; in getPrediction1(in_sentences)
     14   predictions = est.predict(predict_input_fn)
     15   print(predictions)
---&gt; 16   return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]
     17 
     18 

&lt;ipython-input-28-56ea95428bf4&gt; in &lt;listcomp&gt;(.0)
     14   predictions = est.predict(predict_input_fn)
     15   print(predictions)
---&gt; 16   return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]
     17 
     18 

/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in predict(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)
    615         self._create_and_assert_global_step(g)
    616         features, input_hooks = self._get_features_from_input_fn(
--&gt; 617             input_fn, ModeKeys.PREDICT)
    618         estimator_spec = self._call_model_fn(
    619             features, None, ModeKeys.PREDICT, self.config)

/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _get_features_from_input_fn(self, input_fn, mode)
    991   def _get_features_from_input_fn(self, input_fn, mode):
    992     """"""Extracts the `features` from return values of `input_fn`.""""""
--&gt; 993     result = self._call_input_fn(input_fn, mode)
    994     result, _, hooks = estimator_util.parse_input_fn_result(result)
    995     self._validate_features_in_predict_input(result)

/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _call_input_fn(self, input_fn, mode, input_context)
   1111       kwargs['input_context'] = input_context
   1112     with ops.device('/cpu:0'):
-&gt; 1113       return input_fn(**kwargs)
   1114 
   1115   def _call_model_fn(self, features, labels, mode, config):

/usr/local/lib/python3.6/dist-packages/bert/run_classifier.py in input_fn(params)
    727   def input_fn(params):
    728     """"""The actual input function.""""""
--&gt; 729     batch_size = params[""batch_size""]
    730 
    731     num_examples = len(features)

KeyError: 'batch_size'
</code></pre>

<p>batch_size param is present in estimator, but not in the loaded model params.</p>

<pre><code>estimator.params['batch_size'] # 32

est.params['batch_size'] # KeyError: 'batch_size'
</code></pre>
","python, tensorflow, machine-learning, deep-learning, text-classification","<p>You are using <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/SavedModelEstimator"" rel=""noreferrer""><code>SavedModelEstimator</code></a>, which does not provide an option to pass in <code>RunConfig</code> or <code>params</code> arguments, </p>

<pre><code>because the model function graph is defined statically in the SavedModel.
</code></pre>

<p>Since <code>SavedModelEstimator</code> is a subclass of <code>Estimator</code>, the params is merely a dictionary that stores hyperparameters. I think you could modify <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#params"" rel=""noreferrer""><code>params</code></a> by passing the desired (key,value) pair to it before you call <code>getPrediction1</code>.  For example:</p>

<pre><code>est = tf.contrib.estimator.SavedModelEstimator(MODEL_FILE_PATH)
est.params['batch_size'] = 1
predictions = getPrediction1(pred_sentences)
</code></pre>
",5,6,1950,2019-07-02 06:06:35,https://stackoverflow.com/questions/56846266/no-batch-size-while-making-inference-with-bert-model
How to Implement a (statistical) Thematic Comparison of Texts via Text-Mining?,"<p>I try to compare texts in form of 'text-files' concerning their content.<br>
<strong>e.g.</strong>: I got 100 texts about animals and I want to analyze each text about what animals it discusses.<br>
I am looking for an analysis output like: <code>doc1: 60% cats, 10% rabbits, 10% dogs, 0% elephants, 20% else"", ""doc2: 0% cats, 10% rabbits, 40% dogs, ...</code></p>

<p>I have read a lot about Latent Dirichlet Allocation (and the word-probabilities for each topic) for Text Classification but a completely unsupervised approach seemed not to fit my set of documents.</p>

<p>Trying to implement the LDA-Stuff in Python I understood to prepare the data (tokenizing, lemmatizing/stemming) but I don't get the next steps. Do I have to generate training data for each topic (animal) and how could I implement this? </p>

<p>Also I've seen a tutorial manipulating the topics via the <code>eta-value</code> in <code>gensim</code> but I don't know how I could use this in my favor. </p>

<p>I am grateful for any advice that can lead me to the right direction. Thanks!</p>
","python, text-mining, gensim, text-classification, lda","<p>If you want to use LDA you're done with proprocessing (tokenizing, stemming/lemmatizing), the next step would be to create <code>gensim</code> dictionary and corpus. Assuming your set of documents is a list of lists like <code>[['my', 'first', 'doc'], ['the', 'second', 'doc']]</code> you could proceed like this: </p>

<pre><code>dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Then create your model with appropriate number of topics (=animals):</p>

<pre><code>model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_animals)
</code></pre>

<p>You don't need to generate training data yourself at all. After a number of iterations, the LDA algorithm itself performs a quality check on a set of randomly chosen held out test documents which were not used for training. The corresponding measure is often called ""perplexity"" or ""log likelihood"" and will usually be displayed during iteration.</p>

<p>When your model is finally created you can have a look at the words in your topics:</p>

<pre><code>model.print_topics()
</code></pre>

<p>In many cases, you have a collection of documents and a rough idea of the number of topics contained. So the most relevant parameter to play around with is the topic number. 
Since you already know your topic number, you are left to tinker with other parameters. I could imagine that it's difficult to get topics that can be easily attributed to precisely one animal. Keep in mind though that every word appears in every topic, so even ""elephant"" is going to show up in the ""cat"" topic somewhere.</p>

<p>Things to try:</p>

<ul>
<li>Be more rigorous with your stemming/lemmatization to merge more tokens with the same meaning</li>
<li>Check out <code>filter_extremes</code> function of your dictionary to filter for very common or very rare tokens</li>
<li>Apply or expand your stopword filter to get rid of irrelevant terms</li>
<li>Play around with alpha (prevalence of topics per document) and eta (prevalence of tokens per topic) values</li>
</ul>
",0,0,276,2019-07-24 18:26:00,https://stackoverflow.com/questions/57189149/how-to-implement-a-statistical-thematic-comparison-of-texts-via-text-mining
"Multinomial naive bayes classification problem, normalization required?","<p>Classification using multinomial naive bayes is not working, see the code</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction import DictVectorizer
import numpy as np

# training data
data = [
{'house': 100, 'street': 50, 'shop': 25, 'car': 100, 'tree': 20},

{'house': 5, 'street': 5, 'shop': 0, 'car': 10, 'tree': 500, 'river': 1}
] 

dv = DictVectorizer(sparse=False)
X = dv.fit_transform(data)
Y = np.array([10, 20])


mnb=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
mnb.fit(X, Y)

# test data
test_data1 = [
{'testname': 0, 'street': 0, 'shop': 0, 'car': 0, 'Hi': 0, 'Blue': 5},
]


print (mnb.predict(dv.transform(test_data1)) )
</code></pre>

<p>Output is [10], But I was expecting it to be [20].</p>

<p>What is wrong here, my understanding?</p>
","machine-learning, scikit-learn, text-classification, naivebayes, multinomial","<p>Your test set gives the same probability for both 10 and 20.
Here's an example of how Naive Bayes calculates probability of each output category. <a href=""https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf"" rel=""nofollow noreferrer"">https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf</a></p>

<p>In your example, none of the attributes in the test data appears in the training data (The words street, shop and car has a probability of 0).</p>

<p>Try running the code</p>

<pre><code>#Return probability estimates for the test vector X.
print (mnb.predict_proba(dv.transform(test_data1)) ) 
</code></pre>

<p>Both the classes have an accuracy of 0.5.
So the model returns the first class which is 10. </p>
",0,0,326,2019-07-30 10:26:24,https://stackoverflow.com/questions/57269570/multinomial-naive-bayes-classification-problem-normalization-required
Tensorflow retrain if it&#39;s wrong,"<p>I'm new to Tensorflow and AI, so I'm having trouble researching my question. Either that, or my question hasn't been answered.</p>

<p>I'm trying to make a text classifier to put websites into categories based on their keywords. I have at minimum 5,000 sites and maximum 37,000 sites to train with.</p>

<p>What I'm trying to accomplish is: after the model is trained, I want it to continue to train as it makes predictions about the category a website belongs in.</p>

<p>The keywords that the model is trained on is chosen by clients, so it can always be different than the rest of the websites in its category. </p>

<p><strong>How can I make Tensorflow retrain it's model based on corrections made by me if it's prediction is inaccurate?</strong> Basically, to be training for ever.</p>
","python, tensorflow, artificial-intelligence, text-classification","<p>The key phrase you lack is <strong>fine-tuning</strong>.  This is when you take a model that has finished its customary training (whatever that may be), and needs more work for the application you have in mind.  You then give it additional training with new input; when that training has completed (training accuracy plateaus and is close to test accuracy), you then deploy the enhanced model for your purposes.</p>

<p>This is often used in commercial applications -- for instance, when a large predictive model is updated to include the most recent week of customer activity.  Another common use is to find a model in a zoo that is trained for something related to the application you want -- perhaps cats v dogs -- and use its recognition of facial features to shorten training for a model to identify two classes of cartoon characters -- perhaps Pokemon v Tiny Toons.</p>

<p>In this latter case, your fine-tuning will almost entirely eliminate what was learned by the last few layers of the model.  What you gain is the early-layer abilities to find edges, regions, and features through eyes-nose-mouth combinations.  This saves at least 30% of the overall training time.</p>
",0,-2,76,2019-07-30 20:31:35,https://stackoverflow.com/questions/57279611/tensorflow-retrain-if-its-wrong
How should I go about using TF-IDF for text classification on the data I collected?,"<p>I'm working on a personal project to build a text classifier. I scraped around 3000 news articles from 8 categories. I have every single word in every article with its article's category tag in a dataframe. </p>

<p>The answers I saw online referred to using tfidf on entire articles/text blocks. Is there any way to analyze individual words?</p>

<p>Here is an idea of what my data currently looks like:</p>

<pre><code>Word:       Category:

Mobile      Science/tech
Phone       Science/tech
Google      Science/tech
Facebook    Science/tech
Implant     Science/tech
Interest    Business/economy
Bank        Business/economy
IMF         Business/economy
Downturn    Business/economy
President   Politics
Donald      Politics
Trump       Politics
etc...        etc...
</code></pre>

<p>I apologize for the horrible formatting; I'm somewhat new to this.</p>
","python, machine-learning, text-classification, tf-idf","<p>There's no way to analyze individual words with tf-idf, and if you ask this question, I believe TF-IDF is unclear in your mind.</p>

<p>I'll try to be clear about tf-idf.</p>

<p>TF-IDF is a way to calculate a ""score"" or a ""weight"" of some words in a text, relative to a corpus (set of texts). This will give the words the importance they have in the text they are. So, for each text where occurs a given word, you'll have a score. </p>

<p>The first part of TF-IDF is TF :</p>

<ul>
<li>TF for Term-Frequency calculates makes the score of a word grow, the more it's used in a text, the bigger TF will be.</li>
</ul>

<p>The second part is IDF :</p>

<ul>
<li>IDF for Inverse Document Frequency which is another coefficient which should be decreasing following the number of occurences where a term is repeated throughout the corpus. </li>
</ul>

<p>By multiplying those two coefficients, you'll have <strong>the ""importance"" of a word in a text, relatively to the corpus</strong>.</p>

<p>Here's an example, if the word ""Mobile"" occurs in two texts one about Business (like the selling of Mobiles) and the other about Tech, you'll have two scores of ""Mobile"" in the corpus and, when you'll encounter this word in a unknown article you can sum the different scores of the words from the unknown article and you'll be able to say, pretty accurately what's the unknown article talking about. </p>
",4,0,5196,2019-08-05 08:11:27,https://stackoverflow.com/questions/57354595/how-should-i-go-about-using-tf-idf-for-text-classification-on-the-data-i-collect
How can i use the same code for different variables in a dataset in R?,"<p>I am working in a email classification supervised model, the emails are classified in 20 different groups, I have finished the model for the first group (G1) (a very large code) and I would like to know if there's some function which can repeat the code but with the others groups as variable, because changing G1 for (G2...G20)manually would be so tedious.</p>
<p>I have not idea how I could do it.</p>
<pre><code>####G1####
datos_pivot1= cast(datosArg[,c('Descripción', 'Subcategoría_Servicio', 'value')], Descripción ~ Subcategoría_Servicio, mean)

datos_pivot1=datos

datos_G1=datos[datos$G1&gt;0 &amp; is.na(datos$G1)==F ,c('Descripción','G1')]
wordcloud(datos_G1$Descripción, max.words = 200, min.freq = 200, random.order = F, colors = brewer.pal(name = &quot;Dark2&quot;, n = 8))

length(datos_G1$Descripción)

# casos FALSE
datos_G1_no=datos[is.na(datos$G1)==T ,c('Descripción','G1')]

# numero de casos sin O2C
length(datos_G1_no$Descripción)

#para balancear la cantidad de True vs False, se selecciona una muestra del mismo número de casos True
datos_G1_no_sample =datos_G1_no[sample(1:length(datos_G1_no$Descripción),size=length(datos_G1$Descripción)),]
datos_G1_no_sample$G1=0
cor1=head(datos_G1_no_sample)

#unir casos TRUE Y casos FALSE
datos_G1 = rbind(datos_G1, datos_G1_no_sample)
table(datos_G1$G1)
#corpus
base_corpus_G1 &lt;- Corpus(VectorSource(datos_G1$Descripción))
#Matriz de términos
base_tdm_G1 &lt;- TermDocumentMatrix(base_corpus_G1)
#Eliminar términos dispersos
base_tdm_G1 &lt;- removeSparseTerms(base_tdm_G1, sparse = .95)
#Matriz por filas=Analisis, columnas = palabras 
base_mat_G1 &lt;- t(as.matrix(base_tdm_G1))
base_mat_G1= cbind(base_mat_G1, data.frame(G1=c(rep(1,table(datos_G1$G1)[2]),rep(0,table(datos_G1$G1)[2]))))
head(base_mat_G1)

##ENTRENAMIENTO Y PRUEBA
## 75% Train. 
smp_size &lt;- floor(0.75 * nrow(datos_G1))

#definimos una semilla para que cuando volvamos a ejecutar obtengamos la misma muestra
set.seed(456)
train_ind &lt;- sample(seq_len(nrow(base_mat_G1)), size = smp_size)
#Filtro de Entrenamiento 75% (basado en el sample de la linea anterior)
train_G1 &lt;- base_mat_G1[train_ind, ]
#Filtro de prueba 75% (los otros) (el menos indica las contratias)
test_G1 &lt;- base_mat_G1[-train_ind, ]
table(train_G1$G1)

##MODELOS O2C
##Arbol de decision
library(rpart)
tree_G1 &lt;- rpart(G1 ~ ., data = train_G1)
#Predict
pred.tree_G1 &lt;- predict(tree_G1, newdata = test_G1)
pred.tree_G1=(as.data.frame(pred.tree_G1))
names(pred.tree_G1)=c('prob')
pred.tree_G1$G1.pred=0
pred.tree_G1$G1.pred[pred.tree_G1$prob&gt;0.51] = 1
table(test_G1$G1, pred.tree_G1$G1.pred)
rpart.plot(tree_G1)
#Curva ROC
roc.curve(test_G1$G1, pred.tree_G1$G1.pred, curve=TRUE)


##GLM
#train
glm_G1 &lt;- glm(G1 ~ ., family=binomial(logit), data=train_G1)
#predict
pred.glm_G1 = test_G1[,c('G1','G1')]
pred.glm_G1 = cbind(pred.glm_G1, data.frame(predict(glm_G1, newdata=test_G1,type='response')))
pred.glm_G1$G1 = NULL
pred.glm_G1$G1.1 = NULL
names(pred.glm_G1)=c('prob')
pred.glm_G1$G1.pred=0
pred.glm_G1$G1.pred[pred.glm_G1$prob&gt;0.51] = 1
table(test_G1$G1, pred.glm_G1$G1.pred)
#Curva ROC:
roc.curve(test_G1$G1, pred.glm_G1$G1.pred, curve=TRUE)


##KKNN
kknn_G1 &lt;- kknn(G1 ~ ., train_G1, test_G1, distance = 1, k=350, kernel = &quot;optimal&quot;)
pred.kknn_G1 = test_G1[,c('G1','G1')]
pred.kknn_G1$prob&lt;-kknn_G1$fitted.values
pred.kknn_G1$G1 = NULL
pred.kknn_G1$G1.1 = NULL
pred.kknn_G1$G1.pred=0
pred.kknn_G1$G1.pred[pred.kknn_G1$prob&gt;0.51] = 1
table(test_G1$G1, pred.kknn_G1$G1.pred)
#Curva ROC
roc.curve(test_G1$G1, pred.kknn_G1$G1.pred, curve=TRUE)
head(pred.kknn_G1)

</code></pre>
","r, function, classification, repeat, text-classification","<p>If you are using R studio, just press <code>command</code> + <code>F</code> or press the <code>Find/Replace</code> button in your R console you can easily replace all 'G1'  with 'G2'.</p>

<p><a href=""https://i.sstatic.net/jN3Hx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jN3Hx.jpg"" alt=""enter image description here""></a></p>
",0,0,72,2019-08-05 16:53:40,https://stackoverflow.com/questions/57362947/how-can-i-use-the-same-code-for-different-variables-in-a-dataset-in-r
Retrieve the indices for only the resampled instances after oversampling using imbalanced-learn?,"<p>For a binary text classification problem with imbalanced data, I use <a href=""https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html"" rel=""nofollow noreferrer"">imbalanced-learn</a> library's function <code>RandomOverSampler</code> to balance the classes. </p>

<p>Now, I want to retrieve only the instances that were oversampled (replicated) from the original data. For example, if ""item_1"" is the original data and item 2 to 4 are the replicas of ""item_1"", I require only the indices for ""item_2"", ""item_3"", ""item_4"" for further processing and leave out the index for ""item_1"". </p>

<ol>
<li>item_1</li>
<li>item_2</li>
<li>item_3</li>
<li>item_4</li>
</ol>

<p>Here goes the my code:</p>

<pre><code>from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)

X_listed = []
for eachTrainInstance in X_train:
    X_listed.append([eachTrainInstance])

X_tr_resampled, y_tr_resampled = ros.fit_sample(X_listed, y_train)
</code></pre>
","nlp, text-classification, indices, oversampling, imbalanced-data","<p>It seems that all the oversampled instances (and, of course, their corresponding indices) are concatenated at the end of original data subjected to oversampling. </p>

<pre><code>oversampled_instances = y_tr_resampled[len(y_train):]
</code></pre>
",2,0,252,2019-08-12 15:41:28,https://stackoverflow.com/questions/57464275/retrieve-the-indices-for-only-the-resampled-instances-after-oversampling-using-i
How do you classify books by genre (using deep learning) when some books have multiple genres?,"<p>I am trying to build a neural network that looks at the text of a book and guesses the book's genre. I can train a network fine when each book only has one genre. Is there a good way to train a network when a book is associated with multiple genres?</p>

<p>I have tried using a basic SGDClassifier from sklearn. It works wonderfully with a data set where each one book/block of text is tied to one genre. Unfortunately, I do not know how to give it a data set where each book/block of text is associated with multiple genres. </p>

<p>Here is the basic code I am using for context:</p>

<pre><code>from sklearn.linear_model import SGDClassifier

sgd = Pipeline([('vect', CountVectorizer()),

                ('tfidf', TfidfTransformer()),

                ('clf', SGDClassifier('basic parameters')),

               ])

sgd.fit(x_train, y_train)

y_pred = sgd.predict(x_test)
</code></pre>

<p>Does anyone know of a good way to approach this problem? Can anyone link me to a place where a smart person has already solved it?</p>
","python, machine-learning, deep-learning, text-classification","<p>This question should be asked on stats.stackexchange.com but I'll try to answer nonetheless. What you have here is a multilabel ""classification problem"". Say you have 3 genres A, B and C what you  can do is consider each combination of these 3 genres a class and you will get the following classes [0 0 0], [1 0 0], [0 1 0], [0 0 1], [1 1 0], [1 0 1], [0 1 1]. [1 1 1] with [1 0 1] for example being a book that is A and C.</p>

<p>These links should help you understand and deal with your problem</p>

<p><a href=""https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff"" rel=""nofollow noreferrer"">https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff</a>
<a href=""https://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925"" rel=""nofollow noreferrer"">https://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925</a></p>
",2,0,1258,2019-08-15 04:09:52,https://stackoverflow.com/questions/57504770/how-do-you-classify-books-by-genre-using-deep-learning-when-some-books-have-mu
Text Classification with word2vec,"<p>I am doing text classification and plan to use word2vec word embeddings.
I have used gensim module for word2vec Training.</p>

<p>I have tried several Options. But I am getting error that word 'xyz' not in vocabulary. I am not able to find my mistake.</p>

<h1>Text processing</h1>

<pre><code>def clean_text(text):

text = text.translate(string.punctuation)

text = text.lower().split()

stops = set(stopwords.words(""english""))
text = [w for w in text if not w in stops]

text = "" "".join(text)
text = re.sub(r""[^\w\s]"", "" "",text)
text = re.sub(r""[^A-Za-z0-9^,!.\/'+-=]"", "" "",text)

text = text.split()
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(w) for w in text]
text = "" "".join(lemmatized_words)


return text

data['text'] = data['text'].map(lambda x: clean_text(x))
</code></pre>

<p>Please help me to solve my issue.</p>

<h1>Definig Corpus</h1>

<pre><code>def build_corpus(data):
""Creates a list of lists containing words from each sentence""
corpus = []
for col in ['text']:
    for sentence in data[col].iteritems():
        word_list = sentence[1].split("" "")
        corpus.append(word_list)
return corpus

corpus = build_corpus(data)
</code></pre>

<h1>Word2vec model</h1>

<pre><code>from gensim.models import word2vec
 model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=20,    workers=12, sg=1)

words = list(model.wv.vocab)

tokenizer = Tokenizer()
X = data.text
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)
X = pad_sequences(sequences, maxlen=10000)

embedding_vector_size=100

vocab_size = len(words)
embedding_matrix = np.zeros((vocab_size, embedding_vector_size))
for index, word in enumerate(words):    
 embedding_vector = model.wv[word]
 if embedding_vector is not None:
    embedding_matrix[index] = embedding_vector
</code></pre>

<p>Now I am using my created word embeddings on the downstream classification task.</p>

<h1>classification model</h1>

<pre><code>labels = data['Priority']
</code></pre>

<p>where I have two priorities. I want to classify it.</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)
</code></pre>

<p>I am using folllowing network for classification</p>

<pre><code>model3 = Sequential()
model3.add(Embedding(input_dim = vocab_size, output_dim = embedding_vector_size, input_length = max_len, weights=[embedding_matrix]))
model3.add(SpatialDropout1D(0.7))
model3.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))
model3.add(Dense(2, activation='softmax'))
model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
print(model3.summary())
</code></pre>

<p>I am getting error here:</p>

<pre><code>'ValueError: ""input_length"" is 10000, but received input has shape (None, 3)'
</code></pre>

<p>Please help me to solve it out.Thank you.</p>
","python-3.x, word2vec, text-classification","<p>Not all words from <em>corpus</em> will be kept in the word2vec model. </p>

<p>Replace:</p>

<pre><code>vocab_size = len(tokenizer.word_index) + 1
</code></pre>

<p>With:</p>

<pre><code>vocab_size = len(words)
</code></pre>

<p>And replace:</p>

<pre><code>for word, i in tokenizer.word_index.items():
</code></pre>

<p>With:</p>

<pre><code>for i, word in enumerate(words):
</code></pre>

<p>Thus ensuring your embedding matrix contains only words that are in the model. </p>
",1,1,2273,2019-08-16 13:04:12,https://stackoverflow.com/questions/57525190/text-classification-with-word2vec
Why to reshape MSER contours before detecting texts?,"<p>I am using MSER from opencv-python to detect text using the code from this <a href=""https://stackoverflow.com/questions/40078625/opencv-mser-detect-text-areas-python"">stackoverflow question</a>. Can anyone help me understand why the contour p is being reshaped to (-1, 1, 2) before computing the convex hull of the objects?</p>

<p>The code is as below:</p>

<pre><code>import cv2
import numpy as np

#Create MSER object
mser = cv2.MSER_create()

#Your image path i-e receipt path
img = cv2.imread('/home/rafiullah/PycharmProjects/python-ocr-master/receipts/73.jpg')

#Convert to gray scale
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

vis = img.copy()

#detect regions in gray scale image
regions, _ = mser.detectRegions(gray)

hulls = [cv2.convexHull(p.reshape(-1, 1, 2)) for p in regions]

cv2.polylines(vis, hulls, 1, (0, 255, 0))

cv2.imshow('img', vis)

cv2.waitKey(0)

mask = np.zeros((img.shape[0], img.shape[1], 1), dtype=np.uint8)

for contour in hulls:

    cv2.drawContours(mask, [contour], -1, (255, 255, 255), -1)

#this is used to find only text regions, remaining are ignored
text_only = cv2.bitwise_and(img, img, mask=mask)

cv2.imshow(""text only"", text_only)

cv2.waitKey(0)
</code></pre>
","python, opencv, reshape, text-classification, mser","<p><strong>It doesn't matter if you reshape or not.</strong></p>

<p>The reshaping is unnecessary. <code>cv2.convexHull()</code> can take either input format. The following images show that the results are the same whether the <code>contours</code> in <code>regions</code> are reshaped or not.</p>

<pre><code>hulls = [cv2.convexHull(p.reshape(-1, 1, 2)) for p in regions]
hulls1 = [cv2.convexHull(p) for p in regions]
</code></pre>

<p><a href=""https://i.sstatic.net/9ivko.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9ivko.png"" alt=""they are the same""></a></p>

<p>This is how the <code>p</code> contour changes when it is reshaped:</p>

<pre><code>&gt;&gt;&gt; p
array([[305, 382],
       [306, 382],
       [308, 380],
       [309, 380]...


&gt;&gt;&gt; p.reshape(-1, 1, 2)
array([[[305, 382]],    
       [[306, 382]],    
       [[308, 380]],    
       [[309, 380]]...
</code></pre>
",2,1,390,2019-08-27 11:40:37,https://stackoverflow.com/questions/57673903/why-to-reshape-mser-contours-before-detecting-texts
I get &#39;single&#39; characters as learned vocabulary on word2vec genism as an output,"<p>I am new for word2vec and I have trained a text file via word2vec for feature extraction than when I look at the words that are trained I found that it is single characters instead of words, what did I miss here? anyone help</p>

<p>I try to feed tokens instead of the raw text into the models</p>

<pre><code>import nltk

from pathlib import Path
data_folder = Path("""")
file_to_open = data_folder / ""test.txt""
#read the file
file = open(file_to_open , ""rt"")
raw_text = file.read()
file.close()

#tokenization
token_list = nltk.word_tokenize(raw_text)

#Remove Punctuation
from nltk.tokenize import punkt
token_list2 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list))
#upper to lower case
token_list3 = [word.lower() for word in token_list2]
#remove stopwords
from nltk.corpus import stopwords
token_list4 = list(filter(lambda token: token not in stopwords.words(""english""),token_list3))

#lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]
print(""Final Tokens are :"")
print(token_list5,""\n"")
print(""Total tokens : "", len(token_list5))

#word Embedding
from gensim.models import Word2Vec
# train model
model = Word2Vec(token_list5, min_count=2)
# summarize the loaded model

    print(""The model is :"")
    print(model,""\n"")`enter code here`

# summarize vocabulary

    words = list(model.wv`enter code here`.vocab)
    print(""The learned vocabulary words are : \n"",words)

Output- ['p', 'o', 't', 'e', 'n', 'i', 'a', 'l', 'r', 'b', 'u', 'm', 'h', 'd', 'c', 's', 'g', 'q', 'f', 'w', '-']
Expected -[ 'potenial', 'xyz','etc']
</code></pre>
","nlp, gensim, word2vec, feature-extraction, text-classification","<p><code>Word2Vec</code> needs its training corpus to be a sequence where each item (text/sentence) is a <strong>list-of-string-tokens</strong>. </p>

<p>If you instead pass texts that are raw strings, each will appear as a <strong>list-of-one-character-tokens</strong>, and that will result in the final vocabulary you're seeing, where all learned 'words' are just single-characters. </p>

<p>So, take a closer look at your <code>token_list5</code> variable. As it is a list, what is <code>token_list5[0]</code>? (Is it a list-of-strings?) What is <code>token_list5[0][0]</code>? (Is it a full word?)</p>
",1,0,348,2019-08-30 13:59:02,https://stackoverflow.com/questions/57728181/i-get-single-characters-as-learned-vocabulary-on-word2vec-genism-as-an-output
Text Classification - what can you do vs. what are your capabilities?,"<p>Text Classification basically works on the input training sentences. Little or less number of variations of in the sentences do work. But when there is a scenario like</p>

<blockquote>
  <p>What can you do &lt;&lt;==>> What are your capabilities</p>
</blockquote>

<p>This scenario does not work well with the regular classification or bot building platforms.</p>

<p>Are there any approaches for classification that would help me achieve this ?</p>
","nlp, classification, stanford-nlp, text-classification, azure-language-understanding","<p>What you are trying to solve is called Semantic Textual Similarity and is a known and well studied field.</p>

<p>There are many different ways to solve this even if your data is tagged or not.
For example, Google has published the <a href=""https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html"" rel=""nofollow noreferrer"">Universal Sentence Encoder</a> (<a href=""https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb"" rel=""nofollow noreferrer"">code example</a>) which is intended to tell if two sentences are similar like in your case.</p>

<p>Another example would be any solution you can find in <a href=""https://www.kaggle.com/c/quora-question-pairs"" rel=""nofollow noreferrer"">Quora Question Pairs Kaggle competition</a>.</p>

<p>There are also datasets for this problem, for example you can look for SemEval STS (STS for Semantic Textual Similarity), or the <a href=""https://github.com/google-research-datasets/paws"" rel=""nofollow noreferrer"">PAWS dataset</a> </p>
",3,0,69,2019-09-09 05:44:49,https://stackoverflow.com/questions/57848435/text-classification-what-can-you-do-vs-what-are-your-capabilities
The names of the columns in CountVectorier sparse matrix in python,"<p>When I use the code below:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer
X = dataset.Tweet
y = dataset.Type

count_vect = CountVectorizer()
BoW = count_vect.fit_transform(X)
</code></pre>

<p>It returns the term frequency document as a sparse matrix.</p>

<p>I found out how to get the data, indices, and indptr of the sparse matrix.</p>

<p>My problem is how can I get the names of the columns (which should be the features or words)?</p>
","python, sparse-matrix, text-classification, countvectorizer","<p>What you want to use is <code>vectorizer.get_feature_names()</code>. Here is an example from the docs:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?',
]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names())
# ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
print(X.toarray())  
# [[0 1 1 1 0 0 1 0 1]
#  [0 2 0 1 0 1 1 0 1]
#  [1 0 0 1 1 0 1 1 1]
#  [0 1 1 1 0 0 1 0 1]]
</code></pre>

<p>Docs link: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a></p>
",3,2,558,2019-09-09 20:01:31,https://stackoverflow.com/questions/57860515/the-names-of-the-columns-in-countvectorier-sparse-matrix-in-python
How add new samples to the same label using Naive Bayes on php-ml?,"<p>I am newbie on Text Classification and I am trying to create some proof-of-concepts to understand better the concepts of ML using PHP.
So I got <a href=""https://www.softnix.co.th/2018/08/19/naive-bays-text-classification-with-php/"" rel=""nofollow noreferrer"">this example</a>, and I've tried to add a new small text to ""reinforce"" one of my labels (categories), in this case, <strong>Japan</strong>:</p>

<pre><code>&lt;?php
include_once './vendor/autoload.php';
//source: https://www.softnix.co.th/2018/08/19/naive-bays-text-classification-with-php/
use Phpml\Classification\NaiveBayes;
use Phpml\FeatureExtraction\TokenCountVectorizer;
use Phpml\Tokenization\WhitespaceTokenizer;
use Phpml\Tokenization\WordTokenizer;
use Phpml\FeatureExtraction\TfIdfTransformer;

$arr_text = [
    ""London bridge is falling down"",
    ""japan samurai Universal Studio spider man"",
    ""china beijing"",
    ""thai Chiangmai"",
    ""Universal Studio Hollywood"",
    ""2020 Olympic games""
];
$arr_label = [
    ""London"",""Japan"",""China"",""Thailand"",""USA"",""Japan""
];

$tokenize = new WordTokenizer();
$vectorizer = new TokenCountVectorizer($tokenize);

$vectorizer-&gt;fit($arr_text);
$vocabulary = $vectorizer-&gt;getVocabulary();
$arr_transform = $arr_text;
$vectorizer-&gt;transform($arr_transform);

$transformer = new TfIdfTransformer($arr_transform);
$transformer-&gt;transform($arr_transform);

$classifier = new NaiveBayes();
$classifier-&gt;train($arr_transform, $arr_label);

$arr_testset = [
    'Hello Chiangmai I am Siam',
    'I want to go Universal Studio',
    'I want to go Universal Studio because I want to watch spider man',
    'Sonic in 2020'
];

$vectorizer-&gt;transform($arr_testset);
$transformer-&gt;transform($arr_testset);
$result = $classifier-&gt;predict($arr_testset);
var_dump($result);
</code></pre>

<p>The problem is, after added Japan again on array of labels, the result was:</p>

<pre><code>array (size=4)
  0 =&gt; string 'Japan' (length=5)
  1 =&gt; string 'Japan' (length=5)
  2 =&gt; string 'Japan' (length=5)
  3 =&gt; string 'Japan' (length=5)
</code></pre>

<p>But I was expecting:</p>

<pre><code>array (size=4)
  0 =&gt; string 'Thailand' (length=8)
  1 =&gt; string 'USA' (length=3)
  2 =&gt; string 'Japan' (length=5)
  3 =&gt; string 'Japan' (length=5)
</code></pre>

<p><strong>So, How add new samples to the same label?</strong></p>
","php, machine-learning, text-classification, naivebayes, php-ml","<p>There are two problems with your training dataset:</p>

<ol>
<li>It is too small and not representative enough</li>
<li>You gave twice more data when training your <code>Japan</code> label comparing with other labels</li>
</ol>

<p>So, <code>Japan</code> label's model is trained on two sentences whose words are completely non-related and do not repeat. Other labels are trained on just one short sentence. </p>

<p>This leads to <a href=""https://www.datarobot.com/wiki/underfitting/"" rel=""nofollow noreferrer"">underfitted</a> <code>Japan</code> label model that has ""not learned enough"" from the training data, and is not able to model the training data properly nor generalize to new data. In other words, it is too general and triggers on almost any sentence.
Rest labels' models are <a href=""https://www.datarobot.com/wiki/overfitting/"" rel=""nofollow noreferrer"">overfitted</a> - they model the training data too well and trigger only on those sentences that are very close to training set data. </p>

<p>So <code>Japan</code> label catches almost any sentence. And going in the begin of your labels list, it catches all sentences before any label that goes after it in list has a change to evaluate a sentence. Of course you can move <code>Japan</code> labels at the end of the list, but the better solution is - to enlarge your training data set for all labels.</p>

<p>You can also evaluate overfitted label model effect - try for example add to your test set ""London bridge down"" and ""London down"" sentences - the first gives you <code>London</code>, the second - <code>Japan</code>, because the first sentence is close enough to the sentence training set for <code>London</code> label and the second - isn't.</p>

<p>So keep adding the training set data exactly in this manner, just make your training set big and representative enough.</p>
",4,0,1009,2019-09-15 01:57:05,https://stackoverflow.com/questions/57940564/how-add-new-samples-to-the-same-label-using-naive-bayes-on-php-ml
Group features of TF-IDF vector in scikit-learn,"<p>I'm using scikit-learn to train a text classification model based on TF-IDF feature vector by following piece of code:</p>

<pre><code>model = naive_bayes.MultinomialNB()
feature_vector_train = TfidfVectorizer().fit_transform(X)
model.fit(self.feature_vector_train, Y)
</code></pre>

<p>I need to rank the extracted features in decreasing order of their TF-IDF weight and group them into two non-overlapped sets of features and finally train two different classification model. How can I group the main feature vector into an odd-ranked set and an even-ranked set?</p>
","python, scikit-learn, text-classification, tfidfvectorizer","<p>The result of your <code>TfidfVectorizer</code> is an <code>n x m</code> matrix <code>n</code> is the number of documents and <code>m</code> is the number of unique words. Thus, each column in <code>feature_vector_train</code> corresponds to a specific word from your dataset. Adapting a solution from <a href=""https://buhrmann.github.io/tfidf-analysis.html"" rel=""nofollow noreferrer"">this tutorial</a> should allow you to extract the highest and lowest weighted words:</p>

<pre><code>vectorizer = TfidfVectorizer()
feature_vector_train = vectorizer.fit_transform(X)
feature_names = vectorizer.get_feature_names()

total_tfidf_weights = feature_vector_train.sum(axis=0) #this assumes you only want a straight sum of each feature's weight across all documents
#alternatively, you could use vectorizer.transform(feature_names) to get the values of each feature in isolation

#sort the feature names and the tfidf weights together by zipping them
sorted_names_weights = sorted(zip(feature_names, total_tfidf_Weights), key = lambda x: x[1]), reversed=True) #the key argument tells sorted according to column 1. reversed means sort from largest to smallest
#unzip the names and weights
sorted_features_names, sorted_total_tfidf_weights = zip(*sorted_names_weights)
</code></pre>

<p>From this point you should be able to separate the features as you'd like. Once you have them into two groups, <code>group1</code> and <code>group2</code>, you can separate them into two matrices like this:</p>

<pre><code>#create a feature_name to column index mapping
column_mapping = dict((name, i) for i, name, in enumerate(feature_names))

#get the submatrices
group1_column_indexes = [column_mapping[feat] for feat in group1]
group1_feature_vector_train  = feature_vector_train[:,group1_column_indexes] #all rows, but only group1 columns

group2_column_indexes = [column_mapping[feat] for feat in group2]
group2_feature_vector_train  = feature_vector_train[:,group2_column_indexes]
</code></pre>
",1,0,931,2019-09-17 17:27:33,https://stackoverflow.com/questions/57979277/group-features-of-tf-idf-vector-in-scikit-learn
How to change this RNN text classification code to text generation?,"<p>I have this code to do text classification with TensorFlow RNN, but how to change it to do text generation instead?</p>

<p>The following text classification has 3D input, but 2D output. Should it be changed to 3D input and 3D output for text generation? and how?</p>

<p>The example data are:</p>

<pre><code>t0      t1      t2
british gray    is =&gt; cat (y=0)
0       1       2
white   samoyed is =&gt; dog (y=1)
3       4       2 
</code></pre>

<p>For classification feeding ""british gray is"" results in ""cat"". What I wish to get is feeding ""british"" should result in the next word ""gray"".</p>

<pre><code>import tensorflow as tf;
tf.reset_default_graph();

#data
'''
t0      t1      t2
british gray    is =&gt; cat (y=0)
0       1       2
white   samoyed is =&gt; dog (y=1)
3       4       2 
'''
Bsize = 2;
Times = 3;
Max_X = 4;
Max_Y = 1;

X = [[[0],[1],[2]], [[3],[4],[2]]];
Y = [[0],           [1]          ];

#normalise
for I in range(len(X)):
  for J in range(len(X[I])):
    X[I][J][0] /= Max_X;

for I in range(len(Y)):
  Y[I][0] /= Max_Y;

#model
Inputs   = tf.placeholder(tf.float32, [Bsize,Times,1]);
Expected = tf.placeholder(tf.float32, [Bsize,      1]);

#single LSTM layer
#'''
Layer1   = tf.keras.layers.LSTM(20);
Hidden1  = Layer1(Inputs);
#'''

#multi LSTM layers
'''
Layers = tf.keras.layers.RNN([
  tf.keras.layers.LSTMCell(30), #hidden 1
  tf.keras.layers.LSTMCell(20)  #hidden 2
]);
Hidden2 = Layers(Inputs);
'''

Weight3  = tf.Variable(tf.random_uniform([20,1], -1,1));
Bias3    = tf.Variable(tf.random_uniform([   1], -1,1));
Output   = tf.sigmoid(tf.matmul(Hidden1,Weight3) + Bias3);

Loss     = tf.reduce_sum(tf.square(Expected-Output));
Optim    = tf.train.GradientDescentOptimizer(1e-1);
Training = Optim.minimize(Loss);

#train
Sess = tf.Session();
Init = tf.global_variables_initializer();
Sess.run(Init);

Feed = {Inputs:X, Expected:Y};
for I in range(1000): #number of feeds, 1 feed = 1 batch
  if I%100==0: 
    Lossvalue = Sess.run(Loss,Feed);
    print(""Loss:"",Lossvalue);
  #end if

  Sess.run(Training,Feed);
#end for

Lastloss = Sess.run(Loss,Feed);
print(""Loss:"",Lastloss,""(Last)"");

#eval
Results = Sess.run(Output,Feed);
print(""\nEval:"");
print(Results);

print(""\nDone."");
#eof
</code></pre>
","tensorflow, machine-learning, nlp, recurrent-neural-network, text-classification","<p>I found out how to switch it (the code) to do text generation task, use 3D input (X) and 3D labels (Y) as in the source code below:</p>

<p>Source code:</p>

<pre><code>import tensorflow as tf;
tf.reset_default_graph();

#data
'''
t0       t1       t2
british  gray     is  cat
0        1        2   (3)  &lt;=x
1        2        3        &lt;=y
white    samoyed  is  dog
4        5        2   (6)  &lt;=x
5        2        6        &lt;=y 
'''
Bsize = 2;
Times = 3;
Max_X = 5;
Max_Y = 6;

X = [[[0],[1],[2]], [[4],[5],[2]]];
Y = [[[1],[2],[3]], [[5],[2],[6]]];

#normalise
for I in range(len(X)):
  for J in range(len(X[I])):
    X[I][J][0] /= Max_X;

for I in range(len(Y)):
  for J in range(len(Y[I])):
    Y[I][J][0] /= Max_Y;

#model
Input    = tf.placeholder(tf.float32, [Bsize,Times,1]);
Expected = tf.placeholder(tf.float32, [Bsize,Times,1]);

#single LSTM layer
'''
Layer1   = tf.keras.layers.LSTM(20);
Hidden1  = Layer1(Input);
'''

#multi LSTM layers
#'''
Layers = tf.keras.layers.RNN([
  tf.keras.layers.LSTMCell(30), #hidden 1
  tf.keras.layers.LSTMCell(20)  #hidden 2
],
return_sequences=True);
Hidden2 = Layers(Input);
#'''

Weight3  = tf.Variable(tf.random_uniform([20,1], -1,1));
Bias3    = tf.Variable(tf.random_uniform([   1], -1,1));
Output   = tf.sigmoid(tf.matmul(Hidden2,Weight3) + Bias3); #sequence of 2d * 2d

Loss     = tf.reduce_sum(tf.square(Expected-Output));
Optim    = tf.train.GradientDescentOptimizer(1e-1);
Training = Optim.minimize(Loss);

#train
Sess = tf.Session();
Init = tf.global_variables_initializer();
Sess.run(Init);

Feed   = {Input:X, Expected:Y};
Epochs = 10000;

for I in range(Epochs): #number of feeds, 1 feed = 1 batch
  if I%(Epochs/10)==0: 
    Lossvalue = Sess.run(Loss,Feed);
    print(""Loss:"",Lossvalue);
  #end if

  Sess.run(Training,Feed);
#end for

Lastloss = Sess.run(Loss,Feed);
print(""Loss:"",Lastloss,""(Last)"");

#eval
Results = Sess.run(Output,Feed).tolist();
print(""\nEval:"");
for I in range(len(Results)):
  for J in range(len(Results[I])):
    for K in range(len(Results[I][J])):
      Results[I][J][K] = round(Results[I][J][K]*Max_Y);
#end for i      
print(Results);

print(""\nDone."");
#eof
</code></pre>
",0,0,82,2019-09-23 02:53:15,https://stackoverflow.com/questions/58055095/how-to-change-this-rnn-text-classification-code-to-text-generation
Is text classification fast enough for type ahead search?,"<p>I'm working on designing a typeahead service that can be used to search for many different things. I was thinking about creating a text classification model to categorize these searches before actually making the search. </p>

<p>Here's an example of the result I'd want from the classification model.</p>

<p>Input</p>

<pre><code>John Smith
</code></pre>

<p>Output</p>

<pre><code>[
  {
    ""likeliness"": .6,
    ""category"": ""car-name-typeahead-search""
  },
  {
    ""likeliness"": .9,
    ""category"": ""person-name-typeahead-search""
  },
  {
    ""likeliness"": .1,
    ""category"": ""vin-typeahead-search""
  },
  {
    ""likeliness"": .2,
    ""category"": ""help-page""
  },
  {
    ""likeliness"": .2,
    ""category"": ""faq-page""
  }
]
</code></pre>

<p>Then I'd take the categories that have a likeliness above some value and actually do the typeahead search. Also I'd return the results ordered by the likeliness rank.</p>

<p>We have been collecting data about people's searches and tracking what they were actually looking for so we should have the data needed to train a text classification model.</p>

<p>My question is can text classification models be fast enough to be used with a type ahead service and not be prohibitively expensive? Are there certain types of text classification algorithms that I should be looking at?</p>
","machine-learning, text-classification","<p>Usually in modern serving framework (like <a href=""https://github.com/tensorflow/serving"" rel=""nofollow noreferrer"">tensorflow serving</a> running on a standalone server), a standard text classification model based on shallow neural networks should have a latency under 1ms). You can look for a model composed by:</p>

<ul>
<li>word embedding layer (up to millions of words in vocabulary)</li>
<li>hidden layer (1-3)</li>
<li>classification (up to thousands of categories)</li>
</ul>

<p>If your expected response time is &lt;= 200ms, you should not worry about the latency from the classification. In the worst case, 10ms is sufficient using the setup above.</p>
",1,0,172,2019-09-23 23:18:22,https://stackoverflow.com/questions/58071233/is-text-classification-fast-enough-for-type-ahead-search
How to shrink a bag-of-words model?,"<p>The question title says it all: How can I make a bag-of-words model smaller? I use a Random Forest and a bag-of-words feature set. My model reaches 30 GB in size and I am sure that most words in the feature set do not contribute to the overall performance.</p>

<p>How to shrink a big bag-of-words model without losing (too much) performance?</p>
","scikit-learn, nlp, classification, random-forest, text-classification","<p>Use feature selection. Feature selection removes features from your dataset based on their distribution with regards to your labels, using some scoring function. </p>

<p>Features that rarely occur, or occur randomly with all your labels, for example, are very unlikely to contribute to accurate classification, and get low scores.</p>

<p>Here's an example using <code>sklearn</code>:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_selection import SelectPercentile

# Assume some matrix X and labels y
# 10 means only include the 10% best features
selector = SelectPercentile(percentile=10)

# A feature space with only 10% of the features
X_new = selector.fit_transform(X, y)

# See the scores for all features
selector.scores
</code></pre>

<p>As always, be sure to only call <code>fit_transform</code> on your training data. When using dev or test data, only use <code>transform</code>. See <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html"" rel=""nofollow noreferrer"">here</a> for additional documentation.</p>

<p>Note that there is also a <code>SelectKBest</code>, which does the same, but which allows you to specify an absolute number of features to keep, instead of a percentage.</p>
",5,2,1704,2019-10-02 08:29:31,https://stackoverflow.com/questions/58197911/how-to-shrink-a-bag-of-words-model
Is this classification model overfitting?,"<p>I am performing a url classification (phishing - nonphishing) and I plotted the learning curves (training vs cross validation score) for my model (Gradient Boost). </p>

<p><strong>My View</strong></p>

<p>It seems that these two curves converge and the difference is not significant. Tt's normal for the training set to have a slightly higher accuracy). <strong>(Figure 1)</strong></p>

<p><a href=""https://i.sstatic.net/XNxXv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XNxXv.png"" alt=""Gradient boost learning curves""></a></p>

<p><strong>The Question</strong></p>

<p>I have limited experience on machine learning, thus I am asking your opinion. Is the way I am approaching the problem right? Is this model fine or is it overfitting?</p>

<p><em>Note: The classes are balanced and the features are well chosen</em> </p>

<p><strong>Relevant code</strong></p>

<pre><code>from yellowbrick.model_selection import LearningCurve

def plot_learning_curves(Χ, y, model):

       # Create the learning curve visualizer
       cv = StratifiedKFold(n_splits=5)
       sizes = np.linspace(0.1, 1.0, 8)
       visualizer = LearningCurve(model, cv=cv, train_sizes=sizes, n_jobs=4)
       visualizer.fit(Χ, y)  # Fit the data to the visualizer
       visualizer.poof()
</code></pre>
","machine-learning, scikit-learn, classification, text-classification, overfitting-underfitting","<p>Firstly, in your graph there are 8 different models.</p>
<p>It's hard to tell if one of them is overfitting because overfitting can be detected with a &quot;epoch vs performance (train / valid)&quot; graph (there would be 8 in your case).</p>
<p>Overfitting means that, after a certain number of epochs, as the number of epoch increases, training accuracy goes up while validation accuracy goes down. This can be the case, for example, when you have too few data points regarding the complexity of your problem, hence your model is using spurious correlations.</p>
<p>With your graph, what we can say is that the complexity of your problem seems to require a &quot;high&quot; number or training instances because your validation performance keep increasing as you add more training instances. There is a chance that the model with &lt;10000 is overfitting but your &gt;50000 could be overiftting too and we don't see that because you are using early stopping!</p>
",1,0,914,2019-10-03 14:38:21,https://stackoverflow.com/questions/58221470/is-this-classification-model-overfitting
How does Ulmfit&#39;s language model work when applied on a text classification problem?,"<p>I have been playing around with Ulmfit a lot lately and still cannot wrap my head around how the language model’s ability to make sound predictions about the next word affects the classification of texts. I guess my real problem is that I do not understand what is happening at the low level of the network. So correct me if I am wrong but the procedure is like this right (?):</p>

<p>The language model gets pre-trained and then fine-tuned. This part seems clear to me: Based on the current and preceding words you form probabilities about the next words.
Then the model gets stripped from the softmax layer designed to create the probability distribution.
You add the decoder consisting of a reLU-Layer (what is this layer actually doing?) and another softmax layer that outputs the probability of class membership of a given text document. So here are a lot of things I do not understand: How is the text document taken in and processed? Word for word I assume? So how do you end up with the prediction at the end? Is it averaged over all words?
Hmm you can see I am very confused. I hope you can help me understand Ulmfit better! Thanks in advance!</p>
","nlp, lstm, text-classification, language-model, fast-ai","<p>ULMFiT's model is ""a regular LSTM"", which is a special case of a Recurrent Neural Network (RNN).</p>

<p>RNNs ""eat"" the input text word by word (sometimes character by character), and after every ""bite"" they:</p>

<ul>
<li>produce an output</li>
<li>update an internal hidden state </li>
</ul>

<p>In text classification, the output is discarded until the very end. The <strong>updated hidden state</strong> is instead added to the next word to bite. After the RNN ate the last word, you can check the output layer (typically a softmax layer with as many neurons as your labels), compute the loss against the true label, then update the weights accordingly.</p>

<p>After the training phase, suppose you want to classify a new document. The RNN ""eats"" the input again and updates its hidden state after each word. You disregard the output layer until you see the last word: at that point the max element of the output softmax layer will be your predicted label.</p>

<p>I found particularly helpful this <a href=""https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"" rel=""nofollow noreferrer"">PyTorch tutorial</a>.</p>

<p><a href=""https://i.sstatic.net/9JC57.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9JC57.png"" alt=""enter image description here""></a></p>
",3,1,129,2019-10-09 18:01:27,https://stackoverflow.com/questions/58309837/how-does-ulmfits-language-model-work-when-applied-on-a-text-classification-prob
How to distinguish the direction of important features from xgboost or random forest?,"<p>I'm now working on binary text classification problem (like sentiment analysis), and it's trivial to pull out top important features of xgboost or random forest just by <code>feature_importances_</code></p>

<p>Suppose we have two labelling 1 and 0 for this classification problem. Then there's any way to print out the direction of the features (positive or negative)? Say, word feature A has an enrichment or high tfidf with labelling 1.</p>

<p>Certainly I could pull out the tfidf column of this specific word feature, and correlate with the labelling with pearson coefficient, and the +/- of coefficient would indicate the direction, right? Any other more elegant way for this or xgboost and random forest has built-in such functions. (I didn't find)</p>

<p>Thanks</p>
","nlp, random-forest, xgboost, sentiment-analysis, text-classification","<p>It isn't exactly what you're asking for, but I usually use <a href=""https://github.com/marcotcr/lime"" rel=""nofollow noreferrer"">Lime</a> to do this. I like how it works even if I switch models. </p>
",1,1,3482,2019-10-10 02:41:49,https://stackoverflow.com/questions/58314707/how-to-distinguish-the-direction-of-important-features-from-xgboost-or-random-fo
How to solve fix &#39;list index out of range&#39; while accessing large amount of data from file?,"<p>I am working on a classifier that will access 200000 data items from a dataset but it only accesses about 1400 data correctly and shows  <code>list index out of range</code>.</p>

<p>How can I access all of the items from the dataset? </p>

<p>Here the structure of the dataset. </p>

<pre class=""lang-py prettyprint-override""><code>investing: can you profit in agricultural commodities?
bad weather is one factor behind soaring food prices. can you make hay with farm stocks? possibly: but be prepared to harvest gains on a moment's ...
http://rssfeeds.usatoday.com/~r/usatodaycommoney-topstories/~3/qbhb22sut9y/2011-05-19-can-you-make-gains-in-grains_n.htm
0
20 May 2011 15:13:57
ut
business

no tsunami but fifa's corruption storm rages on
though jack warner's threatened soccer ""tsunami"" remains stuck in the doldrums, the corruption storm raging around fifa shows no sign of abating after another extraordinary week for the game's governing body.
http://feeds.reuters.com/~r/reuters/sportsnews/~3/ffa6ftdsudg/us-soccer-fifa-idustre7563p620110607
1
07 Jun 2011 17:54:54
reuters
sport

critic's corner weekend: 'fringe' wraps third season
joshua jackson's show goes out with a bang. plus: amazing race nears the finish line.
http://rssfeeds.usatoday.com/~r/usatoday-lifetopstories/~3/duk9oew5auc/2011-05-05-critics-corner_n.htm
2
06 May 2011 23:36:21
ut
entertainment
</code></pre>

<p>Here is the code: </p>

<pre class=""lang-py prettyprint-override""><code>with open('news', 'r') as f:
    text = f.read()
    news = text.split(""\n\n"")
    count = {'sport': 0, 'world': 0, ""us"": 0, ""business"": 0, ""health"": 0, ""entertainment"": 0, ""sci_tech"": 0}
    for news_item in news:
        lines = news_item.split(""\n"")
        print(lines[6])
        file_to_write = open('data/' + lines[6] + '/' + str(count[lines[6]]) + '.txt', 'w+')
        count[lines[6]] = count[lines[6]] + 1
        file_to_write.write(news_item)  # python will convert \n to os.linesep
        file_to_write.close()
</code></pre>

<p>it shows the following output. </p>

<pre class=""lang-py prettyprint-override""><code>
IndexError                                Traceback (most recent call last)
&lt;ipython-input-1-d04a79ce68f6&gt; in &lt;module&gt;
      5     for news_item in news:
      6         lines = news_item.split(""\n"")
----&gt; 7         print(lines[6])
      8         file_to_write = open('data/' + lines[6] + '/' + str(count[lines[6]]) + '.txt', 'w+')
      9         count[lines[6]] = count[lines[6]] + 1

IndexError: list index out of range
</code></pre>
","python, file, text-classification, filewriter","<p>You are assuming that you always have 7 or more lines in each block. Perhaps your file ends in <code>\n\n</code>, or you have some blocks that are corrupted.</p>

<p>Simply test for the length and skip the block: </p>

<pre><code>for news_item in news:
    lines = news_item.split(""\n"")
    if len(lines) &lt; 7:
        continue
</code></pre>

<p>Note that you really don't need to read the whole file into memory here, you can also loop over the file object and read additional lines from a file object. Personally, I'd create a separate generator object that picks out specific lines from the file:</p>

<pre><code>def block_line_at_n(fobj, n):
    while True:
        for i, line in enumerate(fobj):
            if line == ""\n"":
                # end of block, start a new block
                break
            if i == n:
                yield line
        else:
            # end of the file, exit
            return

with open('news', 'r') as f:
    for line in block_line_at_n(f, 6):
</code></pre>
",1,-2,503,2019-10-22 17:02:01,https://stackoverflow.com/questions/58509143/how-to-solve-fix-list-index-out-of-range-while-accessing-large-amount-of-data
"Sklearn (NLP text classifier newbie) - issue with shape and vectorizer, X and Y not matching up","<p>I want to create a text classifer that looks at research abstracts and determines whether they are focused on access to care, based on a labeled dataset I have.  The data source is an Excel spreadsheet, with three fields (project_number, abstract, and accessclass) and 326 rows of abstracts.  The accessclass is 1 for access related and 0 for not access related (not sure if this is relevant). Anyway, I tried following along a tutorial by wanted to make it relevant by adding my own data and I'm having some issues with my X and Y arrays.  Any help is appreciated. </p>

<pre><code>import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import naive_bayes
from sklearn.metrics import roc_auc_score

df = pd.read_excel(""accessclasses.xlsx"")
df.head()

#TFIDF vectorizer
stopset = set(stopwords.words('english'))
vectorizer = TfidfVectorizer(use_idf=True, lowercase=True, 
strip_accents='ascii', stop_words=stopset)

y = df.accessclass
x = vectorizer.fit_transform(df)

print(x.shape)
print(y.shape)
#above and below seem to be where the issue is.   
x_train, x_test, y_train, y_test = train_test_split(x, y) 
</code></pre>
","python, scikit-learn, nlp, text-classification, sklearn-pandas","<p>You are using your whole dataframe to encode your predictor. Remember to use only the abstract in the transformation (you could also fit the corpus word dictionary before and then transform it afterwards).</p>

<p>Here's a solution:</p>

<pre><code>y = df.accessclass
x = vectorizer.fit_transform(df.abstract)
</code></pre>

<p>The rest looks ok.</p>
",1,0,64,2019-10-31 16:06:40,https://stackoverflow.com/questions/58647270/sklearn-nlp-text-classifier-newbie-issue-with-shape-and-vectorizer-x-and-y
Python NLTK and Pandas - text classifier - (newbie ) - importing my data in a format similar to provided example,"<p>I'm new to text classification, however I get most of the concepts.  In short, I have a list of restaurant reviews in an Excel dataset and I want to use them as my training data.  Where I'm struggling is with the example syntax for importing both the actual review and the classification (1 = pos, 0 = neg) as part of my training dataset.  I understand how to do this if I create my dataset manually in a tuple (i.e., what I have current have #'ed out under train).  Any help is appreciated.  </p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
import pandas as pd

df = pd.read_excel(""reviewclasses.xlsx"")

customerreview= df.customerreview.tolist() #I want this to be what's in 
""train"" below (i.e., ""this is a negative review"")

reviewrating= df.reviewrating.tolist() #I also want this to be what's in 
""train"" below (e.g., 0)

#train = [(""Great place to be when you are in Bangalore."", ""1""),
#  (""The place was being renovated when I visited so the seating was 
limited."", ""0""),
#  (""Loved the ambiance, loved the food"", ""1""),
#  (""The food is delicious but not over the top."", ""0""),
#  (""Service - Little slow, probably because too many people."", ""0""),
#  (""The place is not easy to locate"", ""0""),
#  (""Mushroom fried rice was spicy"", ""1""),
#]

dictionary = set(word.lower() for passage in train for word in 
word_tokenize(passage[0]))

t = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) 
for x in train]

# Step 4 – the classifier is trained with sample data
classifier = nltk.NaiveBayesClassifier.train(t)

test_data = ""The food sucked and I couldn't wait to leave the terrible 
restaurant.""
test_data_features = {word.lower(): (word in 
word_tokenize(test_data.lower())) for word in dictionary}

print (classifier.classify(test_data_features))
</code></pre>
","python, pandas, nlp, nltk, text-classification","<p>I figured it out. I basically just needed to combine two lists into a tuple. </p>

<pre><code>def merge(customerreview, reviewrating): 

    merged_list = [(customerreview[i], reviewrating[i]) for i in range(0, 
len(customerreview))] 
    return merged_list 

train = (merge(customerreview, reviewrating)) 
</code></pre>
",0,0,268,2019-10-31 18:29:06,https://stackoverflow.com/questions/58649351/python-nltk-and-pandas-text-classifier-newbie-importing-my-data-in-a-fo
"Python NLP - Sklearn - text classifier, unigrams and bigrams the same for both negative and positive labels","<p>I'm trying to create a text classifier to determine whether an abstract indicates an access to care research project.  I am importing from a dataset that has two fields: Abstract and Accessclass.  Abstract is a 500 word description about the project and Accessclass is 0 for not access-related and 1 for access-related. I'm still in the developing stages, however when I looked at the unigrams and bigrams for 0 and 1 labels, they were the same, despite very distinctly different tones of text.  Is there something I'm missing in my code? For example, am I accidentally double adding negative or positive? Any help is appreciate. </p>

<pre><code>import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import naive_bayes

df = pd.read_excel(""accessclasses.xlsx"")
df.head()

from io import StringIO
col = ['accessclass', 'abstract']
df = df[col]
df = df[pd.notnull(df['abstract'])]
df.columns = ['accessclass', 'abstract']
df['category_id'] = df['accessclass'].factorize()[0]
category_id_df = df[['accessclass', 'category_id']].drop_duplicates().sort_values('category_id')
category_to_id = dict(category_id_df.values)
id_to_category = dict(category_id_df[['category_id', 'accessclass']].values)
df.head()

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(sublinear_tf=True, min_df=4, norm='l2', encoding='latin-1', ngram_range=(1, 
2), stop_words='english')
features = tfidf.fit_transform(df.abstract).toarray()
labels = df.category_id
print(features.shape)

from sklearn.feature_selection import chi2
import numpy as np
N = 2
for accessclass, category_id in sorted(category_to_id.items()):
   features_chi2 = chi2(features, labels == category_id)
   indices = np.argsort(features_chi2[0])
   feature_names = np.array(tfidf.get_feature_names())[indices]
   unigrams = [v for v in feature_names if len(v.split(' ')) == 1]
   bigrams = [v for v in feature_names if len(v.split(' ')) == 2]
   print(""# '{}':"".format(accessclass))
   print(""  . Most correlated unigrams:\n. {}"".format('\n. '.join(unigrams[-N:])))
   print(""  . Most correlated bigrams:\n. {}"".format('\n. '.join(bigrams[-N:])))
</code></pre>
","python, scikit-learn, nlp, text-classification, sklearn-pandas","<p>I think the problem in your code is setting <code>min_df</code> with a big number like <code>4</code> on this small dataset. According to your data that you have posted, the most common words are stopwords that will be removed after using <code>TfidfVectorizer</code>. Here they are:</p>

<pre><code>to :  19
and :  11
a :  6
the :  6
are :  6
of :  6
for :  5
is :  4
in :  4
will :  4
access :  4
I :  4
times :  4
healthcare :  3
more :  3
have :  3
with :  3
...
</code></pre>

<p>And these are the unigram... the bigram count will be way lower.</p>

<p>You can solve that by either one of these two options:</p>

<ul>
<li>Setting the <code>stopwords</code> argument to <code>None</code> like so <code>stopwords=None</code> </li>
<li>Setting <code>min_df</code> to be lower than <code>4</code> like <code>1</code> or <code>2</code> for example.</li>
</ul>

<p>I recommend using the second option as the first will return stopwords as correlated which isn't helpful at all. I have tried using <code>min_df=1</code> and here is the result:</p>

<pre><code>  . Most correlated unigrams:
. times
. access

  . Most correlated bigrams:
. enjoyed watching
. wait times
</code></pre>
",1,-1,533,2019-11-05 15:17:17,https://stackoverflow.com/questions/58714357/python-nlp-sklearn-text-classifier-unigrams-and-bigrams-the-same-for-both-n
Pandas Get rows if value is in column dataframe,"<p>I have Information Gain dataframe and tf dataframe. the data looks like this :</p>

<p><strong>Information Gain</strong></p>

<pre class=""lang-py prettyprint-override""><code>    Term      IG
0   alqur     0.641328
1   an        0.641328
2   ayatayat  0.641328
3   bagai     0.641328
4   bantai    0.641328
5   besar     0.641328
</code></pre>

<p><strong>Term Frequency</strong></p>

<pre class=""lang-py prettyprint-override""><code>            A   B   A+B
ahli        1   0   1
alas        1   0   1
alqur       0   1   1
an          0   1   1
ayatayat    0   1   1
...        ... ... ...
terus       0   1   1
tuduh       0   1   1
tulis       1   0   1
ulama       1   0   1
upaya       0   1   1
</code></pre>

<p>let's say table Information Gain = IG
and table tf = TF</p>

<p>I wanted to check if IG.Term is in TF.index then get the row values so it should be like this :</p>

<pre class=""lang-py prettyprint-override""><code>    Term      A    B    A+B
0   alqur     0    1    1
1   an        0    1    1
2   ayatayat  0    1    1
3   bagai     1    0    1
4   bantai    1    1    2
5   besar     1    0    1
</code></pre>

<p>NB : I don't need the IG value anymore</p>
","python, pandas, numpy, data-science, text-classification","<p>Filter by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isin.html"" rel=""nofollow noreferrer""><code>Series.isin</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a> and convert index to column:</p>

<pre><code>df = TF[TF.index.isin(IG['Term'])].rename_axis('Term').reset_index()
print (df)
       Term  A  B  A+B
0     alqur  0  1    1
1        an  0  1    1
2  ayatayat  0  1    1
</code></pre>

<p>Or use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>DataFrame.merge</code></a> with default inner join:</p>

<pre><code>df = IG[['Term']].merge(TF, left_on='Term', right_index=True)
print (df)
       Term  A  B  A+B
0     alqur  0  1    1
1        an  0  1    1
2  ayatayat  0  1    1
</code></pre>
",4,1,726,2019-11-12 06:58:05,https://stackoverflow.com/questions/58813515/pandas-get-rows-if-value-is-in-column-dataframe
How to put an array of strings with two variables into a text file?,"<p>I'm trying to make a text classifier app. I have an array of strings which contains two parameters separated by comma like below:</p>

<pre><code>pos_tweets = [('I love this car', 'positive'),
              ('This view is amazing', 'positive'),
              ('I feel great this morning', 'positive')]
</code></pre>

<p>And with that array of strings I can execute this code:</p>

<pre><code>tweets = []
for (words, sentiment) in pos_tweets:
    words_filtered = [e.lower() for e in words.split() if len(e) &gt;= 3] 
    tweets.append((words_filtered, sentiment))
print(tweets)
</code></pre>

<p><strong>with output:</strong></p>

<pre><code>[(['love', 'this', 'car'], 'positive'), (['this', 'view', 'amazing'], 'positive'), (['feel', 'great', 'this', 'morning'], 'positive')]
</code></pre>

<p>What I'm trying to do is to put that array of strings into a text file and still able to execute the code with the same output as above.</p>
","python, arrays, python-3.x, text-classification, naivebayes","<p>You can have a text file where every line contains an element in your pos_tweets array. So in </p>

<p>pos_tweets.txt</p>

<pre><code>I love this car, positive
This view is amazing, positive
</code></pre>

<p>Then you can read in each line with </p>

<pre><code>import csv

tweets = []
with open('pos_tweets.txt') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    for row in csv_reader:
        words = row[0]
        sentiment = row[1]
        words_filtered = [e.lower() for e in words.split() if len(e) &gt;= 3] 
        tweets.append((words_filtered, sentiment))

</code></pre>
",0,0,321,2019-11-12 19:00:22,https://stackoverflow.com/questions/58825049/how-to-put-an-array-of-strings-with-two-variables-into-a-text-file
Is it possible to train the sentiment classification model with the labeled data and then use it to predict sentiment on data that is not labeled?,"<p>I want to do sentiment analysis using machine learning (text classification) approach. For example nltk Naive Bayes Classifier.
But the issue is that a small amount of my data is labeled. (For example, 100 articles are labeled positive or negative) and 500 articles are not labeled.
I was thinking that I train the classifier with labeled data and then try to predict sentiments of unlabeled data. 
Is it possible? 
I am a beginner in machine learning and don't know much about it. </p>

<p>I am using python 3.7. </p>

<p>Thank you in advance. </p>
","nltk, python-3.7, sentiment-analysis, text-classification, training-data","<blockquote>
  <p>Is it possible to train the sentiment classification model with the labeled data and then use it to predict sentiment on data that is not labeled?</p>
</blockquote>

<p>Yes. This is basically the definition of what <em>supervised learning</em> is.</p>

<p>I.e. you train on data that has labels, so that you can then put it into production on categorizing your data that does not have labels.</p>

<p>(Any book on supervised learning will have code examples.)</p>

<p>I wonder if your question might really be: can I use supervised learning to make a model, assign labels to another 500 articles, then do further machine learning on all 600 articles? Well the answer is still yes, but the quality will fall somewhere between these two extremes:</p>

<ul>
<li>Assign random labels to the 500. Bad results.</li>
<li>Get a domain expert assign correct labels to those 500. Good results.</li>
</ul>

<p>Your model could fall anywhere between those two extremes. It is useful to know where it is, so know if it is worth using the data. You can get an estimate of that by taking a sample, say 25 records, and have them also assigned by a domain expert. If all 25 match, there is a reasonable chance your other 475 records also have been given good labels. If  e.g. only 10 of the 25 match, the model is much closer to the random end of the spectrum, and using the other 475 records is probably a bad idea.</p>

<p>(""10"", ""25"", etc. are arbitrary examples; choose based on the number of different labels, and your desired confidence in the results.)</p>
",1,0,431,2019-11-15 03:34:31,https://stackoverflow.com/questions/58869955/is-it-possible-to-train-the-sentiment-classification-model-with-the-labeled-data
How can I test my natural language processing model with &quot;real&quot; cases?,"<p>I am introducing myself to Natural Languaje Processing and artificial neural networks and I have followed this wonderful <a href=""https://realpython.com/python-keras-text-classification/"" rel=""nofollow noreferrer"">tutorial</a>
Once finished it, I would like to know if there is any way to test the model with phrases that I can invent, (That film entertained me a lot) for example.
Because it is very good to know the percentage of success on the test set, but I want to know how to test it.</p>
","python, machine-learning, keras, nlp, text-classification","<p>try</p>

<pre><code>test_phrase = 'That film entertained me a lot'
example = vectorizer.transform([test_phrase])
print(test_phrase + ' was classified as ' + str(classifier.predict(example)))
</code></pre>
",1,-1,89,2019-11-15 14:43:52,https://stackoverflow.com/questions/58879358/how-can-i-test-my-natural-language-processing-model-with-real-cases
How to build keras classification model using two text features as input,"<p>I am trying to build a text classification model which uses two input text features to finally predict <code>10</code> classes but I need to give significant impact for each input branch on the final output i.e. each branch should participate with 50% of the final decision</p>

<p>currently the setup am trying to achieve is as below</p>

<pre><code>from keras import Input
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation
from keras.layers.merge import concatenate

# input
input_1 = Input(shape=(x1_train[0].shape))  # , dtype = 'int32')
input_2 = Input(shape=(x2_train[0].shape))  # , dtype = 'int32')

desc = Sequential()
desc.add(Dense(5, activation='relu', input_shape=x1_train[0].shape))
desc.add(Dropout(0.2))
desc.add(Dense(10, activation='sigmoid'))

tax = Sequential()
tax.add(Dense(5, activation='relu', input_shape=x2_train[0].shape))
tax.add(Dropout(0.2))
tax.add(Dense(10, activation='sigmoid'))

# conact
concat = concatenate([desc, tax])
final_model = Sequential()
final_model.add(concat)
final_model.add(Dropout(0.5))
final_model.add(Dense(10, activation='softmax'))

# compile
model = Model(inputs=[input_1, input_2], outputs=final_model)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
print(model.summary())
</code></pre>

<p>my question is, would this be the best practice or I should look into something else ?</p>
","keras, text-classification","<p>So the simplest form I can come up for this question is the following.</p>

<pre><code>from tensorflow.keras import Input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Activation, Concatenate

n_desc_features = 10
n_tax_features = 20

# Input layers
input_1 = Input(shape=(n_desc_features,))  # , dtype = 'int32')
input_2 = Input(shape=(n_tax_features,))  # , dtype = 'int32')

# Getting model 1 output
desc_out = Dense(5, activation='relu')(input_1)
desc_out = Dropout(0.2)(desc_out)
desc_out = Dense(10, activation='sigmoid')(desc_out)
# Getting model 2 output
tax_out = Dense(5, activation='relu')(input_2)
tax_out = Dropout(0.2)(tax_out)
tax_out = Dense(10, activation='sigmoid')(tax_out)

# Concatenating and creating the final output
concat = Concatenate(axis=-1)([desc_out, tax_out])
final_out = Dropout(0.5)(concat)
final_out = Dense(10, activation='softmax')(final_out)

# Create the model and compile
model = Model(inputs=[input_1, input_2], outputs=final_out)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
print(model.summary())
</code></pre>

<p>Hope it is pretty straight-forward what is happening here. You have two sub-models created. One produces <code>desc_out</code> and the other produces <code>tax_out</code>.</p>

<p>Then using those two outputs you create the final model output <code>final_out</code>. And you use that along with the input layers to create a <code>Model</code> object. </p>

<p>I think using <code>Sequential</code> is unnecessary here and you don't need to explicitly have models for the two sub-models as you're not doing optimization particular to sub-models, rather you optimizer the full thing at once.</p>
",2,0,736,2019-11-17 10:54:35,https://stackoverflow.com/questions/58899755/how-to-build-keras-classification-model-using-two-text-features-as-input
My text classifier model doens&#39;t improve with multiple classes,"<p>I'm trying to train a model for a text classification and the model take a list of maximum 300 integer embedded from articles. The model trains without problem and all but the accuracy won't go up. </p>

<p>The target consists of 41 categories encoded into int from 0 to 41 and were then normalized.</p>

<p>The table would look like this</p>

<p><img src=""https://i.sstatic.net/QIonb.png"" alt=""Table1""></p>

<p>Also, I don't know how my model should look like since I refered on two different example as per below</p>

<ul>
<li>A binary classifier with one input column and one output column <a href=""https://towardsdatascience.com/tensorflow-2-0-data-transformation-for-text-classification-b86ee2ad8877"" rel=""nofollow noreferrer"">Example 1</a></li>
<li>Multiple class classifier with multiple columns as input <a href=""https://medium.com/tensorflow/how-to-train-boosted-trees-models-in-tensorflow-ca8466a53127"" rel=""nofollow noreferrer"">Example 2</a></li>
</ul>

<p>I have tried modifying my model based on both model but the model accuracy won't change and even getting lower per epoch</p>

<p>Should I add more layers to my model or I have done something stupid that I haven't realized?</p>

<p>Note: If the 'df.pickle' download link broken, use <a href=""https://www.dropbox.com/s/76hibe24hmpz3bk/df.pickle?dl=0"" rel=""nofollow noreferrer"">this link</a></p>

<pre><code>from sklearn.model_selection import train_test_split
from urllib.request import urlopen
from os.path import exists
from os import mkdir
import tensorflow as tf
import pandas as pd
import pickle

# Define dataframe path
df_path = 'df.pickle'

# Check if local dataframe exists
if not exists(df_path):
  # Download binary from dropbox
  content = urlopen('https://ucd92a22d5e0d4d29b8edb608305.dl.dropboxusercontent.com/cd/0/get/Askx_25n3JI-jmnZsWXmMmRgd4O2EH1w9l0U6zCMq7xdSXs_IN_i2zuUviseqa9N7-WrReFbGhQi8CeseV5cNsFTO8dzRmSdxjr-MWEDQNpPaZ8Ik29E_58YAjY57qTc4CA/file#').read()

  # Write to file
  with open(df_path, 'wb') as file: file.write(content)

  # Load the dataframe from bytes
  df = pickle.loads(content)
# If the file exists (aka. downloaded)
else:
  # Load the dataframe from file
  df = pickle.load(open(df_path, 'rb'))

# Normalize the category
df['Category_Code'] = df['Category_Code'].apply(lambda x: x / 41)

train_df, test_df = [pd.DataFrame() for _ in range(2)]

x_train, x_test, y_train, y_test = train_test_split(df['Content_Parsed'], df['Category_Code'], test_size=0.15, random_state=8)
train_df['Content_Parsed'], train_df['Category_Code'] = x_train, y_train
test_df['Content_Parsed'], test_df['Category_Code'] = x_test, y_test

# Variable containing the number of words we want to keep in our vocabulary
NUM_WORDS = 10000
# Input/Token length
SEQ_LEN = 300

# Create tokenizer for our data
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS, oov_token='&lt;UNK&gt;')
tokenizer.fit_on_texts(train_df['Content_Parsed'])

# Convert text data to numerical indexes
train_seqs=tokenizer.texts_to_sequences(train_df['Content_Parsed'])
test_seqs=tokenizer.texts_to_sequences(test_df['Content_Parsed'])

# Pad data up to SEQ_LEN (note that we truncate if there are more than SEQ_LEN tokens)
train_seqs=tf.keras.preprocessing.sequence.pad_sequences(train_seqs, maxlen=SEQ_LEN, padding=""post"")
test_seqs=tf.keras.preprocessing.sequence.pad_sequences(test_seqs, maxlen=SEQ_LEN, padding=""post"")

# Create Models folder if not exists
if not exists('Models'): mkdir('Models')

# Define local model path
model_path = 'Models/model.pickle'

# Check if model exists/pre-trained
if not exists(model_path):
  # Define word embedding size
  EMBEDDING_SIZE = 16

  # Create new model
  '''
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_SIZE),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_SIZE)),
    # tf.keras.layers.Dense(EMBEDDING_SIZE, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  '''
  model = tf.keras.Sequential([
      tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_SIZE),
      # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_SIZE)),
      tf.keras.layers.GlobalAveragePooling1D(),
      tf.keras.layers.Dense(EMBEDDING_SIZE, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])

  # Compile the model
  model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
  )

  # Stop training when a monitored quantity has stopped improving.
  es = tf.keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=1)

  # Define batch size (Can be tuned to improve model accuracy)
  BATCH_SIZE = 16
  # Define number or cycle to train
  EPOCHS = 20

  # Using GPU (If error means you don't have GPU. Use CPU instead)
  with tf.device('/GPU:0'):
    # Train/Fit the model
    history = model.fit(
      train_seqs, 
      train_df['Category_Code'].values, 
      batch_size=BATCH_SIZE, 
      epochs=EPOCHS, 
      validation_split=0.2,
      validation_steps=30,
      callbacks=[es]
    )

  # Evaluate the model
  model.evaluate(test_seqs, test_df['Category_Code'].values)

  # Save the model into a file
  with open(model_path, 'wb') as file: file.write(pickle.dumps(model))
else:
  # Load the model
  model = pickle.load(open(model_path, 'rb'))

# Check the model
model.summary()
</code></pre>
","python, pandas, tensorflow, machine-learning, text-classification","<p>After 2 days of tweaking and understanding more examples I found <a href=""https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"" rel=""nofollow noreferrer"">this</a> website which explains quite well about the multi-class classification.</p>
<p>The details of changes I made are as follows:</p>
<ol>
<li><p>Since I'm going to build a model for <strong>multiple classes</strong>, during the model compilation the model should use <code>categorical_crossentropy</code> as it's <strong>loss function</strong> instead of <code>binary_crossentropy</code>.</p>
</li>
<li><p>The model should produce number of output with similar length as your <strong>total class</strong> you're going to classify which in my case <strong>41</strong>. (One hot encoding)</p>
</li>
<li><p>The last layer's activation function should be <code>&quot;softmax&quot;</code> since we're choosing a label with the highest confidence level (closest to <code>1.0</code>).</p>
</li>
<li><p>You will need to tweak the layers accordingly based on the number of classes you're going to classify. See <a href=""https://stats.stackexchange.com/questions/314510/keras-text-classification-overfitting-and-how-to-improve-my-model"">here</a> on how to improve your model.</p>
</li>
</ol>
<p>My final code would look something just like this</p>
<pre><code>from sklearn.model_selection import train_test_split
from urllib.request import urlopen
from functools import reduce
from os.path import exists
from os import listdir
from sys import exit
import tensorflow as tf
import pandas as pd
import pickle
import re

# Specify dataframe path
df_path = 'df.pickle'
# Check if the file exists
if not exists(df_path):
  # Specify url of the dataframe binary
  url = 'https://www.dropbox.com/s/76hibe24hmpz3bk/df.pickle?dl=1'
  # Read the byte content from url
  content = urlopen(url).read()
  # Write to a file to save up time
  with open(df_path, 'wb') as file: file.write(pickle.dumps(content))
  # Unpickle the dataframe
  df = pickle.loads(content)
else:
  # Load the pickle dataframe
  df = pickle.load(open(df_path, 'rb'))

# Useful variables
MAX_NUM_WORDS = 50000                        # Vocabulary size for our tokenizer
MAX_SEQ_LENGTH = 600                         # Maximum length of tokens (for padding later)
EMBEDDING_SIZE = 256                         # Embedding size (Tweak to improve accuracy)
OUTPUT_LENGTH = len(df['Category'].unique()) # Number of class to be classified

# Create our tokenizer
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS, lower=True)
# Fit our tokenizer with words/tokens
tokenizer.fit_on_texts(df['Content_Parsed'].values)
# Get our token vocabulary
word_index = tokenizer.word_index
print('Found {} unique tokens'.format(len(word_index)))

# Parse our text into sequence of numbers using our tokenizer
X = tokenizer.texts_to_sequences(df['Content_Parsed'].values)
# Pad the sequence up to the MAX_SEQ_LENGTH
X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_SEQ_LENGTH)
print('Shape of feature tensor: {}'.format(X.shape))

# Convert our labels into dummy variable (More info on the link provided above)
Y = pd.get_dummies(df['Category']).values
print('Shape of label tensor: {}'.format(Y.shape))

# Split our features and labels into test and train dataset
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

# Creating our model
model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(MAX_NUM_WORDS, EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH))
model.add(tf.keras.layers.SpatialDropout1D(0.2))
# The number 64 could be changed based on your model performance
model.add(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))
# Our output layer with length similar to the OUTPUT_LENGTH
model.add(tf.keras.layers.Dense(OUTPUT_LENGTH, activation='softmax'))
# Compile our model with &quot;categorical_crossentropy&quot; loss function
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Model variables
EPOCHS = 100                          # Number of cycle to run (The early stopping may stop the training process accordingly)
BATCH_SIZE = 64                       # Batch size (Tweaking this may improve model performance a bit)
checkpoint_path = 'model_checkpoints' # Checkpoint path of our model

# Use GPU if available
with tf.device('/GPU:0'):
  # Fit/Train our model
  history = model.fit(
    x_train, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=0.1,
    callbacks=[
      tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001),
      tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path, 
        monitor='val_acc', 
        save_best_only=True, 
        save_weights_only=False
      )
    ],
    verbose=1
  )
</code></pre>
<p>Now, my model accuracies perform well and are increasing each epoch but since the validation accuracies (<code>val_acc</code> around <code>76</code>~<code>77</code> percent) are not performing well, I may need to tweak the model/layers a bit.</p>
<p>The output snapshot is provided below</p>
<p><img src=""https://i.sstatic.net/RUvdw.png"" alt=""Output snapshot.png"" /></p>
",0,1,129,2019-11-18 04:39:27,https://stackoverflow.com/questions/58908050/my-text-classifier-model-doenst-improve-with-multiple-classes
"Keras Prediction result (getting score,use of argmax)","<p>I am trying to use the elmo model for text classification for my own dataset. The training is completed and the number of classes is 4(used keras model and elmo embedding).In the prediction, I got a numpy array. I am attaching the sample code and the result below...</p>

<pre><code>import tensorflow as tf
import keras.backend as K
new_text_pr = np.array(data, dtype=object)[:, np.newaxis]
with tf.Session() as session:
    K.set_session(session)
    session.run(tf.global_variables_initializer())
    session.run(tf.tables_initializer())
    model_elmo = build_model(classes)
    model_elmo.load_weights(model+""/""+elmo_model)
    import time
    t = time.time()
    predicted = model_elmo.predict(new_text_pr)
    print(""time: "", time.time() - t)
    print(predicted)
    # print(predicted[0][0])
    print(""result:"",np.argmax(predicted[0]))
    return np.argmax(predicted[0])
</code></pre>

<p>when I print the predicts variable I got this.</p>

<pre><code>time:  1.561854362487793
 [[0.17483692 0.21439584 0.24001297 0.3707543 ]
 [0.15607062 0.24448264 0.4398888  0.15955798]
 [0.06494818 0.3439018  0.42254424 0.16860574]
 [0.08343349 0.37218323 0.32528472 0.2190985 ]
 [0.14868192 0.25948635 0.32722548 0.2646063 ]
 [0.0365712  0.4194748  0.3321385  0.21181548]
 [0.05350104 0.18225929 0.56712115 0.19711846]
 [0.08343349 0.37218323 0.32528472 0.2190985 ]
 [0.09541835 0.19085276 0.41069734 0.30303153]
 [0.03930932 0.40526104 0.45785302 0.09757669]
 [0.06377257 0.33980298 0.32396355 0.27246094]
 [0.09784496 0.2292052  0.44426462 0.22868524]
 [0.06089798 0.31685832 0.47317514 0.14906852]
 [0.03956613 0.46605557 0.3502095  0.14416872]
 [0.10513227 0.26166025 0.36598155 0.26722598]
 [0.15165758 0.22900137 0.50939053 0.10995051]
 [0.06377257 0.33980298 0.32396355 0.27246094]
 [0.11404029 0.21311268 0.46880838 0.2040386 ]
 [0.07556026 0.20502563 0.52019936 0.19921473]
 [0.11096822 0.23295449 0.36192006 0.29415724]
 [0.05018891 0.16656907 0.60114646 0.18209551]
 [0.08880813 0.2893545  0.44374797 0.1780894 ]
 [0.14868192 0.25948635 0.32722548 0.2646063 ]
 [0.09596984 0.18282187 0.5053091  0.2158991 ]
 [0.09428936 0.13995855 0.62395805 0.14179407]
 [0.10513227 0.26166025 0.36598155 0.26722598]
 [0.08244281 0.15743142 0.5462735  0.21385226]
 [0.07199708 0.2446867  0.44568574 0.23763043]
 [0.1339082  0.27288827 0.43478844 0.15841508]
 [0.07354636 0.24499843 0.44873005 0.23272514]
 [0.08880813 0.2893545  0.44374797 0.1780894 ]
 [0.14868192 0.25948635 0.32722548 0.2646063 ]
 [0.08924995 0.36547357 0.40014726 0.14512917]
 [0.05132649 0.28190497 0.5224545  0.14431408]
 [0.06377257 0.33980292 0.32396355 0.27246094]
 [0.04849219 0.36724472 0.39698333 0.1872797 ]
 [0.07206573 0.31368822 0.4667826  0.14746341]
 [0.05948553 0.28048623 0.41831577 0.2417125 ]
 [0.07582933 0.18771031 0.54879296 0.18766735]
 [0.03858965 0.20433436 0.5596278  0.19744818]
 [0.07443814 0.20681688 0.3933627  0.32538226]
 [0.0639974  0.23687115 0.5357675  0.16336392]
 [0.11005415 0.22901568 0.4279426  0.23298755]
 [0.12625505 0.22987585 0.31619486 0.32767424]
 [0.08893713 0.14554602 0.45740074 0.30811617]
 [0.07906891 0.18683094 0.5214609  0.21263924]
 [0.06316617 0.30398315 0.4475617  0.185289  ]
 [0.07060979 0.17987429 0.4829593  0.26655656]
 [0.0720717  0.27058697 0.41439256 0.24294883]
 [0.06377257 0.33980292 0.32396355 0.27246094]
 [0.04745338 0.25831962 0.46751252 0.22671448]
 [0.06624557 0.20708969 0.54820716 0.17845756]]
 result:3
</code></pre>

<p>Anyone have any idea about what is the use of taking the 0th index value only. Considering this as a list of lists 0th index means first list and the argmax returns index the maximum value from the list. Then what is the use of other values in the lists?. Why isn't it considered?. Also is it possible to get the score from this? I hope the question is clear. Is it the correct way or is it wrong?</p>

<p><strong>I have found the issue. just posting it others who met the same problem.</strong> </p>

<p><strong><em>Answer:</em></strong> When predicting with Elmo model, it expects a list of strings. In code, the prediction data were split and the model predicted for each word. That's why I got this huge array. I have used a temporary fix. The data is appended to a list then an empty string is also appended with the list. The model will predict the both list values but I took only the first predicted data. This is not the correct way but I have done this as a quick fix and hoping to find a fix in the future</p>
","keras, deep-learning, nlp, text-classification, elmo","<p>To find the predicted class for each test example, you need to use <code>axis=1</code>. So, in your case the predicted classes will be:</p>

<pre><code>&gt;&gt;&gt; predicted_classes = predicted.argmax(axis=1)
&gt;&gt;&gt; predicted_classes
[3 2 2 1 2 1 2 1 2 2 1 2 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2
 2 2 2 2 2 2 3 2 2 2 2 2 1 2 2]
</code></pre>

<p>Which means that the <strong>first</strong> test example belongs to the <strong>third</strong> class, and the <strong>second</strong> test example belongs to the <strong>second</strong> class and so on. </p>

<p>The previous part answers your question (I think), now let's see what the <code>np.argmax(predicted)</code> does. Using <code>np.argmax()</code> alone without specifying the axis will flatten your <code>predicted</code> matrix and get the argument of the maximum number. </p>

<p>Let's see this simple example to know what I mean:</p>

<pre><code>&gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4)))
&gt;&gt;&gt; x
matrix([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
&gt;&gt;&gt; x.argmax()
11
</code></pre>

<p><code>11</code> is the index of the 11 which is the biggest number in the whole matrix.</p>
",4,1,5557,2019-11-19 07:04:36,https://stackoverflow.com/questions/58928357/keras-prediction-result-getting-score-use-of-argmax
I want to implement a machine learning or deep learning model for text classification (100 classes),"<p>I have a dataset that is similar to the one where we have movie plots and their genres. The number of classes is around 100. What algorithm should I choose for this 100 class classification? The classification is multi-label because 1 movie can have multiple genres
Please recommend anyone from the following. You are free to suggest any other model if you want to.</p>

<pre><code>1.Naive Bayesian
2.Neural networks
3.SVM
4.Random forest
5.k nearest neighbours
</code></pre>

<p>It would be useful if you also give the necessary library in python</p>
","python, machine-learning, text-classification, multilabel-classification","<p>An important step in machine learning engineering consists of properly inspecting the data. Herby you get some insight that determines what algorithm to choose. Sometimes, you might try out more than one algorithm and compare the models, in order to be sure, that you tried your best on the data.</p>

<p>Since you did not disclose your data, I can only give you the following advice: If your data is ""easy"", meaning that you need only little features and a slight combination of them to solve the task, use Naive Bayes or k-nearest neighbors. If your data is ""medium"" hard, then use Random Forest or SVM. If solving the task requires a very complicated decision boundary combining many dimensions of the features in a non-linear fashion, choose a Neural Network architecture.</p>

<p>I suggest you use python and the scikit-learn package for SVM or Random forest or k-NN.
For Neural Networks, use keras.</p>

<p>I am sorry that I can not give you THE recipe you might expect for solving your problem. Your question is posed really broad.</p>
",1,-1,185,2019-11-22 09:03:40,https://stackoverflow.com/questions/58990776/i-want-to-implement-a-machine-learning-or-deep-learning-model-for-text-classific
Identifying Grammatically Correct Nonsense Sentences,"<p>I have two files <code>file1.csv</code> and <code>file2.csv</code>. <code>file1.csv</code> contains a <code>stupid</code> sentence in each row. <code>file2.csv</code> identify which column it is (<code>type0</code> corresponding to <code>0</code>, <code>type1</code> corresponding to <code>1</code>). I want to do a NLP classification task and I know usually how to do it. But in this situation I am bit confused and do not know how to arrange and organize my dataset, so that I can train my sentences and labels. Appreciate if someone give me a hint to progress. </p>

<p><code>file1.csv</code> in the following format,</p>

<pre><code>id,type0,type1
0,He married to a dinosaur.,He married to a women.
1,She drinks a beer.,She drinks a banana.
2,He lifted a 500 tons.,He lifted a 50kg.
</code></pre>

<p><code>file2.csv</code> in the following format.</p>

<pre><code>id,stupid
0,0
1,1
2,0
</code></pre>

<p>My purpose is to classify the stupid sentences. </p>
","python, python-3.x, nlp, text-classification","<p>Assuming that, 100% of the time, there will be a sentence that is semantically correct, and another that isn't, you can just split the <code>type0</code> and <code>type1</code> sentences into 2 different examples and classify them individually, e.g.:</p>

<pre><code>id,type0,type1
0,He married to a dinosaur.,He married to a women.
1,She drinks a beer.,She drinks a banana.
2,He lifted a 500 tons.,He lifted a 50kg.
</code></pre>

<p>Becomes:</p>

<pre><code>id,sentence
0,He married to a dinosaur
1,He married to a women.
2,She drinks a beer.
3,She drinks a banana.
4,He lifted a 500 tons.
5,He lifted a 50kg.
</code></pre>

<p>However, this won't work if your data contains records where a sentence is slightly less stupid than the other, i.e. there's the actual need to compare both sentences.</p>
",1,1,833,2019-11-22 20:07:54,https://stackoverflow.com/questions/59001065/identifying-grammatically-correct-nonsense-sentences
How can I know to which class each score corresponds to in LibShortText prediction output file?,"<p>I use <a href=""https://www.csie.ntu.edu.tw/~cjlin/libshorttext/"" rel=""nofollow noreferrer"">LibShortText</a> for short-text classification.</p>

<p>I trained a model and use it to get class predictions on my test set by running:</p>

<pre><code>python text-train.py -L 0 -f ./demo/train_file
python text-predict.py ./demo/train_file train_file.model output
</code></pre>

<p>The <code>output</code> file contains the score of each class for each test sample. She is the beginning of the <code>output</code> file:</p>

<pre><code>version: 1
analyzable: 1
text-src: ./demo/train_file
extra-files:    
model-id: 22d9e6defd38ed92e45662d576262915d10c3374

Tickets Tickets 1.045974012515694   -0.1533289000025808 -0.142460215262256  -0.1530588765291932 -0.1249182478102407 -0.1190708362082807 -0.06841237067728836    0.04587568197139553 -0.2283616562229066 -0.102238591774343
Stamps  Stamps  -0.1187719176481736 1.118188003417143   -0.08034439513604429    -0.1973997029054026 -0.06355109135595602    -0.1786639939826796 -0.1169254102259164 -0.01967861752032143    -0.06964465109882922    -0.2732082235438185
Music   Music   -0.1315596826953709 -0.2641082947449856 1.008713836384851   -0.04068831625284784    -0.1545790157496564 -0.1010212095804389 -0.02069378431571431    -0.02404317930606417    0.008960552873498827    -0.2809809066132714
Jewelry &amp; Watches   Jewelry &amp; Watches   -0.0749032450936907 -0.1369122108940684 -0.2159355702219642 0.9582440549577076  -0.141187218792264  -0.1290355317490395 -0.04287756450848382    -0.0919782002284954 -0.04312539181047169    -0.0822891216592294
Tickets Tickets 0.9291396425612148  -0.1597595507175184 -0.07086077554348413    -0.07087036006347401    -0.1111802245732816 -0.2329161314957608 -0.07080154336497513    -0.07093153970747144    -0.07096098431125453    -0.07085853278399512
Books   Books   -0.03482279197164031    -0.02622229736755784    -0.08576360644172253    -0.1209545478269265 0.9735039690597804  -0.02640896142537765    -0.1511226188239169 -0.1785299152500055 -0.1569282110333412 -0.1927510189192921
Tickets Tickets 1.165624491239117   -0.1643444003616841 -0.279795018266336  -0.05911033737681937    -0.1496733471948844 -0.1774767469424229 -0.1806900189575362 -0.05711408596057094    0.06427848575613292 -0.1616990219349959
Art Art -0.07563152438778584    -0.1926345255861422 -0.1379519287608234 -0.1728869014895525 -0.2081235484009353 0.9764371359082827  -0.06097998223834129    -0.06082239643658216    -0.0434090642865785 -0.0239972643215402
Art Art -0.21374038053991   0.0146962630542977  -0.02279914632208601    -0.001108284295731699   -0.2621058759589903 1.016592310148241   0.01436347343617804 -0.04476369315079338    -0.1246095742882179 -0.3765250920829869
Books   Books   -0.08063364674726788    -0.08053738921453879    -0.08032365427931695    -0.1496633152184083 0.9195583554164264  -0.08011940998873018    -0.08053175336913043    -0.16302082274963   -0.1105339242133948 -0.09419443963601073
</code></pre>

<p>How can I know to which class each score corresponds to? </p>

<p>I know I could infer it by looking at the predicted class and the maximum score for several test samples, but I'm hoping there exist some mmore direct way.</p>
","python, nlp, text-classification, libshorttext","<p>The <code>labels</code> member of the <code>PredictionResult</code> returned from <code>predict_text()</code> contains the ordering. So a small addition to <code>classifier_impl.py</code> will expose as column headers in the output file:</p>

<pre><code>*** libshorttext-1.1/libshorttext/classifier/classifier_impl.py.orig
--- libshorttext-1.1/libshorttext/classifier/classifier_impl.py
***************
*** 113,118 ****
--- 113,125 ----

        fmt = '\t{{0:{0}}}'.format(fmt)
        for i in range(len(self.predicted_y)):
+                         if i == 0:
+                                 label_text  = 'Predicted' * 18
+                                 label_text += 'True class' * 18
+                                 for l in self.labels:
+                                         label_text += "" {0: &lt;18}"".format(l)
+                                 fout.write(label_text + ""\n"")
+ 
            fout.write(""{py}\t{y}"".format(py = self.predicted_y[i], y = self.true_y[i]))
            for v in self.decvals[i]:
                fout.write(fmt.format(v))
</code></pre>
",1,0,58,2019-11-24 23:06:52,https://stackoverflow.com/questions/59023318/how-can-i-know-to-which-class-each-score-corresponds-to-in-libshorttext-predicti
Doc2Vec infer_vector not working as expected,"<p>The program should be returning the second text in the list for most similar, as it is same word to word. But its not the case here.</p>

<pre><code>import gensim
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument


data = [""I love machine learning. Its awesome."",
        ""I love coding in python"",
        ""I love building chatbots"",
        ""they chat amagingly well""]


tagged_data=[TaggedDocument(word_tokenize(_d.lower()),tags=[str(i)]) for i,_d in enumerate(data)]

max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                negative=0,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    #print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(""d2v.model"")


loaded_model=Doc2Vec.load(""d2v.model"")
test_data=[""I love coding in python"".lower()]

v1=loaded_model.infer_vector(test_data)

similar_doc=loaded_model.docvecs.most_similar([v1])
print similar_doc
</code></pre>

<p>Output:</p>

<pre><code>[('0', 0.17585766315460205), ('2', 0.055697083473205566), ('3', -0.02361609786748886), ('1', -0.2507985532283783)]
</code></pre>

<p>Its showing the first text in the list as most similar instead of the second text. Can you please help with this ?</p>
","python, text-classification, doc2vec","<p>First, you won't get good results from <code>Doc2Vec</code>-style models with toy-sized datasets. Just four documents, and a vocabulary of about 20 unique words, can't create a meaningfully-contrasting ""dense embedding"" vector model full of 20-dimensional vectors. </p>

<p>Second, if you set <code>negative=0</code> in your model initialization, you're disabling the default model-training-correction mode (<code>negative=5</code>) – and you're not enabling the non-default, less-recommended alternative (<code>hs=1</code>). No training at all will be occurring. There may also be an error shown in the code output – but also, if you're running with at least <code>INFO</code>-level logging, you might notice other issues in the output.</p>

<p>Third, <code>infer_vector()</code> requires a list-of-word-tokens as its argument. You're providing a plain string. That will look like a list of one-character words to the code, so it's like you're asking it to infer on the 23-word sentence:</p>

<pre><code>['i', ' ', 'l', 'o', 'v', 'e', ' ', 'c', ...]
</code></pre>

<p>The argument to <code>infer_vector()</code> should be tokenized exactly the same as the training texts were tokenized. (If you used <code>word_tokenize()</code> during training, use it during inference, too.)</p>

<p><code>infer_vector()</code> will also use a number of repeated inference-passes over the text that's equal to the 'epochs' value inside the <code>Doc2Vec</code> model, unless you specify another value. Since you didn't specify an <code>epochs</code>, the model will still have its default value (inherited from <code>Word2Vec</code>) of <code>epochs=5</code>. Most <code>Doc2Vec</code> work uses 10-20 epochs during training, and using at least as many during inference seems a good practice. </p>

<p>But also:</p>

<p><strong>Don't try to call <code>train()</code> more than once in a loop, or manage <code>alpha</code> in your own code, unless you are an expert.</strong></p>

<p>Whatever online example suggested a code block like your...</p>

<pre class=""lang-py prettyprint-override""><code>for epoch in range(max_epochs):
    #print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha
</code></pre>

<p>...is a <strong>bad</strong> example. It's sending the effective <code>alpha</code> rate down-and-up incorrectly, it's very fragile if you ever want to change the number of <code>epochs</code>, it actually winds up running 500 epochs (100 * model.iter), it's far more code than is necessary. </p>

<p>Instead, don't change default <code>alpha</code> options, and specify your desired number of epochs when the model is created. So, the model will have a meaningful <code>epochs</code> value cached to be used by a later <code>infer_vector()</code>.</p>

<p>Then, <strong>only call train() once</strong>. It will handle all epochs &amp; alpha-management correctly. For example:</p>

<pre class=""lang-py prettyprint-override""><code>model = Doc2Vec(size=vec_size,
                min_count=1,  # not good idea w/ real corpuses but OK
                dm=1,  # not necessary to specify since it's the default but OK  
                epochs=max_epochs)
model.build_vocab(tagged_data)
model.train(tagged_data, 
            total_examples=model.corpus_count, 
            epochs=model.epochs)
</code></pre>
",1,1,1413,2019-11-25 13:17:46,https://stackoverflow.com/questions/59032757/doc2vec-infer-vector-not-working-as-expected
Find how similar a text is - One Class Classifier (NLP),"<p>I have a big dataset containing almost 0.5 billions of tweets. I'm doing some research about how firms are engaged in activism and so far, I have labelled tweets which can be clustered in an activism category according to the presence of certain hashtags within the tweets.</p>

<p>Now, let's suppose firms are tweeting about an activism topic without inserting any hashtag in the tweet. My code won't categorized it and my idea was to run a SVM classifier with only one class.</p>

<p>This lead to the following question:</p>

<ul>
<li>Is this solution data-scientifically feasible?</li>
<li>Does exists any other one-class classifier?</li>
<li>(Most important of all) Are there any other ways to find if a tweet is similar to the ensable of tweets containing activism hashtags?</li>
</ul>

<p>Thanks in advance for your help!</p>
","python, twitter, nlp, classification, text-classification","<p>You have described the setup to a class of problems called ""Positive Unlabelled Learning"", PUL. The name comes from the fact that you have two types of data: positive (""activism"" label) and unlabelled (maybe ""activism"", maybe not). Your idea, to use an SVM, is common, as are random forests. As in all ML problems, neural nets are becoming more common, however.</p>

<p><a href=""https://github.com/t-sakai-kure/pywsl"" rel=""nofollow noreferrer"">pywsl</a> is a ""weak supervision"" library which includes some PUL implementations (PUL is a type of weak supervision). Here is an example of using it on some synthetic data</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.utils.estimator_checks import check_estimator

from pywsl.pul import pumil_mr
from pywsl.utils.syndata import gen_twonorm_pumil
from pywsl.utils.comcalc import bin_clf_err


def main():
    prior = .5
    x, y, x_t, y_t = gen_twonorm_pumil(n_p=30, n_u=200, 
                                       prior_u=prior, n_t=100)
    param_grid = {'prior': [prior], 
                  'lam': np.logspace(-3, 1, 5), 
                  'basis': ['minimax']}
    lambda_list = np.logspace(-3, 1, 5)
    clf = GridSearchCV(estimator=pumil_mr.PUMIL_SL(), 
                       param_grid=param_grid,
                       cv=5, n_jobs=-1)
    clf.fit(x, y)
    y_h = clf.predict(x_t)
    err = 100*bin_clf_err(y_h, y_t, prior)
    print(""MR: {}%"".format(err))


if __name__ == ""__main__"":
    main()
</code></pre>

<p>Also, see this possible duplicate question, <a href=""https://stackoverflow.com/questions/25700724/binary-semi-supervised-classification-with-positive-only-and-unlabeled-data-set"">Binary semi-supervised classification with positive only and unlabeled data set</a></p>
",4,1,706,2019-12-08 17:44:54,https://stackoverflow.com/questions/59238140/find-how-similar-a-text-is-one-class-classifier-nlp
How do I get the sequence of vocabulary from a sparse matrix,"<p>I have a list of vocabularies <code>['Human', 'interface', 'machine', 'binary', 'minors', 'ESP', 'system', 'Graph']</code> and a list of sentences <code>[""Human machine interface for lab abc computer applications"", ""A survey of user opinion of computer system response time"", ""The EPS user interface management system"", ""Relation of user perceived response time to error measurement"", ""The generation of random binary unordered trees"", ""The intersection graph of paths in trees"", ""Graph minors IV Widths of trees and well quasi ordering"", ""Graph minors A survey""]</code>.
I use 'CountVectorizer' from 'sklearn' to fit the sentences into a sparse matrix based on the eight words. And I get a output below.</p>

<pre><code>[[0 0 0 0 0 1 0 1]
 [0 0 0 0 1 0 0 0]
 [0 0 0 0 1 0 0 1]
 [0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0]]
</code></pre>

<p>Now I'm trying to find out the sequence of that eight words in the matrix. Any help will be appreciated.</p>
","python, scikit-learn, text-classification","<p>CountVectorizer use lowercase by default, so 'Human', 'Graph', 'ESP' have no matches. And it seems vocabulary vector is sorted somehow in your result.</p>

<p>You can set lowercase = False.</p>

<blockquote>
  <p>lowercaseboolean, True by default Convert all characters to lowercase
  before tokenizing. <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">sclearn doc</a></p>
</blockquote>

<p>I did like this.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

corpus = [""Human machine interface for lab abc computer applications"", ""A survey of user opinion of computer system response time"", ""The EPS user interface management system"", ""Relation of user perceived response time to error measurement"", ""The generation of random binary unordered trees"", ""The intersection graph of paths in trees"", ""Graph minors IV Widths of trees and well quasi ordering"", ""Graph minors A survey""
]

voc = ['Human', 'interface', 'machine', 'binary', 'minors', 'ESP', 'system', 'Graph']

vectorizer = CountVectorizer(vocabulary=voc, lowercase=False)

X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names())
print(X.toarray())


#     ['Human', 'interface', 'machine', 'binary', 'minors', 'ESP', 'system', 'Graph']
#     [[1 1 1 0 0 0 0 0]
#      [0 0 0 0 0 0 1 0]
#      [0 1 0 0 0 0 1 0]
#      [0 0 0 0 0 0 0 0]
#      [0 0 0 1 0 0 0 0]
#      [0 0 0 0 0 0 0 0]
#      [0 0 0 0 1 0 0 1]
#      [0 0 0 0 1 0 0 1]]
</code></pre>

<p>In matrix, each row is voc matching for a sentence. So this case 'Human', 'interface', 'machine' matched for 1st row(sentence).</p>
",1,0,245,2019-12-30 14:30:45,https://stackoverflow.com/questions/59532458/how-do-i-get-the-sequence-of-vocabulary-from-a-sparse-matrix
"Receiving, &quot;An error was thrown and was not caught: The validation data provided must contain ...&quot; when creating a Text Classifier Model with CreateML","<p>I am using Playground to create a Text Classifier Model using CreateML and keep getting the error:</p>

<pre><code>Playground execution terminated: An error was thrown and was not caught:
▿ The validation data provided must contain class.
  ▿ type : 1 element
    - reason : ""The validation data provided must contain class.""
</code></pre>

<p>My code is relatively simple, using two columns from a data table. The textColumn is labeled ""text"" and the labelColumn is labeled ""class"":</p>

<pre><code>import Cocoa
import CreateML

let data = try MLDataTable(contentsOf: URL(fileURLWithPath: ""/Users/ ... .csv""))
let(trainingData, testingData) = data.randomSplit(by: 0.8, seed: 5)
let sentimentClassifier = try MLTextClassifier(trainingData: trainingData, textColumn: ""text"", labelColumn: ""class"")
let evaluationMetrics = sentimentClassifier.evaluation(on: testingData, textColumn: ""text"", labelColumn: ""class"")
let evaluationAccuracy = (1.0 - evaluationMetrics.classificationError) * 100
</code></pre>

<p>The only difference I can find between this and the code provided in the Apple Developer Documentation is that instead of </p>

<pre><code>let evaluationMetrics = sentimentClassifier.evaluation(on: testingData, textColumn: ""text"", labelColumn: ""class"")
</code></pre>

<p>their documentation is:</p>

<pre><code>let evaluationMetrics = sentimentClassifier.evaluation(on: testingData)
</code></pre>

<p>and version 11.2.1 of Xcode gives me a failure if I try using the line from the Apple Developer Documentation.</p>

<p>Thanks in advance for any help you can offer.</p>
","validation, text-classification, createml","<p>Try This! it works for me</p>
<pre><code>let data = try MLDataTable(contentsOf: URL(fileURLWithPath: &quot;/Users/justinmacbook/Desktop/twitter-sanders-apple3.csv&quot;))

let (trainingData, testingData) = data.randomSplit(by: 0.8, seed: 5 )

let sentimentClassifier = try MLTextClassifier(trainingData: trainingData, textColumn: &quot;class&quot;, labelColumn: &quot;text&quot;)

let evaluationMetrics = sentimentClassifier.evaluation(on: testingData, textColumn: &quot;class&quot;, labelColumn: &quot;text&quot;)

let evaluationAccuracy = (1.0 - evaluationMetrics.classificationError) * 100
</code></pre>
",0,4,203,2020-01-07 22:35:22,https://stackoverflow.com/questions/59637050/receiving-an-error-was-thrown-and-was-not-caught-the-validation-data-provided
scikit-learn: FeatureUnion to include hand crafted features,"<p>I am performing multi-label classification on text data. 
I wish to use combined features of <code>tfidf</code> and custom linguistic features similar to the example <a href=""https://towardsdatascience.com/the-triune-pipeline-for-three-major-transformers-in-nlp-18c14e20530"" rel=""nofollow noreferrer"">here</a> using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html"" rel=""nofollow noreferrer"">FeatureUnion</a>. </p>

<p>I already have generated the custom linguistic features, which are in the form of a dictionary where keys represent the labels and (list of) values represent the features. </p>

<pre><code>custom_features_dict = {'contact':['contact details', 'e-mail'], 
                       'demographic':['gender', 'age', 'birth'],
                       'location':['location', 'geo']}
</code></pre>

<p>Training data structure is as follows:</p>

<pre><code>text                                            contact  demographic  location
---                                              ---      ---          ---
'provide us with your date of birth and e-mail'  1        1            0
'contact details and location will be stored'    1        0            1
'date of birth should be before 2004'            0        1            0
</code></pre>

<p>How can the above <code>dict</code> be incorporated into <code>FeatureUnion</code>? My understanding is that a user-defined function should be called that returns boolean values corresponding to the presence or absence of string values (from <code>custom_features_dict</code>) in the training data. </p>

<p>This gives the following <code>list</code> of <code>dict</code> for the given training data:</p>

<pre><code>[
    {
       'contact':1,
       'demographic':1,
       'location':0
    },
    {
       'contact':1,
       'demographic':0,
       'location':1
    },
    {
       'contact':0,
       'demographic':1,
       'location':0
    },
] 
</code></pre>

<p>How can the above <code>list</code> be used to implement fit and transform?</p>

<p>The code is given below:</p>

<pre><code>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import DictVectorizer
#from sklearn.metrics import accuracy_score
from sklearn.multiclass import OneVsRestClassifier
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from io import StringIO

data = StringIO(u'''text,contact,demographic,location
provide us with your date of birth and e-mail,1,1,0
contact details and location will be stored,0,1,1
date of birth should be before 2004,0,1,0''')

df = pd.read_csv(data)

custom_features_dict = {'contact':['contact details', 'e-mail'], 
                        'demographic':['gender', 'age', 'birth'],
                        'location':['location', 'geo']}

my_features = [
    {
       'contact':1,
       'demographic':1,
       'location':0
    },
    {
       'contact':1,
       'demographic':0,
       'location':1
    },
    {
       'contact':0,
       'demographic':1,
       'location':0
    },
]

bow_pipeline = Pipeline(
    steps=[
        (""tfidf"", TfidfVectorizer(stop_words=stop_words)),
    ]
)

manual_pipeline = Pipeline(
    steps=[
        # This needs to be fixed
        (""custom_features"", my_features),
        (""dict_vect"", DictVectorizer()),
    ]
)

combined_features = FeatureUnion(
    transformer_list=[
        (""bow"", bow_pipeline),
        (""manual"", manual_pipeline),
    ]
)

final_pipeline = Pipeline([
            ('combined_features', combined_features),
            ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),
        ]
)

labels = ['contact', 'demographic', 'location']

for label in labels:
    final_pipeline.fit(df['text'], df[label]) 
</code></pre>
","python, scikit-learn, nlp, text-classification, multilabel-classification","<p>You have to define a Transformer which takes your text as input. Something like that:</p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin

custom_features_dict = {'contact':['contact details', 'e-mail'], 
                   'demographic':['gender', 'age', 'birth'],
                   'location':['location', 'geo']}

#helper function which returns 1, if one of the words occures in the text, else 0
#you can add more words or categories to custom_features_dict if you want
def is_words_present(text, listofwords):
  for word in listofwords:
    if word in text:
      return 1
  return 0

class CustomFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, custom_feature_dict):
       self.custom_feature_dict = custom_feature_dict
    def fit(self, x, y=None):
        return self    
    def transform(self, data):
        result_arr = []
        for text in data:
          arr = []
          for key in self.custom_feature_dict:
            arr.append(is_words_present(text, self.custom_feature_dict[key]))
          result_arr.append(arr)
        return result_arr
</code></pre>

<p>Note: This Transformer generates an array directly looking like this: <code>[1, 0, 1]</code>, it does not generate a dictionary, which allows us to spare the DictVectorizer.</p>

<p>Additionally I changed the way how to process the Multilabel-classification, see <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>#first, i generate a new column in the dataframe, with all the labels per row:
def create_textlabels_array(row):
  arr = []
  for label in ['contact', 'demographic', 'location']:
    if row[label]==1:
      arr.append(label)
  return arr

df['textlabels'] = df.apply(create_textlabels_array, 1) 

#then we generate the binarized Labels:
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer().fit(df['textlabels'])
y = mlb.transform(df['textlabels'])
</code></pre>

<p>Now we can add all together to the pipeline:</p>

<pre><code>bow_pipeline = Pipeline(
    steps=[
        (""tfidf"", TfidfVectorizer(stop_words=stop_words)),
    ]
)

manual_pipeline = Pipeline(
    steps=[
        (""costum_vect"", CustomFeatureTransformer(custom_features_dict)),
    ]
)

combined_features = FeatureUnion(
    transformer_list=[
        (""bow"", bow_pipeline),
        (""manual"", manual_pipeline),
    ]
)

final_pipeline = Pipeline([
        ('combined_features', combined_features),
        ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),
    ]
)

#train your pipeline
final_pipeline.fit(df['text'], y) 

#let's predict something: (Note: of course training data is a bit low in that examplecase here)
pred = final_pipeline.predict([""write an e-mail to our location please""])
print(pred) #output: [0, 1, 1] 

#reverse the predicted array to the actual labels:
print(mlb.inverse_transform(pred)) #output: [('demographic', 'location')]
</code></pre>
",1,1,501,2020-01-10 20:36:20,https://stackoverflow.com/questions/59688670/scikit-learn-featureunion-to-include-hand-crafted-features
Tensorflow: What&#39;s the best practice to get a section of a manual from a question?,"<p>I would like to use Tensorflow to create a smart faq. I've seen how to manage a chatbot, but my need is to let the user searching for help and the result  must be the most probable chapter or section of a manual.</p>

<p>For example the user can ask: </p>

<blockquote>
  <p>""What are the O.S. supported?""</p>
</blockquote>

<p>The reply must be a list of all the possible sections of the manual in which could be the correct answer.
My text record set for the training procedure is only the manual itself. I've followed the text classification example, but i don't think is what i need because in that case it would  only understand if a given text belongs to a category or another one.</p>

<p>What's the best practice to accomplish this task (i use Python)?</p>

<p>Thank you in advance</p>
","python, tensorflow, deep-learning, recurrent-neural-network, text-classification","<p>An idea could be building <a href=""https://en.wikipedia.org/wiki/Word_embedding"" rel=""nofollow noreferrer"">embeddings</a> of your text using <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">Bert</a> or other pretrained models (take a look to <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">transformers</a>) and later compare (for instance using cosine distance) such embeddings with your query (the question) and get the most similar ones interpreting as the section or chapter containing them.</p>
",1,0,97,2020-01-14 10:19:11,https://stackoverflow.com/questions/59731721/tensorflow-whats-the-best-practice-to-get-a-section-of-a-manual-from-a-questio
Snorkel: Can i have different features in data set to for generating labelling function VS training a classifier?,"<p>I have a set of features to build labelling functions (set A)
and another set of features to train a sklearn classifier (set B)</p>

<p>The generative model will output a set of probabilisitic labels which i can use to train my classifier.</p>

<p>Do i need to add in the features (set A) that i used for the labelling functions into my classifier features? (set B)
Or just use the labels generated to train my classifier?</p>

<p>I was referencing the <a href=""https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/01_spam_tutorial.ipynb"" rel=""nofollow noreferrer"">snorkel spam tutorial</a> and i did not see them use the features in the labelling function set to train a new classifier.</p>

<p>As seem in <code>cell 47</code>, featurization is done entirely using a CountVectorizer:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1, 2))
X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())

X_dev = vectorizer.transform(df_dev.text.tolist())
X_valid = vectorizer.transform(df_valid.text.tolist())
X_test = vectorizer.transform(df_test.text.tolist())
</code></pre>

<p>And then straight to fitting a keras model:</p>

<pre><code># Define a vanilla logistic regression model with Keras
keras_model = get_keras_logreg(input_dim=X_train.shape[1])

keras_model.fit(
    x=X_train,
    y=probs_train_filtered,
    validation_data=(X_valid, preds_to_probs(Y_valid, 2)),
    callbacks=[get_keras_early_stopping()],
    epochs=50,
    verbose=0,
)
</code></pre>
","python, machine-learning, text-classification, snorkel","<p>I asked the same question to the snorkel github page and this is the response :</p>

<blockquote>
  <p>you do not need to add in the features (set A) that you used for LFs
  into the classifier features. In order to prevent the end model from
  simply overfitting to the labeling functions, it is better if the
  features for the LFs and end model (set A and set B) are as different
  as possible</p>
</blockquote>

<p><a href=""https://github.com/snorkel-team/snorkel-tutorials/issues/193#issuecomment-576450705"" rel=""nofollow noreferrer"">https://github.com/snorkel-team/snorkel-tutorials/issues/193#issuecomment-576450705</a></p>
",3,1,443,2020-01-20 04:06:59,https://stackoverflow.com/questions/59816879/snorkel-can-i-have-different-features-in-data-set-to-for-generating-labelling-f
How to fine grain neutral sentiment as positive or negative,"<p>I'm working on multimodal sentiment analysis with visual and textual cues.</p>

<p>My input dataset is containing neutral sentiment in ground truth but I require to do a binary classification to categorize  my input samples as either positive/negative</p>

<p>Is there any possibility to use this neutral class in aiding to remove non-opinion key terms thereby increasing the accuracy of binary categorization?</p>

<p>Is it advised only to adopt a multi-class classification algorithm to categorize as positive, negative or neutral?</p>

<p>P.S: My requirement is to do a binary classification</p>

<p>Thanks in advance</p>
","sentiment-analysis, text-classification","<p>If your requirement is to do obligatory binary classification, maybe it is worth to perform a hierarchical analysis in two binary classification steps. First you classify the documents into objectives(neutral) or subjectives(positives and negative). Then, for the subjective ones, you classify into positive or negative. </p>

<p>Otherwise, a better way is to simply work on multi-class classification and classify into three classes. </p>
",0,1,123,2020-01-23 00:30:48,https://stackoverflow.com/questions/59870123/how-to-fine-grain-neutral-sentiment-as-positive-or-negative
How to make a prediction as binary output? - Python (Tensorflow),"<p>I'm learning text classification using movie reviews as data with tensorflow, but I got stuck when I get an output prediction different (not rounded, not binary) to the label.</p>

<p><strong>CODE</strong></p>

<pre><code>predict = model.predict([test_review])

print(""Prediction: "" + str(predict[0])) # [1.8203685e-19] 
print(""Actual: "" + str(test_labels[0])) # 0
</code></pre>

<p>The expected ouput should be:<br /></p>

<blockquote>
  <p>Prediction: [0.]<br />
  Actual: 0</p>
</blockquote>

<p>What the output is giving:<br /></p>

<blockquote>
  <p>Prediction: [1.8203685e-19] <br />
  Actual: 0</p>
</blockquote>

<p><em>The output prediction should be 0 or 1, representing if the review was good or not.</em></p>

<p><strong>FULL CODE</strong></p>

<pre><code>import tensorflow as tf
from tensorflow import keras
import numpy as np

data = keras.datasets.imdb

(train_data, train_labels), (test_data, test_labels) = data.load_data(num_words = 10000)

word_index = data.get_word_index()
word_index = {k:(v + 3) for k, v in word_index.items()} 

word_index['&lt;PAD&gt;'] = 0
word_index['&lt;START&gt;'] = 1 
word_index['&lt;UNK&gt;'] = 2
word_index['&lt;UNUSED&gt;'] = 3

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

train_data = keras.preprocessing.sequence.pad_sequences(train_data, value = word_index['&lt;PAD&gt;'], padding = 'post', maxlen = 256)
test_data = keras.preprocessing.sequence.pad_sequences(test_data, value = word_index['&lt;PAD&gt;'], padding = 'post', maxlen = 256)

def decode_review(text):
    """""" decode the training and testing data into readable words""""""
    return ' '.join([reverse_word_index.get(i, '?') for i in text])

print(""\n"")
print(decode_review(test_data[0]))

model = keras.Sequential()
model.add(keras.layers.Embedding(10000, 16))
model.add(keras.layers.GlobalAveragePooling1D())
model.add(keras.layers.Dense(16, activation = 'relu'))
model.add(keras.layers.Dense(1, activation = 'sigmoid'))
model.summary()

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) 

x_val = train_data[:10000]
x_train = train_data[10000:]

y_val = train_labels[:10000]
y_train = train_labels[10000:]

fitModel = model.fit(x_train, y_train, epochs = 40,
                     batch_size = 512, 
                     validation_data = (x_val, y_val),
                     verbose = 1)

results = model.evaluate(test_data, test_labels)

test_review = test_data[0]
predict = model.predict([test_review])
print(""Review: "")
print(decode_review(test_review))
print(""Prediction: "" + str(predict[0])) # [1.8203685e-19] 
print(""Actual: "" + str(test_labels[0]))
print(""\n[loss, accuracy]: "", results)
</code></pre>
","python, tensorflow, prediction, text-classification","<p>Replace the <code>predict</code> method with <code>predict_classes</code> method:</p>

<pre><code>model.predict_classes([test_review])
</code></pre>
",6,4,5632,2020-01-28 10:22:18,https://stackoverflow.com/questions/59946574/how-to-make-a-prediction-as-binary-output-python-tensorflow
Pytorch dataloader for sentences,"<p>I have collected a small dataset for binary text classification and my goal is to train a model with the method proposed by <a href=""https://arxiv.org/pdf/1408.5882.pdf"" rel=""nofollow noreferrer"">Convolutional Neural Networks for Sentence Classification</a></p>

<p>I started my implementation by using the <code>torch.util.data.Dataset</code>. Essentially every sample in my dataset <code>my_data</code> looks like this (as example):</p>

<pre><code>{""words"":[0,1,2,3,4],""label"":1},
{""words"":[4,9,20,30,4,2,3,4,1],""label"":0}
</code></pre>

<p>Next I took a look at <a href=""https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"" rel=""nofollow noreferrer"">Writing custom dataloaders with pytorch</a>:
using:</p>

<pre><code>dataloader = DataLoader(my_data, batch_size=2,
                    shuffle=False, num_workers=4)
</code></pre>

<p>I would suspect that enumerating over a batch would yield something the following:</p>

<pre><code>{""words"":[[0,1,2,3,4],[4,9,20,30,4,2,3,4,1]],""labels"":[1,0]}
</code></pre>

<p>However it is more like this:</p>

<pre><code>{""words"":[[0,4],[1,9],[2,20],[3,30],[4,4]],""label"":[1,0]}
</code></pre>

<p>I guess it has something to do that they are not equal size.
Do they need to be the same size and if so how can i achieve it? For people knwoing about this paper, what does your training data look like?</p>

<p>edit:</p>

<pre><code>class CustomDataset(Dataset):
def __init__(self, path_to_file, max_size=10, transform=None):

    with open(path_to_file) as f:
        self.data = json.load(f)
    self.transform = transform
    self.vocab = self.build_vocab(self.data)
    self.word2idx, self.idx2word = self.word2index(self.vocab)

def get_vocab(self):
    return self.vocab

def get_word2idx(self):
    return self.word2idx, self.idx2word

def __len__(self):
    return len(self.data)

def __getitem__(self, idx):
    if torch.is_tensor(idx):
        idx = idx.tolist()
    inputs_ = word_tokenize(self.data[idx][0])
    inputs_ = [w for w in inputs_ if w not in stopwords]
    inputs_ = [w for w in inputs_ if w not in punctuation]
    inputs_ = [self.word2idx[w] for w in inputs_]  # convert words to index

    label = {""positive"": 1,""negative"": 0}
    label_ = label[self.data[idx][1]] #convert label to 0|1

    sample = {""words"": inputs_, ""label"": label_}

    if self.transform:
        sample = self.transform(sample)

    return sample

def build_vocab(self, corpus):
    word_count = {}
    for sentence in corpus:
        tokens = word_tokenize(sentence[0])
        for token in tokens:
            if token not in word_count:
                word_count[token] = 1
            else:
                word_count[token] += 1
    return word_count

def word2index(self, word_count):
    word_index = {w: i for i, w in enumerate(word_count)}
    idx_word = {i: w for i, w in enumerate(word_count)}
    return word_index, idx_word
</code></pre>
","python, deep-learning, nlp, pytorch, text-classification","<p>As you correctly suspected, this is mostly a problem of different tensor shapes. Luckily, PyTorch offers you several solutions of varying simplicity to achieve what you desire (batch sizes >= 1 for text samples):</p>

<ul>
<li>The highest-level solution is probably <a href=""https://github.com/pytorch/text"" rel=""nofollow noreferrer""><code>torchtext</code></a>, which provides several solutions out of the box to load (custom) datasets for NLP tasks. If you can make your training data fit in any one of the described loaders, this is probably the recommended option, as there is a decent documentation and several examples.</li>
<li>If you prefer to build a solution, there are padding solutions like <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence"" rel=""nofollow noreferrer""><code>torch.nn.utils.rnn.pad_sequence</code></a>, in combination with <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence"" rel=""nofollow noreferrer""><code>torch.nn.utils.pack_padded_sequence</code></a>, or the combination of both (<a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence"" rel=""nofollow noreferrer"">torch.nn.utils.rnn.pack_sequence</a>. This generally allows you a lot more flexibility, which may or may not be something that you require.</li>
</ul>

<p>Personally, I have had good experiences using just <code>pad_sequence</code>, and sacrifice a bit of speed for a much clearer debugging state, and seemingly <a href=""https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418"" rel=""nofollow noreferrer"">others have similar recommendations</a>.</p>
",2,2,6569,2020-01-29 16:18:22,https://stackoverflow.com/questions/59971324/pytorch-dataloader-for-sentences
Text classification with word2vec stack overflow tag predictor,"<p>I am working stack overflow tag predictor.</p>

<p>I have a dataframe df which contains a feature 'post' and label 'Tags' which can be multi lable.</p>

<p>My df is :</p>

<pre><code>Tags    post

0   [php]   check upload file image without mime type woul...

1   [firefox]   prevent firefox close press ctrl-w favorite ed...

2   [r] r error invalid type list variable import matl...
3   [c#]    replace special character url probably simple ...

4   [php, api]  modify whois contact detail function modify mc...

... ... ...
179995  [delphi]    intraweb isapi module throw unrecognized comma...

179996  [c] opencv argc argv confusion check opencv tutori...

179997  [android]   list data sdcard want display file name reside...

179998  [java, email]   add sort extension imap server mail server sup...

179999  [linux, php]    create carddav ldap server share host via php ...
</code></pre>

<p>So I want to use word2vec for classification and predict the tags.</p>

<p>I want to use all machine learning classifier like SVM,  random forest etc.</p>

<p>I also want classification report of tags.</p>

<p>So please help me.</p>
","python, machine-learning, data-science, text-classification","<p>word2vec is not a classifier it word to vector converter, my suggestion steps
1) Preprocess the text(like stopwords and normalization)
2) convert the words to vector using TF-IDF or word2vec
3) Then apply ml models (for multi classification you can use SVM, Naive Bayes and logistic regression)
4)validate the results  </p>
",1,-1,501,2020-01-31 06:43:58,https://stackoverflow.com/questions/59999113/text-classification-with-word2vec-stack-overflow-tag-predictor
Binary classifier of words in list,"<p>I have extracted text using OCR from a number of stylized documents in Swedish. Now I want formalize the data and extract city names. Due to the OCR working imperfectly, the names are sometimes spelled wrong and shows up at semi-random positions in the text. Therefore regex does not work. </p>

<p>Is it a good procedure/possible to transform the text into bags of words and train an algorithm to binary categorize words as city vs not city? </p>

<p>Consider example data set:</p>

<pre><code>Col1                                          Col2
['Hi', 'there', 'Haag']                      ['Haag']
[‘Paris, ‘is’, ‘better’, ‘than’, ‘Osloe]     [‘Paris’, ‘Oslo’]
['My', 'hometown', 'is', 'New York']         ['New York']
...
</code></pre>

<p>I want to use Col1 as X variable and Col2 as Y variable. Is this a good procedure or should I use another method?</p>
","python, machine-learning, text-classification","<p>One possible way of dealing with spelling mistakes is to first collect your vocabulary (all words in the corpus) and to then select candidate OCR mistakes by their individual term frequencies (assuming that mistakes like <code>""Osloe""</code> are rare). In a second step you could use <a href=""https://en.wikipedia.org/wiki/Edit_distance"" rel=""nofollow noreferrer"">Edit distance</a> to link candidate spelling mistakes to their correct word forms.</p>

<p>To extract cities you could, for example, take a look at <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">spacy</a>'s pretrained models for named entity recognition.</p>

<p>Collocations like <code>""New York""</code> you could identify by contrasting the frequency of the terms to go together in sequence vs the individual term frequencies (depending on your corpus <code>""York""</code> might often go together with <code>""New""</code> much more often than one would assume under a hypothesis of the occurrence of <code>""New""</code> and <code>""York""</code> being independent).</p>
",0,0,73,2020-02-17 10:46:40,https://stackoverflow.com/questions/60260789/binary-classifier-of-words-in-list
Using Naive Bayes to do multi classification,"<p>I have a dataset as below:</p>

<pre><code>data = [[92, 155],
 [56, 186, 117, 210, 224],
 [247, 202, 189, 210, 65, 3, 270, 224],
 [20, 14, 157, 224],
 [17, 89, 158, 224],
 [263, 283, 68, 224],
 [182, 166, 224],
 [176, 37, 100, 224],
 [33, 102, 41, 269, 177, 224],
 [0, 260, 49, 207, 278, 217, 35],
 [119],
 [118],
 [142, 185, 7, 246, 224],
 [104, 22, 101, 224],
 [84, 205, 224],
 [225, 93, 54, 224],
 [98, 32, 78, 224],
 [159, 217, 212, 198, 224],
 [178, 94, 187, 224],
 [211, 149, 193, 149, 66, 139, 67, 28, 106, 224],
 [133, 151],
 [259, 109, 29, 224],
 [215, 241, 73, 255, 77, 144, 224],
 [36, 254, 19, 268, 183, 224],
 [47, 234, 203, 111, 231, 141, 30],
 [127, 275, 220, 161],
 [214, 267, 22, 90, 224],
 [46, 217, 103],
 [17, 89, 128, 224],
 [225, 22, 101, 224],
 [285, 265, 151],
 [215, 206, 264, 43, 224],
 [244, 21, 224],
 [82, 122, 240, 5, 224],
 [259, 136, 162, 194, 224],
 [176, 208, 112, 224],
 [172, 19, 146, 276, 31, 246, 51, 224],
 [45, 10],
 [229, 24, 224],
 [143, 108, 239, 224],
 [225, 282, 83, 224],
 [110, 267, 171],
 [176, 245, 95, 123, 270, 224],
 [248, 195, 139, 261, 173, 281, 232, 80, 18, 224],
 [61, 60, 233],
 [211, 120, 1, 23],
 [225, 267, 249, 224],
 [247, 202, 86, 196, 224],
 [15, 127, 222, 224],
 [247, 202, 186, 226, 145, 224],
 [174, 242, 196, 224],
 [259, 152, 71, 224],
 [235, 44, 230, 224],
 [69, 96, 50, 99, 116],
 [259, 279, 224],
 [228, 70],
 [39, 139, 201, 190, 224],
 [132, 40, 219, 81, 224],
 [159, 221, 224],
 [267, 16, 6, 62],
 [143, 59, 175, 129, 48, 224],
 [280, 140, 224],
 [284, 124, 167, 150, 274],
 [113, 265, 184],
 [179, 4, 257, 145, 224],
 [247, 202, 72, 11, 224],
 [64],
 [192, 125, 105],
 [174, 134, 224],
 [58, 139, 85, 160, 209, 224],
 [130, 169, 137, 256, 224],
 [215, 163, 265, 185, 26],
 [176, 147, 74, 224],
 [0, 266],
 [143, 34, 153, 188, 224],
 [121],
 [243, 75, 135],
 [38, 218, 199, 253, 224],
 [178, 271, 224],
 [154, 164, 180, 27, 270, 224],
 [176, 189, 148, 139, 277, 224],
 [57, 62],
 [91, 168, 251, 224],
 [172, 19, 146, 276, 53, 97, 200, 224],
 [64],
 [8, 237, 224],
 [138, 107, 224],
 [176, 238, 224],
 [204, 217, 63, 165, 224],
 [215, 216, 272, 62, 170, 2, 55, 224],
 [247, 273, 202, 223, 9, 148, 224],
 [258, 267, 181, 224],
 [262, 76, 126, 12, 224],
 [36, 254, 19, 268, 250, 213, 48, 224],
 [227, 42],
 [79, 197, 52, 87, 224],
 [143, 131, 224],
 [156, 88, 115, 236, 224],
 [259, 13, 252, 224],
 [114, 25, 191, 224]]

target = [9,
 31,
 20,
 9,
 3,
 26,
 16,
 11,
 28,
 0,
 9,
 9,
 9,
 9,
 7,
 1,
 33,
 9,
 13,
 15,
 9,
 21,
 9,
 34,
 9,
 9,
 9,
 9,
 3,
 1,
 9,
 27,
 14,
 22,
 21,
 11,
 17,
 9,
 6,
 8,
 1,
 9,
 11,
 9,
 9,
 9,
 1,
 20,
 29,
 20,
 23,
 21,
 9,
 9,
 21,
 9,
 18,
 9,
 9,
 30,
 8,
 9,
 9,
 9,
 9,
 20,
 9,
 32,
 23,
 9,
 24,
 9,
 11,
 9,
 8,
 9,
 9,
 9,
 13,
 10,
 11,
 9,
 12,
 17,
 9,
 5,
 9,
 11,
 9,
 2,
 20,
 9,
 25,
 34,
 9,
 9,
 8,
 4,
 21,
 19]
</code></pre>

<p>I wish to do classification by using naive bayes or any other BEST algorithms available. However, I received error when using naive bayes as below:</p>

<pre><code>from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB(class_prior=[.25,.75])
mnb.fit(data, target)
</code></pre>

<p>Errros:</p>

<pre><code>ValueError: Expected 2D array, got 1D array instead:
array=[list([92, 155]) list([56, 186, 117, 210, 224])
 list([247, 202, 189, 210, 65, 3, 270, 224]) list([20, 14, 157, 224])
 list([17, 89, 158, 224]) list([263, 283, 68, 224]) list([182, 166, 224])
 list([176, 37, 100, 224]) list([33, 102, 41, 269, 177, 224])
 list([0, 260, 49, 207, 278, 217, 35]) list([119]) list([118])
 list([142, 185, 7, 246, 224]) list([104, 22, 101, 224])
 list([84, 205, 224]) list([225, 93, 54, 224]) list([98, 32, 78, 224])
 list([159, 217, 212, 198, 224]) list([178, 94, 187, 224])
 list([211, 149, 193, 149, 66, 139, 67, 28, 106, 224]) list([133, 151])
 list([259, 109, 29, 224]) list([215, 241, 73, 255, 77, 144, 224])
 list([36, 254, 19, 268, 183, 224])
 list([47, 234, 203, 111, 231, 141, 30]) list([127, 275, 220, 161])
 list([214, 267, 22, 90, 224]) list([46, 217, 103])
 list([17, 89, 128, 224]) list([225, 22, 101, 224]) list([285, 265, 151])
 list([215, 206, 264, 43, 224]) list([244, 21, 224])
 list([82, 122, 240, 5, 224]) list([259, 136, 162, 194, 224])
 list([176, 208, 112, 224]) list([172, 19, 146, 276, 31, 246, 51, 224])
 list([45, 10]) list([229, 24, 224]) list([143, 108, 239, 224])
 list([225, 282, 83, 224]) list([110, 267, 171])
 list([176, 245, 95, 123, 270, 224])
 list([248, 195, 139, 261, 173, 281, 232, 80, 18, 224])
 list([61, 60, 233]) list([211, 120, 1, 23]) list([225, 267, 249, 224])
 list([247, 202, 86, 196, 224]) list([15, 127, 222, 224])
 list([247, 202, 186, 226, 145, 224]) list([174, 242, 196, 224])
 list([259, 152, 71, 224]) list([235, 44, 230, 224])
 list([69, 96, 50, 99, 116]) list([259, 279, 224]) list([228, 70])
 list([39, 139, 201, 190, 224]) list([132, 40, 219, 81, 224])
 list([159, 221, 224]) list([267, 16, 6, 62])
 list([143, 59, 175, 129, 48, 224]) list([280, 140, 224])
 list([284, 124, 167, 150, 274]) list([113, 265, 184])
 list([179, 4, 257, 145, 224]) list([247, 202, 72, 11, 224]) list([64])
 list([192, 125, 105]) list([174, 134, 224])
 list([58, 139, 85, 160, 209, 224]) list([130, 169, 137, 256, 224])
 list([215, 163, 265, 185, 26]) list([176, 147, 74, 224]) list([0, 266])
 list([143, 34, 153, 188, 224]) list([121]) list([243, 75, 135])
 list([38, 218, 199, 253, 224]) list([178, 271, 224])
 list([154, 164, 180, 27, 270, 224]) list([176, 189, 148, 139, 277, 224])
 list([57, 62]) list([91, 168, 251, 224])
 list([172, 19, 146, 276, 53, 97, 200, 224]) list([64])
 list([8, 237, 224]) list([138, 107, 224]) list([176, 238, 224])
 list([204, 217, 63, 165, 224]) list([215, 216, 272, 62, 170, 2, 55, 224])
 list([247, 273, 202, 223, 9, 148, 224]) list([258, 267, 181, 224])
 list([262, 76, 126, 12, 224]) list([36, 254, 19, 268, 250, 213, 48, 224])
 list([227, 42]) list([79, 197, 52, 87, 224]) list([143, 131, 224])
 list([156, 88, 115, 236, 224]) list([259, 13, 252, 224])
 list([114, 25, 191, 224])].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>

<p>Please can anyone please help me with this? Or can someone show me some other best examples of algorithms to use for machine learning like decision tree, svm or anything.</p>
","python, text-classification, naivebayes","<p>So the direct answer to why you're getting an error is that you're passing an array of lists as an argument. Sklearn thus think you're passing in a 1D array of lists. It is not possible to transform your <code>data</code> to a 2D matrix because the number of values in your list in inconsistent. </p>

<p>From my understanding (which could be wrong), each row of your input feature matrix need to have the same amount of numbers. Given this is satisfied, then you should be able to feed your data into <code>MultinomialNB</code> no problem.</p>

<p>Consider padding with zeros:</p>

<pre class=""lang-py prettyprint-override""><code>data1 = np.zeros((len(data), 10))
for i in range(len(data)):
    data1[i, :len(data[i])] = data[i] 
</code></pre>
",1,0,93,2020-02-18 04:07:14,https://stackoverflow.com/questions/60273609/using-naive-bayes-to-do-multi-classification
How to Separate Dataframe Based on the Label value?,"<p>Let say I have this dataframe. </p>

<pre><code>+-----------------+-----------+
|     COMMENT     | SENTIMENT |
+-----------------+-----------+
| Good app        | Positive  |
| Bad app         | Negative  |
| Useless feature | Negative  |
| I like this app | Positive  |
+-----------------+-----------+
</code></pre>

<p>I want to split it based on the SENTIMENT column. Like this below.</p>

<pre><code>+-----------------+-----------+
|     COMMENT     | SENTIMENT |
+-----------------+-----------+
| Good app        | Positive  |
| I like this app | Positive  |
+-----------------+-----------+


+-----------------+-----------+
|     COMMENT     | SENTIMENT |
+-----------------+-----------+
| Bad app         | Negative  |
| Useless feature | Negative  |
+-----------------+-----------+

</code></pre>

<p>Anyone knows the solution in Python (Jupyter) for that case? Your help will help my thesis project. Thank you :D</p>
","python, machine-learning, jupyter-notebook, sentiment-analysis, text-classification","<p>Simply you can use <code>df1 = df[df[""SENTIMENT""]==""Positive""]</code></p>

<p>Then <code>df1</code> will have:</p>

<p><a href=""https://i.sstatic.net/vg1Ta.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vg1Ta.png"" alt=""enter image description here""></a></p>

<p><code>df2 = df[df[""SENTIMENT""]==""Negative""]</code></p>

<p>Then <code>df2</code> will have:</p>

<p><a href=""https://i.sstatic.net/s4Snz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/s4Snz.png"" alt=""enter image description here""></a></p>
",1,0,1745,2020-02-27 06:22:39,https://stackoverflow.com/questions/60427333/how-to-separate-dataframe-based-on-the-label-value
One class SVM model for text classification (scikit-learn),"<p>I am attempting to classify a train set of texts to be used for predicting similar texts in the test set of texts. I am using the one_class_svm model. 'author_corpus' contains a list of texts written by a single author and 'test_corpus' contains a list of texts written by both other authors and the original author. I am attempting to use one_class_svm to identify the author in the test texts.</p>

<pre><code>def analyse_corpus(author_corpus, test_corpus):

    vectorizer = TfidfVectorizer()

    author_vectors = vectorizer.fit_transform(author_corpus)
    test_vectors = vectorizer.fit_transform(test_corpus)

    model = OneClassSVM(gamma='auto')

    model.fit(author_vectors)

    test = model.predict(test_vectors)
</code></pre>

<p>I am getting the value error:</p>

<pre><code>X.shape[1] = 2484 should be equal to 1478, the number of features at training time
</code></pre>

<p>How might I implement this model to accurately predict authorship of texts in the test set given the single author in the train set? Any help is appreciated. </p>

<p>For reference, here is a link to the one_class_svm model guide: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM"" rel=""noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM</a></p>
","python-3.x, machine-learning, scikit-learn, text-classification, one-class-classification","<p>You should <code>fit</code> (train) the model on the <code>train</code> data and make the predictions using the trained model on the <code>test</code> data. </p>

<ul>
<li><code>fit</code>: fit (trains) the model   </li>
<li><code>fit_transform</code>: fits the model and then makes the predictions</li>
<li><code>transform</code> : Makes the predicitons</li>
</ul>

<p>The mistake you are doing is </p>

<blockquote>
  <p>test_vectors = vectorizer.fit_transform(test_corpus)</p>
</blockquote>

<p>Sample usage</p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer

train = fetch_20newsgroups(subset='train', categories=['alt.atheism'], shuffle=True, random_state=42).data
test =  fetch_20newsgroups(subset='train', categories=['alt.atheism', 'soc.religion.christian'], shuffle=True, random_state=42).data

vectorizer = TfidfVectorizer()
train_vectors = vectorizer.fit_transform(train)
test_vectors = vectorizer.transform(test)

model = OneClassSVM(gamma='auto')
model.fit(train_vectors)

test_predictions = model.predict(test_vectors)
</code></pre>
",5,5,2172,2020-02-29 06:00:26,https://stackoverflow.com/questions/60462444/one-class-svm-model-for-text-classification-scikit-learn
Fine tuning CNN hyperparameters for complex text classification,"<p>I'm working on a <strong>CNN model</strong> for complex text classification (mainly emails and messages). The dataset contains around 100k entries distributed on 10 different classes. My actual Keras sequential model has the following structure:</p>

<pre><code>model = Sequential(
        [
            Embedding(
                input_dim=10000,
                output_dim=150,
                input_length=400),
            Convolution1D(
                filters=128,
                kernel_size=4,
                padding='same',
                activation='relu'),
                BatchNormalization(),
            MaxPooling1D(),
            Flatten(),
            Dropout(0.4),
            Dense(
                100,
                activation='relu'),
            Dropout(0.4),
            Dense(
                len(y_train[0]),
                activation='softmax')])
</code></pre>

<p>In compiling the model I'm using the <strong>Nadam optimizer</strong>, <strong>categorical_crossentropy</strong> loss with <strong>LabelSmoothing</strong> set to 0.2 . </p>

<p>In a model fit, I'm using <strong>30 Epochs</strong> and <strong>Batch Size</strong> set to <strong>512</strong>. I also use <strong>EarlyStopping</strong> to monitor val_loss and patience set to 8 epochs. The test size is set to 25% of the dataset.</p>

<p>Actually the training stops after 16/18 epochs with values that start to fluctuate a little after 6/7 epoch and then go on till being stopped by EarlyStopping. The values are like these on average:</p>

<p><em>loss: 1.1673 - accuracy: 0.9674 - val_loss: 1.2464 - val_accuracy: 0.8964</em></p>

<p>with a testing accuracy reaching:</p>

<p><em>loss: 1.2461 - accuracy: 0.8951</em></p>

<p>Now I'd like to improve the accuracy of my CNN, I've tried different hyperparameters but as for now, I wasn't able to get a higher value. Therefore I'm trying to figure out:</p>

<ol>
<li>if there is still room for improvements (I bet so)</li>
<li>if the solution is in a fine-tuning of my hyperparameters and, if so, which ones should I change?</li>
<li>if going deeper by adding layers to the model could be of any use and, if so, how to improve my model</li>
<li>is there any other deep-learning/Neural networks approach rather than CNN that could lead to a better result?</li>
</ol>

<p>Thank you very much to anybody who will help! :)</p>
","python, keras, deep-learning, text-classification, conv-neural-network","<p>There are many libraries, but I find this one very flexible. <a href=""https://github.com/keras-team/keras-tuner"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras-tuner</a></p>

<p><strong>Just install with pip.</strong></p>

<p>Your updated model, feel free to choose the search range.</p>

<pre><code>from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch


def build_model(hp):
    model = keras.Sequential()
    model.add(layers.Embedding(input_dim=hp.Int('input_dim',
                                        min_value=5000,
                                        max_value=10000,
                                        step = 1000),
                              output_dim=hp.Int('output_dim',
                                        min_value=200,
                                        max_value=800,
                                        step = 100),
                              input_length = 400))
    model.add(layers.Convolution1D(
                filters=hp.Int('filters',
                                        min_value=32,
                                        max_value=512,
                                        step = 32),
                kernel_size=hp.Int('kernel_size',
                                        min_value=3,
                                        max_value=11,
                                        step = 2),
                padding='same',
                activation='relu')),
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D())
    model.add(layers.Flatten())
    model.add(layers.Dropout(0.4))
    model.add(layers.Dense(units=hp.Int('units',
                                        min_value=64,
                                        max_value=256,
                                        step=32),
                           activation='relu'))
    model.add(layers.Dropout(0.4))
    model.add(layers.Dense(y_train[0], activation='softmax'))
    model.compile(
    optimizer=keras.optimizers.Adam(
        hp.Choice('learning_rate',
                  values=[1e-2, 1e-3, 1e-4])),
    loss='categorical_crossentropy',
    metrics=['accuracy'])
    return model


tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='my_dir',
    project_name='helloworld')
tuner.search_space_summary()

## The following lines are based on your model


tuner.search(x, y,
             epochs=5,
             validation_data=(val_x, val_y))

models = tuner.get_best_models(num_models=2)
</code></pre>

<p>You can try replacing the Conv1D layers with LSTM layers and observe if you get better performance.</p>

<p><code>LSTM(units = 512)</code> <a href=""https://keras.io/layers/recurrent/"" rel=""nofollow noreferrer"">https://keras.io/layers/recurrent/</a></p>

<p>If you want to extract more meaningful features, one approach I found promising is by extracting pre-trained BERT features and then training using a CNN/LSTM.</p>

<p>A great repository to get started is this one - 
<a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>

<p>Once you get the sentence embedding from the BERT/XLNet you can use those features to train another CNN similar to the one you are using except maybe get rid of the embedding layer as it's expensive.</p>
",1,1,1456,2020-03-26 13:11:52,https://stackoverflow.com/questions/60868089/fine-tuning-cnn-hyperparameters-for-complex-text-classification
How to choose the Chi Squared threshold in feature selection,"<p>About this: </p>

<p><a href=""https://stackoverflow.com/questions/52561244/nlp-in-python-obtain-word-names-from-selectkbest-after-vectorizing"">NLP in Python: Obtain word names from SelectKBest after vectorizing</a></p>

<p>I found this code:</p>

<pre><code>    import pandas as pd
    import numpy as np
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_selection import chi2

    THRESHOLD_CHI = 5 # or whatever you like. You may try with
     # for threshold_chi in [1,2,3,4,5,6,7,8,9,10] if you prefer
     # and measure the f1 scores

    X = df['text']
    y = df['labels']

    cv = CountVectorizer()
    cv_sparse_matrix = cv.fit_transform(X)
    cv_dense_matrix = cv_sparse_matrix.todense()

    chi2_stat, pval = chi2(cv_dense_matrix, y)

    chi2_reshaped = chi2_stat.reshape(1,-1)
    which_ones_to_keep = chi2_reshaped &gt; THRESHOLD_CHI
    which_ones_to_keep = np.repeat(which_ones_to_keep ,axis=0,repeats=which_ones_to_keep.shape[1])
</code></pre>

<p>This code computes the chi squared test and should keep the best features within a chosen threshold. 
My question is how to choose a theshold for the chi squared test scores?</p>
","python, scikit-learn, text-classification, tf-idf, feature-selection","<p>Chi square does not have a specific range of outcome, so it's hard to determine a threshold beforehand. Usually what you can do is to sort the variables depending on their p values, the logic is that lower p values are better, because they imply a higher correlation between features and the target variable (we want to discard features that are independent, i.e. not predictors of the target variable). In this case you have anyway to decide how many features to keep, and that is a hyper parameter that you can tune manually or even better by using a grid search.</p>

<p>Be aware that you can avoid to perform the selection manually, sklearn implement already a function <a href=""http://X_new%20=%20SelectKBest(chi2,%20k=20).fit_transform(X,%20y)"" rel=""nofollow noreferrer"">SelectKBest</a> to select the best k features based on chi square, you can use it as follow:</p>

<pre><code>from sklearn.feature_selection import SelectKBest, chi2

X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
</code></pre>

<p>But if for any reason you want to rely solely on the raw chi2 value, you could calculate the minimum and maximum values among the variables, and then divide the interval in n steps to test trough a grid search. </p>
",2,0,1854,2020-03-26 23:56:52,https://stackoverflow.com/questions/60877986/how-to-choose-the-chi-squared-threshold-in-feature-selection
Document classification: Preprocessing and multiple labels,"<p>I have a question about the word representation algorithms:
Which one of the algorithms word2Vec, doc2Vec and Tf-IDF is more suitable for handling text classification tasks ?
The corpus used in my supervised learning classification is composed of a list of multiple sentences, with both short length sentences and long length ones. As discussed in <a href=""https://stackoverflow.com/questions/42643074/document-similarity-vector-embedding-versus-tf-idf-performance"">this thread</a>, doc2vec vs word2vec choice is a matter of document length. As for Tf-Idf vs. word embedding, it's more a matter of text representation.</p>

<p>My other question is, what if for the same corpus I had more than one label to link to the sentences in it ? If I create multiple entries/labels for the same sentence, it affects the decision of the final classification algorithm. How can I tell the model that every label counts equal for every sentence of the document ? </p>

<p>Thank you in advance,</p>
","word2vec, text-classification, tf-idf, doc2vec","<p>You should try multiple methods of turning your sentences into 'feature vectors'. There are no hard-and-fast rules; what works best for your project will depend a lot on your specific data, problem-domains, &amp; classification goals.</p>

<p>(Don't extrapolate guidelines from other answers – such as the one you've linked that's about document-similarity rather than classification – as best practices for your project.)</p>

<p>To get initially underway, you may want to focus on some simple 'binary classification' aspect of your data, first. For example, pick a single label. Train on all the texts, merely trying to predict if that one label applies or not.  </p>

<p>When you have that working, so you have a understanding of each step – corpus prep, text processing, feature-vectorization, classification-training, classification-evaluation – then you can try extending/adapting those steps to either single-label classification (where each text should have exactly one unique label) or multi-label classification (where each text might have any number of combined labels). </p>
",1,1,312,2020-03-27 11:53:07,https://stackoverflow.com/questions/60885461/document-classification-preprocessing-and-multiple-labels
Difference between blank and pretrained models in spacy,"<p>I am currently trying to train a text classifier using <code>spacy</code> and I got stuck with following question: what is the difference between creating a blank model using <code>spacy.blank('en')</code> and using a pretrained model <code>spacy.load('en_core_web_sm')</code>. Just to see the difference I wrote this code:</p>
<pre><code>text = &quot;hello everyone, it's a wonderful day today&quot;

nlp1 = spacy.load('en_core_web_sm')
for token in nlp1(text):
    print(token.text, token.lemma_, token.is_stop, token.pos_)
</code></pre>
<p>and it gave me the following result:</p>
<blockquote>
<p>hello hello False INTJ</p>
<p>everyone everyone True PRON</p>
<p>, , False PUNCT</p>
<p>it -PRON- True PRON</p>
<p>'s be True AUX</p>
<p>a a True DET</p>
<p>wonderful wonderful False ADJ</p>
<p>day day False NOUN</p>
<p>today today False NOUN</p>
</blockquote>
<p>Then I tried this (for the same text)</p>
<pre><code>nlp2 = spacy.blank('en')
for token in nlp2(text):
    print(token.text, token.lemma_, token.is_stop, token.pos_)
</code></pre>
<p>and the result was</p>
<blockquote>
<p>hello hello False</p>
<p>everyone everyone True</p>
<p>, , False</p>
<p>it -PRON- True PRON</p>
<p>'s 's True</p>
<p>a a True</p>
<p>wonderful wonderful False</p>
<p>day day False</p>
<p>today today False</p>
</blockquote>
<p>Not only are the results different (for example, lemma for <code>'s</code> is different) but there are also no POS tagging for most of words in blank model.</p>
<p>So obviously I need a pretrained model for normalizing my data. But I still don't understand how it should be with my data classifier. Should I 1) create a blank model for training text classifier (using <code>nlp.update()</code>) and load a pretrained model for removing stop words, lemmatization and POS tagging or 2) only load a pretrained model for both: normalizing and training my text classifier?</p>
<p>Thanks in advance for any advice!</p>
","python, spacy, text-classification","<p>If you are using spacy's text classifier, then it is fine to start with a blank model. The <code>TextCategorizer</code> doesn't use features from any other pipeline components.</p>

<p>If you're using spacy to preprocess data for another text classifier, then you would need to decide which components make sense for your task. The pretrained models load a tagger, parser, and NER model by default.</p>

<p>The lemmatizer, which isn't implemented as a separate component, is the most complicated part of this. It tries to provide the best results with the available data and models:</p>

<ul>
<li><p>If you don't have the package <code>spacy-lookups-data</code> installed and you create a blank model, you'll get the lowercase form as a default/dummy lemma.</p></li>
<li><p>If you have the package <code>spacy-lookups-data</code> installed and you create a blank model, it will automatically load lookup lemmas if they're available for that language.</p></li>
<li><p>If you load a provided model and the pipeline includes a tagger, the lemmatizer switches to a better rule-base lemmatizer if one is available in spacy for that language (currently: Greek, English, French, Norwegian Bokmål, Dutch, Swedish). The provided models also always include the lookup data for that language so they can be used when the tagger isn't run.</p></li>
</ul>

<p>If you want to get the lookup lemmas from a provided model, you can see them by loading the model without the tagger:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load(""en_core_web_sm"", disable=[""tagger""])
</code></pre>

<p>In general, the lookup lemma quality is not great (there's no information to help with ambiguous cases) and the rule-based lemmas will be a lot better, however it does take additional time to run the tagger, so you can choose lookup lemmas to speed things up if the quality is good enough for your task.</p>

<p>And if you're not using the parser or NER model for preprocessing, you can speed things up by disabling them:</p>

<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load(""en_core_web_sm"", disable=[""ner"", ""parser""])
</code></pre>
",7,4,6754,2020-03-27 14:28:18,https://stackoverflow.com/questions/60888020/difference-between-blank-and-pretrained-models-in-spacy
training fasttext models with social generated content,"<p>I am currently learning about text classification using Facebook FastText. I have found some data from Kaggle that contains characters such as �� or twitter username and hashtags. I tried searching the web however there is no clarification of how you really need to clean/pre-process your text before training a model. </p>

<p>In some blogs I've seen authors writing about tokenisation however its not mentioned in fasttext. Another point it that fasttext git has examples of clean data, such as stackoverflow but nothing for twitter or such platform. </p>

<p>Question is, what is the best practice to pre-process user(social) generated content before training a model? What needs to be redacted? </p>

<p>Thanks</p>
","machine-learning, nlp, text-classification, fasttext","<p>Since the FastText-Classifier does not work with pretrained embeddings, you can pretty much choose your own way how to clean your data. I would suggest you:</p>
<ul>
<li><p>convert everything to lower case (or upper case if you want, it shouldn't matter).</p>
</li>
<li><p>And I would remove special characters beside # and @.</p>
<p>Everything else is up to you. You can decide to keep hashtags, or to remove them, the same is true for usernames. I would probably remove usernames because I guess there isn't a lot information in them. But in some cases it could be informative: Think about tweets about and answers to Donald Trump, his username is often used I guess. Just try what works best for your case. FastText is super fast, so a few experiments won't be much of a problem.</p>
</li>
</ul>
",1,1,144,2020-03-28 15:23:10,https://stackoverflow.com/questions/60902647/training-fasttext-models-with-social-generated-content
Text classification using word embeddings,"<p>I've got a dataset of positive and negative content. So let's assume it's a spam project. </p>

<p>I need to build a model, which can categorize the content in pos/neg. So I am doing a supervised learning task, because I've got a labeled dataset. The best choice therefore must be using a SVC model.</p>

<p>So far so good.</p>

<p>Now the complicated part comes.</p>

<p>I want to solve the same task by using Keras LSTM model. So my question:</p>

<p>Is it still supervised or is it unsupervised , because I am using word embeddings for this task and referring to this post here, word embedding is used for unsupervised tasks: <a href=""https://www.quora.com/Is-deep-learning-supervised-unsupervised-or-something-else"" rel=""nofollow noreferrer"">https://www.quora.com/Is-deep-learning-supervised-unsupervised-or-something-else</a></p>

<p>There it says: </p>

<blockquote>
  <p>Deep learning can be Unsupervised : Word embedding, image encoding
  into lower or higher dimensional etc.</p>
</blockquote>

<p>So - is it now unsupervised or supervised (because my dataset is labeled) ? </p>

<p>And is deep learning another technique like unsupervised and supervised learning or how is the relation between these topics? Is deep learning using supervised and unsupervised techniques? Or do one have to choose between deep learning, unsupervised and supervised learning?</p>

<p>It's so confusing! Please help! Especially for the LSTM task. I need to know where it's supervised (because of the labeled dataset) or unsupervised (because of the usage of word embeddings)</p>

<p>Thanks in advance guys!</p>
","machine-learning, text-classification, word-embedding, unsupervised-learning, supervised-learning","<p>A quick word of encouragement, I recall feeling precisely the same way; <em>insanely frustrated</em> when I started learning this field. It really does get easier!  </p>

<p>Word embeddings are <em>created</em> by unsupervised learning. However, you can use a trained embedding layer <em>within</em> a supervised projected, like you're doing. In other words, your project is one of supervised learning, and one of the layers is using weights that were acquired by an unsupervised training technique.</p>

<p>It may be helpful to further understand embedding layers, how they're made, and what they can do for supervised learning. I'll try to explain in a non-technical way, so you can get a feel for the concept prior to learning the particulars and pedantisms.<br>
Suppose you begin with a giant corpus. You count the frequency of occurrence of every word, and use it to rank each relative to the others (or use some other formula, whatever). This is a method of text ""tokenization."" The point is to get words into numbers. Obviously this is important, since we're fixing to do math with them, but it creates a bit of a pinch: the numerical relationships don't necessarily carry any information about the relationships of the <em>meanings</em> of the words. To ameliorate this, you can train a little network like so: take chunks from your corpus and create <a href=""https://keras.io/preprocessing/sequence#skipgrams"" rel=""nofollow noreferrer"">skipgrams</a>, and teach the network that, after the application of weights and a measure of <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity"" rel=""nofollow noreferrer"">cosine similarity</a>, the output produced should be a <code>1</code> if the words appear near each other (or some other criteria), or <code>0</code> (or perhaps <code>-1</code>, if you prefer) when they do not appear near each other. Over the course of the corpus, words that tend to be used together will move together and likewise the inverse. The objective is to create a kind of map (or a simulacrum, if you will) of the <em>relative meaning</em> of the tokens (which are words); said another way, the objective is to create an n-dimensional representation of the words' relative meanings. Then, after training, the embeddings can be saved for use in projects like yours. Your embedding layer will then look up the token in the saved embeddings and grab its outputs, which are that word's vector representation in the embedding space; their coordinates in our theoretical map. This is considered ""unsupervised"" because you don't have to explicitly supply the ground-truth for comparison; in this case, it's being generated procedurally from the training sample (i.e. skipgrams generated from whatever the input was). Another example would be if the expected output was identical to the input (as in auto-encoders), which is unsupervised (as before) because you don't have to supply an expected output; if you supply an input, it automatically has the expected output.<br>
If all of that is confusing, then just pause and consider your own thoughts: if I ask you for a word that means the same thing as ""big"" in the phrase ""a big pizza,"" you consult your understanding of the meaning of ""big"" as pertains to the indicated phrase, and draw something as close to it as possible: perhaps the word ""large."" Embeddings are a way of making a map where ""big"" and ""large"" are positioned very close together along most axes (i.e. in most dimensions).<br>
So, then, when you load some pre-trained embeddings, you're just loading some weights into one of your layers. Sometimes people initialize layers with zeros, other times people use random normal or gaussian distributions, and sometimes people use specific values (e.g. loading a saved network, or loading embeddings); it's all the same. If you go on to perform supervised training, then you're doing precisely that: performing supervised training. Following the embedding layer, the information you're working with is not arbitrary words, but rather these: relative meanings. And if that isn't just neat, I don't know what is! I find it's helpful to consider what your data represents as it passes through the network.</p>
",2,0,1842,2020-03-30 11:15:50,https://stackoverflow.com/questions/60929359/text-classification-using-word-embeddings
Remove training data from spacy model,"<p>I have trained a spacy textcat model but then I realized that there were some incorrect training data: data from one category happened to be labeled with another category. My question is: is it possible to remove these training examples from the model without retraining it? Something like <code>nlp.update()</code> but in reverse? Would appreciate any help!</p>
","python, spacy, text-classification","<p>You mean to revert specific cases? As far as I know, that's not currently possible in spaCy. </p>

<p>I would suggest to either retrain from scratch with the corrected annotations, or continue training with the updated annotations. If you continue training, make sure that you keep feeding a representative set to your model, so that it doesn't ""forget"" cases it was already predicting correctly before.</p>
",1,1,318,2020-04-01 11:44:46,https://stackoverflow.com/questions/60970109/remove-training-data-from-spacy-model
"How to multi-label classify movies to film festivals based on its metadata, where the metadata is predominantly individual words?","<p>I have created a data-set of various movies produced in the past few years, technicians worked for the film, genre, country it represented, runtime, language, the respective film festival that film has won, etc.</p>

<p>the data-set is similar to <a href=""https://i.sstatic.net/LX5yP.png"" rel=""nofollow noreferrer"">this</a>,  it is an excel file. </p>

<p>I'm interested in multi-label classification of the movies to film festivals based on the inherent features of the movie(irrespective of the plot)</p>

<p>I thought we need to work in numbers/vectors to multi-label classify the data. But, I'm unaware of how vectorization of names(proper nouns) and few individual words can be carried out.</p>

<p>Is there any other way I can carry out the process to achieve my goal of multi-label classification with the above data? Please help me identify it. Thank you.</p>
","python, machine-learning, vectorization, text-classification, multilabel-classification","<p>The dataset you have here is tabular data. You need to vectorise that tabular data in order to be able to pass it to a classification model.</p>

<p>Tabular data is usually made of :</p>

<ol start=""2"">
<li>continuous features (eg: imdb rating, runtime)</li>
<li>categorical features (eg: every other feature in your dataset)</li>
</ol>

<p>The vectorisation of tabular data is simply the concatenation of the vector representation of each feature.
For continuous features, you should normalise the values. For categorical features you should one-hot encode them.</p>

<p><strong>Note</strong>:
In the case of your dataset, you have 3 ""text-like"" features: title, director and writer:</p>

<ul>
<li>title: A title is unique to its film, so there is nothing your model can learn from this, so you should discard it from the dataset.</li>
<li>director and writer: you should treat them as categorical variables and not text. If you encoded them using text vectorisation techniques (Bag of words or TF-IDF) it would mean you assume that a word like <code>Pedro</code> can have predictive power. Is there a point in common between Pedro Gonzalez-Rubio and Pedro Almodovar? If there is, it's maybe that they both speak Spanish, but then I would rather add that as a feature to your model  (eg: language_of_director)</li>
</ul>
",0,0,112,2020-04-03 10:25:44,https://stackoverflow.com/questions/61009786/how-to-multi-label-classify-movies-to-film-festivals-based-on-its-metadata-wher
Improve Keras Model for text classification,"<p>I am making a model for <strong>classification of webpages titles</strong> into one of 101 classes regarding food (most of the titles regard recipes). The medium length of my sequences is 42. I cleaned the text (bad words, changed to lowercase etc) and tokenized it using a Tokenizer. I put a <strong>LSTM</strong> layer in my model, and I get 83% accuracy on the test set. I'm pretty sure this can be improved making some changes to the network, do you have any suggestions? Thank you in advance! That's my model:</p>

<pre><code>model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=x_train.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(101, activation='softmax'))

opt = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
</code></pre>
","python, lstm, text-classification","<p>One thing you could try to do is to add another LSTM layer, but please pay attention to the number of units: increasing them too much can easily lead to overfitting. Otherwise, gradually reducing the learning rate when you reach a plateau could also contribute to an increase.</p>

<p>If you add another one, do not forget to add ""<code>return_sequences=True</code>"" in the first LSTM layer.</p>

<p>You should also have a validation set reserved for metrics (apart from the test set).</p>
",1,0,280,2020-04-06 13:21:28,https://stackoverflow.com/questions/61060815/improve-keras-model-for-text-classification
I can&#39;t get my test accuracy to increase in a sentiment analysis,"<p>I'm not sure if this is the right place but my test accuracy is always at about .40 while I can get my training set accuracy to 1.0. I'm trying to do a sentiment analysis of tweets on trump, I have annotated each tweet with a positive,negative or neutral polarity. I want to be able to predict the polarity of new data based on my model. I've tried different models but the SVM seems to give me the highest test accuracy. I'm unsure as to why my data model accuracy is so low but would appreciate any help or direction.</p>

<pre><code>trump = pd.read_csv(""trump_data.csv"", delimiter = "";"")

#drop all nan values
trump = trump.dropna()
trump = trump.rename(columns = {""polarity,,,"":""polarity""})

#print(trump.columns)
def tokenize(text):
   ps = PorterStemmer()
   return [ps.stem(w.lower()) for w in word_tokenize(text)    


X = trump.text

y = trump.polarity
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .2, random_state = 42)


svm = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'), 
tokenizer=tokenize)), ('svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, 
random_state=42,max_iter=5, tol=None))])

svm.fit(X_train, y_train)


model = svm.score(X_test, y_test)
print(""The svm Test Classification Accuracy is:"", model )
print(""The svm training set accuracy is : {}"".format(naive.score(X_train,y_train)))
y_pred = svm.predict(X)
</code></pre>

<p>This is an example of one of the strings in the text column of the dataset</p>

<p>"".@repbilljohnson congress must step up and overturn president trumpâ€™s discriminatory #eo banning #immigrants &amp; #refugees #oxfam4refugees""</p>

<p><a href=""https://i.sstatic.net/sqryY.png"" rel=""nofollow noreferrer"">Data set</a></p>
","python, machine-learning, nltk, svm, text-classification","<p>Why are you using <code>naive.score</code>? I assume it's a <code>copy-paste</code> mistake. Here are a few steps you can follow.</p>

<ol>
<li>Make sure you enough data points and clean it. <code>Cleaning</code> the dataset is the inevitable process in data science.</li>
<li>Make use of the parameters like <code>ngram_range, max_df, min_df, max_features</code> while featurizing the text with either <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a> or <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a>. You may also try embeddings using <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Word2Vec</a>.</li>
<li>Do a hyperparameter tuning on <code>alpha, penalty</code> &amp; other variables using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">GridSearch</a> or <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"" rel=""nofollow noreferrer"">RandomizedSearchCV</a>. Make sure you are <code>CV</code> currently. Refer the documentation for more info</li>
<li>If the dataset is <code>imbalanced</code>, then try using other matrices like <code>log-loss, precision, recall, f1-score, etc</code>. Refer <a href=""https://sachinkalsi.github.io/blog/category/ml/2018/08/20/top-8-performance-metrics-one-should-know.html"" rel=""nofollow noreferrer"">this</a> for more info.</li>
<li>Make sure your model is neither <code>overfitted</code> not <code>underfitted</code> by checking train-error &amp; test error.</li>
</ol>

<p>Other than <code>SVM</code>, also try the traditional models like <code>Logistic Regression</code>, <code>NV</code>, <code>RF</code> etc. If you have a large number of data points, then you may try Deep Learning models.</p>
",0,0,844,2020-04-12 11:14:16,https://stackoverflow.com/questions/61170528/i-cant-get-my-test-accuracy-to-increase-in-a-sentiment-analysis
Separate the words in the sentence for text classification problem,"<p>I am solving a text classification problem and while annotating my data I found very long words which are sentence itself but are not separated by space. </p>

<p>One of the example which I found while annotating my data point is:</p>

<p>Throughnumerousacquisitionsandtransitions,Anacompstillexiststodaywithagreaterfocusondocumentmanagement</p>

<p>Desired output:</p>

<p>Through numerous acquisitions and transitions, Anacomp still exists today with a greater focus on document management.</p>

<p>I have looked upon various frameworks such as Keras, PyTorch to see if they provide any functionality to solve this issue but I couldn't find anything. </p>
","python-3.x, machine-learning, deep-learning, neural-network, text-classification","<p>The problem that you are trying to solve is text/word segmentation. It is possible to approach this based on ML using a sequence model (such as LSTM) and a word embedding (such as BERT). </p>

<p><a href=""http://nlpprogress.com/chinese/chinese_word_segmentation.html%20details"" rel=""nofollow noreferrer"">This link</a> details such an approach for Chinese language. Chinese language does not adopt spaces, so this sort of approach is necessary as a preprocessing component in Chinese NLP processing tasks.</p>

<p>I would like to describe an automaton based approach using <a href=""https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm"" rel=""nofollow noreferrer"">Aho-Corasick Algorithm</a>.</p>

<p>First do a <code>pip install pyahocorasick</code></p>

<p>I'm resorting to use only the words in your input string for the sake of demonstration. In a real world scenario you could just use a dictionary of words from something like <a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">Wordnet</a>.</p>

<pre><code>import ahocorasick

automaton = ahocorasick.Automaton()

input = 'Throughnumerousacquisitionsandtransitions, Anacompstillexiststodaywithagreaterfocusondocumentmanagement'

# Replace this with a large dictionary of words
word_dictionary = ['Through', 'numerous', 'acquisition', 'acquisitions', 'and', 'transitions', 'Anacomp', 'still',
                   'exists', 'today', 'with', 'a', 'greater', 'focus', 'on', 'document', 'management']

# add dictionary words to automaton
for idx, key in enumerate(word_dictionary):
    automaton.add_word(key, (idx, key))

# Build aho-corasick automaton for search
automaton.make_automaton()

# to check for ambiguity, if there is a longer match then prefer that
previous_rng = range(0, 0)
previous_rs = set(previous_rng)

# Holds the end result dictionary
result = {}

# search the inputs using automaton
for end_index, (insert_order, original_value) in automaton.iter(input):
    start_index = end_index - len(original_value) + 1
    current_rng = range(start_index, end_index)
    current_rs = set(current_rng)
    # ignore previous as there is a longer match available
    if previous_rs.issubset(current_rs):
        # remove ambiguous short entry in favour of the longer entry
        if previous_rng in result:
            del result[previous_rng]
        result[current_rng] = (insert_order, original_value)
        previous_rng = current_rng
        previous_rs = current_rs
    # if there is no overlap of indices, then its a new token, add to result
    elif previous_rs.isdisjoint(current_rs):
        previous_rng = current_rng
        previous_rs = current_rs
        result[current_rng] = (insert_order, original_value)
    # ignore current as it is a subset of previous
    else:
        continue
    assert input[start_index:start_index + len(original_value)] == original_value

for x in result:
    print(x, result[x])

</code></pre>

<p>Produces results :</p>

<pre><code>range(0, 6) (0, 'Through')
range(7, 14) (1, 'numerous')
range(15, 26) (3, 'acquisitions')
range(27, 29) (4, 'and')
range(30, 40) (5, 'transitions')
range(43, 49) (6, 'Anacomp')
range(50, 54) (7, 'still')
range(55, 60) (8, 'exists')
range(61, 65) (9, 'today')
range(66, 69) (10, 'with')
range(71, 77) (12, 'greater')
range(78, 82) (13, 'focus')
range(83, 84) (14, 'on')
range(85, 92) (15, 'document')
range(93, 102) (16, 'management')
</code></pre>
",2,-1,94,2020-04-22 01:56:15,https://stackoverflow.com/questions/61356035/separate-the-words-in-the-sentence-for-text-classification-problem
An algorithm for computing the edit-distance between two words,"<p>I am trying to write Python code that takes a word as an input (e.g. book), and outputs the most similar word with similarity score. </p>

<p>I have tried different off-the-shelf edit-distance algorithms like cosine, Levenshtein and others, but these cannot tell the degree of differences. For example, (book, bouk) and (book,bo0k). I am looking for an algorithm that can gives different scores for these two examples. I am thinking about using fastText or BPE, however they use cosine distance. </p>

<p>Is there any algorithm that can solve this?</p>
","python-3.x, nlp, text-classification, fasttext, edit-distance","<p>That is a very interesting question - probably with many possible answers. You could possibly add in bigram (n-gram) analysis to rank how likely the letters would be related to each other in typical words.</p>

<p>Presuming your system doesn't ""know"" the target word, but someone types ""bouk"". Then it analyses all the bigrams:</p>

<p>bo, ou, uk</p>

<p>or trigrams</p>

<p>bou, ouk</p>

<p>I would guess here that ""bo"", ""ou"", ""bou"" would score well as they are common, but ""uk"" and ""ouk"" would be not likely in English. So this could simply have a 3/5 score, but actually each trigram would have its own frequency score (probability), so the overall number for the proposed word could be quite refined.</p>

<p>Then comparing that to ""bo0k"" you'd look at all bigrams:</p>

<p>bo, o0, 0k</p>

<p>or trigrams</p>

<p>bo0, o0k</p>

<p>Now you can see that only ""bo"" would score well here. All the others would not be found in a common n-gram corpus. So this word would score much lower than ""bouk"" for likelihood, e.g. 1/5 compared to the 3/5 for ""bouk"".</p>

<p>There would be roughly three parts to the solution:</p>

<p>You would need a corpus of established n-gram frequencies for the language. For example this random blog I found discusses that:  <a href=""https://blogs.sas.com/content/iml/2014/09/26/bigrams.html"" rel=""nofollow noreferrer"">https://blogs.sas.com/content/iml/2014/09/26/bigrams.html</a></p>

<p>Then you would need to process (tokenise and scan) your input words into n-grams and then look up their frequencies in the corpus. You could use something like SK Learn, </p>

<p>Then you can sum the parts in whatever way you like to establish the overall score for the word. </p>

<p>Note you may find most tokenisers and n-gram processing for natural language centres around <em>word relations</em> not letters within words. It's easy to get lost on that, as often the fact a library is focused on word-grams is not explicitly mentioned because it's the most common. I've noticed that before, but n-grams are used in all sorts of other data sets too (timeseries, music, any sequence really) This question does discuss how you can convert SK Learn's vectoriser to do letter-grams, but I've not tried this myself: <a href=""https://stackoverflow.com/questions/53033877/n-grams-for-letter-in-sklearn"">N-grams for letter in sklearn</a></p>
",1,1,709,2020-04-22 12:20:11,https://stackoverflow.com/questions/61364975/an-algorithm-for-computing-the-edit-distance-between-two-words
Accuracy and prediction Classifiers,"<p>I have trained LSTM AND decision tree on my data set (type of text classification). I have used K-cross fold validation with k=10. </p>

<p>Decision tree accuracy 61%
LSTM accuracy 90%</p>

<p>Now when I predict on totally unseen data then decision tree predicts more well and good as compared to LSTM.</p>

<p>Why it happens? If LSTM accuracy is more then why decision tree performs more well on unseen data as compare to LSTM?</p>
","machine-learning, keras, lstm, decision-tree, text-classification","<p>Your LSTM model may have greater accuracy than a decision tree when training, but the fact that it doesn't generalize well to unseen data, indicates that the LSTM is overfitting to the training data. Try adjusting the train-validation split and batch size to see if that improves your models.</p>

<p>The validation loss during training would indicate which model is better. You can also try using random forests (cluster of decision trees) which has been known to give better results than one decision tree alone</p>
",2,-1,61,2020-04-23 10:01:38,https://stackoverflow.com/questions/61384429/accuracy-and-prediction-classifiers
How to get unknown class in multi-class&#39; prediction?,"<p>I have a multi-class sentence classification problem and i want to achieve some sort of unknown class in prediction. There are n different labels and sometimes the sentence must not be classified or classified as unknown\None. </p>

<p>If you have some options to achive this behavior, i'd be very pleasured</p>
","tensorflow, keras, recurrent-neural-network, text-classification, multilabel-classification","<p>A classifier can only predict what it sees in the training data. If you do not have an ""unknown"" in the training data, you can never see in at test time either. To make this problem even worse, neural networks tend to be very confident when making incorrect predictions.</p>

<p>If you have the instances belonging to unknown classes in your training data, then use it as any other class. If you don't, you might have a look at <a href=""https://towardsdatascience.com/how-to-be-confident-in-your-neural-network-confidence-10d10dcf8003"" rel=""nofollow noreferrer"">estimating classifier confidence</a>.</p>
",1,0,2054,2020-04-25 13:51:26,https://stackoverflow.com/questions/61426701/how-to-get-unknown-class-in-multi-class-prediction
How to make prediction on Keras Text classification?,"<p>I've trained a model with this reference: <a href=""https://www.tensorflow.org/tutorials/keras/text_classification_with_hub"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/keras/text_classification_with_hub</a></p>

<p>Here is my code:</p>

<pre><code>import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

train_data, validation_data, test_data = tfds.load(
    name=""imdb_reviews"", 
    split=('train[:60%]', 'train[60%:]', 'test'),
    as_supervised=True)

train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))

embedding = ""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1""
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                        dtype=tf.string, trainable=True)
hub_layer(train_examples_batch[:3])

model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1))

model.summary()

model.compile(optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=['accuracy'])

history = model.fit(train_data.shuffle(10000).batch(512),
    epochs=20,
    validation_data=validation_data.batch(512),
    verbose=1)

results = model.evaluate(test_data.batch(512), verbose=2)

model.save(""imdb_model.h5"")
</code></pre>

<p>I've saved the model as <em>imdb_model.h5</em>. I want to make a prediction on a custom text. For example <strong><em>""The best movie, I have ever seen""</em></strong>. How can I do it?</p>
","python, tensorflow, keras, text-classification","<p>You can use</p>

<pre><code>model.predict([""This is the best movie I have ever seen""])
</code></pre>
",0,0,1493,2020-04-26 15:41:13,https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification
How to create a BERT Layer with Keras?,"<p>I am trying to user a BERT layer to classify text comments into positive or negative:</p>

<pre><code># similar to tutorial:
# https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b

# ensure you are running TensorFlow 2.0 Google Colab

try:
    %tensorflow_version 2.x
except Exception:
    pass
import tensorflow as tf

import tensorflow_hub as hub

from tensorflow.keras import layers
import bert
# import dependencies
import pandas as pd
import numpy as np
import csv
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import model_selection, naive_bayes, svm
from sklearn.metrics import accuracy_score

# set random seed
np.random.seed(42)

# add the corpus
with open('labelled_data.csv', newline='') as f:
    reader = csv.reader(f)
    next(reader) # skip header
    labelled_data = [tuple(row) for row in reader]

# generate a balanced data set
# separate into positive and negative comments
positives = []
for tup in labelled_data:
    if tup[1] == ""positive"":
        positives.append(tup)

negatives = []
for tup in labelled_data:
    if tup[1] == ""negative"":
        negatives.append(tup)

# base the number of samples on the lesser of two category counts    
import random
sample_count = min(len(positives), len(negatives))
balanced_positives = random.sample(positives, sample_count)
balanced_negatives = random.sample(negatives, sample_count)
balanced_data = balanced_positives + balanced_negatives
# shuffle the data
random.shuffle(balanced_data)
# convert to a dataframe and label the columns
Corpus = pd.DataFrame(balanced_data)    
Corpus.columns = ['text', 'label']

# Step - a : Remove blank rows if any.
Corpus['text'].dropna(inplace=True)
# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently
Corpus['text'] = [entry.lower() for entry in Corpus['text']]
# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words
Corpus['text']= [word_tokenize(entry) for entry in Corpus['text']]
# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.
# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun
tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J'] = wn.ADJ
tag_map['V'] = wn.VERB
tag_map['R'] = wn.ADV

for index,entry in enumerate(Corpus['text']):
    # Declaring Empty List to store the words that follow the rules for this step
    Final_words = []
    # Initializing WordNetLemmatizer()
    word_Lemmatized = WordNetLemmatizer()
    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.
    for word, tag in pos_tag(entry):
        # Below condition is to check for Stop words and consider only alphabets
        if word not in stopwords.words('english') and word.isalpha():
            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
            Final_words.append(word_Final)
    # The final processed set of words for each iteration will be stored in 'text_final'
    Corpus.loc[index,'text_final'] = str(Final_words)

# create 80/20 training/test split of data
Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.2)

# encode text to numerical matrix
Encoder = LabelEncoder()
Train_Y = Encoder.fit_transform(Train_Y)
Test_Y = Encoder.fit_transform(Test_Y)

# create term-frequency/inverse document frequency matrices to find important features/words
Tfidf_vect = TfidfVectorizer(max_features=5000)
Tfidf_vect.fit(Corpus['text_final'])
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)
# to view vectorized data in format of (row, uniqueId, importance_score)

# create a bert layer class to use in the model

class BertLayer(tf.keras.layers.Layer):
    def __init__(self, n_fine_tune_layers=10, **kwargs):
        self.n_fine_tune_layers = n_fine_tune_layers
        self.trainable = True
        self.output_size = 768,
        self.bert_path=""https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1"",
        super(BertLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.bert = hub.Module(
            self.bert_path,
            trainable=self.trainable,
            name=""{}_module"".format(self.name)
        )
        trainable_vars = self.bert.variables

        # Remove unused layers
        trainable_vars = [var for var in trainable_vars if not ""/cls/"" in var.name]

        # Select how many layers to fine tune
        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]

        # Add to trainable weights
        for var in trainable_vars:
            self._trainable_weights.append(var)

        # Add non-trainable weights
        for var in self.bert.variables:
            if var not in self._trainable_weights:
                self._non_trainable_weights.append(var)

        super(BertLayer, self).build(input_shape)

    def call(self, inputs):
        inputs = [K.cast(x, dtype=""int32"") for x in inputs]
        input_ids, input_mask, segment_ids = inputs
        bert_inputs = dict(
            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids
        )
        result = self.bert(inputs=bert_inputs, signature=""tokens"", as_dict=True)[
            ""pooled_output""
        ]
        return result

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_size)

# Build model
in_id = tf.keras.layers.Input(shape=(Train_X.shape[0],), name=""input_ids"")
in_mask = tf.keras.layers.Input(shape=(Train_X.shape[0],), name=""input_masks"")
in_segment = tf.keras.layers.Input(shape=(Train_X.shape[0],), name=""segment_ids"")
bert_inputs = [in_id, in_mask, in_segment]

# Here the error occurs
bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)
</code></pre>

<p>The error reads: 
<code>Unknown module spec type: &lt;class 'tuple'&gt;</code></p>

<p>The <code>bert_inputs</code> are tensors of <code>shape=(None, 563)</code></p>
","tensorflow, deep-learning, text-classification","<p>Here is how I ultimately integrated a BERT layer:</p>

<pre><code>import tensorflow as tf
import pandas as pd
import tensorflow_hub as hub
import os
import re
import numpy as np
from bert.tokenization import FullTokenizer
from tqdm import tqdm
from tensorflow.keras import backend as K

# Initialize session
sess = tf.Session()


# Load all files from a directory in a DataFrame.
def load_directory_data(directory):
    data = {}
    data[""sentence""] = []
    data[""sentiment""] = []
    for file_path in os.listdir(directory):
        with tf.gfile.GFile(os.path.join(directory, file_path), ""r"") as f:
            data[""sentence""].append(f.read())
            data[""sentiment""].append(re.match(""\d+_(\d+)\.txt"", file_path).group(1))
    return pd.DataFrame.from_dict(data)


# Merge positive and negative examples, add a polarity column and shuffle.
def load_dataset(directory):
    pos_df = load_directory_data(os.path.join(directory, ""pos""))
    neg_df = load_directory_data(os.path.join(directory, ""neg""))
    pos_df[""polarity""] = 1
    neg_df[""polarity""] = 0
    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)


# Download and process the dataset files.
def download_and_load_datasets(force_download=False):
    dataset = tf.keras.utils.get_file(
        fname=""aclImdb.tar.gz"",
        origin=""http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"",
        extract=True,
    )

    train_df = load_dataset(os.path.join(os.path.dirname(dataset), ""aclImdb"", ""train""))
    test_df = load_dataset(os.path.join(os.path.dirname(dataset), ""aclImdb"", ""test""))

    return train_df, test_df


class PaddingInputExample(object):
    """"""Fake example so the num input examples is a multiple of the batch size.
  When running eval/predict on the TPU, we need to pad the number of examples
  to be a multiple of the batch size, because the TPU requires a fixed batch
  size. The alternative is to drop the last batch, which is bad because it means
  the entire output data won't be generated.
  We use this class instead of `None` because treating `None` as padding
  battches could cause silent errors.
  """"""


class InputExample(object):
    """"""A single training/test example for simple sequence classification.""""""

    def __init__(self, guid, text_a, text_b=None, label=None):
        """"""Constructs a InputExample.
    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """"""
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.label = label


def create_tokenizer_from_hub_module(bert_path):
    """"""Get the vocab file and casing info from the Hub module.""""""
    bert_module = hub.Module(bert_path)
    tokenization_info = bert_module(signature=""tokenization_info"", as_dict=True)
    vocab_file, do_lower_case = sess.run(
        [tokenization_info[""vocab_file""], tokenization_info[""do_lower_case""]]
    )

    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)


def convert_single_example(tokenizer, example, max_seq_length=256):
    """"""Converts a single `InputExample` into a single `InputFeatures`.""""""

    if isinstance(example, PaddingInputExample):
        input_ids = [0] * max_seq_length
        input_mask = [0] * max_seq_length
        segment_ids = [0] * max_seq_length
        label = 0
        return input_ids, input_mask, segment_ids, label

    tokens_a = tokenizer.tokenize(example.text_a)
    if len(tokens_a) &gt; max_seq_length - 2:
        tokens_a = tokens_a[0 : (max_seq_length - 2)]

    tokens = []
    segment_ids = []
    tokens.append(""[CLS]"")
    segment_ids.append(0)
    for token in tokens_a:
        tokens.append(token)
        segment_ids.append(0)
    tokens.append(""[SEP]"")
    segment_ids.append(0)

    input_ids = tokenizer.convert_tokens_to_ids(tokens)

    # The mask has 1 for real tokens and 0 for padding tokens. Only real
    # tokens are attended to.
    input_mask = [1] * len(input_ids)

    # Zero-pad up to the sequence length.
    while len(input_ids) &lt; max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)

    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length

    return input_ids, input_mask, segment_ids, example.label


def convert_examples_to_features(tokenizer, examples, max_seq_length=256):
    """"""Convert a set of `InputExample`s to a list of `InputFeatures`.""""""

    input_ids, input_masks, segment_ids, labels = [], [], [], []
    for example in tqdm(examples, desc=""Converting examples to features""):
        input_id, input_mask, segment_id, label = convert_single_example(
            tokenizer, example, max_seq_length
        )
        input_ids.append(input_id)
        input_masks.append(input_mask)
        segment_ids.append(segment_id)
        labels.append(label)
    return (
        np.array(input_ids),
        np.array(input_masks),
        np.array(segment_ids),
        np.array(labels).reshape(-1, 1),
    )


def convert_text_to_examples(texts, labels):
    """"""Create InputExamples""""""
    InputExamples = []
    for text, label in zip(texts, labels):
        InputExamples.append(
            InputExample(guid=None, text_a="" "".join(text), text_b=None, label=label)
        )
    return InputExamples


class BertLayer(tf.keras.layers.Layer):
    def __init__(
        self,
        n_fine_tune_layers=10,
        pooling=""mean"",
        bert_path=""https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1"",
        **kwargs,
    ):
        self.n_fine_tune_layers = n_fine_tune_layers
        self.trainable = True
        self.output_size = 768
        self.pooling = pooling
        self.bert_path = bert_path
        if self.pooling not in [""first"", ""mean""]:
            raise NameError(
                f""Undefined pooling type (must be either first or mean, but is {self.pooling}""
            )

        super(BertLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.bert = hub.Module(
            self.bert_path, trainable=self.trainable, name=f""{self.name}_module""
        )

        # Remove unused layers
        trainable_vars = self.bert.variables
        if self.pooling == ""first"":
            trainable_vars = [var for var in trainable_vars if not ""/cls/"" in var.name]
            trainable_layers = [""pooler/dense""]

        elif self.pooling == ""mean"":
            trainable_vars = [
                var
                for var in trainable_vars
                if not ""/cls/"" in var.name and not ""/pooler/"" in var.name
            ]
            trainable_layers = []
        else:
            raise NameError(
                f""Undefined pooling type (must be either first or mean, but is {self.pooling}""
            )

        # Select how many layers to fine tune
        for i in range(self.n_fine_tune_layers):
            trainable_layers.append(f""encoder/layer_{str(11 - i)}"")

        # Update trainable vars to contain only the specified layers
        trainable_vars = [
            var
            for var in trainable_vars
            if any([l in var.name for l in trainable_layers])
        ]

        # Add to trainable weights
        for var in trainable_vars:
            self._trainable_weights.append(var)

        for var in self.bert.variables:
            if var not in self._trainable_weights:
                self._non_trainable_weights.append(var)

        super(BertLayer, self).build(input_shape)

    def call(self, inputs):
        inputs = [K.cast(x, dtype=""int32"") for x in inputs]
        input_ids, input_mask, segment_ids = inputs
        bert_inputs = dict(
            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids
        )
        if self.pooling == ""first"":
            pooled = self.bert(inputs=bert_inputs, signature=""tokens"", as_dict=True)[
                ""pooled_output""
            ]
        elif self.pooling == ""mean"":
            result = self.bert(inputs=bert_inputs, signature=""tokens"", as_dict=True)[
                ""sequence_output""
            ]

            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)
            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (
                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)
            input_mask = tf.cast(input_mask, tf.float32)
            pooled = masked_reduce_mean(result, input_mask)
        else:
            raise NameError(f""Undefined pooling type (must be either first or mean, but is {self.pooling}"")

        return pooled

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_size)


# Build model
def build_model(max_seq_length):
    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=""input_ids"")
    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=""input_masks"")
    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=""segment_ids"")
    bert_inputs = [in_id, in_mask, in_segment]

    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)
    dense = tf.keras.layers.Dense(256, activation=""relu"")(bert_output)
    pred = tf.keras.layers.Dense(1, activation=""sigmoid"")(dense)

    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)
    model.compile(loss=""binary_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])
    model.summary()

    return model


def initialize_vars(sess):
    sess.run(tf.local_variables_initializer())
    sess.run(tf.global_variables_initializer())
    sess.run(tf.tables_initializer())
    K.set_session(sess)


def main(train_text, train_label, test_text, test_label):
    # Params for bert model and tokenization
    bert_path = ""https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1""
    max_seq_length = 256


    # Instantiate tokenizer
    tokenizer = create_tokenizer_from_hub_module(bert_path)

    # Convert data to InputExample format
    train_examples = convert_text_to_examples(train_text, train_label)
    test_examples = convert_text_to_examples(test_text, test_label)

    # Convert to features
    (
        train_input_ids,
        train_input_masks,
        train_segment_ids,
        train_labels,
    ) = convert_examples_to_features(
        tokenizer, train_examples, max_seq_length=max_seq_length
    )
    (
        test_input_ids,
        test_input_masks,
        test_segment_ids,
        test_labels,
    ) = convert_examples_to_features(
        tokenizer, test_examples, max_seq_length=max_seq_length
    )

    model = build_model(max_seq_length)

    # Instantiate variables
    initialize_vars(sess)

    model.fit(
        [train_input_ids, train_input_masks, train_segment_ids],
        train_labels,
        validation_data=(
            [test_input_ids, test_input_masks, test_segment_ids],
            test_labels,
        ),
        epochs=1,
        batch_size=32,
    )

# get the data
# import dependencies
import pandas as pd
import numpy as np
import csv
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import model_selection, naive_bayes, svm
from sklearn.metrics import accuracy_score

# set random seed
np.random.seed(42)

# add the corpus
with open('labelled_data.csv', newline='') as f:
    reader = csv.reader(f)
    next(reader) # skip header
    labelled_data = [tuple(row) for row in reader]

# generate a balanced data set
# separate into positive and negative comments
positives = []
for tup in labelled_data:
    if tup[1] == ""positive"":
        positives.append(tup)

negatives = []
for tup in labelled_data:
    if tup[1] == ""negative"":
        negatives.append(tup)

# base the number of samples on the lesser of two category counts    
import random
sample_count = min(len(positives), len(negatives))
balanced_positives = random.sample(positives, sample_count)
balanced_negatives = random.sample(negatives, sample_count)
balanced_data = balanced_positives + balanced_negatives
# shuffle the data
random.shuffle(balanced_data)
# convert to a dataframe and label the columns
Corpus = pd.DataFrame(balanced_data)    
Corpus.columns = ['text', 'label']

# Step - a : Remove blank rows if any.
Corpus['text'].dropna(inplace=True)
# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently
Corpus['text'] = [entry.lower() for entry in Corpus['text']]
# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words
Corpus['text']= [word_tokenize(entry) for entry in Corpus['text']]
# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.
# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun
tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J'] = wn.ADJ
tag_map['V'] = wn.VERB
tag_map['R'] = wn.ADV

for index,entry in enumerate(Corpus['text']):
    # Declaring Empty List to store the words that follow the rules for this step
    Final_words = []
    # Initializing WordNetLemmatizer()
    word_Lemmatized = WordNetLemmatizer()
    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.
    for word, tag in pos_tag(entry):
        # Below condition is to check for Stop words and consider only alphabets
        if word not in stopwords.words('english') and word.isalpha():
            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
            Final_words.append(word_Final)
    # The final processed set of words for each iteration will be stored in 'text_final'
    Corpus.loc[index,'text_final'] = str(Final_words)

# create 80/20 training/test split of data
train_text, test_text, train_label, test_label = model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.2)

Encoder = LabelEncoder()
Train_Y = Encoder.fit_transform(train_label)
Test_Y = Encoder.fit_transform(test_label)
Train_X = train_text
Test_X = test_text

main(Train_X, Train_Y, Test_X, Test_Y)
</code></pre>
",3,0,3792,2020-04-27 19:56:41,https://stackoverflow.com/questions/61467389/how-to-create-a-bert-layer-with-keras
Multilingual free-text-items Text Classification for improving a recommender system,"<p>To improve the recomender system for Buyer Material Groups, our company is willing to train a model using customer historial spend data. The model should be trained on historical ""Short text descriptions"" to predict the appropriate BMG. The dataset has more that 500.000 rows and the text descriptions are multilingual (up to 40 characters).</p>

<p>1.Question: can i use supervised learning if i consider the fact that the descriptions are in multiple languages? If Yes, are classic approaches like multinomial naive bayes or SVM suitable?</p>

<p>2.Question: if i want to improve the first model in case it is not performing well, and use unsupervised multilingual emdedding to build a classifier. how can i train this classifier on the numerical labels later?</p>

<p>if you have other ideas or approaches please feel free :). (It is a matter of a simple text classification problem)</p>
","nlp, multilingual, text-classification, unsupervised-learning, supervised-learning","<blockquote>
  <p>Can I use supervised learning if i consider the fact that the descriptions are in multiple languages?</p>
</blockquote>

<p>Yes, this is not a problem except it makes your data more sparse. If you actually only have 40 characters (is that not 40 words?) per item, you may not have enough data. Also the main challenge for supervised learning will be whether you have labels for the data. </p>

<blockquote>
  <p>If Yes, are classic approaches like multinomial naive bayes or SVM suitable?</p>
</blockquote>

<p>They will work as well as they always have, though these days building a vector representation is probably a better choice.</p>

<blockquote>
  <p>If i want to improve the first model in case it is not performing well, and use unsupervised multilingual emdedding to build a classifier. how can i train this classifier on the numerical labels later?</p>
</blockquote>

<p>Assuming the numerical labels are labels on the original data, you can add them as tokens like LABEL001 and the model can learn representations of them if you want to make an unsupervised recommender.</p>

<hr>

<p>Honestly these days I wouldn't start with Naive Bayes or classical models, I'd go straight to word vectors as a first test for clustering. Using fasttext or word2vec is pretty straightforward. The main problem is that if you really only have 40 characters per item, that just might not be enough data to cluster usefully.</p>
",1,1,198,2020-05-06 15:19:32,https://stackoverflow.com/questions/61638870/multilingual-free-text-items-text-classification-for-improving-a-recommender-sys
what is the difference between text classification and feature selection,"<p>can we select features without classification and if I have a text how can i know which are the features to choose? I need example regarding text not real word object example. if anyone can explain please? </p>
","machine-learning, text-classification, feature-selection","<p>Text Classification is classifying the text based on its features. For example, you might classify a sentence as having a positive (""I am so happy"") or negative (""I am so sad"") sentiment.</p>

<p>Text Feature selection is effectively deciding how you want to encode the text so you can run it through the classifier. There are many ways of doing this. For example, you could use a bag of words representation, where each column represents a word in your vocabulary and each cell represents how many times the word appears in the document. </p>

<p>If you had two sentences, ""I am so happy, so very happy"" and ""I am so sad"", your encoding for the sentences might be</p>

<p>| I || am | so | happy | very | sad |</p>

<ol>
<li><ol>
<li><ol start=""2"">
<li><ol start=""2"">
<li><ol>
<li><ol>
0. 
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
1. 
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol>
",2,0,238,2020-05-09 19:05:10,https://stackoverflow.com/questions/61702235/what-is-the-difference-between-text-classification-and-feature-selection
Sklearn text classification: Why is accuracy so low?,"<p>Alright, Im following <a href=""https://medium.com/@phylypo/text-classification-with-scikit-learn-on-khmer-documents-1a395317d195"" rel=""nofollow noreferrer"">https://medium.com/@phylypo/text-classification-with-scikit-learn-on-khmer-documents-1a395317d195</a> and <a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a> trying to classify text based on category. My dataframe is laid out like this and named <code>result</code>:</p>

<pre><code>target   type    post
1      intj    ""hello world shdjd""
2      entp    ""hello world fddf""
16     estj   ""hello world dsd""
4      esfp    ""hello world sfs""
1      intj    ""hello world ddfd""
</code></pre>

<p>The goal would be to categorize a post by its type, and target just assigns number 1-16 to each of the 16 types. To classify the text I do this:</p>

<pre><code>result = result[:1000] #shorten df - was :600

# split the dataset into training and validation datasets
train_x, valid_x, train_y, valid_y = model_selection.train_test_split(result['post'], result['type'], test_size=0.30, random_state=1)

# label encode the target variable
encoder = preprocessing.LabelEncoder()
train_y = encoder.fit_transform(train_y)
valid_y = encoder.fit_transform(valid_y)

def tokenizersplit(str):
    return str.split()
tfidf_vect = TfidfVectorizer(tokenizer=tokenizersplit, encoding='utf-8', min_df=2, ngram_range=(1, 2), max_features=25000)

tfidf_vect.fit(result['post'])
tfidf_vect.transform(result['post'])

xtrain_tfidf = tfidf_vect.transform(train_x)
xvalid_tfidf = tfidf_vect.transform(valid_x)

def train_model(classifier, trains, t_labels, valids, v_labels):
    # fit the training dataset on the classifier
    classifier.fit(trains, t_labels)

    # predict the labels on validation dataset
    predictions = classifier.predict(valids)

    return metrics.accuracy_score(predictions, v_labels)

# Naive Bayes
accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)
print (""NB accuracy: "", accuracy)

# Logistic Regression
accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)
print (""LR accuracy: "", accuracy)
</code></pre>

<p>And depending on how much I shorten result in the beginning, accuracy peaks at around 0.4 for all algorithms. It is supposed to be 0.8-0.9.</p>

<p>I read <a href=""https://stackoverflow.com/questions/39167586/scikit-very-low-accuracy-on-classifiersnaive-bayes-decissiontreeclassifier"">scikit very low accuracy on classifiers(Naive Bayes, DecissionTreeClassifier)</a> but dont see how to apply it to my dataframe. My data is simple - has category (<code>type</code>) and text (<code>post</code>). </p>

<p>What is wrong here?</p>

<p>EDIT - naive bayes take 2:</p>

<pre><code>text_clf = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', MultinomialNB()),
])
text_clf.fit(result.post, result.target)

docs_test = result.post
predicted = text_clf.predict(docs_test)
np.mean(predicted == result.target)

print(""Naive Bayes: "")
print(np.mean(predicted == result.target))
</code></pre>
","python, machine-learning, scikit-learn, text-classification","<h3>What you are doing</h3>

<p>The mistake I believe is in these lines:</p>

<pre><code>encoder = preprocessing.LabelEncoder()
train_y = encoder.fit_transform(train_y)
valid_y = encoder.fit_transform(valid_y)
</code></pre>

<p>By fitting two times you reset the knowledge of the <code>LabelEncoder</code>.<br>
In a more simple example:</p>

<pre><code>from sklearn import preprocessing

le = preprocessing.LabelEncoder()
y_train = le.fit_transform([""class1"", ""class2"", ""class3""])
y_valid = le.fit_transform([""class2"", ""class3""])
print(y_train)
print(y_valid)
</code></pre>

<p>Outputs these label encodings:</p>

<pre><code>[0 1 2]
[0 1]
</code></pre>

<p>This is wrong since the encoded label <code>0</code> is <code>class1</code> for the training and <code>class2</code> for the validation.</p>

<h3>Fix</h3>

<p>I would change your first lines to:</p>

<pre><code>result = result[:1000] #shorten df - was :600

# Encode the labels before splitting
encoder = preprocessing.LabelEncoder()
y_encoded = encoder.fit_transform(result['type'])

# CARE that I changed the target from result['type'] to y_encoded
train_x, valid_x, train_y, valid_y = model_selection.train_test_split(result['post'], y_encoded, test_size=0.30, random_state=1)

def tokenizersplit(str):
    return str.split()

.
.
.
</code></pre>
",1,0,1166,2020-05-09 21:24:47,https://stackoverflow.com/questions/61703947/sklearn-text-classification-why-is-accuracy-so-low
Predicting new content for text-clustering using sklearn,"<p>I am trying to understand how to create clustering of texts using sklearn. I have 800 hundred texts (600 training data and 200 test data) like the following: </p>

<pre><code>Texts # columns name

  1 Donald Trump, Donald Trump news, Trump bleach, Trump injected bleach, bleach coronavirus.
  2 Thank you Janey.......laughing so much at this........you have saved my sanity in these mad times. Only bleach Trump is using is on his heed 🤣
  3 His more uncharitable critics said Trump had suggested that Americans drink bleach. Trump responded that he was being sarcastic.
  4 Outcry after Trump suggests injecting disinfectant as treatment.
  5 Trump Suggested 'Injecting' Disinfectant to Cure Coronavirus?
  6 The study also showed that bleach and isopropyl alcohol killed the virus in saliva or respiratory fluids in a matter of minutes.
</code></pre>

<p>and I would like create clusters from those. 
To transform the corpus into vector space I have used <code>tf-idf</code> and to cluster the documents using the k-means algorithm. 
However, I cannot understand if the results are those expected or not as unfortunately the output is not 'graphical' (I have tried to use CountVectorizer to have a matrix of frequency, but probably I am using it in the wrong way). 
What I would expect by doing tf-idf is that when I test the test dataset
When I TEST:</p>

<p>test_dataset = [""'Please don't inject bleach': Trump's wild coronavirus claims prompt disbelief."", ""Donald Trump has won the shock and ire of the scientific and medical communities after suggesting bogus treatments for Covid-19"", ""Bleach manufacturers have warned people not to inject themselves with disinfectant after Trump falsely suggested it might cure the coronavirus.""]</p>

<p>(the test dataset comes from the column <code>df[""0""]['Names']</code>)
I would like to see which cluster(made by k-means) the texts belongs to.
Please see below the code that I am currently using: </p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import re
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
from nltk.corpus import stopwords

stop_words = stopwords.words('english')

def preprocessing(line):
    line = re.sub(r""[^a-zA-Z]"", "" "", line.lower())
    words = word_tokenize(line)
    words_lemmed = [WordNetLemmatizer().lemmatize(w) for w in words if w not in stop_words]
    return words_lemmed

tfidf_vectorizer = TfidfVectorizer(tokenizer=preprocessing)
vec = CountVectorizer()

tfidf = tfidf_vectorizer.fit_transform(df[""0""]['Names'])
matrix = vec.fit_transform(df[""0""]['Names'])

kmeans = KMeans(n_clusters=2).fit(tfidf)
pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())
</code></pre>

<p>where <code>df[""0""]['Names']</code> is the column '<code>Names</code>' of the <code>0th</code> dataframe.
A visual example, even with a different dataset but pretty same structure of dataframe (just for a better understanding) would be also good, if you prefer. </p>

<p>All the help you will provide will be greatly appreciated. Thanks </p>
","python, scikit-learn, text-classification, tf-idf, tfidfvectorizer","<p>taking your test_data and adding three more sentence to make corpus </p>

<pre><code>train_data = [""'Please don't inject bleach': Trump's wild coronavirus claims prompt disbelief."",
              ""Donald Trump has won the shock and ire of the scientific and medical communities after suggesting bogus treatments for Covid-19"", 
              ""Bleach manufacturers have warned people not to inject themselves with disinfectant after Trump falsely suggested it might cure the coronavirus."",
              ""find the most representative document for each topic"",
              ""topic distribution across documents"",
               ""to help with understanding the topic"",
                ""one of the practical application of topic modeling is to determine""]
</code></pre>

<p>creating dataframe from above dataset</p>

<pre><code> df = pd.DataFrame(train_data, columns = 'text')
</code></pre>

<p>now you can use either Countvectorizer or TfidfVectorizer for vectorizing text, i am using TfidfVectorizer</p>

<pre><code> vect = TfidfVectorizer(tokenizer=preprocessing)

 vectorized_text = vect.fit_transform(df['text'])

 kmeans = KMeans(n_clusters=2).fit(vectorized_text)

 # now predicting the cluster for given dataset

df['predicted cluster'] = kmeans.predict(vectorized_text)
</code></pre>

<p><a href=""https://i.sstatic.net/rDLyB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rDLyB.png"" alt=""enter image description here""></a></p>

<p>Now, when you are going to predict for test data or new data</p>

<pre><code>new_sent = 'coronavirus has created lot of problem in the world'
kmeans.predict(vect.transform([new_sent])) #you have to use transform only and not fit_transfrom 

#op
array([1])
</code></pre>
",2,0,948,2020-05-11 22:39:28,https://stackoverflow.com/questions/61740634/predicting-new-content-for-text-clustering-using-sklearn
"Excellent performances on training test, bad on test set","<p>I'm doing text classification and I'm dealing with weird results. I have two datasets, one labeled and the other one unlabeled. When I use some classifiers (SVM, Naive Bayes, knn, Random Forest, Gradient Boosting) on the labeled one I have excellent performances, even without tuning, with all the classifiers (more than 98% of BAC), but when I try to predict results on the unlabeled dataset I have very different predictions for every classifier. I used TF-IDF as vectorizer and I tried to use also bigrams and trigrams but nothing has changed. I tried also to create different new observations using SMOTE (even if I don't have problems with imbalanced dataset) just in order to see if, with new observations, algorithms would generalize better with new data but, even in this case, nothing has changed. What can I do in order to resolve this problem? Why is this happening? Do you have any idea?</p>
","data-science, text-classification, tfidfvectorizer","<p>Hi and welcome to the forum!</p>

<p>I can think of 3 possibilities (which are in fact slightly overlapping):</p>

<ol>
<li><p>Are you splitting the labeled dataset in a training and a validation set? Maybe you are suffering from the horrible-sounding <a href=""https://machinelearningmastery.com/data-leakage-machine-learning/"" rel=""nofollow noreferrer"">data leakage</a>. Basically, data from the validation set is somehow leaking into the training data, so the model knows more than it should. It's more common than you think.</p></li>
<li><p>Maybe you are <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow noreferrer"">overfitting</a> the training set. Basically, the model memorizes the training data and doesn't generalize very well. You can try stopping the training at an earlier stage. </p></li>
<li><p>The ditribution of the training data and the test data are not similar enough to the model to generalize well. You can try reshuffling them and separating again. A basic thing you can also try to understand the similarity of the datasets is check the distribution of classes among the training and test data, but more complex and useful techniques exist.</p></li>
</ol>

<p>Some of these more complex techniques to compare the training and the test data are:</p>

<ul>
<li><p>Checking whether a classifier can correctly decide whether a datapoint belongs to the train or the test set. This shouldn't be possible if the distributions were similar enough. <a href=""https://towardsdatascience.com/why-you-may-be-getting-low-test-accuracy-try-this-quick-way-of-comparing-the-distribution-of-the-9f06f5a72cfc"" rel=""nofollow noreferrer"">Here</a>'s a tutorial in Python.</p></li>
<li><p>Using the Kolmogorov-Smirnov test (<a href=""https://medium.com/@billy.mosse/why-you-may-be-getting-low-test-accuracy-try-this-simpstatistical-tests-30585b7ee4fa"" rel=""nofollow noreferrer"">another tutorial in Python</a>). scipy.stats implements it: see <a href=""https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ks_2samp.html"" rel=""nofollow noreferrer"">stats.ks_2samp</a>. Beware: this test must be applied to each column separately, so it doesn't work if you are working with, for example, NLP's word embeddings.</p></li>
<li><p>If you are indeed working with word embeddings, you should use the classifier described in the first bullet point or transform your data so it is unidimensional. A simple example is to compute the norm of the word embeddings, but that doesn't quite do the job. The Mahalonobis (also implented by SciPy) distance works a bit better - see the bottom of last bullet point's <a href=""https://medium.com/@billy.mosse/why-you-may-be-getting-low-test-accuracy-try-this-simpstatistical-tests-30585b7ee4fa"" rel=""nofollow noreferrer"">tutorial</a> for details.</p></li>
</ul>
",3,2,125,2020-05-12 14:14:00,https://stackoverflow.com/questions/61754014/excellent-performances-on-training-test-bad-on-test-set
Using fastText Sentence Vector as an Input Feature,"<p>I want to use the fastText Sentence Vector as an input Feature. </p>

<pre><code>vector = model.get_sentence_vector('Original Sentence')
</code></pre>

<p>I am attempting to perform Binary Classification of sentences using MLPs and will train the algorithm using the fixed sized feature generated by the above code. Is this a plausible thing to do?</p>
","text-classification, fasttext, mlp","<p>You can take the mean of the word embeddings, i.e., tokenize the sentence, look up embeddings for all words computing an average. In this way, you will get a NumPy array that you can use as an input to whatever classifier you want. Depending on the classification task, it might be useful to remove function words first.</p>

<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">Gensim</a> has a richer Python API than FastText itself. If you just want to quickly train a classifier, the best option is using the command line interface of <a href=""https://github.com/facebookresearch/fastText#text-classification"" rel=""nofollow noreferrer"">FastText</a>.</p>
",1,0,1428,2020-05-13 07:39:12,https://stackoverflow.com/questions/61768902/using-fasttext-sentence-vector-as-an-input-feature
FastText 0.9.2 - why is recall &#39;nan&#39;?,"<p>I trained a supervised model in FastText using the Python interface and I'm getting weird results for precision and recall.</p>
<p>First, I trained a model:</p>
<pre class=""lang-py prettyprint-override""><code>model = fasttext.train_supervised(&quot;train.txt&quot;, wordNgrams=3, epoch=100, pretrainedVectors=pretrained_model)
</code></pre>
<p>Then I get results for the test data:</p>
<pre class=""lang-py prettyprint-override""><code>def print_results(N, p, r):
    print(&quot;N\t&quot; + str(N))
    print(&quot;P@{}\t{:.3f}&quot;.format(1, p))
    print(&quot;R@{}\t{:.3f}&quot;.format(1, r))

print_results(*model.test('test.txt'))
</code></pre>
<p>But the results are always odd, because they show precision and recall @1 as identical, even for different datasets, e.g. one output is:</p>
<pre><code>N   46425
P@1 0.917
R@1 0.917
</code></pre>
<p>Then when I look for the precision and recall for each label, I always get recall as 'nan':</p>
<pre class=""lang-py prettyprint-override""><code>print(model.test_label('test.txt'))
</code></pre>
<p>And the output is:</p>
<pre><code>{'__label__1': {'precision': 0.9202150724134941, 'recall': nan, 'f1score': 1.8404301448269882}, '__label__5': {'precision': 0.9134956983264135, 'recall': nan, 'f1score': 1.826991396652827}}
</code></pre>
<p>Does anyone know why this might be happening?</p>
<p>P.S.: To try a reproducible example of this behavior, please refer to <a href=""https://github.com/facebookresearch/fastText/issues/1072"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/issues/1072</a> and run it with FastText 0.9.2</p>
","python-3.x, nlp, text-classification, precision-recall, fasttext","<p>It looks like FastText 0.9.2 has a bug in the computation of recall, and that should be fixed with <a href=""https://github.com/facebookresearch/fastText/commit/b64e359d5485dda4b4b5074494155d18e25c8d13"" rel=""noreferrer"">this commit</a>.</p>

<p>Installing a ""bleeding edge"" version of FastText e.g. with</p>

<pre><code>pip install git+https://github.com/facebookresearch/fastText.git@b64e359d5485dda4b4b5074494155d18e25c8d13 --quiet
</code></pre>

<p>and rerunning your code should allow to get rid of the <code>nan</code> values in the recall computation.</p>
",6,5,2125,2020-05-14 00:21:25,https://stackoverflow.com/questions/61787119/fasttext-0-9-2-why-is-recall-nan
Spacy&#39;s BERT model doesn&#39;t learn,"<p>I've been trying to use <code>spaCy</code>'s pretrained BERT model <code>de_trf_bertbasecased_lg</code> to increase accuracy in my classification project. I used to build a model from scratch using <code>de_core_news_sm</code> and everything worked fine: I had an accuracy around 70%. But now I am using BERT pretrained model instead and I'm getting 0% accuracy. I don't believe that it's working so bad, so I'm assuming that there is just a problem with my code. I might have missed something important but I can't figure out what. I used the code in <a href=""https://explosion.ai/blog/spacy-transformers"" rel=""nofollow noreferrer"">this article</a> as an example.</p>

<p>Here is my code:</p>

<pre><code>import spacy
from spacy.util import minibatch
from random import shuffle

spacy.require_gpu()
nlp = spacy.load('de_trf_bertbasecased_lg')

data = get_data()  # get_data() function returns a list with train data (I'll explain later how it looks)

textcat = nlp.create_pipe(""trf_textcat"", config={""exclusive_classes"": False})

for category in categories:  # categories - a list of 21 different categories used for classification
    textcat.add_label(category)
nlp.add_pipe(textcat)

num = 0  # number used for counting batches
optimizer = nlp.resume_training()
for i in range(2):
    shuffle(data)
    losses = {}
    for batch in minibatch(data):
        texts, cats = zip(*batch)
        nlp.update(texts, cats, sgd=optimizer, losses=losses)
        num += 1

        if num % 10000 == 0:  # test model's performance every 10000 batches
            acc = test(nlp)  # function test() will be explained later
            print(f'Accuracy: {acc}')

nlp.to_disk('model/')
</code></pre>

<p>Function <code>get_data()</code> opens files with different categories, creates a tuple like this one <code>(text, {'cats' : {'category1': 0, 'category2':1, ...}})</code>, gathers all these tuples into one array, which is then being returned to the main function.</p>

<p>Function <code>test(nlp)</code> opens the file with test data, predicts categories for each line in the file and checks, whether the prediction was correct.</p>

<p>Again, everything worked just fine with <code>de_core_news_sm</code>, so I'm pretty sure that functions <code>get_data()</code> and <code>test(nlp)</code> are working fine. Code above looks like in example but still 0% accuracy.I don't understand what I'm doing wrong.</p>

<p>Thanks in advance for any help!</p>

<p><strong>UPDATE</strong></p>

<p>Trying to understand the above problem I decided to try the model with only a few examples (just like it is advised <a href=""https://github.com/explosion/spacy-transformers/issues/144"" rel=""nofollow noreferrer"">here</a>). Here is the code:</p>

<pre><code>import spacy
from spacy.util import minibatch
import random
import torch

train_data = [
    (""It is realy cool"", {""cats"": {""POSITIVE"": 1.0, ""NEGATIVE"": 0.0}}),
    (""I hate it"", {""cats"": {""POSITIVE"": 0.0, ""NEGATIVE"": 1.0}})
]

is_using_gpu = spacy.prefer_gpu()
if is_using_gpu:
    torch.set_default_tensor_type(""torch.cuda.FloatTensor"")

nlp = spacy.load(""en_trf_bertbaseuncased_lg"")
textcat = nlp.create_pipe(""trf_textcat"", config={""exclusive_classes"": True})
for label in (""POSITIVE"", ""NEGATIVE""):
    textcat.add_label(label)
nlp.add_pipe(textcat)

optimizer = nlp.resume_training()
for i in range(10):
    random.shuffle(train_data)
    losses = {}
    for batch in minibatch(train_data):
        texts, cats = zip(*batch)
        nlp.update(texts, cats, sgd=optimizer, losses=losses)
    print(i, losses)
print()

test_data = [
    ""It is really cool"",
    ""I hate it"",
    ""Great!"",
    ""I do not think this is cool""
]

for line in test_data:
    print(line)
    print(nlp(line).cats)
</code></pre>

<p>And the output was:</p>

<pre><code>0 {'trf_textcat': 0.125}
1 {'trf_textcat': 0.12423406541347504}
2 {'trf_textcat': 0.12188033014535904}
3 {'trf_textcat': 0.12363225221633911}
4 {'trf_textcat': 0.11996611207723618}
5 {'trf_textcat': 0.14696261286735535}
6 {'trf_textcat': 0.12320466339588165}
7 {'trf_textcat': 0.12096124142408371}
8 {'trf_textcat': 0.15916231274604797}
9 {'trf_textcat': 0.1238454058766365}

It is really cool
{'POSITIVE': 0.47827497124671936, 'NEGATIVE': 0.5217249989509583}
I hate it
{'POSITIVE': 0.47827598452568054, 'NEGATIVE': 0.5217240452766418}
Great!
{'POSITIVE': 0.4782750606536865, 'NEGATIVE': 0.5217249393463135}
I do not think this is cool
{'POSITIVE': 0.478275328874588, 'NEGATIVE': 0.5217246413230896}
</code></pre>

<p>Not only the model performs bad, the loss is not getting smaller and scores for all the test sentences are almost the same. And most importantly: it didn't even get those questions correct, that happened to be in the train data. So my question is: does the model even learn? And what am I doing wrong?</p>

<p>Any thoughts?</p>
","python, spacy, text-classification, multiclass-classification, bert-language-model","<p>Received an answer to my question on <a href=""https://github.com/explosion/spacy-transformers/issues/180"" rel=""nofollow noreferrer"">GitHub</a> and it looks like there must be some optimizer parameters specified, just like in <a href=""https://github.com/explosion/spacy-transformers/blob/v0.6.x/examples/train_textcat.py"" rel=""nofollow noreferrer"">this example</a>.</p>
",1,2,944,2020-05-21 20:35:55,https://stackoverflow.com/questions/61943409/spacys-bert-model-doesnt-learn
How to improve a German text classification model in spaCy,"<p>I am working on a text classification project and using <code>spacy</code> for this. Right now I have an accuracy equal to almost 70% but that is not enough. I've been trying to improve the model for past two weeks, but no successful results so far. And here I am looking for an advice about what I should do or try. Any help would be highly appreciated! </p>

<p>So, here is what I do so far:</p>

<blockquote>
  <p>1) Preparing the data:</p>
</blockquote>

<p>I have an unbalanced dataset of German news with 21 categories (like <code>POLITICS</code>, <code>ECONOMY</code>, <code>SPORT</code>, <code>CELEBRITIES</code> etc). In order to make categories equal I duplicate small classes. As a result I have 21 files with almost <code>700 000</code> lines of text. I then normalize this data using the following code:</p>

<pre><code>import spacy
from charsplit import Splitter

POS = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'NUM']  # allowed parts of speech

nlp_helper = spacy.load('de_core_news_sm')
splitter = Splitter()

def normalizer(texts):
    arr = []  # list of normalized texts (will be returned from the function as a result of normalization)

    docs = nlp_helper.pipe(texts)  # creating doc objects for multiple lines
    for doc in docs:  # iterating through each doc object
        text = []  # list of words in normalized text
        for token in doc:  # for each word in text
            token = token.lemma_.lower()

            if token not in stop_words and token.pos_ in POS:  # deleting stop words and some parts of speech
                if len(word) &gt; 8 and token.pos_ == 'NOUN':  # only nouns can be splitted
                    _, word1, word2 = splitter.split_compound(word)[0]  # checking only the division with highest prob
                    word1 = word1.lower()
                    word2 = word2.lower()
                    if word1 in german and word2 in german:
                        text.append(word1)
                        text.append(word2)
                    elif word1[:-1] in german and word2 in german:  # word[:-1] - checking for 's' that joins two words
                        text.append(word1[:-1])
                        text.append(word2)
                    else:
                        text.append(word)
                else:
                    text.append(word)
        arr.append(re.sub(r'[.,;:?!""()-=_+*&amp;^@/\']', ' ', ' '.join(text))) # delete punctuation
    return arr
</code></pre>

<p>Some explanations to the above code:</p>

<p><code>POS</code> - a list of allowed parts of speech. If the word I'm working with at the moment is a part of speech that is not in this list -> I delete it.</p>

<p><code>stop_words</code> - just a list of words I delete.</p>

<p><code>splitter.split_compound(word)[0]</code> - returns a tuple with the most likely division of the compound word (I use it to divide long German words into shorter and more widely used). Here is the <a href=""https://github.com/dtuggener/CharSplit"" rel=""nofollow noreferrer"">link</a> to the repository with this functionality.</p>

<p>To sum up: I find the lemma of the word, make it lower case, delete stop words and some parts of speech, divide compound words, delete punctuation. I then join all the words and return an array of normalized lines.</p>

<blockquote>
  <p>2) Training the model</p>
</blockquote>

<p>I train my model using <code>de_core_news_sm</code> (to make it possible in the future to use this model not only for classification but also for normalization). Here is the code for training:</p>

<pre><code>nlp = spacy.load('de_core_news_sm')

textcat = nlp.create_pipe('textcat', config={""exclusive_classes"": False, ""architecture"": 'simple_cnn'})
nlp.add_pipe(textcat, last=True)
for category in categories:
    textcat.add_label(category)

pipe_exceptions = [""textcat""]
other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()

    for i in range(n_iter):
        shuffle(data)
        batches = spacy.util.minibatch(data)

        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer, drop=0.25)
</code></pre>

<p>Some explanations to the above code:</p>

<p><code>data</code> - list of lists, where each list includes a line of text and a dictionary with categories (just like in the <a href=""https://spacy.io/usage/training#textcat"" rel=""nofollow noreferrer"">docs</a>)</p>

<p>'categories' - list of categories</p>

<p>'n_iter' - number of iterations for training</p>

<blockquote>
  <p>3) At the end I just save the model with <code>to_disk</code> method.</p>
</blockquote>

<p>With the above code I managed to train a model with 70% accuracy. Here is a list of what I've tried so far to improve this score:</p>

<blockquote>
  <p>1) Using another architecture (<code>ensemble</code>) - didn't give any improvements</p>
  
  <p>2) Training on non normalized data - the result was much worse </p>
  
  <p>3) Using pretrained BERT model - could'n do it (<a href=""https://stackoverflow.com/questions/61943409/spacys-bert-model-doesnt-learn"">here</a> is my unanswered question about it)</p>
  
  <p>4) Training <code>de_core_news_md</code> instead of <code>de_core_news_sm</code> - didn't give any improvements (tried it because according to the <a href=""https://spacy.io/usage/vectors-similarity#basics"" rel=""nofollow noreferrer"">docs</a> there could be an improvement thanks to the vectors (if I understood it correctly). Correct me if I'm wrong)</p>
  
  <p>5) Training on data, normalized in a slightly different way (without lower casing and punctuation deletion) - didn't give any improvements</p>
  
  <p>6) Changing dropout - didn't help</p>
</blockquote>

<p>So right now I am a little stuck about what to do next. I would be very grateful for any hint or advice.</p>

<p>Thanks in advance for your help!</p>
","python, nlp, spacy, text-classification","<p>The first thing I would suggest is increasing your batch size. After that your optimizer (Adam if possible) and learning rate for which I don't see the code here. You can finally try changing your dropout.</p>

<p>Also, if you are trying neural networks and plan on changing a lot, it would be better if you could switch to PyTorch or TensorFlow. In PyTorch, you will have HuggingFace library, which has BERT in-built in it.</p>

<p>Hope this helps you!</p>
",2,1,1254,2020-05-28 21:57:26,https://stackoverflow.com/questions/62075223/how-to-improve-a-german-text-classification-model-in-spacy
sklearn how to use saved model to predict new data,"<p>I use sklearn trained a SVM text classifier, used tf-idf(TfidfVectorizer) to extract the feature.
now I need to save the model and load it to predict the text unseen. I will load the model in another file,  what confuses me is how to extract the new text tf-idf feature</p>
","machine-learning, scikit-learn, svm, text-classification","<p>You need to save the model AND the tfidf transformer. You can either save them separately, or create a pipeline of the two and save the pipeline (this is the preferred option).</p>

<p>Example:</p>

<pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
import pickle

Tfidf = TfidfVectorizer()
LR = LogisticRegression()
pipe = Pipeline([(""Tfidf"", Tfidf), (""LR"", LR)])
pipe.fit(X, y)

with open('pipe.pickle', 'wb') as picklefile:
    pickle.dump(pipe, picklefile)
</code></pre>

<p>You can then load the whole pipeline which upon <code>predict</code> will first apply the vectorizer and then pass it to the model:</p>

<pre><code>with open('pipe.pickle', 'rb') as picklefile:
    saved_pipe = pickle.load(picklefile)

saved_pipe.predict(X_test)
</code></pre>
",4,-2,1597,2020-06-01 09:13:45,https://stackoverflow.com/questions/62129025/sklearn-how-to-use-saved-model-to-predict-new-data
Alternative to TfidfVectorizer,"<p>Is there any alternative to <code>TfidfVectorizer</code> function of <code>sklearn.feature_extraction.text</code> module? I've heard of fastText and GloVe, but couldn't find a good expalnation of how to use it to vectorize text.</p>

<p><strong>Edit</strong>: Basically I've a feature called narration, which consists of English sentences. In order to feed this into any ML algorithm I've to convert it into a numeric matrix representation. TfIdf was one way. Is there any other way that I can try out? (May or may not be under sklearn)</p>
","python, machine-learning, text-classification","<p>What you are looking for is called text embedding, see for example <a href=""https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526"" rel=""nofollow noreferrer"">this</a>. Essentially for your naration feature, you are looking to turn a sequence into vectors, hence seq_to_vec. TfIdf is just one of the simplest ways of doing this, which yields a sparse (many more components are =0, than not). I suggest you look <a href=""https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html"" rel=""nofollow noreferrer"">here</a> for a good starting point.</p>
",2,1,2871,2020-06-02 05:42:24,https://stackoverflow.com/questions/62145587/alternative-to-tfidfvectorizer
&#39;numpy.ndarray&#39; object has no attribute &#39;lower&#39;,"<p>I am fairly new to ML, I am trying to fit some data on my NB-classifier.</p>

<pre><code>from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC

# Naïve Bayes:
text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', MultinomialNB()),
])

# Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', LinearSVC()),
])
</code></pre>

<p>Code for fitting the data: </p>

<pre><code>text_clf_nb.fit(X_train, y_train)
</code></pre>

<p>The shape of my training &amp; test data is </p>

<pre><code>X_train.shape, X_test.shape, y_train.shape, y_test.shape : ((169, 1), (84,), (169, 1), (84,))
</code></pre>

<p>But keep getting :<code>'numpy.ndarray' object has no attribute 'lower'</code></p>

<p>Here is full trace to the error:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-57-139757126594&gt; in &lt;module&gt;
----&gt; 1 text_clf_nb.fit(X_train, y_train)

~\miniconda3\envs\nlp_course\lib\site-packages\sklearn\pipeline.py in fit(self, X, y, **fit_params)
    263             This estimator
    264         """"""
--&gt; 265         Xt, fit_params = self._fit(X, y, **fit_params)
    266         if self._final_estimator is not None:
    267             self._final_estimator.fit(Xt, y, **fit_params)

~\miniconda3\envs\nlp_course\lib\site-packages\sklearn\pipeline.py in _fit(self, X, y, **fit_params)
    228                 Xt, fitted_transformer = fit_transform_one_cached(
    229                     cloned_transformer, Xt, y, None,
--&gt; 230                     **fit_params_steps[name])
    231                 # Replace the transformer of the step with the fitted
    232                 # transformer. This is necessary when loading the transformer

~\miniconda3\envs\nlp_course\lib\site-packages\sklearn\externals\joblib\memory.py in __call__(self, *args, **kwargs)
    340 
    341     def __call__(self, *args, **kwargs):
--&gt; 342         return self.func(*args, **kwargs)
    343 
    344     def call_and_shelve(self, *args, **kwargs):
</code></pre>
","numpy, scikit-learn, text-classification, naivebayes, tfidfvectorizer","<p>You have checked the shape of arrays, but have you tried something like:</p>
<pre><code>data = vectorizer.fit_transform(array.ravel())
</code></pre>
<p>This should do the trick for you</p>
",2,1,2353,2020-06-03 12:54:11,https://stackoverflow.com/questions/62173338/numpy-ndarray-object-has-no-attribute-lower
How to use Tf-idf features for training your model?,"<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(sublinear_tf= True, 
                       min_df = 5, 
                       norm= 'l2', 
                       ngram_range= (1,2), 
                       stop_words ='english')

feature1 = tfidf.fit_transform(df.Rejoined_Stem)
array_of_feature = feature1.toarray()
</code></pre>

<p>I used the above code to get features for my text document.</p>

<pre><code>from sklearn.naive_bayes import MultinomialNB # Multinomial Naive Bayes on Lemmatized Text
X_train, X_test, y_train, y_test = train_test_split(df['Rejoined_Lemmatize'], df['Product'], random_state = 0)
X_train_counts = tfidf.fit_transform(X_train)
clf = MultinomialNB().fit(X_train_counts, y_train)
y_pred = clf.predict(tfidf.transform(X_test))
</code></pre>

<p>Then I used this code to train my model. 
Can someone explain how exactly are the above features being used while training the model as that feature1 variable is not being used anywhere while training ??</p>
","machine-learning, scikit-learn, text-classification, naivebayes, tfidfvectorizer","<p>No, you did not use <code>feature1</code> as you performed another transformation <code>X_train_count</code>.</p>

<p>Let’s go through your code in a logical flow and use only the variables that where used in the feature extraction and model training.</p>

<pre class=""lang-py prettyprint-override""><code># imports used
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

# split data random state 0 and test_size 0.25 default as you did not give the test_size

X_train, X_test, y_train, y_test = train_test_split(df[['Rejoined_Lemmatize']], df['Product'], random_state = 0)

# you initiated your transformer to `fit_transform` X_train, and `transform` X_test

tfidf = TfidfVectorizer(sublinear_tf= True, 
                       min_df = 5, 
                       norm= 'l2', 
                       ngram_range= (1,2), 
                       stop_words ='english')


X_train_counts = tfidf.fit_transform(X_train)
X_test_counts = tfidf.transform(X_test)

# you initiated your model and fit X_train_counts and y_train
clf = MultinomialNB()
cls.fit(X_train_counts, y_train)

# you predicted from your transformed features
y_pred = clf.predict(X_test_counts)
</code></pre>

<p>There is a better way to use Scikit-learn API which eliminates confusion and will help you not get mixed up. That way uses <code>Pipelines</code></p>

<pre class=""lang-py prettyprint-override""><code># imports used: see Pipeline
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

# split data random state 0 and test_size 0.25 default as you did not give the test_size

X_train, X_test, y_train, y_test = train_test_split(df[['Rejoined_Lemmatize']], df['Product'], random_state = 0)

# get the params
tfidf_params = dict(sublinear_tf= True, 
                       min_df = 5, 
                       norm= 'l2', 
                       ngram_range= (1,2), 
                       stop_words ='english')

# create a Pipeline that will do features transformation then pass to the model

clf = Pipeline(steps=[
('features', TfidfVectorizer(**tfidf_params)),
('model', MultinomialNB())
])

# Use clf as a model, fit X_train and y_train
cls.fit(X_train, y_train)

# predicted 
y_pred = clf.predict(X_test)
</code></pre>

<p>What pipeline does, in <code>.fit</code> is doing the fit_transform on the data, and then pass it to the model. In <code>.predict</code>, it will do the transform before passing to the model. </p>

<p>The best the thing about this approach is that you can easily switch models or transformer with ease. Here is an example on baseline comparison of models:</p>

<pre class=""lang-py prettyprint-override""><code># collection to store results 
from collections import defaultdict
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# models to test
from sklearn.linear_model import PassiveAggressiveClassifier 
from sklearn.linear_model import RidgeClassifierCV
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegressionCV     


# insistent our storage 
bench_mark = defaultdict(list)

# split data random state 0 and test_size 0.25 default as you did not give the test_size

X_train, X_test, y_train, y_test = train_test_split(df[['Rejoined_Lemmatize']], df['Product'], random_state = 0)

# get the transformer params
tfidf_params = dict(sublinear_tf= True, 
                       min_df = 5, 
                       norm= 'l2', 
                       ngram_range= (1,2), 
                       stop_words ='english')

# list of models we would like to complete 
models = [
PassiveAggressiveClassifier(C=1e-1,max_iter=1e3,  tol=1e3), 
RidgeClassifierCV(scoring='roc_auc', cv=10),
LogisticRegressionCV(cv=5,solver='saga',scoring='accuracy', random_state=1, n_jobs=-1),
SGDClassifier(loss='log', random_state=1, max_iter=101),
 ]

# train, test and store each model 
for model in models:

    # our pipeline is changed to accept model
    clf = Pipeline(steps=[
        ('features', TfidfVectorizer(**tfidf_params)),
        ('model', model) #just model not model() as we have done that in models list
    ])

    clf.fit(X_train,y_train)
     score = clf.score(X_test,y_test)

    model_name = clf.named_steps['model'].__class__.__name__ # hack to get name

    model_params = clf.named_steps['model']. get_params()


    print(f'{model_name} Scored: {score:.3f}\n')

    bench_mark['model_name'].append(model_name)
    bench_mark['score'].append(score)
    bench_mark['model'].append(clf)
    bench_mark['used_params'].append(model_params)

# in the end, place the bench_mark to DataFrame
models_df = pd.DataFrame(bench_mark)

# now you have the trained modes in DataFrame, their scores and parameters. 
#You can access and use any model.

logistic_reg = models_df[models_df['model_name']=='LogisticRegressionCV']['model'].iloc[0]

y_preds = logistic_reg.predict(X_test)
</code></pre>

<p>Hopes this helps</p>
",10,1,5735,2020-06-03 17:19:35,https://stackoverflow.com/questions/62178829/how-to-use-tf-idf-features-for-training-your-model
Removing stop-words and selecting only names in pandas,"<p>I'm trying to extract top words by date as follows: </p>

<pre><code>df.set_index('Publishing_Date').Quotes.str.lower().str.extractall(r'(\w+)')[0].groupby('Publishing_Date').value_counts().groupby('Publishing_Date')
</code></pre>

<p>in the following dataframe:</p>

<pre><code>import pandas as pd 

# initialize 
data = [['20/05', ""So many books, so little time."" ], ['20/05', ""The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid."" ], ['19/05', 
""Don't be pushed around by the fears in your mind. Be led by the dreams in your heart.""], ['19/05', ""Be the reason someone smiles. Be the reason someone feels loved and believes in the goodness in people.""], ['19/05', ""Do what is right, not what is easy nor what is popular.""]] 

# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Publishing_Date', 'Quotes']) 
</code></pre>

<p>How you can see, there are many stop-words (<code>""the"", ""an"", ""a"", ""be"", ...</code>), that I would like to remove in order to have a better selection. My aim would be to find some key words, i.e. patterns, in common by date so I would be more interested and focused on names rather than verbs. </p>

<p>Any idea on how I could remove stop-words AND keep only names?</p>

<p>Edit</p>

<p>Expected output (based on the results from Vaibhav Khandelwal's answer below): </p>

<pre><code>Publishing_Date         Quotes       Nouns
  20/05                 ....        books, time, person, gentleman, lady, novel
19/05                   ....        fears, mind, dreams, heart, reason, smiles
</code></pre>

<p>I would need to extract only nouns (reasons should be more frequent so it would be ordered based on frequency).</p>

<p>I think it should be useful <code>nltk.pos_tag</code> where tag is in ('NN'). </p>
","python, regex, pandas, text-classification","<p>This is how you can remove stopwords from your text:</p>

<pre><code>import nltk
from nltk.corpus import stopwords

def remove_stopwords(text):
    stop_words = stopwords.words('english')
    fresh_text = []

    for i in text.lower().split():
        if i not in stop_words:
            fresh_text.append(i)

    return(' '.join(fresh_text))

df['text'] = df['Quotes'].apply(remove_stopwords)
</code></pre>

<p><strong>NOTE: If you want to remove words append explicitly in the stopwords list</strong></p>

<p><a href=""https://i.sstatic.net/lA2e7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lA2e7.png"" alt=""output of the above code""></a></p>

<p>For your other half you can add another function to extract nouns:</p>

<pre><code>def extract_noun(text):
token = nltk.tokenize.word_tokenize(text)
result=[]
for i in nltk.pos_tag(token):
    if i[1].startswith('NN'):
        result.append(i[0])

return(', '.join(result))
</code></pre>

<p>df['NOUN'] = df['text'].apply(extract_noun)</p>

<p>The final output will be as follows:</p>

<p><a href=""https://i.sstatic.net/I09CJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/I09CJ.png"" alt=""The final output after the noun extraction""></a></p>
",1,1,480,2020-06-06 16:16:01,https://stackoverflow.com/questions/62234522/removing-stop-words-and-selecting-only-names-in-pandas
Text preprocessing for text classification using fastText,"<p>What text preprocessing produces the best results for supervised text classification using <a href=""https://github.com/facebookresearch/fastText"" rel=""nofollow noreferrer"">fastText</a>?</p>

<p>The official documentation shows a only a <a href=""https://fasttext.cc/docs/en/supervised-tutorial.html#preprocessing-the-data"" rel=""nofollow noreferrer"">simple prepocessing</a> consisting of lower-casing and separating punctuations. Would classic preprocessing like lemmatization, stopwords removal, masking numbers would help?</p>
","python, nlp, text-classification, fasttext","<p>There is no general answer. It very much depends on what task you are trying to solve, how big data you have, and what language the text is in. Usually, if you have enough data, simple tokenization that you described is all you need.</p>

<p><em>Lemmatization</em>: FastText computes the word embeddings from embeddings of character <em>n</em>-grams, it should cover most morphology in most (at least European) languages, given you don't have very small data. In that case, lemmatization might help.</p>

<p><em>Removing stopwords</em>: It depends on the task. If the task is based on grammar/syntax, you definitely should not remove the stopwords, because they form the grammar. If the task depends more on lexical semantics, removing stopwords should help. If your training data is large enough, the model should learn non-informative stopword embeddings that would not influence the classification.</p>

<p><em>Masking numbers:</em>  If you are sure that your task does not benefit from knowing the numbers, you can mask them out. Usually, the problem is that numbers do not appear frequently in the training data, so you don't learn appropriate weights/embeddings for them. Not so much in FastText which will compose their embeddings from embeddings of their substrings. It will make them probably uninformative at the end, not influencing the classification.</p>
",5,2,3448,2020-06-07 11:12:43,https://stackoverflow.com/questions/62244474/text-preprocessing-for-text-classification-using-fasttext
Text Classification of News Articles Using Spacy,"<p><strong>Dataset</strong> : Csv files containing around <strong>1500</strong> data with columns <strong>(Text,Labels)</strong> where Text is the news article of <strong>Nepali Language</strong> and Label is its genre(Health, World,Tourism, Weather) and so on.</p>

<p>I am using <a href=""https://spacy.io/usage/training#textcat"" rel=""nofollow noreferrer"">Spacy</a> to train my Text Classification Model. So far, I have converted the dataset to a dataframe which looks like this <a href=""https://i.sstatic.net/YYmIu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YYmIu.png"" alt=""enter image description here""></a> 
and then into a spacy acceptable format through the code </p>

<pre><code>dataset['tuples'] = dataset.apply(
    lambda row: (row['Text'],row['Labels']), axis=1)
training_data = dataset['tuples'].tolist()
</code></pre>

<p>which gives me the list of tuples in my training dataset like [('text...','label...'),('text...','label...')]</p>

<p>Now, how can I do text classification here?</p>

<p>In the spacy's documentation, I found</p>

<pre><code>textcat.add_label(""POSITIVE"")
textcat.add_label(""NEGATIVE"")
</code></pre>

<p>Do we have to add the labels according to the labels or should we use positive/negative as well? Does spacy generate the labels according to our dataset after training or not? </p>

<p>Any suggestions please?</p>
","machine-learning, classification, spacy, text-classification, multilabel-classification","<p>You have to add your own labels. So, in your case:</p>

<pre><code>textcat.add_label('Health')
textcat.add_label('World')
textcat.add_label('Tourism')
...
</code></pre>

<p><code>spacy</code> then will be able to predict only those categories, that you added in the above block of code</p>

<p>There is a special format for training data: each element of your list with data is a tuple that contains:</p>

<ol>
<li>Text</li>
<li>Dictionary with one element only. <code>cats</code> is a key and another dictionary is a value. That another dictionary contains all your categories as keys and <code>1</code> or <code>0</code> as values indicating whether this category is correct or not.</li>
</ol>

<p>So, your data should look like this:</p>

<p><code>[('text1', {'cats' : {'category1' : 1, 'category2' : 0, ...}}), 
  ('text2', {'cats' : {'category1' : 0, 'category2' : 1, ...}}), 
  ...]</code></p>
",1,0,1432,2020-06-09 09:12:32,https://stackoverflow.com/questions/62278996/text-classification-of-news-articles-using-spacy
Bert Text Classification Loss is Nan,"<p>I'm try to make an model that classify the text in 3 categories.(Negative,Neural,Positive)</p>

<p>I have csv file that contain comments on different apps with their rating.</p>

<p><strong>First I import all the necessary libraries</strong></p>

<pre><code>!pip install transformers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%tensorflow_version 2.x
import tensorflow as tf

from transformers import TFBertForSequenceClassification, BertTokenizer,DistilBertTokenizer,glue_convert_examples_to_features, InputExample,BertConfig,InputFeatures
from sklearn.model_selection import train_test_split
from tqdm import tqdm


%matplotlib inline
</code></pre>

<p><strong>Then i'll get my csv file</strong></p>

<pre><code>!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV
!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv
df = pd.read_csv(""reviews.csv"")
print(df[['content','score']].head())
                                         content  score
0  Update: After getting a response from the deve...      1
1  Used it for a fair amount of time without any ...      1
2  Your app sucks now!!!!! Used to be good but no...      1
3  It seems OK, but very basic. Recurring tasks n...      1
4  Absolutely worthless. This app runs a prohibit...      1
</code></pre>

<p><strong>Converting scores to sentiment</strong></p>

<pre><code>def to_sentiment(rating):
  rating = int(rating)
  if rating &lt;= 2:
    return 0
  elif rating == 3:
    return 1
  else: 
    return 2

df['sentiment'] = df.score.apply(to_sentiment)

tokenizer = BertTokenizer.from_pretrained('bert-base-cased',do_lower_case = True)
</code></pre>

<p><strong>Creating Helper Methods to fit the data into model</strong></p>

<pre><code>def convert_example_to_feature(review):
  return tokenizer.encode_plus(
            review,
            add_special_tokens=True,
            max_length=160, # truncates if len(s) &gt; max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default
        )

def map_example_to_dict(input_ids,attention_mask,token_type_ids,label):
  return {
      ""input_ids"": input_ids,
      ""attention_mask"": attention_mask,
      ""token_type_ids"" : token_type_ids
  },label

def encode_examples(ds):
  # prepare list, so that we can build up final TensorFlow dataset from slices.
  input_ids_list = []
  token_type_ids_list = []
  attention_mask_list = []
  label_list = []

  for index, row in tqdm(ds.iterrows()):
    bert_input = convert_example_to_feature(row['content'])

    input_ids_list.append(bert_input['input_ids'])
    token_type_ids_list.append(bert_input['token_type_ids'])
    attention_mask_list.append(bert_input['attention_mask'])
    label_list.append([row['sentiment']])
  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)

df_train, df_test = train_test_split(df,test_size=0.1)
</code></pre>

<p><strong>Creating Model</strong></p>

<pre><code>model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)
loss = tf.keras.losses.SparseCategoricalCrossentropy()
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss,metrics=metric)

history = model.fit(ds_train_encoded,epochs=1)
14/443 [..............................] - ETA: 3:58 - loss: nan - accuracy: 0.3438
</code></pre>

<p>If i change the count of the sentiment and make it just positive and negative then it works.
But with 3 or more labels creates this problem.</p>
","python, tensorflow, sentiment-analysis, text-classification, bert-language-model","<p>The label classes index should start from 0 not 1.</p>
<blockquote>
<p>TFBertForSequenceClassification requires labels in the range [0,1,...]</p>
</blockquote>
<blockquote>
<p>labels (tf.Tensor of shape (batch_size,), optional, defaults to None)
– Labels for computing the sequence classification/regression loss.
Indices should be in [0, ..., config.num_labels - 1]. If
config.num_labels == 1 a regression loss is computed (Mean-Square
loss), If config.num_labels &gt; 1 a classification loss is computed
(Cross-Entropy).</p>
</blockquote>
<p>Source: <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification</a></p>
",0,1,1606,2020-06-11 08:03:09,https://stackoverflow.com/questions/62319735/bert-text-classification-loss-is-nan
Select texts by topic (LDA),"<p>Would it be possible to look for texts that are within a certain topic (determined by LDA)? </p>

<p>I have a list of 5 topics with 10 words each, found by using lda.</p>

<p>I have analysed the texts in a dataframe’s column. 
I would like to select/filter rows/texts that are in one specific topic. </p>

<p>If you need more information, I will provide you. </p>

<p>What I am referring to is the step that returns this output:</p>

<pre><code>[(0,
  '0.207*""house"" + 0.137*""apartment"" + 0.118*""sold"" + 0.092*""beach"" + '
  '0.057*""kitchen"" + 0.049*""rent"" + 0.033*""landlord"" + 0.026*""year"" + '
  '0.024*""bedroom"" + 0.023*""home""'),
 (1,
  '0.270*""school"" + 0.138*""homeworks"" + 0.117*""students"" + 0.084*""teacher"" + '
  '0.065*""pen"" + 0.038*""books"" + 0.022*""maths"" + 0.020*""exercise"" + '
  '0.020*""friends"" + 0.020*""college""'),
 ... ]
</code></pre>

<p>created by </p>

<pre><code># LDA Model

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto', 
                                           # alpha=[0.01]*num_topics,
                                           per_word_topics=True,
                                           eta=[0.01]*len(id2word.keys()))
</code></pre>

<h1>Print the Keyword in the 10 topics</h1>

<pre><code>from pprint import pprint
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<p>The original column with texts that have been analysed is called <code>Texts</code> and it looks like: </p>

<pre><code>Texts 

""Children are happy to go to school...""
""The average price for buying a house is ... ""
""Our children love parks so we should consider to buy an apartment nearby""

etc etc...
</code></pre>

<p>My expected output would be </p>

<pre><code>Texts                                            Topic 
    ""Children are happy to go to school...""         2
    ""The average price for buying a house is ... ""  1
    ""Our children love parks so we should consider to buy an apartment nearby""                                   

      2
</code></pre>

<p>Thanks </p>
","python, gensim, text-classification, lda","<p><code>doc_lda</code> contains a list of (topic, score) tuple for each sentence. Hence you can flexibly assign a topic to the sentence using any heuristics, for example a simple heuristic would by assigning the topic which has the maximum score.</p>
<p>We can extract the topic scores of each sentence by doing this:</p>
<pre class=""lang-py prettyprint-override""><code>topic_scores = [[topic_score[1] for topic_score in sent] for sent in doc_lda]
</code></pre>
<p>You can also convert the above into a pandas dataframe where each row is a sentence and each column is the topic id. The dataframe data structure usually allows for a flexible and more complex operation on the topic-score sentence relationships</p>
<pre class=""lang-py prettyprint-override""><code>df_topics = pd.DataFrame(topic_scores)
</code></pre>
<p>If you just want to assign a single topic which has the maximum score on a sentence, you can do this:</p>
<pre class=""lang-py prettyprint-override""><code>max_topics = [max(sent, key=lambda x: x[1])[0] for sent in doc_lda]
</code></pre>
",2,3,886,2020-06-16 23:57:19,https://stackoverflow.com/questions/62419353/select-texts-by-topic-lda
Customize the encode module in huggingface bert model,"<p>I am working on a text classification project using <a href=""https://huggingface.co/transformers/glossary.html#token-type-ids"" rel=""nofollow noreferrer"">Huggingface transformers module</a>. The encode_plus function provides the users with a convenient way of generating the input ids, attention masks, token type ids, etc. For instance:</p>

<pre><code>from transformers import BertTokenizer

pretrained_model_name = 'bert-base-cased'
bert_base_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

sample_text = 'Bamboo poles, ‍installation by an unknown building constructor #discoverhongkong #hongkonginsta'

encoding = bert_base_tokenizer.encode_plus(
        cleaned_tweet, hashtag_string,
        max_length=70,
        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
        return_token_type_ids=True,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt',  # Return PyTorch tensors
    )

print('*'*20)
print(encoding['input_ids'])
print(encoding['attention_mask'])
print(encoding['token_type_ids'])
print('*'*20)
</code></pre>

<p>However, my current project requires me to generate <strong>customized ids</strong> for a given text. For instance, for a list of words <code>[HK, US, UK]</code>, I want to generate ids for these words and let other words' ids which do not exist in this list as zero. These ids are used to find embedding in another customized embedding matrix, not from pretrained bert module.</p>

<p>How can I achieve this kind of customized encoder? Any suggestions and solutions are welcomed! Thanks~</p>
","nlp, text-classification, huggingface-transformers, bert-language-model","<p>I think you can use the <code>&lt;unusedxxx&gt;</code> tokens in the BERT vocab and add your custom tokens there. So now you can easily refer to them with a valid token ID.</p>
",2,1,745,2020-06-19 03:39:55,https://stackoverflow.com/questions/62462878/customize-the-encode-module-in-huggingface-bert-model
How to convert list of tokens (after sentence tokenization) in a paragraph format into a numbered list of sentences or convert it to a dataframe?,"<p>I read a pdf file using <code>PDFMiner</code> and extracted the text from it for <code>NLP analysis</code>. As I will be dealing with research articles, I did light cleaning of texts by converting the paragraphs of texts into list of sentence tokens. My goal is to select sentences that contains intext citations for my further analysis.</p>

<p>for instance, 
the data is in the below format:</p>

<pre><code>['this is my new project' , 'I am very excited about this  (Abbasi, 2015)'] 
</code></pre>

<p>Expected output:</p>

<pre><code>1.This is my new project
2.I am very excited about this (Abbasi, 2015)
</code></pre>

<p>Is this possible to convert this into a dataframe so that I can add labels to each sentences?</p>

<p>Or will it be wise to extract only the sentences with in-text citations?</p>
","python, nlp, text-classification","<p>To distinguish whether sentences contain intext citation or not, you can simply use regular expression as follow:</p>

<pre><code>i=[] 
for i in sentences:
    if re.match(pattern, i):
       print(""label (1)"")
       indices.append(i)
    else: print(""label (0)"") or pass
</code></pre>

<p>When pattern matched, append the indices of the connected sentences into an array. Finally, turn them into a CSV dataframe.</p>

<p>NB: Since articles come up with different citation styles, check <a href=""https://www.programiz.com/python-programming/regex"" rel=""nofollow noreferrer"">RE rules</a> to customize your own pattern. </p>
",0,0,298,2020-06-19 11:27:23,https://stackoverflow.com/questions/62469271/how-to-convert-list-of-tokens-after-sentence-tokenization-in-a-paragraph-forma
How do I categorise non-english email using procmail and command line tools?,"<p>I am subscribed to a mail list where some of the messages are non-english which I cannot understand.</p>
<p>How do I filter the non-english messages to <code>/dev/null</code> using <code>procmail</code> and/or command line tools?</p>
<p>I use <code>procmail</code> to filter my email, so ideally any alternative tool would also require a <code>procmail</code> recipe.</p>
<p>I'd prefer not to have to train my own language models.</p>
","email, command-line-interface, text-classification, non-english, procmail","<p>One way is to use the perl <a href=""https://www.let.rug.nl/vannoord/TextCat/"" rel=""nofollow noreferrer"">TextCat</a> package from <a href=""https://www.let.rug.nl/vannoord/"" rel=""nofollow noreferrer"">Gertjan van Noord</a>.</p>
<p>The <code>text_cat</code> script outputs the most likely language for the mail.  This recipe assumes <code>text_cat</code> has been installed under <code>/usr/local/bin</code>.</p>
<p>Here is a simple <code>procmail</code> recipe to call the <code>text_cat</code> script:</p>
<pre><code>:0
* ^Subject.*Jobs.*Board
{
    LANG_=`/usr/local/bin/text_cat`

    :0
    * ! LANG ?? ^english$
    /dev/null

    :0
    jobs/
}
</code></pre>
<p>I've been running text_cat for a few years.  There haven't been any non-english messages classified as english, that is, no false-positives.  I've not been rigorous about checking for false-negatives.</p>
<hr>
<p>A second way, as mentioned by <a href=""https://stackoverflow.com/users/874188/tripleee"">tripleee</a> in a comment, is to use the language categorisation provided by <a href=""https://spamassassin.apache.org/"" rel=""nofollow noreferrer"">spamassassin</a> which also uses the text_cat script.  Spamassassin will unwrap any MIME transfer encodings which the vanilla text_cat version above won't.</p>
<p>Here is an <strong>incompletely tested</strong> <code>procmail</code> recipe for filtering on the spamassassin <code>X-Spam-Languages</code> header:</p>
<pre><code>:0
* ^Subject.*Jobs.*Board
{    
    # Delete non-english language emails using spamassassin header
    # Test for not X-Spam-Languages: en
    :0
    * !^X-Spam-Languages: en$
    foreign/

    # Save english language mails in folder
    :0
    jobs/
}
</code></pre>
<p><strong>Warning:</strong> spamassassin will occasionally provide multiple language categorisations like so:</p>
<pre><code>X-Spam-Languages: en da ro
</code></pre>
<p>which the above recipe does not account for.</p>
<p><strong>Spamassassin Language Categorisation Configuration</strong></p>
<p>Edit <code>/etc/spamassassin/v310.pre</code> and uncomment the following line:</p>
<pre><code>loadplugin Mail::SpamAssassin::Plugin::TextCat
</code></pre>
<p>Configure the plugin in <code>/etc/spamassassin/local.cf</code>:</p>
<pre><code>ok_languages en       # I understand english
inactive_languages '' # Enable all languages
add_header all Languages _LANGUAGES_
# score UNWANTED_LANGUAGE_BODY 5 # Increase score - not necessary and not recommended 
</code></pre>
<p>This recipe was incompletely tested with spamassassin version 3.4.2.</p>
<hr>
<p>To adapt these answers to excluding a different language would involve substituting the other language for <code>english</code> in the first case and substituting the other 2 character language code for <code>en</code> in the second case.</p>
",2,2,259,2020-06-23 16:15:29,https://stackoverflow.com/questions/62539231/how-do-i-categorise-non-english-email-using-procmail-and-command-line-tools
Accuracy not growing across epochs on keras,"<p>I'm new to machine learning and deep learning and I'm trying to classify texts from 5 categories using neural networks. For that, I made a dictionary in order to translate the words to indexes, finally getting an array with lists of indexes. Moreover I change the labels to integers. I also did the padding and that stuff. The problem is that when I fit the model the accuracy keeps quite low (~0.20) and does not change across the epochs. I have tried to change a lot of params, like the size of the vocabulary, number of neurones, dropout probability, optimizer parameter, etc. The key parts of the code are below.</p>
<pre><code># Arrays with indexes (that works fine)
X_train = tokens_to_indexes(tokenized_tr_mrp, vocab, return_vocab=False)
X_test, vocab_dict = tokens_to_indexes(tokenized_te_mrp, vocab)

# Labels to integers
labels_dict = {}
labels_dict['Alzheimer'] = 0
labels_dict['Bladder Cancer'] = 1
labels_dict['Breast Cancer'] = 2
labels_dict['Cervical Cancer'] = 3
labels_dict['Negative'] = 4

y_train = np.array([labels_dict[i] for i in y_tr])
y_test = np.array([labels_dict[i] for i in y_te])

# One-hot encoding of labels
from keras.utils import to_categorical
encoded_train = to_categorical(y_train)
encoded_test = to_categorical(y_test)

# Padding
max_review_length = 235
X_train_pad = sequence.pad_sequences(X_train, maxlen=max_review_length)
X_test_pad = sequence.pad_sequences(X_test, maxlen=max_review_length)

# Model
# Vocab size
top_words = len(list(vocab_dict.keys()))
# Neurone type
rnn = LSTM
# dropout
set_dropout = True
p = 0.2
# embedding size
embedding_vector_length = 64
# regularization strength
L = 0.0005
# Number of neurones
N = 50

# Model
model = Sequential()
# Embedding layer
model.add(Embedding(top_words,
                   embedding_vector_length,
                   embeddings_regularizer=regularizers.l1(l=L),
                   input_length=max_review_length
                    #,embeddings_constraint=UnitNorm(axis=1)
                   ))

# Dropout layer
if set_dropout:
    model.add(Dropout(p))

# Recurrent layer
model.add(rnn(N))

# Output layer
model.add(Dense(5, activation='softmax'))

# Compilation
model.compile(loss='categorical_crossentropy',
             optimizer=Adam(lr=0.001),
             metrics=['Accuracy'])

# Split training set for validation
X_tr, X_va, y_tr_, y_va = train_test_split(X_train_pad, encoded_train, 
                                          test_size=0.3, random_state=2)

# Parameters
batch_size = 50
# N epochs
n_epocas = 20

best_val_acc  = 0
best_val_loss = 1e20
best_i        = 0

best_weights = []
acum_tr_acc = []
acum_tr_loss = []
acum_val_acc = []
acum_val_loss = []

# Training
for e in range(n_epocas):
    h = model.fit(X_tr, y_tr_,
                batch_size=batch_size,
                validation_data=(X_va, y_va),
                epochs=1, verbose=1)
  
    acum_tr_acc  = acum_tr_acc  + h.history['accuracy']
    acum_tr_loss = acum_tr_loss + h.history['loss']
    val_acc  = h.history['val_accuracy'][0]
    val_loss = h.history['val_loss'][0]
    acum_val_acc  = acum_val_acc  + [val_acc]
    acum_val_loss = acum_val_loss + [val_loss]
#   if val_acc  &gt; best_val_acc:
    if val_loss &lt; best_val_loss:
        best_i        = len(acum_val_acc)-1
        best_val_acc  = val_acc
        best_val_loss = val_loss
        best_weights = model.get_weights().copy()
    if len(acum_tr_acc)&gt;1 and (len(acum_tr_acc)+1) % 1 == 0:
        if e&gt;1:
            clear_output()
</code></pre>
","python, keras, deep-learning, recurrent-neural-network, text-classification","<p>Someone gave me the solution. I just had to change this line:</p>
<pre><code>model.compile(loss='categorical_crossentropy',
             optimizer=Adam(lr=0.001),
             metrics=['Accuracy'])
</code></pre>
<p>For this:</p>
<pre><code>model.compile(loss='categorical_crossentropy',
                 optimizer=Adam(lr=0.001),
                 metrics=['acc'])
</code></pre>
<p>I also changed the lines in the final loop relating to accuracy. The one-hot encoding was necessary as well.</p>
",0,1,112,2020-06-26 12:21:11,https://stackoverflow.com/questions/62594647/accuracy-not-growing-across-epochs-on-keras
How to train a model to classify input to one or more classes,"<p>I use this sample code to train a model to classify a random number into a one of 10 classes</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras

samples_number = 1000
features_number = 5
output_classes_number = 10
x_train = np.random.random((samples_number, features_number))
y_train = keras.utils.to_categorical(np.random.randint(output_classes_number, size=(samples_number, 1)), num_classes=output_classes_number)

model = Sequential()
# Dense(64) is a fully-connected layer with 64 hidden units.
model.add(Dense(64, activation='relu', input_dim=features_number))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])
model.fit(x_train, y_train,
          epochs=1000,
          batch_size=128) 
</code></pre>
<p>In this sample, a sample value of <code>x_train[0]</code> is</p>
<pre><code>[0.54071786 0.31048455 0.87843899 0.88947151 0.89052953]
</code></pre>
<p>and a sample output of <code>y_train[0]</code> is</p>
<pre><code>[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
</code></pre>
<p>which means that the <code>x_train[0]</code> values map to the <code>7th</code> feature.</p>
<p>In this online sample sample code, there's only <strong>one</strong> matched class for each input.</p>
<hr />
<p>How can I change my code to train 5 features into one or more than class in the same time?</p>
<p>For example, a possible <code>y_train[z]</code> value may be <code>[0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]</code>?</p>
","python, tensorflow, keras, text-classification","<p>Just change your final activation function to <code>sigmoid</code> and you will get the probability per class, and so it allows multi-label classification.</p>
<p>Naturally, you will need labels that reflect this new task, which you don't seem to have at the moment.</p>
<p>Full example:</p>
<pre><code>import numpy as np
from tensorflow import keras

x_train = np.random.random((1000, 4))
y_train = np.random.randint(0, 2, (1000, 4))
</code></pre>
<p>Multi-label targets:</p>
<pre><code>array([[0, 0, 0, 0],
       [0, 1, 0, 1],
       [1, 0, 0, 0],
       ...,
       [1, 1, 0, 0],
       [0, 1, 0, 1],
       [0, 1, 0, 1]])
</code></pre>
<pre><code>model = keras.models.Sequential()
model.add(keras.layers.Dense(64, activation='relu', input_dim=4))
model.add(keras.layers.Dropout(0.5))
model.add(keras.layers.Dense(4, activation='sigmoid'))

model.compile(loss='categorical_crossentropy',
              optimizer='sgd')

model.fit(x_train, y_train,
          epochs=10,
          batch_size=16)
</code></pre>
<pre><code>model.predict(x_train)
</code></pre>
<p>Probabilities per class:</p>
<pre><code>array([[0.48028257, 0.48918256, 0.4759362 , 0.51707023],
       [0.460468  , 0.50321233, 0.5157731 , 0.51490146],
       [0.5088656 , 0.50617874, 0.47503173, 0.5145618 ],
       ...,
       [0.452385  , 0.48947614, 0.47086555, 0.51236445],
       [0.48170012, 0.475545  , 0.48153797, 0.49793705],
       [0.47959277, 0.5056894 , 0.45207116, 0.50883204]], dtype=float32)
</code></pre>
<p>If it was done with <code>softmax</code>, these probabilities would sum up to 1, because it predicts only one category:</p>
<pre><code>array([[0.2694298 , 0.21779475, 0.23155291, 0.28122255],
       [0.28732255, 0.24838863, 0.23328216, 0.23100664],
       [0.28733823, 0.24516277, 0.23259555, 0.23490342],
       ...,
       [0.28732476, 0.21751696, 0.24203528, 0.253123  ],
       [0.27158916, 0.26262963, 0.22158018, 0.244201  ],
       [0.27889836, 0.25647762, 0.20330393, 0.2613201 ]], dtype=float32)
</code></pre>
",1,0,72,2020-07-01 12:37:18,https://stackoverflow.com/questions/62677043/how-to-train-a-model-to-classify-input-to-one-or-more-classes
ValueError in while predict where Test data is having different shape of word vector,"<p>Below is my code I am trying for text classification model;</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
ifidf_vectorizer = TfidfVectorizer()

X_train_tfidf = ifidf_vectorizer.fit_transform(X_train)
X_train_tfidf.shape

(3, 16)

from sklearn.svm import LinearSVC
clf = LinearSVC()
clf.fit(X_train_tfidf,y_train)
</code></pre>
<p>Till now only training set has been vectorized into a full vocabulary. In order to perform analysis on test set I need to submit it to the same procedures.
So I did</p>
<pre><code>X_test_tfidf = ifidf_vectorizer.fit_transform(X_test) 
X_test_tfidf.shape
(2, 12)
</code></pre>
<p>And finally when trying to predict its showing error;</p>
<pre><code>predictions = clf.predict(X_test_tfidf)

ValueError: X has 12 features per sample; expecting 16
</code></pre>
<p>But when I use pipeline <code>from sklearn.pipeline import Pipeline</code> then it worked fine;</p>
<p>Can’t I code the way I was trying?</p>
","python, machine-learning, scikit-learn, text-classification","<p>The error is with <code>fit_transform</code> of test data. You <code>fit_transform</code> training data and only <code>transform</code> test data:</p>
<pre class=""lang-py prettyprint-override""><code># change this
X_test_tfidf = ifidf_vectorizer.fit_transform(X_test) 
X_test_tfidf.shape
(2, 12)

# to 
X_test_tfidf = ifidf_vectorizer.transform(X_test) 
X_test_tfidf.shape
</code></pre>
<p><strong>Reasons</strong>:
When you do <code>fit_transform</code>, you teach your model the vectors with <code>fit</code>. The model learns the vectors to which they are used to transform data. You use the train data to learn the vectors, then you apply them to both train and test with <code>transform</code></p>
<p>If you do a <code>fit_transform</code> on test data, you replaced the vectors learned in training data and replaced them with test data. Given that your test data is smaller than your train data, it is likely you would get two different vectorisation.</p>
<p><strong>A Better Way</strong>
The best way to do what you do is using <em>Pipelines</em> which will make your flow easy to understand</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline


clf = Pipeline(steps=[
('vectorizer', TfidfVectorizer()),
('model', LinearSVC()),
])

# train
clf.fit(X_train,y_train)

# predict
clf.predict(X_test)

</code></pre>
<p>This is easier as the transformation are taking care for you. You don’t have to worry about <code>fit_transform</code>  when fitting the model or <code>transform</code> when predicting or scoring.</p>
<p>You can access the features independently if you with with</p>
<pre class=""lang-py prettyprint-override""><code>
clf.named_steps('vectorizer') # or 'model'
</code></pre>
<p>Under the hood, when you do <code>clf.fit</code>, your data will pass throw your vectorizer using <code>fit_transform</code> and then to the <code>model</code>. When you predict or score, your data will pass throw your vectorizer with <code>transform</code> before reaching your model.</p>
",1,1,801,2020-07-09 09:51:37,https://stackoverflow.com/questions/62812198/valueerror-in-while-predict-where-test-data-is-having-different-shape-of-word-ve
"What is the difference between args wordNgrams, minn and maxn in fassttext supervised learning?","<p>I'm a little confused after reading Bag of tricks for efficient text classification.
What is the difference between args <code>wordNgrams</code>, <code>minn</code> and <code>maxn</code></p>
<p>For example, a text classification task and Glove embedding as pretrainedVectors</p>
<pre><code>ft.train_supervised(file_path,lr=0.1,epoch=5,wordNgrams=2,dim=300,loss='softmax', minn=2,maxn=3,pretrainedVectors='glove.300d.txt',verbose=0)
</code></pre>
<p>an input sentence is 'I love you'.
Given minn=2,maxn=3, the whole sentence is transformed into <code>[&lt;I, I&gt;], [&lt;l, &lt;lo, lo, lov,.....]</code> etc
For the word love, its fasttext embedding = (emb(love) (as a complete word) + emb(&lt;l)+emb(&lt;lo)+....) / n.
For the sentence, it is splitted into <code>[I love, love you]</code> (because wordNgrams=2) and these 2-gram embeddings are <code>[(fasttext emb(I)+fasttext emb(love))/2, (fasttext emb(love)+fasttext emb(you))/2]</code>.
The sentence embedding is average of 2-gram embeddings and has dimensionality as 300. Then it is fed through a layer which has #labels neurons (i.e. multiplied with a matrix whose size is [300, #labels]).</p>
<p>Is this right? Please correct me if I'm wrong</p>
","text-classification, supervised-learning, fasttext","<p>raju,</p>
<p>You are almost right, but the averaging happens at the very end.</p>
<p>First, how a sentence is tokenized?</p>
<p>The whole sentence is tokenized with spaces. So &quot;I love you&quot; will produce 4 words: &quot;I&quot;, &quot;love&quot;, &quot;you&quot; and a special word EOS (end of sentence). So far we have 4 tokens. Then, for each word, depending of what you set for <code>minn</code> and <code>maxn</code>, fastText will compute the subwords and consider them as tokens as well. So in your case with minn=2, maxn=3, it will be: <code>&quot;&lt;I&quot;, &quot;&lt;I&gt;&quot;, &quot;I&gt;&quot;, &quot;&lt;l&quot;, &quot;&lt;lo&quot;, &quot;lo&quot;, &quot;lov&quot;, &quot;ov&quot;, &quot;ove&quot;, &quot;ve&quot;, &quot;ve&gt;&quot;, &quot;e&gt;&quot;, &quot;&lt;y&quot;, &quot;&lt;yo&quot;, &quot;yo&quot;, &quot;you&quot;, &quot;ou&quot;, &quot;ou&gt;&quot;, &quot;u&gt;&quot;</code> (we add beginning and end of word characters (<code>&lt;</code> and <code>&gt;</code>) as well).</p>
<p>So the overall tokens will be <code>&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, EOS, &quot;&lt;I&quot;, &quot;&lt;I&gt;&quot;, &quot;I&gt;&quot;, &quot;&lt;l&quot;, &quot;&lt;lo&quot;, &quot;lo&quot;, &quot;lov&quot;, &quot;ov&quot;, &quot;ove&quot;, &quot;ve&quot;, &quot;ve&gt;&quot;, &quot;e&gt;&quot;, &quot;&lt;y&quot;, &quot;&lt;yo&quot;, &quot;yo&quot;, &quot;you&quot;, &quot;ou&quot;, &quot;ou&gt;&quot;, &quot;u&gt;&quot;</code>.</p>
<p>Now with wordNgrams=2, we also add tokens corresponding to pair of words: <code>&quot;I love&quot;, &quot;love you&quot;, &quot;you EOS&quot;</code></p>
<p>Once we have the tokens:</p>
<p>In order to compute the hidden layer, the embedding of the sentence will be the average of the embeddings of individual tokens above. This is done by summing the corresponding column vectors of dimension 300 in the input matrix and we divide by the number of tokens to have the average with <a href=""https://github.com/facebookresearch/fastText/blob/a20c0d27cd0ee88a25ea0433b7f03038cd728459/src/model.cc#L49"" rel=""noreferrer"">this line of code</a>.</p>
",7,4,1108,2020-07-10 06:18:04,https://stackoverflow.com/questions/62828346/what-is-the-difference-between-args-wordngrams-minn-and-maxn-in-fassttext-super
Classification: Tweet Sentiment Analysis - Order of steps,"<p>I am currently working on a tweet sentiment analysis and have a few questions regarding the right order of the steps. Please assume that the data was already preprocessed and prepared accordingly. So this is how I would proceed:</p>
<ol>
<li>use <code>train_test_split</code> (80:20 ratio) to withhold a test
data set.</li>
<li>vectorize <code>x_train</code> since the tweets are not numerical.</li>
</ol>
<p>In the next steps, I would like to identify the best classifier. Please assume those were already imported. So I would go on by:</p>
<ol start=""3"">
<li>hyperparameterization (grid-search) including a cross-validation approach.
In this step, I would like to identify the best parameters of each
classifier. For KNN the code is as follows:</li>
</ol>
<pre><code>model = KNeighborsClassifier()
n_neighbors = range(1, 10, 2)
weights = ['uniform', 'distance']
metric = ['euclidean', 'manhattan', 'minkowski']

# define grid search
grid = dict(n_neighbors=n_neighbors, weights=weights ,metric=metric)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(train_tf, y_train)

# summarize results
print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param))
</code></pre>
<ol start=""4"">
<li>compare the accuracy (depending on the best hyperparameters) of the classifiers</li>
<li>choose the best classifier</li>
<li>take the withheld test data set (from <code>train_test_split()</code>) and use the best classifier on the test data</li>
</ol>
<p>Is this the right approach or would you recommend changing something (e. g. doing the cross-validation alone and not within the hyperparametrization)? Does it make sense to test the test data as the final step or should I do it earlier to assess the accuracy for an unknown data set?</p>
","python, machine-learning, classification, sentiment-analysis, text-classification","<p>There are lots of ways to do this and people have strong opinions about it and I'm not always convinced they fully understand what they advocate.</p>
<p>TL;DR: Your methodology looks great and you're asking sensible questions.</p>
<p>Having said that, here are some things to consider:</p>
<ol>
<li>Why are you doing train-test split validation?</li>
<li>Why are you doing hyperparameter tuning?</li>
<li>Why are you doing cross-validation?</li>
</ol>
<p>Yes, each of these techniques are good <em>at doing something specific</em>; but that doesn't necessarily mean they should all be part of the same pipeline.</p>
<p>First off, let's answer these questions:</p>
<ol>
<li><p><strong>Train-Test Split</strong> is useful for testing your classifier's inference abilities. In other words, we want to know how well a classifier performs <em>in general</em> (not on the data we used for training). The test portion allows us to evaluate our classifier without using our training portion.</p>
</li>
<li><p><strong>Hyperparameter-Tuning</strong> is useful for evaluating the effect of hyperparameters on the performance of a classifier. For it to be meaningful, we must compare two (or more) models (using different hyperparameters) but trained preferably using the same training portion (to eliminate selection bias). What do we do once we know the best performing hyperparameters? Will this set of hyperparameters always perform optimally? No. You will see that, due to the stochastic nature of classification, one hyperparameter set may work best in experiment A then another set of hyperparameters may work best on experiment B. Rather, hyperparameter tuning is good for generalizing about which hyperparameters to use when building a classifier.</p>
</li>
<li><p><strong>Cross-validation</strong> is used to smooth out some of the stochastic randomness associated with building classifiers. So, a machine learning pipeline may produce a classifier that is 94% accurate using 1 test-fold and 83% accuracy using another test-fold. What does it mean? It might mean that 1 fold contains samples that are easy. Or it might mean that the classifier, for whatever reason, is actually better. You don't know because it's a black box.</p>
</li>
</ol>
<p><strong>Practically</strong>, how is this helpful?</p>
<p>I see little value in using test-train split <strong>and</strong> cross-validation. I use cross-validation and report accuracy as an average over the n-folds. It is already testing my classifier's performance. I don't see why dividing your training data further to do another round of train-test validation is going to help. Use the average. Having said that, I use the best performing model of the n-fold models created during cross-validation as my final model. As I said, it's black-box, so we can't <strong>know</strong> which model is best but, all else being equal, you may as well use the best performing one. It might actually <em>be</em> better.</p>
<p>Hyperparameter-tuning is useful but it can take forever to do extensive tuning. I suggest adding hyperparameter tuning to your pipeline but only test 2 sets of hyperparameters. So, keep all your hyperparameters constant except 1. e.g. Batch size = {64, 128}. Run that, and you'll be able to say with confidence, &quot;Oh, that made a big difference: 64 works better than 128!&quot; or &quot;Well, that was a waste of time. It didn't make much difference either way.&quot; If the difference is small, ignore that hyperparameter and try another pair. This way, you'll slowly tack towards optimal without all the wasted time.</p>
<p>In practice, I'd say leave the extensive hyperparameter-tuning to academics and take a more pragmatic approach.</p>
<p>But yeah, you're methodology looks good as it is. I think you thinking about what you're doing and that already puts you a step ahead of the pack.</p>
",2,3,257,2020-07-11 10:51:11,https://stackoverflow.com/questions/62848208/classification-tweet-sentiment-analysis-order-of-steps
Saving a trained multi-input classification algorithm in Python,"<p>I developed a script that predicts probable tags for some text, based on previously manually tagged feedback. I used several online articles to help me (namely: <a href=""https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5"" rel=""nofollow noreferrer"">https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5</a>).</p>
<p>Because I want the probability for each tag, here's the code I used:</p>
<pre><code>NB_pipeline = Pipeline([
    ('clf', OneVsRestClassifier(MultinomialNB(alpha=0.3, fit_prior=True, class_prior=None))),
    ])

predictions_en = {}
for category in categories_en:
    NB_pipeline.fit(all_x_en, en_topics[category])
    proba_en = NB_pipeline.predict_proba(pred_x_en)
    predictions_en[category] = proba_en[-1][-1]

preds_en = pd.DataFrame(predictions_en.items())
preds_en = preds_en.sort_values(by=[1], ascending=False)
preds_en = preds_en.reset_index(drop=True)
</code></pre>
<p>It works very well for my purposes: it returns a prediction for each possible tag. But my issue is that it retrains the algorithm every time I try to make a prediction. What I'd like to do is to train the algorithm in a script, save the trained algorithm, load it in another script where the prediction is made.</p>
<p>I'd like to be able to do this in script 1:</p>
<pre><code>for category in categories_en:
    NB_pipeline.fit(all_x_en, en_topics[category])
</code></pre>
<p>And this in the other script:</p>
<pre><code>for category in categories_en:
    proba_en = NB_pipeline.predict_proba(pred_x_en)
    predictions_en[category] = proba_en[-1][-1]
</code></pre>
<p>But I can't seem to make it work. It just gives me the same prediction when I try to separate it.</p>
","python, machine-learning, text-classification, multiclass-classification","<p>You could always use <code>pickle</code> to serialize any python object including yours. So the simplest and fastest way to save your model is to just serialize it to a file, say <code>model.pickle</code>. This is done in the first part after you train your model. After that, all you have to do is to check if the file exists and deserialize it using <code>pickle</code> again.</p>
<p>This is a function that serializes python objects to files:</p>
<pre class=""lang-py prettyprint-override""><code>import pickle

def serialize(obj, file):
    with open(file, 'wb') as f:
        pickle.dump(obj, f)

</code></pre>
<p>This is a function that deserializes python objects from files:</p>
<pre class=""lang-py prettyprint-override""><code>import pickle

def deserialize(file):
    with open(file, 'rb') as f:
        return pickle.load(f)
</code></pre>
<p>After your done training, all you have to do is to call (if <code>NB_pipeline</code> is the object of your model):</p>
<pre><code>serialize(NB_pipeline, 'model.pickle')
</code></pre>
<p>And when you have to load it and use it just call:</p>
<pre><code>NB_pipeline = deserialize('model.pickle')
</code></pre>
",0,0,54,2020-07-14 11:19:25,https://stackoverflow.com/questions/62893978/saving-a-trained-multi-input-classification-algorithm-in-python
Sliding window for long text in BERT for Question Answering,"<p>I've read post which explains how the sliding window works but I cannot find any information on how it is actually implemented.</p>
<p>From what I understand if the input are too long, sliding window can be used to process the text.</p>
<p>Please correct me if I am wrong.
Say I have a text <em><strong>&quot;In June 2017 Kaggle announced that it passed 1 million registered users&quot;</strong></em>.</p>
<p>Given some <code>stride</code> and <code>max_len</code>, the input can be split into chunks with over lapping words (not considering padding).</p>
<pre><code>In June 2017 Kaggle announced that # chunk 1
announced that it passed 1 million # chunk 2
1 million registered users # chunk 3
</code></pre>
<p>If my questions were <em><strong>&quot;when did Kaggle make the announcement&quot;</strong></em> and <em><strong>&quot;how many registered users&quot;</strong></em> I can use <code>chunk 1</code> and <code>chunk 3</code> and <strong>not use</strong> <code>chunk 2</code> <strong>at all</strong> in the model. Not quiet sure if I should still use <code>chunk 2</code> to train the model</p>
<p>So the input will be:
<code>[CLS]when did Kaggle make the announcement[SEP]In June 2017 Kaggle announced that[SEP]</code>
and
<code>[CLS]how many registered users[SEP]1 million registered users[SEP]</code></p>
<hr>
<p>Then if I have a question with no answers do I feed it into the model with all chunks like and indicate the starting and ending index as <strong>-1</strong>? For example <em><strong>&quot;can pigs fly?&quot;</strong></em></p>
<p><code>[CLS]can pigs fly[SEP]In June 2017 Kaggle announced that[SEP]</code></p>
<p><code>[CLS]can pigs fly[SEP]announced that it passed 1 million[SEP]</code></p>
<p><code>[CLS]can pigs fly[SEP]1 million registered users[SEP]</code></p>
<hr>
<p>As suggested in the comments, II tried to run <code>squad_convert_example_to_features</code> (<a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L134"" rel=""noreferrer"">source code</a>) to investigate the problem I have above, but it doesn't seem to work, nor there are any documentation. It seems like <code>run_squad.py</code> from huggingface uses <code>squad_convert_example_to_features</code> with the <code>s</code> in <code>example</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor, squad_convert_example_to_features
from transformers import AutoTokenizer, AutoConfig, squad_convert_examples_to_features

FILE_DIR = &quot;.&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
processor = SquadV2Processor()
examples = processor.get_train_examples(FILE_DIR)

features = squad_convert_example_to_features(
    example=examples[0],
    max_seq_length=384,
    doc_stride=128,
    max_query_length=64,
    is_training=True,
)
</code></pre>
<p>I get the error.</p>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 159.95it/s]
Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 25, in &lt;module&gt;
    sub_tokens = tokenizer.tokenize(token)
NameError: name 'tokenizer' is not defined
</code></pre>
<p>The error indicates that there are no <code>tokenizers</code> but it does not allow us to pass a <code>tokenizer</code>. Though it does work if I add a tokenizer while I am inside the function in debug mode. So how exactly do I use the <code>squad_convert_example_to_features</code> function?</p>
","nlp, text-classification, huggingface-transformers, nlp-question-answering, bert-language-model","<p>I think there is a problem with the examples you pick. Both <a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L273"" rel=""nofollow noreferrer"">squad_convert_examples_to_features</a> and <a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L86"" rel=""nofollow noreferrer"">squad_convert_example_to_features</a> have a sliding window approach implemented because <code>squad_convert_examples_to_features</code> is just a parallelization wrapper for <code>squad_convert_example_to_features</code>. But let's look at the single example function. First of all you need to call <a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L268"" rel=""nofollow noreferrer"">squad_convert_example_to_features_init</a> to make the tokenizer global (this is done automatically for you in <code>squad_convert_examples_to_features</code>):</p>
<pre class=""lang-py prettyprint-override""><code>from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features, squad_convert_example_to_features_init
from transformers import AutoTokenizer, AutoConfig, squad_convert_examples_to_features

FILE_DIR = &quot;.&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
squad_convert_example_to_features_init(tokenizer)

processor = SquadV2Processor()
examples = processor.get_train_examples(FILE_DIR)

features = squad_convert_example_to_features(
    example=examples[0],
    max_seq_length=384,
    doc_stride=128,
    max_query_length=64,
    is_training=True,
)
print(len(features))
</code></pre>
<p>Output:</p>
<pre><code>1
</code></pre>
<p>You might say that this function is not using a sliding window approach, but this is wrong because your example doesn't needed to be split:</p>
<pre class=""lang-py prettyprint-override""><code>print(len(examples[0].question_text.split()) + len(examples[0].doc_tokens))
</code></pre>
<p>Output:</p>
<pre><code>115
</code></pre>
<p>which is less as the max_seq_length which you have set to 384. Now lets try a different one:</p>
<pre><code>print(len(examples[129603].question_text.split()) + len(examples[129603].doc_tokens))

features = squad_convert_example_to_features(
    example=examples[129603],
    max_seq_length=384,
    doc_stride=128,
    max_query_length=64,
    is_training=True,
)
print(len(features))
</code></pre>
<p>Output:</p>
<pre><code>454
3
</code></pre>
<p>Which you can now compare with the original sample:</p>
<pre class=""lang-py prettyprint-override""><code>print('[CLS]' + examples[129603].question_text + '[SEP]' + ' '.join(examples[129603].doc_tokens) + '[SEP]')

for idx, f in enumerate(features):
    print('Split {}'.format(idx))
    print(' '.join(f.tokens))
</code></pre>
<p>Output:</p>
<pre><code>[CLS]How often is hunting occurring in Delaware each year?[SEP]There is a very active tradition of hunting of small to medium-sized wild game in Trinidad and Tobago. Hunting is carried out with firearms, and aided by the use of hounds, with the illegal use of trap guns, trap cages and snare nets. With approximately 12,000 sport hunters applying for hunting licences in recent years (in a very small country of about the size of the state of Delaware at about 5128 square kilometers and 1.3 million inhabitants), there is some concern that the practice might not be sustainable. In addition there are at present no bag limits and the open season is comparatively very long (5 months - October to February inclusive). As such hunting pressure from legal hunters is very high. Added to that, there is a thriving and very lucrative black market for poached wild game (sold and enthusiastically purchased as expensive luxury delicacies) and the numbers of commercial poachers in operation is unknown but presumed to be fairly high. As a result, the populations of the five major mammalian game species (red-rumped agouti, lowland paca, nine-banded armadillo, collared peccary, and red brocket deer) are thought to be quite low (although scientifically conducted population studies are only just recently being conducted as of 2013). It appears that the red brocket deer population has been extirpated on Tobago as a result of over-hunting. Various herons, ducks, doves, the green iguana, the gold tegu, the spectacled caiman and the common opossum are also commonly hunted and poached. There is also some poaching of 'fully protected species', including red howler monkeys and capuchin monkeys, southern tamanduas, Brazilian porcupines, yellow-footed tortoises, Trinidad piping guans and even one of the national birds, the scarlet ibis. Legal hunters pay very small fees to obtain hunting licences and undergo no official basic conservation biology or hunting-ethics training. There is presumed to be relatively very little subsistence hunting in the country (with most hunting for either sport or commercial profit). The local wildlife management authority is under-staffed and under-funded, and as such very little in the way of enforcement is done to uphold existing wildlife management laws, with hunting occurring both in and out of season, and even in wildlife sanctuaries. There is some indication that the government is beginning to take the issue of wildlife management more seriously, with well drafted legislation being brought before Parliament in 2015. It remains to be seen if the drafted legislation will be fully adopted and financially supported by the current and future governments, and if the general populace will move towards a greater awareness of the importance of wildlife conservation and change the culture of wanton consumption to one of sustainable management.[SEP]
Split 0
[CLS] how often is hunting occurring in delaware each year ? [SEP] there is a very active tradition of hunting of small to medium - sized wild game in trinidad and tobago . hunting is carried out with firearms , and aided by the use of hounds , with the illegal use of trap guns , trap cages and s ##nare nets . with approximately 12 , 000 sport hunters applying for hunting licence ##s in recent years ( in a very small country of about the size of the state of delaware at about 512 ##8 square kilometers and 1 . 3 million inhabitants ) , there is some concern that the practice might not be sustainable . in addition there are at present no bag limits and the open season is comparatively very long ( 5 months - october to february inclusive ) . as such hunting pressure from legal hunters is very high . added to that , there is a thriving and very lucrative black market for po ##ache ##d wild game ( sold and enthusiastically purchased as expensive luxury del ##ica ##cies ) and the numbers of commercial po ##ache ##rs in operation is unknown but presumed to be fairly high . as a result , the populations of the five major mammalian game species ( red - rum ##ped ago ##uti , lowland pac ##a , nine - banded arm ##adi ##llo , collar ##ed pe ##cca ##ry , and red brock ##et deer ) are thought to be quite low ( although scientific ##ally conducted population studies are only just recently being conducted as of 2013 ) . it appears that the red brock ##et deer population has been ex ##ti ##rp ##ated on tobago as a result of over - hunting . various heron ##s , ducks , dove ##s , the green i ##gua ##na , the gold te ##gu , the spectacle ##d cai ##man and the common op ##oss ##um are also commonly hunted and po ##ache ##d . there is also some po ##achi ##ng of ' fully protected species ' , including red howl ##er monkeys and cap ##uchi ##n monkeys , southern tam ##and ##ua ##s , brazilian por ##cup ##ines , yellow - footed tor ##to ##ises , [SEP]
Split 1
[CLS] how often is hunting occurring in delaware each year ? [SEP] october to february inclusive ) . as such hunting pressure from legal hunters is very high . added to that , there is a thriving and very lucrative black market for po ##ache ##d wild game ( sold and enthusiastically purchased as expensive luxury del ##ica ##cies ) and the numbers of commercial po ##ache ##rs in operation is unknown but presumed to be fairly high . as a result , the populations of the five major mammalian game species ( red - rum ##ped ago ##uti , lowland pac ##a , nine - banded arm ##adi ##llo , collar ##ed pe ##cca ##ry , and red brock ##et deer ) are thought to be quite low ( although scientific ##ally conducted population studies are only just recently being conducted as of 2013 ) . it appears that the red brock ##et deer population has been ex ##ti ##rp ##ated on tobago as a result of over - hunting . various heron ##s , ducks , dove ##s , the green i ##gua ##na , the gold te ##gu , the spectacle ##d cai ##man and the common op ##oss ##um are also commonly hunted and po ##ache ##d . there is also some po ##achi ##ng of ' fully protected species ' , including red howl ##er monkeys and cap ##uchi ##n monkeys , southern tam ##and ##ua ##s , brazilian por ##cup ##ines , yellow - footed tor ##to ##ises , trinidad pip ##ing gu ##ans and even one of the national birds , the scarlet ib ##is . legal hunters pay very small fees to obtain hunting licence ##s and undergo no official basic conservation biology or hunting - ethics training . there is presumed to be relatively very little subsistence hunting in the country ( with most hunting for either sport or commercial profit ) . the local wildlife management authority is under - staffed and under - funded , and as such very little in the way of enforcement is done to uphold existing wildlife management laws , with hunting occurring both in and out of season , and even in wildlife san ##ct ##uaries . there is some indication that the government is beginning to [SEP]
Split 2
[CLS] how often is hunting occurring in delaware each year ? [SEP] being conducted as of 2013 ) . it appears that the red brock ##et deer population has been ex ##ti ##rp ##ated on tobago as a result of over - hunting . various heron ##s , ducks , dove ##s , the green i ##gua ##na , the gold te ##gu , the spectacle ##d cai ##man and the common op ##oss ##um are also commonly hunted and po ##ache ##d . there is also some po ##achi ##ng of ' fully protected species ' , including red howl ##er monkeys and cap ##uchi ##n monkeys , southern tam ##and ##ua ##s , brazilian por ##cup ##ines , yellow - footed tor ##to ##ises , trinidad pip ##ing gu ##ans and even one of the national birds , the scarlet ib ##is . legal hunters pay very small fees to obtain hunting licence ##s and undergo no official basic conservation biology or hunting - ethics training . there is presumed to be relatively very little subsistence hunting in the country ( with most hunting for either sport or commercial profit ) . the local wildlife management authority is under - staffed and under - funded , and as such very little in the way of enforcement is done to uphold existing wildlife management laws , with hunting occurring both in and out of season , and even in wildlife san ##ct ##uaries . there is some indication that the government is beginning to take the issue of wildlife management more seriously , with well drafted legislation being brought before parliament in 2015 . it remains to be seen if the drafted legislation will be fully adopted and financially supported by the current and future governments , and if the general populace will move towards a greater awareness of the importance of wildlife conservation and change the culture of want ##on consumption to one of sustainable management . [SEP]
</code></pre>
<hr />
<blockquote>
<p>If my questions were &quot;when did Kaggle make the announcement&quot; and &quot;how
many registered users&quot; I can use chunk 1 and chunk 3 and not use chunk
2 at all in the model. Not quiet sure if I should still use chunk 2 to
train the model</p>
</blockquote>
<p>Yes you should also use chunk 2 to train your model, because when you try to predict the same sequence you hope that your model predicts 0:0 as answer span for chunk 2 (i.e. you can easily select the chunk which contains the answer).</p>
",3,6,8550,2020-07-19 10:18:32,https://stackoverflow.com/questions/62978957/sliding-window-for-long-text-in-bert-for-question-answering
How to get last layer before classification layer on fasttext?,"<p>I would like to solve a text similarity problem using fasttext. I am able to create model to get text classification labels using fasttext. But I would like to get document vector which is created for classification layer input generating fasttext model. Then using some similarity methods get the scores .</p>
<p>How can I do that ? Any help would be great</p>
<p>Thanks</p>
","text, similarity, text-classification, fasttext","<p>I'm fairly sure FastText's <code>supervised</code> mode interim value is just an <em>average of all the input text's word-vectors</em>. So you can request the individual word-vectors, then average them together yourself.</p>
",1,0,121,2020-07-23 10:38:56,https://stackoverflow.com/questions/63052335/how-to-get-last-layer-before-classification-layer-on-fasttext
Build a multiclass text classifier which takes vectors generated from word2vec as independent variables to predict a class,"<p>I am dealing with patient data. I want to predict the top N diseases given a set of symptoms.</p>
<p>This is a sample of my dataset: In total I have around 1200 unique Symptoms and around 200 unique Diagnosis</p>
<pre><code>     ID         Symptom combination                              Diagnosis
    Patient1: fever, loss of appetite, cold                        Flu
    Patient2: hair loss, blood pressure                           Thyroid
    Patient3: hair loss, blood pressure                            Flu
    Patient4: throat pain, joint pain                           Viral Fever

    ..
    ..
Patient30000: vomiting, nausea                                   Diarrohea
</code></pre>
<p>What I am planning to do with this dataset is to use the Symptoms column to generate word vectors using Word2vec for each row of patient data. After generating the vectors I want to build a classifier, with the vectors in each row being my independent variable and the Diagnosis being the target categorical variable.</p>
<p>Shall I take the average of the vectors to generate feature vectors generated from word2vec? If so, any clarifications on the same?</p>
","python, machine-learning, nlp, word2vec, text-classification","<p>You can average a bunch of word-vectors for symptoms together to get a single feature-vector of the same dimensionality. (If your word-vectors are 100d each, averaging them together gets a single 100d summary vector.)</p>
<p>But such averaging is fairly crude, and has some risk of diluting the information of each symptom in the averaging.</p>
<p>(As a simplified, stylized example, imagine a nurse took a patients' temperature at 9pm, and found it to be 102.6°F. Then again, at 7am, and found it to be 94.6°F. A doctor asks, &quot;how's our patient's temperature?&quot;, and the nurse says the average, &quot;98.6°F&quot;. &quot;Wow,&quot; says the doctor, &quot;it's rare for someone to be so on-the-dot for the normal healthy temperature. Next patient!&quot; Averaging hid the important information: that the patient had both a fever and dangerous hypothermia.)</p>
<p>It sounds like you have a controlled-vocabulary of symptoms, with just some known, capped, and not-very-large number of symptom tokens: about 1200.</p>
<p>In such a case, turning those into a categorical vector for the presence/absence of each symptom may work far better than word2vec-based approaches. Maybe you have 100 different symptoms or 10,000 different symptoms. Either way, you can turn them into a large vector of 1s and 0s representing each possible symptom in order, and lots of classifiers will do pretty well with that input.</p>
<p>If treating the list-of-symptoms like a text-of-words, a simple &quot;bag of words&quot; representation of the text will essentially be this categorical representation: a 1200-dimensional 'one-hot' vector.</p>
<p>And unless this is some academic exercise where you've been required to use word2vec, it's not a good place to start, and may not be a part of the best solution. To train good word-vectors, you need more data than you have. (To re-use word-vectors from elsewhere, they should be well-matched to your domain.)</p>
<p>Word-vectors are most likely to help if you've gots tens-of-thousands to hundreds-of-thousands of terms, and many contextual examples of each of their uses, to plot their subtle variations-of-meaning in a dense shared space. Only 30,000 'texts', of ~3-5 tokens each, and only ~1200 unique tokens, is fairly small for word2vec.</p>
<p>(I made similar points in my <a href=""https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"" rel=""nofollow noreferrer"">comments on one of your earlier questions</a>.)</p>
<p>Once you've turned each row into a feature vector – whether it's by averaging symptom word-vectors, or probably better creating a bag-of-words representation – you can and should try many different classifiers to see which works best.</p>
<p>Many are drop-in replacements for each other, and with the size of your data, testing many against each other in a loop may take less than an hour or few.</p>
<p>If at a total loss where to start, anything listed in the 'classifiers' upper-left area of <a href=""https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"" rel=""nofollow noreferrer"">this <code>scikit-learn</code> graphical guide</a> is worth trying:</p>
<p><a href=""https://i.sstatic.net/t9nHP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t9nHP.jpg"" alt=""scikit-learn &quot;choosing the right estimator&quot;"" /></a></p>
<p>If you want to consider an even wider range of possibilities, and get a vaguely-intuitive idea of which ones can best discover certain kinds of &quot;shapes&quot; in the underlying high-dimensional data, you can look at all those demonstrated in <a href=""https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"" rel=""nofollow noreferrer"">this <code>scikit-learn</code> &quot;classifier comparison&quot; page</a>, with these graphical representations of how well they handle a noisy 2d classification challenge (instead of your 1200d challenge):</p>
<p><a href=""https://i.sstatic.net/RRJqs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RRJqs.jpg"" alt=""enter image description here"" /></a></p>
",3,0,786,2020-07-26 20:01:10,https://stackoverflow.com/questions/63105091/build-a-multiclass-text-classifier-which-takes-vectors-generated-from-word2vec-a
Features in a classifier,"<p>I am testing different classifiers (SVM, Logistic Regression, Random Forest, Naive Bayes, Gradient Boosting).</p>
<p>My dataset is similar to this:</p>
<pre><code>Text                 User         Date          Label
some text here     LucaDiMauro   2020/02/12        0
learning ML!!!     Mika          2018/12/03        1
Attention please!  user2         2012/02/04        1
</code></pre>
<p>and so on.</p>
<p><code>1</code> identifies normal content; <code>0</code> identifies potential spam content.</p>
<p>I identified the most important features that can capture credibility in subject: presence of numbers in the username, number of words, characters, special characters, use of pronouns, use of number at the beginning of the sentence. I would like to know how to check performance of classifiers (need one, not all) with these selected features.</p>
<p>Some of my features are as follows:</p>
<pre><code>df['Punctuation']=df['Text'].str.findall('[?!&lt;&gt;']+')
Count = df['Text'].str.split().str.len()
df['comma_count'] = df.Text.str.count(',')
df.Text.astype(str).sum(axis=1).str.len()
df['User'] = pd.np.where(Text.str.contains(&quot;0&quot;),&quot;None&quot;,
</code></pre>
<p>I would like just to see how to consider also these features in a model to predict some other spam/not spam email.
It is not clear how to include these features in my classifier. I have always considered Text as variable for cleaning, pre-processing... and I have never taken into consideration other features: only Label (y) and Text as X.</p>
<p>For instance, I used this classifier:</p>
<pre><code># Import train_test_split function
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics

# Split dataset into training set and test set

y=df['Label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, stratify=y)

# all parameters not specified are set to their defaults
logisticRegr = LogisticRegression()
logisticRegr.fit(X_train, y_train)

logisticRegr.predict(X_test[0].reshape(1,-1))

logisticRegr.predict(X_test[0:10])

predictions = logisticRegr.predict(X_test)

score = logisticRegr.score(X_test, y_test)
print(score)

cm = metrics.confusion_matrix(y_test, predictions)
print(cm)
</code></pre>
<p>I would like to know if in this code I am considering the other features or only Text.
If you could give me an example of integration of some features in a classifier would be extremely useful.</p>
","python, nltk, text-classification","<p>It depends on the pre-processing you've done.</p>
<p>If you are unsure if other features are included, just try printing the head of X and see if you have other features included.</p>
<p>Considering the pre-processing you've done, your code most likely considers the other features unless you've deliberately decided to consider only 'Text' for X.</p>
<p><em>On a side note</em>, if you've extracted useful information from the Text attribute and have them as separate attributes, you may not really need 'Text' anymore.</p>
<p>Couldn't add a comment as I am new here. Could you please include the head of X in the question?</p>
<p><strong>Edit:</strong>
You can try this..</p>
<pre><code># Extract features from Text and User as per your observations..
df['Punctuation']= df['Text'].str.contains(&quot;[?!&lt;&gt;']+&quot;)
Count = df['Text'].str.split().str.len()
df['comma_count'] = df.Text.str.count(',')
df['UserHasDigit'] = df['User'].apply(lambda x: 1 if(any(char.isdigit() for char in x)) == True else 0)

# Add more if you find any useful features

# Split the dependent (Target/Label) column from independent (features) columns
y = df['Label']
X = df.drop(columns=['Text', 'User', 'Label', 'Date']) # Drop attributes from which you extracted features and the ones that add no value

# print the head of X and y to see if it is correct
print(&quot;X&quot;)
print(X.head())
print(&quot;------&quot;)
print(&quot;y&quot;)
print(y.head())

# Apply any encoding if needed (Label-encoding / one-hot encoding)
# Now, apply test train split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, stratify=y)

# carry on with the modeling as usual..

# ----- copied from your code ------
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
.
.
.
.
</code></pre>
<p>In this code, it is considering all the features (punctuation, comma_count, UserHasDigit.. more if you added more features) and not 'Text' as it has been dropped.</p>
",0,1,174,2020-07-31 18:35:12,https://stackoverflow.com/questions/63197345/features-in-a-classifier
How can I use GPT 3 for my text classification?,"<p>I am wondering if I can be able to use OpenAI GPT-3 for transfer learning in a text classification problem?
If so, how can I get start on it using Tensorflow, Keras.</p>
","keras, text-classification, transfer-learning, openai-api, gpt-3","<p>(i substituted hateful language with ******** in the following samples)</p>
<p>Given samples like:</p>
<pre><code>(&quot;You look like ****** *** to me *******&quot;, true)
(&quot;**** you *********&quot;, true)
(&quot;**** my ****&quot;, true)
(&quot;hey my name is John can you help me?&quot;, false)
(&quot;hey my name is John, i think you ****** ***!&quot;, true)
(&quot;i have a problem with my network driver hpz-3332d&quot;, false)
</code></pre>
<p>GPT-3 can indeed then decide if a given input is hateful or not. GPT-3 actually is implementing filters that will very effectively tell if an arbitrary comment is hatefull or not. You would just enter the msg and let GPT3 autcomplete the  <code>, true|false)</code> part at the end, setting tokens to about ~6 and temperature setting 90%.</p>
<p>Boolean-ish classification that also relies on more complex context (you can insult someone without using foul-language) id doeable with GPT3 and can also be done with GPT2.</p>
",6,6,10651,2020-08-09 02:17:17,https://stackoverflow.com/questions/63321892/how-can-i-use-gpt-3-for-my-text-classification
what is the difference between pooled output and sequence output in bert layer?,"<p>everyone! I was reading about Bert and wanted to do <strong>text classification</strong> with its word embeddings. I came across this line of code:</p>
<pre><code>pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   
</code></pre>
<p>and then:</p>
<pre><code>clf_output = sequence_output[:, 0, :]
out = Dense(1, activation='sigmoid')(clf_output)
</code></pre>
<p>But I can't understand the use of pooled output. <strong>Doesn't sequence output contain all the information including the word embedding of ['CLS']?</strong> If so, why do we have pooled output?</p>
<p><em>Thanks in advance!</em></p>
","python-3.x, tensorflow, neural-network, text-classification, bert-language-model","<p>If you have given a sequence, &quot;You are on StackOverflow&quot;. The sequence_output will give 768 embeddings of these four words. But, the pooled output will just give you one embedding of 768, it will pool the embeddings of these four words.</p>
",2,4,6206,2020-08-12 13:09:10,https://stackoverflow.com/questions/63377198/what-is-the-difference-between-pooled-output-and-sequence-output-in-bert-layer
Encoding multiple columns in pandas,"<p>I have some doubts regarding encoding (I am not familiar with tasks like these) categorical variables in order to use them as parameters in a model like logistic regression or SVM. My dataset looks like the following:</p>
<pre><code>Text                                  Symbol    Note    Account    Age   Label 
There is a red car                      !        red      John    24   1
My bag was very expensive               ?       orange    Luke    36  0
Where are my keys?                      @        red      Red     58  1
I promise: I will never let you go!    ...       green    Aoife   28  0
</code></pre>
<p>In 'Text', there are stored comments from users in a community. 'Symbol' includes the most used symbol by a user. 'Note' represents its level (green is more experienced; red is a new joiner) 'Account' is the user name. 'Label' gives information about the user’s trustworthiness (if 0 the user is not fake; if 1 the user might be a possible bot.)</p>
<p>I would like to classify new users based on the current information (see columns above). My dataset includes more than 1000 rows and 400 users.
Since to use classifiers I need to encode categorical and text fields, I have tried to do as follows, by using <code>MultiColumnLabelEncoder</code> in sklearn:</p>
<pre><code>MultiColumnLabelEncoder(columns = ['Text', 'Symbol', 'Note', 'Account']).fit_transform(df)
</code></pre>
<p>where df is my dataframe. However, I understood that also OneHotEncoder should be preferable. I also included 'Account' as there might be more comments from the same account, so if I classified an account as fake and I receive a new comment from the same account, then this account could be easily detected as fake.</p>
<p>The aim, as I mentioned, would be to classify with a certain accuracy, new elements from a test set based on the information given (symbol, note, age, texts), i.e. looking for possible correlations among these variables which can allow me to say that a new account is fake (1) or not (0).</p>
<p>The problem, as you can see, is related to classifiers where parameters are not only numerical but also categorical.</p>
<p>For data preprocessing (removing stopwords and cleaning data), I have used  Python packages of NLTK; regarding features extraction ( this should be a key point as it is linked to the next step, i.e. using a classifier to predict class - 1 or 0), I have found difficulties in understanding what output I should expect from the encoding in order to be able to use information above as inputs in my model (where target is called label and it is a binary value).
I am using as classifier logistic regression, but also SVM.</p>
<p>My expected output in case of user X (age 16, symbol #, note Wonderful, and note red - anew joiner) would be classification as fake with a certain percentage.</p>
<p>I would appreciate if someone could explain to me, step by step, the way to transform my dataset in a dataset whose variables I can use within a logistic regression in order to determine the label (fake or not fake) of new users.</p>
","python, encoding, scikit-learn, text-classification","<p>I did this based on some old code of mine that is itself based on <a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">scikit-learn working with text</a>. Let me also reference, <a href=""https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"" rel=""nofollow noreferrer"">Scikit-learn 6.2.3</a> and note that <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a> will be of particular interest as it contains what you want to do with OneHotEncoder and more. From CountVectorizer documentation:</p>
<blockquote>
<p>CountVectorizer implements both tokenization and occurrence counting
in a single class:</p>
</blockquote>
<p>In the example case you provided, you have a total number of 95 words which consist of 22 unique words -- assuming you used all the words which probably isn't what you would want. Put differently, words like &quot;there, is, a, my, was, I, where and which&quot; probably can't help you tell a good account from a bogus one but words like &quot;Nigeria, prince, transfer, bank, penis, or enlargement&quot; are likely indicative of spam.</p>
<p>So, you'd have 22 dimensions (<em>minus whatever excluded ones</em>) of data before you go to the other columns like age, symbol, etc. That's a lot of useless data (all those 0's for nothing you need) so people either store it as a sparse matrix and/or use some sort of dimension reduction like Lasso or Ridge. You might think that's exactly what you want to do right now and that you're on the right track. That would be a bit different than what you asked though. And you kinda are except there are a couple of more points to deal with.</p>
<p>First, and I think this is the important one, some of your fields should be suspect as they are user reported (like age) or are useless/redundant (like the name). No kid goes on a porn or distillery site and says they are 15. No pervy old guy says he's 65 looking to chat with underage kids. Even dating sites where you think people would eventually find out. People lie about their ages. The same goes for names. You can include them if you want but remember the old adage: Garbage In, Garbage Out.</p>
<p>Second, Lasso and Ridge regressions both assign cost functions to help with overfitting models. So, house price base on square footage and zip code makes sense. But when you get down to the last time a property tax assessment was done or the distance to the nearest library you might be thinking &quot;Really?&quot; But that's not really something you have.</p>
<p>Putting those two together, in your case you have Text (definitely useful), symbol (a derivative of text), account and age (see the above note), note (probably useful for the time they've been on and active), and label -- your assessment. So, of five fields, only two are likely to be useful in predicting the assessment. All this is to say that while you can use lasso or ridge, you might be better served using Bayes for this task. If you're up for it there are multiple pages that will show they are equivalent under certain conditions<a href=""https://math.stackexchange.com/questions/2984440/bayesian-interpretation-for-ridge-regression-and-the-lasso""> [example]</a>. But the reason to consider Bayes is the computational load for this example.</p>
<p>Symbols (part iv) I've been loathe to say this but, from experience, punctuation is not a good indicator. The reason I say loathe is that you might come up with some novel implementation. But a lot a have tried, so the odds are small. Part of this is related to Zipf's Law which has to do with words rather than punctuation. However, if you make punctuation to carry some sort of additional semantic meaning, it is essentially another word. Remember the goal is not to find that a symbol is in spam, rather, the goal is to find if the symbol is a reliable indicator of spam <em>and is sufficiently unique</em>.</p>
<p>But if you really wanted to add punctuation as some sort of indicator, you might need to think of it differently. For example, is just the presence of a question mark enough? Or, is having three or more in a row? Or, a high percentage of characters per {text, email, message, post, etc}? This gets into feature engineering which is part of why I would say you need to think through it. Personally (and from a quick look through my spam folder) I'd look at emoji, foreign characters (e.g., £) and perhaps text effects (bold, underlined, etc). But you then have a separate and second question. With text content, you have probabilistic loadings with say an aggerate measurement:</p>
<p><code>print(f&quot;{message} is flagged for consideration at {loading}%.</code></p>
<p>But amongst those options suggested above you would need to develop some sort of weighting for that feature. You <em>could</em> just append the symbol to each Text field but before TF-IDF. But then you need to use a different approach. You could also assign a weighting to the content and a second one to your engineered feature that would be based off Principle Component Analysis and/or Confusion Matrix.</p>
<p>For example - Text 34 is known spam:</p>
<blockquote>
<p>N£w Skinny Pill Kills Too Much Fat? This Diet is Sweeping The Nation</p>
</blockquote>
<p>The Bayesian approach assigns an aggregate probability of 94% spam, well above your threshold of 89%. But it's known spam with a probability of 1(00%). The delta of 6% would be due to what most likely? I'd argue in this case it's the £.</p>
<p>The same applies with label. From your train set, you may have zero accounts over 2 years that send spam and 90% come from accounts less than 1 week.</p>
<p>Anyway, on to the code and implementation.</p>
<h3>1. Mung data.</h3>
<p>This is supervised so 'Label' is critical by definition.</p>
<h3>2. Train-test split</h3>
<p>You didn't mention this but it's worth noting. <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer"">sklearn.model_selection.train_test_split</a></p>
<h3>3. Tokenizing text with scikit-learn.</h3>
<p>This is where what you're specifically asking starts. Turn the corpus (the collection of documents) into a bag-of-words. You said you were using NLTK which is good for academia but I find overly cumbersome. SpacCy is great, Gensim rocks. But I'm using scikit-learn. My code varies a bit from the example in that it shows a bit of what is going on behind the scenes.</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(lowercase=True, tokenizer=None, stop_words='english',
   analyzer='word', max_df=1.0, min_df=1, max_features=None)
count_vect.fit(your training data)

# uncomment if you'd like to know the mapping of the columns to the words.
# count_vect.vocabulary_
# for key in sorted(count_vect.vocabulary_.keys()):
#     print(&quot;{0:&lt;20s} {1}&quot;.format(key, count_vect.vocabulary_[key]))
</code></pre>
<p>About the training set:</p>
<pre><code>X_train_counts = count_vect.transform(your training data)
print(&quot;The type of X_train_counts is {0}.&quot;.format(type(X_train_counts)))
print(&quot;The X matrix has {0} rows (documents) and {1} columns (words).&quot;.format(
        X_train_counts.shape[0], X_train_counts.shape[1]))
</code></pre>
<p>That will give you something like this:</p>
<pre><code>The type of X_train_counts is &lt;class 'scipy.sparse.csr.csr_matrix'&gt;.
The X matrix has 2257 rows (documents) and 35482 columns (words).
</code></pre>
<h3>4. Convert them to frequencies (tf or tf-idf).</h3>
<p>You have occurrences of words. CountVectorizer is just the number of times each word appears in each document. For each document, we would like to normalize by the number of words. This is the term (or word) frequency. IDF is useful in avoiding underflow errors resulting from dividing the one occurrence you have of a word by the gargantuan data set of words. Which is not true in your case but normally is an issue.</p>
<h3>5. Ok, now you can start training a classifier.</h3>
<p>Stick with the Scikit learn example on this, at least for now. They're using naive Bayes and I laid out my reasoning for why I think Lasso and Ridge aren't best suited in this case. But if you want to go with a regression model, you're set up for it too. If you want to add in your other fields (symbol, age, etc) you might consider just appending them to each record.</p>
<p>At this point I  have another couple of steps:</p>
<h3>6. Find out the tokens associated with each category as a sniff test.</h3>
<p>In general, picking the categories and words associated with each is somewhat of an art. You will probably have to iterate on this.</p>
<pre><code>feature_words = count_vect.get_feature_names()
n = 7 #number of top words associated with the category that we wish to see

for cat in range(len(categories)):
    print(f&quot;\nTarget: {cat}, name: {target_names[cat]}&quot;)
    log_prob = nb_model.feature_log_prob_[cat]
    i_topn = np.argsort(log_prob)[::-1][:n]
    features_topn = [feature_words[i] for i in i_topn]
    print(f&quot;Top {n} tokens: &quot;, features_topn)
</code></pre>
<h3>7. Prediction as a second sniff test.</h3>
<p>A new doc or three that you make up going off similar classification. Then:</p>
<pre><code>X_new_counts = count_vect.transform(docs_new)
X_new_tfidf = tfidf_transformer.transform(X_new_counts)
predictions = nb_model.predict(X_new_tfidf)
print('Predictions')
for doc, category in zip(docs_new, predictions):
    print(&quot;{0} =&gt; {1}&quot;.format(doc, twenty_train.target_names[category]))
</code></pre>
<h3>8 &amp; 9. Pipeline and Eval as done in the tutorial.</h3>
<h3>Additional information from follow on questions.</h3>
<ul>
<li>Answers for some parts of your questions have been incorporated above.</li>
<li>Train-test before tokenization or vice versa. I've chosen my words carefully here, so read this part carefully. Currently, it's good practice is to do split then tokenize. The rationale is reproducibility. Others tokenize then split. The rationale is computational efficiency and the term freq would be the same for both. You will see both done <em>all the time</em>. A Data <em><strong>Scientist</strong></em> would test it extensively.</li>
<li>What will output look like? It depends on what your problem is, what stage you're at, and how you code it. You seem to just be doing a spam filter. At some point, you'll have a set of loadings that will typically take the form of <code>word: tf-idf</code> loading with several terms/loadings for each document. You may or may have set at a threshold so you only see the filter results in addition to the model's probabilistic results.</li>
<li>What about the other columns? As I said before, 'Label' is critical as this is supervised learning. Age is useless; Name is probably useless <em><strong>unless all names are unique</strong></em>. Fun fact: there are about 150 named 'math' or 'Math' on StackOverflow. Presumable only one has your user number. 'Symbol' is tricky and you should think hard about it.</li>
</ul>
<p>Last point. There is a reason why this is a field on it's own. There is a reason why books are written on it and why there are multiple article series on it. So, cramming this onto one wall of text, while concise, is probably sub-optimal in that there is so very much not included that you need to know.</p>
",1,1,1216,2020-08-21 13:52:08,https://stackoverflow.com/questions/63524164/encoding-multiple-columns-in-pandas
Extract Dictionary Values from Classifier Output,"<p>I'm trying zero-shot classification. I get an output like below</p>
<pre><code>[{'labels': ['rep_appreciation',
   'cx_service_appreciation',
   'issue_resolved',
   'recommend_product',
   'callback_realted',
   'billing_payment_related',
   'disppointed_product'],
  'scores': [0.9198898673057556,
   0.8672246932983398,
   0.79215407371521,
   0.6239275336265564,
   0.4782547056674957,
   0.39024001359939575,
   0.010263209231197834],
  'sequence': 'Alan Edwards provided me with nothing less the excellent assistance'}
</code></pre>
<p>Above is output for one row in a data frame</p>
<p>I'm hoping to finally build a data frame columns and output values mapped like below. 1s for labels if the scores are above certain threshold</p>
<p><a href=""https://i.sstatic.net/HJ4XU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HJ4XU.png"" alt=""output sample"" /></a></p>
<p>Any nudge/help to solve this is highly appreciated.</p>
","python, pandas, dataframe, dictionary, text-classification","<p>Define a function which returns a key : value dictionary for every row, with key being the label and value being 1/0 based on threshold</p>
<pre><code>def get_label_score_dict(row, threshold):
    result_dict = dict()
    for _label, _score in zip(row['labels'], row['scores']):
        if _score &gt; threshold:
            result_dict.update({_label: 1})
        else:
            result_dict.update({_label: 0})
    return result_dict
</code></pre>
<p>Now if you have a <em>list_of_rows</em> with each row being in the form as shown above, then you can use the <em>map</em> function to get the above mentioned dictionary for every row. Once you get this, convert it into a DataFrame.</p>
<pre><code>th = 0.5    #whatever threshold value you want
result = list(map(lambda x: get_label_score_dict(x, th), list_of_rows))
result_df = pd.DataFrame(result)
</code></pre>
",1,0,155,2020-08-25 14:11:31,https://stackoverflow.com/questions/63580755/extract-dictionary-values-from-classifier-output
What is the state of GPT-3 for text classification in spanish?,"<p>I would like to know if it is possible to reuse gpt-3 in a different language, Spanish in this case.</p>
<p>Do I need a gpt-3 model specifically trained with a Spanish corpus, or can I use transfer learning to produce Spanish text?</p>
","nlp, text-classification","<p>GPT-3 is only available via an API and only to people who <a href=""https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8s0v5E5gdyQhOuZCXNuMR8i1UQjFWVTVUVEpGNkg3U1FNRDVVRFg3U0w4Vi4u"" rel=""nofollow noreferrer"">apply for the access</a>. The model is too big to run it locally on any reasonable hardware and fine-tuning is thus hardly an option.</p>
<p>Given how well GPT-3 works machine translation, my guess is that it will work reasonably well for Spanish by default. However, if your task is text classification, you can do a much better job when using a pre-trained BERT-like model, <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Hugginface's Transformers</a> already have several <a href=""https://huggingface.co/models?search=spanish"" rel=""nofollow noreferrer"">models for Spanish</a>.</p>
",3,1,1654,2020-08-29 20:01:18,https://stackoverflow.com/questions/63651263/what-is-the-state-of-gpt-3-for-text-classification-in-spanish
HuggingFace Transformers model for German news classification,"<p>I've been trying to find a suitable model for my project (multiclass German text classification) but got a little confused with the models offered <a href=""https://huggingface.co/models?search=german"" rel=""nofollow noreferrer"">here</a>. There are models with <code>text-classification</code> tag, but they are for binary classification. Most of the other models are for <code>[MASK]</code> word predicting. I am not sure, which one to choose and if it will work with multiple classes at all</p>
<p>Would appreciate any advice!</p>
","python, nlp, text-classification, bert-language-model, huggingface-transformers","<p>You don't need to look for a specific text classification model when your classes are completely different because most listed models used one of the base models and finetuned the base layers and trained the output layers for their needs. In your case you will remove the output layers and their finetuning of the base layers will not benefit or hurt you much. Sometimes they have extended the vocabulary which could be beneficial for your task but you have to check description (which is often sparse :() and the vocabulary by yourself to get more details about the respective model.</p>
<p>In general I recommend you to work with one of the base models right away and only look for other models in case of insufficient results.</p>
<p>The following is an example for bert with 6 classes:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertForSequenceClassification, BertTokenizer

tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-german-dbmdz-uncased&quot;)
model = BertForSequenceClassification.from_pretrained(&quot;bert-base-german-dbmdz-uncased&quot;, num_labels=6)
</code></pre>
",1,0,605,2020-08-31 13:56:09,https://stackoverflow.com/questions/63672169/huggingface-transformers-model-for-german-news-classification
Keras character level LSTM text classification not training,"<p>I am making a keras model for character level text classification using LSTM (my first model). The model is supposed to classify normal, spam, and rude messages from a twitch chat. However the results I am getting are quite disappointing and confusing.</p>
<p>The LSTM network learns very little and the accuracy is horrible no matter what I do.</p>
<p>This is my code</p>
<pre><code>import tensorflow as tf
''' import tensorflowjs.converters '''
from tensorflow import keras
from tensorflow.keras import layers
import json
import numpy as np
import re
import random
import sys

np.set_printoptions(threshold=sys.maxsize)

vocab = &quot; qwertyuiopasdfghjklñzxcvbnmç1234567890?¿¡!,:-+/@#áéíóú\/&quot;

dropout = 0.2

x_train = []
y_train = []
one_hot_encode = []
sentence = []

# amount of examples in each class
maxofeachtype = 1600

countnormal = 0
countspam = 0
countofensivo = 0

# Load dataset from data.json
with open(&quot;./data.json&quot;, 'r', encoding=&quot;utf8&quot;) as file:
    data = json.load(file)

# suffle it
random.shuffle(data)

# create the vocabulary map
mapping = {}
for x in range(len(vocab)):
    mapping[vocab[x]] = x

# this is some to balance the dataset adjusting it to &quot;maxofeachtype&quot;
for example in data:
    if(example[&quot;y&quot;] == [1, 0, 0] and countnormal &lt; maxofeachtype):
        countnormal += 1
    elif(example[&quot;y&quot;] == [0, 1, 0] and countspam &lt; maxofeachtype):
        countspam += 1
    elif(example[&quot;y&quot;] == [0, 0, 1] and countofensivo &lt; maxofeachtype):
        countofensivo += 1
    elif(countnormal == maxofeachtype or countspam == maxofeachtype or countofensivo == maxofeachtype):
        continue

    # remove unwanted characters to only have the ones in vocab
    cleanexample = re.sub(
        r'[^qwertyuiopasdfghjklñzxcvbnmç1234567890?¿¡!,:@#áéíóú\/]', '', str(example[&quot;x&quot;]))

    # if the sentence is less than 500 characters long (the max you can type in twitch) add spaces until it gets to 500 chars long
    if len(cleanexample) != 500:
        for a in range(500 - len(cleanexample)):
            cleanexample = cleanexample + &quot; &quot;
    for character in cleanexample:
        sentence.append(mapping[character])

    # print(sentence)
    y_train_ohe = tf.one_hot(sentence, depth=len(vocab)).numpy()
    # print(y_train_ohe)
    x_train.append(y_train_ohe)
    y_train.append(np.array(example[&quot;y&quot;]))
    sentence = []

x_train = np.array(x_train)
y_train = np.array(y_train)
&quot;&quot;&quot; print(x_train[0][0:5], x_train[0][-5:], y_train[0]) &quot;&quot;&quot;
print(x_train.shape[1], x_train.shape[2])
print(x_train.shape)
print(y_train.shape)

# Create the model
model = keras.Sequential()

model.add(layers.LSTM(256, activation=&quot;tanh&quot;,
                      return_sequences=True, dropout=dropout, input_shape=(500, 57)))

model.add(layers.LSTM(128, activation=&quot;tanh&quot;,
                      return_sequences=False, dropout=dropout))

model.add(layers.Dense(3, activation=&quot;softmax&quot;))

optimizer = keras.optimizers.Adam(lr=0.01)

model.compile(optimizer=optimizer, loss=&quot;categorical_crossentropy&quot;,
              metrics=[&quot;accuracy&quot;])

model.summary()

model.fit(x=x_train, y=y_train, epochs=15, shuffle=True,
          batch_size=25, validation_split=0.2)

model.save('model_py.h5')

''' tensorflowjs.converters.save_keras_model(model, &quot;./modelo_js&quot;) '''
</code></pre>
<p>The training samples look like this before processing. [1, 0, 0] means normal, [0, 1, 0] rude language and [0, 0, 1] means spam:</p>
<pre><code>&quot;x&quot;: &quot;sentence&quot;,
        &quot;y&quot;: [
            1,
            0,
            0
        ]
</code></pre>
<p>And they look like this after processing. I one hot encode them to a vector length of 57, the vocabulary size. The arrays starting with 1 are spaces:</p>
<pre><code>[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0.]]
  ....
 [1 0 0]
</code></pre>
<p>And after training with 1600 examples of each class and validation_split = 0.2 these are the results:</p>
<pre><code>(4800, 500, 57)
(4800, 3)
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm (LSTM)                  (None, 500, 256)          321536
_________________________________________________________________
lstm_1 (LSTM)                (None, 128)               197120
_________________________________________________________________
dense (Dense)                (None, 3)                 387
=================================================================
Total params: 519,043
Trainable params: 519,043
Non-trainable params: 0
_________________________________________________________________
Epoch 1/15
2020-09-09 12:35:47.606648: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-09 12:35:47.872095: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
154/154 [==============================] - 13s 87ms/step - loss: 1.0811 - accuracy: 0.4120 - val_loss: 2.0132 - val_accuracy: 0.0219
Epoch 2/15
154/154 [==============================] - 12s 78ms/step - loss: 1.0577 - accuracy: 0.4177 - val_loss: 2.0314 - val_accuracy: 0.0000e+00
Epoch 3/15
154/154 [==============================] - 12s 76ms/step - loss: 1.0457 - accuracy: 0.4154 - val_loss: 1.6968 - val_accuracy: 0.0000e+00
Epoch 4/15
154/154 [==============================] - 12s 76ms/step - loss: 1.0506 - accuracy: 0.4161 - val_loss: 1.7731 - val_accuracy: 0.0000e+00
Epoch 5/15
154/154 [==============================] - 11s 73ms/step - loss: 1.0511 - accuracy: 0.4313 - val_loss: 1.9052 - val_accuracy: 0.0000e+00
Epoch 6/15
154/154 [==============================] - 12s 75ms/step - loss: 1.0473 - accuracy: 0.4104 - val_loss: 1.6291 - val_accuracy: 0.0000e+00
Epoch 7/15
154/154 [==============================] - 13s 84ms/step - loss: 1.0464 - accuracy: 0.4135 - val_loss: 1.8916 - val_accuracy: 0.0000e+00
Epoch 8/15
154/154 [==============================] - 12s 76ms/step - loss: 1.0404 - accuracy: 0.4208 - val_loss: 1.8094 - val_accuracy: 0.0000e+00
Epoch 9/15
154/154 [==============================] - 12s 76ms/step - loss: 1.0449 - accuracy: 0.4096 - val_loss: 1.9690 - val_accuracy: 0.0219
Epoch 10/15
154/154 [==============================] - 12s 77ms/step - loss: 1.0489 - accuracy: 0.4104 - val_loss: 1.9596 - val_accuracy: 0.0000e+00
Epoch 11/15
154/154 [==============================] - 13s 83ms/step - loss: 1.0455 - accuracy: 0.4141 - val_loss: 1.8082 - val_accuracy: 0.0000e+00
Epoch 12/15
154/154 [==============================] - 12s 76ms/step - loss: 1.0465 - accuracy: 0.4219 - val_loss: 1.7066 - val_accuracy: 0.0000e+00
Epoch 13/15
154/154 [==============================] - 12s 75ms/step - loss: 1.0424 - accuracy: 0.4161 - val_loss: 1.5192 - val_accuracy: 0.0000e+00
Epoch 14/15
154/154 [==============================] - 12s 75ms/step - loss: 1.0481 - accuracy: 0.4154 - val_loss: 1.5999 - val_accuracy: 0.0000e+00
Epoch 15/15
154/154 [==============================] - 12s 77ms/step - loss: 1.0476 - accuracy: 0.4008 - val_loss: 2.0612 - val_accuracy: 0.0000e+00 
</code></pre>
<p>The weird thing is that if I increase the validation split the results improve. This makes no sense to me because it's less training data.</p>
<p>This is with validation_split = 0.6</p>
<pre><code>77/77 [==============================] - 8s 103ms/step - loss: 1.0352 - accuracy: 0.4432 - val_loss: 1.4233 - val_accuracy: 0.2313
Epoch 2/15
77/77 [==============================] - 7s 93ms/step - loss: 0.9906 - accuracy: 0.4443 - val_loss: 1.7316 - val_accuracy: 0.2937
Epoch 3/15
77/77 [==============================] - 7s 92ms/step - loss: 0.9863 - accuracy: 0.4812 - val_loss: 1.5367 - val_accuracy: 0.2313
Epoch 4/15
77/77 [==============================] - 7s 94ms/step - loss: 0.9874 - accuracy: 0.4635 - val_loss: 1.4075 - val_accuracy: 0.2937
Epoch 5/15
77/77 [==============================] - 7s 93ms/step - loss: 0.9905 - accuracy: 0.4594 - val_loss: 1.5759 - val_accuracy: 0.2937
Epoch 6/15
77/77 [==============================] - 7s 93ms/step - loss: 0.9808 - accuracy: 0.4703 - val_loss: 1.3886 - val_accuracy: 0.2937
Epoch 7/15
77/77 [==============================] - 7s 96ms/step - loss: 0.9815 - accuracy: 0.4781 - val_loss: 1.2495 - val_accuracy: 0.2313
Epoch 8/15
77/77 [==============================] - 7s 96ms/step - loss: 0.9824 - accuracy: 0.4698 - val_loss: 1.4516 - val_accuracy: 0.2313
Epoch 9/15
77/77 [==============================] - 7s 92ms/step - loss: 0.9916 - accuracy: 0.4573 - val_loss: 1.4488 - val_accuracy: 0.2313
Epoch 10/15
77/77 [==============================] - 7s 90ms/step - loss: 0.9858 - accuracy: 0.4760 - val_loss: 1.3868 - val_accuracy: 0.2313
Epoch 11/15
77/77 [==============================] - 7s 93ms/step - loss: 0.9861 - accuracy: 0.4734 - val_loss: 1.5702 - val_accuracy: 0.2313
Epoch 12/15
77/77 [==============================] - 7s 91ms/step - loss: 0.9880 - accuracy: 0.4630 - val_loss: 1.4439 - val_accuracy: 0.2313
Epoch 13/15
77/77 [==============================] - 7s 91ms/step - loss: 0.9796 - accuracy: 0.4865 - val_loss: 1.3597 - val_accuracy: 0.2313
Epoch 14/15
77/77 [==============================] - 7s 91ms/step - loss: 0.9832 - accuracy: 0.4745 - val_loss: 1.5791 - val_accuracy: 0.2313
Epoch 15/15
77/77 [==============================] - 7s 90ms/step - loss: 0.9919 - accuracy: 0.4760 - val_loss: 1.6243 - val_accuracy: 0.2313
</code></pre>
<p>And with validation_split = 0.8</p>
<pre><code>39/39 [==============================] - 7s 171ms/step - loss: 1.1238 - accuracy: 0.4484 - val_loss: 1.3041 - val_accuracy: 0.3158
Epoch 2/15
39/39 [==============================] - 6s 143ms/step - loss: 0.9795 - accuracy: 0.4692 - val_loss: 1.2562 - val_accuracy: 0.3174
Epoch 3/15
39/39 [==============================] - 6s 146ms/step - loss: 0.9757 - accuracy: 0.4724 - val_loss: 1.3583 - val_accuracy: 0.3437
Epoch 4/15
39/39 [==============================] - 6s 149ms/step - loss: 0.9741 - accuracy: 0.4703 - val_loss: 1.3565 - val_accuracy: 0.2976
Epoch 5/15
39/39 [==============================] - 6s 148ms/step - loss: 0.9748 - accuracy: 0.4578 - val_loss: 1.3904 - val_accuracy: 0.2976
Epoch 6/15
39/39 [==============================] - 5s 137ms/step - loss: 0.9697 - accuracy: 0.4755 - val_loss: 1.3418 - val_accuracy: 0.2976
Epoch 7/15
39/39 [==============================] - 5s 136ms/step - loss: 0.9716 - accuracy: 0.4765 - val_loss: 1.3053 - val_accuracy: 0.3262
Epoch 8/15
39/39 [==============================] - 5s 136ms/step - loss: 0.9748 - accuracy: 0.4557 - val_loss: 1.3529 - val_accuracy: 0.2976
Epoch 9/15
39/39 [==============================] - 5s 140ms/step - loss: 0.9768 - accuracy: 0.4505 - val_loss: 1.3260 - val_accuracy: 0.2976
Epoch 10/15
39/39 [==============================] - 5s 136ms/step - loss: 0.9724 - accuracy: 0.4859 - val_loss: 1.3351 - val_accuracy: 0.3627
Epoch 11/15
39/39 [==============================] - 6s 143ms/step - loss: 0.9748 - accuracy: 0.4588 - val_loss: 1.3203 - val_accuracy: 0.3770
Epoch 12/15
39/39 [==============================] - 6s 144ms/step - loss: 0.9690 - accuracy: 0.4640 - val_loss: 1.3207 - val_accuracy: 0.3517
Epoch 13/15
39/39 [==============================] - 5s 137ms/step - loss: 0.9661 - accuracy: 0.4369 - val_loss: 1.3153 - val_accuracy: 0.3681
Epoch 14/15
39/39 [==============================] - 6s 141ms/step - loss: 0.9628 - accuracy: 0.4661 - val_loss: 1.3405 - val_accuracy: 0.2976
Epoch 15/15
39/39 [==============================] - 5s 137ms/step - loss: 0.9625 - accuracy: 0.4703 - val_loss: 1.3586 - val_accuracy: 0.3457
</code></pre>
<p>I've tried using just dense layers and the results are so much better which to me doesn't make sense because they cannot understand sequences. However this clears up the option of the dataset being bad.</p>
<p>With this configuration (validation_split back to 0.2):</p>
<pre><code>model = keras.Sequential()

model.add(layers.Input(shape=(500, 57)))

model.add(layers.Flatten())

model.add(layers.Dense(256, activation=&quot;relu&quot;))

model.add(layers.Dense(128, activation=&quot;relu&quot;))

model.add(layers.Dense(64, activation=&quot;relu&quot;))

model.add(layers.Dense(3, activation=&quot;softmax&quot;))

optimizer = keras.optimizers.Adam(lr=0.01)

model.compile(optimizer=optimizer, loss=&quot;categorical_crossentropy&quot;,
              metrics=[&quot;accuracy&quot;])
</code></pre>
<p>I get these results:</p>
<pre><code>154/154 [==============================] - 1s 6ms/step - loss: 0.7377 - accuracy: 0.7844 - val_loss: 1.4061 - val_accuracy: 0.0250
Epoch 2/15
154/154 [==============================] - 1s 4ms/step - loss: 0.3479 - accuracy: 0.8448 - val_loss: 0.8703 - val_accuracy: 0.6927
Epoch 3/15
154/154 [==============================] - 1s 4ms/step - loss: 0.3033 - accuracy: 0.8794 - val_loss: 1.4597 - val_accuracy: 0.6938
Epoch 4/15
154/154 [==============================] - 1s 4ms/step - loss: 0.2899 - accuracy: 0.8966 - val_loss: 1.6684 - val_accuracy: 0.4896
Epoch 5/15
154/154 [==============================] - 1s 4ms/step - loss: 0.2447 - accuracy: 0.9042 - val_loss: 1.6465 - val_accuracy: 0.4812
Epoch 6/15
154/154 [==============================] - 1s 4ms/step - loss: 0.2269 - accuracy: 0.9211 - val_loss: 3.9954 - val_accuracy: 0.7312
Epoch 7/15
154/154 [==============================] - 1s 4ms/step - loss: 0.2071 - accuracy: 0.9201 - val_loss: 2.7729 - val_accuracy: 0.4698
Epoch 8/15
154/154 [==============================] - 1s 4ms/step - loss: 0.2081 - accuracy: 0.9302 - val_loss: 5.1325 - val_accuracy: 0.4229
Epoch 9/15
154/154 [==============================] - 1s 4ms/step - loss: 0.1581 - accuracy: 0.9378 - val_loss: 4.4410 - val_accuracy: 0.3688
Epoch 10/15
154/154 [==============================] - 1s 4ms/step - loss: 0.2184 - accuracy: 0.9333 - val_loss: 2.6669 - val_accuracy: 0.5396
Epoch 11/15
154/154 [==============================] - 1s 4ms/step - loss: 0.1673 - accuracy: 0.9341 - val_loss: 3.6476 - val_accuracy: 0.2750
Epoch 12/15
154/154 [==============================] - 1s 4ms/step - loss: 0.2111 - accuracy: 0.9443 - val_loss: 1.6768 - val_accuracy: 0.6885
Epoch 13/15
154/154 [==============================] - 1s 4ms/step - loss: 0.1210 - accuracy: 0.9547 - val_loss: 2.6785 - val_accuracy: 0.5406
Epoch 14/15
154/154 [==============================] - 1s 4ms/step - loss: 0.1179 - accuracy: 0.9542 - val_loss: 3.4468 - val_accuracy: 0.4385
Epoch 15/15
154/154 [==============================] - 1s 4ms/step - loss: 0.1265 - accuracy: 0.9469 - val_loss: 2.0159 - val_accuracy: 0.7083
</code></pre>
<p>The model changes val_accuracy quite a bit but at least I know it is learning something.</p>
<p>I've tried (with the LSTM model):</p>
<p>-Changing the number of layers
-Changing the number of neurons of each layer
-Changing the learning rate
-Changing the optimizer to SGD
-Changing the loss function
-Changing the number of epochs
-Changing the number of training samples (duplicating each sample)
-Using decay in Adam</p>
","python, tensorflow, keras, lstm, text-classification","<p>There are two problems I see here:</p>
<ul>
<li>LSTMs don't work well with one-hot input. Use <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"" rel=""nofollow noreferrer"">padded</a> <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"" rel=""nofollow noreferrer"">sequences</a> (e.g., <code>[5, 6, 8]</code>)</li>
<li>You need an <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"" rel=""nofollow noreferrer"">embedding layer</a> before the LSTM layers</li>
</ul>
<p>Changing only the data (and loss function), I made an example that works based on your architecture (with an added embedding layer):</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import sys

np.set_printoptions(threshold=sys.maxsize)

X_train = ['They like my dog', 'I hate my cat', 'We will love my hamster', 
           'I dislike your llama']
X_test = ['We love our hamster', 'They hate our platypus']
y_train = [1, 0, 1, 0]
y_test = [1, 0]

labels = {0: 'negative', 1: 'positive'}

encoder = keras.preprocessing.text.Tokenizer()

encoder.fit_on_texts(X_train)

X_train = encoder.texts_to_sequences(X_train)
X_test = encoder.texts_to_sequences(X_test)

max_length = max(map(len, X_train))

x_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_length)
x_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_length)

x_train = np.array(x_train)
x_test = np.array(x_test)
y_train = np.array(y_train)
y_test = np.array(y_test)

embedding_dim = 4
# print(x_train.shape[1], x_train.shape[2])
print(x_train.shape)
print(y_train.shape)

# Create the model
model = keras.Sequential()

model.add(layers.Embedding(len(encoder.index_word) + 1, embedding_dim))

model.add(layers.LSTM(8, activation=&quot;tanh&quot;,
                      return_sequences=True, dropout=.2))

model.add(layers.LSTM(8, activation=&quot;tanh&quot;,
                      return_sequences=False, dropout=.2))

model.add(layers.Dense(2, activation=&quot;softmax&quot;))

optimizer = keras.optimizers.Adam(lr=0.01)

model.compile(optimizer=optimizer, loss=&quot;sparse_categorical_crossentropy&quot;,
              metrics=[&quot;accuracy&quot;])

model.build(input_shape=x_train.shape)
model.summary()

history = model.fit(x=x_train, y=y_train, epochs=25, shuffle=True,
          batch_size=25, validation_data=(x_test, y_test))

</code></pre>
<p>Let me know if anything needs to be clarified.</p>
",3,2,1447,2020-09-09 12:10:18,https://stackoverflow.com/questions/63811413/keras-character-level-lstm-text-classification-not-training
"ValueError: Input 0 is incompatible with layer lstm_15: expected ndim=3, found ndim=2","<p>I want to feed the output of the CNN layer into LSTM layer but got an error <code>ValueError: Input 0 is incompatible with layer lstm_15: expected ndim=3, found ndim=2</code> with following code:</p>
<pre><code>inp = Input(shape = (max_length,))
xe = Embedding(vocabulary_size, 300, weights = [embedding_matrix], trainable = False)(inp)
x = Conv1D(512,kernel_size = 2, activation='relu',kernel_initializer = &quot;he_uniform&quot;)(xe)
x = GlobalMaxPooling1D()(x)
x = LSTM(128)(x)
x = Dense(11, activation = &quot;sigmoid&quot;)(x)
</code></pre>
<p>inputs shapes:</p>
<pre><code>embedding_matrix: (26441, 300)
inp : TensorShape([Dimension(None), Dimension(3146)])
X_train :(1432, 3146)
Y_train: (1432, 11)
vocabulary_size: 26441
max_length: 3146
</code></pre>
<p>Someone can help me</p>
","python, keras, lstm, text-classification, conv-neural-network","<p>This is because when you apply <code>GlobalMaxPooling1D</code>, it returns the tensor with shape <code>(batch_size, 512)</code>. As mentioned in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D"" rel=""nofollow noreferrer"">docs</a>, this layer <strong>downsamples the input representation by taking the maximum value over the time dimension</strong>. You have two options either not to use <code>GlobalMaxPool1D</code> (you can instead use local pooling layers such as <code>MaxPool1D</code>) or you can use <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector"" rel=""nofollow noreferrer""><code>RepeatVector</code></a> to change the shape of output of <code>GlobalMaxPool1D</code> from <code>(batch_size, 512)</code> to <code>(batch_size, n, 512)</code> where <code>n</code> is the parameter to <code>RepeatVector</code> defining, how many times you want the sequence to be repeated.</p>
",0,0,593,2020-10-10 12:52:18,https://stackoverflow.com/questions/64293642/valueerror-input-0-is-incompatible-with-layer-lstm-15-expected-ndim-3-found-n
Suppressing false positives (incorrectly classified as outlier/anomaly) in Anomaly Detection using Autoencoders,"<p>How does one suppress certain outliers in Anomaly detection?</p>
<p>We built a model using autoencoders and it has detected anomalies.
Some of the data points which are flagged as anomalies  (outside the normal distribution) are not actually anomalies.</p>
<p>How do we train the model to not recognize these as anomalies ?</p>
<p>Do we add multiple duplicates of these data points into the dataset and then train again, or are there any other techniques we can apply here.</p>
<p>Here the normal distribution is of Cosine Similarity (distance) since data points are vectorized representations of text data (log entries).  So if the cosine distance between the input and reconstructed vector does not fall under the normal distribution it is treated as anomaly.</p>
","python, machine-learning, text-classification, autoencoder, anomaly-detection","<p>Since the Anomaly Detector is usually trained unsupervised, it can be hard to incorporate labels directly into that process without loosing outlier detection properties.
A simple alternative is to take the instances that were marked as anomalies, and put them into a classifier that classifies into &quot;real anomaly&quot; vs &quot;not real anomaly&quot;. This classifier would be trained on prior anomalies that have been labeled. It can be either binary classification, or one-class wrt to known &quot;not real&quot; samples. A simple starting point would be k-Nearest-Neighbours or a domain-specific distance function. The classifier can use the latent feature vector as input, or do its own feature extraction.</p>
<p>This kind of system is described in <a href=""https://relayr.io/technology-blog/anomaly-detection-with-false-positive-suppression/"" rel=""nofollow noreferrer"">Anomaly Detection with False Positive Suppression (relayr.io)</a>.
The same basic idea is used in this paper to minimize False Negative Rate:
<a href=""https://ieeexplore.ieee.org/document/8683667"" rel=""nofollow noreferrer"">SNIPER: Few-shot Learning for Anomaly Detection to Minimize False-negative Rate with Ensured True-positive Rate</a></p>
",1,0,487,2020-10-26 07:48:55,https://stackoverflow.com/questions/64533169/suppressing-false-positives-incorrectly-classified-as-outlier-anomaly-in-anoma
Deploying a text classification model on new (unseen) text,"<p>I am working on a text classification problem. I have attached a simple dummy snippet of a text classification model I have trained.</p>
<p>How do I deploy the model on new_text? When the model is used on <code>check_predictions</code>, it classifies text correctly, however, when new data is used, the classification is incorrect.</p>
<p>Is this because the <code>new_text</code> would need to be vectorised? Am I missing something fundamental?</p>
<pre><code>from collections import Counter
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score

df = pd.read_csv(&quot;/Users/veg.csv&quot;)
print (df)
</code></pre>
<p><a href=""https://i.sstatic.net/OXgSQm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OXgSQm.png"" alt=""first 15 rows of df"" /></a></p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'],random_state=1, test_size=0.2)
cv = CountVectorizer()

X_train_vectorized = cv.fit_transform(X_train)
X_test_vectorized = cv.transform(X_test)

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train_vectorized, y_train)
predictions = naive_bayes.predict(X_test_vectorized)

print(&quot;Accuracy score: &quot;, accuracy_score(y_test, predictions))
print('accuracy %s' % accuracy_score(predictions, y_test))
print(classification_report(y_test, predictions))
</code></pre>
<p><a href=""https://i.sstatic.net/pjrzMm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pjrzMm.png"" alt=""Output"" /></a></p>
<pre><code>check_predictions = []
for i in range(len(X_test)):   
    if predictions[i] == 0:
        check_predictions.append('vegetable')
    if predictions[i] == 1:
        check_predictions.append('fruit')
    if predictions[i] == 2:
        check_predictions.append('tree')
        
dummy_df = pd.DataFrame({'actual_label': list(y_test), 'prediction': check_predictions, 'Text':list(X_test)})
dummy_df.replace(to_replace=0, value='vegetable', inplace=True)
dummy_df.replace(to_replace=1, value='fruit', inplace=True)
dummy_df.replace(to_replace=2, value='tree', inplace=True)
print(&quot;DUMMY DF&quot;)
print(dummy_df.head(10))

</code></pre>
<p><a href=""https://i.sstatic.net/F5o1am.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F5o1am.png"" alt=""test df"" /></a></p>
<pre><code>new_data=['carrot', 'grapes',
          'banana', 'potato',
          'birch','carrot', 'grapes',
          'banana', 'potato', 'birch','carrot','grapes',
          'banana', 'potato',
          'birch','carrot', 'grapes',
          'banana', 'potato', 'birch','grapes',
          'banana', 'potato', 'birch']

new_predictions = []
for i in range(len(new_data)):    
    if predictions[i] == 0:
        new_predictions.append('vegetable')
    if predictions[i] == 1:
        new_predictions.append('fruit')
    if predictions[i] == 2:
        new_predictions.append('tree')
        
new_df = pd.DataFrame({'actual_label': list(y_test), 'prediction': new_predictions, 'Text':list(new_data)})        
new_df.replace(to_replace=0, value='vegetable', inplace=True)
new_df.replace(to_replace=1, value='fruit', inplace=True)
new_df.replace(to_replace=2, value='tree', inplace=True)
print(&quot;NEW DF&quot;)
print(new_df.head(10))

</code></pre>
<p><a href=""https://i.sstatic.net/zBOi0m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zBOi0m.png"" alt=""New data df"" /></a></p>
","python, machine-learning, scikit-learn, nlp, text-classification","<p>Whatever (new) text you are feeding into your model must go through the exact same preprocessing steps as your training data - here the  <code>CountVectorizer</code> as already fitted with your <code>X_train</code>:</p>
<pre><code>new_data_vectorized = cv.transform(new_data) # NOT fit_transform
new_predictions = naive_bayes.predict(new_data_vectorized)
</code></pre>
",1,0,323,2020-10-26 12:19:33,https://stackoverflow.com/questions/64536994/deploying-a-text-classification-model-on-new-unseen-text
How does Stanford NLP identify &quot;it&quot; is being referred to whom?,"<p>Let's say we have two sentences:</p>
<ol>
<li>Jacob is going to watch a movie with Justin.</li>
<li>He will be back by 10 pm.</li>
</ol>
<p>How does Stanford NLP identify &quot;he&quot; refers to Jacob and not Justin?</p>
","nlp, stanford-nlp, text-classification","<p>This is called <code>coreference resolution</code> and is a well-studied problem in NLP. As such, there are many possible ways to do it. Stackoverflow is not the right venue for a literature review, but here are a few links to get you started</p>
<ul>
<li><a href=""http://www-labs.iro.umontreal.ca/%7Efelipe/IFT6010-Hiver2015/Presentations/Abbas-Coreference.pdf"" rel=""nofollow noreferrer"">http://www-labs.iro.umontreal.ca/~felipe/IFT6010-Hiver2015/Presentations/Abbas-Coreference.pdf</a></li>
<li><a href=""https://nlp.stanford.edu/projects/coref.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/coref.shtml</a> and links therein</li>
</ul>
",0,0,40,2020-10-27 03:16:49,https://stackoverflow.com/questions/64547843/how-does-stanford-nlp-identify-it-is-being-referred-to-whom
"Ternary Classification of the type &#39;A&#39;, &#39;B&#39; or &#39;any&#39;?","<p>For any <strong>general</strong> machine learning model (though I am currently working with neural networks), for the task of</p>
<blockquote>
<p>classifying the elements of a set into three groups ('A' or 'B' or 'any'),</p>
</blockquote>
<p>(here, labeling as '<em>A</em>' means that the <strong>only</strong> valid label is 'A' (similarly '<em>B</em>'), and '<em>any</em>' means that <strong>both</strong> the tags 'A' and 'B' are equally valid), <em><strong>what kind of loss function should be used</strong></em>?</p>
<p>This can be solved using the techniques related to the more general problem of &quot;ternary classification,&quot; but I think I'll lose some information by this generalization.</p>
<p><em>For the sake of example, let's say we want to classify verbs (English language) according to their tense forms (let us only consider the present and past tense)</em></p>
<p>Then the model should classify</p>
<blockquote>
<p>{&quot;work&quot;, &quot;eat&quot;, &quot;sing&quot;, ...} as &quot;<em>present tense</em>&quot;</p>
</blockquote>
<blockquote>
<p>{&quot;worked&quot;, &quot;ate&quot;, &quot;sang&quot;, ...} as &quot;<em>past tense</em>&quot;</p>
</blockquote>
<p>and,</p>
<blockquote>
<p>{&quot;read&quot;, &quot;put&quot;, &quot;cut&quot;, ...} as &quot;<em>any</em>&quot;</p>
</blockquote>
<p>(note that the pronunciation is different for the present and past tense of 'read', but we are considering text-based classification)</p>
<p>This is different from the task that I am working on but probably should work as a valid example for this particular question.</p>
<p>PS: I am a student, and only have a basic understanding of this field, so if needed, please ask for any clarification regarding the question.</p>
","machine-learning, neural-network, classification, text-classification","<p>I think that you are in the situation of multi-label classification and not multi-class classifcation.</p>
<p>As stated <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">here</a>:</p>
<blockquote>
<p>In machine learning, multi-label classification and the strongly
related problem of multi-output classification are variants of the
classification problem where multiple labels may be assigned to each
instance</p>
</blockquote>
<p>Which means that instances can have more than 1 class associated to them.</p>
<p>Usually, when you work with a binary classification (e.g. 0, 1 classes) you can have as final layer of your network one neuron, which will output continues values between 0 and 1, using as activation function the sigmoid one, and as loss the <code>binary cross-entropy</code></p>
<p>Given your situation you could decide to use:</p>
<ul>
<li>two neurons as output of your neural network</li>
<li>for each one you can use the <code>sigmoid</code> activation function</li>
<li>and as loss the <code>binary-cross entropy</code></li>
</ul>
<p>in this way, each instance can be associated with both classes with a specific probability by the model.</p>
<p>This means that for each instance, you should associate two classes, or rather <em>&quot;labels&quot;</em>.
For example, for your verbs you should have &quot;past&quot;, &quot;present&quot; classes:</p>
<pre><code>         present  past
work:    1      0
worked:  0      1
read     1      1
</code></pre>
<p>And your model will try to output two probabilities, with the architecture explained before:</p>
<pre><code>         present  past   sum
work:    0.9      0.3    1.2
worked:  0.21     0.8    1.01
read     0.86     0.7    1.5
</code></pre>
<p>Basically, you have two independent probabilites (if you check, the sum of one row is not 1), and therefore you can associate to one instance both classes.</p>
<p>Instead, if you wanted a mutually exclusive classification, with more than 2 classes, you should have used the <code>categorical crossentropy</code> as loss, and the <code>softmax activation function</code> in your last layer, the which will basically handle the outputs to generate a vector of probabilities that sums to 1. Example</p>
<pre><code>         present   past     both   sum
work:    0.7       0.2      0.1    1
worked:  0.21      0.7      0.19   1
read     0.33      0.33     0.33   1
</code></pre>
<p>Check <a href=""https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede"" rel=""nofollow noreferrer"">here</a> to see an extensive example</p>
",1,1,657,2020-11-07 13:54:11,https://stackoverflow.com/questions/64728393/ternary-classification-of-the-type-a-b-or-any
Difficulties to get the correct posterior value in a Naive Bayes Implementation,"<p>For studying purposes, I've tried to implement <a href=""https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"" rel=""nofollow noreferrer"">this</a> &quot;lesson&quot; using python but &quot;without&quot; sckitlearn or something similar.</p>
<p>My attempt code is the follow:</p>
<pre><code>import pandas, math

training_data = [
        ['A great game','Sports'],
        ['The election was over','Not sports'],
        ['Very clean match','Sports'],
        ['A clean but forgettable game','Sports'],
        ['It was a close election','Not sports']
]

text_to_predict = 'A very close game'
data_frame = pandas.DataFrame(training_data, columns=['data','label'])
data_frame = data_frame.applymap(lambda s:s.lower() if type(s) == str else s)
text_to_predict = text_to_predict.lower()
labels = data_frame.label.unique()
word_frequency = data_frame.data.str.split(expand=True).stack().value_counts()
unique_words_set = set()
unique_words = data_frame.data.str.split().apply(unique_words_set.update)
total_unique_words = len(unique_words_set)

word_frequency_per_labels = []
for l in labels:
    word_frequency_per_label = data_frame[data_frame.label == l].data.str.split(expand=True).stack().value_counts()
    for w, f in word_frequency_per_label.iteritems():
        word_frequency_per_labels.append([w,f,l])

word_frequency_per_labels_df = pandas.DataFrame(word_frequency_per_labels, columns=['word','frequency','label'])
laplace_smoothing = 1
results = []
for l in labels:
    p = []
    total_words_in_label = word_frequency_per_labels_df[word_frequency_per_labels_df.label == l].frequency.sum()
    for w in text_to_predict.split():
        x = (word_frequency_per_labels_df.query('word == @w and label == @l').frequency.to_list()[:1] or [0])[0]
        p.append((x + laplace_smoothing) / (total_words_in_label + total_unique_words))
    results.append([l,math.prod(p)])

print(results)
result = pandas.DataFrame(results, columns=['labels','posterior']).sort_values('posterior',ascending = False).labels.iloc[0]
print(result)
</code></pre>
<p>In the blog lesson their results are:</p>
<p><a href=""https://i.sstatic.net/PtAkM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PtAkM.png"" alt=""enter image description here"" /></a></p>
<p>But my result were:</p>
<pre><code>[['sports', 4.607999999999999e-05], ['not sports', 1.4293831139825827e-05]]
</code></pre>
<p>So, what did I do wrong in my python implementation? How can I get the same results?</p>
<p>Thanks in advance</p>
","python, text-classification, naivebayes","<p>You haven't multiplied by the priors <code>p(Sport) = 3/5</code> and <code>p(Not Sport) = 2/5</code>. So just updating your answers by these ratios will get you to the correct result. Everything else looks good.</p>
<p>So for example you implement <code>p(a|Sports) x p(very|Sports) x p(close|Sports) x p(game|Sports)</code> in your <code>math.prod(p)</code> calculation but this ignores the term <code>p(Sport)</code>. So adding this in (and doing the same for the not sport condition) fixes things.</p>
<p>In code this can be achieved by:</p>
<pre><code>prior = (data_frame.label == l).mean()
results.append([l,prior*math.prod(p)])
</code></pre>
",2,2,120,2020-11-09 02:46:17,https://stackoverflow.com/questions/64745233/difficulties-to-get-the-correct-posterior-value-in-a-naive-bayes-implementation
Understanding the role of the function build_vocab in Doc2Vec,"<p>I have recently started studying Doc2Vec model.
I have understood its mechanism and how it works.
I'm trying to implement it using gensim framework.
I have transormed my training data into TaggedDocument.
But i have one question :
What is the role of this line <code>model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])</code> ?
is it to create random vectors that represent text ?
Thank you for your help</p>
","nlp, data-science, gensim, text-classification, doc2vec","<p>The <code>Doc2Vec</code> model needs to know several things about the training corpus before it is fully allocated &amp; initialized.</p>
<p>First &amp; foremost, the model needs to know the words present &amp; their frequencies – a working vocabulary – so that it can determine the words that will remain after the <code>min_count</code> floor is applied, and allocate/initialize word-vectors &amp; internal model structures for the relevant words. The word-frequencies will also be used to influence the random sampling of negative-word-examples (for the default negative-sampling mode) and the downsampling of very-frequent words (per the <code>sample</code> parameter).</p>
<p>Additionally, the model needs to know the rough size of the overall training set in order to gradually decrement the internal <code>alpha</code> learning-rate over the course of each epoch, and give meaningful progress-estimates in logging output.</p>
<p>At the end of <code>build_vocab()</code>, all memory/objects needed for the model have been created. Per the needs of the underlying algorithm, all vectors will have been initialized to low-magnitude random vectors to ready the model for training. (It essentially won't use any more memory, internally, through training.)</p>
<p>Also, after <code>build_vocab()</code>, the vocabulary is frozen: any words presented during training (or later inference) that aren't already in the model will be ignored.</p>
",0,0,577,2020-11-16 21:37:08,https://stackoverflow.com/questions/64866067/understanding-the-role-of-the-function-build-vocab-in-doc2vec
CountVectorizer running out of memory when converting from sparse to dense,"<p>I'm trying to run this code to calculate the relative frequency of each word in a bunch of documents (a lot, more than 40000), i cannot reduce the vocabulary size and it throws out of memory error when runnign on Colab with 12 gb RAM.
How can i refactor the code so that i don't have to call X.toarray() converting from sparse to dense and throws an out of memory error (120000 word * 40000 documents).</p>
<pre><code>vect = CountVectorizer(vocabulary=list(word_to_index.keys()), tokenizer=lambda x: x.split())
X = vect.fit_transform(docs)
X_arr = X.toarray()
rel_freq = np.sum(X_arr, axis=0) / len(docs)
names = vect.get_feature_names()
</code></pre>
<p>If you are wondering why i need to do this is because i'm implementing ConWea code:
<a href=""https://github.com/dheeraj7596/ConWea"" rel=""nofollow noreferrer"">https://github.com/dheeraj7596/ConWea</a> with a bigger amount of data than the author.
Thank you very much to everyone.</p>
","python, scikit-learn, nlp, out-of-memory, text-classification","<p>If you only need the frequency, you can sum up using the <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.sum.html"" rel=""nofollow noreferrer"">sum method</a> for sparse matrix:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()

corpus = ['This is the first document.','This is the second second document.',
'And the third one.','Is this the first document?']

X = vectorizer.fit_transform(corpus)

X.sum(axis=0)/len(corpus)
matrix([[0.25, 0.75, 0.5 , 0.75, 0.25, 0.5 , 1.  , 0.25, 0.75]])

X.toarray().sum(axis=0)/ len(corpus)
array([0.25, 0.75, 0.5 , 0.75, 0.25, 0.5 , 1.  , 0.25, 0.75])
</code></pre>
",2,1,950,2020-11-24 16:48:23,https://stackoverflow.com/questions/64991082/countvectorizer-running-out-of-memory-when-converting-from-sparse-to-dense
keyword extraction and Keyword based text classification,"<p>Currently i am working on a project which requires keywords extraction or we can say keyword based text classification . The dataset contains 3 columns text, keywords and cc terms, I need to extract keywords from text and then classify the text based on those keywords, each row in dataset has their own keywords, i want to extract similar kind of keywords. I want to train the by providing text and keyword column so that the model is able to extract keywords for unknown text.please help</p>
<p><a href=""https://i.sstatic.net/xbcdV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xbcdV.png"" alt=""image contains dataset"" /></a></p>
","deep-learning, keyword, feature-extraction, text-classification, keyword-extraction","<p>Keyword extraction is typically done using <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">TF-IDF scores</a> simply by setting a score threshold. When training a classifier, it does not make much sense to cut off the keywords at a certain threshold, knowing that something is not likely to be a keyword might also be a valuable piece of information for the classifier.</p>
<p>The simplest way to get the TF-IDF scores for particular words is using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfIdfVectorizer</a> in scikit-learn that does all the laborious text preprocessing steps (tokenization, removing stop words).</p>
<p>You can probably achieve better results by fine-tuning BERT for your classification task (but of course at the expense of much higher computational costs).</p>
",1,-2,927,2020-11-26 08:33:40,https://stackoverflow.com/questions/65018504/keyword-extraction-and-keyword-based-text-classification
Oversampling after splitting the dataset - Text classification,"<p>I am having some issues with the steps to follow for over-sampling a dataset.
What I have done is the following:</p>
<pre><code># Separate input features and target
y_up = df.Label

X_up = df.drop(columns=['Date','Links', 'Paths'], axis=1)

# setting up testing and training sets

X_train_up, X_test_up, y_train_up, y_test_up = train_test_split(X_up, y_up, test_size=0.30, random_state=27)

class_0 = X_train_up[X_train_up.Label==0]
class_1 = X_train_up[X_train_up.Label==1]


# upsample minority
class_1_upsampled = resample(class_1,
                          replace=True, 
                          n_samples=len(class_0), 
                          random_state=27) #

# combine majority and upsampled minority
upsampled = pd.concat([class_0, class_1_upsampled])
</code></pre>
<p>Since my dataset looks like:</p>
<pre><code>Label     Text 
1        bla bla bla
0        once upon a time 
1        some other sentences
1        a few sentences more
1        this is my dataset!
</code></pre>
<p>I applied a vectorizer to transform string into numbers:</p>
<pre><code>X_train_up=upsampled[['Text']]
y_train_up=upsampled[['Label']]

X_train_up = pd.DataFrame(vectorizer.fit_transform(X_train_up['Text'].replace(np.NaN, &quot;&quot;)).todense(), index=X_train_up.index)
</code></pre>
<p>Then I applied the logistic regression function:</p>
<pre><code>upsampled_log = LogisticRegression(solver='liblinear').fit(X_train_up, y_train_up)
</code></pre>
<p>However, I have got the following error at this step:</p>
<pre><code>X_test_up = pd.DataFrame(vectorizer.fit_transform(X_test_up['Text'].replace(np.NaN, &quot;&quot;)).todense(), index=X_test_up.index)

pred_up_log = upsampled_log.predict(X_test_up)
</code></pre>
<blockquote>
<p>ValueError: X has 3021 features per sample; expecting 5542</p>
</blockquote>
<p>Since it was told me that I should apply the oversampling after splitting my dataset into train e test, I have not vectorised the test set.
My doubts are then the following:</p>
<ul>
<li>is it right to consider later a vectorisation of the test set: <code>X_test_up = pd.DataFrame(vectorizer.fit_transform(X_test_up['Text'].replace(np.NaN, &quot;&quot;)).todense(), index=X_test_up.index)</code></li>
<li>is it right to consider the over-sampling after splitting the dataset into training and test?</li>
</ul>
<p>Alternatively, I tried with Smote function. The code below works, but I would prefer to consider also the oversampling, if possible, rather than SMOTE.</p>
<pre><code>from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

X_train_up, X_test_up, y_train_up, y_test_up=train_test_split(df['Text'],df['Label'], test_size=0.2,random_state=42)

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train_up)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)


sm = SMOTE(random_state=2)
X_train_res, y_train_res = sm.fit_sample(X_train_tfidf, y_train_up)
print(&quot;Shape after smote is:&quot;,X_train_res.shape,y_train_res.shape)

nb = Pipeline([('clf', LogisticRegression())])
nb.fit(X_train_res, y_train_res)
y_pred = nb.predict(count_vect.transform(X_test_up))
print(accuracy_score(y_test_up,y_pred))
</code></pre>
<p>Any comments and suggestions will be appreciated.
Thanks</p>
","python, scikit-learn, vectorization, logistic-regression, text-classification","<p>It is better to do the countVectorizing and transformation on the whole dataset, split into test and train, and keep it as a sparse matrix without converting back into a data.frame.</p>
<p>For example this is a dataset:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

df = pd.DataFrame({'Text':['This is bill','This is mac','here’s an old saying',
                           'at least old','data scientist years','data science is data wrangling', 
                           'This rings particularly','true for data science leaders',
                           'who watch their data','scientists spend days',
                           'painstakingly picking apart','ossified corporate datasets',
                           'arcane Excel spreadsheets','Does data science really',
                           'they just delegate the job','Data Is More Than Just Numbers',
                           'The reason that',
                           'data wrangling is so difficult','data is more than text and numbers'],
                   'Label':[0,1,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0]})
</code></pre>
<p>We perform the vectorization and transformation, followed by split:</p>
<pre><code>count_vect = CountVectorizer()
df_counts = count_vect.fit_transform(df['Text'])
tfidf_transformer = TfidfTransformer()
df_tfidf = tfidf_transformer.fit_transform(df_counts)

X_train_up, X_test_up, y_train_up, y_test_up=train_test_split(df_tfidf,df['Label'].values, 
                                                              test_size=0.2,random_state=42)
</code></pre>
<p>Up sampling can be done by resampling the index of the minority classes:</p>
<pre><code>class_0 = np.where(y_train_up==0)[0]
class_1 = np.where(y_train_up==1)[0]
up_idx = np.concatenate((class_0,
                        np.random.choice(class_1,len(class_0),replace=True)
                       ))

upsampled_log = LogisticRegression(solver='liblinear').fit(X_train_up[up_idx,:], y_train_up[up_idx])
</code></pre>
<p>And the prediction will work:</p>
<pre><code>upsampled_log.predict(X_test_up)
array([0, 1, 0, 0])
</code></pre>
<p>If you have concerns about data leakage, that is some of the information from test actually goes into the training, through the use of TfidfTransformer(). Honestly yet to see concrete proof or demonstration of this, but below is an alternative where you apply the tfid separately:</p>
<pre><code>count_vect = CountVectorizer()
df_counts = count_vect.fit_transform(df['Text'])

X_train_up, X_test_up, y_train_up, y_test_up=train_test_split(df_counts,df['Label'].values, 
                                                              test_size=0.2,random_state=42)

class_0 = np.where(y_train_up==0)[0]
class_1 = np.where(y_train_up==1)[0]
up_idx = np.concatenate((class_0,
                        np.random.choice(class_1,len(class_0),replace=True)
                       ))

tfidf_transformer = TfidfTransformer()
upsample_Xtrain = tfidf_transformer.fit_transform(X_train_up[up_idx,:])
upsamle_y = y_train_up[up_idx]

upsampled_log = LogisticRegression(solver='liblinear').fit(upsample_Xtrain,upsamle_y)

X_test_up = tfidf_transformer.transform(X_test_up)
upsampled_log.predict(X_test_up)
</code></pre>
",2,0,1142,2020-11-30 13:48:30,https://stackoverflow.com/questions/65074784/oversampling-after-splitting-the-dataset-text-classification
How to improve my multiclass text-classification on German text?,"<p>I am new in NLP and it is a bit confusing me.
I am trying to do a text classification with SVC on my dataset.
I have an imbalanced dataset of 6 classes.
The text is news for classes of health, sport, culture, economy, science and web.
I am using TF-IDF for vectorization.</p>
<p>the preprocessing steps: <code>lower-case</code> all the texts and to remove the <code>stop-words</code>. since my text is in German I did not use <code>lemmatization</code></p>
<p>my first try:</p>
<pre><code>from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.2, random_state=42)
X_train = train['text']
y_train = train['category']
X_test = test['text']
y_test = test['category']

# Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC()),])
predictions = text_clf_lsvc.predict(X_test)
</code></pre>
<p>my metrci acuuracy score was: 93%</p>
<p>then I decided to reduce the dimensionality: so on my 2nd try I added TruncatedSVD</p>
<pre><code># Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),('svd', TruncatedSVD()),
                     ('clf', LinearSVC()),])
predictions = text_clf_lsvc.predict(X_test)
</code></pre>
<p>my metrci acuuracy score dropped to 34%.</p>
<p><em><strong>my questions:</strong></em></p>
<p>1- How can I improve my model if I want to stick to TF-IDF and SVC for classification<br />
2- What can I do other than that if I want to have a good classification</p>
","python, nlp, svm, text-classification, tf-idf","<p>The best way to improve accuracy, given that you want to stick with this configuration is through <em>hyperparameter</em> tuning, or by introducing additional components, such as <em>feature selection</em>.</p>
<p><strong>Hyperparameter tuning</strong></p>
<p>Most machine learning algorithms and parts of a machine learning pipeline have several parameters you can change. For example, the <code>TfidfVectorizer</code> has different ngram ranges, different analysis levels, different tokenizers, and many more parameters to vary. Most of these will affect your performance. So, what you can do is systematically vary these parameters (and those of your <code>SVC</code>), while monitoring you accuracy on a <em>development set</em> (i.e., <strong>not the test data!</strong>). Instead of fixed development set, cross-validation is typically used in these kinds of settings.</p>
<p>The best way to do this in <code>sklearn</code> is through a <code>RandomizedSearchCV</code> (see <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"" rel=""nofollow noreferrer"">here</a> for details). This class automatically cross-validates and searches through the possible options you pre-specify by randomly sampling from the option set for a fixed number of iterations. By applying this technique on your training data, you will automatically find models that perform better for your given training data and your options. Ideally, these models would also perform better on your test data. Fair warning: cross-validated search techniques can take a while to run.</p>
<p><strong>Feature Selection</strong></p>
<p>In addition to grid search, another way to improve performance is through <em>feature selection</em>. Feature selection typically consists of a statistical test that determines which features explain variance in the typical task you are trying to solve. The feature selection methods in <code>sklearn</code> are detailed <a href=""https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection"" rel=""nofollow noreferrer"">here</a>.</p>
<p>By far the most important bit here is that the performance of anything you add to your model should be verified on an independent development set, or in cross-validation. Leave your test data alone.</p>
",1,0,765,2020-12-04 12:55:48,https://stackoverflow.com/questions/65143979/how-to-improve-my-multiclass-text-classification-on-german-text
What do the logits and probabilities from RobertaForSequenceClassification represent?,"<p>Being new to the &quot;Natural Language Processing&quot; scene, I am experimentally learning and have implemented the following segment of code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch
    
path = &quot;D:/LM/rb/&quot;
tokenizer = RobertaTokenizer.from_pretrained(path)
model = RobertaForSequenceClassification.from_pretrained(path)
    
inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
pred_logits = outputs.logits
print(pred_logits)
probs = pred_logits.softmax(dim=-1).detach().cpu().flatten().numpy().tolist()
print(probs)
</code></pre>
<p>I <em>understand</em> that applying the model returns a <em>&quot;<code>torch.FloatTensor</code> comprising various elements depending on the configuration (RobertaConfig) and inputs&quot;</em>, and that the logits are accessible using <code>.logits</code>. As demonstrated I have applied the <em>.softmax</em> function to the tensor to return normalised probabilities and have converted the result into a list. I am outputted with the following:</p>
<pre><code>[0.5022980570793152, 0.49770188331604004]
</code></pre>
<p>Do these probabilities represent some kind of overall &quot;masked&quot; probability?</p>
<p><strong>What do the first and second index represent in context of the input?</strong></p>
<hr />
<p>EDIT:</p>
<pre><code>model.num_labels
</code></pre>
<p>Output:</p>
<pre><code>2
</code></pre>
<p><a href=""https://stackoverflow.com/users/6664872/cronoik"">@cronoik</a> explains that the model &quot;tries to classify if a sequence belongs to one class or another&quot;</p>
<p>Am I to assume that because there are no trained output layers these classes don't mean anything yet?</p>
<p>For example, I <em>can</em> assume that the probability that the sentence, post analysis, belongs to class 1 is 0.5. However, what is class 1?</p>
<p>Additionally, model cards with pre-trained output layers such as the <a href=""https://huggingface.co/roberta-large-openai-detector"" rel=""nofollow noreferrer"">open-ai detector</a> help differentiate between what is <a href=""https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46"" rel=""nofollow noreferrer"">&quot;real&quot;</a> and <a href=""https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46"" rel=""nofollow noreferrer"">&quot;fake&quot;</a>, and so I can assume the class that a sentence belongs to. However, how can I confirm these &quot;labels&quot; without some type of &quot;mapping.txt&quot; file?</p>
","python, nlp, pytorch, text-classification, huggingface-transformers","<p>You have initialized a <code>RobertaForSequenceClassification</code> model that per default (in case of <code>roberta-base</code> and <code>roberta-large</code> which have no trained output layers for sequence classification) tries to classify if a sequence belongs to one class or another. I used the expression &quot;belongs to one class or another&quot; because these classes have no meaning yet. The output layer is untrained and it requires a finetuning to give these classes a meaning. <code>Class 0</code> could be <code>X</code> and <code>Class 1</code> could be <code>Y</code> or the other way around. For example, the tutorial for finetuning a sequence classification model for the IMDb review dataset defines negative reviews as <code>Class 0</code> and positive reviews as <code>Class 1</code> (<a href=""https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"" rel=""noreferrer"">link</a>).</p>
<p>You can check the number of supported classes with:</p>
<pre class=""lang-py prettyprint-override""><code>model.num_labels
</code></pre>
<p>Output:</p>
<pre><code>2
</code></pre>
<p>The output you get is the non-normalized probability for each class (i.e. <a href=""https://developers.google.com/machine-learning/glossary/#logits"" rel=""noreferrer"">logits</a>). You applied the softmax function to normalize these probabilities, which leads to 0.5022980570793152 for the first class and 0.49770188331604004 for the second class.</p>
<p>Maybe you got confused because the values are close to each other. Let's try a model with a pretrained output layer (<a href=""https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment"" rel=""noreferrer"">model card</a>):</p>
<pre class=""lang-py prettyprint-override""><code>sentimodel = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')
print(sentimodel.num_labels)
outputs = sentimodel(**inputs)
print(outputs.logits.softmax(dim=-1).tolist())
</code></pre>
<p>Output:</p>
<pre><code>3
[[0.0015561950858682394, 0.019568447023630142, 0.9788752794265747]]
</code></pre>
<p>These values represent the probabilities for the sentence <code>Hello, my dog is cute</code> to be <code>negative</code>, <code>neutral</code>, or <code>positive</code>. We know what these classes are because the authors provided <a href=""https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/mapping.txt"" rel=""noreferrer"">mapping</a> that clarifies it. In case the authors of the model do not provide such a mapping (via a readme or the original training code), we can only guess what each class represents by testing it with random samples.</p>
<p>The <a href=""https://huggingface.co/roberta-large-openai-detector"" rel=""noreferrer"">model card</a> you have mentioned does not provide any useful information regarding the mapping of the classes to what they represent, but the model is <a href=""https://github.com/huggingface/transformers/blob/master/docs/source/pretrained_models.rst"" rel=""noreferrer"">provided by huggingface itself</a> and they provide a <a href=""https://github.com/openai/gpt-2-output-dataset/tree/master/detector"" rel=""noreferrer"">link</a> to the code used for training the model. The <a href=""https://github.com/openai/gpt-2-output-dataset/blob/ddfecb39328f0a9857cd09b40e55819d1f9ad512/detector/dataset.py#L55"" rel=""noreferrer"">dataset.py</a> indicates that <code>fake</code> is represented by <code>Class 0</code> and <code>real</code> by <code>Class 1</code>.</p>
",6,5,11314,2020-12-09 16:43:06,https://stackoverflow.com/questions/65221079/what-do-the-logits-and-probabilities-from-robertaforsequenceclassification-repre
How to deal with a target variable containing nominal data?,"<p>Im working on an NLP project whose target variable contains seven unique sentences which are &quot;inspirational and thought-provoking &quot;, &quot;informative&quot;, &quot;acknowledgment and appreciations&quot; and 4 others. As for my understanding, the target variable as we can't establish a quantitative comparison between them. So my question is what is the best way to encode such variables? And if I encode it using one Hot-encoding then the problem will be of multi-class classification?</p>
","nlp, classification, text-classification, one-hot-encoding, multiclass-classification","<p>In classification it does not matter what the class actually represents, the learning algorithm treats every class as categorical anyway. In other words whether the names of the classes are strings, characters or numbers does not change anything to the model. This is why the most common choice is to simply represent the classes as integers: 1,2,3,... For example in scikit this can be done with <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">LabelEncoder</a>.</p>
<p>It would be a bad idea to use one hot encoding because this would make the problem <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">multi-label</a>. This would make the problem much more complex for the model and would very likely lead to lower performance, or it would require much more data in order to reach the same performance as regular classification. This is because there are much more combinations possible in the multi-label problem, and in this case this higher level of complexity is pointless since there can be only one class.</p>
",2,2,491,2020-12-17 20:32:54,https://stackoverflow.com/questions/65347947/how-to-deal-with-a-target-variable-containing-nominal-data
text classification using neural network in keras - model is weak,"<p>i'm trying to classify verses to book in the bible, the problem is that my model is not good and i can't find a way to improve it.</p>
<p>this is my code:</p>
<pre><code>import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.layers import MaxPooling2D,Conv2D
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import SpatialDropout1D
from sklearn.model_selection import train_test_split
from tensorflow.keras import regularizers

import pandas as pd              
import numpy as np 

data = pd.read_csv(&quot;bible_data_set (with count and testament).csv&quot;)
data
</code></pre>
<p><a href=""https://i.sstatic.net/SMvdO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SMvdO.png"" alt=""enter image description here"" /></a></p>
<pre><code>import nltk
from nltk.stem import PorterStemmer

ps = PorterStemmer() 

vocabulary_size = 0
word2location = {}

def prepare_vocabulary(data):
    index = 0
    for sentance in data['text']:
        #sentance = sentance.lower()
        words = nltk.word_tokenize(sentance)
        for word in words:
            stemed_word = ps.stem(word)
            if stemed_word not in word2location:
                word2location[stemed_word] = index
                index += 1
    return index

def convert2vec(sentance):
    #sentance = sentance.lower()
    res_vec = np.zeros(vocabulary_size)
    words = nltk.word_tokenize(sentance)
    for word in words:
        stemed_word = ps.stem(word)
        if stemed_word in word2location:
            res_vec[word2location[stemed_word]]+=1
    return res_vec

books = ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges',
            'Ruth', '1 Samuel', '2 Samuel', '1 Kings', '2 Kings', '1 Chronicles', '2 Chronicles',
            'Ezra', 'Nehemiah', 'Esther', 'Job', 'Psalms', 'Proverbs', 'Ecclesiastes',
            'Song of Solomon', 'Isaiah', 'Jeremiah', 'Lamentations', 'Ezekiel', 'Daniel',
            'Hosea', 'Joel', 'Amos', 'Obadiah', 'Jonah', 'Micah', 'Nahum', 'Habakkuk',
            'Zephaniah', 'Haggai',    'Zechariah',    'Malachi', 'Matthew', 'Mark', 'Luke', 'John', 'Acts', 'Romans', '1 Corinthians',
            '2 Corinthians', 'Galatians', 'Ephesians', 'Philippians', 'Colossians',
            '1 Thessalonians', '2 Thessalonians', '1 Timothy', '2 Timothy', 'Titus', 'Philemon',
            'Hebrews', 'James', '1 Peter', '2 Peter', '1 John', '2 John', '3 John', 'Jude',
            'Revelation']

def encode(line):
    res_vec = np.zeros(66)
    idx = books.index(data.iloc[line]['book'])
    res_vec[idx] = 1
    return res_vec

vocabulary_size = prepare_vocabulary(data)
print(&quot;the size of the vocabulary is: &quot;, vocabulary_size)
word2location
</code></pre>
<p><a href=""https://i.sstatic.net/ZmOSb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZmOSb.png"" alt=""enter image description here"" /></a></p>
<pre><code>import random

rand = []
for r in range (4500):
    ra = random.randrange(0, 31101)
    if(ra not in rand):
        rand.append(ra)
            
train_x = []
train_y = []
test_x = []
test_y = []
for i in range(len(data['text'])):
    if(i not in rand):
        train_x.append(i)
        train_y.append(i)
        
    elif(i in rand):
        test_x.append(i)
        test_y.append(i)
</code></pre>
<pre><code>data_x = np.array([convert2vec(data.iloc[i]['text']) for i in train_x])
np.random.shuffle(data_x)
data_y = np.array([encode(i) for i in train_y])
np.random.shuffle(data_y)
test_data_x = np.array([convert2vec(data.iloc[i]['text']) for i in test_x])
np.random.shuffle(test_data_x)
test_data_y = np.array([encode(i) for i in test_y])
np.random.shuffle(test_data_y)

</code></pre>
<pre><code>model = Sequential()
model.add(Dense(128, activation = 'sigmoid', input_dim = vocabulary_size))
model.add(Dropout(0.1))
model.add(Dense(128, activation = 'sigmoid'))
model.add(Dropout(0.1))
model.add(Dense(66, activation = 'softmax'))

opt = SGD(lr=0.01)

model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
history = model.fit(data_x, data_y, epochs=50, batch_size=16,validation_data=(test_data_x,test_data_y),callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.00001)])
</code></pre>
<p><a href=""https://i.sstatic.net/qvbfK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qvbfK.png"" alt=""enter image description here"" /></a></p>
<p>i keep getting overfitting or under fitting.
i have tried relu activation for the dense, and changed the loss function and optimizer, but nothing helped.
is there anything i'm missing?</p>
","python, keras, deep-learning, neural-network, text-classification","<p>Here</p>
<pre><code>data_x = np.array([convert2vec(data.iloc[i]['text']) for i in train_x])
np.random.shuffle(data_x)
data_y = np.array([encode(i) for i in train_y])
np.random.shuffle(data_y)
test_data_x = np.array([convert2vec(data.iloc[i]['text']) for i in test_x])
np.random.shuffle(test_data_x)
test_data_y = np.array([encode(i) for i in test_y])
np.random.shuffle(test_data_y)
</code></pre>
<p>You called np.random.shuffle for your train data (data_x) and np.random.shuffle for your train labels (data_y). This should not be correct as your features should remain paired to your label. Just pair these together and random shuffle once and do the same for testing.</p>
",1,0,126,2020-12-22 21:31:07,https://stackoverflow.com/questions/65416045/text-classification-using-neural-network-in-keras-model-is-weak
NLP | LimeTextExplainer for bigrams,"<p>in my NLP task I want to understand the 'rule' of my classifier. For that purpose, I build a LimeTExtExplainer.</p>
<pre><code>c= make_pipeline(cv,naive_bayes)
explainer = LimeTextExplainer(class_names=class_names, random_state=42, bow=False)
exp = explainer.explain_instance(X_test[i], c.predict_proba, num_features=20,) 
fig = exp.as_pyplot_figure()
</code></pre>
<p>The above code creats a nice list of 1grams, exactly as I wanted.
:<a href=""https://i.sstatic.net/Nb7HF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Nb7HF.png"" alt=""enter image description here"" /></a></p>
<p>In a next step I want to do the same, but with bigrams. I changed the feature extractor to only calculate bigrams:</p>
<pre><code>cv = CountVectorizer(strip_accents='ascii', analyzer='word',                                    
                 token_pattern=u'(?ui)\\b\\w*[a-z]+\\w*\\b',                                
                 lowercase=True, stop_words='english',                                      
                 ngram_range=(2,2), max_features=None)
</code></pre>
<p>The problem(s):</p>
<ol>
<li>I use the same code for the Limeexplainer as above. But now, the
graph only shows 1grams as before, but I only calculated bigrams.</li>
<li>As a side question, the horizontal axis of the graphs displays the
absolute probability that the word  accounts to the classification
probability? For instance, the texts class X probabilty is 0.67,
recognit accounts for ~ 0.009 and langugage for ~ 0.007 of the 0.67,
right?</li>
</ol>
<p>Thanks in advance!</p>
","python, nlp, text-classification, lime","<p>At least I got an answer to the second question:</p>
<p>Those are probabilities, but not in the way I thought.</p>
<p>For instance, predicted probability for Class X is 0.808. If now the word 'recognit' would be removed from the underlying corpus, the total predicted probability for the predicted class would shrink by 0.008.--&gt; probability class x equals 0.800 then.</p>
<p>For detailed information about LIME I highly reccomend:
<em>“Why Should I Trust You?” Explaining the Predictions of Any Classifier, Riberio et.al (2016)</em></p>
",1,2,757,2021-01-09 17:02:46,https://stackoverflow.com/questions/65645289/nlp-limetextexplainer-for-bigrams
How to use individual term frequeny features with Naive Bayes classifier?,"<p>I am currently working on a NLP task with Naive Bayes classifier.</p>
<p>My features consist out of bigrams and unigrams.</p>
<p>Now, to fit X_train I want to merge the top 100 unigrams and the top 50 bigrams.</p>
<p>How do i do that?</p>
<p>I only can use either top 100 unigrams or top 50 bigrams.</p>
<p>Is there a smart way, without exporting the grams, to include the merged 150 grams?</p>
<pre><code>cv = CountVectorizer(strip_accents='ascii', analyzer='word',                                                          token_pattern=u'(?ui)\\b\\w*[a-z]+\\w*\\b',                                
                      lowercase=True, stop_words='english',   
                     ngram_range=(1,1), max_features=100)                                              
X_train_cv = cv.fit_transform(X_train)                                                           
X_test_cv  = cv.transform(X_test)
</code></pre>
<p>Sure i can use <code>ngram_range(1,2) = 150 </code> but that is not the same as <code>ngram_range(1,1), max_features=100</code> and <code>ngram_range=(2,2) , max_features=50</code></p>
","python, nlp, text-classification, naivebayes, countvectorizer","<p>One could create 2 separate Vectorizers and merge them using feature union: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html</a></p>
",2,0,98,2021-02-01 12:34:09,https://stackoverflow.com/questions/65992830/how-to-use-individual-term-frequeny-features-with-naive-bayes-classifier
Invalid Syntax when adding Conv1D to CNN model definition,"<p>I'm trying to use CNN for text classification (Using Keras). In most tutorials, they suggest using <code>Conv1D</code> for that, but when I try to define the model, I get an <code>Invalid Syntax</code> error.</p>
<p>These are the libraries I'm importing:</p>
<pre><code>from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
</code></pre>
<p>This is the model definition:</p>
<pre><code>model = Sequential()
model.add(Embedding(vocab_size, 32, input_length=max_length)

model.add(Conv1D(32, 3, activation='relu')) #This is where I'm getting the error
model.add(MaxPooling1D())
model.add(Flatten())
model.add(Dense(250, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
</code></pre>
<p>This is the error message:</p>
<pre><code>File &quot;&lt;ipython-input-29-a0d7dea5e2b2&gt;&quot;, line 3
model.add(Conv1D(32, 3, activation='relu'))
    ^
SyntaxError: invalid syntax
</code></pre>
<p>Can you tell me what did I do wrong, and how can I fix it?</p>
<p>Thanks.</p>
","python-3.x, keras, conv-neural-network, tensorflow2.0, text-classification","<p>You're missing a bracket:</p>
<pre><code>model.add(Embedding(vocab_size, 32, input_length=max_length) ) &lt;----
</code></pre>
",0,-2,311,2021-02-13 13:57:47,https://stackoverflow.com/questions/66185907/invalid-syntax-when-adding-conv1d-to-cnn-model-definition
Resampling dataset for spam classification,"<p>I have a class imbalance problem with the following dataset:</p>
<pre><code>Text                             is_it_capital?     is_it_upper?      contains_num?   Label
an example of text                      0                  0               0            0
ANOTHER example of text                 1                  1               0            1
What's happening?Let's talk at 5        1                  0               1            1
</code></pre>
<p>and similar. I have 5000 rows/texts (4500 with class 0 and 500 with class 1).</p>
<p>I would need to re-sample my classes, but I do not know where to include this step in my model, so I would appreciate if you could have a look and tell me if I am missing some step or if you spot any inconsistencies in the approach.</p>
<p>For train, test I am using the following:</p>
<pre><code>X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.25, random_state=40) 
</code></pre>
<p>Where <code>X</code> is</p>
<pre><code>X=df[['Text','is_it_capital?', 'is_it_upper?', 'contains_num?']]
y=df['Label']

df_train= pd.concat([X_train, y_train], axis=1)
df_test = pd.concat([X_test, y_test], axis=1)


# Separating classes

spam = df_train[df_train.Label == 1]
not_spam = df_train[df_train.Label == 0]

# Oversampling  

oversampl = resample(spam,replace=True,n_samples=len(not_spam), random_state=42)

oversampled = pd.concat([not_spam, oversampl])
df_train = oversampled.copy()
</code></pre>
<p>Output (wrong?):</p>
<pre><code>              precision    recall  f1-score   support

         0.0       0.94      0.98      0.96      3600
         1.0       0.76      0.52      0.62       400

    accuracy                           0.93      4000
   macro avg       0.86      0.77      0.80      4000

weighted avg       0.92      0.93      0.93      4000
</code></pre>
<p>Do you think there is something wrong in my steps for oversampling the dataset as confusion matrix gives me a support of 400 and not higher?</p>
<p>Sorry for the long post, but I think it is worthy to report all the steps for a better understanding of the approach I have taken.</p>
","python, scikit-learn, classification, text-classification, resampling","<p>There is nothing wrong with your method and it's normal that the evaluation report shows imbalanced data. This is because:</p>
<ul>
<li>The resampling is (rightly) done on the training set only, in order to force the model to give more importance to the minority class.</li>
<li>The evaluation is (rightly) made on the test set which follows the original imbalanced distribution. It would be a mistake to resample the test set as well, because the evaluation must be done <a href=""https://datascience.stackexchange.com/q/15630/64377"">on the true distribution of the data</a>.</li>
</ul>
",1,4,240,2021-02-17 14:58:28,https://stackoverflow.com/questions/66244497/resampling-dataset-for-spam-classification
"How to use Google Sheets to categorise data, depending on text values in a column","<p>I ask this, as I previously performed this task using Python. I'm moving on to a new position and I need to pass this task over to someone who does not know how to program.</p>
<p>I am hoping there is a simple and suitable Formula to use on Google Sheets.</p>
<p>in the following table on Google Sheets:</p>
<p><a href=""https://i.sstatic.net/B5UN1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B5UN1.png"" alt=""enter image description here"" /></a></p>
<p>Is there a <strong>formula I could use to automatically categorise that data?</strong></p>
<p>ie in pseudocode</p>
<hr />
<p>If Column A contains &quot;password&quot; or &quot;sign in&quot;:</p>
<p>Column B = &quot;Account Access&quot;</p>
<p>Else If</p>
<p>Column A contains &quot;support&quot; or &quot;contact&quot;:</p>
<p>Column B = &quot;Customer Support&quot;</p>
<hr />
<p>Etc for multiple other categories.</p>
","google-sheets-formula, text-classification, categorization","<p>You can use REGEXMATCH:</p>
<pre><code>=IFS(REGEXMATCH(A1, &quot;password|sign in&quot;),&quot;Account Access&quot;,
     REGEXMATCH(A1, &quot;support|customer&quot;),&quot;Customer Support&quot;,
     TRUE,&quot;other&quot;)
</code></pre>
<p>Expand as needed.</p>
",0,1,1343,2021-02-23 09:32:40,https://stackoverflow.com/questions/66330482/how-to-use-google-sheets-to-categorise-data-depending-on-text-values-in-a-colum
How to add probabilities to model.predict output?,"<p>I've built a functioning classification model following <a href=""https://kavita-ganesan.com/news-classifier-with-logistic-regression-in-python/#Saving-Logistic-Regression-Model"" rel=""nofollow noreferrer"">this tutorial</a>.</p>
<p>The tutorial only outputs the predicted category names. I want it to output the category name and its probability and I only want to output categories above a certain probability. For example, I only want categories over .5</p>
<p>This is the function used to access the model:</p>
<pre><code>import pickle
import numpy as np
category_model_path=&quot;categorymodel.pkl&quot;
category_transformer_path=&quot;categorytransformer.pkl&quot;
sentiment_model_path=&quot;sentimentmodel.pkl&quot;
sentiment_transformer_path=&quot;sentimenttransformer.pkl&quot;

def get_top_k_predictions(model,X_test,k):
    
    # get probabilities instead of predicted labels, since we want to collect top 3
    np.set_printoptions(suppress=True)
    probs = model.predict_proba(X_test)

    # GET TOP K PREDICTIONS BY PROB - note these are just index
    best_n = np.argsort(probs, axis=1)[:,-k:]
    
    # GET CATEGORY OF PREDICTIONS
    preds=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]
    
    preds=[ item[::-1] for item in preds]
    
    return preds

category_loaded_model = pickle.load(open(category_model_path, 'rb'))
category_loaded_transformer = pickle.load(open(category_transformer_path, 'rb'))

sentiment_loaded_model = pickle.load(open(sentiment_model_path, 'rb'))
sentiment_loaded_transformer = pickle.load(open(sentiment_transformer_path, 'rb'))
</code></pre>
<p>Then this code is used to call the function:</p>
<pre><code>category_test_features=category_loaded_transformer.transform([&quot;I absolutley loved the organization &quot;])
get_top_k_predictions(category_loaded_model,category_test_features,2)
</code></pre>
<p>This is the current output:</p>
<pre><code>[['Course Structure', 'Learning Materials']]
</code></pre>
<p>The probabilities are calculated in the function to the <code>probs</code> variable. I do not know how to only get the ones over .5 and add these to the <code>preds</code> output.</p>
","python, machine-learning, classification, text-classification","<p>The <code>best_n</code> array contains the indices to the array of probabilities <code>probs</code>. You can use it in the same way as you do for getting the labels. You can get label-probability tuples like this:</p>
<pre class=""lang-py prettyprint-override""><code>preds = [
    [(model.classes_[predicted_cat], distribution[predicted_cat])
     for predicted_cat in prediction]
    for distribution, prediction in zip(probs, best_n)]
</code></pre>
<p>If you do not want to return the probabilities and only want to filter them, you can do something like:</p>
<pre class=""lang-py prettyprint-override""><code>preds=[
    [model.classes_[predicted_cat]
     for predicted_cat in prediction if distribution[predicted_cat] &gt; 0.5]
    for distribution, prediction in zip(probs, best_n)]
</code></pre>
",0,0,596,2021-02-24 06:08:03,https://stackoverflow.com/questions/66345536/how-to-add-probabilities-to-model-predict-output
"Using google cloud for image classification, cropping and OCR","<p>Please allow me to ask a rather newbie question. So far, I have been using local tools like <a href=""https://en.wikipedia.org/wiki/ImageMagick"" rel=""nofollow noreferrer"">imagemagick</a> or <a href=""http://jocr.sourceforge.net/"" rel=""nofollow noreferrer"">GOCR</a> to perform the job, but that is rather old-fashioned, and I am urged to &quot;move to google cloud AI&quot;.</p>
<h3>The setup</h3>
<p>I have a (training) data set of various documents (as JPG and PDF) of different kinds, and by certain features (like prevailing color, repetitive layout) I intend to classify them, e.g. as <code>invoice type 1</code>, <code>invoice type 2</code>, <code>not an invoice</code>. In a 2nd step, I would like to OCR certain predefined areas of each document and extract e.g. the address of the company sending the invoice and the date.</p>
<h3>The architecture I am envisioning</h3>
<ol>
<li>In a modern platform as a service (<a href=""/questions/tagged/pass"" class=""post-tag"" title=""show questions tagged &#39;pass&#39;"" rel=""tag"">pass</a>), I have already set up an UI where I can upload new files. These are then locally stored in a directory with filenames (or in a MongoDB). Meta info like upload timestamp, user, original file name is stored in a DB.</li>
<li>The newly uploaded file should should then be submitted to google cloud which should do the classification step, and deliver back the label to be saved in the database.</li>
<li>The document pages should be auto-cropped, i.e. black or white margins are removed, most probably with google cloud as well. The parameters of the crop should be persisted in the DB.</li>
<li>In case it is e.g. an invoice, OCR should be performed (again by google cloud) for certain regions of the documents, e.g. a bounding box of spanning from the mid of the page to the right margin in the upper 10% of the cropped page. The results of the OCR should be again persisted locally.</li>
</ol>
<h3>The problem</h3>
<p>I seem to be missing the correct search term to figure out how to do it with google cloud. Is there an google-API (e.g. REST), I can use to upload and which gives me back the results of steps 2 to 4?</p>
","google-cloud-platform, classification, ocr, text-classification","<p>I think that your best option here is to use <a href=""https://cloud.google.com/document-ai/docs/basics#ocr"" rel=""nofollow noreferrer"">Document AI</a> (<a href=""https://cloud.google.com/document-ai/docs/apis"" rel=""nofollow noreferrer"">REST API and Libraries</a>).<br/></p>
<p>Using Document AI, you can:</p>
<ul>
<li>Convert images to text</li>
<li>Classify documents</li>
<li>Analyze and extract entities</li>
</ul>
<p>Additionally, for your use case, we have a new Document AI feature that is still in preview and has limited access which is the <a href=""https://cloud.google.com/document-ai/docs/invoice-parser"" rel=""nofollow noreferrer"">Invoice parser</a>.</p>
<p><strong>Invoice parser</strong> is similar to <strong><a href=""https://cloud.google.com/document-ai/docs/form-parser"" rel=""nofollow noreferrer"">Form parser</a></strong> but for invoices instead of forms. Check out the <strong>Invoice parser</strong> page and you will see what I mean by preview and limited access.</p>
<p>AFIK, there isn't any GCP tool for image edition.</p>
",1,0,785,2021-02-24 10:14:30,https://stackoverflow.com/questions/66348794/using-google-cloud-for-image-classification-cropping-and-ocr
Pseudo Labelling on Text Classification Python,"<p>I'm not good at machine learning. Can someone tell me how to doing text classification with pseudo labeling in python? I never know the right implementation, I have searched everywhere in internet, but I give up as found anything :'( I just found the implementation for numeric datasets, but I found no implementation for text classification (vectorized text).. So I wrote this syntax, but I don't know whether my code is correct or not. Am I doing wrong? Please help me guys, I really need your help.. :'(</p>
<p>This is my <a href=""https://drive.google.com/file/d/1V4_wl2tl5hpVVJDRWgz6g-buKyQhAMba/view?usp=sharing"" rel=""nofollow noreferrer"">datasets</a> if you wanna try. I want to classify 'Label' from 'Content'</p>
<p>My steps are:</p>
<ol>
<li>Split data 0.75 unlabeled, 0.25 labeled</li>
<li>From 0.25 labeld I split: 0.75 train labeled, and 0.25 test labeled</li>
<li>Make vectorizer for train, test and unlabeled datasets</li>
<li>Build first model from train labeled, then labelling the unlabeled datasets</li>
<li>Concatting train labeled data with prediction of unlabeled that have &gt;0.99 (pseudolabeled), and make the second model</li>
<li>Remove pseudolabeled from unabeled datasets</li>
<li>Predict the remaining unlabeled from second model, then iterate step 3 until the probability of predicted pseudolabeled &lt;0.99.</li>
</ol>
<p>This is my code:</p>
<h2>Performing pseudo labelling on text classification</h2>
<pre><code>from sklearn.naive_bayes import MultinomialNB

# Initiate iteration counter
iterations = 0

# Containers to hold f1_scores and # of pseudo-labels
train_f1s = []
test_f1s = []
pseudo_labels = []

# Assign value to initiate while loop
high_prob = [1] 

# Loop will run until there are no more high-probability pseudo-labels
while len(high_prob) &gt; 0:
    
    # Set the vector transformer (from data train)
    columnTransformer = ColumnTransformer([
    ('tfidf',TfidfVectorizer(stop_words=None, max_features=100000),
     'Content')
    ],remainder='drop')

    def transforms(series):
        before_vect = pd.DataFrame({'Content':series})
        vector_transformer = columnTransformer.fit(pd.DataFrame({'Content':X_train}))
        return vector_transformer.transform(before_vect)

    X_train_df = transforms(X_train);
    X_test_df = transforms(X_test);
    X_unlabeled_df = transforms(X_unlabeled)
    
    # Fit classifier and make train/test predictions
    nb = MultinomialNB()
    nb.fit(X_train_df, y_train)
    y_hat_train = nb.predict(X_train_df)
    y_hat_test = nb.predict(X_test_df)

    # Calculate and print iteration # and f1 scores, and store f1 scores
    train_f1 = f1_score(y_train, y_hat_train)
    test_f1 = f1_score(y_test, y_hat_test)
    print(f&quot;Iteration {iterations}&quot;)
    print(f&quot;Train f1: {train_f1}&quot;)
    print(f&quot;Test f1: {test_f1}&quot;)
    train_f1s.append(train_f1)
    test_f1s.append(test_f1)
   
    # Generate predictions and probabilities for unlabeled data
    print(f&quot;Now predicting labels for unlabeled data...&quot;)

    pred_probs = nb.predict_proba(X_unlabeled_df)
    preds = nb.predict(X_unlabeled_df)
    prob_0 = pred_probs[:,0]
    prob_1 = pred_probs[:,1]

    # Store predictions and probabilities in dataframe
    df_pred_prob = pd.DataFrame([])
    df_pred_prob['preds'] = preds
    df_pred_prob['prob_0'] = prob_0
    df_pred_prob['prob_1'] = prob_1
    df_pred_prob.index = X_unlabeled.index
    
    # Separate predictions with &gt; 99% probability
    high_prob = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] &gt; 0.99],
                           df_pred_prob.loc[df_pred_prob['prob_1'] &gt; 0.99]],
                          axis=0)
    
    print(f&quot;{len(high_prob)} high-probability predictions added to training data.&quot;)
    
    pseudo_labels.append(len(high_prob))

    # Add pseudo-labeled data to training data
    X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)
    y_train = pd.concat([y_train, high_prob.preds])      
    
    # Drop pseudo-labeled instances from unlabeled data
    X_unlabeled = X_unlabeled.drop(index=high_prob.index)
    
    print(f&quot;{len(X_unlabeled)} unlabeled instances remaining.\n&quot;)
    
    # Update iteration counter
    iterations += 1
</code></pre>
<p>I think I'm doing something wrong.. Because when I see the f1 scores it is decreasing. Please help me guys :'( I'm stressed.
<a href=""https://i.sstatic.net/acnUh.jpg"" rel=""nofollow noreferrer"">f1 scores image</a></p>
<h2>=================EDIT=================</h2>
<p>So I've search on journal, then I think that I've got misunderstanding about the concept of data splitting in pseudo-labelling.</p>
<p>I initially thought that, the steps starts from splitting the data into labeled and unlabeled data, then from that labeled data, it was splitted into train and test.</p>
<p>But after surfing and searching, I found in <a href=""http://ejnteti.jteti.ugm.ac.id/index.php/JNTETI/article/view/418/339"" rel=""nofollow noreferrer"">this journal</a> that my steps is incorrect. This journal says that the steps pseudo-labeling should start from splitting the data into train and test sets at first, and then from that train sets, data is splited to labeled and unlabeled datasets.</p>
<p>According to that journal, it reach the best result when splitting data into 90% of train sets and 10% of test sets. Then, from that 90% train set, it is splitted into 20% labeled data and 80% unlabeled data sets. This journal trying evidence range from 0.7 till 0.9 as boundary to drop the pseudo labeling, and on that proportion of splitting, the best evidence threshold value is 0.74. So I fix my steps with that new proportion and 0.74 threshold, and I finally got the F1 scores is increasing. Here are my steps:</p>
<ol>
<li>Split data 0.9 train, 0.1 test sets (I labeled the test sets, so I can measure the f1 scores)</li>
<li>From 0.9 train, I split: 0.2 labeled, and 0.8 unlabeled data</li>
<li>Making vectorizer for X value of labeled train, test and unlabeled training datasets</li>
<li>Build first model from labeled train, then labeling the unlabeled training datasets. Then measure the F-1 scores according to the test sets (that already labeled).</li>
<li>Concatting train labeled data with prediction of unlabeled that have probability &gt; 0.74 (threshold based on journal). We call this new data as pseudo-labelled, likened to the actual label), and make the second model from new train data sets.</li>
<li>Remove selected pseudo-labelled from unlabeled datasets</li>
<li>Use the second model to predict the remaining of unlabeled data, then iterate step 3 until there are no probability of predicted pseudo-labelled&gt;0.74</li>
<li>So the last model is the final.</li>
</ol>
<p>My syntax is still the same, I just changing the split proportion and I finally got my f1 scores increasing through 4 iterations: <a href=""https://i.sstatic.net/xN1Ds.jpg"" rel=""nofollow noreferrer"">my new f1 scores</a>.</p>
<p>Am I doing something right? Thank you for all of your attention guys.. So much thank you..</p>
","python, text-classification, semisupervised-learning","<blockquote>
<p>I'm not good at machine learning.</p>
</blockquote>
<p>Overall I would say that you are quite good at Machine Learning: semi-supervised learning is an advanced type of problem and I think your solution is quite good. At least the general principle seems correct, but it's difficult to say for sure (I don't have time to analyze the code in detail sorry). A few comments:</p>
<ul>
<li>One thing which might be improvable is the 0.74 threshold: this value certainly depends on the data, so you could do your own experiment by trying different threshold values and selecting the one which works best with your data.</li>
<li>Preferably it would be better to keep a final test set aside and use a separate validation set during the iterations. This would avoid the risk of data leakage.</li>
<li>I'm not sure about the stop condition for the loop. It might be ok but it might be worth trying other options:
<ul>
<li>Simply iterate a fixed number of times (for instance 10 times).</li>
<li>The stop condition could be based on &quot;no more F1-score improvement&quot; (i.e. stabilization of the performance), but it's a bit more advanced.</li>
</ul>
</li>
</ul>
<p>It's pretty good anyway, my comments are just ideas if you want to improve further. Note that it's been a long time since I've work with semi-supervised, I'm not sure I remember everything very well ;)</p>
",2,3,1440,2021-03-07 04:17:09,https://stackoverflow.com/questions/66513144/pseudo-labelling-on-text-classification-python
Keras multi classifier is always giving 0 output,"<p>I built this keras model for multi classification</p>
<pre><code>model = tf.keras.Sequential([
  tf.keras.layers.Dense(32, activation='relu'),
  tf.keras.layers.Dense(16, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(8, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))
</code></pre>
<p>This is my data, normalized</p>
<p><a href=""https://i.sstatic.net/mFcUW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mFcUW.png"" alt=""Normalized Data"" /></a></p>
<p>And these are my target values from 0 to 5</p>
<p><a href=""https://i.sstatic.net/ufPQ2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ufPQ2.png"" alt=""Target values"" /></a></p>
<p>I used this code to prepare dataset</p>
<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices((dff[:92000], target[:92000]))
test_dataset = tf.data.Dataset.from_tensor_slices((dff[500:520]))
# random slice of test dataset

train_dataset = train_dataset.batch(100)
test_dataset = test_dataset.batch(1)
</code></pre>
<p>Then I trained and tested my model using this</p>
<pre><code>model.fit(train_dataset, epochs=20) 

predictions = model.predict(test_dataset)
classes = np.argmax(predictions, axis = 1)
</code></pre>
<p>classes output is always 0: <code>[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]</code></p>
<p>I already tried without normalizing my data, but output was still same.</p>
","python, tensorflow, keras, text-classification, multilabel-classification","<p>From comments</p>
<blockquote>
<p>After changing it to <code>Dense(5, activation='softmax')</code> has resolved the issue (paraphrased from M.Innat)</p>
</blockquote>
<p>Working code as shown below</p>
<pre><code>model = tf.keras.Sequential([
  tf.keras.layers.Dense(32, activation='relu'),
  tf.keras.layers.Dense(16, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(8, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))
</code></pre>
<blockquote>
<p>Keras multi classifier is always giving 0 output is due to your last &gt; dense layer has only 1 unit which means your outputs has a
size (1,) and you are applying argmax operation that results getting
index 0 everytime (paraphrased from Frightera)</p>
</blockquote>
",0,0,363,2021-03-12 15:22:09,https://stackoverflow.com/questions/66602662/keras-multi-classifier-is-always-giving-0-output
is there a method to match list into other?,"<p>i have a dataframe like :</p>
<pre><code>  target   ;  terms
  Sport    ;  Football,Handball, Tennis
  Mécanique ;  voiture, moto, tracteur, bus
  Technologies;  ordinateur, téléphone,tablette, radio, écouteur
</code></pre>
<p>in input of program i have:</p>
<pre><code>list_terms=  ['Football','Handball','ordinateur','tablette','Tennis']
</code></pre>
<p>expected output :</p>
<pre><code>targets : Sport,Technologies 
</code></pre>
","python-3.x, pandas, nlp, text-classification","<p>You can use <code>apply()</code> on rows and check if <code>terms</code> column has any value in entry list.</p>
<pre class=""lang-py prettyprint-override""><code>def check(row):
    targets = [term for term in row['terms'].split(',') if term in entry_list]

    if len(targets) &gt; 0:
        return row['target'] + ' ({:.2f})'.format(len(targets)/len(entry_list))
    else:
        return np.nan

print(df.apply(check, axis=1).dropna().tolist())

&quot;&quot;&quot;
['Sport (0.50)', 'Technologies (0.33)']
&quot;&quot;&quot;
</code></pre>
<p>If you don't want probability after target, you could use</p>
<pre class=""lang-py prettyprint-override""><code>def check(row):
    targets = [term for term in row['terms'].split(',') if term in entry_list]
    return len(targets) &gt; 0

print(df[df.apply(check, axis=1)]['target'].tolist())

&quot;&quot;&quot;
['Sport', 'Technologies']
&quot;&quot;&quot;
</code></pre>
",0,0,44,2021-04-05 13:07:01,https://stackoverflow.com/questions/66953567/is-there-a-method-to-match-list-into-other
How to handle repeating text data but with Different Labels or Classes?,"<p>I am doing a Multi-class Text Classification. However, I have data that are repeating in the dataset. However, these are not duplicates, as they belong to different classes. The data is valid, these two classes are close to each other, The repeated text training data is not of the same class, but of diff classes with the same shared URLs. What can I do, so that my Text classifier effectively working at predicting the future inputs with higher probability without sharing probability with the other counterpart? Also are there any other techniques
TO NOTE: Only 10 % of training data is repeated with diff classes.</p>
","machine-learning, nlp, data-science, text-classification","<p>The problem you are trying to solve is not multi class classification but <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">multi label classification</a>. There are different methods to solve multi label classification. A starting point can be here : <a href=""https://scikit-learn.org/stable/modules/multiclass.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/multiclass.html</a></p>
",0,-1,779,2021-04-12 08:48:22,https://stackoverflow.com/questions/67055339/how-to-handle-repeating-text-data-but-with-different-labels-or-classes
"expected ndim=3, found ndim=4. when using K.function() in keras backend to get intermediate layers in a model","<p>I am trying to extract the last layer of a classification model trained on some data. The first layer is an <code>Embedding</code> layer, followed by the <code>bilstm</code> and the followed by the output dense layer. My code is sown below. I keep getting a 4d output <code>(1,38,300,300)</code> instead of a 3d <code>(1,38,300)</code>. 1 is the sample size, 38 is the max length of the sentence, and 300 is the word2vec length.</p>
<pre><code>from keras import backend as K
from tensorflow.keras.models import load_model
import numpy as np
import gensim
word2vec = 'GoogleNews-vectors-negative300.txt'


x_matrix = np.zeros((1, 38, 300))
sentene_label = 'the weather today was extremely unpredictable,0'
parts = sentene_label.split(',')
label = int(parts[1])  
sentence = parts[0] 

words = sentence.split(' ')
words = words[:x_matrix.shape[1]]  
for j, word in enumerate(words):
    if word in word2vec:
        # x_matrix[0, j, :] = word2vec[word]
        x_matrix[0, j, :] = loaded_model.word_vec(word)


model = load_model('TrainedModel.h5')
get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[2].output])  
layer_output = get_3rd_layer_output(x_matrix)[0]
print(&quot;Layer Output Shape 1 : &quot;, layer_output.shape)
</code></pre>
<p>I have cross-checked my code several times and I can't seem to figure out why the dimensions are wrong.</p>
<p>this is the Traceback</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/pkg/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 3427, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File &quot;&lt;ipython-input-2-bb840b495480&gt;&quot;, line 1, in &lt;module&gt;
    runfile('/am/vuwstocoisnrin1.vuw.ac.nz/ecrg-solar/kosimadukwe/Data Augmentation/test.py', wdir='/am/vuwstocoisnrin1.vuw.ac.nz/ecrg-solar/kosimadukwe/Data Augmentation')
  File &quot;/am/embassy/vol/x6/jetbrains/apps/PyCharm-P/ch-0/201.7846.77/plugins/python/helpers/pydev/_pydev_bundle/pydev_umd.py&quot;, line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File &quot;/am/embassy/vol/x6/jetbrains/apps/PyCharm-P/ch-0/201.7846.77/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py&quot;, line 18, in execfile
    exec(compile(contents+&quot;\n&quot;, file, 'exec'), glob, loc)
  File &quot;/am/vuwstocoisnrin1.vuw.ac.nz/ecrg-solar/kosimadukwe/Data Augmentation/test.py&quot;, line 451, in &lt;module&gt;
    layer_output = get_3rd_layer_output(x_matrix)[0]
  File &quot;/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/backend.py&quot;, line 4073, in func
    outs = model(model_inputs)
  File &quot;/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File &quot;/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py&quot;, line 424, in call
    return self._run_internal_graph(
  File &quot;/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py&quot;, line 560, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  File &quot;/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/layers/wrappers.py&quot;, line 539, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File &quot;/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 998, in __call__
    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
  File &quot;/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py&quot;, line 219, in assert_input_compatibility
    raise ValueError('Input ' + str(input_index) + ' of layer ' +
ValueError: Input 0 of layer bidirectional_9 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (1, 38, 300, 300)
</code></pre>
<p>The error is triggered on line</p>
<pre><code>layer_output = get_3rd_layer_output(x_matrix)[0]
</code></pre>
<p>The shape of x_matrix before calling get_3rd_layer_output is</p>
<pre><code>The shape of X matrix : (60, 38, 300)

</code></pre>
<p>TrainedModels architecture</p>
<pre><code>model = Sequential()
model.add(Embedding(vocab_size, 300, input_length=38, weights=[embedding_matrix], trainable=True))
model.add(Bidirectional(LSTM(100, dropout=0.2)))
model.add(Dense(3, activation='sigmoid'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='Adagrad', metrics=['accuracy'])
model.summary()

es = EarlyStopping(monitor='val_loss', mode='min', baseline=0.3, patience=100, verbose=1)
mc = ModelCheckpoint('TrainedModel.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)
hist = model.fit(train_sequences, train_y, epochs=200, verbose=False, batch_size=100,validation_data=(val_sequences, val_y),callbacks=[es, mc]) 

</code></pre>
<p>TrainedModels model.summary is</p>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_9 (Embedding)      (None, 38, 300)           7370400   
_________________________________________________________________
bidirectional_9 (Bidirection (None, 200)               320800    
_________________________________________________________________
dense_9 (Dense)              (None, 3)                 603       
=================================================================
Total params: 7,691,803
Trainable params: 7,691,803
Non-trainable params: 0
_________________________________________________________________

</code></pre>
","keras, nlp, text-classification, keras-layer","<p>The correct way to get any intermediate layer output is to create a sub-model that <strong>expects the same input of your trained model</strong>. In your case, the error raises because you pass to your trained model the 3D embedding matrix while you have to pass the same data you use for training (2D data whit integer-encoded words).</p>
<p>Here I produce a dummy example to extract correctly any intermediate output from your model.</p>
<p>Create dummy data:</p>
<pre><code>vocab_size = 111
emb_size = 300
input_length = 38
n_sample = 50
n_classes = 3

embedding_matrix = np.random.uniform(-1,1, (vocab_size, emb_size))
X = np.random.randint(0,vocab_size, (n_sample, input_length))
Y = np.random.randint(0,n_classes, (n_sample,))
</code></pre>
<p>Create model and train:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
from tensorflow.keras import backend as K

model = Sequential()
model.add(Embedding(vocab_size, emb_size, input_length=input_length, 
                    weights=[embedding_matrix], trainable=True))
model.add(Bidirectional(LSTM(100, dropout=0.2)))
model.add(Dense(n_classes, activation='sigmoid'))
model.compile(loss='sparse_categorical_crossentropy', 
              optimizer='Adagrad', metrics=['accuracy'])
model.fit(X,Y, epochs=3)  ### TRAINED WITH X
</code></pre>
<p>Get layer output:</p>
<pre><code>layer_id = 2
get_layer_output = K.function([model.layers[0].input], [model.layers[layer_id].output])
layer_output = get_layer_output(X)[0]  ### EXTRACT FROM X
# equal to:
# sub_model = Model(model.input, model.layers[layer_id].output)
# layer_output = sub_model.predict(X)  ### EXTRACT FROM X
</code></pre>
",1,1,930,2021-04-15 04:27:00,https://stackoverflow.com/questions/67102278/expected-ndim-3-found-ndim-4-when-using-k-function-in-keras-backend-to-get-i
"What is the simplest way to continue training a pre-trained BERT model, on a specific domain?","<p>I want to use a pre-trained BERT model in order to use it on a text classification task (I'm using Huggingface library). However, the pre-trained model was trained on domains that are different than mine, and I have a large unannotated dataset that can be used for fine-tuning it. If I use only my tagged examples and fine-tune it &quot;on the go&quot; while training on the specific task (BertForSequenceClassification), the dataset is too small for adapting the language model for the specific domain. What it the best way to do so?
Thanks!</p>
","nlp, text-classification, bert-language-model, huggingface-transformers, pytorch-lightning","<p>Let's clarify a couple of points first to reduce some ambiguity.</p>
<ol>
<li>BERT uses two pretraining objectives: Masked Language Modeling (MLM) and Next Sentence Prediction.</li>
<li>You mentioned having a large unannotated dataset, which you plan on using to fine-tune your BERT model. This is not how fine-tuning works. In order to fine-tune your pretrained model, you would need an <strong>annotated</strong> dataset i.e. document &amp; class pair for sequence classification downstream task.</li>
</ol>
<p>So what can you do? First, extend your general domain tokenizer with your unannotated dataset consisting of domain-specific vocabulary. Then, using this extended tokenizer you can continue pretraining on MLM and/or NSP objectives to modify your word embeddings. Finally, fine-tune your model using an annotated dataset.</p>
",5,2,2367,2021-04-27 11:27:42,https://stackoverflow.com/questions/67282155/what-is-the-simplest-way-to-continue-training-a-pre-trained-bert-model-on-a-spe
I have a data type problem in the text classification problem,"<p>I want to build deep learning classifiers for Kickstarter campaign prediction. I have a problem with the part of the model but I can not solve this.</p>
<p>My code:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from keras.models import Sequential
from keras import layers


df = pd.read_csv('../input/kickstarter-campaigns-dataset/kickstarter_data_full.csv')

df_X = [] # for x class
df_y = [] # for labels

for i in range(len(df)):
    tmp = str(df['blurb'][i]) + &quot; &quot; + str(df['goal'][i]) + &quot; &quot; + str(df['pledged'][i]) + &quot; &quot; + str(df['country'][i]) + &quot; &quot; + str(df['currency'][i]) + &quot; &quot; + str(df['category'][i]) + &quot; &quot; + str(df['spotlight'][i])  
    df_X.append(tmp)
    df_y.append(str(df['SuccessfulBool'][i]))

X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.25, random_state=1000)
vectorizer = CountVectorizer()
vectorizer.fit(X_train)

X_train = vectorizer.transform(X_train)
X_test  = vectorizer.transform(X_test)

input_dim = X_train.shape[1]

model = Sequential()
model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train,
                     epochs=100,
                     verbose=False,
                     validation_data=(X_test, y_test),
                     batch_size=10)
</code></pre>
<p>In this point, I am getting <em><strong>ValueError: Failed to find data adapter that can handle input: &lt;class 'scipy.sparse.csr.csr_matrix'&gt;, (&lt;class 'list'&gt; containing values of types {&quot;&lt;class 'str'&gt;&quot;})</strong></em></p>
<p>I try np.asarray for solving</p>
<pre><code>X_train = np.asarray(X_train)
y_train = np.asarray(y_train)
X_test = np.asarray(X_test)
y_test = np.asarray(y_test)
</code></pre>
<p>I get this <em><strong>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix).</strong></em></p>
<p>Therefore, I use this:</p>
<pre><code>np.asarray(X_train).astype(np.float32)
np.asarray(y_train).astype(np.float32)
np.asarray(X_test).astype(np.float32)
np.asarray(y_test).astype(np.float32)
</code></pre>
<p>But I get <em><strong>ValueError: setting an array element with a sequence.</strong></em></p>
<p>I try this:</p>
<pre><code>X_train = np.expand_dims(X_train, -1)
y_train   = np.expand_dims(y_train, -1)
X_test = np.expand_dims(X_test, -1)
y_test   = np.expand_dims(y_test, -1)
</code></pre>
<p>But I keep getting the same error in the part of history. <em><strong>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix).</strong></em></p>
<p>I study with Kickstarter Campaigns Dataset at the Kaggle.
<a href=""https://www.kaggle.com/sripaadsrinivasan/kickstarter-campaigns-dataset"" rel=""nofollow noreferrer"">https://www.kaggle.com/sripaadsrinivasan/kickstarter-campaigns-dataset</a></p>
<p>I don't have enough NLP information. I search and try solvings but I can not solve. This is my homework. Can you help me for this problem?</p>
<p>df_X and df_y are equal sizes and their output is as follows:
<a href=""https://i.sstatic.net/7vGuB.png"" rel=""nofollow noreferrer"">x</a>
<a href=""https://i.sstatic.net/I81xp.png"" rel=""nofollow noreferrer"">y</a></p>
","python, numpy, deep-learning, nlp, text-classification","<p>you need to add an embedding layer at the top of your NN to kind of vectorize words. something like this:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras import layers


df = pd.read_csv('../input/kickstarter-campaigns-dataset/kickstarter_data_full.csv')

df_X = [] # for x class
df_y = [] # for labels

for i in range(len(df)):
    tmp = str(df['blurb'][i]) + &quot; &quot; + str(df['goal'][i]) + &quot; &quot; + str(df['pledged'][i]) + &quot; &quot; + str(df['country'][i]) + &quot; &quot; + str(df['currency'][i]) + &quot; &quot; + str(df['category'][i]) + &quot; &quot; + str(df['spotlight'][i])  
    df_X.append(tmp)
    df_y.append(str(df['SuccessfulBool'][i]))

vocab_size = 1000
encoded_docs = [one_hot(d, vocab_size) for d in df_X]
max_length = 20
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
X_train, X_test, y_train, y_test = train_test_split(padded_docs, np.array(df_y)[:, None].astype(int), test_size=0.25, random_state=1000)
model = Sequential()
model.add(layers.Embedding(vocab_size, 100, input_length=max_length))
model.add(layers.Flatten())
model.add(layers.Dense(10, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
print(model.summary())
model.fit(X_train, y_train, 
          epochs=50, 
          verbose=1,
          validation_data=(X_test, y_test),
          batch_size=10)
</code></pre>
",1,0,305,2021-05-02 01:21:47,https://stackoverflow.com/questions/67352227/i-have-a-data-type-problem-in-the-text-classification-problem
How to add extra dense layer on top of BertForSequenceClassification?,"<p>I want to add an extra layer (and dropout) before the classification layer (I'm using PyTorch lightning) What is the best way to do it?</p>
","text-classification, bert-language-model, pytorch-lightning","<p>The class <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer""><code>BertForSequenceClassification</code></a> (that comes from the Huggingface Transformers <a href=""https://www.pytorchlightning.ai/blog/how-to-fine-tune-bert-with-pytorch-lightning"" rel=""nofollow noreferrer"">when using PyTorch lightning</a>) implements a fixed architecture. If you want to change it (e.g., by adding layers), you need to inherit your own module.</p>
<p>This is actually quite simple. You can copy the code of <code>BertForSequenceClassification</code> and modify the code <a href=""https://github.com/huggingface/transformers/blob/v4.5.0/src/transformers/models/bert/modeling_bert.py#L1515"" rel=""nofollow noreferrer"">between getting the pooled BERT output and getting the logits</a>.</p>
<p>Note however that adding a hidden layer to a classifier does not make much difference when finetuning BERT. The capacity of the additional hidden layer is negligible compared to the entire stack of BERT layers. Even If you cannot finetune the entire model, fine-tuning just the last BERT layer is probably better than adding an extra layer to the classifier.</p>
",0,0,1984,2021-05-05 09:40:38,https://stackoverflow.com/questions/67398812/how-to-add-extra-dense-layer-on-top-of-bertforsequenceclassification
Can we have inputs that is more than 1D in Pytorch (e.g word-embedding),"<p>Say I have some text and I want to classify them into three groups <code>food, sports, science</code>. If I have a sentence <code>I dont like to each mushrooms</code> we can use wordembedding (say 100 dimensions) to create a <code>6x100</code> matrix for this particular sentense.</p>
<p>Ususally when training a neural-network our data is a 2D array with the dimensions <code>n_obs x m_features</code></p>
<p>If I want to train a neural network on wordembedded sentences(i'm using Pytorch) then our input is 3D <code>n_obs x (m_sentences x k_words)</code></p>
<p>e.g</p>
<pre><code>#Say word-embedding is 3-dimensions
I = [1,2,3]
dont = [4,5,6]
eat = [7,8,9]
mushrooms = [10,11,12]

&quot;I dont eat mushrooms&quot; = [I,dont,eat,mushrooms] #First observation
</code></pre>
<p>Is the best way, when we have N&gt;2 dimensions, to do some kind of pooling e.g mean, or can we use the actual 2D-features as input?</p>
","neural-network, nlp, pytorch, text-classification","<p>Technically the input will be 1D, but that doesn't matter.</p>
<p>The internal architecture of your neural network will take care of recognizing the different words. You could for example have a convolution with a stride equal to the embedding size.</p>
<p>You can flatten a 2D input to become 1D and it will work fine. This is the way you'd normally do it with word embeddings.</p>
<pre><code>I = [1,2,3]
dont = [4,5,6]
eat = [7,8,9]
mushrooms = [10,11,12]

input = np.array([I,dont,eat,mushrooms]).flatten()
</code></pre>
<p>The inputs of a neural network have to always be of the same size, but as sentences are not, you will probably have to limit the the max length of the sentence to a set length of words and add paddings to the end of the shorter sentences:</p>
<pre><code>I = [1,2,3]
Am = [4,5,6]
short = [7,8,9]
paddingword = [1,1,1]

input = np.array([I,Am,eat,short, paddingword]).flatten()
</code></pre>
<p>Also you might want to look at <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">doc2vec from gensim</a>, which is an easy way to make embeddings for texts, which are then easy to use for a text classification problem.</p>
",1,0,241,2021-05-05 14:14:10,https://stackoverflow.com/questions/67403070/can-we-have-inputs-that-is-more-than-1d-in-pytorch-e-g-word-embedding
Text classification for more than 2 classes with python and keras,"<p>I'm trying categorization with a neuronal network.</p>
<p>my data looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Sentence</th>
<th>category</th>
</tr>
</thead>
<tbody>
<tr>
<td>sentence 1</td>
<td>0</td>
</tr>
<tr>
<td>sentence 2</td>
<td>1</td>
</tr>
<tr>
<td>sentence 3</td>
<td>2</td>
</tr>
<tr>
<td>sentence 4</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>therefore I have 4 different categories.
I seperate the sentences and the labels in 2 different lists and put the first list (with the sencenes) in a tokenizer.</p>
<p><strong>Edit:</strong> Because someone asked about data samples. Here is a link to the Dropbox (<a href=""https://www.dropbox.com/s/s4vog17etmot06y/data_samples.txt?dl=0"" rel=""nofollow noreferrer"">data_samples.txt</a>) and some more explanations. 0 = not needed, 1 = title, 2= name from author, 3 = pages. The sentence is with | from the label separated. There are lot of special character (like -- &gt; ,&quot;) because of the lemmatization but the tokenizer skips these characters.
The content is from different websites and I only use it in the context of study (no for money or in bad intentions)</p>
<pre><code>y_train = array(labels) # it has to be numpy array otherwise it will cause errors, labels is a list of integers
x_train = tokenizer.texts_to_sequences(newLines) # newLines is a list with all sentences
x_train = pad_sequences(x_train, maxlen=sequenceLength)
vocabSize = len(tokenizer.word_index) + 1
</code></pre>
<p>An now i will train my model</p>
<pre><code>mymodel = Sequential()
mymodel.add(Embedding(input_dim=vocabSize, output_dim=100, input_length=sequenceLength))
mymodel.add(Conv1D(32, 3, padding='same', activation='relu'))
mymodel.add(MaxPooling1D())
mymodel.add(Flatten())
mymodel.add(Dense(250, activation='relu'))
mymodel.add(Dense(4, activation='softmax'))
mymodel.compile(optimizer=&quot;adam&quot;, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
history = mymodel.fit(x_train, y_train, epochs=30, batch_size=8)
</code></pre>
<p>But if i start the programm i get the following error:</p>
<ul>
<li>ValueError: Shapes (None, 1) and (None, 4) are incompatible</li>
</ul>
<p>I know it is because of the last Dense Layer. If I write 1 instead of 4, I don't get an error. But I thought the last dense layer need a 4 as a parameter, because I have 4 categories.
I think the last dense layer with a 1 instead of a 4 is not correct, because loss is 0.0000e+00. And that look not correct xD</p>
<p>What did I do wrong?</p>
","python, keras, text-classification","<p>replace loss=&quot;categorical_crossentropy&quot; with &quot;sparse_ categorical_crossentropy&quot;.</p>
<p>The model produces a value per class; the sparse_ loss knows how to compare a single value in range [0-3] to these 4 score values.</p>
",0,0,86,2021-05-12 10:17:08,https://stackoverflow.com/questions/67501656/text-classification-for-more-than-2-classes-with-python-and-keras
How do I show the other sentiment scores from text classification?,"<p>I am doing sentiment analysis, and I was wondering how to <strong>show the other sentiment scores</strong> from classifying my sentence: &quot;Tesla's stock just increased by 20%.&quot;</p>
<p>I have three sentiments: <strong>positive</strong>, <strong>negative</strong> and <strong>neutral</strong>.</p>
<p>This is my code, which contains the sentence I want to classify:</p>
<pre><code>pip install happytransformer
from happytransformer import HappyTextClassification 
happy_tc = HappyTextClassification(&quot;BERT&quot;, &quot;ProsusAI/finbert&quot;, num_labels=3)

result = happy_tc.classify_text(&quot;Tesla's stock just increased by 20%&quot;)
</code></pre>
<p>This is the result code and output:</p>
<pre><code>print(result)

TextClassificationResult(label='positive', score=0.929110586643219)
</code></pre>
<p>This is the sentiment score, which only shows the score for positive:</p>
<pre><code>print(result.label)
print(result.score)

positive
0.92
</code></pre>
<p>Now, how do I make it so that it shows the sentiment scores for negative and neutral as well as the positive?</p>
<p>Something that looks like this:</p>
<pre><code>positive
0.92

negative
0.05

neutral
0.03
</code></pre>
<p>Thanks.</p>
","python, text-classification, bert-language-model","<p>Because HappyTransformer does not support multi class probabilities I suggest to use another library. The library <code>flair</code> provides even more functionality and can give you your desired multi class probabilities, with something like this:</p>
<pre><code>from flair.models import TextClassifier
from flair.data import Sentence

tc = TextClassifier.load('en-sentiment')

sentence = Sentence('Flair is pretty neat!')

tc.predict(sentence, multi_class_prob=True)

print('Sentence above is: ', sentence.labels)

</code></pre>
<p>Just <code>pip install flair</code> for usage.</p>
<p>Note that we use a different model than BERT and only returns two labels not three.</p>
<p>Another option is to use the <code>HuggingFace</code> library. It allows for using self defined labels.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;)
classifier(
    &quot;This is a course about the Transformers library&quot;,
    candidate_labels=[&quot;education&quot;, &quot;politics&quot;, &quot;business&quot;],
)
</code></pre>
<pre><code>{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
</code></pre>
<p>In your case, you switch out the labels to <code>[&quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;]</code>.</p>
<p>The examples are taken from: <a href=""https://huggingface.co/course/chapter1/3?fw=pt"" rel=""nofollow noreferrer"">https://huggingface.co/course/chapter1/3?fw=pt</a></p>
",0,0,797,2021-06-15 13:59:16,https://stackoverflow.com/questions/67987738/how-do-i-show-the-other-sentiment-scores-from-text-classification
Applying abbreviation to the column of a dataframe based on another column of the same dataframe,"<p>I have two columns in the dataframe, one of which is a class and another is a description. In the description I have some abbreviations. I want to expand these abbreviations based on the class value. I have a dictionary with class as key and in the value I have another dictionary with abbreviations and its full form. Since these abbreviations mean different based on the class.
eg :- IT could mean ether Information Transport or Information Technology based on the class label.</p>
<p>I tried groupby, but was not able to get it back in the original dataframe.
Any help is much appreciated.
Thanks</p>
<p>This is how I was trying:</p>
<pre><code>grouped = df.groupby('class')
for n,j in grouped:
    j['description'].str.split().apply(lambda x: ' '.join([abb[n].get(e, e) for e in x]))
</code></pre>
<p><a href=""https://i.sstatic.net/e7TT4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/e7TT4.png"" alt=""example"" /></a></p>
","python, pandas, nlp, pandas-groupby, text-classification","<p>Here is a working example that takes the row as input and looks up the <code>class</code> value in the dictionary, and replaces strings <code>description</code> with the corresponding value in the dict:</p>
<pre><code>import pandas as pd

abb = {'IT':{'SQL':'Structured Query Language'},'Sales':{'SQL':'Sales Qualified Lead'}}

data = [{'class':'IT', 'description':'SQL developer'},{'class':'Sales', 'description':'senior SQL'}]
df = pd.DataFrame(data)

def replace_strings(row):
    text = row['description']
    for key, value in abb[row['class']].items():
        text = text.replace(key, value)
    return text

df['description'] = df.apply(replace_strings, axis=1)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">class</th>
<th style=""text-align: left;"">description</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">IT</td>
<td style=""text-align: left;"">Structured Query Language developer</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">Sales</td>
<td style=""text-align: left;"">senior Sales Qualified Lead</td>
</tr>
</tbody>
</table>
</div>",0,-1,1033,2021-07-10 20:05:47,https://stackoverflow.com/questions/68331174/applying-abbreviation-to-the-column-of-a-dataframe-based-on-another-column-of-th
Is label encoding needed when using TfidfVectorizer?,"<p>Probably a very newbie question:</p>
<p>I'm working on a multi-class text classification project where all my features and labels are text based.</p>
<p>It has just came to my understanding that I'm not encoding the features and labels since I was relaying on the below:</p>
<pre><code>def _create_transformer_from_feature_columns(columns):
    tuples = []
    for col in list(columns):
        tfidf_kwargs = {'ngram_range': (1, 2), 'sublinear_tf': True}
        if col not in NON_LEMMATIZED_COLUMN_NAMES:
            tfidf_kwargs.update({'tokenizer': Tokenizer()})
        tuples.append((f'vec_{col}', TfidfVectorizer(**tfidf_kwargs), col))

    return ColumnTransformer(tuples, remainder='passthrough')

df_list = []
for bug in useful_bugs_dict.values():
    # convert bug data into feature metric
    bug_data = bug.get_data_as_df()
    group_name = bug_data['group_name'][0]
    if group_name not in group_owners_dict:
        owner_id = len(group_owners_dict)
        group_owners_dict[group_name] = owner_id
        group_owner_id_dict[owner_id] = group_name

    df_list.append(bug_data)

training_data = pd.concat(df_list)
training_data.reset_index(drop=True, inplace=True)

columns = training_data.drop('group_name', axis='columns').columns
transformer = _create_transformer_from_feature_columns(columns)



labels = training_data['group_name']
features = training_data.drop('group_name', axis='columns')

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)

</code></pre>
<p>Also I'm using XGBClassifier and I'm getting this warning:</p>
<pre><code>/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/sklearn.py:1146: 
UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. 
To remove this warning, do the following: 
1) Pass option use_label_encoder=False when constructing XGBClassifier object; 
and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
</code></pre>
<p>I was under the impression will do it for me.</p>
<p>Was I wrong?</p>
","python, scikit-learn, text-classification","<p>The warning is unrelated to <code>TfidfVectorizer</code>. Its <code>fit</code> and <code>fit_transform</code> methods only rely on <code>X</code> to compute the tf-idf-weighted document-term matrix. <code>y</code> is ignored in both cases and its encoding is irrelevant.</p>
<p>For the <code>scikit-learn</code> classifiers, encoding <code>y</code> is also not mandatory. Passing string value objects in classification problems is usually not a problem. Note that the following code for a multiclass problem will execute without any issues:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier


X = ['doc one', 'doc two', 'number three']
y = [['yes', 'ok'], ['yes', 'not okay'], ['no', 'not okay']]

vec = TfidfVectorizer()
Xt = vec.fit_transform(X, y)

clf = DecisionTreeClassifier()
clf.fit(Xt, y)
</code></pre>
<p>The warning however is from the <code>XGBClassifier </code> which is not from <code>scikit-learn</code>. And apparently, the internal encoding of <code>y</code> <em>is deprecated and will be removed in a future release</em>. So in this particular case, you will have to do it explicitly yourself <em>in the future</em>, e.g when you use the next version(s).</p>
",1,-1,796,2021-07-11 16:24:23,https://stackoverflow.com/questions/68338047/is-label-encoding-needed-when-using-tfidfvectorizer
"keras - Layer model_131 expects 7 input(s), but it received 1 input tensors","<p>I'm working on a text classification project with a dataframe that has 7 features (columns) and 1 label column (multi class ==&gt; 6 possible labels overall)</p>
<p>All my data is text.</p>
<p>Since I wanted to keep my columns separated I ended up with the below code:</p>
<pre><code>input_message = Input(shape=(128,))
x = Embedding(vocab_size, 64)(input_message)
x = Flatten()(x)
x = Dense(64, activation=&quot;relu&quot;)(x)
x = Dense(32, activation=&quot;relu&quot;)(x)
x = Dense(4, activation=&quot;relu&quot;)(x)
model_message = Model(inputs=input_message, outputs=x)

input_description = Input(shape=(128,))
x = Embedding(vocab_size, 64)(input_description)
x = Flatten()(x)
x = Dense(64, activation=&quot;relu&quot;)(x)
x = Dense(32, activation=&quot;relu&quot;)(x)
x = Dense(4, activation=&quot;relu&quot;)(x)
model_description = Model(inputs=input_description, outputs=x)

input_errors = Input(shape=(2,))
x = Embedding(2, 1)(input_errors)
x = Flatten()(x)
x = Dense(1, activation=&quot;relu&quot;)(x)
model_errors = Model(inputs=input_errors, outputs=x)

input_panics = Input(shape=(2,))
x = Embedding(2, 1)(input_panics)
x = Flatten()(x)
x = Dense(1, activation=&quot;relu&quot;)(x)
model_panics = Model(inputs=input_panics, outputs=x)

input_images = Input(shape=(2,))
x = Embedding(2, 1)(input_images)
x = Flatten()(x)
x = Dense(1, activation=&quot;relu&quot;)(x)
model_images = Model(inputs=input_images, outputs=x)

input_committer = Input(shape=(16,))
x = Embedding(16, 1)(input_committer)
x = Flatten()(x)
x = Dense(1, activation=&quot;relu&quot;)(x)
model_committer = Model(inputs=input_committer, outputs=x)

input_reporter = Input(shape=(6,))
x = Embedding(6, 1)(input_reporter)
x = Flatten()(x)
x = Dense(1, activation=&quot;relu&quot;)(x)
model_reporter = Model(inputs=input_reporter, outputs=x)

combined = Concatenate()([model_message.output, model_description.output, model_errors.output, 
                          model_panics.output, model_images.output, model_committer.output, model_reporter.output])

z = Dense(6, activation='softmax')(combined)
model = Model(inputs=[model_message.input, model_description.input, 
                      model_errors.input, model_panics.input, model_images.input, 
                      model_committer.input, model_reporter.input], outputs=z)



model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=keras.optimizers.RMSprop(),
    metrics=[&quot;accuracy&quot;],
)
model.summary()

history = model.fit(X_train, y_train, batch_size=64, epochs=2, validation_split=0.2)
</code></pre>
<p>When running it, I'm getting the below error:</p>
<p><code>ValueError: Layer model_131 expects 7 input(s), but it received 1 input tensors. Inputs received: [&lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 7) dtype=string&gt;]</code></p>
<p>below is the output of <code>model.summary()</code>:</p>
<pre><code>Model: &quot;model_131&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_128 (InputLayer)          [(None, 128)]        0                                            
__________________________________________________________________________________________________
input_129 (InputLayer)          [(None, 128)]        0                                            
__________________________________________________________________________________________________
embedding_99 (Embedding)        (None, 128, 64)      640000      input_128[0][0]                  
__________________________________________________________________________________________________
embedding_100 (Embedding)       (None, 128, 64)      640000      input_129[0][0]                  
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 8192)         0           embedding_99[0][0]               
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 8192)         0           embedding_100[0][0]              
__________________________________________________________________________________________________
input_130 (InputLayer)          [(None, 2)]          0                                            
__________________________________________________________________________________________________
input_131 (InputLayer)          [(None, 2)]          0                                            
__________________________________________________________________________________________________
input_132 (InputLayer)          [(None, 2)]          0                                            
__________________________________________________________________________________________________
input_133 (InputLayer)          [(None, 16)]         0                                            
__________________________________________________________________________________________________
input_134 (InputLayer)          [(None, 6)]          0                                            
__________________________________________________________________________________________________
dense_189 (Dense)               (None, 64)           524352      flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_192 (Dense)               (None, 64)           524352      flatten_71[0][0]                 
__________________________________________________________________________________________________
embedding_101 (Embedding)       (None, 2, 1)         2           input_130[0][0]                  
__________________________________________________________________________________________________
embedding_102 (Embedding)       (None, 2, 1)         2           input_131[0][0]                  
__________________________________________________________________________________________________
embedding_103 (Embedding)       (None, 2, 1)         2           input_132[0][0]                  
__________________________________________________________________________________________________
embedding_104 (Embedding)       (None, 16, 1)        16          input_133[0][0]                  
__________________________________________________________________________________________________
embedding_105 (Embedding)       (None, 6, 1)         6           input_134[0][0]                  
__________________________________________________________________________________________________
dense_190 (Dense)               (None, 32)           2080        dense_189[0][0]                  
__________________________________________________________________________________________________
dense_193 (Dense)               (None, 32)           2080        dense_192[0][0]                  
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 2)            0           embedding_101[0][0]              
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 2)            0           embedding_102[0][0]              
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 2)            0           embedding_103[0][0]              
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 16)           0           embedding_104[0][0]              
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 6)            0           embedding_105[0][0]              
__________________________________________________________________________________________________
dense_191 (Dense)               (None, 4)            132         dense_190[0][0]                  
__________________________________________________________________________________________________
dense_194 (Dense)               (None, 4)            132         dense_193[0][0]                  
__________________________________________________________________________________________________
dense_195 (Dense)               (None, 1)            3           flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_196 (Dense)               (None, 1)            3           flatten_73[0][0]                 
__________________________________________________________________________________________________
dense_197 (Dense)               (None, 1)            3           flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_198 (Dense)               (None, 1)            17          flatten_75[0][0]                 
__________________________________________________________________________________________________
dense_199 (Dense)               (None, 1)            7           flatten_76[0][0]                 
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 13)           0           dense_191[0][0]                  
                                                                 dense_194[0][0]                  
                                                                 dense_195[0][0]                  
                                                                 dense_196[0][0]                  
                                                                 dense_197[0][0]                  
                                                                 dense_198[0][0]                  
                                                                 dense_199[0][0]                  
__________________________________________________________________________________________________
dense_200 (Dense)               (None, 6)            84          concatenate_17[0][0]             
==================================================================================================
Total params: 2,333,273
Trainable params: 2,333,273
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/2
</code></pre>
<p>I wasn't able to understand what it means.</p>
<p>I do understand that the model expects 7 inputs (they're also being passed to it), so why it claims to only receive 1?</p>
<p>BTW the output of <code>X_train.shape</code> is <code>(11652, 7)</code></p>
<p><strong>Updating the code section per comments discussion:</strong></p>
<p>So I've decreased the number of layers to 2 and ended up with this:</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from keras.layers.merge import concatenate
from keras.preprocessing.text import Tokenizer as Tok
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = 1000
embedding_dim = 16
max_length = 128
trunc_type='post'
padding_type='post'
oov_tok = &quot;&lt;OOV&gt;&quot;


# prepare input data
def prepare_inputs(X_train, X_test):
    oe = OrdinalEncoder()
    oe.fit(X_train)
    X_train_enc = oe.fit_transform(X_train)
    X_test_enc = oe.fit_transform(X_test)
    return X_train_enc, X_test_enc
 
# prepare free text input
def prepare_free_text_inputs(X_train, X_test):
    training_sentences = X_train['message'] + ' ' + X_train['description']
    testing_sentences = X_test['message'] + ' ' + X_test['description']
    tokenizer = Tok(num_words=vocab_size, oov_token=oov_tok)
    tokenizer.fit_on_texts(training_sentences)
    word_index = tokenizer.word_index
    
    training_sequences = tokenizer.texts_to_sequences(training_sentences)
    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
    
    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
    return training_padded, testing_padded
    
    
# prepare target
def prepare_targets(y_train, y_test):
    le = LabelEncoder()
    le.fit(y_train)
    y_train_enc = le.transform(y_train)
    y_test_enc = le.transform(y_test)
    return y_train_enc, y_test_enc
 

# prepare free_text input data
X_train_tokenized, X_test_tokenized = prepare_free_text_inputs(X_train, X_test)

# prepare categorical input data
X_train_ord, X_test_ord = prepare_inputs(X_train.iloc[:, 2:], X_test.iloc[:, 2:])

X_train_enc = pd.concat([pd.DataFrame(X_train_tokenized), pd.DataFrame(X_train_ord)], axis=1)
X_test_enc = pd.concat([pd.DataFrame(X_test_tokenized), pd.DataFrame(X_test_ord)], axis=1)

# prepare output data
y_train_enc, y_test_enc = prepare_targets(y_train, y_test)

print('Train', X_train_enc.shape, y_train_enc.shape)
print('Test', X_test.shape, y_test.shape)

input_free_text = Input(shape=(128,))
x = Embedding(vocab_size, 64)(input_free_text)
x = Flatten()(x)
x = Dense(64, activation=&quot;relu&quot;)(x)
x = Dense(32, activation=&quot;relu&quot;)(x)
x = Dense(4, activation=&quot;relu&quot;)(x)
model_free_text = Model(inputs=input_free_text, outputs=x)

input_categorical = Input(shape=(5,))
x = Embedding(5, 1)(input_categorical)
x = Flatten()(x)
x = Dense(1, activation=&quot;relu&quot;)(x)
model_categorical = Model(inputs=input_categorical, outputs=x)

combined = Concatenate()([model_free_text.output, model_categorical.output])

z = Dense(6, activation='softmax')(combined)
model = Model(inputs=[model_free_text.input, model_categorical.input], outputs=z)


model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=keras.optimizers.RMSprop(),
    metrics=[&quot;accuracy&quot;],
)

model.summary()
keras.utils.plot_model(model, &quot;model.png&quot;, show_shapes=True)
history = model.fit(y_train_enc, y_train_enc, batch_size=64, epochs=2, validation_split=0.2)
</code></pre>
<p>Still got similar error:</p>
<p><code>ValueError: Layer model_235 expects 2 input(s), but it received 1 input tensors. Inputs received: [&lt;tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64&gt;]</code></p>
","python, keras, scikit-learn, text-classification","<p>Answering my own question then:</p>
<p>It turns out that I didn't handle the <code>fit</code> model method correctly.
I wasn't aware it can also get multiple inputs.
This is what I've ended up doing (based on the updated question code):</p>
<pre><code>history = model.fit(x=[X_train_tokenized, X_train_ord], y=y_train_enc, 
                    validation_data=([X_test_tokenized, X_test_ord], y_test_enc), 
                    batch_size=64, epochs=10)

test_scores = model.evaluate(x=[X_test_tokenized, X_test_ord], y=y_test_enc, verbose=0)
</code></pre>
",0,0,1043,2021-07-21 09:34:00,https://stackoverflow.com/questions/68467201/keras-layer-model-131-expects-7-inputs-but-it-received-1-input-tensors
Matmul error when trying to predict a new text using skmultilearn.BinaryRelevance,"<p>I am trying to create a small example of multi label text classification:</p>
<pre><code>import skmultilearn
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from scipy.sparse import csr_matrix
from pandas.core.common import flatten
from sklearn.naive_bayes import MultinomialNB
from skmultilearn.problem_transform import BinaryRelevance

TRAIN_DATA = [

    ['Como efetuar uma conexão com MySQL usando PHP ?', ['desenvolvimento','banco']],
    ['Quais são os melhores clientes de VPN hoje em dia?', ['redes']],
    ['Qual é o equivalente ao tipo booleano no Oracle?', ['banco']],
    ['Como remover entidade indesejada da sessão do Hibernate?', ['desenvolvimento']],
    ['Como implementar o pool de conexão TCP em java?', ['desenvolvimento','redes']],
    ['Como posso me conectar ao banco de dados PostgreSQL remotamente de outra rede?', ['banco','redes']],
    ['Qual a função python para remover acentos em uma string?', ['desenvolvimento']],
    ['Como remover índices no SQL Server?', ['banco']],
    ['Como configurar o firewall com DMZ?', ['redes']]
]

data_frame = pd.DataFrame(TRAIN_DATA, columns=['text','labels'])
corpus = data_frame['text']
unique_labels = set(flatten(data_frame['labels']))
for u in unique_labels:
    data_frame[u] = 0
    data_frame[u] = pd.to_numeric(data_frame[u])
for i, row in data_frame.iterrows():
    for u in unique_labels:
        if u in row.labels:
            data_frame.at[i,u] = 1
tfidf = TfidfVectorizer()
Xfeatures = tfidf.fit_transform(corpus).toarray()
y = data_frame[unique_labels]
binary_rel_clf = BinaryRelevance(MultinomialNB())
binary_rel_clf.fit(Xfeatures,y)
predict_text = ['SQL Server no PHP?']
X_predict = tfidf.fit_transform(predict_text)
br_prediction = binary_rel_clf.predict(X_predict)
print(br_prediction)
</code></pre>
<p>However, I got this error:</p>
<pre><code>ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 56 is different from 4)
</code></pre>
<p>What &quot;dimension&quot; do I need to change to run predict() correctly?</p>
","python, scikit-learn, text-classification, skmultilearn","<p>You are using a <code>TfidfVectorizer</code> to transform your text features. You should fit the transformer <em>only once</em> on the training data, which is <code>corpus</code> in your case. When preparing the data to test/predict, you should however use the <code>transform</code> method and <strong>not</strong> <code>fit_transform</code> again, since that would refit the transformer.</p>
<p>Change the following to make it work:</p>
<pre class=""lang-py prettyprint-override""><code>X_predict = tfidf.transform(predict_text)
</code></pre>
",1,0,78,2021-07-24 19:39:47,https://stackoverflow.com/questions/68513269/matmul-error-when-trying-to-predict-a-new-text-using-skmultilearn-binaryrelevanc
Stanford Classifier producing wrong results,"<p>I'm trying to perform a basic text classification with the <a href=""https://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">Stanford Classifier</a>. My example data set is based on <a href=""https://raw.githubusercontent.com/bigmlcom/python/master/data/spam.csv"" rel=""nofollow noreferrer"">Ham or Spam</a>.</p>
<p>This is my code:</p>
<pre><code>Properties props = new Properties();
ColumnDataClassifier cdc = new ColumnDataClassifier(props);

Classifier&lt;String, String&gt; cl = cdc.makeClassifier(cdc.readTrainingExamples(&quot;data.train&quot;));

for (String line : ObjectBank.getLineIterator(&quot;data.test&quot;, &quot;utf-8&quot;)) {
    Datum&lt;String, String&gt; d = cdc.makeDatumFromLine(line);
    System.out.println(line + &quot;  ==&gt;  &quot; + cl.classOf(d));
}
</code></pre>
<p>However, whatever text I try to classify, it always classifies it as Ham. The following sentence is clearly Spam, still it is classified as Ham:</p>
<blockquote>
<p>FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For terms &amp; conditions, visit <a href=""http://www.example.com"" rel=""nofollow noreferrer"">www.example.com</a></p>
</blockquote>
<p>Where is my mistake?</p>
","java, machine-learning, stanford-nlp, text-classification","<p>My mistake was that I didn't provide any properties:</p>
<pre><code>Properties props = new Properties();
ColumnDataClassifier cdc = new ColumnDataClassifier(props);
</code></pre>
<p>You can either specify the properties directly in the code:</p>
<pre><code>// set up pipeline properties
Properties props = new Properties();
// set the list of annotators to run
props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,kbp,quote&quot;);
// set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
props.setProperty(&quot;coref.algorithm&quot;, &quot;neural&quot;);
</code></pre>
<p>Or you can provide a properties file:</p>
<pre><code>ColumnDataClassifier cdc = new ColumnDataClassifier(propFile);
</code></pre>
",1,1,117,2021-07-28 17:01:51,https://stackoverflow.com/questions/68564577/stanford-classifier-producing-wrong-results
Sklearn train_test_split split a dataset to compare predicted labels with ground truth labels,"<p>I am trying to perform a multi-class text classification using SVM with a small dataset by adapting from <a href=""https://medium.com/@ruixuanl/multi-class-text-classification-with-extremely-small-data-set-deep-learning-b38dfb386f8e"" rel=""nofollow noreferrer"">this guide</a>. The input csv contains a 'text' column and a 'label' column (which have been manually assigned for this specific task).</p>
<p>One label needs to be assigned for each text entry. By using the LinearSVC model and TfidfVectorizer I obtained an accuracy score of 75% which seems more than expected for a very small dataset of only 400 samples. In order to further raise the accuracy I wanted to have a look at the entries that were not correctly classified but here I have an issue. Since I used train_test_split like this:</p>
<pre class=""lang-py prettyprint-override""><code>Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, test_size=0.1, random_state = 1004)
</code></pre>
<p>I don't know which text entries have been used by the train_test_split function (as far as I understand the function chooses randomly the 10% entries for the test_size). So I don't know against which subset of the corpus original entries labels should I compare the list of predicted labels for the test dataset. In other words is there a method to enforce a subset to be assigned for the test_size i.e the last 40 entries from the 400 total entries in the dataset?</p>
<p>This would help to manually compare the predicted labels vs the ground truth labels.</p>
<p>Below is the code:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score

import pandas as pd
import numpy as np
import os


class Config:

    # Data and output directory config
    data_path = r'./take3/Data'
    code_train = r'q27.csv'



if __name__ == &quot;__main__&quot;:


    print('--------Code classification--------\n')

    Corpus = pd.read_csv(os.path.join(Config.data_path, Config.code_train), sep = ',', encoding='cp1252', usecols=['text', 'label'])

    train_text = ['' if type(t) == float else t for t in Corpus['text'].values]


    # todo fine tunining
    tfidf = TfidfVectorizer(
        sublinear_tf=True,
        min_df=3, norm='l2',
        encoding='latin-1',
        ngram_range=(1, 2),
        stop_words='english')

    X = tfidf.fit_transform(train_text)             # Learn vocabulary and idf, return document-term matrix.

    # print('Array mapping from feature integer indices to feature name',tfidf.get_feature_names())
    print('X.shape:', X.shape)

    y = np.array(list(Corpus['label']))
    print('The corpus original labels:',y)
    print('y.shape:', y.shape)

    Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, test_size=0.1, random_state = 1004)


    model = LinearSVC(random_state=1004)
    model.fit(Train_X, Train_Y)

    SVM_predict_test = model.predict(Test_X)
    accuracy = accuracy_score(Test_Y, SVM_predict_test, normalize=True, sample_weight=None)*100
    print('Predicted labels for the test dataset', SVM_predict_test)
    print(&quot;SVM accuracy score: {:.4f}&quot;.format(accuracy))
</code></pre>
<p>And this is the received output:</p>
<pre><code>
                         
--------Code classification--------

X.shape: (400, 136)
The corpus original labels: [15 20  9 14 98 12  3  4  4 22 99  3 98 20 99  1 10 20  8 15 98 12 18  7
 20 99  8  8 13  2  8  6 22  4 98  5 98 12 18  8 98 18 24  4  3 19 12  5
 20  6  8 15  5 14 19 22 16 10 24 16 98  8  8 16  2 20  4  8 20  6 22 98
  3 98 15 12  2 13  5  8  8  1 10 16 20 12  7 20 98 22 99 10 12  8  8 16
 16  4  4 99 20  8 16  2 12 15 16 10  5 22  8  7  7  4  5 12 16 14  1 10
 22 20  4  4  5 99 16  3  5 22 99  5  3  4  4  3  6 99  8 20  2 10 98  6
  6  8 99  3  8 99  2  5 15  6  6  7  8 14  9  4 20  3 99  5 98 15  5  5
 20 10  4 99 99 16 22  8 10 22 98 12  3  5  9 99 14  8  9 18 20 14 15 20
 20  1  6 23 22 20  6  1 18  8 12 10 15 10  6 10  3  4  8 24 14 22  5  3
 22 24 98 98 98  4 15 19  5  8  1 17 16  6 22 19  4  8  2 15 12 99 16  8
  9  1  8 22 14  5 20  2 10 10 22 12 98  3 19  5 98 14 19 22 18 16 98 16
  6  4 24 98 24 98 15  1  3 99  5 10 22  4 16 98 22  1  8  4 20  8  8  5
 20  4  3 20 22  4 20 12  7 21  5  4 16  8 22 20 99  5  6 99  8  3  4 99
  6  8 12  3 10  4  8  5 14 20  6 99  4  4  6  4 98 21  1 23 20 98 19  6
  4 22 98 98 20 10  8 10 19 16 14 98 14 12 10  4 22 14  3 98 10 20 98 10
  9  7  3  8  3  6  6 98  8 99  1 20 18  8  2  6 99 99 99 14 14 16 20 99
  1 98 23  6 12  4  1  3 99 99  3 22  5  7 16 99]
y.shape: (400,)
Predicted labels for the test dataset [ 1  8  5  4 15 10 14 12  6  8  8 16 98 20  7 99 99 12 99 24  4 98 99  3
 20  3  6 14 18 98 99 22  4 99  4 10 14  4  3 98]
SVM accuracy score: 75.0000
</code></pre>
","python-3.x, pandas, scikit-learn, nlp, text-classification","<p>The default behavior of <code>train_test_split</code> is to split data into random train and test subsets. You can enforce a static subset split by setting <code>shuffle=False</code> and removing <code>random_state</code>.</p>
<pre><code>Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, test_size=0.1, shuffle=False)
</code></pre>
<p>See <a href=""https://stackoverflow.com/questions/43838052/how-to-get-a-non-shuffled-train-test-split-in-sklearn"">How to get a non-shuffled train_test_split in sklearn</a></p>
",0,0,677,2021-08-22 18:23:31,https://stackoverflow.com/questions/68884049/sklearn-train-test-split-split-a-dataset-to-compare-predicted-labels-with-ground
correlating between prediction result to label,"<p>I've a keras model that predicted the following results:
(it's a multi-class problem with 6 possible classes)</p>
<p><code>[[0.44599777 0.00667355 0.10674711 0.02558559 0.29180232 0.12319366]]</code></p>
<p>so given the above results the model predicts the 1st class - but I know it's wrong.</p>
<p>I was able to achieve a ~92% accuracy:</p>
<pre><code>Epoch 1/10
1128/1128 [==============================] - 18s 15ms/step - loss: 1.3685 - accuracy: 0.4596 - val_loss: 0.6238 - val_accuracy: 0.7785
Epoch 2/10
1128/1128 [==============================] - 17s 15ms/step - loss: 0.7200 - accuracy: 0.7373 - val_loss: 0.4055 - val_accuracy: 0.8467
Epoch 3/10
1128/1128 [==============================] - 17s 15ms/step - loss: 0.4994 - accuracy: 0.8200 - val_loss: 0.3284 - val_accuracy: 0.8772
Epoch 4/10
1128/1128 [==============================] - 17s 15ms/step - loss: 0.3966 - accuracy: 0.8568 - val_loss: 0.3100 - val_accuracy: 0.9043
Epoch 5/10
1128/1128 [==============================] - 18s 16ms/step - loss: 0.3428 - accuracy: 0.8810 - val_loss: 0.3044 - val_accuracy: 0.9102
Epoch 6/10
1128/1128 [==============================] - 39s 34ms/step - loss: 0.3075 - accuracy: 0.8915 - val_loss: 0.2970 - val_accuracy: 0.9184
Epoch 7/10
1128/1128 [==============================] - 25s 22ms/step - loss: 0.2779 - accuracy: 0.9035 - val_loss: 0.3051 - val_accuracy: 0.9226
Epoch 8/10
1128/1128 [==============================] - 19s 17ms/step - loss: 0.2663 - accuracy: 0.9069 - val_loss: 0.3207 - val_accuracy: 0.9261
Epoch 9/10
1128/1128 [==============================] - 19s 17ms/step - loss: 0.2514 - accuracy: 0.9138 - val_loss: 0.2855 - val_accuracy: 0.9311
Epoch 10/10
1128/1128 [==============================] - 20s 18ms/step - loss: 0.2331 - accuracy: 0.9196 - val_loss: 0.3352 - val_accuracy: 0.9263
Test loss: 0.33516398072242737
Test accuracy: 0.9262799024581909
</code></pre>
<p>Below is how I'm doing the prediction:</p>
<pre><code>bug_name = '51859'
issue = conn.issue(bug_name, expand='changelog')
candidate_bug = Bug(issue, connections_dict)
candidate_bug.extract_all_info()
data = candidate_bug.get_data_as_df()
data = data.drop('group_name', axis='columns')

free_text_tokenized, _ = prepare_free_text_inputs(data, data)

model_inputs = [free_text_tokenized]

res = model.predict(model_inputs)
print(f'expected: {get_group_by_bug_owner(candidate_bug.get_owner())}')
# Generate arg maxes for predictions
print(res)
classes = np.argmax(res, axis=1)
print(classes)
print(np.unique(y_train))
class_index = classes[0]
print(np.unique(y_train)[class_index])
</code></pre>
<p>and here's the output:</p>
<pre><code>expected: D
[[0.44599777 0.00667355 0.10674711 0.02558559 0.29180232 0.12319366]]
[0]
['A' 'B' 'C' 'D' 'E' 'F']
A
</code></pre>
<p>... so I'm afraid my problem is I don't know to &quot;assign&quot; those results to the labels.
I've tried multiple attempts (where I know what the prediction should be) and it always misses the expected result.</p>
<p>Also - I'm using <code>LabelEncoder</code> as follows:</p>
<pre><code>    
# prepare target
def prepare_targets(y_train, y_test):
    le = LabelEncoder()
    le.fit(y_train)
    y_train_enc = le.transform(y_train)
    y_test_enc = le.transform(y_test)
    return y_train_enc, y_test_enc

y_train_enc, y_test_enc = prepare_targets(y_train, y_test)
</code></pre>
<p>What am I missing? Am I using the wrong the wrong list (<code>y_train</code>)?</p>
","python, numpy, keras, scikit-learn, text-classification","<p>Answering my own question (for whoever's gonna be introduced by it).</p>
<p>2 Issues I've found:</p>
<ol>
<li><p>I've (very mistakenly) triggered the transformers on the predicted data (<code>fit_on_text</code>) and it's a big no no! - one's must use the same transformer that was already fitted via the trained data.</p>
</li>
<li><p>the labels are encoded in the <code>LabelEncoder</code> that was originally used before training the model, so I've created a dict to map each label as follows:</p>
</li>
</ol>
<pre><code># prepare target
print('preparing lables')
le = LabelEncoder()
le_name_mapping = {}

le.fit(y_train)
le_name_mapping.update(dict(zip(le.transform(le.classes_), le.classes_)))
print(le_name_mapping)
y_train_enc = le.transform(y_train)
y_test_enc = le.transform(y_test)
</code></pre>
<p>later on I've used it on the prediction results:</p>
<pre><code>res = model.predict(model_inputs)
selected_class_index = np.argmax(res, axis=1)[0]
print(selected_class_index)
print(f'actual: {le_name_mapping[selected_class_index]}')
</code></pre>
",0,0,300,2021-08-25 09:47:56,https://stackoverflow.com/questions/68920570/correlating-between-prediction-result-to-label
Why my LSTM for Multi-Label Text Classification underperforms?,"<p>I'm using Windows 10 machine.
Libraries: Keras with Tensorflow 2.0
Embeddings:Glove(100 dimensions)</p>
<p>I am trying to implement an LSTM architecture for multi-label text classification.</p>
<p>My problem is that no matter how much fine-tuning I do, the results are really bad.</p>
<p>I am not experienced in DL practical implementations that's why I ask for your advice.</p>
<p>Below I will state basic information about my dataset and my model so far.</p>
<p><strong>I can't embed images since I am a new member so they appear as links.</strong></p>
<p><a href=""https://i.sstatic.net/042Xw.png"" rel=""nofollow noreferrer"">Dataset form+Embedings form+train-test-split form</a></p>
<p><a href=""https://i.sstatic.net/kVnTV.png"" rel=""nofollow noreferrer"">Dataset's labels distribution</a></p>
<p><a href=""https://i.sstatic.net/114rf.png"" rel=""nofollow noreferrer"">My Implementation of LSTM</a></p>
<p><a href=""https://i.sstatic.net/dgbwA.png"" rel=""nofollow noreferrer"">Model's Summary</a></p>
<p><a href=""https://i.sstatic.net/eaS2e.png"" rel=""nofollow noreferrer"">Model's Accuracy plot</a></p>
<p><a href=""https://i.sstatic.net/lf0Ql.png"" rel=""nofollow noreferrer"">Model's Loss plot</a></p>
<p>As you can see my dataset is really small (~6.000 examples) and maybe that's one reason why I cannot achieve better results. Still, I chose it because it's unbiased.</p>
<ol>
<li><p>I'd like to know if there is any fundamental mistake in my code regarding the dimensions, shape, activation functions, and loss functions for multi-label text classification?</p>
</li>
<li><p>What would you recommend to achieve better results on my model? Also any general advice regarding optimizing, methods,# of nodes, layers, dropouts, etc is very welcome.</p>
</li>
</ol>
<p>Model's best val accuracy that I achieved so far is ~0.54 and even if I tried to raise it, it seems stuck there.</p>
","keras, deep-learning, nlp, lstm, text-classification","<p>There are many ways to get this wrong but the most common mistake is to get your model overfit the training data.
I suspect that 0.54 accuracy means that your model selects the most common label (offensive) for almost all cases.</p>
<p>So, consider one of these simple solutions:</p>
<ul>
<li>Create balanced training data: like 400 samples from each class.</li>
<li>or sample balanced batches for training (exactly the same number of labels on each training batch)</li>
</ul>
<p>In addition to tracking accuracy and loss, look at precision-recall-f1 or even better try plotting area under curve, maybe different classes need different thresholds of activation. (If you are using Sigmoid on last layer maybe one class could perform better with 0.2 activations and another class with 0.7)</p>
",0,0,1099,2021-09-10 18:33:43,https://stackoverflow.com/questions/69136641/why-my-lstm-for-multi-label-text-classification-underperforms
calibrated classifier ValueError: could not convert string to float,"<p>Dataframe:</p>
<pre><code>id    review                                              name         label
1     it is a great product for turning lights on.        Ashley       
2     plays music and have a good sound.                  Alex        
3     I love it, lots of fun.                             Peter        
</code></pre>
<p>I want to use <strong>probabilistic classifier (linear_svc)</strong> to predict labels (probability of 1) based on review. My code:</p>
<pre><code>from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn import datasets

#Load  dataset
X = training['review']
y = training['label']

linear_svc = LinearSVC()     #The base estimator

# This is the calibrated classifier which can give probabilistic classifier
calibrated_svc = CalibratedClassifierCV(linear_svc,
                                        method='sigmoid',  #sigmoid will use Platt's scaling. Refer to documentation for other methods.
                                        cv=3) 
calibrated_svc.fit(X, y)


# predict
prediction_data = predict_data['review']
predicted_probs = calibrated_svc.predict_proba(prediction_data)
</code></pre>
<p>It gives following error on calibrated_svc.fit(X, y):</p>
<blockquote>
<p>ValueError: could not convert string to float: 'it is a great product
for turning...'</p>
</blockquote>
<p>I would appreciate your help.</p>
","scikit-learn, text-classification, valueerror","<p>SVM models cannot handle text data directly. You need to extract some numeric features from the text first. I recommend reading some content on NLP such as Bag of Words and TF-IDF. In any case, for the example you're suggesting, a functional minimal pipeline would be:</p>
<pre><code>from sklearn.calibration import CalibratedClassifierCV
from sklearn import datasets
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

#Load  dataset
X = training['review']
y = training['label']

linear_svc = make_pipeline(TfIdfVectorizer(), LinearSVC())

# This is the calibrated classifier which can give probabilistic classifier
calibrated_svc = CalibratedClassifierCV(linear_svc,
                                        method='sigmoid',
                                        cv=3) 
calibrated_svc.fit(X, y)


# predict
prediction_data = predict_data['review']
predicted_probs = calibrated_svc.predict_proba(prediction_data)
</code></pre>
<p>You probably also want to clean the text a bit by removing special characters, lowercasing, stemming, etc. Take a look at <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spacy</a> the library for text-processing.</p>
",1,2,333,2021-09-23 04:01:16,https://stackoverflow.com/questions/69293878/calibrated-classifier-valueerror-could-not-convert-string-to-float
My training and validation loss suddenly increased in power of 3,"<p><a href=""https://i.sstatic.net/CCIO8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CCIO8.png"" alt=""Training"" /></a></p>
<p><strong>train function</strong></p>
<pre><code>def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0
    for i, batch in enumerate(iterator):
        optimizer.zero_grad()
        output = model(batch.text)
        loss = criterion(output, torch.unsqueeze(batch.labels, 1))
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(iterator)
</code></pre>
<p><strong>main_script</strong></p>
<pre><code>def main(
        train_file,
        test_file,
        config_file,
        checkpoint_path,
        best_model_path
    ):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    with open(config_file, 'r') as j:
        config = json.loads(j.read())

    for k,v in config['model'].items():
        v = float(v)
        if v &lt; 1.0:
            config['model'][k] = float(v)
        else:
            config['model'][k] = int(v)

    for k,v in config['training'].items():
        v = float(v)
        if v &lt; 1.0:
            config['training'][k] = float(v)
        else:
            config['training'][k] = int(v)

    train_itr, val_itr, test_itr, vocab_size = data_pipeline(
        train_file,
        test_file,
        config['training']['max_vocab'],
        config['training']['min_freq'],
        config['training']['batch_size'],
        device
    )

    model = CNNNLPModel(
        vocab_size,
        config['model']['emb_dim'],
        config['model']['hid_dim'],
        config['model']['model_layer'],
        config['model']['model_kernel_size'],
        config['model']['model_dropout'],
        device
    )
    optimizer = optim.Adam(model.parameters())
    criterion = nn.CrossEntropyLoss()
    num_epochs = config['training']['n_epoch']
    clip = config['training']['clip']
    is_best = False
    best_valid_loss = float('inf')
    model = model.to(device)
    for epoch in tqdm(range(num_epochs)):

        train_loss = train(model, train_itr, optimizer, criterion, clip)
        valid_loss = evaluate(model, val_itr, criterion)

        if (epoch + 1) % 2 == 0:
            print(&quot;training loss {}, validation_loss{}&quot;.format(train_loss,valid_loss))
</code></pre>
<p>I was training a Convolution Neural Network for binary Text classification. Given a sentence, it detects its a hate speech or not. Training loss and validation loss was fine till 5 epoch after that suddenly the training loss and validation loss shot up suddenly from <strong>0.2 to 10,000</strong>.</p>
<p>What could be the reason for such huge increase is loss suddenly?</p>
","deep-learning, nlp, pytorch, conv-neural-network, text-classification","<p>Default learning rate of Adam is 0.001, which, depending on task, might be too high.</p>
<p>It looks like instead of converging your neural network became divergent (it left the previous ~0.2 loss minima and fell into different region).</p>
<p>Lowering your learning rate at some point (after 50% or 70% percent of training) would probably fix the issue.</p>
<p>Usually people divide the learning rate by 10 (0.0001 in your case) or by half (0.0005 in your case). Try with dividing by half and see if the issue persist, in general you would want to keep your learning rate as high as possible until divergence occurs as is probably the case here.</p>
<p>This is what <a href=""https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR"" rel=""nofollow noreferrer"">schedulers</a> are for (gamma specifies learning rate multiplier, might want to change that to 0.5 first).</p>
<p>One can think of lower learning rate phase as fine-tuning already found solution (placing weights in better region of the loss valley) and might require some patience.</p>
",1,2,3100,2021-09-28 11:48:04,https://stackoverflow.com/questions/69361178/my-training-and-validation-loss-suddenly-increased-in-power-of-3
How to use SciBERT in the best manner?,"<p>I'm trying to use BERT models to do text classification. As the text is about scientific texts, I intend to use the <strong>SicBERT</strong> pre-trained model: <a href=""https://github.com/allenai/scibert"" rel=""nofollow noreferrer"">https://github.com/allenai/scibert</a></p>
<p>I have faced several limitations which I want to know if there is any solutions for them:</p>
<ol>
<li><p>When I want to do tokenization and batching, it only allows me to use <code>max_length</code> of &lt;=512. Is there any way to use more tokens. Doen't this limitation of 512 mean that I am actually not using all the text information during training? Any solution to use all the text?</p>
</li>
<li><p>I have tried to use this pretrained library with other models such as DeBERTa or RoBERTa. But it doesn't let me. I has only worked with BERT. Is there anyway I can do that?</p>
</li>
<li><p>I know this is a general question, but any suggestion that I can improve my fine tuning (from data to hyper parameter, etc)? Currently, I'm getting ~75% accuracy. Thanks</p>
</li>
</ol>
<p><strong>Codes:</strong></p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')

encoded_data_train = tokenizer.batch_encode_plus(
    df_train.text.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    padding=True,
    max_length=256
)

input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df_train.label.values)

dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)

dataloader_train = DataLoader(dataset_train, 
                              sampler=RandomSampler(dataset_train), 
                              batch_size=batch_size)

model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased',
                                                      num_labels=len(labels),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

epochs = 1

optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)

scheduler = get_linear_schedule_with_warmup(optimizer,
num_training_steps=len(dataloader_train)*epochs)
</code></pre>
","nlp, pytorch, text-classification, huggingface-transformers, bert-language-model","<blockquote>
<p>When I want to do tokenization and batching, it only allows me to use max_length of &lt;=512. Is there any way to use more tokens. Doen't this limitation of 512 mean that I am actually not using all the text information during training? Any solution to use all the text?</p>
</blockquote>
<p>Yes, you are not using the complete text. And this is one of the limitations of BERT and T5 models, which limit to using 512 and 1024 tokens resp. to the best of my knowledge.</p>
<p>I can suggest you to use <code>Longformer</code> or <code>Bigbird</code> or <code>Reformer</code> models, which can handle sequence lengths up to <code>16k</code>, <code>4096</code>, <code>64k</code> tokens respectively. These are really good for processing longer texts like scientific documents.</p>
<blockquote>
<p>I have tried to use this pretrained library with other models such as DeBERTa or RoBERTa. But it doesn't let me. I has only worked with BERT. Is there anyway I can do that?</p>
</blockquote>
<p><code>SciBERT</code> is actually a pre-trained BERT model.
See this <a href=""https://github.com/huggingface/transformers/issues/2902"" rel=""nofollow noreferrer"">issue</a> for more details where they mention the feasibility of converting BERT to ROBERTa:</p>
<p><code>Since you're working with a BERT model that was pre-trained, you unfortunately won't be able to change the tokenizer now from a WordPiece (BERT) to a Byte-level BPE (RoBERTa).</code></p>
<blockquote>
<p>I know this is a general question, but any suggestion that I can
improve my fine tuning (from data to hyper parameter, etc)? Currently,
I'm getting ~79% accuracy.</p>
</blockquote>
<p>I would first try to tune the most important hyperparameter <code>learning_rate</code>. I would then explore different values for hyperparameters of <code>AdamW</code> optimizer and <code>num_warmup_steps</code> hyperparamter of the scheduler.</p>
",0,0,2468,2021-10-01 13:44:52,https://stackoverflow.com/questions/69406937/how-to-use-scibert-in-the-best-manner
How to configure and train the model using Glove and CNN for text classification?,"<p>I have worked with text classification using Glove and CNN and found the problem below:</p>
<pre><code>File &quot;c:\programfiles_anaconda\anaconda3\envs\math_stat_class\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 1657, in _create_c_op
    raise ValueError(str(e))

ValueError: Negative dimension size caused by subtracting 5 from 1 for '{{node max_pooling1d_9/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=&quot;NHWC&quot;, ksize=[1, 5, 1, 1], padding=&quot;VALID&quot;, strides=[1, 5, 1, 1]](max_pooling1d_9/ExpandDims)' with input shapes: [?,1,1,128].
</code></pre>
<h3>Glove input</h3>
<pre class=""lang-py prettyprint-override""><code>EMBEDDING_DIM = 100
    
embeddings_index = {}
    
f = open(glove_path, encoding='utf-8')  
for line in f:    
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
    f.close()
    
print('Found %s word vectors.' % len(embeddings_index))
    
embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
    
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
</code></pre>
<h3>Layer input of CNN</h3>
<pre class=""lang-py prettyprint-override""><code># apply embedding matrix into an Embedding layer
# trainable=False to prevent the weights from being updated during training
embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)
</code></pre>
<h3>Training 1D CNN</h3>
<pre class=""lang-py prettyprint-override""><code>sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

x = Conv1D(128, 5, activation='relu')(embedded_sequences)   
print(&quot;x shape = &quot;, x)

x = MaxPooling1D(5)(x)  
print(&quot;x shape = &quot;, x)
        
x = Conv1D(128, 5, activation='relu')(x)
print(&quot;x shape = &quot;, x)
    
#-----This line below produced error-----
x = MaxPooling1D(5)(x) #Error this line
#-----This line above produced error-----
        
print(&quot;x shape = &quot;, x)

x = Conv1D(128, 5, activation='relu')(x)
print(&quot;x shape = &quot;, x)
    
x = MaxPooling1D(35)(x)  # global max pooling
print(&quot;x shape = &quot;, x)
    
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
    
preds = Dense(len(labels_index), activation='softmax')(x)
model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])
    
# Learning
model.fit(X_train, y_train, validation_data=(X_val, y_val),
          epochs=2, batch_size=128)
</code></pre>
<h3>My ideas</h3>
<p><strong>1) Are there some issues/problems with Glove input?</strong></p>
<p><strong>2) Conv1D:</strong></p>
<ul>
<li>Change the &quot;kernel_size&quot; from 5 to a New Value.</li>
</ul>
<p><strong>3) MaxPooling1D:</strong></p>
<ul>
<li>Change pool_size from 5 to a New Value.</li>
<li>Specify other parameters: strides, padding and so on.</li>
</ul>
<p><strong>4) I currently use keras on tensorflow 2.20 and python 3.6</strong></p>
<ul>
<li>Do I need to upgrade tensorflow and python?</li>
</ul>
<p>However, I could not figure out any better way to do. May I have your suggestions?</p>
","python, tensorflow, conv-neural-network, stanford-nlp, text-classification","<p>Two things that come to my mind: your max-pooling layers are reducing the size of the input to the next convolutional layers every time and eventually the size is too small to run another max-pooling operation. Try running</p>
<pre><code> tf.print(model.summary) 
</code></pre>
<p>after each max-pooling operation and you will quickly find out that your tensor cannot be further reduced. You can then consider using a different <code>pool_size</code> in your max-pooling layers.</p>
<p>The second thing I notice (I am not sure if it is intentional), but <em>MaxPooling1D != Global Max Pooling</em>. Keras supports both <a href=""https://keras.io/api/layers/pooling_layers/"" rel=""nofollow noreferrer"">operations</a>. Take a look at the documentation.</p>
<p>On a side note, sentence classification with CNNs was widely popularized by the <a href=""https://arxiv.org/pdf/1408.5882.pdf"" rel=""nofollow noreferrer"">work</a> of Yoon Kim. In his work, he shows that global max-pooling operations perform much better than striding max-pooling operations in sentence classification (when using word embeddings, as you are doing).</p>
",0,0,256,2021-10-12 01:01:05,https://stackoverflow.com/questions/69533961/how-to-configure-and-train-the-model-using-glove-and-cnn-for-text-classification
Can&#39;t get dimensions right - CNN for text classification,"<p>This is my CNN class:</p>
<pre><code>class CNN(nn.Module):
    def __init__(
        self,
        vocab_size,
        emb_dim,
        out_channels,
        kernel_sizes,
        dropout,
    ):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[0], emb_dim), 2)
        
        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[1], emb_dim), 2)
        
        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[2], emb_dim), 2)
        
        self.fc = nn.Linear(len(kernel_sizes) * out_channels, 1)
        
        self.dropout = nn.Dropout(dropout)

        
        
    def forward(self, text):
        
        embedded = self.embedding(text)
        print('embedded', embedded.shape)
        embedded = embedded.unsqueeze(1)  # may be reshape here
        print('embedded', embedded.shape)

        conved_0 = F.relu(self.conv_0(embedded)).squeeze(3)  # may be reshape here
        print('conved_0', conved_0.shape)
        conved_1 = F.relu(self.conv_1(embedded)).squeeze(3)  # may be reshape here
        print('conved_1', conved_1.shape)
        conved_2 = F.relu(self.conv_2(embedded)).squeeze(3)  # may be reshape here
        print('conved_2', conved_2.shape)
        
        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)
        print('pooled_0', pooled_0.shape)
        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)
        print('pooled_1', pooled_1.shape)
        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)
        print('pooled_2', pooled_2.shape)
        
        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))
        print('cat', cat.shape)
            
        return self.fc(cat)
</code></pre>
<p>Variables:</p>
<pre><code>kernel_sizes = [3, 4, 5]
vocab_size = len(TEXT.vocab)
out_channels = 64
dropout = 0.2
dim = 300

model = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=out_channels,
            kernel_sizes=kernel_sizes, dropout=dropout)
</code></pre>
<p>And training:</p>
<pre><code>import numpy as np

min_loss = np.inf

cur_patience = 0

for epoch in range(1, max_epochs + 1):
    train_loss = 0.0
    model.train()
    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)
    pbar.set_description(f&quot;Epoch {epoch}&quot;)
    for it, batch in pbar: 
        #YOUR CODE GOES HERE
        opt.zero_grad()
        input = batch.text[0].to(device)
        output = model(input)
        train_loss = loss_func(output, batch.label)
        train_loss.backward()
        opt.step()
        

    train_loss /= len(train_iter)
    val_loss = 0.0
    model.eval()
    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)
    pbar.set_description(f&quot;Epoch {epoch}&quot;)
    for it, batch in pbar:
        # YOUR CODE GOES HERE
        input = batch.text[0].to(device)
        output = model(input)
        val_loss = loss_fn(output, batch.label)

    val_loss /= len(valid_iter)
    if val_loss &lt; min_loss:
        min_loss = val_loss
        best_model = model.state_dict()
    else:
        cur_patience += 1
        if cur_patience == patience:
            cur_patience = 0
            break
    
    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))
model.load_state_dict(best_model)
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 1, 3, 300], but got 3-dimensional input of size [894, 1, 300] instead</p>
</blockquote>
<p>in this line:</p>
<blockquote>
<p>---&gt; 32         conved_0 = F.relu(self.conv_0(embedded)).squeeze(3)</p>
</blockquote>
<p>I've tried using Conv1d, but still had problems with dimensions. Could somebody please explain what should I fix here for the network to train?</p>
<hr />
<p>EDIT:</p>
<pre><code>This is my class but with Conv1d:

class CNN(nn.Module):

    def __init__(
        self,
        vocab_size,
        emb_dim,
        out_channels,
        kernel_sizes,
        dropout,
    ):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.conv_0 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[0])
        
        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[1])
        
        self.conv_2 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[2])
        
        self.fc = nn.Linear(len(kernel_sizes) * out_channels, 1)
        
        self.dropout = nn.Dropout(dropout)

        
        
    def forward(self, text):
        
        embedded = self.embedding(text)
        print('embedded', embedded.shape)
        embedded = embedded.unsqueeze(1)  # may be reshape here
        print('embedded', embedded.shape)

        conved_0 = F.relu(self.conv_0(embedded))  # may be reshape here
        print('conved_0', conved_0.shape)
        conved_1 = F.relu(self.conv_1(embedded))  # may be reshape here
        print('conved_1', conved_1.shape)
        conved_2 = F.relu(self.conv_2(embedded))  # may be reshape here
        print('conved_2', conved_2.shape)
        
        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)
        print('pooled_0', pooled_0.shape)
        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)
        print('pooled_1', pooled_1.shape)
        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)
        print('pooled_2', pooled_2.shape)
        
        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))
        print('cat', cat.shape)
            
        return self.fc(cat)
</code></pre>
<p>Dimensions output:</p>
<pre><code>embedded torch.Size([1115, 300])
embedded torch.Size([1115, 1, 300])
conved_0 torch.Size([1115, 64, 298])
conved_1 torch.Size([1115, 64, 297])
conved_2 torch.Size([1115, 64, 296])
pooled_0 torch.Size([1115, 64])
pooled_1 torch.Size([1115, 64])
pooled_2 torch.Size([1115, 64])
cat torch.Size([1115, 192])
</code></pre>
<p>Error:</p>
<blockquote>
<p>ValueError: Target size (torch.Size([128])) must be the same as input size (torch.Size([1115, 1]))</p>
</blockquote>
","python, conv-neural-network, text-classification, dimensions, word-embedding","<p>What I was missing is that I had <code>batch_first</code> parameter set to <code>True</code>, which <strong>SWAPED</strong> <em>batch_size</em> and <em>seq_len</em>. Once I've set it to <code>False</code>, everything worked perfectly.</p>
",0,0,185,2021-10-29 08:06:41,https://stackoverflow.com/questions/69765540/cant-get-dimensions-right-cnn-for-text-classification
Is there any ideal parameter values for fasttext train_supervised function?,"<p>I working on NLP problem and try to make text classification with word embedding method. I am training my model with fasttext's train_supervised but is there any ideal or best parameter values for this function that you can advise me also I am using Kfold with some values how can I find best K-fold number in this problem ?</p>
<p>My solution is I am using fasttext's autotune function to find best param values for model to train but is there any possible suggestion to give me ? Following image shows my best params in the model. Finally , I am using fasttext's pretrained word vector model for my training.
<a href=""https://i.sstatic.net/cD4tM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cD4tM.png"" alt=""enter image description here"" /></a></p>
","nlp, text-classification, word-embedding, supervised-learning, fasttext","<p>Let me answer my own question you can look at the default and optimum parameters values by clicking  the following link ;</p>
<p><a href=""https://fasttext.cc/docs/en/options.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/options.html</a></p>
<p>and also you can use fasttext's libraries autotune function (Automatic hyperparameter optimization) to find best parameters for your special train and validation dataset  by clicking the following link ;</p>
<p><a href=""https://fasttext.cc/docs/en/autotune.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/autotune.html</a></p>
<p>and finally this is the pretrained word vectors provided by fasttext library to utilize in your model's training process also making positive progress for model , in the following link's site they are in the Model section ;</p>
<p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>
",0,1,1288,2021-11-02 08:43:41,https://stackoverflow.com/questions/69807486/is-there-any-ideal-parameter-values-for-fasttext-train-supervised-function
Text preprocessing for fasttext pretrained models,"<p>I want to use pretreained fastext model for language detection: <a href=""https://fasttext.cc/docs/en/language-identification.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/language-identification.html</a> . Where can I find <strong>the exact Python code</strong> for text preprocessing used for training this specific model? I am not interested in general answers about how should we prepare text for using models - I ma looking for identical transformations as those used for training.</p>
","nlp, text-processing, text-classification, fasttext","<p>When the Facebook engineers have been asked similar questions in their Github repository issues, they've usually pointed to <a href=""https://github.com/facebookresearch/fastText/issues/807#issuecomment-495552561"" rel=""nofollow noreferrer"">one</a> or <a href=""https://github.com/facebookresearch/fastText/issues/401#issuecomment-368935887"" rel=""nofollow noreferrer"">the</a> other of two shell scripts in their public code (&amp; especially the 'normalize_text' functions within).</p>
<p><a href=""https://github.com/facebookresearch/fastText/blob/master/tests/fetch_test_data.sh#L20"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/blob/master/tests/fetch_test_data.sh#L20</a></p>
<pre><code>normalize_text() {
  tr '[:upper:]' '[:lower:]' | sed -e 's/^/__label__/g' | \
    sed -e &quot;s/'/ ' /g&quot; -e 's/&quot;//g' -e 's/\./ \. /g' -e 's/&lt;br \/&gt;/ /g' \
        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\!/ \! /g' \
        -e 's/\?/ \? /g' -e 's/\;/ /g' -e 's/\:/ /g' | tr -s &quot; &quot; | myshuf
}
</code></pre>
<p><a href=""https://github.com/facebookresearch/fastText/blob/master/get-wikimedia.sh#L12"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/blob/master/get-wikimedia.sh#L12</a></p>
<pre><code>normalize_text() {
    sed -e &quot;s/’/'/g&quot; -e &quot;s/′/'/g&quot; -e &quot;s/''/ /g&quot; -e &quot;s/'/ ' /g&quot; -e &quot;s/“/\&quot;/g&quot; -e &quot;s/”/\&quot;/g&quot; \
        -e 's/&quot;/ &quot; /g' -e 's/\./ \. /g' -e 's/&lt;br \/&gt;/ /g' -e 's/, / , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\!/ \! /g' \
        -e 's/\?/ \? /g' -e 's/\;/ /g' -e 's/\:/ /g' -e 's/-/ - /g' -e 's/=/ /g' -e 's/=/ /g' -e 's/*/ /g' -e 's/|/ /g' \
        -e 's/«/ /g' | tr 0-9 &quot; &quot;
}
</code></pre>
<p>They've also referenced <a href=""https://fasttext.cc/docs/en/crawl-vectors.html#tokenization"" rel=""nofollow noreferrer"">this page's section on 'Tokenization'</a> (which names some libraries), and the <a href=""https://arxiv.org/abs/1802.06893"" rel=""nofollow noreferrer"">academic paper which describes the earlier work making individual language vectors</a>.</p>
<p>None of these are guaranteed to exactly match what was used to create their pretrained classification models, &amp; it's a bit frustrating that each release of such models doesn't contain the <em>exact</em> code to reproduce. But, these sources seem to be as much detail as is available, without getting direct answers/help from the team that created them.</p>
",0,0,712,2021-11-05 10:56:50,https://stackoverflow.com/questions/69852169/text-preprocessing-for-fasttext-pretrained-models
What are differences between AutoModelForSequenceClassification vs AutoModel,"<p>We can create a model from AutoModel(TFAutoModel) function:</p>
<pre><code>from transformers import AutoModel 
model = AutoModel.from_pretrained('distilbert-base-uncase')
</code></pre>
<p>In other hand, a model is created by AutoModelForSequenceClassification(TFAutoModelForSequenceClassification):</p>
<pre><code>from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification('distilbert-base-uncase')
</code></pre>
<p>As I know, both models use distilbert-base-uncase library to create models.
From name of methods, the second class( <strong>AutoModelForSequenceClassification</strong> ) is created for Sequence Classification.</p>
<p>But what are really differences in 2 classes? And how to use them correctly?</p>
<p>(I searched in huggingface but it is not clear)</p>
","nlp, text-classification, huggingface-transformers","<p>The difference between <code>AutoModel</code> and <code>AutoModelForSequenceClassification</code> model is that <code>AutoModelForSequenceClassification</code>  has a classification head on top of the model outputs which can be easily trained with the base model</p>
",21,23,22798,2021-11-10 03:33:55,https://stackoverflow.com/questions/69907682/what-are-differences-between-automodelforsequenceclassification-vs-automodel
My function to investigate the impact of sample size on the text classifier performance is not working correctly,"<p>I have defined the following function that returns the AUC and PRC scores for training and test datasets which you can find through the links below:
<strong>train dataset</strong> <a href=""https://drive.google.com/file/d/1466SDm1nOpeDb_3UnW8Qjc1VEY_Be0R5/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1466SDm1nOpeDb_3UnW8Qjc1VEY_Be0R5/view?usp=sharing</a>
<strong>test dataset</strong> <a href=""https://drive.google.com/file/d/1vphjb3xbrklhLHNMYUexN6X_axepm0Xy/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1vphjb3xbrklhLHNMYUexN6X_axepm0Xy/view?usp=sharing</a></p>
<p>Both of the datasets have samples in the following format. The text column contains documents, and the label column gives the sentiment of each document.</p>
<p>label   text
1   I must admit that I'm addicted to &quot;Version 2.0...
0   I think it's such a shame that an enormous tal...
1   The Sunsout No Room at The Inn Puzzle has oddl...
... ...</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_curve, auc,precision_recall_curve
from sklearn.metrics import classification_report
from matplotlib import pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

train = pd.read_csv(&quot;train5-1.csv&quot;)
test = pd.read_csv(&quot;test5.csv&quot;)

def create_model(train_docs, train_y, test_docs, test_y, \
              model_type='svm', stop_words=None, min_df=1, print_result = True, algorithm_para=1.0):
  
  tfidf_vect = TfidfVectorizer(stop_words=stop_words,min_df=min_df)
  tfidf_vect.fit_transform(train[&quot;text&quot;])
  y_test=test['label'].values
  y_train=train[&quot;label&quot;].values
  X_train=tfidf_vect.fit_transform(train['text'].values)
  X_test=tfidf_vect.transform(test['text'].values)

  if 'svm' in model_type:
      clf = svm.SVC(kernel='linear',probability=True)
      clf=svm.LinearSVC(C=algorithm_para).fit(X_train, y_train)
      predicted=clf.predict(X_test)
      
      labels=sorted(train['label'].unique())
      precision, recall, fscore, support=\
          precision_recall_fscore_support(\
          y_test, predicted, labels=labels)
      
      
      if print_result==True:
 
        print(&quot;labels: &quot;, labels)
        print(&quot;precision: &quot;, precision)
        print(&quot;recall: &quot;, recall)
        print(&quot;f-score: &quot;, fscore)
        print(&quot;support: &quot;, support)
        
      predict_p=clf._predict_proba_lr(X_test)
      labels
      predict_p[0:3]
      
      y_test[0:3]
      y_pred = predict_p[:,1]
      
      fpr, tpr, thresholds = roc_curve(y_test,y_pred, pos_label=1)
      precision, recall, thresholds = precision_recall_curve(y_test, y_pred, pos_label=1)
      
      auc_score= auc(fpr, tpr)
      prc_score=auc(recall, precision)


      if print_result==True:
        print(&quot;AUC: {:.2%}&quot;.format(auc_score), &quot;PRC: {:.2%}&quot;.format(prc_score))
        plt.figure();
        plt.plot(fpr, tpr, color='darkorange', lw=2);
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--');
        plt.xlim([0.0, 1.0]);
        plt.ylim([0.0, 1.05]);
        plt.xlabel('False Positive Rate');
        plt.ylabel('True Positive Rate');
        plt.title('AUC of SVM Model');
        plt.show()
        
        plt.figure();
        plt.plot(recall, precision, color='darkorange', lw=2);
        plt.xlim([0.0, 1.0]);
        plt.ylim([0.0, 1.05]);
        plt.xlabel('Recall');
        plt.ylabel('Precision');
        plt.title('Precision_Recall_Curve of SVM Model');
        plt.show();

  else:
    clf=MultinomialNB(alpha=algorithm_para).fit(X_train, y_train)
    predicted=clf.predict(X_test)
    
    labels=sorted(train['label'].unique())
    precision, recall, fscore, support=\
          precision_recall_fscore_support(\
          y_test, predicted, labels=labels)  
      
    if print_result==True:

          
        print(&quot;labels: &quot;, labels)
        print(&quot;precision: &quot;, precision)
        print(&quot;recall: &quot;, recall)
        print(&quot;f-score: &quot;, fscore)
        print(&quot;support: &quot;, support)
        
    predict_p=clf.predict_proba(X_test)
    labels
    predict_p[0:3]
      
    y_test[0:3]
    y_pred = predict_p[:,1]
      
    fpr, tpr, thresholds = roc_curve(y_test,y_pred, pos_label=1)
    precision, recall, thresholds = precision_recall_curve(y_test, y_pred, pos_label=1)
      
    auc_score= auc(fpr, tpr)
    prc_score=auc(recall, precision)


    if print_result==True:
        print(&quot;AUC: {:.2%}&quot;.format(auc_score), &quot;PRC: {:.2%}&quot;.format(prc_score))
        plt.figure();
        plt.plot(fpr, tpr, color='darkorange', lw=2);
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--');
        plt.xlim([0.0, 1.0]);
        plt.ylim([0.0, 1.05]);
        plt.xlabel('False Positive Rate');
        plt.ylabel('True Positive Rate');
        plt.title('AUC of SVM Model');
        plt.show()
        
        plt.figure();
        plt.plot(recall, precision, color='darkorange', lw=2);
        plt.xlim([0.0, 1.0]);
        plt.ylim([0.0, 1.05]);
        plt.xlabel('Recall');
        plt.ylabel('Precision');
        plt.title('Precision_Recall_Curve of SVM Model');
        plt.show();

  return auc_score, prc_score
</code></pre>
<p>Then, to investigate the impact of sample size on the above classifier performance, I defined another function as follows:</p>
<pre><code>def sample_size_impact(train_docs, train_y, test_docs, test_y):

  auc_list_svm=[]
  
  t_size = np.linspace(500,12000, 24)

  for i in range (int(len(train_docs)/500)):
    auc_score_svm= create_model(train_docs[:(i+1)*500], train_y[:(i+1)*500], test_docs, test_y, \
          model_type='svm', stop_words = 'english', min_df = 1, print_result=False, algorithm_para=1.0)
    auc_list_svm.append(auc_score_svm)

  plt.figure();
  plt.plot(auc_list_svm, color='darkorange');
  plt.xlabel('Smple Size');
  plt.ylabel('AUC');
  plt.title('sample size impact comparison');
  plt.show()
</code></pre>
<p>But the sample_size_impact function is not working correctly.
Would you please investigate my code and tell me where I made a mistake?</p>
","python, scikit-learn, svm, text-classification, naivebayes","<p>You have an error in <code>create_model</code>, you're using the global (full) training data <code>train</code> every time instead of the argument <code>train_docs</code>. It should be:</p>
<pre><code>  tfidf_vect.fit_transform(train_docs[&quot;text&quot;])
  y_test=test_docs['label'].values
  y_train=train_docs[&quot;label&quot;].values
  X_train=tfidf_vect.fit_transform(train_docs['text'].values)
  X_test=tfidf_vect.transform(test_docs['text'].values)
</code></pre>
",0,0,111,2021-12-07 06:20:20,https://stackoverflow.com/questions/70255689/my-function-to-investigate-the-impact-of-sample-size-on-the-text-classifier-perf
Text classification from html with BeautifulSoup,"<p>I have obtained html page source code and then parsed it using 'html5lib' with BeautifulSoup.</p>
<p>I have got something like this:</p>
<pre><code>&lt;div class=&quot;V0h1Ob-haAclf OPZbO-KE6vqe o0s21d-HiaYvf&quot; jsaction=&quot;mouseover:pane.wfvdle40;mouseout:pane.wfvdle40&quot; jsan=&quot;7.V0h1Ob-haAclf,7.OPZbO-KE6vqe,7.o0s21d-HiaYvf,0.jsaction&quot; jstcache=&quot;824&quot;&gt;
    &lt;a aria-label=&quot;Muzeum Londynu&quot; class=&quot;a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd&quot; href=&quot;https://www.google.com/maps/place/Muzeum+Londynu/data=!4m5!3m4!1s0x48761b5508c1cbeb:0x407de2c1952a25e4!8m2!3d51.5176183!4d-0.0967782?authuser=0&amp;amp;hl=pl&amp;amp;rclk=1&quot; jsaction=&quot;pane.wfvdle40;focus:pane.wfvdle40;blur:pane.wfvdle40;auxclick:pane.wfvdle40;contextmenu:pane.wfvdle40;keydown:pane.wfvdle40;clickmod:pane.wfvdle40&quot; jsan=&quot;7.a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd,0.aria-label,8.href,0.jsaction&quot; jstcache=&quot;825&quot;&gt;&lt;/a&gt;
    &lt;div class=&quot;CJY91c-jRmmHf-aVTXAb-haAclf-WFkMr&quot; jstcache=&quot;826&quot;&gt;&lt;/div&gt;
    &lt;div aria-label=&quot;Muzeum Londynu&quot; class=&quot;MVVflb-haAclf V0h1Ob-haAclf-d6wfac MVVflb-haAclf-uxVfW-hSRGPd&quot; jsan=&quot;7.MVVflb-haAclf,7.V0h1Ob-haAclf-d6wfac,7.MVVflb-haAclf-uxVfW-hSRGPd,0.aria-label&quot; jstcache=&quot;827&quot;&gt;
        &lt;div class=&quot;CJY91c-jRmmHf-aVTXAb-haAclf-bIWrp&quot; jstcache=&quot;828&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;lI9IFe&quot;&gt;
            &lt;div class=&quot;CJY91c-jRmmHf-aVTXAb-haAclf-HSrbLb&quot; jstcache=&quot;829&quot;&gt;
                &lt;div class=&quot;RnEfrd-jRmmHf-HSrbLb B9Hcub-QFlW2&quot; jsan=&quot;t-pdDsP4P8DQQ,7.RnEfrd-jRmmHf-HSrbLb,7.B9Hcub-QFlW2&quot; jstcache=&quot;933&quot;&gt;
                    &lt;button jstcache=&quot;842&quot; style=&quot;display:none&quot;&gt;&lt;/button&gt;
                    &lt;div class=&quot;Z8fK3b&quot; jsan=&quot;7.Z8fK3b,t-MjeqqY5XOdM&quot; jstcache=&quot;843&quot;&gt; 
                        &lt;div class=&quot;CUwbzc-content gm2-body-2&quot;&gt; &lt;div class=&quot;qBF1Pd-haAclf&quot;&gt;
                            &lt;div class=&quot;qBF1Pd gm2-subtitle-alt-1&quot; jsan=&quot;7.qBF1Pd,7.gm2-subtitle-alt-1,t-u3p6PfXaXm4&quot; jstcache=&quot;845&quot;&gt;
                                &lt;span jstcache=&quot;858&quot;&gt;Muzeum Londynu&lt;/span&gt; 
                            &lt;/div&gt;
                            &lt;h1 jstcache=&quot;846&quot; style=&quot;display:none&quot;&gt;&lt;/h1&gt; 
                            &lt;span class=&quot;RnEfrd-jRmmHf-HSrbLb-title-Btuy5e-haAclf&quot;&gt;&lt;/span&gt; 
                        &lt;/div&gt; 
                        &lt;div class=&quot;section-subtitle-extension&quot; jstcache=&quot;847&quot;&gt;&lt;/div&gt; 
                        &lt;div class=&quot;ZY2y6b-RWgCYc&quot; jsan=&quot;7.ZY2y6b-RWgCYc,t-hEqDOx2FFV0&quot; jstcache=&quot;848&quot;&gt; 
                        &lt;div class=&quot;OEvfgc-wcwwM-haAclf&quot;&gt; 
                            &lt;span class=&quot;RnEfrd-jRmmHf-HSrbLb-wPzPJb-Btuy5e-haAclf&quot; jstcache=&quot;860&quot;&gt;&lt;/span&gt;
                            &lt;span class=&quot;gm2-body-2&quot; jsan=&quot;t-CJ3Gw1VPbAA,7.gm2-body-2&quot; jstcache=&quot;861&quot;&gt;
                            &lt;span jstcache=&quot;868&quot; style=&quot;display:none&quot;&gt;&lt;/span&gt;
                            &lt;span aria-label=&quot; 4,6-gwiazdkowy  Opinie (13 898)  &quot; class=&quot;ZkP5Je&quot; jsan=&quot;7.ZkP5Je,0.aria-label,0.role,t-kqtGnPs-9G0&quot; jstcache=&quot;869&quot; role=&quot;group&quot;&gt;
                            &lt;span aria-hidden=&quot;true&quot; class=&quot;MW4etd&quot; jsan=&quot;7.MW4etd,0.aria-hidden&quot; jstcache=&quot;872&quot;&gt;4,6&lt;/span&gt;
                            &lt;div jstcache=&quot;873&quot; style=&quot;display:none&quot;&gt;&lt;/div&gt;
                            &lt;div class=&quot;QBUL8c&quot; jsan=&quot;7.QBUL8c&quot; jsinstance=&quot;0&quot; jstcache=&quot;874&quot;&gt;&lt;/div&gt;
                            &lt;div class=&quot;QBUL8c&quot; jsan=&quot;7.QBUL8c&quot; jsinstance=&quot;1&quot; jstcache=&quot;874&quot;&gt;&lt;/div&gt;
                            &lt;div class=&quot;QBUL8c&quot; jsan=&quot;7.QBUL8c&quot; jsinstance=&quot;2&quot; jstcache=&quot;874&quot;&gt;&lt;/div&gt;
                            &lt;div class=&quot;QBUL8c&quot; jsan=&quot;7.QBUL8c&quot; jsinstance=&quot;3&quot; jstcache=&quot;874&quot;&gt;&lt;/div&gt;
                            &lt;div class=&quot;QBUL8c cXOKEb-S62Q7b&quot; jsan=&quot;7.QBUL8c,7.cXOKEb-S62Q7b&quot; jsinstance=&quot;*4&quot; jstcache=&quot;874&quot;&gt;&lt;/div&gt; 
                            &lt;span aria-hidden=&quot;true&quot; class=&quot;UY7F9&quot; jsan=&quot;7.UY7F9,0.aria-hidden&quot; jstcache=&quot;875&quot;&gt;(13 898)&lt;/span&gt;
                        &lt;/span&gt;
                     &lt;/span&gt; 
                     &lt;span jstcache=&quot;862&quot; style=&quot;display:none&quot;&gt;
                         &lt;jsl jstcache=&quot;863&quot; style=&quot;display:none&quot;&gt;&lt;/jsl&gt; 
                     &lt;/span&gt; 
                 &lt;/div&gt; 
             &lt;/div&gt; 
             &lt;div class=&quot;ZY2y6b-RWgCYc&quot;&gt; 
                 &lt;span jstcache=&quot;849&quot; style=&quot;display:none&quot;&gt;&lt;/span&gt; 
                 &lt;div class=&quot;ZY2y6b-RWgCYc&quot; jsinstance=&quot;0&quot; jstcache=&quot;850&quot;&gt; 
                     &lt;span jsinstance=&quot;0&quot; jstcache=&quot;851&quot;&gt;
                          &lt;jsl jstcache=&quot;852&quot;&gt; &lt;span jstcache=&quot;884&quot; style=&quot;display:none&quot;&gt;·&lt;/span&gt; 
                          &lt;span jstcache=&quot;885&quot;&gt;Muzeum&lt;/span&gt; &lt;span jstcache=&quot;886&quot; style=&quot;display:none&quot;&gt;&lt;/span&gt; &lt;/jsl&gt; &lt;/span&gt;&lt;span jsinstance=&quot;*1&quot; jstcache=&quot;851&quot;&gt; &lt;jsl jstcache=&quot;852&quot;&gt; &lt;span aria-hidden=&quot;true&quot; class=&quot;bXlT7b-hgDUwe&quot; jsan=&quot;7.bXlT7b-hgDUwe,0.aria-hidden&quot; jstcache=&quot;884&quot;&gt;·&lt;/span&gt; &lt;span jstcache=&quot;885&quot;&gt;150 London Wall&lt;/span&gt; &lt;span jstcache=&quot;886&quot; style=&quot;display:none&quot;&gt;&lt;/span&gt; &lt;/jsl&gt; &lt;/span&gt; &lt;/div&gt;&lt;div class=&quot;ZY2y6b-RWgCYc&quot; jsinstance=&quot;1&quot; jstcache=&quot;850&quot;&gt; &lt;span jsinstance=&quot;*0&quot; jstcache=&quot;851&quot;&gt; &lt;jsl jstcache=&quot;852&quot;&gt; &lt;span jstcache=&quot;884&quot; style=&quot;display:none&quot;&gt;·&lt;/span&gt; &lt;span jstcache=&quot;885&quot;&gt;Historia Londynu od starożytności do dziś&lt;/span&gt; &lt;span jstcache=&quot;886&quot; style=&quot;display:none&quot;&gt;&lt;/span&gt; &lt;/jsl&gt; &lt;/span&gt; &lt;/div&gt;&lt;div class=&quot;ZY2y6b-RWgCYc&quot; jsinstance=&quot;*2&quot; jstcache=&quot;850&quot;&gt; &lt;span jsinstance=&quot;*0&quot; jstcache=&quot;851&quot;&gt; &lt;jsl jstcache=&quot;852&quot;&gt; &lt;span jstcache=&quot;884&quot; style=&quot;display:none&quot;&gt;·&lt;/span&gt; &lt;span jstcache=&quot;885&quot;&gt;Zamknięcie: 17:00&lt;/span&gt; &lt;span jstcache=&quot;886&quot; style=&quot;display:none&quot;&gt;&lt;/span&gt; &lt;/jsl&gt; &lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;CJY91c-jRmmHf-aVTXAb-haAclf-JIbuQc&quot; jstcache=&quot;830&quot;&gt;&lt;/div&gt;&lt;div class=&quot;CJY91c-jRmmHf-aVTXAb-haAclf-HiaYvf&quot; jstcache=&quot;831&quot;&gt;&lt;div class=&quot;xwpmRb qisNDe&quot; jsan=&quot;t-PLs0ILPSy_c,7.xwpmRb,7.qisNDe,5.width,5.height,5.margin-top,5.margin-bottom,5.margin-left,5.margin-right&quot; jstcache=&quot;932&quot; style=&quot;width: 84px; height: 84px; margin: 0px;&quot;&gt;&lt;div class=&quot;Vig8jf-haAclf p0Hhde&quot; jsan=&quot;7.p0Hhde,7.Vig8jf-haAclf,5.min-width,5.min-height&quot; jstcache=&quot;836&quot; style=&quot;min-width:84px;min-height:84px&quot;&gt;&lt;img aria-hidden=&quot;true&quot; decoding=&quot;async&quot; src=&quot;//lh5.googleusercontent.com/proxy/tWfK1sqsGJZNlZu3WTUika5NJAu4mqKhx07Kub2ZjC_yU3PdIv3DWCKe8_cwJ3RBAUHjW5qZp3S6vGLQJ7HnYxCL_4YR4X1T3ju-ISh86JeC5Kb0KGnvp8j8Jt0vvk6Es_gdVz1AyfBfMDSN6DImwkgbwPL0RQ=w138-h92-k-no&quot; style=&quot;position: absolute; top: 50%;left: 50%;width: 126px;height: 84px;-webkit-transform: translateY(-50%) translateX(-50%);transform: translateY(-50%) translateX(-50%);&quot;/&gt;&lt;/div&gt;&lt;button jstcache=&quot;837&quot; style=&quot;display:none&quot;&gt;&lt;/button&gt;&lt;div class=&quot;badge-container&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;CJY91c-jRmmHf-aVTXAb-haAclf-hxbzzc&quot; jstcache=&quot;832&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;CJY91c-jRmmHf-aVTXAb-haAclf-IoWfhc&quot; jstcache=&quot;833&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</code></pre>
<p>The last part was running methong .find_all('a', href=True) which got me something like this:</p>
<pre><code>[&lt;a aria-label=&quot;Muzeum Londynu&quot; class=&quot;a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd&quot; href=&quot;https://www.google.com/maps/place/Muzeum+Londynu/data=!4m5!3m4!1s0x48761b5508c1cbeb:0x407de2c1952a25e4!8m2!3d51.5176183!4d-0.0967782?authuser=0&amp;amp;hl=pl&amp;amp;rclk=1&quot; jsaction=&quot;pane.wfvdle40;focus:pane.wfvdle40;blur:pane.wfvdle40;auxclick:pane.wfvdle40;contextmenu:pane.wfvdle40;keydown:pane.wfvdle40;clickmod:pane.wfvdle40&quot; jsan=&quot;7.a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd,0.aria-label,8.href,0.jsaction&quot; jstcache=&quot;825&quot;&gt;&lt;/a&gt;]
</code></pre>
<p>I am trying to specifically extract longitude and latitude which are [51.5176183, -0.0967782] present in the href.</p>
<p>I've tried using .href method similar to .text method but when i am using .href 'None' is being returned. Could you tell me how to extract those two velues from href body?</p>
<p>Running .text method on the html code returning output like this:</p>
<pre><code>Museum of London         4,6(13 898)           · Museum     · 150 London Wall       · The history of London from antiquity to today       · Closing: 17:00      
</code></pre>
","python, html, beautifulsoup, href, text-classification","<p>According to your question, I use split() method to get the desired output.</p>
<h1>script</h1>
<pre><code>html='''
&lt;html&gt;
 &lt;head&gt;
 &lt;/head&gt;
 &lt;body&gt;
  &lt;a aria-label=&quot;Muzeum Londynu&quot; class=&quot;a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd&quot; href=&quot;https://www.google.com/maps/place/Muzeum+Londynu/data=!4m5!3m4!1s0x48761b5508c1cbeb:0x407de2c1952a25e4!8m2!3d51.5176183!4d-0.0967782?authuser=0&amp;amp;hl=pl&amp;amp;rclk=1&quot; jsaction=&quot;pane.wfvdle40;focus:pane.wfvdle40;blur:pane.wfvdle40;auxclick:pane.wfvdle40;contextmenu:pane.wfvdle40;keydown:pane.wfvdle40;clickmod:pane.wfvdle40&quot; jsan=&quot;7.a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd,0.aria-label,8.href,0.jsaction&quot; jstcache=&quot;825&quot;&gt;
  &lt;/a&gt;
 &lt;/body&gt;
&lt;/html&gt;
'''
from bs4 import BeautifulSoup
soup = BeautifulSoup(html,'html5lib')
#print(soup.prettify())
href=soup.find(&quot;a&quot;,class_=&quot;a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd&quot;).get('href')
lat_lan=','.join(href.split('/')[-1].split('?')[0].split(':')[-1].split('!')[2:]).replace('3d','').replace('4d','').split()
print(lat_lan)
</code></pre>
<h1>Output</h1>
<pre><code>['51.5176183', '-0.0967782']
</code></pre>
",1,1,387,2021-12-08 22:39:39,https://stackoverflow.com/questions/70282594/text-classification-from-html-with-beautifulsoup
Create columns based on whether specific substrings exist in column,"<p>I have a DataFrame with some 270,000 books and I'd like to get_dummies based on the <code>Book-Title</code>, but only use a select number of words. In the end, I'd like to use 200-300 really distinguishing words, like 'Mystery', 'Murder', 'Classic', and 'Science' and see if they're in the <code>Book-Title</code> for each book. I'd like a column for each one of these words that is filled with 0's if the word isn't found and 1's if it is found.</p>
<p>If I try to get_dummies for the <em>entire</em> column, I end up with hundreds of thousands of different columns and I don't have the RAM to do that kind of processing.</p>
<p>Below is an example of what I'd like. I already have a list of the 200 most-frequent words among the Book Titles, I just don't know how to make columns from this list.</p>
<p>Input:</p>
<pre><code>    ISBN        Book-Title                       Book-Author            Year-Of-Publication     Publisher
0   0195153448  Classical Mythology              Mark P. O. Morford     2002     Oxford University Press
1   0002005018  Clara Callan                     Richard Bruce Wright   2001     HarperFlamingo Canada
2   0060973129  Decision in Normandy             Carlo D Este           1991     HarperPerennial
3   0374157065  Flu: The Story of the Great...   Gina Bari Kolata       1999     Farrar Straus Giroux
4   0393045218  The Mummies of Urumchi           E. J. W. Barber        1999     W. W. Norton &amp;amp; Company
</code></pre>
<p>Desired output:</p>
<pre><code>    ISBN        Title                          'World'  'Mythology'  'Mystery'  'Mummies'
0   0195153448  Classical Mythology             0        1            0          0
1   0002005018  Clara Callan                    0        0            0          0
2   0060973129  Decision in Normandy            0        0            0          0
3   0374157065  Flu: The Story of the Great...  0        0            0          0
4   0393045218  The Mummies of Urumchi          0        0            0          1  
</code></pre>
<p>Thank you in advance!</p>
<p>Adam</p>
","python, pandas, dataframe, machine-learning, text-classification","<p>You can apply a function to <code>'Book-Title'</code> column that iterates over the list of words to check if each word exists in each entry; and convert the output to a DataFrame:</p>
<pre><code>lst = ['World', 'Mythology', 'Mystery', 'Mummies']
df[lst] = df['Book-Title'].apply(lambda x: pd.Series([int(w in x) for w in lst]))
</code></pre>
<p>Output:</p>
<pre><code>                    Book-Title  World  Mythology  Mystery  Mummies
0          Classical Mythology      0          1        0        0
1                 Clara Callan      0          0        0        0
2         Decision in Normandy      0          0        0        0
3  Flu: The Story of the Great      0          0        0        0
4       The Mummies of Urumchi      0          0        0        1
</code></pre>
",1,0,50,2021-12-17 04:32:11,https://stackoverflow.com/questions/70388472/create-columns-based-on-whether-specific-substrings-exist-in-column
Why is this accuracy of this Random forest sentiment classification so low?,"<p>I want to use RandomForestClassifier for sentiment classification. The x contains data in string text, so I used LabelEncoder to convert strings. Y contains data in numbers. And my code is this:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import *
from sklearn.ensemble import *
from sklearn import *
from sklearn.preprocessing.label import LabelEncoder

data = pd.read_csv('data.csv')

x = data['Reviews']
y = data['Ratings']

le = LabelEncoder()
x_encoded = le.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_encoded,y, test_size = 0.2)

x_train = x_train.reshape(-1,1)
x_test = x_test.reshape(-1,1)

clf = RandomForestClassifier(n_estimators=100)

clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)
</code></pre>
<p>Then I printed out the accuracy like below:</p>
<pre><code>print(&quot;Accuracy:&quot;, metrics.accuracy_score(y_test, y_pred))
</code></pre>
<p>And here's the output:</p>
<pre><code>Accuracy: 0.5975
</code></pre>
<p>I have read that Random forests has high accuracy, because of the number of decision trees participating in the process. But I think that the accuracy is much lower than it should be. I have looked for some similar questions on Stack Overflow, but I couldn't find a solution for my problem.</p>
<p>Is there any problem in my code using Random Forest library? Or is there any exceptions of cases when using Random forest?</p>
","python, scikit-learn, random-forest, text-classification","<p>It is not a problem regarding Random Forests or the library, it is rather a problem how you transform your text input into a feature or feature vector.</p>
<p>What LabelEncoding does is; given some labels like [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] it transforms those labels into numeric values between 0 and n-1 with n-being the number of distinct input labels. However, I assume Reviews contain texts and not pure labels so to say. This means, all your reviews (if not 100% identical) are transformed into different labels. Eventually, this leads to your classifier doing random stuff. give that input. This means you need something different to transform your textual input into a numeric input that Random Forests can work on.</p>
<p>As a simple start, you can try something like TfIDF or also some simple count vectorizer. Those are available from sklearn <a href=""https://scikit-learn.org/stable/modules/feature_extraction.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/feature_extraction.html</a> section 6.2.3. Text feature extraction. There are more sophisticated ways of transforming texts into numeric vectors but that should be a good start for you to understand what has to happen conceptually.</p>
<p>A last important note is that you fit those vectorizers only on the training set and not on the full dataset. Otherwise, you might leak information from training to evaluation/testing. A good way of doing this would be to build a sklearn pipeline that consists of a feature transformation step and the classifier.</p>
",3,1,409,2022-01-12 06:24:10,https://stackoverflow.com/questions/70677179/why-is-this-accuracy-of-this-random-forest-sentiment-classification-so-low
IndexError: Target is out of bounds,"<p>I am currently trying to replicate the article</p>
<p><a href=""https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f</a></p>
<p>to get an introduction to PyTorch and BERT.</p>
<p>I used some own sample corpus and corresponding tragets as practise, but the code throws the following:</p>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-4-8577755f37de&gt; in &lt;module&gt;()
    201 LR = 1e-6
    202 
--&gt; 203 trainer(model, df_train, df_val, LR, EPOCHS)

3 frames
&lt;ipython-input-4-8577755f37de&gt; in trainer(model, train_data, val_data, learning_rate, epochs)
    162                 output = model(input_id, mask)
    163 
--&gt; 164                 batch_loss = criterion(output, torch.max(train_label,1)[1])
    165                 total_loss_train += batch_loss.item()
    166 

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1101                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1102             return forward_call(*input, **kwargs)
   1103         # Do not call functions when jit is used
   1104         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py in forward(self, input, target)
   1150         return F.cross_entropy(input, target, weight=self.weight,
   1151                                ignore_index=self.ignore_index, reduction=self.reduction,
-&gt; 1152                                label_smoothing=self.label_smoothing)
   1153 
   1154 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   2844     if size_average is not None or reduce is not None:
   2845         reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2846     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
   2847 
   2848 

IndexError: Target 32 is out of bounds.
</code></pre>
<p>The code is mostly identical to the one in the article, except of course the more extensive lable-dict.</p>
<p>Orginial:</p>
<pre><code>labels = {'business':0,
          'entertainment':1,
          'sport':2,
          'tech':3,
          'politics':4
          }
</code></pre>
<p>Mine:</p>
<pre><code>labels = 
{'Macroeconomics': 0,
 'Microeconomics': 1,
 'Labor Economics': 2,
 'Subnational Fiscal Issues': 3,
 'Econometrics': 4,
 'International Economics': 5,
 'Financial Economics': 6,
 'Health, Education, and Welfare': 7,
 'Public Economics': 8,
 'Development and Growth': 9,
 'Industrial Organization': 10,
 'Other': 11,
 'Environmental and Resource Economics': 12,
 'History': 13,
 'Regional and Urban Economics': 14,
 'Development Economics': 15,
 'Corporate Finance': 16,
 'Children': 17,
 'Labor Studies': 18,
 'Economic Fluctuations and Growth': 19,
 'Economics of Aging': 20,
 'Economics of Education': 21,
 'International Trade and Investment': 22,
 'Asset Pricing': 23,
 'Health Economics': 24,
 'Law and Economics': 25,
 'International Finance and Macroeconomics': 26,
 'Monetary Economics': 27,
 'Technical Working Papers': 28,
 'Political Economy': 29,
 'Development of the American Economy': 30,
 'Health Care': 31,
 'Productivity, Innovation, and Entrepreneurship': 32}
</code></pre>
<p>Code:</p>
<pre><code>class Dataset(torch.utils.data.Dataset):

    def __init__(self, df):

        self.labels = torch.LongTensor([labels[label] for label in df[&quot;category&quot;]])
        self.texts = [tokenizer(text, 
                               padding='max_length', max_length = 512, truncation=True,
                                return_tensors=&quot;pt&quot;) for text in df['text']]

    def classes(self):
        return self.labels

    def __len__(self):
        return len(self.labels)

    def get_batch_labels(self, idx):
        # Fetch a batch of labels
        return np.array(self.labels[idx])

    def get_batch_texts(self, idx):
        # Fetch a batch of inputs
        return self.texts[idx]

    def __getitem__(self, idx):
        batch_texts = self.get_batch_texts(idx)
        batch_y = np.array(range(0,len(labels)))

        return batch_texts, batch_y
    
#Splitting the sample into trainingset, validationset and testset (80,10,10)
np.random.seed(112)
df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), 
                                     [int(.8*len(df)), int(.9*len(df))])

print(len(df_train),len(df_val), len(df_test))


from torch import nn

class BertClassifier(nn.Module):

    def __init__(self, dropout=0.5):

        super(BertClassifier, self).__init__()

        self.bert = BertModel.from_pretrained('bert-base-cased')
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 5)
        self.relu = nn.ReLU()

    def forward(self, input_id, mask):

        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        final_layer = self.relu(linear_output)

        return final_layer
    
from torch.optim import Adam
from tqdm import tqdm

def trainer(model, train_data, val_data, learning_rate, epochs):

    train, val = Dataset(train_data), Dataset(val_data)
    
    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)
    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)

    use_cuda = torch.cuda.is_available()
    device = torch.device(&quot;cuda&quot; if use_cuda else &quot;cpu&quot;)

    criterion = nn.CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr= learning_rate)

    if use_cuda:

            model = model.cuda()
            criterion = criterion.cuda()

    for epoch_num in range(epochs):

            total_acc_train = 0
            total_loss_train = 0

            for train_input, train_label in tqdm(train_dataloader):

                train_label = train_label.to(device)
                mask = train_input['attention_mask'].to(device)
                input_id = train_input['input_ids'].squeeze(1).to(device)

                output = model(input_id, mask)
                
                batch_loss = criterion(output, torch.max(train_label,1)[1])
                total_loss_train += batch_loss.item()
                
                acc = (output.argmax(dim=1) == train_label).sum().item()
                total_acc_train += acc

                model.zero_grad()
                batch_loss.backward()
                optimizer.step()
            
            total_acc_val = 0
            total_loss_val = 0

            with torch.no_grad():

                for val_input, val_label in val_dataloader:

                    val_label = val_label.to(device)
                    mask = val_input['attention_mask'].to(device)
                    input_id = val_input['input_ids'].squeeze(1).to(device)

                    output = model(input_id, mask)

                    batch_loss = criterion(output, val_label)
                    total_loss_val += batch_loss.item()
                    
                    acc = (output.argmax(dim=1) == val_label).sum().item()
                    total_acc_val += acc
            
            print(
                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \
                | Train Accuracy: {total_acc_train / len(train_data): .3f} \
                | Val Loss: {total_loss_val / len(val_data): .3f} \
                | Val Accuracy: {total_acc_val / len(val_data): .3f}')
                  
EPOCHS = 5
model = BertClassifier()
LR = 1e-6
              
trainer(model, df_train, df_val, LR, EPOCHS)
</code></pre>
","python, pytorch, text-classification, bert-language-model","<p>You're creating a list of length 33 in your <code>__getitem__</code> call which is one more than the length of the labels list, hence the out of bounds error. In fact, you create <em>the same</em> list each time this method is called. You're supposed to fetch the associated <code>y</code> with the <code>X</code> found at <code>idx</code>.</p>
<p>If you replace <code>batch_y = np.array(range(...))</code> with <code>batch_y = np.array(self.labels[idx])</code>, you'll fix your error. Indeed, this is already implemented in your <code>get_batch_labels</code> method.</p>
",1,0,17022,2022-01-12 10:53:59,https://stackoverflow.com/questions/70680290/indexerror-target-is-out-of-bounds
NLP text classification CountVectorizer Shape Error,"<p>I have a text dataset which has one column for reviews and another column for labels. I want to build a decision tree model by using that dataset, I used vectorizer but it gives <code>ValueError: Number of labels=37500 does not match number of samples=1</code>  error. <code>vect.vocabulary_ returns {'review': 0}</code> review is the column name. So I think it does not fit to all data. Here is the code below, any help is appreciated.</p>
<pre><code>from sklearn.model_selection import train_test_split
X_train, X_test,y_train, y_test = train_test_split(data.iloc[:,:-1],data.iloc[:,-1:],
test_size = 0.25, random_state = 42)

from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()
vect.fit(X_train)
X_train_dtm = vect.transform(X_train)
X_train_dtm = vect.fit_transform(X_train)
X_test_dtm = vect.transform(X_test)

from sklearn.tree import DecisionTreeClassifier 
DTC = DecisionTreeClassifier()
DTC.fit(X_train_dtm, y_train)
y1_pred_class = DTC.predict(X_test_dtm)
</code></pre>
<p>Also X_train_dtm.shape is <code>&lt;bound method spmatrix.get_shape of &lt;1x1 sparse matrix of type '&lt;class 'numpy.int64'&gt;' with 1 stored elements in Compressed Sparse Row format&gt;&gt;</code></p>
","python, scikit-learn, nlp, decision-tree, text-classification","<p>It worked when I changed this part:</p>
<p>X_train, X_test,y_train, y_test = train_test_split(data['text'],
data['tag'],test_size = 0.25, random_state = 42)</p>
",0,0,274,2022-01-15 19:48:23,https://stackoverflow.com/questions/70724874/nlp-text-classification-countvectorizer-shape-error
ValueError: Requesting 5-fold cross-validation but provided less than 5 examples for at least one class,"<p>I have been training a text classifier to then later use to predict characters of a TV show. So far, my code looks like:</p>
<pre><code>vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=0.001, max_df=0.75,stop_words='English')
X = vectorizer.fit_transform(data['text'])
y = data['character']
print(X.shape, y.shape) #prints (5999, 1429) (5999,)

# get baseline performance
kf = KFold(n_splits=5)
most_frequent = DummyClassifier(strategy='most_frequent')
print(cross_val_score(most_frequent , X, y=y, cv=kf, n_jobs= -1, scoring=&quot;accuracy&quot;).mean())

# fine-tune classifier
base_clf = CalibratedClassifierCV(cv=kf, base_estimator=LogisticRegression(n_jobs= -1, solver='lbfgs' ))

param_grid = {'base_estimator__C': [0.01, 0.05, 0.1, 0.5, 1.0, 10, 20, 50],
'base_estimator__class_weight': ['balanced', 'auto']}

search = GridSearchCV(base_clf, param_grid, cv=kf, scoring='f1_micro')
search.fit(X, y)

# use best classifier to get performance estimate
clf = search.best_estimator_.base_estimator
print(cross_val_score(clf, X, y=y, cv=kf, n_jobs= -1, scoring='f1_micro').mean())
</code></pre>
<p>However, I keep getting the following error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
/var/folders/fv/h7n33cb5227g4t5lxym8g_800000gn/T/ipykernel_2208/2611717736.py in &lt;module&gt;
      6 
      7 search = GridSearchCV(base_clf, param_grid, cv=kf, scoring='f1_micro')
----&gt; 8 search.fit(X, y)
      9 
     10 # use best classifier to get performance estimate

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     61             extra_args = len(args) - len(all_args)
     62             if extra_args &lt;= 0:
---&gt; 63                 return f(*args, **kwargs)
     64 
     65             # extra_args &gt; 0

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py in fit(self, X, y, groups, **fit_params)
    878             refit_start_time = time.time()
    879             if y is not None:
--&gt; 880                 self.best_estimator_.fit(X, y, **fit_params)
    881             else:
    882                 self.best_estimator_.fit(X, **fit_params)

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/calibration.py in fit(self, X, y, sample_weight)
    301             if n_folds and np.any([np.sum(y == class_) &lt; n_folds
    302                                    for class_ in self.classes_]):
--&gt; 303                 raise ValueError(f&quot;Requesting {n_folds}-fold &quot;
    304                                  &quot;cross-validation but provided less than &quot;
    305                                  f&quot;{n_folds} examples for at least one class.&quot;)

ValueError: Requesting 5-fold cross-validation but provided less than 5 examples for at least one class.
</code></pre>
<p>I am not quite sure how to resolve this error and would truly appreciate any advice.</p>
<p>Thank you in advance!</p>
","python, scikit-learn, svm, cross-validation, text-classification","<p>You need to check the distribution of your target value <code>data['character']</code> : it seems that the number of values in one of the classes in the target column is too small. To do it you can use : <code>data['character'].value_counts()</code></p>
",0,0,574,2022-02-14 07:09:02,https://stackoverflow.com/questions/71108243/valueerror-requesting-5-fold-cross-validation-but-provided-less-than-5-examples
Label any text with multiple topics in sequence of their occurrence,"<p>I have a DataFrame with an ID and Text like below:</p>
<p><strong>df1</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>I have completed my order</td>
</tr>
<tr>
<td>2</td>
<td>I have made the payment. When can I expect the order to be delivered?</td>
</tr>
<tr>
<td>3</td>
<td>I am unable to make the payment.</td>
</tr>
<tr>
<td>4</td>
<td>I am done with registration and payment. I need the order number?</td>
</tr>
<tr>
<td>5</td>
<td>I am unable to complete registration. How will I even order?</td>
</tr>
</tbody>
</table>
</div>
<p>I have certain topics to classify these texts:
class = [&quot;order&quot;, &quot;payment&quot;, &quot;registration&quot;]</p>
<p>I am doing the following which gets me the results:</p>
<pre><code>classes = [&quot;order&quot;, &quot;payment&quot;, &quot;registration&quot;]
for c in classes:
    word_counter = Counter()
    list_df = []
    field = &quot;Text&quot;
    df2 = pd.DataFrame()
    df2 = df2[df2[field].str.contains(c)] 
    print(c)
    list_df.append(df2)
    final_df = pd.concat(list_df)
    final_df.to_csv(&quot;./&quot; + c + &quot;.csv&quot;)    
</code></pre>
<p>This will generate me 3 CSV files which I will later join again:</p>
<pre><code>file_list = []
os.chdir('&lt;file path&gt;')

for file in os.listdir():
    if file.endswith('.csv'):
        df = pd.read_csv(file, sep=&quot;,&quot;, encoding='ISO-8859-1')
        df['filename'] = file
        file_list.append(df)

df_topic = pd.concat(file_list, ignore_index=True)
df_topic['topic'] = df_topic['filename'].str.split('.').str[0]
df_topic= df_topic.drop('filename', 1)
</code></pre>
<p>The resultant DataFrame looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Text</th>
<th>Topic</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>I have completed my order</td>
<td>order</td>
</tr>
<tr>
<td>2</td>
<td>I have made the payment. When can I expect the order to be delivered?</td>
<td>order</td>
</tr>
<tr>
<td>4</td>
<td>I am done with registration and payment. I need the order number?</td>
<td>order</td>
</tr>
<tr>
<td>2</td>
<td>I have made the payment. When can I expect the order to be delivered?</td>
<td>payment</td>
</tr>
<tr>
<td>3</td>
<td>I am unable to make the payment.</td>
<td>payment</td>
</tr>
<tr>
<td>4</td>
<td>I am done with registration and payment. I need the order number?</td>
<td>payment</td>
</tr>
<tr>
<td>4</td>
<td>I am done with registration and payment. I need the order number?</td>
<td>registration</td>
</tr>
<tr>
<td>5</td>
<td>I am unable to complete registration. How will I even order?</td>
<td>registration</td>
</tr>
</tbody>
</table>
</div>
<p>But, the problem you see here is that same text may have the keywords for the other classes too and can be tagged as either (like text for id=2 has both order and payment). I can only have one record label for each id and thus would prefer to have it as Primary or Secondary topic based on the sequence of their occurrence from the beginning of the text. If a text has more than 2 then first 2 gets preference but just to ensure we may need the third topic (or nth topic) for a future instance I would like to store it as a list in the final field. (Example for id = 4 is illustrated)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Text</th>
<th>Primary Topic</th>
<th>Secondary Topic</th>
<th>Identified Topics</th>
<th>Topics List</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>I have completed my order</td>
<td>order</td>
<td>null</td>
<td>1</td>
<td>[order]</td>
</tr>
<tr>
<td>2</td>
<td>I have made the payment. When can I expect the order to be delivered?</td>
<td>payment</td>
<td>order</td>
<td>2</td>
<td>[payment,order]</td>
</tr>
<tr>
<td>3</td>
<td>I am unable to make the payment.</td>
<td>payment</td>
<td>null</td>
<td>1</td>
<td>[payment]</td>
</tr>
<tr>
<td>4</td>
<td>I am done with registration and payment. I need the order number?</td>
<td>registration</td>
<td>payment</td>
<td>3</td>
<td>[registration,payment,order]</td>
</tr>
<tr>
<td>5</td>
<td>I am unable to complete registeration. How will I even order?</td>
<td>registration</td>
<td>order</td>
<td>2</td>
<td>[registration,order]</td>
</tr>
</tbody>
</table>
</div>
<p>Is it possible to do it this way. If not, what is a good way to approach such labelling issues?</p>
","python, pandas, dataframe, data-manipulation, text-classification","<p>IIUC, you could use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extractall.html"" rel=""nofollow noreferrer""><code>str.extractall</code></a> combined with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html"" rel=""nofollow noreferrer""><code>GroupBy.agg</code></a>:</p>
<pre><code>lst = [&quot;order&quot;, &quot;payment&quot;, &quot;registration&quot;]
regex = f'({&quot;|&quot;.join(lst)})'  # if lst contains special chars, wrap in re.escape
df2 = df.join(df['Text']
              .str.extractall(regex)[0]
              .groupby(level=0).agg(**{'Primary Topic': 'first',
                                       'Secondary Topic': lambda x: x.iloc[1] if len(x)&gt;1 else 'null',
                                       'Identified Topics': 'nunique',
                                       'Topics List': list})
               )
</code></pre>
<p>output:</p>
<pre><code>   ID                                                                   Text Primary Topic Secondary Topic  Identified Topics                     Topics List
0   1                                              I have completed my order         order            null                  1                         [order]
1   2  I have made the payment. When can I expect the order to be delivered?       payment           order                  2                [payment, order]
2   3                                       I am unable to make the payment.       payment            null                  1                       [payment]
3   4      I am done with registration and payment. I need the order number?  registration         payment                  3  [registration, payment, order]
4   5           I am unable to complete registration. How will I even order\  registration           order                  2           [registration, order]
</code></pre>
",1,1,200,2022-03-10 08:16:02,https://stackoverflow.com/questions/71420789/label-any-text-with-multiple-topics-in-sequence-of-their-occurrence
Can&#39;t backward pass two losses in Classification Transformer Model,"<p>For my model I'm using a roberta transformer model and the Trainer from the Huggingface transformer library.</p>
<p>I calculate two losses:
<code>lloss</code> is a Cross Entropy Loss and <code>dloss</code> calculates the loss inbetween hierarchy layers.</p>
<p>The total loss is the sum of lloss and dloss. (Based on <a href=""https://github.com/Ugenteraan/Deep_Hierarchical_Classification/blob/e4f20ae51a2daabfc1c01f6fdab778ef31cc7617/model/hierarchical_loss.py"" rel=""nofollow noreferrer"">this</a>)</p>
<p>When calling <code>total_loss.backwards()</code> however, I get the error:</p>
<pre><code>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed
</code></pre>
<p>Any idea why that happens? Can I force it to only call backwards once? Here is the loss calculation part:</p>
<pre><code>dloss = calculate_dloss(prediction, labels, 3)
lloss = calculate_lloss(predeiction, labels, 3)
total_loss = lloss + dloss 
total_loss.backward()

def calculate_lloss(predictions, true_labels, total_level):
    '''Calculates the layer loss.
    '''

    loss_fct = nn.CrossEntropyLoss()

    lloss = 0
    for l in range(total_level):

        lloss += loss_fct(predictions[l], true_labels[l])

    return self.alpha * lloss

def calculate_dloss(predictions, true_labels, total_level):
    '''Calculate the dependence loss.
    '''

    dloss = 0
    for l in range(1, total_level):

        current_lvl_pred = torch.argmax(nn.Softmax(dim=1)(predictions[l]), dim=1)
        prev_lvl_pred = torch.argmax(nn.Softmax(dim=1)(predictions[l-1]), dim=1)

        D_l = self.check_hierarchy(current_lvl_pred, prev_lvl_pred, l)  #just a boolean tensor

        l_prev = torch.where(prev_lvl_pred == true_labels[l-1], torch.FloatTensor([0]).to(self.device), torch.FloatTensor([1]).to(self.device))
        l_curr = torch.where(current_lvl_pred == true_labels[l], torch.FloatTensor([0]).to(self.device), torch.FloatTensor([1]).to(self.device))

        dloss += torch.sum(torch.pow(self.p_loss, D_l*l_prev)*torch.pow(self.p_loss, D_l*l_curr) - 1)

    return self.beta * dloss
</code></pre>
","python, neural-network, pytorch, text-classification","<p>There is nothing wrong with having a loss that is the sum of two individual losses, here is a small proof of principle adapted <a href=""https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb"" rel=""nofollow noreferrer"">from the docs</a>:</p>
<pre><code>import torch
import numpy
from sklearn.datasets import make_blobs

class Feedforward(torch.nn.Module):
    def __init__(self, input_size, hidden_size):
        super(Feedforward, self).__init__()
        self.input_size = input_size
        self.hidden_size  = hidden_size
        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)
        self.relu = torch.nn.ReLU()
        self.fc2 = torch.nn.Linear(self.hidden_size, 1)
        self.sigmoid = torch.nn.Sigmoid()
    def forward(self, x):
        hidden = self.fc1(x)
        relu = self.relu(hidden)
        output = self.fc2(relu)
        output = self.sigmoid(output)
        return output

def blob_label(y, label, loc): # assign labels
    target = numpy.copy(y)
    for l in loc:
        target[y == l] = label
    return target

x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)
x_train = torch.FloatTensor(x_train)
y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))
y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))

x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)
x_test = torch.FloatTensor(x_test)
y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))
y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))


model = Feedforward(2, 10)
criterion = torch.nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)


model.eval()
y_pred = model(x_test)
before_train = criterion(y_pred.squeeze(), y_test)
print('Test loss before training' , before_train.item())

model.train()
epoch = 20
for epoch in range(epoch):
    optimizer.zero_grad()    # Forward pass
    y_pred = model(x_train)    # Compute Loss
    lossCE= criterion(y_pred.squeeze(), y_train)
    lossSQD = (y_pred.squeeze()-y_train).pow(2).mean()
    loss=lossCE+lossSQD
    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    # Backward pass
    loss.backward()
    optimizer.step()
</code></pre>
<p>There must be a real second time that you call directly or indirectly <code>backward</code> on some varaible that then traverses through your graph. It is a bit too much to ask for the complete code here, only you can check this or at least reduce it to a minimal example (while doing so, you might already find the issue). Apart from that, I would start checking:</p>
<ol>
<li>Does it already occur in the first iteration of training? If not: are you reusing any calculation results for the second iteration without a <code>detach</code>?</li>
<li>When you do <code>backward</code> on your losses individually <code>lloss.backward()</code> followed by <code>dloss.backward()</code> (this has the same effect as adding them together first as gradients are accumulated): what happens? This will let you track down for which of the two losses the error occurs.</li>
</ol>
",1,0,610,2022-03-14 09:06:55,https://stackoverflow.com/questions/71465239/cant-backward-pass-two-losses-in-classification-transformer-model
TypeError: len() of unsized object,"<p>I am trying random forest classifier from sklearn, when i want to print the classifier report, it is give me an error.</p>
<p>This was the code :</p>
<pre><code>randomforestmodel = RandomForestClassifier()
randomforestmodel.fit(train_vectors, data_train['label'])
predict_rfmodel = randomforestmodel.predict(test_vectors)

print(&quot;classification with randomforest&quot;)
print(metrics.classification_report(test_vectors, predict_rfmodel))
</code></pre>
<p>And the error was like this :</p>
<pre><code>    classification with randomforest
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-34-f976cec884e4&gt; in &lt;module&gt;()
      1 print(&quot;classification with randomforest&quot;)
----&gt; 2 print(metrics.classification_report(test_vectors, predict_rfmodel))

2 frames
/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in classification_report(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)
   2108     &quot;&quot;&quot;
   2109 
-&gt; 2110     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
   2111 
   2112     if labels is None:

/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in _check_targets(y_true, y_pred)
     83     &quot;&quot;&quot;
     84     check_consistent_length(y_true, y_pred)
---&gt; 85     type_true = type_of_target(y_true)
     86     type_pred = type_of_target(y_pred)
     87 

/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py in type_of_target(y)
    308 
    309     # Invalid inputs
--&gt; 310     if y.ndim &gt; 2 or (y.dtype == object and len(y) and not isinstance(y.flat[0], str)):
    311         return &quot;unknown&quot;  # [[[1, 2]]] or [obj_1] and not [&quot;label_1&quot;]
    312 

TypeError: len() of unsized object
</code></pre>
","scikit-learn, classification, random-forest, text-classification","<p>You're providing the test instances <em>features</em> (<code>test_vectors</code>) instead of the true test instances <em>labels</em> to <code>classification_report</code>.</p>
<p>As <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"" rel=""nofollow noreferrer"">per the documentation</a>, the first parameter should be:</p>
<blockquote>
<p><strong>y_true</strong>: 1d array-like, or label indicator array / sparse matrix.</p>
<p>Ground truth (correct) target values.</p>
</blockquote>
",0,1,1595,2022-03-24 07:05:22,https://stackoverflow.com/questions/71598259/typeerror-len-of-unsized-object
ValueError: multiclass format is not supported on ROC_Curve for text classification,"<p>I am trying to use ROC for evaluating my emotion text classifier model</p>
<p>This is my code for the ROC :</p>
<pre><code># ROC-AUC Curve
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
fpr, tpr, thresholds = roc_curve(y_test, y_test_hat2)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC CURVE')
plt.legend(loc=&quot;lower right&quot;)
plt.show()
</code></pre>
<p>This is the Error :</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-30-ef4ee0eff994&gt; in &lt;module&gt;()
      2 from sklearn.metrics import roc_curve, auc
      3 import matplotlib.pyplot as plt
----&gt; 4 fpr, tpr, thresholds = roc_curve(y_test, y_test_hat2)
      5 roc_auc = auc(fpr, tpr)
      6 plt.figure()

1 frames
/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py in roc_curve(y_true, y_score, pos_label, sample_weight, drop_intermediate)
    961     &quot;&quot;&quot;
    962     fps, tps, thresholds = _binary_clf_curve(
--&gt; 963         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight
    964     )
    965 

/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py in _binary_clf_curve(y_true, y_score, pos_label, sample_weight)
    729     y_type = type_of_target(y_true)
    730     if not (y_type == &quot;binary&quot; or (y_type == &quot;multiclass&quot; and pos_label is not None)):
--&gt; 731         raise ValueError(&quot;{0} format is not supported&quot;.format(y_type))
    732 
    733     check_consistent_length(y_true, y_score, sample_weight)

ValueError: multiclass format is not supported
</code></pre>
<p>This is the y_test and y_test_hat2 :</p>
<pre><code>y_test = data_test[&quot;label&quot;]


from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
test_vectors = vectorizer.transform(data_test['tweet'])
classifier_linear2 = LinearSVC(verbose=1)
y_test_hat2=classifier_linear2.predict(test_vectors)
</code></pre>
<p>Shape of test_vectors = (1096, 11330)</p>
<p>Shape of y_test       = (1096,)</p>
<p>Label in y_test       = ['0', '1', '2', '3', '4']</p>
","svm, text-classification, roc, multiclass-classification, tfidfvectorizer","<p>A ROC curve is based on <em>soft predictions</em>, i.e. it uses the predicted <strong>probability</strong> of an instance to belong to the positive class rather than the predicted <strong>class</strong>. For example with sklearn one can obtain the probabilities with <code>predict_proba</code> instead of <code>predict</code> (for the classifiers which provide it, <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba"" rel=""nofollow noreferrer"">example</a>).</p>
<p>Note: OP used the tag multiclass-classification, but it's important to note that ROC curves can only be applied to <em>binary</em> classification problems.</p>
<p>One can find a short explanation of ROC curves <a href=""https://datascience.stackexchange.com/a/87363/64377"">here</a>.</p>
",0,1,2356,2022-03-25 12:08:56,https://stackoverflow.com/questions/71616761/valueerror-multiclass-format-is-not-supported-on-roc-curve-for-text-classificat
Converting h5 to tflite,"<p>I have been trying to get this zero-shot text classification <code>joeddav / xlm-roberta-large-xnli</code> to convert from h5 to tflite file (<a href=""https://huggingface.co/joeddav/xlm-roberta-large-xnli"" rel=""nofollow noreferrer"">https://huggingface.co/joeddav/xlm-roberta-large-xnli</a>), but this error pops up and I cant find it described online, how is it fixed? If it can't, is there another zero-shot text classifier I can use that would produce similar accuracy even after becoming tflite?</p>
<pre><code>AttributeError: 'T5ForConditionalGeneration' object has no attribute 'call'
</code></pre>
<p>I have been trying a few different tutorials and the current google colab file I have is an amalgam of a couple of them. <a href=""https://colab.research.google.com/drive/1sYQJqvhM_KEvMt2IP15d8Ud9L-ApiYv6?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1sYQJqvhM_KEvMt2IP15d8Ud9L-ApiYv6?usp=sharing</a></p>
","tensorflow, tensorflow-lite, huggingface-transformers, text-classification, huggingface","<p>[ Convert TFLite from saved .h5 model to TFLite model ]</p>
<p>Conversion using tflite convert there are multiple ways by</p>
<ol>
<li>TF-Lite Convertor <a href=""https://www.tensorflow.org/lite/convert#converting_a_keras_h5_model_"" rel=""nofollow noreferrer"">TF-Lite convertor</a></li>
<li>TF.Lite.TFLiteConverter OR else</li>
</ol>
<p>From the provided links currently they try to convert from saved model .h5 to TFLite, to confirm their question.</p>
<p><strong>[ Sample ]:</strong></p>
<pre><code>&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;
: Model Initialize
&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;
model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=( 32, 32, 3 )),
    tf.keras.layers.Dense(128, activation='relu'),
])
model.compile(optimizer='sgd', loss='mean_squared_error') # compile the model
model.summary()

model.save_weights(checkpoint_path)

&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;
: FileWriter
&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;
if exists(checkpoint_path) :
    model.load_weights(checkpoint_path)
    print(&quot;model load: &quot; + checkpoint_path)


tf_lite_model_converter = tf.lite.TFLiteConverter.from_keras_model(
    model
) # &lt;tensorflow.lite.python.lite.TFLiteKerasModelConverterV2 object at 0x0000021095194E80&gt;
tflite_model = tf_lite_model_converter.convert()

# Save the model.
with open(checkpoint_dir + '\\model.tflite', 'wb') as f:
    f.write(tflite_model)
</code></pre>
",0,-1,781,2022-04-28 00:27:23,https://stackoverflow.com/questions/72036646/converting-h5-to-tflite
"Getting a ValueError: Shapes (None, 1) and (None, 5) are incompatible","<pre><code>X_train = df_train[&quot;Base_Reviews&quot;].values
X_test  = df_test[&quot;Base_Reviews&quot;].values

y_train = df_train['category'].values
y_test  = df_test['category'].values

num_words = 20000 #Max. workds to use per toxic comment
max_features = 15000 #Max. number of unique words in embeddinbg vector
max_len = 200 #Max. number of words per toxic comment to be use
embedding_dims = 128 #embedding vector output dimension 
num_epochs = 5 # (before 5)number of epochs (number of times that the model is exposed to the training dataset)
val_split = 0.2
batch_size2 = 256

tokenizer = tokenizer = Tokenizer(num_words = num_words, lower = False)
tokenizer.fit_on_texts(list(X_train))


X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
X_train = sequence.pad_sequences(X_train, max_len)
X_test  = sequence.pad_sequences(X_test,  max_len)
print('X_train shape:', X_train.shape)
print('X_test shape: ', X_test.shape)
</code></pre>
<p>and this is the shape of our dataset: X_train shape: (11419, 200), X_test shape:  (893, 200)</p>
<pre><code>X_tra, X_val, y_tra, y_val = train_test_split(X_train, y_train, train_size =0.8, random_state=233)
early = EarlyStopping(monitor=&quot;val_loss&quot;, mode=&quot;min&quot;, patience=4)

nn_model = Sequential([
    Embedding(input_dim=max_features, input_length=max_len, output_dim=embedding_dims),
    GlobalMaxPool1D(),
    Dense(50, activation = 'relu'),
    Dropout(0.2),
    Dense(5, activation = 'softmax')
])

def mean_pred(y_true, y_pred):
return K.mean(y_pred)
nn_model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=Adam(0.01), metrics=['accuracy', mean_pred, fmeasure, precision, auroc, recall])
</code></pre>
<p>When I run the below code I get the above error.</p>
<pre><code>nn_model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=Adam(0.01), metrics=['accuracy', mean_pred, fmeasure, precision, auroc, recall])
</code></pre>
<p>When I feed the data to NN Model the above error I get. How can I resolve the error? This is the error:</p>
<pre><code>ValueError                               


Traceback (most recent call last)
&lt;ipython-input-51-a3721a91aa0b&gt; in &lt;module&gt;
----&gt; 1 nn_model_fit = nn_model.fit(X_tra, y_tra, batch_size=batch_size2, epochs=num_epochs, validation_data=(X_val, y_val), callbacks=[early])

~\anaconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~\anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py in autograph_handler(*args, **kwargs)
   1145           except Exception as e:  # pylint:disable=broad-except
   1146             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:
   1149               raise

ValueError: in user code:
**ValueError: Shapes (None, 1) and (None, 5) are incompatible**
</code></pre>
","python, tensorflow, keras, text-classification, multiclass-classification","<p>You have to map your labels to integer values:</p>
<pre><code>import numpy as np

labels_index = dict(zip([&quot;issue&quot;, &quot;supporting&quot;, &quot;decision&quot;, &quot;neutral&quot;, &quot;attacking&quot;], np.arange(5)))

y_train = [labels_index[y] for y in y_train]
</code></pre>
",1,1,77,2022-05-04 11:29:03,https://stackoverflow.com/questions/72112204/getting-a-valueerror-shapes-none-1-and-none-5-are-incompatible
Saved Machine Learning Model using Pickle won&#39;t predict text values properly,"<p>I currently have a Machine Learning model which would predict what part of speech does a current word belong to</p>
<pre><code>penn_results = penn_crf.predict_single(features)
</code></pre>
<p>and then, I made a code wherein it makes a print making a (word, POS) style print;</p>
<pre><code>penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]
</code></pre>
<p>and when I try to run this, it gives me this output.</p>
<p><strong>[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN')] [('The', 'DET'), ('quick', 'NOUN'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'NOUN'), ('over', 'ADP')]</strong></p>
<p>and so  I saved this model using</p>
<pre><code>penn_filename = 'ptcp.sav'
pickle.dump(penn_crf, open(penn_filename, 'wb'))
</code></pre>
<p>Upon trying to run the model by loading hte saved pickle file with this</p>
<pre><code>test = &quot;The quick brown fox jumps over the head&quot;
pickled_model = pickle.load(open('penn_treebank_crf_postagger.sav', 'rb'))
pickled_model.predict(test)
print(pickled_model.predict(test))
</code></pre>
<p>It prints this
<strong>[['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP']]</strong></p>
<p>How can I make it print the accurate predicted values like this
<strong>[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN')] [('The', 'DET'), ('quick', 'NOUN'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'NOUN'), ('over', 'ADP')]</strong></p>
","python, machine-learning, pickle, text-classification","<p><em>Caution: this code was not tested.</em></p>
<p>Replace the last line</p>
<pre><code>print(pickled_model.predict(test))
</code></pre>
<p>with something like this:</p>
<pre><code>tokens_test = test.split()
predictions_test = pickled_model.predict(test)
pairs_test = [(tokens_test[idx], predictions_test[idx]) for idx in range(len(tokens_test))]
print(pairs_test)
</code></pre>
",0,0,725,2022-05-10 08:38:40,https://stackoverflow.com/questions/72183136/saved-machine-learning-model-using-pickle-wont-predict-text-values-properly
How to create word embedding using Word2Vec on Python?,"<p>I have seen many tutorials online on how to use Word2Vec (gensim).</p>
<p>Most tutorials are showing on how to find the <code>.most_similar</code> word or similarity between two words.</p>
<p>But, how if I have text data <code>X</code> and I want to produce the word embedding vector <code>X_vector</code>?</p>
<p>So that, this <code>X_vector</code> can be used for classification algorithms?</p>
","python, gensim, word2vec, text-classification, word-embedding","<p>If <code>X</code> is a word (string token), you can look up its vector with <code>word_model[X]</code>.</p>
<p>If <code>X</code> is a text - say, a list-of-words – well, a <code>Word2Vec</code> model only has vectors for words, not texts.</p>
<p>If you have some desired way to use a list-of-words plus per-word-vectors to create a text-vector, you should apply that yourself. There are many potential approaches, some simple, some complicated, but no one 'official' or 'best' way.</p>
<p>One easy popular baseline (a fair starting point especially on very small texts like titles) is to average together all the word vectors. That can be as simple as (assuming <code>numpy</code> is imported as <code>np</code>):</p>
<pre><code>np.mean([word_model[word] for word in word_list], axis=0)
</code></pre>
<p>But, recent versions of Gensim also have a convenience <code>.get_mean_vector()</code> method for averaging together sets of vectors (specified as their word-keys, or raw vectors), with some other options:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector</a></p>
",2,0,1877,2022-05-23 09:42:48,https://stackoverflow.com/questions/72346412/how-to-create-word-embedding-using-word2vec-on-python
huggingface longformer case sensitive tokenizer,"<p>This <a href=""https://jesusleal.io/2020/11/24/Longformer-with-IMDB/"" rel=""nofollow noreferrer"">page</a> shows how to build a longformer based classification.</p>
<pre><code>import pandas as pd
import datasets
from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig
import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm
import wandb
import os


# load model and tokenizer and define length of the text sequence
model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',
                                                           gradient_checkpointing=False,
                                                           attention_window = 512)
tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)
</code></pre>
<p>I noticed that the tokenizer is sensitive to case of the data. words <code>do</code> and <code>Do</code> get different tokens below. I dont need such behavior. I can always lowercase my data before feeding to longformer. But is there is any other better way to tell tokenizer to ignore case of the data?</p>
<pre><code>encoded_input = tokenizer(&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;)
print(encoded_input)
{'input_ids': [0, 8275, 45, 31510, 459, 11, 5, 5185, 9, 44859, 6, 13, 51, 32, 12405, 8, 2119, 7, 6378, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

encoded_input3 = tokenizer(&quot;do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;)
print(encoded_input3)
{'input_ids': [0, 5016, 45, 31510, 459, 11, 5, 5185, 9, 44859, 6, 13, 51, 32, 12405, 8, 2119, 7, 6378, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
","python, nlp, huggingface-transformers, text-classification, huggingface-tokenizers","<p>In my opinion, it is better not to modify tokenization schemes of pretrained transformers: they were pretrained with a certain vocabulary, and changing it may lead to their deterioration.</p>
<p>If you still want to make the system insensitive to capitalization, and if you want to use the same model and the same tokenizer without modifying them too much, then just lowercasing the input data is a sensible strategy.</p>
<p>However, if you are willing to spend resources on updating the model and the tokenizer, you can do the following:</p>
<ol>
<li>Modify the tokenizer: add a Lowercase <a href=""https://huggingface.co/docs/tokenizers/components#normalizers"" rel=""nofollow noreferrer"">normalizer</a> into its pipeline.</li>
<li>(optionally) Modify the vocabulary of the tokenizer: drop the unused words that contain uppercase characters, and maybe add some lowercase words. The embedding and output layers of the model should be modified accordingly, and there is no standardized code for this, so such manipulations are not recommended unless you understand well what you are doing. Still, such manipulations could improve model performance.</li>
<li>Fine-tune the model (with the original masked language modelling task) on a large dataset using the updated tokenizer. This will make the neural network better aware of the new lowercase texts that it may discover.</li>
</ol>
<p>This will make the system better adapted to uncased texts, but it will cost you time (to write the code for updating the vocabulary) and computational resources (to fine-tune the model).</p>
",2,1,1075,2022-05-23 23:15:31,https://stackoverflow.com/questions/72355671/huggingface-longformer-case-sensitive-tokenizer
Implement metrics using XLMRoBERTa model for text classification,"<p>I have created script for binary (0 and 1) text classification using XLM-ROBERTa model. I would like to put metrics (as Binary Cross-Entropy) but also early stopping with patience of 15.</p>
<p>But I have a problem. I tried to use the path <code>model.compile</code> and <code>model.fit</code>, but XLM-RoBertaForSequenceClassification doesn't have these parameters. I would't like to use Argumentation. It is possible to find some solution?</p>
<p>Already I use AdamW. Finally it is possible to get for each epoch parameters as recall, f1, accuracy? At the moment I get only last data of the last epoch.</p>
<p>Below I put the script during training:</p>
<pre><code>from transformers import XLMRobertaForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = XLMRobertaForSequenceClassification.from_pretrained(
    &quot;xlm-roberta-base&quot;, # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)

# Tell pytorch to run this model on the GPU.
#model.cuda()
model.to(device)
</code></pre>
<p>Here start the training!</p>
<pre><code>import random
import numpy as np
import gc
seed_val = 45
epochs = 15
    # Set the seed value all over the place to make this reproducible.
    random.seed(seed_val)
    np.random.seed(seed_val)
    torch.manual_seed(seed_val)
    torch.cuda.manual_seed_all(seed_val)
    # Store the average loss after each epoch so we can plot them.
    loss_values = []
    training_stats = []
    # Measure how long the training epoch takes.
    total_t0 = time.time()
    # For each epoch...
    for epoch in range(0, epochs):
      print(&quot;&quot;)
        print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))
        stacked_val_labels = []
        targets_list = []
        # ========================================
        #               Training
        # ========================================
        print('Training...')
        # put the model into train mode
        model.train()
        
        # This turns gradient calculations on and off.
        torch.set_grad_enabled(True)
    
        # Measure how long the training epoch takes.
        t0 = time.time()
    
        # Reset the total loss for this epoch.
        total_train_loss = 0
    
        for i, batch in enumerate(train_dataloader):
            train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))
            print(train_status, end='\r')
        
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)
    
            model.zero_grad()        
    
            outputs = model(b_input_ids, 
                        attention_mask=b_input_mask,
                        labels=b_labels)
            
            # Get the loss from the outputs tuple: (loss, logits)
            loss = outputs[0]
            
            # Convert the loss from a torch tensor to a number.
            # Calculate the total loss.
            total_train_loss = total_train_loss + loss.item()
            
            # Zero the gradients
            optimizer.zero_grad()
            # Perform a backward pass to calculate the gradients.
            loss.backward()
            
            # Clip the norm of the gradients to 1.0.
            # This is to help prevent the &quot;exploding gradients&quot; problem.
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            # Use the optimizer to update the weights.
            
            # Optimizer for GPU
            optimizer.step() 
            
            # Optimizer for TPU
            # https://pytorch.org/xla/
            #xm.optimizer_step(optimizer, barrier=True)
            # Measure how long this epoch took.
            training_time = format_time(time.time() - t0)
        print(&quot;&quot;)
        print('Train loss:' ,total_train_loss)
        print(&quot;  Training epcoh took: {:}&quot;.format(training_time))
     
        # ========================================
        #               Validation
        # ========================================
        print('\nValidation...')
    
        # Measure how long the training epoch takes.
        t0 = time.time()
    
        # Put the model in evaluation mode.
        model.eval()
    
        # Turn off the gradient calculations.
        # This tells the model not to compute or store gradients.
        # This step saves memory and speeds up validation.
        torch.set_grad_enabled(False)
          
        # Reset the total loss for this epoch.
        total_val_loss = 0
       
        for j, batch in enumerate(val_dataloader):
            
            val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))
            
            print(val_status, end='\r')
    
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)      
       
            outputs = model(b_input_ids, 
                    attention_mask=b_input_mask, 
                    labels=b_labels)
            
            # Get the loss from the outputs tuple: (loss, logits)
            loss = outputs[0]
            
            # Convert the loss from a torch tensor to a number.
            # Calculate the total loss.
            total_val_loss = total_val_loss + loss.item()
        
            # Get the preds
            preds = outputs[1]
       
            # Move preds to the CPU
            val_preds = preds.detach().cpu().numpy()
            
            # Move the labels to the cpu
            targets_np = b_labels.to('cpu').numpy()
    
            # Append the labels to a numpy list
            targets_list.extend(targets_np)
    
            if j == 0:  # first batch
                stacked_val_preds = val_preds
    
            else:
                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))
    
        # Calculate the validation accuracy
        y_true = targets_list
        y_pred = np.argmax(stacked_val_preds, axis=1)
        
        val_acc = accuracy_score(y_true, y_pred)
        # Measure how long the validation run took.
        validation_time = format_time(time.time() - t0)
        
        print('Val loss:' ,total_val_loss)
        print('Val acc: ', val_acc)
        print(&quot;  Validation took: {:}&quot;.format(validation_time))
        # Record all statistics from this epoch.
        #training_stats = []
        training_stats.append(
            {
                'epoch': epoch + 1,
                'Training Loss': total_train_loss,
                'Valid. Loss': total_val_loss,
                'Valid. Accur.': val_acc,
                'Training Time': training_time,
                'Validation Time': validation_time
            }
        )
    
    print(&quot;&quot;)
    print(&quot;Training complete!&quot;)
    
    print(&quot;Total training took {:} (h:mm:ss)&quot;.format(format_time(time.time()-total_t0)))
    
    # Save the Model
    torch.save(model.state_dict(), '/content/drive/MyDrive/model/model.pt')
        
    # Use the garbage collector to save memory.
    gc.collect()
</code></pre>
","python, text-classification","<p><code>XLMRobertaForSequenceClassification</code> and other classes of the &quot;ForSequenceClassification&quot; family assume classification into multiple classes and use categorical cross-entropy as the loss function. The class is just a lightweight wrapper of the <code>XLMRoberta</code> class.</p>
<p>If you want to use specifically binary cross-entropy, you can either make your own wrapper with a single class output and binary cross-entropy, or you can do the loss computation in the training loop in your code snippet. I.e., instead of using <code>outputs[0]</code>, use the logits <code>outputs[1]</code> as an input to the loss function.</p>
<p>Regarding other metrics, you have the logits in the <code>outputs</code> variable. It should be enough to compute whatever metric you find useful for your task.</p>
",1,0,439,2022-06-06 09:57:23,https://stackoverflow.com/questions/72515966/implement-metrics-using-xlmroberta-model-for-text-classification
KeyError on a certain word,"<p>I am trying to use Naive Bayes for spam-ham classification.</p>
<pre><code>training_set['E_Mail'] = training_set['E_Mail'].str.split()
vocabulary = []
for email in training_set['E_Mail']:
 for word in email:
     vocabulary.append(tuple(word))

vocabulary = list(set(vocabulary))


word_counts_per_email = {unique_word: [0] * len(training_set['E_Mail']) for unique_word in vocabulary}

for index, email in enumerate(training_set['E_Mail']):
 for word in email:
   word_counts_per_email[word][index] += 1
</code></pre>
<p>I am getting a word error repeteadly on here:</p>
<pre><code>word_counts_per_email = {unique_word: [0] * len(training_set['E_Mail']) for unique_word in vocabulary}

for index, email in enumerate(training_set['E_Mail']):
 for word in email:
   word_counts_per_email[word][index] += 1
</code></pre>
<p>The error message is just this:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-30-1706354aaff0&gt; in &lt;module&gt;()
     3 for index, email in enumerate(training_set['E_Mail']):
     4   for word in email:
----&gt; 5     word_counts_per_email[word][index] += 1

KeyError: 'hafta'
</code></pre>
<p>'hafta' is the first word of the pandas dataframe and the trainng dataset.</p>
<p>I tried the solution on <a href=""https://stackoverflow.com/questions/46614713/keyerror-on-the-same-word"">this</a> issue that seemed similar to mine but it didn't work out.</p>
<p>I will appreciate any hint to get this over, thank you.</p>
","python, nlp, text-classification, naivebayes, non-english","<p>My guess is that this line <code>vocabulary.append(tuple(word))</code> should be changed to <code>vocabulary.append(word)</code> since your version might put letters instead of words into <code>vocabulary</code> and therefore <code>word_counts_per_email</code>.</p>
<p>In case this doesn't work, I suggest looking into contents of <code>vocabulary</code>/ <code>word_counts_per_email</code> so you can determine what went wrong.</p>
",0,-1,396,2022-06-10 13:07:55,https://stackoverflow.com/questions/72574801/keyerror-on-a-certain-word
How to classify new data using a pre-trained model - Python Text Classification (NLTK and Scikit),"<p>I am very new to Text Classification and I am trying to classify each line of a dataset composed by twitter comments according to some pre-defined topics.</p>
<p>I have used the code bellow in Jupyter Notebook to build and train a model with a training dataset. I chose to use a supervised approach in python with NLTK and Scikit, as unsupervised ones (like LDA) were not giving me good results.</p>
<p>I followed these steps so far:</p>
<ol>
<li>Mannually categorised the topics of a training dataset;</li>
<li>Applied the training dataset to the code bellow and trained it resulting in an accuracy of aprox. 82%.</li>
</ol>
<p><strong>Now, I want to use this model to automatically categorise the topics of another dataset</strong> (i.e., my test dataset). Most posts only cover the training part, so it's quite frustraiting for a newcommer to understand how to get the trained model and actually use it.</p>
<p>Hence, the question is: <strong>with the code below, how can I now use the trained model to classify a new dataset?</strong></p>
<p>I appreciate your help.</p>
<p>This tutorial is very good, and I used it as a reference for the code below: <a href=""https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b"" rel=""nofollow noreferrer"">https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b</a></p>
<p>My model building and training code:</p>
<pre><code>#Do library and methods import

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from nltk.tokenize import RegexpTokenizer
from nltk import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk as nltk
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
import regex as re
import requests


# Import dataset

df = pd.read_csv(r'C:\Users\user_name\Downloads\Train_data.csv', delimiter=';')


# Tokenize

def tokenize(x):
 tokenizer = RegexpTokenizer(r'\w+')
 return tokenizer.tokenize(x)
df['tokens'] = df['Tweet'].map(tokenize)


# Stem and Lemmatize

nltk.download('wordnet')
nltk.download('omw-1.4')

def stemmer(x):
 stemmer = PorterStemmer()
 return ' '.join([stemmer.stem(word) for word in x])
 
def lemmatize(x):
 lemmatizer = WordNetLemmatizer()
 return ' '.join([lemmatizer.lemmatize(word) for word in x])
df['lemma'] = df['tokens'].map(lemmatize)
df['stems'] = df['tokens'].map(stemmer)


# set up feature matrix and target column

X = df['lemma']
y = df['Topic']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)


# Create out pipeline with a vectorizer and our naive Bayes classifier

pipe_mnnb = Pipeline(steps = [('tf', TfidfVectorizer()), ('mnnb', MultinomialNB())])


# Create parameter grid

pgrid_mnnb = {
 'tf__max_features' : [1000, 2000, 3000],
 'tf__stop_words' : ['english', None],
 'tf__ngram_range' : [(1,1),(1,2)],
 'tf__use_idf' : [True, False],
 'mnnb__alpha' : [0.1, 0.5, 1]
}


# Set up the grid search and fit the model

gs_mnnb = GridSearchCV(pipe_mnnb,pgrid_mnnb,cv=5,n_jobs=-1)
gs_mnnb.fit(X_train, y_train)


# Check the score

gs_mnnb.score(X_train, y_train)
gs_mnnb.score(X_test, y_test)


# Check the parameters

gs_mnnb.best_params_


# Get predictions

preds_mnnb = gs_mnnb.predict(X)
df['preds'] = preds_mnnb


# Print resulting dataset

print(df.shape)
df.head()
</code></pre>
","python, scikit-learn, nltk, text-classification, nltk-trainer","<p>It seems than after training you just have to do as for your validation step using directly the grid-searcher, which in sklearn library is also used after training as a model taking the best found hyperparameters.
So take a X which is what you want to evaluate and run</p>
<pre><code>preds_mnnb = gs_mnnb.predict(X)
</code></pre>
<p>preds_mnnb should contain what you expect</p>
",0,1,849,2022-06-28 09:34:31,https://stackoverflow.com/questions/72784032/how-to-classify-new-data-using-a-pre-trained-model-python-text-classification
Rule based Classification based on a dictionary,"<p>Apologies in advance if this has already been questioned/answered, but I couldn't find any answer close to my problem. I am also somewhat new to Python, so sorry for any wrong formatting or wording.  I am struggling with a task to label/classify a list of documents based on a list of dictionary.
The list of dictionary looks like this:</p>
<pre class=""lang-none prettyprint-override""><code>| Category | Keywords       |
| -------- | -------------- |
| Reg      | A, B, C        |
| Gov      | D,E,F,G        |


Document1 = [Sentences1, Sentences 2,...]
Document2 = [Sentences3, Sentences 4,...]
</code></pre>
<p>I would like to label Documents per Category if keywords belong to this category appear in the document. I have searched through stackoverflow and github but have not found similar problem.</p>
","python, text-classification, topic-modeling","<p>Your question seems to ask this:</p>
<ul>
<li>For a collection of documents, each with multiple sentences, mark each document as belonging to one or more categories if, for any of the given categories, one or more sentences in the document contains at least one keyword from the category.</li>
</ul>
<p>Here is a way to do that:</p>
<pre class=""lang-py prettyprint-override""><code>categories = {'Reg':['A', 'B', 'C'], 'Gov':['D', 'E', 'F', 'G']}
Document1 = ['A quick brown fox.', 'He got a B in calculus.']
Document2 = ['My next job will be in the C suite.', 'You can get there on the D train.']
docs = {'Doc1':Document1, 'Doc2':Document2}
docsByCategory = {}
for doc, sentences in docs.items():
    cats = set()
    for sentence in sentences:
        words = sentence.split()
        for category, keywords in categories.items():
            if any(keyword in words for keyword in keywords):
                cats.add(category)
    docsByCategory[doc] = list(cats)
print(docsByCategory)
</code></pre>
<p>Output:</p>
<pre><code>{'Doc1': ['Reg'], 'Doc2': ['Gov', 'Reg']}
</code></pre>
<p>Explanation:</p>
<ul>
<li>Iterate over the document names and contents using <code>docs.items()</code></li>
<li>Iterate over the list of sentences the make up each document's contents</li>
<li>Split each sentence into words</li>
<li>Iterate over the category names and keywords using <code>categories.items()</code></li>
<li>Use <code>any()</code> with a comprehension (loop) over keywords to see if any of them are found in the sentence's words, in which case add the category name to the variable <code>cats</code> which is a <code>set</code>, so that multiple calls to <code>add()</code> for the same category will result in only a single entry in <code>cats</code> for that category</li>
<li>Convert <code>cats</code> to a list and add it to the result dictionary <code>docsByCategory</code> for the current document name.</li>
</ul>
<p>An alternative solution is this:</p>
<pre><code>categories = {'Reg':['A', 'B', 'C'], 'Gov':['D', 'E', 'F', 'G']}
Document1 = ['A quick brown fox.', 'He got a B in calculus.']
Document2 = ['My next job will be in the C suite.', 'You can get there on the D train.']
docs = {'Doc1':Document1, 'Doc2':Document2}
docsByCategory = {}
for doc, sentences in docs.items():
    sentenceWordSets = [set(sentence.split()) for sentence in sentences]
    cats = set()
    for category, keywords in categories.items():
        keywordSet = set(keywords)
        if any(wordSet &amp; keywordSet for wordSet in sentenceWordSets):
            cats.add(category)
    docsByCategory[doc] = list(cats)
print(docsByCategory)
</code></pre>
",2,1,810,2022-07-02 17:40:50,https://stackoverflow.com/questions/72841219/rule-based-classification-based-on-a-dictionary
Merging text and image keras layers not working,"<p>Please judge me tender. I'm trying to concatenate two inputs, one for images and one for text.</p>
<p>I'm not an expert and I'm new with the functional API, so it's hard for me to identify the problem here.</p>
<p>In the code below, I confirmed that I can train both text_features and image_features models, but when I try to train the end to end model it retrieves the error:</p>
<pre><code>ValueError: Failed to find data adapter that can handle input: (&lt;class 'dict'&gt; containing {&quot;&lt;class 'str'&gt;&quot;} keys and {&quot;&lt;class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'&gt;&quot;} values), &lt;class 'NoneType'&gt;
</code></pre>
<p>I can imagine that I'm facing a rather basic problem, but the thing is that I couldn't find a simple example where both images and text are used, so I can't see where it is.</p>
<p>I will copy the entire code I use and will try to comment on each step so this doesn't becomes a general debugging issue.</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import re
import string
</code></pre>
<p>First, I define a common batch size and seed for both the text and images datasets. Both my images and text files are saved in a single set of 25 folders.</p>
<p>Lets say folder one has a file called sample_1.png. It also has a file called sample_1.txt, which correspond to the text associated with the said image, stored as a single string (using json).</p>
<pre><code>batch_size = 32
seed = 42
</code></pre>
<p>Then, I load the text data. Here, I try to follow this example:  <a href=""https://www.tensorflow.org/tutorials/keras/text_classification"" rel=""nofollow noreferrer"">basic text classification without recurrent layers</a>. The only difference is that my output is not binary.</p>
<pre><code>raw_text_train_ds =tf.keras.utils.text_dataset_from_directory(
    'NEURAL', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
raw_text_val_ds = tf.keras.utils.text_dataset_from_directory(
    'NEURAL', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)
</code></pre>
<p>I follow the processing steps of the referenced example, except that I've previously treated my text for punctuation and similar.</p>
<pre><code>max_features = 7000
sequence_length = 250    
vectorize_layer = layers.TextVectorization(
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)
train_text = raw_text_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)
def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label
text_train_ds = raw_text_train_ds.map(vectorize_text)
text_val_ds = raw_text_val_ds.map(vectorize_text)
</code></pre>
<p>Before applying the AUTOTUNE part of the mentioned example, I upload the image dataset, trying to follow this example: <a href=""https://www.tensorflow.org/tutorials/images/classification"" rel=""nofollow noreferrer"">image classification with augmentation layer</a></p>
<pre><code>img_height = 180
img_width = 180
img_train_ds = tf.keras.utils.image_dataset_from_directory(
  'NEURAL',
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=seed,
  image_size=(img_height, img_width),
  batch_size=batch_size)
img_val_ds = tf.keras.utils.image_dataset_from_directory(
  'NEURAL',
  validation_split=0.2,
  subset=&quot;validation&quot;,
  seed=seed,
  image_size=(img_height, img_width),
  batch_size=batch_size)
</code></pre>
<p>I wonder if applyoing the following data augmantation layer is causing some sort of missmatch, but I don't think so. Again, I'm pretty sure my mistake is more basic than anything else.</p>
<pre><code>data_augmentation = keras.Sequential(
  [
    layers.RandomRotation(0.04,
                         input_shape=(img_height,
                                      img_width,
                                      3)),
    layers.RandomZoom(0.1),
  ]
)
</code></pre>
<p>As both referenced examples recommend to apply the folowing AUTOTUNING, I do it for both data sets at once.</p>
<pre><code>AUTOTUNE = tf.data.AUTOTUNE

text_train_ds = text_train_ds.cache().prefetch(buffer_size=AUTOTUNE)
text_val_ds = text_val_ds.cache().prefetch(buffer_size=AUTOTUNE)
# test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)
img_train_ds = img_train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
img_val_ds = img_val_ds.cache().prefetch(buffer_size=AUTOTUNE)
</code></pre>
<p>Here I define two models, as they are defined, again, in the examples I tried to follow, but trying to adapt them to the API approach.</p>
<pre><code>num_classes = 25 


text_input = keras.Input(shape=(None,), name=&quot;text&quot;)  
text_features = layers.Embedding(max_features+1, 16)(text_input)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32)(text_features)
text_features = keras.Model(text_input,text_features)

image_input = keras.Input(shape=(180, 180, 3),name=&quot;image&quot;)
image_features=data_augmentation(image_input)
image_features=layers.Rescaling(1./255)(image_features)
image_features=layers.Conv2D(16, 3, padding='same', activation='relu')(image_features)
image_features=layers.MaxPooling2D()(image_features)
image_features=layers.Conv2D(32, 3, padding='same', activation='relu')(image_features)
image_features= layers.MaxPooling2D()(image_features)
image_features=layers.Conv2D(64, 3, padding='same', activation='relu')(image_features)
image_features=layers.MaxPooling2D()(image_features)
image_features=layers.Dropout(0.2)(image_features)
image_features=layers.Flatten()(image_features)
image_features=layers.Dense(128, activation='relu')(image_features)
image_features=layers.Dense(32, activation='relu')(image_features)
image_features=keras.Model(image_input,image_features)

x = layers.concatenate([text_features.output, image_features.output])
category_pred = layers.Dense(num_classes, name=&quot;classes&quot;)(x)


model = keras.Model(
    inputs=[text_input, image_input],
    outputs=[category_pred],)
</code></pre>
<p>I tried with different loss, metrics and optimizers, just to try my way out of the problem.</p>
<p>I feel like it's maybe a semantic problem, as the error suggest (remember, not an expert here) that the model doesn't understands what I'm trying to introduce as an input. but this is how inputs are introduced in the examples I strudied, so I'm lost.</p>
<pre><code>model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
epochs = 1
checkpoint_path = &quot;training_3_test_ojo/cp.ckpt&quot;
checkpoint_dir = os.path.dirname(checkpoint_path)
model.fit(
    {'image':img_train_ds,
     'text':text_train_ds,
     },
    epochs=epochs,
    batch_size=32,)
</code></pre>
<p>The problem could be that I naively thought I can just load my two data sets independently and expect my model would find the way to concatenate it.</p>
<p>I'm not specifying the expected output for trainning, again, assuming that the model will extract it from the inputs. But I tried to specify it and it didn't make any difference. I would also vote for this as the problem of my code. Not specifying the expected output works for the 'image classification' example I used, but I do realize it doesn't have to work for a model with multiple inputs.</p>
<p>I will appreciate any solution, guidance or reference.</p>
","python, tensorflow, concatenation, text-classification, image-classification","<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">As per the documentation on <code>fit()</code></a>, if you're passing a dictionary, the keys need to point to an array or tensor. You're using <code>tensorflow.python.data.ops.dataset_ops.PrefetchDataset</code>, which won't work with dict.</p>
",1,0,446,2022-07-18 18:47:18,https://stackoverflow.com/questions/73027195/merging-text-and-image-keras-layers-not-working
How to print a long output to a file without &quot;...&quot; in the output in Python?,"<p>I'm working on a text classification problem via Python. I tried to get the numerical weightage of words using TF-IDF. Here is my code with an example data:</p>
<pre><code>from collections import Counter
from tqdm import tqdm
from scipy.sparse import csr_matrix
import math
import operator
from sklearn.preprocessing import normalize
import numpy as np 
import pandas as pd

corpus = ['this is the first document',
      'this document is the second document',
      'and this is the third one',
      'is this the first document',
]

def IDF(corpus, unique_words):
  idf_dict={}
  N=len(corpus)
  for i in unique_words:
    count=0
    for sen in corpus:
      if i in sen.split():
        count=count+1
      idf_dict[i]=(math.log((1+N)/(count+1)))+1
  return idf_dict

def fit(whole_data):
  # global vocab
  unique_words = set()
  if isinstance(whole_data, (list,)):
    for x in whole_data:
      for y in x.split():
        if len(y)&lt;2:
          continue
        unique_words.add(y)
    unique_words = sorted(list(unique_words))
    vocab = {j:i for i,j in enumerate(unique_words)}
    Idf_values_of_all_unique_words=IDF(whole_data,unique_words)
  return vocab, Idf_values_of_all_unique_words
Vocabulary, idf_of_vocabulary=fit(corpus) 


def transform(dataset,vocabulary,idf_values):
     sparse_matrix= csr_matrix( (len(dataset), len(vocabulary)), dtype=np.float64)
     for row  in range(0,len(dataset)):
       number_of_words_in_sentence=Counter(dataset[row].split())
       for word in dataset[row].split():
           if word in  list(vocabulary.keys()):
               tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])
               sparse_matrix[row,vocabulary[word]]=tf_idf_value
    #  print(&quot;NORM FORM\n&quot;,normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False))
     output =normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)
     return output
final_output=transform(corpus,Vocabulary,idf_of_vocabulary)
</code></pre>
<p>Up to now, everything is OK. But I want to print the output to a txt file. Here is the code I'm using:</p>
<pre><code>with open('satinal22.txt', 'w') as f:
  for i in range(len(corpus)):
    f.write(str(final_output[i].toarray()))
</code></pre>
<p>For a small data, it gives this output in the file (arrays in the size of 9 numbers):</p>
<p>[[0.         0.46979139 0.58028582 0.38408524 0.         0.
0.38408524 0.         0.38408524]][[0.         0.6876236  0.         0.28108867 0.         0.53864762
0.28108867 0.         0.28108867]][[0.51184851 0.         0.         0.26710379 0.51184851 0.
0.26710379 0.51184851 0.26710379]][[0.         0.46979139 0.58028582 0.38408524 0.         0.
0.38408524 0.         0.38408524]]</p>
<p>However, for large data (3000 number long array), the output is problematic, like:</p>
<p>[[0. 0. 0. ... 0. 0. 0.]][[0. 0. 0. ... 0. 0. 0.]][[0. 0. 0. ... 0. 0. 0.]][[0. 0. 0. ... 0. 0. 0.]][[0. 0. 0. ... 0. 0. 0.]][[0. 0. 0. ... 0. 0. 0.]][[0. 0. 0. ... 0. 0. 0.]] (goes on)</p>
<p>How can I print the full output to a txt file without &quot;...&quot;?</p>
","python, arrays, text-classification","<p>Try this.</p>
<pre><code>import sys
import numpy
numpy.set_printoptions(threshold=sys.maxsize)
</code></pre>
<p>Reference: <a href=""https://stackoverflow.com/questions/1987694/how-to-print-the-full-numpy-array-without-truncation"">How to print the full NumPy array, without truncation?</a></p>
",0,0,194,2022-07-19 06:45:31,https://stackoverflow.com/questions/73032281/how-to-print-a-long-output-to-a-file-without-in-the-output-in-python
TextClassification/ Extraction from image How to get single text frame and string Using Core ML from a Image,"<p>Need to mark the rec boxes around string and then to get that string after tapping</p>
","ios, swift, text-classification, coreml, apple-vision","<pre><code>import UIKit
import Vision


class ViewController: UIViewController, ImageGet {

//MARK: OUTLETS

@IBOutlet weak var selectButton: UIButton!

//MARK: VARIABLES
var objU = UtilityClass()
var image:UIImage?
var str:String?
var uiButton : UIButton?
var arrayString = [String]()
var imageView : UIImageView = UIImageView()

//MARK: DELEGATE FUNCTION

func img(image: UIImage) {
    self.image = image
    imageView.image = image
    setUp()
}

override func viewDidLoad() {
    super.viewDidLoad()
    imageView.isUserInteractionEnabled = true
    // Do any additional setup after loading the view.
}

//MARK: SETUPUI
func setUp() {
    let realImg =  resizeImage(image: (imageView.image!) , targetSize:CGSize(width: view.frame.width, height: view.frame.height) )
    self.image = realImg
    self.imageView .image = self.image
    imageView.isUserInteractionEnabled = true
    self.imageView.frame = CGRect(x: 0, y: 0, width: realImg.size.width, height: realImg.size.height)
    view.addSubview(imageView)
    guard let cgimg = realImg.cgImage else {return}
    let requestHandler = VNImageRequestHandler(cgImage: cgimg)
    let req = VNRecognizeTextRequest(completionHandler: recognizeTextHandler)
    req.recognitionLevel = .accurate
    do {
        try requestHandler.perform([req])
    } catch {
        print(&quot;Unable to perform the request: \(error)&quot;)
    }
}

//MARK: SELECT THE IMAGE
@IBAction func selectButtontapped(_ sender: Any) {
    objU.delegate = self
    objU.obj = self
    objU.ImageGet()
}

  func recognizeTextHandler(request : VNRequest , error:Error?) {
    guard let observation = request.results as? [VNRecognizedTextObservation], error == nil else {
        return
    }
    _ = observation.compactMap({
        $0.topCandidates(1).first?.string
    }).joined(separator: &quot;/n&quot;)
    
    for subView in imageView.subviews {
        subView.removeFromSuperview()
    }
    
    let boundingRect :[CGRect]  = observation.compactMap{
        observation in
        guard let candidate = observation.topCandidates(1).first else {return .zero}
        //find the bounding box observation
        let stringRange = candidate.string.startIndex..&lt;candidate.string.endIndex
        let boxObservation = try? candidate.boundingBox(for: stringRange)
        let boundingBox = boxObservation?.boundingBox ?? .zero
        str = candidate.string
        self.arrayString.append(str!)
        let rectInImg = VNImageRectForNormalizedRect(boundingBox, Int((imageView.frame.size.width)), Int((imageView.frame.size.height)))
        let convertedRect = self.getConvertedRect(boundingBox: observation.boundingBox, inImage:image!.size , containedIn: (imageView.bounds.size))
        drawBoundBox(rect: convertedRect)
        return rectInImg
    }
    print(arrayString)
    print(boundingRect)
    
}
func drawBoundBox(rect: CGRect) {
    uiButton = UIButton(type: .custom)
    uiButton?.frame = rect
    uiButton?.layer.borderColor = UIColor.systemPink.cgColor
    uiButton?.setTitle(&quot;&quot;, for: .normal)
    uiButton?.layer.borderWidth = 2
    uiButton?.tag = arrayString.count
    imageView.addSubview(uiButton ?? UIButton())
    uiButton?.addTarget(self, action: #selector(pressed(_:)), for: .touchUpInside)
}

@objc func pressed(_ sender : UIButton) {
    alert(key: arrayString[sender.tag - 1])
    
}

//MARK: CONVERT THE NORMALISED BOUNDING RECT

func getConvertedRect(boundingBox: CGRect, inImage imageSize: CGSize, containedIn containerSize: CGSize) -&gt; CGRect {
    
    let rectOfImage: CGRect
    
    let imageAspect = imageSize.width / imageSize.height
    let containerAspect = containerSize.width / containerSize.height
    
    if imageAspect &gt; containerAspect { /// image extends left and right
        let newImageWidth = containerSize.height * imageAspect /// the width of the overflowing image
        let newX = -(newImageWidth - containerSize.width) / 2
        rectOfImage = CGRect(x: newX, y: 0, width: newImageWidth, height: containerSize.height)
        
    } else { /// image extends top and bottom
        let newImageHeight = containerSize.width * (1 / imageAspect) /// the width of the overflowing image
        let newY = -(newImageHeight - containerSize.height) / 2
        rectOfImage = CGRect(x: 0, y: newY, width: containerSize.width, height: newImageHeight)
    }
    
    let newOriginBoundingBox = CGRect(
        x: boundingBox.origin.x,
        y: 1 - boundingBox.origin.y - boundingBox.height,
        width: boundingBox.width,
        height: boundingBox.height
    )
    
    var convertedRect = VNImageRectForNormalizedRect(newOriginBoundingBox, Int(rectOfImage.width), Int(rectOfImage.height))
    
    /// add the margins
    convertedRect.origin.x += rectOfImage.origin.x
    convertedRect.origin.y += rectOfImage.origin.y
    
    return convertedRect
    
}

//MARK: RESIZE THE IMAGE ACCORD TO DEVICE
func resizeImage(image: UIImage, targetSize: CGSize) -&gt; UIImage {
    let size = image.size
    
    let widthRatio  = targetSize.width  / image.size.width
    let heightRatio = targetSize.height / image.size.height
    
    // Figure out what our orientation is, and use that to form the rectangle
    var newSize: CGSize
    if(widthRatio &gt; heightRatio) {
        newSize = CGSize(width: size.width * heightRatio, height: size.height * heightRatio)
    } else {
        newSize = CGSize(width: size.width * widthRatio,  height: size.height * widthRatio)
    }
    
    // This is the rect that we've calculated out and this is what is actually used below
    let rect = CGRect(x: 0, y: 0, width: newSize.width, height: newSize.height)
    
    // Actually do the resizing to the rect using the ImageContext stuff
    UIGraphicsBeginImageContextWithOptions(newSize, false, 1.0)
    image.draw(in: rect)
    let newImage = UIGraphicsGetImageFromCurrentImageContext()
    UIGraphicsEndImageContext()
    
    return newImage!
}

//MARK: POPPING ALERT WITH STRING
func alert(key:String){
    let alertController = UIAlertController(title: &quot;String&quot;, message: key, preferredStyle: .alert)
    let OKAction = UIAlertAction(title: &quot;OK&quot;, style: .default) {
        (action: UIAlertAction!) in
        // Code in this block will trigger when OK button tapped.
       
    }
    let copyAction = UIAlertAction(title: &quot;Copy&quot;, style: .default) {
        (action: UIAlertAction!) in
        UIPasteboard.general.string = key
       
    }
    alertController.addAction(copyAction)
    alertController.addAction(OKAction)
    self.present(alertController, animated: true, completion: nil)
}
</code></pre>
<p>}</p>
",0,0,173,2022-07-26 04:53:17,https://stackoverflow.com/questions/73117890/textclassification-extraction-from-image-how-to-get-single-text-frame-and-strin
How to get the word on which the text classification has been made?,"<p>I am doing a multi-label text classification using a pre-trained model of BERT. Here is an example of the prediction that has been made for one sentence-
<a href=""https://i.sstatic.net/phwu9.jpg"" rel=""nofollow noreferrer"">pred_image</a></p>
<p>I want to get those words from the sentence on which the prediction has been made. Like this one - <a href=""https://i.sstatic.net/Xc4ij.jpg"" rel=""nofollow noreferrer"">right_one</a></p>
<p>If anyone has any idea, Please enlighten me.</p>
","nlp, text-classification, bert-language-model, multilabel-classification","<p>Multi-Label <a href=""https://huggingface.co/tasks/text-classification"" rel=""nofollow noreferrer"">Text Classification</a> (first image) and <a href=""https://huggingface.co/tasks/token-classification"" rel=""nofollow noreferrer"">Token Classification</a> (second image) are two different tasks for each which the model needs to be specifally trained for.</p>
<p>The first one returns a probability for each label considering the entire sentence. The second returns such predictions for each single word in the sentence while usually considering the rest of the sentence as context.</p>
<p>So you can not really use the output from a Text Classifier and use it for Token Classification because the information you get is not detailed enough.</p>
<p>What you can and should do is train a Token Classification model, although you obviously will need token-level-annotated data to do so.</p>
",0,0,261,2022-07-29 04:29:13,https://stackoverflow.com/questions/73161662/how-to-get-the-word-on-which-the-text-classification-has-been-made
Selection of the three largest values in the numpy table,"<p>I am doing a text classification, I want to use the class probability for the three classes that have the highest probabilities.I need your help. Thanks</p>
<pre><code>import numpy as np
probability = get_predict_proba(X)
print(probability)
</code></pre>
<p>[[0.15682828 0.11664342 0.11088368 0.12925814 0.09544043 0.10655934
0.14538805 0.13899866]]</p>
","python, machine-learning, logistic-regression, text-classification","<p>This:</p>
<pre><code>np.argsort(probability)[-3:] # 3 'best' classes
probability[np.argsort(probability)[-3:]] # 3 'best' probabilities
</code></pre>
<p>(<code>np.argsort</code> gives you the sorted indices.)</p>
",0,-1,33,2022-08-08 09:18:05,https://stackoverflow.com/questions/73275495/selection-of-the-three-largest-values-in-the-numpy-table
Tensorflow target shape not matching - how to properly format data,"<p>I'm trying to build NLP classifier, data consists of 2 columns, one with text other one represents target with 4 classes in total. I've one-hot encoded target, but when running the model.fit() method shapes do not match.</p>
<p>Example of data structure:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text</th>
<th>target</th>
</tr>
</thead>
<tbody>
<tr>
<td>'such a lovely day'</td>
<td>'a'</td>
</tr>
<tr>
<td>'not so great'</td>
<td>'b'</td>
</tr>
<tr>
<td>'hello world'</td>
<td>'c'</td>
</tr>
</tbody>
</table>
</div>
<p>Below is the code that I used:</p>
<pre><code>from sklearn.model_selection import train_test_split
import tensorflow as tf
import re
import numpy as np
import pandas as pd
import string

# load and split data
df = pd.read_csv('train.csv', index_col=[0])
encoder = LabelEncoder()
target_labels = encoder.fit_transform(df['target'])
target_labels = tf.keras.utils.to_categorical(target_labels, 4)

X_train, X_test, y_train, y_test = train_test_split(df[['text']], target_labels, test_size=0.2, random_state=1)
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=1)
# convert to tf dataset
raw_train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))
raw_val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))
raw_test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))

# text cleanup
def custom_standardization(input_data):
  new_line_replace = tf.strings.regex_replace(input_data, '\n', ' ')
  non_alphanum_replace = tf.strings.regex_replace(new_line_replace, '[^a-zA-Z0-9_ ]', '')
  stripped = tf.strings.strip(non_alphanum_replace)
  lowercase = tf.strings.lower(stripped)
  
  return tf.expand_dims(tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),
                                  ''), -1)
# creating layer for text vectoriazation
max_features = 10000
sequence_length = 250

vectorize_layer = tf.keras.layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)

train_text = raw_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)

def vectorize_text(text, label):
  return vectorize_layer(text), label

train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)
test_ds = raw_test_ds.map(vectorize_text)

train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

# modeling
model = tf.keras.Sequential([
  vectorize_layer,
  tf.keras.layers.Embedding(max_features + 1, 16),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(4, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)
</code></pre>
<p>When running this code I get following error:
ValueError: Shapes (4, 1) and (None, 4) are incompatible</p>
<p>I've tried to flatten out and reshape data to be (1,4) but that doesn't work, also if I change dense layer to 1 neuron, output is impossible to interpret and it's not what's required.</p>
<p>train_ds.take(1)
&lt;TakeDataset element_spec=(TensorSpec(shape=(None, 250), dtype=tf.int64, name=None), TensorSpec(<strong>shape=(4,)</strong>, dtype=tf.float32, name=None))&gt;</p>
<p>Anybody faced the same problem before, appreciate any help?</p>
","python, tensorflow, keras, nlp, text-classification","<p>Just write this</p>
<pre><code>target_labels = target_labels.reshape(length of training set, 1, 4)
</code></pre>
<p>after</p>
<pre><code>target_labels = tf.keras.utils.to_categorical(target_labels, 4)
</code></pre>
",0,0,511,2022-08-16 11:27:32,https://stackoverflow.com/questions/73373245/tensorflow-target-shape-not-matching-how-to-properly-format-data
"Can I plot ROC curve for multiclass text classification problem, without using OneVsRestClassifier?","<p>I have a pickle file that when loaded returns a trained RandomForest classifier. I want to plot the ROC curve for the classes, but from what I read online, the classifier must be wrapped in scikit learn's OneVsRestClassifier. The problem is that since I already have the trained model I cannot wrap it in it to fit the model again.</p>
<p>So I would like to know if there is some workaround to plot the ROC curve. From my trained model I have y_test, y_proba. I also have x_test values.</p>
<ul>
<li>The shape of my y_proba examples is: (6715, 5)</li>
</ul>
<p><a href=""https://i.sstatic.net/PoLV8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PoLV8.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>The shape of y_test is (6715, 5)</li>
</ul>
<p><a href=""https://i.sstatic.net/3oKdC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3oKdC.png"" alt=""enter image description here"" /></a></p>
<p>This is the output of the code @dx2-66 suggested:</p>
<p><a href=""https://i.sstatic.net/bgfeM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bgfeM.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/RYQga.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RYQga.png"" alt=""enter image description here"" /></a></p>
","python, scikit-learn, text-classification, roc, multiclass-classification","<p>I assume your <code>y_test</code> is single column with class id, and your <code>y_proba</code> has as much columns as there are classes (at least that's what you'd usually get from <code>predict_proba()</code>.</p>
<p>How about this? It should yield you OvR-style curves:</p>
<pre><code>from sklearn.metrics import roc_curve
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt

classes = range(y_proba.shape[1])

for i in classes:
    fpr, tpr, _ = roc_curve(label_binarize(y_test, classes=classes)[:,i], y_proba[:,i])
    plt.plot(fpr, tpr, alpha=0.7)
    plt.legend(classes)
</code></pre>
<p>Update: solution for non-monotonic class labels:</p>
<pre><code>classes = sorted(list(y_test['label'].unique()))

plt.plot([0, 1], linestyle='--')

for i in range(len(classes)):
    fpr, tpr, _ = roc_curve(label_binarize(y_test, classes=classes)[:,i], y_proba.values[:,i])
    plt.plot(fpr, tpr, alpha=0.7)
    plt.legend(['baseline']+classes) # Fixed the baseline legend
</code></pre>
",1,0,280,2022-08-25 08:38:47,https://stackoverflow.com/questions/73484411/can-i-plot-roc-curve-for-multiclass-text-classification-problem-without-using-o
Extend BERT or any transformer model using manual features,"<p>I have been doing a thesis in my citation classifications. I just implemented Bert model for the classification of citations. I have 4 output classes and I give an input sentence and my model returns an output that tells the category of citation. Now my supervisor gave me another task.</p>
<p>You have to search that whether it is possible to extend BERT or any transformer model using manual features. e.g. You are currently giving a sentence as the only input followed by its class. What if you can give a sentence, and some other features as input; as we do in other classifiers?</p>
<p>I need some guidance about this problem. How can I add an extra feature in my Bert model and the feature would be categorical not numerical.</p>
","huggingface-transformers, text-classification, bert-language-model","<p>The are several ways to achieve that. I will explain just two in the following answer:</p>
<ol>
<li>Add category as a token:<br />
The idea of this approach is rather simple when transformer models like BERT are able to produce contextualized embeddings for a given sentence, why can't we incorporate categorical features as text as well? For example, you use the title of a cited paper as input and also want to incorporate the research area of the paper to provide more context:</li>
</ol>
<pre><code>&quot;Attention is all you need. [Computer Science] [Machine Translation]&quot; -&gt; BERT
</code></pre>
<p>To do that, I would add the categories of your new feature as separate tokens to BERT (that is not required but reduces the sequence length) and fine-tune it for a few epochs:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForSequenceClassification

my_categories = [&quot;[Computer Science]&quot;, &quot;[Machine Translation]&quot;]
sentence=&quot;Attention is all you need. [Computer Science] [Machine Translation]&quot;

t= BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
m=BertForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;)
# tokenized without separate tokens
print(len(t(sentence)[&quot;input_ids&quot;]))

# tokenized without separate tokens
t.add_tokens(my_categories)
print(len(t(sentence)[&quot;input_ids&quot;]))

# Extend embedding layer of model
m.resize_token_embeddings(len(t.get_vocab()))

# Training...
</code></pre>
<p>Output:</p>
<pre><code>18
12
Embedding(28998, 768, padding_idx=0)
</code></pre>
<ol start=""2"">
<li>Separate Embedding layer:<br />
A more traditional way is to hold an embedding for each category and concatenate (or any other method to combine features) it with the contextualized output of BERT before you feed it to the classification layer. For this approach, you can simply copy the code from huggingfaces <a href=""https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/bert/modeling_bert.py#L1510"" rel=""noreferrer"">BertForSequenceClassification</a> class (or whatever class you are using) and make the required changes:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
from transformers import BertPreTrainedModel, BertModel
from typing import Optional

class MyBertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.config = config

        self.bert = BertModel(config)
        classifier_dropout = (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout = torch.nn.Dropout(classifier_dropout)
        
        # Modified +20
        self.classifier = torch.nn.Linear(config.hidden_size +20, config.num_labels)

        # Modified 50 different categories embedding dimension 20 
        self.my_categorical_feature = torch.nn.Embedding(50,20)

        # Initialize weights and apply final processing
        self.post_init()

    # Modified new parameter categorical_feature_ids
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        categorical_feature_ids = None,
    ):

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]

        # Modified get embeddings
        my_categorical_embedding = self.my_categorical_feature(categorical_feature_ids)
        my_categorical_embedding = self.dropout(my_categorical_embedding)

        pooled_output = self.dropout(pooled_output)
        
        # Modified concatenate contextualized embeddings from BERT and your categorical embedding
        pooled_output = torch.cat((pooled_output, my_categorical_embedding), dim=-1)

        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = &quot;regression&quot;
                elif self.num_labels &gt; 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = &quot;single_label_classification&quot;
                else:
                    self.config.problem_type = &quot;multi_label_classification&quot;

            if self.config.problem_type == &quot;regression&quot;:
                loss_fct = MSELoss()
                if self.num_labels == 1:
                    loss = loss_fct(logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(logits, labels)
            elif self.config.problem_type == &quot;single_label_classification&quot;:
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == &quot;multi_label_classification&quot;:
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)
        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return {
            &quot;loss&quot;:loss,
            &quot;logits&quot;:logits,
            &quot;hidden_states&quot;:outputs.hidden_states,
            &quot;attentions&quot;:outputs.attentions,
        }
</code></pre>
<p>You can use this class just as the BertForSerquenceClassification class, the only difference is, that it expects <code>categorical_feature_ids</code> as additional input:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForSequenceClassification

t= BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
m= MyBertForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;)

# batch with two sentences (i.e. the citation text you have already used) 
i = t([&quot;paper title 1&quot;, &quot;paper title 2&quot;], padding=True, return_tensors=&quot;pt&quot;)

# We assume that the first sentence (i.e. paper title 1) belongs to category 23 and the second sentence to category 42
# You probably want to use a dictionary in your own code 
i[&quot;categorical_feature_ids&quot;] = torch.tensor([23,42])

print(m(**i))
</code></pre>
<p>Output:</p>
<pre><code>{'loss': None, 
'logits': tensor([[ 0.6069, -0.1878], [ 0.6347, -0.2608]], grad_fn=&lt;AddmmBackward0&gt;), 
'hidden_states': None, 
'attentions': None}
</code></pre>
",6,6,2738,2022-09-01 09:25:45,https://stackoverflow.com/questions/73567055/extend-bert-or-any-transformer-model-using-manual-features
"ValueError: X has 3 features, but LinearSVC is expecting 64852 features as input","<p>I get the following error when I try to deploy this model.</p>
<pre><code>ValueError: X has 3 features, but LinearSVC is expecting 64852 features as input
</code></pre>
<p>Example of data below.</p>
<pre><code>data = [[3409, False, 'Lorum Ipsum'], [0409, True, 'dolor sit amet consectetuer'], [7869, False, 'Aenean commodo ligula eget dolor']]
df = pd.DataFrame(data, columns=['id', 'booleanv', 'text'] 
</code></pre>
<p>The code where the model gets created below.</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

df = pd.read_csv('cleandata.csv')

# Split dataset into training and validation set
train_size = int(df.shape[0] * 0.8)

train_df = df[:train_size]
val_df = df[train_size:]

# split text and labels
X_train = train_df.text.to_numpy()
Y_train = train_df.booleanv.to_numpy()
X_test = val_df.text.to_numpy()
Y_test = val_df.booleanv.to_numpy()


tfidf = TfidfVectorizer(ngram_range=(1,1))
X_train_tf = tfidf.fit_transform(X_train)
X_test_tf = tfidf.transform(X_test)

model1 = LinearSVC(random_state=0, tol=1e-5)
model1.fit(X_train_tf, Y_train)

import pickle

pickle.dump(model1, open('classification.pickle','wb'))
pickle.dump(tfidf, open('vectorizer.pickle','wb'))
</code></pre>
<p>X_Train and X_Test are both arrays. The input I feed in the API I created is in json format. I suspect that I need to transform my input somehow. Is this correct? If so, how can I do that?</p>
","python, machine-learning, nlp, svm, text-classification","<p>To obtain predictions from your model, you need to follow the same transformation steps that were undertaken during the training phase.</p>
<p>The <code>ValueError</code> you are encountering indicates that you are passing raw data to the classifier without vectorization. As the model has been trained on a sparse matrix consisting of 64852 features (the outcome of <code>tfidf.fit_transform(X_train)</code> ), it expects a vectorized input with the same number of features. Here is how it can be done:</p>
<pre><code>input_data = {
               'id': 1234,  
               'booleanv': False, 
               'text' : 'your input text goes here'
              }

#vectorize 
input_vectorized = tfidf.transform([input_data['text']]) 

#get predictions 
predictions = model.predict(input_vectorized)
</code></pre>
<p>This can, of course, be modified to work with batches instead of single inputs. Moreover, the use of <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"" rel=""nofollow noreferrer"">pipelines</a> is highly recommended to assemble all the different steps.</p>
",2,2,2470,2022-09-05 09:42:33,https://stackoverflow.com/questions/73607406/valueerror-x-has-3-features-but-linearsvc-is-expecting-64852-features-as-input
Save TextVectorization Model to load it later,"<p>I'm not used to the TextVectorization Encoder Layer. I created my vocabulary manually before. I was wondering how one can save a Keras Model which uses the TextVectorization layer. When I tried to do it with simply model.save() and later models.load_model() I was prompted with this error:</p>
<p><code>AssertionError: Found 1 Python objects that were not bound to checkpointed values, likely due to changes in the Python program. Showing 1 of 1 unmatched objects: [&lt;tensorflow.python.ops.lookup_ops.MutableHashTable object at 0x7fb9602df7c0&gt;]</code></p>
","python, tensorflow, keras, text-classification","<p>I've solved my problem by using another version of Keras. If someone faces a similar issue I can recommend to use a different (most of the time newer) version of Keras.</p>
<p>As I already said in my comment. I can't really recommend Keras and or Tensorflow right now. I've started a big NLP project some time ago (half a year). And since then Keras had multiple updates. Their documents changed like 2 times. And the old examples are not there anymore. The new way to create Text Tokens is quite nice but their example uses Masking_zero=True. Which basically means that It will pad the sequences for you and following layers will ignore the zero. That sounds nice but masking is not compatible with Cuda which makes training larger models a time consuming job because it's not hardware accelerated with the GPU. And most NLP models are quite large.</p>
",0,1,508,2022-09-28 21:25:13,https://stackoverflow.com/questions/73887948/save-textvectorization-model-to-load-it-later
"ValueError : Call arguments received: • inputs=tf.Tensor(shape=(None, 1), dtype=float32) • training=None","<p>I get the described error with the Input layer and I can't seem to pinpoint the problem.
I'm working on a text classification dataset and wanted to use the universal sentence encoder model for embeddings but it doesn't seem to work here. When I created my own embeddings using the embedding layer and the text vectorization layer it worked flawlessly.</p>
<pre><code>use = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',trainable=False,dtype=tf.string,input_shape=[])
    class CnnModel(keras.Model):
        def __init__(self,channels):
            super(CnnModel,self).__init__()
            self.conversion = keras.Sequential([
                Input(shape=(1,)),
                use
            ])
            self.computation = keras.Sequential([
                Conv1D(filters=channels,kernel_size=2,strides=1,padding='valid'),
                MaxPool1D(pool_size=2,strides=1,padding='valid'),
                Conv1D(filters=channels,kernel_size=2,strides=1,padding='same'),
            ])
            self.dense = keras.Sequential([
                GlobalMaxPooling1D(),
                Dense(units=1,activation='sigmoid')
            ])
        def call(self,input_tensor):
            print(input_tensor.shape)
            x = self.conversion(input_tensor)
            x = self.computation(x)
            x = self.dense(x)
            return x
    model = CnnModel(16)
</code></pre>
<p>I can't even instantiate this class and get this error:</p>
<pre><code>    ValueError                                Traceback (most recent call last)
c:\Users\gupta\OneDrive\Desktop\GIT\Repo\rough.ipynb Cell 6 in &lt;cell line: 25&gt;()
     23         x = self.dense(x)
     24         return x
---&gt; 25 model = CnnModel(16)

c:\Users\gupta\OneDrive\Desktop\GIT\Repo\rough.ipynb Cell 6 in CnnModel.__init__(self, channels)
      4 def __init__(self,channels):
      5     super(CnnModel,self).__init__()
----&gt; 6     self.conversion = keras.Sequential([
      7         Input(shape=(1,)),
      8         use
      9     ])
     10     self.computation = keras.Sequential([
     11         Conv1D(filters=channels,kernel_size=2,strides=1,padding='valid'),
     12         MaxPool1D(pool_size=2,strides=1,padding='valid'),
     13         Conv1D(filters=channels,kernel_size=2,strides=1,padding='same'),
     14     ])
     15     self.dense = keras.Sequential([
     16         GlobalMaxPooling1D(),
     17         Dense(units=1,activation='sigmoid')
     18     ])

File c:\Users\gupta\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\tracking\base.py:629, in no_automatic_dependency_tracking.&lt;locals&gt;._method_wrapper(self, *args, **kwargs)
...


Call arguments received:
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)
  • training=None
</code></pre>
<p>I also tried making this model using Sequential API and I managed to localise the same error to this:</p>
<p>(this also gives the exact same error)</p>
<pre><code> ann = keras.Sequential([
        Input(shape=(1,)),
        use
    ])
</code></pre>
","python, tensorflow, keras, text-classification","<p>I tried to build model for text classification and it worked for me. Providing the shape as blank and mentioning data type as string in the Input layer worked for me as we are dealing with text data.</p>
<pre><code>keras.Input(shape=[], dtype = tf.string)
</code></pre>
<p>Example Code Snippet:</p>
<pre><code>use = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',trainable=False,dtype=tf.string,input_shape=[])

# build model: Sequential
ann = keras.Sequential([
      keras.Input(shape=[], dtype = tf.string),
      use,
      keras.layers.Dense(1, activation = &quot;sigmoid&quot;)
    ])

# compile model
ann.compile(Adam(2e-5), loss='binary_crossentropy', metrics=['accuracy'])
ann.summary()

#fit model
ann.fit(train_dataset, epochs=1,
                    validation_data=test_dataset,
                    validation_steps=3)
</code></pre>
",2,0,16408,2022-10-09 15:58:11,https://stackoverflow.com/questions/74006276/valueerror-call-arguments-received-inputs-tf-tensorshape-none-1-dtype
Use Spacy with Pandas,"<p>I'm trying to build a multi-class text classifier using Spacy and I have built the model, but facing a problem applying it to my full dataset. The model I have built so far is in the screenshot:</p>
<p><a href=""https://i.sstatic.net/2q2TH.png"" rel=""nofollow noreferrer"">Screenshot</a></p>
<p>Below is the code I used to apply to my full dataset using Pandas:</p>
<pre><code>
Messages = pd.read_csv('Messages.csv', encoding='cp1252')
    
Messages['Body'] = Messages['Body'].astype(str)

Messages['NLP_Result'] = nlp(Messages['Body'])._.cats
</code></pre>
<p>But it gives me the error:</p>
<pre><code>ValueError: [E1041] Expected a string, Doc, or bytes as input, but got: &lt;class 'pandas.core.series.Series'&gt;
</code></pre>
<p>The reason I wanted to use Pandas in this case is the dataset has 2 columns: ID and Body. I want to apply the NLP model only to the Body column, but I want the final dataset to have 3 columns: ID, Body and the NLP result like in the screenshot above.</p>
<p>Thanks so much</p>
<p>I tried Pandas apply method too, but had no luck. Code used:</p>
<pre><code>Messages['NLP_Result'] = Messages['Body'].apply(nlp)._.cats
</code></pre>
<p>The error I got: AttributeError: 'Series' object has no attribute '_'</p>
<p>Expectation is to generate 3 columns as described above</p>
","python, pandas, spacy, text-classification","<p>You should provide a callable into <code>Series.apply</code> call:</p>
<pre class=""lang-py prettyprint-override""><code>Messages['NLP_Result'] = Messages['Body'].apply(lambda x: nlp(x)._.cats)
</code></pre>
<p>Here, each value in the <code>NLP_Result</code> column will be assigned to <code>x</code> variable.</p>
<p>The <code>nlp(x)</code> will create an NLP object that contains the necessary properties you'd like to access. Then, the <code>nlp(x)._.cats</code> will return the expected value.</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
import classy classification
import csv
import pandas as pd 

with open ('Deliveries.txt', 'r') as d:
    Deliveries = d.read().splitlines()
with open (&quot;Not Spam.txt&quot;, &quot;r&quot;) as n:
    Not_Spam = n.read().splitlines()

data = {}
data[&quot;Deliveries&quot;] = Deliveries
data[&quot;Not_Spam&quot;] = Not_Spam

# NLP model
nlp = spacy.blank(&quot;en&quot;)
nlp.add pipe(&quot;text_categorizer&quot;,
    config={
        &quot;data&quot;: data,
        &quot;model&quot;: &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot;,
        &quot;device&quot;: &quot;gpu&quot;
    }
)

Messages['NLP_Result'] = Messages['Body'].apply(lambda x: nlp(x)._.cats)
</code></pre>
",1,2,701,2022-12-02 01:19:30,https://stackoverflow.com/questions/74649908/use-spacy-with-pandas
Having trouble understanding the predictions array in classification model evaluation,"<p>I'm working on a sarcasm detector with the BERT model (binary classification). Currently, I'm having trouble with the model evaluation as I don't really understand the predictions array. The model should output 1 for sarcastic and 0 for not, but the predictions don't output that. Please let me know if more code is needed. Thank you!</p>
<p>model:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = BertForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
    attention_probs_dropout_prob=0.25,
    hidden_dropout_prob=0.25
)

# Tell pytorch to run this model on the GPU.
model.cuda()
</code></pre>
<p>evaluation:</p>
<pre><code>from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd

print('Predicting labels for {:,} test sentences...'.format(len(eval_input_ids)))

# Put model in evaluation mode
model.eval()

predictions , true_labels = [], []


# iterate over test data
for batch in eval_dataloader:
  batch = tuple(t.to(device) for t in batch)
  
  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask, b_labels = batch
  
  # Telling the model not to compute or store gradients, saving memory and 
  # speeding up prediction
  with torch.no_grad():
      # Forward pass, calculate logit predictions.
      result = model(b_input_ids, 
                     token_type_ids=None, 
                     attention_mask=b_input_mask,
                     return_dict=True)

  logits = result.logits

  # Move logits and labels to CPU
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  
  # Store predictions and true labels
  predictions.append(logits)
  true_labels.append(label_ids)

true_labels[1]
predictions[1]
</code></pre>
<p>output:</p>
<pre><code>array([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
   0, 1, 1, 0, 0, 0, 0, 1, 1, 1]) &lt;-- true_labels[1]
array([[ 2.9316974 , -2.855342  ],
   [ 3.4540875 , -3.3177233 ],
   [ 2.7424026 , -2.6472614 ],
   [-3.4326897 ,  3.330751  ],
   [ 3.7238903 , -3.7757814 ],
   [-3.208891  ,  3.175109  ],
   [ 3.0500402 , -2.8103237 ],
   [ 3.8333693 , -3.9073608 ],
   [-3.2779126 ,  3.231213  ],
   [ 1.484127  , -1.2610332 ],
   [ 3.686339  , -3.7582958 ],
   [-2.1883147 ,  2.205132  ],
   [-3.274582  ,  3.2254982 ],
   [-1.606854  ,  1.6213335 ],
   [ 3.7080388 , -3.6854186 ],
   [-2.351147  ,  2.365543  ],
   [-3.7317555 ,  3.4833894 ],
   [ 3.2413306 , -3.2116275 ],
   [ 3.7413723 , -3.7767386 ],
   [-3.6293464 ,  3.4446163 ],
   [ 3.7779078 , -3.9025154 ],
   [-3.5576923 ,  3.403335  ],
   [ 3.6226897 , -3.6370063 ],
   [-3.7081888 ,  3.4720154 ],
   [ 1.1533121 , -0.8105195 ],
   [ 1.0573612 , -0.69238156],
   [ 3.4189024 , -3.4764926 ],
   [-0.13847755,  0.450572  ],
   [ 3.7248163 , -3.7781181 ],
   [-3.2015219 ,  3.1719215 ],
   [-2.1409311 ,  2.1202204 ],
   [-3.470693  ,  3.358798  ]], dtype=float32) &lt;-- predictions[1]
</code></pre>
","deep-learning, bert-language-model, text-classification","<p>There are two values because you have two classes (0=no, 1=yes). These values are logits, which when fed into a softmax function gives the probability of each class. If you want to know whether the sample is classified as sarcasm or not, just take the class with the highest logit:</p>
<pre><code>predictions = a.max(1)[1]
print(predictions)
</code></pre>
",1,0,62,2023-01-09 18:21:10,https://stackoverflow.com/questions/75061462/having-trouble-understanding-the-predictions-array-in-classification-model-evalu
Cosine similarity of two columns in a DataFrame,"<p>I've a dataframe with 2 columns and I am tring to get a cosine similarity score of each pair of sentences.</p>
<p>Dataframe (df)</p>
<pre><code>       A                   B
0    Lorem ipsum ta      lorem ipsum
1    Excepteur sint      occaecat excepteur
2    Duis aute irure     aute irure 
</code></pre>
<p>some of the code pieces that I've tried are:</p>
<pre><code>1. df[&quot;cosine_sim&quot;] = df[[&quot;A&quot;,&quot;B&quot;]].apply(lambda x1,x2:cosine_sim(x1,x2))

2. from spicy.spatial.distance import cosine
df[&quot;cosine_sim&quot;] = df.apply(lambda row: 1 - cosine(row['A'], row['B']), axis = 1)
</code></pre>
<p>The above codes didn't work, and I am still trying different approaches but in the meanwhile I would appreciate any guidance, Thank you in advance!</p>
<p>Desired output:</p>
<pre><code>       A                   B                     cosine_sim
0    Lorem ipsum ta      lorem ipsum                 0.8
1    Excepteur sint      occaecat excepteur          0.5
2    Duis aute irure     aute irure                  0.4
</code></pre>
","python, nlp, text-classification, cosine-similarity","<p>You need to first convert your sentences into a vector, this process is referred to as <em>text vectorization</em>. There are many ways to perform text vectorization depending on the level of sophistication you require, what your corpus looks like, and the intended application. The simplest is the &quot;Bag of Words&quot; (BoW) which I've implemented below. Once you have an understanding of what it means to represent a sentence as a vector, you can progress to other more complex methods of representing <em>lexical</em> similarity. For example:</p>
<ul>
<li><a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tf-idf</a> which weights certain words based on how frequently they occur across many documents (or sentences in your case). You can think of this as a weighted BoW approach.</li>
<li><a href=""https://en.wikipedia.org/wiki/Okapi_BM25"" rel=""nofollow noreferrer"">BM25</a> which fixes a shortcoming of tf-idf in which single mentions of words in a short documents produce high &quot;relevance&quot; scores. It does this by taking into account the length of the document.</li>
</ul>
<p>Advancing to measures of <em>semantic</em> similarity you can employ methods such as <a href=""https://en.wikipedia.org/wiki/Word2vec#doc2vec"" rel=""nofollow noreferrer"">Doc2Vec</a> [<a href=""http://proceedings.mlr.press/v32/le14.pdf"" rel=""nofollow noreferrer"">1</a>] which start to use &quot;embedding spaces&quot; to represent the semantics of text. Finally, the recent methods like <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">SentenceBERT</a> [<a href=""https://arxiv.org/pdf/1908.10084.pdf"" rel=""nofollow noreferrer"">2</a>] and <a href=""https://huggingface.co/docs/transformers/model_doc/dpr"" rel=""nofollow noreferrer"">Dense Passage Retrieval</a> [<a href=""https://arxiv.org/pdf/2004.04906"" rel=""nofollow noreferrer"">3</a>] employ techniques based on the <a href=""https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"" rel=""nofollow noreferrer"">Transformer</a> (encoder-decoder) architecture [<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">4</a>] to allow for &quot;context aware&quot; representations to be formed.</p>
<h2>Solution</h2>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from numpy.linalg import norm

df = pd.DataFrame({
    &quot;A&quot;: [
    &quot;I'm not a party animal, but I do like animal parties.&quot;,
    &quot;That must be the tenth time I've been arrested for selling deep-fried cigars.&quot;,
    &quot;He played the game as if his life depended on it and the truth was that it did.&quot;
    ],
    &quot;B&quot;: [
    &quot;The mysterious diary records the voice.&quot;,
    &quot;She had the gift of being able to paint songs.&quot;,
    &quot;The external scars tell only part of the story.&quot;
    ]
    })

# Combine all to make single corpus of text (i.e. list of sentences)
corpus = pd.concat([df[&quot;A&quot;], df[&quot;B&quot;]], axis=0, ignore_index=True).to_list()
# print(corpus)  # Display list of sentences

# Vectorization using basic Bag of Words (BoW) approach
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
# print(vectorizer.get_feature_names_out())  # Display features
vect_sents = X.toarray()

cosine_sim_scores = []
# Iterate over each vectorised sentence in the A-B pairs from the original dataframe
for A_vect, B_vect in zip(vect_sents, vect_sents[int(len(vect_sents)/2):]):
    # Calculate cosine similarity and store result
    cosine_sim_scores.append(np.dot(A_vect, B_vect)/(norm(A_vect)*norm(B_vect)))
# Append results to original dataframe
df.insert(2, 'cosine_sim', cosine_sim_scores)
print(df)
</code></pre>
<h2>Output</h2>
<pre class=""lang-none prettyprint-override""><code>                                A                                         B  cosine_sim
0  I'm not a party animal, but...          The mysterious diary records ...    0.000000
1  That must be the tenth time...   She had the gift of being able to pa...    0.084515
2  He played the game as if hi...  The external scars tell only part of ...    0.257130
</code></pre>
<h2>References</h2>
<p>[<a href=""http://proceedings.mlr.press/v32/le14.pdf"" rel=""nofollow noreferrer"">1</a>] Le, Q. and Mikolov, T., 2014, June. Distributed representations of sentences and documents. In International conference on machine learning (pp. 1188-1196). PMLR.</p>
<p>[<a href=""https://arxiv.org/pdf/1908.10084.pdf"" rel=""nofollow noreferrer"">2</a>] Reimers, N. and Gurevych, I., 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.</p>
<p>[<a href=""https://arxiv.org/pdf/2004.04906"" rel=""nofollow noreferrer"">3</a>] Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. and Yih, W.T., 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>[<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">4</a>] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
",3,2,1132,2023-01-10 19:56:00,https://stackoverflow.com/questions/75075186/cosine-similarity-of-two-columns-in-a-dataframe
Is splitting a long document of a dataset for BERT considered bad practice?,"<p>I am fine-tuning a BERT model on a labeled dataset with many documents longer than the 512 token limit set by the tokenizer.
Since truncating would lose a lot of data I would rather use, I started looking for a workaround. However I noticed that simply splitting the documents after 512 tokens (or another heuristic) and creating new entries in the dataset with the same label was never mentioned.</p>
<p>In <a href=""https://stackoverflow.com/a/58643108/11614170"">this</a> answer, someone mentioned that you would need to recombine the predictions, is that necessary when splitting the documents?</p>
<p>Is this generally considered bad practice or does it mess with the integrity of the results?</p>
","machine-learning, nlp, classification, bert-language-model, text-classification","<p>You have not mentioned if your intention is to classify, but given that you refer to an article on classification I will refer to an approach where you classify the whole text.</p>
<p>The main question is - which part of the text is the most informative for your purpose - or - in other words - does it make sense to use more than the first / last split of text?</p>
<p>When considering long passages of text, frequently, it is enough to consider the first (or last) 512 tokens to <em>correctly</em> predict the class in substantial majority of cases (say 90%). Even though you may loose some precision, you gain on speed and performance of the overall solution <em>and</em> you are getting rid of a nasty problem of figuring out the correct class out of a set of classifications. Why?</p>
<p>Consider an example of text 2100 tokens long. You split it by 512 tokens, obtaining pieces: 512, 512, 512, 512, 52 (notice the small last piece - should you even consider it?). Your target class for this text is, say, A, however you get the following predictions on the pieces: A, B, A, B, C. So you have now a headache to figure out the right method to determine the class. You can:</p>
<ul>
<li>use majority voting but it is not conclusive here.</li>
<li>weight the predictions by the length of the piece. Again non conclusive.</li>
<li>check that prediction of the last piece is class C but it is barely above the threshold and class C is kinda A. So you are leaning towards A.</li>
<li>re-classify starting the split from the end. In the same order as before you get: A, B, C, A, A. So, clearly A. You also get it when you majority vote combining all of the classifications (forward and backward splits).</li>
<li>consider the confidence of the classifications, e.g. A: 80, B: 70, A: 90, B: 60, C: 55% - avg. 85% for A vs. 65% for B.</li>
<li>reconfirm the correction of labelling of the last piece manually: if it turns out to be B, then it changes all of the above.</li>
<li>then you can train an additional network to classify out of the <em>raw</em> classifications of pieces. Getting again into trouble of figuring out what to do with particularly long sequences or non-conclusive combinations of predictions resulting in poor confidence of the additional classification layer.</li>
</ul>
<p>It turns out that there is no easy way. And you will notice that text is a strange classification material exhibiting all of the above (and more) issues while typically the difference in agreement between the first piece prediction and the annotation vs. the ultimate, perfect classifier is slim at best.</p>
<p><strong>So, spare the effort and strive for simplicity, performance, and heuristic... and clip it!</strong></p>
<p>On details of the best practices you should probably refer to the article from <a href=""https://stackoverflow.com/a/59778726/6573902"">this answer</a>.</p>
",3,1,1562,2023-01-19 23:20:18,https://stackoverflow.com/questions/75179250/is-splitting-a-long-document-of-a-dataset-for-bert-considered-bad-practice
BCELoss between logits and labels not working,"<p>I am using a GPT2 model that outputs <code>logits</code> (before softmax) in the shape <code>(batch_size, num_input_ids, vocab_size)</code> and I need to compare it with the labels that are of shape <code>(batch_size, num_input_ids)</code> to calculate BCELoss. How do I calculate it?</p>
<pre><code>logits = output.logits #--of shape (32, 56, 592)
logits = torch.nn.Softmax()(logits)
labels = labels #---------of shape (32, 56)

torch.nn.BCELoss()(logits, labels)
</code></pre>
<p>but the dimensions do not match, so how do I contract <code>logits</code> to <code>labels</code> shape or expand <code>labels</code> to <code>logits</code> shape?</p>
","pytorch, nlp, loss-function, text-classification, gpt-2","<p><strong>Binary cross-entropy</strong> is used when the final classification layer is a <strong>sigmoid layer</strong>, i.e., for each output dimension, only a true/false output is possible. You can imagine it as assigning some tags to the input. This also means that the <code>labels</code> need to have the same dimension as the <code>logits</code>, having 0/1 for each logit. Statistically speaking, for 592 output dimensions, you predict 592 Bernoulli (= binary) distributions. The expected shape is 32 × 56 × 592.</p>
<p>When using the <strong>softmax layer</strong>, you assume only one target class is possible; you predict a single categorical distribution over 592 possible output classes. However, in this case, the correct loss function is not binary cross-entropy but <strong>categorical cross-entropy</strong>, implemented by the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer""><code>CrossEntropyLoss</code></a> class in PyTorch. Note that it takes the logits directly before the softmax normalization and does the normalization internally. The expected shape is 32 × 56, as in the code snippet.</p>
",0,1,858,2023-01-26 20:12:44,https://stackoverflow.com/questions/75251168/bceloss-between-logits-and-labels-not-working
why smote raise &quot;Found input variables with inconsistent numbers of samples&quot;?,"<p>I try to classify emotion from tweet with dataset of 4401 tweet, when i use smaller sample of data (around 15 tweet) everything just work fine, but when i use the full dataset it raise the error of</p>
<pre><code>Found input variables with inconsistent numbers of samples: [7, 3520]
</code></pre>
<p>the error happen when i try to oversampling the data using smote after transforming the data using countvectorizer.</p>
<p><strong>This is the code where the error raise</strong></p>
<pre><code># N-gram Feature and Term Frequency
vectorizer = CountVectorizer(ngram_range=(1,3))
x_train_tf = vectorizer.fit_transform(str(x_train).split('\n')).toarray()
x_test_tf = vectorizer.transform(str(x_test).split('\n')).toarray()
df_output = pd.DataFrame(data =x_train_tf, columns = vectorizer.get_feature_names_out())
display(df_output)
# the print shape is (7 rows × 250 columns)

smote = SMOTE(random_state=42, k_neighbors=5)
x_smote, y_smote = smote.fit_resample(x_train_tf, y_train)
print(&quot;Total Train Data SMOTE : &quot;,x_smote.shape), print(&quot;Total Train Label SMOTE : &quot;,y_smote)
</code></pre>
<p>i did not understand why this is happening so some explanation could really help.
i already tried to solve it using answers from other similiar question but nothing have worked.</p>
<p><strong>This is the full code</strong></p>
<pre><code>import nltk
import re
#nltk.download()
import string
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
from nltk import everygrams
from collections import Counter
from sklearn import preprocessing
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from imblearn.over_sampling import SMOTE
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix

dataset = pd.read_csv(&quot;G:/TA/Program/dataset/Twitter_Emotion_Dataset.csv&quot;, encoding='latin-1')
# Preprocessing
dataset['case_folding_tweet'] = dataset['tweet'].str.casefold()
dataset['only_alphabet_tweet'] = [re.sub('[^a-zA-Z]+\s*', ' ', s) for s in dataset['case_folding_tweet']]
dataset['data_cleaning_tweet'] = dataset['only_alphabet_tweet'].str.replace(r'\b\w{1}\b','').str.replace(r'\s+', ' ')

slangword_dictionary = (&quot;G:/TA/Program/dataset/kamus_singkatan.csv&quot;)

deslang = {}
list_slangword = open(slangword_dictionary).readlines()
for line in list_slangword:
    slang, unslang = line.strip().split(';')
    deslang[slang] = unslang
deslang[slang] = {r&quot;\b{}\b&quot;.format(k): v for k, v in deslang.items()}

dataset['data_cleaning_tweet'] = dataset['data_cleaning_tweet'].replace(deslang[slang], regex=True)
dataset['convert_slang_tweet'] = dataset['data_cleaning_tweet']

replace_dictionary = {'tidak ': 'tidak', 'bukan ': 'bukan', 'jangan ': 'jangan', 'belum ': 'belum'}
dataset['convert_negation_tweet'] = dataset['convert_slang_tweet'].replace(replace_dictionary, regex=True)
dataset['tokenization_tweet'] = dataset['convert_negation_tweet'].apply(word_tokenize) 
list_stopwords = set(stopwords.words(&quot;indonesian&quot;))
list_stopwords.add('username')
list_stopwords.add('url')
dataset['stopword_removal_tweet'] = dataset['tokenization_tweet'].apply(lambda x: [item for item in x if item not in list_stopwords])

factory = StemmerFactory()
stemmer = factory.create_stemmer()
dataset['stemmed_tweet'] = dataset['stopword_removal_tweet'].apply(lambda x: [stemmer.stem(y) for y in x]) 

# Split data
x = dataset[&quot;stemmed_tweet&quot;].values
y = dataset[&quot;label&quot;].values
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state= 42)

# Get N-gram and TF
vectorizer = CountVectorizer(ngram_range=(1,3))
x_train_tf = vectorizer.fit_transform(str(x_train).split('\n')).toarray()
x_test_tf = vectorizer.transform(str(x_test).split('\n')).toarray()

# Oversampling
smote = SMOTE(random_state=42, k_neighbors=5)
x_smote, y_smote = smote.fit_resample(x_train_tf, y_train)

print(&quot;Total Train Data SMOTE : &quot;,x_smote.shape), print(&quot;Total Train Label SMOTE : &quot;,y_smote)

gnb_classifier = GaussianNB()
gnb_classifier.fit(x_smote, y_smote)
print(gnb_classifier)
y_pred = gnb_classifier.predict(x_test_tf)
print(&quot;Emotion Predicted :&quot;, y_pred)
</code></pre>
<p><a href=""https://drive.google.com/file/d/15JAhVWTSU0oS3zQXdEBT89KOV6dr9Zns/view?usp=share_link"" rel=""nofollow noreferrer"">Link to the dataset</a></p>
","python, machine-learning, classification, text-classification, countvectorizer","<p>i fix the problem using the answer from this post <a href=""https://stackoverflow.com/a/58477381/19640802"">answer</a>
by joining all the train data column before vectorizing.</p>
<pre><code>df_train = pd.DataFrame(data=x_train)
df_test = pd.DataFrame(data=x_test)

series = pd.Series(df_train['stemmed_tweet'])
corpus = series.apply(lambda series: ' '.join(series))
vectorizer = CountVectorizer(ngram_range=(1,3), lowercase=False)
x_train_tf = vectorizer.fit_transform(corpus).toarray()
x_test_tf = vectorizer.transform(str(df_test.values).split(&quot;\n&quot;)).toarray()
</code></pre>
",0,2,139,2023-01-26 22:30:45,https://stackoverflow.com/questions/75252308/why-smote-raise-found-input-variables-with-inconsistent-numbers-of-samples
NLP classification with sparse and numerical features crashes,"<p>I have a dataset of 10 million english shows, which has been cleaned and lemmatized, and their classification into different category types such as comedy, documentary, action, ... etc</p>
<p>I also have a feature called <code>duration</code>, which is the length of the tv show.</p>
<p>Data can be found <a href=""https://drive.google.com/file/d/16kRQfTo_76yfNzQ2_WHBdNg-U5QAQn6l/view?usp=share_link"" rel=""nofollow noreferrer"">here</a></p>
<p>I perform tfidf vectorization on the titles, which returns a sparse matrix and normalization on the duration column.</p>
<p>Then I want to feed the data to a logistic regression classifier.</p>
<p>side question: I want to know if theres a better way to handle combining a sparse matrix and a numerical column</p>
<p>when I try to do it using <code>todense()</code> or <code>toarray()</code>, It works</p>
<p>When i pass it to the logistic regression function, the notebook crashes. But if i dont have the duration col, which means i dont have to apply the toarray() or todense() function, it works perfectly. Is this a memory issue?</p>
<p>This is my code:</p>
<pre><code>import os

import pandas as pd

from sklearn import metrics
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression

def normalize(df, col = ''):
    mms = MinMaxScaler()
    mms_col = mms.fit_transform(df[[col]])
    return mms_col

def tfidf(X, col = ''):
    tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000)
    return tfidf_vectorizer.fit_transform(X[col])

def get_training_data(df):
    df = shuffle(pd.read_csv(df).dropna())
    data = df[['name_title', 'Duration']]

    X_duration = normalize(data, col = 'Duration')
    X_sparse = tfidf(data, col = 'name_title')
    X = pd.DataFrame(X_sparse.toarray())

    X['Duration'] = X_duration
    y = df['target']

    return X, y

def logistic_regression(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    lr = LogisticRegression(C = 100.0, random_state = 1, solver = 'lbfgs', multi_class = 'ovr')
    lr.fit(X_train, y_train)
    y_predict = lr.predict(X_test)
    print(y_predict)
    print(&quot;Logistic Regression Accuracy %.3f&quot; %metrics.accuracy_score(y_test, y_predict))

data_path = '../data/'
X, y = get_training_data(os.path.join(data_path, 'podcasts_en_processed.csv'))
print(X.shape) # this prints (971426, 10001)
logistic_regression(X, y)
</code></pre>
","python, nlp, sparse-matrix, text-classification, tf-idf","<p>It seems like you're encountering a memory issue when combining a large sparse matrix from TF-IDF vectorization with a dense 'duration' feature. Converting a sparse matrix to a dense one with toarray() or todense() dramatically increases memory usage, which is likely causing the crash.</p>
<p>Instead of converting the entire sparse matrix, try combining the sparse TF-IDF features with the dense 'duration' feature while keeping most of the data in sparse format. Use scipy.sparse.hstack for this:</p>
<pre><code>from scipy.sparse import hstack

# Combine the sparse and dense features
X = hstack([X_sparse, X_duration])
</code></pre>
<p>This method maintains the efficiency of sparse data storage. If you're still facing memory issues, consider reducing the number of features in your TF-IDF vectorization [ tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000) , I think 10000 is a bit too much ] or using incremental learning methods like SGDClassifier with a logistic regression loss. These approaches should help manage the large dataset more effectively.</p>
",1,0,84,2023-02-02 16:44:14,https://stackoverflow.com/questions/75326344/nlp-classification-with-sparse-and-numerical-features-crashes
Oversampled train set and test set - machine learning classification,"<p>Let's say that I have oversampled my training set after splitting, then I selected the features of interest to be extracted based on the training set analysis.</p>
<p>After this, do I  use the oversampled training set with the testing set together to determine the classification performance (accuracy, precision, F1 measure, and etc) OR I just use the testing set for it?</p>
","machine-learning, text-classification, oversampling","<p><em>(Not really a programming question but it's important enough to be clarified imho)</em></p>
<p>To measure performance reliably you must <a href=""https://datascience.stackexchange.com/questions/104428/imbalanced-dataset-train-test-split-before-and-after-smote"">use the original test set</a>, without any resampling.</p>
<p>This is one of the reasons why the train/test split should <a href=""https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split"">always be done first</a>, the test set should be kept &quot;fresh&quot;. Resampling the test set would be like cheating, because it makes the problem easier to solve.</p>
<p>Note: in general resampling rarely works, especially with text.</p>
",1,0,301,2023-02-10 20:42:42,https://stackoverflow.com/questions/75415939/oversampled-train-set-and-test-set-machine-learning-classification
Does Vespa support binary classifier model serving at feed time?,"<p>We want to classify documents as they are fed to Vespa using the API, and write those classification scores to document fields.</p>
<p>I'm not sure if it's possible to simply add the ONNX model into Vespa's application package directory, have that model classify any text fed to Vespa, and then write those classifications as document fields.</p>
<p>Does Vespa support model serving at feed time in this way?</p>
","machine-learning, text-classification, onnx, vespa","<p>Yes, you can do that with Vespa.</p>
<p>A custom document processor that reads the input, invokes the model, and stores the model's output in a new field. <a href=""https://docs.vespa.ai/en/stateless-model-evaluation.html#model-inference-using-java"" rel=""nofollow noreferrer"">Stateless model evaluation</a></p>
<p>This example is a good starting point <a href=""https://github.com/vespa-engine/sample-apps/blob/master/billion-scale-image-search/src/main/java/ai/vespa/examples/docproc/DimensionReductionDocProc.java"" rel=""nofollow noreferrer"">DimensionReductionDocProc</a>, a document processor that uses the stateless model evaluation support to perform dimensionality reduction of a vector.</p>
<p>Then you need to export your classifier model to onnx format, and put it in the models folder in the application folder. If you wrap the inference in a component that can be shared between search and docproc and call it <code>Classifier</code> the services.xml of the container cluster looks something like this, plus the <code>ClassifyDocProc</code>.</p>
<pre><code>&lt;container id='default' version='1.0'&gt;
    &lt;nodes count='1'/&gt;
    &lt;component id='ai.vespa.examples.Classifier'/&gt;
    &lt;model-evaluation&gt;
      &lt;onnx&gt;
        &lt;models&gt;
          &lt;model name=&quot;classifier&quot;&gt;
            &lt;intraop-threads&gt;1&lt;/intraop-threads&gt;
          &lt;/model&gt;
        &lt;/models&gt;
      &lt;/onnx&gt;
    &lt;/model-evaluation&gt;
    &lt;search/&gt;
    &lt;document-api/&gt;
    &lt;document-processing&gt;
      &lt;chain id='classifier' inherits='indexing'&gt;
        &lt;documentprocessor id='ai.vespa.examples.docproc.ClassifyDocProc'/&gt;
        &lt;documentprocessor 
      &lt;/chain&gt;
    &lt;/document-processing&gt;
  &lt;/container&gt;
</code></pre>
",2,1,71,2023-02-17 09:58:30,https://stackoverflow.com/questions/75482709/does-vespa-support-binary-classifier-model-serving-at-feed-time
How do I use TextClassifier to load a previously generated model?,"<p>I have used <code>arcgis</code> <code>learn.text</code> to import <code>TextClassifier</code> in order for creating a Machine learning module. Now I want to use the same model in <code>Streamlit</code>for creating an interface for re-use and displaying the predictions.</p>
<p>Code for streamlit-app:</p>
<pre><code>import streamlit as st
import os
from arcgis.learn.text import TextClassifier, SequenceToSequence
import pickle

with st.sidebar:
    st.image('https://www.attomdata.com/wp-content/uploads/2021/05/ATTOM-main-full-1000.jpg')
    st.title(&quot;AutoAttom&quot;)
    st.info(&quot;This project application will help in text classification and sequence to sequence labelling&quot;)

# Text Classifier Section
st.title(&quot;Text Classifier&quot;)
user_input = st.text_input(&quot;&quot;&quot; &quot;&quot;&quot;)
if user_input:
    model_folder = &quot;models/text-classifier&quot;
    print(os.listdir(model_folder))
    model_path = os.path.join(model_folder, 'text-classifier.pth')
    model = TextClassifier.load(model_path, name_or_path=model_path)
    st.write(model.predict(user_input))
</code></pre>
<p>Now, whenever I am running this code I am getting the following error:</p>
<pre><code>NotADirectoryError: [WinError 267] The directory name is invalid: 'models\\text-classifier\\text-classifier.pth'
Traceback:
File &quot;d:\python projects\attom\text2seq\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py&quot;, line 565, in _run_script
    exec(code, module.__dict__)
File &quot;D:\Python Projects\ATTOM\app.py&quot;, line 18, in &lt;module&gt;
    model = TextClassifier.load(model_path, name_or_path=model_path)
File &quot;d:\python projects\attom\text2seq\lib\site-packages\arcgis\learn\text\_text_classifier.py&quot;, line 294, in load
    name_or_path = str(_get_emd_path(name_or_path))
File &quot;d:\python projects\attom\text2seq\lib\site-packages\arcgis\learn\_utils\common.py&quot;, line 460, in _get_emd_path
    list_files = get_files(emd_path, extensions=['.emd'])
File &quot;d:\python projects\attom\text2seq\lib\site-packages\fastai\data_block.py&quot;, line 44, in get_files
    f = [o.name for o in os.scandir(path) if o.is_file()]
</code></pre>
<p>Now, I have checked my folder structure numerous times. And it is correct as follows:</p>
<pre><code>D:\PYTHON PROJECTS\ATTOM\MODELS\TEXT-CLASSIFIER
│   model_metrics.html
│   text-classifier.dlpk
│   text-classifier.emd
│   text-classifier.pth
│
└───ModelCharacteristics
        loss_graph.png
        sample_results.html
</code></pre>
<p>How do i reuse the model that i have generated?</p>
","python, arcgis, text-classification, streamlit","<p>Solved it!</p>
<p>Please use path to the dlpk/emd file inside the text-classifier folder while using the <code>TextClassifier.load()</code> function.</p>
<p>In my case, to make prediction from an already existing pretrained model I had to use <code>from_model()</code> like:</p>
<pre><code>st.title(&quot;Text Classifier&quot;)
user_input = st.text_input(&quot;&quot;&quot; &quot;&quot;&quot;)
if user_input:
    emd_path = os.path.join('models', 'text-classifier', &quot;text-classifier.emd&quot;)
    model = TextClassifier.from_model(emd_path)
    st.write(model.predict(user_input))
</code></pre>
",0,0,98,2023-02-21 11:49:12,https://stackoverflow.com/questions/75520116/how-do-i-use-textclassifier-to-load-a-previously-generated-model
XGBoost only predcinting single class for the unseen data out of 18 classes for Multiclass Text Classification problem,"<p>In the current situation, the XGBoost model that I have trained is only producing a single class for the unseen dataset though the accuracy that I have received on then validation set is around 64% which is not bad for my use case.</p>
<p>In my current use case, I am trying to predict a target class for each text written and these are 18 odd classes and the total data set size is just over a 1000 rows (very small) but my primary question is that why XGB is only producing a single class.</p>
<p>I am using the following code to achieve this:</p>
<pre><code> #tf-idf verctor representation
 tfidf_vect = TfidfVectorizer(analyzer='word',
                     stop_words=stopwords_custom,
                     max_features=total_features,
                     lowercase=True)
 fitted_vectorizer = tfidf_vect.fit(X_train)
 xtrain_tfidf = fitted_vectorizer.transform(X_train)
 xval_tfidf = fitted_vectorizer.transform(X_val)
 #Running the XGB model
 xgb_params = {&quot;max_depth&quot;: (3,5,7),'n_estimators': (50,100,150),
'reg_alpha':[0.1,0.5,1],'reg_lambda':[1,1.5,2],'min_child_weight':[2,4,6]}
 from xgboost import XGBClassifier
 xgb_clf = XGBClassifier()
 grid_xgb = GridSearchCV(estimator=xgb_clf,
                    param_grid=xgb_params,
                    cv=5,
                    n_jobs=-1)
 grid_xgb.fit(xtrain_tfidf,y_train)

 print(grid_xgb.best_params_)
 print(grid_xgb.best_score_)
 #Training the model with best params
 final_xgb = XGBClassifier(max_depth = 5,
                      reg_alpha = 1,
                      reg_lambda = 1,
                      n_estimators = 100,
                      objective='multi:softmax',num_class=18,
                      random_state=42)
 final_xgb.fit(xtrain_tfidf,y_train)
 final_xgb_predict = final_xgb.predict(xval_tfidf)
 xgb_accuracy = metrics.accuracy_score(final_xgb_predict, y_val)
 print (&quot;XGBoost &gt; Accuracy: &quot;, xgb_accuracy)
</code></pre>
<p>Where am I going wrong?</p>
","python-3.x, scikit-learn, xgboost, text-classification","<blockquote>
<p>Where am I going wrong?</p>
</blockquote>
<p>TLDR: you are training/making predictions using sparse data matrices, but you should be using dense data matrices. Convert your <code>fitted_vectorizer.transform(X)</code> results to dense using <code>&lt;ndarray&gt;.todense()</code> method, and see if the situation improves.</p>
<p>XGBoost interprets an empty cell as a missing value, rather than a 0 count.</p>
<p>If you replace <code>XGBClassifier</code> with some Scikit-Learn classifier (eg. <code>GradientBoostingClassifier</code>) then your existing code would work as expected. The reason being that Scikit-Learn interprets empty cells differently, as 0 counts.</p>
",1,0,702,2023-02-28 17:31:30,https://stackoverflow.com/questions/75595450/xgboost-only-predcinting-single-class-for-the-unseen-data-out-of-18-classes-for
"How does Huggingface&#39;s zero-shot classification work in production/webapp, do I need to train the model first?","<p>I have already used huggingface's zero-shot classification: I used &quot;facebook/bart-large-mnli&quot; model as reported here (<a href=""https://huggingface.co/tasks/zero-shot-classification"" rel=""nofollow noreferrer"">https://huggingface.co/tasks/zero-shot-classification</a>). The  accuracy is quite good for my task.</p>
<ul>
<li><p>My question is about productionizing the code: In particular I would like to create a Gradio (or streamlit) webapp. Do I need to train the &quot;facebook/bart-large-mnli&quot; model  first, secondly save the model in a pickle file, and then predict a new (unseen) sentence using the pickle file?</p>
</li>
<li><p>Or can I simply import the &quot;facebook/bart-large-mnli&quot; library and compute the prediction for the production/webapp code?</p>
</li>
</ul>
<p>The latter scenario would be preferable. But I am not sure whether loading the model from scratch would produce the same output as loadingthe pickle file with the saved facebook/bart-large-mnli&quot; model.</p>
<p>Thank you in advance.</p>
","python, huggingface-transformers, text-classification, large-language-model, zeroshot-classification","<h2>Q: How does zero-shot classification work? Do I need train/tune the model to use in production?</h2>
<p>Options:</p>
<ul>
<li>(i) train the &quot;facebook/bart-large-mnli&quot; model first, secondly save the model in a pickle file, and then predict a new (unseen) sentence using the pickle file? or</li>
<li>(ii) can I simply import the &quot;facebook/bart-large-mnli&quot; library and compute the prediction for the production/webapp code?</li>
</ul>
<p>A (human): (ii) You can load up the model with <code>pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)</code> once when the server start, then reuse the pipeline <strong>without</strong> re-initializing it for each request.</p>
<p>When you use the model off-the-shelf, it'll be zero-shot but if you fine-tune a model with limited training data, people commonly refer to that as &quot;few-shot&quot;; take a look at <a href=""https://github.com/huggingface/setfit"" rel=""noreferrer"">https://github.com/huggingface/setfit</a> for few-shot learning.</p>
<hr />
<p>The proof is in the pudding, see if the model you pick fits the task you want. Also, there's more than one way to wield the shiny hammer =)</p>
<p><strong>Disclaimer:</strong> Your Miles May Vary...</p>
<h2>Zero shot classification</h2>
<p><strong>TL;DR:</strong> I don't want to train anything, I don't have labeled data, do something with some labels that I come up with.</p>
<pre><code>from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)

text = &quot;Catan (Base Game) | Ages 10+ | for 3 to 4 Players | Average Playtime 60 Minutes | Made by Catan Studio | TRADE, BUILD AND SETTLE: Embark on a quest to settle the isle of Catan! Guide your settlers to victory by clever trading and cunning development. But beware! Someone might cut off your road or buy a monopoly. And you never know when the wily robber might steal some of your precious games!&quot;

candidate_labels = ['Beauty &amp; Wellness', 'Electronics', 'Toys &amp; Games']

classifier(text, candidate_labels)
</code></pre>
<p>[out]:</p>
<pre><code>{'sequence': 'Catan (Base Game) | Ages 10+ | for 3 to 4 Players | Average Playtime 60 Minutes | Made by Catan Studio | TRADE, BUILD AND SETTLE: Embark on a quest to settle the isle of Catan! Guide your settlers to victory by clever trading and cunning development. But beware! Someone might cut off your road or buy a monopoly. And you never know when the wily robber might steal some of your precious games!',
 'labels': ['Toys &amp; Games', 'Electronics', 'Beauty &amp; Wellness'],
 'scores': [0.511284351348877, 0.38416239619255066, 0.10455326735973358]}
</code></pre>
<h2>Don't classify, translate (or seq2seq)</h2>
<p>Inspiration: <a href=""https://arxiv.org/abs/1812.05774"" rel=""noreferrer"">https://arxiv.org/abs/1812.05774</a></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = &quot;google/flan-t5-large&quot;

tokenizer= AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

text = &quot;Catan (Base Game) | Ages 10+ | for 3 to 4 Players | Average Playtime 60 Minutes | Made by Catan Studio | TRADE, BUILD AND SETTLE: Embark on a quest to settle the isle of Catan! Guide your settlers to victory by clever trading and cunning development. But beware! Someone might cut off your road or buy a monopoly. And you never know when the wily robber might steal some of your precious games!&quot;


prompt=f&quot;&quot;&quot;Which category is this product?
QUERY:{text}
OPTIONS:
 - Beauty &amp; Wellness
 - Electronics
 - Toys &amp; Games
&quot;&quot;&quot;

input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids

tokenizer.decode(model.generate(input_ids)[0], skip_special_tokens=True)
</code></pre>
<p>[out]:</p>
<pre><code>Toys &amp; Games
</code></pre>
<p>And for the fun of it =)</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = &quot;google/flan-t5-large&quot;

tokenizer= AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

prompt=f&quot;&quot;&quot;How does zero-shot classification work? 
QUERY: Do I need tune/modify the model to use in production?
OPTIONS:
 - (i) train the &quot;facebook/bart-large-mnli&quot; model first, secondly save the model in a pickle file, and then predict a new (unseen) sentence using the pickle file
 - (ii) can I simply import the &quot;facebook/bart-large-mnli&quot; library and compute the prediction for the production/webapp code
&quot;&quot;&quot;

input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids

print(tokenizer.decode(model.generate(input_ids)[0], skip_special_tokens=True))
</code></pre>
<p>[out]:</p>
<pre><code>(ii)
</code></pre>
<hr />
<h2>Q: What if both methods above don't work?</h2>
<p>A: Try more models from <a href=""https://huggingface.co/models"" rel=""noreferrer"">https://huggingface.co/models</a> or try different tasks and be creative in how to use what's available to fit your data to solve the problem</p>
<h1>Q: What if none of the models/tasks works?</h1>
<p>A: Then it's time to think about what data you can/need to collect to train the model you need. But before collecting the data, it'll be prudent to first decide how you want to evaluate/measure the success of the model, e.g. F1-score, accuracy, etc.</p>
<p>This is how I'll personally solve NLP problems that fits the frame &quot;X problem, Y approach&quot; solutions, <a href=""https://hackernoon.com/what-kind-of-scientist-are-you"" rel=""noreferrer"">https://hackernoon.com/what-kind-of-scientist-are-you</a> (shameless plug)</p>
<h1>Q: How do I deploy a model after I found the model+task I want?</h1>
<p>There're several ways but it'll be out-of-scope of this question, since it's asking about how zero-shot works and more pertinently &quot;Can I use zero-shot classification models off-the-shelf without training?&quot;.</p>
<p>To deploy a model, take a look at:</p>
<ul>
<li><a href=""https://huggingface.co/docs/sagemaker/inference"" rel=""noreferrer"">https://huggingface.co/docs/sagemaker/inference</a></li>
<li><a href=""https://azure.microsoft.com/en-us/solutions/hugging-face-on-azure/"" rel=""noreferrer"">https://azure.microsoft.com/en-us/solutions/hugging-face-on-azure/</a></li>
<li><a href=""https://huggingface.co/inference-endpoints"" rel=""noreferrer"">https://huggingface.co/inference-endpoints</a></li>
</ul>
",9,4,6154,2023-03-28 12:10:37,https://stackoverflow.com/questions/75866093/how-does-huggingfaces-zero-shot-classification-work-in-production-webapp-do-i
What is the classification head of a hugging face AutoModelForTokenClassification Model,"<p>I am a beginner to hugging face and transformers and have been trying to figure out what is the classification head of the AutoModelForTokenClassification? Is is just a BiLSTM-CRF layer or is it something else?</p>
<p>In general where do find details about the heads of these AutoModels?</p>
<p>I have tried looking into the docs but couldn't find anything.</p>
","python, pytorch, nlp, huggingface-transformers, text-classification","<p>The AutoModel* is not pytorch model implementation, it is an implemented <a href=""https://realpython.com/factory-method-python/"" rel=""nofollow noreferrer"">factory pattern</a>. That means it returns an instance of a different class depending on the provided parameters. For example:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForTokenClassification

m = AutoModelForTokenClassification.from_pretrained(&quot;roberta-base&quot;)
print(type(m))
</code></pre>
<p>Output:</p>
<pre><code>&lt;class 'transformers.models.roberta.modeling_roberta.RobertaForTokenClassification'&gt;
</code></pre>
<p>You can check the head either with the <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/models/roberta/modeling_roberta.py#L1360"" rel=""nofollow noreferrer"">official documentation of the class</a> or with <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters"" rel=""nofollow noreferrer"">parameters</a>:</p>
<pre><code>m.parameters
</code></pre>
<p>Output:</p>
<pre><code>&lt;bound method Module.parameters of RobertaForTokenClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
&lt;... truncated ...&gt;
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)&gt;


</code></pre>
",3,2,2568,2023-03-30 15:51:15,https://stackoverflow.com/questions/75890430/what-is-the-classification-head-of-a-hugging-face-automodelfortokenclassificatio
Hugging Face Transformers BART CUDA error: CUBLAS_STATUS_NOT_INITIALIZE,"<p>I'm trying to finetune the Facebook BART model, I'm following <a href=""https://huggingface.co/transformers/v3.2.0/custom_datasets.html#seq-imdb"" rel=""nofollow noreferrer"">this article</a> in order to classify text using my own dataset.</p>
<p>And I'm using the Trainer object in order to train:</p>
<pre class=""lang-py prettyprint-override""><code>training_args = TrainingArguments(
    output_dir=model_directory,      # output directory
    num_train_epochs=1,              # total number of training epochs - 3
    per_device_train_batch_size=4,  # batch size per device during training - 16
    per_device_eval_batch_size=16,   # batch size for evaluation - 64
    warmup_steps=50,                # number of warmup steps for learning rate scheduler - 500
    weight_decay=0.01,               # strength of weight decay
    logging_dir=model_logs,          # directory for storing logs
    logging_steps=10,
)

model = BartForSequenceClassification.from_pretrained(&quot;facebook/bart-large-mnli&quot;) # bart-large-mnli

trainer = Trainer(
    model=model,                          # the instantiated 🤗 Transformers model to be trained
    args=training_args,                   # training arguments, defined above
    compute_metrics=new_compute_metrics,  # a function to compute the metrics
    train_dataset=train_dataset,          # training dataset
    eval_dataset=val_dataset              # evaluation dataset
)
</code></pre>
<p>This is the tokenizer I used:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BartTokenizerFast
tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-large-mnli')
</code></pre>
<p>But when I use <code>trainer.train()</code> I get the following:</p>
<p>Printing the following:</p>
<pre class=""lang-py prettyprint-override""><code>***** Running training *****
  Num examples = 172
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 11
</code></pre>
<p>Followed by this error:</p>
<pre class=""lang-py prettyprint-override""><code>RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 61, in _worker
    output = module(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1496, in forward
    outputs = self.model(
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1222, in forward
    encoder_outputs = self.encoder(
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 846, in forward
    layer_outputs = encoder_layer(
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 323, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 191, in forward
    query_states = self.q_proj(hidden_states) * self.scaling
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/linear.py&quot;, line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
</code></pre>
<p>I've searched this site and GitHub and hugging face forum but still didn't find anything that helped me fix this for me (I tried adding more memory, lowering batches and warmup, restarting, specifying CPU or GPU, and more, but none worked for me)</p>
<h3>Databricks Clusters:</h3>
<ul>
<li><strong>Runtime:</strong> 12.2 LTS ML (includes Apache Spark 3.3.2, GPU, Scala 2.12)
<strong>Worker Type:</strong> Standard_NC24s_v3 with 4 GPUs, 2 to 10 workers, I think 16GB RAM and 448GB memory for the host</li>
<li><strong>Runtime:</strong> 12.1 ML (includes Apache Spark 3.3.1, Scala 2.12)
<strong>Worker Type:</strong> Standard_L8s (Memory optimized), 2 to 10 workers, 64GB memory with 8 cores</li>
</ul>
<p><strong>Update:</strong> With the second cluster, depending on the flag combination, I sometimes get the error <code>IndexError: Target {i} is out of bounds</code> where <code>i</code> change from time to time</p>
<blockquote>
<p>If you require any other information, comment and I'll add it up asap</p>
</blockquote>
<hr />
<p>My dataset is holding private information but here is an image of how it's built:
<a href=""https://i.sstatic.net/8zXnF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8zXnF.png"" alt=""dataset image"" /></a></p>
<hr />
<h2>Updates:</h2>
<p>I also tried setting the:</p>
<ul>
<li><code>fp16=True</code></li>
<li><code>gradient_checkpointing=True</code></li>
<li><code>gradient_accumulation_steps=4</code></li>
</ul>
<p>Flags but still had the same error when putting each separately and together</p>
<h4>Second cluster error (it get this error only sometimes, based on the flag combination):</h4>
<pre class=""lang-py prettyprint-override""><code>IndexError                                Traceback (most recent call last)
File &lt;command-2692616476221798&gt;:1
----&gt; 1 trainer.train()

File /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:1527, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1522     self.model_wrapped = self.model
   1524 inner_training_loop = find_executable_batch_size(
   1525     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1526 )
-&gt; 1527 return inner_training_loop(
   1528     args=args,
   1529     resume_from_checkpoint=resume_from_checkpoint,
   1530     trial=trial,
   1531     ignore_keys_for_eval=ignore_keys_for_eval,
   1532 )

File /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:1775, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1773         tr_loss_step = self.training_step(model, inputs)
   1774 else:
-&gt; 1775     tr_loss_step = self.training_step(model, inputs)
   1777 if (
   1778     args.logging_nan_inf_filter
   1779     and not is_torch_tpu_available()
   1780     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1781 ):
   1782     # if loss is nan or inf simply add the average of previous logged losses
   1783     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:2523, in Trainer.training_step(self, model, inputs)
   2520     return loss_mb.reduce_mean().detach().to(self.args.device)
   2522 with self.compute_loss_context_manager():
-&gt; 2523     loss = self.compute_loss(model, inputs)
   2525 if self.args.n_gpu &gt; 1:
   2526     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:2555, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2553 else:
   2554     labels = None
-&gt; 2555 outputs = model(**inputs)
   2556 # Save past state if it exists
   2557 # TODO: this needs to be fixed and made cleaner later.
   2558 if self.args.past_index &gt;= 0:

File /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)
   1186 # If we don't have any hooks, we want to skip the rest of the logic in
   1187 # this function, and just call forward.
   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1189         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1190     return forward_call(*input, **kwargs)
   1191 # Do not call functions when jit is used
   1192 full_backward_hooks, non_full_backward_hooks = [], []

File /databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1561, in BartForSequenceClassification.forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1559 elif self.config.problem_type == &quot;single_label_classification&quot;:
   1560     loss_fct = CrossEntropyLoss()
-&gt; 1561     loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
   1562 elif self.config.problem_type == &quot;multi_label_classification&quot;:
   1563     loss_fct = BCEWithLogitsLoss()

File /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)
   1186 # If we don't have any hooks, we want to skip the rest of the logic in
   1187 # this function, and just call forward.
   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1189         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1190     return forward_call(*input, **kwargs)
   1191 # Do not call functions when jit is used
   1192 full_backward_hooks, non_full_backward_hooks = [], []

File /databricks/python/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174, in CrossEntropyLoss.forward(self, input, target)
   1173 def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
-&gt; 1174     return F.cross_entropy(input, target, weight=self.weight,
   1175                            ignore_index=self.ignore_index, reduction=self.reduction,
   1176                            label_smoothing=self.label_smoothing)

File /databricks/python/lib/python3.9/site-packages/torch/nn/functional.py:3026, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   3024 if size_average is not None or reduce is not None:
   3025     reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3026 return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)

IndexError: Target 11 is out of bounds.
</code></pre>
<p>The number 11 changes from time to time.</p>
","python, pytorch, huggingface-transformers, text-classification, huggingface","<p>I was able to reproduce your problem, here is how I solved it (on both of the clusters you provided).</p>
<p>In order to solve it I used (at the <code>from_pretrained</code> call):</p>
<ul>
<li><code>ignore_mismatched_sizes=True</code>: because the model you use has fewer labels than what you have</li>
<li><code>num_labels={}</code>: Insert the number of your labels, I used 16 just to be sure</li>
</ul>
<blockquote>
<p>It's based on both your errors but mainly the second one, I suspect it was also the source of the second error on the GPU, please test and confirm it</p>
</blockquote>
<p>I also used the following (at the <code>TrainingArguments</code> call for memory optimizations):</p>
<ul>
<li><code>fp16=True</code></li>
<li><code>gradient_checkpointing=True</code></li>
</ul>
<p>I tested it with up until <code>num_train_epochs=12</code>, <code>per_device_train_batch_size=16</code>, <code>per_device_eval_batch_size=64</code>, <code>warmup_steps=500</code>
and it worked just fine, hopefully, it will help you get the desired results.</p>
<blockquote>
<p>You can look at the 2 final links I provided for more details about how to optimize the speed and memory while training on both GPU and CPU</p>
</blockquote>
<p>For reference:</p>
<ul>
<li><a href=""https://huggingface.co/transformers/v4.10.1/main_classes/model.html"" rel=""nofollow noreferrer"">Huggingface Models (search for <code>ignore_mismatched_sizes</code>)</a></li>
<li><a href=""https://huggingface.co/transformers/v2.9.1/main_classes/configuration.html"" rel=""nofollow noreferrer"">Huggingface Configuration (search for <code>num_labels</code>)</a></li>
<li><a href=""https://huggingface.co/transformers/v3.2.0/custom_datasets.html"" rel=""nofollow noreferrer"">Fine-tuning with custom datasets</a></li>
<li><a href=""https://huggingface.co/docs/transformers/perf_train_gpu_one"" rel=""nofollow noreferrer"">Efficient Training on a Single GPU</a></li>
<li><a href=""https://huggingface.co/docs/transformers/perf_train_cpu"" rel=""nofollow noreferrer"">Efficient Training on CPU</a></li>
</ul>
",3,3,2052,2023-04-25 08:36:20,https://stackoverflow.com/questions/76099140/hugging-face-transformers-bart-cuda-error-cublas-status-not-initialize
Huggingface - Pipeline with a fine-tuned pre-trained model errors,"<p>I have a pre-trained model from <code>facebook/bart-large-mnli</code> I used the Trainer in order to train it on my own dataset.</p>
<pre class=""lang-py prettyprint-override""><code>model = BartForSequenceClassification.from_pretrained(&quot;facebook/bart-large-mnli&quot;, num_labels=14, ignore_mismatched_sizes=True)
</code></pre>
<p>And then after I train it, I try to use the following (creating a pipeline with the fine-tuned model):</p>
<pre class=""lang-py prettyprint-override""><code># Import the Transformers pipeline library
from transformers import pipeline

# Initializing Zero-Shot Classifier
classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer, id2label=id2label)
</code></pre>
<p>I get the following error from it:</p>
<blockquote>
<p>Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.</p>
</blockquote>
<p>I tried searching the web for a solution but I can't find anything, you can refer to my previous question when I had trouble training it <a href=""https://stackoverflow.com/questions/76099140/hugging-face-transformers-bart-cuda-error-cublas-status-not-initialize"">here</a></p>
<hr />
<h1>How to solve the first error:</h1>
<p>Applying <a href=""https://stackoverflow.com/questions/76213873/how-to-finetune-a-zero-shot-model-for-text-classification/76213874#76213874"">this</a> solves the first error.</p>
<h1>Second error:</h1>
<p>I'm getting the following error:</p>
<pre class=""lang-py prettyprint-override""><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)
</code></pre>
<p>I tried deleting my custom metrics and it fixed it for a while but it didn't last, this error keeps coming.</p>
<p>The error is coming from here:</p>
<pre class=""lang-py prettyprint-override""><code>sequences = &quot;Some text sequence&quot;
classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer)
classifier(sequences, list(id2label.values()), multi_label=False)
# id2label is a dictionary mapping each label to its integer ID
</code></pre>
<p>I also tried <code>trainer.save_model(actual_model)</code> but it saved only some of the stuff and when I loaded it it was like I didn't train it at all.</p>
<hr />
<p>If I change the line to:</p>
<pre class=""lang-py prettyprint-override""><code>classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer) # OLD

classifier = pipeline(&quot;zero-shot-classification&quot;, model=model.to('cpu'), tokenizer=tokenizer) # NEW
</code></pre>
<p>It works fine, but if I change it to:</p>
<pre class=""lang-py prettyprint-override""><code>classifier = pipeline(&quot;zero-shot-classification&quot;, model=model.to('cuda'), tokenizer=tokenizer)
</code></pre>
<p>I get the same error too, my model was trained on a GPU cluster and Iw ant to test it as such, is it possible of am I missing something?</p>
<blockquote>
<p>From what I checked the option the <code>to</code> function can get are: <code>cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, privateuseone</code></p>
</blockquote>
","python, pipeline, huggingface-transformers, text-classification, huggingface","<p>After the model training, your model seems to be still placed on your GPU. The error message you receive:</p>
<blockquote>
<p>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)</p>
</blockquote>
<p>is thrown, because the input tensors that are generated from the pipeline are still on <code>cpu</code>. That is also the reason why the pipeline works as expected when you move the model to cpu with <code>model.to('cpu')</code>.</p>
<p>Per default, the pipeline will perform its actions on <code>cpu</code>, you change that behavior by specifying the <a href=""https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.device"" rel=""nofollow noreferrer"">device</a> parameter.</p>
<pre class=""lang-py prettyprint-override""><code># cuda
classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer, device=0)

#cpu
classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer, device=&quot;cpu&quot;)
</code></pre>
",1,2,3066,2023-05-04 07:32:05,https://stackoverflow.com/questions/76170604/huggingface-pipeline-with-a-fine-tuned-pre-trained-model-errors
Python Text Classification Accuracy Measurement Inconsistency,"<p>I'm trying to get accuracy, recall and precision measurements form NLTK movie review corpus but I get three undesirable outcomes:</p>
<ul>
<li>
<ol>
<li>I follow NLTK guide that has <code>random.shuffle</code> method, which makes accuracy, etc., different each time. This isn't good.</li>
</ol>
</li>
<li>
<ol start=""2"">
<li>I delete the <code>shuffle</code> line, but precision and recall don't work anymore and show 0.0 and &quot;none&quot; respectively.</li>
</ol>
</li>
<li>
<ol start=""3"">
<li>I delete the <code>shuffle</code> line and change the training and test sets to [500:1500] and [:1500] respectively, like in this thread: <a href=""https://stackoverflow.com/questions/45466041/how-to-get-the-precision-and-recall-from-a-nltk-classifier"">How to get the precision and recall from a nltk classifier?</a> The recall and precision do work now, but by doing so, the testing set is larger than the training one, which works at first glance but I believe you can't do that.</li>
</ol>
</li>
</ul>
<pre><code>import nltk
import random
import collections
from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize

documents = [(list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]
random.shuffle(documents)



all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features[word] = (word in document_words)
    return features

featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[300:], featuresets[:300]
print ('train on', len(train_set), 'instances, test on', len(test_set), 'instances')
classifier = nltk.NaiveBayesClassifier.train(train_set)
print(nltk.classify.accuracy(classifier, test_set))
classifier.show_most_informative_features(10)

refsets = collections.defaultdict(set)
testsets = collections.defaultdict(set)
for i, (feats, label) in enumerate(test_set):
    refsets[label].add(i)
    observed = classifier.classify(feats)
    testsets[observed].add(i)
print('Precision:', nltk.precision(refsets['pos'], testsets['pos']))
print('Recall:', nltk.recall(refsets['pos'], testsets['pos']))
print('F_Measure:', nltk.f_measure(refsets['pos'], testsets['pos']))
</code></pre>
<p>Is there anything I can do, or am I just misunderstanding something?</p>
<p>Edit: a sufficient solution is to use <code>random.seed(x)</code> before shuffling, or to average several runs. It doesn't explain, however, why deleting the shuffle breaks the program.</p>
","python-3.x, nlp, nltk, text-classification","<p>The reason why deleting the shuffle breaks the program is that the NaiveBayesClassifier implementation in NLTK assumes that the data is randomly shuffled before splitting into training and testing sets. If you don't shuffle the data, the training and testing sets will have a biased distribution and may not generalize well to new data.
To ensure that you get consistent results, you can set the seed for the random number generator before shuffling the data. This will ensure that the shuffling is done in a deterministic way and you get the same train/test splits every time you run the code. You can use random.seed() function to set the seed.
Here's an updated code with the random seed set to 42:</p>
<pre><code>import nltk
import random
import collections

from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize

random.seed(42)  # Set the random seed for reproducibility

documents = [(list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features[word] = (word in document_words)
    return features

featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[300:], featuresets[:300]
print ('train on', len(train_set), 'instances, test on', len(test_set), 'instances')

classifier = nltk.NaiveBayesClassifier.train(train_set)
print(nltk.classify.accuracy(classifier, test_set))
classifier.show_most_informative_features(10)

refsets = collections.defaultdict(set)
testsets = collections.defaultdict(set)
for i, (feats, label) in enumerate(test_set):
    refsets[label].add(i)
    observed = classifier.classify(feats)
    testsets[observed].add(i)
print('Precision:', nltk.precision(refsets['pos'], testsets['pos']))
print('Recall:', nltk.recall(refsets['pos'], testsets['pos']))
print('F_Measure:', nltk.f_measure(refsets['pos'], testsets['pos']))
</code></pre>
<p>This should give you consistent results each time you run the code.</p>
",0,0,74,2023-05-12 18:52:41,https://stackoverflow.com/questions/76239184/python-text-classification-accuracy-measurement-inconsistency
NLTK Text classifier thinks of any text as negative,"<p>I am building a binary text classifier with NLTK, using its movie_review corpus that has 1000 positive and 1000 negative review. It's working on the test set and the metrics (TPR, TNR) are fine, but when I manually input a review, it always with 99%+ certainty thinks it's negative, even if I put only a single word, that is considered by the model a positive feature.</p>
<pre><code>import nltk
import random
import collections
from nltk.corpus import movie_reviews

documents = []
for category in movie_reviews.categories():
    for fileid in movie_reviews.fileids(category):
        documents.append((movie_reviews.words(fileid), category))
random.seed(432)
random.shuffle(documents)

all_words = [word.lower() for word in movie_reviews.words()]
from nltk.corpus import stopwords
stopwords_english = stopwords.words('english')
all_words_clean = []
for word in all_words:
    if word not in stopwords_english and word.isalpha():
        all_words_clean.append(word)

all_words_frequency = nltk.FreqDist(all_words_clean)
most_common_words = all_words_frequency.most_common(2000)
word_features = [item[0] for item in most_common_words]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in document_words)
    return features
feature_set = [(document_features(doc), category) for (doc, category) in documents]

test_set = feature_set[:400]
train_set = feature_set[400:]

from nltk import NaiveBayesClassifier
classifier = NaiveBayesClassifier.train(train_set)
print(classifier.show_most_informative_features(10))

from nltk import classify
accuracy = classify.accuracy(classifier, test_set)
print('Accuracy:', accuracy)

refsets = collections.defaultdict(set)
testsets = collections.defaultdict(set)
for i, (features, label) in enumerate(test_set):
    refsets[label].add(i)
    observed = classifier.classify(features)
    testsets[observed].add(i)
print('Precision:', nltk.precision(refsets['pos'], testsets['pos']))
print('Recall:', nltk.recall(refsets['pos'], testsets['pos']))
print('F1:', nltk.f_measure(refsets['pos'], testsets['pos']))
print('TNR:', nltk.precision(refsets['neg'], testsets['neg']))
print('NPV:', nltk.recall(refsets['neg'], testsets['neg']))
print('bACC:', (nltk.recall(refsets['pos'], testsets['pos'])+
                nltk.precision(refsets['neg'], testsets['neg']))/2)
print('nF1:', nltk.f_measure(refsets['neg'], testsets['neg']))

from nltk.tokenize import word_tokenize
def class_new(new_text):
    new_feat = document_features(word_tokenize(new_text))
    prob_result = classifier.prob_classify(new_feat)
    print (prob_result.max())
    print (prob_result.prob(&quot;neg&quot;))
    print (prob_result.prob(&quot;pos&quot;))
</code></pre>
<p>The output is:</p>
<pre><code>Most Informative Features
        contains(seagal) = True              neg : pos    =     11.5 : 1.0
   contains(outstanding) = True              pos : neg    =      9.5 : 1.0
         contains(damon) = True              pos : neg    =      8.2 : 1.0
          contains(lame) = True              neg : pos    =      6.1 : 1.0
   contains(wonderfully) = True              pos : neg    =      6.0 : 1.0
        contains(poorly) = True              neg : pos    =      6.0 : 1.0
        contains(wasted) = True              neg : pos    =      5.6 : 1.0
         contains(flynt) = True              pos : neg    =      5.4 : 1.0
    contains(ridiculous) = True              neg : pos    =      5.4 : 1.0
       contains(unfunny) = True              neg : pos    =      5.0 : 1.0
None
Accuracy: 0.8025
Precision: 0.7988505747126436
Recall: 0.7595628415300546
F1: 0.7787114845938375
TNR: 0.8053097345132744
NPV: 0.8387096774193549
bACC: 0.7824362880216644
nF1: 0.8216704288939051
</code></pre>
<p>And here's the input:</p>
<pre><code>class_new('outstanding')
neg
0.9999795728097999
2.0427190192409225e-05
</code></pre>
<p>I'm following this <a href=""https://blog.chapagain.com.np/python-nltk-sentiment-analysis-on-movie-reviews-natural-language-processing-nlp/"" rel=""nofollow noreferrer"">link</a> and the author also gets the same thing. Am I doing something wrong? Is this because of the feature set? If so, why are the metrics showing that the model works fine?</p>
","python, nlp, classification, nltk, text-classification","<p><strong>TL;DR:</strong> The way you try to test your algorithm does not reflect the way it was trained. When you enter a text similar to a review that your <code>nltk</code> dataset provides, you get positive predictions.</p>
<p>Your <code>Naive Bayes</code> classifier not only calculates its probabilities based on words which appear in the <code>input text</code>, but also <strong>words which do not appear</strong>:</p>
<pre><code>print(classifier.show_most_informative_features(2000))

…
contains(film) = False             neg : pos    =      1.3 : 1.0
contains(best) = False             neg : pos    =      1.3 : 1.0
contains(many) = False             neg : pos    =      1.3 : 1.0
contains(well) = False             neg : pos    =      1.2 : 1.0
…
</code></pre>
<p>There are about 100 of these conditions; thus, when you enter one word in your classifier you stack that one word against 100 missing words. Given the results you get, these &quot;negative occurances&quot; predominantly assign a given text to the &quot;negative&quot; category. Note that your algorithm has been trained on documents which contain, on average, 800 words. Therefore, to get useful predictions out of your classifier, you also need to provide a document which is similar to the data it was trained on.</p>
<pre><code># count the average number of words in a document
documents_wordcount = []
for category in movie_reviews.categories():
    
    for fileid in movie_reviews.fileids(category):
        documents_wordcount.append(len([word for word in movie_reviews.words(fileid)]))

print(sum(documents_wordcount)/len(documents_wordcount))  # 791.91
</code></pre>
",1,0,34,2023-05-22 22:04:21,https://stackoverflow.com/questions/76310175/nltk-text-classifier-thinks-of-any-text-as-negative
Low f1 score and also low loss function score,"<p>I am trying to build a multi label text classification model to classify toxic comments.
I followed a medium article from this link: <a href=""https://kyawkhaung.medium.com/multi-label-text-classification-with-bert-using-pytorch-47011a7313b9"" rel=""nofollow noreferrer"">Multi-label Text Classification with BERT using Pytorch</a></p>
<p>I also used this data set from kaggle: <a href=""https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge?select=train.csv"" rel=""nofollow noreferrer"">jigsaw-toxic-comment-classification-challenge</a></p>
<p>Im using google colab to run my model with a V100 gpu runtime settings.</p>
<p>Unfortunately after several hours of training (4 epochs), I suffer from a f1 score of only 0.04214842148421484. my final loss score is 0.00354736</p>
<p>I know that the loss function and the f1 score are 2 different things but for my understanding a low cost function score should affect the f1 score. where did i go wrong?</p>
<p>here is the code:</p>
<pre><code>import torch
import numpy as np
import pandas as pd
import shutil, sys
import transformers
from sklearn import metrics
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertModel, BertConfig

val_targets=[]
val_outputs=[]

class CustomDataset(Dataset):

    def __init__(self, dataframe, tokenizer, max_len,):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.title = dataframe['comment_text']
        self.targets = self.data.target_list
        self.max_len = max_len

    def __len__(self):
        return len(self.title)

    def __getitem__(self, index):
        title = str(self.title[index])
        title = &quot; &quot;.join(title.split(&quot; &quot;))

        inputs = self.tokenizer.encode_plus(
            title,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True,
            truncation=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs[&quot;token_type_ids&quot;]


        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        }
    

class BERTClass(torch.nn.Module):
    def __init__(self):
        super(BERTClass, self).__init__()
        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)
        self.l2 = torch.nn.Dropout(0.3)
        self.l3 = torch.nn.Linear(768, 6)

    def forward(self, ids, mask, token_type_ids):
        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)
        output_2 = self.l2(output_1)
        output = self.l3(output_2)
        return output
    
def loss_fn(outputs, targets):
    return torch.nn.BCEWithLogitsLoss()(outputs, targets)


def save_ckp(state, is_best, checkpoint_path, best_model_path):
    &quot;&quot;&quot;
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    &quot;&quot;&quot;
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    &quot;&quot;&quot;
    checkpoint_path: path to save checkpoint
    model: model that we want to load checkpoint parameters into
    optimizer: optimizer we defined in previous training
    &quot;&quot;&quot;
    # load checkpoint
    checkpoint = torch.load(checkpoint_fpath)

    # initialize state_dict from checkpoint to model
    model.load_state_dict(checkpoint['state_dict'])

    # initialize optimizer from checkpoint to optimizer
    optimizer.load_state_dict(checkpoint['optimizer'])

    # handle valid_loss_min based on its type
    valid_loss_min = checkpoint['valid_loss_min']
    if isinstance(valid_loss_min, torch.Tensor):
        valid_loss_min = valid_loss_min.item()

    # return model, optimizer, epoch value, min validation loss
    return model, optimizer, checkpoint['epoch'], valid_loss_min


def train_model(start_epochs,  n_epochs, valid_loss_min_input,
                training_loader, validation_loader, model,
                optimizer, checkpoint_path, best_model_path):

  # initialize tracker for minimum validation loss
  valid_loss_min = valid_loss_min_input


  for epoch in range(start_epochs, n_epochs+1):
    train_loss = 0
    valid_loss = 0

    model.train()
    print('############# Epoch {}: Training Start   #############'.format(epoch))
    for batch_idx, data in enumerate(training_loader):
        #print('yyy epoch', batch_idx)
        ids = data['ids'].to(device, dtype = torch.long)
        mask = data['mask'].to(device, dtype = torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.float)

        optimizer.zero_grad()
        outputs = model(ids, mask, token_type_ids)
        print(outputs.shape)

        loss = loss_fn(outputs, targets)
        if batch_idx%100==0:
           print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')

        loss.backward()
        optimizer.step()
        #print('before loss data in training', loss.item(), train_loss)
        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))
        #print('after loss data in training', loss.item(), train_loss)

    print('############# Epoch {}: Training End     #############'.format(epoch))

    print('############# Epoch {}: Validation Start   #############'.format(epoch))
    ######################
    # validate the model #
    ######################

    model.eval()


    outputs, targets = do_validation(validation_loader)
    val_preds = (np.array(outputs) &gt; 0.5).astype(int)
    val_targets = (np.array(targets) &gt; 0.5).astype(int)
    accuracy = metrics.accuracy_score(val_targets, val_preds)
    f1_score_micro = metrics.f1_score(val_targets, val_preds, average='micro')
    f1_score_macro = metrics.f1_score(val_targets, val_preds, average='macro')
    print(f&quot;Accuracy Score = {accuracy}&quot;)
    print(f&quot;F1 Score (Micro) = {f1_score_micro}&quot;)
    print(f&quot;F1 Score (Macro) = {f1_score_macro}&quot;)

          
    print('############# Epoch {}: Validation End     #############'.format(epoch))
    # calculate average losses
    #print('before cal avg train loss', train_loss)
    train_loss = train_loss/len(training_loader)
    valid_loss = valid_loss/len(validation_loader)
    # print training/validation statistics
    print('Epoch: {} \tAvgerage Training Loss: {:.6f} \tAverage Validation Loss: {:.6f}'.format(
          epoch,
          train_loss,
          valid_loss
          ))

    # create checkpoint variable and add important data
    checkpoint = {
          'epoch': epoch + 1,
          'valid_loss_min': valid_loss,
          'state_dict': model.state_dict(),
          'optimizer': optimizer.state_dict()
    }

      # save checkpoint
    save_ckp(checkpoint, False, checkpoint_path, best_model_path)

    ## TODO: save the model if validation loss has decreased
    if valid_loss &lt;= valid_loss_min:
      print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))
      # save checkpoint as best model
      save_ckp(checkpoint, True, checkpoint_path, best_model_path)
      valid_loss_min = valid_loss

    print('############# Epoch {}  Done   #############\n'.format(epoch))


  return model


def do_validation(dataloader):
    model.eval()
    fin_targets=[]
    fin_outputs=[]
    with torch.no_grad():
        for _, data in enumerate(dataloader, 0):
            ids = data['ids'].to(device, dtype = torch.long)
            mask = data['mask'].to(device, dtype = torch.long)
            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
            targets = data['targets'].to(device, dtype = torch.float)
            outputs = model(ids, mask, token_type_ids)
            fin_targets.extend(targets.cpu().detach().numpy().tolist())
            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
    return fin_outputs, fin_targets


if __name__ == '__main__':

    # If there's a GPU available...
    if torch.cuda.is_available():

        # Tell PyTorch to use the GPU.
        device = torch.device(&quot;cuda&quot;)

        print('There are %d GPU(s) available.' % torch.cuda.device_count())

        print('We will use the GPU:', torch.cuda.get_device_name(0))

    # If not...
    else:
        print('No GPU available, using the CPU instead.')
        device = torch.device(&quot;cpu&quot;)

    train_df =  pd.read_csv(train_data_location,on_bad_lines='skip')
    test_df = pd.read_csv(test_data_location,on_bad_lines='skip')
    select_labels = train_df.columns.values.tolist()[2:]
    train_df['target_list'] = train_df[select_labels].values.tolist()
    test_df['target_list'] = test_df[select_labels].values.tolist()
    MAX_LEN = 64
    TRAIN_BATCH_SIZE = 8
    VALID_BATCH_SIZE = 8
    EPOCHS = 10
    LEARNING_RATE = 1e-05
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    training_set = CustomDataset(train_df, tokenizer, MAX_LEN)
    validation_set = CustomDataset(test_df, tokenizer, MAX_LEN)
    train_params = {'batch_size': TRAIN_BATCH_SIZE,
                    'shuffle': True,
                    'num_workers': 0
                    }

    test_params = {'batch_size': VALID_BATCH_SIZE,
                    'shuffle': False,
                    'num_workers': 0
                    }

    training_loader = DataLoader(training_set, **train_params)
    validation_loader = DataLoader(validation_set, **test_params)
    model = BERTClass()
    model.to(device)
    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)
    checkpoint_path = '/content/checkpoints/current_checkpoint.pt'
    best_model = '/content/checkpoints/best_model.pt'
    trained_model = train_model(1, EPOCHS, np.Inf, training_loader, validation_loader, model,
                        optimizer,checkpoint_path,best_model)
    

 
</code></pre>
","machine-learning, pytorch, bert-language-model, text-classification","<p>The F1 score is the <a href=""https://en.wikipedia.org/wiki/F-score#Definition"" rel=""nofollow noreferrer"">harmonic mean of precision and recall</a>. It allows the programmer to see precision and recall in a single number. Loss scores are not directly correlated to other performance metrics.</p>
<p>For multi-label text classification, <a href=""https://shiffdag.medium.com/what-is-accuracy-precision-and-recall-and-why-are-they-important-ebfcb5a10df2"" rel=""nofollow noreferrer"">accuracy, precision, and recall</a> are the important metrics. Specifically, examine your overall accuracy score, and then the precision and recall scores per class. Outside of research and commercial purposes, an F1 score is not particularly useful.</p>
<p>As for why you're getting a low F1 score in the first place, did you <a href=""https://www.geeksforgeeks.org/how-to-do-train-test-split-using-sklearn-in-python/"" rel=""nofollow noreferrer"">split your  dataset</a>? I see you imported Sklearn's train_test_split library but you never call it in your code. It seems like you're just passing the entire raw dataset to your training function.</p>
",2,-1,832,2023-08-01 14:10:31,https://stackoverflow.com/questions/76812516/low-f1-score-and-also-low-loss-function-score
How to use different dataset for training and test in text classification while avoiding # of features mismatch?,"<p>I'm working on text classification using two distinct dataset, with the aim to use one dataset for training and other other for testing. Please note I do not wish to merge the dataset to prevent leakage (I think that's what it's called). The test dataset is much smaller (~1000 rows) compared to the training dataset (16k rows)</p>
<p>I'm using CountVectorizer and as the two datasets have different vocabularies, it results in different number of columns - which leads to error during prediction step.</p>
<pre><code>ValueError: X has 55229 features, but DecisionTreeClassifier is expecting 387964 
features as input.
</code></pre>
<p>I've been GPTing and Googling for some time and I'm getting mixed guidance e.g:</p>
<ol>
<li>add zero-filled columns to the smaller x_test</li>
<li>use scikit-learn pipeline</li>
</ol>
<p>Code snippets below:</p>
<pre><code># read dfs
df_1 = pd.read_csv(&quot;data1.csv&quot;,header=0) # for training, has text, and class columns
df_2 = pd.read_csv(&quot;data2.csv&quot;,header=0) # for testing,  has text, and class columns

# vectorise
CV1 = CountVectorizer(ngram_range=(1,3), stop_words=&quot;english&quot;).fit(df_1['text']) 
x_train = CV1.transform(df_1['text'])
y_train = df_1['class']

CV2 = CountVectorizer(ngram_range=(1,3), stop_words=&quot;english&quot;).fit(df_2['text']) 
x_test = CV2.transform(df_2['text'])
y_test = df_test['class']

## shapes of objects
## x_test (1589, 55229), y_test(1589,)
## x_train (16716, 387964), y_train(16716,)

# build classifier and predict
classifier = DecisionTreeClassifier(random_state=1234)
model = classifier.fit(x_train,y_train)
y_pred = model.predict(x_test)

# error ValueError: X has 55229 features, but DecisionTreeClassifier is expecting 387964 features as input.
</code></pre>
","python, scikit-learn, nlp, text-classification","<p>As with <strong>every</strong> preprocessing step, do not fit on the test set.  You should have one instance of <code>CountVectorizer</code> that you <code>fit_transform</code> the training set and <code>transform</code> the test set with.</p>
<p>In your case:</p>
<pre class=""lang-py prettyprint-override""><code>CV = CountVectorizer(ngram_range=(1,3), stop_words=&quot;english&quot;)
x_train = CV.fit_transform(df_1['text'])
y_train = df_1['class']

x_test = CV.transform(df_2['text'])
y_test = df_test['class']
</code></pre>
",2,1,263,2023-08-30 18:31:33,https://stackoverflow.com/questions/77010673/how-to-use-different-dataset-for-training-and-test-in-text-classification-while
How to Change Evaluation Metric from ROC AUC to Accuracy in Hugging Face Transformers Fine-Tuning?,"<p>I'm working on a text classification task using the Hugging Face Transformers library in Python. My code is set up to use ROC AUC as the evaluation metric, but I need to change it to accuracy. I've made attempts to modify the code, but I'm running into issues.</p>
<p>Here's a simplified version of the code I'm working with:</p>
<pre><code># install packages
#!pip install torch transformers memory_profiler datasets accelerate
import time
import datetime
tic = time.time()

# import modules
import torch
import random
from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_metric
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

# Function to define datasets
def create_datasets(X, y):
  # Split Data
  X_train, X_test, y_train, y_test = train_test_split(X.tolist(), y, test_size=0.2)

  # Call the Tokenizer
  tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)

  # Tokenize the text
  train_tokens = tokenizer(X_train, truncation=True, padding=True, max_length=512)
  valid_tokens = tokenizer(X_test, truncation=True, padding=True, max_length=512)

  class MakeTorchData(torch.utils.data.Dataset):
      def __init__(self, tokens, labels):
          self.tokens = tokens
          self.labels = labels

      def __getitem__(self, idx):
          item = {k: torch.tensor(v[idx]) for k, v in self.tokens.items()}
          item[&quot;labels&quot;] = torch.tensor([self.labels[idx]])
          return item

      def __len__(self):
          return len(self.labels)

  # convert our tokenized data into a torch Dataset
  train_dataset = MakeTorchData(train_tokens, y_train.ravel())
  valid_dataset = MakeTorchData(valid_tokens, y_test.ravel())

  return train_dataset, valid_dataset

# Import the required libraries
metric_name = &quot;roc_auc&quot;
metric = load_metric(metric_name)

# Define metrics
def compute_metrics(eval_pred):

  predictions, labels = eval_pred
  predictions = np.argmax(predictions, axis=1)

  # 'micro', 'macro', etc. are for multi-label classification. If you are running a binary classification, leave it as default or specify &quot;binary&quot; for average
  return metric.compute(prediction_scores=predictions, references=labels, average=&quot;macro&quot;)

# Create trainer
# Specifiy the arguments for the trainer
def create_trainer(model_name, train_dataset, valid_dataset, num_epochs=5):
  training_args = TrainingArguments(
      output_dir='./results',          # output directory
      num_train_epochs=num_epochs,     # total number of training epochs
      per_device_train_batch_size=8,   # batch size per device during training
      per_device_eval_batch_size=20,   # batch size for evaluation
      warmup_steps=500,                # number of warmup steps for learning rate scheduler
      weight_decay=0.01,               # strength of weight decay
      logging_dir='./logs',            # directory for storing logs
      load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)
      metric_for_best_model = metric_name,    # select the base metrics
      logging_steps=200,               # log &amp; save weights each logging_steps
      save_steps=200,
      evaluation_strategy=&quot;steps&quot;,     # evaluate each `logging_steps`
  )

  trainer = Trainer(
        model=model,                         # the instantiated Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=valid_dataset,          # evaluation dataset
        compute_metrics=compute_metrics,     # the callback that computes metrics of interest
    )
  return trainer

# Define model name
model_name = &quot;sentence-transformers/all-distilroberta-v1&quot;

# Load the data into a pandas DataFrame | remember to have a similar structure in your Drive so that the data can be read properly.
df = pd.read_csv(r&quot;C:\path\essays.csv&quot;, encoding = &quot;latin-1&quot;)
df = df.replace({'y': 1, 'n': 0})

# Define X and y and create datasets

# TO-DO: Define X and y
X = df['TEXT']
y = df['cEXT']

# Create datasets
train_dataset, valid_dataset = create_datasets(X, y)

# Define a list of dropout probabilities to iterate through
dropout_probs = [0.1, 0.2, 0.3, 0.4, 0.5]

# Double loop to iterate through all combinations
for hidden_dropout_prob in dropout_probs:
    for attention_probs_dropout_prob in dropout_probs:
        # Define the model with the current dropout probabilities
        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,
                                                                  hidden_dropout_prob=hidden_dropout_prob,
                                                                  attention_probs_dropout_prob=attention_probs_dropout_prob).to(&quot;cpu&quot;)

        # Rest of your code for this specific model configuration
        # Train the model
        trainer = create_trainer(model, train_dataset, valid_dataset, num_epochs=3)
        training_results = trainer.train()  # Capture the training results

        # Evaluate the model
        results = trainer.evaluate()

        # Print or save the results for this combination
        print(f&quot;Model with hidden_dropout_prob={hidden_dropout_prob} and attention_probs_dropout_prob={attention_probs_dropout_prob}:&quot;)
        print(f&quot;Training Results: {training_results}&quot;)
        print(f&quot;Results: {results}&quot;)
</code></pre>
<p>In this code, I've defined the compute_metrics function and the evaluation metric as ROC AUC, which works well. However, I would like to replace ROC AUC with accuracy in the evaluation of my models.</p>
<p>I would greatly appreciate any guidance on how to adjust this code to calculate accuracy as the evaluation metric instead of ROC AUC. What modifications should I make to the compute_metrics function and other relevant parts of the code?</p>
<p>My dataset looks like this:</p>
<pre><code>            #AUTHID                                               TEXT  cEXT  \
0  1997_504851.txt  Well, right now I just woke up from a mid-day ...     0   
1  1997_605191.txt  Well, here we go with the stream of consciousn...     0   
2  1997_687252.txt  An open keyboard and buttons to push. The thin...     0   
3  1997_568848.txt  I can't believe it!  It's really happening!  M...     1   
4  1997_688160.txt  Well, here I go with the good old stream of co...     1   
</code></pre>
","python, machine-learning, huggingface-transformers, text-classification","<p>You can modify the <code>compute_metrics</code> function to calculate prediction accuracy:</p>
<pre><code>from sklearn.metrics import accuracy_score

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(y_true=labels, y_pred=predictions)
    return {&quot;accuracy&quot;: accuracy}
</code></pre>
",3,0,1330,2023-10-19 08:19:52,https://stackoverflow.com/questions/77322066/how-to-change-evaluation-metric-from-roc-auc-to-accuracy-in-hugging-face-transfo
Deterministic classification in R using regular expressions?,"<p>I have of list of regular expressions:</p>
<pre><code>regex_list &lt;- list(&quot;First Name&quot; = &quot;^[A-Za-z]+$&quot;,
                   &quot;Postal Code&quot; = &quot;^[0-9]{5}$&quot;,
                   &quot;Email&quot; = &quot;^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}&quot;)
</code></pre>
<p>And then I have a list of strings to be classified:</p>
<pre><code>strings &lt;- c(
  &quot;John&quot;, &quot;12345&quot;, &quot;john.doe@email.com&quot;, &quot;InvalidString&quot;, 
  &quot;Alice&quot;, &quot;54321&quot;, &quot;example.com&quot;, &quot;Bob&quot;, &quot;67890&quot;, &quot;contact@example.org&quot;,
  &quot;Charlie&quot;, &quot;98765&quot;, &quot;test.email@test.co.uk&quot;, &quot;David&quot;, &quot;13579&quot;, &quot;invalid.email&quot;,
  &quot;Eva&quot;, &quot;24680&quot;, &quot;eva.smith@example.com&quot;, &quot;Frank&quot;, &quot;11111&quot;, &quot;frank@email&quot;
)
</code></pre>
<p>Now, I would like to classify each and every string according to the regex_list. While this can be achieved using two nested loops:</p>
<pre><code># Initialize an empty vector for categories
categories &lt;- character(length(strings))

# Categorize the strings based on the regular expressions
for (i in 1:length(strings)) {
  for (j in 1:length(regex_list)) {
    if (grepl(regex_list[[j]], strings[i])) {
      categories[i] &lt;- names(regex_list)[j]
      break
    }
  }
  # If it doesn't fit into any category, set it to &quot;No Category&quot;
  if (is.na(categories[i])) {
    categories[i] &lt;- &quot;No Category&quot;
  }
}
</code></pre>
<p>...I was thinking a more elegant way of achieving this. What it could be? :)</p>
","r, loops, classification, text-classification","<p>Another simple approach:</p>
<pre class=""lang-r prettyprint-override""><code>found &lt;- apply(sapply(regex_list, grepl, x = strings), 1, function(z) which(z)[1])
replace(names(regex_list)[found], is.na(found), &quot;No Category&quot;)
#  [1] &quot;First Name&quot;  &quot;Postal Code&quot; &quot;Email&quot;       &quot;First Name&quot;  &quot;First Name&quot;  &quot;Postal Code&quot; &quot;No Category&quot; &quot;First Name&quot;  &quot;Postal Code&quot; &quot;Email&quot;       &quot;First Name&quot;  &quot;Postal Code&quot; &quot;Email&quot;       &quot;First Name&quot; 
# [15] &quot;Postal Code&quot; &quot;No Category&quot; &quot;First Name&quot;  &quot;Postal Code&quot; &quot;Email&quot;       &quot;First Name&quot;  &quot;Postal Code&quot; &quot;No Category&quot;
</code></pre>
<p>This works by first creating a <code>matrix</code> of &quot;found&quot; or not:</p>
<pre class=""lang-r prettyprint-override""><code>sapply(regex_list, grepl, x = strings)
#       First Name Postal Code Email
#  [1,]       TRUE       FALSE FALSE
#  [2,]      FALSE        TRUE FALSE
#  [3,]      FALSE       FALSE  TRUE
#  [4,]       TRUE       FALSE FALSE
#  [5,]       TRUE       FALSE FALSE
#  [6,]      FALSE        TRUE FALSE
#  [7,]      FALSE       FALSE FALSE
#  [8,]       TRUE       FALSE FALSE
#  [9,]      FALSE        TRUE FALSE
# [10,]      FALSE       FALSE  TRUE
# [11,]       TRUE       FALSE FALSE
# [12,]      FALSE        TRUE FALSE
# [13,]      FALSE       FALSE  TRUE
# [14,]       TRUE       FALSE FALSE
# [15,]      FALSE        TRUE FALSE
# [16,]      FALSE       FALSE FALSE
# [17,]       TRUE       FALSE FALSE
# [18,]      FALSE        TRUE FALSE
# [19,]      FALSE       FALSE  TRUE
# [20,]       TRUE       FALSE FALSE
# [21,]      FALSE        TRUE FALSE
# [22,]      FALSE       FALSE FALSE
</code></pre>
<p>Most of these have one <code>TRUE</code> per row, but some have nothing, so we need to be a little careful here. I'll use <code>apply</code> to operate row-wise (the <code>MARGIN=1</code> means to operate on each row):</p>
<pre class=""lang-r prettyprint-override""><code>sapply(regex_list, grepl, x = strings) |&gt;
  apply(MARGIN = 1, function(z) which(z)[1])
#  [1]  1  2  3  1  1  2 NA  1  2  3  1  2  3  1  2 NA  1  2  3  1  2 NA
</code></pre>
<p>The <code>which(z)</code> gives us which within each row, but when nothing is found it will return an empty vector; the <code>[1]</code> however forces it to return <code>NA</code> in this case (and returns the first match when there is a true).</p>
<p>Those numbers are indices on <code>regex_list</code>, so we can next index the names on them, replacing the <code>NA</code> with the no-category label.</p>
",2,1,70,2023-10-20 12:51:17,https://stackoverflow.com/questions/77331064/deterministic-classification-in-r-using-regular-expressions
Low recall and f1-score for LSTM Text classification,"<p>I am pretty new to Text Classificaiton with LSTM.</p>
<p>I am trying to classify social media data into hate (1) and nothate (0) using an LSTM without any pretrained word embeddings.</p>
<p>I did some pre-processing removing stopwords, lowercasing, lemmatization etc. and used <code>tensorflow.keras.preprocessing.text.Tokenizer</code> for tokenization, padded all entries to lenghth of 512 tokens.</p>
<p>My model is the following:</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Embedding(512, 200)
model.add(LSTM(128, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(1, activation=&quot;sigmoid&quot;))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>
<p>The model summary is the following:</p>
<p><a href=""https://i.sstatic.net/uiZhx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uiZhx.png"" alt=""enter image description here"" /></a></p>
<p>The classification report is the following:</p>
<p><a href=""https://i.sstatic.net/VYJBT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VYJBT.png"" alt=""enter image description here"" /></a></p>
<p>Before training the model i under-sampled the training data to have a balanced dataset. Test data remained unbalanced.
Although precision, recall and f1-score are good for class not-hate, the f1-score and recall seem to perform poorly for hate speech.</p>
","keras, classification, lstm, recurrent-neural-network, text-classification","<p>As you can see from your results, your model can only identify about half of the <code>hate</code> sentences as such (<code>recall=0.51</code>). Thus, your model has problems separating <code>not-hate</code> from <code>hate</code>. Now the key question is: <strong>How can you help your model distinguish the two categories?</strong> In general, there are various approaches you can consider, namely 1) better data, 2) more data, 3) larger model, and/or 4) hyperparameter tuning. Below you can find a more elaborate explanation on my recommendation (1) and some references to the others.</p>
<ol>
<li><strong>Better data</strong></li>
</ol>
<p>Without knowing more about your problem/your data, this is the approach I would recommend you. Hate speech can oftentimes be quite subtle and implicit. Therefore, it is important to understand the <code>context</code> in which words are used. Although your code produces custom-trained word-embeddings, those <strong>won't be <code>contextual</code></strong>, e.g. the embedding for the word <code>dog</code> will be exactly the same for <code>dogs are awesome</code> and <code>dogs are lame</code>. Therefore, to improve your model's ability to separate the two categories, you can look into <code>contextualized word embeddings</code>, e.g. the embeddings <a href=""https://huggingface.co/docs/transformers/model_doc/bert"" rel=""nofollow noreferrer""><code>BERT</code></a> uses. Note that you usually don't train custom, contextualized word-embeddings but fine-tune existing ones. If you're interested in learning more about how to customize <code>BERT</code> using <code>TensorFlow</code>, please read the guide <a href=""https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert"" rel=""nofollow noreferrer"">here</a>.</p>
<ol start=""2"">
<li><strong>More data</strong></li>
</ol>
<p>This one is quite self-explanatory. Models thrive on big data, and maybe your model just hasn't seen enough data (you haven't provided any information about dataset size, and maybe your dataset size is 1,000 sentences while your model needs 100,000 to learn the relationship).</p>
<ol start=""3"">
<li><strong>Larger model</strong></li>
</ol>
<p>Maybe you have enough data, but your model is not complex enough to capture the relationship. Following the takeaways of the <strong>Universal Approximation Theorem</strong>, a more complex model could help you capture the relationship that divides <code>hate</code> from <code>no-hate</code>. If you would like to learn more about this theorem, I found <a href=""https://www.youtube.com/watch?v=lkha188L4Gs&amp;ab_channel=CarnegieMellonUniversityDeepLearning"" rel=""nofollow noreferrer"">this</a> lecture on Youtube very useful</p>
<ol start=""4"">
<li><strong>Hyperparameter Tuning</strong></li>
</ol>
<p>Maybe you have enough data and also your model is of the right complexity, but you model configuration is wrong, e.g. your <code>learning rate</code> is too small, which is why your model is taking a very long time to learn the relationship. You can learn more about <code>hyperparameter tuning on TensorFlow</code> <a href=""https://www.tensorflow.org/tutorials/keras/keras_tuner"" rel=""nofollow noreferrer"">here</a></p>
",2,0,392,2023-11-09 17:08:03,https://stackoverflow.com/questions/77455103/low-recall-and-f1-score-for-lstm-text-classification
Getting different score values between manual cross validation and cross_val_score,"<p>I created a python for loop to split the training dataset into stratified KFolds and used a classifier inside the loop to train it. Then used the trained model to predict with the validation data. The metrics achieved using this process where quite different to that achieved with the cross_val_score function. I expected the same results using both methods.</p>
<p>This code is for text classification and I use TF-IDF to vectorize the text</p>
<p>Code for manual implementation of cross validation:</p>
<pre><code>#Importing metrics functions to measure performance of a  model
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.model_selection import StratifiedKFold
data_validation = []  # list used to store the results of model validation using cross validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
accuracy_val = []
f1_val = []

# use ravel function to flatten the multi-dimensional array to a single dimension
for train_index, val_index in (skf.split(X_train, y_train)):
    X_tr, X_val = X_train.ravel()[train_index], X_train.ravel()[val_index] 
    y_tr, y_val  = y_train.ravel()[train_index] , y_train.ravel()[val_index]
    tfidf=TfidfVectorizer()
    X_tr_vec_tfidf = tfidf.fit_transform(X_tr) # vectorize the training folds
    X_val_vec_tfidf = tfidf.transform(X_val) # vectorize the validation fold    
    #instantiate model 
    model= MultinomialNB(alpha=0.5, fit_prior=False) 
    #Training the empty model with our training dataset
    model.fit(X_tr_vec_tfidf, y_tr)  
    predictions_val = model.predict(X_val_vec_tfidf) # make predictions with the validation dataset
    acc_val = accuracy_score(y_val, predictions_val)
    accuracy_val.append(acc_val)
    f_val=f1_score(y_val, predictions_val)
    f1_val.append(f_val)

avg_accuracy_val = np.mean(accuracy_val)
avg_f1_val = np.mean(f1_val)

# temp list to store the metrics 
temp = ['NaiveBayes']
temp.append(avg_accuracy_val)   #validation accuracy score 
temp.append(avg_f1_val)         #validation f1 score
data_validation.append(temp)    
#Create a table ,using dataframe, which contains the metrics for all the trained and tested ML models
result = pd.DataFrame(data_validation, columns = ['Algorithm','Accuracy Score : Validation','F1-Score  : Validation'])
result.reset_index(drop=True, inplace=True)
result      
</code></pre>
<p>Output:</p>
<pre><code>    Algorithm   Accuracy Score : Validation     F1-Score : Validation
0   NaiveBayes  0.77012                      0.733994
</code></pre>
<p>Now code to use cross_val_score function:</p>
<pre><code>from sklearn.model_selection import cross_val_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
scores = ['accuracy', 'f1']
#Text vectorization of training and testing datasets using NLP technique TF-IDF
tfidf=TfidfVectorizer()
X_tr_vec_tfidf = tfidf.fit_transform(X_train)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
nb=MultinomialNB(alpha=0.5, fit_prior=False) 
for score in [&quot;accuracy&quot;, &quot;f1&quot;]:
    print (f'{score}: {cross_val_score(nb,X_tr_vec_tfidf,y_train,cv=skf,scoring=score).mean()} ')
</code></pre>
<p>Output:</p>
<pre><code>accuracy: 0.7341283583255231 
f1: 0.7062017090972422 
</code></pre>
<p>As can be seen the accuracy and f1 metrics are quite different using the two methods. The difference in metrics is much worse when I use the KNeighborsClassfier.</p>
","python, machine-learning, scikit-learn, cross-validation, text-classification","<p><strong>TL;DR</strong>: The two ways of calculation are <strong>not</strong> equivalent due to the different way you handle the TF-IDF transformation; the <strong>first</strong> calculation is the correct one.</p>
<hr />
<p>In the first calculation you correctly apply <code>fit_transform</code> only to the training data <em>of each fold</em>, and <code>transform</code> to the validation data fold:</p>
<pre><code>X_tr_vec_tfidf = tfidf.fit_transform(X_tr) # vectorize the training folds
X_val_vec_tfidf = tfidf.transform(X_val) # vectorize the validation fold    
</code></pre>
<p>But in the second calculation you do not do that; instead, you apply <code>fit_transform</code> to the whole of the training data, before it is split to training and validation folds:</p>
<pre><code>X_tr_vec_tfidf = tfidf.fit_transform(X_train)
</code></pre>
<p>hence the difference. The fact that you seem to get a better accuracy with the second, wrong way of calculation, is due to information leakage (your validation data is not actually unseen, they have participated in the TF-IDF transformation).</p>
<hr />
<p>The correct way to use <code>cross_val_score</code> when we have transformations is via a <strong>pipeline</strong> (<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn-pipeline-pipeline"" rel=""nofollow noreferrer"">API</a>, <a href=""https://scikit-learn.org/stable/modules/compose.html#pipeline"" rel=""nofollow noreferrer"">User's Guide</a>):</p>
<pre><code>from sklearn.pipeline import Pipeline

tfidf = TfidfVectorizer()
nb = MultinomialNB(alpha=0.5, fit_prior=False) 

pipeline = Pipeline([('transformer', tfidf), ('estimator', nb)])

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(pipeline, X_train, y_train, cv = skf)
</code></pre>
",4,2,118,2023-12-18 16:05:36,https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco
Shap value for binary classification using Pre-Train Bert: How to extract summary graph?,"<p>I used pre-train bert model for binary classification. After training my model with my small data, I wanted to extract summary graph like this <a href=""https://i.sstatic.net/xspuL.png"" rel=""nofollow noreferrer"">the graph I want</a>. However, I want to replace these important features  with words.</p>
<p>However, I am not sure everything is okay because the shape of shap_value is only two dimensional. Actually, this is sensible. Nevertheless, I did not get the graph because I encountered two problems if I use this code:</p>
<pre><code>shap.summary_plot(shap_values[:,:10],feature_names=feature_importance['features'].tolist(),features=comments_text)`
</code></pre>
<p>Problem is too unsensible: If I change <code>shap_values[:,:10]</code> with <code>shap_values</code>  or <code>shap_values[0]</code> or <code>shap_values.values</code> vb. I always come across</p>
<pre><code>516: assert len(shap_values.shape) != 1, &quot;Summary plots need a matrix of 
shap_values, not a vector.&quot; ==&gt; AssertionError: Summary plots need a matrix of 
shap_values, not a vector.
</code></pre>
<p>(fist problem)</p>
<p>By the way, my shap_value consist of 10 input(shape_value.shape). If I choose for max value a range from 1 to 147 everything fine for drawing the graph. However,in this time, the graph is not suitable: My graph consist of only blue dot(-second problem-). Like this <a href=""https://i.sstatic.net/rjAXU.png"" rel=""nofollow noreferrer"">only blue not</a>.</p>
<p>Note: <code>shap_values[:,:10]</code> if the number (10) change different number, the graph show diffent word however the total number of the graph same (max 20). Only some words order can be changing.</p>
<p>Minimal reproducible example:</p>
<pre><code>import nlp
import numpy as np
import pandas as pd
import scipy as sp
import torch
import transformers
import torch
import shap

# load a BERT sentiment analysis model
tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(
    &quot;distilbert-base-uncased&quot;
)
model = transformers.DistilBertForSequenceClassification.from_pretrained(
    &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
).cuda()


if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)
    print('We will use the GPU:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device(&quot;cpu&quot;)

def f(x):
    # Encode the batch of sentenc
    inputs = tokenizer.batch_encode_plus(x.tolist(), max_length=450,add_special_tokens=True, return_attention_mask=True,padding='max_length',truncation=True,return_tensors='pt')

    # Send the tensors to the same device as the model
    input_ids = inputs['input_ids'].to(device)
    attention_masks = inputs['attention_mask'].to(device)
    # Predict
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_masks)[0].detach().cpu().numpy()
    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T
    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units
    return val
# Build an explainer using a token masker
explainer = shap.Explainer(f, tokenizer )

imdb_train = nlp.load_dataset(&quot;imdb&quot;)[&quot;train&quot;]
shap_values = explainer(imdb_train[:10], fixed_context=1, batch_size=16)
cohorts = {&quot;&quot;: shap_values}
cohort_labels = list(cohorts.keys())
cohort_exps = list(cohorts.values())
for i in range(len(cohort_exps)):
    if len(cohort_exps[i].shape) == 2:
        cohort_exps[i] = cohort_exps[i].abs.mean(0)
features = cohort_exps[0].data
feature_names = cohort_exps[0].feature_names
#values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))], dtype=object)
values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))])
feature_importance = pd.DataFrame(list(zip(feature_names, sum(values))), columns=['features', 'importance'])
feature_importance.sort_values(by=['importance'], ascending=False, inplace=True)
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance['features'].tolist(),features=imdb_train['text'][10:20],show=False)

</code></pre>
<p>The above code produce the same result. I spent approximately 200 computer units and I did not succeed it :(. How can I do?</p>
","python, machine-learning, bert-language-model, text-classification, shap","<p>Will you try:</p>
<pre><code>sv = np.array([arr[:100] for arr in shap_values.values])
data = np.array([arr[:100] for arr in shap_values.data])
shap.summary_plot(sv, data, feature_names=feature_importance['features'].tolist())
</code></pre>
<p>I've got a grey plot. This is because your data is non-numeric.</p>
",0,0,764,2024-01-09 08:49:10,https://stackoverflow.com/questions/77785423/shap-value-for-binary-classification-using-pre-train-bert-how-to-extract-summar
How to reset parameters from AutoModelForSequenceClassification?,"<p>Currently to reinitialize a model for <code>AutoModelForSequenceClassification</code>, we can do this:</p>
<pre><code>from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification

m = &quot;moussaKam/frugalscore_tiny_bert-base_bert-score&quot;
config = AutoConfig.from_pretrained(m)
model_from_scratch = AutoModel(config)

model_from_scratch.save_pretrained(&quot;frugalscore_tiny_bert-from_scratch&quot;)

model = AutoModelForSequenceClassification(
  &quot;frugalscore_tiny_bert-from_scratch&quot;, local_files_only=True
)
</code></pre>
<h3>Is there some way to reinitialize the model weights without saving a new pretrained model initialized with <code>AutoConfig</code>?</h3>
<pre><code>model = AutoModelForSequenceClassification(
  &quot;moussaKam/frugalscore_tiny_bert-base_bert-score&quot;, 
  local_files_only=True
  reinitialize_weights=True
)
</code></pre>
<p>or something like:</p>
<pre><code>model = AutoModelForSequenceClassification(
  &quot;moussaKam/frugalscore_tiny_bert-base_bert-score&quot;, 
  local_files_only=True
)

model.reinitialize_parameters()
</code></pre>
","python, machine-learning, huggingface-transformers, text-classification, safe-tensors","<p>That is the purpose of <a href=""https://huggingface.co/docs/transformers/v4.37.1/en/model_doc/auto#transformers.AutoModelForSequenceClassification.from_config"" rel=""nofollow noreferrer"">from_config</a> (i.e. creating a model but not loading the respective weights):</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification

m = &quot;moussaKam/frugalscore_tiny_bert-base_bert-score&quot;
config = AutoConfig.from_pretrained(m)

no_weights_model = AutoModelForSequenceClassification.from_config(config)
weights_model = AutoModelForSequenceClassification.from_pretrained(m)

import torch

print(torch.allclose(no_weights_model.bert.embeddings.word_embeddings.weight,weights_model.bert.embeddings.word_embeddings.weight))
</code></pre>
<p>Output:</p>
<pre><code>False
</code></pre>
",3,1,352,2024-01-25 11:34:07,https://stackoverflow.com/questions/77879635/how-to-reset-parameters-from-automodelforsequenceclassification
Machine learning model predicts training labels themselves as result,"<p>I am trying to build a model to predict &quot;species&quot; based on data with features &quot;message&quot;, &quot;tail&quot;, and &quot;finger&quot;, and label &quot;species&quot;(see the first few rows of data.csv below):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>message</th>
<th>fingers</th>
<th>tail</th>
<th>species</th>
</tr>
</thead>
<tbody>
<tr>
<td>pluvia arbor aquos</td>
<td>4</td>
<td>no</td>
<td>Aquari</td>
</tr>
<tr>
<td>cosmix xeno nebuz odbitaz</td>
<td>5</td>
<td>yes</td>
<td>Zorblax</td>
</tr>
<tr>
<td>solarix glixx novum galaxum quasar</td>
<td>5</td>
<td>yes</td>
<td>Zorblax</td>
</tr>
<tr>
<td>arbor insectus pesros ekos dootix nimbus</td>
<td>2</td>
<td>yes</td>
<td>Florian</td>
</tr>
</tbody>
</table></div>
<p>My code is:</p>
<pre><code>import warnings
warnings.simplefilter(&quot;ignore&quot;)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

df = pd.read_csv(&quot;data.csv&quot;)
X = np.asarray(df[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])
X = [str (item) for item in X]
y = df[&quot;species&quot;]

le = LabelEncoder()
y = le.fit_transform(y)

cv = CountVectorizer()
X = cv.fit_transform(X).toarray()

model = MultinomialNB()
model.fit(X, y)

test_data = pd.read_csv('test.csv')
test_data_array = np.asarray(df[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])
test_data_array = [str (item) for item in test_data_array]
test_data_array = cv.fit_transform(test_data_array).toarray()

y_prediction = model.predict(test_data_array)
y_prediction = le.inverse_transform(y_prediction)

print(y_prediction)
</code></pre>
<p>I followed <a href=""https://heartbeat.comet.ml/using-machine-learning-for-language-detection-517fa6e68f22"" rel=""nofollow noreferrer"">this</a> tutorial for the same.</p>
<p>The problem is, when I tried running it, it just outputs the species column of the original training data word-for-word apart from a few differences (there are 493 results while the test data consisted of 299 entries, and the training data consisted of 500 entries). It doesn't actually predict anything for the test data. I don't understand why the code won't work. Could someone help out?</p>
","python, machine-learning, text-classification, naivebayes, machine-learning-model","<p>The problem is that you read the test data into <code>test_data</code>, but then use the original DataFrame, <code>df</code>, containing the training data, to make the test set.</p>
<p>Change this line:</p>
<pre><code>test_data_array = np.asarray(df[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])
</code></pre>
<p>To:</p>
<pre><code>test_data_array = np.asarray(test_data[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])
</code></pre>
<p>And you should have the correct number of predictions.</p>
<p>Remember to also compare <code>y_prediction</code> to <code>test_data['species']</code>.</p>
",2,-1,49,2024-09-24 04:08:16,https://stackoverflow.com/questions/79016929/machine-learning-model-predicts-training-labels-themselves-as-result
How can I get the confidence variable from a CoreML prediction,"<p>I am using the CreateML tool to train a text classifier, when I use the preview feature and put in a sentence it will give me a prediction along with a confidence variable<a href=""https://i.sstatic.net/Um7NZQqE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Um7NZQqE.png"" alt=""enter image description here"" /></a></p>
<p>Here is how I am using the model on the app</p>
<pre><code>import CoreML
...

    func predict(phrase:String) -&gt; String {
        guard let rollModel = try? Roll(configuration: MLModelConfiguration()) else {
            return &quot;Failed to load the Roll Model.&quot;
        }

        let rollModelInput = RollInput(text: phrase)

        guard let prediction = try? rollModel.prediction(input: rollModelInput, options: MLPredictionOptions()) else {
            return &quot;Roll Model Prediction Failed&quot;
        }
        
        return prediction.label
    }
</code></pre>
<p>This is working, it's providing a prediction.</p>
<p>My data is in the standard text/label format</p>
<p>Even when I export the model to xcode and run the preview in xcode the confidence variable is present.</p>
<p>When I run the prediction on the device I want to know what the confidence variable is, how can I get access?</p>
<p><a href=""https://i.sstatic.net/JmWtvy2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JmWtvy2C.png"" alt=""enter image description here"" /></a></p>
","machine-learning, text-classification, coreml, maximum-entropy","<p>To best use the text classifiers, you should be utilising <a href=""https://developer.apple.com/documentation/naturallanguage/nlmodel"" rel=""nofollow noreferrer"">NLModel</a> to generate the predictions.</p>
<p>The key is using <a href=""https://developer.apple.com/documentation/naturallanguage/nlmodel/predictedlabelhypotheses(for:maximumcount:)"" rel=""nofollow noreferrer"">predictedLabelHypotheses(for:maximumCount:)</a></p>
<p>Your new code would look a little similar to;</p>
<pre><code>let model: FVCompeitionTextClassifier? = {
    let config = MLModelConfiguration()
    return try? Roll(configuration: config)
}()

func predict(phrase:String) throws -&gt; String {
    let rollModel = try NLModel(mlModel: model!.model)
    
    for labelHypothese in rollModel.predictedLabelHypotheses(for: phrase, maximumCount: 1) {
        print(&quot;Label: \(labelHypothese.key), Confidence: \(labelHypothese.value)&quot;)
        // Do logic that you need to do ie sort, then return the label you need
    }
    
}
</code></pre>
",1,0,69,2024-11-15 11:13:21,https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction
euclidian distance from word to sentence after doing Vectorizer,"<p>I have dataframe with 1000 text rows.</p>
<p>I did TfidfVectorizer.</p>
<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>
<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>
<p>I will glad to know how to do that or to hear about another method.</p>
","pandas, dataframe, nlp, text-classification, tf-idf","<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

data = {
    'text': [
        &quot;The king sat on the throne with wisdom.&quot;,
        &quot;A queen ruled the kingdom alongside the king.&quot;,
        &quot;Knights were loyal to their king.&quot;,
        &quot;The empire prospered under the rule of a wise monarch.&quot;
    ]
}
df = pd.DataFrame(data)

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(df['text'])

try:
    king_vector = tfidf.transform([&quot;king&quot;]).toarray()
except KeyError:
    print(&quot;The word 'king' is not in the vocabulary.&quot;)
    king_vector = np.zeros((1, tfidf_matrix.shape[1]))

similarities = cosine_similarity(tfidf_matrix, king_vector).flatten()

feature_names = np.array(tfidf.get_feature_names_out())

def get_top_n_words(row_vector, top_n=5):
    indices = row_vector.argsort()[::-1][:top_n]
    return feature_names[indices]

averages = []
for i in range(tfidf_matrix.shape[0]):
    sentence_vector = tfidf_matrix[i].toarray().flatten()
    top_words = get_top_n_words(sentence_vector)
    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]
    averages.append(np.mean(top_similarities))

df['king_similarity'] = similarities
df['avg_closest_similarity'] = averages

print(df)
</code></pre>
<p>which would give you</p>
<pre><code>                                                text  king_similarity  \
0            The king sat on the throne with wisdom.         0.240614   
1      A queen ruled the kingdom alongside the king.         0.259779   
2                  Knights were loyal to their king.         0.274487   
3  The empire prospered under the rule of a wise ...         0.000000   

   avg_closest_similarity  
0                     0.0  
1                     0.0  
2                     0.0  
3                     0.0  
</code></pre>
<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from scipy.spatial.distance import euclidean

data = {
    'text': [
        &quot;The king sat on the throne with wisdom.&quot;,
        &quot;A queen ruled the kingdom alongside the king.&quot;,
        &quot;Knights were loyal to their king.&quot;,
        &quot;The empire prospered under the rule of a wise monarch.&quot;
    ]
}
df = pd.DataFrame(data)

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(df['text']).toarray()

feature_names = tfidf.get_feature_names_out()
if &quot;king&quot; in feature_names:
    king_index = np.where(feature_names == &quot;king&quot;)[0][0]
    king_vector = np.zeros_like(tfidf_matrix[0])
    king_vector[king_index] = 1
else:
    print(&quot;The word 'king' is not in the vocabulary.&quot;)
    king_vector = np.zeros_like(tfidf_matrix[0])

df['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]

print(df)

</code></pre>
<p>which gives</p>
<pre><code>                                                text  king_distance
0            The king sat on the throne with wisdom.       1.232385
1      A queen ruled the kingdom alongside the king.       1.216734
2                  Knights were loyal to their king.       1.204586
3  The empire prospered under the rule of a wise ...       1.414214
</code></pre>
",1,1,44,2024-12-03 12:25:05,https://stackoverflow.com/questions/79247594/euclidian-distance-from-word-to-sentence-after-doing-vectorizer
Error in getting Captum text explanations for text classification,"<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>
<pre><code>import pandas as pd
import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.metrics import accuracy_score
from captum.attr import IntegratedGradients

# Loading data
train_df = pd.read_csv('train_dataset.csv')
test_df = pd.read_csv('test_dataset.csv')

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_data(df, tokenizer, max_len=128):
    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)
    labels = torch.tensor(df['label'].values)
    return inputs, labels

train_inputs, train_labels = preprocess_data(train_df, tokenizer)
test_inputs, test_labels = preprocess_data(test_df, tokenizer)

# DataLoader
train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)
test_loader = DataLoader(test_dataset, batch_size=16)

# Model setup
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training Loop
model.train()
for epoch in range(3):  # Train for 3 epochs
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)

# Evaluation
model.eval()
correct_predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        correct_predictions.extend(
            (preds == labels).cpu().numpy().tolist()
        )
accuracy = accuracy_score(test_labels.numpy(), correct_predictions)
print(f&quot;Test Accuracy: {accuracy:.2f}&quot;)

# Integrated Gradients
ig = IntegratedGradients(model)

def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor
    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor

    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)
    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)
    # forward function for IG
    def forward_func(input_ids):
        outputs = model(input_ids, attention_mask=attention_mask)
        return outputs.logits

    # Applying Integrated Gradients
    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

# Analysing influential words for correctly predicted texts
for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
        print(influential_words)
</code></pre>
<p>But I am getting the following error in running the above.</p>
<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1 loss: 0.4719192385673523
Epoch 2 loss: 0.39585667848587036
Epoch 3 loss: 0.14659778773784637
Test Accuracy: 0.70
Input IDs shape: torch.Size([1, 8]) dtype: torch.int64
Attention mask shape: torch.Size([1, 8]) dtype: torch.int64
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()
     90 for idx, correct in enumerate(correct_predictions):
     91     if correct:
---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
     94         print(influential_words)

18 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2549         # remove once script supports set_grad_enabled
   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2552 
   2553 

RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
</code></pre>
","machine-learning, pytorch, nlp, huggingface-transformers, text-classification","<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>
<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=""https://captum.ai/tutorials/Bert_SQUAD_Interpret"" rel=""nofollow noreferrer"">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>
<p>Below please find snippet that works:</p>
<pre><code>from captum.attr import LayerIntegratedGradients


def custom_forward(inputs):
    preds = predict(inputs)
    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)
lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)
def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)
    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)

    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)
    
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

results = []

for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
        print(influential_words)
</code></pre>
",1,2,80,2024-12-03 12:47:45,https://stackoverflow.com/questions/79247672/error-in-getting-captum-text-explanations-for-text-classification
