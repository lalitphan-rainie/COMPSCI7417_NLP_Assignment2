Ensure the gensim generate the same Word2Vec model for different runs on the same data,"<p>In <a href=""https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus"">LDA model generates different topics everytime i train on the same corpus</a> , by setting the <code>np.random.seed(0)</code>, the LDA model will always be initialized and trained in exactly the same way. </p>

<p><strong>Is it the same for the Word2Vec models from <code>gensim</code>? By setting the random seed to a constant, would the different run on the same dataset produce the same model?</strong></p>

<p>But strangely, it's already giving me the same vector at different instances. </p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; exit()
alvas@ubi:~$ python
Python 2.7.11 (default, Dec 15 2015, 16:46:19) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; word0 = sentences[0][0]
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
</code></pre>

<p><strong>Is it true then that the default random seed is fixed?</strong> If so, what is the default random seed number? Or is it because I'm testing on a small dataset? </p>

<p>If it's true that the the random seed is fixed and different runs on the same data returns the same vectors, a link to a canonical code or documentation would be much appreciated.  </p>
","python, random, gensim, word2vec, word-embedding","<p>Yes, default random seed is fixed to <code>1</code>, as described by the author in <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a>. Vectors for each word are initialised using a hash of the concatenation of word + str(seed).</p>
<p>Hashing function used, however, is Python’s rudimentary built in hash function and can produce different results if two machines differ in</p>
<ul>
<li>32 vs 64 bit, <a href=""https://stackoverflow.com/questions/16452252/does-pythons-hash-function-remain-identical-across-different-versions"">reference</a></li>
<li>python versions, <a href=""https://stackoverflow.com/questions/34058947/hashing-tuple-in-python-causing-different-results-in-different-systems"">reference</a></li>
<li>different Operating Systems/ Interpreters, <a href=""https://stackoverflow.com/questions/17192418/hash-function-in-python"">reference1</a>, <a href=""https://stackoverflow.com/questions/793761/built-in-python-hash-function"">reference2</a></li>
</ul>
<p>Above list is not exhaustive. Does it cover your question though?</p>
<p><strong>EDIT</strong></p>
<p>If you want to ensure consistency, you can provide your own hashing function as an argument in word2vec</p>
<p>A very simple (and bad) example would be:</p>
<pre><code>def hash(astring):
   return ord(astring[0])

model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4, hashfxn=hash)

print model[sentences[0][0]]
</code></pre>
",11,15,9920,2016-01-16 20:05:51,https://stackoverflow.com/questions/34831551/ensure-the-gensim-generate-the-same-word2vec-model-for-different-runs-on-the-sam
What does tf.nn.embedding_lookup function do?,"<pre><code>tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None)
</code></pre>

<p>I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)?</p>

<p>For instance, in the <code>skip-gram</code> model if we use <code>tf.nn.embedding_lookup(embeddings, train_inputs)</code>, then for each <code>train_input</code> it finds the correspond embedding?</p>
","python, tensorflow, deep-learning, word-embedding, nlp","<p><code>embedding_lookup</code> function retrieves rows of the <code>params</code> tensor. The behavior is similar to using indexing with arrays in numpy. E.g.</p>

<pre><code>matrix = np.random.random([1024, 64])  # 64-dimensional embeddings
ids = np.array([0, 5, 17, 33])
print matrix[ids]  # prints a matrix of shape [4, 64] 
</code></pre>

<p><code>params</code> argument can be also a list of tensors in which case the <code>ids</code> will be distributed among the tensors. For example, given a list of 3 tensors <code>[2, 64]</code>, the default behavior is that they will represent <code>ids</code>: <code>[0, 3]</code>, <code>[1, 4]</code>, <code>[2, 5]</code>. </p>

<p><code>partition_strategy</code> controls the way how the <code>ids</code> are distributed among the list. The partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece.</p>
",150,174,79179,2016-01-19 07:14:40,https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do
Tensorflow embedding_lookup,"<p>I am trying to learn the word representation of the imdb dataset ""from scratch"" through the TensorFlow <code>tf.nn.embedding_lookup()</code> function. If I understand it correctly, I have to set up an embedding layer before the other hidden layer, and then when I perform gradient descent, the layer will ""learn"" a word representation in the weights of this layer. However, when I try to do this, I get a shape error between my embedding layer and the first fully-connected layer of my network.</p>

<pre><code>def multilayer_perceptron(_X, _weights, _biases):
    with tf.device('/cpu:0'), tf.name_scope(""embedding""):
        W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=""W"")
        embedding_layer = tf.nn.embedding_lookup(W, _X)    
    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(embedding_layer, _weights['h1']), _biases['b1'])) 
    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])) 
    return tf.matmul(layer_2, weights['out']) + biases['out']

x = tf.placeholder(tf.int32, [None, n_input])
y = tf.placeholder(tf.float32, [None, n_classes])

pred = multilayer_perceptron(x, weights, biases)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred,y))
train_step = tf.train.GradientDescentOptimizer(0.3).minimize(cost)

init = tf.initialize_all_variables()
</code></pre>

<p>The error I get is:</p>

<pre><code>ValueError: Shapes TensorShape([Dimension(None), Dimension(300), Dimension(128)])
and TensorShape([Dimension(None), Dimension(None)]) must have the same rank
</code></pre>
","python, python-2.7, machine-learning, tensorflow, word-embedding","<p>The shape error arises because you are using a two-dimensional tensor, <code>x</code> to index into a two-dimensional embedding tensor <code>W</code>. Think of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""noreferrer""><code>tf.nn.embedding_lookup()</code></a> (and its close cousin <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""noreferrer""><code>tf.gather()</code></a>) as taking each integer value <code>i</code> in <code>x</code> and replacing it with the row <code>W[i, :]</code>. From the error message, one can infer that <code>n_input = 300</code> and <code>embedding_size = 128</code>. In general, the result of <code>tf.nn.embedding_lookup()</code> number of dimensions equal to <code>rank(x) + rank(W) - 1</code>&hellip; in this case, 3. The error arises when you try to multiply this result by <code>_weights['h1']</code>, which is a (two-dimensional) matrix.</p>

<p>To fix this code, it depends on what you're trying to do, and why you are passing in a matrix of inputs to the embedding. One common thing to do is to <em>aggregate</em> the embedding vectors for each input example into a single row per example using an operation like <a href=""https://www.tensorflow.org/api_docs/python/tf/reduce_sum"" rel=""noreferrer""><code>tf.reduce_sum()</code></a>. For example, you might do the following:</p>

<pre><code>W = tf.Variable(
    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0) ,name=""W"")
embedding_layer = tf.nn.embedding_lookup(W, _X)

# Reduce along dimension 1 (`n_input`) to get a single vector (row)
# per input example.
embedding_aggregated = tf.reduce_sum(embedding_layer, [1])

layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(
    embedding_aggregated, _weights['h1']), _biases['b1'])) 
</code></pre>
",17,6,13547,2016-02-09 14:57:35,https://stackoverflow.com/questions/35295191/tensorflow-embedding-lookup
Bigram vector representations using word2vec,"<p>I want to construct word embeddings for documents using the word2vec tool. I know how to find a vector embedding corresponding to a single word (unigram). Now, I want to find a vector for a bigram. Is it possible to construct a bigram word embedding using word2vec? If yes, how?</p>
","nlp, word2vec, word-embedding","<p>The following snippet will get you the vector representation of a bigram. Note that the bigram you want to convert to a vector needs to have an underscore instead of a space between the words, e.g. <code>bigram2vec(unigrams, ""this report"")</code> is wrong, it should be <code>bigram2vec(unigrams, ""this_report"")</code>. For more details on generating the unigrams, please see the <code>gensim.models.word2vec.Word2Vec</code> class <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">here</a>.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec

def bigram2vec(unigrams, bigram_to_search):
    bigrams = Phrases(unigrams)
    model = word2vec.Word2Vec(bigrams[unigrams])
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return None
</code></pre>
",8,10,6738,2016-03-02 12:27:01,https://stackoverflow.com/questions/35747245/bigram-vector-representations-using-word2vec
How to use word embeddings for prediction in Tensorflow,"<p>I'm trying to work through the Tensorflow tutorials and have gotten stuck trying to enhance the <a href=""https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html"" rel=""nofollow"">RNN/language model tutorial</a> so that I can predict the next word in a sentence. The tutorial uses word embeddings as the representation for the words.</p>

<p>Since the model learns on the word embeddings, I'm assuming that any sort of prediction I add will output the same embeddings. What I can't figure out is how to convert from those embeddings back to the word ids from the dataset. The only example I have seen kept an in memory data structure with the reverse of the mapping of wordid -> embedding and used that for lookups. This obviously won't work for all problems. Is there a better way?</p>
","tensorflow, word-embedding","<p>Assuming you have both <code>word_to_idx</code> and <code>idx_to_word</code> from the vocabulary, this is the pseudocode for what you do</p>

<p>Imagine the input for prediction is ""this is sample""</p>

<pre><code>batch_size = 1
num_steps = 3 # i.e each step for the word in ""this is sample""
hidden_size = 1500
vocab_size = 10000
translate the `word_to_idx` for the input `""this is sample""`
get the word embeddings for the each word in the input
Input to model will be word embedding of size 1x1500 at each time step
Output of model at each time step will be of size 1x1500
y_pred is output at last step from model for the given input 
adding projection to output (i.e y_pred x Weights(hidden_size, vocab_size) + bias(vocab_size, )  = 1x10000)
now sample the output through the below function to get the index with max probability
generate idx_to_word from the index we just got
use the generated word along with the previous input to generate next word until you get `&lt;eos&gt;` or some predefined sentence stopping.
</code></pre>

<p>Here's an example of sampling from <a href=""https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>def sample(a, temperature=1.0):
    # helper function to sample an index from a probability array
    a = np.log(a) / temperature
    a = np.exp(a) / np.sum(np.exp(a))
    return np.argmax(np.random.multinomial(1, a, 1))
</code></pre>
",0,0,1394,2016-05-15 23:21:22,https://stackoverflow.com/questions/37244708/how-to-use-word-embeddings-for-prediction-in-tensorflow
Should I use word2vec to do word embedding including testing data?,"<p>I am a new people in NLP and I am try do the text classification job. Before doing the job, I know that we should do word embedding.
My question is should I do word embedding job only on training data <strong>(so that testing data get vector just from pre-trained vec-model of training data)</strong>, or both on training data &amp; testing data?</p>
","machine-learning, nlp, text-classification, word2vec, word-embedding","<p>This is a very important question. In NN community what typically people do is to use a threshold (i.e. frequency &lt; = 2) in the training set and replace all words which occur less than that threshold by UNK token. Then in the test time, if there is a word that doesn't match an actual training set word, UNK's representation will replace it.</p>
",-1,0,810,2016-05-22 03:43:42,https://stackoverflow.com/questions/37370299/should-i-use-word2vec-to-do-word-embedding-including-testing-data
how to give fixed embedding matrix to EmbeddingLayer in Lasagne?,"<p>I have implemented a deep learning architecture which uses Lasagne EmbeddingLayer.</p>

<p>Now I have the word vectors already learned using word2vec and do not want the word vectors to be the parameters of my network.</p>

<p>After reading the documentation, I think it specifies that the numpy array provided to the 'W' parameter is the initial value for the Embedding Matrix.</p>

<p>How can I declare/specify the EmbeddingLayer in the code so that it uses the input weight matrix as a fixed matrix of word vectors??</p>
","lasagne, word-embedding","<p>The above problem can be solved by adding the 'trainable=False' tag to the weight parameter of the custom layer defined to work as the Embedding Layer. </p>
",0,1,349,2016-06-16 11:32:40,https://stackoverflow.com/questions/37857957/how-to-give-fixed-embedding-matrix-to-embeddinglayer-in-lasagne
Tensorflow sequence-to-sequence LSTM within LSTM (nested),"<p>I would like to build an LSTM with a special word embedding, but I have some questions about how this would work. </p>

<p>As you might know, some LSTMs operate on characters, so it is characters in, characters out. I would like to do the same, with an abstraction on words to learn a robust embedding on them with a nested LSTM to be resistant to slight character-level errors. </p>

<p>So, a tiny LSTM would unroll on every letter of a word, then this would create an embedding of the word. Each embedded word in a sentence would then be fed as an input to a higher level LSTM, which would operate on a word level at every time step, rather than on characters. </p>

<p>Questions:
- I cannot find anymore the research paper that talked about that. If you know of what I talk about, I would like to put a name on what I want to do. 
- Does some TensorFlow open-source code already exist for that?
- Else, do you have an idea on how to implement that? The output of the neural network might be harder to deal with, as we would need to undo the word embedding for the training on characters with an output nested LSTM. The whole thing should be trained once as a single unit (workflow: LSTM chars in, LSTM on words, LSTM chars out). </p>

<p>I guess that <code>rnn_cell.MultiRNNCell</code> would stack LSTMs on top of each other rather than nesting them. </p>

<p>Else would you recommend training the embeddings (in and out) as an autoencoder outside the main LSTM ?</p>
","python, tensorflow, deep-learning, lstm, word-embedding","<p>I don't know the paper you are referring to.</p>

<p>But here is an idea on how I would implement something like this in TensorFlow:</p>

<p>You can create 2 <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L243"" rel=""noreferrer"">LSTMCells</a>.</p>

<p>If you want to support a variable number of characters per word and a variable number of words per sequence you can copy and adapt the code for the dynamic_rnn (see rnn.py)
Instead of a single while loop you would create a nested while loop. The inner one operating on characters calling the first LSTMCell and resetting the state after each word. The outer one operating on embedded words (the output from the inner loop) and calling the second LSTMCell. </p>

<p>Typically, whether or not you should train embeddings separately depends on how much data you have available. If you don't have a lot of data training an embedding separately (on a potentially different dataset) may make sense. If you can afford to and your model is training well, then training the embedding along with your whole network can have benefits because the meaning of characters can be task-specific. </p>

<p>But I'm wondering how this approach compares to first doing spelling correction and then using a standard word embedding. Then you can use some of the <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.framework.html#safe_embedding_lookup_sparse"" rel=""noreferrer"">standard embedding tools</a> and a single LSTMCell with the dynamic_rnn. </p>
",5,4,1316,2016-06-18 18:34:22,https://stackoverflow.com/questions/37900366/tensorflow-sequence-to-sequence-lstm-within-lstm-nested
CBOW v.s. skip-gram: why invert context and target words?,"<p>In <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#vector-representations-of-words"" rel=""noreferrer"">this</a> page, it is said that: </p>

<blockquote>
  <p>[...] skip-gram inverts contexts and targets, and tries to predict each context word from its target word [...]</p>
</blockquote>

<p>However, looking at the training dataset it produces, the content of the X and Y pair seems to be interexchangeable, as those two pairs of (X, Y): </p>

<blockquote>
  <p><code>(quick, brown), (brown, quick)</code></p>
</blockquote>

<p>So, why distinguish that much between context and targets if it is the same thing in the end? </p>

<p>Also, doing <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""noreferrer"">Udacity's Deep Learning course exercise on word2vec</a>, I wonder why they seem to do the difference between those two approaches that much in this problem: </p>

<blockquote>
  <p>An alternative to skip-gram is another Word2Vec model called CBOW (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.</p>
</blockquote>

<p>Would not this yields the same results?</p>
","nlp, tensorflow, deep-learning, word2vec, word-embedding","<p>Here is my oversimplified and rather naive understanding of the difference:</p>
<p>As we know, <strong>CBOW</strong> is learning to predict the word by the context. Or maximize the probability of the target word by looking at the context. And this happens to be a problem for rare words. For example, given the context <code>yesterday was a really [...] day</code> CBOW model will tell you that most probably the word is <code>beautiful</code> or <code>nice</code>. Words like <code>delightful</code> will get much less attention of the model, because it is designed to predict the most probable word. This word will be smoothed over a lot of examples with more frequent words.</p>
<p>On the other hand, the <strong>skip-gram</strong> model is designed to predict the context. Given the word <code>delightful</code> it must understand it and tell us that there is a huge probability that the context is <code>yesterday was really [...] day</code>, or some other relevant context. With <strong>skip-gram</strong> the word <code>delightful</code> will not try to compete with the word <code>beautiful</code> but instead, <code>delightful+context</code> pairs will be treated as new observations.</p>
<p><strong>UPDATE</strong></p>
<p>Thanks to @0xF for sharing <a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures"" rel=""noreferrer"">this article</a></p>
<blockquote>
<p>According to Mikolov</p>
<p><strong>Skip-gram:</strong> works well with small amount of the training data, represents well even rare words or phrases.</p>
<p><strong>CBOW:</strong> several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
</blockquote>
<p>One more addition to the subject is found <a href=""https://groups.google.com/d/msg/word2vec-toolkit/LNPeC5gyhmQ/p8683JkD6LoJ"" rel=""noreferrer"">here</a>:</p>
<blockquote>
<p>In the &quot;skip-gram&quot; mode alternative to &quot;CBOW&quot;, rather than averaging
the context words, each is used as a pairwise training example. That
is, in place of one CBOW example such as [predict 'ate' from
average('The', 'cat', 'the', 'mouse')], the network is presented with
four skip-gram examples [predict 'ate' from 'The'], [predict 'ate'
from 'cat'], [predict 'ate' from 'the'], [predict 'ate' from 'mouse'].
(The same random window-reduction occurs, so half the time that would
just be two examples, of the nearest words.)</p>
</blockquote>
",108,55,41086,2016-07-10 01:21:34,https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words
word2vec: CBOW &amp; skip-gram performance wrt training dataset size,"<p>The question is simple. Which of the CBOW &amp; skip-gram works better for a big dataset? (And the answer for small dataset follows.)</p>

<p>I am confused since, by Mikolov himself, <a href=""https://groups.google.com/forum/#!searchin/word2vec-toolkit/c-bow/word2vec-toolkit/NLvYXU99cAM/E5ld8LcDxlAJ"" rel=""noreferrer"">[Link]</a></p>

<blockquote>
  <p>Skip-gram: works well with <strong>small amount of the training data</strong>, represents well even rare words or phrases. <br/> <br/>
  CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
</blockquote>

<p>but, according to Google TensorFlow, <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html"" rel=""noreferrer"">[Link]</a></p>

<blockquote>
  <p>CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets.<br/><br/>However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have <strong>larger datasets</strong>. We will focus on the skip-gram model in the rest of this tutorial.</p>
</blockquote>

<p>Here is a Quora post which supports the first thought <a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms"" rel=""noreferrer"">[Link]</a>, and then there is the other Quora post which suggests the second thought <a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms"" rel=""noreferrer"">[Link]</a>--both seem derivable from the aforementioned credible sources.</p>

<p>Or is it like what Mikolov said:</p>

<blockquote>
  <p>Overall, the best practice is to try few experiments and see what works the best for you, as different applications have different requirements.</p>
</blockquote>

<p>But surely there is an empirical or analytical verdict or final saying on this matter?</p>
","nlp, word2vec, word-embedding","<p>When Mikolov meant CBOW works good for bigger dataset and SG for smaller dataset, I suppose the quantity of data is considered. Since CBOW considers one target word and many context words, it needs a bigger dataset to train for target vectors compared to datasets used in SG. As in vice versa, in SG due to many target words for single context word, it needs smaller datasets.</p>

<p>Google Tensor Flow speaks about the distribution of words in the dataset for generating quality vectors rather than the quantity of dataset used. As CBOW model considers more over the same context words for all the target words in a sentence, a bigger (distributed) dataset is needed and vice versa for SG.</p>

<p>In common, they both mean the same:</p>

<ul>
<li>CBOW model = dataset with short sentences but high number of samples (bigger dataset)</li>
<li>SG model = dataset with long sentences and low number of samples (smaller dataset)</li>
</ul>
",8,15,5465,2016-08-30 09:50:59,https://stackoverflow.com/questions/39224236/word2vec-cbow-skip-gram-performance-wrt-training-dataset-size
How does Fine-tuning Word Embeddings work?,"<p>I've been reading some NLP with Deep Learning papers and found Fine-tuning seems to be a simple but yet confusing concept. There's been the same question asked <a href=""https://stackoverflow.com/questions/40098450/hows-the-input-word2vec-get-fine-tuned-when-training-cnn/40098823#40098823"">here</a> but still not quite clear. </p>

<p>Fine-tuning pre-trained word embeddings to task-specific word embeddings as mentioned in papers like <em>Y. Kim, “Convolutional Neural Networks for Sentence Classification,”</em> and <em>K. S. Tai, R. Socher, and C. D. Manning, “Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,”</em> had only a brief mention without getting into any details. </p>

<p>My question is: </p>

<p>Word Embeddings generated using word2vec or Glove as pretrained word vectors are used as input features <code>(X)</code> for downstream tasks like parsing or sentiment analysis, meaning those input vectors are plugged into a new neural network model for some specific task, while training this new model, somehow we can get updated task-specific word embeddings.</p>

<p>But as far as I know, during the training, what back-propagation does is updating the weights <code>(W)</code> of the model, it does not change the input features <code>(X)</code>, so how exactly does the original word embeddings get fine-tuned? and where do these fine-tuned vectors come from?</p>
","machine-learning, deep-learning, word-embedding","<p>Yes, if you feed the embedding vector as your input, you can't fine-tune the embeddings (at least easily). However, all the frameworks provide some sort of an <code>EmbeddingLayer</code> that takes as input an integer that is the class ordinal of the word/character/other input token, and performs a embedding lookup. Such an embedding layer is very similar to a fully connected layer that is fed a one-hot encoded class, but is way more efficient, as it only needs to fetch/change one row from the matrix on both front and back passes. More importantly, it allows the weights of the embedding to be learned.</p>

<p>So the classic way would be to feed the actual classes to the network instead of embeddings, and prepend the entire network with a embedding layer, that is initialized with word2vec / glove, and which continues learning the weights. It might also be reasonable to freeze them for several iterations at the beginning until the rest of the network starts doing something reasonable with them before you start fine tuning them.</p>
",17,15,11839,2016-10-31 15:41:28,https://stackoverflow.com/questions/40345607/how-does-fine-tuning-word-embeddings-work
"tf.train.range_input_producer(epoch_size, shuffle=True) does not terminate nor induce CPU/GPU load","<p>I've got a strange issue while working on RNN. I am following <a href=""https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html#recurrent-neural-networks"" rel=""nofollow noreferrer"">TensorFlow RNN Tutorial</a> and trying my own (simpler) implementation which is quite inspired by <a href=""http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html"" rel=""nofollow noreferrer"">R2RT's Blog Post: Recurrent Neural Networks in Tensorflow I </a>.</p>

<p>After debugging I indentified the problem was coming from the <code>ranger_input_producer</code> in <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py"" rel=""nofollow noreferrer"">tensorflow.models.rnn.ptb.reader.py</a> (line. 115).</p>

<p>I isolated it in the smallest example:</p>

<pre><code>import tensorflow as tf

epoch_size = 20
i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()
</code></pre>

<p>Which is what <code>ptb_producer</code> (with variable value for <code>epoch_size</code>).
In turns out that this code, as is, does not terminate (I don't even call any <code>session.run(...)</code> nor use CPU. I guess that the queue is waiting something as <a href=""https://stackoverflow.com/a/40407781/5903959"">suggested by Daving Wong</a>. </p>

<p>Any clues?
Thx</p>

<p>pltrdy</p>
","python, tensorflow, recurrent-neural-network, word-embedding","<p>If you just use code of 
<code>with tf.Session() as sess:</code>,
you must open the thread explicitly with 
<code>threads = tf.train.start_queue_runners()</code>. 
But in ptb_word_lm.py, It uses the codes like this 
<code>sv = tf.train.Supervisor()  with sv.managed_session() as sess:</code>,
the Supervisor() function contains something which starts the thread implicitly</p>
",6,3,1351,2016-11-03 16:16:34,https://stackoverflow.com/questions/40406469/tf-train-range-input-producerepoch-size-shuffle-true-does-not-terminate-nor-i
Questions about word embedding(word2vec),"<p>I am trying to understand word2vec(word embedding) architecture, and I have few questions about it:</p>

<ul>
<li>first, why is word2vec model considered a log-linear model? Is it because it uses a soft max at output layer?</li>
<li>second, why does word2vec remove hidden layer? Is it just because of computational complexity?</li>
<li>third, why does word2vec not use activation function? (as compared to NNLM(Neural Network Language Model).</li>
</ul>
","neural-network, word2vec, word-embedding","<blockquote>
  <p>first, why word2vec model is log-linear model? because it uses a soft max at output layer?</p>
</blockquote>

<p>Exactly, softmax is a log-linear classification model. The intent is to obtain values at the output that can be considered a posterior probability distribution</p>

<blockquote>
  <p>second, why word2vec removes hidden layer? it just because of
  computational complexity?
  third, why word2ved don't use activation function? compare for
  NNLM(Neural Network Language Model).</p>
</blockquote>

<p>I think your second and third question are linked in the sense that an extra hidden layer and an activation function would make the model more complex than necessary. Note that while no activation is explicitly formulated, we could consider it to be a linear classification function. It appears that the dependencies that the word2vec models try to model can be achieved with a linear relation between the input words.</p>

<p>Adding a non-linear activation function allows the neural network to map more complex functions, which could in turn lead to fit the input onto something more complex that doesn't retain the dependencies word2vec seeks.</p>

<p>Also note that linear outputs don't saturate which facilitates gradient-based learning. </p>
",4,1,1035,2017-02-28 05:42:33,https://stackoverflow.com/questions/42501035/questions-about-word-embeddingword2vec
Product merge layers with Keras functionnal API for Word2Vec model,"<p>I am trying to implement a Word2Vec CBOW with negative sampling with Keras, following the code found <a href=""https://github.com/abaheti95/Deep-Learning/blob/master/word2vec/keras/cbow_model.py"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>EMBEDDING_DIM = 100

sentences = SentencesIterator('test_file.txt')
v_gen = VocabGenerator(sentences=sentences, min_count=5, window_size=3,
                       sample_threshold=-1, negative=5)

v_gen.scan_vocab()
v_gen.filter_vocabulary()
reverse_vocab = v_gen.generate_inverse_vocabulary_lookup('test_lookup')

# Generate embedding matrix with all values between -1/2d, 1/2d
embedding = np.random.uniform(-1.0 / (2 * EMBEDDING_DIM),
                              1.0 / (2 * EMBEDDING_DIM),
                              (v_gen.vocab_size + 3, EMBEDDING_DIM))

# Creating CBOW model
# Model has 3 inputs
# Current word index, context words indexes and negative sampled word indexes
word_index = Input(shape=(1,))
context = Input(shape=(2*v_gen.window_size,))
negative_samples = Input(shape=(v_gen.negative,))

# All inputs are processed through a common embedding layer
shared_embedding_layer = (Embedding(input_dim=(v_gen.vocab_size + 3),
                                    output_dim=EMBEDDING_DIM,
                                    weights=[embedding]))

word_embedding = shared_embedding_layer(word_index)
context_embeddings = shared_embedding_layer(context)
negative_words_embedding = shared_embedding_layer(negative_samples)

# Now the context words are averaged to get the CBOW vector
cbow = Lambda(lambda x: K.mean(x, axis=1),
              output_shape=(EMBEDDING_DIM,))(context_embeddings)

# Context is multiplied (dot product) with current word and negative
# sampled words
word_context_product = merge([word_embedding, cbow], mode='dot')
negative_context_product = merge([negative_words_embedding, cbow],
                                 mode='dot',
                                 concat_axis=-1)

# The dot products are outputted
model = Model(input=[word_index, context, negative_samples],
              output=[word_context_product, negative_context_product])

# Binary crossentropy is applied on the output
model.compile(optimizer='rmsprop', loss='binary_crossentropy')
print(model.summary())

model.fit_generator(v_gen.pretraining_batch_generator(reverse_vocab),
                    samples_per_epoch=10,
                    nb_epoch=1)
</code></pre>

<p>However, I get an  error during the merge part because Embedding layer is a 3D tensor while cbow is only 2 dimensions. I assume I need to reshape the embedding (which is [?, 1, 100]) to [1, 100] but I can't find how to reshape with the functional API.
I am using the Tensorflow backend.</p>

<p>Also, if someone can point to an other implementation of CBOW with Keras (Gensim free), I would love to have a look to it!</p>

<p>Thank you!</p>

<p>EDIT: Here is the error</p>

<pre><code>Traceback (most recent call last):
  File ""cbow.py"", line 48, in &lt;module&gt;
    word_context_product = merge([word_embedding, cbow], mode='dot')
    .
    .
    .
ValueError: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [?,1,100], [?,100].
</code></pre>
","python, nlp, keras, word2vec, word-embedding","<pre><code>ValueError: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [?,1,100], [?,100].
</code></pre>

<p>Indeed you need to reshape the <code>word_embedding</code> tensor. Two ways to do it :</p>

<ul>
<li><p>Either you use the <code>Reshape()</code> layer, imported from <code>keras.layers.core</code>, this is done like :</p>

<pre><code>word_embedding = Reshape((100,))(word_embedding)
</code></pre>

<p>the argument of <code>Reshape</code> is a tuple with the target shape.</p></li>
<li><p>Or you can use <code>Flatten()</code> layer, also imported from <code>keras.layers.core</code>, used like this :</p>

<pre><code>word_embedding = Flatten()(word_embedding)
</code></pre>

<p>taking nothing as an argument, it will just remove ""empty"" dimensions.</p></li>
</ul>

<p>Does this help you? </p>

<p><strong>EDIT :</strong></p>

<p>Indeed the second <code>merge()</code> is a bit more tricky. The <code>dot</code> merge in Keras only accepts tensors of the same rank, so same <code>len(shape)</code>.
So what you will do is use a <code>Reshape()</code> layer to add back that 1 empty dimension, then use the feature <code>dot_axes</code> instead of <code>concat_axis</code> which is not relevant for a <code>dot</code> merge.
This is what I propose you for the solution :</p>

<pre><code>word_embedding = shared_embedding_layer(word_index)
# Shape output = (None,1,emb_size)
context_embeddings = shared_embedding_layer(context)
# Shape output = (None, 2*window_size, emb_size)
negative_words_embedding = shared_embedding_layer(negative_samples)
# Shape output = (None, negative, emb_size)

# Now the context words are averaged to get the CBOW vector
cbow = Lambda(lambda x: K.mean(x, axis=1),
                     output_shape=(EMBEDDING_DIM,))(context_embeddings)
# Shape output = (None, emb_size)
cbow = Reshape((1,emb_size))(cbow)
# Shape output = (None, 1, emb_size)

# Context is multiplied (dot product) with current word and negative
# sampled words
word_context_product = merge([word_embedding, cbow], mode='dot')
# Shape output = (None, 1, 1)
word_context_product = Flatten()(word_context_product)
# Shape output = (None,1)
negative_context_product = merge([negative_words_embedding, cbow], mode='dot',dot_axes=[2,2])
# Shape output = (None, negative, 1)
negative_context_product = Flatten()(negative_context_product)
# Shape output = (None, negative)
</code></pre>

<p>Is it working? :)</p>

<p>The problem comes from the rigidity of TF regarding the matrix multiplication. Merge with ""dot"" mode calls the backend <code>batch_dot()</code> function and, as opposed to Theano, TensorFlow requires the matrix to have the same rank : <a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">read here</a>. </p>
",2,2,641,2017-02-28 17:18:24,https://stackoverflow.com/questions/42514986/product-merge-layers-with-keras-functionnal-api-for-word2vec-model
Caffe Embed Layer Inputs,"<p>What type of input does the Embed layer in Caffe take?
Does it take words already encoded in one hot form?</p>

<p>Suppose, N = number of words in input sentence ; M = vocabulary size </p>

<p>Then one hot vector for a single sentence will be of order N x M</p>

<p>Does this mean the the input dim parameter will be N?</p>

<p>Lastly in what format should the sentences be saved so that Caffe embed layer can read it properly?</p>
","machine-learning, neural-network, deep-learning, caffe, word-embedding","<p>Please see the documentation of <a href=""http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1EmbedLayer.html#details"" rel=""nofollow noreferrer""><code>&quot;Embed&quot;</code></a> layer:</p>
<blockquote>
<p>A layer for learning &quot;embeddings&quot; of one-hot vector input. Equivalent to an InnerProductLayer with one-hot vectors as input, but for efficiency the input is the &quot;hot&quot; index of each column itself.</p>
</blockquote>
<p>Therefore, your input is not a &quot;hot vector&quot; representing a word (or a character, or an &quot;item&quot;) but rather a compact representation of the word: an integer index of the word in the disctionary.</p>
<p>So, if you have <code>M=1000</code> words in your dictionary and you want to learn an embedding into 100 dimensional space:</p>
<pre><code>layer {
  name: &quot;embed1000_to_100&quot;
  type: &quot;Embed&quot;
  bottom: &quot;compact_one_hot_dim1000&quot;
  top: &quot;embed1000_to_100&quot;
  embed_param {
    num_output: 100 # output dimension
    input_dim: 1000
  }
}
</code></pre>
<p>Note that the data of <code>&quot;compact_one_hot_dim1000&quot;</code> should be integers in the range (0..999).</p>
<p>See <a href=""http://caffe.help/manual/layers/embed.html"" rel=""nofollow noreferrer"">caffe.help</a> for more information.</p>
",3,1,1324,2017-03-10 15:30:00,https://stackoverflow.com/questions/42722048/caffe-embed-layer-inputs
word2vec word to color association?,"<p>I'm trying to get color associations like so:</p>

<pre><code>apple -&gt; red

banana -&gt; yellow

grass -&gt; green

sky -&gt; blue
</code></pre>

<p>using the GoogleNews-vectors-negative300.bin vectors, I first tried</p>

<pre><code>wv.similarity('apple',color)
</code></pre>

<p>where color is a primary color, eg 'red','yellow','blue' etc.</p>

<p>with fruits 'orange' is always the highest color association, probably because it conflates the color and the fruit. When I remove orange the results are still strange:</p>

<pre><code>apple:

[('violet', 0.24978276994901127), ('green', 0.20656763297902447), ('red', 0.19834849929308024), ('yellow', 0.18963902211016806), ('cyan', 0.17945308073294569), ('blue', 0.13687176308102386)]

cherry:
[('violet', 0.27348741504236473), ('red', 0.25540695681746473), ('yellow', 0.24285150471329794), ('blue', 0.20400566489159569), ('green', 0.18741563150077917), ('cyan', 0.12736182067644364)]

banana:
[('yellow', 0.27708333668133234), ('green', 0.25977272141145935), ('red', 0.24736077659820707), ('violet', 0.23909913025940599), ('cyan', 0.16519069493338848), ('blue', 0.15660144725154587)]
</code></pre>

<p>so apparently 'violet' aligns with 'apple' and 'cherry' along some other dimension (maybe they're plants?).</p>

<p>I tried formulating it as an analogy. This worked for some objects but doesn't generalize well:</p>

<pre><code>wv.most_similar(restrict_vocab=100000, positive=['apple','yellow'], negative=['banana'])
[(u'red', 0.5296207666397095), (u'orange', 0.501822829246521), (u'bright_yellow', 0.49562686681747437), (u'purple', 0.4909234642982483), (u'blue', 0.465557336807251), (u'pink', 0.43768370151519775), (u'colored', 0.4296746551990509), (u'brown', 0.4290006756782532), (u'bright_orange', 0.4261433482170105), (u'yellows', 0.4199957549571991)]

wv.most_similar(restrict_vocab=100000, positive=['grass','yellow'], negative=['banana'])
[(u'bright_yellow', 0.4722655713558197), (u'blue', 0.45448029041290283), (u'red', 0.43442922830581665), (u'lawns', 0.4275570809841156), (u'maroon', 0.4197036325931549), (u'bright_orange', 0.41167205572128296), (u'brown', 0.4110153317451477), (u'purple', 0.4074830412864685), (u'grassy', 0.4017237722873688), (u'striped', 0.40009182691574097)]
</code></pre>

<p>I also tried the facebook fasttext embeddings but results were worse. How should I approach this problem and isolate the vector for ""common color of object""?</p>
","word2vec, word-embedding","<p>This is an interesting thing to try, but whether word-vectors would successfully encode these relationships at all seems to me an open question, and any level of success would depend on a lot of particular training choices, including corpus and parameters. </p>

<p>In particular, the <code>GoogleNews</code> vector set, trained on published news-stories, might not be as strong in associating fruits-with-colors as other choices. (For example, I'd guess the text of children's books, or transcripts of educational programs, would induce stronger vector-arrangements for this purpose.)</p>

<p>A <code>violet</code> is also a flower, so that partially explains its closeness to other fruits. And as you note the prominence of 'orange' as both a fruit and color creates issues when it's involved. There is no disambiguation between multiple word-senses in plain Word2Vec, or the <code>GoogleNews</code> vectors, although there's been some research using or creating word-vectors to distinguish alternate word-senses. (Even words like 'blue', 'green', 'yellow', 'cherry', and 'grass' have alternate meanings that may be affecting vector-positioning.)</p>

<p>I do suspect the analogy/directional approach may have more luck than pure-similarity. (That is, asking ""which colorword is in the direction learned from these other objectword->colorword examples?"", rather than ""which colorword is absolutely closest to this objectword?"")</p>

<p>You might want try objectword->colorword example pairs from a larger domain, or try additional vector-math to see if other definitions/composites better match the answers you expect. </p>

<p>For example. maybe your ""learn-the-direction"" examples should include non-fruits – sky->blue, coal->black, etc. </p>

<p>And I recall seeing once the suggestion that analogy-solving could be improved if many-known-good analogies of the same-relationship were used together, rather than just one. (That is, compose a direction from all of ""England:London"", ""Russia:Moscow"", ""France:Paris"" before probing ""Germany:<em>?</em>"", rather than just one. I'm not sure if adding more vectors to the gensim <code>most_similar()</code> <code>positive</code>/<code>negative</code> lists has the same effect or you need to do the differencing/averaging/norming yourself.)</p>

<p>An interesting paper on interpretation and improvement of analogy results is Levy &amp; Goldberg's ""<a href=""http://www.aclweb.org/anthology/W14-1618"" rel=""nofollow noreferrer"">Linguistic Regularities in Sparse and Explicit Word Representations</a>"".</p>

<p>There's other work that tries to train or skew word (or concept/entity) vectors to be better at question-answering, which might be relevant, but other than suggesting that as a search term, I don't know any technique that's especially appropriate or ready-to-use in available libraries. </p>
",2,2,1063,2017-03-10 16:41:23,https://stackoverflow.com/questions/42723465/word2vec-word-to-color-association
How to get paragraph vector for a new paragraph?,"<p>I have a set of users and their content(1 document per user containing tweets of that user). I am planning to use a distributed vector representation of some size N for each user. One way is to take pre trained wordvectors on twitter data and average them to get distributed vector of an user. I am planning to use doc2vec for better results.But I am not quite sure if I understood the DM model given in <a href=""http://www.jmlr.org/proceedings/papers/v32/le14.pdf"" rel=""nofollow noreferrer"">Distributed Representations of Sentences and Documents</a>.</p>

<p>I understand that we are assigning one vector per paragraph and while predicting next word we are using that and then backpropagating the error to update the paragraph vector as well as word vector. How to use this to predict paragraph vector of a new paragraph? </p>

<p>Edit : Any toy code for gensim to compute paragraph vector of new document would be appreciated.</p>
","machine-learning, deep-learning, word-embedding","<p>The following code is based on gensim's doc2vec <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">tutorial</a>. We can instantiate and train a <a href=""http://proceedings.mlr.press/v32/le14.pdf"" rel=""nofollow noreferrer"">doc2vec</a> model to generate embeddings of size 300 with a context window of size 10 as follows:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model = Doc2Vec(size=300, window=10, min_count=2, iter=64, workers=16)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>Having trained our model, we can compute a vector for a new unseen document as follows:</p>

<pre><code>doc_id = random.randint(0, len(test_corpus))
inferred_vector = model.infer_vector(test_corpus[doc_id])
sims = model.docvecs.most_simlar([inferred_vector], topn=len(model.docvecs))
</code></pre>

<p>This will return a 300-dimensional representation of our test document and compute top-N most similar documents from the training set based on cosine similarity.</p>
",1,1,981,2017-03-27 11:34:55,https://stackoverflow.com/questions/43045295/how-to-get-paragraph-vector-for-a-new-paragraph
Saving output (context) embeddings in word2vec (gensim implementation) as a final model,"<p>I have studied <code>word2vec</code> implementation in gensim, I am aware that input vectors are in <code>syn0</code>, output vectors are in <code>syn1</code> and <code>syn1neg</code> if negative sampling.</p>

<p>I know I can access similarity between input and output embeddings like this:</p>

<pre><code>outv = KeyedVectors()
outv.vocab = model.wv.vocab
outv.index2word = model.wv.index2word  
outv.syn0 = model.syn1neg 
inout_similars = outv.most_similar(positive=[model['cousin']])
</code></pre>

<p>My question is, if it is possible to save output embeddings (from <code>syn1</code> or <code>syn1neg</code> matrix) as final model. For example, when <code>model.save()</code>, so that it outputs output embeddings (or where exactly in the code of <code>word2vec.py</code> I could access and modify that). I need this in order to use these output embeddings as input to classifier. I have done it previously in brute-force approach, so I would like to access output embeddings easily.</p>
","python, gensim, word2vec, word-embedding","<p>Your object <code>outv</code>, as an instance of <code>KeyedVectors</code>, has its own <code>save()</code> method (inherited from the <code>SaveLoad</code> superclass defined in <code>gensim/utils.py</code>) and <code>save_word2vec_format()</code> method. Each would save them in a manner you could reload into Python code again later. </p>
",2,1,1609,2017-03-29 15:57:20,https://stackoverflow.com/questions/43098535/saving-output-context-embeddings-in-word2vec-gensim-implementation-as-a-fina
How to resolve R Error using text2vec glove function: unused argument (grain_size = 100000)?,"<p>Trying to work through the text2vec vignette in the <a href=""https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html#word_embeddings"" rel=""nofollow noreferrer"">documentation</a> and <a href=""https://stackoverflow.com/questions/39514941/preparing-word-embeddings-in-text2vec-r-package"">here</a> to create word embeddings for some tweets:</p>

<pre><code>head(twtdf$Tweet.content)
[1] ""$NFLX $GS $INTC $YHOO $LVS\n$MSFT $HOG $QCOM $LUV $UAL\n$MLNX $UA $BIIB $GOOGL $GM $V\n$SKX $GE $CAT $MCD $AAL $SBUX""            
[2] ""Good news frequent fliers. @AmericanAir says lower fares will be here for awhile""                   
[3] ""Wall St. closing out the week with more earnings. What to watch:\nâ–¶ï¸Ž $MCD\nâ–¶ï¸Ž $AAL\nâ–¶ï¸Ž $CAT\n""
[4] ""Barrons loves $AAL at low multiple bc it's \""insanely profitable\"". Someone tell them how cycles+ multiples work.""               
[5] ""These airlines are now offering in-flight Wi-Fi $DAL $AAL""          
</code></pre>

<p>Pretty much followed the guide as given:</p>

<pre><code>library(text2vec)
require(text2vec)

twtdf &lt;- read.csv(""tweets.csv"",header=T, stringsAsFactors = F)
twtdf$ID &lt;- seq.int(nrow(twtdf))

tokens = twtdf$Tweet.content %&gt;% tolower %&gt;%  word_tokenizer
length(tokens)
it = itoken(tokens)
# create vocabulary
v = create_vocabulary(it) %&gt;% 
  prune_vocabulary(term_count_min = 5)

# create co-occurrence vectorizer
vectorizer = vocab_vectorizer(v, grow_dtm = F, skip_grams_window = 5L)

#dtm &lt;- create_dtm(it, vectorizer, grow_dtm = R)

it = itoken(tokens)
tcm = create_tcm(it, vectorizer)
glove_model = glove(tcm, word_vectors_size = 50, vocabulary = v, x_max = 10, learning_rate = .2)

fit(tcm, glove_model, n_iter = 15)

#when this was executed, R couldn't find the function
#fit &lt;- GloVe(tcm = tcm, word_vectors_size = 50, x_max = 10, learning_rate = 0.2, num_iters = 15)
</code></pre>

<p>However, whenever I get to executing <code>glove_model</code>, I get the following error:</p>

<pre><code>Error in .subset2(public_bind_env, ""initialize"")(...) : 
  unused argument (grain_size = 100000)
In addition: Warning message:
'glove' is deprecated.
Use 'GloVe' instead.
</code></pre>

<p>*I did try using <code>GloVe</code> instead, but I get the error that R can't find the function despite reinstalling the text2vec package and <code>require</code>ing it.</p>

<p>To check to make sure it wasn't some sort of formatting issue with my data, I tried running the code with the <code>movie_review</code> data and encounter the same problem. Just to be thorough, I additionally tried specifying the <code>grain_size</code> argument, but get the same error. I checked the issues on the Git repository and didn't see anything nor anything on this site or in an internet query. </p>

<p>Anyone else encounter this or is a new person problem?</p>
","r, nlp, word-embedding, text2vec","<p>Just use correct constructor for model : <code>glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = vocab, x_max = 10)</code></p>

<p><code>glove()</code> is old one from very old package version.</p>
",1,3,1959,2017-04-11 10:49:26,https://stackoverflow.com/questions/43343820/how-to-resolve-r-error-using-text2vec-glove-function-unused-argument-grain-siz
Analogies from Word2Vec in TensorFlow?,"<p>I implemented Word Embeddings in Tensor Flow similarly to the code <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">here</a> I was able to get the final embeddings (final_embeddings), but I would like to evaluate the embeddings using the analogies typical of this exercise. How can I identify which term corresponds to which row in the final embeddings array? Alternatively, is there an implementation in Tensor Flow for this? Any help would be greatly appreciated (specifics and resources would be a plus;) ). Thanks!</p>
","tensorflow, word2vec, word-embedding","<p>Recommend this <a href=""http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"" rel=""nofollow noreferrer"">conceptual tutorial</a> to you. 
If you are using skip-gram, the input is one-hot encoding. So the index of the <code>1</code> is the index of the vector of the word.</p>

<p><a href=""https://i.sstatic.net/7tZsZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7tZsZ.png"" alt=""enter image description here""></a></p>

<p>The implementation in tensorflow is quite simple. You may want to see this function: <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""nofollow noreferrer"">tf.nn.embedding_lookup</a></p>

<p>For example:</p>

<pre><code>embed = tf.nn.embedding_lookup(embedding, inputs)
</code></pre>

<p>The embed is the vector you are looking for.</p>
",1,2,316,2017-05-03 02:17:36,https://stackoverflow.com/questions/43750010/analogies-from-word2vec-in-tensorflow
Weights update in Tensorflow embedding layer with pretrained fasttext weights,"<p>I'm not sure if my understanding is correct but...</p>

<p>While training a seq2seq model, one of the purpose I want to initiated a set of pre-trained fasttext weights in the embedding layers is to decrease the unknown words in the test environment (these unknown words are not in training set). Since pre-trained fasttext model has larger vocabulary, during test environment, the unknown word can be represented by fasttext out-of-vocabulary word vectors, which supposed to have similar direction of the semantic similar words in the training set.</p>

<p>However, due to the fact that the initial fasttext weights in the embedding layers will be updated through the training process (updating weights generates better results). I am wondering if the updated embedding weights would distort the relationship of semantic similarity between words and undermine the representation of fasttext out-of-vocabulary word vectors? (and, between those updated embedding weights and word vectors in the initial embedding layers but their corresponding ID didn't appear in the training data)</p>

<p>If the input ID can be distributed represented vectors extracted from pre-trained model and, then, map these pre-trained word vectors (fixed weights while training) via a lookup table to the embedding layers (these weights will be updated while training), would it be a better solution?</p>

<p>Any suggestions will be appreciated!</p>
","tensorflow, word-embedding","<p>You are correct about the problem: when using pre-trained vector and fine-tuning them in your final model, the words that are infrequent or hasn't appear in your training set won't get any updates.</p>

<p>Now, usually one can test how much of the issue for your particular case this is. E.g. if you have a validation set, try fine-tuning and not fine-tuning the weights and see what's the difference in model performance on validation set.</p>

<p>If you see a big difference in performance on validation set when you are not fine-tuning, here is a few ways to handle this:</p>

<p>a) Add a linear transformation layer after not-trainable embeddings. Fine-tuning embeddings in many cases does affine transformations to the space, so one can capture this in a separate layer that can be applied at test time.</p>

<p>E.g. A is pre-trained embedding matrix:</p>

<pre class=""lang-py prettyprint-override""><code>embeds = tf.nn.embedding_lookup(A, tokens)
X = tf.get_variable(""X"", [embed_size, embed_size])
b = tf.get_vairable(""b"", [embed_size])
embeds = tf.mul(embeds, X) + b
</code></pre>

<p>b) Keep pre-trained embeddings in the not-trainable embedding matrix A. Add trainable embedding matrix B, that has a smaller vocab of popular words in your training set and embedding size. Lookup words both in A and B (and if word is out of vocab use ID=0 for example), concat results and use it input to your model. This way you will teach your model to use mostly A and sometimes rely on B for popular words in your training set.</p>

<pre class=""lang-py prettyprint-override""><code>fixed_embeds = tf.nn.embedding_lookup(A, tokens)
B = tf.get_variable(""B"", [smaller_vocab_size, embed_size])
oov_tokens = tf.where(tf.less(tokens, smaller_vocab_size), tokens, tf.zeros(tf.shape(tokens), dtype=tokens.dtype))
dyn_embeds = tf.nn.embedding_lookup(B, oov_tokens)
embeds = tf.concat([fixed_embeds, dyn_embeds], 1)
</code></pre>
",1,2,2125,2017-05-19 02:44:43,https://stackoverflow.com/questions/44060592/weights-update-in-tensorflow-embedding-layer-with-pretrained-fasttext-weights
Does it make sense to talk about skip-gram and cbow when using The Glove method?,"<p>I'm trying different word embeddings methods, in order to pick the approache that works the best for me. I tried word2vec and FastText. Now, I would like to try Glove. In both word2vec and FastText, there is two versions: Skip-gram (predict context from word) and CBOW (predict word from context). But in Glove python package, there is no parameter that enables you to choose whether you want to use skipg-gram or Cbow. </p>

<p>Given that Glove does not work the same way as w2v, I'm wondering: Does it make sense to talk about skip-gram and cbow when using The Glove method ?</p>

<p>Thanks in Advance</p>
","python-3.x, word2vec, word-embedding","<p>Not really, skip-gram and CBOW are simply the names of the two Word2vec models. They are shallow neural networks that generate word embeddings by predicting a context from a word and vice versa, and then treating the output of the hidden layer as the vector/representation. GloVe uses a different approach, making use of the global statistics of a corpus by training on a co-occurrence matrix, rather than local context windows.</p>
",3,2,901,2017-05-22 12:40:34,https://stackoverflow.com/questions/44113128/does-it-make-sense-to-talk-about-skip-gram-and-cbow-when-using-the-glove-method
Importing word vectors from tensorflow into gensim,"<p>I want to import word vecters created from tensorflow and utilize it at gensim.</p>

<p>there is a method <code>gensim.models.KeyedVectors.load_word2vec_format</code></p>

<p>so I tried this method by following exactly the same way in <a href=""https://stackoverflow.com/questions/42186543/training-wordvec-in-tensorflow-importing-to-gensim"">Training wordvec in Tensorflow, importing to Gensim</a></p>

<p>Example:</p>

<blockquote>
  <p>2 3</p>
  
  <p>word0 -0.000737 -0.002106 0.001851</p>
  
  <p>word1 -0.000878 -0.002106 0.002834</p>
</blockquote>

<p>Save the file and then load with kwarg binary=False:</p>

<pre><code>model = Word2Vec.load_word2vec_format(filename, binary=False)
</code></pre>

<p>but error like</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    model=gensim.models.KeyedVectors.load_word2vec_format('test.w2v')
  File ""C:\Users\cbj\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 243, in load_word2vec_format
    raise EOFError(""unexpected end of input; is count incorrect or file otherwise damaged?"")
EOFError: unexpected end of input; is count incorrect or file otherwise damaged?
</code></pre>

<p>raised</p>

<p>how can I solve this problem?</p>
","python, tensorflow, gensim, word2vec, word-embedding","<p>This error is raised when the number of vector data doesn't match the number you provided at the first line.</p>

<p>If the first line wrote <code>2 3</code>, you should have exactly <code>2</code> lines below. Make sure that there's no empty line at the end of your file and of course some where in your file.</p>
",1,1,2488,2017-06-05 15:00:06,https://stackoverflow.com/questions/44371835/importing-word-vectors-from-tensorflow-into-gensim
Invalid Argument Exception Indices Not in Vectors in Word Embeddings in TensorFlow,"<p>I am trying to run the LSTM Code and for this trying to connect the word2Vec word embeddings input, But getting error in getting the embedding look up.</p>

<p>Following is the code:</p>

<pre><code>batchSize = 24
lstmUnits = 64
numClasses = 2
iterations = 100000
maxSeqLength = 250
numDimensions = 128    
import tensorflow as tf
tf.reset_default_graph()

labels = tf.placeholder(tf.float32, [batchSize, numClasses])
input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])

data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)
# word Vector Shape = (13277, 128)
data = tf.nn.embedding_lookup(wordVectors,input_data)




saver = tf.train.Saver()
sess.run(tf.global_variables_initializer())
try:
    for i in range(iterations):
   #nextBatch shape is (24, 250)
        nextBatch, nextBatchLabels = getTrainBatch()
        sess.run(optimizer, feed_dict={input_data: nextBatch, labels: nextBatchLabels})
except Exception as ex:
    print(ex)
</code></pre>

<p>There might be small step i'm missing. what can it be.
When i run the code, I get exception as :
<a href=""https://i.sstatic.net/HUNM7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HUNM7.png"" alt=""enter image description here""></a></p>
","python, tensorflow, lstm, word-embedding","<p>I have simplified the code that you have reported in order to let you understand how to use word embeddings in your case. In addition, you haven't specified everything (see the <code>optimizer</code> variable) so it is not possible to completely reproduce your code. </p>

<p>I report here a simple snippet that will allow you to get word embeddings from an input matrix of shape <code>(batchSize, maxSeqLength)</code>.</p>

<pre><code>batchSize = 24
lstmUnits = 64
numClasses = 2
iterations = 100000
maxSeqLength = 250
numDimensions = 128
numTokens = 50

import tensorflow as tf
import numpy as np

session = tf.InteractiveSession()
input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])
# you should NOT use tf.Variable() but tf.get_variable() instead
embeddings_weights = tf.get_variable(""embeddings_weights"",  initializer=tf.random_normal_initializer(0, 1), shape=(numTokens, numDimensions))
input_embeddings = tf.nn.embedding_lookup(embeddings_weights, input_data)
result = session.run(input_embeddings, feed_dict={input_data: np.random.randint(0, numTokens, (batchSize, maxSeqLength))})
print(result.shape)
// should print (24, 250, 300)
</code></pre>

<p>If you are trying to understand why you receive that error you should debug your code and see if in the training data specified there are indexes which are not valid. In my snippet code, by using <code>np.random.randint()</code>, I forced the output elements to be in the range (0, numTokens) in order to avoid the error that you got. This happens because TensorFlow is not able to complete the lookup operation for an ID which goes out of range! </p>
",0,0,703,2017-06-19 19:54:10,https://stackoverflow.com/questions/44638843/invalid-argument-exception-indices-not-in-vectors-in-word-embeddings-in-tensorfl
How does the tensorflow word2vec tutorial update embeddings?,"<p>This thread comes close: <a href=""https://stackoverflow.com/questions/37982478/what-is-the-purpose-of-weights-and-biases-in-tensorflow-word2vec-example?noredirect=1&amp;lq=1"">What is the purpose of weights and biases in tensorflow word2vec example?</a> </p>

<p>But I am still missing something from my interpretation of this: <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a></p>

<p>From what I understand, you feed the network the indices of target and context words from your dictionary.</p>

<pre><code>_, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
average_loss += loss_val
</code></pre>

<p>The batch inputs are then looked up to return the vectors that are randomly generated at the beginning</p>

<pre><code>    embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_inputs)
</code></pre>

<p>Then an optimizer adjusts the weights and biases to best predict the label as opposed to num_sampled random alternatives</p>

<pre><code> loss = tf.reduce_mean(
  tf.nn.nce_loss(weights=nce_weights,
                 biases=nce_biases,
                 labels=train_labels,
                 inputs=embed,
                 num_sampled=num_sampled,
                 num_classes=vocabulary_size))

  # Construct the SGD optimizer using a learning rate of 1.0.
  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)
</code></pre>

<p>My questions are as follows:</p>

<ol>
<li><p><strong>Where do the embeddings variable get updated?</strong>. It appears to me that I could get the final result by either running the index of a word through the neural network, or by just taking the <code>final_embeddings</code> vectors and using that. But I do not understand where <code>embeddings</code> is ever changed from its random initialization.</p></li>
<li><p>If I were to draw this computation graph, what would it look like (or better yet, <strong>what is the best way to actually do so</strong>)?</p></li>
<li><p>Is this running all of the context/target pairs in the batch at once? Or one by one?</p></li>
</ol>
","python, tensorflow, word2vec, word-embedding","<p><strong>Embeddings:</strong> Embeddings is a variable. It gets updated every time you do backprop (while running optimizer with loss)</p>

<p><strong>Grpah:</strong> Did you try saving the graph and displaying it in tensorboard ? Is this what you're looking for ?</p>

<p><strong>Batching:</strong> Atleast in the example you linked, he is doing batch processing using the function at line 96. <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L96"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L96</a></p>

<p>Please correct me if I misunderstood your question. </p>
",2,2,508,2017-06-22 16:22:32,https://stackoverflow.com/questions/44704751/how-does-the-tensorflow-word2vec-tutorial-update-embeddings
Reloading Keras Tokenizer during Testing,"<p>I followed the tutorial here: (<a href=""https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"" rel=""noreferrer"">https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html</a>)</p>

<p>However, I modified the code to be able to save the generated model through <code>h5py</code>. Thus, after running the training script, I have a generated <code>model.h5</code> in my directory. </p>

<p>Now, when I want to load it, my problem is that I'm confused as to how to re-initiate the <code>Tokenizer</code>. The tutorial has the following line of code:</p>

<pre><code>tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
</code></pre>

<p>But hypothetically, if I reload the <code>model.h5</code> in a different module, I'll need to create another <code>Tokenizer</code> to tokenize the test set. But then, the new <code>Tokenizer</code> will be fit on the test data thus creating a completely different word table.</p>

<p>Therefore, my question is: How do I reload the <code>Tokenizer</code> that was trained on the training dataset? Am I in some way misunderstanding the functionality of the <code>Embedding</code> layer in Keras? Right now, I'm assuming that since we mapped certain word indices to their corresponding embedding vectors based on the pre-trained word embeddings, the word indices need to be consistent. However, this is not possible if we perform another <code>fit_on_texts</code> on the test dataset.</p>

<p>Thank you and looking forward to your answers!</p>
","tensorflow, keras, tokenize, text-classification, word-embedding","<p><a href=""https://stackoverflow.com/questions/44758405/keras-tokenizer-random-results-due-to-different-mapping-of-text-tokens-and-ids"">Check out this question</a> 
The commenter recommends using a pickle to save the object &amp; state, though the question still remains why this kind of functionality is not built into keras. </p>
",2,8,3587,2017-06-26 13:31:26,https://stackoverflow.com/questions/44760961/reloading-keras-tokenizer-during-testing
"Will Word2Vec be more efficient in text based Plagiarism detection than WordNet or any other word embeddings like GloVe, fastText etc?","<p>I am a beginner in learning Word2Vec and just started to do some study on Word2vec from the Internet. I have gone through almost all the questions in Quora and StackOverflow but didn't get my answer anywhere from the previous questions. So my question is-</p>
<ol>
<li>Is it possible to apply word2vec in plagiarism detection?</li>
<li>If yes, then will Word2Vec be more efficient in text-based Plagiarism detection than WordNet or any other word embeddings like GloVe, fastText, etc?</li>
</ol>
<p>Thanks in advance.</p>
","nlp, wordnet, word2vec, word-embedding, plagiarism-detection","<p>Yes, these ""dense embedding"" models of word meaning like word2vec may be useful in plagiarism detection. (They're also likely useful in obfuscating plagiarism from simple detectors, as they can assist automated transforms on existing text that change the words while keeping the meaning similar.)</p>

<p>Only by testing within a particular system and with respect to quantitative evaluations will you know for sure how well it can work, or whether a particular embedding is better or worse than something like WordNet. </p>

<p>Among word2vec, fastttext, and GloVE, results will probably be very similar – they all use roughly the same info (word co-occurrences within a sliding context window) to make maximally-predictive word-vectors – so they behave very similarly with similar training data. </p>

<p>Any differences are subtle – the non-GLoVe options might work better for very larger vocabularies; fasttext is essentially the word2vec in some modes, but adds new options for either modeling subword ngrams (which can then help to create better-than-random vectors for future out-of-vocabulary words) or optimizing the vectors for classification problems. </p>

<p>But the vectors for known words, which can be trained with plentiful training data, are going to be very similar in capabilities if the training processes are similarly meta-optimized for your task.  </p>
",3,1,1253,2017-06-27 17:56:54,https://stackoverflow.com/questions/44786888/will-word2vec-be-more-efficient-in-text-based-plagiarism-detection-than-wordnet
"Word Embedding, LookupTable, Word Embedding Visualizations","<p>I need to ask few questions regarding word embeddings.....could be basic.</p>
<ol>
<li>When we convert a one-hot vector of a word for instance king <code>[0 0 0 1 0]</code> into an embedded vector <code>E = [0.2, 0.4, 0.2, 0.2]</code>.... is there any importance for each index in resultant word vector? For instance <code>E[1]</code> which is 0.2.... what specifically <code>E[1]</code> defines (although I know its basically a transformation into another space).... or word vector collectively defines context but not individually...</li>
<li>How the dimension (reduced or increased) of a word vector matters as compared to the original one-hot vector?</li>
<li>How can we define a lookup table in terms of an embedding layer?</li>
<li>is lookup table a kind of random generated table or it already been trained separately with respect to data instance in data and we just use it later on in Neural Network operations?</li>
<li>Is there any method to visualize an embedded vector at Hidden Layer (as we do have in Image based Neural Network Processing)?</li>
</ol>
<p>Thanks in advance</p>
","deep-learning, text-mining, word2vec, word-embedding","<p>1: Each element (or a group of element) in embedding vector have some meaning, but mostly unknown for human. Depend on what algorithm you use, a word embedding vector may have different meaning, but usually useful. 
For example, <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a>, similar word 'frog', 'toad' stay near each other in vector space. King - man result in vector similar to Queen. </p>

<ol start=""3"">
<li><p>Turn vocab into index. For example, you have a vocabulary list:
[dog, cat, mouse, feed, play, with]
Then the sentences: Dog play with cat => 0, 4, 5, 1
While, you have embedding matrix as follow</p>

<p>[0.1, 0.1, 0]  # comment: this is dog <br>
[0.2, 0.5, 0.1] # this is cat <br>
[...] <br>
[...] <br>
[...] <br>
[...] <br></p></li>
</ol>

<p>where first row is embedding vector of dog, second row is cat, then so on
Then, you use the index (0, 4, 5, 1) after lookup would become a matrix [[0.1, 0.1, 0][...][...][0.2, 0.5, 0.1]]</p>

<ol start=""4"">
<li>either or both

<ul>
<li>You can randomly init embedding vector and training it with gradient descent</li>
<li>You can take pretrained word vector and keep it fixed (i.e: read-only, no change). 
You can train your word vector in model and use it in another model. Our you can download pretrained word vector online. Example Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip on <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a></li>
<li>You can init with pretrained word vector and train with your model by  gradient descent</li>
</ul></li>
</ol>

<p>Update:
<strong>One-hot vector</strong> does not contain any information. You can think that one-hot vector is index of that vector in vocabulary. 
For example, Dog =>  [1, 0, 0, 0, 0, 0] and cat =>  [0, 1, 0, 0, 0, 0]. There are some different between one-hot vs index: </p>

<ul>
<li><p>if you input a list of index: [0, 4, 5, 1] to your multi-layer perceptron, it cannot learn anything (I tried...).But if you input a matrix of one-hot vector [[...1][1...][...][...]], it learn something. But it costly in term of RAM and CPU. </p></li>
<li><p>One-hot cost a lot of memory to store zeros. Thus, I suggest randomly init embedding matrix if you don't have one. Store dataset as index, and use index to look up embedding vector</p></li>
</ul>

<blockquote>
  <p>""its mean that lookup table is just a matrix of embedded vectors
  (already been trained seperately via word2vec or...) for each word in
  the vocabulary. and while in the process of neural network either we
  can use an Embedding Layer or we can just refer to embedded vector in
  lookup table for that particular embedded vector against particular
  one-hot vector.""</p>
</blockquote>

<p>Use the ""INDEX"" to look-up in lookup table. Turn dog into 0, cat into 1. One-hot vector and index contain same information, but one-hot cost more memory to store. Moreover, a lot of deeplearning framework accept index as input to embedding layer (which, output is a vector represent for a word in that index.)</p>

<blockquote>
  <p>"". How we get this embedding vector...""</p>
</blockquote>

<p>=> read paper. Here is paper about <a href=""https://arxiv.org/abs/1301.3781"" rel=""noreferrer"">Word2vec</a> and <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a>. Ask your lecturers for more detail, they are willing to help you.  </p>
",7,7,9796,2017-07-03 09:24:22,https://stackoverflow.com/questions/44881999/word-embedding-lookuptable-word-embedding-visualizations
python word2vec context similarity using surrounding words,"<p>I would like to use embeddings made by w2v in order to obtain the most likely substitute words GIVEN a context (surrounding words), rather than supplying an individual word.</p>

<p>Example:
sentence = 'I would like to go to the park tomorrow after school'</p>

<p>If I want to find candidates similar to ""park"", typically I would just leverage the similarity function from the Gensim model</p>

<pre><code>model.most_similar('park')
</code></pre>

<p>and obtain semantically similar words. However this could give me similar words to the verb 'park' instead of the noun 'park', which I was after.</p>

<p>Is there any way to query the model and give it surrounding words as context to provide better candidates?</p>
","python, gensim, word2vec, word-embedding","<p>Word2vec is not, primarily, a word-prediction algorithm. Internally it tries to do semi-predictions, to train its word-vectors, but usually these training-predictions aren't the end-use for which word-vectors are wanted.</p>

<p>That said, recent versions of gensim added a <code>predict_output_word()</code> method that (for some model modes) approximates the predictions done during training. It might be useful for your purposes. </p>

<p>Alternatively, checking for the words <code>most_similar()</code> to your initial target word that are <em>also</em> somewhat-similar to the context words might help. </p>

<p>There have been some research papers about ways to disambiguate multiple word senses (like 'to /park/ a car' versus 'walk in a /park/') during word-vector training, but I haven't seen them implemented in open source libraries. </p>
",3,1,855,2017-07-14 17:05:14,https://stackoverflow.com/questions/45108291/python-word2vec-context-similarity-using-surrounding-words
Word Embedding Relations,"<p>I want to learn more about the algebra function I can perform over the word embedding vectors. I know that by cosine similarity I can get the most similar word. But I need to do one more level of inference and get the relations below:</p>

<p>The relation of X1 to X2 is like relation of X3 to X4.</p>

<p>As and example I can say the relation of princess to prince is like women to men. I have X1 to X3 and my problem is how efficiently I can figure out what X4 can be. I tried cosine to absolute difference of vectors but it is not working.</p>
","numpy, matrix, word2vec, algebra, word-embedding","<p>You can look at exactly how the original Google-released <code>word2vec</code> code solves analogies in its <code>word-analogy.c</code> code: </p>

<p><a href=""https://github.com/tmikolov/word2vec/blob/master/word-analogy.c"" rel=""nofollow noreferrer"">https://github.com/tmikolov/word2vec/blob/master/word-analogy.c</a></p>

<p>If you're more familiar with Python, you can look how the gensim Word2Vec implementation tests analogies, in its <code>accuracy()</code> method, by reading the analogy ""a:b :: c:expected"" from the <code>questions-words.txt</code> file (as provided in the original Google word2vec package), then using <code>b</code> and <code>c</code> as positive (added) examples, and <code>a</code> as a negative example (subtracted), to then find words near the resulting vector:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/keyedvectors.py#L697"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/keyedvectors.py#L697</a></p>

<p>The operation of the used <code>most_similar()</code> function, which accepts multiple <code>positive</code> and <code>negative</code> examples before returning a list of closest vectors, is seen at: </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/keyedvectors.py#L290"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/keyedvectors.py#L290</a></p>
",0,0,382,2017-08-01 18:02:56,https://stackoverflow.com/questions/45444821/word-embedding-relations
Initializing Out of Vocabulary (OOV) tokens,"<p>I am building TensorFlow model for NLP task, and I am using pretrained Glove 300d word-vector/embedding dataset.</p>

<p>Obviously some tokens can't be resolved as embeddings, because were not included into training dataset for word vector embedding model, e.g. rare names.</p>

<p>I can replace those tokens with vectors of 0s, but rather than dropping this information on the floor, I prefer to encode it somehow and include to my training data.</p>

<p>Say, I have 'raijin' word, which can't be resolved as embedding vector, what would be the best way to encode it consistently with Glove embedding dataset? What is the best approach to convert it to 300d vector?</p>

<p>Thank you. </p>
","tensorflow, embedding, word-embedding, oov","<p>Instead of assigning all the <code>Out of Vocabulary</code> tokens to a common <code>UNK</code> vector (zeros), it is better to assign them a unique random vector. At-least this way when you find the similarity between them with any other word, each of them will be unique and the model can learn something out of it. In the <code>UNK case</code>, they will all be same and so all the UNK words will be treated as having the same context.</p>

<p>I tried this approach and got a 3% accuracy improvement on the <code>Quora Duplicate question pair detection</code> dataset using an <code>LSTM</code> model.</p>
",13,3,8769,2017-08-03 21:58:11,https://stackoverflow.com/questions/45495190/initializing-out-of-vocabulary-oov-tokens
What is the effect of adding new word vector embeddings onto an existing embedding space for Neural networks,"<p>In Word2Vector, the word embeddings are learned using co-occurrence and updating the vector's dimensions such that words that occur in each other's context come closer together.</p>

<p>My questions are the following:</p>

<p>1) If you already have a pre-trained set of embeddings, let's say a 100 dimensional space with 40k words, can you add 10 additional words onto this embedding space without changing the existing word embeddings. So you would only be updating the dimensions of the new words using the existing word embeddings. I'm thinking of this problem with respect to the  ""word 2 vector"" algorithm, but if people have insights on how GLoVe embeddings work in this case, I am still very interested.</p>

<p>2) Part 2 of the question is; Can you then use the NEW word embeddings in a NN that was trained with the previous embedding set and expect reasonable results. For example, if I had trained a NN for sentiment analysis, and the word ""nervous"" was previously not in the vocabulary, then would ""nervous"" be correctly classified as ""negative"".</p>

<p>This is a question about how sensitive (or robust) NN are with respect to the embeddings. I'd appreciate any thoughts/insight/guidance.</p>
","neural-network, word2vec, word-embedding","<p>The initial training used info about known words to plot them in a useful N-dimensional space. </p>

<p>It is of course theoretically possible to then use new information, about new words, to also give them coordinates in the same space. You would want lots of varied examples of the new words being used together with the old words. </p>

<p>Whether you want to freeze the positions of old words, or let them also drift into new positions based on the new examples, could be an important choice to make. If you've already trained a pre-existing classifier (like a sentiment classifier) using the older words, and didn't want to re-train that classifier, you'd probably want to lock the old words in place, and force the new words into compatible positioning (even if the newer combined text examples would otherwise change the relative positions of older words). </p>

<p>Since after an effective train-up of the new words, they should generally be near similar-meaning older words, it would be reasonable to expect classifiers that worked on the old words to still do something useful on the new words. But how well that'd work would depend on lots of things, including how well the original word-set covered all the generalizable 'neighborhoods' of meaning. (If the new words bring in shades of meaning of which there were no examples in the old words, that area of the coordinate-space may be impoverished, and the classifier may have never had a good set of distinguishing examples, so performance could lag.)</p>
",2,1,2601,2017-08-03 23:05:02,https://stackoverflow.com/questions/45495885/what-is-the-effect-of-adding-new-word-vector-embeddings-onto-an-existing-embeddi
What is the difference between wmd (word mover distance) and wmd based similarity?,"<p>I am using WMD to calculate the similarity scale between sentences. For example:</p>

<pre><code>distance = model.wmdistance(sentence_obama, sentence_president)
</code></pre>

<p>Reference: <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>However, there is also WMD based similarity method <code>(WmdSimilarity).</code></p>

<p>Reference: 
<a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>What is the difference between the two except the obvious that one is distance and another similarity? </p>

<p><strong>Update:</strong> Both are exactly the same except with their different representation. </p>

<pre><code>n_queries = len(query)
result = []
for qidx in range(n_queries):
    # Compute similarity for each query.
    qresult = [self.w2v_model.wmdistance(document, query[qidx]) for document in self.corpus]
    qresult = numpy.array(qresult)
    qresult = 1./(1.+qresult)  # Similarity is the negative of the distance.

    # Append single query result to list of all results.
    result.append(qresult)
</code></pre>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py</a></p>
","nlp, nltk, gensim, word2vec, word-embedding","<p>I think with the 'update' you'e more-or-less answered your own question. </p>

<p>That one is a distance, and the other a similarity, is the only difference between the two calculations. As the notebook you link notes in the <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html#Part-2:-Similarity-queries-using-WmdSimilarity"" rel=""noreferrer"">relevant section</a>:</p>

<blockquote>
  <p>WMD is a measure of distance. The similarities in WmdSimilarity are simply the negative distance. Be careful not to confuse distances and similarities. Two similar documents will have a high similarity score and a small distance; two very different documents will have low similarity score, and a large distance.</p>
</blockquote>

<p>As the code you've excerpted shows, the similarity measure being used there is not exactly the 'negative' distance, but scaled so all similarity values are from 0.0 (exclusive) to 1.0 (inclusive). (That is, a zero distance becomes a 1.0 similarity, but ever-larger distances become ever-closer to 0.0.)</p>
",5,4,2261,2017-08-04 08:59:17,https://stackoverflow.com/questions/45502464/what-is-the-difference-between-wmd-word-mover-distance-and-wmd-based-similarit
How to calculate a One-Hot Encoding value into a real-valued vector?,"<p>In Word2Vec, i've learned that both of CBOW and Skip-gram produce a one-hot encoding value to create a vector (cmiiw), I wonder how to calculate or represents a One-Hot Encoding value into a real-valued vector, for example (source: <a href=""http://blog.districtdatalabs.com/nlp-research-lab-part-1-distributed-representations"" rel=""nofollow noreferrer"">DistrictDataLab's Blog about Distributed Representations</a>)
from this:
<img src=""https://s1.postimg.org/naavbr5lr/Lev_Konstantinovskiy_Next_generation_of_word_embeddings_in_Gensim_2.jpg"" alt=""One-Hot Encoding&#39;s example"">
into:
<img src=""https://s1.postimg.org/734kvoylb/Lev_Konstantinovskiy_Next_generation_of_word_embeddings_in_Gensim_2.jpg"" alt=""One-Hot Encoding&#39;s example"">
please help, I was struggling on finding this information.</p>
","nlp, deep-learning, word2vec, word-embedding","<p>The word2vec algorithm itself is what incrementally learns the real-valued vector, with varied dimension values. </p>

<p>In contrast to the one-hot encoding, these vectors are often called ""dense embeddings"". They're ""dense"" because unlike the one-hot encoding, which is ""sparse"" with many dimensions and mostly zero values, they have fewer dimensions and (usually) no zero-values. They're an ""embedding"" because they've ""embed"" a discrete set-of-words into another continuous-coordinate-system.</p>

<p>You'd want to read the <a href=""https://arxiv.org/abs/1301.3781"" rel=""nofollow noreferrer"">original word2vec paper</a> for a full formal description of how the dense embeddings are made. </p>

<p>But the gist is that the dense vectors start totally random, and so at first the algorithm's internal neural network is useless for predicting neighboring words. But each (context)->(target) word training example from a text corpus is tried against the network, and each time the difference from the desired prediction is used to apply a tiny nudge, towards a better prediction, to both word-vector and internal-network-weight values. </p>

<p>Repeated many times, initially with larger nudges (higher learning-rate) then with ever-smaller nudges, the dense vectors rearrange their coordinates from their initial randomness to a useful relative-arrangement – one that's about-as-good as possible for predicting the training text, given the limits of the model itself. (That is, any further nudge that improves predictions on some examples, worsens it on others – so you might as well consider training done.)</p>

<p>You then read the resulting dense embedding real-valued vectors out of the model, and use them for purposes other than just nearby-word prediction.  </p>
",4,2,945,2017-08-06 11:18:31,https://stackoverflow.com/questions/45531476/how-to-calculate-a-one-hot-encoding-value-into-a-real-valued-vector
Extract more meaningful words from publicly available word embedding,"<p>I have two publicly available word embeddings such as Glove and Google Word2vec.
However, in their vocabulary, there are too many misspelling words or garbage words(e.g., ##AA##, adirty, etc). To avoid this words, I would like to extract frequent word(e.g., top 50000 words) since I think relatively high frequent words has normal forms.</p>

<p>So, I wonder if there is a way to find word frequency in above two pretrained word embeddings. If not, I want to know if there are some techniques to exclude this words. </p>
","machine-learning, word2vec, word-embedding","<p>The GoogleNews vector set does not contain frequency information, but does seem to be sorted from most-frequent to least-frequent. So, if you change the code that loads it to only load the first N words, you should get the N most-frequent words.</p>

<p>(The Python gensim library for training or working with word-vectors includes this as a <code>limit</code> option on the <code>load_word2vec_format()</code> function.)</p>

<p>GLoVe may follow the same convention – a look over the order-of-words in the file should give a good idea.</p>
",3,3,314,2017-08-11 09:43:53,https://stackoverflow.com/questions/45631962/extract-more-meaningful-words-from-publicly-available-word-embedding
Evaluating Word2Vec model by finding linear algebraic structure of words,"<p>I have built Word2Vecmodel using gensim library in python.I want to evaluate my word embedding as follows</p>

<blockquote>
  <p>If A is related to B and C is related to D, then A-C+B should be equal to D. For example, embedding vector arithmetic of ""India""-""Rupee""+""Japan"" should be equal to the embedding of ""Yen"".</p>
</blockquote>

<p>I have used in built functions of gensim like predict_output_word,most_similar but couldn't get desired results.</p>

<pre><code>new_model.predict_output_word(['india','rupee','japan'],topn=10)
new_model.most_similar(positive=['india', 'rupee'], negative=['japan'])
</code></pre>

<p>Kindly help me in evaluating my model as per the criteria above.</p>
","nlp, word2vec, word-embedding","<p>You should you the <code>most_similar()</code> method's <code>positive</code> and <code>negative</code> arguments in the same manner as the <code>accuracy()</code> method:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/718b1c6bd1a8a98625993d73b83d98baf385752d/gensim/models/keyedvectors.py#L697"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/718b1c6bd1a8a98625993d73b83d98baf385752d/gensim/models/keyedvectors.py#L697</a></p>

<p>Specifically, if you have a analogy of the form ""A is to B as C is to [expected]"", you should look at:</p>

<pre><code>results = model.most_similar(positive=[word_b, word_c], negative=[word_a])
</code></pre>

<p>Or in your example:</p>

<pre><code>results = model.most_similar(positive=['rupee', 'japan'], negative=['india'])
</code></pre>
",2,0,378,2017-08-12 06:53:55,https://stackoverflow.com/questions/45647439/evaluating-word2vec-model-by-finding-linear-algebraic-structure-of-words
Word embedding training,"<p>I have one corpus for word embedding. Using this corpus, I trained my word embedding. However, whenever I train my word embedding, the results are quite different(this results are based on K-Nearest Neighbor(KNN)). For example, in the first training, '<strong>computer</strong>' nearest neighbor words are '<strong>laptops</strong>', '<strong>computerized</strong>' ,'<strong>hardware</strong>'. But, in the second training, this knn words are '<strong>software</strong>', '<strong>machine</strong>',...('laptops' is low ranked!) - all training are performed independently 20 epochs, and hyper-parameters are all the same.</p>

<p>I want to train my word embedding very similar(e.g., 'laptops' is high ranked). How should i do? Should I modulate hyper-parameters(learning rate, initializing, etc)?</p>
","machine-learning, deep-learning, word2vec, word-embedding","<p>You didn't say what word2vec software you're using, which might change the relevant factors. </p>

<p>The word2vec algorithm inherently uses randomness, in both initialization and several aspects of its training (like the selection of negative-examples, if using negative-sampling, or random downsampling of very-frequent words). Additionally, if you're doing multithreaded training, the essentially-random jitter in the OS thread scheduling will change the order of training examples, introducing another source of randomness. So you shouldn't necessarily expect subsequent runs, even with the exact same parameters and corpus, to give identical results. </p>

<p>Still, with enough good data, suitable parameters, and a proper training loop, the relative-neighbors results <em>should</em> be fairly similar from run-to-run. If it's not, more data or more iterations might help. </p>

<p>Wildly-different results would be most likely if the model is overlarge (too many dimensions/words) for your corpus – and thus prone to overfitting. That is, it finds a great configuration for the data, through essentially memorizing its idiosyncracies, without achieving any generalization power. And if such overfitting is possible, there are typically many equally-good such memorizations – so they can be very different from run-to-tun. Meanwhile, a right-sized model with lots of data will instead be capturing true generalities, and <em>those</em> would be more consistent from run-to-run, despite any randomization.</p>

<p>Getting more data, using smaller vectors, using more training passes, or upping the minimum-count of word-occurrences to retain/train a word all might help. (Very-infrequent words don't get high-quality vectors, so wind up just interfering with the quality of other words, and then randomly intruding in most-similar lists.)</p>

<p>To know what else might be awry, you should clarify in your question things like:</p>

<ul>
<li>software used</li>
<li>modes/metaparameters used</li>
<li>corpus size, in number of examples, average example size in words, and unique-words count (both in the raw corpus, and after any minumum-count is applied)</li>
<li>methods of preprocessing</li>
<li>code you're using for training (if you're managing the multiple training-passes yourself)</li>
</ul>
",2,3,590,2017-08-18 01:30:31,https://stackoverflow.com/questions/45747170/word-embedding-training
embedded vectors doesn&#39;t converge in gensim,"<p>I am training a word2vec model using gensim on 800k browser useragent. My dictionary size is between 300 and 1000 depending on the word frequency limit.
I am looking at few embedding vectors and similarities to see if the algorithm has been converged.
here is my code:</p>

<pre><code>wv_sim_min_count_stat={}
window=7;min_count=50;worker=10;size=128
total_iterate=1000
from copy import copy
for min_count in [50,100,500]:
    print(min_count)

    wv_sim_min_count_stat[min_count]={}
    model=gensim.models.Word2Vec(size=size,window=window,min_count=min_count,iter=1,sg=1)
    model.build_vocab(ua_parsed)


    wv_sim_min_count_stat[min_count]['vocab_counts']=[len(ua_parsed),len(model.wv.vocab),len(model.wv.vocab)/len(ua_parsed)]
    wv_sim_min_count_stat[min_count]['test']=[]

    alphas=np.arange(0.025,0.001,(0.001-0.025)/(total_iterate+1))
    for i in range(total_iterate):
        model.train(ua_parsed,total_examples=model.corpus_count,
                    epochs=model.iter,start_alpha=alphas[i],end_alpha=alphas[i+1])

        wv_sim_min_count_stat[min_count]['test'].append(
        (copy(model.wv['iphone']),copy(model.wv['(windows']),copy(model.wv['mobile']),copy(model.wv['(ipad;']),copy(model.wv['ios']),
         model.similarity('(ipad;','ios')))
</code></pre>

<p>unfortunately even after 1000 epochs there is no sign of convergence in embedding vectors. for example I plot embedding of the first dimension of '(ipad''s embedding vector vs number of epochs below:</p>

<pre><code>for min_count in [50,100,500]:
    plt.plot(np.stack(list(zip(*wv_sim_min_count_stat[min_count]['test']))[3])[:,1],label=str(min_count))

plt.legend() 
</code></pre>

<p><a href=""https://i.sstatic.net/dfsJI.png"" rel=""nofollow noreferrer"">embedding of '(ipad' vs number of epochs</a></p>

<p>I looked at many blogs and papers and it seems nobody trained the word2vec beyond 100 epochs. What I am missing here?</p>
","python, gensim, convergence, word-embedding","<p>Your dataset, user-agent strings, may be odd for word2vec. It's not natural-language; it might not have the same variety of co-occurences that causes word2vec to do useful things for natural language. (Among other things, a dataset of 800k natural-language sentences/docs would tend to have a much larger vocabulary than just ~1,000 words.)</p>

<p>Your graphs do look like they're roughly converging, to me. In each case, as the learning-rate <code>alpha</code> decreases, the dimension magnitude is settling towards a final number. </p>

<p>There is no reason to expect the magnitude of a particular dimension, of a particular word, would reach the <em>same</em> absolute value in different runs. That is: you shouldn't expect the three lines you're plotting, under different model parameters, to all tend towards the same final value. </p>

<p>Why not? </p>

<p>The algorithm includes random initialization, randomization-during-training (in negative-sampling or frequent-word downsampling), and then in its multi-threading some arbitrary re-ordering of training-examples due to OS thread-scheduling jitter. As a result, even with exactly the same metaparameters, and the same training corpus, a single word could land at different coordinates in subsequent training runs. But, its distances and orientation with regard to other words <em>in the same run</em> should be about-as-useful. </p>

<p>With different metaparameters like <code>min_count</code>, and thus a different ordering of surviving words during initialization, and then wildly different random-initialization, the final coordinates per word could be especially different. There is no inherent set-of-best-final-coordinates for any word, even with regard to a particular fixed corpus or initialization. There's just coordinates that work increasingly well, through a particular randomized initialization/training session, balanced over all the other co-trained words/examples.</p>
",1,2,734,2017-08-22 19:28:32,https://stackoverflow.com/questions/45825532/embedded-vectors-doesnt-converge-in-gensim
Why Word2Vec&#39;s most_similar() function is giving senseless results on training?,"<p>I am running the gensim word2vec code on a corpus of resumes(stopwords removed) to identify similar context words in the corpus from a list of pre-defined keywords.</p>

<p>Despite several iterations with input parameters,stopword removal etc the similar context words are not at all making sense(in terms of distance or context)
Eg. correlation and matrix occurs in the same window several times yet matrix doesnt fall in the most_similar results for correlation </p>

<p>Following are the details of the system and codes
gensim 2.3.0 ,Running on Python 2.7 Anaconda
Training Resumes :55,418 sentences
Average words per sentence : 3-4 words(post stopwords removal)
Code :</p>

<pre><code>    wordvec_min_count=int()
    size = 50
    window=10
    min_count=5
    iter=50
    sample=0.001
    workers=multiprocessing.cpu_count()
    sg=1
    bigram = gensim.models.Phrases(sentences, min_count=10, threshold=5.0)
    trigram = gensim.models.Phrases(bigram[sentences], min_count=10, threshold=5.0)
    model=gensim.models.Word2Vec(sentences = trigram[sentences], size=size, alpha=0.005, window=window, min_count=min_count,max_vocab_size=None,sample=sample, seed=1, workers=workers, min_alpha=0.0001, sg=sg, hs=1, negative=0, cbow_mean=1,iter=iter)

model.wv.most_similar('correlation')
Out[20]: 
[(u'rankings', 0.5009744167327881),
 (u'salesmen', 0.4948525130748749),
 (u'hackathon', 0.47931140661239624),
 (u'sachin', 0.46358123421669006),
 (u'surveys', 0.4472047984600067),
 (u'anova', 0.44710394740104675),
 (u'bass', 0.4449636936187744),
 (u'goethe', 0.4413239061832428),
 (u'sold', 0.43735259771347046),
 (u'exceptional', 0.4313117265701294)]
</code></pre>

<p>I am lost as to why the results are so random ? Is there anyway to check the accuracy for word2vec ?</p>

<p>Also is there an alternative of word2vec for most_similar() function ? I read about gloVE but was not able to install the package.</p>

<p>Any information in this regard would be helpful</p>
","python-2.7, gensim, word2vec, word-embedding","<p>Enable INFO-level logging and make sure that it indicates real training is happening. (That is, you see incremental progress taking time over the expected number of texts, over the expected number of iterations.)</p>

<p>You may be hitting this <a href=""https://github.com/RaRe-Technologies/gensim/issues/1401"" rel=""nofollow noreferrer"">open bug issue in <code>Phrases</code></a>, where requesting the Phrase-promotion (as with <code>trigram[sentences]</code>) only offers a single-iteration, rather than the multiply-iterable collection object that <code>Word2Vec</code> needs. </p>

<p><code>Word2Vec</code> needs to pass over the corpus once for vocabulary-discovery, then <code>iter</code> times again for training. If <code>sentences</code> or the phrasing-wrappers only support single-iteration, only the vocabulary will be discovered – training will end instantly, and the model will appear untrained.</p>

<p>As you'll see in that issue, a workaround is to perform the Phrases-transformation and save the results into an in-memory list (if it fits) or to a separate text corpus on disk (that's already been phrase-combined). Then, use a truly restartable iterable on that – which will also save some redundant processing. </p>
",1,0,924,2017-09-11 14:19:26,https://stackoverflow.com/questions/46157937/why-word2vecs-most-similar-function-is-giving-senseless-results-on-training
How Word Mover&#39;s Distance (WMD) uses word2vec embedding space?,"<p>According to WMD <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">paper</a>, it's inspired by word2vec model and use word2vec vector space for moving document 1 towards document 2 (in the context of Earth Mover Distance metric). From the paper:</p>

<pre><code>Assume we are provided with a word2vec embedding matrix
X ∈ Rd×n for a finite size vocabulary of n words. The 
ith column, xi ∈ Rd, represents the embedding of the ith
word in d-dimensional space. We assume text documents
are represented as normalized bag-of-words (nBOW) vectors,
d ∈ Rn. To be precise, if word i appears ci times in
the document, we denote di = ci/cj (for j=1 to n). An nBOW vector
d is naturally very sparse as most words will not appear in
any given document. (We remove stop words, which are
generally category independent.)
</code></pre>

<p>I understand the concept from the paper, however, I couldn't understand how wmd uses word2vec embedding space from the code in Gensim. </p>

<p><strong><em>Can someone explain it in a simple way? Does it calculate the word vectors in a different way because I couldn't understand where in this code word2vec embedding matrix is used?</em></strong> </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">WMD Fucntion from Gensim:</a></p>

<pre><code>   def wmdistance(self, document1, document2):
    # Remove out-of-vocabulary words.
    len_pre_oov1 = len(document1)
    len_pre_oov2 = len(document2)
    document1 = [token for token in document1 if token in self]
    document2 = [token for token in document2 if token in self]

    dictionary = Dictionary(documents=[document1, document2])
    vocab_len = len(dictionary)

    # Sets for faster look-up.
    docset1 = set(document1)
    docset2 = set(document2)

    # Compute distance matrix.
    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)
    for i, t1 in dictionary.items():
        for j, t2 in dictionary.items():
            if t1 not in docset1 or t2 not in docset2:
                continue
            # Compute Euclidean distance between word vectors.
            distance_matrix[i, j] = sqrt(np_sum((self[t1] - self[t2])**2))

    def nbow(document):
        d = zeros(vocab_len, dtype=double)
        nbow = dictionary.doc2bow(document)  # Word frequencies.
        doc_len = len(document)
        for idx, freq in nbow:
            d[idx] = freq / float(doc_len)  # Normalized word frequencies.
        return d

    # Compute nBOW representation of documents.
    d1 = nbow(document1)
    d2 = nbow(document2)

    # Compute WMD.
    return emd(d1, d2, distance_matrix)
</code></pre>
","nlp, nltk, gensim, word2vec, word-embedding","<p>For the purposes of WMD, a text is considered a bunch of 'piles' of meaning. Those piles are placed at the coordinates of the text's words – and that's why WMD calculation is dependent on a set of word-vectors from another source. Those vectors position the text's piles.</p>

<p>The WMD is then the minimal amount of work needed to shift one text's piles to match another text's piles. And the measure of the work needed to shift from one pile to another is the euclidean distance between those pile's coordinates. </p>

<p>You could just try a naive shifting of the piles: look at the first word from text A, shift it to the first word from text B, and so forth. But that's unlikely to be the <em>cheapest</em> shifting – which would likely try to match nearer words, to send the 'meaning' on the shortest possible paths. So actually calculating the WMD is an iterative optimization problem – significantly more expensive than just a simple euclidean-distance or cosine-distance between two points. </p>

<p>That optimization is done inside the <code>emd()</code> call in the code you excerpt. But what that optimization requires is the pairwise distances between all words in text A, and all words in text B – because those are all the candidate paths across which meaning-weight might be shifted. You can see those pairwise distances calculated in the code to populate the <code>distance_matrix</code>, using the word-vectors already loaded in the model and accessible via <code>self[t1]</code>, <code>self[t2]</code>, etc. </p>
",2,2,1652,2017-09-13 15:09:18,https://stackoverflow.com/questions/46201029/how-word-movers-distance-wmd-uses-word2vec-embedding-space
Word Mover&#39;s distance calculation between word pairs of two documents,"<p>According to WMD <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">paper</a>, travel cost or Euclidean distance between word pairs is calculated the way shown in the figure below. </p>

<p><a href=""https://i.sstatic.net/RZIyb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RZIyb.png"" alt=""enter image description here""></a></p>

<p>Is this distance calculated in pair wise in a specific order? Such that the first, second and so on from each document as shown in the figure Or Obama's distance is calculated from all four words in D0 and then the minimum of these four is shown in the figure only. </p>

<p>Can someone explain how this works?</p>

<p>Also, why is then all three words in D3 compared with President in D0?</p>
","machine-learning, nlp, word2vec, word-embedding, wmd","<p>The calculation of WMD requires finding the cheapest shifting of word-weight-configuration in a first text, into the word-weight-configuration of the second text. </p>

<p>The word-order is irrelevant. Any word's mass in one text could be shifted to the position of any word in the other text. The optimization process which finds the best shifts thus will consider many possible pairings. After it finds the best, the final single WMD number is the total travel distance in that best solution. </p>

<p>Because of differences in word count, words may not be shifted one-to-one, but as proportion of the full text's mass. So consider the bottom example in the graphic you included: the top text <em>D0</em> has 4 significant words, and the bottom text <em>D3</em> has just 3 significant words. So each of the top text's 4 words can be thought of as having 0.25 mass, and each of the bottom text's words can be thought of as having 0.33 mass. </p>

<p>'Obama' might thus map very closely to 'President' - but even moving 0.25 of 'Obama' mass to 'President' leaves 0.08 mass left over that must travel to another <em>D0</em> word. Similarly with 'Illinois' and 'Chicago' – even if a 0.25 of 'Illinois' mass is moved to 'Chicago', 0.08 is left-over that must travel to another <em>D0</em> word. The exact mix of paths and proportions chosen will be the best possible, but will typically involve some words being fractionally shifted across multiple other words.</p>
",2,2,1096,2017-09-13 16:13:12,https://stackoverflow.com/questions/46202308/word-movers-distance-calculation-between-word-pairs-of-two-documents
What is the initial value of Embedding layer?,"<p>I am studying embedding for word representations. In many dnn libraries, they support embedding layer. And this is really nice tutorial.</p>

<p><a href=""http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"" rel=""nofollow noreferrer"">Word Embeddings: Encoding Lexical Semantics</a></p>

<p>But I am not still sure how to calculate embed value. In below example, it outputs some value even before any trainings. Does it use some random weights? I realize a purpose of <code>Embedding(2, 5)</code>, but not sure its initial calculation. And I am no sure about how to learn weights of its Embedding too.</p>

<pre><code>word_to_ix = {""hello"": 0, ""world"": 1}
embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings
lookup_tensor = torch.LongTensor([word_to_ix[""hello""]])
hello_embed = embeds(autograd.Variable(lookup_tensor))
print(hello_embed)
--------
Variable containing:
-2.9718  1.7070 -0.4305 -2.2820  0.5237
[torch.FloatTensor of size 1x5]
</code></pre>

<p>I break down my thought to be sure. First of all, upper <code>Embedding(2, 5)</code> is a matrix of shape <code>(2, 5)</code>.</p>

<pre><code>Embedding(2, 5) = 
 [[0.1,-0.2,0.3,0.4,0.1],
 [-0.2,0.1,0.8,0.2,0.3]] # initiated by some function, like random normal distribution
</code></pre>

<p>Then, <code>hello</code> is <code>[1, 0]</code>. Then <code>hello</code> representation is calculated by <code>[1, 0].dot(Embedding(2, 5)) = [0.1,-0.2,0.3,0.4,0.1]</code>. This is actually first row of the Embedding. Am I understanding right?</p>

<hr>

<h1>Updates</h1>

<p>I found a code of embedding which is exactly use normal distribution for its value. Yes, but it is just a default value, and we can set arbitrary weights for embedding layers.
<a href=""https://github.com/chainer/chainer/blob/adba7b846d018b9dc7d19d52147ef53f5e555dc8/chainer/links/connection/embed_id.py#L58"" rel=""nofollow noreferrer"">https://github.com/chainer/chainer/blob/adba7b846d018b9dc7d19d52147ef53f5e555dc8/chainer/links/connection/embed_id.py#L58</a></p>
","machine-learning, nlp, deep-learning, word2vec, word-embedding","<p>Yes. You start off with random weights. I think it is more common to use a truncated normal distribution instead of the regular normal distribution. But, that probably doesn't make much of a difference.</p>
",1,3,1942,2017-09-22 03:09:46,https://stackoverflow.com/questions/46356227/what-is-the-initial-value-of-embedding-layer
How to create gensim word2vec model using pre trained word vectors?,"<p>I have created word vectors using a distributed word2vec algorithm. Now I have words and their corresponding vectors. How to build a gensim word2vec model using these words and vectors? </p>
","nlp, gensim, word2vec, text-analysis, word-embedding","<p>I am not sure if you created word2vec model using <code>gensim</code> or some other tools but if understand your question correctly you want to just load the word2vec model using gensim. This is done in the following way:</p>

<pre><code>import gensim
w2v_file = codecs.open(WORD2VEC_PATH, encoding='utf-8')
model = gensim.models.KeyedVectors.load_word2vec_format(w2v_file, binary=True)  # or binary=False if the model is not compressed
</code></pre>

<p>If, however, what you want to do is to train word2vec model from scratch (i.e. from raw text) using purely <code>gensim</code> here is a <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">tutorial on how to train word2vec model using gensim</a>.</p>
",4,3,3335,2017-10-12 03:59:07,https://stackoverflow.com/questions/46701173/how-to-create-gensim-word2vec-model-using-pre-trained-word-vectors
"word2vec - what is best? add, concatenate or average word vectors?","<p>I am working on a recurrent language model. To learn word embeddings that can be used to initialize my language model, I am using gensim's word2vec model. 
After training, the word2vec model holds two vectors for each word in the vocabulary: the word embedding (rows of input/hidden matrix) and the context embedding (columns of hidden/output matrix).</p>

<p>As outlined in <a href=""https://stackoverflow.com/questions/36731784/wordvectors-how-to-concatenate-word-vectors-to-form-sentence-vector"">this post</a> there are at least three common ways to combine these two embedding vectors:</p>

<ol>
<li>summing the context and word vector for each word</li>
<li>summing &amp; averaging</li>
<li>concatenating the context and word vector</li>
</ol>

<p>However, I couldn't find proper papers or reports on the best strategy. So my questions are:</p>

<ol>
<li>Is there a common solution whether to sum, average or concatenate the vectors?</li>
<li>Or does the best way depend entirely on the task in question? If so, what strategy is best for a word-level language model?</li>
<li>Why combine the vectors at all? Why not use the ""original"" word embeddings for each word, i.e. those contained in the weight matrix between input and hidden neurons.</li>
</ol>

<p>Related (but unanswered) questions: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/42119824/word2vec-summing-concatenate-inside-and-outside-vector?rq=1"">word2vec: Summing/concatenate inside and outside vector</a> </li>
<li><a href=""https://stackoverflow.com/questions/46065773/why-we-use-input-hidden-weight-matrix-to-be-the-word-vectors-instead-of-hidden-o?rq=1"">why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?</a></li>
</ul>
","python, word2vec, gensim, word-embedding, language-model","<p>I have found an answer in the Stanford lecture ""Deep Learning for Natural Language Processing"" (Lecture 2, March 2016). It's available <a href=""https://www.youtube.com/watch?v=aRqn8t1hLxs"" rel=""noreferrer"">here</a>. In minute 46 Richard Socher states that the common way is to <strong>average</strong> the two word vectors.</p>
",8,21,20184,2017-10-23 12:44:40,https://stackoverflow.com/questions/46889727/word2vec-what-is-best-add-concatenate-or-average-word-vectors
Do word vectors mean anything on their own?,"<p>From my understanding, word vectors are only ever used in terms of relations to other word vectors. For example, the word vector for ""king"" minus the word vector for ""boy"" should give a vector close to ""queen"". </p>

<p>Given a vector of some unknown word, can assumptions about the word be made based solely on the values of that vector?</p>
","machine-learning, data-science, word2vec, text-analysis, word-embedding","<p>The individual coordinates – such as dimension #7 of a 300-dimensional vector, etc – don't have easily interpretable meanings. </p>

<p>It's primarily the relative <em>distances</em> to other words (neighborhoods), and relative <em>directions</em> with respect to other constellations of words (orientations without regard to the perpendicular coordinate axes, that may be vaguely interpretable, because they correlate with natural-language or natural-thinking semantics. </p>

<p>Further, the pre-training initialization of the model, and much of the training itself, uses randomization. So even on the exact same data, words can wind up in different coordinates on repeated training runs. </p>

<p>The resulting word-vectors should after each run be about as useful <em>with respect to each other</em>, in terms of distances and directions, but neighborhoods like ""words describing seasons"" or ""things that are 'hot'"" could be in very different places in subsequent runs. Only vectors that trained together are comparable.</p>

<p>(There are some constrained variants of word2vec that try to force certain dimensions or directions to be more useful for certain purposes, such as answering questions or detecting hypernym/hyponym relationships – but that requires extra constraints or inputs to the training process. Plain vanilla word2vec won't be as cleanly interpretable.)</p>
",1,-2,228,2017-10-23 20:57:13,https://stackoverflow.com/questions/46898402/do-word-vectors-mean-anything-on-their-own
TensorFlow Word2Vec model running on GPU,"<p>In <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""nofollow noreferrer"">this TensorFlow example</a> a training of skip-gram Word2Vec model described. It contains the following code fragment, which explicitly requires CPU device for computations, i.e. <code>tf.device('/cpu:0')</code>:</p>

<pre><code>batch_size = 128
embedding_size = 128  # Dimension of the embedding vector.
skip_window = 1  # How many words to consider left and right.
num_skips = 2  # How many times to reuse an input to generate a label.

# We pick a random validation set to sample nearest neighbors. Here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent. 
valid_size = 16  # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.array(random.sample(range(valid_window), valid_size))
num_sampled = 64  # Number of negative examples to sample.

graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # Variables.
    embeddings = tf.Variable(
        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    # Model.
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)

    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
        tf.nn.sampled_softmax_loss(weights=softmax_weights,
                                   biases=softmax_biases, inputs=embed,
                                   labels=train_labels, num_sampled=num_sampled,
                                   num_classes=vocabulary_size))

    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities 
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)

    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
</code></pre>

<p>When trying switch to GPU, the following exception is raised:</p>

<blockquote>
  <p><strong>InvalidArgumentError</strong> (see above for traceback): Cannot assign a device for operation 'Variable_2/Adagrad': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.</p>
</blockquote>

<p>I wonder what is the reason why the provided graph cannot be computed on GPU? Does it happen due to <code>tf.int32</code> type? Or should I switch to another optimizer? In other words, is there any way to make possible processing Word2Vec model on GPU? (Without types casting).</p>

<hr>

<p><strong>UPDATE</strong></p>

<p>Following Akshay Agrawal recommendation, here is an updated fragment of the original code that achieves required result:</p>

<pre><code>with graph.as_default(), tf.device('/gpu:0'):
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    embeddings = tf.Variable(
        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))    
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)

    with tf.device('/cpu:0'):
        loss = tf.reduce_mean(
            tf.nn.sampled_softmax_loss(weights=softmax_weights,
                                       biases=softmax_biases,
                                       inputs=embed,
                                       labels=train_labels,
                                       num_sampled=num_sampled,
                                       num_classes=vocabulary_size))

    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)

    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
</code></pre>
","python, word2vec, tensorflow, word-embedding","<p>The error is raised because <code>AdagradOptimizer</code> does not have a GPU kernel for its sparse apply operation; a sparse apply is triggered because differentiating through the embedding lookup results in a sparse gradient. </p>

<p><code>GradientDescentOptimizer</code> and <code>AdamOptimizer</code> do support sparse apply operations. If you were to switch to one of these optimizers, you would unfortunately see another error: tf.nn.sampled_softmax_loss appears to create an op that does not have a GPU kernel. To get around that, you could wrap the <code>loss = tf.reduce_mean(...</code> line with a <code>with tf.device('/cpu:0'):</code> context, though doing so would introduce cpu-gpu communication overhead.</p>
",2,0,2854,2017-11-03 09:21:23,https://stackoverflow.com/questions/47092185/tensorflow-word2vec-model-running-on-gpu
NLP - Embeddings selection of `start` and `end` of sentence tokens,"<p>Suppose we're training a neural network model to learn the mapping from the following input to output, where the output is <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Name Entity</a> (NE).</p>

<p><strong>Input</strong>: EU rejects German call to boycott British lamb .</p>

<p><strong>Output</strong>: ORG O MISC O O O MISC O O</p>

<p>A sliding window is created to capture the context information and its outcomes are fed into the training model as model_input. The sliding window generates results as following:</p>

<pre><code> [['&lt;s&gt;', '&lt;s&gt;', 'EU', 'rejects', 'German'],\
 ['&lt;s&gt;', 'EU', 'rejects', 'German', 'call'],\
 ['EU', 'rejects', 'German', 'call', 'to'],\
 ['rejects', 'German', 'call', 'to', 'boycott'],\
 ['German', 'call', 'to', 'boycott', 'British'],\
 ['call', 'to', 'boycott', 'British', 'lamb'],\
 ['to', 'boycott', 'British', 'lamb', '.'],\
 ['boycott', 'British', 'lamb', '.', '&lt;/s&gt;'],\
 ['British', 'lamb', '.', '&lt;/s&gt;', '&lt;/s&gt;']]
</code></pre>

<p><code>&lt;s&gt;</code> represents start of sentence token and <code>&lt;/s&gt;</code> represents end of sentence token, and every sliding window corresponds to one NE in output. </p>

<p>To process these tokens, a pre-trained embedding model is used converting words to vectors (e.g., Glove), but those pre-trained models do not include tokens such as <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code>. I think random initialization for <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> won't be a good idea here, because the scale of such random results might not be consistent with other Glove embeddings.</p>

<p><strong>Question</strong>:
What suggestions of setting up embeddings for <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> and why?</p>
","machine-learning, nlp, deep-learning, word2vec, word-embedding","<p>In general, the answer depends on how you intend to use the embeddings in your task.</p>

<p>I suspect that the use of <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> tokens is dictated by LSTM or other recurrent neural network, that goes after embedding layer. If you were to train word embeddings themselves, I'd suggest you to simply get rid of these tokens, because they don't add any value. Start and stop tokens do matter in LSTM (though not always), but their word embeddings can be arbitrary, small random numbers will do fine, because this vector would be equally far from all ""normal"" vectors.</p>

<p>If you don't want to mess with pre-trained GloVe vectors, I would suggest you to <em>freeze the embedding layer</em>. For example, in tensorflow this can be achieved by <a href=""https://www.tensorflow.org/api_docs/python/tf/stop_gradient"" rel=""noreferrer""><code>tf.stop_gradient</code></a> op right after the embedding lookup. This way the network won't learn any relation between <code>&lt;s&gt;</code> and other words, but it's totally fine, and any existing relations won't change.</p>
",6,6,6650,2017-11-07 00:51:13,https://stackoverflow.com/questions/47148247/nlp-embeddings-selection-of-start-and-end-of-sentence-tokens
Given a word vector get the word of it in word2vec,"<p>I obtain word vectors from my code. e.g., </p>

<pre><code>array([ -3.09521449e-04,   2.73033947e-06,   2.15601496e-04, ...,
         5.12349070e-04,   5.04256517e-04,   8.16784304e-05], dtype=float32)
</code></pre>

<p>Now, I want to identify what is the word that represents this word vector in wor2vec genism.</p>

<p>I tried it using the below code. However it did not work.</p>

<pre><code>print(model.wv.index2word(kmeans_clustering.cluster_centers_))
</code></pre>

<p>Please help me.</p>
","python, word2vec, gensim, word-embedding, doc2vec","<p>The gensim <code>most_similar()</code> method will take a vector as an argument, as well, but you have to explicitly supply it as one item inside a list of <code>positive</code> examples – so that it's not misunderstood as a something else. </p>

<p>For example:</p>

<pre><code>wv = model.wv.['book']
similars = model.wv.most_similar(positive=[wv,])
</code></pre>

<p>Naturally, 'book' will be at the top of this list of words most-similar to its own vector. </p>
",0,1,2169,2017-11-07 10:21:23,https://stackoverflow.com/questions/47155414/given-a-word-vector-get-the-word-of-it-in-word2vec
Why are multiple model files created in gensim word2vec?,"<p>When I try to create a word2vec model (skipgram with negative sampling) I received 3 files as output as follows.</p>

<pre><code>word2vec (File)
word2vec.syn1nef.npy (NPY file)
word2vec.wv.syn0.npy (NPY file)
</code></pre>

<p>I am just worried why this happens as for my previous test examples in word2vec I only received one model(no npy files).</p>

<p>Please help me.</p>
","python, word2vec, gensim, word-embedding","<p>Models with larger internal vector-arrays can't be saved via Python 'pickle' to a single file, so beyond a certain threshold, the gensim <code>save()</code> method will store subsidiary arrays in separate files, using the more-efficient raw format of numpy arrays (<code>.npy</code> format). </p>

<p>You still <code>load()</code> the model by just specifying the root model filename; when the subsidiary arrays are needed, the loading code will find the side files – as long as they're kept beside the root file. So when moving a model elsewhere, be sure to keep all files with the same root filename together.  </p>
",33,19,5264,2017-11-08 07:07:15,https://stackoverflow.com/questions/47173538/why-are-multiple-model-files-created-in-gensim-word2vec
tf.nn.embedding_lookup with float input?,"<p>I would like to implement an embedding table with float inputs instead of int32 or 64b.
The reason is that instead of words like in a simple RNN, I would like to use percentages.
For example in case of a recipe; I may have 1000 or 3000 ingredients; but in every recipe I may have a maximum of 80.
The ingredients will be represented in percentage for example: ingredient1=0.2 ingredient2=0.8... etc</p>

<p>my problem is that tensorflow forces me to use integers for my embedding table:</p>

<blockquote>
  <p><strong>TypeError</strong>: Value passed to parameter ‘indices’ has DataType float32 not in list of allowed values: int32, int64</p>
</blockquote>

<p>any suggestion?
I appreciate your feedback,</p>

<p>example of embedding look up: </p>

<pre><code>inputs = tf.placeholder(tf.float32, shape=[None, ninp], name=“x”)
n_vocab = len(int_to_vocab)
n_embedding = 200 # Number of embedding features 
with train_graph.as_default():
    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))
    embed = tf.nn.embedding_lookup(embedding, inputs)
</code></pre>

<p>the error is caused by </p>

<pre><code>inputs = tf.placeholder(**tf.float32,** shape=[None, ninp], name=“x”)
</code></pre>

<p>I have thought of an algorithm that could work using loops. But, I was wondering if there is a more direct solution. </p>

<p>Thanks! </p>
","machine-learning, tensorflow, nlp, word-embedding","<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup</code></a> can't allow float input, because the point of this function is to select the embeddings at the <strong>specified rows</strong>.</p>

<p>Example:</p>

<p><a href=""https://i.sstatic.net/11LeF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/11LeF.png"" alt=""embed-lookup""></a></p>

<p>Here there are 5 words and 5 embedding 3D vectors, and the operation returns the 3-rd row (with 0-indexing). This is equivalent to this line in tensorflow:</p>

<pre><code>embed = tf.nn.embedding_lookup(embed_matrix, [3])
</code></pre>

<p>You can't possibly look up a floating point index, such as <code>0.2</code> or <code>0.8</code>, because there is no <code>0.2</code> and <code>0.8</code> row index in the matrix. Highly recommend <a href=""http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"" rel=""nofollow noreferrer"">this post by Chris McCormick</a> about word2vec.</p>

<p>What you describe sounds more like a softmax loss function, which outputs a probability distribution over the target classes.</p>
",2,2,1585,2017-11-09 23:52:56,https://stackoverflow.com/questions/47213602/tf-nn-embedding-lookup-with-float-input
"Keras: Dense vs. Embedding - ValueError: Input 0 is incompatible with layer repeat_vector_9: expected ndim=2, found ndim=3","<p>I have the following network which works fine:</p>

<pre><code>left = Sequential()
left.add(Dense(EMBED_DIM,input_shape=(ENCODE_DIM,)))
left.add(RepeatVector(look_back))
</code></pre>

<p>However, I need to replace the Dense layer with the Embedding layer:</p>

<pre><code>left = Sequential()
left.add(Embedding(ENCODE_DIM, EMBED_DIM, input_length=1))
left.add(RepeatVector(look_back))
</code></pre>

<p>Then I got the following error when I use Embedding layer:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-119-5a5f11c97e39&gt; in &lt;module&gt;()
     29 left.add(Embedding(ENCODE_DIM, EMBED_DIM, input_length=1))
---&gt; 30 left.add(RepeatVector(look_back))
     31 
     32 leftOutput = left.output

/usr/local/lib/python3.4/dist-packages/keras/models.py in add(self, layer)
    467                           output_shapes=[self.outputs[0]._keras_shape])
    468         else:
--&gt; 469             output_tensor = layer(self.outputs[0])
    470             if isinstance(output_tensor, list):
    471                 raise TypeError('All layers in a Sequential model '

/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py in __call__(self, inputs, **kwargs)
    550                 # Raise exceptions in case the input is not compatible
    551                 # with the input_spec specified in the layer constructor.
--&gt; 552                 self.assert_input_compatibility(inputs)
    553 
    554                 # Collect input shapes to build layer.

/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py in assert_input_compatibility(self, inputs)
    449                                      self.name + ': expected ndim=' +
    450                                      str(spec.ndim) + ', found ndim=' +
--&gt; 451                                      str(K.ndim(x)))
    452             if spec.max_ndim is not None:
    453                 ndim = K.ndim(x)

ValueError: Input 0 is incompatible with layer repeat_vector_9: expected ndim=2, found ndim=3
</code></pre>

<p>What additional changes do I need when replacing the Dense layer with an Embedding layer? Thanks!</p>
","keras, embedding, keras-layer, word-embedding, keras-2","<p>The output shape of the <code>Dense</code> layer is <code>(None, EMBED_DIM)</code>. However, the output shape of the <code>Embedding</code> layer is <code>(None, input_length, EMBED_DIM)</code>. With <code>input_length=1</code>, it'll be <code>(None, 1, EMBED_DIM)</code>. You can add a <code>Flatten</code> layer after the <code>Embedding</code> layer to remove axis 1.</p>

<p>You can print out the output shape to debug your model. For example,</p>



<pre class=""lang-py prettyprint-override""><code>EMBED_DIM = 128
left = Sequential()
left.add(Dense(EMBED_DIM, input_shape=(ENCODE_DIM,)))
print(left.output_shape)
(None, 128)

left = Sequential()
left.add(Embedding(ENCODE_DIM, EMBED_DIM, input_length=1))
print(left.output_shape)
(None, 1, 128)

left.add(Flatten())
print(left.output_shape)
(None, 128)
</code></pre>
",3,0,691,2017-11-18 07:32:00,https://stackoverflow.com/questions/47363698/keras-dense-vs-embedding-valueerror-input-0-is-incompatible-with-layer-repe
Get most similar words using GloVe,"<p>I am new to GloVe. I successfully ran their <a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">demo.sh</a> as given in their website. After running demo I got several files created such as <code>vocab</code>, <code>vectors</code> etc. But they haven't any documentation or anything that describes what files we need to use and how to use to find most similar words.</p>

<p>Hence, please help me to find the most similar words given a word in GloVe (using cosine similarity)? (e.g., like <code>most.similar</code> in Gensim word2vec)</p>

<p>Please help me!</p>
","nlp, stanford-nlp, word-embedding","<p>It doesn't really matter how word vectors are generated, you can always calculate cosine similarity between the words. The easiest way to achieve what you asked for is (considering you have gensim):</p>

<pre><code>python -m gensim.scripts.glove2word2vec –input &lt;GloVe vector file&gt; –output &lt;Word2vec vector file&gt;
</code></pre>

<p>This will convert glove vector file to w2v format. You can do it manually too - just add extra line to your GloVe file containing total number of vectors and their dimensionality at the top of your file. It looks something a kin of:</p>

<pre><code>180000 300
&lt;The rest of your file&gt;
</code></pre>

<p>After that you can just load the file into gensim and everything is working as if it is a regular w2v model. </p>
",2,1,3235,2017-11-18 11:25:19,https://stackoverflow.com/questions/47365480/get-most-similar-words-using-glove
How does mask_zero in Keras Embedding layer work?,"<p>I thought <code>mask_zero=True</code> will output 0's when the input value is 0, so the following layers could skip computation or something.</p>

<p>How does <code>mask_zero</code> works? </p>

<p>Example: </p>

<pre><code>data_in = np.array([
  [1, 2, 0, 0]
])
data_in.shape
&gt;&gt;&gt; (1, 4)

# model
x = Input(shape=(4,))
e = Embedding(5, 5, mask_zero=True)(x)

m = Model(inputs=x, outputs=e)
p = m.predict(data_in)
print(p.shape)
print(p)
</code></pre>

<p>The actual output is: (the numbers are random)</p>

<pre><code>(1, 4, 5)
[[[ 0.02499047  0.04617121  0.01586803  0.0338897   0.009652  ]
  [ 0.04782704 -0.04035913 -0.0341589   0.03020919 -0.01157228]
  [ 0.00451764 -0.01433611  0.02606953  0.00328832  0.02650392]
  [ 0.00451764 -0.01433611  0.02606953  0.00328832  0.02650392]]]
</code></pre>

<p>However, I thought the output will be:</p>

<pre><code>[[[ 0.02499047  0.04617121  0.01586803  0.0338897   0.009652  ]
  [ 0.04782704 -0.04035913 -0.0341589   0.03020919 -0.01157228]
  [ 0 0 0 0 0]
  [ 0 0 0 0 0]]]
</code></pre>
","python, machine-learning, keras, word-embedding","<p>Actually, setting <code>mask_zero=True</code> for the Embedding layer does not result in returning a zero vector. Rather, the behavior of the Embedding layer would not change and it would return the embedding vector with index zero. You can confirm this by checking the Embedding layer weights (i.e. in the example you mentioned it would be <code>m.layers[0].get_weights()</code>). Instead, it would affect the behavior of the following layers such as RNN layers. </p>

<p>If you inspect the source code of Embedding layer you would see a method called <a href=""https://github.com/keras-team/keras/blob/920e8af34a43ad2cd11190a21200a2acbfd83e11/keras/layers/embeddings.py#L112"" rel=""noreferrer""><code>compute_mask</code></a>:</p>

<pre><code>def compute_mask(self, inputs, mask=None):
    if not self.mask_zero:
        return None
    output_mask = K.not_equal(inputs, 0)
    return output_mask
</code></pre>

<p>This output mask will be passed, as the <code>mask</code> argument, to the following layers which support masking. This has been implemented in the <a href=""https://github.com/keras-team/keras/blob/d2ebf181b603e7d03e65df941dd754df5de32913/keras/engine/base_layer.py#L442"" rel=""noreferrer""><code>__call__</code></a> method of base layer, <code>Layer</code>:</p>

<pre><code># Handle mask propagation.
previous_mask = _collect_previous_mask(inputs)
user_kwargs = copy.copy(kwargs)
if not is_all_none(previous_mask):
    # The previous layer generated a mask.
    if has_arg(self.call, 'mask'):
        if 'mask' not in kwargs:
            # If mask is explicitly passed to __call__,
            # we should override the default mask.
            kwargs['mask'] = previous_mask
</code></pre>

<p>And this makes the following layers to ignore (i.e. does not consider in their computations) this inputs steps. Here is a minimal example:</p>

<pre><code>data_in = np.array([
  [1, 0, 2, 0]
])

x = Input(shape=(4,))
e = Embedding(5, 5, mask_zero=True)(x)
rnn = LSTM(3, return_sequences=True)(e)

m = Model(inputs=x, outputs=rnn)
m.predict(data_in)

array([[[-0.00084503, -0.00413611,  0.00049972],
        [-0.00084503, -0.00413611,  0.00049972],
        [-0.00144554, -0.00115775, -0.00293898],
        [-0.00144554, -0.00115775, -0.00293898]]], dtype=float32)
</code></pre>

<p>As you can see the outputs of the LSTM layer for the second and forth timesteps are the same as the output of first and third timesteps, respectively. This means that  those timesteps have been masked.</p>

<p><strong>Update:</strong> The mask will also be considered when computing the loss since the loss functions are internally augmented to support masking using <a href=""https://github.com/keras-team/keras/blob/920e8af34a43ad2cd11190a21200a2acbfd83e11/keras/engine/training_utils.py#L375"" rel=""noreferrer""><code>weighted_masked_objective</code></a>:</p>

<pre><code>def weighted_masked_objective(fn):
    """"""Adds support for masking and sample-weighting to an objective function.
    It transforms an objective function `fn(y_true, y_pred)`
    into a sample-weighted, cost-masked objective function
    `fn(y_true, y_pred, weights, mask)`.
    # Arguments
        fn: The objective function to wrap,
            with signature `fn(y_true, y_pred)`.
    # Returns
        A function with signature `fn(y_true, y_pred, weights, mask)`.
    """"""
</code></pre>

<p><a href=""https://github.com/keras-team/keras/blob/920e8af34a43ad2cd11190a21200a2acbfd83e11/keras/engine/training.py#L142"" rel=""noreferrer"">when compiling the model</a>:</p>

<pre><code>weighted_losses = [weighted_masked_objective(fn) for fn in loss_functions]
</code></pre>

<p>You can verify this using the following example:</p>

<pre><code>data_in = np.array([[1, 2, 0, 0]])
data_out = np.arange(12).reshape(1,4,3)

x = Input(shape=(4,))
e = Embedding(5, 5, mask_zero=True)(x)
d = Dense(3)(e)

m = Model(inputs=x, outputs=d)
m.compile(loss='mse', optimizer='adam')
preds = m.predict(data_in)
loss = m.evaluate(data_in, data_out, verbose=0)
print(preds)
print('Computed Loss:', loss)

[[[ 0.009682    0.02505393 -0.00632722]
  [ 0.01756451  0.05928303  0.0153951 ]
  [-0.00146054 -0.02064196 -0.04356086]
  [-0.00146054 -0.02064196 -0.04356086]]]
Computed Loss: 9.041069030761719

# verify that only the first two outputs 
# have been considered in the computation of loss
print(np.square(preds[0,0:2] - data_out[0,0:2]).mean())

9.041070036475277
</code></pre>
",51,38,21090,2017-11-25 11:03:49,https://stackoverflow.com/questions/47485216/how-does-mask-zero-in-keras-embedding-layer-work
Embeddings (word or other) standard file format,"<p>I am creating my own word embeddings and I have various versions of them.</p>

<p>What is the standard way (if there is one) to save embeddings to files, so that others could easily read and use them later?</p>

<p>If there are several accepted methods, I'd appreciate an answer that explains each method.</p>
",word-embedding,"<p>I have found how standard text format of word embeddings look like</p>

<pre><code>&lt;vocabulary_length&gt; &lt;embedding_dimensions&gt;
&lt;word1&gt; &lt;emb1_dim1&gt; &lt;emb1_dim2&gt; ...... &lt;emb1_dim_n&gt;
&lt;word2&gt; &lt;emb2_dim1&gt; &lt;emb2_dim2&gt; ...... &lt;emb2_dim_n&gt;
.
.
&lt;word_m&gt; &lt;embm_dim1&gt; &lt;embm_dim2&gt; ...... &lt;embm_dim_n&gt;
</code></pre>

<p>Where in this example <code>vocabulary_length</code> is <code>m</code> and <code>embedding_dimensions</code> is <code>n</code></p>
",1,0,372,2017-11-27 14:00:36,https://stackoverflow.com/questions/47512727/embeddings-word-or-other-standard-file-format
Does word2vec realization from gensim go beyond sentence level when examining context?,"<p>I found <a href=""https://stackoverflow.com/questions/36790867"">this question</a> which provides evidence that sentence order probably matters (but effect can be also a result of different random initialization).</p>

<p>I want to process <a href=""https://files.pushshift.io/reddit/comments/"" rel=""nofollow noreferrer"">Reddit comment dumps</a> for my project, but the strings extracted from json would be unsorted and belong to very different subreddits and topics, so I don't want to mess up contexts:</p>

<pre><code>{""gilded"":0,""author_flair_text"":""Male"",""author_flair_css_class"":""male"",""retrieved_on"":1425124228,""ups"":3,""subreddit_id"":""t5_2s30g"",""edited"":false,""controversiality"":0,""parent_id"":""t1_cnapn0k"",""subreddit"":""AskMen"",""body"":""I can't agree with passing the blame, but I'm glad to hear it's at least helping you with the anxiety. I went the other direction and started taking responsibility for everything. I had to realize that people make mistakes including myself and it's gonna be alright. I don't have to be shackled to my mistakes and I don't have to be afraid of making them. "",""created_utc"":""1420070668"",""downs"":0,""score"":3,""author"":""TheDukeofEtown"",""archived"":false,""distinguished"":null,""id"":""cnasd6x"",""score_hidden"":false,""name"":""t1_cnasd6x"",""link_id"":""t3_2qyhmp""}
</code></pre>

<p>So does the neighbor sentences matter for gensim word2vec and should I recover whole comment tree structure, or I can simply extract ""bag of sentences"" and train the model on it?</p>
","word2vec, gensim, word-embedding","<p>The corpus expected by gensim Word2Vec is an <em>iterable</em> of <em>lists-of-tokens</em>. (For example, a list of lists-of-tokens would work, but for larger corpuses you'd usually want to provide a restartable iterable that streams text examples from persistent storage, to avoid holding the whole corpus in memory.) </p>

<p>The word-vector training only considrs context <em>within individual text examples</em>. That is, within one list-of-tokens. So if two consecutive examples are...</p>

<pre><code>['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']
['Everybody', 'needs', 'a', 'thneed']
</code></pre>

<p>...there's no influence, in these examples between 'ham' and 'Everybody'. (The contexts are only within each example.)</p>

<p>Still, there could be subtle effects on quality if the ordering of examples clumps together all words or topics of a certain type. For example, you'd not want all examples of word X to happen in the beginning of the corpus, and all examples of word Y to happen late – that prevents the sort of interleaved variety-of-examples that achieves the best results.</p>

<p>So if your corpus comes in any kind of sorted order, clumped by topic or author or size or language, it is often beneficial to perform an initial shuffle to remove such clumping. (Re-shuffling any more, such as between training passes, usually has negligible additional benefit.)</p>
",1,0,175,2017-12-01 17:19:25,https://stackoverflow.com/questions/47598369/does-word2vec-realization-from-gensim-go-beyond-sentence-level-when-examining-co
Keras throws `&#39;Tensor&#39; object has no attribute &#39;_keras_shape&#39;` when splitting a layer output,"<p>
I have sentence embedding output X of a sentence pair of dimension <code>2*1*300</code>. I want to split this output into two vectors of shape <code>1*300</code> to calculate its absolute difference and product.
</p>

<pre><code>x = MaxPooling2D(pool_size=(1,MAX_SEQUENCE_LENGTH),strides=(1,1))(x)
x_A = Reshape((1,EMBEDDING_DIM))(x[:,0])
x_B = Reshape((1,EMBEDDING_DIM))(x[:,1])

diff = keras.layers.Subtract()([x_A, x_B])
prod = keras.layers.Multiply()([x_A, x_B])


nn = keras.layers.Concatenate()([diff, prod])
</code></pre>



<p>Currently, when I do <code>x[:,0]</code> it throws an error saying <code>AttributeError: 'Tensor' object has no attribute '_keras_shape'</code>. I assume the result of splitting of tensor object is a tensor object that doesn't have <code>_keras_shape</code>. </p>

<p>Can someone help me solve this? Thanks.</p>


","python, tensorflow, keras, word-embedding, sentence-similarity","<p>Keras adds some info to tensors when they're processed in layers. Since you're splitting the tensor outside layers, it loses that info.</p>

<p>The solution involves returning the split tensors from Lambda layers:</p>

<pre><code>x_A = Lambda(lambda x: x[:,0], output_shape=notNecessaryWithTensorflow)(x)
x_B = Lambda(lambda x: x[:,1], output_shape=notNecessaryWithTensorflow)(x)
x_A = Reshape((1,EMBEDDING_DIM))(x_A)
x_B = Reshape((1,EMBEDDING_DIM))(x_B)
</code></pre>
",1,2,2655,2017-12-03 08:21:28,https://stackoverflow.com/questions/47616588/keras-throws-tensor-object-has-no-attribute-keras-shape-when-splitting-a
What does a weighted word embedding mean?,"<p>In the <a href=""http://www.aclweb.org/anthology/S17-2100"" rel=""noreferrer"">paper</a> that I am trying to implement, it says,</p>

<blockquote>
  <p>In this work, tweets were modeled using three types of text
  representation. The first one is a bag-of-words model weighted by
  tf-idf (term frequency
  - inverse document frequency) (Section
  2.1.1). The second represents a sentence by averaging the word embeddings of all words (in the sentence) and the third represents a
  sentence by averaging the weighted word embeddings of all words, the
  weight of a word is given by tf-idf (Section
  2.1.2).</p>
</blockquote>

<p>I am not sure about the <em>third representation</em> which is mentioned as the weighted word embeddings which is using the weight of a word is given by tf-idf. I am not even sure if they can used together. </p>
","machine-learning, nlp, word2vec, tf-idf, word-embedding","<p>Averaging (possibly weighted) of word embeddings makes sense, though depending on the main algorithm and the training data this sentence representation may not be the best. The intuition is the following: </p>

<ul>
<li>You might want to handle sentences of different length, hence the averaging (better than plain sum).</li>
<li>Some words in a sentence are usually much more valuable than others. TF-IDF is the simplest measure of the word value. Note that the scale of the result doesn't change.</li>
</ul>

<p>See also <a href=""http://aclweb.org/anthology/P/P16/P16-1089.pdf"" rel=""noreferrer"">this paper by Kenter et al</a>. There is a <a href=""http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"" rel=""noreferrer"">nice post</a> that performs the comparison of these two approaches in different algorithms, and concludes that none is significantly better than the other: some algorithms favor simple averaging, some algorithms perform better with TF-IDF weighting.</p>
",22,18,15201,2017-12-09 09:16:02,https://stackoverflow.com/questions/47727078/what-does-a-weighted-word-embedding-mean
word2Vec vector representation for text classification algorithm,"<p>I am trying to use word2vec in text classification algorithm.
I want t create vectorizer using word2vec, I have used below script. But I am not able to get one row for each document instead I am getting matrix of different dimension for every document. 
For example for 1st document matrix of 31X100,  2nd 163X100 and 3rd 73X100 and so on.
Actually I need dimension of every document as 1X100 , so that i can use these as input feature for training model</p>

<p>Can anyone help me here.</p>

<pre><code>import os
import pandas as pd       
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords # Import the stop word list
import gensim
import numpy as np

train = pd.read_csv(""Data.csv"",encoding='cp1252')
wordnet_lemmatizer = WordNetLemmatizer()

def Description_to_words(raw_Description):
    Description_text = BeautifulSoup(raw_Description).get_text() 
    letters_only = re.sub(""[^a-zA-Z]"", "" "", Description_text)
    words = word_tokenize(letters_only.lower())    
    stops = set(stopwords.words(""english"")) 
    meaningful_words = [w for w in words if not w in stops]
    return( "" "".join(wordnet_lemmatizer.lemmatize(w) for w in meaningful_words))

num_Descriptions = train[""Summary""].size
clean_train_Descriptions = []
print(""Cleaning and parsing the training set ticket Descriptions...\n"")
clean_train_Descriptions = []
for i in range( 0, num_Descriptions ):
    if( (i+1)%1000 == 0 ):
        print(""Description %d of %d\n"" % ( i+1, num_Descriptions ))
    clean_train_Descriptions.append(Description_to_words( train[""Summary""][i] ))

model = gensim.models.Word2Vec(clean_train_Descriptions, size=100)
w2v = dict(zip(model.wv.index2word, model.wv.syn0))

class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        #self.dim = len(word2vec.itervalues().next())
        self.dim = 100

    def fit(self, X, y):
        return self

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

a=MeanEmbeddingVectorizer(w2v)
clean_train_Descriptions[1]
a.transform(clean_train_Descriptions[1])

train_Descriptions = []
for i in range( 0, num_Descriptions ):
    if( (i+1)%1000 == 0 ):
        print(""Description %d of %d\n"" % ( i+1, num_Descriptions ))
    train_Descriptions.append(a.transform("" "".join(clean_train_Descriptions[i])))
</code></pre>
","python, word2vec, word-embedding","<p>You have 2 issues in your code causing problems, both easily solved.</p>

<p>First, Word2Vec requires sentences to be actually a list of words, rather than an actual sentence as a single string. So from your <code>Description_to_words</code> just return the list, don't join.</p>

<p><code>return [wordnet_lemmatizer.lemmatize(w) for w in meaningful_words]</code></p>

<p>Since word2vec iterates over each sentence to get the words, previously it was iterating over a string, and you were actually getting a character level embedding from <code>wv</code>.</p>

<p>Secondly, a similar issue with the way you are calling transform - <code>X</code> is expected to be a list of documents, not an individual document. So when you are doing <code>for words in X</code>, you are actually creating a list of characters, and then iterating over that to create embedding. So your output was actually the individual character embeddings for each character in your sentences. Simply changed, just convert all documents at once!</p>

<p><code>train_Descriptions = a.transform(clean_train_Descriptions)</code></p>

<p>(to do one at a time, wrap in a list (<code>[clean_train_Descriptions[1]]</code>), or select 1 using the range selector( <code>clean_train_Descriptions[1:2]</code>).</p>

<p>With those two changes you should get 1 row back per input sentence.</p>
",2,2,2755,2017-12-22 06:02:29,https://stackoverflow.com/questions/47936578/word2vec-vector-representation-for-text-classification-algorithm
How to convert gensim Word2Vec model to FastText model?,"<p>I have a Word2Vec model which was trained on a huge corpus. While using this model for Neural network application I came across quite a few ""Out of Vocabulary"" words. Now I need to find word embeddings for these ""Out of Vocabulary"" words. So I did some googling and found that Facebook has recently released a FastText library for this. Now my question is how can I convert my existing word2vec model or Keyedvectors to FastText model?</p>
","nlp, word2vec, gensim, word-embedding, fasttext","<p>FastText is able to create vectors for subword fragments by including those fragments in the initial training, from the original corpus. Then, when encountering an out-of-vocabulary ('OOV') word, it constructs a vector for those words using fragments it recognizes. For languages with recurring word-root/prefix/suffix patterns, this results in vectors that are better than random guesses for OOV words. </p>

<p>However, the FastText process does <em>not</em> extract these subword vectors from final full-word vectors. Thus there's no simple way to turn full-word vectors into a FastText model that also includes subword vectors.</p>

<p>There might be workable way to approximate the same effect, for example by taking all known-words with the same subword fragment, and extracting some common average/vector-component to be assigned to the subword. Or modeling OOV words as some average of in-vocabulary words that are a short edit-distance from the OOV word. But these techniques wouldn't quite be FastText, just vaguely analogous to it, and how well they work, or could be made to work with tweaking, would be an experimental question. So, it's not a matter of grabbing an off-the-shelf library.</p>

<p>There are a couple of research papers with other OOV-bootstrapping ideas, mentioned in <a href=""http://ruder.io/word-embeddings-2017/index.html#oovhandling"" rel=""nofollow noreferrer"">this blog post by Sebastien Ruder</a>.</p>

<p>If you need the FastText OOV functionality, the best-grounded approach would be to train FastText vectors from scratch on the same corpus as was used for your traditional full-word-vectors.</p>
",3,3,3001,2017-12-29 04:18:46,https://stackoverflow.com/questions/48017343/how-to-convert-gensim-word2vec-model-to-fasttext-model
Can I export the embedding matrix of words in tensorflow?,"<pre><code>def word_embedding(shape, dtype=tf.float32, name='word_embedding'):
  with tf.device('/cpu:0'), tf.variable_scope(name):
    return tf.get_variable('embedding', shape, dtype=dtype, initializer=tf.random_normal_initializer(stddev=0.1), trainable=True,partitioner=tf.fixed_size_partitioner(20))
embedding = word_embedding([vocab_size, embed_size])
inputs_embedding = tf.contrib.layers.embedding_lookup_unique(embedding, inputs)
</code></pre>

<p>Here is my code and the <code>embedding</code> is the variable for word to look up their own embedding vector. </p>

<p>I have trained the embedding matrix and I want to extract it from the model saved. The model also contain other parameters for example the neural networks above embeddings. Can I implement it?</p>
","tensorflow, neural-network, nlp, deep-learning, word-embedding","<p>See <a href=""https://stackoverflow.com/a/47892839/712995"">my answer</a> to a similar question.</p>

<p>The simplest way is to evaluate the embeddings matrix into a numpy array and write it to the file along with resolved words.</p>



<pre class=""lang-py prettyprint-override""><code>with tf.Session() as sess:
  embedding_val = sess.run(embedding)
  with open('embedding.txt', 'w') as file_:
    for i in range(vocabulary_size):
      embed = embedding_val[i, :]
      word = word_to_idx[i]
      file_.write('%s %s\n' % (word, ' '.join(map(str, embed))))
</code></pre>

<p>If you want to save the embeddings just for <em>this</em> graph, you can create <code>tf.train.Saver</code> and pass the list of variables to save:</p>

<pre class=""lang-py prettyprint-override""><code>saver = tf.train.Saver([embedding])
with tf.Session() as sess:
  saver.save(sess, 'path/to/checkpoint')
</code></pre>
",3,3,4603,2017-12-29 08:41:07,https://stackoverflow.com/questions/48019799/can-i-export-the-embedding-matrix-of-words-in-tensorflow
why embedding_lookup only used as encoder but no decoder in ptb_word_ln.py,"<p>I have a question about embedding_lookup while I looking the tensorflow's official sample code ptb_word_ln.py.
<a href=""https://i.sstatic.net/jIc00.png"" rel=""nofollow noreferrer"">the embedding_lookup node</a></p>

<p>I found it is only used as an input. the output doesn't use this. so the loss evaluation cannot be benefit from this embedding. so what is the benefit using embedding_lookup here? If I want to use this word-embedding in the optimizer, shouldn't I connect it with loss function explicitly?</p>

<p>the source code as following:</p>

<pre><code>self._input = input_

batch_size = input_.batch_size
num_steps = input_.num_steps
size = config.hidden_size
vocab_size = config.vocab_size

def lstm_cell():
  # With the latest TensorFlow source code (as of Mar 27, 2017),
  # the BasicLSTMCell will need a reuse parameter which is unfortunately not
  # defined in TensorFlow 1.0. To maintain backwards compatibility, we add
  # an argument check here:
  if 'reuse' in inspect.getargspec(
      tf.contrib.rnn.BasicLSTMCell.__init__).args:
    return tf.contrib.rnn.BasicLSTMCell(
        size, forget_bias=0.0, state_is_tuple=True,
        reuse=tf.get_variable_scope().reuse)
  else:
    return tf.contrib.rnn.BasicLSTMCell(
        size, forget_bias=0.0, state_is_tuple=True)
attn_cell = lstm_cell
if is_training and config.keep_prob &lt; 1:
  def attn_cell():
    return tf.contrib.rnn.DropoutWrapper(
        lstm_cell(), output_keep_prob=config.keep_prob)
cell = tf.contrib.rnn.MultiRNNCell(
    [attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)

self._initial_state = cell.zero_state(batch_size, data_type())

with tf.device(""/cpu:0""):
  embedding = tf.get_variable(
      ""embedding"", [vocab_size, size], dtype=data_type())
  inputs = tf.nn.embedding_lookup(embedding, input_.input_data)#only use embeddings here

if is_training and config.keep_prob &lt; 1:
  inputs = tf.nn.dropout(inputs, config.keep_prob)

outputs = []
state = self._initial_state
with tf.variable_scope(""RNN""):
  for time_step in range(num_steps):
    if time_step &gt; 0: tf.get_variable_scope().reuse_variables()
    (cell_output, state) = cell(inputs[:, time_step, :], state)
    outputs.append(cell_output)

output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])
softmax_w = tf.get_variable(
    ""softmax_w"", [size, vocab_size], dtype=data_type())
softmax_b = tf.get_variable(""softmax_b"", [vocab_size], dtype=data_type())
logits = tf.matmul(output, softmax_w) + softmax_b
loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
    [logits],
    [tf.reshape(input_.targets, [-1])],
    [tf.ones([batch_size * num_steps], dtype=data_type())])
self._cost = cost = tf.reduce_sum(loss) / batch_size
self._final_state = state

if not is_training:
  return

self._lr = tf.Variable(0.0, trainable=False)
tvars = tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                  config.max_grad_norm)
optimizer = tf.train.GradientDescentOptimizer(self._lr)
self._train_op = optimizer.apply_gradients(
    zip(grads, tvars),
    global_step=tf.contrib.framework.get_or_create_global_step())

self._new_lr = tf.placeholder(
    tf.float32, shape=[], name=""new_learning_rate"")
self._lr_update = tf.assign(self._lr, self._new_lr)
</code></pre>
","python, tensorflow, tensorboard, sample, word-embedding","<p>Actually output <em>does</em> use the embedding lookup.
TensorFlow programs are usually structured into a construction phase, that assembles a graph, and an execution phase that uses a session to execute ops in the graph. </p>

<p>In your case in order to compute loss you have to compute the following nodes on the graph in this order:</p>

<p><code>loss -&gt; logits -&gt; output -&gt; outputs -&gt; cell -&gt; inputs -&gt; embedding_lookup
</code>
Another way to look at it is if those were nested function calls:
<code>
loss(logits(output(outputs(cell_output(cell(inputs(embedding_lookup(embedding))))))))
</code>
I have emitted additional arguments from each function (op) to make it more clear.</p>
",2,2,215,2017-12-30 23:56:11,https://stackoverflow.com/questions/48038889/why-embedding-lookup-only-used-as-encoder-but-no-decoder-in-ptb-word-ln-py
Explicit CPU placement in TensorFlow,"<p>I found there are a piece of code in official model sample which confused me. </p>

<pre><code>with tf.device(""/cpu:0""):
  embedding = tf.get_variable(
      ""embedding"", [vocab_size, size], dtype=data_type())
  inputs = tf.nn.embedding_lookup(embedding, input_.input_data)
</code></pre>

<p>Why using <code>tf.device(""/cpu:0"")</code> here? Except the case GPU memory leak, is there any other situation which we need to designate CPU operations explicitly?</p>
","tensorflow, nlp, gpu, cpu, word-embedding","<p>The reason to place the embedding matrix on a CPU is that <code>tf.nn.embedding_lookup</code> <a href=""https://github.com/tensorflow/tensorflow/issues/2502"" rel=""nofollow noreferrer"">isn't supported</a> on a GPU yet:</p>

<blockquote>
  <p>So, given the basic word2vec example being bound to CPU (#514), we can
  see that <code>tf.nn.embedding_lookup</code> doesn't work on GPU. Therefore, ops
  that use <code>embedding_lookup</code> internally doesn't support GPU either (for
  example, <code>nce_loss</code>).</p>
</blockquote>

<p>This means that GPU placement of <code>embedding</code> variable will only lead to unnecessary transfer of data from the main memory to the GPU memory and vice versa. Hence, it would be more efficient to explicitly place the variable on a CPU.</p>
",3,3,923,2018-01-01 08:46:22,https://stackoverflow.com/questions/48048297/explicit-cpu-placement-in-tensorflow
Gensim word embedding training with initial values,"<p>I have a dataset with documents separated into different years, and my objective is to train an embedding model for each year's data, while at the same time, the same word appearing in different years will have similar vector representations. Like this: for word 'compute', its vector in year 1 is</p>

<pre><code>[0.22, 0.33, 0.20]
</code></pre>

<p>and in year 2 it's something around:</p>

<pre><code>[0.20, 0.35, 0.18]
</code></pre>

<p>Is there a way to accomplish this? For example, train the model of year 2 with both initial values (if the word is trained already in year 1, modify its vector) and randomness (if this is a new word for the corpus).</p>
","machine-learning, nlp, word2vec, gensim, word-embedding","<p>I think the easiest solution is to <em>save</em> the embeddings after training on the first data set, then <em>load</em> the trained model and continue training for the second data set. This way you shouldn't expect the embeddings to drift away from the saved state much (unless your data sets are very different).</p>

<p>It would also make sense to create a single vocabulary from all documents: vocabulary words that aren't present in a particular document will get some random representation, but still it will be a working word2vec model.</p>

<p>Example from the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<pre><code>&gt;&gt;&gt; model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model.save(fname)
&gt;&gt;&gt; model = Word2Vec.load(fname)  # continue training with the loaded model
</code></pre>
",0,1,181,2018-01-09 09:25:03,https://stackoverflow.com/questions/48164954/gensim-word-embedding-training-with-initial-values
Is tensorflow embedding_lookup differentiable?,"<p>Some of the tutorials I came across, described using a randomly initialized embedding matrix and then using the <code>tf.nn.embedding_lookup</code> function to obtain the embeddings for the integer sequences. I am under the impression that since the <code>embedding_matrix</code> is obtained through <code>tf.get_variable</code>, the optimizer would add appropriate <strong>ops</strong> for updating it. </p>

<p>What I don't understand is how backpropagation happens through the lookup function which seems to be hard rather than being soft. What is the gradient of the this operation wrt. one of it's input ids?</p>
","tensorflow, nlp, deep-learning, word-embedding, sequence-to-sequence","<p>Embedding matrix lookup is mathematically equivalent to dot product with the one-hot encoded matrix (see <a href=""https://stackoverflow.com/q/47868265/712995"">this question</a>), which is a smooth linear operation. </p>

<p>For example, here's a lookup at the index <code>3</code>:</p>

<p><a href=""https://i.sstatic.net/11LeF.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/11LeF.png"" alt=""look-up""></a></p>

<p>Here's the formula for the gradient:</p>

<p><a href=""https://i.sstatic.net/izqid.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/izqid.png"" alt=""gradient""></a></p>

<p>... where left-hand side is the derivative of negative log-likelihood (i.e., the objective function), <code>x</code> are the input words, <code>W</code> is the embedding matrix and <code>delta</code> is the error signal.</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""noreferrer""><code>tf.nn.embedding_lookup</code></a> is optimized so that no one-hot encoding conversion happens, but the backprop is working according to the same formula.</p>
",10,4,1572,2018-01-09 11:01:27,https://stackoverflow.com/questions/48166721/is-tensorflow-embedding-lookup-differentiable
"Tensorflow, compare an indexed value in a tensor with an integer for if condition","<p>I am using <strong>TensorFlow</strong> to do a customized embedding training similar to continuous bag of words (CBOW) model. However, unlike 'CBOW', which has a fixed length sliding window, my sliding window can be considered as flexible. Here is the problem:</p>

<p>Let's say, the embedding is word embedding. For word t, I have a tensor showing indexes of its context words: [-1, 1, 2, -1]. The maximum window size is 4, so the length of the vector is 4. But sometimes I do not have 4 context words for a word, so I use '-1' to mean 'no word in this position', and other integers are the index of a word. I also have an 'embedding' tensor, which is the embeddings for all the words.   </p>

<p>What I am trying to do is to get the average embedding for the context words in order to represent the context. For example, if the context words are [-1, 1, 2, -1], I would get (1 * (embedding for word 1) + 2 * (embedding for word 2) ) / 2. I just need to neglect all the -1.</p>

<p>So in my code, I try to loop through the context word tensor to compare each value with -1 and use an if condition to control if I would add the embedding of this context word. I tried different ways for this, but always get 'TypeError: Using a <code>tf.Tensor</code> as a Python <code>bool</code> is not allowed.'</p>

<p>Is there a way to solve this problem? Or even better, is there a better representation of positions with no words so I can compute more efficiently (Tried to use NaN but also get a lot of troubles...)?</p>

<p>Thanks a lot for the help and hopefully I have a clear description of the problem.</p>
","python, tensorflow, word-embedding","<p>I think it will be helpful to understand how to get values from the Tensorflow tensor. Take a look on the accepted answer <a href=""https://stackoverflow.com/questions/33633370/how-to-print-the-value-of-a-tensor-object-in-tensorflow"">here</a>. Remember that Tensorflow may be seen as a symbolic language, so <code>tf.Tensor</code> is a symbol which tales certain values while evaluating within a session, not when it is defined or stack with other operations. </p>

<p>If you want to make a comparison within a graph, use tensorflow special functions for tensors comparisons like <code>tf.equal</code>, <code>tf.less</code> etc. For instace, I compare the first element of a tensor <code>a</code> with another constant tensor b:</p>

<pre><code># Here is an example of two tensors
a = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])
b = tf.constant(1, dtype=tf.float32)

# generate a tensor representing a result of comparison
c = tf.equal(a[0, 0], b)
# Evaluate the output tensor in a session
print(tf.Session().run(c))
</code></pre>

<p>The output is True </p>

<p>Also, you may try to extract values of the tensor as follows:</p>

<pre><code># Here is an example of the tensor
a = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])

# Evaluate the tensor in a session
print(tf.Session().run(a))
</code></pre>

<p>What is done here is conversion of the tf tensor into an numpy array, which you may process in any desired way. </p>
",1,0,2978,2018-01-10 20:59:19,https://stackoverflow.com/questions/48195979/tensorflow-compare-an-indexed-value-in-a-tensor-with-an-integer-for-if-conditio
How can I get a vector after each training iter in word2vec?,"<p>I want to get a vector of words every few iter in <code>word2vec</code>, e.g., I would like to use the model below.</p>

<pre><code>embedding_model = Word2Vec(test_set, size=300, 
                           window=4, workers=6, 
                           iter=300, sg=1, min_count=10)
</code></pre>

<p>In this model, I want to get the 300-dimensional vectors learned for every 50 iterations, because I want to show continuous learning contents in html d3.</p>

<p>How can I do this?</p>
","python-3.x, nlp, word2vec, gensim, word-embedding","<p>You can call <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.train"" rel=""nofollow noreferrer""><code>train()</code></a> method iteratively 6 times, each with <code>epochs=50</code>:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec(size=300, window=4, workers=6, sg=1, 
                                        min_count=10)
model.build_vocab(sentences)
for i in range(6):
  model.train(sentences, total_examples=model.corpus_count, epochs=50)
  print(model.wv.word_vec('the'))  # get the intermediate vector(s)
</code></pre>
",2,2,285,2018-01-15 12:32:11,https://stackoverflow.com/questions/48263122/how-can-i-get-a-vector-after-each-training-iter-in-word2vec
Why does skipgram model take more time than CBOW,"<p>Why does skipgram model take more time than CBOW model. I train the model with same parameters (Vector size and window size).</p>
","word2vec, word-embedding","<p>The skip-gram approach involves more calculations.</p>

<p>Specifically, consider a single 'target word' with a context-window of 4 words on either side. </p>

<p>In CBOW, the vectors for all 8 nearby words are averaged together, then used as the input for the algorithm's prediction neural-network. The network is run forward, and its success at predicting the target word is checked. Then back-propagation occurs: all neural-network connection values – including the 8 contributing word-vectors – are nudged to make the prediction slightly better. </p>

<p>Note, though, that the 8-word-window and one-target-word only require one forward-propagation, and one-backward-propagation – and the initial averaging-of-8-values and final distribution-of-error-correction-over-8-vectors are each relatively quick/simple operations. </p>

<p>Now consider instead skip-gram. Each of the 8 context-window words is in turn individually provided as input to the neural-network, forward-checked for how well the target word is predicted, then backward-corrected. Though the averaging/splitting is not done, there's 8 times as much of the neural-network operations. Hence, much more net computation and more run-time. </p>

<p>Note the extra effort/time may pay itself back by improving vector quality on your final evaluations. Whether and to what extent depends on your specific goals and corpus. </p>
",5,2,1411,2018-01-18 23:30:17,https://stackoverflow.com/questions/48331975/why-does-skipgram-model-take-more-time-than-cbow
What is the difference between syntactic analogy and semantic analogy?,"<p>At 15:10 of <a href=""https://www.youtube.com/watch?v=CHcExDsDeHU&amp;feature=youtu.be&amp;t=15m"" rel=""noreferrer"">this</a> video about fastText it mentions syntactic analogy and semantic analogy. But I am not sure what the difference is between them.</p>

<p>Could anybody help explain the difference with examples?</p>
","nlp, word-embedding, fasttext","<p><strong>Syntactic</strong> means syntax, as in tasks that have to do with the structure of the sentence, these include tree parsing, POS tagging, usually they need less context and a shallower understanding of world knowledge</p>

<p><strong>Semantic</strong> tasks mean meaning related, a higher level of the language tree, these also typically involve a higher level understanding of the text and might involve tasks s.a. question answering, sentiment analysis, etc...</p>

<p>As for analogies, he is referring to the mathematical operator like properties exhibited by word embedding, in this context a syntactic analogy would be related to plurals, tense or gender, those sort of things, and semantic analogy would be word meaning relationships s.a. man + queen = king, etc... See for instance <a href=""http://www.aclweb.org/anthology/W14-1618"" rel=""noreferrer"">this article</a> (and many others)</p>
",9,5,2305,2018-01-20 12:58:33,https://stackoverflow.com/questions/48356421/what-is-the-difference-between-syntactic-analogy-and-semantic-analogy
How to initialize word-embeddings for Out of Vocabulary Word?,"<p>I am trying to use CoNLL-2003 NER (English) Dataset and I am trying to utilize pretrained embeddings for it. I am using SENNA pretrained embeddings. Now I have around 20k words in my vocabulary and out of this I have embedding available for only 9.5k words.<br>
My current approach is to initialize an array of <code>20k X embedding_size</code> with zeros and initialize the 9.5k words whose embeddings is known to me and make all the embeddings learn-able.<br></p>

<p>My question is what is the best way to do this? Any reference to such research will be very helpful?</p>
","machine-learning, nlp, deep-learning, word-embedding","<p>I would suggest three ways to tackle this problem, each with different strengths: </p>

<ul>
<li>Instead of using the SENNA embeddings, try using <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">FastText</a> embeddings. The advantage here is that they can infer embeddings for OOV words using character n-grams. For the exact methodology used, check the associated paper. <a href=""https://radimrehurek.com/gensim/models/wrappers/fasttext.html"" rel=""nofollow noreferrer"">Gensim</a> has implemented all the functionality needed. This will greatly reduce the problem, and you can further fine-tune the induced embeddings as you describe. The inconvenience is that you have to change from Senna to FastText.</li>
<li>Try using morphological or sementic similarity to initialize the OOV words. For morphological, I mean using a distance like Levenshtein to select an embedding. For an OOV word like <code>apple</code>, choose the closest (according to Levenshtein distance) word that you have an embeddings for, e.g., <code>apples</code>. In my experience, this can work remarkably well. On the other hand, semantic similarity would suggest using for instance synonyms, obtained from resources like WordNet or even averaging the embeddings of words that the OOV frequently co-occurs with. </li>
<li>After having reduced the sparsity with the ways described above, then proceed with  or random initialization that is discussed in other responses. </li>
</ul>
",4,2,4985,2018-01-23 06:35:42,https://stackoverflow.com/questions/48395570/how-to-initialize-word-embeddings-for-out-of-vocabulary-word
How do I create a Keras Embedding layer from a pre-trained word embedding dataset?,"<p>How do I load a pre-trained word-embedding into a Keras <code>Embedding</code> layer? </p>

<p>I downloaded the <code>glove.6B.50d.txt</code> (glove.6B.zip file from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">https://nlp.stanford.edu/projects/glove/</a>) and I'm not sure how to add it to a Keras Embedding layer. See: <a href=""https://keras.io/layers/embeddings/"" rel=""noreferrer"">https://keras.io/layers/embeddings/</a></p>
","python, tensorflow, keras, word2vec, word-embedding","<p>You will need to pass an embeddingMatrix to the <code>Embedding</code> layer as follows:</p>

<p><code>Embedding(vocabLen, embDim, weights=[embeddingMatrix], trainable=isTrainable)</code></p>

<ul>
<li><code>vocabLen</code>: number of tokens in your vocabulary</li>
<li><code>embDim</code>: embedding vectors dimension (50 in your example)</li>
<li><code>embeddingMatrix</code>: embedding matrix built from glove.6B.50d.txt</li>
<li><code>isTrainable</code>: whether you want the embeddings to be trainable or froze the layer</li>
</ul>

<p>The <code>glove.6B.50d.txt</code> is a list of whitespace-separated values: word token + (50) embedding values. e.g. <code>the 0.418 0.24968 -0.41242 ...</code></p>

<p>To create a <code>pretrainedEmbeddingLayer</code> from a Glove file:</p>

<pre><code># Prepare Glove File
def readGloveFile(gloveFile):
    with open(gloveFile, 'r') as f:
        wordToGlove = {}  # map from a token (word) to a Glove embedding vector
        wordToIndex = {}  # map from a token to an index
        indexToWord = {}  # map from an index to a token 

        for line in f:
            record = line.strip().split()
            token = record[0] # take the token (word) from the text line
            wordToGlove[token] = np.array(record[1:], dtype=np.float64) # associate the Glove embedding vector to a that token (word)

        tokens = sorted(wordToGlove.keys())
        for idx, tok in enumerate(tokens):
            kerasIdx = idx + 1  # 0 is reserved for masking in Keras (see above)
            wordToIndex[tok] = kerasIdx # associate an index to a token (word)
            indexToWord[kerasIdx] = tok # associate a word to a token (word). Note: inverse of dictionary above

    return wordToIndex, indexToWord, wordToGlove

# Create Pretrained Keras Embedding Layer
def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):
    vocabLen = len(wordToIndex) + 1  # adding 1 to account for masking
    embDim = next(iter(wordToGlove.values())).shape[0]  # works with any glove dimensions (e.g. 50)

    embeddingMatrix = np.zeros((vocabLen, embDim))  # initialize with zeros
    for word, index in wordToIndex.items():
        embeddingMatrix[index, :] = wordToGlove[word] # create embedding: word index to Glove word embedding

    embeddingLayer = Embedding(vocabLen, embDim, weights=[embeddingMatrix], trainable=isTrainable)
    return embeddingLayer

# usage
wordToIndex, indexToWord, wordToGlove = readGloveFile(""/path/to/glove.6B.50d.txt"")
pretrainedEmbeddingLayer = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, False)
model = Sequential()
model.add(pretrainedEmbeddingLayer)
...
</code></pre>
",11,7,8845,2018-02-08 03:30:38,https://stackoverflow.com/questions/48677077/how-do-i-create-a-keras-embedding-layer-from-a-pre-trained-word-embedding-datase
Getting embedding matrix of all zeros after performing word embedding on any input data,"<p>I am trying to do word embeddings in Keras. I am using 'glove.6B.50d.txt' for the purpose. I am able to get correct output till the preparation of embedding index from the ""glove.6B.50d.txt"" file.</p>

<p>But I'm always getting embedding matrix full of zeros whenever I map the word from the input provided by me to that in the embedding index.</p>



<p>Here is the code:</p>

<pre class=""lang-python prettyprint-override""><code>#here is the example sentence given as input

line=""The quick brown fox jumped over the lazy dog""
line=line.split("" "")

#this is my embedding file
EMBEDDING_FILE='glove.6B.50d.txt'

embed_size = 10 # how big is each word vector
max_features = 10000 # how many unique words to use (i.e num rows in embedding vector)
maxlen = 10 # max number of words in a comment to use


tokenizer = Tokenizer(num_words=max_features,split="" "",char_level=False)
tokenizer.fit_on_texts(list(line))
list_tokenized_train = tokenizer.texts_to_sequences(line)
sequences = tokenizer.texts_to_sequences(line)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)

print(sequences)
print(word_index)
print('Shape of data tensor:', X_t.shape)

#got correct output here as 

 # Found 8 unique tokens.
    #[[1], [2], [3], [4], [5], [6], [1], [7], [8]]
    #{'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumped': 5, 'over': 6, 'lazy': 7, 'dog': 8}
   # Shape of data tensor: (9, 10)


#loading the embedding file to prepare embedding index matrix
embeddings_index = {}
for i in open(EMBEDDING_FILE, ""rb""):
    values = i.split()
    word = values[0]
    #print(word)
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs

print('Found %s word vectors.' % len(embeddings_index))

#Found 400000 word vectors.

#making the embedding matrix

embedding_matrix = np.zeros((len(word_index) + 1, embed_size))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
</code></pre>

<p>Here when I print the embedding matrix ,I get all zeros in it (i.e not a single word in input is recognized).</p>

<pre class=""lang-python prettyprint-override""><code>array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre>

<p>Also if I print the <code>embeddings_index.get(word)</code> for each iteration, it is unable to fetch the word and returns NONE.</p>

<p>Where am I going wrong in the code?</p>
","keras, tokenize, word2vec, word-embedding","<p>Got the problem solved today.
Seems like <code>embeddings_index.get(word)</code> was unable to get the word because of some encoding issues.</p>

<p>I changed <code>for i in open(EMBEDDING_FILE, ""rb""):</code> present in the preparation of embedding matrix to <code>for i in open(EMBEDDING_FILE, 'r', encoding='utf-8'):</code>
and this solved the problem.</p>
",0,1,1681,2018-02-14 10:50:24,https://stackoverflow.com/questions/48785325/getting-embedding-matrix-of-all-zeros-after-performing-word-embedding-on-any-inp
How to search Word2Vec or GloVe Embedding to find words by semantic relationship,"<p>Common examples of showing Word Embedding's strength is to show semantic relationship between some words such <code>king:queen = male:female</code>. How can this type of relationship be discovered? Is that through some kind of visualization based on geometric clustering? Any pointer will be appreciated.</p>
","machine-learning, nlp, keras, word2vec, word-embedding","<p>If by ""discovered"" you mean <strong>supervised</strong> learning, there are <a href=""https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip"" rel=""nofollow noreferrer"">datasets</a> that contain lots of already extracted relationships, such as ""city-in-state"", ""capital-world"", ""superlative"", etc.</p>

<p><img src=""https://i.sstatic.net/e6KhY.png"" width=""360"" >
<img src=""https://i.sstatic.net/V4Eex.png"" width=""245"" ></p>

<p>This dataset is a popular choice for intrinsic evaluation of word vectors 
in completing word vector analogies. See also <a href=""https://stackoverflow.com/q/29591485/712995"">this question</a>.</p>

<p>Efficient <strong>unsupervised</strong> extraction of these relationships can be tricky. A naive algorithm requires O(n<sup>2</sup>) time and memory, where n is the number of words in a vocabulary, which is huge. In general, this problem boils down to efficient index construction.</p>

<p>But if you want just to train it yourself and play around with word embeddings, you can simply use <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">gensim</a>:</p>



<pre class=""lang-py prettyprint-override""><code>model = gensim.models.word2vec.Word2Vec(sentences=sentences, size=100, window=4,
                                        workers=5, sg=1, min_count=20, iter=10)
word_vectors = model.wv
similar = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])
# [(u'queen', 0.7188869714736938), (u'empress', 0.6739267110824585), ...
</code></pre>

<p>Note that you'll need a big corpus for that, such as <a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow noreferrer"">text8</a>.</p>
",1,1,1088,2018-02-20 03:10:13,https://stackoverflow.com/questions/48877277/how-to-search-word2vec-or-glove-embedding-to-find-words-by-semantic-relationship
Export gensim doc2vec embeddings into separate file to use with keras Embedding layer later,"<p>I am a bit new to gensim and right now I am trying to solve the problem which involves using the doc2vec embeddings in keras. I wasn't able to find existing implementation of doc2vec in keras - as far as I see in all examples I found so far everyone just uses the gensim to get the document embeddings.</p>

<p>Once I trained my doc2vec model in gensim I need to export embeddings weights from genim into keras somehow and it is not really clear on how to do that. I see that</p>

<pre><code>model.syn0
</code></pre>

<p>Supposedly gives the word2vec embedding weights (according to <a href=""https://codekansas.github.io/blog/2016/gensim.html"" rel=""nofollow noreferrer"">this</a>). But it is unclear how to do the same export for document embeddings. Any advise?</p>

<p>I know that in general I can just get the embeddings for each document directly from gensim model but I want to fine-tune the embedding layer in keras later on, since doc embeddings will be used as a part of a larger task hence they might be fine-tuned a bit.</p>
","keras, gensim, word-embedding, doc2vec","<p>I figured this out.</p>

<p>Assuming you already trained the gensim model and used string tags as document ids:</p>

<pre><code>#get vector of doc
model.docvecs['2017-06-24AEON']
#raw docvectors (all of them)
model.docvecs.doctag_syn0
#docvector names in model
model.docvecs.offset2doctag
</code></pre>

<p>You can export this doc vectors into keras embedding layer as below, assuming your DataFrame df has all of the documents out there. Notice that in the embedding matrix you need to pass only integers as inputs. I use raw number in dataframe as the id of the doc for input. Also notice that embedding layer requires to not touch index 0 - it is reserved for masking, so when I pass the doc id as input to my network I need to ensure it is >0</p>

<pre><code>#creating embedding matrix
embedding_matrix = np.zeros((len(df)+1, text_encode_dim))
for i, row in df.iterrows():
    embedding = modelDoc2Vec.docvecs[row['docCode']]
    embedding_matrix[i+1] = embedding

#input with id of document
doc_input = Input(shape=(1,),dtype='int16', name='doc_input')
#embedding layer intialized with the matrix created earlier
embedded_doc_input = Embedding(output_dim=text_encode_dim, input_dim=len(df)+1,weights=[embedding_matrix], input_length=1, trainable=False)(doc_input)
</code></pre>

<h1>UPDATE</h1>

<p>After late 2017, with the introduction of Keras 2.0 API very last line should be changed to:</p>

<pre><code>embedded_doc_input = Embedding(output_dim=text_encode_dim, input_dim=len(df)+1,embeddings_initializer=Constant(embedding_matrix), input_length=1, trainable=False)(doc_input)
</code></pre>
",4,2,1744,2018-02-27 00:19:17,https://stackoverflow.com/questions/48999199/export-gensim-doc2vec-embeddings-into-separate-file-to-use-with-keras-embedding
Train only some word embeddings (Keras),"<p>In my model, I use GloVe pre-trained embeddings. I wish to keep them non-trainable in order to decrease the number of model parameters and avoid overfit. However, I have a special symbol whose embedding I <em>do</em> want to train.</p>

<p>Using the provided Embedding Layer, I can only use the parameter 'trainable' to set the trainability of <strong>all</strong> embeddings in the following way:</p>

<pre><code>embedding_layer = Embedding(voc_size,
                        emb_dim,
                        weights=[embedding_matrix],
                        input_length=MAX_LEN,
                        trainable=False)
</code></pre>

<p>Is there a Keras-level solution to training only a subset of embeddings?  </p>

<p>Please note:</p>

<ol>
<li>There is not enough data to generate new embeddings for all words.</li>
<li><a href=""https://stackoverflow.com/questions/35803425/update-only-part-of-the-word-embedding-matrix-in-tensorflow"">These</a> answers only relate to native TensorFlow.</li>
</ol>
","python, nlp, keras, word-embedding","<p>Found some nice workaround, inspired by Keith's two embeddings layers.</p>

<p><strong>Main idea:</strong></p>

<p>Assign the special tokens (and the OOV) with the highest IDs. Generate a 'sentence' containing only special tokens, 0-padded elsewhere. Then apply non-trainable embeddings to the 'normal' sentence, and trainable embeddings to the special tokens. Lastly, add both.</p>

<p>Works fine to me.</p>

<pre><code>    # Normal embs - '+2' for empty token and OOV token
    embedding_matrix = np.zeros((vocab_len + 2, emb_dim))
    # Special embs
    special_embedding_matrix = np.zeros((special_tokens_len + 2, emb_dim))

    # Here we may apply pre-trained embeddings to embedding_matrix

    embedding_layer = Embedding(vocab_len + 2,
                        emb_dim,
                        mask_zero = True,
                        weights = [embedding_matrix],
                        input_length = MAX_SENT_LEN,
                        trainable = False)

    special_embedding_layer = Embedding(special_tokens_len + 2,
                            emb_dim,
                            mask_zero = True,
                            weights = [special_embedding_matrix],
                            input_length = MAX_SENT_LEN,
                            trainable = True)

    valid_words = vocab_len - special_tokens_len

    sentence_input = Input(shape=(MAX_SENT_LEN,), dtype='int32')

    # Create a vector of special tokens, e.g: [0,0,1,0,3,0,0]
    special_tokens_input = Lambda(lambda x: x - valid_words)(sentence_input)
    special_tokens_input = Activation('relu')(special_tokens_input)

    # Apply both 'normal' embeddings and special token embeddings
    embedded_sequences = embedding_layer(sentence_input)
    embedded_special = special_embedding_layer(special_tokens_input)

    # Add the matrices
    embedded_sequences = Add()([embedded_sequences, embedded_special])
</code></pre>
",10,13,3494,2018-02-27 13:02:46,https://stackoverflow.com/questions/49009386/train-only-some-word-embeddings-keras
What is &quot;unk&quot; in the pretrained GloVe vector files (e.g. glove.6B.50d.txt)?,"<p>I found ""unk"" token in the glove vector file glove.6B.50d.txt downloaded <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">from https://nlp.stanford.edu/projects/glove/</a>. Its value is as follows:</p>

<pre><code>unk -0.79149 0.86617 0.11998 0.00092287 0.2776 -0.49185 0.50195 0.00060792 -0.25845 0.17865 0.2535 0.76572 0.50664 0.4025 -0.0021388 -0.28397 -0.50324 0.30449 0.51779 0.01509 -0.35031 -1.1278 0.33253 -0.3525 0.041326 1.0863 0.03391 0.33564 0.49745 -0.070131 -1.2192 -0.48512 -0.038512 -0.13554 -0.1638 0.52321 -0.31318 -0.1655 0.11909 -0.15115 -0.15621 -0.62655 -0.62336 -0.4215 0.41873 -0.92472 1.1049 -0.29996 -0.0063003 0.3954
</code></pre>

<p>Is it a token to be used for unknown words or is it some kind of abbreviation?</p>
","neural-network, deep-learning, nlp, word-embedding, glove","<p><strong>The <code>unk</code> token in the pretrained GloVe files is not an unknown token!</strong></p>

<p>See this <a href=""https://groups.google.com/forum/#!searchin/globalvectors/unk|sort:date/globalvectors/9w8ZADXJclA/hRdn4prm-XUJ"" rel=""noreferrer"">google groups thread</a> where Jeffrey Pennington (GloVe author) writes:</p>

<blockquote>
  <p>The pre-trained vectors do not have an unknown token, and currently the code just ignores out-of-vocabulary words when producing the co-occurrence counts.</p>
</blockquote>

<p>It's an embedding learned like any other on occurrences of ""unk"" in the corpus (which appears to happen occasionally!)</p>

<p>Instead, Pennington suggests (in the same post):</p>

<blockquote>
  <p>...I've found that just taking an average of all or a subset of the word vectors produces a good unknown vector.</p>
</blockquote>

<p>You can do that with the following code (should work with any pretrained GloVe file):</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

GLOVE_FILE = 'glove.6B.50d.txt'

# Get number of vectors and hidden dim
with open(GLOVE_FILE, 'r') as f:
    for i, line in enumerate(f):
        pass
n_vec = i + 1
hidden_dim = len(line.split(' ')) - 1

vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)

with open(GLOVE_FILE, 'r') as f:
    for i, line in enumerate(f):
        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)

average_vec = np.mean(vecs, axis=0)
print(average_vec)
</code></pre>

<p>For <code>glove.6B.50d.txt</code> this gives:</p>

<pre><code>[-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011
  0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802
  0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912
  0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227
  0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107
 -0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544
 -0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303
 -0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408
  0.00653003  0.01685157]
</code></pre>

<p>And because it is fairly compute intensive to do this with the larger glove files, I went ahead and computed the vector for <code>glove.840B.300d.txt</code> for you:</p>

<pre><code>0.22418134 -0.28881392 0.13854356 0.00365387 -0.12870757 0.10243822 0.061626635 0.07318011 -0.061350107 -1.3477012 0.42037755 -0.063593924 -0.09683349 0.18086134 0.23704372 0.014126852 0.170096 -1.1491593 0.31497982 0.06622181 0.024687296 0.076693475 0.13851812 0.021302193 -0.06640582 -0.010336159 0.13523154 -0.042144544 -0.11938788 0.006948221 0.13333307 -0.18276379 0.052385733 0.008943111 -0.23957317 0.08500333 -0.006894406 0.0015864656 0.063391194 0.19177166 -0.13113557 -0.11295479 -0.14276934 0.03413971 -0.034278486 -0.051366422 0.18891625 -0.16673574 -0.057783455 0.036823478 0.08078679 0.022949161 0.033298038 0.011784158 0.05643189 -0.042776518 0.011959623 0.011552498 -0.0007971594 0.11300405 -0.031369694 -0.0061559738 -0.009043574 -0.415336 -0.18870236 0.13708843 0.005911723 -0.113035575 -0.030096142 -0.23908928 -0.05354085 -0.044904727 -0.20228513 0.0065645403 -0.09578946 -0.07391877 -0.06487607 0.111740574 -0.048649278 -0.16565254 -0.052037314 -0.078968436 0.13684988 0.0757494 -0.006275573 0.28693774 0.52017444 -0.0877165 -0.33010918 -0.1359622 0.114895485 -0.09744406 0.06269521 0.12118575 -0.08026362 0.35256687 -0.060017522 -0.04889904 -0.06828978 0.088740796 0.003964443 -0.0766291 0.1263925 0.07809314 -0.023164088 -0.5680669 -0.037892066 -0.1350967 -0.11351585 -0.111434504 -0.0905027 0.25174105 -0.14841858 0.034635577 -0.07334565 0.06320108 -0.038343467 -0.05413284 0.042197507 -0.090380974 -0.070528865 -0.009174437 0.009069661 0.1405178 0.02958134 -0.036431845 -0.08625681 0.042951006 0.08230793 0.0903314 -0.12279937 -0.013899368 0.048119213 0.08678239 -0.14450377 -0.04424887 0.018319942 0.015026873 -0.100526 0.06021201 0.74059093 -0.0016333034 -0.24960588 -0.023739101 0.016396184 0.11928964 0.13950661 -0.031624354 -0.01645025 0.14079992 -0.0002824564 -0.08052984 -0.0021310581 -0.025350995 0.086938225 0.14308536 0.17146006 -0.13943303 0.048792403 0.09274929 -0.053167373 0.031103406 0.012354865 0.21057427 0.32618305 0.18015954 -0.15881181 0.15322933 -0.22558987 -0.04200665 0.0084689725 0.038156632 0.15188617 0.13274793 0.113756925 -0.095273495 -0.049490947 -0.10265804 -0.27064866 -0.034567792 -0.018810693 -0.0010360252 0.10340131 0.13883452 0.21131058 -0.01981019 0.1833468 -0.10751636 -0.03128868 0.02518242 0.23232952 0.042052146 0.11731903 -0.15506615 0.0063580726 -0.15429358 0.1511722 0.12745973 0.2576985 -0.25486213 -0.0709463 0.17983761 0.054027 -0.09884228 -0.24595179 -0.093028545 -0.028203879 0.094398156 0.09233813 0.029291354 0.13110267 0.15682974 -0.016919162 0.23927948 -0.1343307 -0.22422817 0.14634751 -0.064993896 0.4703685 -0.027190214 0.06224946 -0.091360025 0.21490277 -0.19562101 -0.10032754 -0.09056772 -0.06203493 -0.18876675 -0.10963594 -0.27734384 0.12616494 -0.02217992 -0.16058226 -0.080475815 0.026953284 0.110732645 0.014894041 0.09416802 0.14299914 -0.1594008 -0.066080004 -0.007995227 -0.11668856 -0.13081996 -0.09237365 0.14741232 0.09180138 0.081735 0.3211204 -0.0036552632 -0.047030564 -0.02311798 0.048961394 0.08669574 -0.06766279 -0.50028914 -0.048515294 0.14144728 -0.032994404 -0.11954345 -0.14929578 -0.2388355 -0.019883996 -0.15917352 -0.052084364 0.2801028 -0.0029121689 -0.054581646 -0.47385484 0.17112483 -0.12066923 -0.042173345 0.1395337 0.26115036 0.012869649 0.009291686 -0.0026459037 -0.075331464 0.017840583 -0.26869613 -0.21820338 -0.17084768 -0.1022808 -0.055290595 0.13513643 0.12362477 -0.10980586 0.13980341 -0.20233242 0.08813751 0.3849736 -0.10653763 -0.06199595 0.028849555 0.03230154 0.023856193 0.069950655 0.19310954 -0.077677034 -0.144811
</code></pre>
",31,24,10773,2018-03-12 16:20:44,https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt
Python Tf idf algorithm,"<p>I would like to find the most relevant words over a set of documents.</p>

<p>I would like to call a Tf Idf algorithm over 3 documents and return a csv file containing each word and its frequency. </p>

<p>After that, I will take only the ones with a high number and I will use them.</p>

<p>I found this implementation that does what I need <a href=""https://github.com/mccurdyc/tf-idf/"" rel=""nofollow noreferrer"">https://github.com/mccurdyc/tf-idf/</a>. </p>

<p>I call that jar using the <code>subprocess</code> library. But there is a huge problem in that code: it commits a lot of mistake in analyzing words. It mixs some words, it has problems with <code>'</code> and <code>-</code> (I think). I am using it over the text of 3 books (Harry Potter) and , for example, I am obtaining words such <code>hermiones, hermionell, riddlehermione, thinghermione</code> instead of just <code>hermione</code> in the csv file.</p>

<p>Am I doing wrong something? Can you give me a working implementation of the Tf idf algorithm? Is there a python library that does that?</p>
","python, tf-idf, words, word-embedding","<p>Here is an implementation of the Tf-idf algorithm using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""noreferrer"">scikit-learn</a>.
Before applying it, you can <a href=""http://www.nltk.org/api/nltk.tokenize.html"" rel=""noreferrer""><code>word_tokenize()</code></a> and <a href=""http://www.nltk.org/api/nltk.stem.html"" rel=""noreferrer"">stem</a> your words.</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer

def tokenize(text):
    tokens = word_tokenize(text)
    stems = []
    for item in tokens: stems.append(PorterStemmer().stem(item))
    return stems

# your corpus
text = [""This is your first text book"", ""This is the third text for analysis"", ""This is another text""]
# word tokenize and stem
text = ["" "".join(tokenize(txt.lower())) for txt in text]
vectorizer = TfidfVectorizer()
matrix = vectorizer.fit_transform(text).todense()
# transform the matrix to a pandas df
matrix = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())
# sum over each document (axis=0)
top_words = matrix.sum(axis=0).sort_values(ascending=False)
</code></pre>
",7,2,7532,2018-03-14 12:34:21,https://stackoverflow.com/questions/49277926/python-tf-idf-algorithm
Word2Vec Python similarity,"<p>I made a word embedding with this code:</p>

<pre><code>with open(""text.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)
    model = Word2Vec(sentences,workers=4, window=5)
</code></pre>

<p>I want now to calculate the similarity between two word and see what are the neighbours of them.
What is the difference between <code>model[""word""]</code>,<code>model.wv.most_similar()</code>, <code>model.similar_by_vector()</code> and <code>model.similarity()</code>?
Which one should I use?</p>
","python, similarity, word2vec, gensim, word-embedding","<p>Edit: Maybe we should tag <code>gensim</code> here, because it is the library we are using</p>

<p>If you want to find the neighbours of both you can use
<code>model.wv.most_similar()</code> this will give you a dict (top n) for each word and its similarities for a given string (word). This method will calculate the cosine similarity between the word-vectors.</p>

<p>Note that the other methods you mentioned are deprecated in <code>3.4.0</code>, use <code>model.wv.similarity()</code> and <code>model.wv.similar_by_vector()</code> instead.</p>

<p>You can also use <code>model.wv.similar_by_vector()</code> to do the exact same thing but by passing a vector. Eg. <code>model[""woman""]</code> would give you such a vector. Actually if you look at the implementation, all the method does is call <code>most_similar()</code></p>

<pre><code>def similar_by_vector(self, vector, topn=10, restrict_vocab=None):
   return self.most_similar(positive=[vector], topn=topn, restrict_vocab=restrict_vocab)
</code></pre>

<p>Same goes for the <code>similar_by_word()</code> method. I actually don't know why these methods exist in the first place.</p>

<p>To find a similarity measure between exactly two words you can either use
<code>model.wv.similarity()</code> to find the cosine similarity or <code>model.wv.distance()</code> to find the cosine distance between the two.</p>

<p>To answer your actual question, I would simply compute the similarity between the two instead of comparing the results of <code>most_similar()</code>.</p>

<p>I hope this helps. Look at the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">docs</a> or the source files to get even more information, the code documentation is pretty good I think.</p>
",3,4,3371,2018-03-20 09:11:53,https://stackoverflow.com/questions/49380138/word2vec-python-similarity
Gensim Word2Vec most similar different result python,"<p>I have the first Harry Potter book in txt format. From this, I created two new txt files: in the first, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_1</code>; in the second, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_2</code>. Then I concatenated these 2 text to create one long text and I used this as input for Word2Vec.
This is my code:</p>

<pre><code>import os
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

with open(""HarryPotter1.txt"", 'r') as original, \
        open(""HarryPotter1_1.txt"", 'w') as mod1, \
        open(""HarryPotter1_2.txt"", 'w') as mod2:

    data=original.read()
    data_1 = data.replace(""Hermione"", 'Hermione_1')
    data_2 = data.replace(""Hermione"", 'Hermione_2')
    mod1.write(data_1 + r""\n"")
    mod2.write(data_2 + r""\n"")

with open(""longText.txt"",'w') as longFile:
    with open(""HarryPotter1_1.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)
    with open(""HarryPotter1_2.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)


model = """"
word_vectors = """"
modelName = ""ModelTest""
vectorName = ""WordVectorsTestst""

answer2 = raw_input(""Overwrite  embeddig? (yes or n)"")
if(answer2 == 'yes'):
    with open(""longText.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)

    model = Word2Vec(sentences,workers=4, window=5,min_count=5)

    model.save(modelName)
    model.wv.save_word2vec_format(vectorName+"".bin"",binary=True)
    model.wv.save_word2vec_format(vectorName+"".txt"", binary=False)
    model.wv.save(vectorName)

    word_vectors = model.wv

else:
    model = Word2Vec.load(modelName)
    word_vectors = KeyedVectors.load_word2vec_format(vectorName + "".bin"", binary=True)

    print(model.wv.similarity(""Hermione_1"",""Hermione_2""))
    print(model.wv.distance(""Hermione_1"",""Hermione_2""))
    print(model.wv.most_similar(""Hermione_1""))
    print(model.wv.most_similar(""Hermione_2""))
</code></pre>

<p>How is possible that <code>model.wv.most_similar(""Hermione_1"")</code> and <code>model.wv.most_similar(""Hermione_2"")</code> give me different output? 
Their neighbour are completely different. This is the output of the four print:</p>

<pre><code>0.00799602753634
0.992003972464
[('moments,', 0.3204237222671509), ('rose;', 0.3189219534397125), ('Peering', 0.3185565173625946), ('Express,', 0.31800806522369385), ('no...', 0.31678506731987), ('pushing', 0.3131707012653351), ('triumph,', 0.3116190731525421), ('no', 0.29974159598350525), ('them?""', 0.2927379012107849), ('first.', 0.29270970821380615)]
[('go?', 0.45812922716140747), ('magical', 0.35565727949142456), ('Spells.""', 0.3554503619670868), ('Scabbets', 0.34701400995254517), ('cupboard.""', 0.33982667326927185), ('dreadlocks', 0.3325180113315582), ('sickening', 0.32789379358291626), ('First,', 0.3245708644390106), ('met', 0.3223033547401428), ('built', 0.3218075931072235)]
</code></pre>
","python, string, word2vec, gensim, word-embedding","<p>Training word2Vec models is random to an extent. That is why you may get different results. Also, <code>Hermione_2</code> starts appearing in the second half of the text data. In my understanding over the course of processing the data when the <code>Hermione_1</code> context is already established and so is the vector for this word you introduce a second word in exactly the same context and the algorithm tries to find what differentiates the two.
Secondly, you use a very short vector which may under-represent the complexity of the conceptual space. Due to the simplifications you get two vectors without any overlap.</p>
",1,0,966,2018-03-21 09:08:39,https://stackoverflow.com/questions/49402113/gensim-word2vec-most-similar-different-result-python
How can I load Word2vec with Gensim without getting an AttributeError?,"<p>I am new to Gensim, and I am trying to load my given (pre-trained) Word2vec model. I have 2 files: <em>xxxx.model.wv</em> and a bigger one <em>xxxx.model.wv.syn0.npy</em>.</p>

<p>When I call the following line:</p>

<pre><code>gensim.models.Word2Vec.load('xxxx.model.wv')
</code></pre>

<p>I get the following error:</p>

<pre><code>AttributeError: 'EuclideanKeyedVectors' object has no attribute 'negative'
</code></pre>

<p>How can I solve this error?</p>
","python, word2vec, gensim, word-embedding","<p>Are you sure your <code>xxxx.model.wv</code> file was a saved full <code>Word2Vec</code> model object? </p>

<p>That error suggests it was instead a <code>EuclideanKeyedVectors</code> – just the vectors, and not a full model with all properties like <code>negative</code> – so you might need to load it as that instead. </p>
",1,2,1204,2018-03-22 13:29:27,https://stackoverflow.com/questions/49429971/how-can-i-load-word2vec-with-gensim-without-getting-an-attributeerror
How to use Keras Embedding layer when there are more than 1 text features,"<p>I understand how to use the Keras Embedding layer in case there is a single text feature like in IMDB review classification. However, I am confused how to use the Embedding Layers when I have a Classification problem, where there are more than a single text feature. For example, I have a dataset with 2 text features Diagnosis Text, and Requested Procedure and the label is binary class (1 for approved, 0 for not approved). In the example below, x_train has 2 columns Diagnosis and Procedure, unlike the IMDB dataset. Do I need to create 2 Embedding layers, one for Diagnosis, and Procedure? If so, what code changes would be required? </p>



<pre class=""lang-python prettyprint-override""><code>x_train = preprocessing.sequences.pad_sequences(x_train, maxlen=20)
x_test = preprocessing.sequences.pad_sequences(x_test, maxlen=20)
model = Sequential()
model.add(Embedding(10000,8,input_length=20)
model.add(Flatten())
model.add(Dense(1, activation='sigmoid')
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
</code></pre>
","keras, word2vec, word-embedding","<p>You have some choices, you could concatenate the the two features into one
and create a single embedding for both of them. Here is the logic</p>

<pre class=""lang-python prettyprint-override""><code>all_features = np.hstack(X['diag'] + X['proc'])
X = pad_sequence(all_features, max_len)
# build model as usual, as you can see on a single embedding layer is
# needed.
</code></pre>
<p>or you can use the <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">Functional api</a> and build multiple input model</p>
<pre class=""lang-python prettyprint-override""><code>diag_inp = Input()
diag_emb = Embedding(512)(diag_input)
proc_inp = Input()
proc_emb = Embedding(512)(proc_input)

# concatenate them to makes a single vector per sample
merged = Concatenate()[diag_emb, proc_emb]
out = Dense(2,  activation='sigmoid')(merged)
model = Model(inputs=[diag_inp, proc_inp], outputs=[out])
</code></pre>
<p>That is you can learn an embedding for the concatenation or you can learn
multiple embeddings and concatenate them while training.</p>
",6,9,4311,2018-04-02 05:28:48,https://stackoverflow.com/questions/49605800/how-to-use-keras-embedding-layer-when-there-are-more-than-1-text-features
PyTorch / Gensim - How do I load pre-trained word embeddings?,"<p>I want to load a pre-trained word2vec embedding with gensim into a PyTorch embedding layer.</p>
<p>How do I get the embedding weights loaded by gensim into the PyTorch embedding layer?</p>
","python, pytorch, neural-network, gensim, word-embedding","<p>I just wanted to report my findings about loading a gensim embedding with PyTorch.</p>

<hr>

<ul>
<li><h2>Solution for PyTorch <code>0.4.0</code> and newer:</h2></li>
</ul>

<p>From <code>v0.4.0</code> there is a new function <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained"" rel=""noreferrer""><code>from_pretrained()</code></a> which makes loading an embedding very comfortable.
Here is an example from the documentation.</p>

<pre><code>import torch
import torch.nn as nn

# FloatTensor containing pretrained weights
weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
embedding = nn.Embedding.from_pretrained(weight)
# Get embeddings for index 1
input = torch.LongTensor([1])
embedding(input)
</code></pre>

<p>The weights from <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer""><em>gensim</em></a> can easily be obtained by:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')
weights = torch.FloatTensor(model.vectors) # formerly syn0, which is soon deprecated
</code></pre>

<p>As noted by @Guglie: in newer gensim versions the weights can be obtained by <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer""><code>model.wv</code></a>:</p>

<pre><code>weights = model.wv
</code></pre>

<hr>

<ul>
<li><h2>Solution for PyTorch version <code>0.3.1</code> and older:</h2></li>
</ul>

<p>I'm using version <code>0.3.1</code> and <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained"" rel=""noreferrer""><code>from_pretrained()</code></a> isn't available in this version.</p>

<p>Therefore I created my own <code>from_pretrained</code> so I can also use it with <code>0.3.1</code>.</p>

<p><em>Code for <code>from_pretrained</code> for PyTorch versions <code>0.3.1</code> or lower:</em></p>

<pre><code>def from_pretrained(embeddings, freeze=True):
    assert embeddings.dim() == 2, \
         'Embeddings parameter is expected to be 2-dimensional'
    rows, cols = embeddings.shape
    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)
    embedding.weight = torch.nn.Parameter(embeddings)
    embedding.weight.requires_grad = not freeze
    return embedding
</code></pre>

<p>The embedding can be loaded then just like this:</p>

<pre><code>embedding = from_pretrained(weights)
</code></pre>

<p>I hope this is helpful for someone.</p>
",85,52,61685,2018-04-07 18:21:06,https://stackoverflow.com/questions/49710537/pytorch-gensim-how-do-i-load-pre-trained-word-embeddings
How to measure the word weight using doc2vec vector,"<p>I'm using the word2vec algorithm to detect the most important words in a document, my question is about how to compute the weight of an important word using the vector obtained from doc2vec, my code is like that:</p>

<pre><code>model = Doc2Vec.load(fname)
word=[""suddenly""]
vectors=model.infer_vector(word)
</code></pre>

<p>thank you for your consideration.</p>
","python, algorithm, word-embedding, doc2vec","<p>Let's say you can find the vector <code>R</code> corresponding to an entire document, using doc2vec. Let's also assume that using word2vec, you can find the vector <code>v</code> corresponding to any word <code>w</code> as well. And finally, let's assume that <code>R</code> and <code>v</code> are in the <em>same</em> N-dimensional space.</p>

<p>Assuming all this, you may then utilize plain old vector arithmetic to find out some correlations between <code>R</code> and <code>v</code>.</p>

<p>For starters, you can normalize <code>v</code>. Normalizing, after all, is just dividing each dimension with the magnitude of <code>v</code>. (i.e. <code>|v|</code>) Let's call the normalized version of <code>v</code> as <code>v_normal</code>.</p>

<p>Then, you may project <code>v_normal</code> onto the line represented by the vector <code>R</code>. That projection operation is just finding the dot product of <code>v_normal</code> and <code>R</code>, right? Let's call that scalar result of the dot product as <code>len_projection</code>. Well, you can consider <code>len_projection / |v_normal|</code> as an indication of how <em>parallel</em> the context of the word is to that of the overall document. In fact, considering just <code>len_projection</code> is enough, because in this case, as <code>v_normal</code> is normalized, <code>|v_normal| == 1</code>.</p>

<p>Now, you may apply this procedure to all words in the document, and consider the words that lead to greatest <code>len_projection</code> values as the most significant words of that document.</p>

<p>Note that this method may end up finding frequently-used words like <em>""I""</em>, or <em>""and""</em> as the most important words in a document, as such words appear in many different contexts. If that is an issue you would want to remedy, you may want do a post-processing step to filter such common words.</p>

<p>I sort of thought of this method on the spot here, and I am not sure whether this approach has any scientific backing. But, it may make sense, if you think about how most vector embeddings for words work. Word vectors are usually trained to represent the <em>context</em> in which a word is used. Thinking in terms of vector arithmetic, projecting the vector onto a line may reveal how parallel the context of that word <code>w</code> is, to the overall context represented by that line.</p>

<p>Last but not least, as I have worked only with word2vec before, I am not sure if doc2vec and word2vec data can be used adjointly like I mentioned above. As I stated in the first paragraph of my answer, it is really critical that <code>R</code> and <code>v</code> has to be in the <em>same</em> N-dimensional space.</p>
",0,0,971,2018-04-08 11:41:32,https://stackoverflow.com/questions/49717440/how-to-measure-the-word-weight-using-doc2vec-vector
How to generate GloVe embeddings for POS tags? Python,"<p>For a sentence analysis task, I would like to take the sequence of POS tags associated with the sentence and feed it to my model as if the POS tags are words. </p>

<p>I am using GloVe to make representations of each word in the sentence and SpaCy to generate POS tags. However, GloVe embeddings do not make much sense for POS tags. So I will have to somehow create embeddings for each POS tag. What is the best way to do create embeddings for POS tags, so that I can feed POS sequences into my model in the same way I would feed sentences? Could anyone point to code examples of how to do this with GloVe in Python?</p>

<p><strong>Added context</strong></p>

<p>My task is a binary classification of sentence pairs, based on their resemblance (similar meaning vs different meaning). </p>

<p>I would like to use POS tags as words, so that the POS tags serve as an additional bit of information to compare the sentences. My current model does not use an LSTM as a way to predict sequences. </p>
","python-3.x, machine-learning, nlp, spacy, word-embedding","<p>Most word embedding models still rely on an underlying assumption that the meaning of a word is induced by its usage context. For example, learning a word2vec embedding with skipgram or continuous bag of words formulations implicitly assumes a model in which the representation vector of the word is based on the context words that co-occur with the target word, specifically by learning to create embeddings that best solve the classification task of distinguishing pairs of words that contextually co-occur from random pairs of words (so-called negative sampling).</p>

<p>But if the input is changed to be a sequence of discrete labels (POS tags), this assumption doesn't seem like it needs to remain accurate or reasonable. Part of speech labels have an assigned meaning that is not really induced by the context of being surrounded by other part of speech labels, so it's unlikely that standard learning tasks which are used to produce word embeddings would work when treating POS labels as if they were words from a much smaller vocabulary.</p>

<p>What is the overall sentence analysis task in your situation? </p>

<p><em>Added after question was updated with the learning task at hand.</em></p>

<p>Let's assume you can create POS input vectors for each sentence example. If there are N different POS labels possible, it means your input will consist of one vector from word embeddings and another vector of length N, where the value in component <code>i</code> represents the number of terms in the input sentence that possess POS label <code>P_i</code>.</p>

<p>For example, let's pretend the only POS labels possible are 'article', 'noun' and 'verb', and you have a sentence with ['article', 'noun', 'verb', 'noun']. Then this transforms into <code>[1, 2, 1]</code>, and probably you want to normalize it by the length of the sentence. Let's call this input <code>pos1</code> for sentence number 1 and <code>pos2</code> for sentence number 2.</p>

<p>Let's call the word embedding vector input for sentence 1 as <code>sentence1</code>. <code>sentence1</code> will be calculated by looking up each word embedding from a separate source, like a pretrained word2vec model or fastText or GloVe, and summing them up (using continuous bag of words). And the same for <code>sentence2</code>.</p>

<p>It's assumed that your batches of training data would already be processed into these vector formats, so a given single input would be a 4-tuple of vectors: the looked up CBOW embedding vector for sentence 1, same for sentence 2, and the calculated discrete representation vector for POS labels of sentence 1, and same for sentence 2.</p>

<p>A model that could work from this data might be like this:</p>

<pre><code>from keras.engine.topology import Input
from keras.layers import Concatenate
from keras.layers.core import Activation, Dense
from keras.models import Model


sentence1 = Input(shape=word_embedding_shape)
sentence2 = Input(shape=word_embedding_shape)
pos1 = Input(shape=pos_vector_shape)
pos2 = Input(shape=pos_vector_shape)

# Note: just choosing 128 as an embedding space dimension or intermediate
# layer size... in your real case, you'd choose these shape params
# based on what you want to model or experiment with. They don't mean
# anything here.

sentence1_branch = Dense(128)(sentence1)
sentence1_branch = Activation('relu')(sentence1_branch)
# ... do whatever other sentence1-only stuff

sentence2_branch = Dense(128)(sentence2)
sentence2_branch = Activation('relu')(sentence2_branch)
# ... do whatever other sentence2-only stuff

pos1_embedding = Dense(128)(pos1)
pos1_branch = Activation('relu')(pos1_embedding)
# ... do whatever other pos1-only stuff

pos2_embedding = Dense(128)(pos2)
pos2_branch = Activation('relu')(pos2_embedding)
# ... do whatever other pos2-only stuff


unified = Concatenate([sentence1_branch, sentence2_branch,
                       pos1_branch, pos2_branch])
# ... do dense layers, whatever, to the concatenated intermediate 
# representations

# finally boil it down to whatever final prediction task you are using, 
# whether it is predicting a sentence similarity score (Dense(1)), 
# or predicting a binary label that indicates whether the sentence 
# pairs are similar or not (Dense(2) then followed by softmax activation, 
# or Dense(1) followed by some type of probability activation like sigmoid).

# Assume your data is binary labeled for similar sentences...
unified = Activation('softmax')(Dense(2)(unified))
unified.compile(loss='binary_crossentropy', other parameters)

# Do training to learn the weights...


# A separate model that will just produce the embedding output
# from a POS input vector, relying on weights learned from the
# training process.
pos_embedding_model = Model(inputs=[pos1], outputs=[pos1_embedding])
</code></pre>
",2,5,2921,2018-04-12 19:44:48,https://stackoverflow.com/questions/49804717/how-to-generate-glove-embeddings-for-pos-tags-python
Fasttext algorithm use only word and subword? or sentences too?,"<p>I read the paper and googled as well if there is any good example of the learning method(or more likely learning procedure)</p>

<p>For word2vec, suppose there is corpus sentence</p>

<blockquote>
  <p>I go to school with lunch box that my mother wrapped every morning</p>
</blockquote>

<p>Then with window size 2, it will try to obtain the vector for 'school' by using surrounding words </p>

<blockquote>
  <p>['go', 'to', 'with', 'lunch']</p>
</blockquote>

<p>Now, FastText says that it uses the subword to obtain the vector, so it is definitely use n gram subword, for example with n=3,</p>

<blockquote>
  <p>['sc', 'sch', 'cho', 'hoo', 'ool', 'school']</p>
</blockquote>

<p>Up to here, I understood.
But it is not clear that if the other words are being used for learning for 'school'. I can only guess that other surrounding words are used as well like the word2vec, since the paper mentions</p>

<p>=> the terms <em>Wc</em> and <em>Wt</em> are both used in functions</p>

<p>where Wc is context word and Wt is word at sequence t.</p>

<p>However, it is not clear that how FastText learns the vectors for word.</p>

<p>.</p>

<p>.</p>

<p>Please clearly explain how FastText learning process goes in procedure?</p>

<p>.</p>

<p>.</p>

<p>More precisely I want to know that if FastText also follows the same procedure as Word2Vec while it learns the n-gram characterized subword <strong><em>in addition</em></strong>. Or only n-gram characterized subword with word being used?</p>

<p>How does it vectorize the subword at initial? etc</p>
","nlp, vectorization, word2vec, word-embedding, fasttext","<p>Any context word has its candidate input vector assembled from the combination of both its full-word token and all its character-n-grams. So if the context word is 'school', and you're using 3-4 character n-grams, the in-training input vector is a combination of the full-word vector for <code>school</code>, <em>and</em> all the n-gram vectors for <code>['sch', 'cho', 'hoo', 'ool', 'scho', 'choo', 'hool']</code>.)</p>

<p>When that candidate vector is adjusted by training, <em>all</em> the constituent vectors are adjusted. (This is a little like how in word2vec CBOW, mode, all the <em>words</em> of the single average context input vector get adjusted together, when their ability to predict a single target output word is evaluated and improved.) </p>

<p>As a result, those n-grams that happen to be meaningful hints across many similar words – for example, common word-roots or prefixes/suffixes – get positioned where they confer that meaning. (Other n-grams may remain mostly low-magnitude noise, because there's little meaningful pattern to where they appear.)</p>

<p>After training, reported vectors for individual in-vocabulary words are also constructed by combining the full-word vector and all n-grams. </p>

<p>Then, when you also encounter an out-of-vocabulary word, to the extent it shares some or many n-grams with morphologically-similar in-training words, it will get a similar calculated vector – and thus be better than nothing, in guessing what that word's vector should be. (And in the case of small typos or slight variants of known words, the synthesized vector may be pretty good.)</p>
",10,8,4426,2018-04-13 07:22:50,https://stackoverflow.com/questions/49811479/fasttext-algorithm-use-only-word-and-subword-or-sentences-too
"Why does Tensorflow&#39;s sampled_softmax_loss force you to use a bias, when experts recommend no bias be used for Word2Vec?","<p>All the tensorflow implementations of Word2Vec that I have seen has a bias in the negative sampling softmax function, including on the official tensorflow website</p>

<p><a href=""https://www.tensorflow.org/tutorials/word2vec#vector-representations-of-words"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/word2vec#vector-representations-of-words</a></p>

<pre><code>loss = tf.reduce_mean(
  tf.nn.nce_loss(weights=nce_weights,
                 biases=nce_biases,
                 labels=train_labels,
                 inputs=embed,
                 num_sampled=num_sampled,
                 num_classes=vocabulary_size))
</code></pre>

<p>This is from Google's free Deep Learning course <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb</a></p>

<pre><code> loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,
                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))
</code></pre>

<p>However, from both Andrew Ng and Richard Socher's lectures, they do not include a bias in their negative sampling softmaxes. </p>

<p>Even where this idea originated, Mikolov states that:</p>

<blockquote>
  <p>biases are not used in the neural network, as no significant
  improvement of performance was observed - following the Occam's razor,
  the solution is as simple as it needs to be.</p>
</blockquote>

<p>Mikolov, T.: Statistical Language Models Based on Neural Networks, p. 29
<a href=""http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf"" rel=""nofollow noreferrer"">http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf</a></p>

<p>So why do the official tensorflow implementations have a bias, and why does there not seem to be an option to not include a bias in the sampled_softmax_loss function ?</p>
","tensorflow, deep-learning, word2vec, word-embedding","<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""nofollow noreferrer"">exercise you link</a> defines <code>softmax_biases</code> to be zeros:</p>

<pre><code>softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))
</code></pre>

<p>That is: they're not using any actual bias in their word2vec example.</p>

<p>The <code>sampled_softmax_loss()</code> function is generic and used for many neural-networks; its decision to require a <code>biases</code> argument is unrelated to what's best for one particular neural-network application (word2vec), and accommodates the word2vec case by allowing (as here) all zeros.</p>
",1,0,477,2018-04-24 20:36:26,https://stackoverflow.com/questions/50010376/why-does-tensorflows-sampled-softmax-loss-force-you-to-use-a-bias-when-experts
How to use GloVe word-embeddings file on Google colaboratory,"<p>I have downloaded the data with wget</p>

<pre><code>!wget http://nlp.stanford.edu/data/glove.6B.zip
 - ‘glove.6B.zip’ saved [862182613/862182613]
</code></pre>

<p>It is saved as zip and I would like to use glove.6B.300d.txt file from the zip file. What I want to achieve is : </p>

<pre><code>embeddings_index = {}
with io.open('glove.6B.300d.txt', encoding='utf8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:],dtype='float32')
        embeddings_index[word] = coefs
</code></pre>

<p>Of course I am having this error:</p>

<pre><code>IOErrorTraceback (most recent call last)
&lt;ipython-input-47-d07cafc85c1c&gt; in &lt;module&gt;()
      1 embeddings_index = {}
----&gt; 2 with io.open('glove.6B.300d.txt', encoding='utf8') as f:
      3     for line in f:
      4         values = line.split()
      5         word = values[0]

IOError: [Errno 2] No such file or directory: 'glove.6B.300d.txt'
</code></pre>

<p>How can I unzip and use that file in my code above on Google colab?</p>
","python, google-colaboratory, word-embedding","<p>Its simple, checkout this <a href=""https://stackoverflow.com/questions/3451111/unzipping-files-in-python"">older post</a> from SO.</p>

<pre><code>import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
</code></pre>
",4,8,37717,2018-04-27 10:16:30,https://stackoverflow.com/questions/50060241/how-to-use-glove-word-embeddings-file-on-google-colaboratory
semantic and syntactic performance of Doc2vec model,"<p>I am trying to check the semantic and syntactic performance of a doc2vec model- <code>doc2vec_model.accuracy(questions-words)</code>, but it doesnt seem to function since <a href=""https://radimrehurek.com/gensim/models/deprecated/doc2vec.html"" rel=""nofollow noreferrer"">models.deprecated.doc2vec – Deep learning with paragraph2vec</a>, says it has been deprecated since version 3.3.0 in the gensim package.It gives this error message </p>

<pre><code>AttributeError: 'Doc2Vec' object has no attribute 'accuracy'
</code></pre>

<p>Though it works with word2vec model well, is there any way I can get it done apart from <code>doc2vec_model.accuracy(questions-words)</code>? or it's impossible?</p>
","python-3.x, word-embedding, doc2vec","<p>A few notes: </p>

<p>That 'accuracy()' test is <em>only</em> a test of word-vectors on analogy problems – an easy evaluation to run, used in a number of papers, but <em>not</em> the final authority on whether a set of word-vectors is better than others for a particular purpose. (When I've had a project-specific scoring method, sometimes the word-vectors that score best on project-specific goals don't score best on those analogies – especially if the word-vectors are being used for a classification or information-retrieval task.)</p>

<p>Further, the popular and fast PV-DBOW <code>Doc2Vec</code> mode (<code>dm=0</code> in gensim) doesn't train word-vectors at all, unless you add another setting (<code>dbow_words=1</code>). Such untrained word-vectors will be in random locations, scoring awfully on the analogies-accuracy. </p>

<p>But, using either PV-DM (<code>dm=1</code>) mode, or adding <code>dbow_words=1</code> to PV-DBOW, will get word-vectors from <code>Doc2Vec</code>, and you might still want to run the analogies test. Fortunately, analogy-evaluation options have been retained &amp; even expanded on the <code>KeyedVectors</code> object that's held in the <code>Doc2Vec</code> <code>wv</code> property. You can call the old <code>accuracy()</code> method there:</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.accuracy"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.accuracy</a></p>

<p>But there's also a slightly-different scoring <code>evaluate_word_pairs()</code>:</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs</a></p>

<p>(And in the 4.0.0 release there'll be a <code>[evaluate_word_analogies()][1]</code> which replaces `accuracy().)</p>
",1,0,366,2018-04-28 11:39:03,https://stackoverflow.com/questions/50076131/semantic-and-syntactic-performance-of-doc2vec-model
Why cant I build this maven as an executable,"<p>The maven project for <a href=""http://data.dws.informatik.uni-mannheim.de/rdf2vec/code/RDF2Walks.zip"" rel=""nofollow noreferrer"">rdf2vec</a> builds an executable without any errors however the jar file throws ""no main manifest attribute"" error. </p>

<p>My attempt to resolve it:</p>

<p>Replace the following snippet :</p>

<p><code>&lt;manifest&gt;
    &lt;mainClass&gt;fully.qualified.MainClass&lt;/mainClass&gt;
&lt;/manifest&gt;</code></p>

<p>with :</p>

<p><code>&lt;manifest&gt;
    &lt;addClasspath&gt;true&lt;/addClasspath&gt;
    &lt;mainClass&gt;walks.WalkGenerator&lt;/mainClass&gt;
&lt;/manifest&gt;</code></p>

<p>Thereby making WalkGenerator as the default main class for the entire package. 
This sadly, does not do the trick.</p>

<p>Another issue is that I cant find the MANIFEST file. </p>
","maven, rdf, semantic-web, word-embedding","<p>You need to use <code>maven-jar-plugin</code> to set main class for you jar</p>

<pre><code>&lt;plugin&gt;
  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
  &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
  &lt;configuration&gt;
    &lt;archive&gt;
      &lt;manifest&gt;                              
        &lt;mainClass&gt;walks.WalkGenerator&lt;/mainClass&gt;
      &lt;/manifest&gt;
    &lt;/archive&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>
",0,1,63,2018-04-28 16:36:48,https://stackoverflow.com/questions/50078768/why-cant-i-build-this-maven-as-an-executable
torch.nn.embedding has run time error,"<p>I want to use <code>torch.nn.Embedding</code>. I have followed the codes in the documentation of embedding command.
here is the code:</p>

<pre><code># an Embedding module containing 10 tensors of size 3
embedding = nn.Embedding(10, 3)
# a batch of 2 samples of 4 indices each
input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
embedding(input)
</code></pre>

<p>The documentation says that you will receive this output:</p>

<pre><code>tensor([[[-0.0251, -1.6902,  0.7172],
         [-0.6431,  0.0748,  0.6969],
         [ 1.4970,  1.3448, -0.9685],
         [-0.3677, -2.7265, -0.1685]],

        [[ 1.4970,  1.3448, -0.9685],
         [ 0.4362, -0.4004,  0.9400],
         [-0.6431,  0.0748,  0.6969],
         [ 0.9124, -2.3616,  1.1151]]])
</code></pre>

<p>but I don't receive this output. instead I receive this error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/mahsa/PycharmProjects/PyTorch_env_project/PyTorchZeroToAll-master/temporary.py"", line 12, in &lt;module&gt;
    embedding(input)
  File ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/site-packages/torch/nn/modules/sparse.py"", line 94, in forward
    self.scale_grad_by_freq, self.sparse
RuntimeError: save_for_backward can only save input or output tensors, but argument 0 doesn't satisfy this condition
</code></pre>

<p>Can anybody guide me about this error? and about the work of <code>torch.nn.Embedding</code>?</p>
","python, pytorch, word-embedding","<p>if we change this line:</p>

<pre><code>input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
</code></pre>

<p>with this:</p>

<pre><code>input = autograd.Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))
</code></pre>

<p>the problem is solved!</p>
",0,0,386,2018-05-06 05:43:29,https://stackoverflow.com/questions/50196608/torch-nn-embedding-has-run-time-error
Keras image captioning model not compiling because of concatenate layer when mask_zero=True in a previous layer,"<p>I am new to Keras and I am trying to implement a model for an image captioning project. </p>

<p>I am trying to reproduce the model from <a href=""https://i.sstatic.net/O8YhD.jpg"" rel=""nofollow noreferrer"">Image captioning pre-inject architecture</a> (The picture is taken from this paper: <a href=""https://arxiv.org/pdf/1703.09137.pdf"" rel=""nofollow noreferrer"">Where to put the image in an image captioning generator</a>) (but with a minor difference: generating a word at each time step instead of only generating a single word at the end), in which the inputs for the LSTM at the first time step are the embedded CNN features. The LSTM should support variable input length and in order to do this I padded all the sequences with zeros so that all of them have maxlen time steps.</p>

<p>The code for the model I have right now is the following:</p>



<pre class=""lang-python prettyprint-override""><code>def get_model(model_name, batch_size, maxlen, voc_size, embed_size, 
        cnn_feats_size, dropout_rate):

    # create input layer for the cnn features
    cnn_feats_input = Input(shape=(cnn_feats_size,))

    # normalize CNN features 
    normalized_cnn_feats = BatchNormalization(axis=-1)(cnn_feats_input)

    # embed CNN features to have same dimension with word embeddings
    embedded_cnn_feats = Dense(embed_size)(normalized_cnn_feats)

    # add time dimension so that this layer output shape is (None, 1, embed_size)
    final_cnn_feats = RepeatVector(1)(embedded_cnn_feats)

    # create input layer for the captions (each caption has max maxlen words)
    caption_input = Input(shape=(maxlen,))

    # embed the captions
    embedded_caption = Embedding(input_dim=voc_size,
                                 output_dim=embed_size,
                                 input_length=maxlen)(caption_input)

    # concatenate CNN features and the captions.
    # Ouput shape should be (None, maxlen + 1, embed_size)
    img_caption_concat = concatenate([final_cnn_feats, embedded_caption], axis=1)

    # now feed the concatenation into a LSTM layer (many-to-many)
    lstm_layer = LSTM(units=embed_size,
                      input_shape=(maxlen + 1, embed_size),   # one additional time step for the image features
                      return_sequences=True,
                      dropout=dropout_rate)(img_caption_concat)

    # create a fully connected layer to make the predictions
    pred_layer = TimeDistributed(Dense(units=voc_size))(lstm_layer)

    # build the model with CNN features and captions as input and 
    # predictions output
    model = Model(inputs=[cnn_feats_input, caption_input], 
                  outputs=pred_layer)

    optimizer = Adam(lr=0.0001, 
                     beta_1=0.9, 
                     beta_2=0.999, 
                     epsilon=1e-8)

    model.compile(loss='categorical_crossentropy',optimizer=optimizer)
    model.summary()

    return model
</code></pre>

<p>The model (as it is above) compiles without any errors (see: <a href=""https://i.sstatic.net/hSvA7.jpg"" rel=""nofollow noreferrer"">model summary</a>) and I managed to train it using my data. However, it doesn't take into account the fact that my sequences are zero-padded and the results won't be accurate because of this. When I try to change the Embedding layer in order to support masking (also making sure that I use voc_size + 1 instead of voc_size, as it's mentioned in the documentation) like this:</p>

<pre class=""lang-python prettyprint-override""><code>embedded_caption = Embedding(input_dim=voc_size + 1,
                             output_dim=embed_size,
                             input_length=maxlen, mask_zero=True)(caption_input)
</code></pre>

<p>I get the following error:</p>

<pre class=""lang-python prettyprint-override""><code>Traceback (most recent call last):
  File ""/export/home/.../py3_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1567, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 200 and 1. Shapes are [200] and [1]. for 'concatenate_1/concat_1' (op: 'ConcatV2') with input shapes: [?,1,200], [?,25,1], [] and with computed input tensors: input[2] = &lt;1&gt;
</code></pre>

<p>I don't know why it says the shape of the second array is [?, 25, 1], as I am printing its shape before the concatenation and it's [?, 25, 200] (as it should be). 
I don't understand why there'd be an issue with a model that compiles and works fine without that parameter, but I assume there's something I am missing.</p>

<p>I have also been thinking about using a Masking layer instead of mask_zero=True, but it should be before the Embedding and the documentation says that the Embedding layer should be the first layer in a model (after the input). </p>

<p>Is there anything I could change in order to fix this or is there a workaround to this ?</p>
","tensorflow, keras, deep-learning, lstm, word-embedding","<p>The non-equal shape error refers to the mask rather than the tensors/inputs. With <code>concatenate</code> supporting masking, it need to <a href=""https://github.com/keras-team/keras/blob/2.1.5/keras/engine/topology.py#L605-L620"" rel=""nofollow noreferrer"">handle mask propagation</a>. Your <code>final_cnn_feats</code> doesn't have mask (<code>None</code>), while your <code>embedded_caption</code> has a mask of shape <code>(?, 25)</code>. You can find this out by doing:</p>

<pre><code>print(embedded_caption._keras_history[0].compute_mask(caption_input))
</code></pre>

<p>Since <code>final_cnn_feats</code> has no mask, <code>concatenate</code> will <a href=""https://github.com/keras-team/keras/blob/2.1.5/keras/layers/merge.py#L389-L391"" rel=""nofollow noreferrer"">give it a all non-zero mask</a> for proper mask propagation. While this is correct, the shape of the mask, however, has the same shape as <code>final_cnn_feats</code> which is <code>(?, 1, 200)</code> rather than <code>(?, 1)</code>, i.e. masking all features at all time step rather than just all time step. This is where the non-equal shape error comes from (<code>(?, 1, 200)</code> vs <code>(?, 25)</code>).</p>

<p>To fix it, you need to give <code>final_cnn_feats</code> a correct/matching mask. Now I'm not familiar with your project here. One option is to apply a <code>Masking</code> layer to <code>final_cnn_feats</code>, since it is designed to <a href=""https://keras.io/layers/core/#masking"" rel=""nofollow noreferrer"">mask timestep(s)</a>.</p>

<pre><code>final_cnn_feats = Masking()(RepeatVector(1)(embedded_cnn_feats))
</code></pre>

<p>This can be correct only when not all 200 features in <code>final_cnn_feats</code> are zero, i.e. there is always at least one non-zero value in <code>final_cnn_feats</code>. With that condition, <code>Masking</code> layer will give a <code>(?, 1)</code> mask and will not mask the single time step in <code>final_cnn_feats</code>.</p>
",2,2,309,2018-05-10 15:05:53,https://stackoverflow.com/questions/50276020/keras-image-captioning-model-not-compiling-because-of-concatenate-layer-when-mas
Gensim Doc2Vec: I&#39;m gettting different vectors from documents that are identical,"<p>I have the following code and I think I am getting the vectors in a wrong way, because for example the vectors of two documents that are 100% identical are not the same.</p>

<pre><code>def getDocs(corpusPath):
    """"""Function for processings documents as TaggedDocument""""""
    # Loop over all the files in corpus
    for file in glob.glob(os.path.join(corpusPath, '*.csv')):
        # getWords is a function that gets the words from the provided directory
        # os.path.basename(file) takes the filename from the complete path
        yield TaggedDocument(words=getWords(file), tags=[os.path.basename(file)])

def getModel(corpusPath, outputName):
    # Get documents words from path
    documents = getDocs(corpusPath)

    cores = multiprocessing.cpu_count()

    # Initialize the model
    model = models.doc2vec.Doc2Vec(vector_size=100, epochs=10, min_count=1, max_vocab_size=None, alpha=0.025, min_alpha=0.01, workers=cores)

    # Build Vocabulary
    model.build_vocab(documents)

    # Train the model
    model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)

    # Save the model as shown below
    model.save_word2vec_format(outputName, doctag_vec=True, word_vec=False, prefix="""")
</code></pre>

<p>And the output has to be like this:</p>

<pre><code>12571 100
134602.csv 0.00691074 0.157398 0.0921498 0.126362 0.158668 -0.0753151 -0.164655 0.0883756 0.0407546 0.15239 -0.0145177 0.061617 -0.0891562 -0.0417054 -0.0858589 0.00102948 0.0161595 2.13553e-05 -0.0668119 0.0450828 0.117537 -0.0729031 -0.0580456 -0.00258632 -0.104359 0.136366 -0.144994 -0.12065 -0.121757 0.0830929 -0.16462 -0.0151503 0.0399056 0.160027 -0.0787732 -0.00789994 -0.094897 0.00608254 -0.0661624 0.129721 0.163127 -0.0793746 -0.0964145 0.0606208 0.0875067 0.0161015 -0.132051 -0.0491245 -0.154828 0.133222 -0.0687664 0.120808 -0.111705 -0.053042 -0.0912231 -0.111089 0.0443708 -0.139493 0.0607425 -0.161168 0.0786498 0.150048 0.146688 -0.0837242 -0.0553738 -0.117545 0.0986267 -0.0923841 0.098877 -0.12193 -0.062616 -0.0845228 -0.0636123 0.0823107 -0.0826875 0.139011 -0.0923962 0.0288433 0.137355 0.121588 -0.145517 0.160373 0.0628389 -0.0764258 -0.107213 0.0421445 0.137447 -0.0658571 0.0424128 0.0672861 0.109817 -0.126953 -0.0453275 0.0834503 0.0974179 0.00825522 -0.165445 -0.0213084 -0.0292943 -0.162938
125202.csv 0.106642 0.167441 -0.0275412 0.130408 -0.107533 0.091452 0.0103496 -0.0214623 0.0873943 -0.0465384 -0.165227 -0.0540914 -0.00923723 0.175378 -0.051865 0.0107003 -0.179349 0.0683971 -0.159605 0.0644916 0.136338 0.111336 -0.0805002 0.00214934 -0.0490576 0.151279 -0.0397022 0.075442 -0.0278023 -0.0636982 0.174473 0.087985 -0.0714066 -0.0800442 -0.103995 -0.0228613 0.157171 -0.0678672 -0.161953 0.0839289 -0.155191 -0.00721683 0.0586751 -0.0474399 -0.122106 0.170611 0.157929 0.075531 -0.13505 0.093849 -0.119415 0.0386302 0.0139714 0.0756701 -0.0810199 -0.111754 0.112905 0.130293 -0.126257 -0.00654255 -0.0369909 -0.072449 0.0257127 0.0716955 0.103714 -0.0842208 -0.0534867 -0.095218 0.127797 -0.029322 0.161806 -0.177695 -0.0684089 0.0623551 0.06396 0.0828089 -0.0590939 0.0180832 -0.0591218 0.136139 -0.153984 0.108085 -0.127018 -0.0847872 -0.167081 0.0199622 0.0209045 0.0320618 0.0591803 0.0809688 0.0799196 0.15632 -0.0519707 0.0270171 -0.163197 -0.0846849 -0.176135 -0.0120047 -0.0697305 0.014441
116200.csv -0.0182099 -0.130409 -0.138414 -0.0310527 -0.0274882 -0.0711805 -0.0628653 -0.144249 -0.166021 -0.0242265 -0.130593 -0.141916 0.0119525 0.0500143 -0.147568 -0.036778 0.110357 0.0439302 -0.132496 -0.105203 0.0356234 0.0982645 0.134903 -0.0648039 -0.0566216 0.138991 -0.0467151 -0.140643 0.139711 0.0943256 0.0576583 0.0644239 0.00136725 -0.0296913 0.0612566 0.148131 0.067239 0.100442 0.0665155 0.104861 -0.0498524 0.0995954 -0.115922 -0.00524584 0.0491675 0.159028 0.132554 0.0479373 0.141164 0.173129 0.022317 -0.000446397 0.0867293 -0.155649 -0.0675728 -0.0981307 -0.0806008 -0.0107237 -0.103454 -0.0753868 -0.0551634 0.170743 0.0495554 0.11536 -0.0294355 0.061617 0.126016 -0.04804 -0.0315217 -0.169522 -0.0892494 -0.025444 0.0672556 0.166157 0.0647261 0.0944827 -0.0792354 0.0182105 0.118192 0.000124603 -0.10565 -0.155033 0.107355 0.150469 -0.104327 -0.162604 -0.0218357 0.145972 -0.145784 -0.00176559 0.153054 -0.16377 -0.11736 0.0892985 -0.0212026 0.0511168 -0.146278 -0.0134697 -0.0540684 0.0791529
148597.csv -0.15473 0.0955252 0.0432369 -0.0945614 0.136283 -0.102851 0.0847211 -0.0396431 -0.0467567 0.17154 0.153097 0.0693114 0.163837 0.135897 0.146128 -0.167215 -0.152268 -0.11602 0.0282252 -0.0779752 -0.0829204 0.018318 0.00621094 0.0707405 0.0968831 0.00652018 -0.0568833 0.0916579 -0.0400151 -0.0391421 -0.0548217 -0.173926 -0.110223 -0.0317329 -0.02952 -0.129147 0.0698902 -0.154276 -0.157658 -0.14261 0.032107 -0.0385964 -0.0587693 0.0212146 0.143626 0.142041 -0.0530896 -0.133748 0.131452 0.13672 0.148338 0.160325 -0.113424 0.0678939 -0.0229337 -0.170486 -0.156904 0.0710402 0.00277802 0.120395 0.0360002 -0.0593753 0.155915 -0.0620641 -0.112055 0.0153659 0.147731 -0.0249911 0.0360584 -0.0402479 0.022273 0.00174414 -0.0178126 -0.116679 0.0191754 -0.0089874 0.083151 -0.168562 -0.160357 -0.0659622 0.0248376 0.045583 0.127733 -0.0675122 -0.0734585 0.113653 0.166756 0.0723445 0.0554671 -0.0751338 0.0481711 -0.00127609 0.0560728 0.124651 -0.0495638 0.0985305 -0.110315 0.0672438 0.096637 0.104245
166916.csv 0.168698 0.0629846 0.0248923 -0.105248 0.172408 -0.0322083 0.174124 -0.113572 -0.0104922 0.0429484 -0.0306917 0.022368 -0.0584265 0.0337984 -0.0225754 0.143456 -0.121288 -0.133673 0.0677091 0.0583681 0.0390327 -0.141176 0.0694527 -0.0290526 -0.129707 -0.0765447 0.071578 0.146411 -0.112526 0.103688 -0.110703 0.0781341 0.0318269 0.105218 0.0177797 0.123248 0.158062 0.0370042 -0.137394 0.0246147 0.00653834 0.166063 -0.100149 -0.0479191 -0.0702838 0.0690037 0.114349 -0.0274343 0.014801 -0.0421596 0.0694873 0.0662955 -0.12477 -0.0088994 0.104959 0.149459 0.16611 0.0265376 -0.134808 0.101123 0.0431258 0.0584757 -0.0315779 0.121671 -0.0380923 -0.0897689 -0.0237933 0.110452 -0.0039647 0.106183 -0.165717 -0.16557 0.136988 0.121843 0.0722612 -0.00844494 0.175932 -0.0751714 0.152611 -0.0646956 0.105122 -0.108245 0.0583691 0.113012 0.171521 -0.0258976 0.0851889 -0.0941529 0.153386 0.0455267 -0.0259182 -0.0437207 -0.150415 0.132313 -0.143572 -0.0281547 -0.00231613 -0.00760185 -0.147233 -0.167408
148291.csv 0.00976907 0.168438 -0.0919878 -0.164332 -0.138181 -0.149775 -0.0394723 0.027946 0.0662307 -0.00850593 0.12174 0.106023 -0.11512 0.0694538 0.128228 0.066019 0.0805346 0.00220964 -0.0465066 0.0923588 0.121286 0.168551 0.0462572 0.0221805 -0.119831 0.00797117 -0.00709804 -0.0222688 0.0938169 0.100695 0.133902 0.15964 0.0544278 -0.0504766 -0.0539783 -0.0158389 0.0280565 -0.10531 0.112356 -0.0349924 0.155673 0.0491142 0.171533 -0.044268 0.0560867 -0.135758 0.114202 -0.120608 0.0373457 -0.0847815 0.0285375 -0.0101114 0.0169282 -0.00141743 -0.028344 -0.00979434 -0.0599551 0.0554465 -0.0583942 -0.169627 0.167471 -0.00661054 0.114252 -0.00489984 0.167312 0.144928 0.0376684 -0.118885 0.0426739 0.169052 0.00265325 0.146609 0.163534 -0.100965 -0.101386 0.127619 0.148285 -0.0881821 -0.100448 -0.044064 0.106071 0.0239426 0.0733384 -0.0962991 0.0939341 0.0659483 0.122844 -0.140426 -0.0485195 0.0645185 0.037179 0.0963829 -0.109955 -0.151168 -0.0413991 -0.0556731 -0.173456 -0.167728 -0.128145 0.150923
...
</code></pre>

<p>Where the first word of each line is the name of each file, and what follows is the corresponding vector for that file. I need to save the vectors in this way to use an external software.</p>
","python, gensim, word-embedding, doc2vec","<p>The algorithm ('Paragraph Vector') behind <code>Doc2Vec</code> makes use of randomness during initialization and training. Training also never reaches a point where all adjustments stop – just a point where it's believed that further updates will have negligible net value. </p>

<p>So, identical texts won't achieve identical vectors – they're each being updated, alongside the model's internals, with each training cycle, against a slightly-different base model, with slightly different randomization choices. If you have enough data, good parameters, and are engaging in enough training, they should become <em>very close</em>. And, your downstream evaluations/uses should be tolerant of such small variances. </p>

<p>Similarly, two runs on the same corpus won't result in identical end-vectors unless extreme care is taken to force determinism – for example by limiting training to a single worker thread, so that OS thread scheduling unpredictability doesn't slightly change the order of training examples. So vectors should only be compared if they were co-trained together, in the same model – and again, downstream applications should be tolerant of slight jitter from run to run or example to example.</p>

<p>Other notes about your setup:</p>

<ul>
<li><p><code>min_count=1</code> is almost always a bad choice - words with single (or few) examples just add noise to the training, making resulting vectors worse.</p></li>
<li><p>stochastic gradient descent optimization typically ends after the learning-rate <code>alpha</code> has been smoothly reduced to a tiny, near-zero value (such as <code>0.0001</code>) – you're using a final alpha (<code>0.01</code>) that's a full 40% of the starting alpha.</p></li>
<li><p>you may also want to save your models using gensim's native <code>.save()</code>, because <code>.save_word2vec_format()</code> discards most model internals, and squashes the doc-vectors into the same namespace as any word-vectors. </p></li>
</ul>
",2,1,872,2018-05-18 10:00:20,https://stackoverflow.com/questions/50408740/gensim-doc2vec-im-gettting-different-vectors-from-documents-that-are-identical
Visualize Gensim Word2vec Embeddings in Tensorboard Projector,"<p>I've only seen a few questions that ask this, and none of them have an answer yet, so I thought I might as well try. I've been using gensim's word2vec model to create some vectors. I exported them into text, and tried importing it on tensorflow's live model of the embedding projector. One problem. <em>It didn't work</em>. It told me that the tensors were improperly formatted. So, being a beginner, I thought I would ask some people with more experience about possible solutions.<br>
Equivalent to my code:  </p>

<pre><code>import gensim
corpus = [[""words"",""in"",""sentence"",""one""],[""words"",""in"",""sentence"",""two""]]
model = gensim.models.Word2Vec(iter = 5,size = 64)
model.build_vocab(corpus)
# save memory
vectors = model.wv
del model
vectors.save_word2vec_format(""vect.txt"",binary = False)
</code></pre>

<p>That creates the model, saves the vectors, and then prints the results out nice and pretty in a tab delimited file with values for all of the dimensions. I understand how to do what I'm doing, I just can't figure out what's wrong with the way I put it in tensorflow, as the documentation regarding that is pretty scarce as far as I can tell.<br>
One idea that has been presented to me is implementing the appropriate tensorflow code, but I don’t know how to code that, just import files in the live demo.  </p>

<p>Edit: I have a new problem now. The object I have my vectors in is non-iterable because gensim apparently decided to make its own data structures that are non-compatible with what I'm trying to do.<br>
  Ok. Done with that too! Thanks for your help!</p>
","python, tensorflow, gensim, tensorboard, word-embedding","<p>What you are describing is possible. What you have to keep in mind is that Tensorboard reads from saved tensorflow binaries which represent your variables on disk.</p>
<blockquote>
<p>More information on saving and restoring tensorflow graph and variables <a href=""https://www.tensorflow.org/programmers_guide/saved_model"" rel=""noreferrer"">here</a></p>
</blockquote>
<p><strong>The main task is therefore to get the embeddings as saved tf variables.</strong></p>
<blockquote>
<p>Assumptions:</p>
<ul>
<li><p>in the following code <code>embeddings</code> is a python dict <code>{word:np.array (np.shape==[embedding_size])}</code></p>
</li>
<li><p>python version is 3.5+</p>
</li>
<li><p>used libraries are <code>numpy as np</code>, <code>tensorflow as tf</code></p>
</li>
<li><p>the directory to store the tf variables is <code>model_dir/</code></p>
</li>
</ul>
</blockquote>
<hr />
<h1>Step 1: Stack the embeddings to get a single <code>np.array</code></h1>
<pre><code>embeddings_vectors = np.stack(list(embeddings.values(), axis=0))
# shape [n_words, embedding_size]
</code></pre>
<hr />
<h1>Step 2: Save the <code>tf.Variable</code> on disk</h1>
<pre><code># Create some variables.
emb = tf.Variable(embeddings_vectors, name='word_embeddings')

# Add an op to initialize the variable.
init_op = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables and save the
# variables to disk.
with tf.Session() as sess:
   sess.run(init_op)

# Save the variables to disk.
   save_path = saver.save(sess, &quot;model_dir/model.ckpt&quot;)
   print(&quot;Model saved in path: %s&quot; % save_path)
</code></pre>
<blockquote>
<p><code>model_dir</code> should contain files <code>checkpoint</code>, <code>model.ckpt-1.data-00000-of-00001</code>, <code>model.ckpt-1.index</code>, <code>model.ckpt-1.meta</code></p>
</blockquote>
<hr />
<h1>Step 3: Generate a <code>metadata.tsv</code></h1>
<p>To have a beautiful labeled cloud of embeddings, you can provide tensorboard with metadata as Tab-Separated Values (tsv) (<em>cf.</em> <a href=""https://www.tensorflow.org/versions/r1.0/get_started/embedding_viz#metadata"" rel=""noreferrer"">here</a>).</p>
<pre><code>words = '\n'.join(list(embeddings.keys()))

with open(os.path.join('model_dir', 'metadata.tsv'), 'w') as f:
   f.write(words)

# .tsv file written in model_dir/metadata.tsv
</code></pre>
<hr />
<h1>Step 4: Visualize</h1>
<p>Run <code>$ tensorboard --logdir model_dir</code> -&gt; <strong>Projector</strong>.</p>
<p>To load metadata, the magic happens here:</p>
<p><a href=""https://i.sstatic.net/N81kM.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/N81kM.png"" alt=""load_meta"" /></a></p>
<hr />
<p>As a reminder, some <em>word2vec</em> embedding projections are also available on <a href=""http://projector.tensorflow.org/"" rel=""noreferrer"">http://projector.tensorflow.org/</a></p>
",17,13,9258,2018-05-23 15:50:18,https://stackoverflow.com/questions/50492676/visualize-gensim-word2vec-embeddings-in-tensorboard-projector
Fine-tuning Glove Embeddings,"<p>Has anyone tried to fine-tune <strong>Glove embeddings</strong> on a domain-specific corpus?<br>
<strong>Fine-tuning word2vec</strong> embeddings has proven very efficient for me in a various NLP tasks, but I am wondering whether generating a cooccurrence matrix on my domain-specific corpus, and training glove embeddings (initialized with pre-trained embeddings) on that corpus would generate similar improvements.</p>
","machine-learning, nlp, word2vec, word-embedding","<p>I myself am trying to do the exact same thing. You can try <a href=""https://github.com/ashutoshsingh0223/mittens"" rel=""nofollow noreferrer"">mittens</a>.</p>

<p>They have succesfully built a framework for it. Christopher D. Manning(co-author of GloVe) is associated with it.</p>
",4,3,4120,2018-06-18 12:38:34,https://stackoverflow.com/questions/50909726/fine-tuning-glove-embeddings
loading of fasttext pre trained german word embedding&#39;s .vec file throwing out of memory error,"<p>I am using gensim to load the fasttext's pre-trained word embedding</p>

<p><code>de_model = KeyedVectors.load_word2vec_format('wiki.de\wiki.de.vec')</code></p>

<p>But this gives me a memory error.</p>

<p>Is there any way I can load it?</p>
","nlp, gensim, word-embedding, fasttext","<p>Other than working on a machine with more memory, the <code>gensim</code> <code>load_word2vec_format()</code> methods have a <code>limit</code> option which can be given a count <em>n</em> of vectors to read. Only the first <em>n</em> vectors of the file will be loaded. </p>

<p>For example, to load just the 1st 100,000 words:</p>

<pre><code>de_model = KeyedVectors.load_word2vec_format('wiki.de\wiki.de.vec', limit=100000)
</code></pre>

<p>Since such files usually sort the more-frequent words first, and the 'long tail' of rarer words tend to be weaker vectors, many applications don't lose too much power by discarding rarer words. </p>
",6,4,2777,2018-06-18 13:08:57,https://stackoverflow.com/questions/50910287/loading-of-fasttext-pre-trained-german-word-embeddings-vec-file-throwing-out-o
Gensim Word2Vec select minor set of word vectors from pretrained model,"<p>I have a large pretrained Word2Vec model in gensim from which I want to use the pretrained word vectors for an embedding layer in my Keras model. </p>

<p>The problem is that the embedding size is enormous and I don't need most of the word vectors (because I know which words can occure as Input). So I want to get rid of them to reduce the size of my embedding layer.</p>

<p>Is there a way to just keep desired wordvectors (including the coresponding indices!), based on a whitelist of words?</p>
","python, keras, word2vec, gensim, word-embedding","<p>Thanks to <a href=""https://stackoverflow.com/a/54258997/7339624"">this answer</a> (I've changed the code a little bit to make it better). you can use this code for solving your problem.</p>

<p>we have all our minor set of words in <code>restricted_word_set</code>(it can be either list or set) and <code>w2v</code> is our model, so here is the function:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

def restrict_w2v(w2v, restricted_word_set):
    new_vectors = []
    new_vocab = {}
    new_index2entity = []
    new_vectors_norm = []

    for i in range(len(w2v.vocab)):
        word = w2v.index2entity[i]
        vec = w2v.vectors[i]
        vocab = w2v.vocab[word]
        vec_norm = w2v.vectors_norm[i]
        if word in restricted_word_set:
            vocab.index = len(new_index2entity)
            new_index2entity.append(word)
            new_vocab[word] = vocab
            new_vectors.append(vec)
            new_vectors_norm.append(vec_norm)

    w2v.vocab = new_vocab
    w2v.vectors = np.array(new_vectors)
    w2v.index2entity = np.array(new_index2entity)
    w2v.index2word = np.array(new_index2entity)
    w2v.vectors_norm = np.array(new_vectors_norm)
</code></pre>

<blockquote>
  <p><strong>WARNING:</strong> when you first create the model the <code>vectors_norm == None</code> so
  you will get an error if you use this function there. <code>vectors_norm</code>
  will get a value of the type <code>numpy.ndarray</code> after the first use. so
  before using the function try something like <code>most_similar(""cat"")</code> so
  that <code>vectors_norm</code> not be equal to <code>None</code>.</p>
</blockquote>

<p>It rewrites all of the variables which are related to the words based on the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">Word2VecKeyedVectors</a>.</p>

<p>Usage:</p>

<pre><code>w2v = KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)
w2v.most_similar(""beer"")
</code></pre>

<blockquote>
  <p>[('beers', 0.8409687876701355),<br>
   ('lager', 0.7733745574951172),<br>
   ('Beer', 0.71753990650177),<br>
   ('drinks', 0.668931245803833),<br>
   ('lagers', 0.6570086479187012),<br>
   ('Yuengling_Lager', 0.655455470085144),<br>
   ('microbrew', 0.6534324884414673),<br>
   ('Brooklyn_Lager', 0.6501551866531372),<br>
   ('suds', 0.6497018337249756),<br>
   ('brewed_beer', 0.6490240097045898)]</p>
</blockquote>

<pre><code>restricted_word_set = {""beer"", ""wine"", ""computer"", ""python"", ""bash"", ""lagers""}
restrict_w2v(w2v, restricted_word_set)
w2v.most_similar(""beer"")
</code></pre>

<blockquote>
  <p>[('lagers', 0.6570085287094116),<br>
   ('wine', 0.6217695474624634),<br>
   ('bash', 0.20583480596542358),<br>
   ('computer', 0.06677375733852386),<br>
   ('python', 0.005948573350906372)]</p>
</blockquote>

<p>it can be used for removing some words either.</p>
",14,8,2820,2018-06-18 17:32:32,https://stackoverflow.com/questions/50914729/gensim-word2vec-select-minor-set-of-word-vectors-from-pretrained-model
Paragraph Vector or Doc2vec model size,"<p>I am using deeplearning4j java library to build paragraph vector model (doc2vec) of dimension 100. I am using a text file. It has around 17 million lines, and size of the file is 330 MB. 
I can train the model and calculate paragraph vector which gives reasonably good results.</p>

<p>The problem is that when I try to save  the model (by writing to disk) with WordVectorSerializer.writeParagraphVectors (dl4j method) it takes around 20 GB of space.  And around 30GB when I use native java serializer. </p>

<p>I'm thinking may be the model is size is too big for that much data. Is the model size 20GB reasonable for the text data of 300 MB?  </p>

<p>Comments are also welcome from people who have used doc2vec/paragraph vector in other library/language. </p>

<p>Thank you!</p>
","nlp, gensim, word-embedding, doc2vec, deeplearning4j","<p>I'm not familiar with the dl4j implementation, but model size is dominated by the number of unique word-vectors/doc-vectors, and the chosen vector size. </p>

<p>(330MB / 17 million) means each of your documents averages only 20 bytes – very small for <code>Doc2Vec</code>! </p>

<p>But if for example you're training up a 300-dimensional doc-vector for each doc, and each dimension is (as typical) a 4-byte float, then (17 million * 300 dims * 4 bytes/dim) = 20.4GB. And then there'd be more space for word-vectors and model inner-weights/vocabulary/etc, so the storage sizes you've reported aren't implausible.</p>

<p>With the sizes you've described, there's also a big risk of overfitting - if using 300-dimensions, you'd be modeling docs of &lt;20 bytes source material as (300*4=) 1200-byte doc-vectors. </p>

<p>To some extent, that makes the model tend towards a giant, memorized-inputs lookup table, and thus less-likely to capture generalizable patterns that help understand training docs, or new docs. Effective learning usually instead looks somewhat like compression: modeling the source materials as something smaller but more salient. </p>
",4,3,1199,2018-06-20 10:17:47,https://stackoverflow.com/questions/50945820/paragraph-vector-or-doc2vec-model-size
Keras: Embadding on Sentences Array as Input,"<p>I am new to keras. I try to create a neural network with an embedding layer as input layer. As far as I understand it right now the structure is like the following:</p>

<pre><code>model.add(Embedding(word_count,embedding_size,input_length=sentence_length).
</code></pre>

<p>This works with a dataset of labelled sentences. </p>

<p>Now I want to change the structure of my dataset. I will have labelled sets of sentences (arrays of sentences).</p>

<p>As far as I know, I can't use the Embedding layer anymore as input layer. This is because it expects a sentence as input and not an array of sentences. Can I change something so that I still can use the embedding layer in my model but have arrays of sentences as input? </p>

<p>The array length of my arrays of sentences is all the time the same as my sentence length stays the same overall sentences.</p>
","python, machine-learning, neural-network, keras, word-embedding","<p>Let's say you have arrays of fixed-length sentences and, as you mentioned, the number of sentences in all arrays are the same. Therefore, if you store all of your data in one tensor it would have a shape of <code>(num_of_arrays, num_of_sentences, length_of_sentence)</code>. And each array of sentences has its own label. So basically your model should take an array of sentences as an input and predicts a label for it. Now to use an embedding layer we first reshape our data, then pass it to an embedding layer and then (if necessary) we reshape it back. Here is an example:</p>

<pre><code>from keras import models, layers

# the following numbers are just for demonstration
vocab_size = 1000
embed_dim = 50

num_arrays = 100
num_sentences = 200
len_sentence = 300

model = models.Sequential()
model.add(layers.Reshape((num_sentences*len_sentence,), input_shape=(num_sentences, len_sentence)))
model.add(layers.Embedding(vocab_size, embed_dim, input_length=num_sentences*len_sentence))
model.add(layers.Reshape((num_sentences, len_sentence, embed_dim)))
# add whatever layers as you wish to complete your model

model.summary()
</code></pre>

<p>Here is the model summary:</p>

<pre class=""lang-none prettyprint-override""><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape_1 (Reshape)          (None, 60000)             0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 60000, 50)         50000     
_________________________________________________________________
reshape_2 (Reshape)          (None, 200, 300, 50)      0         
=================================================================
Total params: 50,000
Trainable params: 50,000
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>As you can see, each of the sentences in an array is now represented by a matrix of shape <code>(sentence_length, embed_dim)</code>. Now you can add more layers to complete your model. I am not sure if this is what you asked for. Let me know in the comments if you meant something else.</p>
",1,1,720,2018-07-03 16:49:37,https://stackoverflow.com/questions/51159532/keras-embadding-on-sentences-array-as-input
How to get word vectors from Keras Embedding Layer,"<p>I'm currently working with a Keras model which has a embedding layer as first layer. In order to visualize the relationships and similarity of words between each other I need a function that returns the mapping of words and vectors of every element in the vocabulary (e.g. 'love' - [0.21, 0.56, ..., 0.65, 0.10]).</p>

<p>Is there any way to do it?</p>
","python, dictionary, keras, keras-layer, word-embedding","<p>You can get the word embeddings by using the <code>get_weights()</code> method of the embedding layer (i.e. essentially the weights of an embedding layer are the embedding vectors):</p>

<pre class=""lang-py prettyprint-override""><code># if you have access to the embedding layer explicitly
embeddings = emebdding_layer.get_weights()[0]

# or access the embedding layer through the constructed model 
# first `0` refers to the position of embedding layer in the `model`
embeddings = model.layers[0].get_weights()[0]

# `embeddings` has a shape of (num_vocab, embedding_dim) 

# `word_to_index` is a mapping (i.e. dict) from words to their index, e.g. `love`: 69
words_embeddings = {w:embeddings[idx] for w, idx in word_to_index.items()}

# now you can use it like this for example
print(words_embeddings['love'])  # possible output: [0.21, 0.56, ..., 0.65, 0.10]
</code></pre>
",51,25,16735,2018-07-08 18:53:29,https://stackoverflow.com/questions/51235118/how-to-get-word-vectors-from-keras-embedding-layer
Could use help formatting data correctly for a Keras SimpleRNN,"<p>I'm struggling a bit getting data into the right format for a simpleRNN, or I'm struggling to define the model correctly. I'm hoping someone can spot the problem?</p>

<p>I'm trying to do classification of a list <code>X</code> of vectors of length 278 that contain integer values chosen from a dictionary <code>vocab</code> of length 9026 features as either belonging to class 0 or 1. Here's an example of my input data:</p>

<pre><code>X=[[1,822,773,54,51,...],[2,3,1,41,3,...],[121,17,311,4,12,...],...]
y=[0,1,1,...]
</code></pre>

<p>So for example <code>np.array(X).shape=(1000,278)</code> and <code>len(y)=1000</code>
My model is:</p>

<pre><code>model.add(L.InputLayer([None],dtype='int32'))
model.add(L.Embedding(input_dim=len(vocab)+1,\
                      output_dim=64,\
                      input_length=278))
model.add(L.SimpleRNN(64,return_sequences=True))
model.add(L.TimeDistributed(L.Dense(1,activation='softmax')))
model.compile(optimizer='adam',\
              loss='categorical_crossentropy',\
              metrics=['accuracy']
             )
print(model.summary())

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_8 (Embedding)      (None, 278, 64)           577728    
_________________________________________________________________
simple_rnn_7 (SimpleRNN)     (None, 278, 64)           8256      
_________________________________________________________________
time_distributed_7 (TimeDist (None, 278, 1)            65        
=================================================================
Total params: 586,049
Trainable params: 586,049
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>

<p>I prepare them as follows:</p>

<pre><code>X=np.array(X)
y=keras.utils.to_categorical(y)

frac=0.3
random_state=42
X_train,X_tmp,y_train,y_tmp = \
    train_test_split(X,y,test_size=frac,random_state=random_state,\
                         stratify=y)
train=(X_train,y_train)
test=(X_tmp,y_tmp)
</code></pre>

<p>When I run the model:</p>

<pre><code>model.fit(train[0],train[1],verbose=0,\
              batch_size=batch_size,\
              epochs=epochs,validation_data=test)
</code></pre>

<p>I get the following error:</p>

<pre><code>ValueError: Error when checking target: expected time_distributed_1 
to have 3 dimensions, but got array with shape (5450, 2)
</code></pre>

<p>If I change the input data to</p>

<pre><code>train=(X_train,y_train[:,:,np.newaxis])
test=(X_tmp,y_tmp[:,:,np.newaxis])
</code></pre>

<p>and run the model, I get this error:</p>

<pre><code>ValueError: Error when checking target: expected time_distributed_1 
to have shape (278, 2) but got array with shape (2, 1)
</code></pre>

<p>OK so obviously I've got something wrong since my final dense layer is looking for shape 278 and not 2. So I tried this model without explicitly defining the input_length:</p>

<pre><code>model.add(L.InputLayer([None],dtype='int32'))
model.add(L.Embedding(input_dim=len(vocab)+1,\
                      output_dim=64))
model.add(L.SimpleRNN(64,return_sequences=True))
model.add(L.TimeDistributed(L.Dense(1,activation='softmax')))
model.compile(optimizer='adam',\
              loss='categorical_crossentropy',\
              metrics=['accuracy']
             )
print(model.summary())

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_10 (Embedding)     (None, None, 64)          577728    
_________________________________________________________________
simple_rnn_9 (SimpleRNN)     (None, None, 64)          8256      
_________________________________________________________________
time_distributed_9 (TimeDist (None, None, 2)           130       
=================================================================
Total params: 586,114
Trainable params: 586,114
Non-trainable params: 0
</code></pre>

<p>and when I run the model, I get</p>

<pre><code>ValueError: Error when checking target: expected time_distributed_9
to have shape (None, 2) but got array with shape (2, 1)
</code></pre>

<p>I'm very confused. Can anyone help me diagnose this?</p>
","python, keras, classification, recurrent-neural-network, word-embedding","<p>Why are you using <code>TimeDistributed</code>? You don't need that at all. For each vector of length 278 you want to predict one and only one number which is between 0 and 1. Therefore the output shape of last layer should be <code>(None,1)</code>. Remove the <code>return_sequences</code> argument of <code>SimpleRNN</code> layer as well.You don't need that as well. Like this:</p>

<pre><code>model.add(L.SimpleRNN(64))
model.add(L.Dense(1,activation='sigmoid'))
</code></pre>

<p>Further, you should use <code>'sigmoid'</code> as the activation function to make the last layer outputs a value between 0 and 1. And change the <code>'categorical_crossentropy'</code> to <code>'binary_crossentropy'</code>. You also don't need to convert the <code>y</code> to categorical. It is already zeros and ones and that's fine (just convert it to a numpy array); remember, you are doing binary classification here.</p>

<p>Plus, use the first model. Your second model does not make sense since you mentioned all of the input vectors are the same length (i.e. 278).</p>

<p>And one final point: remove that <code>InputLayer</code>. It is redundant. You are already setting the input shape in your embedding layer.</p>
",1,0,255,2018-07-15 14:46:47,https://stackoverflow.com/questions/51349484/could-use-help-formatting-data-correctly-for-a-keras-simplernn
"Gensim Doc2vec trained, but not saved","<p>While I trained d2v on a large text corpus I received these 3 files: </p>

<pre><code>doc2vec.model.trainables.syn1neg.npy

doc2vec.model.vocabulary.cum_table.npy

doc2vec.model.wv.vectors.npy
</code></pre>

<p>Bun final model has not saved, because there was not enough free space available on the disk. </p>

<pre><code>OSError: 5516903000 requested and 4427726816 written
</code></pre>

<p>Is there a way to resave my model using these files in a shorter time, than all training time? </p>

<p>Thank you in advance! </p>
","model, save, gensim, word-embedding, doc2vec","<p>If you still have the model in RAM, in an environment (like a Jupyter notebook) where you can run new code, you could try to clear space (or attach a new volume) and then try a <code>.save()</code> again. That is, you don't need to re-train, just re-save what's already in RAM. </p>

<p>There's no routine for saving ""just what isn't already saved"". So even though the subfiles that <em>did</em> save could potentially be valuable if you were desperate to salvage anything from the 1st training run (perhaps via a process like in <a href=""https://stackoverflow.com/questions/51281241/gensim-word2vec-model-trained-but-not-saved"">my <code>Word2Vec</code> answer here</a>, though it's a bit more complicated with <code>Doc2Vec</code>), trying another save to the same place/volume would require getting those existing files out-of-the-way. (Maybe you could transfer them to remote storage in case they'll be needed, but delete them locally to free space?)</p>

<p>If you try to save to a filename that ends "".gz"", gensim will try to save everything compressed, which might help a little. (Unfortunately, the main vector arrays don't compress very well, so this might not be enough savings alone.)</p>

<p>There's no easy way to slim an already-trained model in memory, without potentially destroying some of its capabilities. (There are hard ways, but only if you're sure you can discard things a full model could do... and it's not yet clear you're in that situation.) </p>

<p>The major contributors to model size are the number of unique-words, and the number of unique doc-tags. </p>

<p>Specifying a larger <code>min_count</code> <em>before training</em> will discard more low-frequency words – and very-low-frequency words often just hurt the model anyway, so this trimming often improves three things simultaneously: faster training, smaller model, and higher-quality-results on downstream tasks. </p>

<p>If you're using plain-int doc-tags, the model will require vector space for all doc-tag ints from 0 to your highest number. So even if you trained just 2 documents, if they had plain-int doc-tags of <code>999998</code> and <code>999999</code>, it'd still need to allocate (and save) garbage vectors for 1 million tags, 0 to 999,999. So in some cases people's memory/disk usage is higher than expected because of that – and either using contiguous IDs starting from <code>0</code>, or switching to string-based doc-tags, reduces size a lot. (But, again, this has to be chosen before training.)</p>
",1,0,795,2018-07-17 08:05:51,https://stackoverflow.com/questions/51376241/gensim-doc2vec-trained-but-not-saved
How to correctly use mask_zero=True for Keras Embedding with pre-trained weights?,"<p>I am confused about how to format my own pre-trained weights for Keras <code>Embedding</code> layer if I'm also setting <code>mask_zero=True</code>. Here's a concrete toy example.</p>

<p>Suppose I have a vocabulary of 4 words <code>[1,2,3,4]</code> and am using vector weights defined by:</p>

<pre><code>weight[1]=[0.1,0.2]
weight[2]=[0.3,0.4]
weight[3]=[0.5,0.6]
weight[4]=[0.7,0.8]
</code></pre>

<p>I want to embed sentences of length up to 5 words, so I have to zero pad them before feeding them into the Embedding layer. I want to mask out the zeros so further layers don't use them. </p>

<p>Reading the Keras docs for Embedding, it says the 0 value can't be in my vocabulary. </p>

<blockquote>
  <p>mask_zero: Whether or not the input value 0 is a special ""padding""
  value that should be masked out. This is useful when using recurrent
  layers which may take variable length input. If this is True then all
  subsequent layers in the model need to support masking or an exception
  will be raised. If mask_zero is set to True, as a consequence, index 0
  cannot be used in the vocabulary (input_dim should equal size of
  vocabulary + 1).</p>
</blockquote>

<p>So what I'm confused about is how to construct the weight array for the Embedding layer, since ""index 0 cannot be used in the vocabulary."" If I build the weight array as</p>

<pre><code>[[0.1,0.2],
 [0.3,0.4],
 [0.5,0.6],
 [0.7,0.8]]
</code></pre>

<p>then normally, word <code>1</code> would point to index 1, which in this case holds the weights for word <code>2</code>. Or is it that when you specify <code>mask_zero=True</code>, Keras internally makes it so that word <code>1</code> points to index 0? Alternatively, do you just prepend a vector of zeros in index zero, as follows?</p>

<pre><code>[[0.0,0.0],
 [0.1,0.2],
 [0.3,0.4],
 [0.5,0.6],
 [0.7,0.8]]
</code></pre>

<p>This second option seems to me to put the zero into the vocabulary. In other words, I'm very confused. Can anyone shed light on this?</p>
","python, tensorflow, keras, word-embedding","<p>You're second approach is correct. You will want to construct your embedding layer in the following way</p>

<pre><code>embedding = Embedding(
   output_dim=embedding_size,
   input_dim=vocabulary_size + 1,
   input_length=input_length,
   mask_zero=True,
   weights=[np.vstack((np.zeros((1, embedding_size)),
                       embedding_matrix))],
   name='embedding'
)(input_layer)
</code></pre>

<p>where <code>embedding_matrix</code> is the second matrix you provided.</p>

<p>You can see this by looking at the <a href=""https://github.com/keras-team/keras/blob/master/keras/layers/embeddings.py#L15"" rel=""noreferrer"">implementation of keras' embedding</a> layer. Notably, how mask_zero is only used to <em>literally</em> mask the inputs</p>

<pre><code>def compute_mask(self, inputs, mask=None):
    if not self.mask_zero:
        return None
    output_mask = K.not_equal(inputs, 0)
    return output_mask
</code></pre>

<p>thus the entire kernel is still multiplied by the input, meaning all indexes are shifted up by one. </p>
",6,7,3786,2018-07-17 13:29:04,https://stackoverflow.com/questions/51382664/how-to-correctly-use-mask-zero-true-for-keras-embedding-with-pre-trained-weights
How to check via callbacks if alpha is decreasing? + How to load all cores during training?,"<p>I'm training doc2vec, and using callbacks trying to see if alpha is decreasing over training time using this code:</p>

<pre><code>class EpochSaver(CallbackAny2Vec):
'''Callback to save model after each epoch.'''

    def __init__(self, path_prefix):
        self.path_prefix = path_prefix
        self.epoch = 0

        os.makedirs(self.path_prefix, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = get_tmpfile(
            '{}_epoch{}.model'.format(self.path_prefix, self.epoch)
        )
        model.save(savepath)
        print(
            ""Model alpha: {}"".format(model.alpha), 
            ""Model min_alpha: {}"".format(model.min_alpha),
            ""Epoch saved: {}"".format(self.epoch + 1),
            ""Start next epoch""
        )
        self.epoch += 1


def train():

    workers = multiprocessing.cpu_count()*4
    model = Doc2Vec(
        DocIter(),
        vec_size=600, alpha=0.03, min_alpha=0.00025, epochs=20,
        min_count=10, dm=1, hs=1, negative=0, workers=workers,
        callbacks=[EpochSaver(""./checkpoints"")]
    )
    print(
        ""HS"", model.hs, ""Negative"", model.negative, ""Epochs"", 
         model.epochs, ""Workers: "", model.workers, ""Model alpha: 
         {}"".format(model.alpha)
    )  
</code></pre>

<p>And while training I see that alpha is not changing over time. On each callback I see alpha = 0.03.<br>
Is it possible to check if alpha is decreasing? Or it really not decreasing at all during training? </p>

<p>One more question: 
How can I benefit from all my cores while training doc2vec?</p>

<p><a href=""https://i.sstatic.net/Bo1uh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Bo1uh.jpg"" alt=""Loading of cores""></a></p>

<p>As we can see, each core is not loaded more than +-30%. </p>
","callback, gensim, multicore, word-embedding, doc2vec","<p>The <code>model.alpha</code> property only holds the initially-configured starting-<code>alpha</code> – it's not updated to the effective learning-rate through training. </p>

<p>So, even if the value is being decreased properly (and I expect that it is), you wouldn't see it in the logging you've added. </p>

<p>Separate observations about your code:</p>

<ul>
<li><p>in gensim versions at least through 3.5.0, maximum training throughput is most often reached with some value for <code>workers</code> between 3 and the number of cores – but usually not the full number of cores (if it's higher than 12) or larger. So <code>workers=multiprocessing.cpu_count()*4</code> is likely going to much slower than what you could achieve with a lower number. </p></li>
<li><p>if your corpus is large enough to support 600-dimensional vectors, and discarding words with fewer than <code>min_count=10</code> examples, negative sampling may work faster <em>and</em> get better results than the <code>hs</code> mode. (The pattern in published work seems to be to prefer negative-sampling with larger corpuses.)</p></li>
</ul>
",1,0,849,2018-07-19 08:45:36,https://stackoverflow.com/questions/51418154/how-to-check-via-callbacks-if-alpha-is-decreasing-how-to-load-all-cores-durin
Pytorch nn.embedding error,"<p>I was reading pytorch documentation on <a href=""https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#word-embeddings-in-pytorch"" rel=""nofollow noreferrer"">Word Embedding</a>. </p>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(5)

word_to_ix = {""hello"": 0, ""world"": 1, ""how"":2, ""are"":3, ""you"":4}
embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings
lookup_tensor = torch.tensor(word_to_ix[""hello""], dtype=torch.long)
hello_embed = embeds(lookup_tensor)
print(hello_embed)
</code></pre>

<p>Output:</p>

<pre><code>tensor([-0.4868, -0.6038, -0.5581,  0.6675, -0.1974])
</code></pre>

<p>This looks good but if I replace  line lookup_tensor by </p>

<pre><code>lookup_tensor = torch.tensor(word_to_ix[""how""], dtype=torch.long)
</code></pre>

<p>I am getting the error as:</p>

<p><code>RuntimeError: index out of range at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:343</code></p>

<p>I don't understand why it gives RunTime error on line <code>hello_embed = embeds(lookup_tensor)</code>. </p>
","pytorch, word-embedding","<p>When you declare <code>embeds = nn.Embedding(2, 5)</code> the vocab size is 2 and embedding size is 5. i.e each word will be represented by a vector of size 5 and there are only 2 words in vocab.</p>

<p><code>lookup_tensor = torch.tensor(word_to_ix[""how""], dtype=torch.long)</code> embeds will try to look up vector corresponding to the third word in vocab, but embedding has vocab size of 2. and that is why you get the error.</p>

<p>If you declare <code>embeds = nn.Embedding(5, 5)</code> it should work fine.</p>
",3,1,2818,2018-07-21 12:27:18,https://stackoverflow.com/questions/51456059/pytorch-nn-embedding-error
gensim word2vec - update model data,"<p>I have an issue similar to the one discussed here - <a href=""https://stackoverflow.com/questions/40727093/gensim-word2vec-updating-word-embeddings-with-newcoming-data"">gensim word2vec - updating word embeddings with newcoming data</a></p>

<p>I have the following code that saves a model as <strong>text8_gensim.bin</strong></p>

<pre><code>sentences = word2vec.Text8Corpus('train/text8')
model = word2vec.Word2Vec(sentences, size=200, workers=12, min_count=5,sg=0, window=8, iter=15, sample=1e-4,alpha=0.05,cbow_mean=1,negative=25)
model.save(""./savedModel/text8_gensim.bin"")
</code></pre>

<p>Here is the code that adds more data to the saved model (after loading it)</p>

<pre><code>fname=""savedModel/text8_gensim.bin""
model = word2vec.Word2Vec.load(fname)
model.epochs=15

#Custom words
docs = [""start date"", ""end date"", ""eft date"",""termination date""]
model.build_vocab(docs, update=True)
model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)
model.wv.similarity('start','eft')
</code></pre>

<p>The model loads fine; however when I try to call <strong>model.wv.similarity</strong> function I get the following error</p>

<p><strong>KeyError: ""word 'eft' not in vocabulary""</strong></p>

<p>Am I missing something here?</p>
","gensim, word2vec, word-embedding","<p>Those <code>docs</code> aren't in the right format: each text should be a list-of-string-tokens, not a string. </p>

<p>And, the same <code>min_count</code> threshold will apply to incremental updates: words less frequent that that threshold will be ignored. (Since a <code>min_count</code> higher than 1 is almost always a good idea, a word that appears only once in any update will never be added to the model.)</p>

<p>Incrementally adding words introduces lots of murky issue with unclear proper choices with regard to model quality, balancing the effects of early-vs-late training, management of the <code>alpha</code> learning-rate, and so forth. It won't necessarily improve your model; with the wrong choices it could make it worse, by adjusting some words with your new texts in ways that move them out-of-compatible-alignment with earlier-batch-only words.</p>

<p>So be careful and always check with a repeatable automated quantitative quality check that your changes are helping. (The safest approach is to retrain with old and new texts in one combined corpus, so that all words get trained against one another equally on all data.)</p>
",2,1,1267,2018-07-26 20:49:50,https://stackoverflow.com/questions/51547315/gensim-word2vec-update-model-data
Embedding in Keras,"<p>Which algorithm is used for embedding in Keras built-in function?
Word2vec? Glove? Other?</p>

<p><a href=""https://keras.io/layers/embeddings/"" rel=""nofollow noreferrer"">https://keras.io/layers/embeddings/</a></p>
","keras, nlp, data-science, word-embedding","<p>The short answer is <strong>neither</strong>. In essence, an embedding layer such as <em>Word2Vec</em> of <em>GloVe</em> is just a small neural network module (fully-connected layer usually) that projects higher, sparse dimensionality into a lower, n-dimensional vector.</p>

<p>When you insert a <em>fresh</em> random embedding layer in Keras into your neural network, Keras will construct a dense learnable matrix of shape <code>[input_dim, output_dim]</code>.</p>

<p>Concretely, let's say that you're inserting an Embedding layer to encode <em>integer scalar</em> month information (12 unique values) into a <em>float vector</em> of size 3. In Keras, you're going to declare your embedding as follows:</p>

<pre><code>import numpy as np
import keras
from keras.models import Sequential, Model
from keras.layers import Embedding, Input
x = Input(shape=(1000,)) # suppose seq_len=1000
embedding = Embedding(12+1, 3, input_length=1000)(x)
model = Model(inputs=x, outputs= embedding) # Functional API
model.summary()
</code></pre>

<p>Your embedding layer would have a summary as follows:</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1000)              0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 1000, 3)           39        
=================================================================
Total params: 39
Trainable params: 39
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>Notice that the learnable parameters are <code>39 = 13*3</code> (the +1 is needed by Keras to encode values that don't belong to any of the 12 unique months - <em>just in case</em>).</p>

<p>Also notice that while the input shape to embedding is shaped <code>(None, 1000)</code>, the output of the embedding is shaped <code>(None, 1000, 3)</code>. This means the very small dense weight matrix of size <code>[13, 3]</code> is applied to <strong>each</strong> of the 1000 input time-steps. Which means, every month integer input of <code>0-11</code> will be converted into a float vector of size <code>(3,)</code>.</p>

<p>This also means that when you do backpropagation from the final layer into the embedding layer, the gradient to <em>each</em> of the 1000 time-steps embedding output will also flow (in a <code>time_distributed</code> manner) to the small neural network weights (which is, <em>essentially</em>, the <strong>embedding</strong> layer) of size <code>[13,3]</code>.</p>

<p>Please also refer to official Keras documentation for Embedding layer: <a href=""https://keras.io/layers/embeddings/"" rel=""noreferrer"">https://keras.io/layers/embeddings/</a>.</p>
",7,3,1834,2018-07-29 11:17:43,https://stackoverflow.com/questions/51579761/embedding-in-keras
Seralizing a keras model with an embedding layer,"<p>I've trained a model with pre-trained word embeddings like this: </p>

<pre><code>embedding_matrix = np.zeros((vocab_size, 100))
for word, i in text_tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

embedding_layer = Embedding(vocab_size,
                        100,
                        embeddings_initializer=Constant(embedding_matrix),
                        input_length=50,
                        trainable=False)
</code></pre>

<p>With the architecture looking like this: </p>

<pre><code>sequence_input = Input(shape=(50,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
text_cnn = Conv1D(filters=5, kernel_size=5, padding='same',     activation='relu')(embedded_sequences)
text_lstm = LSTM(500, return_sequences=True)(embedded_sequences)


char_in = Input(shape=(50, 18, ))
char_cnn = Conv1D(filters=5, kernel_size=5, padding='same', activation='relu')(char_in)
char_cnn = GaussianNoise(0.40)(char_cnn)
char_lstm = LSTM(500, return_sequences=True)(char_in)



merged = concatenate([char_lstm, text_lstm]) 

merged_d1 = Dense(800, activation='relu')(merged)
merged_d1 = Dropout(0.5)(merged_d1)

text_class = Dense(len(y_unique), activation='softmax')(merged_d1)
model = Model([sequence_input,char_in], text_class)
</code></pre>

<p>When I go to convert the model to json, I get this error:</p>

<pre><code>ValueError: can only convert an array of size 1 to a Python scalar
</code></pre>

<p>Similarly, if I use the <code>model.save()</code> function, it seems to save correctly, but when I go to load it, I get <code>Type Error: Expected Float32</code>. </p>

<p>My question is: is there something I am missing when trying to serialize this model? Do I need some sort of <code>Lambda</code> layer or something of the sorts? </p>

<p>Any help would be greatly appreciated!</p>
","tensorflow, keras, word-embedding","<p>You can use the <code>weights</code> argument in <code>Embedding</code> layer to provide initial weights.</p>



<pre class=""lang-py prettyprint-override""><code>embedding_layer = Embedding(vocab_size,
                            100,
                            weights=[embedding_matrix],
                            input_length=50,
                            trainable=False)
</code></pre>

<p>The weights should remain non-trainable after model saving/loading:</p>

<pre class=""lang-py prettyprint-override""><code>model.save('1.h5')
m = load_model('1.h5')
m.summary()

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_3 (InputLayer)            (None, 50)           0
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 50, 18)       0
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 50, 100)      1000000     input_3[0][0]
__________________________________________________________________________________________________
lstm_4 (LSTM)                   (None, 50, 500)      1038000     input_4[0][0]
__________________________________________________________________________________________________
lstm_3 (LSTM)                   (None, 50, 500)      1202000     embedding_1[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 50, 1000)     0           lstm_4[0][0]
                                                                 lstm_3[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 50, 800)      800800      concatenate_2[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 50, 800)      0           dense_2[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 50, 15)       12015       dropout_2[0][0]
==================================================================================================
Total params: 4,052,815
Trainable params: 3,052,815
Non-trainable params: 1,000,000
__________________________________________________________________________________________________
</code></pre>
",3,1,1574,2018-08-01 21:17:17,https://stackoverflow.com/questions/51642381/seralizing-a-keras-model-with-an-embedding-layer
Can I interpret doc2vec components?,"<p>I am solving a binary text classification problem with corporate filings. Using Doc2Vec embeddings of length 100 with LightGBM is producing great results. However, for this project it would be very valuable to approximate a thematic meaning for at least one of the components. Ideally, this would be a feature ranked with high importance by LightGBM explained anecdotally with a few examples.</p>

<p>Has anyone attempted this, or should interpretation be off the table for a high-dimensional model with this level of complexity?</p>
","nlp, word2vec, word-embedding, doc2vec, lightgbm","<p>The individual dimensions of a <code>Doc2Vec</code> representation should not be considered independent, interpretable features. They're only useful in concert with each other, and the exact directions aligned with individual coordinate-axes may not be strongly meaningful in any human-describable sense.</p>

<p>However, <em>neighborhoods</em> of the space may loosely fit describable themes, and certain <em>directions</em> (not specifically parallel with coordinate-axes) may loosely fit semantic themes. </p>

<p>But to characterize those, you might try to find the centroid points of groups-of-related-documents, or discovered clusters, and compare the relative distances/directions between those centroids.  </p>
",0,1,371,2018-08-06 17:48:05,https://stackoverflow.com/questions/51713078/can-i-interpret-doc2vec-components
Keras - Look up an embedding,"<p><strong>What I am trying to do:</strong></p>

<p>I am trying to look up the word embeddings for each word from a sequence. This is a sequence of numbers generated from text.</p>

<p><strong>Background:</strong></p>

<p>My sequence (of shape (200,)) looks something like this:</p>

<pre><code>50, 2092, 3974,  398,   10, 9404,    5, 1001, 3975,   15,  512... &lt;snip&gt;
</code></pre>

<p>These number represent a word from a vocabulary (of 10000 words). I have an some embedding weights that I created using the negative sampling method found <a href=""http://adventuresinmachinelearning.com/word2vec-keras-tutorial/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The extracted embedding weights are of shape (10000 , 106) of which I can load into a new embedding layer. </p>

<p>I want to look up each number in the sequence from this new embedding layer with the loaded weights and have it return 200 vectors of size 106 corresponding to the sequence.</p>

<p><strong>Here is what I have done so far:</strong></p>

<pre><code>embedding_weights = np.genfromtxt('embedding_weights.csv', delimiter=',')

    input_layer = Input(shape=(200,), name='text_input')
    embedding = Embedding(input_length=200, input_dim=vocabulary_size, output_dim=106, 
                           name='embedding_layer', trainable=False, weights=[embedding_weights])
    embedded_text = embedding(input_layer)
</code></pre>

<p>Is this the correct way to look up the embeddings?</p>
","python, keras, word-embedding","<p>Yes, that looks correct. To actually extract the embeddings, you can wrap the layers you defined in a <code>Model</code>:</p>

<pre><code>import numpy as np
from keras.layers import Input, Embedding
from keras.models import Model

# Generate some random weights
embedding_weights = np.random.rand(10000, 106)
vocabulary_size = 10000

input_layer = Input(shape=(200,), name='text_input')
embedding = Embedding(input_length=200, input_dim=vocabulary_size, output_dim=106, 
                       name='embedding_layer', trainable=False, weights=[embedding_weights])
embedded_text = embedding(input_layer)

embedding_model = Model(inputs=input_layer, outputs=embedded_text)

# Random input sequence of length 200
input_sequence = np.random.randint(0,10000,size=(1,200))
# Extract the embeddings by calling the .predict() method
sequence_embeddings = embedding_model.predict(input_sequence)
</code></pre>
",5,3,3938,2018-08-09 21:23:18,https://stackoverflow.com/questions/51775966/keras-look-up-an-embedding
"Keras example word-level model with integer sequences gives `expected ndim=3, found ndim=4`","<p>I'm trying to implement the Keras word-level example on <a href=""https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"" rel=""nofollow noreferrer"">their blog</a> listed under the <em>Bonus Section</em> -> <strong>What if I want to use a word-level model with integer sequences?</strong></p>

<p>I've marked up the layers with names to help me reconnect the layers from a loaded model to a inference model later. I think I've followed their example model:</p>

<pre><code># Define an input sequence and process it - where the shape is (timesteps, n_features)
encoder_inputs = Input(shape=(None, src_vocab), name='enc_inputs')
# Add an embedding layer to process the integer encoded words to give some 'sense' before the LSTM layer
encoder_embedding = Embedding(src_vocab, latent_dim, name='enc_embedding')(encoder_inputs)
# The return_state constructor argument configures a RNN layer to return a list where the first entry is the outputs
# and the next entries are the internal RNN states. This is used to recover the states of the encoder.
encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True, name='encoder_lstm')(encoder_embedding)
# We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# Set up the decoder, using `encoder_states` as initial state of the RNN.
decoder_inputs = Input(shape=(None, target_vocab), name='dec_inputs')
decoder_embedding = Embedding(target_vocab, latent_dim, name='dec_embedding')(decoder_inputs)
# The return_sequences constructor argument, configuring a RNN to return its full sequence of outputs (instead of
# just the last output, which the defaults behavior).
decoder_lstm = LSTM(latent_dim, return_sequences=True, name='dec_lstm')(decoder_embedding, initial_state=encoder_states)
decoder_outputs = Dense(target_vocab, activation='softmax', name='dec_outputs')(decoder_lstm)
# Put the model together
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
</code></pre>

<p>but I get </p>

<pre><code>ValueError: Input 0 is incompatible with layer encoder_lstm: expected ndim=3, found ndim=4
</code></pre>

<p>on the line </p>

<pre><code>encoder_outputs, state_h, state_c = LSTM(...
</code></pre>

<p>What am I missing? Or is the example on the blog assuming a step that I've skipped? </p>

<p><strong>Update:</strong></p>

<p>And I'm training with:</p>

<pre><code>X = [source_data, target_data]
y = offset_data(target_data)
model.fit(X, y, ...)
</code></pre>

<p><strong>Update 2</strong>:</p>

<p>So, I'm still not quite there. I have my <code>decoder_lstm</code> and <code>decoder_outputs</code> defined like above and have fixed the inputs. When I load my model from an <code>h5</code> file and build my inference model, I try and connect to the training <code>model</code> with </p>

<pre><code>decoder_inputs = model.input[1]  # dec_inputs (Input(shape=(None,)))
# decoder_embedding = model.layers[3]  # dec_embedding (Embedding(target_vocab, latent_dim)) 
target_vocab = model.output_shape[2]
decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')  # named to avoid conflict
decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
# Use decoder_lstm from the training model
# decoder_lstm = LSTM(latent_dim, return_sequences=True)
decoder_lstm = model.layers[5] # dec_lstm
decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)
</code></pre>

<p>but I get an error</p>

<pre><code>ValueError: Input 0 is incompatible with layer dec_lstm: expected ndim=3, found ndim=2
</code></pre>

<p>Trying to pass <code>decoder_embedding</code> rather than <code>decoder_inputs</code> fails too.</p>

<p>I'm trying to adapt the example of <a href=""https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq_restore.py"" rel=""nofollow noreferrer"">lstm_seq2seq_restore.py</a> but it doesn't include the complexity of the embedding layer.</p>

<p><strong>Update 3</strong>:</p>

<p>When I use <code>decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, ...)</code> to build the inference model I've confirmed that <code>decoder_embedding</code> is an object of type <code>Embedding</code> but I get:</p>

<pre><code>ValueError: Layer dec_lstm was called with an input that isn't a symbolic tensor. Received type: &lt;class 'keras.layers.embeddings.Embedding'&gt;. Full input: [&lt;keras.layers.embeddings.Embedding object at 0x1a1f22eac8&gt;, &lt;tf.Tensor 'input_3:0' shape=(?, 256) dtype=float32&gt;, &lt;tf.Tensor 'input_4:0' shape=(?, 256) dtype=float32&gt;]. All inputs to the layer should be tensors.
</code></pre>

<p>The full code for this model is on <a href=""https://bitbucket.org/niccottrell/nmt-keras/src/master/models/attention2.py"" rel=""nofollow noreferrer"">Bitbucket</a>.</p>
","python, machine-learning, keras, lstm, word-embedding","<p>The problem is in the input shape of <code>Input</code> layer. An embedding layer accepts a sequence of integers as input which corresponds to words indices in a sentence. Since here the number of words in sentences is not fixed, therefore you must set the input shape of <code>Input</code> layer as <code>(None,)</code>. </p>

<p>I think you are mistaking it with the case that we don't have an Embedding layer in our model and therefore the input shape of the model is <code>(timesteps, n_features)</code> to make it compatible with LSTM layer. </p>

<p><strong>Update:</strong></p>

<p>You need to pass the <code>decoder_inputs</code> to the Embedding layer first and then pass the resulting output tensor to the <code>decoder_lstm</code> layer like this:</p>

<pre><code>decoder_inputs = model.input[1] # (Input(shape=(None,)))
# pass the inputs to the embedding layer
decoder_embedding = model.get_layer(name='dec_embedding')(decoder_inputs) 

# ...

decoder_lstm = model.get_layer(name='dec_lstm') # dec_lstm
decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, ...)
</code></pre>

<p><strong>Update 2:</strong></p>

<p>In training time, when creating the <code>decoder_lstm</code> layer you need to set <code>return_state=True</code>:</p>

<pre><code>decoder_lstm, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True, name='dec_lstm')(decoder_embedding, initial_state=encoder_states)
</code></pre>
",1,0,583,2018-08-13 20:04:46,https://stackoverflow.com/questions/51829810/keras-example-word-level-model-with-integer-sequences-gives-expected-ndim-3-fo
how to assign weights to articles in the corpus for generating word embedding (e.g. word2vec)?,"<p>there are certain articles in the corpus that I found much more important than other articles (for instance I like their wording more). As a result, I would like to increase their ""weights"" in the entire corpus during the process of generating word vectors. Is there a way to implement this? The current solution that I can think of is to copy the more important articles multiple times, and add them to the corpus. However, will this work for the word embedding process? And is there a better way to achieve this? Many thanks!</p>
","word2vec, corpus, word-embedding","<p>The word2vec library with which I am most familiar, in gensim for Python, doesn't have a feature to overweight certain texts. However, your idea of simply repeating the more important texts should work. </p>

<p>Note though that:</p>

<ul>
<li><p>it'd probably work better if the texts don't repeat consecutively in your corpus - spreading out the duplicated contexts so that they're encountered in an interleaved fashion with other diverse usage examples</p></li>
<li><p>the algorithm really benefits from diverse usage examples – repeating the same rare examples 10 times is nowhere near as good as 10 naturally-subtly-contrasting usages, to induce the kinds of continuous gradations-of-meaning that people want from word2vec</p></li>
<li><p>you should be sure to test your overweighting strategy, with a quantitative quality score related to your end purpose, to be sure it's helping as you hope. It might be extra code/training-effort for negligible benefit, or even harm some word vectors' quality.</p></li>
</ul>
",1,1,501,2018-08-15 14:08:39,https://stackoverflow.com/questions/51860339/how-to-assign-weights-to-articles-in-the-corpus-for-generating-word-embedding-e
Error when checking model input keras when predicting new results,"<p>I am trying to use a keras model I built on new data, except I have an input error when trying to predict the predictions.</p>

<p>Here's my code for the model:</p>

<pre><code>def build_model(max_features, maxlen):
    """"""Build LSTM model""""""
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length=maxlen))
    model.add(LSTM(128))
    model.add(Dropout(0.5))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    model.compile(loss='binary_crossentropy',
                  optimizer='rmsprop')

    return model
</code></pre>

<p>And my code to predict the output predictions of my new data:</p>

<pre><code>LSTM_model = load_model('LSTMmodel.h5')
data = pickle.load(open('traindata.pkl', 'rb'))


#### LSTM ####

""""""Run train/test on logistic regression model""""""

# Extract data and labels
X = [x[1] for x in data]
labels = [x[0] for x in data]

# Generate a dictionary of valid characters
valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}

max_features = len(valid_chars) + 1
maxlen = np.max([len(x) for x in X])

# Convert characters to int and pad
X = [[valid_chars[y] for y in x] for x in X]
X = sequence.pad_sequences(X, maxlen=maxlen)

# Convert labels to 0-1
y = [0 if x == 'benign' else 1 for x in labels]


y_pred = LSTM_model.predict(X)
</code></pre>

<p>The error I get when running this code:</p>

<pre><code>ValueError: Error when checking input: expected embedding_1_input to have shape (57,) but got array with shape (36,)
</code></pre>

<p>My error comes from <code>maxlen</code> because for my training data, <code>maxlen=57</code> and with my new data, <code>maxlen=36</code>. </p>

<p>So I tried to set in my prediction code <code>maxlen=57</code> but then I get this error:</p>

<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[31,53] = 38 is not in [0, 38)
     [[Node: embedding_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](embedding_1/embeddings/read, embedding_1/Cast, embedding_1/embedding_lookup/axis)]]
</code></pre>

<p>What should I do in order to resolve these issues? Change my embedding layer?</p>
","python, tensorflow, machine-learning, keras, word-embedding","<p>Either set the <code>input_length</code> of the Embedding layer to the maximum length you would see in the dataset, or just use the same <code>maxlen</code> value you used when constructing the model in <code>pad_sequences</code>. In that case any sequence shorter than <code>maxlen</code> would be padded and any sequence longer than <code>maxlen</code> would be truncated.</p>

<p>Further make sure that the features you use are the same in both train and test time (i.e. their numbers should not change).</p>
",1,1,792,2018-08-17 12:59:27,https://stackoverflow.com/questions/51896013/error-when-checking-model-input-keras-when-predicting-new-results
InvalidArgumentError Sentiment analyser with keras,"<p>I have built a sentiment analyzer using Keras as a binary classification problem. I am using the Imdb dataset using GRU.
My code is:</p>

<pre><code># coding=utf-8
# ==========
#   MODEL
# ==========

# imports
from __future__ import print_function
from timeit import default_timer as timer
from datetime import timedelta
from keras.models import Sequential
from keras.preprocessing import sequence
from keras import regularizers
from keras.layers import Dense, Embedding
from keras.layers import GRU, LeakyReLU, Bidirectional
from keras.datasets import imdb

#start a timer
start = timer()

# Hyperparameters
Model_Name = 'my_model.h5'
vocab_size = 5000
maxlen = 1000
batch_size = 512
hidden_layer_size = 2
test_split = 0.3
dropout = 0.1
num_epochs = 1
alpha = 0.2
validation_split = 0.25
l1 = 0.01
l2 = 0.01

# Dataset loading
print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(path=""imdb.npz"",
                                                      maxlen=maxlen)

print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

# Data preprocessing
# Sequence padding
print('Pad sequences (samples x time)')
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

# Network building
print('Build model...')
model = Sequential()
model.add(Embedding(vocab_size, hidden_layer_size))
model.add(Bidirectional(GRU(hidden_layer_size, kernel_initializer='uniform', kernel_regularizer=regularizers.l1_l2(l1=l1,l2=l2), dropout=dropout, recurrent_dropout=dropout,return_sequences=True)))
model.add(LeakyReLU())
model.add(Bidirectional(GRU(hidden_layer_size, kernel_initializer='uniform', dropout=dropout, kernel_regularizer=regularizers.l1_l2(l1=l1,l2=l2), recurrent_dropout=dropout)))
model.add(LeakyReLU())
model.add(Dense(1, activation='softmax', kernel_initializer='uniform', kernel_regularizer=regularizers.l1_l2(l1=l1,l2=l2)))

# Compile my model
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

print('Train...')

# Fit the model
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=validation_split)
score, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)

# Create a summary, a plot and print the scores of the model
model.summary()

print('Test score:', score)
print('Test accuracy:', acc)

# Save model architecture, weights, training configuration (loss,optimizer),
# and also the state of the optimizer, so you can resume where you stopped
model.save(Model_Name)
end = timer()
print('Running time:  ' + str(timedelta(seconds=(end - start))) + '  in Hours:Minutes:Seconds')
</code></pre>

<p>I keep receiving an Error message which I don't completely understand:</p>

<pre><code>InvalidArgumentError (see above for traceback): indices[502,665] = 5476 is not in [0, 5000)
[[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=/job:localhost/replica:0/task:0/device:CPU:0](embedding_1/embeddings/read, embedding_1/Cast)]]
</code></pre>

<p>Can anyone help me understand what causes this error and how to solve it?</p>
","python-3.x, keras, deep-learning, sentiment-analysis, word-embedding","<p>The error complains about a non-existent word index. That's because you are only limiting the number of Emedding features (i.e. there is a word with index 5476 which is not in the range <code>[0, 5000)</code>, which 5000 refers to the <code>vocab_size</code> you have set). To resolve this, you also need to pass the <code>vocab_size</code> as <code>num_words</code> argument of <code>load_data</code> function, like this:</p>

<pre><code>... = imdb.load_data(num_words=vocab_size, ...)
</code></pre>

<p>This way you are limiting the words to the most frequent words (i.e. top <code>vocab_size</code> words with the most frequency in the dataset) with their indices in range <code>[0, vocab_size)</code>.</p>
",2,0,50,2018-08-23 10:15:03,https://stackoverflow.com/questions/51983456/invalidargumenterror-sentiment-analyser-with-keras
Word2Vec Skipgrams - Should couples span sentences?,"<h1>Background</h1>

<p>I am trying to train a Skip-gram word2vec model using negative sampling. from what I understand I need to generate couples (target, context) and a label where 0 = not in context and 1 = in context.</p>

<h1>What I am unsure about:</h1>

<p>Should we make skipgram couples sentence by sentence? or should we flatten the sentences in to one large sentence and generate skipgrams from that? <strong>In other words, should the generated couples span sentences?</strong></p>

<p>The only difference between the two code snippets below is one of them generates couples that span the two sentences like so:</p>

<pre><code>data = ['this is some stuff.', 'I have a cookie.']
</code></pre>

<p><strong>results</strong>:</p>

<pre><code>...SNIP...
[some, have]
[stuff, this]
[stuff, is]
[stuff, some]
[stuff, i]
[stuff, have]
[stuff, a]
[i, is]
[i, some]
[i, stuff]
[i, have]
[i, a]
[i, cookie]
[have, some]
[have, stuff]
...SNIP...
</code></pre>

<p>We can see that there are couples that stretch across sentences</p>

<p>Or we can have couples that don't span sentences:</p>

<pre><code>...SNIP...
[some, stuff]
[stuff, this]
[stuff, is]
[stuff, some]
[i, have]
[i, a]
[i, cookie]
[have, i]
[have, a]
[have, cookie]
...SNIP...
</code></pre>

<h1>What I have done so far.</h1>

<p><strong>Get data</strong></p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
newsgroups_train = fetch_20newsgroups(subset='train',
                          remove=('headers', 'footers', 'quotes'))
</code></pre>

<p><strong>Initialize some variables</strong></p>

<pre><code>vocabulary_size = 8
window_size = 3 
neg_samples = 0.0
</code></pre>

<p><strong>Flatten the sentences into one large sequence</strong></p>

<pre><code>sents = newsgroups_train.data
tokenizer = Tokenizer(num_words= vocabulary_size, lower=True, filters=filters)
tokenizer.fit_on_texts(sents)
word_index_inv = {v: k for k, v in tokenizer.word_index.items()}
sequences = tokenizer.texts_to_sequences(sents)
couples, labels = skipgrams(list(itertools.chain.from_iterable(sequences)), vocabulary_size=vocabulary_size, window_size=window_size, shuffle=False, negative_samples=neg_samples)
word_target, word_context = zip(*couples)
word_target = np.array(word_target, dtype=""int32"")
word_context = np.array(word_context, dtype=""int32"")
</code></pre>

<h1>OR:</h1>

<p><strong>Split dataset into sentences and generate couples based off each sentence.</strong></p>

<pre><code>sents = [nltk.sent_tokenize(s) for s in newsgroups_train.data]
sents = list(itertools.chain.from_iterable(sents))

tokenizer = Tokenizer(num_words= vocabulary_size, lower=True, filters=filters)
tokenizer.fit_on_texts(sents)
word_index_inv = {v: k for k, v in tokenizer.word_index.items()}
sequences = tokenizer.texts_to_sequences(sents)    
couples = []
labels = []
for seq in sequences:
    c,l = skipgrams(seq, vocabulary_size=vocabulary_size, 
            window_size=window_size, shuffle=False, 
            negative_samples=neg_samples)
    couples.extend(c)
    labels.extend(l)
word_target, word_context = zip(*couples)
word_target = np.array(word_target, dtype=""int32"")
word_context = np.array(word_context, dtype=""int32"")
</code></pre>

<p><strong>Print out our words</strong></p>

<pre><code>for couple in couples:
    print('[{}, {}]'.format(word_index_inv[couple[0]], word_index_inv[couple[1]]))
</code></pre>
","python, keras, word2vec, word-embedding","<p>Usually it doesn't matter that much either way. </p>

<p>Even when libraries/APIs talk about ""sentences"" they really mean ""texts"" which might be multiple sentences.</p>

<p>The worst case, if you wind up with some contexts spanning texts that have no essential relation, is that it adds a little noise to training... which might require a little more training to reach optimality on the other non-noise contexts. But often run-together texts are actually related, from the same original source, so such contexts might still capture real useful patterns, and thus be a net positive compared to smaller text fragments. </p>

<p>You could try it both ways and score the results against each other to see if one or the other is better with your corpus and end task. </p>
",1,0,345,2018-08-27 22:14:28,https://stackoverflow.com/questions/52047625/word2vec-skipgrams-should-couples-span-sentences
Using pretrained gensim Word2vec embedding in keras,"<p>I have trained word2vec in gensim. In Keras, I want to use it to make matrix of sentence using that word embedding. As storing the matrix of all the sentences is very space and memory inefficient. So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?</p>

<p>PS: It is different from other questions because I am using gensim for word2vec training instead of keras.</p>
","python, keras, gensim, word2vec, word-embedding","<p>Let's say you have following data that you need  to encode</p>

<pre><code>docs = ['Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!',
        'Weak',
        'Poor effort!',
        'not good',
        'poor work',
        'Could have done better.']
</code></pre>

<p>You must then tokenize it using the <code>Tokenizer</code> from Keras like this and find the <code>vocab_size</code></p>

<pre><code>t = Tokenizer()
t.fit_on_texts(docs)
vocab_size = len(t.word_index) + 1
</code></pre>

<p>You can then enocde it to sequences like this</p>

<pre><code>encoded_docs = t.texts_to_sequences(docs)
print(encoded_docs)
</code></pre>

<p>You can then pad the sequences so that all the sequences are of a fixed length</p>

<pre><code>max_length = 4
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
</code></pre>

<p>Then use the word2vec model to make embedding matrix </p>

<pre><code># load embedding as a dict
def load_embedding(filename):
    # load embedding into memory, skip first line
    file = open(filename,'r')
    lines = file.readlines()[1:]
    file.close()
    # create a map of words to vectors
    embedding = dict()
    for line in lines:
        parts = line.split()
        # key is string word, value is numpy array for vector
        embedding[parts[0]] = asarray(parts[1:], dtype='float32')
    return embedding

# create a weight matrix for the Embedding layer from a loaded embedding
def get_weight_matrix(embedding, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = zeros((vocab_size, 100))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = embedding.get(word)
    return weight_matrix

# load embedding from file
raw_embedding = load_embedding('embedding_word2vec.txt')
# get vectors in the right order
embedding_vectors = get_weight_matrix(raw_embedding, t.word_index)
</code></pre>

<p>Once you have the embedding matrix you can use it in <code>Embedding</code> layer like this</p>

<pre><code>e = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=4, trainable=False)
</code></pre>

<p>This layer can be used in making a model like this</p>

<pre><code>model = Sequential()
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)
model.add(e)
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())
# fit the model
model.fit(padded_docs, labels, epochs=50, verbose=0)
</code></pre>

<p>All the codes are adapted from <a href=""https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"" rel=""noreferrer"">this</a> awesome blog post. follow it to know more about Embeddings using Glove</p>

<p>For using word2vec see <a href=""https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"" rel=""noreferrer"">this</a> post</p>
",18,12,16274,2018-09-01 08:53:27,https://stackoverflow.com/questions/52126539/using-pretrained-gensim-word2vec-embedding-in-keras
Use pretrained embedding in Spanish with Torchtext,"<p>I am using Torchtext in an NLP project. I have a pretrained embedding in my system, which I'd like to use. Therefore, I tried:</p>
<pre><code>my_field.vocab.load_vectors(my_path)
</code></pre>
<p>But, apparently, this only accepts the names of a short list of pre-accepted embeddings, for some reason. In particular, I get this error:</p>
<pre><code>Got string input vector &quot;my_path&quot;, but allowed pretrained vectors are ['charngram.100d', 'fasttext.en.300d', ..., 'glove.6B.300d']
</code></pre>
<p>I found some <a href=""https://github.com/pytorch/text/issues/201"" rel=""nofollow noreferrer"">people with similar problems</a>, but the solutions I can find so far are &quot;change Torchtext source code&quot;, which I would rather avoid if at all possible.</p>
<p>Is there any other way in which I can work with my pretrained embedding? A solution that allows to use another Spanish pretrained embedding is acceptable.</p>
<p>Some people seem to think it is not clear what I am asking. So, if the title and final question are not enough: &quot;I need help using a pre-trained Spanish word-embedding in Torchtext&quot;.</p>
","nlp, deep-learning, pytorch, word-embedding, torchtext","<p>It turns out there is a relatively simple way to do this without changing Torchtext's source code. Inspiration from this <a href=""https://github.com/pytorch/text/issues/30"" rel=""nofollow noreferrer"">Github thread</a>.</p>

<p><strong>1. Create numpy word-vector tensor</strong></p>

<p>You need to load your embedding so you end up with a numpy array with dimensions (number_of_words, word_vector_length):</p>

<p>my_vecs_array[word_index] should return your corresponding word vector.</p>

<p>IMPORTANT. The indices (word_index) for this array array MUST be taken from Torchtext's word-to-index dictionary (field.vocab.stoi). Otherwise Torchtext will point to the wrong vectors!</p>

<p>Don't forget to convert to tensor:</p>

<pre><code>my_vecs_tensor = torch.from_numpy(my_vecs_array)
</code></pre>

<p><strong>2. Load array to Torchtext</strong></p>

<p>I don't think this step is really necessary because of the next one, but it allows to have the Torchtext field with both the dictionary and vectors in one place.</p>

<pre><code>my_field.vocab.set_vectors(my_field.vocab.stoi, my_vecs_tensor, word_vectors_length)
</code></pre>

<p><strong>3. Pass weights to model</strong></p>

<p>In your model you will declare the embedding like this:</p>

<pre><code>my_embedding = toch.nn.Embedding(vocab_len, word_vect_len)
</code></pre>

<p>Then you can load your weights using:</p>

<pre><code>my_embedding.weight = torch.nn.Parameter(my_field.vocab.vectors, requires_grad=False)
</code></pre>

<p>Use requires_grad=True if you want to train the embedding, use False if you want to freeze it.</p>

<p>EDIT: It looks like there is <a href=""https://www.innoq.com/en/blog/handling-german-text-with-torchtext/"" rel=""nofollow noreferrer"">another way</a> that looks a bit easier! The improvement is that apparently you can pass the pre-trained word vectors directly during the vocabulary-building step, so that takes care of steps 1-2 here.</p>
",5,0,2850,2018-09-07 14:23:18,https://stackoverflow.com/questions/52224555/use-pretrained-embedding-in-spanish-with-torchtext
Calculation of Cosine Similarity of a single word in 2 different Word2Vec Models,"<p>I build two word embedding (word2vec models) using <code>gensim</code> and save it as (word2vec1 and word2vec2) by using the <code>model.save(model_name)</code> command for two different corpus (the two corpuses are somewhat similar, similar means they are related like part 1 and part 2 of a book). Suppose, the top words (in terms of frequency or occurrence) for the two corpuses is the same word (let's say it as <code>a</code>). </p>

<p>How to compute the degree of similarity (<code>cosine-similarity or similarity</code>) of the extracted top word (say 'a'), for the two word2vec models? Does <code>most_similar()</code> will work in this case efficiently? </p>

<p>I want to know by how much degree of similarity, does the same word (a), is related for two different generated models?</p>

<p>Any idea is deeply appreciated.</p>
","python-3.x, gensim, word2vec, word-embedding","<p>You seem to have the wrong idea about word2vec. It doesn't provide one absolute vector for one word. It manages to find a representation for a word relative to other words. So, for the same corpus, if you run word2vec twice, you will get 2 different vectors for the same word. The meaning comes in when you compare it relative to other word vectors. </p>

<p><code>king</code> - <code>man</code> will always be close(cosine similarity wise) to <code>queen</code> - <code>woman</code> no matter how many time you train it. But they will have different vectors after each train.</p>

<p>In your case, since the 2 models are trained differently, comparing vectors of the same word is the same as comparing two random vectors. You should rather compare the relative relations. Maybe something like: <code>model1.most_similar('dog')</code> vs <code>model2.most_similar('dog')</code></p>

<p>However, to answer your question, if you wanted to compare the 2 vectors, you could do it as below. But the results will be meaningless.</p>

<p>Just take the vectors from each model and manually calculate cosine similarity.</p>

<pre><code>vec1 = model1.wv['computer']
vec2 = model2.wv['computer']
print(np.sum(vec1*vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))
</code></pre>
",5,1,2956,2018-09-11 13:43:28,https://stackoverflow.com/questions/52277384/calculation-of-cosine-similarity-of-a-single-word-in-2-different-word2vec-models
"How does Keras 1d convolution layer work with word embeddings - text classification problem? (Filters, kernel size, and all hyperparameter)","<p>I am currently developing a text classification tool using Keras. It works (it works fine and I got up to 98.7 validation accuracy) but I can't wrap my head around about how exactly 1D-convolution layer works with text data.</p>

<p>What hyper-parameters should I use?</p>

<p>I have the following sentences (input data):</p>

<ul>
<li>Maximum words in the sentence: 951 (if it's less - the paddings are added)</li>
<li>Vocabulary size: ~32000</li>
<li>Amount of sentences (for training): 9800</li>
<li>embedding_vecor_length: 32 (how many relations each word has in word embeddings)</li>
<li>batch_size: 37 (it doesn't matter for this question)</li>
<li>Number of labels (classes): 4</li>
</ul>

<p>It's a very simple model (I have made more complicated structures but, strangely it works better - even without using LSTM):</p>

<pre><code>model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))
model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(labels_count, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
</code></pre>

<p>My main question is: What hyper-parameters should I use for Conv1D layer?</p>

<pre><code>model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))
</code></pre>

<p>If I have following input data:</p>

<ul>
<li>Max word count: 951</li>
<li>Word-embeddings dimension: 32 </li>
</ul>

<p>Does it mean that <code>filters=32</code> will only scan first 32 words completely discarding the rest (with <code>kernel_size=2</code>)? And I should set filters to 951 (max amount of words in the sentence)?</p>

<p>Examples on images:</p>

<p>So for instance this is an input data: <a href=""http://joxi.ru/krDGDBBiEByPJA"" rel=""noreferrer"">http://joxi.ru/krDGDBBiEByPJA</a></p>

<p>It's the first step of a convoulution layer (stride 2): <a href=""http://joxi.ru/Y2LB099C9dWkOr"" rel=""noreferrer"">http://joxi.ru/Y2LB099C9dWkOr</a></p>

<p>It's the second step (stride 2): <a href=""http://joxi.ru/brRG699iJ3Ra1m"" rel=""noreferrer"">http://joxi.ru/brRG699iJ3Ra1m</a></p>

<p>And if <code>filters = 32</code>, layer repeats it 32 times? Am I correct?
So I won't get to say 156-th word in the sentence, and thus this information will be lost?</p>
","python, tensorflow, keras, conv-neural-network, word-embedding","<p>I would try to explain how 1D-Convolution is applied on a sequence data. I just use the example of a sentence consisting of words but obviously it is not specific to text data and it is the same with other sequence data and timeseries.</p>
<p>Suppose we have a sentence consisting of <code>m</code> words where each word has been represented using word embeddings:</p>
<p><a href=""https://i.sstatic.net/tUhfW.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/tUhfW.png"" alt=""Input data"" /></a></p>
<p>Now we would like to apply a 1D convolution layer consisting of <code>n</code> different filters with kernel size of <code>k</code> on this data. To do so, sliding windows of length <code>k</code> are extracted from the data and then each filter is applied on each of those extracted windows. Here is an illustration of what happens (here I have assumed <code>k=3</code> and removed the bias parameter of each filter for simplicity):</p>
<p><a href=""https://i.sstatic.net/zH2sr.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/zH2sr.png"" alt=""Filters"" /></a></p>
<p>As you can see in the figure above, the response of each filter is equivalent to the result of its convolution (i.e. element-wise multiplication and then summing all the results) with the extracted window of length <code>k</code> (i.e. <code>i</code>-th to <code>(i+k-1)</code>-th words in the given sentence). Further, note that each filter has the same number of channels as the number of features (i.e. word-embeddings dimension) of the training sample (hence performing convolution, i.e. element-wise multiplication, is possible). Essentially, each filter is detecting the presence of a particular feature of pattern in a <strong>local</strong> window of training data (e.g. whether a couple of specific words exist in this window or not). After all the filters have been applied on all the windows of length <code>k</code> we would have an output of like this which is the result of convolution:</p>
<p><a href=""https://i.sstatic.net/NH7JT.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/NH7JT.png"" alt=""filters response"" /></a></p>
<p>As you can see, there are <code>m-k+1</code> windows in the figure since we have assumed that the <code>padding='valid'</code> and <code>stride=1</code> (default behavior of <a href=""https://keras.io/layers/convolutional/#conv1d"" rel=""noreferrer""><code>Conv1D</code></a> layer in Keras). The <code>stride</code> argument determines how much the window should slide (i.e. shift) to extract the next window (e.g. in our example above, a stride of 2 would extract windows of words: <code>(1,2,3), (3,4,5), (5,6,7), ...</code> instead). The <code>padding</code> argument determines whether the window should entirely consists of the words in training sample or there should be paddings at the beginning and at the end; this way, the convolution response may have the same length (i.e. <code>m</code> and not <code>m-k+1</code>) as the training sample (e.g. in our example above, <code>padding='same'</code> would extract windows of words: <code>(PAD,1,2), (1,2,3), (2,3,4), ..., (m-2,m-1,m), (m-1,m, PAD)</code>).</p>
<p>You can verify some of the things I mentioned using Keras:</p>
<pre><code>from keras import models
from keras import layers

n = 32  # number of filters
m = 20  # number of words in a sentence
k = 3   # kernel size of filters
emb_dim = 100  # embedding dimension

model = models.Sequential()
model.add(layers.Conv1D(n, k, input_shape=(m, emb_dim)))

model.summary()
</code></pre>
<p>Model summary:</p>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_2 (Conv1D)            (None, 18, 32)            9632      
=================================================================
Total params: 9,632
Trainable params: 9,632
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>As you can see the output of convolution layer has a shape of <code>(m-k+1,n) = (18, 32)</code> and the number of parameters (i.e. filters weights) in the convolution layer is equal to: <code>num_filters * (kernel_size * n_features) + one_bias_per_filter = n * (k * emb_dim) + n = 32 * (3 * 100) + 32 = 9632</code>.</p>
",31,16,8727,2018-09-16 08:52:31,https://stackoverflow.com/questions/52352522/how-does-keras-1d-convolution-layer-work-with-word-embeddings-text-classificat
What is the operation behind the word analogy in Word2vec?,"<p>According to <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">https://code.google.com/archive/p/word2vec/</a>: </p>

<blockquote>
  <p>It was recently shown that the word vectors capture many linguistic
  regularities, for example vector operations vector('Paris') -
  vector('France') + vector('Italy') results in a vector that is very
  close to vector('Rome'), and vector('king') - vector('man') +
  vector('woman') is close to vector('queen') [3, 1]. You can try out a
  simple demo by running demo-analogy.sh.</p>
</blockquote>

<p>So we can try from the supplied demo script:</p>

<pre><code>+ ../bin/word-analogy ../data/text8-vector.bin
Enter three words (EXIT to break): paris france berlin

Word: paris  Position in vocabulary: 198365

Word: france  Position in vocabulary: 225534

Word: berlin  Position in vocabulary: 380477

                                              Word              Distance
------------------------------------------------------------------------
                                           germany      0.509434
                                          european      0.486505
</code></pre>

<p>Please note that <code>paris france berlin</code> is the input hint the demo suggest. The problem is that I'm unable to reproduce this behavior if I open the same word vectors in <code>Gensim</code> and try to compute the vectors myself. For example:</p>

<pre><code>&gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format(BIGDATA, binary=True)
&gt;&gt;&gt; v = word_vectors['paris'] - word_vectors['france'] + word_vectors['berlin']
&gt;&gt;&gt; word_vectors.most_similar(np.array([v]))
[('berlin', 0.7331711649894714), ('paris', 0.6669869422912598), ('kunst', 0.4056406617164612), ('inca', 0.4025722146034241), ('dubai', 0.3934606909751892), ('natalie_portman', 0.3909246325492859), ('joel', 0.3843030333518982), ('lil_kim', 0.3784593939781189), ('heidi', 0.3782389461994171), ('diy', 0.3767407238483429)]
</code></pre>

<p>So, what is the word analogy actually doing? How should I reproduce it?</p>
","python, gensim, word2vec, word-embedding","<p>You should be clear about exactly which word-vector set you're using: different sets will have a different ability to perform well on analogy tasks. (Those trained on the tiny <code>text8</code> dataset might be pretty weak; the big <code>GoogleNews</code> set Google released would probably do well, at least under certain conditions like discarding low-frequnecy words.)</p>

<p>You're doing the wrong arithmetic for the analogy you're trying to solve. For an analogy ""A is to B as C is to ?"" often written as:</p>

<pre><code>A : B :: C : _?_
</code></pre>

<p>You begin with 'B', subtract 'A', then add 'C'. So the example:</p>

<pre><code>France : Paris :: Italy : _?_
</code></pre>

<p>...gives the formula in your excerpted text:</p>

<pre><code>wv('Paris') - wv('France') + wv('Italy`) = target_coordinates  # close-to wv('Rome')
</code></pre>

<p>And to solve instead:</p>

<pre><code>Paris : France :: Berlin : _?_
</code></pre>

<p>You would try:</p>

<pre><code>wv('France') - wv('Paris') + wv('Berlin') = target_coordinates
</code></pre>

<p>...then see what's closest to <code>target_coordinates</code>. (Note the difference in operation-ordering to your attempt.)</p>

<p>You can think of it as:</p>

<ol>
<li>start at a country-vector ('France')</li>
<li>subtract the (country&amp;capital)-vector ('Paris'). This leaves you with an interim vector that's, sort-of, ""zero"" country-ness, and ""negative"" capital-ness.</li>
<li>add another (country&amp;capital)-vector ('Berlin'). This leaves you with a result vector that's, again sort-of, ""one"" country-ness, and ""zero"" capital-ness. </li>
</ol>

<p>Note also that <code>gensim</code>'s <code>most_similar()</code> takes multiple positive and negative word-examples, to do the arithmetic for you. So you can just do:</p>

<pre><code>sims = word_vectors.most_similar(positive=['France', 'Berlin'], negative=['Paris'])
</code></pre>
",5,5,4766,2018-09-17 09:30:43,https://stackoverflow.com/questions/52364632/what-is-the-operation-behind-the-word-analogy-in-word2vec
Non English Word Embedding from English Word Embedding,"<p>How can i generate non-english (french , spanish , italian ) word embedding from english word embedding ?</p>

<p>What are the best ways to generate high quality word embedding for non - english words .</p>

<p>Words may include (samsung-galaxy-s9)</p>
","tensorflow, nlp, gensim, word-embedding, chainer","<p>For non-english words, you can try to use a bilingual dictionary to translate English words with embedding vectors.</p>

<p>You need a large corpus to generate high-quality word embeddings. For non-english, you need to add the bilingual constraints into the original w2v loss with the input of bilingual corpora.</p>

<p>You can regard the compound word as a whole word or split it according to your applications.  </p>
",0,0,644,2018-09-19 09:34:49,https://stackoverflow.com/questions/52402693/non-english-word-embedding-from-english-word-embedding
How to store word vectors embeddings?,"<p>I'm very new to <code>NLP</code> and <code>Deep Learning</code> field and want to understand that after vectorization of a whole corpus using <code>Word2Vec</code>, Do I need to store the word vector values locally?
If yes I want to make a chatbot for <code>android.</code> Can anyone please guide me for this?</p>
","deep-learning, lstm, word2vec, opennlp, word-embedding","<p>word2vec embeddings can be saved:</p>

<ul>
<li>in first layers of your deep model. It's rare approach, because in this case you can't use this word2vec for other tasks.</li>
<li>as independent file on disk. It's more viable apporach for most use cases.</li>
</ul>

<p>I'd suggest to use gensim framework for training of word2vec. Here you can learn more how to train word2vec and save them to disk: <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>Particularly, saving is performed via: </p>

<pre><code>model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
model.save(""word2vec.model"")
</code></pre>

<p>Training of chatbot is much more difficult problem. I can try to suggest you a possible workflow, but you should to clarify what type of chatbot do you have in mind? E.g. should it answer on any question (open domain)? Should it generate answers or it will have predefined answers only? </p>
",3,1,2760,2018-09-20 08:51:14,https://stackoverflow.com/questions/52421121/how-to-store-word-vectors-embeddings
Difference of Pre-Padding and Post-Padding text when preprossing different text sizes for tf.nn.embedding_lookup,"<p>I have seen two types of padding when feeding to embedding layers. </p>

<blockquote>
  <p><strong>eg:</strong></p>
  
  <p>considering two sentences:</p>
  
  <p>word1 = ""I am a dog person.""</p>
  
  <p>word2 = ""Krishni and Pradeepa both love cats.""</p>
  
  <p>word1_int = [1,2,3,4,5,6] </p>
  
  <p>word2_int = [7,8,9,10,11,12,13]</p>
  
  <p>padding both words to length = 8</p>
  
  <p><strong>padding method 1</strong>(putting 0s at the beginning)</p>
  
  <p>word1_int = [0,0,1,2,3,4,5,6] </p>
  
  <p>word2_int = [0,7,8,9,10,11,12,13]</p>
  
  <p><strong>padding method 2</strong>(putting 0s at the end)</p>
  
  <p>word1_int = [1,2,3,4,5,6,0,0] </p>
  
  <p>word2_int = [7,8,9,10,11,12,13,0]</p>
</blockquote>

<p>I am trying to do an <strong>online</strong> classification using the 20 news groups dataset. and I am currently using the 1st method to pad my text. </p>

<p><strong>Question</strong>: Is there any advantage of using the 1st method over the other one in my implementation?</p>

<p>Thank you in advance!</p>

<p>My code is shown below:</p>

<pre><code>from collections import Counter
import tensorflow as tf
from sklearn.datasets import fetch_20newsgroups
import matplotlib as mplt
mplt.use('agg') # Must be before importing matplotlib.pyplot or pylab!
import matplotlib.pyplot as plt
from string import punctuation
from sklearn.preprocessing import LabelBinarizer
import numpy as np
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')



def pre_process():
    newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))

    words = []
    temp_post_text = []
    print(len(newsgroups_data.data))

    for post in newsgroups_data.data:

        all_text = ''.join([text for text in post if text not in punctuation])
        all_text = all_text.split('\n')
        all_text = ''.join(all_text)
        temp_text = all_text.split("" "")

        for word in temp_text:
            if word.isalpha():
                temp_text[temp_text.index(word)] = word.lower()

        # temp_text = [word for word in temp_text if word not in stopwords.words('english')]
        temp_text = list(filter(None, temp_text))
        temp_text = ' '.join([i for i in temp_text if not i.isdigit()])
        words += temp_text.split("" "")
        temp_post_text.append(temp_text)

    # temp_post_text = list(filter(None, temp_post_text))

    dictionary = Counter(words)
    # deleting spaces
    # del dictionary[""""]
    sorted_split_words = sorted(dictionary, key=dictionary.get, reverse=True)
    vocab_to_int = {c: i for i, c in enumerate(sorted_split_words,1)}

    message_ints = []
    for message in temp_post_text:
        temp_message = message.split("" "")
        message_ints.append([vocab_to_int[i] for i in temp_message])


    # maximum message length = 6577

    # message_lens = Counter([len(x) for x in message_ints])AAA

    seq_length = 6577
    num_messages = len(temp_post_text)
    features = np.zeros([num_messages, seq_length], dtype=int)
    for i, row in enumerate(message_ints):
        print(features[i, -len(row):])
        features[i, -len(row):] = np.array(row)[:seq_length]
        print(features[i, -len(row):])

    lb = LabelBinarizer()
    lbl = newsgroups_data.target
    labels = np.reshape(lbl, [-1])
    labels = lb.fit_transform(labels)

    return features, labels, len(sorted_split_words)+1


def get_batches(x, y, batch_size=1):
    for ii in range(0, len(y), batch_size):
        yield x[ii:ii + batch_size], y[ii:ii + batch_size]


def plot(noOfWrongPred, dataPoints):
    font_size = 14
    fig = plt.figure(dpi=100,figsize=(10, 6))
    mplt.rcParams.update({'font.size': font_size})
    plt.title(""Distribution of wrong predictions"", fontsize=font_size)
    plt.ylabel('Error rate', fontsize=font_size)
    plt.xlabel('Number of data points', fontsize=font_size)

    plt.plot(dataPoints, noOfWrongPred, label='Prediction', color='blue', linewidth=1.8)
    # plt.legend(loc='upper right', fontsize=14)

    plt.savefig('distribution of wrong predictions.png')
    # plt.show()



def train_test():
    features, labels, n_words = pre_process()

    print(features.shape)
    print(labels.shape)

    # Defining Hyperparameters

    lstm_layers = 1
    batch_size = 1
    lstm_size = 200
    learning_rate = 0.01

    # --------------placeholders-------------------------------------

    # Create the graph object
    graph = tf.Graph()
    # Add nodes to the graph
    with graph.as_default():

        tf.set_random_seed(1)

        inputs_ = tf.placeholder(tf.int32, [None, None], name=""inputs"")
        # labels_ = tf.placeholder(dtype= tf.int32)
        labels_ = tf.placeholder(tf.float32, [None, None], name=""labels"")

        # output_keep_prob is the dropout added to the RNN's outputs, the dropout will have no effect on the calculation of the subsequent states.
        keep_prob = tf.placeholder(tf.float32, name=""keep_prob"")

        # Size of the embedding vectors (number of units in the embedding layer)
        embed_size = 300

        # generating random values from a uniform distribution (minval included and maxval excluded)
        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1),trainable=True)
        embed = tf.nn.embedding_lookup(embedding, inputs_)

        print(embedding.shape)
        print(embed.shape)
        print(embed[0])

        # Your basic LSTM cell
        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)


        # Add dropout to the cell
        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)

        # Stack up multiple LSTM layers, for deep learning
        cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)

        # Getting an initial state of all zeros
        initial_state = cell.zero_state(batch_size, tf.float32)

        outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)

        # hidden layer
        hidden = tf.layers.dense(outputs[:, -1], units=25, activation=tf.nn.relu)

        print(hidden.shape)

        logit = tf.contrib.layers.fully_connected(hidden, num_outputs=20, activation_fn=None)

        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=labels_))

        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

        saver = tf.train.Saver()

    # ----------------------------online training-----------------------------------------

    with tf.Session(graph=graph) as sess:
        tf.set_random_seed(1)
        sess.run(tf.global_variables_initializer())
        iteration = 1
        state = sess.run(initial_state)
        wrongPred = 0
        noOfWrongPreds = []
        dataPoints = []

        for ii, (x, y) in enumerate(get_batches(features, labels, batch_size), 1):

            feed = {inputs_: x,
                    labels_: y,
                    keep_prob: 0.5,
                    initial_state: state}

            embedzz = sess.run(embedding, feed_dict=feed)

            print(embedzz)


            predictions = tf.nn.softmax(logit).eval(feed_dict=feed)

            print(""----------------------------------------------------------"")
            print(""Iteration: {}"".format(iteration))

            isequal = np.equal(np.argmax(predictions[0], 0), np.argmax(y[0], 0))

            print(np.argmax(predictions[0], 0))
            print(np.argmax(y[0], 0))

            if not (isequal):
                wrongPred += 1

            print(""nummber of wrong preds: "",wrongPred)

            if iteration%50 == 0:
                noOfWrongPreds.append(wrongPred/iteration)
                dataPoints.append(iteration)

            loss, states, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)

            print(""Train loss: {:.3f}"".format(loss))
            iteration += 1

        saver.save(sess, ""checkpoints/sentiment.ckpt"")
        errorRate = wrongPred / len(labels)
        print(""ERRORS: "", wrongPred)
        print(""ERROR RATE: "", errorRate)
        plot(noOfWrongPreds, dataPoints)


if __name__ == '__main__':
    train_test()
</code></pre>

<p>This is the code sample that I am using to pad all the sentences.</p>

<pre><code>  seq_length = 6577
  num_messages = len(temp_post_text)
    features = np.zeros([num_messages, seq_length], dtype=int)
    for i, row in enumerate(message_ints):
        print(features[i, -len(row):])
        features[i, -len(row):] = np.array(row)[:seq_length]
        print(features[i, -len(row):])
</code></pre>
","python-3.x, tensorflow, machine-learning, text-classification, word-embedding","<p>Commonly, when we use LSTM or RNN's, we use the final output or the hidden state and pass it along to make predictions. You are also doing the same thing as seen in this line: </p>

<pre><code>logit = tf.contrib.layers.fully_connected(hidden, num_outputs=20, activation_fn=None)
</code></pre>

<p>Here the two methods of padding get differentiated. If you use the 2nd method of padding, post-padding, then the final hidden state would get flushed out as mostly it will be <code>0</code>, whereas by using the 1st method, we make sure that the hidden state output is correct.</p>
",1,1,4717,2018-09-20 10:36:00,https://stackoverflow.com/questions/52423147/difference-of-pre-padding-and-post-padding-text-when-preprossing-different-text
word embeddings in tensorflow (no pre_trained),"<p>I am new to tensorflow and trying to look at different examples of tensorflow to understand it better.</p>

<p>Now I have seen this line being used in many tensorflow examples without mentioning of any specific embedding algorithm being used for getting the words embeddings. </p>

<pre><code>embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))
embed = tf.nn.embedding_lookup(embeddings, input_data)
</code></pre>

<p>Here are some examples:</p>

<ul>
<li><a href=""https://github.com/Decalogue/dlnd_tv_script_generation/blob/master/dlnd_tv_script_generation.py"" rel=""nofollow noreferrer"">https://github.com/Decalogue/dlnd_tv_script_generation/blob/master/dlnd_tv_script_generation.py</a></li>
<li><a href=""https://github.com/ajmaradiaga/cervantes-text-generation/blob/master/cervants_nn.py"" rel=""nofollow noreferrer"">https://github.com/ajmaradiaga/cervantes-text-generation/blob/master/cervants_nn.py</a></li>
</ul>

<p>I understand that the first line will initialize the embedding of the words by random distribution but will the embedding vectors further be trained in the model to give more accurate representation of the words (and change the initial random values to more accurate numbers) and if yes what is the actual method being used when there is no mention of any obvious embedding methods such as using word2vec and glove inside the code (or feeding the pre_tained vectors of these methods instead of random numbers in the beginning)?</p>
","tensorflow, deep-learning, embedding, word-embedding","<p>Yes, those embeddings are trained further just like <code>weights</code> and <code>biases</code> otherwise representing words with some random values wouldn't make any sense. Those embeddings are updated while training like you would update a <code>weight</code> matrix, that is, by using optimization methods like Gradient Descent or Adam optimizer, etc.</p>

<p>When we use pre-trained embeddings like <code>word2vec</code>, they're already trained on very large datasets and are quite accurate representations of words already hence, they don't need any further training. If you are asking how those are trained, there are two main training algorithms that can be used to learn the embedding from the text; they are Continuous Bag of Words (CBOW) and Skip Grams. Explaining them completely is not possible here but I would suggest taking help from Google. <a href=""https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8"" rel=""nofollow noreferrer"">This</a> article might get you started.</p>
",0,1,259,2018-09-21 12:43:56,https://stackoverflow.com/questions/52444089/word-embeddings-in-tensorflow-no-pre-trained
LSTM network on pre trained word embedding gensim,"<p>I am new to deep learning. I am trying to make very basic LSTM network on word embedding feature. I have written the following code for the model but I am unable to run it.    </p>

<pre><code>from keras.layers import Dense, LSTM, merge, Input,Concatenate
from keras.layers.recurrent import LSTM
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten


max_sequence_size = 14
classes_num = 2

LSTM_word_1 = LSTM(100, activation='relu',recurrent_dropout = 0.25, dropout = 0.25)
lstm_word_input_1 = Input(shape=(max_sequence_size, 300))
lstm_word_out_1 = LSTM_word_1(lstm_word_input_1)


merged_feature_vectors = Dense(50, activation='sigmoid')(Dropout(0.2)(lstm_word_out_1))

predictions = Dense(classes_num, activation='softmax')(merged_feature_vectors)

my_model = Model(input=[lstm_word_input_1], output=predictions)
print my_model.summary()
</code></pre>

<p>The error I am getting is <code>ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (3019, 300)</code>. On searching, I found that people have used <code>Flatten()</code> which will compress all the 2-D features (3019,300) for the dense layer. But I am unable to fix the issue. </p>

<p>While explaining, kindly let me know how do the dimension work out.</p>

<p>Upon request:</p>

<p>My X_training had dimension issues, so I am providing the code below to clear out the confusion,</p>

<pre><code>def makeFeatureVec(words, model, num_features):
    # Function to average all of the word vectors in a given
    # paragraph
    #
    # Pre-initialize an empty numpy array (for speed)
    featureVec = np.zeros((num_features,),dtype=""float32"")
    #
    nwords = 0.
    #
    # Index2word is a list that contains the names of the words in
    # the model's vocabulary. Convert it to a set, for speed
    index2word_set = set(model.wv.index2word)
    #
    # Loop over each word in the review and, if it is in the model's
    # vocaublary, add its feature vector to the total
    for word in words:
        if word in index2word_set:
            nwords = nwords + 1.
            featureVec = np.add(featureVec,model[word])
    #
    # Divide the result by the number of words to get the average
    featureVec = np.divide(featureVec,nwords)
    return featureVec
</code></pre>

<p>I think the following code is giving 2-D numpy array as I am initializing it that way</p>

<pre><code>def getAvgFeatureVecs(reviews, model, num_features):
    # Given a set of reviews (each one a list of words), calculate
    # the average feature vector for each one and return a 2D numpy array
    #
    # Initialize a counter
    counter = 0.
    #
    # Preallocate a 2D numpy array, for speed
    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=""float32"")

    for review in reviews:

       if counter%1000. == 0.:
           print ""Question %d of %d"" % (counter, len(reviews))

       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, \
           num_features)

       counter = counter + 1.
    return reviewFeatureVecs


def getCleanReviews(reviews):
    clean_reviews = []
    for review in reviews[""question""]:
        clean_reviews.append( KaggleWord2VecUtility.review_to_wordlist( review, remove_stopwords=True ))
    return clean_reviews
</code></pre>

<p>My objective is just to use gensim pretrained model for LSTM on some comments that I have.</p>

<pre><code>trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )
</code></pre>
","python, machine-learning, deep-learning, lstm, word-embedding","<p>You should try using <code>Embedding layer</code> before LSTM layer. Also, since you have pre-trained vectors of 300-dimensions for 3019 comments, you can initialize the weights for embedding layer with this matrix.</p>

<pre><code>inp_layer = Input((maxlen,))
x = Embedding(max_features, embed_size, weights=[trainDataVecs])(x)
x = LSTM(50, dropout=0.1)(x)
</code></pre>

<p>Here, <code>maxlen</code> is the maximum length of your comments, <code>max_features</code> is the maximum number of unique words or vocabulary size of your dataset, and <code>embed_size</code> is dimensions of your vectors, which is 300 in your case.</p>

<p>Note that shape of trainDataVecs should be (max_features, embed_size), so if you have pre-trained word vectors loaded into <code>trainDataVecs</code>, this should work.</p>
",0,0,1301,2018-09-26 21:02:56,https://stackoverflow.com/questions/52525990/lstm-network-on-pre-trained-word-embedding-gensim
Merging layers on Keras (dot product),"<p>I've been following Towards Data Science's tutorial about word2vec and skip-gram models, but I stumbled upon a problem that I cannot solve, despite searching about it for hours and trying a lot of unsuccessful solutions.</p>

<p><a href=""https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"" rel=""noreferrer"">https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa</a></p>

<p>The step that it shows you how to build the skip-gram model architecture seems deprecated because of the use of the Merge layer from keras.layers.</p>

<p>I've seem many discussions about it, and the majority of answers was the you need to use the Functional API of Keras to merge layers now. But the problem is, I'm a total beginner in Keras and have no idea how to translate my code from Sequential to Functional, here's the code that the author used (and I copied):</p>

<pre><code>from keras.layers import Merge
from keras.layers.core import Dense, Reshape
from keras.layers.embeddings import Embedding
from keras.models import Sequential

# build skip-gram architecture
word_model = Sequential()
word_model.add(Embedding(vocab_size, embed_size,
                         embeddings_initializer=""glorot_uniform"",
                         input_length=1))
word_model.add(Reshape((embed_size, )))

context_model = Sequential()
context_model.add(Embedding(vocab_size, embed_size,
                  embeddings_initializer=""glorot_uniform"",
                  input_length=1))
context_model.add(Reshape((embed_size,)))

model = Sequential()
model.add(Merge([word_model, context_model], mode=""dot""))
model.add(Dense(1, kernel_initializer=""glorot_uniform"", activation=""sigmoid""))
model.compile(loss=""mean_squared_error"", optimizer=""rmsprop"")

# view model summary
print(model.summary())

# visualize model structure
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, 
rankdir='TB').create(prog='dot', format='svg'))
</code></pre>

<p>And when I run the block, the following error is shown:</p>

<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-79-80d604373468&gt; in &lt;module&gt;()
----&gt; 1 from keras.layers import Merge
      2 from keras.layers.core import Dense, Reshape
      3 from keras.layers.embeddings import Embedding
      4 from keras.models import Sequential
      5 

ImportError: cannot import name 'Merge'
</code></pre>

<p>What I'm asking here is some guidance on how to transform this Sequential into a Functional API structure.</p>
","python, tensorflow, keras, word2vec, word-embedding","<p>This did indeed change. For a dot product, you can now use the <code>dot</code> layer:</p>

<pre><code>from keras.layers import dot
...
dot_product = dot([target, context], axes=1, normalize=False)
...
</code></pre>

<p>You have to set the <code>axis</code> parameter according to your data, of course. If you set <code>normalize=True</code>, this gives the cosine proximity. For more information, see <a href=""https://keras.io/layers/merge/#dot_1"" rel=""noreferrer"">the documentation</a>.</p>

<p>To learn about the functional API to Keras, there is a good <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""noreferrer"">guide to the functional API</a> in the documentation. It's not difficult to switch if you already understand the sequential API.</p>
",7,5,11957,2018-09-27 17:30:06,https://stackoverflow.com/questions/52542275/merging-layers-on-keras-dot-product
How to classify text documents in legal domain,"<p>I've been working on a project which is about <strong>classifying text documents in the legal domain</strong> <em>(Legal Judgment Prediction class of problems)</em>.<br>
The given data set consists of 700 legal documents (well balanced in two classes). After the preprocessing, which consists in applying all the best practices (such as deleting stopwords,etc.), there are 3 paragraphs for each document, which I could consider all together or separately. On average, the text documents size is 2285 words.</p>

<p>I aim to use something different from the classical n-grams model (which doesn't take into account any words order or semantic) :</p>

<ul>
<li>Using a Neural Network (<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2Vec</a>) for transforming <strong>the text of each document into a vector</strong> in a continuous domain; in order to create a dataset with the vectors, representing the documents, and the corresponding labels (as I said there are 2 possible labels: 0 or 1);</li>
<li>Training a SVM for classifying the samples, I've been using a 10-fold cross-validation.</li>
</ul>

<p>I was wondering if there's someone who has some experience in this particular domain, who can suggest me other ways or how to improve the model since I'm not getting particularly good results: 74% accuracy. </p>

<p><strong>Is it correct using Doc2Vec for transforming text into vectors and using them for feeding a classifier?</strong></p>

<p>My model represantation:</p>

<p><a href=""https://i.sstatic.net/UKXE4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UKXE4.png"" alt=""enter image description here""></a></p>
","python, svm, text-classification, word-embedding, doc2vec","<p>Doc2Vec is a reasonable way to tranform a variable-length text into a summary-vector, and these vectors are often useful for classification – especially topical or sentiment classification (two applications highlighted in the original 'Paragraph Vector' paper). </p>

<p>However, 700 docs is extremely small as a training set. Published work has tended to use corpuses of tens-of-thousands to millions of documents. </p>

<p>Also, your specific classification target – predicting a legal judgment – strikes me as much harder than topical or sentiment classification. Knowing how a case will be decided depends on a large body of outside law/precedent (that's not in the training-set), and logical deductions, sometimes on individual fine points of a situation. Those are things the fuzzy-summary of a single-text-vector are unlikely to capture. </p>

<p>Against that, your reported 74% accuracy sounds downright impressive. (Would a lay person do as well, with just these summaries?) I wonder if there are certain 'tells' in the summaries – with word choices of the summarizer strongly hinting, or downright revealing, the actual judgment. If that's the strongest signal in the text (barring actual domain knowledge &amp; logical reasoning), you might get just-as-good results from a more simple n-grams/bag-of-words representation and classifier. </p>

<p>Meta-optimizing your training parameters might incrementally improve results, but I'd think you'd need a lot more data, and perhaps far more advanced learning techniques, to really approximate the kind of legally-competent human-level predictions you may be aiming for. </p>
",1,2,456,2018-10-01 12:49:39,https://stackoverflow.com/questions/52591572/how-to-classify-text-documents-in-legal-domain
What is the most efficient way to store a set of points (embeddings) such that queries for closest points are computed quickly,"<p>Given a set of embeddings, i.e. set of [name, vector representation]
how should I store it such that queries on the closest points are computed quickly. For example given 100 embeddings in 2-d space, if I query the data struct on the 5 closest points to (10,12), it returns   { [a,(9,11.5)] , [b,(12,14)],...}</p>

<p>The trivial approach is calculate all distances, sort and return top-k points. Alternatively, one might think of storing in a 2-d array in blocks/units of mXn space to cover the range of the embedding space. I don't think this is extensible to higher dimensions, but I'm willing to be corrected.</p>
","information-retrieval, embedding, word-embedding, data-retrieval","<p>There are standard approximate nearest neighbors library such as <a href=""https://github.com/facebookresearch/faiss"" rel=""nofollow noreferrer"">faiss</a>, <a href=""http://www.cs.ubc.ca/research/flann/"" rel=""nofollow noreferrer"">flann</a>, <a href=""https://github.com/tdebatty/java-LSH"" rel=""nofollow noreferrer"">java-lsh</a> etc. (which are either <a href=""https://en.wikipedia.org/wiki/Locality-sensitive_hashing"" rel=""nofollow noreferrer"">LSH</a> or <a href=""https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf"" rel=""nofollow noreferrer"">Product Quantization</a> based), which you may use.</p>

<p>The quickest solution (which I found useful) is to transform a vector of (say 100 dimensions) to a long variable (64 bits) by using the <a href=""https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma"" rel=""nofollow noreferrer"">Johnson–Lindenstrauss</a> transform. You can then use Hamming similarity (i.e. 64 minus the number of bits set in <strong>a</strong> XOR <strong>b</strong>) to compute the similarity between bit vectors <strong>a</strong> and <strong>b</strong>. You could use the <a href=""https://www.felixcloutier.com/x86/POPCNT.html"" rel=""nofollow noreferrer"">POPCOUNT</a> machine instruction to this effect (which is very fast).</p>

<p>In effect, if you use <a href=""https://www.quora.com/What-is-__builtin_popcount-in-c++"" rel=""nofollow noreferrer"">POPCOUNT</a> in C, even if you do a complete iteration over the whole set of binary transformed vectors (long variables of 64 bits), it still will be very fast. </p>
",3,2,508,2018-10-02 19:46:24,https://stackoverflow.com/questions/52615380/what-is-the-most-efficient-way-to-store-a-set-of-points-embeddings-such-that-q
How to evaluate Word2Vec model,"<p>Hi have my own corpus and I train several Word2Vec models on it.
What is the best way to evaluate them one against each-other and choose the best one? (Not manually obviously - I am looking for various measures).</p>

<p>It worth noting that the embedding is for items and not word, therefore I can't use any existing benchmarks.</p>

<p>Thanks!</p>
","python, nlp, word2vec, embedding, word-embedding","<p>There's no generic way to assess token-vector quality, if you're not even using real words against which other tasks (like the popular analogy-solving) can be tried. </p>

<p>If you have a custom ultimate task, you have to devise your own repeatable scoring method. That will likely either be some subset of your actual final task, or well-correlated with that ultimate task. Essentially, whatever ad-hoc method you may be using the 'eyeball' the results for sanity should be systematized, saving your judgements from each evaluation, so that they can be run repeatedly against iterative model improvements. </p>

<p>(I'd need more info about your data/items and ultimate goals to make further suggestions.)</p>
",9,9,10815,2018-10-04 11:22:37,https://stackoverflow.com/questions/52645459/how-to-evaluate-word2vec-model
How to use my own sentence embeddings in Keras?,"<p>I am new to Keras and I created my own tf_idf sentence embeddings with shape (no_sentences, embedding_dim). I am trying to add this matrix as input to an LSTM layer. My network looks something like this:</p>

<pre><code>q1_tfidf = Input(name='q1_tfidf', shape=(max_sent, 300))
q2_tfidf = Input(name='q2_tfidf', shape=(max_sent, 300))

q1_tfidf = LSTM(100)(q1_tfidf)
q2_tfidf = LSTM(100)(q2_tfidf)
distance2 = Lambda(preprocessing.exponent_neg_manhattan_distance, output_shape=preprocessing.get_shape)(
        [q1_tfidf, q2_tfidf])
</code></pre>

<p>I'm struggling with how the matrix should be shaped. I am getting this error: </p>

<pre><code>ValueError: Error when checking input: expected q1_tfidf to have 3 dimensions, but got array with shape (384348, 300)
</code></pre>

<p>I already checked this post: <a href=""https://stackoverflow.com/questions/45001437/sentence-embedding-in-keras"">Sentence Embedding Keras</a> but still can't figure it out. It seems like I'm missing something obvious.</p>

<p>Any idea how to do this?</p>
","keras, nlp, lstm, word-embedding, sentence-similarity","<p>Ok as far as I understood, you want to predict the difference between two sentences.
What about reusing the LSTM layer (the language model should be the same) and just learn a single sentence embedding and use it twice:</p>

<pre><code>q1_tfidf = Input(name='q1_tfidf', shape=(max_sent, 300))
q2_tfidf = Input(name='q2_tfidf', shape=(max_sent, 300))

lstm = LSTM(100)

lstm_out_q1= lstm (q1_tfidf)
lstm_out_q2= lstm (q2_tfidf)
predict = concatenate([lstm_out_q1, lstm_out_q2])
model = Model(inputs=[q1_tfidf ,q1_tfidf ], outputs=predict)

predict = concatenate([q1_tfidf , q2_tfidf])
</code></pre>

<p>You could also introduce your custom distance in an additional lambda layer, but therefore you need to use a different reshaping in concatenation.</p>
",1,2,1150,2018-10-08 14:57:02,https://stackoverflow.com/questions/52704988/how-to-use-my-own-sentence-embeddings-in-keras
Keras - Translation from Sequential to Functional API,"<p>I've been following Towards Data Science's tutorial about word2vec and skip-gram models, but I stumbled upon a problem that I cannot solve, despite searching about it a lot and trying multiple unsuccessful solutions.</p>

<p><a href=""https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"" rel=""nofollow noreferrer"">https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa</a></p>

<p>The step that it shows you how to build the skip-gram model architecture seems deprecated because of the use of the Merge layer from keras.layers.</p>

<p>What I tried to do was translate his piece of code - which is implemented in the Sequential API of Keras - to the Functional API to solve the deprecation of the Merge layer, by replacing it with the keras.layers.Dot layer. However, I'm still stuck in this step of merging the two models (word and context) into the final model, whose architecture must be like this:</p>

<p><a href=""https://i.sstatic.net/eMt8A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eMt8A.png"" alt=""Skip-gram model summary and architecture""></a></p>

<p>Here's the code that the author used:</p>

<pre><code>from keras.layers import Merge
from keras.layers.core import Dense, Reshape
from keras.layers.embeddings import Embedding
from keras.models import Sequential

# build skip-gram architecture
word_model = Sequential()
word_model.add(Embedding(vocab_size, embed_size,
                         embeddings_initializer=""glorot_uniform"",
                         input_length=1))
word_model.add(Reshape((embed_size, )))

context_model = Sequential()
context_model.add(Embedding(vocab_size, embed_size,
                  embeddings_initializer=""glorot_uniform"",
                  input_length=1))
context_model.add(Reshape((embed_size,)))

model = Sequential()
model.add(Merge([word_model, context_model], mode=""dot""))
model.add(Dense(1, kernel_initializer=""glorot_uniform"", activation=""sigmoid""))
model.compile(loss=""mean_squared_error"", optimizer=""rmsprop"")
</code></pre>

<p>And here is my attempt to translate the Sequential code implementation into the Functional one:</p>

<pre><code>from keras import models
from keras import layers
from keras import Input, Model

word_input = Input(shape=(1,))
word_x = layers.Embedding(vocab_size, embed_size, embeddings_initializer='glorot_uniform')(word_input)
word_reshape = layers.Reshape((embed_size,))(word_x)

word_model = Model(word_input, word_reshape)    

context_input = Input(shape=(1,))
context_x = layers.Embedding(vocab_size, embed_size, embeddings_initializer='glorot_uniform')(context_input)
context_reshape = layers.Reshape((embed_size,))(context_x)

context_model = Model(context_input, context_reshape)

model_input = layers.dot([word_model, context_model], axes=1, normalize=False)
model_output = layers.Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')

model = Model(model_input, model_output)
</code></pre>

<p>However, when executed, the following error is returned:</p>

<blockquote>
  <p>ValueError: Layer dot_5 was called with an input that isn't a symbolic
  tensor. Received type: . Full
  input: [,
  ]. All inputs to
  the layer should be tensors.</p>
</blockquote>

<p>I'm a total beginner to the Functional API of Keras, I will be grateful if you could give me some guidance in this situation on how could I input the context and word models into the dot layer to achieve the architecture in the image.              </p>
","python, tensorflow, keras, word2vec, word-embedding","<p>You are passing <code>Model</code> instances to the layer, however as the error suggests you need to pass Keras Tensors (i.e. outputs of layers or models) to layers in Keras. You have two option here. One is to use the <code>.output</code> attribute of the <code>Model</code> instance like this:</p>

<pre><code>dot_output = layers.dot([word_model.output, context_model.output], axes=1, normalize=False)
</code></pre>

<p>or equivalently, you can use the output tensors directly:</p>

<pre><code>dot_output = layers.dot([word_reshape, context_reshape], axes=1, normalize=False)
</code></pre>

<p>Further, you need to apply the <code>Dense</code> layer which is followed on the <code>dot_output</code> and pass instances of <code>Input</code> layer as inputs of <code>Model</code>. Therefore:</p>

<pre><code>model_output = layers.Dense(1, kernel_initializer='glorot_uniform',
                            activation='sigmoid')(dot_output)

model = Model([word_input, context_input], model_output)
</code></pre>
",3,3,1118,2018-10-10 16:05:02,https://stackoverflow.com/questions/52744467/keras-translation-from-sequential-to-functional-api
Discrepancy documentation and implementation of spaCy vectors for German words?,"<p>According to <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>spaCy's small models (all packages that end in sm) don't ship with
  word vectors, and only include context-sensitive tensors. [...]
  individual tokens won't have any vectors assigned.</p>
</blockquote>

<p>But when I use the <code>de_core_news_sm</code> model, the tokens Do have entries for <code>x.vector</code> and <code>x.has_vector=True</code>. </p>

<p>It looks like these are context_vectors, but as far as I understood the documentation only word vectors are accessible through the <code>vector</code> attribute and <code>sm</code> models should have none. Why does this work for a ""small model""?</p>
","documentation, spacy, word-embedding","<p><code>has_vector</code> behaves differently than you expect.</p>

<p>This is discussed in the comments on an <a href=""https://github.com/explosion/spaCy/issues/2523"" rel=""nofollow noreferrer"">issue</a> raised on github. The gist is, since vectors are available, it is <code>True</code>, even though those vectors are context vectors. Note that you can still use them, eg to compute similarity.</p>

<p>Quote from spaCy contributor <a href=""https://github.com/ines"" rel=""nofollow noreferrer"">Ines</a>:</p>

<blockquote>
  <p>We've been going back and forth on how the has_vector should behave in
  cases like this. There is a vector, so having it return False would be
  misleading. Similarly, if the model doesn't come with a pre-trained
  vocab, technically all lexemes are OOV.</p>
</blockquote>

<p>Version 2.1.0 has been announced to include German word vectors.</p>
",0,2,83,2018-10-17 12:44:41,https://stackoverflow.com/questions/52855178/discrepancy-documentation-and-implementation-of-spacy-vectors-for-german-words
Merging sequence embedding with Time Series Features,"<p>I am having trouble around certain aspects of the <strong>Keras</strong> implementation of <strong>LSTM</strong>. This is a description of my problem:</p>

<p>I am trying to train a model for word correctness prediction. My model has two types of inputs: </p>

<ol>
<li>A word sequence (sentence) </li>
<li>And a sequence of features vector (for each word I compute a features victor of 6).</li>
</ol>

<p><strong>e.g.</strong> </p>

<pre><code>input_1 = ['we', 'have', 'two', 'review'] 
input_2 = [
           [1.25, 0.01, 0.000787, 5.235, 0.0, 0.002091], 
           [ 0.0787, 0.02342, 5.4595, 0.002091, 0.003477, 0.0], 
           [0.371533, 0.529893, 0.371533, 0.6, 0.0194156, 0.003297],
           [0.471533, 0.635, 0.458, 0.7, 0.0194156, 0.0287]
          ] 

 gives output = [1, 1, 2, 1]
</code></pre>

<p>As each sentence in my training set has different length, I should zero-pad all of my sentences such that they all have the same length. </p>

<p>My question is how about the second input, should I do padding! and how? as they are vectors.</p>

<p><strong>Model Architecture :</strong></p>

<pre><code>input1 = Input(shape=(seq_length,), dtype='int32')
emb = Embedding(input_dim=num_words, output_dim = num_dimension, 
input_length=seq_length, weights=[embeddings], mask_zero=True,trainable=False)(input_layer)

input2 = Input(shape=(seq_length,6 ))
x = keras.layers.concatenate([emb, input2],axis=2)

lstm = LSTM(64, return_sequences=True)(x)
ackwards = LSTM(128, return_sequences=True, go_backwards=True)(x)

common = merge([forwards, backwards], mode='concat', concat_axis=-1)
out = TimeDistributed(Dense(no_targets, activation='softmax'))(lstm)
</code></pre>
","keras, lstm, word-embedding, sequence-to-sequence","<p>You are on the right track and yes you would need to pad your second input with zero rows to match the sentence lengths. Essentially it would look like this:</p>

<pre><code># Input 1
X1 = [[12, 34, 3], [6, 7, 0]] # where numbers are word indices and 0 is padding
# Input 2
X2 = [[[1.23,...,2.4], [1.24, ...], [0.6, ...]], [[3.25, ...], [2.4, ...], [0,0,0,0,0]]]
# So the padded words get zero feature vectors as well and the shapes match
</code></pre>

<p>But fear not, because you concatenate <code>emb</code> with <code>input2</code> the <code>mask_zero=True</code> also gets propagated to the concatenated vector so the LSTM actually ignores the padding from second input as well.</p>
",1,1,1080,2018-11-02 16:06:37,https://stackoverflow.com/questions/53122116/merging-sequence-embedding-with-time-series-features
Add exception in Spacy tokenizer to not break the tokens with whitespaces?,"<p>I am trying to find word similarity between a list of 5 words and a list of 3500 words.                                                                            </p>

<p>The problem that I am facing:    </p>

<p>The List of 5 words I have are as below   </p>

<pre><code> List_five =['cloud','data','machine learning','virtual server','python']
</code></pre>

<p>In the list of 3500 words, there are words like     </p>

<pre><code> List_threek =['cloud computing', 'docker installation', 'virtual server'.....]                                                                     
</code></pre>

<p>The Spacy models through their 'nlp' object seem to break the tokens in the second list into cloud, computing, docket, installation.</p>

<p>This in turn causes similar words to appear inaccurately, For example when I run the following code                                                                                </p>

<pre><code>tokens = "" "".join(List_five)
doc = nlp(tokens)

top5 = "" "".join(List_threek)
doc2 = nlp(top5)

similar_words = []
for token1 in doc:
    list_to_sort = [] 
    for token2 in doc2:
    #print(token1, token2)
        list_to_sort.append((token1.text, token2.text, token1.similarity(token2)))
</code></pre>

<p>I get results like (cloud, cloud) while I expected (cloud, cloud computing). It looks like the word 'cloud computing' is broken into two separate tokens.</p>

<p>Are there any workarounds? Any help is appreciated.</p>

<p>I would want an exception where contextually linked words like 'cloud computing' is not broken into two like 'cloud' , 'computing' but retained as 'cloud computing'</p>
","python-3.x, nlp, spacy, cosine-similarity, word-embedding","<p>Spacy also lets you do document similarity (averages word embeddings for words, but that is better than what you are doing now) - so, one way to approach this is to compare an item in list1 and list2 directly without doing it token by token. For example,</p>

<pre><code>import spacy

nlp = spacy.load('en_core_web_sm')

l1 =['cloud','data','machine learning','virtual server','python']
l2=['cloud computing', 'docker installation', 'virtual server']
for item1 in l1:
   for item2 in l2:
       print((item1, item2), nlp(item1).similarity(nlp(item2)))
</code></pre>

<p>This will print me something like:</p>

<pre><code>('cloud', 'cloud computing') 0.6696009166814865
('cloud', 'docker installation') 0.6003896898695236
('cloud', 'virtual server') 0.5484600148958506
('data', 'cloud computing') 0.3544642116905426
('data', 'docker installation') 0.4123695793059489
('data', 'virtual server') 0.4785382246303466
... and so on.
</code></pre>

<p>Is this what you want? </p>
",4,3,426,2018-11-03 07:59:16,https://stackoverflow.com/questions/53129516/add-exception-in-spacy-tokenizer-to-not-break-the-tokens-with-whitespaces
Test data giving prediction error in Keras in the model with Embedding layer,"<p>I have trained a Bi-LSTM model to find NER on a set of sentences. For this I took the different words present and I did a mapping between a word and a number and then created the Bi-LSTM model using those numbers. I then create and pickle that model object.</p>

<p>Now I get a set of new sentences containing certain words that the training model has not seen. Thus these words do not have a numeric value till now. Thus when I test it on my previously existing model, it would give an error. It is not able to find the words or features as the numeric values for those do not exist.</p>

<p>To circumvent this error I gave a new integer value to all the new words that I see.</p>

<p>However, when I load the model and test it, it gives the error that: </p>

<pre><code>InvalidArgumentError: indices[0,24] = 5444 is not in [0, 5442)   [[Node: embedding_14_16/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true,
_device=""/job:localhost/replica:0/task:0/device:CPU:0""](embedding_14_16/embeddings/read, embedding_14_16/Cast)]]
</code></pre>

<p>The training data contains 5445 words including the padding word. Thus = [0, 5444]</p>

<p>5444 is the index value I have given to the paddings in the test sentences. Not clear why it is assuming the index values to range between [0, 5442).</p>

<p>I have used the base code available on the following link: <a href=""https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm"" rel=""nofollow noreferrer"">https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm</a></p>

<p>The code:</p>

<pre><code>input = Input(shape=(max_len,))
model = Embedding(input_dim=n_words, output_dim=50
                  , input_length=max_len)(input)

model = Dropout(0.1)(model)
model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)
out = TimeDistributed(Dense(n_tags, activation=""softmax""))(model)  # softmax output layer

model = Model(input, out)
model.compile(optimizer=""rmsprop"", loss=""categorical_crossentropy"", metrics=[""accuracy""])

#number of  epochs - Also for output file naming
epoch_num=20
domain=""../data/Laptop_Prediction_Corrected""
output_file_name=domain+""_E""+str(epoch_num)+"".xlsx""

model_name=""../models/Laptop_Prediction_Corrected""
output_model_filename=model_name+""_E""+str(epoch_num)+"".sav""


history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=epoch_num, validation_split=0.1, verbose=1)
</code></pre>

<p><code>max_len</code> is the total number of words in a sentence and <code>n_words</code> is the vocab size. In the model the padding has been done using the following code where <code>n_words=5441</code>:</p>

<pre><code>X = pad_sequences(maxlen=max_len, sequences=X, padding=""post"", value=n_words)
</code></pre>

<p>The padding in the new dataset:</p>

<pre><code>max_len = 50
# this is to pad sentences to the maximum length possible
#-&gt; so all records of X will be of the same length

#X = pad_sequences(maxlen=max_len, sequences=X, padding=""post"", value=res_new_word2idx[""pad_blank""])

#X = pad_sequences(maxlen=max_len, sequences=X, padding=""post"", value=5441)
</code></pre>

<p>Not sure which of these paddings is correct?</p>

<p>However, the vocab only includes the words in the training data. When I say:</p>

<pre><code>p = loaded_model.predict(X)
</code></pre>

<p>How to use <code>predict</code> for text sentences which contain words that are not present in the initial vocab?</p>
","python, tensorflow, keras, nlp, word-embedding","<p>You can use Keras <a href=""https://keras.io/preprocessing/text/#tokenizer"" rel=""nofollow noreferrer""><code>Tokenizer</code></a> class and its methods to easily tokenize and preprocess the input data. Specify the vocab size when instantiating it and then use its <code>fit_on_texts()</code> method on the training data to construct a vocabulary based on the given texts. After that you can use its <code>text_to_sequences()</code> method to convert each text string to a list of word indices. The good thing is that only the words in the vocabulary is considered and all the other words are ignored (you can set those words to one by passing <code>oov_token=1</code> to <code>Tokenizer</code> class):</p>

<pre><code>from keras.preprocessing.text import Tokenizer

# set num_words to limit the vocabulary to the most frequent words
tok = Tokenizer(num_words=n_words)

# you can also pass an arbitrary token as `oov_token` argument 
# which will represent out-of-vocabulary words and its index would be 1
# tok = Tokenizer(num_words=n_words, oov_token='[unk]')

tok.fit_on_texts(X_train)

X_train = tok.text_to_sequences(X_train)
X_test = tok.text_to_sequences(X_test)  # use the same vocab to convert test data to sequences
</code></pre>

<p>You can optionally use <code>pad_sequences</code> function to pad them with zeros or truncate them to make them all have the same length:</p>

<pre><code>from keras.preprocessing.sequence import pad_sequences

X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)
</code></pre>

<p>Now, the vocab size would be equal to <code>n_words+1</code> if you have not used oov token or <code>n_words+2</code> if you have used it. And then you can pass the correct number to embedding layer as its <code>input_dim</code> argument (first positional argument):</p>

<pre><code>Embedding(correct_num_words, embd_size, ...)
</code></pre>
",2,0,1037,2018-11-06 09:01:27,https://stackoverflow.com/questions/53168652/test-data-giving-prediction-error-in-keras-in-the-model-with-embedding-layer
Keras autoencoder with pretrained embeddings returning incorrect number of dimensions,"<p>I have been attempting to replicate a sentence autoencoder loosely based off of <a href=""https://github.com/PacktPublishing/Deep-Learning-with-Keras/blob/master/Chapter07/sent-thoughts-rnn.py"" rel=""nofollow noreferrer"">an example from the Deep Learning with Keras book</a>.</p>

<p>I recoded the example to use an embedding layer instead of the sentence generator and to use <code>fit</code> vs. <code>fit_generator</code>. </p>

<p>My code is as follows: </p>

<pre><code>df_train_text = df['string']

max_length = 80
embedding_dim = 300
latent_dim = 512
batch_size = 64
num_epochs = 10

# prepare tokenizer
t = Tokenizer(filters='')
t.fit_on_texts(df_train_text)
word_index = t.word_index
vocab_size = len(t.word_index) + 1

# integer encode the documents
encoded_train_text = t.texts_to_matrix(df_train_text)

padded_train_text = pad_sequences(encoded_train_text, maxlen=max_length, padding='post')

padding_train_text = np.asarray(padded_train_text, dtype='int32')

embeddings_index = {}
f = open('/Users/embedding_file.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))
#Found 51328 word vectors.

embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector


embedding_layer = Embedding(vocab_size,
                            embedding_dim,
                            weights=[embedding_matrix],
                            input_length=max_length,
                            trainable=False)
inputs = Input(shape=(max_length,), name=""input"")
embedding_layer = embedding_layer(inputs)
encoder = Bidirectional(LSTM(latent_dim), name=""encoder_lstm"", merge_mode=""sum"")(embedding_layer)
decoder = RepeatVector(max_length)(encoder)
decoder = Bidirectional(LSTM(embedding_dim, name='decoder_lstm', return_sequences=True), merge_mode=""sum"")(decoder)
autoencoder = Model(inputs, decoder)
autoencoder.compile(optimizer=""adam"", loss=""mse"")


autoencoder.fit(padded_train_text, padded_train_text,
                epochs=num_epochs, 
                batch_size=batch_size,
                callbacks=[checkpoint])
</code></pre>

<p>I verified that my layer shapes are the same as those in the example, however when I try to fit my autoencoder, I get the following error:</p>

<pre><code>ValueError: Error when checking target: expected bidirectional_1 to have 3 dimensions, but got array with shape (36320, 80)
</code></pre>

<p>A few other things I tried included switching <code>texts_to_matrix</code> to <code>texts_to_sequence</code> and wrapping/not wrapping my padded strings </p>

<p>I also came across <a href=""https://stackoverflow.com/questions/44731059/keras-lstm-autoencoder-with-embedding-layer"">this post</a> which seems to indicate that I am going about this the wrong way. Is it possible to fit an autoencoder with the embedding layer as I have coded it? If not, can someone help explain the fundamental difference between what is going on with the provided example and my version? </p>

<p>EDIT: I removed the <code>return_sequences=True</code> argument in the last layer and got the following error: <code>ValueError: Error when checking target: expected bidirectional_1 to have shape (300,) but got array with shape (80,)</code></p>

<p>After updating my layer shapes are: </p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           (None, 80)                0         
_________________________________________________________________
embedding_8 (Embedding)      (None, 80, 300)           2440200   
_________________________________________________________________
encoder_lstm (Bidirectional) (None, 512)               3330048   
_________________________________________________________________
repeat_vector_8 (RepeatVecto (None, 80, 512)           0         
_________________________________________________________________
bidirectional_8 (Bidirection (None, 300)               1951200   
=================================================================
Total params: 7,721,448
Trainable params: 5,281,248
Non-trainable params: 2,440,200
_________________________________________________________________
</code></pre>

<p>Am I missing a step between the <code>RepeatVector</code> layer and the last layer of the model so that I can return a shape of (None, 80, 300) rather than the (None, 300) shape it is currently generating? </p>
","python, machine-learning, keras, deep-learning, word-embedding","<p><code>Embedding</code> layer takes as input a sequence of integers (i.e. word indices) with a shape of <code>(num_words,)</code> and gives the corresponding embeddings as output with a shape of <code>(num_words, embd_dim)</code>. So after fitting the <code>Tokenizer</code> instance on the given texts, you need to use its <code>texts_to_sequences()</code> method to transform each text to a sequence of integers:</p>

<pre><code>encoded_train_text = t.texts_to_sequences(df_train_text)
</code></pre>

<p>Further, since after padding <code>encoded_train_text</code> it would have a shape of <code>(num_samples, max_length)</code>, the output shape of the network must also have the same shape (i.e. since we are creating an autoencoder) and therefore you need to remove the <code>return_sequences=True</code> argument of last layer. Otherwise, it would give us a 3D tensor as output which does not make sense.</p>

<p>As a side note, the following line is redundant as <code>padded_train_text</code> is already a numpy array (and by the way you have not used <code>padding_train_text</code> at all):</p>

<pre><code>padding_train_text = np.asarray(padded_train_text, dtype='int32')
</code></pre>
",2,0,879,2018-11-06 21:20:07,https://stackoverflow.com/questions/53180173/keras-autoencoder-with-pretrained-embeddings-returning-incorrect-number-of-dimen
How to get token ids using spaCy (I want to map a text sentence to sequence of integers),"<p>I want to use spacy to tokenize sentences to get a sequence of integer token-ids that I can use for downstream tasks. I expect to use it something like below. Please fill in <code>???</code></p>
<pre><code>import spacy

# Load English tokenizer, tagger, parser, NER and word vectors
nlp = spacy.load('en_core_web_lg')

# Process whole documents
text = (u&quot;When Sebastian Thrun started working on self-driving cars at &quot;)

doc = nlp(text)

idxs = ???

print(idxs)
</code></pre>
<p>I want the output to be something like:</p>
<blockquote>
<p>array([ 8045, 70727, 24304, 96127, 44091, 37596, 24524, 35224, 36253])</p>
</blockquote>
<p>Preferably the integers refers to some special embedding id in <code>en_core_web_lg</code>..</p>
<p>spacy.io/usage/vectors-similarity does not give a hint what attribute in doc to look for.</p>
<p>I asked this on <a href=""https://stats.stackexchange.com/questions/376011/how-to-get-token-ids-using-spacy-i-want-to-map-a-text-sentence-to-sequence-of-i"">crossvalidated</a> but it was determined as OT. Proper terms for googling/describing this problem is also helpful.</p>
","nlp, spacy, word-embedding","<p>Solution;</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_md')
text = (u""When Sebastian Thrun started working on self-driving cars at "")

doc = nlp(text)

ids = []
for token in doc:
    if token.has_vector:
        id = nlp.vocab.vectors.key2row[token.norm]
    else:
        id = None
    ids.append(id)

print([token for token in doc])
print(ids)
#&gt;&gt; [When, Sebastian, Thrun, started, working, on, self, -, driving, cars, at]
#&gt;&gt; [71, 19994, None, 369, 422, 19, 587, 32, 1169, 1153, 41]
</code></pre>

<p>Breaking this down;</p>

<pre><code># A Vocabulary for which __getitem__ can take a chunk of text and returns a hash
nlp.vocab 
# &gt;&gt;  &lt;spacy.vocab.Vocab at 0x12bcdce48&gt;
nlp.vocab['hello'].norm # hash
# &gt;&gt; 5983625672228268878


# The tensor holding the word-vector
nlp.vocab.vectors.data.shape
# &gt;&gt; (20000, 300)

# A dict mapping hash -&gt; row in this array
nlp.vocab.vectors.key2row
# &gt;&gt; {12646065887601541794: 0,
# &gt;&gt;  2593208677638477497: 1,
# &gt;&gt;  ...}

# So to get int id of 'earth'; 
i = nlp.vocab.vectors.key2row[nlp.vocab['earth'].norm]
nlp.vocab.vectors.data[i]

# Note that tokens have hashes but may not have vector
# (Hence no entry in .key2row)
nlp.vocab['Thrun'].has_vector
# &gt;&gt; False
</code></pre>
",7,5,3891,2018-11-08 16:45:45,https://stackoverflow.com/questions/53212374/how-to-get-token-ids-using-spacy-i-want-to-map-a-text-sentence-to-sequence-of-i
Python/Gensim - What is the meaning of syn0 and syn0norm?,"<p>I know that in <em>gensims</em> <em><code>KeyedVectors</code>-model</em>, one can access the embedding matrix by the attribute <code>model.syn0</code>. There is also a <code>syn0norm</code>, which doesn't seem to work for the <em>glove</em> model I recently loaded. I think I also have seen <code>syn1</code> somewhere previously. </p>

<p>I haven't found a doc-string for this and I'm just wondering what's the logic behind this?</p>

<p>So if <code>syn0</code> is the embedding matrix, what is <code>syn0norm</code>? What would then <code>syn1</code> be and generally, what does <code>syn</code> stand for?</p>
","python, deep-learning, nlp, gensim, word-embedding","<p>These names were inherited from the original Google <code>word2vec.c</code> implementation, upon which the <code>gensim</code> <code>Word2Vec</code> class was based. (I believe <code>syn0</code> only exists in recent versions for backward-compatbility.)</p>

<p>The <code>syn0</code> array essentially holds raw word-vectors. From the perspective of the neural-network used to train word-vectors, these vectors are a 'projection layer' that can convert a one-hot encoding of a word into a dense embedding-vector of the right dimensionality. </p>

<p>Similarity operations tend to be done on the <em>unit-normalized</em> versions of the word-vectors. That is, vectors that have all been scaled to have a magnitude of 1.0. (This makes the cosine-similarity calculation easier.) The <code>syn0norm</code> array is filled with these unit-normalized vectors, the first time they're needed. </p>

<p>This <code>syn0norm</code> will be empty until either you do an operation (like <code>most_similar()</code>) that requires it, or you explicitly do an <code>init_sims()</code> call. If you explicitly do an <code>init_sims(replace=True)</code> call, you'll actually clobber the raw vectors, in-place, with the unit-normed vectors. This saves the memory that storing both vectors for every word would otherwise require. (However, some word-vector uses may still be interested in the original raw vectors of varying magnitudes, so only do this when you're sure <code>most_similar()</code> cosine-similarity operations are all you'll need.)</p>

<p>The <code>syn1</code> (or <code>syn1neg</code> in the more common case of negative-sampling training) properties, when they exist on a full model (and not for a plain <code>KeyedVectors</code> object of only word-vectors), are the model neural network's internal 'hidden' weights leading to the output nodes. They're needed during model training, but not a part of the typical word-vectors collected after training. </p>

<p>I believe the <code>syn</code> prefix is just a convention from neural-network variable-naming, likely derived from 'synapse'. </p>
",10,9,8695,2018-11-14 13:56:33,https://stackoverflow.com/questions/53301916/python-gensim-what-is-the-meaning-of-syn0-and-syn0norm
Using pre-trained word embeddings - how to create vector for unknown / OOV Token?,"<p>I wan't to add <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">pre-trained embeddings</a> to a model. But as it seems there is no <em>out-of-vocabulary (OOV)</em> token resp. no vector for unseen words existent. </p>

<p>So what can I do to handle <em>OOV-tokens</em> I come across? I have some ideas, but none of them seem to be very good:</p>

<ul>
<li><p>I could just create a random vector for this token, but ideally I'd like the vector to within the <em>logic</em> of the existing model. If I just create it randomly I'm afraid the vector accidentally could be very similar to a very frequent word like <em>'the', 'for', 'that'</em> etc. which is not my intention.</p></li>
<li><p>Or should I just initialize the vector with plain zeros instead?</p></li>
<li><p>Another idea would be averaging the token over other existing vectors. But averaging on what vectors then? On all? This doesn't seem to be very conclusive either.</p></li>
<li><p>I also thought about trying to train this vector. However this doesn't come very handy if I want to freeze the rest of the embedding during training. </p></li>
</ul>

<p><em>(A general solution is appreciated, but I wanted to add that I'm using PyTorch - just in case PyTorch already comes with a handy solution to this problem.)</em></p>

<p><strong>So what would be a good and <em>easy</em> strategy to create such a vector?</strong></p>
","neural-network, deep-learning, nlp, pytorch, word-embedding","<p>There are multiple ways you can deal with it. I don't think I can cite references about which works better.</p>
<p><strong>Non-trainable option</strong>:</p>
<ol start=""0"">
<li>Random vector as embedding</li>
<li>You can use an all-zero vector for OOV.</li>
<li>You can use the mean of all the embedding vectors, that way you avoid the risk of being away from the actual distribution.</li>
<li>Also embeddings generally come with &quot;unk&quot; vectors learned during the training you can use that.</li>
</ol>
<p><strong>Trainable Option</strong>:</p>
<p>You can declare a separate embedding vector for OOV and make it trainable keeping other embedding fixed. You might have to over-write the forward method of embedding lookup for this. You can declare a new trainable <code>Variable</code> and in the forward pass use this vector as embedding for OOV instead of doing a look-up.</p>
<hr />
<p>Addressing the comments of OP:</p>
<p>I am not sure which of the three non-trainable methods may work better and I am not sure if there is some work about this. But method 4) should be working better.</p>
<p>For trainable option, you can create a new embedding layer as below.</p>
<pre><code>class Embeddings_new(torch.nn.Module): 
    def __init__(self, dim, vocab): 
        super().__init__() 
        self.embedding = torch.nn.Embedding(vocab, dim) 
        self.embedding.weight.requires_grad = False
        # vector for oov 
        self.oov = torch.nn.Parameter(data=torch.rand(1,dim)) 
        self.oov_index = -1 
        self.dim = dim 

    def forward(self, arr): 
        N = arr.shape[0] 
        mask =  (arr==self.oov_index).long() 
        mask_ = mask.unsqueeze(dim=1).float() 
        embed =(1-mask_)*self.embedding((1-mask)*arr) + mask_*(self.oov.expand((N,self.dim))) 
        return embed 
</code></pre>
<p>Usage:</p>
<pre><code>model = Embeddings_new(10,20000)    
out = model.forward(torch.tensor([-1,-1, 100, 1, 0]))
# dummy loss
loss = torch.sum(a**2)
loss.backward()
</code></pre>
",5,4,3848,2018-11-15 09:26:29,https://stackoverflow.com/questions/53316174/using-pre-trained-word-embeddings-how-to-create-vector-for-unknown-oov-token
Using GLOVEs pretrained glove.6B.50.txt as a basis for word embeddings R,"<p>I'm trying to convert textual data into vectors using GLOVE in r. My plan was to average the word vectors of a sentence, but I can't seem to get to the word vectorization stage. I've downloaded the glove.6b.50.txt file and it's parent zip file from: <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a> and I have visited text2vec's website and tried running through their example where they load wikipedia data. But I dont think its what I'm looking for (or perhaps I am not understanding it). I'm trying to load the pretrained embeddings into a model so that if I have a sentence (say 'I love lamp') I can iterate through that sentence and turn each word into a vector that I can then average (turning unknown words into zeros) with a function like vectorize(word). How do I load the pretrained embeddings into a glove model as my corpus (and is that even what I need to do to accomplish my goal?)</p>
","r, word-embedding, text2vec, glove","<p>I eventually figured it out. The embeddings matrix is all I needed. It already has the words in their vocab as rownames, so I use those to determine the vector of each word. </p>

<p>Now I need to figure out how to update those vectors!</p>
",1,1,1458,2018-11-17 05:18:54,https://stackoverflow.com/questions/53348473/using-gloves-pretrained-glove-6b-50-txt-as-a-basis-for-word-embeddings-r
How to train a model with only an Embedding layer in Keras and no labels,"<p>I have some text without any labels. Just a bunch of text files. And I want to train an Embedding layer to map the words to embedding vectors. Most of the examples I've seen so far are like this:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense

model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
model.compile(optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics=['acc'])
model.fit(x_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(x_val, y_val))
</code></pre>

<p>They all assume that the Embedding layer is part of a bigger model which tries to predict a label. But in my case, I have no label. I'm not trying to classify anything. I just want to train the mapping from words (more precisely integers) to embedding vectors. But the <code>fit</code> method of the model, asks for <code>x_train</code> and <code>y_train</code> (as the example given above).</p>

<p>How can I train a model only with an Embedding layer and no labels?</p>

<p><strong>[UPDATE]</strong></p>

<p>Based on the answer I've got from @Daniel Möller, Embedding layer in Keras is implementing a supervised algorithm and thus cannot be trained without labels. Initially, I was thinking that it is a variation of Word2Vec and thus does not need labels to be trained. Apparently, that's not the case. Personally, I ended up using the <a href=""https://github.com/facebookresearch/fastText"" rel=""nofollow noreferrer"">FastText</a> which has nothing to do with Keras or Python.</p>
","python, machine-learning, keras, word-embedding","<p>Does it make sense to do that without a label/target?</p>

<p>How will your model decide which values in the vectors are good for anything if there is no objective?</p>

<p>All embeddings are ""trained"" for a purpose. If there is no purpose, there is no target, if there is no target, there is no training. </p>

<p>If you really want to transform words in vectors without any purpose/target, you've got two options:</p>

<ul>
<li>Make one-hot encoded vectors. You may use the Keras <code>to_categorical</code> function for that.        </li>
<li>Use a pretrained embedding. There are some available, such as glove, embeddings from Google, etc. (All of they were trained at some point for some purpose).    </li>
</ul>

<hr>

<h3>A very naive approach based on our chat, considering word distance</h3>

<p>Warning: I don't really know anything about Word2Vec, but I'll try to show how to add the rules for your embedding using some naive kind of word distance and how to use dummy ""labels"" just to satisfy Keras' way of training.</p>

<pre><code>from keras.layers import Input, Embedding, Subtract, Lambda
import keras.backend as K
from keras.models import Model

input1 = Input((1,)) #word1
input2 = Input((1,)) #word2

embeddingLayer = Embedding(...params...)

word1 = embeddingLayer(input1)
word2 = embeddingLayer(input2)

#naive distance rule, subtract, expect zero difference
word_distance = Subtract()([word1,word2])

#reduce all dimensions to a single dimension
word_distance = Lambda(lambda x: K.mean(x, axis=-1))(word_distance)

model = Model([input1,input2], word_distance)
</code></pre>

<p>Now that our model outputs directly a word distance, our labels will be ""zero"", they're not really labels for a supervised training, but they're the expected result of the model, something necessary for Keras to work.</p>

<p>We can have as loss function the <code>mae</code> (mean absolute error) or <code>mse</code> (mean squared error), for instance.</p>

<pre><code>model.compile(optimizer='adam', loss='mse')
</code></pre>

<p>And training with word2 being the word after word1:</p>

<pre><code>xTrain = entireText
xTrain1 = entireText[:-1]
xTrain2 = entireText[1:]
yTrain = np.zeros((len(xTrain1),))

model.fit([xTrain1,xTrain2], yTrain, .... more params.... ) 
</code></pre>

<p>Although this may be completely wrong regarding what Word2Vec really does, it shows the main points that are:</p>

<ul>
<li>Embedding layers don't have special properties, they're just trainable lookup tables    </li>
<li>Rules for creating an embedding should be defined by the model and expected outputs    </li>
<li>A Keras model will need ""targets"", even if those targets are not ""labels"" but a mathematical trick for an expected result.      </li>
</ul>
",5,4,5841,2018-11-18 00:31:01,https://stackoverflow.com/questions/53356849/how-to-train-a-model-with-only-an-embedding-layer-in-keras-and-no-labels
vocab size versus vector size in word2vec,"<p>I have a data with 6200 sentences(which are triplets of form ""sign_or_symptoms diagnoses Pathologic_function""), however the unique words(vocabulary) in these sentence is 181, what would be the appropriate vector size to train a model on the sentences with such low vocabulary. Is there any resource or research on appropriate vector size depending on vocabulary size?</p>
","word2vec, word-embedding","<p>The best practice is to test it against your true end-task. </p>

<p>That's an incredibly small corpus and vocabulary-size for word2vec. It might not be appropriate at all, as it gets its power from large, varied training sets. </p>

<p>But on the bright side, you can run lots of trials with different parameters very quickly!</p>

<p>You absolutely can't use a vector dimensionality as large as your vocabulary (181), or even really very close. In such a case, the model is certain to 'overfit' – just memorizing the effects of each word in isolation, with none of the necessary trading-off 'tug-of-war', forcing words to be nearer/farther to each other, that creates the special value/generality of word2vec models. </p>

<p>My very loose rule-of-thumb would be to investigate dimensionalities around the square-root of the vocabulary size. And, multiples-of-4 tend to work best in the underlying array routines (at least when performance is critical, which it might not be with such a tiny data set). So I'd try 12 or 16 dimensions first, and then explore other lower/higher values based on some quantitative quality evaluation on your real task. </p>

<p>But again, you're working with a dataset so tiny, unless your 'sentences' are actually really long, word2vec may be a very weak technique for you without more data.</p>
",1,0,1397,2018-11-20 05:44:36,https://stackoverflow.com/questions/53386911/vocab-size-versus-vector-size-in-word2vec
How to sentence embed from gensim Word2Vec embedding vectors?,"<p>I have a <code>pandas</code> dataframe containing descriptions. I would like to cluster descriptions based on meanings usign <code>CBOW</code>. My challenge for now is to document embed each row into equal dimensions vectors. At first I am training the word vectors using <code>gensim</code> as so:</p>

<pre><code>from gensim.models import Word2Vec

vocab = pd.concat((df['description'], df['more_description']))
model = Word2Vec(sentences=vocab, size=100, window=10, min_count=3, workers=4, sg=0)
</code></pre>

<p>I am however a bit confused now on how to replace the full sentences from my <code>df</code> with document vectors of equal dimensions.</p>

<p>For now, my workaround is repacing each word in each row with a vector then applying PCA dimentinality reduction to bring each vector to similar dimensions. Is there a better way of doing this though <code>gensim</code>, so that I could say something like this:</p>

<pre><code>df['description'].apply(model.vectorize)
</code></pre>
","python-3.x, gensim, word2vec, word-embedding, doc2vec","<p>I think you are looking for sentence embedding. There are a lot ways of generating sentence embedding from word embeddings. You may find this useful: <a href=""https://stats.stackexchange.com/questions/286579/how-to-train-sentence-paragraph-document-embeddings"">https://stats.stackexchange.com/questions/286579/how-to-train-sentence-paragraph-document-embeddings</a></p>
",1,1,4578,2018-11-22 12:26:22,https://stackoverflow.com/questions/53430997/how-to-sentence-embed-from-gensim-word2vec-embedding-vectors
keras Bidirectional layer using 4 dimension data,"<p>I'am designing keras model for classification based on article data.</p>

<p>I have data with 4 dimension as follows </p>

<pre><code>[batch, article_num, word_num, word embedding size]
</code></pre>

<p>and i want to feed each (word_num, word embedding) data into keras Bidirectional layer</p>

<p>in order to get result with 3 dimension as follows.</p>

<pre><code>[batch, article_num, bidirectional layer output size]
</code></pre>

<p>when i tried to feed 4 dimension data for testing like this</p>

<pre class=""lang-py prettyprint-override""><code>inp = Input(shape=(article_num, word_num, ))
# dims = [batch, article_num, word_num]

x = Reshape((article_num * word_num, ), input_shape = (article_num, word_num))(inp)
# dims = [batch, article_num * word_num]

x = Embedding(word_num, word_embedding_size, input_length = article_num * word_num)(x)
# dims = [batch, article_num * word_num, word_embedding_size]

x = Reshape((article_num , word_num, word_embedding_size), 
             input_shape = (article_num * word_num, word_embedding_size))(x)
# dims = [batch, article_num, word_num, word_embedding_size]

x = Bidirectional(CuDNNLSTM(50, return_sequences = True), 
                  input_shape=(article_num , word_num, word_embedding_size))(x)
</code></pre>

<p>and i got the error</p>

<pre><code>ValueError: Input 0 is incompatible with layer bidirectional_12: expected ndim=3, found ndim=4
</code></pre>

<p>how can i achieve this?</p>
","machine-learning, keras, lstm, recurrent-neural-network, word-embedding","<p>If you don't want it to touch the <code>article_num</code> dimension, you can try using a <code>TimeDistributed</code> wrapper. But I'm not certain that it will be compatible with bidirectional and other stuff.</p>

<pre class=""lang-py prettyprint-override""><code>inp = Input(shape=(article_num, word_num))    

x = TimeDistributed(Embedding(word_num, word_embedding_size)(x))

#option 1
#x1 shape : (batch, article_num, word_num, 50)
x1 = TimeDistributed(Bidirectional(CuDNNLSTM(50, return_sequences = True)))(x)

#option 2
#x2 shape : (batch, article_num, 50)
x2 = TimeDistributed(Bidirectional(CuDNNLSTM(50)))(x)
</code></pre>

<p>Hints: </p>

<ul>
<li>Don't use <code>input_shape</code> everywhere, you only need it at the <code>Input</code> tensor.    </li>
<li>You probably don't need any of the reshapes if you also use a <code>TimeDistributed</code> in the embedding.    </li>
<li>If you don't want <code>word_num</code> in the final dimension, use <code>return_sequences=False</code>. </li>
</ul>
",0,0,542,2018-11-26 13:13:06,https://stackoverflow.com/questions/53481899/keras-bidirectional-layer-using-4-dimension-data
"word2vec: user-level, document-level embeddings with pre-trained model","<p>I am currently developing a Twitter content-based recommender system and have a word2vec model pre-trained on 400 million tweets.</p>

<p>How would I go about using those word embeddings to create a document/tweet-level embedding and then get the user embedding based on the tweets they had posted? </p>

<p>I was initially intending on averaging those words in a tweet that had a word vector representation and then averaging the document/tweet vectors to get a user vector but I wasn't sure if this was optimal or even correct. Any help is much appreciated.</p>
","python, twitter, nlp, word2vec, word-embedding","<p>Averaging the vectors of all the words in a short text is one way to get a summary vector for the text. It often works OK as a quick baseline. (And, if all you have, is word-vectors, may be your main option.) </p>

<p>Such a representation might sometimes improve if you did a weighted average based on some other measure of relative term importance (such as TF-IDF), or used raw word-vectors (before normalization to unit-length, as pre-normalization raw magnitudes can sometimes hints at strength-of-meaning). </p>

<p>You could create user-level vectors by averaging all their texts, or by (roughly equivalently) placing all their authored words into a pseudo-document and averaging all those words together. </p>

<p>You might retain more of the variety of a user's posts, especially if their interests span many areas, by first clustering their tweets into N clusters, then modeling the user as the N centroid vectors of the clusters. Maybe even the N varies per user, based on how much they tweet or how far-ranging in topics their tweets seem to be. </p>

<p>With the original tweets, you could also train up per-tweet vectors using an algorithm like 'Paragraph Vector' (aka 'Doc2Vec' in a library like Python gensim.) But, that can have challenging RAM requirements with 400 million distinct documents. (If you have a smaller number of users, perhaps they can be the 'documents', or they could be the predicted classes of a FastText-in-classification-mode training session.)</p>
",2,1,511,2018-11-30 21:40:14,https://stackoverflow.com/questions/53565271/word2vec-user-level-document-level-embeddings-with-pre-trained-model
What is the meaning of &quot;size&quot; of word2vec vectors [gensim library]?,"<p>Assume that we have 1000 words (A1, A2,..., A1000) in a dictionary. As fa as I understand, in words embedding or word2vec method, it aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary. Is it correct to say there should be 999 dimensions in each vector, or the size of each word2vec vector should be 999?</p>

<p>But with Gensim Python, we can modify the value of ""size"" parameter for Word2vec, let's say size = 100 in this case. So what does ""size=100"" mean? If we extract the output vector of A1, denoted (x1,x2,...,x100), what do x1,x2,...,x100 represent in this case?</p>
","python, gensim, word2vec, word-embedding","<p>It is <em>not</em> the case that ""[word2vec] aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary"". </p>

<p>Rather, given a certain target dimensionality, like say 100, the Word2Vec algorithm gradually trains word-vectors of 100-dimensions to be better and better at its training task, which is predicting nearby words. </p>

<p>This iterative process tends to force words that are related to be ""near"" each other, in rough proportion to their similarity - and even further the various ""directions"" in this 100-dimensional space often tend to match with human-perceivable semantic categories. So, the famous ""wv(king) - wv(man) + wv(woman) ~= wv(queen)"" example often works because ""maleness/femaleness"" and ""royalty"" are vaguely consistent regions/directions in the space. </p>

<p>The individual dimensions, alone, don't mean anything. The training process includes randomness, and over time just does ""whatever works"". The meaningful directions are not perfectly aligned with dimension axes, but angled through all the dimensions. (That is, you're not going to find that a <code>v[77]</code> is a gender-like dimension. Rather, if you took dozens of alternate male-like and female-like word pairs, and averaged all their differences, you might find some 100-dimensional vector-dimension that is suggestive of the gender direction.)</p>

<p>You can pick any 'size' you want, but 100-400 are common values when you have enough training data. </p>
",5,2,3213,2018-12-03 05:29:29,https://stackoverflow.com/questions/53587960/what-is-the-meaning-of-size-of-word2vec-vectors-gensim-library
Should Kernel size be same as word size in 1D Convolution?,"<p>In CNN literature, it is often illustrated that  kernel size is same as size of the longest word in the vocabulary list that one has, when it sweeps across a sentence.</p>

<p>So if we use embedding to represent the text, then shouldn't the kernel size be same as the embedding dimension so that it gives the same effect as sweeping word by word?</p>

<p>I see difference sizes of kernel used, despite the word length.</p>

<p><a href=""https://i.sstatic.net/k0yAb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/k0yAb.png"" alt=""enter image description here""></a></p>
","keras, deep-learning, conv-neural-network, word-embedding","<p>Well... these are 1D convolutions, for which the kernels are 3 dimensional. </p>

<p>It's true that one of these 3 dimensions must match the embedding size (otherwise it would be pointless to have this size)    </p>

<p>These three dimensions are: </p>

<pre><code>(length_or_size, input_channels, output_channels)  
</code></pre>

<p>Where:</p>

<ul>
<li><code>length_or_size</code> (<code>kernel_size</code>): anything you want. In the picture, there are 6 different filters with sizes 4, 4, 3, 3, 2, 2, represented by the ""vertical"" dimension.    </li>
<li><code>input_channels</code> (automatically the <code>embedding_size</code>): the size of the embedding - this is somwehat mandatory (in Keras this is automatic and almost invisible), otherwise the multiplications wouldn't use the entire embedding, which is pointless. In the picture, the ""horizontal"" dimension of the filters is constantly 5 (the same as the word size - this is not a spatial dimension).   </li>
<li><code>output_channels</code> (<code>filters</code>): anything you want, but it seems the picture is talking about 1 channel only per filter, since it's totally ignored, and if represented would be something like ""depth"". </li>
</ul>

<p>So, you're probably confusing which dimensions are which. When you define a conv layer, you do:</p>

<pre><code>Conv1D(filters = output_channels, kernel_size=length_or_size)
</code></pre>

<p>While the <code>input_channels</code> come from the embedding (or the previous layer) automatically.</p>

<h2>Creating this model in Keras</h2>

<p>To create this model, it would be something like:</p>

<pre><code>sentence_length = 7
embedding_size=5

inputs = Input((sentence_length,))
out = Embedding(total_words_in_dic, embedding_size)
</code></pre>

<p>Now, supposing these filters have 1 channel only (since the image doesn't seem to consider their depth...), we can join them in pairs of 2 channels:</p>

<pre><code>size1 = 4
size2 = 3
size3 = 2
output_channels=2

out1 = Conv1D(output_channels, size1, activation=activation_function)(out)
out2 = Conv1D(output_channels, size2, activation=activation_function)(out)
out3 = Conv1D(output_channels, size3, activation=activation_function)(out)
</code></pre>

<p>Now, let's collapse the spatial dimensions and remain with the two channels:</p>

<pre><code>out1 = GlobalMaxPooling1D()(out1)
out2 = GlobalMaxPooling1D()(out2)
out3 = GlobalMaxPooling1D()(out3)
</code></pre>

<p>And create the 6 channel output:</p>

<pre><code>out = Concatenate()([out1,out2,out3])
</code></pre>

<p>Now there is a mistery jump from 6 channels to 2 channels which cannot be explained by the picture. Perhaps they're applying a Dense layer or something.......</p>

<pre><code>#????????????????
out = Dense(2, activation='softmax')(out)

model = Model(inputs, out)
</code></pre>
",2,3,5926,2018-12-03 07:47:24,https://stackoverflow.com/questions/53589471/should-kernel-size-be-same-as-word-size-in-1d-convolution
Unable to install genism on google colab,"<p>I'm trying to insall genism on google colab instance using the following command:       </p>

<pre><code>!pip install genism
</code></pre>

<p>But I'm getting an error:</p>

<blockquote>
  <p>Could not find a version that satisfies the requirement genism (from versions: )
      No matching distribution found for genism</p>
</blockquote>
","python, google-colaboratory, word-embedding","<p>Are you looking for </p>

<pre><code>!pip install gensim
</code></pre>

<p>?</p>

<p>(note spelling of ""gensim"" in my answer vs the spelling of ""genism"" in your question)</p>
",5,0,2832,2018-12-03 17:03:05,https://stackoverflow.com/questions/53598435/unable-to-install-genism-on-google-colab
"Understanding word embeddings, convolutional layer and max pooling layer in LSTMs and RNNs for NLP Text Classification","<p>Here is my input data:</p>

<pre><code>data['text'].head()

0    process however afforded means ascertaining di...
1          never occurred fumbling might mere mistake 
2    left hand gold snuff box which capered hill cu...
3    lovely spring looked windsor terrace sixteen f...
4    finding nothing else even gold superintendent ...
Name: text, dtype: object
</code></pre>

<p>And here is the one hot encoded label (multi-class classification where the number of classes = 3)</p>

<pre><code>[[1 0 0]
 [0 1 0]
 [1 0 0]
 ...
 [1 0 0]
 [1 0 0]
 [0 1 0]]
</code></pre>

<p>Here is what I think happens step by step, please correct me if I'm wrong:</p>

<ol>
<li><p>Converting my input text <code>data['text']</code> to a bag of indices (sequences)</p>

<pre><code>vocabulary_size = 20000

tokenizer = Tokenizer(num_words = vocabulary_size)
tokenizer.fit_on_texts(data['text'])
sequences = tokenizer.texts_to_sequences(data['text'])

data = pad_sequences(sequences, maxlen=50)
</code></pre></li>
</ol>

<p>What is happening is my <code>data['text'].shape</code> which is of shape <code>(19579, )</code> is being converted into an array of indices of shape <code>(19579, 50)</code>, where each word is being replaced by the index found in <code>tokenizer.word_index.items()</code></p>

<ol start=""2"">
<li><p>Loading the <code>glove 100d</code> word vector</p>

<pre><code>embeddings_index = dict()
f = open('/Users/abhishekbabuji/Downloads/glove.6B/glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print(embedding_index)
    {'the': array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,
    -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,
     0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,
    -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,
     0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,
    -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,
     0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,
     0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,
    -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,
    -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,
    -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,
    -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,
    -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,
    -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,
    -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,
     0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,
    -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32),
</code></pre></li>
</ol>

<p>So what we have now are the word vectors for every word of 100 dimensions. </p>

<ol start=""3"">
<li><p>Creating the embedding matrix using the glove word vector </p>

<pre><code>vocabulary_size = 20000
embedding_matrix = np.zeros((vocabulary_size, 100))

for word, index in tokenizer.word_index.items():
    if index &gt; vocabulary_size - 1:
        break
    else:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector
</code></pre></li>
</ol>

<p>So we now have the a <code>vector</code> of 100 dimensions for EACH of the 20000 words. The </p>

<p>And here is the architecture:</p>

<pre><code>model_glove = Sequential()
model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))
model_glove.add(Dropout(0.5))
model_glove.add(Conv1D(64, 5, activation='relu')) 
model_glove.add(MaxPooling1D(pool_size=4))
model_glove.add(LSTM(100))
model_glove.add(Dense(3, activation='softmax'))
model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model_glove.summary())
</code></pre>

<p>I get </p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_7 (Embedding)      (None, 50, 100)           2000000   
_________________________________________________________________
dropout_7 (Dropout)          (None, 50, 100)           0         
_________________________________________________________________
conv1d_7 (Conv1D)            (None, 46, 64)            32064     
_________________________________________________________________
max_pooling1d_7 (MaxPooling1 (None, 11, 64)            0         
_________________________________________________________________
lstm_7 (LSTM)                (None, 100)               66000     
_________________________________________________________________
dense_7 (Dense)              (None, 3)                 303       
=================================================================
Total params: 2,098,367
Trainable params: 98,367
Non-trainable params: 2,000,000
_________________________________________________________________
</code></pre>

<p>The input to the above architecture will be the training data</p>

<pre><code>array([[    0,     0,     0, ...,  4867,    22,   340],
       [    0,     0,     0, ...,    12,   327,  2301],
       [    0,     0,     0, ...,   255,   388,  2640],
       ...,
       [    0,     0,     0, ...,    17, 15609, 15242],
       [    0,     0,     0, ...,  9517,  9266,   442],
       [    0,     0,     0, ...,  3399,   379,  5927]], dtype=int32)
</code></pre>

<p>of shape <code>(19579, 50)</code></p>

<p>and labels as one hot encodings..</p>

<p>My trouble is understanding the following what exactly is happening to my <code>(19579, 50)</code> as it goes through each of the following lines:</p>

<pre><code>model_glove = Sequential()
model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))
model_glove.add(Dropout(0.5))
model_glove.add(Conv1D(64, 5, activation='relu')) 
model_glove.add(MaxPooling1D(pool_size=4))
</code></pre>

<p>I understand why we need <code>model_glove.add(Dropout(0.5))</code>, this is to shut down some hidden units with a probability of 0.5 to avoid the model from being overly complex. But I have no idea why we need the <code>Conv1D(64, 5, activation='relu')</code>, the <code>MaxPooling1D(pool_size=4)</code> and how this goes into my <code>model_glove.add(LSTM(100))</code> unit..</p>
","python, tensorflow, nlp, lstm, word-embedding","<p>The simplest way to understand a <code>convolution</code> is to think of it as a mapping that tells a neural network to which features (pixels in the case of image recognition, where you would use a 2D convolution; or words before or after a given word for text, where you would use a 1D convolution) are nearby. Without this, the network has no way of knowing that words just before or just after a given word are more relevant than words that are much further away. It typically also results in information being presented in a much more densely packed format, thereby greatly reducing the number of parameters (in your case down from 2 million to 30 thousand). I find that this answer explains the technicality of how it works rather well: <a href=""https://stackoverflow.com/a/52353721/141789"">https://stackoverflow.com/a/52353721/141789</a></p>

<p><code>Max pooling</code> is a method that downsamples your data. It is often used directly after convolutions and achieves two things: </p>

<ol>
<li>It again reduces the number of parameters. In your case, it will represent four values with a single value (the max of the four values). It does this by taking the first four values, then taking a ""stride"" of size four and taking the next four values etc. In other words, there will be no overlap between the pools. (This is what keras does by default, but you could also set the stride to <code>2</code> for example)</li>
<li>Secondly, because it takes the <code>max</code> value, in theory in ""sharpens"" the contrast between the pools by taking the maximum value instead of for example taking the average.</li>
</ol>

<p>Max pooling is not ""learnt""; it is just a simple arithmetic calculation. That is why the number of parameters is given as zero. The same for <code>dropout</code>.</p>

<p>An <code>LSTM</code> expects a three dimensional input of shape <code>(number of samples, number of timesteps, number of features)</code>. Having performed the previous convolution and max pooling steps you've reduced the representation of your initial embedding to <code>number of timesteps = 11</code> and <code>number of features = 64</code>. The first value <code>number of samples = None</code> is a placeholder for the <code>batch size</code> you plan to use. By initializing an LSTM with <code>100 units</code> (also known as <code>hidden states</code>) you are parameterizing the size of the ""memory"" of the LSTM: essentially the accumulation of its input, output and forget gates through time.</p>
",4,4,2941,2018-12-05 00:28:43,https://stackoverflow.com/questions/53623432/understanding-word-embeddings-convolutional-layer-and-max-pooling-layer-in-lstm
How to change the tensor shape in middle layers?,"<p>Saying I have a 2000x100 matrix, I put it into 10 dimension embedding layer, which gives me 2000x100x10 tensor. so it's 2000 examples and each example has a 100x10 matrix. and then, I pass it to a conv1d and KMaxpolling to get 2000x24 matrix, which is 2000 examples and each example has a 24 dimension vector. and now, I would like to recombine those examples before I apply another layer. I would like to combine the first 10 examples together, and such and such, so I get a tuple. and then I pass that tuple to the next layer.
My question is, Can I do that with Keras? and any idea on how to do it?   </p>
","tensorflow, keras, nlp, embedding, word-embedding","<p>The idea of using ""samples"" is that these samples should be unique and not relate to each other. </p>

<p>This is something Keras will demand from your model: if it started with 2000 samples, it must end with 2000 samples. Ideally, these samples do not talk to each other, but you can use custom layers to hack this, but only in the middle. You will need to end with 2000 samples anyway.</p>

<p>I believe you're going to end your model with 200 groups, so maybe you should already start with shape <code>(200,10,100)</code> and use <code>TimeDistributed</code> wrappers:</p>



<pre><code>inputs = Input((10,100))                       #shape (200,10,100)
out = TimeDistributed(Embedding(....))(inputs) #shape (200,10,100,10)
out = TimeDistributed(Conv1D(...))(out)        #shape (200,10,len,filters)

#here, you use your layer that will work on the groups without TimeDistributed.
</code></pre>

<p>To reshape a tensor without changing the batch size, use the <code>Reshape(newShape)</code> layer, where <code>newShape</code> does not include the first dimension (batch size).    </p>

<p>To reshape a tensor including the batch size, use a <code>Lambda(lambda x: K.reshape(x,newShape))</code> layer, where <code>newShape</code> includes the first dimension (batch size)    - Here you must remember the warning above: somewhere you will need to undo this change so you end up with the same batch size as the input. </p>
",0,0,376,2018-12-07 04:31:53,https://stackoverflow.com/questions/53663293/how-to-change-the-tensor-shape-in-middle-layers
What does the embedding layer for a network looks like?,"<p>I just start with text classification, and I got stuck in the embedding layer. If I have a batch of sequences encoded as integer corresponding to each word, what does the embedding layer looks like? Is there neurons like normal neural layer? </p>

<p>I've seen the <code>keras.layers.Embedding</code>, but after looking for the document I'm really confused about how does it works. I can understand <code>input_dim</code>, but why is <code>output_dim</code> a 2D matrix? How many weights do I have in this embedding layer?</p>

<p>I'm sorry if my question is not explained clearly, I've no experience in NLP, if this problem about word embedding is common basics in NLP, please tell me and I will check for it.</p>
","machine-learning, keras, nlp, word-embedding","<p>Embedding layer is just a trainable look-up table: it takes as input an integer index and returns as output the word embedding associated with that index:</p>

<pre><code>index |                            word embeddings
=============================================================================
  0   |  word embedding for the word with index 0 (usually used for padding)
-----------------------------------------------------------------------------
  1   |  word embedding for the word with index 1
-----------------------------------------------------------------------------
  2   |  word embedding for the word with index 2
-----------------------------------------------------------------------------
  .   |
  .   |
  .   |
-----------------------------------------------------------------------------
  N   |  word embedding for the word with index N
-----------------------------------------------------------------------------
</code></pre>

<p>It is trainable in that sense the embeddings values are not necessarily fixed and could be changed during training. The <code>input_dim</code> argument is actually the number of words (or more generally the number of distinct elements in the sequences). The <code>output_dim</code> argument specifies the dimension of each word embedding. For example in case of using <code>output_dim=100</code> each word embedding would be a vector of size 100. Further, since the input of an embedding layer is a sequence of integers (corresponding to the words in a sentence) therefore its output would have a shape of <code>(num_sequences, len_sequence, output_dim)</code>, i.e. for each integer in a sequence an embedding vector of size <code>output_dim</code> is returned.</p>

<p>As for the number of weights in an embedding layer it is very easy to calculate: there are <code>input_dim</code> unique indices and each index is associated with a word embedding of size <code>output_dim</code>. Therefore the number of weights in an embedding layer is <code>input_dim x ouput_dim</code>.  </p>
",2,1,809,2018-12-13 12:49:09,https://stackoverflow.com/questions/53762243/what-does-the-embedding-layer-for-a-network-looks-like
"Value Error problem with multicell Dimensions must be equal, but are 20 and 13","<p>I am working with python 3.6.5 and tensorflow 1.8.0
Nr of neurons are 10 at the moment, input in this example is 3 </p>

<p>I have already build a recurrent neuronal network and now wanted to improve it. I need some help!</p>

<p>Here is a little excerpt of the code to reproduce my error: You can also replace BasicRNN by LSTM or GRU to get the other messages. </p>

<pre><code>import numpy      as np
import tensorflow as tf  
batch_size = 10
nr_inputs = 3  
nr_outputs  = 4   
nr_steps = 4             
nr_layers  = 2
def  mini_batch ( Xdata, ydata, batch_size ) :  
    global  global_counter
    result      = None
    Xbatch      = np.zeros( shape=[batch_size, nr_steps,  nr_inputs],  dtype = np.float32 )
    ybatch      = np.zeros( shape=[batch_size, nr_outputs],            dtype = np.float32 )
    return  Xbatch, ybatch
X = tf.placeholder( tf.float32, [ None, nr_steps,    nr_inputs ] )
y = tf.placeholder( tf.float32, [ None, nr_outputs ]             )
neurons = tf.contrib.rnn.BasicRNNCell(num_units = 10)
neurons = tf.contrib.rnn.MultiRNNCell( [neurons] * nr_layers, state_is_tuple = True )
X_train = np.zeros( shape=[1000, nr_steps,  nr_inputs],  dtype = np.float32 )
y_train = np.zeros( shape=[1000, nr_outputs],            dtype = np.float32 )
X_test      = np.zeros( shape=[1000,  nr_steps,  nr_inputs],  dtype = np.float32 )
y_test      = np.zeros( shape=[1000,  nr_outputs],            dtype = np.float32 )
rnn_outputs, rnn_states = tf.nn.dynamic_rnn( neurons, X, dtype=tf.float32 )
logits = tf.contrib.layers.fully_connected( inputs        = rnn_states, num_outputs = nr_outputs, activation_fn = None )
xentropy    = tf.nn.sigmoid_cross_entropy_with_logits( labels = y,  logits = logits )
loss        = tf.reduce_mean( xentropy )
optimizer   = tf.train.AdamOptimizer( learning_rate = 0.01 )
training_op = optimizer.minimize( loss )
init         = tf.global_variables_initializer()
with tf.Session() as sess :
    init.run()
    global_counter = 0
    for epoch in range(100) :
        for iteration in range( 4) :
            X_batch, y_batch = mini_batch ( X_train, y_train, batch_size )
            sess.run( training_op, feed_dict={ X : X_batch,  y : y_batch } ) 
        loss_train = loss.eval( feed_dict={ X : X_batch,  y : y_batch } )
        loss_test  = loss.eval( feed_dict={ X : X_test,   y : y_test  } )
    sess.close()
</code></pre>

<p>I was trying this <code>neurons = tf.contrib.rnn.MultiRNNCell([neurons]*nr_layers, state_ist_tuple = True)</code></p>

<p>and received the error</p>

<pre><code>ValueError: Dimensions must be equal, but are 20 and 13 for 'rnn/.../MatMul1'(op 'MatMul') with input shapes [?,20], [13, 10] for a tf.contrib.rnn.BasicRNNCell(num_units = nr_neurons)

with input shapes [?,20], [13, 20] for a tf.contrib.rnn.GRUCell(num_units = nr_neurons)
</code></pre>

<p>and</p>

<pre><code>with input shapes [?,20], [13, 40] for a tf.contrib.rnn.BasicLSTMCell(num_units = nr_neurons, state_is_tuple = True)
</code></pre>

<p>is there an error in the <code>MatMul_1</code>? Has anyone ever had similar problems? 
Thank you so much!</p>
","python, tensorflow, multidimensional-array, valueerror, word-embedding","<p>Instead of using the <code>BasicRNNCell</code> instance multiple times,one instance per RNN layer should be created - for example in this way:</p>

<pre><code>neurons = [tf.contrib.rnn.BasicRNNCell(num_units=10) for _ in range(nr_layers)]
neurons = tf.contrib.rnn.MultiRNNCell( neurons, state_is_tuple = True )
</code></pre>

<p>In addition, there are other mistakes on your codes.<code>rnn_states</code> is a tuple containing cell state and hidden state, and its shape is ((None,10),(None,10)).
I assume you want to use hidden state,replace it:</p>

<pre><code>logits = tf.contrib.layers.fully_connected( inputs = rnn_states[1], num_outputs = nr_outputs, activation_fn = None )
</code></pre>

<p>That's OK!</p>
",2,1,113,2018-12-13 18:11:49,https://stackoverflow.com/questions/53767829/value-error-problem-with-multicell-dimensions-must-be-equal-but-are-20-and-13
Concatenate char embeddings and word embeddings,"<p>I want to use char sequences and word sequences as inputs. Each of them will be embedded its related vocabulary and then resulted embeddings will be concatenated. I write following code to concatenate two embeddings:</p>

<pre><code>char_model = Sequential()
char_model.add(Embedding(vocab_size, char_emnedding_dim,input_length=char_size,embeddings_initializer='random_uniform',trainable=False, input_shape=(char_size, )))

word_model = Sequential()
word_model.add(Embedding(word_vocab_size,word_embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False,input_shape=(max_length, )))

model = Sequential()
model.add(Concatenate([char_model, word_model]))
model.add(Dropout(drop_prob))
model.add(Conv1D(filters=250, kernel_size=3, padding='valid', activation='relu', strides = 1))
model.add(GlobalMaxPooling1D())
model.add(Dense(hidden_dims)) # fully connected layer
model.add(Dropout(drop_prob)) 
model.add(Activation('relu'))
model.add(Dense(num_classes)) 
model.add(Activation('softmax'))
print(model.summary())
</code></pre>

<p>When I execute the code, I have the following error:</p>

<pre><code>ValueError: This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. 
</code></pre>

<p>I defined <code>input_shape</code> for each embedding, but I still have same error. How can I concatenate two sequential model?</p>
","python, machine-learning, keras, nlp, word-embedding","<p>The problem is in this line:</p>

<pre><code>model.add(Concatenate([char_model, word_model]))
</code></pre>

<p>Let alone you are calling <code>Concatenate</code> layer wrongly, you can't have a concatenation layer in a Sequential model since it would no longer be a Sequential model by definition. Instead, use <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">Keras Functional API</a> to define such a model.</p>
",0,0,1090,2018-12-16 23:53:55,https://stackoverflow.com/questions/53807526/concatenate-char-embeddings-and-word-embeddings
Value of alpha in gensim word-embedding (Word2Vec and FastText) models?,"<p>I just want to know the effect of the value of alpha in gensim <code>word2vec</code> and <code>fasttext</code> word-embedding models? I know that alpha is the <code>initial learning rate</code> and its default value is <code>0.075</code> form Radim blog.</p>

<p>What if I change this to a bit higher value i.e. 0.5 or 0.75? What will be its effect? Does it is allowed to change the same? However, I have changed this to 0.5 and experiment on a large-sized data with D = 200, window = 15, min_count = 5, iter = 10, workers = 4 and results are pretty much meaningful for the word2vec model. However, using the fasttext model, the results are bit scattered, means less related and unpredictable high-low similarity scores.</p>

<p>Why this imprecise result for same data with two popular models with different precision? Does the value of <code>alpha</code> plays such a crucial role during building of the model?</p>

<p>Any suggestion is appreciated.</p>
","python-3.x, gensim, word2vec, word-embedding, fasttext","<p>The default starting <code>alpha</code> is <code>0.025</code> in gensim's Word2Vec implementation. </p>

<p>In the stochastic gradient descent algorithm for adjusting the model, the effective <code>alpha</code> affects how strong of a correction to the model is made after each training example is evaluated, and will decay linearly from its starting value (<code>alpha</code>) to a tiny final value (<code>min_alpha</code>) over the course of all training. </p>

<p>Most users won't need to adjust these parameters, or might only adjust them a little, after they have a reliable repeatable way of assessing whether a change improves their model on their end tasks. (I've seen starting values of <code>0.05</code> or less commonly <code>0.1</code>, but never as high as your reported <code>0.5</code>.)</p>
",5,4,4500,2018-12-17 12:36:39,https://stackoverflow.com/questions/53815402/value-of-alpha-in-gensim-word-embedding-word2vec-and-fasttext-models
"NN in Keras - expected dense_2 to have 3 dimensions, but got array with shape (10980, 3)","<p>I want to train a <strong>Neutral Network</strong> for <strong>Multi-Classification Sentiment Analysis</strong> using <strong>word embedding</strong> for tweets.</p>
<p>Here is my code:</p>
<pre><code>import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, GRU
from keras.layers.embeddings import Embedding


from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline
</code></pre>
<h1>Import the data</h1>
<pre><code>df = pd.DataFrame()
df = pd.read_csv('Tweets.csv', encoding='utf-8')
</code></pre>
<h1>clean the tweets</h1>
<pre><code>def remove_mentions(input_text):
    return re.sub(r'@\w+', '', input_text)

def remove_stopwords(input_text):
    stopwords_list = stopwords.words('english')
    whitelist = [&quot;n't&quot;, &quot;not&quot;, &quot;no&quot;]
    words = input_text.split() 
    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) &gt; 1] 
    return &quot; &quot;.join(clean_words) 

df.text = df.text.apply(remove_stopwords).apply(remove_mentions)
df.text = [tweet for tweet in df.text if type(tweet) is str]

X = df['text']
y = df['airline_sentiment']
</code></pre>
<h1>Split my data into train and test</h1>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=37)
</code></pre>
<h1>One-Hot Encode the field &quot;Sentiment&quot;</h1>
<p>Originally the labels are of type string: 'neutral', 'positive', 'negative'. So I first transform them to integer and then apply one-hot encoding:</p>
<pre><code>le = LabelEncoder()
y_train_num = le.fit_transform(y_train.values)
y_test_num = le.fit_transform(y_test.values)

nb_classes = 3
y_train = np_utils.to_categorical(y_train_num, nb_classes)
y_test = np_utils.to_categorical(y_test_num, nb_classes)
</code></pre>
<h1>Prepare for Word Embedding</h1>
<pre><code>tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(X)
max_length = max([len(tweet.split()) for tweet in X])
print(&quot;max_length=%s&quot; % (max_length))

vocab_size = len(tokenizer_obj.word_index) + 1 
print(&quot;vocab_size=%s&quot; % (vocab_size))

X_train_tokenized = tokenizer_obj.texts_to_sequences(X_train)
X_test_tokenized = tokenizer_obj.texts_to_sequences(X_test)

X_train_pad = pad_sequences(X_train_tokenized, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_tokenized, maxlen=max_length, padding='post')
</code></pre>
<h1>Define and Apply my NN Model</h1>
<pre><code>EMBEDDING_DIM = 100
    
model = Sequential()
model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    
print(model.summary())

model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)
</code></pre>
<p><strong>The reason I chose my last layer to have 3 output units is because it's a multi-classification task and I have 3 classes.</strong></p>
<p>Here is the model summary:</p>
<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 23, 100)           1488200   
_________________________________________________________________
dense_1 (Dense)              (None, 23, 8)             808       
_________________________________________________________________
dense_2 (Dense)              (None, 23, 3)             27        
=================================================================
Total params: 1,489,035
Trainable params: 1,489,035
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>When the code gets to <code>model.fit()</code>, I get the following error:</p>
<pre><code>ValueError: Error when checking target: expected dense_2 to have 3 dimensions, but got array with shape (10980, 3)
</code></pre>
<p>What am I doing wrong?</p>
","python, tensorflow, keras, neural-network, word-embedding","<p>As you can see in the output of the <code>model.summary()</code>, the model output shape is <code>(None, 23, 3)</code> whereas you want it to be <code>(None, 3)</code>. That happens because <a href=""https://stackoverflow.com/a/52092176/2099607"">the Dense layer is applied on the last axis of its input</a> and does not flatten its input automatically (if it has more than 2 dimensions). Therefore, one way to resolve this is to use a <code>Flatten</code> layer right after the <code>Embedding</code> layer:</p>

<pre><code>model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))
model.add(Flatten())
</code></pre>

<p>This way the output of the <code>Embedding</code> layer would be flattened and the following Dense layers would have 2D output.</p>

<p>As a bonus(!), you might be a able to get a better accuracy if you use a <code>LSTM</code> layer right after the <code>Embedding</code> layer:</p>

<pre><code>model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))
model.add(LSTM(32))
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))
</code></pre>

<p>However, this is not guaranteed. You must experiment and tune your model properly.</p>
",2,2,2261,2018-12-20 16:16:35,https://stackoverflow.com/questions/53872347/nn-in-keras-expected-dense-2-to-have-3-dimensions-but-got-array-with-shape-1
How do I get word embedding using CoreNlp from Stanford?,"<p>I am using CoreNlp to get the information extraction from a large text. However, its using the ""triple"" approach where a single sentence produce many output which is good, but there are some sentences that doesn't make sense. I tried to eliminate this by running another unsupervised NLP and try to utilize function in CoreNlp, yet I stuck at getting word vector form CoreNlp. Can anyone point where do I need to start searching for codes that do the word embedding in CoreNlp? Also I am newbie in java and IT. </p>

<p>There are some open libraries like glove, word2vec, text2vec, but I noticed glove already been used in CoreNlp (correct me if wrong). </p>
","java, vectorization, stanford-nlp, word-embedding","<p>since training your own model from scratch might turn out to be a time-consuming task, you could just download pretrained vectors from:
<a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a></p>

<p>however, there is an example with dl4j here that might do to trick:
<a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/glove/GloVeExample.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/glove/GloVeExample.java</a></p>
",1,0,1178,2018-12-31 09:27:04,https://stackoverflow.com/questions/53985757/how-do-i-get-word-embedding-using-corenlp-from-stanford
T-SNE visualisation on list of word vectors,"<p>I have a list of ~20k word vectors ('tuple_vectors'), with no labels, each looks like the below</p>

<pre><code>[-2.84658718e+00 -7.74899840e-01 -2.24296474e+00 -8.69364500e-01
  3.90927410e+00 -2.65316987e+00 -9.71897244e-01 -2.40408254e+00
  1.16272974e+00 -2.61649752e+00 -2.87350488e+00 -1.06603658e+00
  2.93374014e+00  1.07194626e+00 -1.86619771e+00  1.88549474e-01
 -1.31901133e+00  3.83382154e+00 -3.46174908e+00 ...
</code></pre>

<p>is there a quick, concise way to visualise using t-sne? </p>

<p>I've tried with the following</p>

<pre><code>from sklearn.manifold import TSNE

n_sne = 21060


tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results = tsne.fit_transform(tuple_vectors)
plt(tsne_results)
</code></pre>
","python, scikit-learn, nlp, data-visualization, word-embedding","<p>If you are vectorizing your text first, I suggest using <code>yellowbrick</code> library. Since TSNE is very expensive, <code>TSNEVisualizer</code> in <code>yellowbrick</code> applies a simpler decomposition ahead of time (SVD with 50 components by default), then performs the t-SNE embedding. The visualizer then plots the scatter plot which can be colored by cluster or by class. Here is a simple example using tf-idfvectorizer:</p>

<pre><code>from yellowbrick.text import TSNEVisualizer
from sklearn.feature_extraction.text import TfidfVectorizer

# vectorize the text
tfidf  = TfidfVectorizer()
tuple_vectors = tfidf.fit_transform(sample_text)

# Create the visualizer and draw the vectors
tsne = TSNEVisualizer()
tsne.fit(tuple_vectors)
tsne.poof()
</code></pre>
",5,3,1814,2019-01-17 15:08:18,https://stackoverflow.com/questions/54238783/t-sne-visualisation-on-list-of-word-vectors
tf.nn.static_rnn - input must be a sequence,"<p>I'm trying to create a 2 layer lstm (incl. dropout) but get an error message that 'inputs must be a sequence'. </p>

<p>I use embeddings as the input and not sure how to change these to be a sequence? Any explanations are greatly appreciated. </p>

<p>This is my graph definition: </p>

<pre><code>    with tf.name_scope('Placeholders'):
        input_x = tf.placeholder(tf.int32, [None, n_steps], name='input_x')
        input_y = tf.placeholder(tf.float32, [None, n_classes], name='input_y')
        dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')


    with tf.name_scope('Embedding_layer'):
        embeddings_var = tf.Variable(tf.random_uniform([vocab_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)
        embedded_chars = tf.nn.embedding_lookup(embeddings_var, input_x)
        print(embedded_chars, 'embed')


    def get_a_cell(lstm_size, keep_prob):
        lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)
        drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=dropout_keep_prob)
        return drop


    with tf.name_scope('lstm'):
        cell = tf.nn.rnn_cell.MultiRNNCell(
            [get_a_cell(num_hidden, dropout_keep_prob) for _ in range(num_layers)]
        )

    lstm_outputs, state = tf.nn.static_rnn(cell=cell,inputs=embedded_chars, dtype=tf.float32)

    with tf.name_scope('Fully_connected'):
        W = tf.Variable(tf.truncated_normal([num_hidden, n_classes], stddev=0.1))
        b = tf.Variable(tf.constant(0.1, shape=n_classes))
        output = tf.nn.xw_plus_b(lstm_outputs,W,b)
        predictions = tf.argmax(output, 1, name='predictions')

    with tf.name_scope('Loss'):
        # Cross-entropy loss and optimizer initialization
        loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=input_y))
        global_step = tf.Variable(0, name=""global_step"", trainable=False)
        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss1, global_step=global_step)

    with tf.name_scope('Accuracy'):
        # Accuracy metrics
        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.nn.softmax(output)), input_y), tf.float32))

    with tf.name_scope('num_correct'):
        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))
        num_correct = tf.reduce_sum(tf.cast(correct_predictions, 'float'), name='num_correct')
</code></pre>

<p>EDIT: 
when changing static_rnn to dynamic_rnn the error message changes to the following, failing on the bias (b) variable: </p>

<pre><code>TypeError: 'int' object is not iterable
</code></pre>

<p>After I changed the bias term to this:</p>

<pre><code>b = tf.Variable(tf.random_normal([n_classes]))
</code></pre>

<p>and get a new error message: </p>

<pre><code>ValueError: Shape must be rank 2 but is rank 3 for 'Fully_connected/xw_plus_b/MatMul' (op: 'MatMul') with input shapes: [?,27,128], [128,6].
</code></pre>
","python, python-3.x, tensorflow, lstm, word-embedding","<p>If we assume you use <code>tf.dynamic_rnn</code> (for the case of <code>tf.static_rnn</code>, the first problem is because you don't give the input in the right format, <code>tf.static_rnn</code> except a sequence of tensor such as list of tensors <code>[batch_size x seq_len]</code> and not a single tensor with shape <code>[batch_size x seq_len x dim]</code> whereas <code>tf.dynamic_rnn</code> deals with such tensors as input)</p>
<p>I invite you to read the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">tf.nn_dynamic_rnn</a> to see that for your classification problem you might not want to use <code>lstm_outputs</code> but <code>state</code> which basically contain the last output of your RNN, because lstm_output contains all the outputs , whereas here you are interested on only in the last_output (except if you want to do something like attention for classification , here you'll need all the outputs).</p>
<p>To get the last output you'll basically need to do that:</p>
<pre><code>lstm_outputs, state = tf.nn.dynamic_rnn(cell=cell,inputs=embedded_chars, dtype=tf.float32)
last_output = state[-1].h
</code></pre>
<p><code>state[-1]</code> to take the state of the last cell, then <code>h</code> contains the last output and pass <code>last_output</code> to your feed forward network.</p>
<h1>Full code</h1>
<p>(working, but compute wrong accuracy see comments)</p>
<pre><code>n_classes = 6
n_steps = 27
num_hidden=128
dropout_keep_prob =0.5
vocab_size=10000
EMBEDDING_DIM=300
num_layers = 2

with tf.name_scope('Placeholders'):
    input_x = tf.placeholder(tf.int32, [None, n_steps], name='input_x')
    input_y = tf.placeholder(tf.float32, [None, n_classes], name='input_y')
    dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')


with tf.name_scope('Embedding_layer'):
    embeddings_var = tf.Variable(tf.random_uniform([vocab_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)
    embedded_chars = tf.nn.embedding_lookup(embeddings_var, input_x)
    print(embedded_chars, 'embed')


def get_a_cell(lstm_size, keep_prob):
    lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)
    drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=dropout_keep_prob)
    return drop


with tf.name_scope('lstm'):
    cell = tf.nn.rnn_cell.MultiRNNCell(
        [get_a_cell(num_hidden, dropout_keep_prob) for _ in range(num_layers)]
    )

lstm_outputs, state = tf.nn.dynamic_rnn(cell=cell,inputs=embedded_chars, dtype=tf.float32)
last_output = state[-1].h
with tf.name_scope('Fully_connected'):
    W = tf.Variable(tf.truncated_normal([num_hidden, n_classes], stddev=0.1))
    b = tf.Variable(tf.constant(0.1, shape=[n_classes]))
    output = tf.nn.xw_plus_b(last_output,W,b)
    predictions = tf.argmax(output, 1, name='predictions')

with tf.name_scope('Loss'):
    # Cross-entropy loss and optimizer initialization
    loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=input_y))
    global_step = tf.Variable(0, name=&quot;global_step&quot;, trainable=False)
    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss1, global_step=global_step)

with tf.name_scope('Accuracy'):
    # Accuracy metrics
    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.nn.softmax(output)), input_y), tf.float32))

with tf.name_scope('num_correct'):
    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))
    num_correct = tf.reduce_sum(tf.cast(correct_predictions, 'float'), name='num_correct')
</code></pre>
",3,2,1043,2019-01-18 19:26:52,https://stackoverflow.com/questions/54260301/tf-nn-static-rnn-input-must-be-a-sequence
Better way to combine Word embedding to get embedding of a sentence,"<p>I have seen in many kaggle kernels and tutorials, average word embeddings is considered to get embedding of a sentence. But, i am wondering if this is a correct approach.Since it discards the positional information of the words in the sentence. is there a better way to combine embedding? maybe hierarchically combining them in a particular way?</p>
","deep-learning, nlp, text-processing, word-embedding","<p>If you need a simple but yet effective approach, <a href=""https://openreview.net/pdf?id=SyK00v5xx"" rel=""noreferrer"">Sif embedding</a> is perfectly fine. It averages word vector in a sentence and removes its first principal component. It is much superior to averaging word vectors. The code available online <a href=""https://github.com/PrincetonML/SIF/tree/master/examples"" rel=""noreferrer"">here</a>. Here is the main part:</p>

<pre><code>svd = TruncatedSVD(n_components=1, random_state=rand_seed, n_iter=20)
svd.fit(all_vector_representation)
svd = svd.components_

XX2 = all_vector_representation - all_vector_representation.dot(svd.transpose()) * svd
</code></pre>

<p>Where <code>all_vector_representation</code> is the average embedding of all sentences in your dataset.</p>

<p>Other sophisticated approaches also exist out there like <a href=""https://tfhub.dev/google/elmo/2"" rel=""noreferrer"">ELMO</a>, <a href=""https://tfhub.dev/google/universal-sentence-encoder-large/1"" rel=""noreferrer"">Transformer</a> and etc.</p>
",5,1,2171,2019-01-20 09:23:55,https://stackoverflow.com/questions/54275073/better-way-to-combine-word-embedding-to-get-embedding-of-a-sentence
"In tf.keras.layers.Embedding, why it is important to know the size of dictionary?","<p>Same as the title, in tf.keras.layers.Embedding, why it is important to know the size of dictionary as input dimension?</p>
","tensorflow, word-embedding","<p>In such setting, the dimensions/shapes of the tensors are the following:</p>

<ul>
<li>The input tensor has size <code>[batch_size, max_time_steps]</code> such that each element of that tensor can have a value in the range <code>0 to vocab_size-1</code>.</li>
<li>Then, each of the values from the input tensor pass through an embedding layer, that has a shape <code>[vocab_size, embedding_size]</code>. The output of the embedding layer is of shape <code>[batch_size, max_time_steps, embedding_size]</code>.</li>
<li>Then, in a typical seq2seq scenario, this <code>3D</code> tensor is the input of a recurrent neural network.</li>
<li>...</li>
</ul>

<p>Here's how this is implemented in Tensorflow so you can get a better idea:</p>

<pre><code>inputs = tf.placeholder(shape=(batch_size, max_time_steps), ...)
embeddings = tf.Variable(shape=(vocab_size, embedding_size], ...)
inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
</code></pre>

<p>Now, the output of the embedding lookup table has the <code>[batch_size, max_time_steps, embedding_size]</code> shape.</p>
",3,2,5462,2019-02-06 15:47:07,https://stackoverflow.com/questions/54557468/in-tf-keras-layers-embedding-why-it-is-important-to-know-the-size-of-dictionary
ELMo Embedding layer with Keras,"<p>I have been using Keras default embedding layer with word embeddings in my architecture. Architecture looks like this - </p>

<pre><code>left_input = Input(shape=(max_seq_length,), dtype='int32')
right_input = Input(shape=(max_seq_length,), dtype='int32')

embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length,
                            trainable=False)

# Since this is a siamese network, both sides share the same LSTM
shared_lstm = LSTM(n_hidden, name=""lstm"")

left_output = shared_lstm(encoded_left)
right_output = shared_lstm(encoded_right)
</code></pre>

<p>I want to replace the embedding layer with ELMo embeddings. So I used a custom embedding layer - found in this repo - <a href=""https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb"" rel=""nofollow noreferrer"">https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb</a>. Embedding layer looks like this - </p>

<pre><code>class ElmoEmbeddingLayer(Layer):
def __init__(self, **kwargs):
    self.dimensions = 1024
    self.trainable=True
    super(ElmoEmbeddingLayer, self).__init__(**kwargs)

def build(self, input_shape):
    self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,
                           name=""{}_module"".format(self.name))

    self.trainable_weights += K.tf.trainable_variables(scope=""^{}_module/.*"".format(self.name))
    super(ElmoEmbeddingLayer, self).build(input_shape)

def call(self, x, mask=None):
    result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),
                  as_dict=True,
                  signature='default',
                  )['default']
    return result

def compute_mask(self, inputs, mask=None):
    return K.not_equal(inputs, '--PAD--')

def compute_output_shape(self, input_shape):
    return (input_shape[0], self.dimensions)
</code></pre>

<p>I changed the architecture for the new embedding layer. </p>

<pre><code> # The visible layer
left_input = Input(shape=(1,), dtype=""string"")
right_input = Input(shape=(1,), dtype=""string"")

embedding_layer = ElmoEmbeddingLayer()

# Embedded version of the inputs
encoded_left = embedding_layer(left_input)
encoded_right = embedding_layer(right_input)

# Since this is a siamese network, both sides share the same LSTM
shared_lstm = LSTM(n_hidden, name=""lstm"")

left_output = shared_gru(encoded_left)
right_output = shared_gru(encoded_right)
</code></pre>

<p>But I am getting error - </p>

<p>ValueError: Input 0 is incompatible with layer lstm: expected ndim=3, found ndim=2</p>

<p>What am I doing wrong here? </p>
","python, keras, deep-learning, lstm, word-embedding","<p>I also used that repository as a guide to build a CustomELMo + BiLSTM + CRF model, and I needed to change the dict lookup to 'elmo' instead of 'default'. As Anna Krogager pointed out, when the dict lookup is 'default' the output is (batch_size, dim), which isn't enough dimensions for the LSTM. However when the dict lookup is ['elmo'] the layer returns a tensor of the right dimensions, namely of shape (batch_size, max_length, 1024).</p>

<p>Custom ELMo Layer:</p>

<pre><code>class ElmoEmbeddingLayer(Layer):
def __init__(self, **kwargs):
    self.dimensions = 1024
    self.trainable = True
    super(ElmoEmbeddingLayer, self).__init__(**kwargs)

def build(self, input_shape):
    self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,
                           name=""{}_module"".format(self.name))

    self.trainable_weights += K.tf.trainable_variables(scope=""^{}_module/.*"".format(self.name))
    super(ElmoEmbeddingLayer, self).build(input_shape)

def call(self, x, mask=None):
    result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),
                       as_dict=True,
                       signature='default',
                       )['elmo']
    print(result)
    return result

# def compute_mask(self, inputs, mask=None):
#   return K.not_equal(inputs, '__PAD__')

def compute_output_shape(self, input_shape):
    return input_shape[0], 48, self.dimensions
</code></pre>

<p>And the model is built as follows:</p>

<pre><code>def build_model(): # uses crf from keras_contrib
    input = layers.Input(shape=(1,), dtype=tf.string)
    model = ElmoEmbeddingLayer(name='ElmoEmbeddingLayer')(input)
    model = Bidirectional(LSTM(units=512, return_sequences=True))(model)
    crf = CRF(num_tags)
    out = crf(model)
    model = Model(input, out)
    model.compile(optimizer=""rmsprop"", loss=crf_loss, metrics=[crf_accuracy, categorical_accuracy, mean_squared_error])
    model.summary()
    return model
</code></pre>

<p>I hope my code is useful to you, even if it's not exactly the same model. Note that I had to comment out the compute_mask method as it throws </p>

<pre><code>InvalidArgumentError: Incompatible shapes: [32,47] vs. [32,0]    [[{{node loss/crf_1_loss/mul_6}}]]
</code></pre>

<p>where 32 is batch size and 47 is one less than my specified max_length (presumably meaning it's accounting for a pad token itself). I haven't worked out the cause of that error yet, so it might be fine for you and your model. However I notice you're using GRU's, and there's an unresolved issue on the repository about adding GRU's. So I'm curious whether you get that isue too.</p>
",3,1,6886,2019-02-11 20:23:44,https://stackoverflow.com/questions/54638544/elmo-embedding-layer-with-keras
Mapping word vector to the most similar/closest word using spaCy,"<p>I am using spaCy as part of a topic modelling solution and I have a situation where I need to map a derived word vector to the ""closest"" or ""most similar"" word in a vocabulary of word vectors.</p>

<p>I see gensim has a function (WordEmbeddingsKeyedVectors.similar_by_vector) to calculate this, but I was wondering if spaCy has something like this to map a vector to a word within its vocabulary (nlp.vocab)?</p>
","nlp, spacy, word2vec, word-embedding","<p>After a bit of experimentation, I found a scikit function (cdist in scikit.spatial.distance) that finds a ""close"" vector in a vector space to the input vector. </p>

<pre><code># Imports
from scipy.spatial import distance
import spaCy

# Load the spacy vocabulary
nlp = spacy.load(""en_core_web_lg"")

# Format the input vector for use in the distance function
# In this case we will artificially create a word vector from a real word (""frog"")
# but any derived word vector could be used
input_word = ""frog""
p = np.array([nlp.vocab[input_word].vector])

# Format the vocabulary for use in the distance function
ids = [x for x in nlp.vocab.vectors.keys()]
vectors = [nlp.vocab.vectors[x] for x in ids]
vectors = np.array(vectors)

# *** Find the closest word below ***
closest_index = distance.cdist(p, vectors).argmin()
word_id = ids[closest_index]
output_word = nlp.vocab[word_id].text
# output_word is identical, or very close, to the input word
</code></pre>
",12,12,9003,2019-02-15 21:43:21,https://stackoverflow.com/questions/54717449/mapping-word-vector-to-the-most-similar-closest-word-using-spacy
Can I use a 3D input on a Keras Dense Layer?,"<p>As an exercise I need to use only dense layers to perform text classifications. I want to leverage words embeddings, the issue is that the dataset then is 3D (samples,words of sentence,embedding dimension). Can I input a 3D dataset into a dense layer?</p>

<p>Thanks</p>
","keras, text-classification, keras-layer, word-embedding, nlp","<p>As stated in <a href=""https://keras.io/layers/core/"" rel=""noreferrer"">the keras documentation</a> you can use 3D (or higher rank) data as input for a Dense layer but the input gets flattened first:</p>

<blockquote>
  <p>Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.</p>
</blockquote>

<p>This means that if your input has shape <code>(batch_size, sequence_length, dim)</code>, then the dense layer will first flatten your data to shape <code>(batch_size * sequence_length, dim)</code> and then apply a dense layer as usual. The output will have shape <code>(batch_size, sequence_length, hidden_units)</code>. This is actually the same as applying a Conv1D layer with kernel size 1, and it might be more explicit to use a Conv1D layer instead of a Dense layer.</p>
",12,2,5028,2019-02-21 10:52:50,https://stackoverflow.com/questions/54805345/can-i-use-a-3d-input-on-a-keras-dense-layer
keras understanding Word Embedding Layer,"<p>From the <a href=""https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"" rel=""nofollow noreferrer"">page</a> I got the below code:</p>

<pre><code>from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
# define documents
docs = ['Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!',
        'Weak',
        'Poor effort!',
        'not good',
        'poor work',
        'Could have done better.']
# define class labels
labels = array([1,1,1,1,1,0,0,0,0,0])
# integer encode the documents
vocab_size = 50
encoded_docs = [one_hot(d, vocab_size) for d in docs]
print(encoded_docs)
# pad documents to a max length of 4 words
max_length = 4
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
print(padded_docs)
# define the model
model = Sequential()
model.add(Embedding(vocab_size, 8, input_length=max_length))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())
# fit the model
model.fit(padded_docs, labels, epochs=50, verbose=0)
# evaluate the model
loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)
print('Accuracy: %f' % (accuracy*100))
</code></pre>

<ol>
<li>I looked at <code>encoded_docs</code> and noticed that words <code>done</code> and <code>work</code> both have  one_hot encoding of 2, why? Is it because <code>unicity of word to index mapping non-guaranteed.</code> as per this <a href=""https://keras.io/preprocessing/text/#one_hot"" rel=""nofollow noreferrer"">page</a>?</li>
<li>I got <code>embeddings</code> by command <code>embeddings = model.layers[0].get_weights()[0]</code>. in such case why do we get <code>embedding</code> object of size 50? Even though two words have same one_hot number, do they have different embedding?</li>
<li>how could i understand which embedding is for which word i.e. <code>done</code> vs <code>work</code></li>
<li><p>I also found below code at the <a href=""https://stackoverflow.com/questions/51235118/how-to-get-word-vectors-from-keras-embedding-layer"">page</a> that could help with finding embedding of each word. But i dont know how to create <code>word_to_index</code></p>

<p><code>word_to_index</code> is a mapping (i.e. dict) from words to their index, e.g. <code>love</code>: 69
words_embeddings = {w:embeddings[idx] for w, idx in word_to_index.items()}</p></li>
<li><p>Please ensure that my understanding of <code>para #</code> is correct.</p></li>
</ol>

<p>The first layer has 400 parameters because total word count is 50 and embedding have 8 dimensions so 50*8=400. </p>

<p>The last layer has 33 parameters because each sentence has 4 words max. So 4*8 due to dimensions of embedding and 1 for bias. 33 total</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param#   
=================================================================
embedding_3 (Embedding)      (None, 4, 8)              400       
_________________________________________________________________
flatten_3 (Flatten)          (None, 32)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 33        
=================================================================
</code></pre>

<ol start=""6"">
<li>Finally, if 1 above is correct, is there a better way to get embedding layer <code>model.add(Embedding(vocab_size, 8, input_length=max_length))
</code> without doing one hot coding <code>encoded_docs = [one_hot(d, vocab_size) for d in docs]
</code></li>
</ol>

<p>+++++++++++++++++++++++++++++++
update - providing the updated code</p>

<pre><code>from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
# define documents
docs = ['Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!',
        'Weak',
        'Poor effort!',
        'not good',
        'poor work',
        'Could have done better.']
# define class labels
labels = array([1,1,1,1,1,0,0,0,0,0])


from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()

#this creates the dictionary
#IMPORTANT: MUST HAVE ALL DATA - including Test data
#IMPORTANT2: This method should be called only once!!!
tokenizer.fit_on_texts(docs)

#this transforms the texts in to sequences of indices
encoded_docs2 = tokenizer.texts_to_sequences(docs)

encoded_docs2

max_length = 4
padded_docs2 = pad_sequences(encoded_docs2, maxlen=max_length, padding='post')
max_index = array(padded_docs2).reshape((-1,)).max()



# define the model
model = Sequential()
model.add(Embedding(max_index+1, 8, input_length=max_length))# you cannot use just max_index 
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())
# fit the model
model.fit(padded_docs2, labels, epochs=50, verbose=0)
# evaluate the model
loss, accuracy = model.evaluate(padded_docs2, labels, verbose=0)
print('Accuracy: %f' % (accuracy*100))

embeddings = model.layers[0].get_weights()[0]

embeding_for_word_7 = embeddings[14]
index = tokenizer.texts_to_sequences([['well']])[0][0]
tokenizer.document_count
tokenizer.word_index
</code></pre>
","python, tensorflow, keras, word-embedding","<p>1 - Yes, word unicity is not guaranteed, see the <a href=""https://keras.io/preprocessing/text"" rel=""noreferrer"">docs</a>:</p>

<ul>
<li>From <code>one_hot</code>: This is a wrapper to the <code>hashing_trick</code> function...</li>
<li>From <code>hashing_trick</code>: ""Two or more words may be assigned to the same index, due to <strong>possible collisions by the hashing function</strong>. The probability of a collision is in relation to the dimension of the hashing space and the number of distinct objects.""</li>
</ul>

<p>It would be better to use a <code>Tokenizer</code> for this. (See question 4)</p>

<p><strong>It's very important to remember</strong> that you should involve <strong>all words at once</strong> when creating indices. You cannot use a function to create a dictionary with 2 words, then again with 2 words, then again.... This will create very wrong dictionaries. </p>

<hr>

<p>2 - Embeddings have the size <code>50 x 8</code>, because that was defined in the embedding layer:</p>

<pre><code>Embedding(vocab_size, 8, input_length=max_length)
</code></pre>

<ul>
<li><code>vocab_size = 50</code> - this means there are 50 words in the dictionary   </li>
<li><code>embedding_size= 8</code> - this is the true size of the embedding: each word is represented by a vector of 8 numbers.</li>
</ul>

<hr>

<p>3 - You don't know. They use the same embedding. </p>

<p>The system will use the same embedding (the one for index = 2). This is not healthy for your model at all. You should use another method for creating indices in question 1.</p>

<hr>

<p>4 - You can create a word dictionary manually, or use the <code>Tokenizer</code> class.</p>

<p><strong>Manually</strong>:</p>

<p>Make sure you remove punctuation, make all words lower case.</p>

<p>Just create a dictionary for each word you have:</p>

<pre><code>dictionary = dict()
current_key = 1

for doc in docs:
    for word in doc.split(' '):
        #make sure you remove punctuation (this might be boring)
        word = word.lower()

        if not (word in dictionary):
            dictionary[word] = current_key
            current_key += 1
</code></pre>

<p><strong>Tokenizer:</strong></p>

<pre><code>from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()

#this creates the dictionary
#IMPORTANT: MUST HAVE ALL DATA - including Test data
#IMPORTANT2: This method should be called only once!!!
tokenizer.fit_on_texts(docs)

#this transforms the texts in to sequences of indices
encoded_docs2 = tokenizer.texts_to_sequences(docs)
</code></pre>

<p>See the output of <code>encoded_docs2</code>:</p>

<pre><code>[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]
</code></pre>

<p>See the maximum index:</p>

<pre><code>padded_docs2 = pad_sequences(encoded_docs2, maxlen=max_length, padding='post')
max_index = array(padded_docs2).reshape((-1,)).max()
</code></pre>

<p>So, your <code>vocab_size</code> should be 15 (otherwise you'd have lots of useless - and harmless - embedding rows). Notice that <code>0</code> was not used as an index. It will appear in padding!!!</p>

<blockquote>
  <p>Do not ""fit"" the tokenizer again! Only use <code>texts_to_sequences()</code> or other methods <a href=""https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py"" rel=""noreferrer"">here</a> that are not related to ""fitting"". </p>
</blockquote>

<p><strong>Hint:</strong> it might be useful to include <code>end_of_sentence</code> words in your text sometimes. </p>

<p><strong>Hint2:</strong> it is a good idea to save your <code>Tokenizer</code> to be used later (since it has a specific dictoinary for your data, created with <code>fit_on_texts</code>).</p>

<pre><code>#save:
text_to_save = tokenizer.to_json()

#load:
from keras.preprocessing.text import tokenizer_from_json
tokenizer = tokenizer_from_json(loaded_text)
</code></pre>

<hr>

<p>5 - Params for embedding are correct.</p>

<p><strong>Dense:</strong></p>

<p>Params for <code>Dense</code> are always based on the preceding layer (the <code>Flatten</code> in this case). </p>

<p>The formula is: <code>previous_output * units + units</code></p>

<p>This results in <code>32 (from the Flatten) * 1 (Dense units) + 1 (Dense bias=units) = 33</code></p>

<p><strong>Flatten:</strong></p>

<p>It gets all the previous dimensions multiplied = <code>8 * 4</code>.<br>
The <code>Embedding</code> outputs <code>lenght = 4</code> and <code>embedding_size = 8</code>.</p>

<hr>

<p>6 - The <code>Embedding</code> layer is not dependent of your data and how you preprocess it. </p>

<p>The <code>Embedding</code> layer has simply the size 50 x 8 because you told so. (See question 2)</p>

<p>There are, of course, better ways of preprocessing the data - See question 4.</p>

<p>This will lead you to select better the <code>vocab_size</code> (which is dictionary size).</p>

<h2>Seeing the embedding of a word:</h2>

<p>Get the embeddings matrix:</p>

<pre><code>embeddings = model.layers[0].get_weights()[0]
</code></pre>

<p>Choose any word index:</p>

<pre><code>embeding_for_word_7 = embeddings[7]
</code></pre>

<p>That's all. </p>

<p>If you're using a tokenizer, get the word index with:</p>

<pre><code>index = tokenizer.texts_to_sequences([['word']])[0][0]
</code></pre>
",11,3,1586,2019-02-22 23:18:11,https://stackoverflow.com/questions/54836522/keras-understanding-word-embedding-layer
Combining TF-IDF with pre-trained Word embeddings,"<p>I have a list of website meta-description (128k descriptions; each with avg. 20-30 words), and am trying to build a similarity ranker (as in: show me the 5 most similar sites to this site meta description)</p>
<p><strong>It worked AMAZINGLY well with TF-IDF uni- and bigram</strong>, and I thought that I could additionally improve it by adding pre-trained word embeddings (spacy &quot;en_core_web_lg&quot; to be exact). <strong>Plot twist: it does not work at all</strong>. Literally did not get one good guess, and its suddenly spits out completely random suggestions.</p>
<p>Below is my code. Any thoughts on where I might have gone wrong? Am I overseeing something highly intuitive?</p>

<pre class=""lang-python prettyprint-override""><code>import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import sys
import pickle
import spacy
import scipy.sparse
from scipy.sparse import csr_matrix
import math
from sklearn.metrics.pairwise import linear_kernel
nlp=spacy.load('en_core_web_lg')


&quot;&quot;&quot; Tokenizing&quot;&quot;&quot;
def _keep_token(t):
    return (t.is_alpha and 
            not (t.is_space or t.is_punct or 
                 t.is_stop or t.like_num))
def _lemmatize_doc(doc):
    return [ t.lemma_ for t in doc if _keep_token(t)]

def _preprocess(doc_list):     
    return [_lemmatize_doc(nlp(doc)) for doc in doc_list]
def dummy_fun(doc):
    return doc

# Importing List of 128.000 Metadescriptions:
Web_data=open(&quot;./data/meta_descriptions&quot;,&quot;r&quot;, encoding=&quot;utf-8&quot;)
All_lines=Web_data.readlines()
# outputs a list of meta-descriptions consisting of lists of preprocessed tokens:
data=_preprocess(All_lines) 

# TF-IDF Vectorizer:    
vectorizer = TfidfVectorizer(min_df=10,tokenizer=dummy_fun,preprocessor=dummy_fun,)
    tfidf = vectorizer.fit_transform(data)    
dictionary = vectorizer.get_feature_names()

# Retrieving Word embedding vectors:
temp_array=[nlp(dictionary[i]).vector for i in range(len(dictionary))]

# I had to build the sparse array in several steps due to RAM constraints
# (with bigrams the vocabulary gets as large as &gt;1m 
dict_emb_sparse=scipy.sparse.csr_matrix(temp_array[0])
for arr in range(1,len(temp_array),100000):
    print(str(arr))        
    dict_emb_sparse=scipy.sparse.vstack([dict_emb_sparse, scipy.sparse.csr_matrix(temp_array[arr:min(arr+100000,len(temp_array))])])

# Multiplying the TF-IDF matrix with the Word embeddings: 
tfidf_emb_sparse=tfidf.dot(dict_emb_sparse)

# Translating the Query into the TF-IDF matrix and multiplying with the same Word Embeddings:
query_doc= vectorizer.transform(_preprocess([&quot;World of Books is one of the largest online sellers of second-hand books in the world Our massive collection of over million cheap used books also comes with free delivery in the UK Whether it s the latest book release fiction or non-fiction we have what you are looking for&quot;]))
query_emb_sparse=query_doc.dot(dict_emb_sparse)

# Calculating Cosine Similarities:
cosine_similarities = linear_kernel(query_emb_sparse, tfidf_emb_sparse).flatten()

related_docs_indices = cosine_similarities.argsort()[:-10:-1]

# Printing the Site descriptions with the highest match:    
for ID in related_docs_indices:
    print(All_lines[ID])
</code></pre>
<p>I stole parts of the code/logic from <a href=""https://github.com/crownpku/text2vec"" rel=""nofollow noreferrer"">this Github</a> Rep
Does anybody see any straightforward errors here?
Many thanks!!</p>
","nlp, spacy, tf-idf, word-embedding, tfidfvectorizer","<p>You should try to train embeding on your own corpus. There are many package: gensim, glove.
You can use embeding from BERT without retraining on your own corpus.</p>

<p>You should know that the probability distribution on different corpus is always different. For example, the count of 'basketball' in posts about food is very different from the count of the term in news about sport, so the gap of word embeding of 'basketball' in those corpus is huge.</p>
",3,5,6087,2019-02-24 00:21:31,https://stackoverflow.com/questions/54847574/combining-tf-idf-with-pre-trained-word-embeddings
Is it possible to freeze only certain embedding weights in the embedding layer in pytorch?,"<p>When using GloVe embedding in NLP tasks, some words from the dataset might not exist in GloVe. Therefore, we instantiate random weights for these unknown words.</p>

<p>Would it be possible to freeze weights gotten from GloVe, and train only the newly instantiated weights?</p>

<p>I am only aware that we can set:
model.embedding.weight.requires_grad = False</p>

<p>But this makes the new words untrainable..</p>

<p>Or are there better ways to extract semantics of words.. </p>
","python, nlp, pytorch, word-embedding, glove","<h1>1. Divide embeddings into two separate objects</h1>
<p>One approach would be to use two separate embeddings <strong>one for pretrained</strong>, another for the one <strong>to be trained</strong>.</p>
<p>The GloVe one should be frozen, while the one for which there is no pretrained representation would be taken from the trainable layer.</p>
<p>If you format your data that for pretrained token representations it is in smaller range than the tokens without GloVe representation it could be done. Let's say your pretrained indices are in the range [0, 300], while those without representation are [301, 500]. I would go with something along those lines:</p>
<pre><code>import numpy as np
import torch


class YourNetwork(torch.nn.Module):
    def __init__(self, glove_embeddings: np.array, how_many_tokens_not_present: int):
        self.pretrained_embedding = torch.nn.Embedding.from_pretrained(glove_embeddings)
        self.trainable_embedding = torch.nn.Embedding(
            how_many_tokens_not_present, glove_embeddings.shape[1]
        )
        # Rest of your network setup

    def forward(self, batch):
        # Which tokens in batch do not have representation, should have indices BIGGER
        # than the pretrained ones, adjust your data creating function accordingly
        mask = batch &gt; self.pretrained_embedding.num_embeddings

        # You may want to optimize it, you could probably get away without copy, though
        # I'm not currently sure how
        pretrained_batch = batch.copy()
        pretrained_batch[mask] = 0

        embedded_batch = self.pretrained_embedding(pretrained_batch)

        # Every token without representation has to be brought into appropriate range
        batch -= self.pretrained_embedding.num_embeddings
        # Zero out the ones which already have pretrained embedding
        batch[~mask] = 0
        non_pretrained_embedded_batch = self.trainable_embedding(batch)

        # And finally change appropriate tokens from placeholder embedding created by
        # pretrained into trainable embeddings.
        embedded_batch[mask] = non_pretrained_embedded_batch[mask]

        # Rest of your code
        ...
</code></pre>
<p>Let's say your pretrained indices are in the range [0, 300], while those without representation are [301, 500].</p>
<h1>2. Zero gradients for specified tokens.</h1>
<p>This one is a bit tricky, but I think it's pretty concise and easy to implement. So, if you obtain the indices of tokens which got no GloVe representation, you can explicitly zero their gradient after backprop, so those rows will not get updated.</p>
<pre><code>import torch

embedding = torch.nn.Embedding(10, 3)
X = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])

values = embedding(X)
loss = values.mean()

# Use whatever loss you want
loss.backward()

# Let's say those indices in your embedding are pretrained (have GloVe representation)
indices = torch.LongTensor([2, 4, 5])

print(&quot;Before zeroing out gradient&quot;)
print(embedding.weight.grad)

print(&quot;After zeroing out gradient&quot;)
embedding.weight.grad[indices] = 0
print(embedding.weight.grad)
</code></pre>
<p>And the output of the second approach:</p>
<pre><code>Before zeroing out gradient
tensor([[0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0833, 0.0833, 0.0833],
        [0.0417, 0.0417, 0.0417],
        [0.0833, 0.0833, 0.0833],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417]])
After zeroing out gradient
tensor([[0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417]])
</code></pre>
",20,16,8933,2019-02-28 11:23:39,https://stackoverflow.com/questions/54924582/is-it-possible-to-freeze-only-certain-embedding-weights-in-the-embedding-layer-i
Why the FastText word embedding could generate the representation of a word from another language?,"<p>Recently, I trained a FastText word embedding from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> to get the representation for English words. However, today just for a trial, I run the FastText module on a couple of Chinese words, for instance:</p>

<pre><code>import gensim.models as gs

path = r'\data\word2vec'

w2v = gs.FastText.load(os.path.join(path, 'fasttext_model'))

w2v.wv['哈哈哈哈']
</code></pre>

<p>It outputs:</p>

<pre><code>array([ 0.00303676,  0.02088235, -0.00815559,  0.00484574, -0.03576371,
       -0.02178247, -0.05090654,  0.03063928, -0.05999983,  0.04547168,
       -0.01778449, -0.02716631, -0.03326027, -0.00078981,  0.0168153 ,
        0.00773436,  0.01966593, -0.00756055,  0.02175765, -0.0050137 ,
        0.00241255, -0.03810823, -0.03386266,  0.01231019, -0.00621936,
       -0.00252419,  0.02280569,  0.00992453,  0.02770403,  0.00233192,
        0.0008545 , -0.01462698,  0.00454278,  0.0381292 , -0.02945416,
       -0.00305543, -0.00690968,  0.00144188,  0.00424266,  0.00391074,
        0.01969502,  0.02517333,  0.00875261,  0.02937791,  0.03234404,
       -0.01116276, -0.00362578,  0.00483239, -0.02257918,  0.00123061,
        0.00324584,  0.00432153,  0.01332884,  0.03186348, -0.04119627,
        0.01329033,  0.01382102, -0.01637722,  0.01464139,  0.02203292,
        0.0312229 ,  0.00636201, -0.00044287, -0.00489291,  0.0210293 ,
       -0.00379244, -0.01577058,  0.02185207,  0.02576622, -0.0054543 ,
       -0.03115215, -0.00337738, -0.01589811, -0.01608399, -0.0141606 ,
        0.0508234 ,  0.00775024,  0.00352813,  0.00573649, -0.02131752,
        0.01166397,  0.00940598,  0.04075769, -0.04704212,  0.0101376 ,
        0.01208556,  0.00402935,  0.0093914 ,  0.00136144,  0.03284211,
        0.01000613, -0.00563702,  0.00847146,  0.03236216, -0.01626745,
        0.04095127,  0.02858841,  0.0248084 ,  0.00455458,  0.01467448],
      dtype=float32)
</code></pre>

<p>Hence, I really want to know why the FastText module trained from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> could do this. Thank you!</p>
","python, gensim, word-embedding, fasttext, nlp","<p>In fact, the proper behavior for a FastText model, based on the behavior of Facebook's original/reference implementation, is to <em>always</em> return a vector for an out-of-vocabulary word. </p>

<p>Essentially, if none of the supplied string's character n-grams are present, a vector will still be synthesized from whatever random vectors happen to be at the same lookup slots in the model's fixed-size collection of n-gram vectors. </p>

<p>In Gensim up through at least 3.7.1, the <code>FastText</code> class will throw a <code>KeyError: 'all ngrams for word _____ absent from model'</code> error if none of an out-of-vocabulary word's n-grams are present – but that's a buggy behavior that will be reversed, to match Facebook's FastText, in a future Gensim release. (The <a href=""https://github.com/RaRe-Technologies/gensim/pull/2370"" rel=""nofollow noreferrer"">PR to correct this behavior has been merged</a> to Gensim's develop branch and thus should take effect in the next release after 3.7.1.)</p>

<p>I'm not sure why you're not getting such an error with the specific model and dataset you've described. Perhaps your <code>fasttext_model</code> was actually trained with different text than you think? Or, trained with a very-small non-default <code>min_n</code> parameter, such that a single <code>哈</code> appearing inside the <code>sentiment140</code> data is enough to contribute to a synthesized vector for <code>哈哈哈哈</code>? </p>

<p>But given that the standard FastText behavior is to always report some synthesized vector, and Gensim will match that behavior in a future release, you shouldn't count on getting an error here. Expect to get back an essentially-random vector for completely unknown words with no resemblance to training data.</p>
",3,1,1351,2019-03-06 08:17:24,https://stackoverflow.com/questions/55018426/why-the-fasttext-word-embedding-could-generate-the-representation-of-a-word-from
Illegal Hardware Instruction Error when using GloVe,"<p>I am trying to train GloVe embeddings. In the GloVe implementation from <a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">stanfordnlp</a> there are 4 scripts to run. However, running the second script, <code>coocur</code>, results in an <code>Illegal Hardware Instruction</code>-Error. I don't understand how this error is produced. </p>

<p>With the input file <code>3.txt</code> my commands look like this:</p>

<pre><code>$ ./vocab_count -min-count 1 -verbose 2 &lt; 3.txt &gt; vocab.txt
BUILDING VOCABULARY
Processed 8354 tokens.
Counted 3367 unique words.
Using vocabulary of size 3367.

$ ./cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 &lt; 3.txt &gt; cooccurrence.bin
zsh: illegal hardware instruction  ./cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 &lt; 3.tx
</code></pre>

<p>I am running these commands on a remote server (Debian GNU/Linux 9 (stretch)). When I run the same commands on the same data locally (18.04.2 LTS (Bionic Beaver)), there is no problem. What could be the cause of this?</p>
","nlp, stanford-nlp, word-embedding, glove, illegal-instruction","<p>I've hit the same issue in recent days. </p>

<p>The Docker image was built on a server using Jenkins. It has been running fine until the underlying cluster host orchestration software and physical hardware was upgraded. </p>

<p>My solution has been to remove the build of GloVe from the Dockerfile and instead put the build/make inside a script which runs when the container starts. </p>

<p>The actual cause of the error may be caused by the <code>CFLAGS</code>: <code>-march=native</code> set in the Glove Makefile: <a href=""https://github.com/stanfordnlp/GloVe/blob/07d59d5e6584e27ec758080bba8b51fce30f69d8/Makefile#L4"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/blob/07d59d5e6584e27ec758080bba8b51fce30f69d8/Makefile#L4</a> This will cause the GloVe build to rely on the underlying CPU instruction set on which the Docker image is built. </p>

<p>There's a discussion of this further here: <a href=""https://stackoverflow.com/questions/54039176/mtune-and-march-when-compiling-in-a-docker-image"">mtune and march when compiling in a docker image</a></p>
",2,0,1095,2019-03-08 11:15:09,https://stackoverflow.com/questions/55062077/illegal-hardware-instruction-error-when-using-glove
Use word2vec word embeding as feature vector for text classification (simlar to count vectorizer/tfidf feature vector),"<p>I am trying to perform some text classification using machine learning and for that I have extracted feature vectors from the per-processed textual data using simple bag of words approach(count vectorizer) and tfidf vectorizer.  </p>

<p>Now I want to use word2vec i.e. word embedding as my feature vector similar as that of count vectorizer/tfidf vectorizer where I should be able to learn vocabulary from the train data and transform or fit the test data with the learned vocab but I can't find a way to implement that.  </p>

<pre><code>//I need something like this with word2vec

count = CountVectorizer()
train_feature_ vector =count.fit_transform(train_data)
test_feature_vector = count.fit(test_data)

//So I can train my model like this
mb = MultinomialNB()
mb.fit(train_feature_vector,y_train)
acc_score = mb.score(test_feature_vector,y_test)
print(""Accuracy ""+str(acc_score))
</code></pre>
","machine-learning, scikit-learn, word2vec, text-classification, word-embedding","<p>You first should understand what Word Embeddings are. When you apply a CountVectorizer or TfIdfVectorizer what you get is a <strong>sentence</strong> representation in a sparse way, commonly known as a One Hot encoding. The word embeddings representation are used to represent a <strong>word</strong> in a high dimensional space of real numbers.</p>

<p>Once you get your per word representation there are some ways to do this, check:<a href=""https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence"">How to get vector for a sentence from the word2vec of tokens in sentence</a></p>
",1,-1,2048,2019-03-11 07:06:19,https://stackoverflow.com/questions/55096725/use-word2vec-word-embeding-as-feature-vector-for-text-classification-simlar-to
Document similarity with Word Mover Distance and Bert-Embedding,"<p>I am trying to calculate the document similarity (nearest neighbor) for two arbitrary documents using word embeddings based on <a href=""https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"" rel=""noreferrer"">Google's BERT</a>.
In order to obtain word embeddings from Bert, I use <a href=""https://github.com/hanxiao/bert-as-service"" rel=""noreferrer"">bert-as-a-service</a>.
Document similarity should be based on Word-Mover-Distance with the python <a href=""https://github.com/src-d/wmd-relax"" rel=""noreferrer"">wmd-relax</a> package.</p>

<p>My previous tries are orientated along this tutorial from the <code>wmd-relax</code> github repo: <a href=""https://github.com/src-d/wmd-relax/blob/master/spacy_example.py"" rel=""noreferrer"">https://github.com/src-d/wmd-relax/blob/master/spacy_example.py</a></p>

<pre><code>import numpy as np
import spacy
import requests
from wmd import WMD
from collections import Counter
from bert_serving.client import BertClient

# Wikipedia titles
titles = [""Germany"", ""Spain"", ""Google"", ""Apple""]

# Standard model from spacy
nlp = spacy.load(""en_vectors_web_lg"")

# Fetch wiki articles and prepare as specy document
documents_spacy = {}
print('Create spacy document')
for title in titles:
    print(""... fetching"", title)
    pages = requests.get(
        ""https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=%s""
        ""&amp;prop=extracts&amp;explaintext"" % title).json()[""query""][""pages""]
    text = nlp(next(iter(pages.values()))[""extract""])
    tokens = [t for t in text if t.is_alpha and not t.is_stop]
    words = Counter(t.text for t in tokens)
    orths = {t.text: t.orth for t in tokens}
    sorted_words = sorted(words)
    documents_spacy[title] = (title, [orths[t] for t in sorted_words],
                              np.array([words[t] for t in sorted_words],
                                       dtype=np.float32))


# This is the original embedding class with the model from spacy
class SpacyEmbeddings(object):
    def __getitem__(self, item):
        return nlp.vocab[item].vector


# Bert Embeddings using bert-as-as-service
class BertEmbeddings:
    def __init__(self, ip='localhost', port=5555, port_out=5556):
        self.server = BertClient(ip=ip, port=port, port_out=port_out)

    def __getitem__(self, item):
        text = nlp.vocab[item].text
        emb = self.server.encode([text])
        return emb


# Get the nearest neighbor of one of the atricles
calc_bert = WMD(BertEmbeddings(), documents_spacy)
calc_bert.nearest_neighbors(titles[0])
</code></pre>

<p>Unfortunately, the calculations fails with a dimensions mismatch in the distance calculation:
<code>ValueError: shapes (812,1,768) and (768,1,812) not aligned: 768 (dim 2) != 1 (dim 1)</code></p>
","python, nlp, similarity, word-embedding","<p><code>bert-as-service</code> output's shape is (batch_size, sequence_len, embedding_dimension. In your case, sequence_len is 1 since you are pooling the results.</p>

<p>Now, you can transpose the other one to match with this using the <code>transpose</code> method of the <code>numpy.ndarray</code>.</p>
",1,5,3717,2019-03-11 19:00:31,https://stackoverflow.com/questions/55108636/document-similarity-with-word-mover-distance-and-bert-embedding
"Why no word embeddings (Glove, word2vecetc) used in first attention paper?","<p>In the paper <a href=""https://arxiv.org/abs/1409.0473"" rel=""nofollow noreferrer"">Neural Machine Translation by Jointly Learning to Align and Translate Bahdanau et. al.</a> why are there no word embeddings such as Glove or word2vec used? </p>

<p>I understand that this was a 2014 paper, but the current implementations of the paper on github don't use any word embeddings as well?</p>

<p>For trying to code the paper is using word embeddings reasonable?</p>
","nlp, word-embedding, machine-translation, attention-model","<p>In short - the model certainly does use word embeddings, they are just not pre-trained embeddings like Glove or word2vec; instead, the embeddings are randomly initialised and jointly trained along with the rest of the network.</p>

<p>In the full description of the network in section A.2 of the original Bahdanau et al. paper, you'll see the word embedding matrices <code>E</code> described for both the encoder and decoder. How they were initialised is also described in section B.1.</p>

<p>This usually works as well as or better than pre-trained embeddings in situations where you have enough data. That said, in a low-resource setting, it can help to initialise the embedding matrix with pre-trained embeddings. <a href=""https://arxiv.org/abs/1804.06323"" rel=""nofollow noreferrer"">This paper</a> might help you explore that idea in further detail.</p>

<p>In addition, your statement that current implementations don't do this is not entirely accurate - while it's true that the embeddings are usually jointly trained by default, many existing neural MT toolkits have the option to initialise the embeddings with pre-trained vectors. For example, <a href=""http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-pretrained-embeddings-e-g-glove"" rel=""nofollow noreferrer"">OpenNMT-py</a>, <a href=""https://marian-nmt.github.io/docs/#custom-embeddings"" rel=""nofollow noreferrer"">Marian</a>.</p>
",2,1,437,2019-03-12 12:30:14,https://stackoverflow.com/questions/55121521/why-no-word-embeddings-glove-word2vecetc-used-in-first-attention-paper
Is there an alternative to fully loading pre-trained word embeddings in memory?,"<p>I want to use pre-trained word embeddings in my machine learning model. The word embedings file I have is about 4GB. I currently read the entire file into memory in a dictionary and whenever I want to map a word to its vector representation I perform a lookup in that dictionary.</p>

<p>The memory usage is very high and I would like to know if there is another way of using word embeddings without loading the entire data into memory.</p>

<p>I have recently come across generators in Python. Could they help me reduce the memory usage?</p>

<p>Thank you!</p>
","python, machine-learning, memory-management, nlp, word-embedding","<p>What task do you have in mind? If this is a similarity based task, you could simply use the <code>load_word2vec_format</code> method in gensim, this allows you to pass in a limit to the number of vectors loaded. The vectors in something like the Googlenews set are ordered by frequency, this will give you the critical vectors. 
This also makes sense theoretically as the words with low frequency will usually have relatively bad representations.</p>
",1,1,413,2019-03-18 21:53:17,https://stackoverflow.com/questions/55230575/is-there-an-alternative-to-fully-loading-pre-trained-word-embeddings-in-memory
How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?,"<p>I have seen that NLP models such as <a href=""https://github.com/google-research/bert"" rel=""noreferrer"">BERT</a> utilize WordPiece for tokenization. In WordPiece, we split the tokens like <strong><code>playing</code></strong> to <strong><code>play</code></strong> and <strong><code>##ing</code></strong>. It is mentioned that it covers a wider spectrum of Out-Of-Vocabulary (OOV) words. Can someone please help me explain how WordPiece tokenization is actually done, and how it handles effectively helps to rare/OOV words? </p>
","nlp, word-embedding","<p>WordPiece and BPE are two similar and commonly used techniques to segment words into subword-level in NLP tasks.
In both cases, the vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the symbols in the vocabulary are iteratively added to the vocabulary.</p>

<p>Consider the WordPiece algorithm from the <a href=""https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf"" rel=""noreferrer"">original paper</a> (wording slightly modified by me):</p>

<blockquote>
  <ol>
  <li>Initialize the word unit inventory with all the characters in the text.</li>
  <li>Build a language model on the training data using the inventory from 1.</li>
  <li>Generate a new word unit by combining two units out of the current word inventory to increment the word unit inventory by one. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model.</li>
  <li>Goto 2 until a predefined limit of word units is reached or the likelihood increase falls below a certain threshold.</li>
  </ol>
</blockquote>

<p>The <a href=""https://www.aclweb.org/anthology/P16-1162"" rel=""noreferrer"">BPE</a> algorithm only differs in Step 3, where it simply chooses the new word unit as the combination of the next most frequently occurring pair among the current set of subword units.</p>

<p><strong>Example</strong></p>

<p><em>Input text</em>: she walked . he is a dog walker . i walk</p>

<p><em>First 3 BPE Merges</em>:</p>

<ol>
<li><code>w</code> <code>a</code> = <code>wa</code></li>
<li><code>l</code> <code>k</code> = <code>lk</code></li>
<li><code>wa</code> <code>lk</code> = <code>walk</code></li>
</ol>

<p>So at this stage, your vocabulary includes all the initial characters, along with <code>wa</code>, <code>lk</code>, and <code>walk</code>. You usually do this for a fixed number of merge operations.</p>

<p><strong>How does it handle rare/OOV words?</strong></p>

<p>Quite simply, OOV words are impossible if you use such a segmentation method. Any word which does not occur in the vocabulary will be broken down into subword units. Similarly, for rare words, given that the number of subword merges we used is limited, the word will not occur in the vocabulary, so it will be split into more frequent subwords.</p>

<p><strong>How does this help?</strong></p>

<p>Imagine that the model sees the word <code>walking</code>. Unless this word occurs at least a few times in the training corpus, the model can't learn to deal with this word very well. However, it may have the words <code>walked</code>, <code>walker</code>, <code>walks</code>, each occurring only a few times. Without subword segmentation, all these words are treated as completely different words by the model.</p>

<p>However, if these get segmented as <code>walk@@ ing</code>, <code>walk@@ ed</code>, etc., notice that all of them will now have <code>walk@@</code> in common, which will occur much frequently while training, and the model might be able to learn more about it.</p>
",76,59,23126,2019-03-27 16:52:34,https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble
Limited range for TensorFlow Universal Sentence Encoder Lite embeddings?,"<p>Starting from the <a href=""https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder"" rel=""nofollow noreferrer"">universal-sentence-encoder</a> in TensorFlow.js, I noticed that the range of the numbers in the embeddings wasn't what I expected.  I was expecting some distribution between [0-1] or [-1,1] but don't see either of these.</p>

<p>For the sentence ""cats are great!"" here's a visualization, where each dimension is projected onto a scale from [-0.5, 0.5]:</p>

<p><a href=""https://i.sstatic.net/QICFt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QICFt.png"" alt=""enter image description here""></a></p>

<p>Here's the same kind of visualization for ""i wonder what this sentence's embedding will be"" (the pattern is similar for the first ~10 sentences I tried):</p>

<p><a href=""https://i.sstatic.net/fJivm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fJivm.png"" alt=""enter image description here""></a></p>

<p>To debug, I looked at whether the same kind of thing comes up in the <a href=""https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb"" rel=""nofollow noreferrer"">demo Colab notebook</a>, and it seems like it is.  Here's what I see if I see for the range of the embeddings for those two sentences:</p>

<pre class=""lang-py prettyprint-override""><code># NEW: added this, with different messages
messages = [""cats are great!"", ""sometimes models are confusing""]
values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  message_embeddings = session.run(
      encodings,
      feed_dict={input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape})

  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):
    print(""Message: {}"".format(messages[i]))
    print(""Embedding size: {}"".format(len(message_embedding)))
    message_embedding_snippet = "", "".join(
        (str(x) for x in message_embedding[:3]))
    print(""Embedding: [{}, ...]\n"".format(message_embedding_snippet))
    # NEW: added this, to show the range of the embedding output
    print(""Embedding range: [{}, {}]"".format(min(message_embedding), max(message_embedding)))
</code></pre>

<p>And the output shows:</p>

<pre><code>Message: cats are great!
Embedding range: [-0.05904272198677063, 0.05903803929686546]

Message: sometimes models are confusing
Embedding range: [-0.060731519013643265, 0.06075377017259598]
</code></pre>

<p>So this again isn't what I'm expecting - the range is more narrow than I'd expect.  I thought this might be a TF convention that I missed, but couldn't see it in the <a href=""https://tfhub.dev/google/universal-sentence-encoder-lite/2"" rel=""nofollow noreferrer"">TFHub page</a> or the <a href=""https://www.tensorflow.org/guide/embedding"" rel=""nofollow noreferrer"">guide to text embeddings</a> or in the <a href=""https://arxiv.org/pdf/1803.11175.pdf"" rel=""nofollow noreferrer"">paper</a> so am not sure where else to look without digging into the training code.</p>

<p>The colab notebook example code has an example sentence that says:</p>

<blockquote>
  <p>Universal Sentence Encoder embeddings also support short paragraphs.
  There is no hard limit on how long the paragraph is. Roughly, the
  longer the more 'diluted' the embedding will be.</p>
</blockquote>

<p>But the range of the embedding is roughly the same for all the other examples in the colab, even one word examples.</p>

<p>I'm assuming this range is not just arbitrary, and it does make sense to me that the range is centered in zero and small, but I'm trying to understand how this scale came to be.</p>
","tensorflow, word-embedding, tensorflow.js, tensorflowjs-converter","<p>The output of the universal sentence encoder is a vector of length 512, with an L2 norm of (approximately) 1.0.  You can check this by calculating the inner product </p>

<pre><code>ip = 0
for i in range(512):
  ip +=  message_embeddings[0][i] * message_embeddings[0][i]

print(ip)

&gt; 1.0000000807544893
</code></pre>

<p>The implications are that:</p>

<ul>
<li>Most values are likely to be in a narrow range centered around zero</li>
<li>The largest possible single value in the vector is 1.0 - and this would only happen if all other values are exactly 0.</li>
<li>Similarly the smallest possible value is -1.</li>
<li>If we take a random vector of length 512, with values distributed uniformly, and then normalize it to unit magnitude, we expect to see values in a range similar to what  you see.</li>
</ul>

<pre><code>rand_uniform = np.random.uniform(-1, 1, 512)
l2 = np.linalg.norm(rand_uniform)
plt.plot(rand_uniform / l2, 'b.')
axes = plt.gca()
axes.set_ylim([-0.5, 0.5])
</code></pre>

<p><a href=""https://i.sstatic.net/fMeED.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/fMeED.png"" alt=""enter image description here""></a></p>

<p>Judging visually, the distribution of excitations does not look uniform, but rather is biased toward extremes.</p>
",6,2,1324,2019-03-28 02:48:17,https://stackoverflow.com/questions/55389456/limited-range-for-tensorflow-universal-sentence-encoder-lite-embeddings
How to specify an input with a list of arrays to Embedding layer in Keras?,"<p>I'm trying to do some word-level text generation and stuck with the foloowing problem:</p>

<p>My input looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>   tokenized_seq = [[w2v_model.wv.vocab[word].index for word in w2v_data[i]] for i in range(len(w2v_data))]
   x_seq = []
   y_seq = []

   for seq in tokenized_seq:
      x_seq.append(seq[:-1])
      y_seq.append([seq[-1]])
</code></pre>

<p>So, I'm going along the sequence (encoded words usnig word2vec) with the rolling window of the fixed size (tokenized _seq is a list of sequence with fixed length).</p>

<p>Look the example:  </p>

<p>Code block:</p>

<pre class=""lang-py prettyprint-override""><code>print(x_seq[0], '-&gt;', y_seq[0])  
print(' '.join([w2v_model.wv.index2word[i] for i in x_seq[0]]), '-&gt;', w2v_model.wv.index2word[y_seq[0].pop()]) 
</code></pre>

<p>Output:</p>

<pre><code>[608, 1661, 1, 4260, 1, 3, 2978, 741, 0, 153, 740, 1, 12004] -&gt; [109]
часть первая . i . — eh bien , mon prince . gênes -&gt; et
</code></pre>

<p>So, then, I'm trying to input all above to Embedding layer.</p>

<pre class=""lang-py prettyprint-override""><code>model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emdedding_size,
                    input_length=avg_sent_len-1,
                    weights=[predtrained_weights]
                    trainable=False))

model.add(Bidirectional(LSTM(units=128)))

model.add(Dense(units=vocab_size, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_seq, y_seq,
                   epochs=10,
                   batch_size=128,
                   validation_split=0.2,
                   verbose=2)

</code></pre>

<p>Embedding parameters are:</p>

<pre><code>predtrained_weights = w2v_model.wv.vectors
vocab_size, emdedding_size = w2v_model.wv.vectors.shape
</code></pre>

<p><code>avg_sent_len</code> is the len of each seqence in <code>x_seq</code></p>

<p>The model compiles well, but when fitting I get the following error:</p>

<pre><code>ValueError: Error when checking target: expected dense_40 to have shape (31412,) but got array with shape (223396,) 
</code></pre>

<p>(31412,) is <code>vocab_size</code>
223396 is <code>x_seq</code> or <code>y_seq</code> length (number of input sequences)
So, could anybody help me?</p>
","python, tensorflow, keras, word2vec, word-embedding","<p>You input <code>x_seq</code> should be one numpy array of shape <code>(batch_size, seq_len)</code>. Try adding <code>x_seq = np.array(x_seq)</code>.</p>
",0,1,1707,2019-04-02 09:05:49,https://stackoverflow.com/questions/55470854/how-to-specify-an-input-with-a-list-of-arrays-to-embedding-layer-in-keras
Keras input specification for word2vec vectors,"<p>I read all the other answers regarding this topic, but my use case is slightly different.</p>

<p>I have a numpy array of shape (800,128,1). Each element in the 800 elements stores a word2vec embedding of shape (128,1). Now I wanted to send this as input with batch size 64 to a Keras model with first layer as Input layer. I'm getting the following error:</p>

<pre><code>expected party to have 2 dimensions, but got array with shape (800, 128, 1)
</code></pre>

<p>I understand that the input layer requires 2 dimensions, but which two? Or should I specify input shape on my own as three dimensional?</p>

<p>The input layer is currently this:</p>

<pre><code>Input(shape = (embedding_size, ), name = 'party')
</code></pre>
","python, tensorflow, keras, word2vec, word-embedding","<p>The shape of input is <code>(embedding_size,)</code>, where <code>embedding_size</code> is presumably 128. So the input is expected to be an array of shape <code>(batch_size, embedding_size)</code>, not <code>(batch_size, embedding_size, 1)</code>. You need to reshape your array to omit the last dimension of size 1.  </p>
",0,-1,382,2019-04-02 10:36:17,https://stackoverflow.com/questions/55472683/keras-input-specification-for-word2vec-vectors
Is there a way to increase dimensionality of pre-trained Word Embeddings?,"<p>I am almost newly exposed to NLP research, struggling with NLP and Machine Learning techniques that are used in NLP.</p>

<p>The question that I'm dealing with now is if there is some method to increase the dimensionality of pre-trained word embeddings (like GloVe embeddings) from a fixed size 100 to let's say 512? </p>

<p>The reason I'm asking such question is that I used these embeddings to train RNN network with a pre-defined dimension of 100. Now, I have switched to self-attention mechanism (Transformers) where the model is highly sensitive to training parameters. So, I was wondering if I could somehow (for example, using perceptron, or maybe MLP) to transform 100d embeddings to a new space with 512d. </p>

<p>I googled this before asking here, but ended up with finding no reliable source in the end.</p>
","machine-learning, neural-network, nlp, word-embedding","<p>I understand that you want to feed a <code>100d</code> embedding to a network that takes an input of dimension <code>512</code>. To do this, you need to project up the embedding vectors to a higher dimension. You can use a simple feed-forward/linear layer that takes in the input of size 100. Make the hidden size of the layer to be the desired size which is 512 in this case. Also, note that this should be part of the entire network that is being trained, ie., the feed-forward layer should be trainable.</p>
",2,2,1395,2019-04-05 17:21:06,https://stackoverflow.com/questions/55540541/is-there-a-way-to-increase-dimensionality-of-pre-trained-word-embeddings
How to cluster similar sentences using BERT,"<p>For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a sentence and using HDBSCAN/KMeans clustering to group similar sentences.</p>

<p>A good example of the implementation can be seen in this short article: <a href=""http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/"" rel=""noreferrer"">http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/</a></p>

<p>I would like to do the same thing using BERT (using the BERT python package from hugging face), however I am rather unfamiliar with how to extract the raw word/sentence vectors in order to input them into a clustering algorithm. I know that BERT can output sentence representations - so how would I actually extract the raw vectors from a sentence?</p>

<p>Any information would be helpful.</p>
","python, nlp, artificial-intelligence, word-embedding, bert-language-model","<p>As <a href=""https://stackoverflow.com/users/4935974/subham-kumar"">Subham Kumar</a> <a href=""https://stackoverflow.com/a/62859000/395857"">mentioned</a>, one can use this Python 3 library to compute sentence similarity: <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>
<p>The library has a few <a href=""https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/clustering"" rel=""noreferrer"">code examples</a> to perform clustering:</p>
<p><a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/fast_clustering.py"" rel=""noreferrer""><code>fast_clustering.py</code></a>:</p>
<pre><code>&quot;&quot;&quot;
This is a more complex example on performing clustering on large scale dataset.

This examples find in a large set of sentences local communities, i.e., groups of sentences that are highly
similar. You can freely configure the threshold what is considered as similar. A high threshold will
only find extremely similar sentences, a lower threshold will find more sentence that are less similar.

A second parameter is 'min_community_size': Only communities with at least a certain number of sentences will be returned.

The method for finding the communities is extremely fast, for clustering 50k sentences it requires only 5 seconds (plus embedding comuptation).

In this example, we download a large set of questions from Quora and then find similar questions in this set.
&quot;&quot;&quot;
from sentence_transformers import SentenceTransformer, util
import os
import csv
import time


# Model for computing sentence embeddings. We use one trained for similar questions detection
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# We donwload the Quora Duplicate Questions Dataset (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)
# and find similar question in it
url = &quot;http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv&quot;
dataset_path = &quot;quora_duplicate_questions.tsv&quot;
max_corpus_size = 50000 # We limit our corpus to only the first 50k questions


# Check if the dataset exists. If not, download and extract
# Download dataset if needed
if not os.path.exists(dataset_path):
    print(&quot;Download dataset&quot;)
    util.http_get(url, dataset_path)

# Get all unique sentences from the file
corpus_sentences = set()
with open(dataset_path, encoding='utf8') as fIn:
    reader = csv.DictReader(fIn, delimiter='\t', quoting=csv.QUOTE_MINIMAL)
    for row in reader:
        corpus_sentences.add(row['question1'])
        corpus_sentences.add(row['question2'])
        if len(corpus_sentences) &gt;= max_corpus_size:
            break

corpus_sentences = list(corpus_sentences)
print(&quot;Encode the corpus. This might take a while&quot;)
corpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True)


print(&quot;Start clustering&quot;)
start_time = time.time()

#Two parameters to tune:
#min_cluster_size: Only consider cluster that have at least 25 elements
#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar
clusters = util.community_detection(corpus_embeddings, min_community_size=25, threshold=0.75)

print(&quot;Clustering done after {:.2f} sec&quot;.format(time.time() - start_time))

#Print for all clusters the top 3 and bottom 3 elements
for i, cluster in enumerate(clusters):
    print(&quot;\nCluster {}, #{} Elements &quot;.format(i+1, len(cluster)))
    for sentence_id in cluster[0:3]:
        print(&quot;\t&quot;, corpus_sentences[sentence_id])
    print(&quot;\t&quot;, &quot;...&quot;)
    for sentence_id in cluster[-3:]:
        print(&quot;\t&quot;, corpus_sentences[sentence_id])

</code></pre>
<p><a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/kmeans.py"" rel=""noreferrer""><code>kmeans.py</code></a>:</p>
<pre><code>&quot;&quot;&quot;
This is a simple application for sentence embeddings: clustering

Sentences are mapped to sentence embeddings and then k-mean clustering is applied.
&quot;&quot;&quot;
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Corpus with example sentences
corpus = ['A man is eating food.',
          'A man is eating a piece of bread.',
          'A man is eating pasta.',
          'The girl is carrying a baby.',
          'The baby is carried by the woman',
          'A man is riding a horse.',
          'A man is riding a white horse on an enclosed ground.',
          'A monkey is playing drums.',
          'Someone in a gorilla costume is playing a set of drums.',
          'A cheetah is running behind its prey.',
          'A cheetah chases prey on across a field.'
          ]
corpus_embeddings = embedder.encode(corpus)

# Perform kmean clustering
num_clusters = 5
clustering_model = KMeans(n_clusters=num_clusters)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = [[] for i in range(num_clusters)]
for sentence_id, cluster_id in enumerate(cluster_assignment):
    clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in enumerate(clustered_sentences):
    print(&quot;Cluster &quot;, i+1)
    print(cluster)
    print(&quot;&quot;)
</code></pre>
<p><a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/agglomerative.py"" rel=""noreferrer""><code>agglomerative.py</code></a>:</p>
<pre><code>&quot;&quot;&quot;
This is a simple application for sentence embeddings: clustering

Sentences are mapped to sentence embeddings and then agglomerative clustering with a threshold is applied.
&quot;&quot;&quot;
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
import numpy as np

embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Corpus with example sentences
corpus = ['A man is eating food.',
          'A man is eating a piece of bread.',
          'A man is eating pasta.',
          'The girl is carrying a baby.',
          'The baby is carried by the woman',
          'A man is riding a horse.',
          'A man is riding a white horse on an enclosed ground.',
          'A monkey is playing drums.',
          'Someone in a gorilla costume is playing a set of drums.',
          'A cheetah is running behind its prey.',
          'A cheetah chases prey on across a field.'
          ]
corpus_embeddings = embedder.encode(corpus)

# Normalize the embeddings to unit length
corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)

# Perform kmean clustering
clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = {}
for sentence_id, cluster_id in enumerate(cluster_assignment):
    if cluster_id not in clustered_sentences:
        clustered_sentences[cluster_id] = []

    clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in clustered_sentences.items():
    print(&quot;Cluster &quot;, i+1)
    print(cluster)
    print(&quot;&quot;)
</code></pre>
",8,35,47623,2019-04-10 18:31:20,https://stackoverflow.com/questions/55619176/how-to-cluster-similar-sentences-using-bert
What are some of the data preparation steps or techniques one needs to follow when dealing with multi-lingual data?,"<p>I'm working on multilingual word embedding code where I need to train my data on English and test it on Spanish. I'll be using the MUSE library by Facebook for the word-embeddings.
I'm looking for a way to pre-process both my data the same way. I've looked into diacritics restoration to deal with the accents. </p>

<p>I'm having trouble coming up with a way in which I can carefully remove stopwords, punctuations and weather or not I should lemmatize. </p>

<p>How can I uniformly pre-process both the languages to create a vocabulary list which I can later use with the MUSE library.</p>
","nlp, word-embedding","<p>Hi Chandana I hope you're doing well.  I would look into using the library spaCy <a href=""https://spacy.io/api/doc"" rel=""nofollow noreferrer"">https://spacy.io/api/doc</a> the man that created it has a youtube video in which he discusses the implementation of of NLP in other languages.  Below you will find code that will lemmatize and remove stopwords.  as far as punctuation you can always set specific characters such as accent marks to ignore.  Personally I use KNIME which is free and open source to do preprocessing.  You will have to install nlp extentions but what is nice is that they have different extensions for different languages you can install here: <a href=""https://www.knime.com/knime-text-processing"" rel=""nofollow noreferrer"">https://www.knime.com/knime-text-processing</a> the Stop word filter (since 2.9) and the Snowball stemmer node can be applied for Spanish language. Make sure to select the right language in the dialog of the node. Unfortunately there is no part of speech tagger node for Spanish so far.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code># Create functions to lemmatize stem, and preprocess

# turn beautiful, beautifuly, beautified into stem beauti 
def lemmatize_stemming(text):
    stemmer = PorterStemmer()
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

# parse docs into individual words ignoring words that are less than 3 letters long
# and stopwords: him, her, them, for, there, ect since ""their"" is not a topic.
# then append the tolkens into a list
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        newStopWords = ['your_stopword1', 'your_stop_word2']
        if token not in gensim.parsing.preprocessing.STOPWORDS and token not in newStopWords and len(token) &gt; 3:
            nltk.bigrams(token)
            result.append(lemmatize_stemming(token))
    return result</code></pre>
</div>
</div>
</p>

<p>I hope this helps let me know if you have any questions :)</p>
",1,3,250,2019-04-12 18:43:38,https://stackoverflow.com/questions/55657802/what-are-some-of-the-data-preparation-steps-or-techniques-one-needs-to-follow-wh
Encoding problem while training my own Glove model,"<p>I am training a GloVe model with my own corpus and I have troubles to save it/load it in an <code>utf-8</code> format.</p>

<p>Here what I tried: </p>

<pre><code>from glove import Corpus, Glove

#data
lines = [['woman', 'umbrella', 'silhouetted'], ['person', 'black', 'umbrella']]

#GloVe training
corpus = Corpus() 
corpus.fit(lines, window=4)
glove = Glove(no_components=4, learning_rate=0.1)
glove.fit(corpus.matrix, epochs=10, no_threads=8, verbose=True)
glove.add_dictionary(corpus.dictionary)
glove.save('glove.model.txt')
</code></pre>

<p>The saved file <code>glove.model.txt</code> is unreadable and I can't succeed to save it with a <code>utf-8</code> encoding.</p>

<p>When I try to read it, for exemple by converting it in a Word2Vec format:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove2word2vec(glove_input_file=""glove.model.txt"", 
word2vec_output_file=""gensim_glove_vectors.txt"")    

model = KeyedVectors.load_word2vec_format(""gensim_glove_vectors.txt"", binary=False)
</code></pre>

<p>I have the following error:</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>Any idea on how I could use my own GloVe model ?  </p>
","python, encoding, nlp, word-embedding, glove","<p>I just found myself a way to save the data with an <code>utf-8</code> format, I'm sharing it here in case someone faces the same problem</p>

<p>Instead of using the glove saving method <code>glove.save('glove.model.txt')</code> try to simulate by yourself a glove record:</p>

<pre><code>with open(""results_glove.txt"", ""w"") as f:
    for word in glove.dictionary:
        f.write(word)
        f.write("" "")
        for i in range(0, vector_size):
            f.write(str(glove.word_vectors[glove.dictionary[word]][i]))
            f.write("" "")
        f.write(""\n"")
</code></pre>

<p>Then you will be able to read it. </p>
",2,1,1452,2019-04-15 16:13:35,https://stackoverflow.com/questions/55693318/encoding-problem-while-training-my-own-glove-model
keras - embedding layer mask_zero causing exception at subsequent layers,"<p>I am working on a model based on <a href=""https://arxiv.org/abs/1709.04250"" rel=""nofollow noreferrer"">this</a> paper and I am getting an exception due to <code>GlobalMaxPooling1D</code> layer not supporting masking. </p>

<p>I have an <code>Embedding</code> layer with <code>mask_zero</code> argument set to <code>True</code>. However, since a subsequent <code>GlobalMaxPooling1D</code> layer does not support masking, I am getting an exception. The exception is expected, as it is actually stated in <a href=""https://keras.io/layers/embeddings/"" rel=""nofollow noreferrer"">the documentation of the Embedding layer</a> that <em>any subsequent layers after an <code>Embedding</code> layer with <code>mask_zero = True</code> should support masking</em>.</p>

<p>However, as I am processing sentences with variable number of words in them, I do need the masking in the <code>Embedding</code> layer. (i.e. due to the varying length of input) My question is, how should I alter my model that masking remains a part of the model, and does not cause a problem at <code>GlobalMaxPooling1D</code> layer?</p>

<p>Below is the code for the model.</p>

<pre><code>model = Sequential()
embedding_layer = Embedding(dictionary_size, num_word_dimensions,
                            weights=[embedding_weights], mask_zero=True,
                            embeddings_regularizer=regularizers.l2(0.0001))
model.add(TimeDistributed(embedding_layer,
                          input_shape=(max_conversation_length, timesteps)))

model.add(TimeDistributed(Bidirectional(LSTM(m // 2, return_sequences=True,
                                             kernel_regularizer=regularizers.l2(0.0001)))))
model.add(TimeDistributed(Dropout(0.2)))
model.add(TimeDistributed(GlobalMaxPooling1D()))
model.add(Bidirectional(LSTM(h // 2, return_sequences = True,
                             kernel_regularizer=regularizers.l2(0.0001)),
                        merge_mode='concat'))
model.add(Dropout(0.2))
crf = CRF(num_tags, sparse_target=False, kernel_regularizer=regularizers.l2(0.0001))
model.add(crf)
model.compile(optimizer, loss = crf.loss_function, metrics=[crf.accuracy])
</code></pre>
","python, machine-learning, keras, keras-layer, word-embedding","<blockquote>
  <p>However, as I am processing sentences with variable number of words in them, I do need the masking in the Embedding layer.</p>
</blockquote>

<p>Are you padding the sentences to make them have equal lengths? If so, then instead of masking, you can let the model find out on its own that the 0 is padding and therefore should be ignored. Therefore, you would not need an explicit masking. This approach is also used for dealing with missing values in the data as suggested in this <a href=""https://stackoverflow.com/a/52570297/2099607"">answer</a>.</p>
",2,1,398,2019-04-16 21:18:48,https://stackoverflow.com/questions/55716755/keras-embedding-layer-mask-zero-causing-exception-at-subsequent-layers
keras - evaluate_generator yielding different accuracy rates with same training data,"<p><strong>TL;DR</strong> My model is trained for 1 epochs - for testing purposes. Yet, when evaluated multiple times, it yields a different accuracy every time I run <code>evaluate_generator</code> method <strong>with the same training data</strong>. Why does that happen, and is there any way to get the same accuracy rate when evaluating the same trained data on the same model, multiple times?</p>

<hr>

<p>I am working on the linguistic problem of dialogue act classification and my model is based on <a href=""https://arxiv.org/abs/1709.04250"" rel=""nofollow noreferrer"">this</a> paper. Using tools provided by <code>keras</code> and <code>keras_contrib</code> repositories, I am replicating the exact model, but I have a question on why the evaluation gives out a different accuracy rate.</p>

<p>For reference, I trained the model for a single epoch, and then saved the trained model in a file, using the utility <code>save_load_utils</code> provided by <code>keras_contrib</code> module. However, whenever I run the model with <em>those</em> weights, which were trained for a single epoch, I am getting a different accuracy rate. I have tried it for 5-10 times and it ranges between 68% to 74%, which is rather large. As I am loading the pre-trained (i.e. for 1 epoch) model weights, I am expecting to get the same accuracy. (i.e. short of any precision differences of floating-point numbers) However, the variance in the results at this rate suggest that I may have done something incorrectly.</p>

<p>Does anyone have any idea as to why the <code>model.evaluate_generator</code> method generates results that are so different each time I run it with the same weight, even though I use the same, 1-epoch-trained model's weights to evaluate it? Is there any way to fix my evaluation code so that the accuracy obtained for the same trained model is the same every time I evaluate? (i.e. factoring in any minor differences due to floating-point arithmetic)</p>

<p>Below is all the relevant code. The code sample <em>is</em> a little more lengthy compared to a standard StackOverflow question, but I wanted to include all the relevant portions of the code. Apologies to Python programmers for the length of the code. I am a novice Python programmer and I probably could have coded the entire thing in a more concise, Python-idiomatic way.</p>

<p>Model preparation coda:</p>

<pre><code>def prepare_kadjk_model(max_mini_batch_size,
                        max_conversation_length, timesteps, num_word_dimensions,
                        word_to_index, word_vec_dict,
                        num_tags):
    #Hyperparameters
    m = timesteps
    h = timesteps

    model = Sequential()

    dictionary_size = len(word_to_index) + 1

    embedding_weights = numpy.zeros((dictionary_size, num_word_dimensions))
    for word, index in word_to_index.items():
        embedding_weights[index, :] = word_vec_dict[word]

    # define inputs here
    embedding_layer = Embedding(dictionary_size, num_word_dimensions,
                                weights=[embedding_weights],
                                embeddings_regularizer=regularizers.l2(0.0001))
    model.add(TimeDistributed(embedding_layer,
                              input_shape=(max_conversation_length, timesteps)))

    model.add(TimeDistributed(Bidirectional(LSTM(m // 2, return_sequences=True,
                                            kernel_regularizer=regularizers.l2(0.0001)))))
    model.add(TimeDistributed(Dropout(0.2)))
    model.add(TimeDistributed(GlobalMaxPooling1D()))
    model.add(Bidirectional(LSTM(h // 2, return_sequences = True,
                                 kernel_regularizer=regularizers.l2(0.0001)), merge_mode='concat'))
    model.add(Dropout(0.2))
    crf = CRF(num_tags, sparse_target=False, kernel_regularizer=regularizers.l2(0.0001))
    model.add(crf)
    model.compile(optimizer, loss = crf_loss,
                  metrics=[crf_accuracy])
    return model
</code></pre>

<p>Batch preparation functions:</p>

<pre><code>def form_mini_batches(dataset_x, max_mini_batch_size):
    num_conversations = len(dataset_x)

    # Form mini batches of equal-length conversations
    mini_batches = {}
    for i in range(num_conversations):
        num_utterances = len(dataset_x[i])
        if num_utterances in mini_batches:
            mini_batches[num_utterances].append( i )
        else:
            mini_batches[num_utterances] = [ i ]

    # Enforce max_batch_size on previously formed mini batches
    mini_batch_list = []
    for conversations in mini_batches.values():
        mini_batch_list += [conversations[x: x + max_mini_batch_size] for x in range(0, len(conversations), max_mini_batch_size)]

    return mini_batch_list


def kadjk_batch_generator(dataset_x, dataset_y, tag_indices,
                          mini_batch_list, max_conversation_length,
                          timesteps, num_word_dimensions, num_tags,
                          word_index_to_append, tag_index_to_append):
    num_mini_batches = len(mini_batch_list)

    # Shuffle the order of batches
    index_list = [x for x in range(num_mini_batches)]
    random.shuffle(index_list)

    k = -1
    while True:
        k = (k + 1) % len(index_list)
        index = index_list[k]
        conversation_indices = mini_batch_list[index]

        num_conversations = len(conversation_indices)
        batch_features = numpy.empty(shape = (num_conversations, max_conversation_length, timesteps),
                                     dtype = int)
        label_list = []

        for i in range(num_conversations):
            utterances = dataset_x[conversation_indices[i]]
            labels = copy.deepcopy(dataset_y[conversation_indices[i]])
            num_utterances = len(utterances)
            num_labels_to_append = max(0, max_conversation_length - len(labels))
            labels += [tag_index_to_append] * num_labels_to_append
            tags = to_categorical(labels, num_tags)
            del labels

            for j in range(num_utterances):
                utterance = copy.deepcopy(utterances[j])
                num_to_append = max(0, timesteps - len(utterance))
                if num_to_append &gt; 0:
                    appendage = [word_index_to_append] * num_to_append
                    utterance += appendage

                batch_features[i][j] = utterance
                del utterance

            remaining_space = (max_conversation_length - num_utterances, timesteps)
            batch_features[i][num_utterances:] = numpy.ones(remaining_space) * word_index_to_append
            label_list.append(tags)

        batch_labels = numpy.array(label_list)
        del label_list

        yield batch_features, batch_labels
</code></pre>

<p>Training function:</p>

<pre><code>def train_kadjk(model, training, validation, num_epochs_to_train, tag_indices, max_mini_batch_size,
                max_conversation_length, timesteps, num_word_dimensions, num_tags,
                end_of_line_word_index, uninterpretable_label_index):
    training_mini_batch_list = form_mini_batches(training[0], max_mini_batch_size)
    validation_mini_batch_list = form_mini_batches(validation[0], max_mini_batch_size)

    num_training_steps = len(training_mini_batch_list)
    num_validation_steps = len(validation_mini_batch_list)

    early_stop = EarlyStopping(patience = 5)
    change_learning_rate = LearningRateScheduler(learning_rate_scheduler)

    model.fit_generator(kadjk_batch_generator(training[0], training[1], tag_indices,
                                              training_mini_batch_list, max_conversation_length,
                                              timesteps, num_word_dimensions, num_tags,
                                              end_of_line_word_index, uninterpretable_label_index),
                        steps_per_epoch = num_training_steps,
                        epochs = num_epochs_to_train,
                        validation_data = kadjk_batch_generator(validation[0], validation[1],
                                                                tag_indices,
                                                                validation_mini_batch_list, 
                                                                max_conversation_length, timesteps,
                                                                num_word_dimensions, num_tags,
                                                                end_of_line_word_index,
                                                                uninterpretable_label_index),
                        validation_steps = num_validation_steps,
                        callbacks = [early_stop, change_learning_rate])
</code></pre>

<p>Evaluation function:</p>

<pre><code>def evaluate_kadjk(model, testing, tag_indices, max_mini_batch_size, max_conversation_length,
                   timesteps, num_word_dimensions, num_tags,
                   end_of_line_word_index, uninterpretable_label_index):
    testing_mini_batch_list = form_mini_batches(testing[0], max_mini_batch_size)
    num_testing_steps = len(testing_mini_batch_list)
    score = model.evaluate_generator(kadjk_batch_generator(testing[0], testing[1],
                                                           tag_indices,
                                                           testing_mini_batch_list, 
                                                           max_conversation_length, timesteps,
                                                           num_word_dimensions, num_tags,
                                                           end_of_line_word_index,
                                                           uninterpretable_label_index),
                                     steps = num_testing_steps)
    print(""len(score):"" + str(len(score)))
    print(""score:"" + str(score))
</code></pre>

<p>You may navigate <a href=""https://github.com/ilimugur/short-text-classification"" rel=""nofollow noreferrer"">here</a> for a fuller perspective of the graduate thesis project I am working on, but I tried to provide all the relevant bits of functionality required for anyone that can help out.</p>
","python, machine-learning, keras, deep-learning, word-embedding","<p>I dove deep into the Github issues of <code>keras</code> and found the probable cause of the error in <a href=""https://github.com/keras-team/keras/issues/6977#issuecomment-309241595"" rel=""nofollow noreferrer"">this</a> comment.</p>

<p>Apparently, similar to batch normalization layers, using <code>Dropout</code> layers causes variations as I described in the question. <code>Dropout</code> layers casues neurons to be dropped during training. So, when the training of the model is finished, not all the neurons in the initially compiled model are present.</p>

<p>If the model weights are saved using the function <code>keras_contrib.save_load_utils.save_all_weights</code>, then the model's weights are saved. However, once you terminate that process without saving the final neuron configuration (not just the weights) the final configuration of the model is lost. As stated <a href=""https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/utils/save_load_utils.py"" rel=""nofollow noreferrer"">here</a>, <code>save_all_weights</code>, which is the function I used to save the model, does not save the configuration of the model itself.</p>

<p>Consequently, if you compile the model within a different process, and load the weights you saved by using <code>keras_contrib.save_load_utils.load_all_weights</code>, even if you test the model with the same data you tested it with in the previous run, the newly compiled model has some extra neurons that were dropped out during the training of the original model. This difference in configuration, combined with the fact that they may be (and in this case <em>are</em>) initialized with random weights, causes the evaluation to give a different accuracy rate every time it is run.</p>

<p>The solution seems to be in recording not only the weights, but also all the configuration. This can simply be done by using the <code>save</code> method of the model instance instead of <code>keras_contrib.save_load_utils.save_all_weights</code>. Obviously, to load the entire model back in a different process, <code>keras.models.load_model</code> should be used instead of <code>keras_contrib.save_load_utils.load_all_weights</code>.</p>
",1,1,1069,2019-04-17 15:32:40,https://stackoverflow.com/questions/55731245/keras-evaluate-generator-yielding-different-accuracy-rates-with-same-training
Load a part of Glove vectors with gensim,"<p>I have a word list like<code>['like','Python']</code>and I want to load pre-trained Glove word vectors of these words, but the Glove file is too large, is there any fast way to do it? </p>

<p><strong>What I tried</strong></p>

<p>I iterated through each line of the file to see if the word is in the list and add it to a dict if True. But this method is a little slow.</p>

<pre><code>def readWordEmbeddingVector(Wrd):
    f = open('glove.twitter.27B/glove.twitter.27B.200d.txt','r')
    words = []
    a = f.readline()
    while a!= '':
        vector = a.split()
        if vector[0] in Wrd:
            words.append(vector)
            Wrd.remove(vector[0])
        a = f.readline()
    f.close()
    words_vector = pd.DataFrame(words).set_index(0).astype('float')
    return words_vector
</code></pre>

<p>I also tried below, but it loaded the whole file instead of vectors I need</p>

<pre class=""lang-py prettyprint-override""><code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('word2vec.twitter.27B.200d.txt')
</code></pre>

<p><strong>What I want</strong></p>

<p>Method like <code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</code> but I can set a word list to load.</p>
","python, gensim, word-embedding, glove","<p>There's no existing <code>gensim</code> support for filtering the words loaded via <code>load_word2vec_format()</code>. The closest is an optional <code>limit</code> parameter, which can be used to limit how many word-vectors are read (ignoring all subsequent vectors). </p>

<p>You could conceivably create your own routine to perform such filtering, using the source code for <code>load_word2vec_format()</code> as a model. As a practical matter, you might have to read the file twice: 1st, to find out exactly how many words in the file intersect with your set-of-words-of-interest (so you can allocate the right-sized array without trusting the declared size at the front of the file), then a second time to actually read the words-of-interest.</p>
",0,0,954,2019-04-19 15:25:32,https://stackoverflow.com/questions/55764137/load-a-part-of-glove-vectors-with-gensim
How can I train the word2vec model on my own corpus in R?,"<p>I would like to train the word2vec model on my own corpus using the <code>rword2vec</code> package in R. </p>

<p>The <code>word2vec</code> function that is used to train the model requires a <code>train_file</code>. The package's documentation in R simply notes that this is the training text data, but doesn't specify how it can be created. </p>

<p>The training data used in the example on GitHub can be downloaded here:
<a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow noreferrer"">http://mattmahoney.net/dc/text8.zip</a>. I can't figure out what type of file it is. </p>

<p>I've looked through the README file on the <a href=""https://github.com/mukul13/rword2vec"" rel=""nofollow noreferrer"">rword2vec GitHub page</a> and checked out the official word2vec page on <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google Code</a>.</p>

<p>My corpus is a <code>.csv</code> file with about 68,000 documents. File size is roughly 300MB. I realize that training the model on a corpus of this size might take a long time (or be infeasible), but I'm willing to train it on a subset of the corpus. I just don't know how to create the <code>train_file</code> required by the function.</p>
","r, word2vec, word-embedding, nlp","<p>After you unzip text8, you can open it with a text editor.  You'll see that it is one long document.  You will need to decide how many of your 68,000 documents you want to use for training and whether you want to concatenate them together of keep them as separate documents.  See <a href=""https://datascience.stackexchange.com/questions/11077/using-several-documents-with-word2vec"">https://datascience.stackexchange.com/questions/11077/using-several-documents-with-word2vec</a></p>
",2,3,681,2019-04-30 23:41:25,https://stackoverflow.com/questions/55929977/how-can-i-train-the-word2vec-model-on-my-own-corpus-in-r
What&#39;s the major difference between glove and word2vec?,"<p>What is the difference between word2vec and glove? 
Are both the ways to train a word embedding? if yes then how can we use both?</p>
","machine-learning, nlp, word2vec, word-embedding, glove","<p>Yes, they're both ways to train a word embedding. They both provide the same core output: one vector per word, with the vectors in a useful arrangement. That is, the vectors' relative distances/directions roughly correspond with human ideas of overall word relatedness, and even relatedness along certain salient semantic dimensions.</p>
<p>Word2Vec does incremental, 'sparse' training of a neural network, by repeatedly iterating over a training corpus.</p>
<p>GloVe works to fit vectors to model a giant word co-occurrence matrix built from the corpus.</p>
<p>Working from the same corpus, creating word-vectors of the same dimensionality, and devoting the same attention to meta-optimizations, the quality of their resulting word-vectors will be roughly similar. (When I've seen someone confidently claim one or the other is definitely better, they've often compared some tweaked/best-case use of one algorithm against some rough/arbitrary defaults of the other.)</p>
<p>I'm more familiar with Word2Vec, and my impression is that Word2Vec's training better scales to larger vocabularies, and has more tweakable settings that, if you have the time, might allow tuning your own trained word-vectors more to your specific application. (For example, using a small-versus-large <code>window</code> parameter can have a strong effect on whether a word's nearest-neighbors are 'drop-in replacement words' or more generally words-used-in-the-same-topics. Different downstream applications may prefer word-vectors that skew one way or the other.)</p>
<p>Conversely, some proponents of GLoVe tout that it does fairly well without needing metaparameter optimization.</p>
<p>You probably wouldn't use both, unless comparing them against each other, because they play the same role for any downstream applications of word-vectors.</p>
",24,28,18378,2019-05-10 06:10:19,https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec
What must be the output dim for word embedding in keras?,"<p>What must be the dimension of the dense embedding? How can we set the value of output_dim in keras for word_embedding?</p>

<pre><code> keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)
</code></pre>
","keras, deep-learning, word-embedding","<p>Embeding layer convert categorical variable(words) to vector. Output dimension specify how long this vector will be.</p>

<p>If you chose 10, than every word will be converted to vector with size 10. Value of this vector will be optimized during training. If you need figure out which output dimension is best for your problem, I recommend to find similar project and try use their output dimension size. Other option is try some sizes and judge which one suits best.</p>
",5,2,2027,2019-05-10 11:30:40,https://stackoverflow.com/questions/56076817/what-must-be-the-output-dim-for-word-embedding-in-keras
"Read GloVe pre-trained embeddings into R, as a matrix","<p>Working in R. I know the pre-trained GloVe embeddings (e.g., ""glove.6B.50d.txt"") can be found here: <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a>. However, I've had zero luck reading this text file into R so that the product is the word embedding matrix of words by vectors. Has anyone successfully done this, either pulling from a saved .txt file or from the site itself, and if so how was that text converted to a matrix in R?</p>
","r, nlp, word-embedding, text2vec, glove","<p>The text file is already in a tabular form, just use <code>read.csv(""path/to/glove.6B.50d.txt"", sep = "" "")</code> - note that the field/cell separator, in this case, is a space, not a comma.</p>
",1,1,1737,2019-05-10 11:33:41,https://stackoverflow.com/questions/56076874/read-glove-pre-trained-embeddings-into-r-as-a-matrix
Use Word2Vec to build a sense embedding,"<p>I really accept every hint on the following problem, because all what i want is to obtain that embedding from that dataset, I will write my all solution because (hopefully) the problem is just in some parts that i didn't consider.</p>
<p>I'm working with an annotated corpus, such that i have disambiguate words in a given sentence thanks to WordNet synsets id, that i will call tags. For example:</p>
<h3>Dataset</h3>
<pre class=""lang-xml prettyprint-override""><code>&lt;sentence&gt;
  &lt;text&gt;word1 word2 word3&lt;/text&gt;
  &lt;annotations&gt;
    &lt;annotation anchor=word1 lemma=lemma1&gt;tag1&lt;/annotation&gt;
    &lt;annotation anchor=word2 lemma=lemma2&gt;tag2&lt;/annotation&gt;
    &lt;annotation anchor=word3 lemma=lemma3&gt;tag3&lt;/annotation&gt;
  &lt;annotations&gt;
&lt;/sentence&gt;
</code></pre>
<p>Starting from this, given an embedding dimension that i will call n, i would like to build an embedding like this:</p>
<h3>Embedding</h3>
<pre><code>lemma1_tag1 dim 1 dim 2 dim 3 ... dim n
lemma2_tag2 dim 1 dim 2 dim 3 ... dim n
lemma3_tag3 dim 1 dim 2 dim 3 ... dim n
</code></pre>
<p>I thought to generate a corpus for Word2Vec starting from each text of each sentence, and replace each <code>anchor</code> with the respective <code>lemma1_tag1</code> (some words can contain more underscore, because i replaced space in lemmas with underscores). Since not every single word is annotated, after a simple preprocessing performed to remove stopwords and other punctuation, in the end i have something like the following example:</p>
<h3>Corpus Example</h3>
<pre><code>let just list most_recent_01730444a headline_06344461n
</code></pre>
<p>Since I'm just interested in annotated words, I also generated a predefined vocabulary to use it as Word2Vec vocabulary. This file contains on each row entries like:</p>
<h3>Vocabulary Example</h3>
<pre><code>lemma1_tag1
lemma2_tag2
</code></pre>
<p>So, after having defined a corpus and a vocabulary, I used them in Word2Vec toolkit:</p>
<h3>Terminal emulation</h3>
<pre><code>./word2vec -train data/test.txt -output data/embeddings.vec -size 300 -window 7 -sample 1e-3 -hs 1 -negative 0 -iter 10 -min-count 1 -read-vocab data/dictionary.txt -cbow 1
</code></pre>
<h3>Output</h3>
<pre><code>Starting training using file data/test.txt
Vocab size: 80
Words in train file: 20811
</code></pre>
<p>The problem is that the number of words in the corpus is 32000000+ and the number of words in the predefined vocabulary file is about 80000. I even tried in Python with Gensim, but (of course) I had the very same output. I think that the problem is that Word2Vec doesn't consider words in the format <code>lemma1_tag1</code> because of the underscore, and i don't know how to solve this problem. Any hint is appreciated, thank you in advance!</p>
","python, gensim, word2vec, word-embedding","<p>Both the original <code>word2vec.c</code> from Google, and gensim's <code>Word2Vec</code>, handle words with underscores just fine. </p>

<p>If both are looking at your input file, and both reporting just 80 unique words where you're expecting 100,000-plus, there's probably something wrong with your input-file. </p>

<p>What does <code>wc data/test.txt</code> report?</p>
",1,1,268,2019-05-16 11:12:27,https://stackoverflow.com/questions/56167224/use-word2vec-to-build-a-sense-embedding
How to use a Keras trained Embedded layer?,"<p>My model is:</p>

<pre><code>        model = Sequential()
        model.add(Embedding(input_dim=vocab_size,
                            output_dim=1024, input_length=self.SEQ_LENGTH))

        model.add(LSTM(vocab_size))

        model.add(Dropout(rate=0.5))
        model.add(Dense(vocab_size - 1, activation='softmax'))
</code></pre>

<p>And I have it trained. But now during inference time, how can I use that embedding?</p>
","python, tensorflow, keras, word-embedding","<p>Your question is solved <a href=""https://github.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/blob/master/lstm-gru-sentiment-analysis.ipynb"" rel=""nofollow noreferrer"">here</a>. As skeleton you can use this code:</p>

<pre><code>from tensorflow.python.keras.preprocessing.text import Tokenizer

tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(your_dataset) 

...

max_length = max_number_words
X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)
X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')

score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)
</code></pre>
",1,0,70,2019-05-21 21:15:54,https://stackoverflow.com/questions/56246472/how-to-use-a-keras-trained-embedded-layer
How to handle unseen words for pre-trained Glove word-embedding to avoid keyerror?,"<p>I want to extract features from pre-trained Glove embedding. But I got Keyerror  for certain words. Here is the list of word token. </p>

<pre><code>words1=['nuclear','described', 'according', 'called','physics', 'account','interesting','holes','theoretical','like','space','radiation','property','impulsed','darkfield']
</code></pre>

<p>I got Keyerror from 'impulsed', 'darkfield' words because probably these are the unseen words. How can I avoid this error ? . </p>

<p>Here is my full code: </p>

<pre><code>gloveFile = ""glove.6B.50d.txt""
import numpy as np
def loadGloveModel(gloveFile):
    print (""Loading Glove Model"")
    with open(gloveFile, encoding=""utf8"" ) as f:
        content = f.readlines()
    model = {}
    for line in content:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print (""Done."",len(model),"" words loaded!"")
    return model

model = loadGloveModel(gloveFile)

words1=['nuclear','described', 'according', 'called','physics', 'account','interesting','holes','theoretical','like','space','radiation','property','impulsed','darkfield']

import numpy as np
vector_2 = np.mean([model[word] for word in words1],axis=0) ## Got error message
</code></pre>

<p>Error message for 'impulsed' word </p>

<p><a href=""https://i.sstatic.net/QCuTD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QCuTD.png"" alt=""enter image description here""></a></p>

<h2>Is there any way to skip these unseen words?.</h2>
","python, nlp, word-embedding","<p>I would suggest below</p>

<ul>
<li>All missing words assigned to some unique vector (say all zeros)</li>
<li>Find words similar to it and use their embedding:

<ul>
<li>Try ngrams (prefix or suffux) of the words and check if it is in vocab</li>
<li>Stem the word and check if it is in vocab</li>
</ul></li>
<li>Simplest Solution: Use FastText. It assembles word vectors from subword n-grams which allows it to handle out of vocabulary words.</li>
</ul>
",2,2,4102,2019-05-24 05:46:38,https://stackoverflow.com/questions/56286510/how-to-handle-unseen-words-for-pre-trained-glove-word-embedding-to-avoid-keyerro
Word2Vec with POS not producing expected results?,"<p>I am trying to gauge the impact of part of speech information with Word2Vec embeddings but am not obtaining expected results.</p>

<p>I expected POS included word2vec embeddings to perform better in a machine translation task but it is actually performing worse.</p>

<p>I am creating two sets of embedding off of the same corpus using Gensim, one is normal Word2Vec, the other, I am changing tokens to ""[WORD]__[POS]"".</p>

<p>I am gauging differences in performance by using the embeddings in a Seq2Seq machine translation task. I am evaluating the two approaches with BLEU</p>

<p>This is how I am training the word2vec + POS embeddings with SpaCy:</p>

<pre><code>sentences = []
    for sent in doc.sents:
        tokens = []
        for t in sent:
            tokens += [""{}__{}"".format(t.text, t.pos_)]
        sentences += tokens
    pos_train += [sentences]
</code></pre>

<p>This is my benchmark machine translation model with Keras + Tensorflow:</p>

<pre><code>encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder = LSTM(LATENT_DIM, return_state=True)
_, state_h, state_c = encoder(encoder_inputs)
encoder_states = [state_h, state_c]

decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
</code></pre>

<p>With BLEU, the Word2Vec+POS approach consistently scores the same as Word2Vec or 0.01-0.02 points below the normal Word2Vec embeddings.</p>

<p>Does anyone know why this might be happening? Is there a gap in my reasoning or expectations?</p>
","keras, nlp, word2vec, word-embedding, seq2seq","<p>I, too, would have expected accurate part-of-speech info to improve translation – but I don't know if others have reported such an improvement. Some (uninformed) conjectures as to why it might not:</p>

<ul>
<li><p>maybe the POS tagging isn't very accurate with regard to one of the languages, or there are some other anomalous challenges specific to your data</p></li>
<li><p>maybe the method of creating composite tokens, with the internal <code>__</code>, is in a few corner cases interfering with evaluation – for example, if the original corpus retains any tokens which already had <code>__</code> in them</p></li>
<li><p>maybe for certain cases of insufficient data, the collision of similar-meaning homographs of different parts-of-speech actually <em>helps</em> thicken the vaguer meaning-to-meaning translation. (For example, maybe given the semantic relatedness of <code>shop_NOUN</code> and <code>shop_VERB</code>, it's better to have 100 colliding examples of <code>shop</code> than 50 of each.)</p></li>
</ul>

<p>Some debugging ideas (in addition to the obvious ""double-check everything""): </p>

<ul>
<li><p>look closely at exactly those test cases where the plain-vs-POS approaches differ in their scoring; see if there are any patterns – like strange tokens/punctuation, nonstandard grammar, etc – giving clues to where the <code>__POS</code> decorations hurt</p></li>
<li><p>try other language pairs, and other (private or public) concordance datasets, to see if elsewhere (or in general) the POS-tagging does help, and there's something extra-challenging about your particular dataset/language-pair</p></li>
<li><p>consider that the multiplication-of-tokens (by splitting homographs into POS-specific variants) has changed the model size &amp; word-distributions, in ways that might interact with other limits (like <code>min_count</code>, <code>max_vocab_size</code>, etc) in ways that modify training. In particular, perhaps the larger-vocabulary POS model should get more training epochs, or a larger word-vector dimensionality, to reflect its larger vocabulary with a lower average number of word-occurrences.</p></li>
</ul>

<p>Good luck!</p>
",2,0,324,2019-05-26 18:08:18,https://stackoverflow.com/questions/56316146/word2vec-with-pos-not-producing-expected-results
Uses of Embedding/ Embedding layer in deep learning,"<p>I am exploring deep learning methods especially LSTM to predict next word. Suppose, My data set is like this: Each data point consists of 7 features (7 different words)(A-G here) of different length.</p>

<pre><code> Group1  Group2............ Group 38
   A        B                   F
   E        C                   A
   B        E                   G
   C        D                   G
   C        F                   F
   D        G                   G
   .        .                   .
   .        .                   . 
</code></pre>

<p>I used one hot encoding as an Input layer. Here is the model </p>

<pre><code>main_input= Input(shape=(None,action_count),name='main_input')
lstm_out= LSTM(units=64,activation='tanh')(main_input)
lstm_out=Dropout(0.2)(lstm_out)
lstm_out=Dense(action_count)(lstm_out)
main_output=Activation('softmax')(lstm_out)
model=Model(inputs=[main_input],outputs=main_output)
print(model.summary())
</code></pre>

<p>Using this model. I got an accuracy of about 60%. 
My <strong>question</strong> is how can I use embedding layer for my problem. Actually, I do not know much about <strong>embedding</strong> (why, when and how it works)[I only know one hot vector does not carry much information]. I am wondering if <strong>embedding</strong> can improve accuracy. If someone can provide me guidance in these regards, it will be greatly beneficial for me. (At least whether uses of embedding is logical or not for my case)</p>
","deep-learning, lstm, recurrent-neural-network, word-embedding","<blockquote>
  <p>What are Embedding layers?</p>
</blockquote>

<p>They are layers which converts positive integers ( maybe word counts ) into fixed size dense vectors. They learn the so called embeddings for a particular text dataset ( in NLP tasks ). </p>

<blockquote>
  <p>Why are they useful?</p>
</blockquote>

<p>Embedding layers slowly learn the relationships between words. Hence, if you have a large enough corpus ( which probably contains all possible English words ), then vectors for words like ""king"" and ""queen"" will show some similarity in the mutidimensional space of the embedding.</p>

<blockquote>
  <p>How are used in Keras?</p>
</blockquote>

<p>The <a href=""https://keras.io/layers/embeddings/#embedding"" rel=""nofollow noreferrer""><code>keras.layers.Embedding</code></a> has the following configurations:</p>

<pre><code>keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None) 
</code></pre>

<p><em>Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]
This layer can only be used as the first layer in a model.</em></p>

<p>When the <code>input_dim</code> is the vocabulary size + 1. Vocabulary is the corpus of all the words used in the dataset. The <code>input_length</code> is the length of the input sequences whereas <code>output_dim</code> is the dimensionality of the output vectors ( the dimensions for the vector of a particular word ).</p>

<p>The layer can also be used wih <a href=""https://link.medium.com/6o98KzhF4W"" rel=""nofollow noreferrer"">pretrained word embeddings</a> like Word2Vec or GloVE.</p>

<blockquote>
  <p>Are they suitable for my use case?</p>
</blockquote>

<p>Absolutely, yes. For sentiment analysis, if we could generate a context ( embedding ) for a particular word then we could definitely increase its efficiency.</p>

<blockquote>
  <p>How can I use them in my use case?</p>
</blockquote>

<p>Follow the steps:</p>

<ol>
<li>You need to tokenize the sentences. Maybe with <code>keras.preprocessing.text.Tokenizer</code>.</li>
<li>Pad the sequences to a fixed length using <code>keras.preprocessing.sequence.pad_sequences</code>. This will be the <code>input_length</code> parameter for the Embedding layer.</li>
<li>Initialize the model with Embedding layer as the first layer.</li>
</ol>

<p>Hope this helps.</p>
",1,1,2264,2019-05-28 01:07:47,https://stackoverflow.com/questions/56333914/uses-of-embedding-embedding-layer-in-deep-learning
Why are word embeddings with linguistic features (e.g. Sense2Vec) not used?,"<p>Given that embedding systems such as Sense2Vec incorporate linguistic features such as part-of-speech, why are these embeddings not more commonly used?</p>

<p>Across popular work in NLP today, Word2Vec and GloVe are the most commonly used word embedding systems. Despite the fact that they only incorporate word information and does not have linguistic features of the words.</p>

<p>For example, in sentiment analysis, text classification or machine translation tasks, it makes logical sense that if the input incorporates linguistic features as well, performance could be improved. Particular when disambiguating words such as ""duck"" the verb and ""duck"" the noun.</p>

<p>Is this thinking flawed? Or is there some other practical reason why these embeddings are not more widely used.</p>
","nlp, artificial-intelligence, word2vec, word-embedding, sense2vec","<p>It's a very subjective question.  One reason is the pos-tagger itself. Pos-tagger is a probabilistic model which could add to the overall error/confusion.  </p>

<p>For eg. say you have dense representations for <code>duck-NP</code> and <code>duck-VB</code> but during run/inference time your pos-tagger tags 'duck' as something else then you wont even find it. Moreover it also effectively reduces the total number of times your system sees the word <code>duck</code> hence one could argue that representations generated would be weak.  </p>

<p>To top it off the main problem which <code>sense2vec</code> was addressing is contextualisation of word representations which has been solved by contextual representations like <code>BERT</code> and <code>ElMo</code> etc. without producing any of the above problems. </p>
",1,2,286,2019-05-31 02:44:59,https://stackoverflow.com/questions/56388012/why-are-word-embeddings-with-linguistic-features-e-g-sense2vec-not-used
Discrepancies in gensim doc2vec embedding vectors,"<p>I use gensim Doc2Vec package to train doc2vec embeddings. I would expect that two models trained with the identical parameters and data would have very close values of the doc2vec vectors. However, in my experience it is only true with doc2vec trained in the PV-DBOW without training word embedding (dbow_words = 0).
For PV-DM and for PV-DBOW with dbow_words = 1, i.e. every case the word embedding are trained along with doc2vec, the doc2vec embedding vectors for identically trained models are fairly different. </p>

<p>Here is my code</p>

<pre class=""lang-py prettyprint-override""><code>    from sklearn.datasets import fetch_20newsgroups
    from gensim import models
    import scipy.spatial.distance as distance
    import numpy as np
    from nltk.corpus import stopwords
    from string import punctuation
    def clean_text(texts,  min_length = 2):
        clean = []
        #don't remove apostrophes
        translator = str.maketrans(punctuation.replace('\'',' '), ' '*len(punctuation))
        for text in texts:
            text = text.translate(translator)
            tokens = text.split()
            # remove not alphabetic tokens
            tokens = [word.lower() for word in tokens if word.isalpha()]
            # filter out stop words
            stop_words = stopwords.words('english')
            tokens = [w for w in tokens if not w in stop_words]
            # filter out short tokens
            tokens = [word for word in tokens if len(word) &gt;= min_length]
            tokens = ' '.join(tokens)
            clean.append(tokens)
        return clean
    def tag_text(all_text, tag_type =''):
        tagged_text = []
        for i, text in enumerate(all_text):
            tag = tag_type + '_' + str(i)
            tagged_text.append(models.doc2vec.TaggedDocument(text.split(), [tag]))
        return tagged_text

    def train_docvec(dm, dbow_words, min_count, epochs, training_data):
        model = models.Doc2Vec(dm=dm, dbow_words = dbow_words, min_count = min_count)
        model.build_vocab(tagged_data)
        model.train(training_data, total_examples=len(training_data), epochs=epochs)    
        return model

    def compare_vectors(vector1, vector2):
        cos_distances = []
        for i in range(len(vector1)):
            d = distance.cosine(vector1[i], vector2[i])
            cos_distances.append(d)
        print (np.median(cos_distances))
        print (np.std(cos_distances))    

    dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))
    n_samples = len(dataset.data)
    data = clean_text(dataset.data)
    tagged_data = tag_text(data)
    data_labels = dataset.target
    data_label_names = dataset.target_names

    model_dbow1 = train_docvec(0, 0, 4, 30, tagged_data)
    model_dbow2 = train_docvec(0, 0, 4, 30, tagged_data)
    model_dbow3 = train_docvec(0, 1, 4, 30, tagged_data)
    model_dbow4 = train_docvec(0, 1, 4, 30, tagged_data)
    model_dm1 = train_docvec(1, 0, 4, 30, tagged_data)
    model_dm2 = train_docvec(1, 0, 4, 30, tagged_data)

    compare_vectors(model_dbow1.docvecs, model_dbow2.docvecs)
    &gt; 0.07795828580856323
    &gt; 0.02610614028793008

    compare_vectors(model_dbow1.docvecs, model_dbow3.docvecs)
    &gt; 0.6476179957389832
    &gt; 0.14797587172616306

    compare_vectors(model_dbow3.docvecs, model_dbow4.docvecs)
    &gt; 0.19878000020980835
    &gt; 0.06362519480831186

    compare_vectors(model_dm1.docvecs, model_dm2.docvecs)
    &gt; 0.13536489009857178
    &gt; 0.045365127475424386

    compare_vectors(model_dbow1.docvecs, model_dm1.docvecs)
    &gt; 0.6358324736356735
    &gt; 0.15150255674571805
</code></pre>

<blockquote>
  <p>UPDATE</p>
</blockquote>

<p>I tried, as suggested by gojomo, to compare the differences between the vectors, and, unfortunately, those are even worse:</p>

<pre class=""lang-py prettyprint-override""><code>def compare_vector_differences(vector1, vector2):
    diff1 = []
    diff2 = []
    for i in range(len(vector1)-1):
        diff1.append( vector1[i+1] - vector1[i])
    for i in range(len(vector2)-1):
        diff2[i].append(vector2[i+1] - vector2[i])
    cos_distances = []
    for i in range(len(diff1)):
        d = distance.cosine(diff1[i], diff2[i])
        cos_distances.append(d)
    print (np.median(cos_distances))
    print (np.std(cos_distances))    

compare_vector_differences(model_dbow1.docvecs, model_dbow2.docvecs)
&gt; 0.1134452223777771
&gt; 0.02676398444178949

compare_vector_differences(model_dbow1.docvecs, model_dbow3.docvecs)
&gt; 0.8464127033948898
&gt; 0.11423789350773429

compare_vector_differences(model_dbow4.docvecs, model_dbow3.docvecs)

&gt; 0.27400463819503784
&gt; 0.05984108730423529
</code></pre>

<blockquote>
  <p>SECOND UPDATE</p>
</blockquote>

<p>This time, after I finally understood gojomo, the things look fine.</p>

<pre class=""lang-py prettyprint-override""><code>def compare_distance_differences(vector1, vector2):
    diff1 = []
    diff2 = []
    for i in range(len(vector1)-1):
        diff1.append( distance.cosine(vector1[i+1], vector1[i]))
    for i in range(len(vector2)-1):
        diff2.append( distance.cosine(vector2[i+1], vector2[i]))
    diff_distances = []
    for i in range(len(diff1)):
        diff_distances.append(abs(diff1[i] - diff2[i]))
    print (np.median(diff_distances))
    print (np.std(diff_distances))    

compare_distance_differences(model_dbow1.docvecs, model_dbow2.docvecs)
&gt;0.017469733953475952
&gt;0.01659284710785352

compare_distance_differences(model_dbow1.docvecs, model_dbow3.docvecs)
&gt;0.0786697268486023
&gt;0.06092163158218411

compare_distance_differences(model_dbow3.docvecs, model_dbow4.docvecs)
&gt;0.02321992814540863
&gt;0.023095123172320778
</code></pre>
","gensim, word-embedding, doc2vec","<p>The doc-vectors (or word-vectors) of <code>Doc2Vec</code> &amp; <code>Word2Vec</code> models are only meaningfully comparable to other vectors that were co-trained, in the same interleaved training sessions. </p>

<p>Otherwise, randomness introduced by the algorithms (random-initialization &amp; random-sampling) and by slight differences in training ordering (from multithreading) will cause the trained positions of individual vectors to wander to arbitrarily different positions. Their <em>relative</em> distances/directions, to other vectors that shared interleaved training, should be about as equally-useful from one model to the next. </p>

<p>But there's no one right place for such a vector, and measuring the differences between the vector for document '1' (or word 'foo') in one model, and the corresponding vector in another model, isn't reflective of anything the models/algorithms are trained to provide.</p>

<p>There's more information in the Gensim FAQ:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">Q11: I've trained my Word2Vec/Doc2Vec/etc model repeatedly using the exact same text corpus, but the vectors are different each time. Is there a bug or have I made a mistake?</a></p>
",1,0,306,2019-06-02 04:51:47,https://stackoverflow.com/questions/56412272/discrepancies-in-gensim-doc2vec-embedding-vectors
word embedding of a lstm sequence,"<p>Suppose, I have a Seq2Seq model. I want to have the Embedding layer in this model.</p>

<p>Based on my research I can do it in three ways:</p>

<ol>
<li><p>train a word embedding separately on my data set or download a pre-trained word embedding, then use the weights of those embedding as the weight of the words in my data set. So here I do not need to have an embedding layer at all, I just load the weights of the already trained words into the words in my data set.</p></li>
<li><p>I create an embedding layer and set the trainable true, so not only I have an embedding, but also, that embedding will be trained based on my task</p></li>
<li><p>I create an Embedding layer, load already trained weights, and set trainable False. in this case, the weights will not get updated.</p></li>
</ol>

<p>(please correct me if Im wrong).</p>

<p>I have used the <code>first approach</code>. I want to know what will be the interpretation of the output of this code:</p>

<pre><code>model_wv = Word2Vec.load(""word2vec_50d_7w"")
embeddings = np.zeros((len(model_wv.wv.vocab), emb_dim))
for i in range(len(model_wv.wv.vocab)):
    # print(i)
    embedding_vector = model_wv.wv[model_wv.wv.index2word[i]]
    if embedding_vector is not None:
        embeddings[i] = embedding_vector

print(embeddings[[1,2,3],[3,4,1]])
</code></pre>

<p>this is the output:</p>

<pre><code>[-0.01566689 -1.36469996  0.59684211]
</code></pre>

<p>consider this <code>[1,2,3],[3,4,1]</code> as two sequence with <code>length=3</code>.</p>

<p>I was thinking we use word embedding in lstm to transform each word of the sequence into an embedding. I expected to see <code>two vectors</code> and <code>three items</code> in each vector.</p>

<p>The embedding is the word2vec in gensim,</p>

<p>Appreciate it if someone shed light on it where I am getting lost?</p>

<p>Thanks~</p>
","tensorflow, keras, lstm, gensim, word-embedding","<p>Your interpretations on the three variants of setting up the embedding layer is exactly correct as what I understand. </p>

<p>There are two major transfer learning techniques.</p>

<ol>
<li><p>Using the pre-learned model as a <strong>feature-vector</strong>. In your case, the word2vec model would be used as a lookup service to pre-process/convert tokens to ids and then to embedding vectors. These embedding vectors become the actual feature when you train your own model. (this is your #1)</p></li>
<li><p>Using a <strong>fine-tuning</strong> approach. Here you can choose to either continue training the pre-learned model (setting trainable=True) or fix the prelearned model (setting trainable=False). There could benefits in either approach. (This is your #2 and #3)</p></li>
</ol>

<p>(#1 and #3) produce similar result regarding quality from my experience. 
If you own a decent amount of training data, fine-tuning with trainable=True (#2) would be the best approach from my experience. </p>

<p>You problem here is a numpy issue. you probably should say, </p>

<pre><code>print(embeddings[[1,2,3]], embeddings[[3,4,1]])
</code></pre>

<p>Otherwise the <a href=""https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing"" rel=""nofollow noreferrer"">indexing</a> is not working as you expected. </p>

<pre><code>embeddings[[1,2,3],[3,4,1]]
</code></pre>

<p>This actually lookups the rows with indices 1, 2, 3 and get the column with indices 3, 4, 1 respectively. In other words, it picks up</p>

<pre><code>column 3 for row 1
column 4 for row 2
column 1 for row 3
</code></pre>
",1,0,556,2019-06-02 21:11:50,https://stackoverflow.com/questions/56418980/word-embedding-of-a-lstm-sequence
Getting error while adding embedding layer to lstm autoencoder,"<p>I have a seq2seq model which is working fine. I want to add an embedding layer in this network which I faced with an error.</p>

<p>this is my architecture using pretrained word embedding which is working fine(Actually the code is almost the same code available <a href=""https://github.com/PacktPublishing/Deep-Learning-with-Keras/blob/master/Chapter07/sent-thoughts-rnn.py"" rel=""nofollow noreferrer"">here</a>, but I want to include the Embedding layer in the model rather than using the pretrained embedding vectors):</p>

<pre><code>LATENT_SIZE = 20

inputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name=""input"")

encoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode=""sum"", name=""encoder_lstm"")(inputs)
encoded = Lambda(rev_ent)(encoded)
decoded = RepeatVector(SEQUENCE_LEN, name=""repeater"")(encoded)
decoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences=True), merge_mode=""sum"", name=""decoder_lstm"")(decoded)
autoencoder = Model(inputs, decoded)
autoencoder.compile(optimizer=""sgd"", loss='mse')
autoencoder.summary()
NUM_EPOCHS = 1

num_train_steps = len(Xtrain) // BATCH_SIZE
num_test_steps = len(Xtest) // BATCH_SIZE

checkpoint = ModelCheckpoint(filepath=os.path.join('Data/', ""simple_ae_to_compare""), save_best_only=True)
history = autoencoder.fit_generator(train_gen, steps_per_epoch=num_train_steps, epochs=NUM_EPOCHS, validation_data=test_gen, validation_steps=num_test_steps, callbacks=[checkpoint])
</code></pre>

<p>This is the summary:</p>

<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           (None, 45, 50)            0         
_________________________________________________________________
encoder_lstm (Bidirectional) (None, 20)                11360     
_________________________________________________________________
lambda_1 (Lambda)            (512, 20)                 0         
_________________________________________________________________
repeater (RepeatVector)      (512, 45, 20)             0         
_________________________________________________________________
decoder_lstm (Bidirectional) (512, 45, 50)             28400  
</code></pre>

<p>when I change the code to add the embedding layer like this:</p>

<pre><code>inputs = Input(shape=(SEQUENCE_LEN,), name=""input"")

embedding = Embedding(output_dim=EMBED_SIZE, input_dim=VOCAB_SIZE, input_length=SEQUENCE_LEN, trainable=True)(inputs)
encoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode=""sum"", name=""encoder_lstm"")(embedding)
</code></pre>

<p>I received this error:</p>

<pre><code>expected decoder_lstm to have 3 dimensions, but got array with shape (512, 45)
</code></pre>

<p>So my question, what is wrong with my model?</p>

<p><strong>Update</strong></p>

<p>So, this error is raised in the training phase. I also checked the dimension of the data being fed to the model, it is <code>(61598, 45)</code> which clearly do not have the number of features or here, <code>Embed_dim</code>.</p>

<p>But why this error raises in the decoder part? because in the encoder part I have included the Embedding layer, so it is totally fine. though when it reached the decoder part and it does not have the embedding layer so it can not correctly reshape it to three dimensional.</p>

<p>Now the question comes why this is not happening in a similar code?
this is my view, correct me if I'm wrong. because Seq2Seq code usually being used for Translation, summarization. and in those codes, in the decoder part also there is input (in the translation case, there is the other language input to the decoder, so the idea of having embedding in the decoder part makes sense).
Finally, here I do not have seperate input, that's why I do not need any separate embedding in the decoder part. However, I don't know how to fix the problem, I just know why this is happening:|</p>

<p><strong>Update2</strong></p>

<p>this is my data being fed to the model:</p>

<pre><code>   sent_wids = np.zeros((len(parsed_sentences),SEQUENCE_LEN),'int32')
sample_seq_weights = np.zeros((len(parsed_sentences),SEQUENCE_LEN),'float')
for index_sentence in range(len(parsed_sentences)):
    temp_sentence = parsed_sentences[index_sentence]
    temp_words = nltk.word_tokenize(temp_sentence)
    for index_word in range(SEQUENCE_LEN):
        if index_word &lt; sent_lens[index_sentence]:
            sent_wids[index_sentence,index_word] = lookup_word2id(temp_words[index_word])
        else:
            sent_wids[index_sentence, index_word] = lookup_word2id('PAD')

def sentence_generator(X,embeddings, batch_size, sample_weights):
    while True:
        # loop once per epoch
        num_recs = X.shape[0]
        indices = np.random.permutation(np.arange(num_recs))
        # print(embeddings.shape)
        num_batches = num_recs // batch_size
        for bid in range(num_batches):
            sids = indices[bid * batch_size : (bid + 1) * batch_size]
            temp_sents = X[sids, :]
            Xbatch = embeddings[temp_sents]
            weights = sample_weights[sids, :]
            yield Xbatch, Xbatch
LATENT_SIZE = 60

train_size = 0.95
split_index = int(math.ceil(len(sent_wids)*train_size))
Xtrain = sent_wids[0:split_index, :]
Xtest = sent_wids[split_index:, :]
train_w = sample_seq_weights[0: split_index, :]
test_w = sample_seq_weights[split_index:, :]
train_gen = sentence_generator(Xtrain, embeddings, BATCH_SIZE,train_w)
test_gen = sentence_generator(Xtest, embeddings , BATCH_SIZE,test_w)
</code></pre>

<p>and parsed_sentences is 61598 sentences which are padded.</p>

<p>Also, this is the layer I have in the model as Lambda layer, I just added here in case it has any effect ever:</p>

<pre><code>def rev_entropy(x):
        def row_entropy(row):
            _, _, count = tf.unique_with_counts(row)
            count = tf.cast(count,tf.float32)
            prob = count / tf.reduce_sum(count)
            prob = tf.cast(prob,tf.float32)
            rev = -tf.reduce_sum(prob * tf.log(prob))
            return rev

        nw = tf.reduce_sum(x,axis=1)
        rev = tf.map_fn(row_entropy, x)
        rev = tf.where(tf.is_nan(rev), tf.zeros_like(rev), rev)
        rev = tf.cast(rev, tf.float32)
        max_entropy = tf.log(tf.clip_by_value(nw,2,LATENT_SIZE))
        concentration = (max_entropy/(1+rev))
        new_x = x * (tf.reshape(concentration, [BATCH_SIZE, 1]))
        return new_x
</code></pre>

<p>Any help is appreciated:)</p>
","tensorflow, keras, lstm, autoencoder, word-embedding","<p>I tried the following example on Google colab (TensorFlow version <code>1.13.1</code>),</p>

<pre><code>from tensorflow.python import keras
import numpy as np

SEQUENCE_LEN = 45
LATENT_SIZE = 20
EMBED_SIZE = 50
VOCAB_SIZE = 100

inputs = keras.layers.Input(shape=(SEQUENCE_LEN,), name=""input"")

embedding = keras.layers.Embedding(output_dim=EMBED_SIZE, input_dim=VOCAB_SIZE, input_length=SEQUENCE_LEN, trainable=True)(inputs)

encoded = keras.layers.Bidirectional(keras.layers.LSTM(LATENT_SIZE), merge_mode=""sum"", name=""encoder_lstm"")(embedding)
decoded = keras.layers.RepeatVector(SEQUENCE_LEN, name=""repeater"")(encoded)
decoded = keras.layers.Bidirectional(keras.layers.LSTM(EMBED_SIZE, return_sequences=True), merge_mode=""sum"", name=""decoder_lstm"")(decoded)
autoencoder = keras.models.Model(inputs, decoded)
autoencoder.compile(optimizer=""sgd"", loss='mse')
autoencoder.summary()
</code></pre>

<p>And then trained the model using some random data,</p>

<pre><code>
x = np.random.randint(0, 90, size=(10, 45))
y = np.random.normal(size=(10, 45, 50))
history = autoencoder.fit(x, y, epochs=NUM_EPOCHS)
</code></pre>

<p>This solution worked fine. I feel like the issue might be the way you are feeding in labels/outputs for <code>MSE</code> calculation.</p>

<h2>Update</h2>

<h3>Context</h3>

<p>In the original problem, you are attempting to reconstruct word embeddings using a seq2seq model, where embeddings are fixed and pre-trained. However you want to use a trainable embedding layer as a part of the model it becomes very difficult to model this problem. Because you don't have fixed targets (i.e. targets change every single iteration of the optimization because your embedding layer is changing). Furthermore this will lead to a very unstable optimization problem, because the targets are changing all the time.</p>

<h3>Fixing your code</h3>

<p>If you do the following you should be able to get the code working. Here <code>embeddings</code> is the pre-trained GloVe vector <code>numpy.ndarray</code>.</p>

<pre><code>def sentence_generator(X, embeddings, batch_size):
    while True:
        # loop once per epoch
        num_recs = X.shape[0]
        embed_size = embeddings.shape[1]
        indices = np.random.permutation(np.arange(num_recs))
        # print(embeddings.shape)
        num_batches = num_recs // batch_size
        for bid in range(num_batches):
            sids = indices[bid * batch_size : (bid + 1) * batch_size]
            # Xbatch is a [batch_size, seq_length] array
            Xbatch = X[sids, :] 

            # Creating the Y targets
            Xembed = embeddings[Xbatch.reshape(-1),:]
            # Ybatch will be [batch_size, seq_length, embed_size] array
            Ybatch = Xembed.reshape(batch_size, -1, embed_size)
            yield Xbatch, Ybatch
</code></pre>
",1,3,1675,2019-06-03 20:16:43,https://stackoverflow.com/questions/56433993/getting-error-while-adding-embedding-layer-to-lstm-autoencoder
Sentence iterator to pass to Gensim language model,"<p>I am relatively new to NLP and I am trying to create my own words embeddings trained in my personal corpus of docs.</p>

<p>I am trying to implement the following code to create my own wordembedings:</p>

<pre><code>model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>with sentences being a list of sentences.
Since I can not pass thousands and thousands of sentences I need an iterator</p>

<pre><code># with mini batch_dir a directory with the text files
# MySentences is a class iterating over sentences.
sentences = MySentences(minibatch_dir) # a memory-friendly iterator
</code></pre>

<p>I found this solution by the creator of gensim:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>It does not work for me.
How can I create an iterator if I know how to get the list of sentences from every document?</p>

<p>And second very related question:
If I am aiming to compare documents similarity in a particular corpus, is always better to create from scratch word embeddings with all the documents of that particular corpus than using GloVec or word2vec? 
The amount of docs is around 40000.</p>

<p>cheers</p>

<p>More pre</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Your illustrated class <code>MySentences</code> assumes one sentence per line. That might not be the case for your data.</p>
<p>One thing to note is - calling Word2Vec(sentences, iter=1) will run two passes over the sentences iterator (or, in general iter+1 passes; default iter=5). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. These two (or, iter+1) passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and you’re able to initialize the vocabulary some other way:</p>
<pre><code>model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet
model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator
model.train(other_sentences)  # can be a non-repeatable, 1-pass generator
</code></pre>
<p>For example, if you are trying to read dataset stored in a database, your generator function to stream text directly from a database, will throw TypeError:</p>
<pre><code>TypeError: You can't pass a generator as the sentences argument. Try an iterator.
</code></pre>
<p>A generator can be consumed only once and then it’s forgotten. So, you can write a wrapper which has an iterator interface but uses the generator under the hood.</p>
<pre><code>class SentencesIterator():
    def __init__(self, generator_function):
        self.generator_function = generator_function
        self.generator = self.generator_function()

    def __iter__(self):
        # reset the generator
        self.generator = self.generator_function()
        return self

    def __next__(self):
        result = next(self.generator)
        if result is None:
            raise StopIteration
        else:
            return result
</code></pre>
<p>The generator function is stored as well so it can reset and be used in Gensim like this:</p>
<pre><code>from gensim.models import FastText

sentences = SentencesIterator(tokens_generator)
model = FastText(sentences) 
</code></pre>
",5,2,2396,2019-06-05 22:29:49,https://stackoverflow.com/questions/56468865/sentence-iterator-to-pass-to-gensim-language-model
Understanding number of params in Keras RNN and output shape dimension in Keras Embedding when RNN and Embedding are chained together,"<p>I have this Keras code from some youtube video:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN

model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32))
model.summary()
</code></pre>

<p>The output of the summary is this:</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 32)          320000    
_________________________________________________________________
simple_rnn_1 (SimpleRNN)     (None, 32)                2080      
=================================================================
Total params: 322,080
Trainable params: 322,080
Non-trainable params: 
</code></pre>

<p>First I don't understand why the number of params is 2080 in simple RNN. Next I don't get why output shape from the embedding layer is (None, None, 32)</p>
","keras, recurrent-neural-network, word-embedding","<p>For calculating the number of params of simpleRNN 
<a href=""https://stackoverflow.com/questions/50134334/number-of-parameters-for-keras-simplernn"">Number of parameters for Keras SimpleRNN</a>  </p>

<p>For your second question, the output shape of embedding layer is <code>(batch_size, input_length, output_dim)</code> since you didn't specifiy the <code>input_length</code> argument (length of input sequences) of embedding layer, it would take the default value which is <code>None</code> (variable).  </p>

<p>Also, since RNN blocks run in each time-step, you can add it to a variable time-step layer. However if you want to add Flatten followed by Dense Layers which take the whole previous layer as input, you have to specifiy the <code>input_length</code> in Embedding Layer</p>
",2,1,2586,2019-06-13 07:05:21,https://stackoverflow.com/questions/56575043/understanding-number-of-params-in-keras-rnn-and-output-shape-dimension-in-keras
Copying embeddings for gensim word2vec,"<p>I wanted to see if I can simply set new weights for gensim's Word2Vec without training. I get the 20 News Group data set from scikit-learn (from sklearn.datasets import fetch_20newsgroups) and trained an instance of Word2Vec on it:</p>

<pre><code>model_w2v = models.Word2Vec(sg = 1, size=300)
model_w2v.build_vocab(all_tokens)
model_w2v.train(all_tokens, total_examples=model_w2v.corpus_count, epochs = 30)
</code></pre>

<p>Here all_tokens is the tokenized data set. 
Then I created a new instance of Word2Vec without training </p>

<pre><code>model_w2v_new = models.Word2Vec(sg = 1, size=300)
model_w2v_new.build_vocab(all_tokens)
</code></pre>

<p>and set the embeddings of the new Word2Vec equal to the first one</p>

<pre><code>model_w2v_new.wv.vectors = model_w2v.wv.vectors
</code></pre>

<p>Most of the functions work as expected, e.g.</p>

<pre><code>model_w2v.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
model_w2v_new.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
</code></pre>

<p>and</p>

<pre><code>model_w2v.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
model_w2v_new.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
</code></pre>

<p>and</p>

<pre><code>entities_list = list(model_w2v.wv.vocab.keys()).remove('religion')

model_w2v.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
model_w2v_new.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
</code></pre>

<p>However, most_similar doesn't work:</p>

<pre><code>model_w2v.wv.most_similar(positive=['religion'], topn=3)
[('religions', 0.4796232581138611),
 ('judaism', 0.4426296651363373),
 ('theists', 0.43141329288482666)]

model_w2v_new.wv.most_similar(positive=['religion'], topn=3)
&gt;[('roderick', 0.22643062472343445),
&gt; ('nci', 0.21744996309280396),
&gt; ('soviet', 0.20012077689170837)]
</code></pre>

<p>What am I missing? </p>

<p>Disclaimer. I posted this question on <a href=""https://datascience.stackexchange.com/questions/53601/copying-embeddings-for-gensim-word2vec"">datascience.stackexchange</a> but got no response, hoping to have a better luck here. </p>
","gensim, word2vec, word-embedding","<p>Generally, your approach should work. </p>

<p>It's likely the specific problem you're encountering was caused by an extra probing step you took and is not shown in your code, because you had no reason to think it significant: some sort of <code>most_similar()</code>-like operation on <code>model_w2v_new</code> <strong>after</strong> its <code>build_vocab()</code> call but <strong>before</strong> the later, malfunctioning operations.</p>

<p>Traditionally, <code>most_similar()</code> calculations operate on a version of the vectors that has been normalized to unit-length. The 1st time these unit-normed vectors are needed, they're calculated – and then cached inside the model. So, if you then  replace the raw vectors with other values, but don't discard those cached values, you'll see results like you're reporting – essentially random, reflecting the randomly-initialized-but-never-trained starting vector values. </p>

<p>If this is what happened, just discarding the cached values should cause the next <code>most_similar()</code> to refresh them properly, and then you should get the results you expect:</p>

<pre><code>model_w2v_new.wv.vectors_norm = None
</code></pre>
",1,0,557,2019-06-14 03:52:24,https://stackoverflow.com/questions/56591149/copying-embeddings-for-gensim-word2vec
How to find similar words in Keras Word Embedding layer,"<p>From Stanford's CS244N course, I know Gensim provides a fantastic method to play around the embedding data: most_similar</p>

<p>I was trying to find some equivalent in Keras Embedding layer but I couldn't. It isn't possible out of the box from Keras? Or was it any wrapper on top of it?</p>
","keras, word-embedding","<p>A simple implementation would be:</p>

<pre class=""lang-py prettyprint-override""><code>def most_similar(emb_layer, pos_word_idxs, neg_word_idxs=[], top_n=10):
    weights = emb_layer.weights[0]

    mean = []
    for idx in pos_word_idxs:
        mean.append(weights.value()[idx, :])

    for idx in neg_word_idxs:
        mean.append(weights.value()[idx, :] * -1)

    mean = tf.reduce_mean(mean, 0)

    dists = tf.tensordot(weights, mean, 1)
    best = tf.math.top_k(dists, top_n)

    # Mask words used as pos or neg
    mask = []
    for v in set(pos_word_idxs + neg_word_idxs):
        mask.append(tf.cast(tf.equal(best.indices, v), tf.int8))
    mask = tf.less(tf.reduce_sum(mask, 0), 1)

    return tf.boolean_mask(best.indices, mask), tf.boolean_mask(best.values, mask)
</code></pre>

<p>Of course you need to know the indices of the words. I assume you have a <code>word2idx</code> mapping, so you can get them like this: <code>[word2idx[w] for w in pos_words]</code>.</p>

<p>To use it:</p>

<pre class=""lang-py prettyprint-override""><code># Assuming the first layer is the Embedding and you are interested in word with idx 10
idxs, vals = most_similar(model.layers[0], [10])

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    idxs = sess.run(idxs)
    vals = sess.run(vals)
</code></pre>

<p>Some potential improvements for that function:</p>

<ul>
<li>Make sure it returns <code>top_n</code> words (after the mask it returns less words)</li>
<li><code>gensim</code> uses normalised embeddings (L2_norm)</li>
</ul>
",2,2,917,2019-06-17 20:08:47,https://stackoverflow.com/questions/56638258/how-to-find-similar-words-in-keras-word-embedding-layer
Building a neural network that takes a created feature vector,"<p>To be more precise. Lets say I already have a vector that represents something (word, object, image...) and that I can not change the way I get it. What I would like to do is create a NN without the embedding and pooling layer and am wondering if tensorflow supports this kind of aproach. </p>

<p>Lets say my vector is 10 features long (10 floats). For each vector I also have a label, lets say there are 3 labels to chose from.</p>

<p>What I am (struggling/trying) to do is this. I would like to push this sort of vector input into a keras dense layer with relu activation and 10 neurons (stack maybe 2 or 3) and then as a final layer use sigmoid activation with 3 output neurons. </p>

<p>Then fit with labels on 40(?) epochs and so on...</p>

<p>My main question is well.. Is this possible? I have yet to finish the code and maybe I am asking this a bit too soon, but nevertheless. 
Is this how one would approach this or would you build the model from embedding layer down and would not use the already made vectors?</p>
","python, tensorflow, feature-extraction, word-embedding","<p>Indeed it is possible.</p>

<p>One way to do it is to create a generator function yielding the vectors (that will do your vector representation, whatever it is) you want to pass to the network. Then create a TensorFlow dataset by calling <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer""><code>tf.data.Dataset.from_generator</code></a>.</p>

<p>The model will be then probably just a <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential"" rel=""nofollow noreferrer""><code>Sequential</code></a> of dense layers.</p>
",1,0,319,2019-06-21 08:55:16,https://stackoverflow.com/questions/56700035/building-a-neural-network-that-takes-a-created-feature-vector
Create a matrix from a dict of dicts for calculating similarities between docs,"<p>Here is my problem:</p>

<p>I have a dataframe like this:</p>

<pre><code>id   tfidf_weights   
1    {word1: 0.01, word2: 0.01, word3: 0.01, ...}
2    {word4: 0.01, word5: 0.01, word6: 0.01, ...}
3    {word7: 0.01, word8: 0.01, word9: 0.01, ...}
4    {word10: 0.01, word11: 0.01, word12: 0.01, ...}
5    {word13: 0.01, word14: 0.01, word15: 0.01, ...}    
.
.
.
</code></pre>

<p>column 'id' represent the ids of the docs and 'tfidf_weights' the tfidf weight for each word of each docs.</p>

<p>from this dataframe, i can obtain a dict with the following structure:</p>

<pre><code>mydict = {1:{word1: 0.01, word2: 0.01, word3: 0.01, ...}, 2:{word4: 0.01, word5: 0.01, word6: 0.01, ...}, 3:{word7: 0.01, word8: 0.01, word9: 0.01, ...}, 4:{word10: 0.01, word11: 0.01, word12: 0.01, ...}, 5:{word13: 0.01, word14: 0.01, word15: 0.01, ...}, ...}
</code></pre>

<p>what i want to do is, from this dictionary, obtain a matrix like this:</p>

<pre><code>      word1     word2     word3     word4   ...
1     0.01      0.01      0.01      0.01     
2     0.01      0.01      0.01      0.01
3     0.01      0.01      0.01      0.01
4     0.01      0.01      0.01      0.01
5     0.01      0.01      0.01      0.01
.
.
.
</code></pre>

<p>Thank you for your help !</p>
","matrix, nlp, similarity, tf-idf, word-embedding","<p>You can convert a list of dictionaries into a dataframe by using the pandas DataFrame class directly.</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

a = [{""0"": 0}, {""1"": 1}]
df = pd.DataFrame(a)
</code></pre>

<p>To apply this to your problem, all you have to do is turn <code>mydict</code> into a list of dictionaries instead of a dictionary of dictionaries.</p>
",0,0,42,2019-07-03 13:04:56,https://stackoverflow.com/questions/56870824/create-a-matrix-from-a-dict-of-dicts-for-calculating-similarities-between-docs
How to check the performance of word embedding,"<p>I have used the gensim Word2Vec model and applied it in my list of documents. Well , the word embedding is getting created. I want to know if Word2Vec is performing well on my list of documents. Is there any metrics to measure that? How will I understand if Word2Vec has really worked well on my document corpus or should I try some different embedding?
Below is the code I have used from gensim.</p>

<pre><code>import gensim
model = gensim.models.Word2Vec(documents , size=150, window=10, min_count=2, sg=1, workers=10)
</code></pre>
","python, word2vec, word-embedding","<p>There's no universal definition of ""performing well"". It depends on your end-goals. </p>

<p>Why do you want to create word-vectors? What value do you expect them to provide? </p>

<p>With the answer to those questions, you can 1st review the results in an informal, ad-hoc fashion: look at some of the words nearest-neighbors (the results of <code>wordvecs.most_similar(query_word)</code>) to see if they make sense to you, for your needs and problem-domain. </p>

<p>But to really test whether your models are doing better over time, as you improve your data or model-parameters, you should form some repeatable, quantitative tests that match your end-goal. (For example: do you need certain pairs of words to be closer to each other than to some third word? Do you use the word-vectors as input to some other classification or info-retrieval process that has some known,desirable results?)</p>

<p>Run those tests, to score the model, then compare one model's score against another. </p>
",0,0,1333,2019-07-11 08:25:54,https://stackoverflow.com/questions/56984758/how-to-check-the-performance-of-word-embedding
Why does the TensorBoard display the wrong cosine distance?,"<p>i want to visualize word embeddings in the Projector from TensorBoard, but the cosine distances doesnt seem right.</p>

<p>If i compute the cosine distances via sklearn i get different results.</p>

<p>Am i using the TensorBoard Projector wrong?</p>

<p>TensorBoard:
<a href=""https://i.sstatic.net/3lEv0.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/3lEv0.png</a></p>

<p>Sklearn:
<a href=""https://i.sstatic.net/QGilv.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/QGilv.png</a> </p>

<pre><code>import os
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.contrib.tensorboard.plugins import projector

LOG_DIR = 'logs'
metadata = os.path.join(LOG_DIR, 'metadata.tsv')

emb_arr = []

arr = []

# category -&gt; dictionary
# category[""Category 1""] -&gt; array([[...,...,...,...,]]) # 300 dimensions

for category in category_embeddings:
    arr.appendcategory_embeddings[category][0]) 
embds_arr = np.asarray(arr)

with open(metadata, 'w', encoding=""utf-8"") as metadata_file:
    for key in category_embeddings.keys():
        metadata_file.write(key + ""\n"")

embds = tf.Variable(embds_arr, name='embeds')

with tf.Session() as sess:  
    saver = tf.train.Saver([embds])

    sess.run(embds.initializer)
    saver.save(sess, os.path.join(LOG_DIR, 'category.ckpt'))

    config = projector.ProjectorConfig()    
    config.model_checkpoint_path = os.path.join(LOG_DIR, 'checkpoint')

    config = projector.ProjectorConfig()
    embedding = config.embeddings.add()
    embedding.tensor_name = embds.name
    embedding.metadata_path = metadata

    projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)
</code></pre>
","python, tensorboard, word-embedding","<p>Solved,</p>

<p>i tested it with different datasets and training cycles, it seems to be a bug within TensorBoard.
Sklearn returns the correct reuslts for the original vector space and TensorBoard possibly calculates the distance from a reduced dimensionality.</p>

<p><a href=""https://github.com/tensorflow/tensorboard/issues/2421"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorboard/issues/2421</a></p>
",0,1,358,2019-07-12 15:07:39,https://stackoverflow.com/questions/57009664/why-does-the-tensorboard-display-the-wrong-cosine-distance
How to train a word embedding representation with gensim fasttext wrapper?,"<p>I would like to train my own word embeddings with fastext. However, after following the tutorial I can not manage to do it properly. So far I tried:</p>

<p>In:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

# Set file names for train and test data
corpus = df['sentences'].values.tolist()

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(sentences=corpus)
model_gensim
</code></pre>

<p>Out:</p>

<pre><code>&lt;gensim.models.fasttext.FastText at 0x7f6087cc70f0&gt;
</code></pre>

<p>In:</p>

<pre><code># train the model
model_gensim.train(
    sentences = corpus, 
    epochs = model_gensim.epochs,
    total_examples = model_gensim.corpus_count, 
    total_words = model_gensim.corpus_total_words
)

print(model_gensim)
</code></pre>

<p>Out:</p>

<pre><code>FastText(vocab=107, size=100, alpha=0.025)
</code></pre>

<p>However, when I try to look in a vocabulary words:</p>

<pre><code>print('return' in model_gensim.wv.vocab)
</code></pre>

<p>I get <code>False</code>, even the word is present in the sentences I am passing to the fast text model. Also, when I check the most similar words to return I am getting characters:</p>

<pre><code>model_gensim.most_similar(""return"")

[('R', 0.15871645510196686),
 ('2', 0.08545402437448502),
 ('i', 0.08142799884080887),
 ('b', 0.07969795912504196),
 ('a', 0.05666942521929741),
 ('w', 0.03705815598368645),
 ('c', 0.032348938286304474),
 ('y', 0.0319858118891716),
 ('o', 0.027745068073272705),
 ('p', 0.026891689747571945)]
</code></pre>

<p>What is the correct way of using gensim's fasttext wrapper?</p>
","machine-learning, nlp, gensim, word-embedding, fasttext","<p>The gensim <code>FastText</code> class doesn't take plain strings as its training texts. It expects lists-of-words, instead. If you pass plain strings, they will look like lists-of-single-characters, and you'll get a stunted vocabulary like you're seeing.</p>

<p>Tokenize each item of your <code>corpus</code> into a list-of-word-tokens and you'll get closer-to-expected results. One super-simple way to do this might just be:</p>

<pre><code>corpus = [s.split() for s in corpus]
</code></pre>

<p>But, usually you'd want to do other things to properly tokenize plain-text as well – perhaps case-flatten, or do something else with punctuation, etc.</p>
",2,1,1332,2019-07-15 05:14:25,https://stackoverflow.com/questions/57033566/how-to-train-a-word-embedding-representation-with-gensim-fasttext-wrapper
"ValueError: cannot reshape array of size 3800 into shape (1,200)","<p>I am trying to apply word embedding on tweets. I was trying to create a vector for each tweet by taking the average of the vectors of the words present in the tweet as follow:</p>

<pre><code>def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += model_w2v[word].reshape((1, size))
            count += 1.
        except KeyError: # handling the case where the token is not in vocabulary

            continue
    if count != 0:
        vec /= count
    return vec
</code></pre>

<p>Next, when I try to Prepare word2vec feature set as follow:</p>

<pre><code>wordvec_arrays = np.zeros((len(tokenized_tweet), 200))
#the length of the vector is 200

for i in range(len(tokenized_tweet)):
    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)

wordvec_df = pd.DataFrame(wordvec_arrays)
wordvec_df.shape
</code></pre>

<p>I get the following error inside the loop:</p>

<blockquote>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-32-72aee891e885&gt; in &lt;module&gt;
      4 # wordvec_arrays.reshape(1,200)
      5 for i in range(len(tokenized_tweet)):
----&gt; 6     wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)
      7 
      8 wordvec_df = pd.DataFrame(wordvec_arrays)

&lt;ipython-input-31-9e6501810162&gt; in word_vector(tokens, size)
      4     for word in tokens:
      5         try:
----&gt; 6             vec += model_w2v.wv.__getitem__(word).reshape((1, size))
      7             count += 1.
      8         except KeyError: # handling the case where the token is not in vocabulary

ValueError: cannot reshape array of size 3800 into shape (1,200)
</code></pre>
</blockquote>

<p>I checked all the available posts in stackOverflow but non of them really helped me.</p>

<p>I tried reshaping the array and it still give me the same error. </p>

<p>My model is:</p>

<pre><code>tokenized_tweet = df['tweet'].apply(lambda x: x.split()) # tokenizing

model_w2v = gensim.models.Word2Vec(
            tokenized_tweet,
            size=200, # desired no. of features/independent variables 
            window=5, # context window size
            min_count=2,
            sg = 1, # 1 for skip-gram model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 2, # no.of cores
            seed = 34)

model_w2v.train(tokenized_tweet, total_examples= len(df['tweet']), epochs=20)
</code></pre>

<p>any suggestions please? </p>
","python, deep-learning, tokenize, word2vec, word-embedding","<p>It looks like the intent of your <code>word_vector()</code> method is to take a list of words, and then with respect to a given <code>Word2Vec</code> model, return the average of all those words' vectors (when present). </p>

<p>To do that, you shouldn't need to do any explicit re-shaping of vectors – or even specification of <code>size</code>, because that's forced by what the model already provides. You could use utility methods from <code>numpy</code> to simplify the code a lot. For example, the <code>gensim</code> <code>n_similarity()</code> method, as part of its comparision of <strong>two</strong> lists-of-words, already does an averaging much like what you're trying, and you can look at its source as a model:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/f97d0e793faa57877a2bbedc15c287835463eaa9/gensim/models/keyedvectors.py#L996"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/f97d0e793faa57877a2bbedc15c287835463eaa9/gensim/models/keyedvectors.py#L996</a></p>

<p>So, while I haven't tested this code, I think your <code>word_vector()</code> method could be essentially replaced with:</p>

<pre><code>import numpy as np

def average_words_vectors(tokens, wv_model):
    vectors = [wv_model[word] for word in tokens 
               if word in wv_model]  # avoiding KeyError
    return np.array(vectors).mean(axis=0)
</code></pre>

<p>(It's sometimes the case that it makes sense to work with vectors that have been normalized to unit-length - as the linked <code>gensim</code> code via applying <code>gensim.matutils.unitvec()</code> to the average. I haven't done this here, as your method hadn't taken that step – but it is something to consider.)</p>

<p>Separate observations about your <code>Word2Vec</code> training code:</p>

<ul>
<li><p>typically words with just 1, 2, or a few occurrences <strong>don't</strong> get good vectors (due to limited number &amp; variety of examples), but <strong>do interfere</strong> with the improvement of other more-common-word vectors. That's why the default is <code>min_count=5</code>. So just be aware: your surviving vectors may get better if you use a default (or even larger) value here, discarding more of the rarer words.</p></li>
<li><p>the dimensions of a ""dense embedding"" like word2vec-vectors aren't really ""independent variables"" (or standalone individually-interpretable ""features"") as implied by your code-comment, even though they may seem that way as separate values/slots in the data. For example, you can't pick one dimension out and conclude, ""that's the foo-ness of this sample"" (like 'coldness' or 'hardness' or 'positiveness' etc). Rather, any of those human-describable meanings tend to be other directions in the combined-space, not perfectly aligned with any of the individual dimensions. You can sort-of tease those out by comparing vectors, and downstream ML algorithms can make use of those complicated/entangled multi-dimensional interactions. But if you think of each dimensions as its own ""feature"" – in any way other than yes, it's technically a single number associated with the item – you may be prone to misinterpreting the vector-space.</p></li>
</ul>
",2,1,3503,2019-07-20 13:42:09,https://stackoverflow.com/questions/57125306/valueerror-cannot-reshape-array-of-size-3800-into-shape-1-200
understanding FastText multilingual,"<p>I am working with this modified version of FastText (<a href=""https://github.com/Babylonpartners/fastText_multilingual"" rel=""nofollow noreferrer"">fastText_multilingual</a>) that will let me align words in two languages.</p>

<p>I am trying to understand their fasttext.py and especially the <a href=""https://github.com/Babylonpartners/fastText_multilingual/blob/master/fasttext.py"" rel=""nofollow noreferrer"">Fast Vector class</a>. 
In the example file <a href=""https://github.com/Babylonpartners/fastText_multilingual/blob/master/align_your_own.ipynb"" rel=""nofollow noreferrer"">align_your_own.ipynb</a>the authors show how to measure similarity between two words. I would like to iterate the process for the whole set of words, instead of measuring similarity every time for a single word. To do this I need to understand how to access to these FastVector objects. 
That's why I am trying to understand the Fast vector class. </p>

<p>I am stuck here:</p>

<pre><code> def __init__(self, vector_file='', transform=None):
    """"""Read in word vectors in fasttext format""""""
    self.word2id = {}

    # Captures word order, for export() and translate methods
    self.id2word = []

    print('reading word vectors from %s' % vector_file)
    with open(vector_file, 'r') as f:
        (self.n_words, self.n_dim) = \
            (int(x) for x in f.readline().rstrip('\n').split(' '))
        self.embed = np.zeros((self.n_words, self.n_dim))
        for i, line in enumerate(f):
            elems = line.rstrip('\n').split(' ')
            self.word2id[elems[0]] = i
            self.embed[i] = elems[1:self.n_dim+1]
            self.id2word.append(elems[0])
</code></pre>

<p>I have never created a class in python, so this make things more difficult for me. These are the lines that I can't understand in depth:</p>

<pre><code> 1. (self.n_words, self.n_dim) = \
 2. self.word2id = {}, self.id2word = [], 
 3. self.embed = np.zeros((self.n_words, self.n_dim))
</code></pre>

<p>These are my questions:</p>

<ul>
<li>What does that <strong>""= \""</strong> mean on 1? </li>
<li>Where <strong>word2id</strong>, <strong>id2word</strong> and <strong>embed</strong> are defined? Are they keywords of python?</li>
</ul>
","python, text-alignment, word-embedding, fasttext","<p>A backslash at the end of a line tells Python to extend the current logical line over across to the next physical line.
In your case, you can read the two lines as a single line:</p>

<pre><code>(self.n_words, self.n_dim) = (int(x) for x in f.readline().rstrip('\n').split(' '))
</code></pre>

<p>In Python, a variable is created the moment you first assign a value to it (<a href=""https://www.w3schools.com/python/python_variables.asp"" rel=""nofollow noreferrer"">https://www.w3schools.com/python/python_variables.asp</a>). 
So, word2id, id2word and embed are not keywords; they are created when a value is assigned to them.</p>
",2,1,2318,2019-07-23 13:44:42,https://stackoverflow.com/questions/57165579/understanding-fasttext-multilingual
Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker,"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.</code> as an error. I have included the part where I save my model below:</p>

<pre><code>import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
def tfhub_to_savedmodel(model_name, export_path):

    model_path = '{}/{}/00000001'.format(export_path, model_name)
    tfhub_uri = 'http://tfhub.dev/google/universal-sentence-encoder-large/3'

    with tf.Session() as sess:
        module = hub.Module(tfhub_uri)
        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])
        input_params = module.get_input_info_dict()
        dtype = input_params['text'].dtype
        shape = input_params['text'].get_shape()

        # define the model inputs
        inputs = {'text': tf.placeholder(dtype, shape, 'text')}
        output = module(inputs['text'])
        outputs = {
            'vector': output,
        }

        # export the model
        tf.saved_model.simple_save(
            sess,
            model_path,
            inputs=inputs,
            outputs=outputs)  

    return model_path
</code></pre>

<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders</p>
","amazon-web-services, tensorflow, word-embedding, amazon-sagemaker, tensorflow-hub","<p>I was running into this exact issue earlier this week while trying to modify this example <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_serving_container/tensorflow_serving_container.ipynb"" rel=""nofollow noreferrer"">Sagemaker notebook</a>. Particularly the part where serving the model. That is, running <code>predictor.predict()</code> on the Sagemaker Tensorflow Estimator.</p>

<p>The solution outlined in the issue worked perfectly for me- <a href=""https://github.com/awslabs/amazon-sagemaker-examples/issues/773#issuecomment-509433290"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/issues/773#issuecomment-509433290</a></p>

<p>I think it's just because <code>tf.tables_initializer()</code> only runs for training but it needs to be specified through the <code>legacy_init_op</code> if you want to run it during prediction.</p>
",0,1,420,2019-07-24 18:37:12,https://stackoverflow.com/questions/57189292/failed-precondition-table-not-initialized-on-deployed-universal-sentence-encod
Cosine similarities and totally different results using same source,"<p>I am learning word embeddings and cosine similarity. My data is composed of two sets of same words but in 2 different languages. </p>

<p>I did two tests:</p>

<ol>
<li>I measured the cosine similarity using the average of the word vectors (that I think it should be called soft cosine similarity instead)</li>
<li>I measured the cosine similarity using the word vectors</li>
</ol>

<p>Should I expect to obtain quite the same results? I noticed that sometimes I have two opposite results. Since I am new on this, I am trying to figure out if I did something wrong or if there is an explanation behind. According to what I have been reading, soft cosine similarity should be more accurate than the usual cosine similarity.</p>

<p>Now, it's time for some data to show you. Unfortunately I can't post a part of my data (the words themselves), but I will try my best to give you the max of information I can give you.</p>

<p>Some other details before:</p>

<ul>
<li>I am using FastText to create the embeddings, skipgram model with
default parameters.</li>
<li>For the soft cosine similarity, I am using <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html"" rel=""nofollow noreferrer"">Scipy
spatial distance cosine</a>. Following some people suggestions, to measure cosine similarity it seems that I should subtract 1 from the formula, such as:</li>
</ul>

<p><code>(1-distance.cosine(data['LANG1_AVG'].iloc[i],data['LANG2_AVG'].iloc[i]))</code></p>

<ul>
<li><p>For the usual cosine similarity I am using the <a href=""https://github.com/Babylonpartners/fastText_multilingual/blob/master/fasttext.py"" rel=""nofollow noreferrer"">Fast Vector cosine similarity</a> from <a href=""https://github.com/Babylonpartners/fastText_multilingual"" rel=""nofollow noreferrer"">FastText Multilingual</a>, defined in this way:</p>

<p><code>@classmethod
def cosine_similarity(cls, vec_a, vec_b):
    """"""Compute cosine similarity between vec_a and vec_b""""""
    return np.dot(vec_a, vec_b) / \
        (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))</code></p></li>
</ul>

<p>As you will see from the image here, for some words I obtained the same results or quite similar using the two methods. For others I obtained two totally different results. How can I explain this? </p>

<p><a href=""https://i.sstatic.net/H2xYz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H2xYz.png"" alt=""My data v2""></a></p>
","python, machine-learning, cosine-similarity, word-embedding, machine-translation","<p>After some more additional research, I found a 2014 paper (<a href=""http://www.scielo.org.mx/pdf/cys/v18n3/v18n3a7.pdf"" rel=""nofollow noreferrer"">Soft Similarity and Soft Cosine Measure:
Similarity of Features in Vector Space Model</a>) that explains when and how it could be useful to use averages of the features, and it explains also what is exactly a soft cosine measure:</p>

<blockquote>
  <p>Our idea is more general: we propose to modify the
  manner of calculation of similarity in Vector Space Model
  taking into account similarity of features. If we apply
  this idea to the cosine measure, then the “soft cosine
  measure” is introduced, as opposed to traditional “hard
  cosine”, which ignores similarity of features. Note that
  when we consider similarity of each pair of features, it
  is equivalent to introducing new features in the VSM.
  Essentially, we have a matrix of similarity between pairs
  of features and all these features represent new dimensions in the VSM.</p>
</blockquote>
",0,0,2319,2019-07-24 23:45:54,https://stackoverflow.com/questions/57192563/cosine-similarities-and-totally-different-results-using-same-source
Interepretation of word2vec evaluation result,"<p>I have created word embeddings (Word2vec) using my own dataset. I have used Gensim module to create word embeddings. I want to evaluate my word embeddings.</p>

<p>I have used Wordsim353 dataset to evaluate word embeddings. The following code Shows the result of Evaluation. </p>

<p>Code:</p>

<pre><code>from gensim.test.utils import datapath

similarities = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))

print(similarities)
</code></pre>

<p>Result:</p>

<pre><code>((0.09410256722489568, 0.3086953732794174), SpearmanrResult(correlation=0.06101508426787973, pvalue=0.5097769955392246), 66.28895184135978)
</code></pre>

<p>How can I interprete the result?</p>

<p>Please help me to interprete the results.</p>
","word2vec, evaluation, word-embedding","<p>The way we evaluate the quality of word embeddings is to see how closely the similarities computed by embeddings match the actual similarities assigned by human judgements.</p>

<p>Your Pearson and Spearmanr's pValue are too high with approximately 0.3 (70%) and 0.5 (50%). I suggest you should use pretrained word embeddings or collect more dataset.</p>

<p>I have strived to evaluate with glove-twitter-25 and received very great pvalue. </p>

<pre><code>import gensim.downloader as api
from gensim.test.utils import datapath

m = api.load(""glove-twitter-25"")
m.evaluate_word_pairs(datapath(""wordsim353.tsv""))
</code></pre>

<p>output:</p>

<pre><code>((0.36409317297819943, pvalue=2.969053896450154e-12), SpearmanrResult(correlation=0.36452011505868487, pvalue=2.788781738485533e-12), 2.26628895184136)
</code></pre>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer"">evaluate_word_pairs - Gensim module</a></p>
",1,3,982,2019-07-25 09:11:09,https://stackoverflow.com/questions/57198286/interepretation-of-word2vec-evaluation-result
I want to know how can we give a categorical variable as an input to an embedding layer in keras and train that embedding layer?,"<p>let's say we have a data frame where we have a categorical column which has 7 categories - Monday, Tuesday, Wednesday, Thursday, Friday, Saturday and Sunday. Let's say we have 100 data points and we want to give the categorical data as an input to the embedding layer and train the embedding layer using Keras. How do we actually achieve it? Can you share some intuition with code examples? </p>

<p>I have tried this code but it gives me an error which says ""ValueError: ""input_length"" is 1, but received input has shape (None, 26)"". I have referred to this blog <a href=""https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9"" rel=""nofollow noreferrer"">https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9</a>, but I didn't get how to use it for my particular case.</p>

<pre><code>from sklearn.preprocessing import LabelEncoder
l_encoder=LabelEncoder()
l_encoder.fit(X_train[""Weekdays""])

encoded_weekdays_train=l_encoder.transform(X_train[""Weekdays""])
encoded_weekdays_test=l_encoder.transform(X_test[""Weekdays""])

no_of_unique_cat=len(X_train.school_state.unique())
embedding_size = min(np.ceil((no_of_unique_cat)/2),50)
embedding_size = int(embedding_size)
vocab  = no_of_unique_cat+1

#Get the flattened LSTM output for categorical text
input_layer2 = Input(shape=(embedding_size,))
embedding = Embedding(input_dim=vocab, output_dim=embedding_size, input_length=1, trainable=True)(input_layer2)
flatten_school_state = Flatten()(embedding)
</code></pre>

<p>I want to know in case of 7 categories, what will be the shape of input_layer2? What should be the vocab size, output dim and input_length? Can anyone explain, or correct my code? Your insights will be really helpful.</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-46-e28d41acae85&gt; in &lt;module&gt;
      1 #Get the flattened LSTM output for input text
      2 input_layer2 = Input(shape=(embedding_size,))
----&gt; 3 embedding = Embedding(input_dim=vocab, output_dim=embedding_size, input_length=1, trainable=True)(input_layer2)
      4 flatten_school_state = Flatten()(embedding)

~/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)
    472             if all([s is not None
    473                     for s in to_list(input_shape)]):
--&gt; 474                 output_shape = self.compute_output_shape(input_shape)
    475             else:
    476                 if isinstance(input_shape, list):

~/anaconda3/lib/python3.7/site-packages/keras/layers/embeddings.py in compute_output_shape(self, input_shape)
    131                         raise ValueError(
    132                             '""input_length"" is %s, but received input has shape %s' %
--&gt; 133                             (str(self.input_length), str(input_shape)))
    134                     elif s1 is None:
    135                         in_lens[i] = s2

ValueError: ""input_length"" is 1, but received input has shape (None, 26)


</code></pre>
","keras, nlp, lstm, categorical-data, word-embedding","<p><code>embedding_size</code> can never be the input size. </p>

<p>A Keras embedding takes ""integers"" as input. You should have your data as numbers from 0 to 6. </p>

<p>If your 100 data points form a sequence of days, you cannot restrict the length of the sequences in the embedding to 1.</p>

<p>Your input shape should be <code>(length_of_sequence,)</code>. Which means your training data should have shape <code>(any, length_of_sequence)</code>. Which is probably <code>(1, 100)</code> by your description. </p>

<p>All the rest is automatic. </p>
",0,0,174,2019-07-28 19:55:20,https://stackoverflow.com/questions/57244472/i-want-to-know-how-can-we-give-a-categorical-variable-as-an-input-to-an-embeddin
How to combine 3D token embeddings into 2D vectors?,"<p>I have a set of strings that I am tokenizing. I am sending each string into the <code>word2vec</code> model in <code>gensim</code>. Say, if there are 100 tokens (e.g. 'I', 'ate', 'pizza', etc.), it is generating a 100 * 100 3D matrix (<code>list</code> of <code>list</code> in <code>python</code>). How is it possible to convert the generated 3D token embeddings in to a 2D vector?  </p>

<p>I am sending this 3D into a model in <code>Tensorflow</code> library. I am doing the following,</p>

<p><code>model.add(Embedding(max_features, 128, input_length=maxlen))</code></p>

<p>Here max_features is the size of the token vector i.e. 100 and input_length is also the same. </p>

<p>But I am not sure If this is getting the job done. Is it the right way to convert 3D token embeddings in to 2D vectors? Ideally, I want to covert the embeddings into 2D vectors before sending into the model.</p>
","python, tokenize, gensim, word2vec, word-embedding","<p>The final results of training aren't really ""3D"" in usual <code>Word2Vec</code>/<code>gensim</code> terminology. If you've used <code>Word2Vec</code> with its default <code>vector_size=100</code>, and you had 100 vocabulary words, then you'd have 100 vectors of 100-dimensions each. </p>

<p>(Note: you would never want to create such high-dimensional ""dense embedding"" vectors for such a tiny vocabulary. The essential benefits of such dense representations come from forcing a much-larger set of entities into many-fewer dimensions, so that they are ""compressed"" into subtle, continuous, meaningful relative positions against each other. Giving 100 words a full 100 continuous dimensions, before <code>Word2Vec</code> training, will leave the model prone to severe overfitting. It could in fact then trend towards a ""one-hot""-like encoding of each word, and become very good at the training task without really learning to pack related words near each other in a shared space – which is the usually-desired result of training. In my experience, for 100-dimension vectors, you probably want at least a 100^2 count of vocabulary words. If you really just care about 100 words, then you'd want to use much-smaller vectors – but also remember <code>Word2Vec</code> &amp; related techniques are really meant for ""large data"" problems, with many subtly-varied training examples, and just barely sometimes give meaningful results on toy-sized data.)</p>

<p>The 100 vectors of 100-dimensions each are internally stored inside the <code>Word2Vec</code> model (&amp; related components) as a raw <code>numpy</code> <code>ndarray</code>, which could be thought of as a ""2d array"" or ""2d matrix"". (It's not really a <code>list</code> of <code>list</code> unless you convert it to be that less-optimal form – though of course with Pythonic polymorphism you can generally pretend it was a <code>list</code> of <code>list</code>). If your <code>gensim</code> <code>Word2Vec</code> model is in <code>w2v_model</code>, then the raw <code>numpy</code> array of learned vectors is inside the <code>w2v_model.wv.vectors</code> property, though the interpretation of which row corresponds to which word-token depends on the <code>w2v_model.wv.vocab</code> dictionary entries.</p>

<p>As far as I can tell, the Tensorflow <code>Embedding</code> class is for training your own embeddings inside TF (though perhaps it can be initialized with vectors trained elsewhere). Its 1st initialization argument should the size-of-the-vocabulary (per your conjectured case 100), its second is the size-of-the-desired-embeddings (per your conjectured case, also 100 - but as noted above, this match of vocab-size and dense-embedding-size is inappropriate, and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"" rel=""nofollow noreferrer"">the example values in the TF docs of 1000 words and 64 dimensions</a> would be more appropriately balanced). </p>
",0,0,696,2019-07-30 03:56:34,https://stackoverflow.com/questions/57264086/how-to-combine-3d-token-embeddings-into-2d-vectors
Word shows up more than once in TSNE plot,"<p>When plotting word embedding TSNE results, words show up more than once.</p>

<p>I am reducing dimensionality of a Word2Vec word embedding, but when I plot the results for a subset of the most similar words (manually enter several words for which I want the most similar ones), the same words show up more than once:</p>

<pre><code>from sklearn.manifold import TSNE

words = sum([[k] + v for k, v in similar_words.items()], [])
wvs = model.wv[words]

tsne = TSNE(n_components=3, random_state=0, n_iter=10000, perplexity=29)
np.set_printoptions(suppress=True)
T = tsne.fit_transform(wvs)
labels = words

plt.figure(figsize=(16, 12))
plt.scatter(T[:, 0], T[:, 1], c='purple', edgecolors='purple')
for label, x, y in zip(labels, T[:, 0], T[:, 1]):
    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')
</code></pre>

<p>Is this a normal behavior for PCA and TSNE word similarity dimensionality reduction, or is there something off with my code? Is it possible that the plot is treating each of the similar words subsets as independent from each other?</p>
","matplotlib, scikit-learn, nlp, word2vec, word-embedding","<p>Each word has two vectors: as a center word and as a context word. <a href=""https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=2513s"" rel=""nofollow noreferrer"">Stanford University word2vec lecture</a> starting at 41:37.</p>
",0,0,56,2019-08-05 00:02:21,https://stackoverflow.com/questions/57350998/word-shows-up-more-than-once-in-tsne-plot
Universal sentence encoding embedding digits very similar,"<p>I have task of sentence similarity where i calculate the cosine of two sentence to decide how similar they are . It seems that for sentence with digits the similarity is not affected no matter how ""far"" the numbers are . For an example:</p>

<p>a = generate_embedding('issue 845')</p>

<p>b = generate_embedding('issue 11')</p>

<p>cosine_sim(a,b) = 0.9307</p>

<p>is there a way to distance the hashing of numbers or any other hack to handle that issue?</p>
","tensorflow, nlp, word-embedding","<p>If your sentence embedding are produced using the embeddings of individual words (or tokens), then a  hack could be the following: </p>

<p>to add dimensions to the word embedding. These dimensions would be set to zero for all non-numeric tokens, and for numeric tokens these dimensions would contain values reflecting the magnitude of the numeric value. It would get a bit mathematical because cosine similarity uses angles, so the extra dimensions added to the embedding would have to reflect the magnitude of the numeric values <em>through larger or smaller angles</em>.</p>

<p>An easier (workaround) hack would be to extract the numeric values from the sentences using regular expressions and compute their distance and combine that information with the similarity score in order to obtain a new similarity score.</p>
",1,0,324,2019-08-14 07:03:26,https://stackoverflow.com/questions/57489639/universal-sentence-encoding-embedding-digits-very-similar
RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 &#39;index&#39;,"<p>I'm working with the project 'lda2vec-pytorch' on Google CoLab,
runnin pytorch 1.1.0</p>

<p><a href=""https://github.com/TropComplique/lda2vec-pytorch"" rel=""nofollow noreferrer"">https://github.com/TropComplique/lda2vec-pytorch</a></p>

<pre><code>device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
cuda:0
</code></pre>

<p>I'm getting an exception in the forward method adding 'noise' in my 
class negative_sampling_loss(nn.Module):</p>

<pre><code>        noise = self.multinomial.draw(batch_size*window_size*self.num_sampled)
        noise = Variable(noise).view(batch_size, window_size*self.num_sampled)

        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
        self.embedding = self.embedding.to(device)
        #print(""negative_sampling_loss::forward() self.embedding"", self.embedding.is_cuda)  This line get's an error.

        # shape: [batch_size, window_size*num_sampled, embedding_dim]
        noise = self.embedding(noise)  # Exception HERE
</code></pre>

<p>Here's the stack trace:</p>

<pre><code>Traceback (most recent call last):
  File ""train.py"", line 36, in &lt;module&gt;
    main()
  File ""train.py"", line 32, in main
    save_every=20, grad_clip=5.0
  File ""../utils/training.py"", line 138, in train
    neg_loss, dirichlet_loss = model(doc_indices, pivot_words, target_words)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""../utils/lda2vec_loss.py"", line 82, in forward
    neg_loss = self.neg(pivot_words, target_words, doc_vectors, w)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""../utils/lda2vec_loss.py"", line 167, in forward
    noise = self.embedding(noise)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py"", line 117, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py"", line 1506, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'
</code></pre>

<p>Any ideas?</p>
","pytorch, word-embedding","<p>Variable <code>noise</code> is available on CPU while <code>self.embedding</code> is on GPU. We can send <code>noise</code> to GPU as well:</p>

<pre><code>noise = noise.to(device)
</code></pre>
",2,0,1627,2019-08-17 20:55:15,https://stackoverflow.com/questions/57539872/runtimeerror-expected-object-of-backend-cuda-but-got-backend-cpu-for-argument
"Should I train embeddings using data from both training,validating and testing corpus?","<p>I am in a case that I don't have any pre-trained words embedding for my domain (Vietnamese food reviews). so I got a though of embedding from the general and specific corpus.</p>

<p>And the point here is can I use the dataset of training, test and validating (did preprocess) as a source for creating my own word embeddings. If don't, hope you can give your experience.</p>

<p>Based on my intuition, and some experiments a wide corpus appears to be better, but I'd like to know if there's relevant research or other relevant results.</p>
","nlp, word-embedding","<blockquote>
  <p>can I use the dataset of training, test and validating (did
  preprocess) as a source for creating my own word embeddings</p>
</blockquote>

<p>Sure, embeddings are not your features for your machine learning model. They are the ""computational representation"" of your data. In short, they are made of words represented in a vector space. With embeddings, your data is less sparse. Using word embeddings could be considered <em>part of the pre-processing step</em> of NLP.</p>

<p>Usually (I mean, using the most used technique, word2vec), the representation of a word in the vector space is defined by its surroundings (the words that it commonly goes along with). </p>

<p>Therefore, <strong>to create embeddings, the larger the corpus, the better</strong>, since it can better place a word vector in the vector space (and hence compare it to other similar words).</p>
",2,2,2262,2019-08-21 07:14:06,https://stackoverflow.com/questions/57586325/should-i-train-embeddings-using-data-from-both-training-validating-and-testing-c
Should the vocabulary be restricted to the training-set vocabulary when training an NN model with pretrained word2vec like GLOVE?,"<p>I wanted to use word embeddings for the embedding Layer in my neural network using pre-trained vectors from GLOVE. Do I need to restrict the vocabulary to the training-set when constructing the word2index dictionary? 
Wouldn't that lead to a limited non-generalizable model? 
Is considering all the vocabulary of GLOVE a recommended practice?</p>
","keras, neural-network, word-embedding, glove","<p>Yes, it is better to restrict your vocab size. Because pre-trained embeddings (like GLOVE) have many words in them that are not very useful (and so Word2Vec) and the bigger vocab size the more RAM you need and other problems.</p>

<p>Select your tokens from all of your data. it won't lead to a limited non-generalizable model if your data is big enough. if you think that your data does not have as many tokens as are needed, then you should know 2 things:</p>

<ol>
<li>Your data is not good enough and you have to gather more.</li>
<li>Your model can't generate well on the tokens that it hasn't seen at training! so it has no point to having many unused words on your embedding and better to gather more data to cover those words.</li>
</ol>

<p>I have an answer to show how you can select a minor set of word vectors from a pre-trained model <a href=""https://stackoverflow.com/a/55725093/7339624"">in here</a></p>
",1,0,322,2019-08-28 05:33:07,https://stackoverflow.com/questions/57685633/should-the-vocabulary-be-restricted-to-the-training-set-vocabulary-when-training
Gensim&#39;s FastText KeyedVector out of vocab,"<p>I want to use the read-only version of Gensim's FastText Embedding to save some RAM compared to the full model.</p>

<p>After loading the KeyVectors version, I get the following Error when fetching a vector:</p>

<p><code>IndexError: index 878080 is out of bounds for axis 0 with size 761210</code></p>

<p>The error occurs when using words that should be out-of-vocabulary e.g. ""lawyerxy"" instead of ""lawyer"". The full model returns a vector for both. </p>

<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load(""model.kv"")
model .wv.__getitem__(""lawyerxy"")
</code></pre>

<p>So, my assumption is that the KeyedVectors do not offer FastText's out of vacabulary function - a key feature for my usecase. This limitation is not given in the documentation:
<a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>Can anyone prove that assumption and/or name a fix to allow vectors for ""lawyerxy"" etc. ?</p>
","gensim, word-embedding, fasttext","<p>The <code>KeyedVectors</code> name is (as of <code>gensim-3.8.0</code>) just an <a href=""https://github.com/RaRe-Technologies/gensim/blob/a47eed80cf225181717cba09761922d4a54027d8/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">alias</a> for class <code>Word2VecKeyedVectors</code>, which only maintains a simple word (as key) to vector (as value) mapping.</p>

<p>You shouldn't expect FastText's advanced ability to synthesize vectors for out-of-vocabulary words to appear in any model/representation that doesn't explicitly claim to offer that ability. </p>

<p>(I would expect a lookup of an out-of-vocabulary word to give a clearer <code>KeyError</code> rather than the <code>IndexError</code> you've reported. But, you'd need to show exactly what code created the file you're loading, and triggered the error, and the full error stack, to further guess what's going wrong in your case.)</p>

<p>Depending on how your <code>model.kv</code> file was saved, you might be able to load it, with retained OOV-vector functionality, by using the class <a href=""https://github.com/RaRe-Technologies/gensim/blob/a47eed80cf225181717cba09761922d4a54027d8/gensim/models/keyedvectors.py#L1943"" rel=""nofollow noreferrer""><code>FastTextKeyedVectors</code></a> instead of plain <code>KeyedVectors</code>. </p>
",2,0,1434,2019-09-12 09:28:22,https://stackoverflow.com/questions/57903695/gensims-fasttext-keyedvector-out-of-vocab
How are the TokenEmbeddings in BERT created?,"<p>In the <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">paper describing BERT</a>, there is this paragraph about WordPiece Embeddings. </p>

<blockquote>
  <p>We use WordPiece embeddings (Wu et al.,
  2016) with a 30,000 token vocabulary. The first
  token of every sequence is always a special classification
  token ([CLS]). The final hidden state
  corresponding to this token is used as the aggregate
  sequence representation for classification
  tasks. Sentence pairs are packed together into a
  single sequence. We differentiate the sentences in
  two ways. First, we separate them with a special
  token ([SEP]). Second, we add a learned embedding
  to every token indicating whether it belongs
  to sentence A or sentence B. As shown in Figure 1,
  we denote input embedding as E, the final hidden
  vector of the special [CLS] token as C 2 RH,
  and the final hidden vector for the ith input token
  as Ti 2 RH.
  For a given token, its input representation is
  constructed by summing the corresponding token,
  segment, and position embeddings. A visualization
  of this construction can be seen in Figure 2.
  <a href=""https://i.sstatic.net/QCcYF.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/QCcYF.png"" alt=""Fig 2 from the paper""></a></p>
</blockquote>

<p>As I understand, WordPiece splits Words into wordpieces like #I #like #swim #ing, but it does not generate Embeddings. But I did not find anything in the paper and on other sources how those Token Embeddings are generated. Are they pretrained before the actual Pre-training? How? Or are they randomly initialized? </p>
","machine-learning, nlp, word-embedding","<p>The wordpieces are trained separately, such the most frequent words remain together and the less frequent words get split eventually down to characters.</p>

<p>The embeddings are trained jointly with the rest of BERT. The back-propagation is done through all the layers up to the embeddings which get updated just like any other parameters in the network.</p>

<p>Note that only the embeddings of tokens which are actually present in the training batch get updated and the rest remain unchanged. This also a reason why you need to have relatively small word-piece vocabulary, such that all embeddings get updated frequently enough during the training.</p>
",16,14,10856,2019-09-16 16:29:57,https://stackoverflow.com/questions/57960995/how-are-the-tokenembeddings-in-bert-created
Gensim find vectors/words in ball of radius r,"<p>I would like take word ""book"" (for example) get its vector representation, call it v_1 and find all words whose vector representation is within ball of radius r of v_1 i.e. ||v_1 - v_i||&lt;=r, for some real number r.</p>

<p>I know gensim has <code>most_similar</code> function, which allows to state number of top vectors to return, but it is not quite what I need. I surely can use brute force search and get the answer, but it will be to slow. </p>
","python, gensim, word-embedding","<p>If you call <code>most_similar()</code> with a <code>topn=0</code>, it will return the raw <em>unsorted</em> cosine-similarities to <em>all</em> other words known to the model. (These similarities will not be in tuples with the words, but simply in the same order as the words in the <code>index2entity</code> property.) </p>

<p>You could then filter those similarities for those higher than your preferred threshold, and return just those indexes/words, using a function like <code>numpy</code>'s <a href=""https://stackoverflow.com/a/21632554/130288""><code>argwhere</code></a>.</p>

<p>For example:</p>

<pre><code>target_word = 'apple'
threshold = 0.9
all_sims = wv.most_similar(target_word, topn=0)
satisfactory_indexes = np.argwhere(all_sims &gt; threshold)
satisfactory_words = [wv.index2entity[i] for i in satisfactory_indexes]
</code></pre>
",1,0,39,2019-09-16 16:45:21,https://stackoverflow.com/questions/57961188/gensim-find-vectors-words-in-ball-of-radius-r
Train LSTM with custom padding in each batch size,"<p>I am training a RNN for text classification. I want to train the model with a batch size <code>X</code>. However, for each batch, I want to create a zero padding where <code>word_padding_length = maximum string length in each batch - current_word_length</code>. I have tried search but was not able to find anything related to this. This should happen during when I fit the model.</p>
","keras, lstm, word-embedding, zero-padding","<p>you cannot change the input tensor shape during the training process. In your case, you want to have:<br>
 <code>(batch_size, len1, num_features) -&gt; (batch_size, len2, num_features)</code> for each steps.  </p>

<p>You can have sth similar to this:</p>

<pre><code>from keras.models import Model
from keras.layers import Input, LSTM

x = Input(shape=(None,128))
hidden = LSTM(32)(x)
model = Model(x, hidden)

for batch in batch_lst:
     model.train_on_batch(batch)
</code></pre>

<p>Noted that <code>Input</code> has shape of (<strong>None</strong>, 128), which means <strong>variable batch_size, variable time_steps, and fixed num_feature=128</strong>  </p>

<p>Moreover, you might consider using <a href=""https://keras.io/zh/layers/core/#masking"" rel=""nofollow noreferrer"">masking layer</a> to ignore all your padding values, so that you can have one tensor input for whole training set while the model performance is not affected by padding.</p>
",1,0,458,2019-09-29 22:48:41,https://stackoverflow.com/questions/58159808/train-lstm-with-custom-padding-in-each-batch-size
Word Embedding Model,"<p>I have been searching and attempting to implement a word embedding model to predict similarity between words. I have a dataset made up 3,550 company names, the idea is that the user can provide a new word (which would not be in the vocabulary) and calculate the similarity between the new name and existing ones.</p>

<p>During preprocessing I got rid of stop words and punctuation (hyphens, dots, commas, etc). In addition, I applied stemming and separated prefixes with the hope to get more precision. Then words such as <code>BIOCHEMICAL</code> ended up as <code>BIO</code> <code>CHEMIC</code> which is the word divided in two (prefix and stem word)</p>

<p>The average company name length is made up 3 words with the following frequency:</p>

<p><a href=""https://i.sstatic.net/yKeFP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yKeFP.png"" alt=""enter image description here""></a></p>

<p>The tokens that are the result of preprocessing are sent to word2vec:</p>

<pre><code>#window: Maximum distance between the current and predicted word within a sentence
#min_count: Ignores all words with total frequency lower than this.
#workers: Use these many worker threads to train the model
#sg: The training algorithm, either CBOW(0) or skip gram(1). Default is 0s
word2vec_model = Word2Vec(prepWords,size=300, window=2, min_count=1, workers=7, sg=1)
</code></pre>

<p>After the model included all the words in the vocab , the average sentence vector is calculated for each company name:
    df['avg_vector']=df2.apply(lambda row : avg_sentence_vector(row, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word)).tolist())</p>

<p>Then, the vector is saved for further lookups:</p>

<pre><code>##Saving name and vector values in file
df.to_csv('name-submission-vectors.csv',encoding='utf-8', index=False)
</code></pre>

<p>If a new company name is not included in the vocab after preprocessing (removing stop words and punctuation), then I proceed to create the model again and calculate the average sentence vector and save it again.</p>

<p>I have found this model is not working as expected. As an example, calculating the most similar words <code>pet</code> is getting the following results:</p>

<pre><code>ms=word2vec_model.most_similar('pet')

('fastfood', 0.20879755914211273)
('hammer', 0.20450574159622192)
('allur', 0.20118337869644165)
('wright', 0.20001833140850067)
('daili', 0.1990675926208496)
('mgt', 0.1908089816570282)
('mcintosh', 0.18571510910987854)
('autopart', 0.1729743778705597)
('metamorphosi', 0.16965581476688385)
('doak', 0.16890916228294373)
</code></pre>

<p>In the dataset, I have words such as paws or petcare, but other words are creating relationships with <code>pet</code> word.</p>

<p>This is the distribution of the nearer words for <code>pet</code>:</p>

<p><a href=""https://i.sstatic.net/KpGoJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KpGoJ.png"" alt=""enter image description here""></a></p>

<p>On the other hand, when I used the <code>GoogleNews-vectors-negative300.bin.gz</code>, I could not add new words to the vocab, but the similarity between <code>pet</code> and words around was as expected:</p>

<pre><code>ms=word2vec_model.most_similar('pet')
('pets', 0.771199643611908)
('Pet', 0.723974347114563)
('dog', 0.7164785265922546)
('puppy', 0.6972636580467224)
('cat', 0.6891531348228455)
('cats', 0.6719794869422913)
('pooch', 0.6579219102859497)
('Pets', 0.636363685131073)
('animal', 0.6338439583778381)
('dogs', 0.6224827170372009)
</code></pre>

<p>This is the distribution of the nearest words:</p>

<p><a href=""https://i.sstatic.net/2jt2o.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2jt2o.png"" alt=""enter image description here""></a></p>

<p>I would like to get your advice about the following:</p>

<ul>
<li>Is this dataset appropriate to proceed with this model? </li>
<li>Is the length of the dataset enough to allow <code>word2vec</code> ""learn"" the relationships between the words?</li>
<li>What can I do to improve the model to make <code>word2vec</code> create relationships of the same type as GoogleNews where for instance word <code>pet</code> is correctly set among similar words?</li>
<li>Is it feasible to implement another alternative such as <code>fasttext</code> considering the nature of the current dataset?</li>
<li>Do you know any public dataset that can be used along with the current dataset to create those relationships?</li>
</ul>

<p>Thanks</p>
","machine-learning, deep-learning, word2vec, word-embedding, fasttext","<p>3500 texts (company names) of just ~3 words each is only around 10k total training words, with a much smaller vocabulary of unique words. </p>

<p>That's very, very small for word2vec &amp; related algorithms, which rely on lots of data, and sufficiently-varied data, to train-up useful vector arrangements.</p>

<p>You may be able to squeeze some meaningful training from limited data by using <em>far more</em> training epochs than the default <code>epochs=5</code>, and <em>far smaller</em> vectors than the default <code>size=100</code>. With those sorts of adjustments, you may start to see more meaningful <code>most_similar()</code> results.</p>

<p>But, it's unclear that word2vec, and specifically word2vec in your averaging-of-a-name's-words comparisons, is matched to your end goals. </p>

<p>Word2vec needs lots of data, doesn't look at subword units, and can't say anything about word-tokens not seen during training. An average-of-many-word-vectors can often work as an easy baseline for comparing multiword texts, but might also dilute some word's influence compared to other methods.</p>

<p>Things to consider might include:</p>

<ul>
<li><p>Word2vec-related algorithms like FastText that also learn vectors for subword units, and can thus bootstrap not-so-bad guess vectors for words not seen in training. (But, these are also data hungry, and to use on a small dataset you'd again want to reduce vector size, increase epochs, and additionally shrink the number of <code>buckets</code> used for subword learning.)</p></li>
<li><p>More sophisticated comparisons of multi-word texts, like ""Word Mover's Distance"". (That can be quite expensive on longer texts, but for names/titles of just a few words may be practical.)</p></li>
<li><p>Finding more data that's compatible with your aims for a stronger model. A larger database of company names might help. If you just want your analysis to understand English words/roots, more generic training texts might work too.</p></li>
<li><p>For many purposes, a mere lexicographic comparison - edit distances, count of shared character-n-grams – may be helpful too, though it won't detect all synonyms/semantically-similar words.</p></li>
</ul>
",1,1,846,2019-10-04 05:03:40,https://stackoverflow.com/questions/58230214/word-embedding-model
Word embeddings with multiple categorial features for a single word,"<p>I'm looking for a method to implement word embedding network with LSTM layers in Pytorch such that the input to the <code>nn.Embedding</code> layer has a different form than vectors of words IDs.</p>

<p>Each word in my case has a corresponding vector and the sentence in my corpus is consequently a vector of vectors. So, for example, I may have the word ""King"" with vector [500, 3, 18] where 500 is the Word ID, 3 is the word color, and 18 is the font size, etc. The embedding layer role here is to do some automatic feature reduction/extraction.</p>

<p>How can I feed the embedding layer with such form data? Or do you have any better suggestions?</p>
","python, python-3.x, pytorch, word-embedding","<p>I am not sure what do you mean by word2vec algorithm with LSTM because the original word2vec algorithm does not use LSTMs and uses directly embeddings to predict surrounding words.</p>

<p>Anyway, it seems you have multiple categorical variables to embed. In the example, it is word ID, color ID, and font size (if you round it to integer values). You have two option:</p>

<ol>
<li><p>You can create new IDs for all possible combinations of your features and use <code>nn.Embedding</code> for them. There is however a risk that most of the IDs will appear too sparsely in the data to learn reliable embeddings.</p></li>
<li><p>Have separate embedding for each of the features. Then, you will need to combine the embeddings for the features together. You have basically three options how to do it:</p>

<ul>
<li>Just concatenate the embeddings and let the following layers of the network to resolve the combination.</li>
<li>Choose the same embedding dimension for all features and average them. (I would start with this one probably.)</li>
<li>Add a <code>nn.Dense</code> layer (or two, the first one with ReLU activation and the second without activation) that will explicitly combine the embeddings for your features.</li>
</ul></li>
</ol>

<p>If you need to include continuous features that cannot be discretized,  you can always take the continuous features, apply a layer or two on top of them and combine them with the embeddings of the discrete features.</p>
",2,0,2308,2019-10-08 07:27:31,https://stackoverflow.com/questions/58281876/word-embeddings-with-multiple-categorial-features-for-a-single-word
Interpreting training.log in Flair (Zalando Research),"<p>I was playing with the Flair library in order to see if there is a big difference (in terms of results) between fine-tuning (implemented separately) and embedding projection. The problem that I'm facing involves reading the results (in this case, the experiment was done by using BERT embeddings).
In the training.log I get this:</p>

<pre><code>2019-10-10 16:27:02,964 Testing using best model ...
2019-10-10 16:27:02,966 loading file best-model.pt

2019-10-10 16:37:23,793 0.7539  0.7539  0.7539

2019-10-10 16:37:23,795

MICRO_AVG: acc 0.605 - f1-score 0.7539
MACRO_AVG: acc 0.5467 - f1-score 0.6925

0 tp: 1420 - fp: 438 - fn: 144 - tn: 363 - precision: 0.7643 - recall: 0.9079 - accuracy: 0.7093 - f1-score: 0.8299
1 tp: 363 - fp: 144 - fn: 438 - tn: 1420 - precision: 0.7160 - recall: 0.4532 - accuracy: 0.3841 - f1-score: 0.5551

2019-10-10 16:37:23,796
</code></pre>

<p>My test dataset contains 2365 instances for a binary text classification task. What do the last 2 lines mean? The 0 and 1 followed by the true positives, precision, recall and so on? What is 0? And what is 1?
I also loaded separately the best model and tested on my test dataset and I obtained different results.</p>

<p>Any help would be greatly appreciated.</p>
","python, word-embedding","<p>Since, you are finetuning for binary classification, precision, recall and F1 measure are a way to evaluate the model, and  whatever you see are the evaluation on the model.</p>

<p>The 1st character 0 or 1, indicates the class 0 or class 1 (2 classes, as its binary classification). And for each class it mentions the number of true-positives (tp), false-positives(fp), false-negatives(fn) and true-negatives(tn). You can sum them all, it will be equal to the number of examples in your test-set.</p>

<p>A short description of tp,tn,fp,fn:</p>

<p>For class 0 (as positive class):</p>

<p>tp: number of actual examples of class 0, correctly predicted as class 0</p>

<p>fn: number of actual examples of class 1, correctly predicted as class 1</p>

<p>fp: number of actual examples of class 1, wrongly predicted  as class 0</p>

<p>tn: number of actual examples of class 0, wrongly predicted  as class 1</p>

<p>And its vice-versa for the 2nd line for class 1.</p>
",1,0,147,2019-10-11 08:50:41,https://stackoverflow.com/questions/58337547/interpreting-training-log-in-flair-zalando-research
Is there an equivalent of word2vec for images?,"<p>I'm wondering if it would be possible to create dense vector representations for an image, similar to how you might create a word embedding with an algorithm like Word2Vec?</p>

<p>I understand that there are some big differences between text and image data – specifically the fact that word2vec uses the word's context to train – but I'm hoping to find a similar counterpart for images.</p>

<p>If a simplistic example of w2v (<a href=""https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469"" rel=""nofollow noreferrer"">from Allison Parrish's GitHub Gist</a>) is:</p>

<pre><code>            | cuteness (0-100) | size (0-100) |
|–––––––––––|––––––––––––––––––|––––––––––––––|
| kitten    |        95        |     15       |
| tarantula |         8        |      3       |
| panda     |        75        |     40       |
| mosquito  |         1        |      1       |
| elephant  |        65        |     90       |
</code></pre>

<p>And another example being <code>king - man + woman = queen</code></p>

<p>Is there some analog (or way of creating some type of analog) for images where you might get something generally along these lines (with some made-up numbers):</p>

<pre><code>                             | amount of people | abstract-ness |
                             | in image (0-100) |    (0-100)    |
|––––––––––––––––––––––––––––|––––––––––––––––––|–––––––––––––––|
| Starry Night               |         0        |       75      |
| Mona Lisa                  |         1        |        9      |
| American Gothic            |         2        |        7      |
| Garden of Earthly Delights |        80        |       50      |
| Les Demoiselles d'Avignon  |         5        |       87      |
</code></pre>

<p>(and just to clarify, know the actual vectors created by an algorithm like Word2Vec wouldn't cleanly fit into human-interpretable categories, but I just wanted to give an analogy to the Word2Vec example.)</p>

<p>or <code>(starry night) - (landscape) + (man) = (van Gogh self portrait)</code> or <code>= (abstract self portrait)</code> or something generally along those lines.</p>

<p>Those might not be the best examples but just to recap, I'm looking for some sort of algorithm for creating an abstract n-dimensional learned representation for an image that can be grouped or compared with vectors representing other images.</p>

<p>Thanks for your help!</p>
","image-processing, deep-learning, word2vec, word-embedding","<p>Absolutely! But...</p>

<p>Such models tend to need significantly <em>larger</em> and <em>deeper</em> neural-networks to learn the representations. </p>

<p>Word2vec uses a very-shallow network, and performs a simple prediction of neighboring-words, often from a tightly limited vocabulary, as the training-goal which (as a beneficial side-effect) throws off compact vectors for each word. </p>

<p>Image-centric algorithms instead try to solve labeling/classification tasks, or regenerate original images under compressed-representation (or adversarial-classifier) constraints. They use 'convolutions' or other multi-layer constructs to intepret the much-larger space of possible pixel values, and some of the interim neural-network layers can be interpretable as compact vectors for the input images.  </p>

<p>Note that even in textual word2vec, the individual ""dense embedding"" dimensions, learned in an unsupervised fashion, <strong>don't</strong> have neat human-interpretability (like ""bigness"", ""cuteness"", etc.). Often, certain directions/neighborhood of the high-dimensional space are vaguely-interpretable, but they're not precise nor aligned exactly with major dimension axes. </p>

<p>Similarly, any compact representations from deep-neural-network image-modeling won't inherently have individual dimensions with clear meanings (unless specific extra constraints with those goals were designed-in) – but again, certain directions/neighborhoods of the high-dimensional space tend to be meaningful (""crowds"", ""a car"", ""smiles"", etc). </p>

<p>A nice overview of some key papers in deep-learning based image-analysis – the kinds of algorithms which throw off compact &amp; meaningful vector summaries of images – that I just found is at:</p>

<p><a href=""https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html"" rel=""nofollow noreferrer"">https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html</a></p>
",2,4,2389,2019-10-15 18:54:36,https://stackoverflow.com/questions/58401016/is-there-an-equivalent-of-word2vec-for-images
Input Shape Error Adding Embedding Layers to LSTM,"<p>I'm trying to add an embedding layer to my LSTM that predicts characters.</p>

<p>I've tried adding an embedding layer in this format, </p>

<pre><code>num_words_in_vocab = 83
max_sentence_length = 40


# build the model: a single LSTM
model = Sequential()
model.add(Embedding(num_words_in_vocab,128,input_length=max_sentence_length))
model.add(LSTM(256, return_sequences=True))
.
.
.
</code></pre>

<p>However, keras throws this error </p>

<pre><code>Error when checking input: expected embedding_8_input to have 2 dimensions, but got array with shape (36736, 40, 83)
</code></pre>

<p>I'm confused because there is no place in the embedding layer to set a variable for the number of examples in the dataset. And I'm not sure how to reshape this dataset to make it work with the embedding layer.</p>

<p>Here is my full code.</p>

<pre><code># -*- coding: utf-8 -*-
#imports
import re
import sys
import numpy
import random
import requests
import numpy as np
import keras.backend as K
from keras import Input, Model
from keras.layers import Permute, multiply, Embedding
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils
from keras.models import Sequential
from keras.optimizers import RMSprop
from keras.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split

#loading book data
html = requests.get(""http://www.gutenberg.org/files/11/11-0.txt"")
text = html.text
#removing some garbage
text = re.sub(r'[^\x00-\x7f]',r'', text)

#making the word plot, but not using it to train bc 57 chars is better than X,xxx words.
split_text = text.splitlines()

def cleanText(text):
  cleanWords = []
  for exerpt in text:
    if exerpt == '':
      pass
    else:
      cleanWords.append(exerpt)
  #take the clean words and make a LIST of clean words
  clean_word_list = []
  for exerpt in cleanWords:
    temp_list = exerpt.split()
    for word in temp_list:
      if word not in clean_word_list:
        clean_word_list.append(word)
      else:
        pass
  #init dict for counting top 50 words
  dict_prevelence = {}
  for exerpt in cleanWords:
    temp_list = exerpt.split()
    for word in temp_list:
      #if not in dict, add to dict_prevelence, else, increment val
      if word not in dict_prevelence:
        dict_prevelence[word] = 1
      else:
        dict_prevelence[word] += 1
  return clean_word_list, dict_prevelence

#cleaning up the alice in wonderland and getting unsorted prevelence dict
clean_word_list, dict_prevelence = cleanText(split_text)
#sorting dict
dict_prevelence = sorted(dict_prevelence.items(), key=lambda x: x[1], reverse=True)




processed_text = text

#getting list of unique chars
chars = sorted(list(set(processed_text)))
print('Total Unique Chars:', len(chars))
#making dicts so we can translate between the two
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

#cutting the text into strings of 100 chars, but incrementing them by 3 chars 
#each time b/c if we incremented by 1, 99% of the string would be the same and 
#it wouldn't train that fast.

#!!! I'm guessing this is knind of a good middle ground between using words and chars and the data,
#with words you get a lot more context from each, but with letters there isn't a huge overhead of empty 
#vectors!!!!!
maxlen = 40
step = 3
sentences = []
next_chars = []
for i in range(0, len(processed_text) - maxlen, step):
    sentences.append(processed_text[i: i + maxlen])
    next_chars.append(processed_text[i + maxlen])

#here we're making the empty data vectors
x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
#now we add each 'sentence' that overlaps by 3 as a data, after encoding it.
#so each x data entry is a 100 int number that corresponds to a slightly overlapping sentence I guess
#and each y data entry would be the NEXT char in that sentence if it continued.
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        x[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

#add a thing here for test train split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, shuffle=False)

print('X_train Data Shape:', X_train.shape)
print('y_train Data Shape:', y_train.shape)

num_words_in_vocab = 83
max_sentence_length = 40


# build the model: a single LSTM
model = Sequential()
model.add(Embedding(num_words_in_vocab,128,input_length=max_sentence_length))
model.add(LSTM(256, return_sequences=True))
model.add(Dropout(0.2))
model.add(Dense(num_words_in_vocab, activation='softmax'))

optimizer = RMSprop(lr=0.01)
model.compile(loss='categorical_crossentropy', optimizer=optimizer)
model.summary()

#putting in this dope thing called callbacks so we can save weights in case we die during training like we have been.
from keras.callbacks import ModelCheckpoint

# checkpoint
filepath=""weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5""
checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='max')

#TRAIN, THAT, MODEL!!
model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=25, batch_size=64,verbose=1)




Any help would be great!
</code></pre>
","python, keras, lstm, word-embedding","<p>In regards to the number of samples, Keras automatically infers that from the input data shape:  X_train, in this case.  </p>

<p>In terms of the use of the embedding layer, the idea is to convert a matrix of integers into a vector.  In your case, it seems like you might be essentially doing that already in the step where you populate ""x"".  You might, instead, want to consider letting the embedding layer compute a vector for each index.  To do this, I believe you would modify ""x"" to be of shape (num_of_sentences, num_of_chars_per_sentence), where the value at each datapoint is the char index for that particular character.  </p>

<p>Also, you might want to set the LSTM return_sequences to ""False"".  I believe you are only looking for the final result from that layer.</p>

<p>I hope this helps.   </p>
",1,0,361,2019-10-22 05:05:49,https://stackoverflow.com/questions/58497395/input-shape-error-adding-embedding-layers-to-lstm
"Calculate Cross-Lingual Phrase Similarity (using e.g., MUSE and Gensim)","<p>I am new to NLP and Word Embeddings and still need to learn many concepts within these topics, so any pointers would be appreciated. This question is related to <a href=""https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages"">this</a> and <a href=""https://stackoverflow.com/questions/51233632/word2vec-gensim-multiple-languages?noredirect=1&amp;lq=1"">this</a>, and I think there may have been developments since these questions had been asked. Facebook <a href=""https://arxiv.org/pdf/1710.04087.pdf"" rel=""nofollow noreferrer"">MUSE</a> provides aligned, supervised <a href=""https://github.com/facebookresearch/MUSE#multilingual-word-embeddings"" rel=""nofollow noreferrer"">word embeddings for 30 languages</a>, and it can be used to calculate word similarity across different languages. As far as I understand, The embeddings provided by MUSE satisfy the requirement of <a href=""https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages"">coordinate space compatibilty</a>. It seems that it is possible to <a href=""https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim"">load these embeddings into libraries such as Gensim</a>, but I wonder: </p>

<ol>
<li>Is it possible to load multiple-language word embeddings 
into Gensim (or other libraries), and if so:</li>
<li>What type of similarity measure
might fit in this use case?</li>
<li>How to use these loaded word embeddings
to calculate cross-lingual similarity score of phrases* instead of
words?</li>
</ol>

<p>*e.g., ""<em>ÖPNV</em>"" in German vs ""<em>Trasporto pubblico locale</em>"" in Italian for the English term ""<em>Public Transport</em>"". </p>

<p>I am open o any implementation (libraries/languages/embeddings) though I may need some time to learn this topic. Thank you in advance.</p>
","python, nlp, multilingual, gensim, word-embedding","<p>It is quite usual to average multiple word embeddings to get a phrase or sentence representation. After all, this is exactly what FastText does by default when it is used for sentence classification.</p>

<p>You can, of course, load as many word-embeddings sets in Gensim, but you would need to implement the cross-lingual comparison yourself. You can the vector just using the square bracket notation:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.fasttext.load_facebook_model('your_path')
vector = model['computer']
</code></pre>

<p>Just use cosine similarity for comparing the vector. If you don't want to write it yourself, use <a href=""https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html"" rel=""nofollow noreferrer"">scipy</a>. </p>
",2,1,1030,2019-10-25 10:37:05,https://stackoverflow.com/questions/58556924/calculate-cross-lingual-phrase-similarity-using-e-g-muse-and-gensim
How to save self-trained word2vec to a txt file with format like &#39;word2vec-google-news&#39; or &#39;glove.6b.50d&#39;,"<p>I wonder that how can I save a self-trained word2vec to txt file with the format like 'word2vec-google-news' or 'glove.6b.50d' which has the tokens followed by matched vectors.<a href=""https://i.sstatic.net/YdCbv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YdCbv.png"" alt="".""></a></p>

<p>I export my self-trained vectors to txt file which only has vectors but no tokens in the front of those vectors.
<a href=""https://i.sstatic.net/gu2yr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gu2yr.png"" alt=""enter image description here""></a></p>

<p>My code for training my own word2vec:</p>

<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import math
import random
import numpy as np
from six.moves import xrange
import zipfile
import tensorflow as tf
import pandas as pd

filename = ('data/data.zip')

# Step 1: Read the data into a list of strings.
def read_data(filename):
  with zipfile.ZipFile(filename) as f:
    data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data

words = read_data(filename)
#print('Data size', len(words))


# Step 2: Build the dictionary and replace rare words with UNK token.
vocabulary_size = 50000
def build_dataset(words):
    count = [['UNK', -1]]
    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    #print(""count"",len(count))
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0
            unk_count += 1
        data.append(index)
    count[0][1] = unk_count
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reverse_dictionary

data, count, dictionary, reverse_dictionary = build_dataset(words)

#del words  # Hint to reduce memory.
#print('Most common words (+UNK)', count[:5])
#print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])



data_index = 0

# Step 3: Function to generate a training batch for the skip-gram model.
def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips &lt;= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1  # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window  # target label at the center of the buffer
        targets_to_avoid = [skip_window]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]
            labels[i * num_skips + j, 0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    return batch, labels

batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)
#for i in range(8):
 #print(batch[i], reverse_dictionary[batch[i]],'-&gt;', labels[i, 0], reverse_dictionary[labels[i, 0]])

# Step 4: Build and train a skip-gram model.
batch_size = 128
embedding_size = 128
skip_window = 2
num_skips = 2
valid_size = 9
valid_window = 100
num_sampled = 64    # Number of negative examples to sample.
valid_examples = np.random.choice(valid_window, valid_size, replace=False)

graph = tf.Graph()
with graph.as_default():
    # Input data.
    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # Ops and variables pinned to the CPU because of missing GPU implementation
    with tf.device('/cpu:0'):
        # Look up embeddings for inputs.
        embeddings = tf.Variable(
            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
        embed = tf.nn.embedding_lookup(embeddings, train_inputs)

        # Construct the variables for the NCE loss
        nce_weights = tf.Variable(
            tf.truncated_normal([vocabulary_size, embedding_size],
                                stddev=1.0 / math.sqrt(embedding_size)))
        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),dtype=tf.float32)

    # Compute the average NCE loss for the batch.
    # tf.nce_loss automatically draws a new sample of the negative labels each
    # time we evaluate the loss.
    loss = tf.reduce_mean(
            tf.nn.nce_loss(weights=nce_weights,biases=nce_biases, inputs=embed, labels=train_labels,
                 num_sampled=num_sampled, num_classes=vocabulary_size))

    # Construct the SGD optimizer using a learning rate of 1.0.
    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)

    # Compute the cosine similarity between minibatch examples and all embeddings.
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)

    # Add variable initializer.
    init = tf.global_variables_initializer()

# Step 5: Begin training.
num_steps = 20000

with tf.Session(graph=graph) as session:
    # We must initialize all variables before we use them.
    init.run()
    #print(""Initialized"")

    average_loss = 0
    for step in xrange(num_steps):
        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)
        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

        # We perform one update step by evaluating the optimizer op (including it
        # in the list of returned values for session.run()
        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
        average_loss += loss_val

        #if step % 2000 == 0:
         #   if step &gt; 0:
          #      average_loss /= 2000
            # The average loss is an estimate of the loss over the last 2000 batches.
           # print(""Average loss at step "", step, "": "", average_loss)
            #average_loss = 0

    final_embeddings = normalized_embeddings.eval()


np.savetxt('data/w2v.txt', final_embeddings)
</code></pre>
","machine-learning, nlp, word2vec, word-embedding","<p>You may want to look at the implementation of <code>_save_word2vec_format()</code> in <code>gensim</code> for an example of Python code which writes that format:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/utils_any2vec.py#L104"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/utils_any2vec.py#L104</a></p>

<pre class=""lang-py prettyprint-override""><code>def _save_word2vec_format(fname, vocab, vectors, fvocab=None, binary=False, total_vec=None):
    """"""Store the input-hidden weight matrix in the same format used by the original
    C word2vec-tool, for compatibility.
    Parameters
    ----------
    fname : str
        The file path used to save the vectors in.
    vocab : dict
        The vocabulary of words.
    vectors : numpy.array
        The vectors to be stored.
    fvocab : str, optional
        File path used to save the vocabulary.
    binary : bool, optional
        If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.
    total_vec : int, optional
        Explicitly specify total number of vectors
        (in case word vectors are appended with document vectors afterwards).
    """"""
    if not (vocab or vectors):
        raise RuntimeError(""no input"")
    if total_vec is None:
        total_vec = len(vocab)
    vector_size = vectors.shape[1]
    if fvocab is not None:
        logger.info(""storing vocabulary in %s"", fvocab)
        with utils.open(fvocab, 'wb') as vout:
            for word, vocab_ in sorted(iteritems(vocab), key=lambda item: -item[1].count):
                vout.write(utils.to_utf8(""%s %s\n"" % (word, vocab_.count)))
    logger.info(""storing %sx%s projection weights into %s"", total_vec, vector_size, fname)
    assert (len(vocab), vector_size) == vectors.shape
    with utils.open(fname, 'wb') as fout:
        fout.write(utils.to_utf8(""%s %s\n"" % (total_vec, vector_size)))
        # store in sorted order: most frequent words at the top
        for word, vocab_ in sorted(iteritems(vocab), key=lambda item: -item[1].count):
            row = vectors[vocab_.index]
            if binary:
                row = row.astype(REAL)
                fout.write(utils.to_utf8(word) + b"" "" + row.tostring())
            else:
                fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
</code></pre>
",1,0,2714,2019-10-28 08:59:27,https://stackoverflow.com/questions/58588057/how-to-save-self-trained-word2vec-to-a-txt-file-with-format-like-word2vec-googl
Finding both target and center word2vec matrices,"<p>I've read and heard(In the CS224 of Stanford) that the Word2Vec algorithm actually trains two matrices(that is, two sets of vectors.) These two are the U and the V set, one for words being a target and one for words being the context. The final output is the average of these two.
I have two questions in mind. one is that:  </p>

<ul>
<li><p>Why do we get an average of two vectors? Why it makes sense? Don't we lose some information?  </p></li>
<li><p>The second question is, using pre-trained word2vec models, how can I get access to both matrices? Is there any downloadable word2vec with both sets of vectors? I don't have enough resources to train a new one.</p></li>
</ul>

<p>Thanks</p>
","nlp, word2vec, word-embedding","<p>That relayed description isn't quite right. The word-vectors traditionally retrieved from a word2vec model come from a ""projection matrix"" which converts individual words to a right-sized input-vector for the shallow neural network. </p>

<p>(You could think of the projection matrix as turning a one-hot encoding into a dense-embedding for that word, but libraries typically implement this via a dictionary-lookup – eg: ""what row of the vectors-matrix should I consult for this word-token?"")</p>

<p>There's another matrix of weights leading to the model's output nodes, whose interpretation varies based on the training mode. In the common default of negative-sampling, there's one node per known word, so you could also interpret this matrix as having a vector per word. (In hierarchical-softmax mode, the known-words aren't encoded as single output nodes, so it's harder to interpret the relationship of this matrix to individual words.)</p>

<p>However, this second vector per word is rarely made directly available by libraries. Most commonly, the word-vector is considered simply the trained-up input vector, from the projection matrix. For example, the export format from Google's original <code>word2vec.c</code> release only saves-out those vectors, and the large ""GoogleNews"" vector set they released only has those vectors. (There's no averaging with the other output-side representation.)</p>

<p>Some work, especially that of Mitra et all of Microsoft Research (in ""<a href=""https://www.microsoft.com/en-us/research/project/dual-embedding-space-model-desm/"" rel=""nofollow noreferrer"">Dual Embedding Space Models</a>"" &amp; associated writeups) has noted those output-side vectors may be of value in some applications as well – but I haven't seen much other work using those vectors. (And, even in that work, they're not <em>averaged</em> with the traditional vectors, but consulted as a separate option for some purposes.)</p>

<p>You'd have to look at the code of whichever libraries you're using to see if you can fetch these from their full post-training model representation. In the Python <code>gensim</code> library, this second matrix in the negative-sampling case is a model property named <code>syn1neg</code>, following the naming of the original <code>word2vec.c</code>.</p>
",0,0,156,2019-10-30 15:13:03,https://stackoverflow.com/questions/58628480/finding-both-target-and-center-word2vec-matrices
word2vec - KeyError: &quot;word X not in vocabulary&quot;,"<p>Using the <code>Word2Vec</code> implementation of the module <code>gensim</code> in order to construct word embeddings for the sentences I do have in a plain text file. Despite the word <code>happy</code> is defined in the vocabulary, getting the error <code>KeyError: ""word 'happy' not in vocabulary""</code>. Tried to apply the given the answers to <a href=""https://stackoverflow.com/questions/41133844/keyerror-word-word-not-in-vocabulary-in-word2vec"">a similar question</a>, but did not work. Hence, posted my own question.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    # When I debug, both of the words 'happy' and 'birthday' exist in the variable 'data'
    word2vec = Word2Vec(data, min_count=5, size=10000, window=5, workers=4)

    # Print result
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru word2vec: {word2vec.similarity(word_1, word_2)}')
except Exception as err:
    print(f'An error happened! Detail: {str(err)}')
</code></pre>
","gensim, word2vec, word-embedding","<p>When you get a ""not in vocabulary"" error like this from <code>Word2Vec</code>, you can trust it: <code>'happy'</code> really isn't in the model. </p>

<p>Even if your visual check shows <code>'happy'</code> inside your file, a few reasons why it might not wind up inside the model include:</p>

<ul>
<li><p>it doesn't occur at least <code>min_count=5</code> times</p></li>
<li><p>the <code>data</code> format isn't correct for <code>Word2Vec</code>, so it's not seeing the words you expect it to see. </p></li>
</ul>

<p>Looking at how <code>data</code> is prepared by your code, it looks like a giant list of all words in your file. <code>Word2Vec</code> instead expects a sequence that has, as each item, a list-of-words for that one text. So: not a list-of-words, but a list where each item is a list-of-words. </p>

<p>If you've supplied...</p>

<pre class=""lang-py prettyprint-override""><code>[
  'happy',
  'birthday',
]
</code></pre>

<p>...instead of the expected...</p>

<pre class=""lang-py prettyprint-override""><code>[
  ['happy', 'birthday',],
]
</code></pre>

<p>...those single-word-strings will be seen a lists-of-characters, so <code>Word2Vec</code> will think you want to learn word-vectors for a bunch of one-character words. You can check if this has affected your model by seeing if the vocabulary size seems small (<code>len(model.wv)</code>) or if a sample of learned-words is only single-character words ('model.wv.index2entity[:10]`).</p>

<p>If you supply a word in the right format, at least <code>min_count</code> times, as part of the training-data, it will wind up with a vector in the model.</p>

<p>(Separately: <code>size=10000</code> is a choice way outside the usual range of 100-400. I've never seen a project using such high-dimensionality for word-vectors, and it would only be theoretically justifiable if you had a massively-large vocabulary and training-set. Oversized vectors with smaller vocabularies/data are likely to create uselessly overfit results.)</p>
",2,0,3011,2019-11-01 22:45:31,https://stackoverflow.com/questions/58666699/word2vec-keyerror-word-x-not-in-vocabulary
fastText - Throws exception without any reasons,"<p>I'm using <code>fastText</code> implementation of the module <code>gensim</code>. Despite getting no reasons, my program throws an exception.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    fastText = FastText(data, min_count=1, size=10000, window=5, workers=4)

    # Print results
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru fastText: {fastText.similarity(word_1, word_2)}')
except Exception as err:
    print(f'\n!!!!! An error happened! Detail: {str(err)}')
</code></pre>

<p>The end of the output:</p>

<pre><code>!!!!! An error happened! Detail: 
</code></pre>
","python-3.x, gensim, word-embedding, fasttext","<p>Per my answer on <a href=""https://stackoverflow.com/questions/58666699/word2vec-keyerror-word-x-not-in-vocabulary"">your other question</a>, your <code>data</code> doesn't appear to be in the right format (where each item is a list-of-strings), and <code>size=10000</code> is far outside of the usual range of sensible vector-sizes. </p>

<p>But mainly, if you want more exception info, you shouldn't be catching <code>Exception</code> and printing your own minimal, cryptic error message. Remove the <code>try</code>/<code>except</code> handling from your code, run it again, and you should see a more helpful error message, including a call stack which shows exactly which line of your code (and lines of called library code) are involved in the error condition. </p>

<p>If that alone doesn't guide you to fix the issue, you could add the extra details of the full error &amp; call stack to your question to help others see what's happening. </p>
",0,0,198,2019-11-01 23:02:35,https://stackoverflow.com/questions/58666807/fasttext-throws-exception-without-any-reasons
How to use word embedding as features for CRF (sklearn-crfsuite) model training,"<p>I want to develop an NER model where I want to use word-embedding features to train CRF model. Code perfectly working without word-embedding features but when I insert embedding as features for CRF training, got error messages. Here is the part of snippet of my code: </p>

<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('ggplot')

from itertools import chain

import nltk
import sklearn
import scipy.stats
from sklearn.metrics import make_scorer
#from sklearn.cross_validation import cross_val_score
#from sklearn.grid_search import RandomizedSearchCV

import sklearn_crfsuite
from sklearn_crfsuite import scorers
from sklearn_crfsuite import metrics
import pickle
from gensim.models import KeyedVectors
import numpy as np
# Load vectors directly from the file
model1 = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) ### Loading pre-trainned word2vec model
### Embedding function 
def get_features(word):
    word=word.lower()
    vectors=[]
    try:
        vectors.append(model1[word])
    except:
        pass
    #vectors=np.array(vectors)
    #vectors=vectors[0]
    return vectors

def word2features(sent, i):
    word = sent[i][0]
    wordembdding=get_features(word)   ## word embedding vector 
    wordembdding=np.array(wordembdding) ## vectors 
    #wordembdding= 
    #wordembdding=wordembdding[0]
    postag = sent[i][1]
    tag1=sent[i][2]
    tag2=sent[i][4]
    tag3 = sent[i][5]


    features = {
        'bias': 1.0,
        'word.lower()': word.lower(),
        'word[-3:]': word[-3:],
        'word[-2:]': word[-2:],
        'wordembdding': wordembdding,
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit(),
        'postag': postag,
        'postag[:2]': postag[:2],
        'tag1': tag1,
        'tag1[:2]': tag1[:2],
        'tag2': tag2,
        'tag2[:2]': tag2[:2],
        'tag3': tag3,
        'tag3[:2]': tag3[:2],
        'wordlength': len(word),
        'wordinitialcap': word[0].isupper(),
        'wordmixedcap': len([x for x in word[1:] if x.isupper()])&gt;0,
        'wordallcap': len([x for x in word if x.isupper()])==len(word),
        'distfromsentbegin': i
    }
    if i &gt; 0:
        word1 = sent[i-1][0]
        wordembdding1= get_features(word1)
        wordembdding1=np.array(wordembdding1)
        #wordembdding1=f2(wordembdding1)
        postag1 = sent[i-1][1]
        tag11=sent[i-1][2]
        tag22=sent[i-1][4]
        tag33 = sent[i-1][5]
        features.update({
            '-1:word.lower()': word1.lower(),
            '-1:word.istitle()': word1.istitle(),
            '-1:word.isupper()': word1.isupper(),
            '-1:wordembdding': wordembdding1,   # word embedding features 
            '-1:postag': postag1,
            '-1:postag[:2]': postag1[:2],
            '-1:tag1': tag1,
            '-1:tag1[:2]': tag1[:2],
            '-1:tag2': tag2,
            '-1:tag2[:2]': tag2[:2],
            '-1:tag3': tag3,
            '-1:tag3[:2]': tag3[:2],
            '-1:wordlength': len(word),
            '-1:wordinitialcap': word[0].isupper(),
            '-1:wordmixedcap': len([x for x in word[1:] if x.isupper()])&gt;0,
            '-1:wordallcap': len([x for x in word if x.isupper()])==len(word),
        })
    else:
        features['BOS'] = True

    if i &lt; len(sent)-1:
        word1 = sent[i+1][0]
        wordembdding1= get_features(word1)
        wordembdding1= get_features(word1)
        wordembdding1=np.array(wordembdding1) ## word embedding features 
        #wordembdding1=f2(wordembdding)
        postag1 = sent[i+1][1]
        tag11=sent[i+1][2]
        tag22=sent[i+1][4]
        tag33 = sent[i+1][5]
        features.update({
            '+1:word.lower()': word1.lower(),
            '+1:word.istitle()': word1.istitle(),
            '+1:word.isupper()': word1.isupper(),
            '+1:wordembdding': wordembdding1,
            '+1:postag': postag1,
            '+1:postag[:2]': postag1[:2],
            '+1:tag1': tag1,
            '+1:tag1[:2]': tag1[:2],
            '+1:tag2': tag2,
            '+1:tag2[:2]': tag2[:2],
            '+1:tag3': tag3,
            '+1:tag3[:2]': tag3[:2],
            '+1:wordlength': len(word),
            '+1:wordinitialcap': word[0].isupper(),
            '+1:wordmixedcap': len([x for x in word[1:] if x.isupper()])&gt;0,
            '+1:wordallcap': len([x for x in word if x.isupper()])==len(word),
        })
    else:
        features['EOS'] = True

    return features


def sent2features(sent):
    return [word2features(sent, i) for i in range(len(sent))]

def sent2labels(sent):
    return [label for token, postag, tag1, label, tag2, tag3 in sent]

def sent2tokens(sent):
    return [token for token, postag, tag1, label, tag2, tag3, tag4, tag5 in sent]



X_train = [sent2features(s) for s in train_sents]
y_train = [sent2labels(s) for s in train_sents]

X_test = [sent2features(s) for s in test_sents]
y_test = [sent2labels(s) for s in test_sents]


%%time
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,
    c2=0.1,
    max_iterations=100,
    all_possible_transitions=True
)
crf.fit(X_train, y_train)   ### Error message when try to train
</code></pre>

<p>When I want to train the CRF model I got this error messages:</p>

<p><strong><code>TypeError: only size-1 arrays can be converted to Python scalars</code></strong></p>

<p>Can anyone suggest me how to use word embedding vectors to train CRF model ?</p>
","python, word-embedding, crfsuite, python-crfsuite","<p>As you can read <a href=""https://github.com/scrapinghub/python-crfsuite/issues/39"" rel=""noreferrer"">here</a>, currently <em>python-crfsuite</em> and <em>sklearn-crfsuite</em> don't support array features, like word embeddings.</p>

<p>Instead, you can pass every vector component as a feature.</p>

<pre><code>{...
 'v0': 1.81583762e-02,
 'v1': 2.83553465e-02,
  ...
 'v299': -4.26079705e-02,
 ...}
</code></pre>

<p>I suggest to replace your <strong>get_features</strong> function:</p>

<pre><code>def get_features(word):
    word=word.lower()
    try:
         vector=model1[word]
    except:
        # if the word is not in vocabulary,
        # returns zeros array
        vector=np.zeros(300,)

    return vector   
</code></pre>

<p>Then modify <strong>word2features</strong> function, to return a new feature for every component of the vector:</p>

<pre><code>def word2features(sent, i):
    word = sent[i][0]
    wordembdding=get_features(word)   ## word embedding vector 
    postag = sent[i][1]
    tag1=sent[i][2]
    tag2=sent[i][4]
    tag3 = sent[i][5]


    features = {
        'bias': 1.0,
        'word.lower()': word.lower(),
        'word[-3:]': word[-3:],
        'word[-2:]': word[-2:],
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit(),
        'postag': postag,
        'postag[:2]': postag[:2],
        'tag1': tag1,
        'tag1[:2]': tag1[:2],
        'tag2': tag2,
        'tag2[:2]': tag2[:2],
        'tag3': tag3,
        'tag3[:2]': tag3[:2],
        'wordlength': len(word),
        'wordinitialcap': word[0].isupper(),
        'wordmixedcap': len([x for x in word[1:] if x.isupper()])&gt;0,
        'wordallcap': len([x for x in word if x.isupper()])==len(word),
        'distfromsentbegin': i
    }

    # here you add 300 features (one for each vector component)
    for iv,value in enumerate(wordembdding):
        features['v{}'.format(iv)]=value

# And so on...
</code></pre>

<p><em>Two small notes:</em></p>

<ul>
<li>if in your text there are many words, which are not in the vocabulary, word embeddings cannot improve much your NER model. Maybe you can use Fasttext (also integrated in Gensim), which can properly handle unseen words.</li>
<li>even if it useful, adding vector embeddings for each word can make your training set very big, produce long training time and a very big classifier.</li>
</ul>
",7,6,3836,2019-11-06 18:36:06,https://stackoverflow.com/questions/58736548/how-to-use-word-embedding-as-features-for-crf-sklearn-crfsuite-model-training
Question pairs (ground truth) datasets for Word2Vec model testing?,"<p>I'm looking for test datasets to optimize my Word2Vec model. I have found a good one from gensim:</p>

<p>gensim/test/test_data/questions-words.txt </p>

<p>Does anyone know other similar datasets?</p>

<p>Thank you!</p>
","machine-learning, nlp, word2vec, word-embedding","<p>It is important to note that there isn't really a ""ground truth"" for word-vectors. There are interesting tasks you can do with them, and some arrangements of word-vectors will be better on a specific tasks than others.</p>

<p>But also, the word-vectors that are best on one task – such as analogy-solving in the style of the <code>questions-words.txt</code> problems – might not be best on another important task – like say modeling texts for classification or info-retrieval.</p>

<p>That said, you can make your own test data in the same format as <code>questions-words.txt</code>.  Google's original <code>word2vec.c</code> release, which also included a tool for statistically combining nearby words into multi-word phrases, also included a <a href=""https://github.com/tmikolov/word2vec/blob/master/questions-phrases.txt"" rel=""nofollow noreferrer""><code>questions-phrases.txt</code></a> file, in the same format, that can be used to test word-vectors that have been similarly constructed for 'words' that are actually short multiple-word phrases. </p>

<p>The Python <code>gensim</code> word-vectors support includes an extra method, <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer""><code>evaluate_word_pairs()</code></a> for checking word-vectors not on analogy-solving but on conformance to collections of human-determined word-similarity-rankings. The documentation for that method includes a link to an appropriate test-set for that method, <a href=""https://fh295.github.io//simlex.html"" rel=""nofollow noreferrer""><code>SimLex-999</code></a>, and you may be able to find other test sets of the same format elsewhere. </p>

<p>But, again, none of these should be considered the absolute test of word-vectors' overall quality. The best test, for your particular project's use of word-vectors, would be some repeatable domain-specific evaluation score you devise yourself, that's inherently correlated to your end goals.</p>
",0,-2,494,2019-11-08 17:40:47,https://stackoverflow.com/questions/58771410/question-pairs-ground-truth-datasets-for-word2vec-model-testing
"In Fasttext skipgram training, what will happen if some sentences in the corpus have just one word?","<p>Imagine that you have a corpus in which some lines have just one word, so there is no context around some of the words. In this situation how does Fasttext perform to provide embeddings for these single words? <em>Note that the frequency of some of these words are one and there is no cut-off to get rid of them.</em>    </p>
","neural-network, word2vec, word-embedding, fasttext","<p>There's no way to train a <code>context_word -&gt; target_word</code> skip-gram pair for such words (in either 'context' or 'target' roles), so such words can't receive trained representations. Only texts with at least 2 tokens contribute anything to word2vec or FastText word-vector training. </p>

<p>(One possible exception: FastText in its 'supervised classification' mode <em>might</em> be able to make use of, and train vectors for, such words, because then even single words can be used to predict the known-label of training texts.)</p>

<p>I suspect that such corpuses will still result in the model counting the word in its initial vocabulary-discovery scan, and thus it will be allocated a vector (if it appears at least <code>min_count</code> times), and that vector will receive the usual small-random-vector initialization. But the word-vector will receive no further training – so when you request the vector back after training, it will be of low-quality, with the only meaningful contributions coming from any char n-grams shared with other words that received real training. </p>

<p>You should consider any text-breaking process that results in single-word texts as buggy for the purposes of FastText. If those single-word texts come from another meaningful context where they were once surrounded by other contextual words, you should change your text-breaking process to work in larger chunks that retain that context. </p>

<p>Also note: it's rare for <code>min_count=1</code> to be a good idea for word-vector models, at least when the training text is real natural-language material where word-token frequencies roughly follow Zipf's law. There will be many, many 1-occurrence (or few-occurrence) words, but with just one to a few example usage contexts, not likely representing the true breadth and subtleties of that word's real usages, it's nearly impossible for such words to receive good vectors that generalize to other uses of those same words elsewhere. </p>

<p>Training good vectors require a variety of usage examples, and just one or a few examples will practically be ""noise"" compared to the tens-to-hundreds of examples of other words' usage. So keeping these rare words, instead of dropping them like a default <code>min_count=5</code> (or higher in larger corpuses) would do, tends to slow training, slow convergence (""settling"") of the model, and lower the quality of the <em>other</em> more-frequent word vectors at the end – due to the significant-but-largely-futile efforts of the algorithm to helpfully position these many rare words.</p>
",1,0,325,2019-11-11 16:08:37,https://stackoverflow.com/questions/58804843/in-fasttext-skipgram-training-what-will-happen-if-some-sentences-in-the-corpus
NLP Transformers: Best way to get a fixed sentence embedding-vector shape?,"<p>I'm loading a language model from torch hub (<a href=""https://camembert-model.fr/#about"" rel=""nofollow noreferrer"">CamemBERT</a> a French RoBERTa-based model) and using it do embed some french sentences:  </p>

<pre class=""lang-py prettyprint-override""><code>import torch
camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')
camembert.eval()  # disable dropout (or leave in train mode to finetune)


def embed(sentence):
   tokens = camembert.encode(sentence)
   # Extract all layer's features (layer 0 is the embedding layer)
   all_layers = camembert.extract_features(tokens, return_all_hiddens=True)
   embeddings = all_layers[0]
   return embeddings

# Here we see that the shape of the embedding vector depends on the number of tokens in the sentence

u = embed(sentence=""Bonjour, ça va ?"")
u.shape # torch.Size([1, 7, 768])
v = embed(sentence=""Salut, comment vas-tu ?"")
v.shape # torch.Size([1, 9, 768])

</code></pre>

<p>Imagine now in order to do some <strong>semantic search</strong>, I want to calculate the <code>cosine distance</code> between the vectors (tensors in our case) <code>u</code> and <code>v</code> : </p>

<pre class=""lang-py prettyprint-override""><code>cos = torch.nn.CosineSimilarity(dim=1)
cos(u, v) # will throw an error since the shape of `u` is different from the shape of `v`
</code></pre>

<p>I'm asking what is the best method to use in order to always get the <strong>same embedding shape</strong> for a sentence <strong>regardless the count of its tokens</strong>?</p>

<p>=> The first solution I'm thinking of is calculating the <code>mean on axis=1</code> (embedding of a sentence is the mean embedding its tokens) since axis=0 and axis=2 have always the same size:</p>

<pre class=""lang-py prettyprint-override""><code>cos = torch.nn.CosineSimilarity(dim=1)
cos(u.mean(axis=1), v.mean(axis=1)) # works now and gives 0.7269
</code></pre>

<p>But, I'm afraid that I'm hurting the embedding of the sentence when calculating the mean since it gives the same weight for each token (maybe multiplying by <strong>TF-IDF</strong>?).</p>

<p>=> The second solution is to pad shorter sentences out. That means:  </p>

<ul>
<li>giving a list of sentences to embed at a time (instead of embedding sentence by sentence)</li>
<li>look up for the sentence with the longest tokens and embed it, get its shape <code>S</code></li>
<li>for the rest of sentences embed then pad zero to get the same shape <code>S</code> (the sentence has 0 in the rest of dimensions)</li>
</ul>

<p>What are your thoughts?
What other techniques would you use and why?</p>

<p>Thanks in advance!</p>
","machine-learning, deep-learning, nlp, pytorch, word-embedding","<p><a href=""https://github.com/hanxiao/bert-as-service/"" rel=""nofollow noreferrer"">Bert-as-service</a> is a great example of doing exactly what you are asking about. </p>

<p>They use padding. But read the FAQ, in terms of which layer to get the representation from how to pool it: long story short, depends on the task. </p>

<p>EDIT: I am not saying ""use Bert-as-service""; I am saying ""rip off what Bert-as-service does."" </p>

<p>In your example, you are getting word embeddings (because of the layer you are extracting from). <a href=""https://github.com/hanxiao/bert-as-service/#getting-elmo-like-contextual-word-embedding"" rel=""nofollow noreferrer"">Here is how Bert-as-service does that</a>. So, it actually shouldn't surprise you that this depends on sentence length.</p>

<p>You then talk about getting sentence embeddings by mean pooling over word embeddings. That is... a way to do it. But, using <a href=""https://github.com/hanxiao/bert-as-service/#getting-elmo-like-contextual-word-embedding"" rel=""nofollow noreferrer"">Bert-as-service as a guide for how to get a fixed-length representation from Bert</a>...</p>

<blockquote>
  <p>Q: How do you get the fixed representation? Did you do pooling or something?</p>
  
  <p>A: Yes, pooling is required to get a fixed representation of a sentence. In the default strategy REDUCE_MEAN, I take the second-to-last hidden layer of all of the tokens in the sentence and do average pooling.</p>
</blockquote>

<p>So, to do Bert-as-service's default behavior, you'd do</p>

<pre class=""lang-py prettyprint-override""><code>def embed(sentence):
   tokens = camembert.encode(sentence)
   # Extract all layer's features (layer 0 is the embedding layer)
   all_layers = camembert.extract_features(tokens, return_all_hiddens=True)
   pooling_layer = all_layers[-2]
   embedded = pooling_layer.mean(1)  # 1 is the dimension you want to average ovber
   # note, using numpy to take the mean is bad if you want to stay on GPU
   return embedded
</code></pre>
",4,9,11391,2019-11-25 11:36:12,https://stackoverflow.com/questions/59030907/nlp-transformers-best-way-to-get-a-fixed-sentence-embedding-vector-shape
ValueError:Layer conv1d was called with an input that isn&#39;t a symbolic tensor.All inputs to the layer should be tensors,"<p>I built this model and it was working fine. </p>

<pre><code>###Building the Model. 
input_layer= Embedding(num_words, 300, input_length=35, weights=[embedding_matrix],trainable=True)
conv_blocks = []
filter_sizes = (2,3,4)
for fx in filter_sizes:
    conv_layer= Conv1D(100, kernel_size=fx, activation='relu', data_format='channels_first')(input_layer)
    maxpool_layer = MaxPooling1D(pool_size=4)(conv_layer)
    flat_layer= Flatten()(maxpool_layer)
    conv_blocks.append(flat_layer)
#conc_layer=concatenate(conv_blocks, axis=1)
conc_layer=Concatenate(axis=-1)([conv_blocks])
graph = Model(inputs=input_layer, outputs=conc_layer)

model = Sequential()
model.add(graph)
model.add(Dropout(0.2))
model.add(Dense(3, activation='sigmoid'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
</code></pre>

<p>I recently reran it and I'm getting an error </p>

<pre><code>Traceback (most recent call last):
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
  File ""/am/embassy/vol/x6/jetbrains/apps/PyCharm-P/ch-0/191.6183.50/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/am/embassy/vol/x6/jetbrains/apps/PyCharm-P/ch-0/191.6183.50/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/kosimadukwe/PycharmProjects/untitled/WordEmb.py"", line 128, in &lt;module&gt;
conv_layer= Conv1D(100, kernel_size=fx, activation='relu', data_format='channels_first')(input_layer)   #filters=100, kernel_size=3
  File ""/home/kosimadukwe/PycharmProjects/untitled/venv/lib/python3.7/site-packages/keras/engine/base_layer.py"", line 414, in __call__
self.assert_input_compatibility(inputs)
  File ""/home/kosimadukwe/PycharmProjects/untitled/venv/lib/python3.7/site-packages/keras/engine/base_layer.py"", line 285, in assert_input_compatibility
str(inputs) + '. All inputs to the layer '
ValueError: Layer conv1d_1 was called with an input that isn't a symbolic tensor. Received type: &lt;class 'keras.layers.embeddings.Embedding'&gt;. Full input: [&lt;keras.layers.embeddings.Embedding object at 0x7fae61513c18&gt;]. All inputs to the layer should be tensors.
</code></pre>

<p>I have checked similar post here but none is very similar to mine. I have tried their suggestions like adding the axis to Concatenate() or using concatenate instead but nothing changed. </p>

<pre><code>[embedding_matrix] is a 2d array
</code></pre>
","keras, conv-neural-network, sequential, word-embedding, tf.keras","<p>Error is thrown because <code>input_layer</code> is <code>Layer</code> and not <code>Tensor</code>.
You are passing <code>Embedding</code> ""layer"" as input to <code>Conv1D</code>, in this case you have not provided any input to embedding layer.</p>

<p>Change this one:</p>

<pre><code>input_layer= Embedding(num_words, 300, input_length=35, weights=[embedding_matrix],trainable=True)
</code></pre>

<p>and add input tensor to this layer:</p>

<pre><code>input_layer= Embedding(num_words, 300, input_length=35, weights=[embedding_matrix],trainable=True)(input_tensor)
</code></pre>

<p><br>
Also I think you are trying to <code>Concatenate</code> outputs from three separate filters, if that is the case then:</p>

<pre><code>conc_layer=Concatenate(axis=-1)([conv_blocks])
graph = Model(inputs=input_layer, outputs=conc_layer)
</code></pre>

<p>this part would come outside loop.</p>
",2,2,2900,2019-11-26 00:51:30,https://stackoverflow.com/questions/59042165/valueerrorlayer-conv1d-was-called-with-an-input-that-isnt-a-symbolic-tensor-al
Natural Language Processing techniques for understanding contextual words,"<p>Take the following sentence:</p>

<pre><code>I'm going to change the light bulb
</code></pre>

<p>The meaning of <code>change</code> means <code>replace</code>, as in someone is going to replace the light bulb. This could easily be solved by using a dictionary api or something similar. However, the following sentences</p>

<pre><code>I need to go the bank to change some currency

You need to change your screen brightness
</code></pre>

<p>The first sentence does not mean <code>replace</code> anymore, it means <code>Exchange</code>and the second sentence, <code>change</code> means <code>adjust</code>.</p>

<p>If you were trying to understand the meaning of <code>change</code> in this situation, what techniques would someone use to extract the correct definition based off of the context of the sentence? What is what I'm trying to do called?</p>

<p>Keep in mind, the input would only be one sentence. So something like:</p>

<pre><code>Screen brightness is typically too bright on most peoples computers.
People need to change the brightness to have healthier eyes.
</code></pre>

<p>Is not what I'm trying to solve, because you can use the previous sentence to set the context. Also this would be for lots of different words, not just the word <code>change</code>.</p>

<p>Appreciate the suggestions.</p>

<p><strong>Edit:</strong> I'm aware that various embedding models can help gain insight on this problem. If this is your answer, how do you interpret the word embedding that is returned? These arrays can be upwards of 500+ in length which isn't practical to interpret. </p>
","machine-learning, deep-learning, nlp, word-embedding, linguistics","<p>What you're trying to do is called <a href=""https://en.wikipedia.org/wiki/Word-sense_disambiguation"" rel=""nofollow noreferrer"">Word Sense Disambiguation</a>. It's been a subject of research for many years, and while probably not the most popular problem it <a href=""https://github.com/sebastianruder/NLP-progress/blob/master/english/word_sense_disambiguation.md"" rel=""nofollow noreferrer"">remains a topic of active research</a>. Even now, just picking the most common sense of a word is a strong baseline.</p>

<p>Word embeddings may be useful but their use is orthogonal to what you're trying to do here.</p>

<p>Here's a bit of example code from <a href=""https://github.com/alvations/pywsd"" rel=""nofollow noreferrer"">pywsd</a>, a Python library with implementations of some classical techniques:</p>

<pre><code>&gt;&gt;&gt; from pywsd.lesk import simple_lesk
&gt;&gt;&gt; sent = 'I went to the bank to deposit my money'
&gt;&gt;&gt; ambiguous = 'bank'
&gt;&gt;&gt; answer = simple_lesk(sent, ambiguous, pos='n')
&gt;&gt;&gt; print answer
Synset('depository_financial_institution.n.01')
&gt;&gt;&gt; print answer.definition()
'a financial institution that accepts deposits and channels the money into lending activities'
</code></pre>

<p>The methods are mostly kind of old and I can't speak for their quality but it's a good starting point at least.</p>

<p>Word senses are usually going to come from <a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a>.</p>
",2,3,827,2019-12-10 06:32:04,https://stackoverflow.com/questions/59261462/natural-language-processing-techniques-for-understanding-contextual-words
Memory efficiently loading of pretrained word embeddings from fasttext library with gensim,"<p>I would like to load pretrained multilingual word embeddings from the fasttext library with gensim; here the link to the embeddings:</p>

<p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>

<p>In particular, I would like to load the following word embeddings: </p>

<ul>
<li>cc.de.300.vec (4.4 GB) </li>
<li>cc.de.300.bin (7 GB)</li>
</ul>

<p>Gensim offers the following two options for loading fasttext files:</p>

<ol>
<li><p><code>gensim.models.fasttext.load_facebook_model(path, encoding='utf-8')</code>    </p>

<blockquote>
  <ul>
  <li><em>Load the input-hidden weight matrix from Facebook’s native fasttext
  .bin output file.</em></li>
  <li><em>load_facebook_model() loads the full model, not just
  word embeddings, and enables you to continue model training.</em></li>
  </ul>
</blockquote></li>
<li><p><code>gensim.models.fasttext.load_facebook_vectors(path, encoding='utf-8')</code></p>

<blockquote>
  <ul>
  <li><em>Load word embeddings from a model saved in Facebook’s native fasttext .bin format.</em></li>
  <li><em>load_facebook_vectors() loads the word embeddings only. Its faster, but does not enable you to continue training.</em></li>
  </ul>
</blockquote></li>
</ol>

<p>Source Gensim documentation: 
<a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>

<p>Since my laptop has only 8 GB RAM, I am continuing to get MemoryErrors or the loading takes a very long time (up to several minutes).</p>

<p>Is there an option to load these large models from disk more memory efficient?</p>
","python, nlp, gensim, word-embedding, fasttext","<p>As vectors will typically take at least as much addressable-memory as their on-disk storage, it will be challenging to load fully-functional versions of those vectors into a machine with only 8GB RAM. In particular:</p>

<ul>
<li><p>once you start doing the most common operation on such vectors – finding lists of the <code>most_similar()</code> words to a target word/vector – the gensim implementation will also want to cache a set of the word-vectors that's been normalized to unit-length – which nearly doubles the required memory</p></li>
<li><p>current versions of gensim's FastText support (through at least 3.8.1) also waste a bit of memory on some unnecessary allocations (especially in the full-model case)</p></li>
</ul>

<p>If you'll only be using the vectors, not doing further training, you'll definitely want to use only the <code>load_facebook_vectors()</code> option. </p>

<p>If you're willing to give up the model's ability to synthesize new vectors for out-of-vocabulary words, not seen during training, then you could choose to load just a subset of the full-word vectors from the plain-text <code>.vec</code> file. For example, to load just the 1st 500K vectors:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
KeyedVectors.load_word2vec_format('cc.de.300.vec', limit=500000)
</code></pre>

<p>Because such vectors are typically sorted to put the more-frequently-occurring words first, often discarding the long tail of low-frequency words isn't a big loss. </p>
",6,3,5903,2019-12-11 09:29:46,https://stackoverflow.com/questions/59282572/memory-efficiently-loading-of-pretrained-word-embeddings-from-fasttext-library-w
"Word2Vec - How to rid of &quot;TypeError: unhashable type: &#39;list&#39;&quot; and &quot;AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found&quot;?","<p>Getting <code>TypeError: unhashable type: 'list'</code> and <code>AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found</code> errors when I create my model based on <code>Word2Vec</code> implementation of the <code>gensim</code> module.</p>

<p><strong>Each entry has three parts</strong> which are presented within a list. And, <strong>the model contains three entries</strong> for the sake of demonstration.</p>

<p>Here is what I have tried:</p>

<pre><code>model = Word2Vec(sentences=features, size=100, sg=1, window=3, min_count=1, iter=10, workers=Pool()._processes)

model.build_vocab(features)

model.train(features)
</code></pre>

<p>The value of the <code>features</code> is: </p>

<pre><code>  [
    [
      ['permission.ACCESS_WIFI_STATE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.CHANGE_WIFI_STATE'],
       ['intent.action.MAIN', 'intent.action.BATTERY_CHANGED_ACTION', 'intent.action.SIG_STR', 'intent.action.BOOT_COMPLETED'],
       []
    ],
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.INSTALL_PACKAGES', 'permission.SEND_SMS', 'permission.DELETE_PACKAGES'],
      ['intent.action.BOOT_COMPLETED', 'intent.action.USER_PRESENT', 'intent.action.PHONE_STATE', 'intent.action.MAIN'],
      []
    ], 
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_FINE_LOCATION', 'permission.INTERNET', 'permission.READ_PHONE_STATE', 'permission.ACCESS_COARSE_LOCATION', 'permission.CALL_PHONE', 'permission.READ_CONTACTS', 'permission.READ_SMS'], 
      ['intent.action.PHONE_STATE', 'intent.action.MAIN'], 
      []
    ]
  ]
</code></pre>

<p><strong>Edit:</strong> The error stack trace after correcting the form of the feature vector according to the comment of @gojomo.</p>

<pre><code>Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 3 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 2 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 1 more threads
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 0 more threads
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - EPOCH - 10 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - training on a 60 raw words (2 effective words) took 0.1s, 21 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:WARNING - under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
2019-12-13 12:24:34,521:gensim.utils:INFO - saving Word2Vec object under model/word2vec_model, separately None
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
    debugger.enable_tracing(apply_to_all_threads=True)  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
      File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
2019-12-13 12:24:34,521:gensim.utils:INFO - not storing attribute vectors_norm
        func = self.__getitem__(name)func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__

2019-12-13 12:24:34,522:gensim.utils:INFO - not storing attribute cum_table
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
    func = self._FuncPtr((name_or_ordinal, self))AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found

AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found
func = self._FuncPtr((name_or_ordinal, self))
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
AttributeError: dlsym(0x7fed18d2ff70, AttachDebuggerTracing): symbol not found
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,524:gensim.utils:INFO - saved model/word2vec_model
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
        pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
result = lib.AttachDebuggerTracing(  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18b09320, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
</code></pre>
","gensim, word2vec, word-embedding","<p>Gensim's <code>Word2Vec</code> expects its corpus <code>sentences</code> to be a <em>sequence</em> where each individual item is a <em>list of string tokens</em>. (That is, those string tokens are words.)</p>

<p>Instead, you have a list (which is acceptable as a sequence), where each of its items is a list (which is also acceptable), but then each of those lists instead has as each item yet another <em>list</em> – when for <code>Word2Vec</code> training, each of those items should be a string token (word). </p>

<p>(I've edited your example data to be be structurally-indented, to make the levels of nesting clearer.)</p>

<p>If those innermost lists-of-strings are your real individual ""sentences"", you need to be sure they're the items in your outermost list. </p>

<p>(If, on the other hand, you really want a cluster like <code>['intent.action.PHONE_STATE', 'intent.action.MAIN']</code> to be a single ""word"" in your model, you'll want to change that list into a single string token, so it can look like a word – and thus hashable key – to <code>Word2Vec</code> and Python.)</p>
",2,0,1404,2019-12-12 20:08:37,https://stackoverflow.com/questions/59312001/word2vec-how-to-rid-of-typeerror-unhashable-type-list-and-attributeerro
AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;endswith&#39;,"<p>When I run my .py file containing the following code </p>

<pre><code>if not os.path.exists('model_out'):
    model1 = gensim.models.Word2Vec(l, min_count = 1, size = 100, window = 5)
    model1.save('model_out')
model1.load('model_out')
model11 = gensim.models.keyedvectors.KeyedVectors.load(model1)
max_size = len(model.wv.vocab)-1
</code></pre>

<p>The following error is generated</p>

<blockquote>
  <p>Traceback (most recent call last):   File ""assignment.py"", line 35, in
  
      model11 = gensim.models.keyedvectors.KeyedVectors.load(model1)   File
  ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/models/keyedvectors.py"",
  line 1540, in load
      model = super(WordEmbeddingsKeyedVectors, cls).load(fname_or_handle, **kwargs)   File
  ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/models/keyedvectors.py"",
  line 228, in load
      return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)   File ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/utils.py"",
  line 424, in load
      compress, subname = SaveLoad._adapt_by_suffix(fname)   File ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/utils.py"",
  line 513, in _adapt_by_suffix
      compress, suffix = (True, 'npz') if fname.endswith('.gz') or fname.endswith('.bz2') else (False, 'npy') AttributeError: 'Word2Vec'
  object has no attribute 'endswith'</p>
</blockquote>
","python, machine-learning, nlp, artificial-intelligence, word-embedding","<p>I think some functions might be deprecated. Try</p>

<pre><code>from gensim import models
w = models.KeyedVectors.load_word2vec_format('model', binary=True)
</code></pre>

<p>Or</p>

<pre><code>from gensim import models
w = models.Word2Vec.load_word2vec_format('model', binary=True)
</code></pre>
",3,2,3791,2019-12-14 07:28:08,https://stackoverflow.com/questions/59333165/attributeerror-word2vec-object-has-no-attribute-endswith
"Extract Keras concatenated layer of 3 embedding layers, but it&#39;s an empty list","<p>I am constructing a Keras Classification model with Multiple Inputs (3 actually) to predict one single output. Specifically, my 3 <strong>inputs</strong> are:</p>

<ol>
<li>Actors</li>
<li>Plot Summary</li>
<li>Relevant Movie Features</li>
</ol>

<p><strong>Output:</strong></p>

<ol>
<li>Genre tags</li>
</ol>

<p><strong>Python Code (create the multiple input keras)</strong></p>

<pre><code>def kera_multy_classification_model():

    sentenceLength_actors = 15
    vocab_size_frequent_words_actors = 20001

    sentenceLength_plot = 23
    vocab_size_frequent_words_plot = 17501

    sentenceLength_features = 69
    vocab_size_frequent_words_features = 20001

    model = keras.Sequential(name='Multy-Input Keras Classification model')

    actors = keras.Input(shape=(sentenceLength_actors,), name='actors_input')
    plot = keras.Input(shape=(sentenceLength_plot,), name='plot_input')
    features = keras.Input(shape=(sentenceLength_features,), name='features_input')

    emb1 = layers.Embedding(input_dim = vocab_size_frequent_words_actors + 1,
                            # based on keras documentation input_dim: int &gt; 0. Size of the vocabulary, i.e. maximum integer index + 1.
                            output_dim = Keras_Configurations_model1.EMB_DIMENSIONS,
                            # int &gt;= 0. Dimension of the dense embedding
                            embeddings_initializer = 'uniform', 
                            # Initializer for the embeddings matrix.
                            mask_zero = False,
                            input_length = sentenceLength_actors,
                            name=""actors_embedding_layer"")(actors)
    encoded_layer1 = layers.LSTM(100)(emb1)

    emb2 = layers.Embedding(input_dim = vocab_size_frequent_words_plot + 1,
                            output_dim = Keras_Configurations_model2.EMB_DIMENSIONS,
                            embeddings_initializer = 'uniform',
                            mask_zero = False,
                            input_length = sentenceLength_plot,
                            name=""plot_embedding_layer"")(plot)
    encoded_layer2 = layers.LSTM(100)(emb2)

    emb3 = layers.Embedding(input_dim = vocab_size_frequent_words_features + 1,
                            output_dim = Keras_Configurations_model3.EMB_DIMENSIONS,
                            embeddings_initializer = 'uniform',
                            mask_zero = False,
                            input_length = sentenceLength_features,
                            name=""features_embedding_layer"")(features)
    encoded_layer3 = layers.LSTM(100)(emb3)

    merged = layers.concatenate([encoded_layer1, encoded_layer2, encoded_layer3])

    layer_1 = layers.Dense(Keras_Configurations_model1.BATCH_SIZE, activation='relu')(merged)

    output_layer = layers.Dense(Keras_Configurations_model1.TARGET_LABELS, activation='softmax')(layer_1)

    model = keras.Model(inputs=[actors, plot, features], outputs=output_layer)

    print(model.output_shape)

    print(model.summary())

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['sparse_categorical_accuracy'])
</code></pre>

<p><strong>Model's Structure</strong></p>

<p><a href=""https://i.sstatic.net/9wfri.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9wfri.png"" alt=""enter image description here""></a></p>

<p><strong>My problem:</strong></p>

<p>After successfully fitting and training the model on some training data, I would like to extract the embeddings of this model for later use. My main approach before using a multiple input keras model, was to train 3 different keras models and extract 3 different embedding layers of shape 100. Now that I have the multiple input keras model, <strong>I want to extract the concatenated embedding layer</strong> with output shape (None, 300).</p>

<p>Although, when I try to use this python command:</p>

<pre><code>embeddings = model_4.layers[9].get_weights()
print(embeddings)
</code></pre>

<p>or </p>

<pre><code>embeddings = model_4.layers[9].get_weights()[0]
print(embeddings)
</code></pre>

<p>I get either an empty list (1st code sample) either an <em>IndenError: list index out of range</em> (2nd code sample).</p>

<p>Thank you in advance for any advice or help on this matter. Feel free to ask on the comments any additional information that I may have missed, to make this question more complete.</p>

<p><em>Note: Python code and model's structure have been also presented to this previously answered <a href=""https://stackoverflow.com/questions/59489625/model-fit-keras-classification-multiple-inputs-single-output-gives-error-attr"">question</a></em></p>
","python, tensorflow, keras, nlp, word-embedding","<p>Concatenate layer does not have any weights (it does not have trainable parameter as you ca see from your model summary) hence your <code>get_weights()</code> output is coming empty. Concatenation is an operation.
<br>
For your case you can get weights of your individual embedding layers after training.</p>

<pre><code>model.layers[3].get_weights() # similarly for layer 4 and 5
</code></pre>

<p>Alternatively if you want to store your embedding in (None, 300) you can use numpy to concatenate weights.<br></p>

<pre><code>out_concat = np.concatenate([mdoel.layers[3].get_weights()[0], mdoel.layers[4].get_weights()[0], mdoel.layers[5].get_weights()[0]], axis=-1)
</code></pre>

<p>Although you can get output tensor of concatenate layer:</p>

<pre><code>out_tensor = model.layers[9].output
# &lt;tf.Tensor 'concatenate_3_1/concat:0' shape=(?, 300) dtype=float32&gt;
</code></pre>
",1,1,978,2019-12-29 17:02:47,https://stackoverflow.com/questions/59521480/extract-keras-concatenated-layer-of-3-embedding-layers-but-its-an-empty-list
"People name embedding from name, commas and spaces to keys","<p>I'm trying to figure out a good algorithm for embedding name as such.<br>
space = 0, word = 1, comma = 2, double quotations = 3  </p>

<p>So ""Bob Dylan"" should embed as ""101""
While ""Brown, Millie Bobby"" should embed as ""120101""<br>
and ""Dwayne ""The Rock"" Johnson"" should embed as ""103101301""</p>
","regex, string, word-embedding","<p>I would suggest a very simple solution:</p>

<ul>
<li>Search for all the words using <code>\w+</code> and replace them with <code>1</code>.</li>
<li>Then for spaces <code>\s</code> and replace it with <code>0</code>.</li>
<li>Comma <code>,</code> and replace it with <code>2</code>.</li>
<li>And eventually double quote <code>""</code> with <code>3</code>.</li>
</ul>
",1,0,95,2019-12-30 10:17:48,https://stackoverflow.com/questions/59529160/people-name-embedding-from-name-commas-and-spaces-to-keys
Keras word embedding matrix has first row of zeros,"<p>I am looking at the Keras Glove word embedding example and it is not clear why the first row of the embedding matrix is populated with zeros.</p>

<p>First, the embedding index is created where words are associated with arrays.</p>

<pre><code>embeddings_index = {}
with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, 'f', sep=' ')
        embeddings_index[word] = coefs
</code></pre>

<p>Then the embedding matrix is created by looking at words from the index created by tokenizer. </p>

<pre><code># prepare embedding matrix
num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if i &gt;= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
</code></pre>

<p>Since the loop will start with <code>i=1</code>, then the first row will contain only zeros and random numbers if the matrix is initialized differently. Is there a reason for skipping the first row?</p>
","keras, word-embedding, glove","<p>The whole started from the fact that the <code>Tokenizer</code>'s programmers reserved the index <code>0</code> for some reason, maybe for some compatibility (some other languages use indexing from <code>1</code>) or coding technic reasons.</p>

<p>However they use numpy, where they want to indexing with the simple:</p>

<pre><code>embedding_matrix[i] = embedding_vector
</code></pre>

<p>indexing, so the <code>[0]</code> indexed row stays full of zeros and there is no case where, as wrote <em>""random numbers if the matrix is initialized differently""</em>, because this array has been initialized with <strong>zeros</strong>. 
So from this line we don't need the first row at all, but you can't delete it as the numpy array would lost the aligning its indexing with the tokenizer's indexing.</p>
",1,1,443,2019-12-30 15:43:43,https://stackoverflow.com/questions/59533346/keras-word-embedding-matrix-has-first-row-of-zeros
where can i download a pretrained word2vec map?,"<p>I have been learning about NLP models and came across word embedding, and saw the examples in which it is possible to see relations between words by calculating their dot products and such.</p>

<p>What I am looking for is just a dictionary, mapping words to their representative vectors, so I can play around with it. I know that I can build a model and train it and create my own map but I just want the already trained map as a python variable. </p>
","python, nlp, word2vec, word-embedding","<p>You can try out Google's <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">word2vec</a> model trained with about 100 billion words from various news articles.</p>
<p>An interesting fact about word vectors, <code>w2v(king) - w2v(man) + w2v(woman) ≈ w2v(queen)</code></p>
",9,10,12121,2020-01-04 13:10:19,https://stackoverflow.com/questions/59590993/where-can-i-download-a-pretrained-word2vec-map
What is the difference between Sentence Encodings and Contextualized Word Embeddings?,"<p>I have seen both terms used while reading papers about BERT and ELMo so I wonder if there is a difference between them.</p>
","nlp, word-embedding, elmo, bert-language-model","<ul>
<li>A <strong>contextualized word embeding</strong> is a vector representing a word in a special context. The <em>traditional word embeddings</em> such as Word2Vec and GloVe generate one vector for each word, whereas a contextualized word embedding generates a vector for a word depending on the context. Consider the sentences <code>The duck is swimming</code>and <code>You shall duck when someone shoots at you</code>. With traditional word embeddings, the word vector for <code>duck</code>would be the same in both sentences, whereas it should be a different one in the contextualized case. </li>
<li>While word embeddings encode words into a vector representation, there is also the question on how to represent a whole sentence in a way a computer can easily work with. These <strong>sentence encodings</strong> can embedd a whole sentence as one vector , doc2vec for example which generate a vector for a sentence. But also BERT generates a representation for the whole sentence, the [CLS]-token. </li>
</ul>

<p>So in short, a conextualized word embedding represents a word in a context, whereas a sentence encoding represents a whole sentence.</p>
",21,13,8889,2020-01-23 11:20:54,https://stackoverflow.com/questions/59877385/what-is-the-difference-between-sentence-encodings-and-contextualized-word-embedd
Word2Vec - How can I store and retrieve extra information regarding each instance of corpus?,"<p>I need to combine Word2Vec with my <code>CNN</code> model. To this end, I need to persist a flag (a binary one is enough) for each sentence as my corpus has two types (<em>a.k.a.</em> target classes) of sentences. So, I need to retrieve this flag of each vector after creation. How can I store and retrieve this information inside the input sentences of <code>Word2Vec</code> as I need both of them in order to train my deep neural network?</p>

<p>p.s. I'm using <code>Gensim</code> implementation of <code>Word2Vec</code>.</p>

<p>p.s. My corpus has <strong>6,925</strong> sentences, and <code>Word2Vec</code> produces <strong>5,260</strong> vectors.</p>

<p><strong>Edit:</strong> More detail regarding my corpus (as requested):</p>

<p>The structure of the corpus is as follows:</p>

<ol>
<li><p>sentences (label: <code>positive</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
<li><p>sentences (label: <code>negative</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
</ol>

<p>Then all the sentences were given as the input to <code>Word2Vec</code>.</p>

<pre><code>word2vec = Word2Vec(all_sentences, min_count=1)
</code></pre>

<p>I'll feed my CNN with the extracted features (which is the <code>vocabulary</code> in this case) and the <code>targets</code> of sentences. So, I need these labels of the sentences as well.</p>
","deep-learning, gensim, word2vec, one-hot-encoding, word-embedding","<p>Because the <code>Word2Vec</code> model doesn't retain any representation of the individual training texts, this is entirely a matter for you in your own Python code. </p>

<p>That doesn't seem like very much data. (It's rather tiny for typical <code>Word2Vec</code> purposes to have just a 5,260-word final vocabulary.) </p>

<p>Unless each text (aka 'sentence') is very long, you could even just use a Python dict where each key is the full string of a sentence, and the value is your flag. </p>

<p>But if, as is likely, your source data has some other unique identifier per text – like a unique database key, or even a line/row number in the canonical representation – you should use that identifier as a key instead. </p>

<p>In fact, if there's a canonical source ordering of your 6,925 texts, you could just have a list <code>flags</code> with 6,925 elements, in order, where each element is your flag. When you need to know the status of a text from position <code>n</code>, you just look at <code>flags[n]</code>. </p>

<p>(To make more specific suggestions, you'd need to add more details about the original source of the data, and exactly when/why you'd need to be checking this extra property later.)</p>
",0,2,443,2020-01-27 00:56:17,https://stackoverflow.com/questions/59924168/word2vec-how-can-i-store-and-retrieve-extra-information-regarding-each-instanc
What does each element in an embedding mean?,"<p>I've been working with facial embeddings but I think Word2Vec is a more common example.</p>

<p>Each entry in that matrix is a number that came from some prediction program/algorithm, but what are they? Are they learned features?</p>
","python, word2vec, feature-extraction, embedding, word-embedding","<p>Those numbers are learned vectors that each represents a dimension that best separates each word from each other, given some limiting number of dimensions (normally ~200). So if one group of words tends to appear in the same context, then they'd likely share a similar score on one or more dimensions. </p>

<p>For example, words like North, South, East, West are likely to be very close since they are interchangeable in many contexts. </p>

<p>The dimensions are chosen by algorithm to maximize the variance they encode, and what they mean is not necessarily something we can talk about in words. But imagine a bag of fridge-magnets each representing a letter of the alphabet - if you shine a light on them so as to cast a shadow, there will be some orientations of the letters that yield more discriminatory information in the shadows than for other orientations. </p>

<p>The dimensions in a word-embedding represent the best ""orientations"" that give light to the most discriminatory ""shadows"". Sometimes these dimensions might approximate things we recognise as having direct meaning, but very often, they wont.</p>

<p>That being said, if you collect words that do have similar functions, and find the vectors from those words to other words that are the endpoint of some kind of fixed relationship - say England, France, Germany as one set of words consisting of Countries, and London, Paris, Berlin as another set of words consisting of the respective Capital-Cities, you will find that the <em>relative</em> vectors between each country and its capital are often very, very similar in both direction and magnitude. </p>

<p>This has an application for search because you can start with a new word location, say ""Argentina"" and by looking in the location arrived at by applying the relative ""has_capital_city"" vector, you <em>should</em> arrive at the word ""Buenos Aires"".</p>

<p>So the raw dimensions probably have little meaning of their own, but by performing these A is to B as X is to Y comparisons, it is possible to derive relative vectors that do have a meaning of sorts. </p>
",4,0,508,2020-02-05 13:05:55,https://stackoverflow.com/questions/60076497/what-does-each-element-in-an-embedding-mean
Why aren&#39;t all bigrams created in gensim&#39;s `Phrases` tool?,"<p>I have created a bigram model using gensim and the try to get the bigram sentences but it's not picking all bigram sentences why?</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.phrases import Phrases, Phraser
phrases = Phrases(sentences, min_count=1, threshold=1)
bigram_model = Phraser(phrases)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram_model[sent])
[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>
<p>Can anyone explain how to get all bigrams.</p>
<h1>Why only 'new_york' not 'the_mayor' and others?</h1>
","python, nlp, gensim, n-gram, word-embedding","<p>The <code>Phrases</code> algorithm decides which word-pairs to promote to bigrams by a statistical analysis, which compares the base frequencies of each word individually with their frequency together. </p>

<p>So, some word-pairs will pass this test, and be combined, and others won't. If you're not getting the pairings you expect, then you can tune the algorithm somewhat using the <code>Phrases</code> class options, including <code>threshold</code>, <code>min_count</code>, and at least one alternate scoring-mechanism. </p>

<p>But, even maximally tuned, it won't typically create all the phrases that we, as natural-language speakers, would perceive – as it knows nothing of grammar, or the actually logically-related entities of the world. It only knows frequency statistics in the training text. </p>

<p>So there will be pairings it misses we'd see as natural &amp; desirable, and pairings it creates we'd see as illogical. Still, even with these unaesthetic pairings – creating text that doesn't look right to people – the transformed text can often work better in certain downstream classification or information-retrieval tasks. </p>

<p>If you really just wanted all possible bigrams, that'd be a much more simple text transformation, not requiring the multiple-passes &amp; internal statistics-collection of gensim's <code>Phrases</code>. </p>

<p>But also, if you do want to use gensim's <code>Phrases</code> technique, it will only perform well when it has a lot of training data.  Toy-sized texts of just a few dozen words –or even many tens-of-thousands of words – won't give good results. You'd want millions to tens-of-millions of training words to have some chance of it really detecting statistically-valid word-pairings.</p>
",2,1,1289,2020-02-07 07:30:54,https://stackoverflow.com/questions/60108919/why-arent-all-bigrams-created-in-gensims-phrases-tool
how to concatenate pre trained embedding layer and Input layer,"<pre><code>normal_input = Input(shape=(56,))

pretrained_embeddings = Embedding(num_words, 200, input_length=max_length, trainable=False,
                                                            weights=[ft_embedding_matrix])

concatenated = concatenate([normal_input, pretrained_embeddings])

dense = Dense(256, activation='relu')(concatenated)
</code></pre>

<p>My idea was to create an input with 256 dimension and pass it to a dense layer.</p>

<p>I got the following error.</p>

<p><strong>ValueError</strong>: Layer concatenate_10 was called with an input that isn't a symbolic tensor. Received type: . Full input: [, ]. All inputs to the layer should be tensors.</p>

<p>Please help me how to do this.</p>
","python, tensorflow, keras, concatenation, word-embedding","<p>You need an input to select which embedding you're using. </p>

<p>Since you're using 150 words, your embeddings will have shape <code>(batch,150,200)</code>, which is not possible to concatenate with <code>(batch, 56)</code> in any way. You need to transform something somehow to match the shapes. I suggest you try a <code>Dense</code> layer to transform 56 into 200... </p>

<pre><code>word_input = Input((150,))
normal_input = Input((56,))

embedding = pretrained_embeddings(word_input)
normal = Dense(200)(normal_input)

#you could add some normalization here - read below

normal = Reshape((1,200))(normal)
concatenated = Concatenate(axis=1)([normal, embedding]) 
</code></pre>

<p>I also suggest, since embeddings and your inputs are from different natures, that you apply a normalization so they become more similar:</p>

<pre><code>embedding = BatchNormalization(center=False, scale=False)(embedding)
normal = BatchNormalization(center=False, scale=False)(normal)
</code></pre>

<hr>

<p>Another possibility (I can't say which is best) is to concatenate in the other dimension, transforming the 56 into 150 instead:    </p>

<pre><code>word_input = Input((150,))
normal_input = Input((56,))

embedding = pretrained_embeddings(word_input)
normal = Dense(150)(normal_input)

#you could add some normalization here - read below

normal = Reshape((150,1))(normal)
concatenated = Concatenate(axis=-1)([normal, embedding]) 
</code></pre>

<p>I believe this is more suited to recurrent and convolutional networks, you add a new channel instead of adding a new step. </p>

<hr>

<p>You could even try a double concatenation, which sounds cool :D </p>

<pre><code>word_input = Input((150,))
normal_input = Input((56,))

embedding = pretrained_embeddings(word_input)
normal150 = Dense(150)(normal_input)
normal201 = Dense(201)(normal_input)

embedding = BatchNormalization(center=False, scale=False)(embedding)
normal150 = BatchNormalization(center=False, scale=False)(normal150)
normal201 = BatchNormalization(center=False, scale=False)(normal201)


normal150 = Reshape((150,1))(normal150)
normal201 = Reshape((1,201))(normal201)
concatenated = Concatenate(axis=-1)([normal150, embedding]) 
concatenated = Concatenate(axis= 1)([normal201, concatenated])
</code></pre>
",1,1,1440,2020-02-13 13:49:25,https://stackoverflow.com/questions/60209407/how-to-concatenate-pre-trained-embedding-layer-and-input-layer
Embedding Layer in Keras: Vocab Size +1,"<p>From a number of examples I have seen, when we use <code>text_tokenizer</code> from keras, when specifying the input size for the input layer, we use vocab size +1.  This naturally yields an embedding space with +1 'rows'.  </p>

<p>For example, I fit a simple model to estimate the embedding vectors for a vocab of size 3 = <code>I like turtles</code>.  The embedding space has length 5 per word in our vocabulary.</p>

<p>The embedding weights are:</p>

<pre><code>0.01209533  0.034303080 -0.04666784 0.02803965  -0.03691160
-0.01302978 -0.030584216    -0.02506201 0.04771456  0.01906699
0.02800793  0.042204402 0.05223191  -0.01184921 0.02000498
0.02692273  -0.008792922    0.01560913  -0.02783649 0.02692282

</code></pre>

<p>My question:  I assume that the first ""row"" in our matrix is the 0-based vector, such that rows 2, 3, and 4 would be associated with ""I"", ""like"", and ""turtles"" respectively.   </p>

<p>Is this the case?  I want to ensure that I align my vocabulary properly, but I haven't been able to pin down any documentation to confirm this assumption.</p>
","r, keras, tensorflow2.0, word-embedding","<p>I understand that you are wanting to extract the embedding for each word, but I think the real question is: What is the output the tokenizer is producing.</p>

<p>Also, that tokenizer is a bit of a mess. You'll see what I mean below.</p>

<p>Because the tokenizer will filter words (assuming a non-trivial vocabulary), I don't want to assume that the words are stored in the order in which they are found. So here I programmatically determine the vocabulary using <code>word_index</code>. I then explicitly check what words are tokenized <em>after</em> filtering for the most frequently used words. (Word_index remembers <strong>all</strong> words; i.e. the pre-filtered values.)</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
corpus = 'I like turtles'
num_words = len(corpus.split())
oov = 'OOV'
tokenizer = Tokenizer(num_words=num_words + 2, oov_token=oov)
tokenizer.fit_on_texts(corpus.split())
print(f'word_index: {tokenizer.word_index}')
print(f'vocabulary: {tokenizer.word_index.keys()}')
text = [key for key in tokenizer.word_index.keys()]
print(f'keys: {text}: {tokenizer.texts_to_sequences(text)}')

text = 'I like turtles'.split()
print(f'{text}: {tokenizer.texts_to_sequences(text)}')

text = 'I like marshmallows'.split() 
print(f'{text}: {tokenizer.texts_to_sequences(text)}')
</code></pre>

<p>This produces the following output:</p>

<pre><code>word_index: {'OOV': 1, 'i': 2, 'like': 3, 'turtles': 4}
vocabulary: dict_keys(['OOV', 'i', 'like', 'turtles'])
keys: ['OOV', 'i', 'like', 'turtles']: [[1], [2], [3], [4]]
['I', 'like', 'turtles']: [[2], [3], [4]]
['I', 'like', 'marshmallows']: [[2], [3], [1]]
</code></pre>

<p>However, if you specify oov_token, the output looks like this:</p>

<pre><code>{'OOV': 1, 'i': 2, 'like': 3, 'turtles': 4}
</code></pre>

<p>Notice how I had to specify <code>num_words=num_words + 2</code> instead of the expected '+1'.
That's because we're explicitly defining an OOV token, which gets added to the vocabulary, which is a bit nuts imo.</p>

<p>If you specify an OOV token and you set <code>num_words=num_words + 1</code> (as documented), then 'I like turtles' gets the same encoding as 'I like marshmallows'. Also nuts.</p>

<p>Hopefully, you now have to tools to know what the tokenizer is feeding the encoding layer. Then hopefully, it'll be trivial to correlate the tokens with their embeddings.</p>

<p>Please let us know what you find. :) </p>

<p>(For more on the madness, check out <a href=""https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work"">this</a> StackOverflow post.)</p>
",4,4,3502,2020-02-18 22:57:03,https://stackoverflow.com/questions/60290640/embedding-layer-in-keras-vocab-size-1
No module named &#39;gensim&#39; but already installed it,"<p>i'm having this error problem, i have ran this script in jupyter notebook in base (root) environment, the log said that gensim library has been installed and i have run the command <strong>!pip install gensim</strong> before i import it, but it still can not be imported, and the error said <strong>ModuleNotFoundError: No module named 'gensim'</strong></p>

<pre><code>!pip install gensim
import gensim
from gensim.models import KeyedVectors
model = KeyedVectors.load('model_fasttext2.vec')
model.vector_size
------------------------------------------------------------------------
Requirement already satisfied: gensim in c:\users\ip-03\anaconda3\lib\site-packages (3.8.1)
Requirement already satisfied: scipy&gt;=0.18.1 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.4.1)
Requirement already satisfied: six&gt;=1.5.0 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.14.0)
Requirement already satisfied: smart-open&gt;=1.8.1 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.9.0)
Requirement already satisfied: numpy&gt;=1.11.3 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.18.1)
Requirement already satisfied: boto&gt;=2.32 in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (2.49.0)
Requirement already satisfied: boto3 in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (1.12.3)
Requirement already satisfied: bz2file in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (0.98)
Requirement already satisfied: requests in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (2.22.0)
Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.3.3)
Requirement already satisfied: botocore&lt;1.16.0,&gt;=1.15.3 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (1.15.3)
Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.9.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2019.11.28)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (1.25.8)
Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2.8)
Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in c:\users\ip-03\anaconda3\lib\site-packages (from botocore&lt;1.16.0,&gt;=1.15.3-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2.8.1)
Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in c:\users\ip-03\anaconda3\lib\site-packages (from botocore&lt;1.16.0,&gt;=1.15.3-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.15.2)
</code></pre>

<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-10-ee4a48d372cd&gt; in &lt;module&gt;
      1 get_ipython().system('pip install gensim')
----&gt; 2 import gensim
      3 from gensim.models import KeyedVectors
      4 model = KeyedVectors.load('model_fasttext2.vec')
      5 model.vector_size

ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Is there anyone who can help this problem? i will really appreciate your help, it will help my thesis work, thank you for your attention </p>
","python, machine-learning, jupyter-notebook, gensim, word-embedding","<p>It may be that your jupyter lab maybe running the base kernel and not the kernel of the virtual environment.</p>

<p>Check by doing the following:</p>

<pre class=""lang-py prettyprint-override""><code>import sys
sys.executable
</code></pre>

<p>into my notebook and got the result</p>

<pre><code>'/anaconda3/bin/python'
</code></pre>

<p>If you get the above instead of the below then that means you're using the wrong kernel.</p>

<pre><code>'/anaconda3/envs/myenv/bin/python'
</code></pre>

<p>You can solve it by creating a new iPython kernel for your new environment. Read more <a href=""https://ipython.readthedocs.io/en/stable/install/kernel_install.html#kernels-for-different-environments"" rel=""nofollow noreferrer"">here</a>.</p>

<pre class=""lang-sh prettyprint-override""><code>conda install -n myenv ipython
conda activate myenv
python -m ipykernel install --user --name myenv --display-name ""Python (myenv)""
```Then, to run Jupyter Lab in the new environment:
</code></pre>
",1,0,4430,2020-02-20 11:05:14,https://stackoverflow.com/questions/60318511/no-module-named-gensim-but-already-installed-it
Starspace: What is the interpretation of the labelDoc fileFormat?,"<p>The starspace documentation is unclear on the parameter 'fileFormat' which takes the value 'labelDoc' or 'fastText'.
I would like to understand intuitively what material difference setting this paramter would have.</p>

<p>Currently, my best guess is that if you set fileFormat to 'fastText' then all tokens in the training file that do not have the prefix '__label__' will be broken down into character-level n-grams as in fastText.
Alternatively, if you set fileFormat to 'labelDoc' then starspace will assume that all tokens are actually labels, and you do not need to prepend '__label__' to the tokens, because they will be recognized as labels anyway.</p>

<p>Is my thinking correct?</p>
","facebook, nlp, word-embedding","<p>The way StarSpace uses the labels highly depends on the trainMode you are using. The <em>labelDoc</em> format is useful when you go for a trainMode that just relies on labels (trainMode 1 through 4) where it may be the same thing to use a <em>fastText</em> format specifying the <code>__label__</code> prefix but some trainModes benefit from <em>labelDoc</em> format (i.e. trainMode 1 or 3) to use a whole sentence as a label element for that trainMode.</p>

<p>So to clarify that, if you are performing a text classification task(as explained in <a href=""https://github.com/facebookresearch/StarSpace#tagspace-word--tag-embeddings"" rel=""nofollow noreferrer"">this example</a> <em>labelDoc</em> wouldn't have any input recognized but on the other hand, as you stated, using <em>fastText</em> format will breakdown all non-labeled text as input and learn to predict the <code>__label__</code> tags.</p>

<p>And an example for <em>labelDoc</em> format would be developing a content based recommender system (as explained in <a href=""https://github.com/facebookresearch/StarSpace#docspace-document-recommendation"" rel=""nofollow noreferrer"">this example</a>) every tab separated sentence is used at LHS or RHS during training time. But if you go on a collaborative approach (the content of the articles or wherever you sentences come from is not taken in account) it can be trained either with <em>fastText</em> (specifying the <code>__label__</code> prefix) or <em>labelDoc</em> file format as labels are picked randomly during training time for LHS or RHS. (This second example is explained <a href=""https://github.com/facebookresearch/StarSpace#pagespace-user--page-embeddings"" rel=""nofollow noreferrer"">here</a>).</p>
",1,1,148,2020-03-05 01:55:10,https://stackoverflow.com/questions/60537187/starspace-what-is-the-interpretation-of-the-labeldoc-fileformat
"Invalid argument: indices[0,0] = -4 is not in [0, 40405)","<p>I have a model that was kinda working on some data. I've added in some tokenized word data in the dataset (somewhat truncated for brevity):</p>

<pre><code>vocab_size = len(tokenizer.word_index) + 1
comment_texts = df.comment_text.values

tokenizer = Tokenizer(num_words=num_words)

tokenizer.fit_on_texts(comment_texts)
comment_seq = tokenizer.texts_to_sequences(comment_texts)
maxtrainlen = max_length(comment_seq)
comment_train = pad_sequences(comment_seq, maxlen=maxtrainlen, padding='post')
vocab_size = len(tokenizer.word_index) + 1

df.comment_text = comment_train

x = df.drop('label', 1) # the thing I'm training

labels = df['label'].values  # Also known as Y

x_train, x_test, y_train, y_test = train_test_split(
    x, labels, test_size=0.2, random_state=1337)        

n_cols = x_train.shape[1]

embedding_dim = 100  # TODO: why?

model = Sequential([
            Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_shape=(n_cols,)),
            LSTM(32),
            Dense(32, activation='relu'),
            Dense(512, activation='relu'),
            Dense(12, activation='softmax'),  # for an unknown type, we don't account for that while training
        ])
model.summary()

model.compile(optimizer='rmsprop',
                      loss='categorical_crossentropy',
                      metrics=['acc'])

# convert the y_train to a one hot encoded variable
encoder = LabelEncoder()
encoder.fit(labels)  # fit on all the labels
encoded_Y = encoder.transform(y_train)  # encode on y_train
one_hot_y = np_utils.to_categorical(encoded_Y)

model.fit(x_train, one_hot_y, epochs=10, batch_size=16)

</code></pre>

<p>Now, I get this error:</p>

<pre><code>Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 12, 100)           4040500   
_________________________________________________________________
lstm (LSTM)                  (None, 32)                17024     
_________________________________________________________________
dense (Dense)                (None, 32)                1056      
_________________________________________________________________
dense_1 (Dense)              (None, 512)               16896     
_________________________________________________________________
dense_2 (Dense)              (None, 12)                6156      
=================================================================
Total params: 4,081,632
Trainable params: 4,081,632
Non-trainable params: 0
_________________________________________________________________
Train on 4702 samples
Epoch 1/10
2020-03-04 22:37:59.499238: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: indices[0,0] = -4 is not in [0, 40405)
</code></pre>

<p>I think this must be coming from my comment_text column since that is the only thing I added.</p>

<p>Here is what comment_text looks like before I make the substitution:
<a href=""https://i.sstatic.net/9jMRA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9jMRA.png"" alt=""before""></a></p>

<p>And here is after:
<a href=""https://i.sstatic.net/5APNW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5APNW.png"" alt=""after""></a></p>

<p>My full code (before I made the change) is here:
<a href=""https://colab.research.google.com/drive/1y8Lhxa_DROZg0at3VR98fi5WCcunUhyc#scrollTo=hpEoqR4ne9TO"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1y8Lhxa_DROZg0at3VR98fi5WCcunUhyc#scrollTo=hpEoqR4ne9TO</a></p>
","keras, word-embedding, implementation","<p>You should be training with <code>comment_train</code>, not with <code>x</code> which is taking whatever is in the unknown <code>df</code>.    </p>

<p>The <code>embedding_dim=100</code> is free to choose. It's like the number of units in a hidden layer. You can tune this parameter to find which is best for your model as well as you can tune the number of units in hidden layers. </p>

<hr>

<p>In your case, you will need a model with two or more inputs:</p>

<ul>
<li>One input for the comments, passing through the embedding and processing text    </li>
<li>Another input for the rest of the data, passing probably through a standard netork.</li>
</ul>

<p>At some point you will concatenate these two branches and keep on going.    </p>

<p>This link has a good tutorial about the <strong>functional API</strong> models and shows a model that has two text inputs and an extra input: <a href=""https://www.tensorflow.org/guide/keras/functional"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/functional</a></p>
",1,2,210,2020-03-05 03:46:20,https://stackoverflow.com/questions/60545512/invalid-argument-indices0-0-4-is-not-in-0-40405
Freeze only some lines of a torch.nn.Embedding object,"<p>I am quite a newbie to Pytorch, and I am trying to implement a sort of ""post-training"" procedure on embeddings. </p>

<p>I have a vocabulary with a set of items, and I have learned one vector for each of them. 
I keep the learned vectors in a nn.Embedding object. 
What I'd like to do now is to add a new item to the vocabulary without updating the already learned vectors. The embedding for the new item would be initialized randomly, and then trained while keeping all the other embeddings frozen.</p>

<p>I know that in order to prevent a nn.Embedding to be trained, I need to set to <code>False</code> its <code>requires_grad</code> variable. I have also found <a href=""https://stackoverflow.com/questions/54924582/is-it-possible-to-freeze-only-certain-embedding-weights-in-the-embedding-layer-i"">this other question</a> that is similar to mine. The best answer proposes to </p>

<ol>
<li><p>either store the frozen vectors and the vector to train in different nn.Embedding objects, the former with <code>requires_grad = False</code> and the latter with <code>requires_grad = True</code></p></li>
<li><p>or store the frozen vectors and the new one in the same nn.Embedding object, computing the gradient on all vectors, but descending it is only on the dimensions of the vector of of the new item. This, however, leads to a relevant degradation in performances (which I want to avoid, of course).</p></li>
</ol>

<p>My problem is that I really need to store the vector for the new item in the same nn.Embedding object as the frozen vectors of the old items. The reason for this constraint is the following: when building my loss function with the embeddings of the items (old and new), I need to lookup the vectors based on the ids of the items, and for performances reasons I need to use Python slicing. In other words, given a list of item ids <code>item_ids</code>, I need to do something like <code>vecs = embedding[item_ids]</code>. If I used two different nn.Embedding items for the old items and the and new one I would need to use an explicit for-loop with if-else conditions, which would lead to worse performances.  </p>

<p>Is there any way I can do this?</p>
","python, pytorch, gradient-descent, embedding, word-embedding","<p>If you look at the implementation of <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding"" rel=""noreferrer"">nn.Embedding</a> it uses the functional form of <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#embedding"" rel=""noreferrer"">embedding</a> in the forward pass. Therefore, I think you could implement a custom module that does something like this:</p>

<pre><code>import torch
from torch.nn.parameter import Parameter
import torch.nn.functional as F

weights_freeze = torch.rand(10, 5)  # Don't make parameter
weights_train = Parameter(torch.rand(2, 5))
weights = torch.cat((weights_freeze, weights_train), 0)

idx = torch.tensor([[11, 1, 3]])
lookup = F.embedding(idx, weights)

# Desired result
print(lookup)
lookup.sum().backward()
# 11 corresponds to idx 1 in weights_train so this has grad
print(weights_train.grad)
</code></pre>
",5,2,2504,2020-03-10 10:35:06,https://stackoverflow.com/questions/60615832/freeze-only-some-lines-of-a-torch-nn-embedding-object
Issues while loading a trained fasttext model using gensim,"<p>I am trying to load a trained fasttext model using gensim. The model has been trained on some data. Earlier, I have used <code>model.save()</code> with a extension of <code>.bin</code> to use it later. After the training process and saving the model using <code>model.save</code> in <code>.bin</code> format, generates 3 files respectively. They are:</p>

<p>1) .bin  </p>

<p>2) bin.trainable vectors_ngrams_lockf</p>

<p>3) bin.wv.vectors_ngrams </p>

<p>Now I am unable to load the trained binary file (.bin).  </p>

<p>But I don't understand why I am getting a error named:</p>

<blockquote>
  <p>raise NotImplementedError(""Supervised fastText models are not supported"")
  NotImplementedError: Supervised fastText models are not supported</p>
</blockquote>

<p>After going through many blogs, peoples have suggested that <code>gensim</code> does not supports supervised training. It's fine. My question is how can I be able to load the trained binary model. Shall I need to train the model differently.</p>

<p>Any help is appreciated.</p>

<p>What I have tried after the training process: </p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from gensim.models import FastText, fasttext
model = FastText.load_fasttext_format('m1.bin')
print(model)
</code></pre>
","python, python-3.x, gensim, word-embedding, fasttext","<p>If the model was saved with <code>gensim</code>'s native <code>.save()</code> method, you'd load it with <code>.load()</code> - <strong>not</strong> <code>load_fasttext_format()</code>, which is only for models saved in the raw format used by Facebook's original FastText C++ code. </p>
",3,3,1328,2020-03-14 12:18:28,https://stackoverflow.com/questions/60682634/issues-while-loading-a-trained-fasttext-model-using-gensim
Subword vectors to a word vector tokenized by Sentencepiece,"<p>There are some embedding models that have used the <strong>Sentencepiece</strong> model for tokenization. So they give subword vectors for unknown words that are not in the vocabulary. But I want to get word vector for each word like Word2vec, fastText.
<strong>Should I average subword vectors to represent a word vector?</strong> </p>
","nlp, word-embedding","<ul>
<li><p>I have done some experiments on similar lines, averaging all subword
embeddings has better cosine similarity to the synonym of a whole
word.</p></li>
<li><p>So yes <strong>averaging makes sense and best option with tokenizers like
wordpiece and sentencepiece</strong></p></li>
</ul>
",0,2,999,2020-03-17 10:56:42,https://stackoverflow.com/questions/60720939/subword-vectors-to-a-word-vector-tokenized-by-sentencepiece
[Word2Vec][gensim] Handling missing words in vocabulary with the parameter min_count,"<p>Some similar questions have been asked regarding this topic, but I am not really satisfied with the replies so far; please excuse me for that first.</p>

<p>I'm using the function <code>Word2Vec</code> from the python library <code>gensim</code>.</p>

<p>My problem is that I <strong>can't run my model on every word of my corpus as long as I set the parameter <code>min_count</code> greater than one</strong>. Some would say it's logic cause I choose to ignore the words appearing only once. But the function is behaving weird cause it gives an <strong>error saying <em>word 'blabla' is not in the vocabulary</em></strong>, whereas this is exactly what I want ( I want this word to be out of the vocabulary).</p>

<p>I can imagine this is not very clear, then find below a reproducible example:</p>

<pre><code>import gensim
from gensim.models import Word2Vec

# My corpus
corpus=[[""paris"",""not"",""great"",""city""],
       [""praha"",""better"",""great"",""than"",""paris""],
       [""praha"",""not"",""country""]]

# Load a pre-trained model - The orignal one based on google news 
model_google = gensim.models.KeyedVectors.load_word2vec_format(r'GoogleNews-vectors-negative300.bin', binary=True)

# Initializing our model and upgrading it with Google's 
my_model = Word2Vec(size=300, min_count=2)#with min_count=1, everything works fine
my_model.build_vocab(corpus)
total_examples = my_model.corpus_count
my_model.build_vocab([list(model_google.vocab.keys())], update=True)
my_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, lockf=1.0)
my_model.train(corpus, total_examples=total_examples, epochs=my_model.iter)

# Show examples
print(my_model['paris'][0:10])#works cause 'paris' is present twice
print(my_model['country'][0:10])#does not work cause 'country' appears only once
</code></pre>

<p>You can find Google's model <a href=""https://github.com/mmihaltz/word2vec-GoogleNews-vectors"" rel=""nofollow noreferrer"">there</a> for example, but feel free to use any model or just do without, this is not the point of my post.</p>

<p>As notified in the commentaries of the code: running the model on 'paris' works but not on 'country'. And of course, if I set the parameter <code>min_count</code> to 1, everything works fine.</p>

<p>I hope it is clear enough.</p>

<p>Thanks.</p>
","python, nlp, gensim, word2vec, word-embedding","<p>It is supposed to throw an error if you ask for a word that's not present because you chose not to learn vectors for rare words, like <code>'country'</code> in your example. (And: such words with few examples usually don't get good vectors, and retaining them can worsen the vectors for remaining words, so a <code>min_count</code> as large as you can manage, and perhaps much larger than <code>1</code>, is usually a good idea.)</p>

<p>The fix is to do one of the following:</p>

<ol>
<li>Don't ask for words that aren't present. Check first, via something like Python's <code>in</code> operator. For example:</li>
</ol>

<pre><code>if 'country' in my_model:
    print(my_model['country'][0:10])
else: 
    pass  # do nothing, since `min_count=2` means there's no 'country' vector
</code></pre>

<ol start=""2"">
<li>Catch the error, falling back to whatever you want to happen for absent words:</li>
</ol>

<pre><code>try:
    print(my_model['country'][0:10])
except:
    pass  # do nothing, or perhaps print an error, whatever
</code></pre>

<ol start=""3"">
<li>Change to using a model that always returns something for any word, like <code>FastText</code> – which will try to synthesize a vector for unknown words, using subwords learned during training. (It might be garbage, it might be pretty good if the unknown word is highly similar to known words in characters &amp; meaning, but for some uses it's better than nothing.) </li>
</ol>
",2,2,4747,2020-03-17 17:05:46,https://stackoverflow.com/questions/60727025/word2vecgensim-handling-missing-words-in-vocabulary-with-the-parameter-min-c
Doc2Vec Pre training and Inferring vectors,"<p>Suppose I have trained the doc2vec model with 50000 documents and I want to infer vectors for a separate dataset containing 36000 documents. In this case will the inferred vectors be effective for the downstream task of classification, becasue my assumption is that the inferred vectors depend on the size of documents with which the model is trained. </p>

<p>Note: Both dataset i.e one used for training doc2vec and other for inferring vectors are unique but from the same domain of US supreme court.</p>

<p>Please correct me if I am wrong with valid reason.</p>
","python, nlp, word-embedding, doc2vec, pre-trained-model","<p>With such a tiny dataset, no answer I can give will be as useful as just trying it to see if it works. </p>

<p>50000 is smallish for a training set, but some useful <code>Doc2Vec</code> results have been based on similar corpuses. </p>

<p>Vector inference, like training, reduces documents of any length to a fixed-size vector. (But note: gensim silently limits any text fed to a <code>2Vec</code> model to 10000 tokens.) </p>

<p>But, if you've trained a model on documents that are all about 1000 words, then try inference on 10-word fragments, those doc-vectors might not be as useful, or useful in the same way, as inferred vectors on documents more similar to the training set. But you'd still need to try it to find out. (Also note: words not learned during training are completely ignored during inference, so later inferences on docs with many/all unknown words will be weak or meaningless.)</p>

<p>Is that the the case with your inference docs – they are very different from training docs in size &amp; vocabulary? And if so, why? (Can you train with more representative documents?)</p>

<p>If the set of 36000 documents is fixed before training begins, it may also be valid/defensible to include them in the unsupervised <code>Doc2Vec</code> training. They're data, they help learn the domain lingo, and they don't have in them any form of the ""right"" answers for classification. </p>
",1,0,290,2020-03-20 13:04:39,https://stackoverflow.com/questions/60774762/doc2vec-pre-training-and-inferring-vectors
Text classification using word embeddings,"<p>I've got a dataset of positive and negative content. So let's assume it's a spam project. </p>

<p>I need to build a model, which can categorize the content in pos/neg. So I am doing a supervised learning task, because I've got a labeled dataset. The best choice therefore must be using a SVC model.</p>

<p>So far so good.</p>

<p>Now the complicated part comes.</p>

<p>I want to solve the same task by using Keras LSTM model. So my question:</p>

<p>Is it still supervised or is it unsupervised , because I am using word embeddings for this task and referring to this post here, word embedding is used for unsupervised tasks: <a href=""https://www.quora.com/Is-deep-learning-supervised-unsupervised-or-something-else"" rel=""nofollow noreferrer"">https://www.quora.com/Is-deep-learning-supervised-unsupervised-or-something-else</a></p>

<p>There it says: </p>

<blockquote>
  <p>Deep learning can be Unsupervised : Word embedding, image encoding
  into lower or higher dimensional etc.</p>
</blockquote>

<p>So - is it now unsupervised or supervised (because my dataset is labeled) ? </p>

<p>And is deep learning another technique like unsupervised and supervised learning or how is the relation between these topics? Is deep learning using supervised and unsupervised techniques? Or do one have to choose between deep learning, unsupervised and supervised learning?</p>

<p>It's so confusing! Please help! Especially for the LSTM task. I need to know where it's supervised (because of the labeled dataset) or unsupervised (because of the usage of word embeddings)</p>

<p>Thanks in advance guys!</p>
","machine-learning, text-classification, word-embedding, unsupervised-learning, supervised-learning","<p>A quick word of encouragement, I recall feeling precisely the same way; <em>insanely frustrated</em> when I started learning this field. It really does get easier!  </p>

<p>Word embeddings are <em>created</em> by unsupervised learning. However, you can use a trained embedding layer <em>within</em> a supervised projected, like you're doing. In other words, your project is one of supervised learning, and one of the layers is using weights that were acquired by an unsupervised training technique.</p>

<p>It may be helpful to further understand embedding layers, how they're made, and what they can do for supervised learning. I'll try to explain in a non-technical way, so you can get a feel for the concept prior to learning the particulars and pedantisms.<br>
Suppose you begin with a giant corpus. You count the frequency of occurrence of every word, and use it to rank each relative to the others (or use some other formula, whatever). This is a method of text ""tokenization."" The point is to get words into numbers. Obviously this is important, since we're fixing to do math with them, but it creates a bit of a pinch: the numerical relationships don't necessarily carry any information about the relationships of the <em>meanings</em> of the words. To ameliorate this, you can train a little network like so: take chunks from your corpus and create <a href=""https://keras.io/preprocessing/sequence#skipgrams"" rel=""nofollow noreferrer"">skipgrams</a>, and teach the network that, after the application of weights and a measure of <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity"" rel=""nofollow noreferrer"">cosine similarity</a>, the output produced should be a <code>1</code> if the words appear near each other (or some other criteria), or <code>0</code> (or perhaps <code>-1</code>, if you prefer) when they do not appear near each other. Over the course of the corpus, words that tend to be used together will move together and likewise the inverse. The objective is to create a kind of map (or a simulacrum, if you will) of the <em>relative meaning</em> of the tokens (which are words); said another way, the objective is to create an n-dimensional representation of the words' relative meanings. Then, after training, the embeddings can be saved for use in projects like yours. Your embedding layer will then look up the token in the saved embeddings and grab its outputs, which are that word's vector representation in the embedding space; their coordinates in our theoretical map. This is considered ""unsupervised"" because you don't have to explicitly supply the ground-truth for comparison; in this case, it's being generated procedurally from the training sample (i.e. skipgrams generated from whatever the input was). Another example would be if the expected output was identical to the input (as in auto-encoders), which is unsupervised (as before) because you don't have to supply an expected output; if you supply an input, it automatically has the expected output.<br>
If all of that is confusing, then just pause and consider your own thoughts: if I ask you for a word that means the same thing as ""big"" in the phrase ""a big pizza,"" you consult your understanding of the meaning of ""big"" as pertains to the indicated phrase, and draw something as close to it as possible: perhaps the word ""large."" Embeddings are a way of making a map where ""big"" and ""large"" are positioned very close together along most axes (i.e. in most dimensions).<br>
So, then, when you load some pre-trained embeddings, you're just loading some weights into one of your layers. Sometimes people initialize layers with zeros, other times people use random normal or gaussian distributions, and sometimes people use specific values (e.g. loading a saved network, or loading embeddings); it's all the same. If you go on to perform supervised training, then you're doing precisely that: performing supervised training. Following the embedding layer, the information you're working with is not arbitrary words, but rather these: relative meanings. And if that isn't just neat, I don't know what is! I find it's helpful to consider what your data represents as it passes through the network.</p>
",2,0,1842,2020-03-30 11:15:50,https://stackoverflow.com/questions/60929359/text-classification-using-word-embeddings
Gensim word2vec downsampling sample=0,"<p>Does <code>sample= 0</code> in Gensim word2vec mean that no downsampling is being used during my training? The documentation says just that </p>

<blockquote>
  <p>""useful range is (0, 1e-5)""</p>
</blockquote>

<p>However putting the threshold to 0 would cause P(wi) to be equal to 1, meaning that no word would be discarded, am I understanding it right or not? </p>

<p>I'm working on a relatively small dataset of 7597 Facebook posts (18945 words) and my embeddings perform far better using <code>sample= 0</code>rather than anything else within the recommended range. Is there any particular reason? Text size? </p>
","python, math, gensim, word-embedding, subsampling","<p>That seems an incredibly tiny dataset for <code>Word2Vec</code> training. (Is that only 18945 unique words, or 18945 words total, so hardly more than 2 words per post?) </p>

<p>Sampling is most useful on larger datasets - where there are <em>so many</em> examples of common words, more training examples of them aren't adding much – but they are stealing time from, and overwieghting those words' examples compared to, other less-frequent words. </p>

<p>Yes, <code>sample=0</code> means no down-sampling.</p>
",2,1,1137,2020-03-30 19:42:38,https://stackoverflow.com/questions/60938299/gensim-word2vec-downsampling-sample-0
Speed up embedding of 2M sentences with RoBERTa,"<p>I have roughly 2 million sentences that I want to turn into vectors using Facebook AI's RoBERTa-large,fine-tuned on NLI and STSB for sentence similarity (using the awesome <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">sentence-transformers</a> package).</p>

<p>I already have a dataframe with two columns: ""utterance"" containing each sentence from the corpus, and ""report"" containing, for each sentence, the title of the document from which it is from.</p>

<p>From there, my code is the following:</p>

<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer
from tqdm import tqdm

model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')

print(""Embedding sentences"")

data = pd.read_csv(""data/sentences.csv"")

sentences = data['utterance'].tolist()

sentence_embeddings = []

for sent in tqdm(sentences):
    embedding = model.encode([sent])
    sentence_embeddings.append(embedding[0])

data['vector'] = sentence_embeddings
</code></pre>

<p>Right now, tqdm estimates that the whole process will take around 160 hours on my computer, which is more than I can spare.</p>

<p>Is there any way I could speed this up by changing my code? Is creating a huge list in memory then appending it to the dataframe the best way to proceed here? (I suspect not).</p>

<p>Many thanks in advance!</p>
","python, nlp, word-embedding, transformer-model","<p>I found a ridiculous speedup using this package by feeding in the utterances as a list instead of looping over the list. I assume there is some nice internal vectorisation going on.</p>
<pre><code>%timeit utterances_enc = model.encode(utterances[:10])                                                                                                                                                                                                                 
3.07 s ± 53.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<pre><code>%timeit utterances_enc = [model.encode(utt) for utt in utterances[:10]]
4min 1s ± 8.08 s per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<p>The full code would be as follows:</p>
<pre><code>from sentence_transformers import SentenceTransformer
from tqdm import tqdm

model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')

print(&quot;Embedding sentences&quot;)

data = pd.read_csv(&quot;data/sentences.csv&quot;)

sentences = data['utterance'].tolist()

sentence_embeddings = model.encode(sentences)

data['vector'] = sentence_embeddings
</code></pre>
",9,6,2997,2020-05-04 08:50:15,https://stackoverflow.com/questions/61588381/speed-up-embedding-of-2m-sentences-with-roberta
How does Word Embeddings in Deep Learning works?,"<p>I have a very basic doubt in Word Embeddings. I have an understanding that word embeddings are used to represent text data in a numeric format without losing the context, which is very helpful in training deep models.</p>

<p>Now my question is, does the word embedding algorithm need to learn all the data once and then represent each record in numeric format? Or else, each record will be represented individually with knowing what other records.</p>

<p>Tensorflow code:</p>

<p><a href=""https://i.sstatic.net/iHFIB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iHFIB.png"" alt=""enter image description here""></a></p>

<p>This is an experiment I did with sample code where embeddings independently reframe the data into the specified dimension.</p>

<p>Is my understanding correct?</p>
","tensorflow, deep-learning, word-embedding","<p>No it doesnt need to learn all the data once and then represent each record in numeric format , it is done individually .
What you did is correct , but there is much methods for Natural Language Processing , i can recommand you a good method too , is to transform each letter to a number , so here you can use the prediction letter by letter , is it true that it wont be fast but it can garantee a good accuracy because hte vocabulary of letters is less than the word's , it can be something like this :</p>

<pre><code>vocab = set( your_text ) # extract each distinct letter
vocab_to_int = {l:i for i,l in enumerate(vocab)} # transforms letter to number
int_to_vocab = {i:l for i,l in enumerate(vocab)} # do the inverse

transformed_text = [vocab_to_int[l] for l in your_text] # all text transformed
</code></pre>
",1,-1,63,2020-05-04 15:01:02,https://stackoverflow.com/questions/61595286/how-does-word-embeddings-in-deep-learning-works
Size of input and output layers in Keras implementation of an RNN Language Model,"<p>As part of my thesis, I am trying to build a recurrent Neural Network Language Model. </p>

<p>From theory, I know that the input layer should be a one-hot vector layer with a number of neurons equal to the number of words of our Vocabulary, followed by an Embedding layer, which, in Keras, it apparently translates to a single Embedding layer in a Sequential model. I also know that the output layer should also be the size of our vocabulary so that each output value maps 1-1 to each vocabulary word.</p>

<p>However, in both the Keras documentation for the Embedding layer (<a href=""https://keras.io/layers/embeddings/"" rel=""nofollow noreferrer"">https://keras.io/layers/embeddings/</a>) and in this article (<a href=""https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/#comment-533252"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/#comment-533252</a>), the vocabulary size is arbitrarily augmented by one for both the input and the output layers! Jason gives an explenation that this is due to the implementation of the Embedding layer in Keras but that doesn't explain why we would also use +1 neuron in the output layer. I am at the point of wanting to order the possible next words based on their probabilities and I have one probability too many that I do not know to which word to map it too.</p>

<p>Does anyone know what is the correct way of acheiving the desired result? Did Jason just forget to subtrack one from the output layer and the Embedding layer just needs a +1 for implementation reasons (I mean it's stated in the official API)?</p>

<p>Any help on the subject would be appreciated (why is Keras API documentation so laconic?).</p>

<p><strong>Edit:</strong></p>

<p>This post <a href=""https://stackoverflow.com/questions/43227938/keras-embedding-layer-masking-why-does-input-dim-need-to-be-vocabulary-2?rq=1"">Keras embedding layer masking. Why does input_dim need to be |vocabulary| + 2?</a> made me think that Jason does in fact have it wrong and that the size of the Vocabulary should not be incremented by one when our word indices are: <code>0, 1, ..., n-1</code>.</p>

<p>However, when using Keras's Tokenizer our word indices are: <code>1, 2, ..., n</code>. In this case, the correct approach is to:</p>

<ol>
<li><p>Set <code>mask_zero=True</code>, to treat 0 differently, as there is never a
0 (integer) index input in the Embedding layer and keep the
vocabulary size the same as the number of vocabulary words (<code>n</code>)?</p></li>
<li><p>Set <code>mask_zero=True</code> but augment the vocabulary size by one?</p></li>
<li><p>Not set <code>mask_zero=True</code> and keep the vocabulary size the same as the
number of vocabulary words?</p></li>
</ol>
","tensorflow, keras, neural-network, word-embedding, language-model","<p>the reason why we add +1 leads to the possibility that we can encounter a chance to see an unseen word(out of our vocabulary) during testing or in production, it is common to consider a generic term for those UNKNOWN and that is why we add a <code>OOV</code> word in front which resembles all out of vocabulary words.
Check this issue on github which explains it in detail:</p>

<p><a href=""https://github.com/keras-team/keras/issues/3110#issuecomment-345153450"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/issues/3110#issuecomment-345153450</a></p>
",0,0,297,2020-05-04 17:23:55,https://stackoverflow.com/questions/61598029/size-of-input-and-output-layers-in-keras-implementation-of-an-rnn-language-model
Tensorflow Embedding Layer Vocabulary Size,"<p>I am learning Tensorflow and have come across the Embedding layer in tensorflow used to learn one's own word embeddings. The layer takes the following parameters:</p>

<pre><code>keras.layers.Embedding(input_dim, 
                       output_dim, 
                       embeddings_initializer='uniform',
                       embeddings_regularizer=None, 
                       activity_regularizer=None, 
                       embeddings_constraint=None, 
                       mask_zero=False, 
                       input_length=None)
</code></pre>

<p>The 'input dim' should be the same size as the vocabulary i.e. unique words. If I wanted to limit the vocabulary to only the first 25000 most frequent words - how should I do this? </p>

<p>Can I simply change 'input_dim' to 25000 or would I have to go through my corpus and replace any word that is outside the top 25000 words with an  token for example?</p>
","python, tensorflow, word-embedding","<p>Actually, if you use <code>tensorflow.keras</code> you have to make sure in your corpus, the tokens don't exceed the vocabulary_size or the <code>input_dim</code> of embedding layer, otherwise you'll get error.</p>

<p>If you use <code>keras</code>, then you can just change the <code>input_dim</code> in your embedding layer without changing anything in corpus or tokens. <code>keras</code> will replace out of vocabulary tokens with a <code>zero</code> vector.</p>

<p>First of all, there is an error if you use tensorflow.keras.</p>

<p><strong>tensorflow</strong></p>

<pre><code>from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, Input
import numpy as np

ip = Input(shape = (3,))
emb = Embedding(1, 2, trainable=True, mask_zero=True)(ip)

model = Model(ip, emb)
input_array = np.array([[5, 3, 1], [1, 2, 3]]) # out of vocabulary

model.compile(""rmsprop"", ""mse"")

output_array = model.predict(input_array)

print(output_array)

print(output_array.shape)

model.summary()
</code></pre>

<p><a href=""https://i.sstatic.net/bCzub.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/bCzub.png"" alt=""enter image description here""></a></p>

<p>But if I use keras 2.3.1, I don't get any error.</p>

<p><strong>keras 2.3.1</strong></p>

<pre><code>from keras.models import Model
from keras.layers import Embedding, Input
import numpy as np

ip = Input(shape = (3,))
emb = Embedding(1, 2, trainable=True, mask_zero=True)(ip)

model = Model(ip, emb)
input_array = np.array([[5, 3, 1], [1, 2, 3]])

model.compile(""rmsprop"", ""mse"")

output_array = model.predict(input_array)

print(output_array)

print(output_array.shape)

model.summary()
</code></pre>

<p><a href=""https://i.sstatic.net/kN8N6.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/kN8N6.png"" alt=""enter image description here""></a></p>

<p>keras has different implementations for embedding layer. To validate that, let's go to keras embedding layer.</p>

<p><a href=""https://github.com/keras-team/keras/blob/master/keras/layers/embeddings.py#L16"" rel=""noreferrer"">https://github.com/keras-team/keras/blob/master/keras/layers/embeddings.py#L16</a></p>

<p>For now let's just look into call function.</p>

<pre><code>    def call(self, inputs):
        if K.dtype(inputs) != 'int32':
            inputs = K.cast(inputs, 'int32')
        out = K.gather(self.embeddings, inputs)
        return out
</code></pre>

<p>N.B: If you want the exact source code for keras 2.3.1 go here and download source code: <a href=""https://github.com/keras-team/keras/releases"" rel=""noreferrer"">https://github.com/keras-team/keras/releases</a></p>

<p>But if we go to tensorflow implementation, it's different.</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py</a></p>

<p>Just to verify, the call function is differently written.</p>

<pre><code>  def call(self, inputs):
    dtype = K.dtype(inputs)
    if dtype != 'int32' and dtype != 'int64':
      inputs = math_ops.cast(inputs, 'int32')
    out = embedding_ops.embedding_lookup(self.embeddings, inputs)
    return out
</code></pre>

<p>Let's design a simple network like before and observe the weight matrix.</p>

<pre><code>from keras.models import Model
from keras.layers import Embedding, Input
import numpy as np

ip = Input(shape = (3,))
emb = Embedding(1, 2, trainable=True, mask_zero=True)(ip)

model = Model(ip, emb)
input_array = np.array([[5, 3, 1], [1, 2, 3]])

model.compile(""rmsprop"", ""mse"")

output_array = model.predict(input_array)

print(output_array)

print(output_array.shape)

model.summary()
</code></pre>

<p>The model gives the following output.</p>

<pre><code>[[[0. 0.]
  [0. 0.]
  [0. 0.]]

 [[0. 0.]
  [0. 0.]
  [0. 0.]]]
(2, 3, 2)
Model: ""model_18""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_21 (InputLayer)        (None, 3)                 0         
_________________________________________________________________
embedding_33 (Embedding)     (None, 3, 2)              2         
=================================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
</code></pre>

<p>Okay, we are getting bunch of zeros but the default weight_initializer is not zeros!</p>

<p>So, let's observe the weight matrix now.</p>

<pre><code>import keras.backend as K

w = model.layers[1].get_weights()
print(w)

</code></pre>

<pre><code>[array([[ 0.03680499, -0.04904002]], dtype=float32)]
</code></pre>

<p>In fact, it is not all zeros.</p>

<p>So, why are we getting zeros?</p>

<p>Let's change our input to the model.</p>

<p>As the only in vocabulary word index for input_dim = 1, is 0. Let's pass 0 as one of the inputs.</p>

<pre><code>from keras.models import Model
from keras.layers import Embedding, Input
import numpy as np

ip = Input(shape = (3,))
emb = Embedding(1, 2, trainable=True, mask_zero=True)(ip)

model = Model(ip, emb)
input_array = np.array([[5, 0, 1], [1, 2, 0]])

model.compile(""rmsprop"", ""mse"")

output_array = model.predict(input_array)

print(output_array)

print(output_array.shape)

model.summary()
</code></pre>

<p>Now, we get non-zero vectors for the positions where we passed 0.</p>

<pre><code>[[[ 0.          0.        ]
  [-0.04339869 -0.04900574]
  [ 0.          0.        ]]

 [[ 0.          0.        ]
  [ 0.          0.        ]
  [-0.04339869 -0.04900574]]]
(2, 3, 2)
Model: ""model_19""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_22 (InputLayer)        (None, 3)                 0         
_________________________________________________________________
embedding_34 (Embedding)     (None, 3, 2)              2         
=================================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
</code></pre>

<p>In short, Keras maps any out of vocabulary word index with a zero vector and this is reasonable as for those positions the forward pass will ensure all the contributions are NIL (the biases may have a role though). That is a little bit counter-intuitive as passing out of vocabulary tokens to the model seems an overhead (rather than just removing them in the pre-processing step) and bad practice but it is a good fix to test different <code>input_dim</code> without re-calculating tokens.</p>
",5,2,3845,2020-05-05 09:30:19,https://stackoverflow.com/questions/61609929/tensorflow-embedding-layer-vocabulary-size
"WARNING: WARNING:tensorflow:Model was constructed with shape (None, 150) , but it was called on an input with incompatible shape (None, 1)","<p>So I'm trying to build a word embedding model but I keep getting this error.
During training, the accuracy does not change and the val_loss remains ""nan""</p>

<p>The raw shape of the data is</p>

<pre><code>x.shape, y.shape
((94556,), (94556, 2557))
</code></pre>

<p>Then I reshape it so:</p>

<pre><code>xr= np.asarray(x).astype('float32').reshape((-1,1))
yr= np.asarray(y).astype('float32').reshape((-1,1))
((94556, 1), (241779692, 1))
</code></pre>

<p>Then I run it through my model</p>

<pre><code>model = Sequential()
model.add(Embedding(2557, 64, input_length=150, embeddings_initializer='glorot_uniform'))
model.add(Flatten())
model.add(Reshape((64,), input_shape=(94556, 1)))
model.add(Dense(512, activation='sigmoid'))
model.add(Dense(128, activation='sigmoid'))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='sigmoid'))
model.add(Dense(1, activation='relu'))
# compile the mode
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# summarize the model
print(model.summary())
plot_model(model, show_shapes = True, show_layer_names=False)
</code></pre>

<p><strong>After training, I get a constant accuracy and a val_loss nan for every epoch</strong></p>

<pre><code>history=model.fit(xr, yr, epochs=20, batch_size=32, validation_split=3/9)

Epoch 1/20
WARNING:tensorflow:Model was constructed with shape (None, 150) for input Tensor(""embedding_6_input:0"", shape=(None, 150), dtype=float32), but it was called on an input with incompatible shape (None, 1).
WARNING:tensorflow:Model was constructed with shape (None, 150) for input Tensor(""embedding_6_input:0"", shape=(None, 150), dtype=float32), but it was called on an input with incompatible shape (None, 1).
1960/1970 [============================&gt;.] - ETA: 0s - loss: nan - accuracy: 0.9996WARNING:tensorflow:Model was constructed with shape (None, 150) for input Tensor(""embedding_6_input:0"", shape=(None, 150), dtype=float32), but it was called on an input with incompatible shape (None, 1).
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 2/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 3/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 4/20
1970/1970 [==============================] - 8s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 5/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 6/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 7/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 8/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 9/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 10/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 11/20
1970/1970 [==============================] - 8s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 12/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 13/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 14/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 15/20
1970/1970 [==============================] - 8s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 16/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 17/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 18/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 19/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
Epoch 20/20
1970/1970 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.9996 - val_loss: nan - val_accuracy: 0.9996
</code></pre>

<p>I think it has to do whit the input/output shape but I'm not certain. I tried modifying the model in various ways, adding layers/ removing layers/ different optimizers/ different batch sizes and nothing worked so far.</p>
","python, tensorflow, keras, reshape, word-embedding","<p>Ok so, here is what I understood, correct me if I'm wrong:</p>

<ul>
<li><code>x</code> contains 94556 integers, each being the index of one out of 2557 words.</li>
<li><code>y</code> contains 94556 vectors of 2557 integers, each containing also the index of one word, but this time it is a one-hot encoding instead of a categorical encoding.</li>
<li>Finally, a corresponding pair of words from <code>x</code> and <code>y</code> represents two words that are close by in the original text.</li>
</ul>

<p>If I am correct so far, then the following runs correctly:</p>

<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import *

x = np.random.randint(0,2557,94556)
y = np.eye((2557))[np.random.randint(0,2557,94556)]
xr = x.reshape((-1,1))


print(""x.shape: {}\nxr.shape:{}\ny.shape: {}"".format(x.shape, xr.shape, y.shape))


model = Sequential()
model.add(Embedding(2557, 64, input_length=1, embeddings_initializer='glorot_uniform'))
model.add(Reshape((64,)))
model.add(Dense(512, activation='sigmoid'))
model.add(Dense(2557, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

history=model.fit(xr, y, epochs=20, batch_size=32, validation_split=3/9)
</code></pre>

<p>The most import modifications:</p>

<ul>
<li>The <code>y</code> reshaping was losing the relationship between elements from <code>x</code> and <code>y</code>. </li>
<li>The <code>input_length</code> in the <code>Embedding</code> layer should correspond to the second dimension of <code>xr</code>.</li>
<li>The output of the last layer from the network should be the same dimension as the second dimension of <code>y</code>.</li>
</ul>

<p>I am actually surprised the code ran without crashing.</p>

<p>Finally, from my research, it seems that people are not training skipgrams like this in practice, but rather they are trying to predict whether a training example is correct (the two words are close by) or not. Maybe this is the reason you came up with an output of dimension one.</p>

<p>Here is a model inspired from <a href=""https://github.com/PacktPublishing/Deep-Learning-with-Keras/blob/master/Chapter05/keras_skipgram.py"" rel=""noreferrer"">https://github.com/PacktPublishing/Deep-Learning-with-Keras/blob/master/Chapter05/keras_skipgram.py</a> :</p>

<pre><code>word_model = Sequential()
word_model.add(Embedding(2557, 64, embeddings_initializer=""glorot_uniform"", input_length=1))
word_model.add(Reshape((embed_size,)))

context_model = Sequential()
context_model.add(Embedding(2557, 64, embeddings_initializer=""glorot_uniform"", input_length=1))
context_model.add(Reshape((64,)))

model = Sequential()
model.add(Merge([word_model, context_model], mode=""dot"", dot_axes=0))
model.add(Dense(1, kernel_initializer=""glorot_uniform"", activation=""sigmoid""))
</code></pre>

<p>In that case, you would have 3 vectors, all from the same size <code>(94556, 1)</code> (or probably even bigger than 94556, since you might have to generate additional negative samples): </p>

<ul>
<li><code>x</code> containing integers from 0 to 2556</li>
<li><code>y</code> containing integers from 0 to 2556</li>
<li><code>output</code> containing 0s and 1s, whether each pair from <code>x</code> and <code>y</code> is a negative or a positive example</li>
</ul>

<p>and the training  would look like:</p>

<pre><code>history = model.fit([x, y], output, epochs=20, batch_size=32, validation_split=3/9)
</code></pre>
",11,6,48986,2020-05-07 11:17:31,https://stackoverflow.com/questions/61656444/warning-warningtensorflowmodel-was-constructed-with-shape-none-150-but-i
How to compare cosine similarities across three pretrained models?,"<p>I have two corpora - one with all women leader speeches and the other with men leader speeches. I would like to test the hypothesis that cosine similarity between two words in the one corpus is significantly different than cosine similarity between the same two words in another corpus. Is such a t-test (or equivalent) logical and possible?</p>

<p>Further, if the cosine similarities are different across the two corpora, how could I examine if cosine similarity between the same two words in a third corpus is more similar to the first or the second corpus?</p>
","nlp, gensim, word2vec, word-embedding, glove","<p>It's certainly <em>possible</em>. Whether it's meaningful, given a certain amount of data, is harder to answer. </p>

<p>Note that in separate training sessions, a given word <em>A</em> won't necessarily wind up in the same coordinates, due to inherent randomness used by the algorithm. That's even the case when training on the <em>exact same data</em>. </p>

<p>It's just the case that in general, the distances/directions to other words <em>B</em>, <em>C</em>, etc should be of similar overall usefulness, when there's sufficient data/training and well-chosen parameters. So <em>A</em>, <em>B</em>, <em>C</em>, etc may be in different places, with slightly-different distances/directions – but the relative relationships are still similar, in terms of neighborhoods-of-words, or the <em>(A-B)</em> direction still be predictive of certain human-perceptible meaning-differences if applied to other words <em>C</em> etc. </p>

<p>So, you should avoid making direct cosine-similarity comparisons between words from different training-runs or corpuses, but you may find meaning in differences in similarities ( <em>A-B</em> vs <em>A' - B'</em> ) or top-N lists or relative-rankings. (This could also be how to compare against 3rd corpora: to what extent is there variance or correlation in certain pairwise-similarities, or top-N lists, or ordinal ranks of relevant words in each other words' 'most similar' results.)</p>

<p>You might want to perform a sanity check on your measures, by seeing to what extent they imply meaningful differences in comparisons where they logically ""shouldn't"". For example, multiple runs against the exact same corpus that's just bee reshuffled, or against random subsets of the exact same corpus. (I'm not aware of anything as formal as a 't-test' in checking the significance of differences between word2vec models, but checking whether some differences are enough to distinguish a truly-different corpus, from just a 1/Nth random subset of the same corpus, to a certain confidence level might be a grounded way to assert meaningful differences.)</p>

<p>To the extent such ""oughtta be very similar"" runs instead show end vector results that are tangibly different, it could be suggestive that either:</p>

<ul>
<li><p>the corpus is too small, with too few varied usage examples per word - word2vec benefits from lots of data, and political speech collections may be quite small compared to the sometimes hundreds-of-billions training words used for large word2vec models</p></li>
<li><p>the model is mis-parameterized - an oversized (and thus prone to overfitting) model, or insufficient training passes, or other suboptimal parameters may yield models that vary more for the same training data</p></li>
</ul>

<p>You'd also want to watch out for mismatches in training-corpus size. A corpus that's 10x as large means many more words would pass a fixed <code>min_count</code> threshold, and any chosen <em>N</em> <code>epochs</code> of training will involve 10x as many examples of common-words, and support stable results in a larger (vector-size) model - whereas the same model parameters with a smaller corpus would give more volatile results. </p>

<p>Another technique you could consider would be combining corpuses into one training set, but munging the tokens of key words-of-interest to be different depending on the relevant speaker. For example, you'd replace the word <code>'family'</code> with <code>'f-family'</code> or <code>'m-family'</code>, depending on the gender of the speaker. (You might do this for every occurrence, or some fraction of the occurrences. You might also enter each speech into your corpus more than once, sometimes with the actual words and sometimes with some-or-all replaced with the context-labeled alternates.)</p>

<p>In that case, you'd wind up with one final model, and all words/context-tokens in the 'same' coordinate space for direct comparison. But, the pseudowords <code>'f-family'</code> and <code>'m-family'</code> would have been more influenced by their context-specific usages - and thus their vectors might vary from each other, and from the original <code>'family'</code> (if you've also retained unmunged instances of its use) in interestingly suggestive ways.</p>

<p>Also note: if using the 'analogy-solving' methods of the original Google word2vec code release, or other libraries that have followed its example (like <code>gensim</code>), note that it specifically <em>won't</em> return as an answer any of the words supplied as input. So when solving the gender-fraught analogy <code>'man' : 'doctor' :: 'woman' : _?_</code>, via the call <code>model.most_similar(positive=['doctor', 'woman'], negative=['man'])</code>, even if the underlying model <em>still</em> has <code>'doctor'</code> as the closest word to the target-coordinates, it is automatically skipped as one of the input words, yielding the second-closest word instead. </p>

<p>Some early ""bias-in-word-vectors"" write-ups ignored this detail and thus tended to imply larger biases, due to this implementation artifact, even where such biases small-to-nonexistent. (You can supply raw vectors, instead of string-tokens, to <code>most_similar()</code> - and then get full results, without any filtering of input-tokens.)</p>
",4,1,1397,2020-05-11 18:38:20,https://stackoverflow.com/questions/61736874/how-to-compare-cosine-similarities-across-three-pretrained-models
"Using Dropout on output of embedding layer changes array values, Why?","<p>Observing the outputs of embedding layer with and without dropout shows that values in the arrays are replaced with 0. <strong>But along with this why other values of array changed ?</strong></p>

<p>Following is my model:-</p>

<pre><code>input = Input(shape=(23,)) 
model = Embedding(input_dim=n_words, output_dim=23, input_length=23)(input)
model = Dropout(0.2)(model)
model = Bidirectional(LSTM(units=LSTM_N, return_sequences=True, recurrent_dropout=0.1))(model)
out = TimeDistributed(Dense(n_tags, activation=""softmax""))(model) # softmax output layer
model = Model(input, out)
</code></pre>

<p>Building  model2 from trained model , with input as the input layer and output as the output of Dropout(0.2) . -</p>

<pre><code>from keras import backend as K
model2 = K.function([model.layers[0].input ,  K.learning_phase()],
                  [model.layers[2].output]   )
dropout = model2([X_train[0:1] , 1])[0]
nodrop = model2([X_train[0:1] , 0])[0]
</code></pre>

<p>Printing the first array of both dropout and no dropout:  </p>

<pre><code>dropout[0][0]
</code></pre>

<p>Output-</p>

<pre><code>array([ 0.        , -0.        , -0.        , -0.04656423, -0.        ,
        0.28391626,  0.12213208, -0.01187495, -0.02078421, -0.        ,
        0.10585815, -0.        ,  0.27178472, -0.21080771,  0.        ,
       -0.09336889,  0.07441022,  0.02960865, -0.2755439 , -0.11252255,
       -0.04330419, -0.        ,  0.04974075], dtype=float32)   
</code></pre>

<p>-</p>

<pre><code>nodrop[0][0]
</code></pre>

<p>Output-</p>

<pre><code>array([ 0.09657606, -0.06267098, -0.00049554, -0.03725138, -0.11286845,
    0.22713302,  0.09770566, -0.00949996, -0.01662737, -0.05788678,
    0.08468652, -0.22405024,  0.21742778, -0.16864617,  0.08558936,
   -0.07469511,  0.05952817,  0.02368692, -0.22043513, -0.09001804,
   -0.03464335, -0.05152775,  0.0397926 ], dtype=float32)
</code></pre>

<p>Some values are replaced with 0 , agreed, but why are other values changed ?
As the embedding outputs have a meaning and are unique for each of the words, if these are changed by applying dropout, then <strong>is it correct to apply dropout after embedding layer ?</strong></p>

<blockquote>
  <p>Note- I have used ""learning_phase"" as 0 and 1 for testing(nodropout)
  and training(droput) respectively.</p>
</blockquote>
","python, python-3.x, keras, deep-learning, word-embedding","<p>It is how the dropout regularization works. After applying the dropout, the values are divided by the keeping probability (in this case 0.8).</p>

<p>When you use dropout, the function receives the probability of turning a neuron to zero as input, e.g., 0.2, which means it has 0.8 chance of keeping any given neuron. So, the values remaining will be multiplied by 1/(1-0.2).</p>

<p>This is called ""inverted dropout technique"" and it is done in order to ensure that the expected value of the activation remains the same. Otherwise, predictions will be wrong during inference when dropout is not used.</p>

<p>You'll notice that your dropout is 0.2, and all your values have been multiplied by 0.8 after you applied dropout.</p>

<p>Look what happens if I divide your second output bu the first:</p>

<pre><code>import numpy as np
a = np.array([ 0.        , -0.        , -0.        , -0.04656423, -0.        ,
        0.28391626,  0.12213208, -0.01187495, -0.02078421, -0.        ,
        0.10585815, -0.        ,  0.27178472, -0.21080771,  0.        ,
       -0.09336889,  0.07441022,  0.02960865, -0.2755439 , -0.11252255,
       -0.04330419, -0.        ,  0.04974075])

b = np.array([ 0.09657606, -0.06267098, -0.00049554, -0.03725138, -0.11286845,
    0.22713302,  0.09770566, -0.00949996, -0.01662737, -0.05788678,
    0.08468652, -0.22405024,  0.21742778, -0.16864617,  0.08558936,
   -0.07469511,  0.05952817,  0.02368692, -0.22043513, -0.09001804,
   -0.03464335, -0.05152775,  0.0397926 ])

print(b/a)
</code></pre>

<pre><code>[       inf        inf        inf 0.79999991        inf 0.80000004
 0.79999997 0.8        0.8000001         inf 0.8               inf
 0.80000001 0.80000001        inf 0.79999998 0.79999992 0.8
 0.80000004 0.8        0.79999995        inf 0.8       ]
</code></pre>
",1,1,776,2020-05-18 15:44:28,https://stackoverflow.com/questions/61873516/using-dropout-on-output-of-embedding-layer-changes-array-values-why
From numpy array of sentences to array of embedding,"<p>I'm learning to use tensorflow and trying to classify text. I have a dataset where each text is associated with a label 0 or 1. My goal is to use some sentence embedding to do the classification. First I've created an embedding from the whole text using the Gnews precompile embedding:</p>

<pre><code>embedding = ""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1""
hub_layer = hub.KerasLayer(embedding, input_shape=[2], dtype=tf.string,
                           trainable=True, output_shape=[None, 20])
</code></pre>

<p>Now I'd like to try something else (similar to this method <a href=""http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"" rel=""nofollow noreferrer"">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</a>) and I wanted to:</p>

<ul>
<li>Separate each text into setences.</li>
<li>Create an array of embeddings for each text, one per sentence.</li>
<li>Use that as input for my model.</li>
</ul>

<p>I'm able to separate the texts in sentences. Each text is an array of sentences saved as:</p>

<pre><code>[array(['AITA - Getting Hugged At The Bar .',
       'This all happened less than an hour ago..',
       'I was at a bar I frequent and talking to some people I know, suddenly I feel someone from behind me hugging and starting to grind against me.',
       ""I know a lot of people at the bar, and assume it's a friend of mine, but when I look down at the shoes I do not recognize them."",
       'I look back and I see a dude I do not know, nor have I ever seen.',
       ""He looks back at me, with horror in his eyes, because I'm a dude too..."",
       'I feel an urge of rage inside me and shove him in the chest with my elbow so I can get away..',
       'He goes to his table and I go back to mine.',
       'I was with my roommate and his girlfriend.',
       'They asked what happened and I told them, then I see the guy who hugged me looking around for me.',
       'Him and two of his friends come up to us and he says:  .',
       '""I just wanted to apologize, I thought you were someone else."".',
       'I respond, ""I understand, just check before you hug people.',
       'Now, please fuck off"".',
       'He repeats his last statement, so do I.',
       'This happens one more time and at this point his friends have surrounded me, my roommate is on his feet and I have left my beer at the table.',
       'His friend goes in my face and says.', '.',
       '""He just wanted to apologize, you really shouldn\'t be yelling at us"" and starts waiving his finger at me.. We are at a rock bar, it\'s loud, I was speaking louder just to be sure I am heard..',
       'The manager knows me so he comes asking me what happened.',
       'I explain the situation and he speaks with them then he tells me.',
       '.', '""They want to say sorry, can you guys shake hand?', '"".',
       '""Yeah sure, I just want them to leave me alone.""', '.',
       ""Honestly I didn't even want to touch the guy, but whatever."",
       ""We shake hands and they go away.. Me and my roommate look at their table and there's no one that looks anything like me."",
       'So, reddit, did I overreact?', 'Am I The Asshole here?'],
      dtype='&lt;U190')
 array([""AITA if i don't want to pay my friend 5 dollars for a slice of pizzaSo, my friend bought herself, our other friend and I a pizza to eat for lunch."",
       'Me and other friend ate 1 slice of pizza from an extra large pizza.',
       'Other friend has already paid my friend that bought the pizza 5 dollars..',
       'I am trying to save money wherever i can, but she really wants me to pay her 5 dollars ""so its fair"".. AITA?'],
      dtype='&lt;U146')
</code></pre>

<p>Now when I try to create an embedding from one element of the array it works. Here is my embedding function:</p>

<pre><code>def embedding_f(test):
    print(""test shape:"", test.shape)
    # a = tf.constant(test)
    embedding = ""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1""
    hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string,
                               trainable=True, output_shape=[None, 20])
    ret = hub_layer(test)
    # print(ret)
    return ret.numpy()

# Works
emb = cnn.embedding_f(train_data[0])
</code></pre>

<p>But if I try to input a batch of data (as will be done later in the pipeline, the program crashes</p>

<pre><code># Crashes
emb = cnn.embedding_f(train_data[0:2])



---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-8-76f4f9171cad&gt; in &lt;module&gt;
----&gt; 1 emb = cnn.embedding_f(train_data[0:2])

~/AITA/aita/cnn.py in embedding_f(test)
     22     hub_layer = hub.KerasLayer(embedding, input_shape=[2], dtype=tf.string,
     23                                trainable=True, output_shape=[None, 20])
---&gt; 24     ret = hub_layer(test)
     25     # print(ret)
     26     return ret.numpy()

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    817           return ops.convert_to_tensor_v2(x)
    818         return x
--&gt; 819       inputs = nest.map_structure(_convert_non_tensor, inputs)
    820       input_list = nest.flatten(inputs)
    821 

/usr/lib/python3.8/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    615 
    616   return pack_sequence_as(
--&gt; 617       structure[0], [func(*x) for x in entries],
    618       expand_composites=expand_composites)
    619 

/usr/lib/python3.8/site-packages/tensorflow/python/util/nest.py in &lt;listcomp&gt;(.0)
    615 
    616   return pack_sequence_as(
--&gt; 617       structure[0], [func(*x) for x in entries],
    618       expand_composites=expand_composites)
    619 

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _convert_non_tensor(x)
    815         # `SparseTensors` can't be converted to `Tensor`.
    816         if isinstance(x, (np.ndarray, float, int)):
--&gt; 817           return ops.convert_to_tensor_v2(x)
    818         return x
    819       inputs = nest.map_structure(_convert_non_tensor, inputs)

/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1276     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.
   1277   """"""
-&gt; 1278   return convert_to_tensor(
   1279       value=value,
   1280       dtype=dtype,

/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1339 
   1340     if ret is None:
-&gt; 1341       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1342 
   1343     if ret is NotImplemented:

/usr/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)
     50 def _default_conversion_function(value, dtype, name, as_ref):
     51   del as_ref  # Unused.
---&gt; 52   return constant_op.constant(value, dtype, name=name)
     53 
     54 

/usr/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    259     ValueError: if called on a symbolic tensor.
    260   """"""
--&gt; 261   return _constant_impl(value, dtype, shape, name, verify_shape=False,
    262                         allow_broadcast=True)
    263 

/usr/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    268   ctx = context.context()
    269   if ctx.executing_eagerly():
--&gt; 270     t = convert_to_eager_tensor(value, ctx, dtype)
    271     if shape is None:
    272       return t

/usr/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     94       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     95   ctx.ensure_initialized()
---&gt; 96   return ops.EagerTensor(value, ctx.device_name, dtype)
     97 
     98 

ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).
</code></pre>

<p>The error states that it's not possible to convert a Numpy array to a tensor. I've tried changing the input_shape parameter of the KerasLayer to no avail. The only solution I see is to calculate the embedding for each text by looping through all of them one by one before finding the result to the rest of the network but that seems highly inefficient (and requires too much memory for my laptop). Examples I see with word embedding, do it this way however.</p>

<p>What is the correct way to go about getting a list of embedding from multiple sentences?</p>
","tensorflow, keras, nlp, embedding, word-embedding","<p>I think your <code>output_shape</code> should be set to <code>[20]</code> (from <a href=""https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer</a>):</p>
<pre><code>hub.KerasLayer(&quot;/tmp/text_embedding_model&quot;,
    output_shape=[20],  # Outputs a tensor with shape [batch_size, 20].
    input_shape=[],     # Expects a tensor of shape [batch_size] as input.
    dtype=tf.string)    # Expects a tf.string input tensor.
</code></pre>
<p>Using TF 2.4.1 and tensorflow_hub 0.11.0, this works for me:</p>
<pre><code>data = np.array(['AITA - Getting Hugged At The Bar .', 'This all happened less than an hour ago..'])
model_url = &quot;https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1&quot;
embedding = hub.KerasLayer(model_url, input_shape=[], dtype=tf.string,
                           trainable=True, output_shape=[20])(data)
</code></pre>
<p>If you don't want to add layers on top of the KerasLayer, you can also just call</p>
<pre><code>model = hub.load(model_url)
embedding = model(data)
</code></pre>
",1,2,3276,2020-05-26 12:34:30,https://stackoverflow.com/questions/62022277/from-numpy-array-of-sentences-to-array-of-embedding
How to measure how distinct a document is based on predefined linguistic categories?,"<p>I have 3 categories of words that correspond to different types of psychological drives (need-for-power, need-for-achievement, and need-for-affiliation). Currently, for every document in my sample (n=100,000), I am using a tool to count the number of words in each category, and calculating a proportion score for each category by converting the raw word counts into a percentage based on total words used in the text. </p>

<pre><code>                 n-power   n-achieve  n-affiliation
Document1        0.010      0.025      0.100  
Document2        0.045      0.010      0.050
:                :          :          :
:                :          :          :
Document100000   0.100      0.020      0.010
</code></pre>

<p>For each document, I would like to get a measure of distinctiveness that indicates the degree to which the content of a document on the three psychological categories differs from the average content of all documents (i.e., the prototypical document in my sample). Is there a way to do this?</p>
","nlp, data-science, topic-modeling, cosine-similarity, word-embedding","<p>Essentially what you have is a clustering problem. Currently you made a representation of each of your documents with 3 numbers, lets call them a vector (essentially you cooked up some embeddings). To do what you want you can
1) Calculate an average vector for the whole set. Basically add up all numbers in each column and divide by the number of documents.
2) Pick a metric you like which will reflect an alignment of your document vectors with an average. You can just use (Euclidian)
<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html#sklearn.metrics.pairwise.euclidean_distances"" rel=""nofollow noreferrer"">sklearn.metrics.pairwise.euclidean_distances</a>
or cosine
<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html#sklearn.metrics.pairwise.cosine_distances"" rel=""nofollow noreferrer"">sklearn.metrics.pairwise.cosine_distances</a>
X will be you list of document vectors and Y will be a single average vector in the list. This is a good place to start.</p>

<p>If I would do it I would ignore average vector approach as you are in fact dealing with clustering problem. So I would use <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"" rel=""nofollow noreferrer"">KMeans</a>
see more here <a href=""https://scikit-learn.org/stable/modules/clustering.html#k-means"" rel=""nofollow noreferrer"">guide</a></p>

<p>Hope this helps!</p>
",2,3,90,2020-05-27 08:07:36,https://stackoverflow.com/questions/62038309/how-to-measure-how-distinct-a-document-is-based-on-predefined-linguistic-categor
How does gensim word2vec word embedding extract training word pair for 1 word sentence?,"<p>Refer to below image (the process of how word2vec skipgram extract training datasets-the word pair from the input sentences). </p>

<p>E.G. ""I love you."" ==> [(I,love), (I, you)]</p>

<p>May I ask what is the word pair when the sentence contains only one word? </p>

<p>Is it  ""Happy!"" ==> [(happy,happy)] ?</p>

<p>I tested the word2vec algorithm in genism, when there is just one word in the training set sentences, (and this word is not included in other sentences), the word2vec algorithm can still construct an embedding vector for this specific word. I am not sure how the algorithm is able to do so.</p>

<p><a href=""https://i.sstatic.net/zQPX6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zQPX6.png"" alt=""enter image description here""></a></p>

<p>===============UPDATE===============================</p>

<p>As the answer posted below, I think the word embedding vector created for the word in the 1-word-sentence is just the random initialization of neural network weights.</p>
","nlp, text-mining, gensim, word2vec, word-embedding","<p>No word2vec training is possible from a 1-word sentence, because there's no neighbor words to use as input to predict a center/target word. Essentially, that sentence is skipped.</p>

<p>If that was the only appearance of the word in the corpus, and you're seeing a vector for that word, it's just the starting random-initialization of the word, with no further training. (And, you should probably use a higher <code>min_count</code>, as keeping such rare words is usually a mistake in word2vec: they won't get good vectors, and other nearby words' vectors will improve if the 'noise' from all such insufficiently model-able rare words is removed.)</p>

<p>If that 1-word sentence actually appeared next-to other real sentences in your corpus, it could make sense to combine it with surrounding texts. There's nothing magic about actual sentences for this kind word-from-surroundings modeling - the algorithm is just working on 'neighbors', and it's common to use multi-sentence chunks as the texts for training, and sometimes even punctuation (like sentence-ending periods) is also retained as 'words'. Then words from an actually-separate sentence – but still related by having appeared in the same document – will appear in each other's contexts.</p>
",1,0,728,2020-06-05 08:42:07,https://stackoverflow.com/questions/62211396/how-does-gensim-word2vec-word-embedding-extract-training-word-pair-for-1-word-se
how to calculate mean of words&#39; glove embedding in a sentence,"<p>I have downloaded the glove trained matrix and used it in a Keras layer. however, I need the sentence embedding for another task.</p>

<p>I want to calculate the mean of all the word embeddings that are in that sentence.</p>

<p>what is the most efficient way to do that since there are about 25000 sentences?</p>

<p>also, I don't want to use a Lambda layer in Keras to get the mean of them.</p>
","python, word-embedding, glove","<p>the best way to do this is to use a GlobalAveragePooling1D layer. it receives the embeddings of tokens inside the sentences from the Embedding layer with the shapes (n_sentence, n_token, emb_dim) and computes the average of each token present in the sentence. the result has shape (n_sentence, emb_dim)</p>

<p>here a code example</p>

<pre><code>embedding_dim = 128
vocab_size = 100
sentence_len = 20

embedding_matrix = np.random.uniform(-1,1, (vocab_size,embedding_dim))
test_sentences = np.random.randint(0,vocab_size, (3,sentence_len))

inp = Input((sentence_len))
embedder = Embedding(vocab_size, embedding_dim,
                     trainable=False, weights=[embedding_matrix])(inp)
avg = GlobalAveragePooling1D()(embedder)

model = Model(inp, avg)
model.summary()

model(test_sentences) # the mean of all the word embeddings inside sentences 
</code></pre>
",1,0,1628,2020-06-09 16:41:58,https://stackoverflow.com/questions/62287631/how-to-calculate-mean-of-words-glove-embedding-in-a-sentence
PyTorch: Loading word vectors into Field vocabulary vs. Embedding layer,"<p>I'm coming from Keras to PyTorch. <strong>I would like to create a PyTorch Embedding layer</strong> (a matrix of size <code>V x D</code>, where <code>V</code> is over vocabulary word indices and <code>D</code> is the embedding vector dimension) with GloVe vectors but am confused by the needed steps.</p>

<p>In Keras, <a href=""https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"" rel=""noreferrer"">you can load the GloVe vectors</a> by having the Embedding layer constructor take a <code>weights</code> argument:</p>

<pre class=""lang-py prettyprint-override""><code># Keras code.
embedding_layer = Embedding(..., weights=[embedding_matrix])
</code></pre>

<p>When looking at PyTorch and the TorchText library, I see that the embeddings should be loaded <strong>twice</strong>, once in a <code>Field</code> and then again in an <code>Embedding</code> layer. Here is <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb"" rel=""noreferrer"">sample code</a> that I found:</p>

<pre class=""lang-py prettyprint-override""><code># PyTorch code.

# Create a field for text and build a vocabulary with 'glove.6B.100d'
# pretrained embeddings.
TEXT = data.Field(tokenize = 'spacy', include_lengths = True)

TEXT.build_vocab(train_data, vectors='glove.6B.100d')


# Build an RNN model with an Embedding layer.
class RNN(nn.Module):
    def __init__(self, ...):

        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        ...

# Initialize the embedding layer with the Glove embeddings from the
# vocabulary. Why are two steps needed???
model = RNN(...)
pretrained_embeddings = TEXT.vocab.vectors
model.embedding.weight.data.copy_(pretrained_embeddings)
</code></pre>

<p>Specifically:</p>

<ol>
<li>Why are the GloVe embeddings loaded in a <code>Field</code> in addition to the <code>Embedding</code>?</li>
<li>I thought the <code>Field</code> function <code>build_vocab()</code> just builds its vocabulary from the training data. How are the GloVe embeddings involved here during this step?</li>
</ol>

<p>Here are other StackOverflow questions that did <strong>not</strong> answer my questions:</p>

<p><a href=""https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings"">PyTorch / Gensim - How to load pre-trained word embeddings</a></p>

<p><a href=""https://stackoverflow.com/questions/50747947/embedding-in-pytorch"">Embedding in pytorch</a></p>

<p><a href=""https://stackoverflow.com/questions/50340016/pytorch-lstm-using-word-embeddings-instead-of-nn-embedding"">PyTorch LSTM - using word embeddings instead of nn.Embedding()</a></p>

<p>Thanks for any help.</p>
","python, machine-learning, pytorch, word-embedding","<p>When <code>torchtext</code> builds the vocabulary, it aligns the the token indices with the embedding. If your vocabulary doesn't have the same size and ordering as the pre-trained embeddings, the indices wouldn't be guaranteed to match, therefore you might look up incorrect embeddings. <code>build_vocab()</code> creates the vocabulary for your dataset with the corresponding embeddings and discards the rest of the embeddings, because those are unused.</p>

<p>The GloVe-6B embeddings includes a vocabulary of size 400K. For example the <a href=""https://pytorch.org/text/datasets.html#torchtext.datasets.IMDB"" rel=""noreferrer"">IMDB dataset</a> only uses about 120K of these, the other 280K are unused.</p>

<pre class=""lang-py prettyprint-override""><code>import torch
from torchtext import data, datasets, vocab

TEXT = data.Field(tokenize='spacy', include_lengths=True)
LABEL = data.LabelField()

train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)
TEXT.build_vocab(train_data, vectors='glove.6B.100d')

TEXT.vocab.vectors.size() # =&gt; torch.Size([121417, 100])

# For comparison the full GloVe
glove = vocab.GloVe(name=""6B"", dim=100)
glove.vectors.size() # =&gt; torch.Size([400000, 100])

# Embedding of the first token is not the same
torch.equal(TEXT.vocab.vectors[0], glove.vectors[0]) # =&gt; False

# Index of the word ""the""
TEXT.vocab.stoi[""the""] # =&gt; 2
glove.stoi[""the""] # =&gt; 0

# Same embedding when using the respective index of the same word
torch.equal(TEXT.vocab.vectors[2], glove.vectors[0]) # =&gt; True
</code></pre>

<p>After having built the vocabulary with its embeddings, the input sequences will be given in the tokenised version where each token is represented by its index. In the model you want to use the embedding of these, so you need to create the embedding layer, but with the embeddings of your vocabulary. The easiest and recommended way is <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained"" rel=""noreferrer""><code>nn.Embedding.from_pretrained</code></a>, which is essentially the same as the Keras version.</p>

<pre class=""lang-py prettyprint-override""><code>embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors)

# Or if you want to make it trainable
trainable_embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)
</code></pre>

<p>You didn't mention how the <code>embedding_matrix</code> is created in the Keras version, nor how the vocabulary is built such that it can be used with the <code>embedding_matrix</code>. If you do that by hand (or with any other utility), you don't need <code>torchtext</code> at all, and you can initialise the embeddings just like in Keras. <code>torchtext</code> is purely for convenience for common data related tasks.</p>
",9,6,6331,2020-06-09 20:28:05,https://stackoverflow.com/questions/62291303/pytorch-loading-word-vectors-into-field-vocabulary-vs-embedding-layer
"Word embedding with gensim and FastText, training on pretrained vectors","<p>I am trying to load the pretrained vec file of Facebook fasttext crawl-300d-2M.vec with the next code:</p>

<pre><code>from gensim.models.fasttext import load_facebook_model, load_facebook_vectors

model_facebook = load_facebook_vectors('fasttext/crawl-300d-2M.vec')
</code></pre>

<p>But it fails with the next error:</p>

<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>

<p>It is not possible to load this vector?</p>

<p>If it is possible, afterwards can I train it with my own sentences?</p>

<p>Thanks in advance.</p>

<p>Whole error trace:</p>

<pre><code>---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-181-f8262e0857b8&gt; in &lt;module&gt;
----&gt; 1 model_facebook = load_facebook_vectors('fasttext/crawl-300d-2M.vec')

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in load_facebook_vectors(path, encoding)
   1196 
   1197     """"""
-&gt; 1198     model_wrapper = _load_fasttext_format(path, encoding=encoding, full_model=False)
   1199     return model_wrapper.wv
   1200 

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in _load_fasttext_format(model_file, encoding, full_model)
   1220     """"""
   1221     with gensim.utils.open(model_file, 'rb') as fin:
-&gt; 1222         m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)
   1223 
   1224     model = FastText(

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in load(fin, encoding, full_model)
    339         model.update(dim=magic, ws=version)
    340 
--&gt; 341     raw_vocab, vocab_size, nwords, ntokens = _load_vocab(fin, new_format, encoding=encoding)
    342     model.update(raw_vocab=raw_vocab, vocab_size=vocab_size, nwords=nwords, ntokens=ntokens)
    343 

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _load_vocab(fin, new_format, encoding)
    192     # Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)
    193     if nlabels &gt; 0:
--&gt; 194         raise NotImplementedError(""Supervised fastText models are not supported"")
    195     logger.info(""loading %s words for fastText model from %s"", vocab_size, fin.name)
    196 

NotImplementedError: Supervised fastText models are not supported
</code></pre>
","python, gensim, word-embedding, fasttext","<p>I believe, but am not certain, that in this particular case you're getting this error because you're trying to load a set of just-plain vectors (which FastText projects tend to name as files ending <code>.vec</code>) with a method that's designed for use on the FastText-specific format that includes subword/model info.</p>

<p>As a result, it's misinterpreting the file's leading bytes as declaring the model as one using FastText's '-supervised' mode. (Gensim truly doesn't support such full models, in that less-common mode. But it could load the end-vectors from such a model, and in any case your file isn't truly from that mode.)</p>

<p>Released files that will work with <code>load_facebook_vectors()</code> typically end with <code>.bin</code>. See the docs for this method for more details:</p>

<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors</a></p>

<p>So, you could either:</p>

<ul>
<li><p>Supply an alternate <code>.bin</code>-named, Facebook-FastText-formatted set of vectors (with subword info) to this method. (From a quick look at their download options, I believe their file analogous to your 1st try would  be named <code>crawl-300d-2M-subword.bin</code> &amp; be about 7.24GB in size.)</p></li>
<li><p>Load the file you have, with just its full-word vectors, via:</p></li>
</ul>

<pre class=""lang-py prettyprint-override""><code>    from gensim.models import KeyedVectors
    model = KeyedVectors.load_word2vec_format('fasttext/crawl-300d-2M.vec', binary=False)
</code></pre>

<p>In this latter case, no FastText-specific features (like the synthesis of guess-vectors for out-of-vocabulary words using subword vectors) will be available - but that info isn't in the 'crawl-300d-2M.vec' file, anyway. (Those features would be available if you used the larger <code>.bin</code> file &amp; <code>.load_facebook_vectors()</code> method above.)</p>
",4,2,2021,2020-06-10 16:24:41,https://stackoverflow.com/questions/62308418/word-embedding-with-gensim-and-fasttext-training-on-pretrained-vectors
How to improve code to speed up word embedding with transformer models?,"<p>I need to compute words embeddings for a bunch of documents with different language models.
No problem with that, the script is doing fine, except I'm working on a notebook, without GPU and each text needs around 1.5s to be processed which is by far too long (I have thousands of texts to process).</p>

<p>Here is how I'm doing it with pytorch and transformers lib:</p>

<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import CamembertModel, CamembertTokenizer

docs = [text1, text2, ..., text20000]
tok = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertModel.from_pretrained('camembert-base', output_hidden_states=True)
# let try with a batch size of 64 documents
docids = [tok.encode(
  doc, max_length=512, return_tensors='pt', pad_to_max_length=True) for doc in docs[:64]]
ids=torch.cat(tuple(docids))
device = 'cuda' if torch.cuda.is_available() else 'cpu' # cpu in my case...
model = model.to(device)
ids = ids.to(device)
model.eval()
with torch.no_grad():
    out = model(input_ids=ids)
# 103s later...
</code></pre>

<p>Do someone has any idea or suggestions to improve speed?</p>
","nlp, pytorch, word-embedding, huggingface-transformers, bert-language-model","<p>I don't think that there is a trivial way to <em>significantly</em> improve the speed, without using a GPU.</p>

<p>Some of the ways I could think of include smart batching, which is used by <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">Sentence-Transformers</a>, where you basically sort inputs of similar length together, to avoid padding to the full 512 token limit. I'm not sure how much of a speedup this is going to get you, but the only way that you can improve it significantly in a short period of time.</p>

<p>Otherwise, if you have access to <a href=""https://colab.research.google.com/"" rel=""nofollow noreferrer"">Google colab</a>, you can also utilize their GPU environment, if the processing can be completed in reasonable time.</p>
",2,0,1849,2020-06-15 09:19:02,https://stackoverflow.com/questions/62385092/how-to-improve-code-to-speed-up-word-embedding-with-transformer-models
Why does &#39;dimension&#39; mean several different things in the machine-learning world?,"<p>I've noticed that AI community refers to various tensors as 512-d, meaning 512 dimensional tensor, where the term 'dimension' seems to mean 512 different float values in the representation for a single datapoint. e.g. in 512-d word-embeddings means 512 length vector of floats used to represent 1 english-word e.g. <a href=""https://medium.com/@jonathan_hui/nlp-word-embedding-glove-5e7f523999f6"" rel=""nofollow noreferrer"">https://medium.com/@jonathan_hui/nlp-word-embedding-glove-5e7f523999f6</a></p>

<p>But it isn't 512 different dimensions, it's only 1 dimensional vector? Why is the term <code>dimension</code> used in such a different manner than usual? </p>

<p>When we use the term <code>conv1d</code> or <code>conv2d</code> which are convolutions over 1-dimension and 2-dimensions, a dimension is used in the typical way it's used in math/sciences but in the word-embedding context, a 1-d vector is said to be a 512-d vector, or am I missing something?</p>

<p>Why is this overloaded use of the term <code>dimension</code>? What context determines what <code>dimension</code> means in machine-learning as the term seems overloaded?</p>
","tensorflow, deep-learning, neural-network, pytorch, word-embedding","<p>In the context of <em>word embeddings</em> in neural networks, <em>dimensionality reduction</em>, and many other machine learning areas, it is indeed correct to call the vector (which is typically, an 1D array or tensor) as <strong>n-dimensional</strong> where <strong><code>n</code></strong> is usually greater than 2. This is because we usually work in the <a href=""https://en.wikipedia.org/wiki/Euclidean_space"" rel=""nofollow noreferrer"">Euclidean space</a> where a (data) <em>point</em> in a certain dimensional (Euclidean) space is represented as an <a href=""https://en.wikipedia.org/wiki/Tuple"" rel=""nofollow noreferrer"">n-tuple</a> of real numbers (i.e. real n-space ℝ<sup>n</sup>).</p>

<p>Below is an example<sup>ref</sup> of a (data) point in a 3D (Euclidean) space. To represent any point in this space, say <code>d</code><sub>1</sub>, we need a tuple of three real numbers (<code>x</code><sub>1</sub>, <code>y</code><sub>1</sub>, <code>z</code><sub>1</sub>).</p>

<p><a href=""https://i.sstatic.net/qwHfe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qwHfe.png"" alt=""a point in 3D (Euclidean) space""></a></p>

<p>Now, your confusion arises why this point <code>d</code><sub>1</sub> is called as 3 dimensional instead of 1 dimensional array. The reason is because it <em>lies</em> or <em>lives</em> in this 3D space. The same argument can be extended to all points in any n-dimensional real space, as it is done in the case of embeddings with <code>300d</code>, <code>512d</code>, <code>1024d</code> vector etc. </p>

<p>However, in all <em>nD</em> array compute frameworks such as NumPy, PyTorch, TensorFlow etc, these are still 1D arrays because the <em>length</em> of the above said vectors can be represented using a single number. </p>

<p>But, what if you have more than 1 data point? Then, you have to stack them in some (unique) way. And this is where the need for a second dimension arises. So, let's say you stack 4 of these <code>512d</code> vectors vertically, then you'd end up with a 2D array/tensor of shape <code>(4, 512)</code>. Note that here we call the array as 2D because two integer numbers are required to represent the extent/length along each axis.</p>

<p>To understand this better, please refer my other answer on <a href=""https://stackoverflow.com/a/48177457"">axis parameter visualization</a> for <em>nD</em> arrays, the visual representation of which I will include it below.</p>

<p><a href=""https://i.sstatic.net/p2PGi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/p2PGi.png"" alt=""axis parameter visualization""></a></p>

<hr>

<p><sup>ref</sup>: <a href=""https://en.wikipedia.org/wiki/Euclidean_space"" rel=""nofollow noreferrer"">Euclidean space wiki</a></p>
",3,4,1828,2020-06-15 19:14:10,https://stackoverflow.com/questions/62395315/why-does-dimension-mean-several-different-things-in-the-machine-learning-world
Pad vectors in tf.keras for LSTM,"<p>Keras has a preprocessing util to <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"" rel=""nofollow noreferrer"">pad sequences</a>, but it assumes that the sequences are integer numbers.</p>

<p>My sequences are vectors (my own embeddings, I do not want to use Keras embeddings), is there any way in which I can pad them to use in a LSTM?</p>

<p>Sequences can be made equal in Python, but the padding methods in Keras provide additional metainformation for layers like LSTM to consider for <a href=""https://www.tensorflow.org/guide/keras/masking_and_padding"" rel=""nofollow noreferrer"">masking</a>.</p>
","keras, lstm, padding, tf.keras, word-embedding","<p>this is a possibility to pad an array of float of different length with zeros</p>

<p>to mask the zeros you can use the masking layer (otherwise remove it)</p>

<p>I initialize your embeddings in a list because numpy can't handle array of different lenght. in the example, I use 4 samples of different lengths. the relative embeddings are stored in this list list([1,300],[2,300],[3,300],[4,300]) </p>

<pre><code># recreate your embed
emb = []
for i in range(1,5):
    emb.append(np.random.uniform(0,1, (i,300)))

# custom padding function
def pad(x, max_len):
    new_x = np.zeros((max_len,x.shape[-1]))
    new_x[:len(x),:] = x # post padding
    return new_x

# pad own embeddings
emb = np.stack(list(map(lambda x: pad(x, max_len=100), emb)))

emb_model = tf.keras.Sequential()
emb_model.add(tf.keras.layers.Masking(mask_value=0., input_shape=(100, 300)))
emb_model.add(tf.keras.layers.LSTM(32))

emb_model(emb) 
</code></pre>
",1,2,795,2020-06-16 22:49:14,https://stackoverflow.com/questions/62418753/pad-vectors-in-tf-keras-for-lstm
Why pytorch transformer src_mask doesn&#39;t block positions from attending?,"<p>I am trying to train word embedding with transformer encoder by masking the word itself with diagonal src_mask:</p>
<pre><code>def _generate_square_subsequent_mask(self, sz):
    mask = torch.diag(torch.full((sz,),float('-inf')))
    return mask

def forward(self, src):

    if self.src_mask is None or self.src_mask.size(0) != len(src):
        device = src.device
        mask = self._generate_square_subsequent_mask(len(src)).to(device)
        self.src_mask = mask
    
    src = self.embedding(src) * math.sqrt(self.ninp)
    src = self.dropout(src)
    src = self.pos_encoder(src)
    src = self.transformer_encoder(src, self.src_mask)
    output = self.decoder(src) # Linear layer
    return output
</code></pre>
<p>After training the model predicts exactly the same sentence from the input. If I change any word in the input - it predict the new word. So the model doesn't block according to the mask.</p>
<p>Why is it ?</p>
<p>I understand that there is a mistake in my logic because BERT would probably be much simpler if it worked. But where am I wrong ?</p>
<p><strong>Edit:</strong></p>
<p>I am using the a sequence of word indices as input. Output is the same sequence as input.</p>
","pytorch, word-embedding, transformer-model","<p>As far as I understand - the model doesn't prevent each word to indirectly “see itself” in multylayer context. I tried to use one layer - it looks like the model works. But training is too slow.</p>
",1,1,1070,2020-06-20 11:30:27,https://stackoverflow.com/questions/62485231/why-pytorch-transformer-src-mask-doesnt-block-positions-from-attending
How to encode multiple sentences using transformers.BertTokenizer?,"<p>I would like to create a minibatch by encoding multiple sentences using transform.BertTokenizer. It seems working for a single sentence. How to make it work for several sentences?</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# tokenize a single sentence seems working
tokenizer.encode('this is the first sentence')
&gt;&gt;&gt; [2023, 2003, 1996, 2034, 6251]

# tokenize two sentences
tokenizer.encode(['this is the first sentence', 'another sentence'])
&gt;&gt;&gt; [100, 100] # expecting 7 tokens
</code></pre>
","word-embedding, huggingface-transformers, huggingface-tokenizers","<p><strong>transformers &gt;= 4.0.0</strong>:<br />
Use <code>__call__</code> method of the <a href=""https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase"" rel=""noreferrer"">tokenizer</a>. It will generate a dictionary which contains the <code>input_ids</code>, <code>token_type_ids</code> and the <code>attention_mask</code> as list for each input sentence:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer(['this is the first sentence', 'another setence'])
</code></pre>
<p>Output:</p>
<pre><code>{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}
</code></pre>
<p><strong>transformers &lt; 4.0.0</strong>:<br />
Use <code>tokenizer.batch_encode_plus</code> (<a href=""https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus"" rel=""noreferrer"">documentation</a>). It will generate a dictionary which contains the <code>input_ids</code>, <code>token_type_ids</code> and the <code>attention_mask</code> as list for each input sentence:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer.batch_encode_plus(['this is the first sentence', 'another setence'])
</code></pre>
<p>Output:</p>
<pre><code>{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}
</code></pre>
<p><strong>Applies to <strong>call</strong> and batch_encode_plus:</strong><br />
In case you only want to generate the input_ids, you have to set <code>return_token_type_ids</code> and <code>return_attention_mask</code> to False:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer.batch_encode_plus(['this is the first sentence', 'another setence'], return_token_type_ids=False, return_attention_mask=False)
</code></pre>
<p>Output:</p>
<pre><code>{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]]}
</code></pre>
",26,15,24101,2020-07-01 03:32:24,https://stackoverflow.com/questions/62669261/how-to-encode-multiple-sentences-using-transformers-berttokenizer
How to store Word vector Embeddings?,"<p>I am using BERT Word Embeddings for sentence classification task with 3 labels. I am using Google Colab for coding. My problem is, since I will have to execute the embedding part every time I restart the kernel, is there any way to save these word embeddings once it is generated? Because, it takes a lot of time to generate those embeddings.</p>
<p>The code I am using to generate BERT Word Embeddings is -</p>
<pre><code>[get_features(text_list[i]) for text_list[i] in text_list]
</code></pre>
<p>Here, gen_features is a function which returns word embedding for each i in my list text_list.</p>
<p>I read that converting embeddings into bumpy tensors and then using np.save can do it. But I actually don't know how to code it.</p>
","python-3.x, keras, nlp, word-embedding, bert-language-model","<p>You can save your embeddings data to a numpy file by following these steps:</p>
<pre><code>all_embeddings = here_is_your_function_return_all_data()
all_embeddings = np.array(all_embeddings)
np.save('embeddings.npy', all_embeddings)
</code></pre>
<p>If you're saving into google colab, then you can download it to your local computer. Whenever you need it, just upload it and load it.</p>
<pre><code>all_embeddings = np.load('embeddings.npy')
</code></pre>
<p>That's it.</p>
<p>Btw, You can also directly save your file to google drive.</p>
",14,9,18035,2020-07-03 07:51:33,https://stackoverflow.com/questions/62710872/how-to-store-word-vector-embeddings
Using Gensim Fasttext model with LSTM nn in keras,"<p>I have trained fasttext model with Gensim over the corpus of very short sentences (up to 10 words). I know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like &quot;Oxytocin&quot; &quot;Lexitocin&quot;, &quot;Ematrophin&quot;,'Betaxitocin&quot;</p>
<p>given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gram</p>
<p>How do i incorporate the fasttext model inside a LSTM keras network without losing the fasttext model to just a list of vectors in the vocab? because then I won't handle any OOV even when fasttext do it well.</p>
<p>Any idea?</p>
","tensorflow, keras, nlp, gensim, word-embedding","<p>here the procedure to incorporate the fasttext model inside an LSTM Keras network</p>
<pre><code># define dummy data and precproces them

docs = ['Well done',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent',
        'Weak',
        'Poor effort',
        'not good',
        'poor work',
        'Could have done better']

docs = [d.lower().split() for d in docs]

# train fasttext from gensim api

ft = FastText(size=10, window=2, min_count=1, seed=33)
ft.build_vocab(docs)
ft.train(docs, total_examples=ft.corpus_count, epochs=10)

# prepare text for keras neural network

max_len = 8

tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)
tokenizer.fit_on_texts(docs)

sequence_docs = tokenizer.texts_to_sequences(docs)
sequence_docs = tf.keras.preprocessing.sequence.pad_sequences(sequence_docs, maxlen=max_len)

# extract fasttext learned embedding and put them in a numpy array

embedding_matrix_ft = np.random.random((len(tokenizer.word_index) + 1, ft.vector_size))

pas = 0
for word,i in tokenizer.word_index.items():
    
    try:
        embedding_matrix_ft[i] = ft.wv[word]
    except:
        pas+=1

# define a keras model and load the pretrained fasttext weights matrix

inp = Input(shape=(max_len,))
emb = Embedding(len(tokenizer.word_index) + 1, ft.vector_size, 
                weights=[embedding_matrix_ft], trainable=False)(inp)
x = LSTM(32)(emb)
out = Dense(1)(x)

model = Model(inp, out)

model.predict(sequence_docs)
</code></pre>
<p>how to deal unseen text</p>
<pre><code>unseen_docs = ['asdcs work','good nxsqa zajxa']
unseen_docs = [d.lower().split() for d in unseen_docs]

sequence_unseen_docs = tokenizer.texts_to_sequences(unseen_docs)
sequence_unseen_docs = tf.keras.preprocessing.sequence.pad_sequences(sequence_unseen_docs, maxlen=max_len)

model.predict(sequence_unseen_docs)
</code></pre>
",7,6,4095,2020-07-05 16:39:07,https://stackoverflow.com/questions/62743531/using-gensim-fasttext-model-with-lstm-nn-in-keras
similarity score is way off using doc2vec embedding,"<p>I'm trying out document de-duplication on an <em>NY-Times</em> corpus that I've prepared very recently. It contains data related to financial fraud.</p>
<p>First, I convert the article snippets to a list of <code>TaggedDocument</code> objects.</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)

def create_tagged_doc(doc, nlp):        
    toks = nlp(doc)
    lemmatized_toks = [tok.lemma_ for tok in toks if not tok.is_stop]
    return lemmatized_toks

df_fraud = pd.read_csv('...local_path...')
df_fraud_list = df_fraud['snippet'].to_list()
documents = [TaggedDocument(create_tagged_doc(doc, nlp), [i]) for i, doc in enumerate(df_fraud_list)]
</code></pre>
<p>A sample <code>TaggedDocument</code> looks as follows:</p>
<pre><code>TaggedDocument(words=['Chicago', 'woman', 'fall', 'mortgage', 'payment', 
'victim', 'common', 'fraud', 'know', 'equity', 'strip', '.'], tags=[1])
</code></pre>
<p>Now I compile and train the Doc2Vec model.</p>
<pre><code>cores = multiprocessing.cpu_count()
model_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0, workers=cores)
model_dbow.build_vocab(documents)
model_dbow.train(documents, 
                total_examples=model_dbow.corpus_count, 
                epochs=model_dbow.epochs)
</code></pre>
<p>Let's define the cosine similarity:</p>
<pre><code>cosine_sim = lambda x, y: np.inner(x, y) / (norm(x) * norm(y))
</code></pre>
<p>Now, the trouble is, if I define two sentences which are almost similar and take their cosine similarity score, it's coming very low. E.g.</p>
<pre><code>a = model_dbow.infer_vector(create_tagged_doc('That was a fradulent transaction.', nlp))
b = model_dbow.infer_vector(create_tagged_doc('That transaction was fradulant.', nlp))

print(cosine_sim(a, b)) # 0.07102317
</code></pre>
<p>Just to make sure, I checked with exact same vector repeated, and it's proper.</p>
<pre><code>a = model_dbow.infer_vector(create_tagged_doc('That was a fradulent transaction.', nlp))
b = model_dbow.infer_vector(create_tagged_doc('That was a fradulent transaction.', nlp))

print(cosine_sim(a, b)) # 0.9980062
</code></pre>
<p>What's going wrong in here?</p>
","python, nlp, word-embedding, doc2vec","<p>Looks like it's an issue with number of epochs. When creating a Doc2Vec instance without specifying number of epochs, e.g. <code>model_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0, workers=cores)</code>, it's set to 5 by default. Apparently that wasn't sufficient for my corpus. I set the epochs to 50, and re-trained the model, and voila! It worked.</p>
",1,1,497,2020-07-08 16:59:39,https://stackoverflow.com/questions/62799856/similarity-score-is-way-off-using-doc2vec-embedding
PCA on word2vec embeddings using pre existing model,"<p>I have a word2vec model trained on Tweets. I also have a list of words, and I need to get the embeddings from the words, compute the first two principal components, and plot each word on a 2 dimensional space.</p>
<p>I'm trying to follow tutorials such as this one: <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/develop-word-embeddings-python-gensim/</a></p>
<p>However in all such tutorials, they create a model based on a random sentence they use and then calculate PCA on all the words in the model. I don't want to do that, I only want to calculate and plot specific words. How can I use the model that I already have, which has thousands of words, and compute the first two principal components for a set list of words I have (around 20)?</p>
<p>So like in the link above, they have &quot;model&quot; with only the words from the sentence they wrote. And then they do &quot;X = model[model.wv.vocab]&quot;, then &quot;pca.fit_transform(X)&quot;. If I were to copy this code, I would do a PCA on the huge model, which I don't want to do. I just want to extract the embeddings of some words from that model and then compute PCA on those few words. Hopefully this makes sense, thanks in advance. Please let me know if I need to clarify anything.</p>
","python, nlp, jupyter-notebook, pca, word-embedding","<p>Create a collection with the same structure (a dictionary) as</p>
<pre><code>model.wv.vocab,
</code></pre>
<p>fill it with your target words, and compute PCA.</p>
<p>You can do this using the following code:</p>
<pre><code>my_vocab = {}
for w in my_words:
    my_vocab[w] = model.vw.vocab[w]

X = model[my_vocab]
pca.fit_transform(X)
</code></pre>
",2,1,2115,2020-07-14 20:11:24,https://stackoverflow.com/questions/62903275/pca-on-word2vec-embeddings-using-pre-existing-model
Will the document vectors generated by Doc2Vec be similar to document vectors obtained through Word2Vec?,"<p>I came across few blog posts stating that, Document vectors can be generated not only by Doc2Vec, but also by averaging the word vectors obtained by running Word2vec algorithm.
In that case, would the vectors generated through both the Algorithms be the same?
Which would be the most efficient way to generate the Document vectors and Why?</p>
<p>Any reference links in this regard would be of great help!!</p>
<p>Thanks in Advance</p>
","nlp, word2vec, word-embedding, doc2vec","<p>Those are two different methods of creating a vector for a set-of-words.</p>
<p>The vectors will be in different positions, and of different quality.</p>
<p>Averaging is quite fast, especially if you've already got word-vectors. But it's a very simple approach that won't capture many shades of meaning – indeed it is completely oblivious to word ordering/relative proximities, and the act of averaging can tend to 'cancel out' contrasting meanings in the text.</p>
<p><code>Doc2Vec</code> instead trains vectors for full texts in a manner very similar to word-vectors (and often, alongside word-vectors). Essentially, a pretend-word that's assigned to the text 'floats' alonside the word-vector training, as if it were 'near' all the other word-training (for that one text). It's a slightly more sophisticated approach, but as it uses a very-similar algorithm (&amp; model-complexity) on the same data, results on many downstream evaluations are often similar.</p>
<p>To obtain summary text-vectors capturing more subtle shades of meaning, as implied by grammatical rules and more advanced language usage, can require yet-more-sophisticated methods, such as those employing larger deep networks.</p>
<p>There's no single most efficient approach, as all real uses depend a lot on the type, quantity, and quality of your texts, and your intended uses of the vectors.</p>
",0,1,191,2020-07-15 15:31:37,https://stackoverflow.com/questions/62918448/will-the-document-vectors-generated-by-doc2vec-be-similar-to-document-vectors-ob
Download pre-trained BERT model locally,"<p>I am using the SentenceTransformers library (here: <a href=""https://pypi.org/project/sentence-transformers/#pretrained-models"" rel=""noreferrer"">https://pypi.org/project/sentence-transformers/#pretrained-models</a>) for creating embeddings of sentences using the pretrained model <code>bert-base-nli-mean-tokens</code>. I have an application that will be deployed to a device that does not have internet access. How can I save this model locally so that when I call it, it loads the model locally, rather than attempting to download from the internet? As the library maintainers make clear, the method <code>SentenceTransformer</code> downloads the model from the internet (see here: <a href=""https://pypi.org/project/sentence-transformers/#pretrained-models"" rel=""noreferrer"">https://pypi.org/project/sentence-transformers/#pretrained-models</a>) and I cannot find a method for saving the model locally.</p>
","python-3.x, word-embedding","<p><a href=""https://huggingface.co/sentence-transformers/bert-base-nli-max-tokens?text=Paris%20is%20the%20%5BMASK%5D%20of%20France."" rel=""noreferrer"">Hugging face usage</a></p>
<p>You can download the models locally by using the Hugging Face transformer library method.</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;sentence-transformers/bert-base-nli-mean-tokens&quot;)
model = AutoModel.from_pretrained(&quot;sentence-transformers/bert-base-nli-mean-tokens&quot;)
tokenizer.save_pretrained('./local_directory/')
model.save_pretrained('./local_directory/')
</code></pre>
",6,11,18024,2020-07-20 18:55:18,https://stackoverflow.com/questions/63002081/download-pre-trained-bert-model-locally
How to concatenate Glove 100d embedding and 1d array which contains additional signal?,"<p>I new to NLP and trying out some text classification algorithms. I have 100d GloVe vector representing each entry as a list of embeddings. Also, I have NER feature of shape (2234,) which shows if there is named entity or not. Array with GloVe embeddings is of shape (2234, 100).</p>
<p>How to correctly concatenate these array so each row represents its word?</p>
<p>Sorry for not including reproducible example. Please, use variables of your choice to explain the concatenation procedure.</p>
<p>Using <code>np.concatenate</code> did not work as I have expected but i don't know how to deal with dimensionality of embeddings.</p>
","numpy, nlp, concatenation, stanford-nlp, word-embedding","<p>Just in case someone accidentally gets here. Use:</p>
<pre><code>my_arr.reshape(2234,1)
</code></pre>
<p>Don't be me:)</p>
",0,0,209,2020-07-21 14:34:39,https://stackoverflow.com/questions/63016888/how-to-concatenate-glove-100d-embedding-and-1d-array-which-contains-additional-s
extracting numpy value from tensorflow object during transformation,"<p>i am trying to get word embeddings using tensorflow, and i have created adjacent work lists using my corpus.</p>
<p>Number of unique words in my vocab are 8000 and number of adjacent word lists are around 1.6 million</p>
<p><a href=""https://i.sstatic.net/ufZmx.png"" rel=""nofollow noreferrer"">Word Lists sample photo</a></p>
<p>Since the data is very large i am trying to write the word lists in batches to TFRecords file.</p>
<pre><code>def save_tfrecords_wordlist(toprocess_word_lists, path ):    
    writer = tf.io.TFRecordWriter(path)

    for word_list in toprocess_word_lists:
        features=tf.train.Features(
            feature={
        'word_list_X': tf.train.Feature( bytes_list=tf.train.BytesList(value=[word_list[0].encode('utf-8')] )),
        'word_list_Y': tf.train.Feature( bytes_list=tf.train.BytesList(value=[word_list[1].encode('utf-8') ]))
                }
            )
        example = tf.train.Example(features = features)
        writer.write(example.SerializeToString())
    writer.close()
</code></pre>
<h2>defining batches</h2>
<pre><code>batches = [0,250000,500000,750000,1000000,1250000,1500000,1641790]

for i in range(len(batches) - 1 ):

    batches_start = batches[i]
    batches_end = batches[i + 1]
    print( str(batches_start) + &quot; -- &quot; + str(batches_end ))

    toprocess_word_lists = word_lists[batches_start:batches_end]
    save_tfrecords_wordlist( toprocess_word_lists, path +&quot;/TFRecords/data_&quot; + str(i) +&quot;.tfrecords&quot;)
</code></pre>
<p>##############################</p>
<pre><code>def _parse_function(example_proto):

  features = {&quot;word_list_X&quot;: tf.io.FixedLenFeature((), tf.string),
          &quot;word_list_Y&quot;: tf.io.FixedLenFeature((), tf.string)}
  parsed_features = tf.io.parse_single_example(example_proto, features)

  &quot;&quot;&quot;
  word_list_X  = parsed_features['word_list_X'].numpy()
  word_list_Y  = parsed_features['word_list_Y'].numpy()

  ## need help is getting the numpy values from parsed_features variable so that i can get the one hot encoding matrix     which can be directly sent to tensorflow for training

  sample word_list_X value is &lt;tf.Tensor: shape=(10,), dtype=string,   numpy=array([b'for', b'for', b'for', b'you', b'you', b'you', b'you', b'to',b'to', b'to'], dtype=object)&gt;
  sample word_list_Y value is &lt;tf.Tensor: shape=(10,), dtype=string, numpy=array([b'is', b'to', b'recommend', b'to', b'for', b'contact', b'is',b'contact', b'you', b'the'], dtype=object)&gt;)

  &quot;&quot;&quot;
  return parsed_features['word_list_X'],parsed_features['word_list_Y']

filenames = [ path + &quot;/JustEat_TFRecords/data.tfrecords&quot; ]
dataset = tf.data.TFRecordDataset(filenames)

dataset = dataset.map(_parse_function)
dataset = dataset.batch(10)

# Defining the size of the embedding
embed_size = 100

# Defining the neural network
inp = tf.keras.Input(shape=(7958,))
x = tf.keras.layers.Dense(units=embed_size, activation='linear')(inp)
x = tf.keras.layers.Dense(units=7958, activation='softmax')(x)

model =  tf.keras.Model(inputs=inp, outputs=x)
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')

# Optimizing the network weights
#model.fit( x=X, y=Y, batch_size=256,epochs= 100)
model.fit(dataset,epochs= 2)
</code></pre>
","python, tensorflow, word-embedding, tfrecord, tf.data.dataset","<h3>It appears that you can't call the .numpy() function from inside the mapping function (<a href=""https://github.com/tensorflow/tensorflow/issues/27811#issuecomment-483370929"" rel=""nofollow noreferrer"">1</a>, <a href=""https://github.com/tensorflow/tensorflow/issues/14732#issuecomment-347276740"" rel=""nofollow noreferrer"">2</a>) although i was able to manage by using the py_function from (<a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer"">doc</a>).</h3>
<p>On the example below i have <strong>mapped my parsed dataset</strong> to a function that <strong>converts my images</strong> to <code>np.uint8</code> in order to <strong>plot them</strong> using matplotlib.</p>
<pre class=""lang-py prettyprint-override""><code>records_path = data_directory+'TFRecords'+'/data_0.tfrecord'
# Create a dataset
dataset = tf.data.TFRecordDataset(filenames=records_path)
# Map our dataset to the parsing function 
parsed_dataset = dataset.map(parsing_fn)
converted_dataset = parsed_dataset.map(lambda image,label:
                                       tf.py_function(func=converting_function,
                                                      inp=[image,label],
                                                      Tout=[np.uint8,tf.int64]))

# Gets the iterator
iterator = tf.compat.v1.data.make_one_shot_iterator(converted_dataset) 

for i in range(5):
    image,label = iterator.get_next()
    plt.imshow(image)
    plt.show()
    print('label: ', label)
</code></pre>
<h3>Output:</h3>
<p><a href=""https://i.sstatic.net/6BY7f.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6BY7f.png"" alt=""enter image description here"" /></a></p>
<h3>Parsing Function:</h3>
<pre class=""lang-py prettyprint-override""><code>def parsing_fn(serialized):
    # Define a dict with the data-names and types we expect to
    # find in the TFRecords file.
    features = \
        {
            'image': tf.io.FixedLenFeature([], tf.string),
            'label': tf.io.FixedLenFeature([], tf.int64)            
        }

    # Parse the serialized data so we get a dict with our data.
    parsed_example = tf.io.parse_single_example(serialized=serialized,
                                             features=features)
    # Get the image as raw bytes.
    image_raw = parsed_example['image']

    # Decode the raw bytes so it becomes a tensor with type.
    image = tf.io.decode_jpeg(image_raw)
    
    # Get the label associated with the image.
    label = parsed_example['label']
    
    # The image and label are now correct TensorFlow types.
    return image, label
</code></pre>
<p>Related issue: <a href=""https://stackoverflow.com/questions/50538038/tf-data-dataset-mapmap-func-with-eager-mode"">TF.data.dataset.map(map_func) with Eager Mode</a></p>
<p>Update: Didn't actually checked out but tf.shape() seems also to be a promising alternative.</p>
",2,2,1201,2020-07-23 20:04:09,https://stackoverflow.com/questions/63062304/extracting-numpy-value-from-tensorflow-object-during-transformation
How to normalize word embeddings (word2vec),"<p>I have a pre trained Word2Vec model with embeddings. I need to normalize some embeddings to do analyses with the words. Is there a simple line (or block) of code to do this? I've been searching online but can't find a simple answer.</p>
","python, nlp, normalization, word2vec, word-embedding","<p>This will work fine with embeddings</p>
<pre><code>model.init_sims(replace=True)
</code></pre>
",1,1,4249,2020-07-24 20:46:59,https://stackoverflow.com/questions/63081245/how-to-normalize-word-embeddings-word2vec
How to interpret output from gensim&#39;s Word2vec most similar method and understand how it&#39;s coming up with the output values,"<p>I am trying to implement word2vec on a problem. I will briefly explain my problem statement:</p>
<p>I am dealing with clinical data. I want to predict the top N diseases given a set of symptoms.</p>
<pre><code>Patient1: ['fever', 'loss of appetite', 'cold', '#flu#']
Patient2: ['hair loss', 'blood pressure', '#thyroid']
Patient3: ['hair loss', 'blood pressure', '#flu]
..
..
Patient30000: ['vomiting', 'nausea', '#diarrohea']
</code></pre>
<p>Note:
1.words with #prefix are diagnosis and the rest are symptoms</p>
<ol start=""2"">
<li>My corpus doesn't have any sentences or paragraphs. It just contains symptom names and diagnosis for a patient</li>
</ol>
<p>Applying word2vec on this corpus, I am able to generate the top 10 diagnosis given a set of input symptoms. Now, I want to understand how that output is generated. I know it's cosine similarity by adding the input vectors but I am unable to validate this output. Or understand how to improve this.  Really want to understand what exactly is going on in the background which leads to these output.</p>
<p>Can anyone help me answer these questions or highlight what are the drawbacks/advantages of this approach</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Word2vec is going to give you n-dimensional vectors that represent each of the diseases based on their co-occurrence. This means that you are representing each of the symptoms as a vector.</p>
<p>One row -</p>
<pre><code>X = ['fever', 'loss of appetite']

X_onehot= [[0,0,0,1,0,0,0,0,0,0,0],
           [0,0,0,0,0,0,0,0,1,0,0]]

X_word2vec= [[0.002,0.25,-0.1,0.335,0.7264],
             [0.746,0.6463,0.0032,0.6301,0.223]]

Y = #flu
</code></pre>
<p>Now, you can represent each row in the data by taking the average of the word2vec such as -</p>
<pre><code>X_avg = [[0.374 ,0.44815, -0.0484, 0.48255, 0.4747]]
</code></pre>
<p>Now you have a 5 length feature vector and a class for each row in your dataset. Next, you can treat it like any other machine learning problem.</p>
<p>If you want to predict the disease then just use a classification model after train-test split. That way you can validate the data.</p>
<p><strong>Using cosine similarity to the word2vec vectors only yields similar symptoms. It will not let you build a disease recommendation model, because then you will be recommending a symptom based on other similar symptoms.</strong></p>
",1,0,217,2020-07-26 06:36:54,https://stackoverflow.com/questions/63096909/how-to-interpret-output-from-gensims-word2vec-most-similar-method-and-understan
Identifying personnal information from column description,"<p>I have a question about the identification of GDPR (General Data Protection Regulation) related sentences.
Is there a tool / method in Python, Java, ... that identifies whether a database column contains personnally identifiable information from its description only ?</p>
<p>We may think about using word embedding to get the &quot;most_similar&quot; or &quot;most_similar_cosmul&quot; words given a sentence and afterwards identifying keywords related to GDPR (biometric, personnal, id, photo...) but the results depend on the robustness of the word embedding model.</p>
<p>Thank you in advance,</p>
","java, python, nlp, privacy, word-embedding","<p>There is no such thing as &quot;personally identifiable information&quot; in GDPR. The term (from <a href=""https://gdpr-info.eu/art-4-gdpr/"" rel=""nofollow noreferrer"">GDPR article 4(1)</a>) is &quot;personal data&quot;, defined as:</p>
<blockquote>
<p>any information relating to an identified or identifiable natural person</p>
</blockquote>
<p>and it doesn't itself have to be identifying to qualify. What's an &quot;identifiable natural person&quot;? GDPR says:</p>
<blockquote>
<p>an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person</p>
</blockquote>
<p>The key thing that turns regular &quot;data&quot; into &quot;personal data&quot; here is that &quot;one or more factors&quot; phrase. A single field, such as a phone number, could reasonably be considered as uniquely identifying a person. By itself a postal code probably doesn't, but when combined with a street address and a first name, we'd be very close to being able to identify someone, and hence all other data would become &quot;personal&quot;. It's hard to evaluate whether a collection of fields is enough to uniquely identify someone or not – you might think that first name and city might not identify an individual, given &quot;John&quot; and &quot;London&quot;, but &quot;Esmerelda&quot; and &quot;Ulaanbaatar&quot; might be pretty easy to track down, and it's the &quot;worst case&quot; that counts.</p>
<p>For a simpler example: A colour value such as <code>#663399</code> <strong>by itself</strong> is just plain &quot;data&quot;, is not &quot;personal data&quot;, and is not subject to GDPR. That exact same value stored as &quot;favourite colour&quot; in a field in a table linking that data to a person <strong>is</strong> personal data. &quot;City&quot; in a table of cities is not personal data, but a &quot;city&quot; field in a user table is.</p>
<p>In short, you're not going to be able to do what you want. You can't tell whether a field is personal data or not from its name because you have insufficient context.</p>
",0,0,127,2020-07-28 08:48:17,https://stackoverflow.com/questions/63130327/identifying-personnal-information-from-column-description
Tensorflow 2 Glove could not broadcast input array Can&#39;t prepare the embedding matrix but not +1,"<p>I get a <code>ValueError: could not broadcast input array from shape (50) into shape (100)</code> preparing embedding matrix I have loaded glove and made the word to vec Found 400000 word vectors.</p>
<p>I did look at a bunch of similar questions but <strong>they all seem to deal with forgetting to add the +1 in the max number words</strong>, I think I have that covered but still have the issue. Any help deeply appreciated.</p>
<pre><code>num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)
</code></pre>
<p>I also tried</p>
<pre><code>num_words = min(MAX_NUM_WORDS, len(word2idx_inputs)) + 1
</code></pre>
<p><a href=""https://stackoverflow.com/questions/56880252/using-pre-trained-word-embeddings-in-a-keras-model"">Using pre-trained word embeddings in a keras model?</a></p>
<p>I also tried this one as well</p>
<p><a href=""https://stackoverflow.com/questions/47215195/keras-word-embeddings-glove-cant-prepare-the-embedding-matrix"">Keras word embeddings Glove: can&#39;t prepare the embedding matrix</a></p>
<p>but also was the +1 issue</p>
<p>FYI: Extreme newbie at this 1st time doing Seq to seq to due to the translating Tagalog into English</p>
<p><strong>The Error that is received</strong></p>
<pre><code>
Filling pre-trained embeddings...

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-16-acf0d8a4c4ca&gt; in &lt;module&gt;
     8     if embedding_vector is not None:
     9       # words not found in embedding index will be all zeros.
---&gt; 10       embedding_matrix[i] = embedding_vector
    11 
    12 # create embedding layer

ValueError: could not broadcast input array from shape (50) into shape (100)

</code></pre>
<p><strong>Code</strong></p>
<pre><code>
# prepare embedding matrix
print('Filling pre-trained embeddings...')
num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word2idx_inputs.items():
 if i &lt; MAX_NUM_WORDS:
   embedding_vector = word2vec.get(word)
   if embedding_vector is not None:
     # words not found in embedding index will be all zeros.
     embedding_matrix[i] = embedding_vector

# create embedding layer
embedding_layer = Embedding(
 num_words,
 EMBEDDING_DIM,
 weights=[embedding_matrix],
 input_length=max_len_input,
 # trainable=True
)

# create targets, since we cannot use sparse
# categorical cross entropy when we have sequences
decoder_targets_one_hot = np.zeros(
 (
   len(input_texts),
   max_len_target,
   num_words_output
 ),
 dtype='float32'
)

# assign the values
for i, d in enumerate(decoder_targets):
 for t, word in enumerate(d):
   if word != 0:
     decoder_targets_one_hot[i, t, word] = 1


</code></pre>
","python, tensorflow, keras, stanford-nlp, word-embedding","<p>check the EMBEDDING_DIM value ,probably pre-trained data have less limit,
as error shows shape(50) into shape(100).
So set EMBEDDING_DIM  =50.</p>
",1,0,581,2020-07-31 14:26:34,https://stackoverflow.com/questions/63193730/tensorflow-2-glove-could-not-broadcast-input-array-cant-prepare-the-embedding-m
how to fine tune spacys word vectors,"<p>I am predicting similarities of documents using the pre trained spacy word embeddings. Because I have a lot of domain specific words, I want to fine tune my vectors on a rather small data set containing my domain specific vocabulary.</p>
<p>My idea was to just train the spacy model again with my data. But since the word vectors in spacy are built-in, I am not sure how to do that. Is there a way to train the spacy model again with my data?</p>
<p>After some research, I found out, that I can train my own vectors using Gensim. There I would have to download a pre trained model for example the Google News dataset model and afterwards I could train it again with my data set. Is this the only way? Or is there a way to proceed with my spacy model?</p>
<p>Any help is greatly appreciated.</p>
","spacy, word-embedding","<p>update: the right term here was &quot;incremental training&quot; and thats not possible with the pre-trained spacy models.
It is however possible, to perform incremental training on a <code>gensim</code> model. I did that with the help of another pretrained vector set (i went with the <code>fasttext</code> model) and then I trained this <code>gensim</code> model trained with the <code>fasttext</code> vectors again with my own corpus. This worked pretty well</p>
",2,0,696,2020-08-06 13:03:03,https://stackoverflow.com/questions/63284211/how-to-fine-tune-spacys-word-vectors
Error with input shape in Keras while training CBOW model,"<p>I am training a continuous bowl of words model for word embeddings where each of the one-hot vectors has a shape is a column vector with the shape of (V, 1). I'm using a generator to generate training examples and labels based on the corpus, but I have an error with the input shape.</p>
<p>(Here V = 5778)</p>
<p>Here's my code:</p>
<pre><code>def windows(words, C):
    i = C
    while len(words) - i &gt; C:
        center = words[i]
        context_words = words[i-C:i] + words[i+1:i+C+1]
        i += 1
        yield context_words, center

def one_hot_rep(word, word_to_index, V):
    vec = np.zeros((V, 1))
    vec[word_to_index[word]] = 1
    return vec

def context_to_one_hot(words, word_to_index, V):
    arr = [one_hot_rep(w, word_to_index, V) for w in words]
    return np.mean(arr, axis=0)
</code></pre>
<pre><code>def get_training_examples(words, C, words_to_index, V):
    for context_words, center_word in windows(words, C):
        yield context_to_one_hot(context_words, words_to_index, V), one_hot_rep(center_word, words_to_index, V)
</code></pre>
<pre><code>V = len(vocab)
N = 50

w2i, i2w = build_dict(vocab)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=(V, )),
    keras.layers.Dense(units=N, activation='relu'),
    keras.layers.Dense(units=V, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit_generator(get_training_examples(data, 2, w2i, V), epochs=5, steps_per_epoch=20)
</code></pre>
<p><a href=""https://i.sstatic.net/huskt.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/huskt.jpg"" alt=""Error I'm getting"" /></a></p>
","numpy, keras, nlp, word-embedding","<p>I figured out what was causing the error. The model is expecting an input_shape = (None, V) where None holds the batch_size for Keras when the training starts but I was sending in arrays of shape (1, V) which when sent as a batch get an extra first dimension something like (128, 1, V) is being sent which clashes with that of the expected input_shape.</p>
",0,0,103,2020-08-12 08:23:32,https://stackoverflow.com/questions/63372648/error-with-input-shape-in-keras-while-training-cbow-model
Why word embedding technique works,"<p>I have look into some word embedding techniques, such as</p>
<ol>
<li>CBOW: from context to single word. Weight matrix produced used as embedding vector</li>
<li>Skip gram: from word to context (from what I see, its acutally word to word, assingle prediction is enough). Again Weight matrix produced used as embedding</li>
</ol>
<p>Introduction to these tools would always quote &quot;cosine similarity&quot;, which says words of similar meanning would convert to similar vector.</p>
<p>But these methods all based on the 'context', account only for words around a target word. I should say they are 'syntagmatic' rather than 'paradigmatic'. So why the close in distance in a sentence indicate close in meaning? I can think of many counter example that frequently occurs</p>
<ol>
<li>&quot;Have a good day&quot;. (good and day are vastly different, though close in distance).</li>
<li>&quot;toilet&quot; &quot;washroom&quot; (two words of similar meaning, but a sentence contains one would unlikely to contain another)</li>
</ol>
<p>Any possible explanation?</p>
","nlp, word2vec, word-embedding","<p>This sort of &quot;why&quot; isn't a great fit for StackOverflow, but some thoughts:</p>
<p>The essence of word2vec &amp; similar embedding models may be <strong>compression</strong>: the model is forced to predict neighbors using <strong>far less internal state</strong> than would be required to remember the entire training set. So it has to force similar words together, in similar areas of the parameter space, and force groups of words into various useful relative-relationships.</p>
<p>So, in your second example of 'toilet' and 'washroom', even though they rarely appear together, they do tend to appear around the same neighboring words. (They're synonyms in many usages.) The model tries to predict them both, to similar levels, when typical words surround them. And vice-versa: when they appear, the model should generally predict the same sorts of words nearby.</p>
<p>To achieve that, their vectors must be nudged quite close by the iterative training. The only way to get 'toilet' and 'washroom' to predict the same neighbors, through the shallow feed-forward network, is to corral their word-vectors to nearby places. (And further, to the extent they have slightly different shades of meaning – with 'toilet' more the device &amp; 'washroom' more the room – they'll still skew slightly apart from each other towards neighbors that are more 'objects' vs 'places'.)</p>
<p>Similarly, words that are formally antonyms, but easily stand-in for each-other in similar contexts, like 'hot' and 'cold', will be somewhat close to each other at the end of training. (And, their various nearer-synonyms will be clustered around them, as they tend to be used to describe similar nearby paradigmatically-warmer or -colder words.)</p>
<p>On the other hand, your example &quot;have a good day&quot; probably doesn't have a giant influence on either 'good' or 'day'. Both words' more unique (and thus <strong>predictively-useful</strong>) senses are more associated with other words. The word 'good' alone can appear everywhere, so has weak relationships everywhere, but still a strong relationship to other synonyms/antonyms on an evaluative (&quot;good or bad&quot;, &quot;likable or unlikable&quot;, &quot;preferred or disliked&quot;, etc) scale.</p>
<p>All those random/non-predictive instances tend to cancel-out as noise; the relationships that have <strong>some</strong> ability to predict nearby words, even slightly, eventually find <strong>some</strong> relative/nearby arrangement in the high-dimensional space, so as to help the model for some training examples.</p>
<p>Note that a word2vec model isn't necessarily an <strong>effective</strong> way to predict nearby words. It might never be good at that task. But the <strong>attempt</strong> to become good at neighboring-word prediction, with fewer free parameters than would allow a perfect-lookup against training data, forces the model to reflect underlying semantic or syntactic patterns in the data.</p>
<p>(Note also that some research shows that a larger <code>window</code> influences word-vectors to reflect more topical/domain similarity – &quot;these words are used about the same things, in the broad discourse about X&quot; – while a tiny <code>window</code> makes the word-vectors reflect a more syntactic/typical similarity - &quot;these words are drop-in replacements for each other, fitting the same role in a sentence&quot;. See for example Levy/Goldberg &quot;Dependency-Based Word Embeddings&quot;, around its Table 1.)</p>
",4,1,711,2020-08-12 15:03:41,https://stackoverflow.com/questions/63379360/why-word-embedding-technique-works
List of 2d Tensors to one 3d Tensor,"<p>I have a list of sentences as of word embeddings. So every sentence is a matrix in 16*300, so it is a 2d tensor. I want to connect them to a 3d tensor and use this 3d tensor as input for a CNN model. Unfortunately, I cannot get it into this 3d tensor.</p>
<p>In my opinion, at least connecting two of these 2d tensors to a smaller 3d tensor via tf.concat should work. Unfortunately, I get the following error message</p>
<pre><code>tf.concat(0, [Tweets_final.Text_M[0], Tweets_final.Text_M[1]])

ValueError: Shape (3, 16, 300) must have rank 0
</code></pre>
<p>If it works with two 2d tensors I would probably work with one loop</p>
<p>One of these 2d tensors in the list looks like this one:</p>
<pre><code>&lt;tf.Tensor: shape=(16, 300), dtype=float32, numpy= array([[-0.03571776,  0.07699937, -0.02208528, ...,  0.00873246,
    -0.05967658, -0.03735098],
   [-0.03044251,  0.050944  , -0.02236165, ..., -0.01745957,
     0.01311598,  0.01744673],
   [ 0.        ,  0.        ,  0.        , ...,  0.        ,
     0.        ,  0.        ],
   ...,
   [ 0.        ,  0.        ,  0.        , ...,  0.        ,
     0.        ,  0.        ],
   [ 0.        ,  0.        ,  0.        , ...,  0.        ,
     0.        ,  0.        ],
   [ 0.        ,  0.        ,  0.        , ...,  0.        ,
     0.        ,  0.        ]], dtype=float32)&gt;
</code></pre>
","python, tensorflow, tensor, conv-neural-network, word-embedding","<p>You can found the solution in the documentation:
<a href=""https://www.tensorflow.org/api_docs/python/tf/stack"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/stack</a></p>
<p><strong>tf.stack</strong>: Stacks a list of rank-R tensors into one rank-(R+1) tensor.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; x = tf.constant([1, 4])
&gt;&gt;&gt; y = tf.constant([2, 5])
&gt;&gt;&gt; z = tf.constant([3, 6])
&gt;&gt;&gt; tf.stack([x, y, z])
&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)&gt;
&gt;&gt;&gt; tf.stack([x, y, z], axis=1)
&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)&gt;
</code></pre>
",6,2,1998,2020-08-14 14:57:45,https://stackoverflow.com/questions/63415216/list-of-2d-tensors-to-one-3d-tensor
Dropout layer after embedding layer,"<pre><code>model = tf.keras.Sequential([
    tf.keras.layers.Embedding(1000, 16, input_length=20), 
    tf.keras.layers.Dropout(0.2),                           # &lt;- How does the dropout work?
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
</code></pre>
<p>I can understand when dropout is applied between Dense layers, which randomly drops and prevents the former layer neurons from updating parameters. I don't understand how dropout works after an <code>Embedding layer</code>.</p>
<p>Let's say the output shape of the <code>Embedding layer</code> is <code>(batch_size,20,16)</code> or simply <code>(20,16)</code> if we ignore the batch size. How is dropout applied to the embedding layer's output?</p>
<p>Randomly dropout rows or columns?</p>
","tensorflow, nlp, lstm, recurrent-neural-network, word-embedding","<p>The dropout layer drops the output of previous layers.<br />
It will randomly force previous outputs to 0.<br />
In your case, the output of your Embedding layer will be 3d tensor (size, 20, 16)</p>
<pre><code>import tensorflow as tf
import numpy as np
tf.random.set_seed(0)
layer = tf.keras.layers.Dropout(0.5)
data = np.arange(1,37).reshape(3, 3, 4).astype(np.float32)
data
</code></pre>
<p>Output</p>
<pre><code>array([[[ 1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12.]],

       [[13., 14., 15., 16.],
        [17., 18., 19., 20.],
        [21., 22., 23., 24.]],

       [[25., 26., 27., 28.],
        [29., 30., 31., 32.],
        [33., 34., 35., 36.]]], dtype=float32)
</code></pre>
<p>Code:</p>
<pre><code>outputs = layer(data, training=True)
outputs
</code></pre>
<p>Output:</p>
<pre><code>&lt;tf.Tensor: shape=(3, 3, 4), dtype=float32, numpy=
array([[[ 0.,  0.,  6.,  8.],
        [ 0., 12.,  0., 16.],
        [18.,  0., 22., 24.]],

       [[26.,  0.,  0., 32.],
        [34., 36., 38.,  0.],
        [ 0.,  0., 46., 48.]],

       [[50., 52., 54.,  0.],
        [ 0., 60.,  0.,  0.],
        [ 0.,  0.,  0., 72.]]], dtype=float32)&gt;
</code></pre>
<p>One way you should consider is SpatialDropout1d which will essentially drop the entire column.</p>
<pre><code>layer = tf.keras.layers.SpatialDropout1D(0.5)
outputs = layer(data, training=True)
</code></pre>
<p>Output:</p>
<pre><code>&lt;tf.Tensor: shape=(3, 3, 4), dtype=float32, numpy=
array([[[ 2.,  0.,  6.,  8.],
        [10.,  0., 14., 16.],
        [18.,  0., 22., 24.]],

       [[26., 28.,  0., 32.],
        [34., 36.,  0., 40.],
        [42., 44.,  0., 48.]],

       [[ 0.,  0., 54., 56.],
        [ 0.,  0., 62., 64.],
        [ 0.,  0., 70., 72.]]], dtype=float32)&gt;
</code></pre>
<p>I hope this clears your confusion.</p>
",4,1,1700,2020-08-21 01:03:56,https://stackoverflow.com/questions/63515122/dropout-layer-after-embedding-layer
Construct dataframe from pairwise Word Mover Distance score list,"<p>I'd like to run PCA analysis on a list of pairwise sentence distance (word mover distance) I had.  So far I've gotten a similarity score on each pair of sentences. Stored all the pairwise similarity scores in a list. My main question is:</p>
<p>How to construct a matrix that contains these similarity score with the original sentences' index? Currently, the list only contains each pair's score. Haven't found a way to map the scores back to the sentence itself yet.</p>
<p>My ideal dataframe looks like this:</p>
<pre><code>&gt;             Sentence1  Sentence2  Sentence3   
 Sentence1.     1          0.5        0.8
 Sentence2      0.5        1          0.4
 Sentence3      0.8        0.4        1
</code></pre>
<p>However, the similarity score list I have looks like this, without index:</p>
<blockquote>
<p>[0.5, 0.8, 0.4]</p>
</blockquote>
<p>How do I transform it to a dataframe that I can run PCA on? Thanks!</p>
<p>----steps I took to construct the pairwise similarity score</p>
<pre><code># Tokenize all sentences in a column
tokenized_sentences = [s.split() for s in df[col]]

# calculate distance between 2 responses using wmd
def find_similar_docs(sentence_1, sentence_2):
   distance = model.wv.wmdistance(sentence_1, sentence_2)
   return distance

# find response pairs
pairs_sentences = list(combinations(tokenized_sentences, 2))

# get all similiarity scores between sentences
list_of_sim = []
for sent_pair in pairs_sentences:
   sim_curr_pair = find_similar_docs(sent_pair[0], sent_pair[1])
   list_of_sim.append(sim_curr_pair)
</code></pre>
<p>It would be a lot easier if I have &quot;1&quot; instead of tokenized sentence ([&quot;I&quot;, &quot;open&quot;, &quot;communication&quot;, &quot;culture&quot;]) as index. :) So I'm a bit stuck here...</p>
","python, matrix, nlp, word-embedding","<p>Make a distance matrix with numpy, then convert to a pandas dataframe.</p>
<pre><code>import numpy as np
import pandas as pd

# calculate distance between 2 responses using wmd
def find_similar_docs(sentence_1, sentence_2):
    distance = model.wv.wmdistance(sentence_1, sentence_2)
    return distance
  
# create distance matrix
tokenized_sentences = [s.split() for s in df[col]]
l = len(tokenized_sentences)
distances = np.zeros((l, l))
for i in range(l):
    for j in range(l):
        distances[i, j] = find_similar_docs(tokenized_sentences[i], tokenized_sentences[j])

# make pandas dataframe
labels = ['sentence' + str(i + 1) for i in range(l)]
df = pd.DataFrame(data=distances, index=labels, columns=labels)
print(df)
</code></pre>
",1,1,209,2020-08-21 02:32:15,https://stackoverflow.com/questions/63515664/construct-dataframe-from-pairwise-word-mover-distance-score-list
How to get feature names for a glove vectors,"<p>Countvectorizer has feature names, like this.</p>
<pre><code>vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=15000)
vectorizer.fit(X_train['essay'].values) # fit has to happen only on train data

X_train_essay_bow = vectorizer.transform(X_train['essay'].values)
feature_names= vectorizer.get_feature_names()
</code></pre>
<p>What would be the feature names for a glove vector?</p>
<p>How to get those feature names?</p>
<pre><code>with open('glove_vectors', 'rb') as f:
    model = pickle.load(f)
    glove_words =  set(model.keys())
</code></pre>
<p>I have the glove vector file of 300 dimensions like the above shown.</p>
<p>What would be the name of the 300 dimensions of the glove vectors?</p>
","python, vector, nlp, stanford-nlp, word-embedding","<p>There is no name for the Glove features. The countvectorizer counts the occurrences of each token in each sentence. So the features have easily understandable names. The feature &quot;cat&quot; is the count in each sentence of the token &quot;cat&quot;.</p>
<p>For Glove Vectors, the strategy is totally different and there is no equivalent representation of the features. Glove vectors are embeddings of words in an abstract N-dimensional space.</p>
<p>The Glove vector for a token comes from passing the token as an input into a trained neural network, and taking the activations of an auto-encoding layer in the middle.</p>
<p>If you've ever trained a deep neural network, imagine choosing some hidden layer within. What is the feature_name for each node in that hidden layer? It's a meaningless question because the nodes aren't features; they exist to pass the activation to the next layer. The same is true of Glove vector features; they are the activation values of a hidden layer in a network.</p>
",2,2,422,2020-08-21 10:13:04,https://stackoverflow.com/questions/63520893/how-to-get-feature-names-for-a-glove-vectors
Construct word2vec (CBOW) training data from beginning of sentence,"<p>When constructing training data for CBOW, <a href=""https://arxiv.org/abs/1301.3781"" rel=""nofollow noreferrer"">Mikolov et al.</a> suggest using the word from the center of a context window. What is the &quot;best&quot; approach to capturing words at the beginning/end of a sentence (I put best in quotes because I'm sure this depends on the task). Implementations I see online do something like the this:</p>
<pre><code>for i in range(2, len(raw_text) - 2):
    context = [raw_text[i - 2], raw_text[i - 1],
               raw_text[i + 1], raw_text[i + 2]]
</code></pre>
<p>I see two issues arising from this approach.</p>
<ul>
<li><strong>Issue 1:</strong> The approach gives imbalanced focus to the middle of the sentence. For example, the first word of the sentence can only appear in 1 context window and will never appear as the target word. Compare this to the 4th word in the sentence which will appear in 4 context windows and will also be a target word. This will be an issue as some words appear frequently at the beginning of sentences (i.e. however, thus, etc.). Wouldn't this approach minimize their use?</li>
<li><strong>Issue 2:</strong> Sentences with 4 or fewer words are completely ignored, and the importance of short sentences is minimized. For example, a sentence with 5 words can only contribute one training sample while a sentence of length 8 will contribute 4 training samples.</li>
</ul>
<p>Can anyone offer insight as to how much these issues affect the results or any alternative approaches for constructing the training data? (I considered letting the first word be the target word and using the next N words as the context, but this creates issues of it's own).</p>
<p>Related question on Stack Exchange:
<a href=""https://datascience.stackexchange.com/questions/81249/construct-word2vec-cbow-training-data-from-beginning-of-sentence"">Construct word2vec (CBOW) training data from beginning of sentence</a></p>
","neural-network, nlp, text-mining, word2vec, word-embedding","<p>All actual implementations I've seen, going back to the original <code>word2vec.c</code> by Mikolov, tend to let every word take turns being the 'center target word', but truncate the context-window to whatever is available.</p>
<p>So for example, with a <code>window=5</code> (on both sides), and the 'center word' as the 1st word of a text, only the 5 following words are used. If the center word is the 2nd word, 1 word preceding, and 5 words following, will be used.</p>
<p>This is easy to implement and works fine in practice.</p>
<p>In CBOW mode, every center word is still part of the same same number of neural-network forward-propagations (roughly, prediction attempts), though words 'near the ends' participate as inputs slightly less often. But even then, they're subject to an incrementally larger update - such as when they're 1 of just 5 words, instead of 1 of just 10.</p>
<p>(In SG mode, words near-the-ends will both inputs and target-words slightly less often.)</p>
<p>Your example code – showing words without full context windows never being the center target – is not something I've seen, and I'd only expect that choice in a buggy/unsophisticated implementation.</p>
<p>So neither of your issues arise in common implementations, where texts are longer than 1 word long. (In even a text of 2 words, the 1st word will be predicted using a window of just the 2nd, and the 2nd will be predicted with a window of just the 1st.)</p>
<p>While the actual word-sampling does result in slightly-different treatment of words at either end, it's hard for me to imagine these slight differences in word-treatment making any difference in results, in appropriate training corpuses for word2vec – large &amp; varied with plentiful contrasting examples for all relevant words.</p>
<p>(Maybe it'd be an issue in some small or synthetic corpus, where some rare-but-important tokens only appear in leading- or ending-positions. But that's far from the usual use of word2vec.)</p>
<p>Note also that while some descriptions &amp; APIs describe the units of word2vec training as 'sentences', the algorithm really just works on 'lists of tokens'. Often each list-of-tokens will span paragraphs or documents. Sometimes they retain things like punctuation, including sentence-ending periods, as pseudo-words. Bleeding the windows across sentence-boundaries rarely hurts, and often help, as the cooccurrences of words leading out of one sentence and into the next may be just as instructive as the cooccurrences of words inside one sentence. So in common practice of many-sentence training text, even fewer 'near-the-ends' words have even a slightly-different sampling treatment that you may have thought.</p>
",2,0,582,2020-09-04 21:02:41,https://stackoverflow.com/questions/63747999/construct-word2vec-cbow-training-data-from-beginning-of-sentence
is word embedding in Keras a dimensionality reduction technique also?,"<p>I wanted to understand the purpose of <code>embedding_dim</code> vs using a one hot vector of the entire <code>vocab_size</code>, Is it a dimension reduction to the one hot vector from <code>vocab_size</code> dim to <code>embedding_dim</code> dimensions or is there any other utility intuitively? Also how should one decide the <code>embedding_dim</code> number?</p>
<p>Code -</p>
<pre><code>    vocab_size = 10000
    embedding_dim = 16
    max_length = 120
    
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(6, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
    model.summary()
</code></pre>
<p>O/P -</p>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
flatten (Flatten)            (None, 1920)              0         
_________________________________________________________________
dense (Dense)                (None, 6)                 11526     
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 171,533
Trainable params: 171,533
Non-trainable params: 0
_________________________________________________________________
</code></pre>
","tensorflow, keras, deep-learning, nlp, word-embedding","<p>When you have a small number of categorical features and less training data you have to use a one-hot encoding. If you have large training data and a large number of categorical features you have to use embeddings.</p>
<p><strong>Why were Embeddings developed?</strong><br />
If you have a large number of categorical features and you used one-hot encoding you will end up getting a huge sparse matrix with most of the elements as zero. This is not suitable for training ML models. Your data will suffer from the curse of dimensionality. With embeddings, you can essentially represent a large number of categorical features using a smaller dimension. Also, the output is a dense vector rather than a sparse vector.</p>
<p><strong>Drawbacks of embeddings:</strong></p>
<ul>
<li>Requires time to train</li>
<li>Requires a large amount of training data</li>
</ul>
<p><strong>Advantage</strong></p>
<ul>
<li>Embeddings can tell you about the semantics of items. It groups related items close together. This is not the case with one-hot encoding. One-hot encoding is just an orthogonal representation of an item in another dimension.</li>
</ul>
<p><strong>What size to select for embedding vector.</strong></p>
<pre><code>embedding_dimensions =  vocab_size ** 0.25
</code></pre>
<p>You can see <a href=""https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p><strong>Note:</strong> This is just a thumb rule. You can select embedding dimensions smaller or greater than this. The quality of word embedding increases with higher dimensionality. But after reaching some point, the marginal gain will diminish.</p>
",2,1,1027,2020-09-05 16:13:37,https://stackoverflow.com/questions/63756001/is-word-embedding-in-keras-a-dimensionality-reduction-technique-also
Is it possible to get output of embedding keras layer?,"<p>I want to use time as an input feature to my deep learning model. So I need to use Embedding layer to convert it to embedded vectors. SO I used:</p>
<pre><code>from keras.layers import Embedding
hours_input=Input(shape=(1,),name='hours_input')
hours_embedding=Embedding(24,64)hours_input
</code></pre>
<p>I need the output of Embedding layer (weights I mean). I used   hours_embedding.get_weights().
But I got an error:
get_weights() missing 1 required positional argument: 'self'
So, How can I get embedding weight matrix?</p>
","python, deep-learning, keras-layer, word-embedding","<p>Create your model First.</p>
<pre><code>hours_input=Input(shape=(1,),name='hours_input')
hours_embedding=Embedding(24,64)(hours_input)
model = keras.models.Model(inputs = hours_input, outputs = hours_embedding)
</code></pre>
<p>And then you can access:</p>
<pre><code>model.layers[1].get_weights()
</code></pre>
<p>Output:</p>
<pre><code>[array([[ 0.00782292, -0.03037642, -0.03229956, ..., -0.02188529,
         -0.02597365, -0.04166167],
        [-0.04877049, -0.03961046,  0.01000347, ...,  0.00204592,
          0.01949279, -0.00540505],
        [ 0.0323245 , -0.02847096, -0.0023482 , ...,  0.02859743,
         -0.04320076,  0.01578701],
        ...,
        [ 0.01989252,  0.00970422,  0.00193944, ...,  0.02689132,
         -0.00167314,  0.00353283],
        [ 0.01885528,  0.00589638, -0.03409225, ..., -0.00504225,
          0.01269731,  0.04380948],
        [-0.01756806, -0.00950485, -0.0189078 , ...,  0.023773  ,
         -0.00471363, -0.03708603]], dtype=float32)]
</code></pre>
",2,1,975,2020-09-09 13:33:30,https://stackoverflow.com/questions/63812826/is-it-possible-to-get-output-of-embedding-keras-layer
Sentence similarity using universal sentence encoder by passing threshold,"<p>I have a data which is having more than 1500 rows. Each row has a sentence. I am trying to find out the best method to find the most similar sentences among all. I have tried this <a href=""https://stackoverflow.com/questions/63718559/finding-most-similar-sentences-among-all-in-python/63719145?noredirect=1#comment112681466_63719145"">example</a> but the processing is so much slow that it took around 20 minutes for 1500 rows data.</p>
<p>I have used the code from my previous question and tried many types to improve the speed but it doesn't affect much. I came across <strong>universal sentence encoder</strong> using tensorflow which seems fast and having good accuracy. I am working on colab you can check it <a href=""https://colab.research.google.com/drive/1UjO0L7QctsCoA8z4rk_ugljt4pjMZWIe?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns

module_url = &quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot; #@param [&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;, &quot;https://tfhub.dev/google/universal-sentence-encoder-large/5&quot;, &quot;https://tfhub.dev/google/universal-sentence-encoder-lite/2&quot;]
model = hub.load(module_url)
print (&quot;module %s loaded&quot; % module_url)
def embed(input):
  return model(input)

df = pd.DataFrame(columns=[&quot;ID&quot;,&quot;DESCRIPTION&quot;], data=np.matrix([[10,&quot;Cancel ASN WMS Cancel ASN&quot;],
                                                                [11,&quot;MAXPREDO Validation is corect&quot;],
                                                                [12,&quot;Move to QC&quot;],
                                                                [13,&quot;Cancel ASN WMS Cancel ASN&quot;],
                                                                [14,&quot;MAXPREDO Validation is right&quot;],
                                                                [15,&quot;Verify files are sent every hours for this interface from Optima&quot;],
                                                                [16,&quot;MAXPREDO Validation are correct&quot;],
                                                                [17,&quot;Move to QC&quot;],
                                                                [18,&quot;Verify files are not sent&quot;]
                                                                ]))

message_embeddings = embed(messages)

for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):
  print(&quot;Message: {}&quot;.format(messages[i]))
  print(&quot;Embedding size: {}&quot;.format(len(message_embedding)))
  message_embedding_snippet = &quot;, &quot;.join(
      (str(x) for x in message_embedding[:3]))
  print(&quot;Embedding: [{}, ...]\n&quot;.format(message_embedding_snippet))
</code></pre>
<p><strong>What I am looking for</strong></p>
<p>I want an approach where I can pass a threshold example 0.90 data in all rows which are similar to each other above 0.90% should be returned as a result.</p>
<pre><code>Data Sample
ID    |   DESCRIPTION
-----------------------------
10    | Cancel ASN WMS Cancel ASN   
11    | MAXPREDO Validation is corect
12    | Move to QC  
13    | Cancel ASN WMS Cancel ASN   
14    | MAXPREDO Validation is right
15    | Verify files are sent every hours for this interface from Optima
16    | MAXPREDO Validation are correct
17    | Move to QC  
18    | Verify files are not sent 
</code></pre>
<p><strong>Expected result</strong></p>
<pre><code>Above data which are similar upto 0.90% should get as a result with ID

ID    |   DESCRIPTION
-----------------------------
10    | Cancel ASN WMS Cancel ASN
13    | Cancel ASN WMS Cancel ASN
11    | MAXPREDO Validation is corect  # even spelling is not correct
14    | MAXPREDO Validation is right
16    | MAXPREDO Validation are correct
12    | Move to QC  
17    | Move to QC 
</code></pre>
","python, tensorflow, word-embedding, tensorflow-hub, sentence-similarity","<p>There are multiple ways in which you can find similarity between two embedding vectors.
The most common is <code>cosine_similarity</code>.</p>
<p>Therefore the first thing you have to do is calculate the similarity matrix:</p>
<p><strong>Code:</strong></p>
<pre><code>message_embeddings = embed(list(df['DESCRIPTION']))
cos_sim = sklearn.metrics.pairwise.cosine_similarity(message_embeddings)
</code></pre>
<p>You get a <code>9*9</code> matrix with similarity value.
You can create a heatmap of this matrix to visualize it.</p>
<p><strong>Code:</strong></p>
<pre><code>def plot_similarity(labels, corr_matrix):
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr_matrix,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap=&quot;YlOrRd&quot;)
  g.set_xticklabels(labels, rotation=90)
  g.set_title(&quot;Semantic Textual Similarity&quot;)

plot_similarity(list(df['DESCRIPTION']), cos_sim)
</code></pre>
<p><strong>Output:</strong></p>
<p><a href=""https://i.sstatic.net/JVKXI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JVKXI.png"" alt=""Matrix"" /></a></p>
<p>The darker box means more similarity.</p>
<p>And finally, you iterate over this cos_sim matrix  to get all the similar sentence using threshold:</p>
<pre><code>threshold = 0.8
row_index = []
for i in range(cos_sim.shape[0]):
  if i in row_index:
    continue
  similar = [index for index in range(cos_sim.shape[1]) if (cos_sim[i][index] &gt; threshold)]
  if len(similar) &gt; 1:
    row_index += similar

sim_df = pd.DataFrame()
sim_df['ID'] = [df['ID'][i] for i in row_index]
sim_df['DESCRIPTION'] = [df['DESCRIPTION'][i] for i in row_index]
sim_df
</code></pre>
<p>The data frame looks like this.<br />
<strong>Output:</strong></p>
<p><a href=""https://i.sstatic.net/W4dMw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/W4dMw.png"" alt=""This"" /></a></p>
<p>There, are different methods with which you can generate the similarity matrix.
You can take a look at <a href=""https://scikit-learn.org/stable/modules/classes.html#pairwise-metrics"" rel=""nofollow noreferrer"">this</a> for more methods.</p>
",3,1,1594,2020-09-10 07:36:05,https://stackoverflow.com/questions/63824934/sentence-similarity-using-universal-sentence-encoder-by-passing-threshold
"In CBOW model, do we need to take Average at Hidden layer?","<p>I search and read some articles about CBOW. But seem to have difference between these articles.</p>
<p>As I understand:</p>
<ul>
<li>Input is a batch vector. And we will feed it to Hidden layer. So that we will get another <strong>batch vector H</strong> at Hidden layer.</li>
<li>In an <a href=""https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"" rel=""nofollow noreferrer"">article (part 2.2.1)</a>, they say that we will not use any Activation Function at Hidden layer, but we will <strong>take average on <em>batch vector H</em></strong> to get a single vector (not a batch anymore). Then we will feed this average vector to Output layer and apply Softmax on it.</li>
</ul>
<p><a href=""https://i.sstatic.net/x1Gsb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/x1Gsb.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>However, in this <a href=""https://www.coursera.org/lecture/probabilistic-models-in-nlp/training-a-cbow-model-forward-propagation-Vphwi"" rel=""nofollow noreferrer"">Coursera's video</a>, they <em>don't</em> take average on batch vector H. They just feed this batch vector H to Output layer and apply Softmax on batch Output vector. And then calculate Cost function on it.</li>
<li>And, in <a href=""https://www.coursera.org/lecture/probabilistic-models-in-nlp/training-a-cbow-model-forward-propagation-Vphwi"" rel=""nofollow noreferrer"">Coursera's video</a>, they say that we can use RelU as Activation function at Hidden layer. Is this a new method? Because I read many articles, but they always say that there is no Activation function at Hidden layer.</li>
</ul>
<p>Can you please help me to answer it?</p>
","machine-learning, nlp, word2vec, word-embedding","<p>In actual implementations – whose source code you can review – the set of context-word vectors are averaged together <em>before</em> being fed as the &quot;input&quot; to the neural-network.</p>
<p>Then, any back-propagated adjustments to the input are also applied to all the vectors contributing to that average.</p>
<p>(For example, in the original <code>word2vec.c</code> released with Google's original word2vec paper, you can see the tallying of vectors into <code>neu1</code>, then averaging via division by the context-window count <code>cw</code>, at:</p>
<p><a href=""https://github.com/tmikolov/word2vec/blob/master/word2vec.c#L444-L448"" rel=""nofollow noreferrer"">https://github.com/tmikolov/word2vec/blob/master/word2vec.c#L444-L448</a>
)</p>
",1,0,710,2020-09-24 20:31:18,https://stackoverflow.com/questions/64053985/in-cbow-model-do-we-need-to-take-average-at-hidden-layer
"Load fasttext quantized model (.ftz), and look up words","<p>I have a pretrained embeddings file, which was quantized, in .ftz format. I need it to look up words, find the nearest neighbours. But I fail to find any toolkits that can do that. FastText can load the embeddings file, yet not able to look up the nearest neighbour, Gensim can lookup the nearest neighbour, but not be able to load the model...</p>
<p>Or it's me not finding the right function?</p>
<p>Thank you!</p>
","python, gensim, word-embedding, fasttext","<p>FastText models come in two flavours:</p>
<ul>
<li><strong>unsupervised models</strong> that produce word embeddings and can find similar words. The native Facebook package does not support quantization for them.</li>
<li><strong>supervised models</strong> that are used for text classification and can be quantized natively, but generally do not produce meaningful word embeddings.</li>
</ul>
<p>To compress unsupervised models, I have created a package <a href=""https://github.com/avidale/compress-fasttext"" rel=""nofollow noreferrer"">compress-fasttext</a> which is a wrapper around Gensim that can reduce the size of unsupervised models by pruning and quantization. <a href=""https://towardsdatascience.com/compressing-unsupervised-fasttext-models-eb212e9919ca"" rel=""nofollow noreferrer"">This post</a> describes it in more details.</p>
<p>With this package, you can lookup similar words in small models as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import compress_fasttext
small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(
    'https://github.com/avidale/compress-fasttext/releases/download/v0.0.4/cc.en.300.compressed.bin'
)
print(small_model.most_similar('Python'))
# [('PHP', 0.5253), ('.NET', 0.5027), ('Java', 0.4897),  ... ]
</code></pre>
<p>Of course, it works only if the model has been compressed using the same package. You can compress your own unsupervised model this way:</p>
<pre><code>import compress_fasttext
from gensim.models.fasttext import load_facebook_model
big_model = load_facebook_model('path-to-original-model').wv
small_model = compress_fasttext.prune_ft_freq(big_model, pq=True)
small_model.save('path-to-new-model')
</code></pre>
",0,1,975,2020-09-25 15:39:22,https://stackoverflow.com/questions/64067272/load-fasttext-quantized-model-ftz-and-look-up-words
How to store Bag of Words or Embeddings in a Database,"<p>I would like to store vector features, like Bag-of-Words or Word-Embedding vectors of a large number of texts, in a dataset, stored in a SQL Database.
What're the data structures and the best practices to save and retrieve these features?</p>
","python, database, nlp, dataset, word-embedding","<p>Word vectors should generally be stored as BLOBs if possible. If not they can be stored as json arrays. Since the only reasonable operation for word vectors is to look them up by the word key the other details don't particularly matter.</p>
<p>For bag of words you would typically need three columns, this is what it would look like in sqlite.</p>
<pre><code>create table bow (
  doc_id int,
  word text,
  count int)
</code></pre>
<p>Where your document IDs come from somewhere else. If you need to you can make <code>(doc_id, word)</code> the key.</p>
<p>However, storing features like this in a SQL DB is generally not helpful. When you access word counts or word vectors you typically don't need a subset of them, you need them all at once, so the relational features of SQL aren't helpful.</p>
",2,4,6608,2020-09-29 13:29:30,https://stackoverflow.com/questions/64120659/how-to-store-bag-of-words-or-embeddings-in-a-database
text2vec word embeddings : compound some tokens but not all,"<p>I am using {text2vec} word embeddings to build a dictionary of similar terms pertaining to a certain semantic category.</p>
<p>Is it OK to compound some tokens in the corpus, but not all? For example, I want to calculate terms similar to “future generation” or “rising generation”, but these collocations occur as separate terms in the original corpus of course. I am wondering if it is bad practice to gsub &quot;rising generation&quot; --&gt; &quot;rising_generation&quot;, without compounding all other terms that occur frequently together such as “climate change.”</p>
<p>Thanks!</p>
","nlp, tokenize, word-embedding, text2vec","<p>Yes, it's fine. It may or may not work exactly the way you want but it's worth trying.</p>
<p>You might want to look at the code for <a href=""http://text2vec.org/collocations.html"" rel=""nofollow noreferrer"">collocations</a> in text2vec, which can automatically detect and join phrases for you. You can certainly join phrases on top of that if you want. In Gensim in Python I would use the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">Phrases</a> code for the same thing.</p>
<p>Given that training word vectors usually doesn't take too long, it's best to try different techniques and see which one works better for your goal.</p>
",0,1,179,2020-10-04 11:59:17,https://stackoverflow.com/questions/64194322/text2vec-word-embeddings-compound-some-tokens-but-not-all
Word2Vec- does the word embedding change?,"<p>just wanted to know if there are 2 sentences-</p>
<ol>
<li>The <em><strong>bank</strong></em> remains closed on public holidays</li>
<li>Don't go near the river <em><strong>bank</strong></em></li>
</ol>
<p>The word 'bank' will have different word embeddings or same? If we use word2vec or glove?</p>
","nlp, stanford-nlp, word2vec, word-embedding","<p>You can't meaningfully train a dense word embedding on just 2 texts. You'd need these, and dozens (or ideally hundreds) more examples of the use of <code>'bank'</code> in subtly-varying contexts to get a good word-vector for <code>'bank'</code>. (And that word-vector would only have meaning in comparison to other word-vectors for other well-sampled words in the same trained model.)</p>
<p>Let's assume you do have a large, diverse training corpus with many examples of <code>'bank'</code> in contexts. And you've trained a model, either word2vec or GLoVe on that corpus.</p>
<p>Then, imagine that corpus was changed so that there were relatively more contexts that included the 'river' sense. (Perhaps, a bunch of new texts are added that talk about nature, parks, boating, &amp; irrigation.) Then, you retrain your model, from scratch, on the new corpus.</p>
<p>In the new model, <code>'bank'</code> (and related words) will typically have been nudged to have more 'river bank'-like neighbors.</p>
<p>These words may be in totally different coordinates, overall, as each run includes enough randomness to change words' ending positions a lot. But their <em>relative neighborhoods</em> &amp; <em>relative directions</em> will tend to be of similar value from subsequent runs, and changes in the mix of examples will tend to nudge results in one direction or another.</p>
<p>This is the case for both GLoVe and word2vec: their end results will both be influenced by the relative preponderance of alternate word senses.</p>
<p>(That words have multiple contrasting meanings is generally referred to in the relevant literature as 'polysemy', so searches like [polysemy word-vectors] should turn up a lot more work related to your question.)</p>
",1,0,680,2020-10-08 11:25:20,https://stackoverflow.com/questions/64261521/word2vec-does-the-word-embedding-change
Extracting word features from BERT model,"<p>So as you know, we can extract BERT features of word in a sentence. My question is, can we also extract word features that are not included in a sentence? For example, bert features of single words such as &quot;dog&quot;, &quot;human&quot;, etc.</p>
","word-embedding, bert-language-model, latent-semantic-analysis","<p>The very first layer of BERT is a static embeddings table, so you can use it as any other embeddings table and embeddings for words (or more frequently subwords) that BERT uses input to the first self-attentive layer. The static embeddings are only comparable with each other, not with the standard contextual embeddings. If need them comparable embeddings, you can try passing single-word sentences to BERT, but note that this will be an embeddings of a single-word sentenece, not the  word in general.</p>
<p>However, BERT is a sentence-level model that is supposed to get embeddings of words in context. It is not designed for static word embeddings, and methods specifically designed for static word embeddings (such as FastText) would certainly get better results.</p>
",2,0,907,2020-10-15 05:55:08,https://stackoverflow.com/questions/64365639/extracting-word-features-from-bert-model
Is there a pretrained Gensim phrase model?,"<p>Is there a pretrained <code>Gensim</code>'s <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">Phrases</a> model? If not, would it be possible to reverse engineer and create a phrase model using a pretrained word embedding?</p>
<p>I am trying to use <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">GoogleNews-vectors-negative300.bin</a> with Gensim's <code>Word2Vec</code>. First, I need to map my words into phrases so that I can look up their vectors from the Google's pretrained embedding.</p>
<p>I search on the official Gensim's documentation but could not find any info. Thanks!</p>
","python, machine-learning, gensim, word-embedding, phrase","<p>I'm not aware of anyone sharing a <code>Phrases</code> model. Any such model would be very sensitive to the preprocessing/tokenization step, and the specific parameters, the creator used.</p>
<p>Other than the high-level algorithm description, I haven't seen Google's exact choices for tokenization/canonicalization/phrase-combination done to the data that fed into the <code>GoogleNews</code> 2013 word-vectors have been documented anywhere. Some guesses about preprocessing can be made by reviewing the tokens present, but I'm unaware of any code to apply similar choices to other text.</p>
<p>You could try to mimic their unigram tokenization, then speculatively combine strings of unigrams into ever-longer multigrams up to some maximum, check if those combinations are present, and when not present, revert to the unigrams (or largest combination present). This might be expensive if done naively, but be amenable to optimizations if really important - especially for some subset of the more-frequent words – as the <code>GoogleNews</code> set appears to obey the convention of listing words in descending frequency.</p>
<p>(In general, though it's a quick &amp; easy starting set of word-vectors, I think <code>GoogleNews</code> is a bit over-relied upon. It will lack words/phrases and new senses that have developed since 2013, and any meanings it does capture are determined by news articles in the years leading up to 2013... which may not match the dominant senses of words in other domains. If your domain isn't specifically news, and you have sufficient data, deciding your own domain-specific tokenization/combination will likely perform better.)</p>
",1,1,405,2020-10-15 18:43:57,https://stackoverflow.com/questions/64377890/is-there-a-pretrained-gensim-phrase-model
How to evaluate word embeddings quality using AvgSimC and MaxSimC,"<p>I am working in a project of topical word embeddings, where I need to evaluate the quality of word embedidngs based on multi-sense of a word. I have seen in some research papers using  AvgSimC and  MaxSimC. As per my understanding, sense of a word predict by considering context words using these two methods.  Unfortunately I didn't get the clear implementation concepts and source code for these tow methods.</p>
<p>Source code (python or c) of implementation AvgSimC and  MaxSimC using SCWS data set and any kinds of documentation/tutorial  or any references will be more appreciated.</p>
<p>Thank you for your valuable time.</p>
","python, similarity, word-embedding, topic-modeling","<p>For two word vectors word1 and word2 in python</p>
<pre><code>   def AvgSimC(word1, word2):
       cosine_similarity = 1 - spatial.distance.cosine(word1, word1)
       return np.mean(cosine_similarity)

   def MaxSimC(word1, word2):
       cosine_similarity = 1 - spatial.distance.cosine(word1, word1)
       return np.max(cosine_similarity)
</code></pre>
",1,0,276,2020-10-21 13:25:36,https://stackoverflow.com/questions/64464540/how-to-evaluate-word-embeddings-quality-using-avgsimc-and-maxsimc
Sentence embedding using T5,"<p>I would like to use state-of-the-art LM T5 to get sentence embedding vector.
I found this repository <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">https://github.com/UKPLab/sentence-transformers</a>
As I know, in BERT I should take the first token as [CLS] token, and it will be the sentence embedding.
In this repository I see the same behaviour on T5 model:</p>
<pre><code>cls_tokens = output_tokens[:, 0, :]  # CLS token is first token
</code></pre>
<p>Does this behaviour correct? I have taken encoder from T5 and encoded two phrases with it:</p>
<pre><code>&quot;I live in the kindergarden&quot;
&quot;Yes, I live in the kindergarden&quot;
</code></pre>
<p>The cosine similarity between them was only &quot;0.2420&quot;.</p>
<p>I just need to understand how sentence embedding works - should I train network to find similarity to reach correct results? Or I it is enough of base pretrained language model?</p>
","python, nlp, pytorch, word-embedding","<p>In order to obtain the sentence embedding from the T5, you need to take the take the <code>last_hidden_state</code> from the T5 encoder output:</p>
<pre><code>model.encoder(input_ids=s, attention_mask=attn, return_dict=True)
pooled_sentence = output.last_hidden_state # shape is [batch_size, seq_len, hidden_size]
# pooled_sentence will represent the embeddings for each word in the sentence
# you need to sum/average the pooled_sentence
pooled_sentence = torch.mean(pooled_sentence, dim=1)
</code></pre>
<p>You have now a sentence embeddings from T5</p>
",7,6,6989,2020-10-28 18:35:23,https://stackoverflow.com/questions/64579258/sentence-embedding-using-t5
How to get vectors for each document using Google News Word2Vec,"<p>I am trying out Google's word2vec pre-trained model to get word embeddings. I am able to load the model in my code and I can see that I get a 300-dimensional representation of a word. Here is the code -</p>
<pre><code>import gensim
from gensim import models
from gensim.models import Word2Vec
model = gensim.models.KeyedVectors.load_word2vec_format('/Downloads/GoogleNews-vectors-negative300.bin', binary=True)
dog = model['dog']
print(dog.shape)
</code></pre>
<p>which gives me below output -</p>
<pre><code>&gt;&gt;&gt; print(dog.shape)
(300,)
</code></pre>
<p>This works but I am interested in obtaining a vector representation for entire document and not just one word. How can I do it using word2vec model ?</p>
<pre><code>dog_sentence = model['it is a cute little dog']
KeyError: &quot;word 'it is a cute little dog' not in vocabulary&quot;
</code></pre>
<p>I plan to apply these on many documents and then train a clustering model on topic of it to do unsupervised learning and topic modeling.</p>
","python, word2vec, word-embedding","<p>That's a set of word-vectors. There's no single canonical way to turn word-vectors into vectors for longer runs of text, like sentences or documents.</p>
<p>You can try simply averaging the word-vectors for each word in the text. (To do this, you wouldn't pass the whole string text, but break it into words, look up each word-vector, then average all those vectors.)</p>
<p>That's quick and simple to calculate, and works OK as a baseline for some tasks, especially topical-analyses on very-short texts. But as it takes no account of grammar/word-order, and dilutes all words with all others, it's often outperformed by more sophisticated analyses.</p>
<p>Note also: that set of word-vectors was calculated by Google around 2013, from news articles. It will miss words and word-senses that have arisen since then, and its vectors will be flavored by the way news-articles are written - very different from other domains of language. If you have enough data, training your own word-vectors, on your own domain's texts, may outperform them in both word-coverage and vector-relevance.</p>
",0,1,2328,2020-11-02 18:01:14,https://stackoverflow.com/questions/64650845/how-to-get-vectors-for-each-document-using-google-news-word2vec
Embedding layer in neural machine translation with attention,"<p>I am trying to understanding how to implement a seq-to-seq model with attention from this <a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html?highlight=attention"" rel=""nofollow noreferrer"">website</a>.</p>
<p>My question: Is nn.embedding just returns some IDs for each word, so the embedding for each word would be the same during whole training? Or are they getting changed during the procedure of training?</p>
<p>My second question is because I am confused whether after training, the output of nn.embedding is something such as word2vec word embeddings or not.</p>
<p>Thanks in advance</p>
","pytorch, recurrent-neural-network, word-embedding, attention-model, sequence-to-sequence","<p>According to the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"" rel=""nofollow noreferrer"">PyTorch docs</a>:</p>
<blockquote>
<p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</p>
</blockquote>
<p>In short, <code>nn.Embedding</code> embeds a sequence of vocabulary indices into a new embedding space. You can indeed roughly understand this as a word2vec style mechanism.</p>
<p>As a dummy example, let's create an embedding layer that takes as input a total of 10 vocabularies (i.e. the input data only contains a total of 10 unique tokens), and returns embedded word vectors living in 5-dimensional space. In other words, each word is represented as 5-dimensional vectors. The dummy data is a sequence of 3 words with indices 1, 2, and 3, in that order.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; embedding = nn.Embedding(10, 5)
&gt;&gt;&gt; embedding(torch.tensor([1, 2, 3]))
tensor([[-0.7077, -1.0708, -0.9729,  0.5726,  1.0309],
        [ 0.2056, -1.3278,  0.6368, -1.9261,  1.0972],
        [ 0.8409, -0.5524, -0.1357,  0.6838,  3.0991]],
       grad_fn=&lt;EmbeddingBackward&gt;)
</code></pre>
<p>You can see that each of the three words are now represented as 5-dimensional vectors. We also see that there is a <code>grad_fn</code> function, which means that the weights of this layer will be adjusted through backprop. This answers your question of whether embedding layers are trainable: the answer is yes. And indeed this is the whole point of embedding: we expect the embedding layer to learn meaningful representations, the famous example of <code>king - man = queen</code> being the classic example of what these embedding layers can learn.</p>
<hr />
<p><strong>Edit</strong></p>
<p>The embedding layer is, as the documentation states, a simple lookup table from a matrix. You can see this by doing</p>
<pre><code>&gt;&gt;&gt; embedding.weight
Parameter containing:
tensor([[-1.1728, -0.1023,  0.2489, -1.6098,  1.0426],
        [-0.7077, -1.0708, -0.9729,  0.5726,  1.0309],
        [ 0.2056, -1.3278,  0.6368, -1.9261,  1.0972],
        [ 0.8409, -0.5524, -0.1357,  0.6838,  3.0991],
        [-0.4569, -1.9014, -0.0758, -0.6069, -1.2985],
        [ 0.4545,  0.3246, -0.7277,  0.7236, -0.8096],
        [ 1.2569,  1.2437, -1.0229, -0.2101, -0.2963],
        [-0.3394, -0.8099,  1.4016, -0.8018,  0.0156],
        [ 0.3253, -0.1863,  0.5746, -0.0672,  0.7865],
        [ 0.0176,  0.7090, -0.7630, -0.6564,  1.5690]], requires_grad=True)
</code></pre>
<p>You will see that the first, second, and third rows of this matrix corresponds to the result that was returned in the example above. In other words, for a vocabulary whose index is <code>n</code>, the embedding layer will simply &quot;lookup&quot; the <code>n</code>th row in its weights matrix and return that row vector; hence the lookup table.</p>
",4,1,932,2020-11-04 06:37:03,https://stackoverflow.com/questions/64675228/embedding-layer-in-neural-machine-translation-with-attention
Open source pre-trained models for taxonomy/general word classification,"<p>Given any two words I'd like to understand if there's some sort of taxonomy/semantic field based relationship. For example given the words &quot;Dog&quot; and &quot;Cat&quot; I'd like to have a model which can return words in which &quot;Dog&quot; and &quot;Cat&quot; match, for example some words that this model would return in this case could be &quot;Animal&quot;, &quot;Mammal&quot;, &quot;Pet&quot; etcetera.</p>
<p>Is there an open source pre-trained model that can do this out of the box requiring no training dataset beforehand?</p>
","machine-learning, nlp, classification, word-embedding","<p>Sounds like <strong>WordNet</strong> would be a good fit for this task. WordNet is a lexical database that organises words in a hierarchical tree structure, like a taxonomy, and contains additional semantic information for many words. See e.g. <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=cat&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h="" rel=""nofollow noreferrer"">WordNet for &quot;cat&quot; here</a> for a browser-based demo. A word that's one hierarchy level above another word is a so called 'hypernym'. The hypernym for cat is e.g. 'feline'. With <a href=""https://www.nltk.org/howto/wordnet.html"" rel=""nofollow noreferrer"">WordNet in NLTK</a> you can get the hypernyms of two words until you get the same hypernym.</p>
<p>For 'cat' and 'dog' the common hypernym is 'animal'. See example code here:</p>
<pre><code>from nltk.corpus import wordnet as wn

wn.synsets('cat')
# output: [Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'),  Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), ...]
wn.synset('cat.n.01').hypernyms()
# output: [Synset('feline.n.01')]
wn.synset('feline.n.01').hypernyms()
wn.synset('carnivore.n.01').hypernyms()
wn.synset('placental.n.01').hypernyms()
wn.synset('mammal.n.01').hypernyms()
wn.synset('vertebrate.n.01').hypernyms()
wn.synset('chordate.n.01').hypernyms()
# output: 'animal'

wn.synsets('dog')
# output: [Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('pawl.n.01'), Synset('chase.v.01')]
wn.synset('dog.n.01').hypernyms()
wn.synset('domestic_animal.n.01').hypernyms()
# output: 'animal'
</code></pre>
<p>You ask for a <strong>machine learning</strong> solution in your question. A classical approach would be <strong>word vectors via <a href=""https://github.com/RaRe-Technologies/gensim"" rel=""nofollow noreferrer"">Gensim</a></strong>, but they will not give you a clear common category based on a database created by experts (like WordNet), but just give you words that often occur next to your target words (&quot;cat&quot;, &quot;dog&quot;) in the training data. I think that machine learning is not necessarily the best tool here.
See example:</p>
<pre><code>import gensim.downloader as api

model_glove = api.load(&quot;glove-wiki-gigaword-100&quot;)

model_glove.most_similar(positive=[&quot;dog&quot;, &quot;cat&quot;], negative=None, topn=10)

# output: [('dogs', 0.7998143434524536),
 ('pet', 0.7550237774848938),
 ('puppy', 0.7239114046096802),
 ('rabbit', 0.7165164351463318),
 ('cats', 0.7114559412002563),
 ('monkey', 0.6967265605926514),
 ('horse', 0.6890867948532104),
 ('animal', 0.6713783740997314),
 ('mouse', 0.6644925475120544),
 ('boy', 0.6607726812362671)]
</code></pre>
",1,0,103,2020-11-05 19:57:18,https://stackoverflow.com/questions/64704461/open-source-pre-trained-models-for-taxonomy-general-word-classification
Conceptnet Numberbatch (multilingual) OOV words,"<p>I'm working on a text classification problem (on a French corpus) and I'm experimenting with different Word Embeddings. I was very interested in what ConceptNet has to offer so I decided to give it a shot.</p>
<p>I wasn't able to find a dedicated tutorial for my particular task, so I took the advice from their <a href=""http://blog.conceptnet.io/word-embeddings/"" rel=""nofollow noreferrer"">blog</a>:</p>
<blockquote>
<p>How do I use ConceptNet Numberbatch?</p>
<p>To make it as straightforward as possible:</p>
<p>Work through any tutorial on machine learning for NLP that uses
semantic vectors. Get to the part where they tell you to use word2vec.
(A particularly enlightened tutorial may tell you to use GloVe 1.2.)</p>
<p>Get the ConceptNet Numberbatch data, and use it instead. Get better
results that also generalize to other languages.</p>
</blockquote>
<p>Below you may find my approach (note that 'numberbatch.txt' is the file containing the recommended multilingual version: ConceptNet Numberbatch 19.08):</p>
<pre><code>embeddings_index = dict()

f = open('numberbatch.txt')

for line in f:
    values = line.split()
    word = values[0]
    coefs = asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Loaded %s word vectors.' % len(embeddings_index))
</code></pre>
<p>I started by testing whether a word exists:</p>
<pre><code>word = 'fille'
missingWords = 0
if word not in embeddings_index:
    missingWords += 1
print(missingWords)
</code></pre>
<p>I found surprising that a simple word like '<em>fille</em>' (girl in French) is not found. I then created a function for printing all the OOV words from my corpus. I was even more surprised when analyzing the results: over 22k of the words weren't found (including words such as '<em>nous</em>'(we), '<em>être</em>'(to be), etc.).</p>
<p>I also tried the approach proposed on the <a href=""https://github.com/commonsense/conceptnet-numberbatch"" rel=""nofollow noreferrer"">GitHub page</a> for the OOV words (with the same result):</p>
<blockquote>
<p>Out-of-vocabulary strategy</p>
<p>ConceptNet Numberbatch is evaluated with an out-of-vocabulary strategy
that helps its performance in the  presence of unfamiliar words. The
strategy is implemented in the ConceptNet code base.  It can be
summarized as follows:</p>
<p>Given an unknown word whose language is not English, try looking up
the equivalently-spelled word in the  English embeddings (because
English words tend to end up in text of all languages).</p>
<p>Given an
unknown word, remove a letter from the end, and see if that is a
prefix of known words. If so,  average the embeddings of those known
words.</p>
<p>If the prefix is still unknown, continue removing letters from
the end until a known prefix is found.  Give up when a single
character remains.</p>
</blockquote>
<p>Am I doing something wrong in my approach?</p>
","python, word-embedding, conceptnet","<p>Are you taking into account ConceptNet Numberbatch's format? As shown in the <a href=""https://github.com/commonsense/conceptnet-numberbatch#downloads"" rel=""nofollow noreferrer"">project's GitHub</a>, it looks like this:</p>
<blockquote>
<p>/c/en/absolute_value -0.0847 -0.1316 -0.0800 -0.0708 -0.2514 -0.1687 -...</p>
<p>/c/en/absolute_zero 0.0056 -0.0051 0.0332 -0.1525 -0.0955 -0.0902 0.07...</p>
</blockquote>
<p>This format means that <code>fille</code> will not be found, but <code>/c/fr/fille</code> will.</p>
",3,3,1072,2020-11-06 15:14:33,https://stackoverflow.com/questions/64717185/conceptnet-numberbatch-multilingual-oov-words
Can the gensim pretrained models be used for doc2vec models?,"<p>I am trying to load a pretrained model <a href=""https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html#sphx-glr-auto-examples-howtos-run-downloader-api-py"" rel=""nofollow noreferrer"">listed here</a> to test the similarity of a handful of paragraphs.</p>
<p>Can gensim's pretrained models only be used with word-level vectors, or can the models also be used for document-length vectors?</p>
","python, gensim, word-embedding, doc2vec","<p>Most of the models currently listed there (as of 2020-11-21) are just sets of word-vectors - allowing lookup of vectors, by individual word, but not the full algorithmic model that would allow for followup training. (The only exception I see is the FastText model, which *might8 be a full FastText model, I'm not sure. But even there, the model only reports word-vectors for known words, or synthesizes a vector for out-of-vocabulary words - with no native method of creating vectors for larger texts.)</p>
<p>From any set of word-vectors, there are some crude ways to either create a simple vector for larger texts (such as averaging all the word-vectors for the words of the text together), or do other comparisons between sets of words using the word-vectors to influence the similarity (such as the &quot;<a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html"" rel=""nofollow noreferrer"">Word Mover's Distance</a>&quot; algorithm, available on Gensim word-vector sets as <code>wmdistance()</code>.)</p>
<p>But none of those models availabe via the <code>gensim.downloader</code> utility are for algorithms that inherently create vectors for larger texts (such as <code>Doc2Vec</code>).</p>
<p>(Separately: I would strongly recommend downloading models explicitly, as data, from their original locations, rather than using the <code>gensim.downloader</code> utility. It obscures key aspects of the process, including running extra 'shim' code for each dataset that is downloaded outside of normal code-versioning &amp; package-installation processes, a practice that I consider <a href=""https://github.com/RaRe-Technologies/gensim/issues/2283"" rel=""nofollow noreferrer"">recklessly insecure</a>.)</p>
",2,0,280,2020-11-22 01:23:40,https://stackoverflow.com/questions/64949799/can-the-gensim-pretrained-models-be-used-for-doc2vec-models
Which document embedding model for document similarity,"<p>First, I want to explain my task. I have a dataset of 300k documents with an average of 560 words (no stop word removal yet) 75% in German, 15% in English and the rest in different languages. The goal is to recommend similar documents based on an existing one. At the beginning I want to focus on the German and English documents.  </p>
<p>To achieve this goal I looked into several methods on feature extraction for document similarity, especially the word embedding methods have impressed me because they are context aware in contrast to simple TF-IDF feature extraction and the calculation of cosine similarity. </p>
<p>I'm overwhelmed by the amount of methods I could use and I haven't found a proper evaluation of those methods yet. I know for sure that the size of my documents are too big for BERT, but there is FastText, Sent2Vec, Doc2Vec and the Universal Sentence Encoder from Google. My favorite method based on my research is Doc2Vec even though there aren't any or old pre-trained models which means I have to do the training on my own.</p>
<p>Now that you know my task and goal, I have the following questions:</p>
<ul>
<li>Which method should I use for feature extraction based on the rough overview of my data?</li>
<li>My dataset is too small to train Doc2Vec on it. Do I achieve good results if I train the model on English / German Wikipedia? </li>
</ul>
","python, gensim, word-embedding, doc2vec, fasttext","<p>You really have to try the different methods on your data, with your specific user tasks, with your time/resources budget to know which makes sense.</p>
<p>You 225K German documents and 45k English documents are each plausibly large enough to use <code>Doc2Vec</code> - as they match or exceed some published results. So you wouldn't necessarily need to add training on something else (like Wikipedia) instead, and whether adding that to your data would help or hurt is another thing you'd need to determine experimentally.</p>
<p>(There might be special challenges in German given compound words using common-enough roots but being individually rare, I'm not sure. FastText-based approaches that use word-fragments might be helpful, but I don't know a <code>Doc2Vec</code>-like algorithm that necessarily uses that same char-ngrams trick. The closest that might be possible is to use Facebook FastText's supervised mode, with a rich set of meaningful known-labels to bootstrap better text vectors - but that's highly speculative and that mode isn't supported in Gensim.)</p>
",1,-1,601,2020-11-26 18:45:04,https://stackoverflow.com/questions/65027694/which-document-embedding-model-for-document-similarity
How to use my own corpus on word embedding model BERT,"<p>I am trying to create a question-answering model with the word embedding model BERT from google. I am new to this and would really want to use my own corpus for the training. At first I used an example from the <a href=""https://huggingface.co/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2"" rel=""nofollow noreferrer"">huggingface site</a> and that worked fine:</p>
<pre><code>from transformers import pipeline

qa_pipeline = pipeline(
    &quot;question-answering&quot;,
    model=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;,
    tokenizer=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;
)

qa_pipeline({
    'context': &quot;Amsterdam is de hoofdstad en de dichtstbevolkte stad van Nederland.&quot;,
    'question': &quot;Wat is de hoofdstad van Nederland?&quot;})
</code></pre>
<p>output</p>
<pre><code>&gt; {'answer': 'Amsterdam', 'end': 9, 'score': 0.825619101524353, 'start': 0}
</code></pre>
<p>So, I tried creating a .txt file to test if it was possible to interchange the sentence in the context parameter with the exact same sentence but in a .txt file.</p>
<pre><code>with open('test.txt') as f:
    lines = f.readlines()

qa_pipeline = pipeline(
    &quot;question-answering&quot;,
    model=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;,
    tokenizer=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;
)

qa_pipeline({
    'context': lines,
    'question': &quot;Wat is de hoofdstad van Nederland?&quot;})
</code></pre>
<p>But this gave me the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-7-2bae0ecad43e&gt; in &lt;module&gt;()
     10 qa_pipeline({
     11     'context': lines,
---&gt; 12     'question': &quot;Wat is de hoofdstad van Nederland?&quot;})

5 frames
/usr/local/lib/python3.6/dist-packages/transformers/data/processors/squad.py in _is_whitespace(c)
     84 
     85 def _is_whitespace(c):
---&gt; 86     if c == &quot; &quot; or c == &quot;\t&quot; or c == &quot;\r&quot; or c == &quot;\n&quot; or ord(c) == 0x202F:
     87         return True
     88     return False

TypeError: ord() expected a character, but string of length 66 found
</code></pre>
<p>I was just experimenting with ways to read and use a .txt file, but I don't seem to find a different solution. I did some research on the huggingface pipeline() function and this is what was written about the question and context parameters:</p>
<p><a href=""https://i.sstatic.net/Yk4bM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Yk4bM.png"" alt=""enter image description here"" /></a></p>
","word-embedding, bert-language-model, huggingface-transformers","<p>Got it! The solution was really easy. I assumed that the variable 'lines' was already a str but that wasn't the case. Just by casting to a string the question-answering model accepted my test.txt file.</p>
<p>so from:</p>
<pre><code>with open('test.txt') as f:
    lines = f.readlines()
</code></pre>
<p>to:</p>
<pre><code>with open('test.txt') as f:
    lines = str(f.readlines())
</code></pre>
",1,1,301,2020-12-15 10:29:14,https://stackoverflow.com/questions/65304058/how-to-use-my-own-corpus-on-word-embedding-model-bert
Deal with Out of vocabulary word with Gensim pretrained GloVe,"<p>I am working on an NLP assignment and loaded the GloVe vectors provided by Gensim:</p>
<pre><code>import gensim.downloader
glove_vectors = gensim.downloader.load('glove-twitter-25')
</code></pre>
<p>I am trying to get the word embedding for each word in a sentence, but some of them are not in the vocabulary.</p>
<p>What is the best way to deal with it working with the Gensim API?</p>
<p>Thanks!</p>
","nlp, stanford-nlp, gensim, word-embedding","<p>Load the <a href=""https://radimrehurek.com/gensim/downloader.html"" rel=""nofollow noreferrer"">model</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import gensim.downloader as api
model = api.load(&quot;glove-twitter-25&quot;)  # load glove vectors
# model.most_similar(&quot;cat&quot;)  # show words that similar to word 'cat'
</code></pre>
<p>There is a very simple way to find out if the words exist in the model's vocabulary.</p>
<pre class=""lang-py prettyprint-override""><code>result = print('Word exists') if word in model.wv.vocab else print('Word does not exist&quot;)
</code></pre>
<p>Apart from that, I had used the following logic to create sentence embedding (25 dim) with <strong>N</strong> tokens:</p>
<pre class=""lang-py prettyprint-override""><code>from __future__ import print_function, division
import os
import re
import sys
import regex
import numpy as np
from functools import partial

from fuzzywuzzy import process
from Levenshtein import ratio as lev_ratio

import gensim
import tempfile


def vocab_check(model, word):
    similar_words = model.most_similar(word)
    match_ratio = 0.
    match_word = ''
    for sim_word, sim_score in similar_words:
        ratio = lev_ratio(word, sim_word)
        if ratio &gt; match_ratio:
            match_word = sim_word
    if match_word == '':
        return similar_words[0][1]
    return model.similarity(word, match_word)


def sentence2vector(model, sent, dim=25):
    words = sent.split(' ')
    emb = [model[w.strip()] for w in words]
    weights = [1. if w in model.wv.vocab else vocab_check(model, w) for w in words]
    
    if len(emb) == 0:
        sent_vec = np.zeros(dim, dtype=np.float16)
    else:
        sent_vec = np.dot(weights, emb)

    sent_vec = sent_vec.astype(&quot;float16&quot;)
    return sent_vec   
</code></pre>
",2,2,2996,2020-12-19 16:35:40,https://stackoverflow.com/questions/65372032/deal-with-out-of-vocabulary-word-with-gensim-pretrained-glove
ValueError: invalid vector on line 440902 | while loading wiki.ar.vec using gensim.models.keyedvectors.word2vec() function,"<p>I'm trying to load wiki.ar.vec arabic word embedding file using word2vec function from gensim.</p>
<p>Below is the code use to load embedding file.</p>
<pre><code>import gensim.models.keyedvectors as word2vec 
print( &quot;Word Embedding is loading&quot;)
embedding = word2vec.KeyedVectors.load_word2vec_format('/home/user/Documents/wiki.ar.vec', binary=False)
print( &quot;Word Embedding is loaded&quot;)
</code></pre>
<p>Facing the Error describe in below screenshot:</p>
<p><a href=""https://i.sstatic.net/wZbVP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wZbVP.jpg"" alt=""enter image description here"" /></a></p>
<p>or any other way to load wiki.ar.vec embedding file?</p>
<p>Any suggestion and answers are highly appriciated.</p>
","python, arabic, gensim, word2vec, word-embedding","<p>This error indicates the file is not in the proper format, at that specified line/vector.</p>
<p>Where did the file come from? Are you sure it's a binary-format file of the right format?</p>
<p>Have you tried re-downloading the file to ensure it hasn't been corrupted or truncated?</p>
",0,0,277,2020-12-22 09:52:23,https://stackoverflow.com/questions/65406526/valueerror-invalid-vector-on-line-440902-while-loading-wiki-ar-vec-using-gens
Preprocessing a corpus for different Word Embedding Algorithms,"<p>For my Bachelorthesis I need to train different word embedding algorithms on the same corpus to benchmark them.
I am looking to find preprocessing steps but am not sure which ones to use and which ones might be less useful.</p>
<p><em>I already looked for some studies but also wanted to ask if someone has experience with this.</em></p>
<p>My objective is to train Word2Vec, FastText and GloVe Embeddings on the same corpus. Not too sure which one now, but I think of Wikipedia or something similar.</p>
<p>In my opinion:</p>
<ul>
<li>POS-Tagging</li>
<li>remove non-alphabetic characters with regex or similar</li>
<li>Stopword removal</li>
<li>Lemmatization</li>
<li>catching Phrases</li>
</ul>
<p>are the logical options.</p>
<p>But I heard that stopword removal can be kind of tricky, because there is a chance that some embeddings still contain stopwords due to the fact that automatic stopword removal might not fit to any model/corpus.</p>
<p>Also I have not decided if I want to choose spacy or nltk as library, spacy is mightier but nltk is mainly used at the chair I am writing.</p>
","nlp, word-embedding","<p>Preprocessing is like hyperparameter optimization or neural architecture search. There isn't a theoretical answer to &quot;which one should I use&quot;. The applied section of this field (NLP) is far ahead of the theory. You just run different combinations until you find the one that works best (according to your choice of metric).</p>
<p>Yes Wikipedia is great, and almost everyone uses it (plus other datasets). I've tried spacy and it's powerful, but I think I made a mistake with it and I ended up writing my own tokenizer which worked better. YMMV. Again, you just have to jump in and try almost everything. Check with your advisor that you have enough time and computing resources.</p>
",1,1,374,2020-12-22 16:19:44,https://stackoverflow.com/questions/65412179/preprocessing-a-corpus-for-different-word-embedding-algorithms
Which trained embeddings vectors from Gensim (word2vec model) should be used for Tensorflow? Unnormalised or normalised ones?,"<p>I want to use the Gensim (word2vec model) trained vectors inside a neural network (Tensorflow). There are two kinds of weights I can use for this purpose. The first group is <code>model.syn0</code> and the second group is <code>model.vectors_norm</code> (after calling <code>model.init_sims(replace=True)</code>). The second one is the group of vectors we use for calculating similarity. Which one has the correct order (match with <code>model.wv.index2word</code> and <code>model.wv.vocab[X].index</code>) and weights for the embedding layer of a neural network?</p>
","tensorflow, keras, gensim, word2vec, word-embedding","<p>If you are using Google's<code>GoogleNews-vectors</code> as pretrained model you can use <code>model.syn0</code>. If you are using Facebook's <code>fastText</code> word embeddings you can directly load the binary file.<br />
Below are the example to load both the instances.</p>
<p><strong>Load GoogleNews pretrained embedding:</strong></p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,limit=500000) # To load the model first time.
model.wv.save_word2vec_format(model_path) #You can save the loaded model to binary file to load the model faster
model = gensim.models.KeyedVectors.load(model_path,mmap='r')
model.syn0norm = model.syn0
index2word_set = set(model.index2word)

model[word] gives the vector representation of the word which can be used to find similarity. 
</code></pre>
<p><strong>Load fastText pretrained embeddings:</strong></p>
<pre><code>import gensim
from gensim.models import FastText
model = FastText.load_fasttext_format('cc.en.300') # to load the model for first time.
model.save(&quot;fasttext_en_bin&quot;) # Save the model to binary file to load faster.
model = gensim.models.KeyedVectors.load(&quot;fasttext_en_bin&quot;,mmap=&quot;r&quot;)
index2word_set = set(model.index2word)

model[word] gives the vector representation of the word which can be used to find similarity. 
</code></pre>
<p>General example:</p>
<pre><code>if word in index2word:
   feature_vec = model[word]
</code></pre>
",2,1,966,2020-12-29 16:55:46,https://stackoverflow.com/questions/65495775/which-trained-embeddings-vectors-from-gensim-word2vec-model-should-be-used-for
"Tensorflow embeddings InvalidArgumentError: indices[18,16] = 11905 is not in [0, 11905) [[node sequential_1/embedding_1/embedding_lookup","<p>I am using TF 2.2.0 and trying to create a Word2Vec CNN text classification model. But however I tried there has been always an issue with the model or embedding layers. I could not found clear solutions in the internet so decided to ask it.</p>
<pre><code>import multiprocessing
modelW2V = gensim.models.Word2Vec(filtered_stopwords_list, size= 100, min_count = 5, window = 5, sg=0, iter = 10, workers= multiprocessing.cpu_count() - 1)
model_save_location = &quot;3000tweets_notbinary&quot;
modelW2V.wv.save_word2vec_format(model_save_location)

word2vec = {}
with open('3000tweets_notbinary', encoding='UTF-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vec = np.asarray(values[1:], dtype='float32')
        word2vec[word] = vec

num_words = len(list(tokenizer.word_index))

embedding_matrix = np.random.uniform(-1, 1, (num_words, 100))
for word, i in tokenizer.word_index.items():
    if i &lt; num_words:
        embedding_vector = word2vec.get(word)
        if embedding_vector is not None:
          embedding_matrix[i] = embedding_vector
        else:
          embedding_matrix[i] = np.zeros((100,))
</code></pre>
<p>I have created my word2vec weights by the code above and then converted it to embedding_matrix as I followed on many tutorials. But since there are a lot of words seen by word2vec but not available in embeddings, if there is no embedding I assign 0 vector. And then fed data and this embedding to tf sequential model.</p>
<pre><code>seq_leng = max_tokens
vocab_size = num_words
embedding_dim = 100
filter_sizes = [3, 4, 5]
num_filters = 512
drop = 0.5
epochs = 5
batch_size = 32

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Embedding(input_dim= vocab_size,
                                                              output_dim= embedding_dim,
                                                              weights = [embedding_matrix],
                                                              input_length= max_tokens,
                                                              trainable= False),
                                    tf.keras.layers.Conv1D(num_filters, 7, activation= &quot;relu&quot;, padding= &quot;same&quot;),
                                    tf.keras.layers.MaxPool1D(2),
                                    tf.keras.layers.Conv1D(num_filters, 7, activation= &quot;relu&quot;, padding= &quot;same&quot;),
                                    tf.keras.layers.MaxPool1D(),
                                    tf.keras.layers.Dropout(drop),
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(32, activation= &quot;relu&quot;, kernel_regularizer= tf.keras.regularizers.l2(1e-4)),
                                    tf.keras.layers.Dense(3, activation= &quot;softmax&quot;)
])

model.compile(loss= &quot;categorical_crossentropy&quot;, optimizer= tf.keras.optimizers.Adam(learning_rate= 0.001, epsilon= 1e-06),
              metrics= [&quot;accuracy&quot;, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

model.summary()

history = model.fit(x_train_pad, y_train2, batch_size= 60, epochs= epochs, shuffle= True, verbose= 1)
</code></pre>
<p>But when I run this code, tensorflow gives me the following error in any random time of the training process. But I could not find any solution to it. I have tried adding + 1 to vocab_size but when I do that I get size mismatch error which does not let me even compile my model. Can anyone please help me?</p>
<pre><code>InvalidArgumentError:  indices[18,16] = 11905 is not in [0, 11905)
     [[node sequential_1/embedding_1/embedding_lookup (defined at &lt;ipython-input-26-ef1b16cf85bf&gt;:1) ]] [Op:__inference_train_function_1533]

Errors may have originated from an input operation.
Input Source operations connected to node sequential_1/embedding_1/embedding_lookup:
 sequential_1/embedding_1/embedding_lookup/991 (defined at /usr/lib/python3.6/contextlib.py:81)

Function call stack:
train_function
</code></pre>
","tensorflow, nlp, word2vec, embedding, word-embedding","<p>I solved this solution. I was adding a new dimension to vocab_size by doing it vocab_size + 1 as suggested by others. However, since sizes of layer dimensions and embedding matrix don't match I got this issue in my hands. I added a zero vector at the end of my embedding matrix which solved the issue.</p>
",2,1,7352,2020-12-30 23:43:28,https://stackoverflow.com/questions/65514944/tensorflow-embeddings-invalidargumenterror-indices18-16-11905-is-not-in-0
ValueError: need at least one array to concatenate in Top2Vec Error,"<p>docs = ['Consumer discretionary, healthcare and technology are preferred China equity  sectors.',
'Consumer discretionary remains attractive, supported by China’s policy to revitalize domestic consumption. Prospects of further monetary and fiscal stimulus  should reinforce the Chinese consumption theme.',
'The healthcare sector should be a key beneficiary of the coronavirus outbreak,  on the back of increased demand for healthcare services and drugs.',
'The technology sector should benefit from increased demand for cloud services  and hardware demand as China continues to recover from the coronavirus  outbreak.',
'China consumer discretionary sector is preferred. In our assessment, the sector  is likely to outperform the MSCI China Index in the coming 6-12 months.']</p>
<p>model = Top2Vec(docs, embedding_model = 'universal-sentence-encoder')</p>
<p>while running the above command, I'm getting an error that is not clearly visible for debugging what could be the root cause for the error?</p>
<p>Error:</p>
<h2>2021-01-19 05:17:08,541 - top2vec - INFO - Pre-processing documents for training
INFO:top2vec:Pre-processing documents for training
2021-01-19 05:17:08,562 - top2vec - INFO - Downloading universal-sentence-encoder model
INFO:top2vec:Downloading universal-sentence-encoder model
2021-01-19 05:17:13,250 - top2vec - INFO - Creating joint document/word embedding
INFO:top2vec:Creating joint document/word embedding
WARNING:tensorflow:5 out of the last 6 calls to &lt;function recreate_function..restored_function_body at 0x7f8c4ce57d90&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to <a href=""https://www.tensorflow.org/guide/function#controlling_retracing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/function#controlling_retracing</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/function</a> for  more details.
WARNING:tensorflow:5 out of the last 6 calls to &lt;function recreate_function..restored_function_body at 0x7f8c4ce57d90&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to <a href=""https://www.tensorflow.org/guide/function#controlling_retracing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/function#controlling_retracing</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/function</a> for  more details.
2021-01-19 05:17:13,548 - top2vec - INFO - Creating lower dimension embedding of documents
INFO:top2vec:Creating lower dimension embedding of documents
2021-01-19 05:17:15,809 - top2vec - INFO - Finding dense areas of documents
INFO:top2vec:Finding dense areas of documents
2021-01-19 05:17:15,823 - top2vec - INFO - Finding topics
INFO:top2vec:Finding topics</h2>
<p>ValueError                                Traceback (most recent call last)
 in ()
----&gt; 1 model = Top2Vec(docs, embedding_model = 'universal-sentence-encoder')</p>
<p>2 frames
&lt;<strong>array_function</strong> internals&gt; in vstack(*args, **kwargs)</p>
<p>/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py in vstack(tup)
281     if not isinstance(arrs, list):
282         arrs = [arrs]
--&gt; 283     return _nx.concatenate(arrs, 0)
284
285</p>
<p>&lt;<strong>array_function</strong> internals&gt; in concatenate(*args, **kwargs)</p>
<p>ValueError: need at least one array to concatenate</p>
","python, arrays, concatenation, word2vec, word-embedding","<p>You need to use more docs and unique words for it to find at least 2 topics. As an example, I just multiply your list by 10 and it works:</p>
<pre><code>from top2vec import Top2Vec

docs = ['Consumer discretionary, healthcare and technology are preferred China equity  sectors.',
'Consumer discretionary remains attractive, supported by China’s policy to revitalize domestic consumption. Prospects of further monetary and fiscal stimulus  should reinforce the Chinese consumption theme.',
'The healthcare sector should be a key beneficiary of the coronavirus outbreak,  on the back of increased demand for healthcare services and drugs.',
'The technology sector should benefit from increased demand for cloud services  and hardware demand as China continues to recover from the coronavirus  outbreak.',
'China consumer discretionary sector is preferred. In our assessment, the sector  is likely to outperform the MSCI China Index in the coming 6-12 months.']

docs = docs*10 
model = Top2Vec(docs, embedding_model='universal-sentence-encoder')
print(model)
</code></pre>
<blockquote>
<p>&lt;top2vec.Top2Vec.Top2Vec object at 0x13eef6210&gt;</p>
</blockquote>
<p>I had few (30) long docs of up to 130 000 characters, so I just split them into smaller docs every 5000 characters:</p>
<pre><code>
docs_split = []
for doc in docs:
    skip_n = 5000
    for i in range(0,130000,skip_n):
        docs_split.append(doc[i:i+skip_n])
</code></pre>
",3,0,3157,2021-01-19 05:30:23,https://stackoverflow.com/questions/65785949/valueerror-need-at-least-one-array-to-concatenate-in-top2vec-error
How to load pre-trained glove model with gensim load_word2vec_format?,"<p>I am trying to load a pre-trained glove as a word2vec model in gensim. I have downloaded the glove file from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">here</a>. I am using the following script:</p>
<pre><code>from gensim import models
model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)
</code></pre>
<p>but get the following error</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-38-e0b48b51f433&gt; in &lt;module&gt;()
      1 from gensim import models
----&gt; 2 model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py in &lt;genexpr&gt;(.0)
    171     with utils.smart_open(fname) as fin:
    172         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 173         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    174         if limit:
    175             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: 'the'
</code></pre>
<p>What is the underlying problem? Does gensim need a specific format to be able to load it?</p>
","stanford-nlp, gensim, word2vec, word-embedding","<p>The GLoVe format is slightly different – missing a 1st-line declaration of vector-count &amp; dimensions – than the format that <code>load_word2vec_format()</code> supports.</p>
<p>There's a <code>glove2word2vec</code> utility script included you can run once to convert the file:</p>
<p><a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/scripts/glove2word2vec.html</a></p>
<p>Also, starting in Gensim 4.0.0 (currentlyu in prerelease testing), the <code>load_word2vec_format()</code> method gets a new optional <code>no_header</code> parameter:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=load_word2vec_format#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=load_word2vec_format#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</a></p>
<p>If set as <code>no_header=True</code>, the method will deduce the count/dimensions from a preliminary scan of the file - so it can read a GLoVe file with that option – but at the cost of two full-file reads instead of one. (So, you may still want to re-save the object with <code>.save_word2vec_format()</code>, or use the <code>glove2word2vec</code> script, to make future loads faster.)</p>
",7,4,8903,2021-02-03 04:08:58,https://stackoverflow.com/questions/66021131/how-to-load-pre-trained-glove-model-with-gensim-load-word2vec-format
BERT embeddings for entire sentences vs. verbs,"<p>First off, I am drawing upon assumption that majority of the semantic <em>value</em> of the sentence is mediated by verbs that connect the subject and the object of said verb. I am aware that I simplify a bit here as there can be multiple verbs and such. But abstracting from that, I'd be curious whether it applies that a BERT generated embedding vector for entire sentence, say <strong>&quot;Two halves make a whole&quot;</strong> would be measurably more similar to an embedding vector for a singular verb like <strong>&quot;make&quot;</strong> as compared to say a vector for verb <strong>&quot;eat&quot;</strong>.</p>
","nlp, word-embedding, bert-language-model","<p>To answer your &quot;question&quot;, if I were you I would try to do a practical test. For an easy way to use bert for sentence embeddings, check this <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">repo</a>: it is summarily simple to use.</p>
<p>Once you have the embedding vectors, you can use any <a href=""https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity"" rel=""nofollow noreferrer"">similarity function</a> to validate your hypothesis.</p>
<p>However for what is my (limited) experience, I think that the vector of &quot;make&quot; is more similar than that of &quot;eat&quot; also only because &quot;make&quot; is present in the other sentence and therefore contributes to the ambedding of the sentence.</p>
",1,-1,370,2021-02-20 17:46:14,https://stackoverflow.com/questions/66294710/bert-embeddings-for-entire-sentences-vs-verbs
pass from a model of type gensim.models.keyedvectors.Word2VecKeyedVectors to a model of type gensim.models.word2vec.Word2Vec,"<p>I downloaded a word embedding already train in &quot;glove.txt&quot; format
I imported it in as a model of type gensim.models.keyedvectors.Word2VecKeyedVectors thanks to this documentation :</p>
<p><a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/scripts/glove2word2vec.html</a></p>
<p>But I would like a model of type gensim.models.word2vec.Word2Vec</p>
<p>Will there be a way to convert it or import it directly into the desired format?</p>
","python, deep-learning, nlp, gensim, word-embedding","<p>A set of word-vectors isn't enough to create a full <code>Word2Vec</code> algorithm model, which includes a lot more information from training, including extra internal model weights &amp; word-frequencies. (The word-vectors alone are less than half the state of the model.)</p>
<p>Why do you want a full model rather than just the vectors?</p>
<p>Can you train your own model from text data that's the same as, or similar is size/value to, the text used for creating the <code>glove.txt</code> word-vectors?</p>
",1,0,583,2021-03-09 20:58:10,https://stackoverflow.com/questions/66554689/pass-from-a-model-of-type-gensim-models-keyedvectors-word2veckeyedvectors-to-a-m
What is the best approach to measure a similarity between texts in multiple languages in python?,"<p>So, I have a task where I need to measure the similarity between two texts. These texts are short descriptions of products from a grocery store. They always include a name of a product (for example, milk), and they may include a producer and/or size, and maybe some other characteristics of a product.</p>
<p>I have a whole set of such texts, and then, when a new one arrives, I need to determine whether there are similar products in my database and measure how similar they are (on a scale from 0 to 100%).</p>
<p>The thing is: the texts may be in two different languages: Ukrainian and Russian. Also, if there is a foreign brand (like, <code>Coca Cola</code>), it will be written in English.</p>
<p>My initial idea on solving this task was to get multilingual word embeddings (where similar words in different languages are located nearby) and find the distance between those texts. However, I am not sure how efficient this will be, and if it is ok, what to start with.</p>
<p>Because each text I have is just a set of product characteristics, some word embeddings based on a context may not work (I'm not sure in this statement, it is just my assumption).</p>
<p>So far, I have tried to get familiar with the <a href=""https://github.com/facebookresearch/MUSE"" rel=""noreferrer"">MUSE</a> framework, but I encountered an <a href=""https://github.com/facebookresearch/faiss/issues/1755"" rel=""noreferrer"">issue</a> with <code>faiss</code> installation.</p>
<p>Hence, my questions are:</p>
<ul>
<li>Is my idea with word embeddings worth trying?</li>
<li>Is there maybe a better approach?</li>
<li>If the idea with word embeddings is okay, which ones should I use?</li>
</ul>
<p><em>Note:</em> I have Windows 10 (in case some libraries don't work on Windows), and I need the library to work with Ukrainian and Russian languages.</p>
<p>Thanks in advance for any help! Any advice would be highly appreciated!</p>
","python, nlp, multilingual, similarity, word-embedding","<p>Let's say that your task is about a fine-grained entity recognition. I think you have a well defined entities: brand, size etc...
So, these features that defines a product each could be a vector, which means your products could be represented with a matrix.
You can potentially represent each feature with an embedding.
Or mixture of the embedding and one-hot vectors.</p>
<p>Here is how.</p>
<ol start=""0"">
<li>Define a list of product features:
product name, brand name, size, weight.</li>
<li>For each product feature, you need a text recognition model:
E.g. with brand recognition you find what part of the text is its brand name.</li>
<li>Use machine translation if it is possible to make unified language representation for all sub texts. E.g. <code>Coca Cola</code> to
<code>ru Кока-Кола, en Coca Cola</code>.</li>
<li>Use contextual embeddings (i.e. huggingface multilingial BERT or something better) to convert prompted text into one vector.</li>
<li>In order to compare two products, compare their feature vectors: what is the average similarity between two feature array. You can also decide what is the weight on each feature.</li>
<li>Try other vectorization methods. Perhaps you dont want to mix brand knockoffs: &quot;Coca Cola&quot; is similar to &quot;Cool Cola&quot;. So, maybe embeddings aren't good for brand names and size and weight but good enough for product names. If you want an exact match, you need a hash function for their text. On their multi-lingual prompt-engineered text.</li>
<li>You can also extend each feature vectors, with concatenations of several embeddings or one hot vector of their source language and things like that.</li>
</ol>
<p>There is no definitive answer here, you need to experiment and test to see what is the best solution. You cam create a test set and make benchmarks for your solutions.</p>
",0,5,2596,2021-03-12 22:49:29,https://stackoverflow.com/questions/66608244/what-is-the-best-approach-to-measure-a-similarity-between-texts-in-multiple-lang
Error while loading wiki40b embeddings from tensorflow hub,"<p>I'm trying to use this module (<a href=""https://tfhub.dev/google/wiki40b-lm-nl/1"" rel=""nofollow noreferrer"">https://tfhub.dev/google/wiki40b-lm-nl/1</a>) loading it with <code>KerasLayer</code>, but not sure why this error is raised.</p>
<pre><code>import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text

hub_url = &quot;https://tfhub.dev/google/wiki40b-lm-nl/1&quot;
embed = hub.KerasLayer(hub_url, input_shape=[], 
                   dtype=tf.string)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-37-4e8ab0d5082c&gt; in &lt;module&gt;()
      5 hub_url = &quot;https://tfhub.dev/google/wiki40b-lm-nl/1&quot;
      6 embed = hub.KerasLayer(hub_url, input_shape=[], 
----&gt; 7                        dtype=tf.string)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py in _get_callable(self)
    300     if self._signature not in self._func.signatures:
    301       raise ValueError(&quot;Unknown signature %s in %s (available signatures: %s).&quot;
--&gt; 302                        % (self._signature, self._handle, self._func.signatures))
    303     f = self._func.signatures[self._signature]
    304     if not callable(f):

ValueError: Unknown signature default in https://tfhub.dev/google/wiki40b-lm-nl/1 (available signatures: _SignatureMap({'neg_log_likelihood': &lt;ConcreteFunction pruned(text) at 0x7F3044A93210&gt;, 'tokenization': &lt;ConcreteFunction pruned(text) at 0x7F3040B7D190&gt;, 'token_neg_log_likelihood': &lt;ConcreteFunction pruned(token) at 0x7F3040D14810&gt;, 'word_embeddings': &lt;ConcreteFunction pruned(text) at 0x7F303D3FF2D0&gt;, 'activations': &lt;ConcreteFunction pruned(text) at 0x7F303D3FFF50&gt;, 'prediction': &lt;ConcreteFunction pruned(mem_4, mem_5, mem_6, mem_7, mem_8, mem_9, mem_10, mem_11, input_tokens, mem_0, mem_1, mem_2, mem_3) at 0x7F303C189090&gt;, 'detokenization': &lt;ConcreteFunction pruned(token_ids) at 0x7F3039860790&gt;, 'token_word_embeddings': &lt;ConcreteFunction pruned(token) at 0x7F3038FC2110&gt;, 'token_activations': &lt;ConcreteFunction pruned(token) at 0x7F303BAF9150&gt;})).
</code></pre>
<p>I tried to set the signature <code>signature=&quot;word_embeddings&quot;, signature_outputs_as_dict=True</code>, but it turns out that the embeddings do not accept strings as input, only a tensor.</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-36-e98cfe451175&gt; in &lt;module&gt;()
----&gt; 1 embed('ik')

5 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_with_flat_signature(self, args, kwargs, cancellation_manager)
   1733         raise TypeError(&quot;{}: expected argument #{}(zero-based) to be a Tensor; &quot;
   1734                         &quot;got {} ({})&quot;.format(self._flat_signature_summary(), i,
-&gt; 1735                                              type(arg).__name__, str(arg)))
   1736     return self._call_flat(args, self.captured_inputs, cancellation_manager)
   1737 

TypeError: pruned(text): expected argument #0(zero-based) to be a Tensor; got str (ik)
</code></pre>
<p>My question is, how to use this embedding with <code>str</code> as input, as they point out in the module's page (section Inputs)?</p>
","tensorflow, word-embedding, tensorflow-hub","<p>Passing the text wrapped in a <code>tf.constant</code> to <code>embed()</code> and setting the <code>output_key</code> keyword should make it work:</p>
<pre><code>import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text

embed = hub.KerasLayer(&quot;https://tfhub.dev/google/wiki40b-lm-nl/1&quot;,
                       signature=&quot;word_embeddings&quot;,
                       output_key=&quot;word_embeddings&quot;)
embed(tf.constant(&quot;\n_START_ARTICLE_\n1001 vrouwen uit de Nederlandse &quot;
                  &quot;geschiedenis\n_START_SECTION_\nSelectie van vrouwen&quot;
                  &quot;\n_START_PARAGRAPH_\nDe 'oudste' biografie in het boek &quot;
                  &quot;is gewijd aan de beschermheilige&quot;))
</code></pre>
<p>(tested with TF 2.4.1 and tensorflow_hub 0.11.0)</p>
",2,0,561,2021-03-29 10:07:57,https://stackoverflow.com/questions/66852256/error-while-loading-wiki40b-embeddings-from-tensorflow-hub
I am getting the following error when importing import texthero as hero,"<p>I have been trying to load texthero into python but keep getting this error. I have already upgraded the gensim module.</p>
<p>Error</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-24-0692be95f55a&gt; in &lt;module&gt;()
----&gt; 1 import texthero as hero
      2 import pandas as pd

1 frames
/usr/local/lib/python3.7/dist-packages/texthero/preprocessing.py in &lt;module&gt;()
     22 warnings.filterwarnings(action=&quot;ignore&quot;, category=UserWarning, module=&quot;gensim&quot;)
     23 
---&gt; 24 from gensim.sklearn_api.phrases import PhrasesTransformer
     25 
     26 

ModuleNotFoundError: No module named 'gensim.sklearn_api'
</code></pre>
","python, word-embedding","<p>Seems like a potential conflict with a newer version of <code>gensim</code>, in my case 4.0.1.</p>
<p><code>pip install &quot;gensim==3.8.1&quot;</code></p>
<p>solved this for me.</p>
",2,1,3013,2021-04-02 15:35:54,https://stackoverflow.com/questions/66921706/i-am-getting-the-following-error-when-importing-import-texthero-as-hero
How to get three dimensional vector embedding for a list of words,"<p>I have been asked to create three dimensional vector embeddings for a series of words. Although I understand what an embedding is and that <code>word2vec</code> will be able to create the vector embeddings, I cannot find a resource that shows me how to create a <em>three</em> dimensional vector (all the resources show many more dimensions than this).</p>
<p>The format I have to create the file in is:</p>
<pre><code>house    34444     0.3232 0.123213 1.231231
dog    14444    0.76762 0.76767 1.45454
</code></pre>
<p>which is in the format <code>&lt;token&gt;\t&lt;word_count&gt;\t&lt;vector_embedding_separated_by_spaces&gt;</code></p>
<p>Can anyone point me towards a resource that will show me how to create the desired file format given some training text?</p>
","nlp, word2vec, word-embedding","<p>Once you've decided on a programming language, and word2vec library, its documentation will likely highlight a configurable parameter that lets you specify the dimensionality of the vectors it trains. So, you just need to change that parameter from its typical values , like <code>100</code> or <code>300</code>, to <code>3</code>.</p>
<p>(Note, though, that 3-dimensional word-vectors are unlikely to show the interesting &amp; useful property of higher-dimensional vectors.)</p>
<p>Once you've used such a library to create the vectors-in-memory, writing them out in your specified format becomes just a file-IO problem, unrelated to word2vec itself. In typical languages, you'd open a new file for writing, loop over your data printing each line properly, then close the file.</p>
<p>(To get a more detailed answer from StackOverflow, you'd want to pick a specific language/library, show what you've already tried with actual code, and show how the results/errors achieved fall short of your goal.)</p>
",1,0,452,2021-04-05 09:35:50,https://stackoverflow.com/questions/66950909/how-to-get-three-dimensional-vector-embedding-for-a-list-of-words
Tensorflow 2 - How to apply adapted TextVectorization to a text dataset,"<h1>Question</h1>
<p>Please help understand the cause of the error when applying the adapted <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization"" rel=""nofollow noreferrer"">TextVectorization</a> to a text Dataset.</p>
<h1>Background</h1>
<p><a href=""https://keras.io/getting_started/intro_to_keras_for_engineers/#the-ideal-machine-learning-model-is-endtoend"" rel=""nofollow noreferrer"">Introduction to Keras for Engineers</a> has a part to apply an adapted TextVectorization layer to a text dataset.</p>
<pre><code>from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
training_data = np.array([[&quot;This is the 1st sample.&quot;], [&quot;And here's the 2nd sample.&quot;]])
vectorizer = TextVectorization(output_mode=&quot;int&quot;)
vectorizer.adapt(training_data)

integer_data = vectorizer(training_data)  # &lt;----- Apply the adapted TextVectorization
</code></pre>
<h2>Problem</h2>
<p>Try to do the same by first adapt a TextVectorization layer to the PTB text, then apply it to the Shakespeare text.</p>
<h3>Adapted a TextVectorization to PTB</h3>
<pre><code>f = &quot;ptb.train.txt&quot;
path_to_ptb = tf.keras.utils.get_file(
    str(pathlib.Path().absolute()) + '/' + f,
    f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/{f}'
)

ptb_ds = tf.data.TextLineDataset(
    filenames=path_to_file, compression_type=None, buffer_size=None, num_parallel_reads=True
)\
.filter(lambda x: tf.cast(tf.strings.length(x), bool))\
.shuffle(10000)

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
vectorizer = TextVectorization(output_mode=&quot;int&quot;, ngrams=None)
vectorizer.adapt(ptb_ds)
</code></pre>
<h3>Apply the TextVectorization layer to the Shakespeare text and got an error</h3>
<pre><code>path_to_shakespeare = tf.keras.utils.get_file(
    'shakespeare.txt', 
    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'
)

shakespeare_ds = tf.data.TextLineDataset(path_to_shakespeare)\
    .filter(lambda x: tf.cast(tf.strings.length(x), bool))

shakespeare_vector_ds =\
    vectorizer(shakespeare_ds.batch(128).prefetch(tf.data.AUTOTUNE))  &lt;----- Error
</code></pre>
<h2>Error</h2>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-48-216442e69438&gt; in &lt;module&gt;
----&gt; 1 shakespeare_vector_ds = vectorizer(shakespeare_ds.batch(128).prefetch(tf.data.AUTOTUNE))
...
alueError: Attempt to convert a value (&lt;PrefetchDataset shapes: (None,), types: tf.string&gt;) with an unsupported type (&lt;class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'&gt;) to a Tensor.
</code></pre>
<h1>Solution</h1>
<p>This works but not clear why above causes the error, although it seems to be doing the same.</p>
<ul>
<li><a href=""https://www.tensorflow.org/tutorials/text/word2vec"" rel=""nofollow noreferrer"">Tensorflow Word2Vec tutorial</a></li>
</ul>
<pre><code>shakespeare_vector_ds =\
   shakespeare_ds.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorizer).unbatch()
</code></pre>
","tensorflow, word-embedding","<p><code>tf.data.Dataset.map</code> applies a function to each element (a Tensor) of a dataset.  The <code>__call__</code> method of the <code>TextVectorization</code> object expects a <code>Tensor</code>, not a <code>tf.data.Dataset</code> object. Whenever you want to apply a function to the elements of a <code>tf.data.Dataset</code>, you should use <code>map</code>.</p>
",1,0,1252,2021-04-09 09:07:04,https://stackoverflow.com/questions/67018234/tensorflow-2-how-to-apply-adapted-textvectorization-to-a-text-dataset
How to explain gensim word2vec output?,"<p>I run the following code and just wonder why the top 3 most similar words for &quot;exposure&quot; don't include &quot;charge&quot; and &quot;lend&quot;?</p>
<pre><code>from gensim.models import Word2Vec
corpus = [['total', 'exposure', 'charge', 'lend'],
          ['customer', 'paydown', 'rate', 'months', 'month']]
gens_mod = Word2Vec(corpus, min_count=1, vector_size=300, window=2, sg=1, workers=1, seed=1)
keyword=&quot;exposure&quot;
gens_mod.wv.most_similar(keyword)

Output:
[('customer', 0.12233059108257294),
 ('month', 0.008674687705934048),
 ('total', -0.011738087050616741),
 ('rate', -0.03600010275840759),
 ('months', -0.04291829466819763),
 ('paydown', -0.044823747128248215),
 ('lend', -0.05356598272919655),
 ('charge', -0.07367636263370514)]
</code></pre>
","python, nlp, gensim, word2vec, word-embedding","<p>The word2vec algorithm is only useful &amp; valuable with large amounts of training data, where every word of interest has a variety of realistic, subtly-contrasting usage examples.</p>
<p>A toy-sized dataset won't show its value. It's always a bad idea to set <code>min_count=1</code>. And, it's nonsensical to try to train 300-dimensional word-vectors from a corpus of only 9 words, 9 unique words, and most of the words having the exact same neighbors.</p>
<p>Try it on a more realistic dataset - tens-of-thousands of unique words, all with multiple usage examples – and you'll see more intuitively-correct similarity results.</p>
",2,0,455,2021-04-14 17:37:56,https://stackoverflow.com/questions/67096547/how-to-explain-gensim-word2vec-output
When should I consider to use pretrain-model word2vec model weights?,"<p>Suppose my corpus is reasonably large - having tens-of-thousands of unique words. I can either use it to build a word2vec model directly(Approach #1 in the code below) or initialize a new word2vec model with pre-trained model weights and fine tune it with my own corpus(Approach #2). Is the approach #2 worth consideration? If so, is there a rule of thumb on when I should consider a pre-trained model?</p>
<pre><code># Approach #1
from gensim.models import Word2Vec
model = Word2Vec(my_corpus, vector_size=300, min_count=1)

# Approach #2
model = Word2Vec(vector_size=300, min_count=1)
model.build_vocab(my_corpus)
model.intersect_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True, lockf=1.0)
model.train(my_corpus, total_examples=len(my_corpus))
</code></pre>
","python, gensim, word2vec, word-embedding, pre-trained-model","<p>The general answer to this type of question is: you should try them both, and see which works better for your purposes.</p>
<p>No one without your exact data &amp; project goals can be sure which will work better in your situation, and you'll need to exact same kind of ability-to-evaluate alterante choices to do all sorts of very basic, necessary tuning of your work.</p>
<p>Separately:</p>
<ul>
<li>&quot;fine-tuning&quot; word2vec-vectors can mean many things, and can introduce a number of expert-leve thorny tradeoff-decisions - the sorts of tradeoffs that can only be navigated if you've got a robust way to test different choices against each other.</li>
<li>The specific simple tuning approach your code shows - which relies on an experimental method (<code>intersect_word2vec_format()</code>) that might not work in the latest Gensim – is pretty limited, and since it discards all the words in the outside vectors that aren't already in your own corpus, also discards one of the major reasons people often want to mix older vectors in - to cover more words not in their training data. (I doubt that approach will be useful in many cases, but as per above, to be sure you'd want to try it with respect to your data/goals.</li>
<li>It's almost always a bad idea to use <code>min_count=1</code> with word2vec &amp; similar algorithms. If such rare words are truly important, find more training examples so good vectors can be trained for them. But without enough training examples, they're usually better to ignore - keeping them even makes the vectors for surrounding words worse.</li>
</ul>
",1,0,1218,2021-04-14 22:06:59,https://stackoverflow.com/questions/67099706/when-should-i-consider-to-use-pretrain-model-word2vec-model-weights
Calculate Cosine Similarity for a word2vec model in R,"<p>I´m working with the package &quot;word2vec&quot; model in R and got a huge problem. I wanna figure out which words are the closest synonyms to &quot;uncertainty&quot; and &quot;economy&quot; like the paper of Azqueta-Gavaldon (2020): &quot;Economic policy uncertainty in the euro area: An unsupervised machine learning approach&quot;.So I did the word2vec function of the word2vec package to create my own word2vec model. With the function predict (object, ...) I can create a table which shows me the words which are closest to my considered words.The problem is that the similarity of this function is defined as the (sqrt(sum(x . y) / ncol(x))) which is not the cosine similarity.
I know that I can use the function cosine(x,y). This function but just works to calculate the cosine similarity between two vectors and can´t do the output like the predict function which I described above.</p>
<p>Does anyone know how to determine the cosine similarity for each word in my Word2Vec model to the other and give me an output of the most similar words to a given word based on these values?</p>
<p>This would really help me a lot and I am already grateful for your answers.</p>
<p>Kind regards,
Tom</p>
","r, word2vec, word-embedding, cosine-similarity","<p>following github-code explains how you can use the cosine similarity in Word2Vec Models in R:
<a href=""https://gist.github.com/adamlauretig/d15381b562881563e97e1e922ee37920"" rel=""nofollow noreferrer"">https://gist.github.com/adamlauretig/d15381b562881563e97e1e922ee37920</a></p>
<p>You can use this function at every matrix in R and therefore for every Word2Vec Model built in R.</p>
<p>Kind Regards,
Tom</p>
",0,0,892,2021-04-15 08:34:57,https://stackoverflow.com/questions/67104985/calculate-cosine-similarity-for-a-word2vec-model-in-r
Can you integrate your pre-trained word embeddings in a custom spaCy model?,"<p>Currently I am trying to develop a spaCy model for NER in the romanian legal domain. I was suggested to use specific WE that are presented at the following link (the links to download the WE are on the last pages - slides 25, 26, 27):</p>
<p><a href=""https://www1.ids-mannheim.de/fileadmin/kl/CoRoLa_based_Word_Embeddings.pdf"" rel=""nofollow noreferrer"">https://www1.ids-mannheim.de/fileadmin/kl/CoRoLa_based_Word_Embeddings.pdf</a></p>
<p>I already trained and tested a model without &quot;touching&quot; the pre-implemented WE but I do not know how to use external WE in computing a new spaCy model. Any relevant advice is appreciated. Although, an example of code will be preferable.</p>
","spacy, word-embedding, named-entity-recognition","<p>Yes, convert your vectors from word2vec text format with <code>spacy init vectors</code> and then specify that model as <code>[initialize.vectors]</code> in your config along with <code>include_static_vectors = true</code> for the relevant tok2vec models.</p>
<p>A config excerpt:</p>
<pre><code>[components.tok2vec.model.embed]
@architectures = &quot;spacy.MultiHashEmbed.v1&quot;
width = ${components.tok2vec.model.encode.width}
attrs = [&quot;ORTH&quot;, &quot;SHAPE&quot;]
rows = [5000, 2500]
include_static_vectors = true

[initialize]
vectors = &quot;my_vector_model&quot;
</code></pre>
<p>You can also use <code>spacy init config -o accuracy config.cfg</code> to generate a sample config including vectors that you can edit and adjust as you need.</p>
<p>See:</p>
<ul>
<li><a href=""https://spacy.io/api/cli#init-vectors"" rel=""nofollow noreferrer"">https://spacy.io/api/cli#init-vectors</a></li>
<li><a href=""https://spacy.io/usage/embeddings-transformers#static-vectors"" rel=""nofollow noreferrer"">https://spacy.io/usage/embeddings-transformers#static-vectors</a></li>
</ul>
",2,1,989,2021-04-21 16:32:43,https://stackoverflow.com/questions/67199914/can-you-integrate-your-pre-trained-word-embeddings-in-a-custom-spacy-model
`Highway.forward: input must be present` in ELMo embedding?,"<p>I use <a href=""https://nlp.johnsnowlabs.com/2020/01/31/elmo.html"" rel=""nofollow noreferrer"">Elmo Embeddings</a> for my NLP task. The <em>pretrain</em> was in the Indonesian language from <a href=""https://github.com/HIT-SCIR/ELMoForManyLangs"" rel=""nofollow noreferrer"">this git</a>. Importing the library by using the syntax</p>
<p><code>from elmoformanylangs import Embedder</code></p>
<p>causing the following error:</p>
<p><code>TypeError: Highway.forward: input must be present</code></p>
<p>Please help me to understand what the error message means.</p>
","python, nlp, embedding, word-embedding, elmo","<p>Not sure if this helps, but this refers to the unimplemented superclass method (forward) in <code>torch.nn.Module</code>. This class has the following definiton.</p>
<p><code>forward: Callable[..., Any] = _forward_unimplemented</code></p>
<p>If you scroll down a bit you will see the definiton of _forward_unimplemented:</p>
<pre><code>def _forward_unimplemented(self, *input: Any) -&gt; None:
</code></pre>
<p>The Highway forward definiton has to match this signature too, therefore you will need a <code>*input</code> argument too.
I got my Hungarian version working with the following signature and first line, probably this could help you too.</p>
<pre><code>    def forward(self, *input: torch.Tensor) -&gt; type(None): #pylint: disable=arguments-differ
    current_input = input[0]
</code></pre>
<p>I just edited my \elmoformanylangs\modules\highway.py file under the site-packages of my python environment, and got it working.</p>
",0,0,499,2021-05-01 16:31:16,https://stackoverflow.com/questions/67348511/highway-forward-input-must-be-present-in-elmo-embedding
Community detection for larger than memory embeddings dataset,"<p>I currently have a dataset of textual embeddings (768 dimensions). The current number of records is ~1 million. I am looking to detect related embeddings through a community detection algorithm. For small data sets, I have been able to use this one:</p>
<p><a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/fast_clustering.py"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/fast_clustering.py</a></p>
<p>It works great, but, it doesn't really scale as the data set grows larger than memory.</p>
<p>The key here is that I am able to specify a threshold for community matches. I have been able to find clustering algorithms that scale to larger than memory, but I always have to specify a fixed number of clusters ahead of time. I need the system to detect the number of clusters for me.</p>
<p>I'm certain there are a class of algorithms - and hopefully a python library - that can handle this situation, but I have been unable to locate it. Does anyone know of an algorithm or a solution I could use?</p>
","python, algorithm, word-embedding","<p>That seems small enough that you could just rent a bigger computer.</p>
<p>Nevertheless, to answer the question, typically the play is to cluster the data into a few chunks (overlapping or not) that fit in memory and then apply a higher-quality in-memory clustering algorithm to each chunk. One typical strategy for cosine similarity is to cluster by <a href=""https://en.wikipedia.org/wiki/SimHash"" rel=""nofollow noreferrer"">SimHashes</a>, but</p>
<ol>
<li>there's a whole literature out there;</li>
<li>if you already have a scalable clustering algorithm you like, you can use that.</li>
</ol>
",2,1,798,2021-05-03 21:53:20,https://stackoverflow.com/questions/67376283/community-detection-for-larger-than-memory-embeddings-dataset
How do we use a Random Forest for sentence-classification using word-embedding,"<p>When we have a random forest, we have n-inputs and m-features e.g for 3 observations and 2 features we have</p>
<pre class=""lang-py prettyprint-override""><code>X = [[1,23],[0,-12],[-0.5,29]]
y = [1,0,1]
</code></pre>
<p>and we can train a RandomForest with</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.ensemble import RandomForestClassifier
model = RandomForest()
model.fit(X,y)
</code></pre>
<p>If I have made a word-embedding using, say, a 100-dimensional vector, how do we create the <code>X</code> matrice, where each input is a sentence?</p>
<p>Say we have the following 3-dimensional embedding of the words <code>[&quot;I&quot;,&quot;like&quot;,&quot;dogs&quot;,&quot;cats&quot;]</code>:</p>
<pre class=""lang-py prettyprint-override""><code>I = [-0.5,0,1]
like = [5,2,3]
dogs = [1,2,3]
cats = [3,2,1]
</code></pre>
<p>then the dataset [&quot;I like dogs&quot;,&quot;I like cats&quot;] would be</p>
<pre class=""lang-py prettyprint-override""><code>X = [
[[-0.5,0,1], [5,2,3], [1,2,3]],
[[-0.5,0,1], [5,2,3], [3,2,1]]
]
y = [&quot;dog-lover&quot;,&quot;cat-lover&quot;]
</code></pre>
<p>which a RF naturally cannot train thus giving the erropr <code>ValueError: Found array with dim 3. Estimator expected &lt;= 2.</code></p>
<p>Apart from RF might not be suitable for NLP - is there a way to do so?</p>
","python, nlp, random-forest, word-embedding","<p>I don't think performing Random Forest classifier on the 3-dimensional input will be possible, but as an alternative way, you can use <em>sentence embedding</em> instead of word embedding. Therefore your input data will be 2-dimensional (<code>(n_samples, n_features)</code>) as this classifier expected.<br />
There are many ways to get the sentence embedding vector, including <em>Doc2Vec</em> and <em>SentenceBERT</em>, but the most simple and commonly used method is to make an element-wise average over all the word embedding vectors.<br />
In your provided example, the embedding length was considered as 3. Suppose that the sentence is <em>&quot;I like dogs&quot;</em>. So the sentence embedding vector will be computed as follow:</p>
<pre><code>I = [-0.5,0,1]
like = [5,2,3]
dogs = [1,2,3]
cats = [3,2,1]

# sentence: 'I like dogs'
sentence = [-0.5+5+1, 0+2+2, 1+3+3] / 3
         = [5.5, 4, 7] / 3
         = [1.8333, 1.3333, 2.3333]
</code></pre>
",3,3,1884,2021-05-04 09:17:45,https://stackoverflow.com/questions/67381956/how-do-we-use-a-random-forest-for-sentence-classification-using-word-embedding
NLP ELMo model pruning input,"<p>I am trying to retrieve embeddings for words based on the pretrained ELMo model available on tensorflow hub. The code I am using is modified from here: <a href=""https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/</a></p>
<p>The sentence that I am inputting is
<br />
bod =&quot; is coming up in and every project is expected to do a video due on we look forward to discussing this with you at our meeting this this time they have laid out the selection criteria for the video award s go for the top spot this time &quot;</p>
<p>and these are the keywords I want embeddings for:
<br />
words=[&quot;do&quot;, &quot;a&quot;, &quot;video&quot;]</p>
<pre><code>embeddings = elmo([bod],
signature=&quot;default&quot;,
as_dict=True)[&quot;elmo&quot;]
init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
</code></pre>
<p>this sentence is 236 characters in length.
this is the picture showing that
<a href=""https://i.sstatic.net/qlvQN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qlvQN.png"" alt=""lenbod"" /></a></p>
<p>but when I put this sentence into the ELMo model, the tensor that is returned is only contains a string of length 48
<a href=""https://i.sstatic.net/w90QH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/w90QH.png"" alt=""tensor dim"" /></a></p>
<p>and this becomes a problem when i try to extract embeddings for keywords that are outside the 48 length limit because the indices of the keywords are shown to be outside this length:
<a href=""https://i.sstatic.net/kruxD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kruxD.png"" alt=""kywordlen"" /></a></p>
<p>this is the code I used to get the indices for the words in 'bod'(as shown above)</p>
<pre><code>num_list=[]
for item in words:
  print(item)
  index = bod.index(item)
  num_list.append(index)
num_list
</code></pre>
<p>But i keep running into this error:
<a href=""https://i.sstatic.net/1Qc2V.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1Qc2V.png"" alt=""error"" /></a></p>
<p>I tried looking for ELMo documentation to explain why this is happening but I have not found anything related to this problem of pruned input.</p>
<p>Any advice is much appreciated!</p>
<p>Thank You</p>
","neural-network, nlp, word-embedding, allennlp, elmo","<p>This is not really an AllenNLP issue since you are using a tensorflow-based implementation of ELMo.</p>
<p>That said, I think the problem is that ELMo embeds tokens, not characters. You are getting 48 embeddings because the string has 48 tokens.</p>
",0,0,136,2021-05-16 16:06:41,https://stackoverflow.com/questions/67558874/nlp-elmo-model-pruning-input
How to interpret doc2vec classifier in terms of words?,"<p>I have trained a doc2vec (PV-DM) model in <code>gensim</code> on documents which fall into a few classes. I am working in a non-linguistic setting where both the number of documents and the number of unique words are small (~100 documents, ~100 words) for practical reasons. Each document has perhaps 10k tokens. My goal is to show that the doc2vec embeddings are more predictive of document class than simpler statistics and to explain which <em>words</em> (or perhaps word sequences, etc.) in each document are indicative of class.</p>
<p>I have good performance of a (cross-validated) classifier trained on the embeddings compared to one compared on the other statistic, but I am still unsure of how to connect the results of the classifier to any features of a given document. Is there a standard way to do this? My first inclination was to simply pass the co-learned word embeddings through the document classifier in order to see which words inhabited which classifier-partitioned regions of the embedding space. The document classes output on word embeddings are very consistent across cross validation splits, which is encouraging, although I don't know how to turn these effective labels into a statement to the effect of &quot;Document X got label Y because of such and such properties of words A, B and C in the document&quot;.</p>
<p>Another idea is to look at similarities between word vectors and document vectors. The ordering of similar word vectors is pretty stable across random seeds and hyperparameters, but the output of this sort of labeling does not correspond at all to the output from the previous method.</p>
<p>Thanks for help in advance.</p>
<p><strong>Edit</strong>: Here are some clarifying points. The tokens in the &quot;documents&quot; are ordered, and they are measured from a discrete-valued process whose states, I suspect, get their &quot;meaning&quot; from context in the sequence, much like words. There are only a handful of classes, usually between 3 and 5. The documents are given unique tags and the classes are not used for learning the embedding. The embeddings have rather dimension, always &lt; 100, which are learned over many epochs, since I am only worried about overfitting when the classifier is learned, not the embeddings. For now, I'm using a multinomial logistic regressor for classification, but I'm not married to it. On that note, I've also tried using the normalized regressor coefficients as vector in the embedding space to which I can compare words, documents, etc.</p>
","gensim, word2vec, word-embedding, doc2vec","<p>That's a very small dataset (100 docs) and vocabulary (100 words) compared to much published work of <code>Doc2Vec</code>, which has usually used tens-of-thousands or millions of distinct documents.</p>
<p>That each doc is thousands of words and you're using PV-DM mode that mixes both doc-to-word and word-to-word contexts for training helps a bit. I'd still expect you might need to use a smaller-than-defualt dimensionaity (vector_size&lt;&lt;100), &amp; more training epochs - but if it does seem to be working for you, great.</p>
<p>You don't mention how many classes you have, nor what classifier algorithm you're using, nor whether known classes are being mixed into the (often unsupervised) <code>Doc2Vec</code> training mode.</p>
<p>If you're only using known classes as the doc-tags, and your &quot;a few&quot; classes is, say, only 3, then to some extent you only have 3 unique &quot;documents&quot;, which you're training on in fragments. Using only &quot;a few&quot; unique doctags might be prematurely hiding variety on the data that could be useful to a downstream classifier.</p>
<p>On the other hand, if you're giving each doc a unique ID - the original 'Paragraph Vectors' paper approach, and then you're feeding those to a downstream classifier, that can be OK alone, but may also benefit from adding the known-classes as extra tags, in addition to the per-doc IDs. (And perhaps if you have many classes, those may be OK as the only doc-tags. It can be worth comparing each approach.)</p>
<p>I haven't seen specific work on making <code>Doc2Vec</code> models explainable, other than the observation that when you are using a mode which co-trains both doc- and word- vectors, the doc-vectors &amp; word-vectors have the same sort of useful similarities/neighborhoods/orientations as word-vectors alone tend to have.</p>
<p>You could simply try creating synthetic documents, or tampering with real documents' words via targeted removal/addition of candidate words, or blended mixes of documents with strong/correct classifier predictions, to see how much that changes either (a) their doc-vector, &amp; the nearest other doc-vectors or class-vectors; or (b) the predictions/relative-confidences of any downstream classifier.</p>
<p>(A wishlist feature for <code>Doc2Vec</code> for a while has been to synthesize a pseudo-document from a doc-vector. See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2459"" rel=""nofollow noreferrer"">this issue</a> for details, including a link to one partial implementation. While the mere ranked list of such words would be nonsense in natural language, it might give doc-vectors a certain &quot;vividness&quot;.)</p>
<p>Whn you're not using real natural language, some useful things to keep in mind:</p>
<ul>
<li>if your 'texts' are really unordered bags-of-tokens, then <code>window</code> may not really be an interesting parameter. Setting it to a very-large number can make sense (to essentially put all words in each others' windows), but may not be practical/appropriate given your large docs. Or, trying PV-DBOW instead - potentially even mixing known-classes &amp; word-tokens in either <code>tags</code> or <code>words</code>.</li>
<li>the default <code>ns_exponent=0.75</code> is inherited from word2vec &amp; natural-language corpora, &amp; at least one research paper (linked from the class documentation) suggests that for other applications, especially recommender systems, very different values may help.</li>
</ul>
",2,0,651,2021-05-18 05:24:35,https://stackoverflow.com/questions/67580388/how-to-interpret-doc2vec-classifier-in-terms-of-words
Calculate cosine similarity for elmo model,"<p>I am trying to calculate the cosine similarity of wordsim set using the Elmo model. This may not make sense since it is designed for sentence word embedding, but I want to see how the model performs in the situations like these. The Elmo I am using is from:</p>
<p><a href=""https://tfhub.dev/google/elmo/3"" rel=""nofollow noreferrer"">https://tfhub.dev/google/elmo/3</a></p>
<p>If I run the following code (it is modified from the documentation page to comply with TF 2.0), it will generate the tensor representation of the word.</p>
<pre><code>import tensorflow_hub as hub
import tensorflow as tf


elmo = hub.load(&quot;https://tfhub.dev/google/elmo/3&quot;)
tensor_of_strings = tf.constant([&quot;Gray&quot;,
                                 &quot;Quick&quot;,
                                 &quot;Lazy&quot;])
elmo.signatures['default'](tensor_of_strings)
</code></pre>
<p>If I try to calculate cosine similarity directly I will get the error,  <code>NotImplementedError: Cannot convert a symbolic Tensor (strided_slice_59:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported </code>. I am not sure how to convert the Tensor to the numpy array directly, or is there a better evaluator for tensors instead of cosine similarity?</p>
<p>Edit: This is what I did for calculate cosine similarity</p>
<pre><code>def cos_sim(a, b):
    return np.inner(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))

print(&quot;ELMo:&quot;, cos_sim(elmo.signatures['default'](tensor_of_strings)['word_emb'][0], elmo.signatures['default'](tensor_of_strings)['word_emb'][1]))
</code></pre>
","python, tensorflow, word-embedding, elmo","<p>In this thread here: <a href=""https://stackoverflow.com/questions/66207609/notimplementederror-cannot-convert-a-symbolic-tensor-lstm-2-strided-slice0-t/66207610"">NotImplementedError: Cannot convert a symbolic Tensor (lstm_2/strided_slice:0) to a numpy array. T</a>. The solution is to change the <code>numpy</code> version (<code>1.19.5</code> perhaps would be a suitable version).</p>
<p>I think it would be important to provide all the versions of (Python + TensorFlow + NumPy).</p>
<p>Also, like @Edwin Cheong mentioned in the comment it is likely you mingled numpy and Tensorflow code in the loss function. It would be also important to provide us that information, here the issue was the loss function computation/creation: <a href=""https://stackoverflow.com/questions/58479556/notimplementederror-cannot-convert-a-symbolic-tensor-2nd-target0-to-a-numpy"">NotImplementedError: Cannot convert a symbolic Tensor (2nd_target:0) to a numpy array</a>.</p>
",1,2,586,2021-05-19 22:32:28,https://stackoverflow.com/questions/67611744/calculate-cosine-similarity-for-elmo-model
How did online training work in the Word2vec model using Genism,"<p>Using the Genism library, we can load the model and update the vocabulary when the new sentence will be added. That’s means If you save the model you can continue training it later. I checked with sample data, let’s say I have a word in my vocabulary that was previously trained (i.e. “women”). And after that let’s say I have new sentences and using model.build_vocab(new_sentence, update=True) and model.train(new_sentence), the model is updated. Now, in my new_sentence I have some word that already exists(“women”) in the previous vocabulary list and have some new word(“girl”) that not exists in the previous vocabulary list. After updating the vocabulary, I have both old and new words in the corpus. And I checked using model.wv[‘women’], the vector is updated after update and training new sentence. Also, get the word embedding vector for a new word i.e. model.wv[‘girl’]. All other words that were previously trained and not in the new_sentence, those word vectors not changed.</p>
<pre><code>model = Word2Vec(old_sentences, vector_size=100,window=5, min_count=1) 
model.save(&quot;word2vec.model&quot;)
model = Word2Vec.load(&quot;word2vec.model&quot;) //load previously save model 
model.build_vocab(new_sentences,update=True,total_examples=model.corpus_count, epochs=model.epochs)   
model.train(new_sentences)
</code></pre>
<p>However, just don’t understand the inside depth explanation of how the online training is working. Please let me know if anybody knows in detail. I get the code but want to understand how the online training working in theoretically. Is it re-train the model on the old and new training data from scratch?</p>
<p>Here is the link that I followed: <a href=""https://rutumulkar.com/blog/2015/word2vec/"" rel=""nofollow noreferrer"">Online training</a></p>
","python-3.x, nlp, word2vec, word-embedding","<p>When you perform a new call to <code>.train()</code>, it only trains on the new data. So only words in the new data can possibly be updated.</p>
<p>And to the extent that the new data may be smaller, and more idiosyncratic in its word usages, any words in the new data will be trained to only be consistent with other words being trained in the new data. (Depending on the size of the new data, and the training parameters chosen like <code>alpha</code> &amp; <code>epochs</code>, they might be pulled via the new examples arbitrarily far from their old locations - and thus start to lose comparability to words that were trained earlier.)</p>
<p>(Note also that when providing an different corpus that the original, you shouldn't use a parameter like <code>total_examples=model.corpus_count</code>, reusing <code>model.corpus_count</code>, a value cahced in the model from the earlier data. Rather, parameters should describe the current batch of data.)</p>
<p>Frankly, I'm not a fan of this feature. It's possible it could be useful to advanced users. But most people drawn to it are likely misuing it, expecting any number of tiny incremental updates to constantly expand &amp; improve the model - when there's no good support for the idea that will reliably happen with naive use.</p>
<p>In fact, there's reasons to doubt such updates are generally a good idea. There's even an established term for the risk that incremental updates to a neural-network wreck its prior performance: <a href=""https://en.wikipedia.org/wiki/Catastrophic_interference"" rel=""nofollow noreferrer"">catastrophic forgetting</a>.</p>
<p>The straightforward &amp; best-grounded approach to updating word-vectors for new expanded data is to re-train from scratch, so all words are on equal footing, and go through the same interleaved training, on the same unified optimization (SGD) schedule. (The new new vectors at the end of such a process will not be in a compatible coordinate space, but should be equivalently useful, or better if the data is now bigger and better.)</p>
",2,0,649,2021-05-26 02:27:18,https://stackoverflow.com/questions/67697776/how-did-online-training-work-in-the-word2vec-model-using-genism
How to concatenate new vectors into existing Bert vector?,"<p>For a sentence,I may extract a few entities and each of the entities is embedded with 256 dimension vectors. Then I compute an average for these entities to be a single vector to represent these entity representations.</p>
<p>Now, I want to concatenate the bert's 'pooled output' layer with this entity vector together as input of the next layer. This might improve the original Bert's performance. How to do this in Keras?</p>
<p>This is the fine-tune code to define a text classifier model from a tutorial:</p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)
</code></pre>
<p>How to combine the pooled_output of 512 dimension with the entity vector?</p>
","tensorflow, embedding, bert-language-model, word-embedding","<p>You first need to ensure that the vector with whom you want to concatenate has the same dimension on the axis you want to concatenate (e.g. <code>(512,1)</code> <code>(512,1)</code> -&gt; <code>(1024,1)</code>).</p>
<p>Then you could use:
<code>tf.keras.layers.Concatenate([pooled_output,entity_layer],axis=axis)</code></p>
<p>on the desired axis.</p>
<p>You can also have a look here for more details: <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate</a></p>
",0,0,709,2021-06-24 07:13:20,https://stackoverflow.com/questions/68111122/how-to-concatenate-new-vectors-into-existing-bert-vector
Training SVM classifier (word embeddings vs. sentence embeddings),"<p>I want to experiment with different embeddings such Word2Vec, ELMo, and BERT but I'm a little confused about whether to use the word embeddings or sentence embeddings, and why. I'm using the embeddings as features input to SVM classifier.</p>
<p>Thank you.</p>
","svm, word2vec, bert-language-model, word-embedding, elmo","<p>Though both approaches can prove efficient for different datasets, as a rule of thumb I would advice you to use word embeddings when your input is of a few words, and sentence embeddings when your input in longer (e.g. large paragraphs).</p>
",2,4,753,2021-07-02 12:22:12,https://stackoverflow.com/questions/68225126/training-svm-classifier-word-embeddings-vs-sentence-embeddings
Set the parameters of Word2Vec for a practical example,"<p>I have a database containing about 2.8 million texts (more precisely tweets, so they are short texts). I put clean tweets (removing hashtags, tags, stop words...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each tweet).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>This was just an attempt, I would need some help to set the parameters (<code>size</code>, <code>window</code>, <code>min_count</code>, <code>workers</code>, <code>sg</code>) of <code>Word2Vec</code> in the most appropriate and consistent way.</p>
<p>Consider that my goal is to use</p>
<p><code>model.most_similar(terms)</code> (where <code>terms</code> is a list of words)</p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>The words in <code>terms</code> belong to the same topic and I would like to see if there are other words within the texts that could have to do with the topic.</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Generally, the usual approach is:</p>
<ul>
<li>Start with the defaults, to get things initially working at a baseline level, perhaps only on a faster-to-work-with subset of the data.</li>
<li>Develop an objective way to determine whether one model is better than another, for your purposes. This might start as a bunch of ad hoc, manual comparisons of results for some representative probes - but <em>should</em> become a process that can automatically score each variant model, giving a higher score to the 'better' model according to some qualitative, repeatable process.</li>
<li>Either tinker with parameters one-by-one, or run a large search over many permutations, to find which model does best on your scoring.</li>
</ul>
<p>Separately: the quality of word2vec results is almost always improved by discarding the very rarest words, such as those appearing only once. (The default value of <code>min_count</code> is <code>5</code> for good reason.)</p>
<p>The algorithm can't make good word-vectors from words that only appear once, or a few times. It <em>needs</em> multiple, contrasting examples of its usage. But, given the typical Zipfian distribution of word usages in a corpus, there are a lot of such rare words. Discarding them speeds training, shrinks the model, &amp; eliminates what's essentially 'noise' from the training of other words - leaving those remaining word-vectors much better. (If you really need vectors for such words – gather more data.)</p>
",2,1,1333,2021-07-02 12:56:58,https://stackoverflow.com/questions/68225624/set-the-parameters-of-word2vec-for-a-practical-example
Tensorflow hub-NNLM word embedding using sentiment140 data gives input shape error,"<p>I am using tensorflow hub &quot;https://tfhub.dev/google/nnlm-en-dim128/2&quot; word embedding for the sentiment analysis of Kaggle &quot;sentiment140&quot; dataset.</p>
<p>Data set : Kaggle(&quot;sentiment140&quot;) <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">https://www.kaggle.com/kazanova/sentiment140</a>
Tensorflow-Hub : <a href=""https://tfhub.dev/google/nnlm-en-dim128/2"" rel=""nofollow noreferrer"">https://tfhub.dev/google/nnlm-en-dim128/2</a></p>
<p>Here i am using keras sequential layer when i fit the model it gives value error</p>
<pre><code>ValueError: Python inputs incompatible with input_signature:
      inputs: (
        Tensor(&quot;IteratorGetNext:0&quot;, shape=(None, 128), dtype=float32))
      input_signature: (
        TensorSpec(shape=(None,), dtype=tf.string, name=None))
</code></pre>
<p>My code:</p>
<pre><code>    import pandas as pd
import tensorflow as tf
from sklearn.model_selection import  train_test_split
import seaborn as sns
import tensorflow_hub as hub
from tensorflow.keras import Sequential
import keras

tweet_df = pd.read_csv(&quot;training.1600000.processed.noemoticon.csv&quot;, names=['polarity', 'id', 'date', 'query', 'user', 'text'],encoding='latin-1')

tweet_df.info()

tweet_df.head()

&quot;&quot;&quot;#### 2.) Data Visualization&quot;&quot;&quot;

tweet_df['polarity'] = tweet_df['polarity'].replace(to_replace=4,value=1)

### Print two movies reviews from each class

print(&quot;Movie Review Polarity Negative class 0 :\n&quot;, tweet_df[tweet_df['polarity']==0]['text'].head(2) )

print(&quot;\n\nMovie Review Polarity Positive class 1 :\n&quot;, tweet_df['text'][tweet_df['polarity']==1].head(2) )

class_dist = tweet_df['polarity'].value_counts().rename_axis('Class Label').reset_index(name='Tweets')
#class_dist = class_dist['Class Label'].replace({0:'Negative',1:'Positve'})
class_dist

## Bar graph of Distribution of Classes
class_dist['class'] = ['Positive','Negative']
sns.set_theme(style='whitegrid')
sns.barplot(x='Class Label', y='Tweets', hue='class', data= class_dist)

### Train and test split 
X = tweet_df.iloc[:,5]
y = tweet_df.iloc[:,0]
X_train, X_test,y_train, y_test = train_test_split(X,y,random_state=5, test_size=0.2)

print(&quot;Training shape of X and y : &quot;, X_train.shape ,y_train.shape)
print(&quot;Testing shape of X and y : &quot;, X_test.shape ,y_test.shape)

&quot;&quot;&quot;#### 3.) Data Pre-processing&quot;&quot;&quot;

embed = hub.load(&quot;https://tfhub.dev/google/nnlm-en-dim128/2&quot;)
X_train_embed = embed(X_train)

y_train = tf.keras.utils.to_categorical(y_train,2)

X_train_embed.shape


X_sample = X_train_embed[:1000]
y_sample = y_train[:1000]
y_sample = tf.keras.utils.to_categorical(y_sample,2)


&quot;&quot;&quot;#### 4.) Model Building&quot;&quot;&quot;

hub_layer = hub.KerasLayer('https://tfhub.dev/google/nnlm-en-dim128/2',input_shape=[],dtype=tf.string,trainable=False)

model = Sequential()
model.add(hub_layer)
model.add(keras.layers.Dense(128, 'relu', name ='layer_1'))
model.add(keras.layers.Dense(64, 'relu', name = 'layer_2'))
model.add(keras.layers.Dense(2, activation='sigmoid', name='output'))

model.compile(optimizer='adam',loss= 'BinaryCrossentropy',  #'categorical_crossentropy' ,
              metrics=['accuracy'] )

NN_model = model.fit(X_sample, y_sample, epochs=20, validation_split=0.1, verbose=1)
</code></pre>
<p>Input shape:</p>
<pre><code>X_sample.shape
</code></pre>
<p>TensorShape([1000, 128])</p>
<pre><code>y_sample.shape
</code></pre>
<p>(1000, 2, 2)</p>
<pre><code>X_sample

&lt;tf.Tensor: shape=(1000, 128), dtype=float32, numpy=
array([[ 0.10381411,  0.07044576, -0.0282673 , ...,  0.08205549,
0.15822364, -0.10019408],
[-0.03332436, -0.00529242,  0.20348714, ..., -0.14174528,
0.05178985, -0.12599435],
[ 0.2461916 , -0.03084931,  0.05861813, ...,  0.07956063,
-0.03579932,  0.07493019],
[ 0.4102695 ,  0.15445013,  0.19045362, ...,  0.12681636,
0.12362286, -0.03969387],
[-0.0144283 , -0.05236297,  0.04851832, ...,  0.05562773,
0.01529189,  0.12605236],
[ 0.29280087,  0.05795274, -0.11779188, ..., -0.01890504,
0.02824693, -0.13629636]], dtype=float32)&gt;
</code></pre>
","keras, sentiment-analysis, word-embedding, tensorflow-hub, language-model","<p>As described on <a href=""https://tfhub.dev/google/nnlm-en-dim128/2"" rel=""nofollow noreferrer"">https://tfhub.dev/google/nnlm-en-dim128/2</a>, the model expects a vector of strings as input. You're basically calling the model twice since you're executing</p>
<pre><code>embed = hub.load(&quot;https://tfhub.dev/google/nnlm-en-dim128/2&quot;)
X_train_embed = embed(X_train)  # (n, 128) float matrix
</code></pre>
<p>and then passing that embedding to <code>model</code>, which actually takes strings as input since it starts with the NNLM KerasLayer.</p>
<p>I'd propose to remove <code>embed</code> and <code>X_train_embed</code> and just call <code>model.fit</code> with <code>X_train</code>:</p>
<pre><code>model.fit(np.array([&quot;Lyx is cool&quot;, &quot;Lyx is not cool&quot;]), np.array([1, 0]), epochs=20, validation_split=0.1, verbose=1)
</code></pre>
",1,0,409,2021-07-13 13:41:57,https://stackoverflow.com/questions/68363587/tensorflow-hub-nnlm-word-embedding-using-sentiment140-data-gives-input-shape-err
Merging Two CSV Files Based on Many Criteria,"<p>I have two CSV files. They have a same column but each of rows in the same column are not unique, like this:</p>
<pre><code>gpo_full.csv:
    Date           hearing_sub_type   topic      Specific_Date
    January,1997   Oversight          weather    January 12,1997
    June,2000      General            life       June 5,2000
    January,1997   General            forest     January 1,1997
    April,2001     Oversight          people     NaN 
    June,2000      Oversight          depressed  June 6,2000
    January,1997   General            weather    January 1,1997
    June,2000      Oversight          depressed  June 5,2000

CAP_cols.csv:
    majortopic   id     Chamber   topic           Date           Specific_Date
    21           79846  1         many forest     January,1997   January 1,1997
    4            79847  2         emotion         June,2000      June 6,2000
    13           79848  1         NaN             May,2001       NaN
    7            79849  2         good life       June,2000      June 5,2000
    21           79850  1         good weather    January,1997   January 1,1997
    25           79851  1         rain &amp; cloudy   January,1997   January 12,1997
    6            79852  2         sad &amp; depressed June,2000      June 5,2000
</code></pre>
<p>I want to use three criteria to match these data: Specific_Date, Date, and topic. <br />
First, I want to use &quot;Date&quot; column to group these data. Next, I try to use &quot;Specific_Date&quot; column to narrow down the scope since some data are lost in this column. Finally, I want to use &quot;topic&quot; column by similar words like word-embedding to make sure which rows in gpo_full can be corresponding with a unique row in CAP_cols. <br />
I have tried to use &quot;Date&quot; column to group the data and merge them into JSON file. However, I am trapped in achieving the next step to narrow down the scope by specific date and topic. <br />
My thought for this output would be like:</p>
<pre><code>{
&quot;Date&quot;: &quot;January,1997&quot;,
&quot;Specific_Date&quot;: &quot;January 12,1997&quot;
&quot;Topic&quot;: {&quot;GPO&quot;: &quot;weather&quot;, &quot;CAP&quot;: &quot;rain &amp; cloudy&quot;}
&quot;GPO&quot;: {
    &quot;hearing_sub_type&quot;: &quot;Oversight&quot;,
    and other columns
}
&quot;CAP&quot;: {
    &quot;majortopic&quot;: &quot;25&quot;,
    &quot;id&quot;: &quot;79851&quot;,
    &quot;Chamber&quot;: &quot;1&quot;
}
},
{
&quot;Date&quot;: &quot;January,1997&quot;,
&quot;Specific_Date&quot;: &quot;January 1,1997&quot;
&quot;Topic&quot;: {&quot;GPO&quot;: &quot;forest&quot;, &quot;CAP&quot;: &quot;many forest&quot;}
&quot;GPO&quot;: {
    &quot;hearing_sub_type&quot;: &quot;General&quot;,
    and other columns
}
&quot;CAP&quot;: {
    &quot;majortopic&quot;: &quot;21&quot;,
    &quot;id&quot;: &quot;79846&quot;,
    &quot;Chamber&quot;: &quot;1&quot;
}
and similar for others}
</code></pre>
<p>I have been thinking for three days and have no idea. Any idea for achieving this would be very helpful! Greatly appreciated!</p>
","python, json, csv, merge, word-embedding","<p>There's a couple of issues with the matching of topics, so you'll need to expand the <code>match_topic()</code> method I used, but I added some logic to see what didn't match at the end.</p>
<p>The <code>results</code> variable contains a list of dict which you can easily save as a  JSON file.</p>
<p>Check the inline comments for the reasoning of the logic I used.</p>
<p><strong>Sidenote:</strong></p>
<p>I would slightly restructure the JSON if I were you. Putting the <code>topic</code> as a key/value pair under the <code>GPO</code> and <code>CAP</code> keys makes more sense to me than having a <code>Topic</code> key with a separate <code>GPO</code> and <code>CAP</code> key/value pair...</p>
<pre class=""lang-py prettyprint-override""><code>import csv
from pprint import pprint
import json


# load gpo_full.csv into a list of dict using
# csv.DictReader &amp; list comprehension
with open(&quot;path/to/file/gpo_full.csv&quot;) as infile:
    gpo_full = [item for item in csv.DictReader(infile)]


# do the same for CAP_cols.csv
with open(&quot;path/to/file/CAP_cols.csv&quot;) as infile:
    cap_cols = [item for item in csv.DictReader(infile)]


def match_topic(gpo_topic: str, cap_topic: str) -&gt; bool:
    &quot;&quot;&quot;We need a function as some of the mapping is not simple

    Args:
        gpo_topic (str): gpo topic
        cap_topic (str): CAP topic

    Returns:
        bool: True if topics match
    &quot;&quot;&quot;
    # this one is simple
    if gpo_topic in cap_topic:
        return True
    # you need to repeat the below conditional check
    # for each custom topic matching
    elif gpo_topic == &quot;weather&quot; and cap_topic == &quot;rain &amp; cloudy&quot;:
        return True 
    # example secondary topic matching
    elif gpo_topic == &quot;foo&quot; and cap_topic == &quot;bar&quot;:
        return True 
    # finally return false for no matches
    return False


# we need this later
gpo_length = len(gpo_full)
results = []
cap_left_over = []
# do the actual mapping
# this could've been done above, but I separated it intentionally
for cap in cap_cols:
    found = False
    # first find the corresponding gpo
    for index, gpo in enumerate(gpo_full):
        if (
            gpo[&quot;Specific_Date&quot;] == cap[&quot;Specific_Date&quot;] # check by date
            and match_topic(gpo[&quot;topic&quot;], cap[&quot;topic&quot;]) # check if topics match
        ):
            results.append({
                &quot;Date&quot;: gpo[&quot;Date&quot;],
                &quot;Specific_Date&quot;: gpo[&quot;Specific_Date&quot;],
                &quot;Topic&quot;: {
                    &quot;GPO&quot;: gpo[&quot;topic&quot;],
                    &quot;CAP&quot;: cap[&quot;topic&quot;]
                },
                &quot;GPO&quot;: {
                    &quot;hearing_sub_type&quot;: gpo[&quot;hearing_sub_type&quot;]
                },
                &quot;CAP&quot;: {
                    &quot;majortopic&quot;: cap[&quot;majortopic&quot;],
                    &quot;id&quot;: cap[&quot;id&quot;],
                    &quot;Chamber&quot;: cap[&quot;Chamber&quot;]
                }
            })
            # pop &amp; break to remove the gpo item
            # this is so you're left over with a list of
            # gpo items that didn't match
            # it also speeds up further matches
            gpo_full.pop(index)
            found = True
            break
    # this is to check if there's stuff left over
    if not found:
        cap_left_over.append(cap)


with open('path/to/file/combined_json.json', 'w') as outfile:
    json.dump(results, outfile, indent=4)


pprint(results)
print(f'\nLength:\n  Results: {len(results)}\n  CAP: {len(cap)}\n  GPO: {gpo_length}')
print('\nLeftover GPO:')
pprint(gpo_full)
print('\nLeftover CAP:')
pprint(cap_left_over)
</code></pre>
<p><strong>OUTPUT</strong><br />
I've removed the <code>pprint(results)</code> from the output, see the JSON further down</p>
<pre class=""lang-none prettyprint-override""><code>Length:
  Results: 5
  CAP: 6
  GPO: 7

Leftover GPO:
[{'Date': 'April,2001',
  'Specific_Date': 'NaN ',
  'hearing_sub_type': 'Oversight',
  'topic': 'people'},
 {'Date': 'June,2000',
  'Specific_Date': 'June 6,2000',
  'hearing_sub_type': 'Oversight',
  'topic': 'depressed'}]

Leftover CAP:
[{'Chamber': '2',
  'Date': 'June,2000',
  'Specific_Date': 'June 6,2000',
  'id': '79847',
  'majortopic': '4',
  'topic': 'emotion'},
 {'Chamber': '1',
  'Date': 'May,2001',
  'Specific_Date': 'NaN',
  'id': '79848',
  'majortopic': '13',
  'topic': 'NaN'}]
</code></pre>
<p><strong>path/to/file/gpo_full.csv</strong></p>
<pre><code>Date,hearing_sub_type,topic,Specific_Date
&quot;January,1997&quot;,Oversight,weather,&quot;January 12,1997&quot;
&quot;June,2000&quot;,General,life,&quot;June 5,2000&quot;
&quot;January,1997&quot;,General,forest,&quot;January 1,1997&quot;
&quot;April,2001&quot;,Oversight,people,NaN 
&quot;June,2000&quot;,Oversight,depressed,&quot;June 6,2000&quot;
&quot;January,1997&quot;,General,weather,&quot;January 1,1997&quot;
&quot;June,2000&quot;,Oversight,depressed,&quot;June 5,2000&quot;
</code></pre>
<p><strong>path/to/file/CAP_cols.csv</strong></p>
<pre><code>majortopic,id,Chamber,topic,Date,Specific_Date
21,79846,1,many forest,&quot;January,1997&quot;,&quot;January 1,1997&quot;
4,79847,2,emotion,&quot;June,2000&quot;,&quot;June 6,2000&quot;
13,79848,1,NaN,&quot;May,2001&quot;,&quot;NaN&quot;
7,79849,2,good life,&quot;June,2000&quot;,&quot;June 5,2000&quot;
21,79850,1,good weather,&quot;January,1997&quot;,&quot;January 1,1997&quot;
25,79851,1,rain &amp; cloudy,&quot;January,1997&quot;,&quot;January 12,1997&quot;
6,79852,2,sad &amp; depressed,&quot;June,2000&quot;,&quot;June 5,2000&quot;
</code></pre>
<p><strong>path/to/file/combined_json.json</strong></p>
<pre class=""lang-json prettyprint-override""><code>[
    {
        &quot;Date&quot;: &quot;January,1997&quot;,
        &quot;Specific_Date&quot;: &quot;January 1,1997&quot;,
        &quot;Topic&quot;: {
            &quot;GPO&quot;: &quot;forest&quot;,
            &quot;CAP&quot;: &quot;many forest&quot;
        },
        &quot;GPO&quot;: {
            &quot;hearing_sub_type&quot;: &quot;General&quot;
        },
        &quot;CAP&quot;: {
            &quot;majortopic&quot;: &quot;21&quot;,
            &quot;id&quot;: &quot;79846&quot;,
            &quot;Chamber&quot;: &quot;1&quot;
        }
    },
    {
        &quot;Date&quot;: &quot;June,2000&quot;,
        &quot;Specific_Date&quot;: &quot;June 5,2000&quot;,
        &quot;Topic&quot;: {
            &quot;GPO&quot;: &quot;life&quot;,
            &quot;CAP&quot;: &quot;good life&quot;
        },
        &quot;GPO&quot;: {
            &quot;hearing_sub_type&quot;: &quot;General&quot;
        },
        &quot;CAP&quot;: {
            &quot;majortopic&quot;: &quot;7&quot;,
            &quot;id&quot;: &quot;79849&quot;,
            &quot;Chamber&quot;: &quot;2&quot;
        }
    },
    {
        &quot;Date&quot;: &quot;January,1997&quot;,
        &quot;Specific_Date&quot;: &quot;January 1,1997&quot;,
        &quot;Topic&quot;: {
            &quot;GPO&quot;: &quot;weather&quot;,
            &quot;CAP&quot;: &quot;good weather&quot;
        },
        &quot;GPO&quot;: {
            &quot;hearing_sub_type&quot;: &quot;General&quot;
        },
        &quot;CAP&quot;: {
            &quot;majortopic&quot;: &quot;21&quot;,
            &quot;id&quot;: &quot;79850&quot;,
            &quot;Chamber&quot;: &quot;1&quot;
        }
    },
    {
        &quot;Date&quot;: &quot;January,1997&quot;,
        &quot;Specific_Date&quot;: &quot;January 12,1997&quot;,
        &quot;Topic&quot;: {
            &quot;GPO&quot;: &quot;weather&quot;,
            &quot;CAP&quot;: &quot;rain &amp; cloudy&quot;
        },
        &quot;GPO&quot;: {
            &quot;hearing_sub_type&quot;: &quot;Oversight&quot;
        },
        &quot;CAP&quot;: {
            &quot;majortopic&quot;: &quot;25&quot;,
            &quot;id&quot;: &quot;79851&quot;,
            &quot;Chamber&quot;: &quot;1&quot;
        }
    },
    {
        &quot;Date&quot;: &quot;June,2000&quot;,
        &quot;Specific_Date&quot;: &quot;June 5,2000&quot;,
        &quot;Topic&quot;: {
            &quot;GPO&quot;: &quot;depressed&quot;,
            &quot;CAP&quot;: &quot;sad &amp; depressed&quot;
        },
        &quot;GPO&quot;: {
            &quot;hearing_sub_type&quot;: &quot;Oversight&quot;
        },
        &quot;CAP&quot;: {
            &quot;majortopic&quot;: &quot;6&quot;,
            &quot;id&quot;: &quot;79852&quot;,
            &quot;Chamber&quot;: &quot;2&quot;
        }
    }
]
</code></pre>
",1,0,203,2021-07-19 14:43:32,https://stackoverflow.com/questions/68442446/merging-two-csv-files-based-on-many-criteria
Fast Text unsupervised model loss,"<p>I wanted to create a fastText unsupervised model for my text data of size 1GB. I'm using fastText command line tool to implement the model training process.</p>
<pre><code>./fasttext skipgram -input PlainText.txt -output FastText-PlainText- -dim 50 -epoch 50 
</code></pre>
<p>The above are few arguments I used for created word representation.</p>
<pre><code>Read 207M words
Number of words:  501986
Number of labels: 0
Progress:  97.5% words/sec/thread:   87224 lr:  0.001260 avg.loss:  0.089536 ETA:   0h 4m 9s
</code></pre>
<p>Here, in the output of the fastText command, I see this avg.loss and the learning rate has been decreased from default (0.5) to 0.001. I don't really understand, what does this avg.loss mean and why is the learning rate is dropped?</p>
<ol>
<li>Should I want to increase the epoch to make fastText to learn my data better?</li>
<li>Can I use any loss function to improve the loss? If yes, what kind of loss function will be better?</li>
<li>And how can I evaluate my fastText model's learning whether is good or bad?</li>
<li>Just out of interest, Can I use wordngrams to make my model learn better with context in unsupervised learning?</li>
</ol>
","word-embedding, fasttext","<p>I can't answer all your questions in depth, but I try to give you some advice.</p>
<ul>
<li>you can understand better <em>avg.loss</em>, reading <a href=""https://github.com/facebookresearch/fastText/issues/690#issuecomment-441570351"" rel=""noreferrer"">this thread</a></li>
<li>learning rate is updated according <em>lrUpdateRate</em> option (read <a href=""https://fasttext.cc/docs/en/options.html"" rel=""noreferrer"">this</a>).</li>
<li>in general, increasing the number of epochs can improve learning. However, as you can read in <a href=""https://arxiv.org/pdf/1906.06669.pdf"" rel=""noreferrer"">this paper</a>, the most popular language models have a number of epochs between 10 and 100.</li>
<li>default loss function is softmax. You can also choose hs (hierarchical softmax) or ns. You can read more in the <a href=""https://fasttext.cc/docs/en/supervised-tutorial.html#advanced-readers-hierarchical-softmax"" rel=""noreferrer"">official tutorial</a>.</li>
<li>if you want to learn more about the effects of the <em>ws</em> and <em>wordngrams</em> parameters, you can read <a href=""https://stackoverflow.com/questions/57507056/difference-between-max-length-of-word-ngrams-and-size-of-context-window"">this answer</a>.</li>
</ul>
",5,1,1673,2021-07-21 09:10:36,https://stackoverflow.com/questions/68466879/fast-text-unsupervised-model-loss
How to tune FastText parameter for OOV word?,"<p>I already heard that FastText is generating OOV word vectors using its n-gram's. It is already automatically built-in at FastText architecture or we should like to tune specific parameters to it? like an oov_tokens in Keras tokenizer. I already looking for what parameters to tune in Fast Text but I couldn't find any.</p>
<p>If anyone knows and wants to share their knowledge I would be very appreciative of that.</p>
<p>Thank you.</p>
","parameters, word-embedding, fasttext, oov","<p>Vector generation for OOV words is integrated into fastText (at least in the original implementation by Facebook).</p>
<p><strong>To generate these vectors, fastText uses subword n-grams</strong>. To learn more, you can read <a href=""https://github.com/facebookresearch/fastText/issues/475"" rel=""nofollow noreferrer"">this thread</a> and <a href=""https://amitness.com/2020/06/fasttext-embeddings/"" rel=""nofollow noreferrer"">this visual guide</a>.</p>
<p>For this reason, the <strong>parameters</strong> that most influence the creation of vectors for OOV words are the following:</p>
<ul>
<li><code>minn</code> (min length of char ngram)</li>
<li><code>maxn</code> (max length of char ngram)</li>
</ul>
<p>For more information about fastText options/parameters, see <a href=""https://fasttext.cc/docs/en/options.html"" rel=""nofollow noreferrer"">the official documentation</a>.</p>
",1,0,670,2021-07-26 02:13:49,https://stackoverflow.com/questions/68523810/how-to-tune-fasttext-parameter-for-oov-word
Structure of Gensim Word Embedding corpus,"<p>I want to train a word2vec model using Gensim. I preprocessed my corpus, which is made of hundreds of thousands of articles from a specific newspaper. I preprocessed them (lower casing, lemmatizing, removing stop words and punctuations, etc.) and then make a list of lists, in which each element is a list of words.</p>
<pre><code>corpus = [['first', 'sentence', 'second', 'dictum', 'third', 'saying', 'last', 'claim'],
          ['first', 'adage', 'second', 'sentence', 'third', 'judgment', 'last', 'pronouncement']]
</code></pre>
<p>I wanted to know if it is the right way, or it should be like the following:</p>
<pre><code>corpus = [['first', 'sentence'], ['second', 'dictum'], ['third', 'saying'], ['last', 'claim'], ['first', 'adage'], ['second', 'sentence'], ['third', 'judgment'], ['last', 'pronouncement']]
</code></pre>
","gensim, word2vec, word-embedding, corpus","<p>Both would minimally work.</p>
<p>But in the second, no matter how big your <code>window</code> parameter, the fact all texts are no more than 2 tokens long means words will only affect their immediate neighbors. That's probably not what you want.</p>
<p>There's no real harm in longer texts, except to note that:</p>
<ul>
<li>Tokens all in the same list will appear in each other's <code>window</code>-sized neighborhood - so don't run words together that shouldn't imply any realistic use alongside each other. (But, in large-enough corpuses, even the noise of some run-together unrelated texts won't make much difference, swamped by the real relationships in the bulk of the texts.)</li>
<li>Each text shouldn't be more than 10,000 tokens long, as an internal implementation limit will cause any tokens beyond that limit to be ignored.</li>
</ul>
",2,0,73,2021-07-27 21:45:48,https://stackoverflow.com/questions/68552107/structure-of-gensim-word-embedding-corpus
"Gensim doc2vec produce more vectors than given documents, when I pass unique integer id as tags","<p>I'm trying to make documents vectors of gensim example using doc2vec.
I passed TaggedDocument which contains 9 docs and 9 tags.</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
idx = [0,1,2,3,4,5,6,7,100]
documents = [TaggedDocument(doc, [i]) for doc, i in zip(common_texts, idx)]
model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)
</code></pre>
<p>and it produces 101 vectors like this image.
<a href=""https://i.sstatic.net/b1ciY.png"" rel=""nofollow noreferrer"">gensim doc2vec produced 101 vectors</a></p>
<p>and what I want to know is</p>
<ol>
<li>How can I be sure that the tag I passed is attached to the right vector?</li>
<li>How did the vectors with the tags which I didn't pass (8~99 in my case) come out? Were they computed as a blank?</li>
</ol>
","machine-learning, gensim, word-embedding, doc2vec","<p>If you use plain ints as your document-tags, then the <code>Doc2Vec</code> model will allocate enough doc-vectors for every int up to the highest int you provide - even if you don't use some of those ints.</p>
<p>This assumption, that all ints up to the highest declared are used, allows the code to avoid creating a redundant {tag -&gt; slot} dictionary, saving a little memory. That specific potential savings is the main reason for supporting plain ints (rather than unique strings) as tag names.</p>
<p>Any such doc-vectors allocated but never subject to any traiing will be randomly-initialized the same as others - but never adjusted by training.</p>
<p>If you want to use plain int tag names, you should either be comfortable with this over-allocation, or make sure you only use all contiguous int IDs from <code>0</code> to your max ID, with none ununused. But unless your training data is very large, using unique string tags, and allowing the {tag -&gt; slot} dictionary to be created, is straightforward and not too expensive in memory.</p>
<p>(Separately: <code>min_count=1</code> is almost always a bad idea in these algorithms, as discarding rare tokens tends to give better results than letting their thin example usages interfere with other training.)</p>
",1,0,201,2021-07-31 16:39:33,https://stackoverflow.com/questions/68604006/gensim-doc2vec-produce-more-vectors-than-given-documents-when-i-pass-unique-int
Extracting embedding values of NLP pertained models from tokenized strings,"<p>I am using <a href=""https://huggingface.co/transformers/pretrained_models.html"" rel=""nofollow noreferrer"">huggingface</a> pipeline to extract embeddings of words in a sentence. As far as I know, first a sentence will be turned into a tokenized strings. I think the length of the tokenized string might not be equal to the number of words in the original sentence. I need to retrieve word embedding of a particular sentence.</p>
<p>For example, here is my code:</p>
<pre><code>#https://discuss.huggingface.co/t/extracting-token-embeddings-from-pretrained-language-models/6834/6

from transformers import pipeline, AutoTokenizer, AutoModel
import numpy as np
import re

model_name = &quot;xlnet-base-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

tokenizer.add_special_tokens({'pad_token': '[PAD]'})
model.resize_token_embeddings(len(tokenizer))

model_pipeline = pipeline('feature-extraction', model=model_name, tokenizer=tokenizer)

def find_wordNo_sentence(word, sentence):
    
    print(sentence)
    splitted_sen = sentence.split(&quot; &quot;)
    print(splitted_sen)
    index = splitted_sen.index(word)


    for i,w in enumerate(splitted_sen):
        if(word == w):
            return i

    print(&quot;not found&quot;) #0 base




  
def return_xlnet_embedding(word, sentence):
        
    word = re.sub(r'[^\w]', &quot; &quot;, word)
    word = &quot; &quot;.join(word.split())
    
    sentence = re.sub(r'[^\w]', ' ', sentence)
    sentence = &quot; &quot;.join(sentence.split())
    
    id_word = find_wordNo_sentence(word, sentence)
    
   
        
    try:
        data = model_pipeline(sentence)
        
        n_words = len(sentence.split(&quot; &quot;))
        #print(sentence_emb.shape)
        n_embs  = len(data[0])
        print(n_embs, n_words)
        print(len(data[0]))
    
        if (n_words != n_embs):
            &quot;There is extra tokenized word&quot;
            
            
        results = data[0][id_word]  
        return np.array(results)
    
    except:
        return &quot;word not found&quot;

return_xlnet_embedding('your', &quot;what is your name?&quot;)
</code></pre>
<p>Then the output is:</p>
<blockquote>
<p>what is your name ['what', 'is', 'your', 'name'] 6 4 6</p>
</blockquote>
<p>So the length of tokenized string that is fed to the pipeline is two more than number of my words.
How can I find which one (among these 6 values) are the embedding of my word?</p>
","python, nlp, tokenize, word-embedding, huggingface-tokenizers","<p>As you may know, huggingface tokenizer contains frequent subwords as well as complete ones. So if you are willing to extract word embeddings for some tokens you should consider that may contain more than one vector! In addition, huggingface pipelines encode input sentences at the first steps and this would be performed by adding special tokens to beginning &amp; end of the actual sentence.</p>
<pre><code>string = 'This is a test for clarification'
print(pipeline.tokenizer.tokenize(string))
print(pipeline.tokenizer.encode(string))
</code></pre>
<p>output:</p>
<pre><code>['this', 'is', 'a', 'test', 'for', 'cl', '##ari', '##fication']

[101, 2023, 2003, 1037, 3231, 2005, 18856, 8486, 10803, 102]
</code></pre>
",1,0,722,2021-08-17 06:56:11,https://stackoverflow.com/questions/68812870/extracting-embedding-values-of-nlp-pertained-models-from-tokenized-strings
How to combine embeddins vectors of bert with other features?,"<p>I am working on a classification task with 3 labels (0,1,2 = neg, pos, neu). Data are sentences. So to produce vectors/embeddings of sentences, I use a Bert encoder to get embeddings for each sentence and then I used a simple knn to make predictions.</p>
<p>My data look like this : each sentence has a label and other numerical value of classification.</p>
<p>For example, my data look like this</p>
<pre><code>Sentence embeddings_BERT level sub-level label

je mange  [0.21, 0.56]    2     2.1      pos
il hait   [0.25, 0.39]   3     3.1      neg
.....
</code></pre>
<p>As you can see each sentence has other categories but the are not the final one but indices to help figure the label when a human annotated the data. I want my model to take into consideration those two values when predicting the label. I was wondering if I have to concatenate them with the embeddings generate by the bert encoding or is there another way ?</p>
","python, python-3.x, bert-language-model, word-embedding","<p>There is not one perfect way to tackle this problem, but a simple solution will be to concat the bert embeddings with hard-coded features. The BERT embeddings (sentence embeddings) will be of dimension 768 (if you have used BERT base). These embeddings can be treated as features of the sentence itself. The additional features can be concatenated to form a higher dimensional vector. If the features are categorical, it will be ideal to convert to one-hot vectors and concatenate them. For example, if you want to use <code>level</code> in your example as set of input features, it will be best to convert it into one-hot feature vector and then concatenate with BERT embeddings. However, in some cases, your hard coded features can be dominant feature to bias the classifier, and in some other cases, it can have no influence at all. It all depends on the data that you have.</p>
",4,3,3595,2021-08-17 10:45:26,https://stackoverflow.com/questions/68815926/how-to-combine-embeddins-vectors-of-bert-with-other-features
Bert model output interpretation,"<p>I searched a lot for this but havent still got a clear idea so I hope you can help me out:</p>
<p>I am trying to translate german texts to english! I udes this code:</p>
<pre><code>
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]

results = model(batch)  
</code></pre>
<p>Which returned me a size error! I fixed this problem (thanks to the community: <a href=""https://github.com/huggingface/transformers/issues/5480"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/5480</a>) with switching the last line of code to:</p>
<pre><code>results = model(input_ids = batch,decoder_input_ids=batch)
</code></pre>
<p>Now my output looks like a really long array. What is this output precisely? Are these some sort of word embeddings? And if yes: How shall I go on with converting these embeddings to the texts in the english language? Thanks alot!</p>
","translation, bert-language-model, huggingface-transformers, word-embedding","<p>Adding to Timbus's answer,</p>
<blockquote>
<p>What is this output precisely? Are these some sort of word embeddings?</p>
</blockquote>
<p><code>results</code> is of type <code>&lt;class 'transformers.modeling_outputs.Seq2SeqLMOutput'&gt;</code> and you can do</p>
<pre><code>results.__dict__.keys()
</code></pre>
<p>to check that <code>results</code> contains the following:</p>
<pre><code>dict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'])
</code></pre>
<p>You can read more about this class in the <a href=""https://huggingface.co/transformers/v3.3.1/main_classes/output.html#seq2seqlmoutput"" rel=""nofollow noreferrer"">huggingface documentation</a>.</p>
<blockquote>
<p>How shall I go on with converting these embeddings to the texts in the
english language?</p>
</blockquote>
<p>To interpret the text in English, you can use <code>model.generate</code> which is easily decodable in the following way:</p>
<pre><code>predictions = model.generate(batch)
english_text = tokenizer.batch_decode(predictions)
</code></pre>
",1,1,1444,2021-08-17 13:11:18,https://stackoverflow.com/questions/68817989/bert-model-output-interpretation
the repetitions in Gensim Word2Vec training corpus,"<p>I am using <code>Gensim</code> to train a <code>Word2Vec</code> embedding on different corpora, pertaining to different years, to compare the embedding vectors.
My question is: if I repeat the documents of a specific year twice and documents of another year just once, do the resulting embeddings give more weight to the repeated documents?
I have in my mind to make a corpus that gives more weight to recent documents and less weight to documents from far past.
I simply train the model on my <code>Line Sentence</code> corpus file.</p>
<pre><code>Word2Vec(corpus_file=corpus, vector_size=100, window=5, min_count=5, workers=4)
</code></pre>
","python, gensim, word2vec, word-embedding, corpus","<p>Sure, repeating some texts (even more than the re-iterations controlled by the <code>epochs</code> count) means they'll have more influence on the final model.</p>
<p>In general, repeating identical texts isn't as good as truly varied alternative examples of the same words. For example, if you only have one text using a certain word, repeating it 5 times might make the word survive the <code>min_count=5</code> cutoff, but the lack of many subtly-contrasting appearances means its final vector will only reflect that one peculiar, repeated use. The kind of good relative word-vector positions that people are usually seeking need a training tug-of-war between all the ways a word is used.</p>
<p>But, in this case, you should still have many examples, you're just overtraining on some of them.</p>
<p>Do note that it might be a <em>little</em> bit better to ensure the repeats are shuffled throughout the whole corpus, at least once before training begins, rather than clustered all together. (Repeating a text 10 times in a row will overtrain those words/contexts – but not in as balanced of a way as if interleaved with all the other differently-weighted training.)</p>
<p>And, that you might not want to turn all the 1-occurrence words in a subset that you repeat 5 times to automatically survive the <code>min_count</code> cut - because they still just have that one true weak context example. So you might want to learn the vocabulary from a non-reweighted corpus, but then train on the new corpus with artificial repeats (being sure to provide your <code>.train()</code> call with the right new <code>total_examples</code> count for it to report progress &amp; adjust the learning-rate properly).</p>
",1,0,395,2021-08-21 11:03:35,https://stackoverflow.com/questions/68872427/the-repetitions-in-gensim-word2vec-training-corpus
Using weight from a Gensim Word2Vec model as a starting point of another model,"<p>I have two corpora that are from the same field, but with a temporal shift, say one decade. I want to train Word2vec models on them, and then investigate the different factors affecting the semantic shift.</p>
<p>I wonder how should I initialize the second model with the first model's embeddings to avoid as much as possible the effect of variance in co-occurrence estimates.</p>
","python, gensim, word2vec, word-embedding, fine-tuning","<p>At a naive &amp; easy level, you can just load one existing model, and <code>.train()</code> on new data. But note if doing that:</p>
<ul>
<li>Any words not already known by the model will be ignored, and the word-frequencies that feed algorithmic steps will only be from the initial survey</li>
<li>While all words in the current corpus will get as many training-updates as their appearances (&amp; your <code>epochs</code> setting) dictate, and thus be nudged arbitrarily-far from their original-model locations, other words from the seed model will stay exactly where they were. But, it's only the interleaved tug-of-war between words in the same training session that makes them usefully comparable. So doing this sequential training – updating only some words in a new training session – is likely to degrade the meaningfulness of word-to-word comparisons, in hard-to-measure ways.</li>
</ul>
<p>Another approach that might be woth trying could be to train single model over the combined corpus - but transform/repeat the era-specific texts/words in certain ways to be able to distinguish earlier-usages from later-usages. There are more details about this suggestion in the context of word-vectors varying over usage-eras in a couple previous answers:</p>
<p><a href=""https://stackoverflow.com/a/57400356/130288"">https://stackoverflow.com/a/57400356/130288</a></p>
<p><a href=""https://stackoverflow.com/a/59095246/130288"">https://stackoverflow.com/a/59095246/130288</a></p>
",1,0,403,2021-08-28 10:15:35,https://stackoverflow.com/questions/68963361/using-weight-from-a-gensim-word2vec-model-as-a-starting-point-of-another-model
Gensim fast text get vocab or word index,"<p>Trying to use <code>gensim's fasttext</code>, testing the sample code from <code>gensim</code> with a small change of replacing the arguement to <code>corpus_iterable</code></p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html</a></p>
<p><code>gensim_version == 4.0.1</code></p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts  # some example sentences

print(common_texts[0])
['human', 'interface', 'computer']
print(len(common_texts))
9
model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)
</code></pre>
<p>It works, but is there any way to <code>get the vocab</code> for the model. For example, in <code>Tensorflow Tokenizer</code> there is a <code>word_index</code> which will return <code>all the words</code>. Is there something similar here?</p>
","machine-learning, nlp, gensim, word-embedding, fasttext","<p>The model stores word vectors in <code>.wv</code> object. I don't know which gensim version you're using, but for Gensim 4 you can get keyed vectors by calling <code>model.wv.key_to_index</code>. You'll get a dict with words and their indices</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts  # some example sentences
print(common_texts[0])
# ['human', 'interface', 'computer']
print(len(common_texts))
# 9
model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)
# get vocab keys with indices
vocab = model.wv.key_to_index
print(vocab)
# output
# {'system': 0, 'graph': 1, 'trees': 2, 'user': 3, 'minors': 4, 'eps': 5, 'time': 6, 
# 'response': 7, 'survey': 8, 'computer': 9, 'interface': 10, 'human': 11}
</code></pre>
",2,0,2208,2021-09-02 02:42:48,https://stackoverflow.com/questions/69023485/gensim-fast-text-get-vocab-or-word-index
gensim word2vec vocabulary size fluctuates up &amp; down as corpus grows despite `max_vocab_size` setting,"<p>I am training word embeddings using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim Word2Vec</a> model with a multi-million sentence corpus that is made of 3 million unique tokens with <code>max_vocab_size = 32_000</code>.</p>
<p>Even though I set <code>min_count = 1</code>, model creates a vocabulary of far less than 32_000. When I use a subset of the corpus, vocabulary size increases!</p>
<p>In order to troubleshoot, I set up an experiment where I control the size of vocabulary with different sized subcorpus. The size of the vocabulary flactuates!</p>
<p>You can re-produce with the code below:</p>
<pre><code>import string
import numpy as np
from gensim.models import Word2Vec

letters = list(string.ascii_lowercase)

# creating toy sentences
sentences = []
number_of_sentences = 100_000

for _ in range(number_of_sentences):
    number_of_tokens = np.random.randint(1, 15, 1)[0]
    sentence = []
    for i in range(number_of_tokens):
        token = &quot;&quot;
        len_of_token = np.random.randint(1, 5, 1)[0]
        for j in range(len_of_token):
            token += np.random.choice(letters)
        sentence.append(token)
    sentences.append(sentence)

# Sanity check to ensure that input data is a list of list of strings(tokens)
for _ in range(4):
    print(np.random.choice(sentences))

# collecting some statistics about tokens
flattened = []
for sublist in sentences:
    for item in sublist:
        flattened.append(item)
        
unique_tokens = {}
for token in flattened:
    if token not in unique_tokens:
        unique_tokens[token] = len(unique_tokens)

print('Number of tokens:', f'{len(flattened):,}')
print('Number of unique tokens:', f'{len(unique_tokens):,}')


# gensim model
vocab_size = 32_000
min_count = 1
collected_data = []
for num_sentence in range(5_000, number_of_sentences + 5_000, 5_000):
    model = Word2Vec(min_count=min_count, max_vocab_size= vocab_size)
    model.build_vocab(sentences[:num_sentence])

    collected_data.append((num_sentence, len(model.wv.key_to_index)))

for duo in collected_data:
    print('Vocab size of', duo[1], 'for', duo[0], 'number of sentences!')
</code></pre>
<p>Output:</p>
<pre><code>['cpi', 'bog', 'df', 'tgi', 'xck', 'kkh', 'ktw', 'ay']
['z', 'h', 'w', 'jek', 'w', 'dqm', 'wfb', 'agq', 'egrg']
['kgwb', 'lahf', 'kzx', 'd', 'qdok', 'xka', 'hbiz', 'bjo', 'fvk', 'j', 'hx']
['old', 'c', 'ik', 'n', 'e', 'n', 'o', 'r', 'ehx', 'dlud', 'd']

Number of tokens: 748,383
Number of unique tokens: 171,485

Vocab size of 16929 for 5000 number of sentences!
Vocab size of 30314 for 10000 number of sentences!
Vocab size of 19017 for 15000 number of sentences!
Vocab size of 31394 for 20000 number of sentences!
Vocab size of 19564 for 25000 number of sentences!
Vocab size of 31831 for 30000 number of sentences!
Vocab size of 19543 for 35000 number of sentences!
Vocab size of 31744 for 40000 number of sentences!
Vocab size of 19536 for 45000 number of sentences!
Vocab size of 31642 for 50000 number of sentences!
Vocab size of 18806 for 55000 number of sentences!
Vocab size of 31255 for 60000 number of sentences!
Vocab size of 18497 for 65000 number of sentences!
Vocab size of 31166 for 70000 number of sentences!
Vocab size of 18142 for 75000 number of sentences!
Vocab size of 30886 for 80000 number of sentences!
Vocab size of 17693 for 85000 number of sentences!
Vocab size of 30390 for 90000 number of sentences!
Vocab size of 17007 for 95000 number of sentences!
Vocab size of 30196 for 100000 number of sentences!
</code></pre>
<p>I tried increasing <code>min_count</code> but it did not help this flactuation of vocabulary size. What am I missing?</p>
","python, nlp, gensim, word2vec, word-embedding","<p>In Gensim, the <code>max_vocab_size</code> parameter is a <em>very</em> crude mechanism to limit RAM usage during the initial scan of the training corpus to discover the vocabulary. You should only use this parameter if it's the only way to work around RAM problems.</p>
<p>Essentially: try without using <code>max_vocab_size</code>. If you want control over which words are retained, use alternate parameters like <code>min_count</code> (to discard words less-frequent than a certain threshold) or <code>max_final_vocab</code> (to take no more than a set number of the most-frequent words).</p>
<p><em>If and only if</em> you hit out-of-memory errors (or massive virtual-memory swapping), then consider using <code>max_vocab_size</code>.</p>
<p>But even then, because of the way it works, you still wouldn't want to set <code>max_vocab_size</code> to the actual final size you want. Instead, you should set it to some value much, much larger - but just small enough to not exhaust your RAM.</p>
<p>This allows the most accurate possible word-counts <em>before</em> other parameters (like <code>min_count</code> &amp; <code>max_final_vocab</code>) are applied.</p>
<p>If you instead use a low <code>max_vocab_size</code>, the running survey will prematurely trim the counts any time the number of known words reaches that value. That is, as soon as the interim count reaches that many entries, say <code>max_vocab_size=32000</code>, many of the least-frequent counts are <em>forgotten</em> to cap memory usage (and more each time the threshold is reached).</p>
<p>That makes all final counts approximate (based on how often a term missed the cutoff), and means the final number of unique tokens in the full survey will be some value even less than <code>max_vocab_size</code>, somewhat arbitrarily based on how recently a forgetting-trim was triggered. (Hence, the somewhat random, but always lower than <code>max_vocab_size</code>, counts seen in your experiment output.)</p>
<p>So: <code>max_vocab_size</code> is unlikely to do what most people want, or in a predictable way. Still, it can help a fuzzy survey complete for extreme corpora where unique terms would otherwise overflow RAM.</p>
<p>Separately: <code>min_count=1</code> is usually a bad idea in word2vec, as words that lack sufficient varied usage examples won't themselves get good word-vectors, but leaving all such poorly-represented words in the training data tends to serve as noise that dilutes (&amp; delays) what can be learned about adequately-frequent words.</p>
",2,0,682,2021-09-19 21:01:40,https://stackoverflow.com/questions/69247049/gensim-word2vec-vocabulary-size-fluctuates-up-down-as-corpus-grows-despite-ma
How do I feed an array of Tokenized Sentences to Word2vec to get embeddings?,"<p>Hi all: I cannot figure out the code required to get embeddings from a word2vec model.</p>
<p>Here is how my df is structured (it is some android based log):</p>
<p>logDateTime | lineNum | processID | threadID | priority | app | message | eventTemplate | eventID
ts            int       int         int        str        str   str       str             str</p>
<p>Essentially, I created a unique subset of events out of log messages and assigned a template with an associated id:</p>
<pre><code>def eventCreation(df):
    df['eventTemplate'] = df['message'].str.replace('\d+', '*')
    df['eventTemplate'] = df['eventTemplate'].str.replace('true', '*')
    df['eventTemplate'] = df['eventTemplate'].str.replace('false', '*')
    df['eventID'] = df.groupby(df.eventTemplate.tolist(), sort=False).ngroup() + 1
    df['eventID'] = 'E'+df['eventID'].astype(str)

def seqGen(arr, k):
    for i in range(len(arr)-k+1):
        yield arr[i:i+k]

#define the variables here
cwd = os.getcwd()
#create a dataframe of the logs concatenated
df = pd.DataFrame.from_records(process_files(cwd,getFiles))
# call functions to establish df
cleanDf(df)
featureEng(df)
eventCreation(df)
df['eventToken'] = df.eventTemplate.apply(lambda x: word_tokenize(x))
seq = []
eventArray = df[[&quot;eventToken&quot;]].to_numpy()
for sequence in seqGen(eventArray, 9):
    seq.append(eventArray)
</code></pre>
<p>So, 'seq' ends up looking like this:</p>
<pre><code>[array([['[*,com.blah.blach.blahMainblach] '],
        ['[*,*,*,com.blah.blah/.permission.ui.blah,finish-imm] '],
        ['[*,*,*,*,startingNewTask] '],
        ...,
        ['mfc, isSoftKeyboardVisible in WMS : * '],
        ['mfc, isSoftKeyboardVisible in WMS : * '],
        ['Calling a method in the system process without a qualified user: android.app.ContextImpl.startService:* android.content.ContextWrapper.startService:* android.content.ContextWrapper.startService:* com.blahblah.usbmountreceiver.USBMountReceiver.onReceive:* android.app.ActivityThread.handleReceiver:* ']],
       dtype=object),
</code></pre>
<p>The sequences are arrays with lists of tokenized log messages. The plan was after training the model, I can get the embedding of a log event by multiplying the onehot vector and the weight matrix... there is more to do, but I am stuck at getting the embeddings.</p>
<p>I am a newbie trying to develop a solution for anomaly detection.</p>
","python-3.x, logging, word2vec, word-embedding, anomaly-detection","<p>If you're using the Gensim library in Python for its <code>Word2Vec</code> implementation, it wants its corpus as a <em>re-iterable sequence</em> where each item is itself a <em>list of string tokens</em>.</p>
<p>A list which itself has each item as a list-of-string-tokens would work.</p>
<p>Your <code>seq</code> is close, but:</p>
<ol>
<li>It doesn't need to be (&amp; thus probably shouldn't be) a <code>numpy</code> array of objects.</li>
<li>Each of your <code>object</code> items is a <code>list</code> (good) but each has only has a single untokenized string inside (bad). You need to break those strings into the individual 'words' that you want the model to learn.</li>
</ol>
",0,0,487,2021-09-26 15:29:58,https://stackoverflow.com/questions/69336366/how-do-i-feed-an-array-of-tokenized-sentences-to-word2vec-to-get-embeddings
Keeping Numbers in Doc2Vec Tokenization,"<p>I’m in the process of trying to get document similarity values for a corpus of approximately 5,000 legal briefs with Doc2Vec (I recognize that the corpus may be a little bit small, but this is a proof-of-concept project for a larger corpus of approximately 15,000 briefs I’ll have to compile later).</p>
<p>Basically, every other component in the creation of the model is going relatively well so far – each brief I have is in a text file within a larger folder, so I compiled them in my script using <code>glob.glob</code> – but I’m running into a tokenization problem.  The difficulty is, as these documents are legal briefs, they contain numbers that I’d like to keep, and many of the guides I’ve been using to help me write the code use Gensim’s simple preprocessing, which I believe eliminates digits from the corpus, in tandem with the TaggedDocument feature. However, I want to do as little preprocessing on the texts as possible.</p>
<p>Below is the code I’ve used, and I’ve tried swapping simple_preprocess for genism.utils.tokenize, but when I do that, I get generator objects that don’t appear workable in my final Doc2Vec model, and I can’t actually see how the corpus looks. When I’ve tried to use other tokenizers, like <code>nltk</code>, I don’t know how to fit that into the TaggedDocument component.</p>
<pre><code>brief_corpus = []
for brief_filename in brief_filenames:
    with codecs.open(brief_filename, &quot;r&quot;, &quot;utf-8&quot;) as brief_file:
        brief_corpus.append(
            gensim.models.doc2vec.TaggedDocument(
                gensim.utils.simple_preprocess( 
                    brief_file.read()),
                    [&quot;{}&quot;.format(brief_filename)])) #tagging each brief with its filename
</code></pre>
<p>I’d appreciate any advice that anyone can give that would help me combine a tokenizer that just separated on whitespace and didn’t eliminate any numbers with the TaggedDocument feature. Thank you!</p>
<p><strong>Update:</strong> I was able to create a rudimentary code for some basic tokenization (I do plan on refining it further) without having to resort to Gensim's simple_preprocessing function. However, I'm having difficulty (again!) when using the TaggedDocument feature - but this time, the tags (which I want to be the file names of each brief) don't match the tokenized document. Basically, each document has a tag, but it's not the right one.</p>
<p>Can anyone possibly advise where I might have gone wrong with the new code below? Thanks!</p>
<pre><code>briefs = []
BriefList = [p for p in os.listdir(FILEPATH) if p.endswith('.txt')]
for brief in BriefList:
     str = open(FILEPATH + brief,'r').read()
     tokens = re.findall(r&quot;[\w']+|[.,!?;]&quot;, str)
     tagged_data = [TaggedDocument(tokens, [brief]) for brief in BriefList]
     briefs.append(tagged_data)
</code></pre>
","python, tokenize, word-embedding, doc2vec","<p>You're likely going to want to write your own preprocessing/tokenization functions. But don't worry, it's not hard to outdo Gensim's <code>simple_preprocess</code>, even with very crude code.</p>
<p>The only thing <code>Doc2Vec</code> needs as the <code>words</code> of a <code>TaggedDocument</code> is a list of string tokens (typically words).</p>
<p>So first, you might be surprised how well it works to just do a default Python string <code>.split()</code> on your raw strings - which just breaks text on whitespace.</p>
<p>Sure, a bunch of the resulting tokens will then be mixes of words &amp; adjoining punctuation, which may be nearly nonsense.</p>
<p>For example, the word <code>'lawsuit'</code> at the end of the sentence might appear as <code>'lawsuit.'</code>, which then won't be recognized as the same token as <code>'lawsuit'</code>, and might not appear enough <code>min_count</code> times to even be considered, or otherwise barely rise above serving as noise.</p>
<p>But especially for both longer documents, and larger datasets, no one token, or even 1% of all tokens, has that much influence. This isn't exact-keyword-search, where failing to return a document with <code>'lawsuit.'</code> for a query on <code>'lawsuit'</code> would be a fatal failure. A bunch of words 'lost' to such cruft may have hadly any effect on the overall document, or model, performance.</p>
<p>As your datasets seem manageable enough to run lots of experiments, I'd suggest trying this dumbest-possible tokenization – only <code>.split()</code> – just as a baseline to become confident that the algorithm still mostly works as well as some more intrusive operation (like <code>simple_preprocess()</code>).</p>
<p>Then, as you notice, or suspect, or ideally measure with some repeatable evaluation, that some things you'd <em>want</em> to be meaningful tokens aren't treated right, gradually add extra steps of stripping/splitting/canonicalizing characters or tokens. But as much as possible: checking that the extra complexity of code, and runtime, is actually delivering benefits.</p>
<p>For example, further refinements could be some mix of:</p>
<ul>
<li>For each token created by the simple <code>split()</code>, strip off any non-alphanumeric leading/trailing chars. (Advantages: eliminates that punctuation-fouling-words cruft. Disadvantages: might lose useful symbols, like the leading <code>$</code> of monetary amounts.)</li>
<li>Before splitting, replace certain single-character punctuation-marks (like say <code>['.', '&quot;', ',', '(', ')', '!', '?', ';', ':']</code>) with the same character with spaces on both sides - so that they're never connected with nearby words, and instead survive a simple <code>.split()</code> as standalone tokens. (Advantages: also prevents words-plus-punctuation cruft. Disadvantages: breaks up numbers like <code>2,345.77</code> or some useful abbreviations.)</li>
<li>At some appropriate stage in tokenization, canonicalize many varied tokens into a smaller set of tokens that may be more meaningful than each of them as rare standalone tokens. For example, <code>$0.01</code> through <code>$0.99</code> might all be turned into <code>$0_XX</code> - which then has a better chance of influencting the model, &amp; being associated with 'tiny amount' concepts, than the original standalone tokens. Or replacing all digits with <code>#</code>, so that numbers of similar magnitudes share influence, without diluting the model with a token for every single number.</li>
</ul>
<p>The exact mix of heuristics, and order of operations, will depend on your goals. But with a corpus only in the thousands of docs (rather than hundreds-of-thousands or millions), even if you do these replacements in a fairly inefficient way (lots of individual string- or regex- replacements in serial), it'll likely be a manageable preprocessing cost.</p>
<p>But you can start simple &amp; only add complexity that your domain-specific knowledge, and evaluations, justifies.</p>
",0,1,637,2021-09-26 20:21:24,https://stackoverflow.com/questions/69338657/keeping-numbers-in-doc2vec-tokenization
Using Tagged Document and Loops in Gensim,"<p>I’m in the process of trying to get document similarity values for a corpus of approximately 5,000 legal briefs with <code>Doc2Vec</code> (I recognize that the corpus may be a little bit small, but this is a proof-of-concept project for a larger corpus of approximately 15,000 briefs I’ll have to compile later). Being somewhat new to Python, I initially ran into some trouble creating a preprocessing function for the 5,000 text files I have assembled in a folder, but I’ve managed to create one.</p>
<p>The trouble is that, when I used the <code>Tagged Document</code> feature to assign a “tag” to each document (“words”), only the text from one of the 5,000 documents (<code>.txt</code> files) is used for the “words” portion, and repeats, while the tag (the filename) for each document is used. Basically, one brief is getting tagged 5,000 times, each with a different tag, when I obviously want 5,000 briefs each with a unique tag of its filename.</p>
<p>Below is the code I used. I’m wondering if anyone can help me figure out where I went wrong with this. I don't know if it's a <code>Tagged Document</code> feature, or if it's a problem with the loop I created - perhaps I need another within it, or there's some issue with the way I have the loop read the filepath? I'm relatively new to Python, so that's completely possible.</p>
<p>Thank you!</p>
<pre><code>briefs = []
BriefList = [p for p in os.listdir(FILEPATH) if p.endswith('.txt')]
for brief in BriefList:
     str = open(FILEPATH + brief,'r').read()
     tokens = re.findall(r&quot;[\w']+|[.,!?;]&quot;, str)
     tagged_data = [TaggedDocument(tokens, [brief]) for brief in BriefList]
     briefs.append(tagged_data)
</code></pre>
","python, loops, tokenize, word-embedding, doc2vec","<p>At the end of your code, is <code>len(briefs)</code> what you expect it to be? Does looking at items like <code>briefs[0]</code> or <code>briefs[-1]</code> show the individual <code>TaggedDocument</code> items you expect?</p>
<p>You probably don't want two nested <code>for … in</code> loops - one going over all briefs to open the files, and the other, for each brief, <em>again</em> going over all briefs to assign them <em>all</em> the same <code>tokens</code> value.</p>
<p>Try changing your lines:</p>
<pre class=""lang-py prettyprint-override""><code>     tagged_data = [TaggedDocument(tokens, [brief]) for brief in BriefList]
     briefs.append(tagged_data)
</code></pre>
<p>...to simply construct &amp; append <em>one</em> <code>TaggedDocument</code> at a time...</p>
<pre class=""lang-py prettyprint-override""><code>     briefs.append(TaggedDocument(tokens, [brief])
</code></pre>
",0,0,154,2021-09-27 15:30:21,https://stackoverflow.com/questions/69349305/using-tagged-document-and-loops-in-gensim
Training fasttext word embedding on your own corpus,"<p>I want to train fasttext on my own corpus. However, I have a small question before continuing. Do I need each sentences as a different item in corpus or can I have many sentences as one item?</p>
<p>For example, I have this DataFrame:</p>
<pre><code> text                                               |     summary
 ------------------------------------------------------------------
 this is sentence one this is sentence two continue | one two other
 other similar sentences some other                 | word word sent
</code></pre>
<p>Basically, the column <code>text</code> is an article so it has many sentences. Because of the preprocessing, I no longer have full stop <code>.</code>. So the question is can I do something like this directly or do I need to split each sentences.</p>
<pre><code>docs = df['text']
vectorizer = TfidfVectorizer()
vectorizer.fit_transform(docs)
</code></pre>
<p>From the tutorials I read, I need list of words for each sentences but what if I have list of words from an article? What are the differences? Is this the right way of training fasttext in your own corpus?</p>
<p>Thank you!</p>
","python, tensorflow, gensim, word-embedding, fasttext","<p>FastText requires <em>text</em> as its training data - not anything that's pre-vectorized, as if by <code>TfidfVectorizer</code>. (If that's part of your FastText process, it's misplaced.)</p>
<p>The Gensim FastText support requires the training corpus as a <em>Python iterable</em>, where each item is a <em>list of string word-tokens</em>.</p>
<p>Each list-of-tokens is typically some cohesive text, where the neighboring words have the relationship of usage together in usual natural-language. It might be a sentence, a paragraph, a post, an article/chapter, or whatever. Gensim's only limitation is that each text shouldn't be more than 10,000 tokens long. (If your texts are longer than that, they should be fragmented into separate 10,000-or-fewer parts. But don't worry too much about the loss of association around the split points - in training sets sufficiently large for an algorithm like FastText, any such loss-of-contexts is negligible.)</p>
",1,2,1619,2021-10-15 11:21:42,https://stackoverflow.com/questions/69583960/training-fasttext-word-embedding-on-your-own-corpus
Can&#39;t get dimensions right - CNN for text classification,"<p>This is my CNN class:</p>
<pre><code>class CNN(nn.Module):
    def __init__(
        self,
        vocab_size,
        emb_dim,
        out_channels,
        kernel_sizes,
        dropout,
    ):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[0], emb_dim), 2)
        
        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[1], emb_dim), 2)
        
        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[2], emb_dim), 2)
        
        self.fc = nn.Linear(len(kernel_sizes) * out_channels, 1)
        
        self.dropout = nn.Dropout(dropout)

        
        
    def forward(self, text):
        
        embedded = self.embedding(text)
        print('embedded', embedded.shape)
        embedded = embedded.unsqueeze(1)  # may be reshape here
        print('embedded', embedded.shape)

        conved_0 = F.relu(self.conv_0(embedded)).squeeze(3)  # may be reshape here
        print('conved_0', conved_0.shape)
        conved_1 = F.relu(self.conv_1(embedded)).squeeze(3)  # may be reshape here
        print('conved_1', conved_1.shape)
        conved_2 = F.relu(self.conv_2(embedded)).squeeze(3)  # may be reshape here
        print('conved_2', conved_2.shape)
        
        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)
        print('pooled_0', pooled_0.shape)
        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)
        print('pooled_1', pooled_1.shape)
        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)
        print('pooled_2', pooled_2.shape)
        
        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))
        print('cat', cat.shape)
            
        return self.fc(cat)
</code></pre>
<p>Variables:</p>
<pre><code>kernel_sizes = [3, 4, 5]
vocab_size = len(TEXT.vocab)
out_channels = 64
dropout = 0.2
dim = 300

model = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=out_channels,
            kernel_sizes=kernel_sizes, dropout=dropout)
</code></pre>
<p>And training:</p>
<pre><code>import numpy as np

min_loss = np.inf

cur_patience = 0

for epoch in range(1, max_epochs + 1):
    train_loss = 0.0
    model.train()
    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)
    pbar.set_description(f&quot;Epoch {epoch}&quot;)
    for it, batch in pbar: 
        #YOUR CODE GOES HERE
        opt.zero_grad()
        input = batch.text[0].to(device)
        output = model(input)
        train_loss = loss_func(output, batch.label)
        train_loss.backward()
        opt.step()
        

    train_loss /= len(train_iter)
    val_loss = 0.0
    model.eval()
    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)
    pbar.set_description(f&quot;Epoch {epoch}&quot;)
    for it, batch in pbar:
        # YOUR CODE GOES HERE
        input = batch.text[0].to(device)
        output = model(input)
        val_loss = loss_fn(output, batch.label)

    val_loss /= len(valid_iter)
    if val_loss &lt; min_loss:
        min_loss = val_loss
        best_model = model.state_dict()
    else:
        cur_patience += 1
        if cur_patience == patience:
            cur_patience = 0
            break
    
    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))
model.load_state_dict(best_model)
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 1, 3, 300], but got 3-dimensional input of size [894, 1, 300] instead</p>
</blockquote>
<p>in this line:</p>
<blockquote>
<p>---&gt; 32         conved_0 = F.relu(self.conv_0(embedded)).squeeze(3)</p>
</blockquote>
<p>I've tried using Conv1d, but still had problems with dimensions. Could somebody please explain what should I fix here for the network to train?</p>
<hr />
<p>EDIT:</p>
<pre><code>This is my class but with Conv1d:

class CNN(nn.Module):

    def __init__(
        self,
        vocab_size,
        emb_dim,
        out_channels,
        kernel_sizes,
        dropout,
    ):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.conv_0 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[0])
        
        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[1])
        
        self.conv_2 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[2])
        
        self.fc = nn.Linear(len(kernel_sizes) * out_channels, 1)
        
        self.dropout = nn.Dropout(dropout)

        
        
    def forward(self, text):
        
        embedded = self.embedding(text)
        print('embedded', embedded.shape)
        embedded = embedded.unsqueeze(1)  # may be reshape here
        print('embedded', embedded.shape)

        conved_0 = F.relu(self.conv_0(embedded))  # may be reshape here
        print('conved_0', conved_0.shape)
        conved_1 = F.relu(self.conv_1(embedded))  # may be reshape here
        print('conved_1', conved_1.shape)
        conved_2 = F.relu(self.conv_2(embedded))  # may be reshape here
        print('conved_2', conved_2.shape)
        
        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)
        print('pooled_0', pooled_0.shape)
        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)
        print('pooled_1', pooled_1.shape)
        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)
        print('pooled_2', pooled_2.shape)
        
        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))
        print('cat', cat.shape)
            
        return self.fc(cat)
</code></pre>
<p>Dimensions output:</p>
<pre><code>embedded torch.Size([1115, 300])
embedded torch.Size([1115, 1, 300])
conved_0 torch.Size([1115, 64, 298])
conved_1 torch.Size([1115, 64, 297])
conved_2 torch.Size([1115, 64, 296])
pooled_0 torch.Size([1115, 64])
pooled_1 torch.Size([1115, 64])
pooled_2 torch.Size([1115, 64])
cat torch.Size([1115, 192])
</code></pre>
<p>Error:</p>
<blockquote>
<p>ValueError: Target size (torch.Size([128])) must be the same as input size (torch.Size([1115, 1]))</p>
</blockquote>
","python, conv-neural-network, text-classification, dimensions, word-embedding","<p>What I was missing is that I had <code>batch_first</code> parameter set to <code>True</code>, which <strong>SWAPED</strong> <em>batch_size</em> and <em>seq_len</em>. Once I've set it to <code>False</code>, everything worked perfectly.</p>
",0,0,185,2021-10-29 08:06:41,https://stackoverflow.com/questions/69765540/cant-get-dimensions-right-cnn-for-text-classification
The inputs into BERT are token IDs. How do I get the corresponding the input token VECTORs into BERT?,"<p>I am new and learning about transformers.</p>
<p>In a lot of BERT tutorials, I see the input is just the token id of the words. But surely we need to convert this token ID to a vector representation (it can be one hot encoding, or any initial vector representation for each token ID) so that it can be used by the model.</p>
<p>My question is: Where cam I find this initial vector representation for each token?</p>
","nlp, huggingface-transformers, bert-language-model, word-embedding","<p>In BERT, the input is a <code>string</code> itself. THen, BERT manages to convert it into a token and then, create its vector. Let's see an example:</p>
<pre class=""lang-py prettyprint-override""><code>prep_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
enc_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4' 
bert_preprocess = hub.KerasLayer(prep_url)
bert_encoder = hub.KerasLayer(enc_url)

text = ['Hello I&quot;m new to stack overflow']

# First, you need to preprocess the data

preprocessed_text = bert_preprocess(text)
# this will give you a dict with a few keys such us input_word_ids, that is, the tokenizer

encoded = bert_encoder(preprocessed_text)
# and this will give you the (1, 768) vector with the context value of the previous text. the output is encoded['pooled_output']

# you can play with both dicts, printing its keys()
</code></pre>
<p>I recommend you to go to both links above and do a little of research. To recap, BERT uses string as inputs and then tokenize it (with its own tokenzer!). If you want to tokenize with the same values, you need the same vocab file, but for a fresh start like you are doing this should be enough.</p>
",0,0,2715,2021-11-01 21:19:12,https://stackoverflow.com/questions/69802895/the-inputs-into-bert-are-token-ids-how-do-i-get-the-corresponding-the-input-tok
Is there any ideal parameter values for fasttext train_supervised function?,"<p>I working on NLP problem and try to make text classification with word embedding method. I am training my model with fasttext's train_supervised but is there any ideal or best parameter values for this function that you can advise me also I am using Kfold with some values how can I find best K-fold number in this problem ?</p>
<p>My solution is I am using fasttext's autotune function to find best param values for model to train but is there any possible suggestion to give me ? Following image shows my best params in the model. Finally , I am using fasttext's pretrained word vector model for my training.
<a href=""https://i.sstatic.net/cD4tM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cD4tM.png"" alt=""enter image description here"" /></a></p>
","nlp, text-classification, word-embedding, supervised-learning, fasttext","<p>Let me answer my own question you can look at the default and optimum parameters values by clicking  the following link ;</p>
<p><a href=""https://fasttext.cc/docs/en/options.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/options.html</a></p>
<p>and also you can use fasttext's libraries autotune function (Automatic hyperparameter optimization) to find best parameters for your special train and validation dataset  by clicking the following link ;</p>
<p><a href=""https://fasttext.cc/docs/en/autotune.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/autotune.html</a></p>
<p>and finally this is the pretrained word vectors provided by fasttext library to utilize in your model's training process also making positive progress for model , in the following link's site they are in the Model section ;</p>
<p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>
",0,1,1288,2021-11-02 08:43:41,https://stackoverflow.com/questions/69807486/is-there-any-ideal-parameter-values-for-fasttext-train-supervised-function
fastText producing zero vector,"<p>I am facing the following error which calculating the cosine similarity on embeddings produced by fastText:</p>
<pre><code>/home/kgarg8/anaconda3/envs/CiteKP/lib/python3.6/site-packages/scipy/spatial/distance.py:721: RuntimeWarn
ing: invalid value encountered in float_scalars                                                          
  dist = 1.0 - uv / np.sqrt(uu * vv)    
</code></pre>
<p>Relevant code snippets:</p>
<pre><code># fastText supervised training:
model = fasttext.train_supervised('merged_data_labels_prepended.txt')
model.save_model('fasttext_supervised.bin')

# model loading
model = fasttext.load_model(&quot;fasttext_supervised.bin&quot;)
</code></pre>
<pre><code># calculating cosine similarity
from scipy import spatial
def cosine_distance_wordembedding_method(s1, s2):
    vec1   = np.mean([model[word] for word in s1],axis=0)
    vec2   = np.mean([model[word] for word in s2],axis=0)
    cosine = spatial.distance.cosine(vec1, vec2)
    return round((1-cosine)*100, 2)

cosine_distance_wordembedding_method(pred.split(), label.split()) # function call
</code></pre>
<p>Initial Analysis:</p>
<p>fastText is producing all zeros embeddings (either vec1 or vec2 is zero sometimes) for words not in the vocabulary. So, how can I handle these OOV words to get non-zero embeddings?</p>
","python, spatial, word-embedding, fasttext","<p>Apparently, setting <code>maxn=6</code> fixed the problem. By default, it is <code>0</code>.</p>
",0,0,573,2021-11-06 17:34:12,https://stackoverflow.com/questions/69866356/fasttext-producing-zero-vector
How to get cosine similarity of word embedding from BERT model,"<p>I was interesting in how to get the similarity of word embedding in different sentences from BERT model (actually, that means words have different meanings in different scenarios).</p>
<p>For example:</p>
<pre><code>sent1 = 'I like living in New York.'
sent2 = 'New York is a prosperous city.'
</code></pre>
<p>I want to get the cos(New York, New York)'s value from sent1 and sent2, even if the phrase 'New York' is same, but it appears in different sentence. I got some intuition from <a href=""https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2"" rel=""noreferrer"">https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2</a></p>
<p>But I still do not know which layer's embedding I need to extract and how to caculate the cos similarity for my above example.</p>
<p>Thanks in advance for any suggestions!</p>
","python, bert-language-model, word-embedding, transformer-model","<p>Okay let's do this.</p>
<p>First you need to understand that BERT has 13 layers. The first layer is basically just the embedding layer that BERT gets passed during the initial training. You can use it but probably don't want to since that's essentially a static embedding and you're after a dynamic embedding. For simplicity I'm going to only use the last hidden layer of BERT.</p>
<p>Here you're using two words: &quot;New&quot; and &quot;York&quot;. You could treat this as one during preprocessing and combine it into &quot;New-York&quot; or something if you really wanted. In this case I'm going to treat it as two separate words and average the embedding that BERT produces.</p>
<p>This can be described in a few steps:</p>
<ol>
<li>Tokenize the inputs</li>
<li>Determine where the tokenizer has word_ids for New and York (suuuuper important)</li>
<li>Pass through BERT</li>
<li>Average</li>
<li>Cosine similarity</li>
</ol>
<p>First, what you need to import: <code>from transformers import AutoTokenizer, AutoModel</code></p>
<p>Now we can create our tokenizer and our model:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
model = model = AutoModel.from_pretrained('bert-base-cased', output_hidden_states=True).eval()
</code></pre>
<p>Make sure to use the model in evaluation mode unless you're trying to fine tune!</p>
<p>Next we need to tokenize (step 1):</p>
<pre><code>tok1 = tokenizer(sent1, return_tensors='pt')
tok2 = tokenizer(sent2, return_tensors='pt')
</code></pre>
<p>Step 2. Need to determine where the index of the words match</p>
<pre><code># This is where the &quot;New&quot; and &quot;York&quot; can be found in sent1
sent1_idxs = [4, 5]
sent2_idxs = [0, 1]

tok1_ids = [np.where(np.array(tok1.word_ids()) == idx) for idx in sent1_idxs]
tok2_ids = [np.where(np.array(tok2.word_ids()) == idx) for idx in sent2_idxs]
</code></pre>
<p>The above code checks where the word_ids() produced by the tokenizer overlap the word indices from the original sentence. This is necessary because the tokenizer splits rare words. So if you have something like &quot;aardvark&quot;, when you tokenize it and look at it you actually get this:</p>
<pre><code>In [90]: tokenizer.convert_ids_to_tokens( tokenizer('aardvark').input_ids)
Out[90]: ['[CLS]', 'a', '##ard', '##var', '##k', '[SEP]']

In [91]: tokenizer('aardvark').word_ids()
Out[91]: [None, 0, 0, 0, 0, None]
</code></pre>
<p>Step 3. Pass through BERT</p>
<p>Now we grab the embeddings that BERT produces across the token ids that we've produced:</p>
<pre><code>with torch.no_grad():
    out1 = model(**tok1)
    out2 = model(**tok2)

# Only grab the last hidden state
states1 = out1.hidden_states[-1].squeeze()
states2 = out2.hidden_states[-1].squeeze()

# Select the tokens that we're after corresponding to &quot;New&quot; and &quot;York&quot;
embs1 = states1[[tup[0][0] for tup in tok1_ids]]
embs2 = states2[[tup[0][0] for tup in tok2_ids]]
</code></pre>
<p>Now you will have two embeddings. Each is shape <code>(2, 768)</code>. The first size is because you have two words we're looking at: &quot;New&quot; and &quot;York. The second size is the embedding size of BERT.</p>
<p>Step 4. Average</p>
<p>Okay, so this isn't necessarily what you want to do but it's going to depend on how you treat these embeddings. What we have is two <code>(2, 768)</code> shaped embeddings. You can either compare New to New and York to York or you can combine New York into an average. I'll just do that but you can easily do the other one if it works better for your task.</p>
<pre><code>avg1 = embs1.mean(axis=0)
avg2 = embs2.mean(axis=0)
</code></pre>
<p>Step 5. Cosine sim</p>
<p>Cosine similarity is pretty easy using <code>torch</code>:</p>
<pre><code>torch.cosine_similarity(avg1.reshape(1,-1), avg2.reshape(1,-1))

# tensor([0.6440])

</code></pre>
<p>This is good! They point in the same direction. They're not exactly 1 but that can be improved in several ways.</p>
<ol>
<li>You can fine tune on a training set</li>
<li>You can experiment with averaging different layers rather than just the last hidden layer like I did</li>
<li>You can try to be creative in combining New and York. I took the average but maybe there's a better way for your exact needs.</li>
</ol>
",23,6,15526,2021-11-21 19:39:24,https://stackoverflow.com/questions/70057975/how-to-get-cosine-similarity-of-word-embedding-from-bert-model
Gensim doc2vec&#39;s d2v.wv.most_similar() gives not relevant words with high similarity scores,"<p>I've got a dataset of job listings with about 150 000 records. I extracted skills from descriptions using NER using a dictionary of 30 000 skills. Every skill is represented as an unique identificator.</p>
<p>My data example:</p>
<pre><code>          job_title    job_id                                         skills
1  business manager         4               12 13 873 4811 482 2384 48 293 48
2    java developer        55    48 2838 291 37 484 192 92 485 17 23 299 23...
3    data scientist        21    383 48 587 475 2394 5716 293 585 1923 494 3
</code></pre>
<p>Then, I train a doc2vec model using these data where job titles (their ids to be precise) are used as tags and skills vectors as word vectors.</p>
<pre><code>def tagged_document(df):
    for index, row in df.iterrows():
        yield gensim.models.doc2vec.TaggedDocument(row['skills'].split(), [str(row['job_id'])])
        
        
data_for_training = list(tagged_document(data[['job_id', 'skills']]))

model_d2v = gensim.models.doc2vec.Doc2Vec(dm=0, dbow_words=1, vector_size=80, min_count=3, epochs=100, window=100000)

model_d2v.build_vocab(data_for_training)

model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)
</code></pre>
<p>It works mostly okay, but I have issues with some job titles. I tried to collect more data from them, but I still have an unpredictable behavior with them.</p>
<p>For example, I have a job title &quot;Director Of Commercial Operations&quot; which is represented as 41 data records having from 11 to 96 skills (mean 32). When I get most similar words for it (skills in my case) I get the following:</p>
<pre><code>docvec = model_d2v.docvecs[id_]
model_d2v.wv.most_similar(positive=[docvec], topn=5)
</code></pre>
<pre><code>capacity utilization 0.5729076266288757
process optimization 0.5405482649803162
goal setting 0.5288119316101074
aeration 0.5124399662017822
supplier relationship management 0.5117508172988892
</code></pre>
<p>These are top 5 skills and 3 of them look relevant. However the top one doesn't look too valid together with &quot;aeration&quot;. The problem is that none of the job title records have these skills at all. It seems like a noise in the output, but why it gets one of the highest similarity scores (although generally not high)?
Does it mean that the model can't outline very specific skills for this kind of job titles?
Can the number of &quot;noisy&quot; skills be reduced? Sometimes I see much more relevant skills with lower similarity score, but it's often lower than 0.5.</p>
<p>One more example of correct behavior with similar amount of data:
BI Analyst, 29 records, number of skills from 4 to 48 (mean 21). The top skills look alright.</p>
<pre><code>business intelligence 0.6986587047576904
business intelligence development 0.6861011981964111
power bi 0.6589289903640747
tableau 0.6500121355056763
qlikview (data analytics software) 0.6307920217514038
business intelligence tools 0.6143202781677246
dimensional modeling 0.6032138466835022
exploratory data analysis 0.6005223989486694
marketing analytics 0.5737696886062622
data mining 0.5734485387802124
data quality 0.5729933977127075
data visualization 0.5691111087799072
microstrategy 0.5566076636314392
business analytics 0.5535123348236084
etl 0.5516749620437622
data modeling 0.5512707233428955
data profiling 0.5495884418487549
</code></pre>
","nlp, gensim, word2vec, word-embedding, doc2vec","<p>If the your gold standard of what the model should report is skills that appeared in the training data, are you sure you don't want a simple count-based solution? For example, just provide a ranked list of the skills that appear most often in <code>Director Of Commercial Operations</code> listings?</p>
<p>On the other hand, the essence of compressing N job titles, and 30,000 skills, into a smaller (in this case <code>vector_size=80</code>) coordinate-space model is to force some non-intuitive (but perhaps real) relationships to be reflected in the model.</p>
<p>Might there be some real pattern in the model – even if, perhaps, just some idiosyncracies in the appearance of less-common skills – that makes <code>aeration</code> necessarily slot near those other skills? (Maybe it's a rare skill whose few contextual appearances co-occur with other skills very much near 'capacity utilization' -meaning with the tiny amount of data available, &amp; tiny amount of overall attention given to this skill, there's no better place for it.)</p>
<p>Taking note of whether your 'anomalies' are often in low-frequency skills, or lower-freqeuncy job-ids, might enable a closer look at the data causes, or some disclaimering/filtering of <code>most_similar()</code> results. (The <code>most_similar()</code> method can limit its returned rankings to the more frequent range of the known vocabulary, for cases when the long-tail or rare words are, in with their rougher vectors, intruding in higher-quality results from better-reqpresented words. See the <code>restrict_vocab</code> parameter.)</p>
<p>That said, tinkering with training parameters may result in rankings that better reflect your intent. A larger <code>min_count</code> might remove more tokens that, lacking sufficient varied examples, mostly just inject noise into the rest of training. A different <code>vector_size</code>, smaller or larger, might better capture the relationships you're looking for. A more-aggressive (smaller) <code>sample</code> could discard more high-frequency words that might be starving more-interesting less-frequent words of a chance to influence the model.</p>
<p>Note that with <code>dbow_words=1</code> &amp; a large window, and records with (perhaps?) dozens of skills each, the words are having a much-more <em>neighborly</em> effect on each other, in the model, than the <code>tag</code>&lt;-&gt;<code>word</code> correlations. That might be good or bad.</p>
",0,1,459,2021-12-14 14:53:20,https://stackoverflow.com/questions/70350954/gensim-doc2vecs-d2v-wv-most-similar-gives-not-relevant-words-with-high-simila
rare misspelled words messes my fastText/Word-Embedding Classfiers,"<p>I'm currently trying to make a sentiment analysis on the IMDB review dataset as a part of homework assignment for my college, I'm required to firstly do some preprocessing e.g. : tokenization, stop words removal, stemming, lemmatization. then use different ways to convert this data to vectors to be classfied by different classfiers, Gensim FastText library was one of the required models to obtain word embeddings on the data I got from text pre-processing step.</p>
<p>the problem I faced with Gensim is that I firstly tried to train on my data using vectors of feature size (100,200,300) but yet they always fail at some point, I tried later to use many pre-trained Gensim data vectors, but none of them worked to find word embeddings for all of the words, they'd rather fail at some point with error</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-28-644253c70aa3&gt; in &lt;module&gt;()
----&gt; 1 model.most_similar(some-rare-word)

1 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(&quot;word '%s' not in vocabulary&quot; % word)
    453 
    454     def get_vector(self, word):

KeyError: &quot;word some-rare-word not in vocabulary&quot;
</code></pre>
<p>the ones I've tried so far are :<br />
conceptnet-numberbatch-17-06-300 : doesn't contain &quot;glass&quot;<br />
word2vec-google-news-300 : ram insufficient in Google Colab<br />
glove-twitter-200 : doesn't contain &quot;5&quot;<br />
crawl-300d-2M : doesn't contain &quot;waltons&quot;<br />
wiki-news-300d-1M : doesn't contain &quot;waltons&quot;<br />
glove-wiki-gigaword-300 : doesn't contain &quot;riget&quot;</p>
<p>got their names from these sources, <a href=""https://github.com/RaRe-Technologies/gensim-data#readme"" rel=""nofollow noreferrer"">here</a> and <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">here</a></p>
<p>by inspecting the failing words, I found that even the largest libraries would usually fail because of the misspelled words that has no meaning like 'riget', 'waltons',...etc</p>
<p>Is their a way to classify and neglect this strange words before trying to inject them to Gensim and receiving this error ? or am I using Gensim very wrong and there's another way to use it ?</p>
<p>any snippet of code or some sort of lead on what to do would be appreciated</p>
<p>my code so far :</p>
<pre><code>import gensim.downloader as api
model = api.load(&quot;glove-wiki-gigaword-300&quot;) # this can be any vector-library of the previously mentioned ones
train_word_embeddings = []
# train_lemm is a vector of size (number of examples, number of words remaining in example sentence i after removal of stop words and lemmatization to nouns)
# they're mainly 25000 example review sentence while the second dimension is of random value depending on number of words
for i in range (len(train_lemm)): 
  train_word_embeddings.append(model.wv[train_lemm[i]])
</code></pre>
","python, gensim, sentiment-analysis, word-embedding","<p>If you train your own word-vector model, then it will contain vectors for all the words you told it to learn. If a word that was in your training data doesn't appear to have a vector, it likely did not appear the required <code>min_count</code> number of times. (These models tend to <em>improve</em> if you discard rare words who few example usages may not be suitably-informative, so the default <code>min_words=5</code> is a good idea.)</p>
<p>It's often reasonable for downstream tasks, like feature engineering using the text &amp; set of word-vectors, to simply ignore words with no vector. That is, if <code>some_rare_word in model.wv</code> is <code>False</code>, just don't try to use that word – &amp; its missing vector – for anything. So you don't necessarily need to find, or train, a set of word-vectors with <em>every</em> word you need. Just elide, rather than worry-about, the rare missing words.</p>
<p>Separate observations:</p>
<ul>
<li>Stemming/lemmatization &amp; stop-word removal aren't always worth the trouble, with all corpora/algorithms/goals. (And, stemming/lemmatization may wind up creating pseudowords that limit the model's interpretability &amp; easy application to any texts that don't go through identical preprocessing.) So if those are required parts of laerning exercise, sure, get some experience using them. But don't assume they're necessarily helping, or worth the extra time/complexity, unless you verify that rigrously.</li>
<li>FastText models will also be able to supply synthetic vectors for words that <em>aren't</em> known to the model, based on substrings. These are often pretty weak, but may better than nothing - especially when they give vectors for typos, or rare infelcted forms, similar to morphologically-related known words. (Since this deduced similarity, from many similarly-written tokens, provides some of the same value as stemming/lemmatization via a different path that <em>required</em> the original variations to all be present during initial training, you'd especially want to pay attention to whether FastText &amp; stemming/lemmatization mix well for your goals.) Beware, though: for very-short unknown words – for which the model learned no reusable substring vectors – FastText may still return an error or all-zeros vector.</li>
<li>FastText has a <code>supervised</code> classification mode, but it's not supported by Gensim. If you want to experiment with that, you'd need to use the Facebook FastText implementation. (You could still use a traditional, non-<code>supervised</code> FastText word vector model as a contributor of features for other possible representations.)</li>
</ul>
",1,0,754,2021-12-16 19:53:52,https://stackoverflow.com/questions/70384870/rare-misspelled-words-messes-my-fasttext-word-embedding-classfiers
Incomplete word embedding model conversion with plasticityai/magnitude,"<p>I want to convert the word embedding model <a href=""https://conceptnet.s3.amazonaws.com/downloads/2019/numberbatch/numberbatch-19.08.txt.gz"" rel=""nofollow noreferrer"">Numberbatch 19.08</a> to the .magnitude format used in <a href=""https://github.com/plasticityai/magnitude"" rel=""nofollow noreferrer"">plasticityai/magnitude</a>. As I want to be able to use approximate nearest neighbor algorithms I run the command</p>
<pre><code>python -m pymagnitude.converter -i numberbatch.txt -o numberbatch.magnitude -a
</code></pre>
<p>The size of the unpacked numberbatch.txt is about 20GB. I am using Windows10.</p>
<p>At first, the conversion seems to run fine (for some hours), showing progress like</p>
<blockquote>
<p>Writing vectors... (this may take some time)</p>
<p>1% completed ... 99% completed</p>
</blockquote>
<p>then</p>
<blockquote>
<p>Committing written vectors... (this may take some time)</p>
</blockquote>
<p>and finally</p>
<blockquote>
<p>Creating search index... (this may take some time)</p>
<p>Creating spatial search index for dimension 2 (it has high entropy)... (this may take some time)</p>
<p>Creating approximate nearest neighbors index... (this may take some time)</p>
</blockquote>
<p>However, I never get a final message that the conversion is complete. Rather, the program stops without any further messages.</p>
<p>And that stage I am left with the following three files in the target folder:</p>
<pre><code>    15.891.668.992 numberbatch.magnitude.tmp
           557.056 numberbatch.magnitude.tmp-shm
       281.227.112 numberbatch.magnitude.tmp-wal
</code></pre>
<p>The intended end result, numberbatch.magnitude, is missing.</p>
<p>Any hint about what might have gone wrong would be much appreciated. Is there maybe any way to complete the conversion using the three tmp files?</p>
","python, sqlite, word-embedding, file-conversion","<p>I guess I found a partial answer to my own question in a <a href=""https://github.com/plasticityai/magnitude/issues/34"" rel=""nofollow noreferrer"">closed issue</a> of the plasticity/ai project:</p>
<p>It seems that pymagnitude.converter cannot handle vector file sizes in the multi GB range when used together with the -a flag which produces the approximate nearest neighbors index. It was speculated in the issue that this is a problem of the underlying <a href=""https://github.com/spotify/annoy"" rel=""nofollow noreferrer"">Annoy</a> library, though the precise cause was never fully resolved.</p>
<p>At this stage, the provisional remedy then is to abstain from using the -a flag.</p>
",0,0,95,2021-12-22 10:19:42,https://stackoverflow.com/questions/70447478/incomplete-word-embedding-model-conversion-with-plasticityai-magnitude
Gensim train not updating weights,"<p>I have a domain specific corpus for which I am trying to train embeddings. Since I want to be comprehensive in vocabulary, I am adding word vectors from <code>glove.6B.50d.txt</code>. Post adding vectors from here, I am training the model using the corpus I have.</p>
<p>I am trying the solutions from <a href=""https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights"">here</a> but the word embeddings don't seem to update.</p>
<p>This is the solution I have so far.</p>
<pre><code>#read glove embeddings
glove_wv = KeyedVectors.load_word2vec_format(GLOVE_PATH, binary=False)

#initialize w2v model
model =  Word2Vec(vector_size=50, min_count=0, window=20, epochs=10, sg=1, workers=10, 
                      hs=1, ns_exponent=0.5, seed=42, sample=10**-2, shrink_windows=True)
model.build_vocab(sentences_tokenized)
training_examples_count = model.corpus_count

# add vocab from glove
model.build_vocab([list(glove_wv.key_to_index.keys())], update=True)
model.wv.vectors_lockf = np.zeros(len(model.wv)) # ALLOW UPDATE OF WEIGHTS FROM BACK PROP; 0 WILL SUPPRESS

# add glove embeddings
model.wv.intersect_word2vec_format(GLOVE_PATH,binary=False, lockf=1.0)
</code></pre>
<p>Below I am training the model and checking word embedding of a particular word explicitly present in training</p>
<pre><code># train model
model.train(sentences_tokenized,total_examples=training_examples_count, epochs=model.epochs)

#CHECK IF EMBEDDING CHANGES FOR 'oyo'
print(model.wv.get_vector('oyo'))
print(glove_wv.get_vector('oyo'))
</code></pre>
<p>The word embeddings of the word <code>oyo</code> comes out to be same before and after the training. Where am I going wrong?</p>
<p>The input corpus- <code>sentences_tokenized</code> contains few sentences that contains the word <code>oyo</code>. One of such sentences-</p>
<pre><code>'oyo global platform empowers entrepreneur small business hotel home providing full stack technology increase earnings eas operation bringing affordable trusted accommodation guest book instantly india largest budget hotel chain oyo room one preferred hotel booking destination vast majority student country hotel chain offer many benefit include early check in couple room id card flexibility oyo basically network budget hotel completely different famous hotel aggregator like goibibo yatra makemytrip partner zero two star hotel give makeover room bring customer hotel website mobile app'
</code></pre>
","python, stanford-nlp, gensim, word2vec, word-embedding","<p>You're improvising a lot here with a bunch of potential errors or suboptimalities. Note especially that:</p>
<ul>
<li>While (because it's Python) you can always mutate the models however you want for interesting effects, seeding a model with outside word-vectors then continuing training isn't formally- or well-supported by Gensim. As far as I can tell – &amp; I wrote a bunch of this code! – there aren't any good docs/examples of doing it well, or doing the necessary tuning/validation of results, or demonstrating a reliable advantage of this technique. Most examples online are of eager people plowing ahead unaware of the tradeoffs, seeing a trivial indicator of completion or a tiny bit of encouraging results, and then overconfidently showing their work as if this were a well-grounded technique or best-practice. It isn't. Without a deep understanding of the model, &amp; review of the source code, &amp; regular re-checking of your results for sanity/improvement, there will be hidden gotchas. It is especially the case that fresh training on just a subset of all words could pull those words out of compatible coordinate alignment with other words not receiving training.</li>
<li>The <code>intersect_word2vec_format()</code> feature, and especially the <code>lockf</code> function, are also experimental - one stab at maybe offering a way to mix in other word-vectors, but without any theoretical support. (I also believe <code>intersect_word2vec_format()</code> remains slightly broken in recent (circa 4.1.2) Gensim versions, though there may be a <a href=""https://github.com/RaRe-Technologies/gensim/issues/3094"" rel=""nofollow noreferrer"">simple workaround</a>.) Still, the <code>lockf</code> functionality may require tricky manual initialization &amp; adaptation to other non-standard steps. To use it, it'd be best to read &amp; understand the Gensim source code where related variables appear.</li>
</ul>
<p>So, if you really need a larger vocabulary than your initial domain-specific corpus, the safest approach is probably to extend your training corpus with more texts that feature the desired words, as used in similar language contexts. (For example, if you rdomain is scientific discourse, you'd want to extend your corpus with more similar scientific text to learn compatible words – not, say, classic fiction.) Then all words go through the well-characterized simultaneous training process.</p>
<p>That said, if you really want to continue experimenting with this potentially complicated and error-prone improvised approach, your main problems might be:</p>
<ul>
<li>using strings as your sentences instead of lists-of-tokens (so the training 'words' wind up actually just being single-characters)</li>
<li>something related to the <code>intersect_word2vec_format</code> bug; check if <code>.vectors_lockf</code> is the right length, with <code>1.0</code> in the all the right slots for word-updates, before training</li>
</ul>
<p>Separately, other observations:</p>
<ul>
<li><code>min_count=0</code> is usually a bad idea: these models improve when you discard rare words entirely. (Though, when doing a <code>.build_vocab(…, update=True)</code> vocab-expansion, a bunch of things with the usual neat handling of low-frequency words and frequency-sorted vocabularies become screwy.)</li>
<li><code>hs=1</code> should generally not be set without also disabling the usually-preferred default negative-sampling with <code>negative=0</code>. (Otherwise, you're creating a hybrid franken-model, using both modes on one side of the internal neural network, that share the same input word-vectors: a much slower approach not especially likely to be better than either alone.)</li>
<li><code>ns_exponent=0.5</code> is non-standard, and using non-standard values for the parameter is most-likely to offer benefit in peculiar situations (like training texts that aren't true natural language sentences), and should only be tweaked within a harness for comparing results with alternate values.</li>
<li><code>sample=10**-2</code> is also non-standard, and such a large value might be nearly the same as turning off <code>sample</code> (say with a <code>0</code> value) entirely. It's more common to want to make this parameter more-aggressive (smaller than the default), if you have plentiful training data.</li>
</ul>
<p>In general, while the defaults aren't sacred, you generally should avoid tinkering with them until you have both (a) a good idea of why your corpus/goals might benefit from a different value; &amp; (b) a system for verifying which alterations are helping or hurting, such as a grid-search over many parameter combinations that scores the fitness of resulting models on (some proxy for) your true end task.</p>
",1,1,392,2021-12-30 14:48:04,https://stackoverflow.com/questions/70533179/gensim-train-not-updating-weights
Fasttext model representations for numbers,"<p>I would like to create a fasttext model for numbers. Is this a good approach?</p>
<p><strong>Use Case:</strong></p>
<p>I have a given number set of about 100.000 integer invoice-numbers.
Our OCR sometimes creates false invoice-numbers like 1000o00 or 383I338, so my idea was to use fasttext to predict nearest invoice-number based on my 100.000 integers.
As correct invoice-numbers are known in advance, I trained a fastext model with all invoice-numbers to create a word-embeding space just with invoices-numbers.</p>
<p>But it is not working and I don´t know if my idea is completly wrong? But I would assume that even if I have no sentences, embedding into vector space should work and therefore also a similarity between 383I338 and 3831338 should be found by the model.</p>
<p>Here some of my <strong>code</strong>:</p>
<pre><code>import pandas as pd
from random import seed
from random import randint
import fasttext
</code></pre>
<pre><code># seed random number generator
seed(9999)
number_of_vnr = 100000
min_vnr = 1111    
max_vnr = 999999999

# generate vnr integers
versicherungsscheinnummern = [randint(min_vnr, max_vnr) for i in range(number_of_vnr)]

# save numbers as csv
df_vnr = pd.DataFrame(versicherungsscheinnummern, columns=['VNR'])
df_vnr['VNR'].dropna().astype(str).to_csv('vnr_str.csv', index=False)

</code></pre>
<pre><code># train model
model = fasttext.train_unsupervised('vnr_str.csv',&quot;cbow&quot;, minn=2, maxn=5)  
</code></pre>
<p>Even data in the space is not found</p>
<pre><code>model.get_nearest_neighbors(&quot;833803015&quot;)
[(0.10374893993139267, '&lt;/s&gt;')]
</code></pre>
<p>model has no words</p>
<pre><code>model.words
[&quot;'&lt;/s&gt;'&quot;]
</code></pre>
","python, word-embedding, fasttext","<p>I doubt FastText is the right approach for this.</p>
<p>Unlike in natural-languages, where word roots/prefixes/suffixes (character n-grams) can be hints to meaning, most invoice number schemes are just incrementing numbers.</p>
<p>Every '###' or '####' is going to have a similar frequency. (Well, perhaps there'd be a little bit of a bias towards lower digits to the left, for Benford's Law-like reasons.) Unless the <em>exact same</em> invoice numbers repeat often* throughout the corpus, so that the whole token, &amp; its fragments, acquire a word-like meaning from surrounding other tokens, FastText's post-training nearest-neighbors are unlikely to offer any hints about correct numbers. (For it to have a chance to help, you'd want the same invoice-numbers to not just repeat many times, but for a lot of those appeearances to have similar OCR errors - but I strongly suspet your corpus instead has invoice numbers only on individual texts.)</p>
<p>Is the real goal to correct the invoice-numbers, or just to have them be less-noisy in a model thaat's trained on a lot more meaningful, text-like tokens? (If the latter, it might be better just to discard anything that looks like an invoice number – with or without OCR glitches – or is similarly so rare it's likely an OCR scanno.)</p>
<p>That said, statistical &amp; edit-distance methods could potentially help if the real need is correcting OCR errors - just not semantic-context-dependent methods like FastText. You might get useful ideas from Peter Norvig's classic writeup on &quot;<a href=""http://%20https://norvig.com/spell-correct.html"" rel=""nofollow noreferrer"">How to Write a Spelling Corrector</a>&quot;.</p>
",0,0,307,2022-01-07 18:22:51,https://stackoverflow.com/questions/70625591/fasttext-model-representations-for-numbers
How to use Embedding Layer along with textvectorization in functional API,"<p>Just starting on tensorflow</p>
<p>Working on imdb dataset. Process: Text encoding using textvectorization layer and passing it to embedded layer:</p>
<pre><code># Create a custom standardization function to strip HTML break tags '&lt;br /&gt;'.
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '&lt;br /&gt;', ' ')
  return tf.strings.regex_replace(stripped_html,
                              '[%s]' % re.escape(string.punctuation), '')


# Vocabulary size and number of words in a sequence.
vocab_size = 10000
sequence_length = 100

# Use the text vectorization layer to normalize, split, and map strings to
# integers. Note that the layer uses the custom standardization defined above.
# Set maximum_sequence length as all samples are not of the same length.
vectorize_layer = TextVectorization(
standardize=custom_standardization,
max_tokens=vocab_size,
output_mode='int',
output_sequence_length=sequence_length)

# Make a text-only dataset (no labels) and call adapt to build the vocabulary.
text_ds = train_ds.map(lambda x, y: x)
vectorize_layer.adapt(text_ds)
</code></pre>
<p>I then try to build a functional API:</p>
<pre><code>embedding_dim=16
text_model_catprocess2 = vectorize_layer
text_model_embedd = tf.keras.layers.Embedding(vocab_size, embedding_dim, name = 'embedding')(text_model_catprocess2)
text_model_embed_proc = tf.keras.layers.Lambda(embedding_mean_standard)(text_model_embedd)
text_model_dense1 = tf.keras.layers.Dense(2, activation = 'relu')(text_model_embed_proc)
text_model_dense2 = tf.keras.layers.Dense(2, activation = 'relu')(text_model_dense1)
text_model_output = tf.keras.layers.Dense(1, activation = 'sigmoid')(text_model_dense2)
</code></pre>
<p>However, this is giving the following error:</p>
<pre><code>~\anaconda3\lib\site-packages\keras\backend.py in dtype(x)
1496 
1497   &quot;&quot;&quot;
-&gt; 1498   return x.dtype.base_dtype.name
1499 
1500 

AttributeError: Exception encountered when calling layer &quot;embedding&quot; (type Embedding).

'str' object has no attribute 'base_dtype'

Call arguments received:
  • inputs=&lt;keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x0000029B483AADC0&gt;
</code></pre>
<p>Upon making a sequential API like this, it is working fine:</p>
<pre><code>embedding_dim=16
modelcheck = tf.keras.Sequential([
vectorize_layer,
tf.keras.layers.Embedding(vocab_size, embedding_dim, name=&quot;embedding&quot;),
tf.keras.layers.GlobalAveragePooling1D(),
tf.keras.layers.Dense(16, activation='relu'),
tf.keras.layers.Dense(1)
])
</code></pre>
<p>I am not sure why this is happening. Is it necessary for the functional API to have an input? Please help!</p>
","python, tensorflow, keras, word-embedding, functional-api","<p>You have two options. Either you use a <code>Sequential</code> model and it will work as you have confirmed because you do not have to define an <code>Input</code> layer, or you use the <code>functional</code> API where you have to define an <code>Input</code> layer:</p>
<pre class=""lang-py prettyprint-override""><code>embedding_dim = 16
text_model_input = tf.keras.layers.Input(dtype=tf.string, shape=(1,))
text_model_catprocess2 = vectorize_layer(text_model_input)
text_model_embedd = tf.keras.layers.Embedding(vocab_size, embedding_dim, name = 'embedding')(text_model_catprocess2)
text_model_embed_proc = tf.keras.layers.Lambda(embedding_mean_standard)(text_model_embedd)
text_model_dense1 = tf.keras.layers.Dense(2, activation = 'relu')(text_model_embed_proc)
text_model_dense2 = tf.keras.layers.Dense(2, activation = 'relu')(text_model_dense1)
text_model_output = tf.keras.layers.Dense(1, activation = 'sigmoid')(text_model_dense2)
model = tf.keras.Model(text_model_input, text_model_output)
</code></pre>
",3,1,2984,2022-01-11 21:38:59,https://stackoverflow.com/questions/70673763/how-to-use-embedding-layer-along-with-textvectorization-in-functional-api
Is there a faster way to convert sentences to TFHUB embeddings?,"<p>So I am involved in a project that involves feeding a combination of text embeddings and image vectors into a DNN to arrive at the result. Now for the word embedding part, I am using TFHUB's Electra while for the image part I am using a NASNet Mobile network.</p>
<p>However, the issue I am facing is that while running the word embedding part, using the code shown below, the code just keeps running nonstop. It has been over 2 hours now and my training dataset has just 14900 rows of tweets.</p>
<p>Note - The input to the function is just a list of 14900 tweets.</p>
<pre><code>tfhub_handle_encoder=&quot;https://tfhub.dev/google/electra_small/2&quot; 
tfhub_handle_preprocess=&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3

# Load Models 
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) 
bert_model = hub.KerasLayer(tfhub_handle_encoder)

def get_text_embedding(text):

  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='Preprocessor')   
  encoder_inputs = preprocessing_layer(text)   encoder = 
  hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='Embeddings')   outputs = 
  encoder(encoder_inputs)   text_repr = outputs['pooled_output']   text_repr = 
  tf.keras.layers.Dense(128, activation='relu')(text_repr)

  return text_repr

text_repr = get_text_embedding(train_text)
</code></pre>
<p>Is there a faster way to get text representation using these models?</p>
<p>Thanks for the help!</p>
","deep-learning, nlp, bert-language-model, word-embedding, tensorflow-hub","<p>The operation performed in the code is quadratic in its nature. While I managed to execute your snippet with 10000 samples within a few minutes, a 14900 long input ran out of memory on 32GB RAM runtime. Is it possible that your runtime is experiencing swapping?</p>
<p>It is not clear what is the snippet trying to achieve. Do you intend to train model? In such case you can define the text_input as an Input layer and use fit to train. Here is an example: <a href=""https://www.tensorflow.org/text/tutorials/classify_text_with_bert#define_your_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/classify_text_with_bert#define_your_model</a></p>
",0,0,103,2022-01-23 07:30:21,https://stackoverflow.com/questions/70820006/is-there-a-faster-way-to-convert-sentences-to-tfhub-embeddings
Understanding results of word2vec gensim for finding substitutes,"<p>I have implemented the word2vec model on transaction data <a href=""https://docs.google.com/spreadsheets/d/1mcFnagqajPm9XqJhBjgXW2waGraJ4mVq/edit?usp=sharing&amp;ouid=105487340749910932665&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">(link)</a> of a single category. <br>
My goal is to find substitutable items from the data.<br>
The model is giving results but I want to make sure that my model is giving results based on customers historical data (considering context) and not just based on content (semantic data). Idea is similar to the recommendation system. <br>
I have implemented this using the gensim library, where I passed the data (products) in form of a list of lists.</p>
<p>Eg.</p>
<pre><code>[['BLUE BELL ICE CREAM GOLD RIM', 'TILLAMK CHOC CHIP CK DOUGH  IC'],
 ['TALENTI SICILIAN PISTACHIO GEL', 'TALENTI BLK RASP CHOC CHIP GEL'],
 ['BREYERS HOME MADE VAN ICE CREAM',
  'BREYERS HOME MADE VAN ICE CREAM',
  'BREYERS COFF ICE CREAM']]
</code></pre>
<p>Here, each of the sub lists is the past one year purchase history of a single customer. <br></p>
<pre><code># train word2vec model
model = Word2Vec(window = 5, sg = 0,
                 alpha=0.03, min_alpha=0.0007,
                 seed = 14)

model.build_vocab(purchases_train, progress_per=200)

model.train(purchases_train, total_examples = model.corpus_count, 
            epochs=10, report_delay=1)

# extract all vectors
X = []
words = list(model.wv.index_to_key)
for word in words:
    x = model.wv.get_vector(word)
    X.append(x)
Y = np.array(X)
Y.shape

def similar_products(v, n = 3):
    
    # extract most similar products for the input vector
    ms = model.wv.similar_by_vector(v, topn= n+1)[1:]
    
    # extract name and similarity score of the similar products
    new_ms = []
    for j in ms:
        pair = (products_dict[j[0]][0], j[1]) 
        new_ms.append(pair)
        
    return new_ms 

similar_products(model.wv['BLUE BELL ICE CREAM GOLD RIM'])
</code></pre>
<p>Results:</p>
<pre><code> [('BLUE BELL ICE CREAM BROWN RIM', 0.7322707772254944),
     ('BLUE BELL ICE CREAM LIGHT', 0.4575043022632599),
     ('BLUE BELL ICE CREAM NSA', 0.3731085956096649)]
</code></pre>
<p>To get intuitive understanding of word2vec and its working on how results are obtained, I created a dummy <a href=""https://docs.google.com/spreadsheets/d/16n2CbXiiZSgFy3wAroC4uV4QUdiS3T8-/edit?usp=sharing&amp;ouid=105487340749910932665&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">dataset</a> where I wanted to find subtitutes of <code>'FOODCLUB VAN IC PAIL'</code>. <br>
If two products are in the same basket multiple times then they are substitutes. <br>
Looking at the data first substitute should be <code>'FOODCLUB CHOC CHIP IC PAIL'</code> but the results I obtained are:</p>
<pre><code>[('FOODCLUB NEAPOLITAN IC PAIL', 0.042492810636758804),
 ('FOODCLUB COOKIES CREAM ICE CREAM', -0.04012278839945793),
 ('FOODCLUB NEW YORK VAN IC PAIL', -0.040678512305021286)]
</code></pre>
<ol>
<li>Can anyone help me understand the intuitive working of word2vec model in gensim? Will each product be treated as word and customer list as sentence?</li>
<li>Why are my results so absurd in dummy dataset? How can I improve?</li>
<li>What hyperparameters play a significant role w.r.t to this model? Is negative sampling required?</li>
</ol>
","python, gensim, word2vec, word-embedding, recommendation-engine","<p>You may not get a very good intuitive understanding of usual word2vec behavior using these sorts of product-baskets as training data. The algorithm was originally developed for natural-language texts, where texts are runs of tokens whose frequencies, &amp; co-occurrences, follow certain indicative patterns.</p>
<p>People certainly do use word2vec on runs-of-tokens that aren't natural language - like product baskets, or logs-of-actions, etc – but to the extent such tokens have very-different patterns, it's possible extra preprocessing or tuning will be necessary, or useful results will be harder to get.</p>
<p>As just a few ways customer-purchases might be different from real language, depending on what your &quot;pseudo-texts&quot; actually represent:</p>
<ul>
<li>the ordering within a text might be an artifact of how you created the data-dump rather than anything meaningful</li>
<li>the nearest-neighbors to each token within the <code>window</code> may or may not be significant, compared to more distant tokens</li>
<li>customer ordering patterns might in general not be as reflective of shades-of-relationships as words-in-natural-language text</li>
</ul>
<p>So it's not automatic that word2vec will give interesting results here, for recommendatinos.</p>
<p>That's especially the case with small datasets, or tiny dummy datasets. Word2vec requires <em>lots</em> of varied data to pack elements into interesting relative positions in a high-dimensional space. Even small demos usually have a vocabulary (count of unique tokens) of tens-of-thousands, with training texts that provide varied usage examples of every token dozens of times.</p>
<p>Without that, the model never learns anything interesing/generalizable. That's especially the case if trying to create a many-dimensions model (say the default <code>vector_size=100</code>) with a tiny vocabulary (just dozens of unique tokens) with few usage examples per example. And it only gets worse if tokens appear fewer than the default <code>min_count=5</code> times – when they're ignored entirely. So don't expect anything interesting to come from your dummy data, at all.</p>
<p>If you want to develop an intuition, I'd try some tutorials &amp; other goals with real natural language text 1st, with a variety of datasets &amp; parameters, to get a sense of what has what kind of effects on result usefulness – &amp; only after that try to adapt word2vec to other data.</p>
<p>Negative-sampling is the default, &amp; works well with typical datasets, especially as they grow large (where negative-sampling suffes less of a performance hit than hierarchical-softmax with large vocabularies). But a toggle between those two modes is unlike to cause giant changes in quality unless there are other problems.</p>
<p>Sufficient data, of the right kind, is the key – &amp; then tweaking parameters may nudge end-result usefulness in a better direction, or shift it to be better for certain purposes.</p>
<p>But more specific parameter tips are only possible with clearer goals, once some baseline is working.</p>
",2,0,448,2022-01-25 05:32:55,https://stackoverflow.com/questions/70843845/understanding-results-of-word2vec-gensim-for-finding-substitutes
calculate similarity between one given word and a RANDOM list of words,"<p>I want to calculate the similarity between a given one word and a RANDOM list of words, then would rank the result in a new list, for example:</p>
<pre><code>list = ['bark','black','cat','bite','human','book'] #it could be another list
</code></pre>
<p>is similar to the word:</p>
<pre><code>word = ['dog']
</code></pre>
<p>--</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_md')


bark = nlp(&quot;bark&quot;)
bite = nlp(&quot;bite&quot;)
human = nlp(&quot;human&quot;)
book = nlp(&quot;book&quot;)
cat = nlp(&quot;cat&quot;)
black = nlp(&quot;black&quot;)

print(&quot;dog - bark&quot;, dog.similarity(bark)) #0.4258176903285793
print(&quot;dog - bite&quot;, dog.similarity(bite)) #0.4781574605069981
print(&quot;dog - human&quot;, dog.similarity(human)) #0.35814872466230835
print(&quot;dog - book&quot;, dog.similarity(book)) #0.22838638167627964
print(&quot;dog - cat&quot;, dog.similarity(cat)) #0.8016854705531046
print(&quot;dog - black&quot;, dog.similarity(black)) #0.30601667459001575
</code></pre>
<p>So how I can calculate the similarity of every word in the list to the given word automatically?</p>
","python, nlp, spacy, similarity, word-embedding","<p>You can do something like that:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('en_core_web_md')

words = ['bark','black','cat','bite','human','book']
word = 'dog'
word_nlp = nlp(word)

new_words = [(w, word_nlp.similarity(nlp(w))) for w in words]
new_words.sort(key=lambda x: x[1], reverse=True)

for w, value in new_words:
    print(f&quot;{word} - {w}&quot;, value)

</code></pre>
",0,0,418,2022-01-27 18:25:26,https://stackoverflow.com/questions/70883842/calculate-similarity-between-one-given-word-and-a-random-list-of-words
Use word2vec to expand a glossary in order to classify texts,"<p>I have a database containing about 3 million texts (tweets). I put clean texts (removing stop words, tags...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each text).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>I have also a list of words (belonging to the same topic, in this case: economics) called <code>terms</code>. I found that 7% of the texts contain at least one of these words (so we can say that 7% of total tweets talk about economics).</p>
<p>My goal is to expand the list <code>terms</code> in order to retrieve more texts belonging to the economic topic.</p>
<p>Then I use</p>
<p><code>results = model.most_similar(terms, topn=5000)</code></p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>Finally if I create the data frame</p>
<p><code>df = pd.DataFrame(results, columns=['key', 'similarity'])</code></p>
<p>I get something like that:</p>
<pre><code>key       similarity
word1     0.795432
word2     0.787954
word3     0.778942
...       ...
</code></pre>
<p>Now I think I have two possibilities to define the expanded glossary:</p>
<ul>
<li>I take the first N words (what should be the value of N?);</li>
<li>I look at the suggested words one by one and decide which one to include in the expanded glossary based on my knowledge (does this word really belong to the economic glossary?)</li>
</ul>
<p>How should I proceed in a case like this?</p>
","python, nlp, gensim, word2vec, word-embedding","<p>There's no general answer for what the cutoff should be, or how much you should use your own manual judgement versus cruder (but fast/automatic) processes. Those are inherently decisions which will be heavily influenced by your data, model quality, &amp; goals – so you have to try different approaches &amp; see what works there.</p>
<p>If you had a goal for what percentage of the original corpus you want to take – say, 14% instead of 7% – you could go as deeply into the ranked candidate list of 'similar words' as necessary to hit that 14% target.</p>
<p>Note that when you retrieve <code>model.most_similar(terms)</code>, you are asking the model to 1st average all words in <code>terms</code> together, then return words close to that one average point. To the extent your seed set of terms is tightly around the idea of <code>economics</code>, that might find words close to that generic average idea – but might not find other interesting words, such as close sysnonyms of your seed words that you just hadn't thought of. For that, you might want to get not 5000 neighbors for one generic average point, but (say) 3 neighbors for every individual term. To the extent the 'shape' of the topic isn't a perfect sphere around someplace in the word-vector-space, but rather some lumpy complex volume, that might better reflect your intent.</p>
<p>Instead of using your judgement of the candidate words standing alone to decide whether a word is <code>economics</code>-related, you could instead look at the texts that a word uniquely brings in. That is, for new word X, look at the N texts that contain that word. How many, when applying your full judgement to their full text, deserve to be in your 'economics' subset? Only if it's above some threshold T would you want to move X into your glossary.</p>
<p>But such an exercise may just highlight: using a simple glossary – &quot;for any of these hand-picked N words, every text mentioning at least 1 word is in&quot; – is a fairly crude way of assessing a text's topic. There are other ways to approach the goal of &quot;pick a relevant subset&quot; in an automated way.</p>
<p>For example, you could view your task as that of training a text binary classifier to classify texts as 'economics' or 'not-economics'.</p>
<p>In such a case, you'd start with some training data - a set of example documents that are already labeled 'economics' or 'not-economics', perhaps via individual manual review, or perhaps via some crude bootstrapping (like labeling all texts with some set of glossary words as 'economics', &amp; all others 'not-economics'). Then you'd draw from the full range of potential text-preprocessing, text-feature-extracton, &amp; classification options to train &amp; evaluate classifiers that make that judgement for you. Then you'd evaluate/tune those – a process wich might also improve your training data, as you add new definitively 'economics' or 'not-economics' texts – &amp; eventually settle on one that works well.</p>
<p>Alternatively, you could use some other richer topic-modeling methods (LDA, word2vec-derived <code>Doc2Vec</code>, deeper neural models etc) for modeling the whole dataset, then from some seed-set of definite-'economics' texts, expand outward from them – finding nearest-examples to known-good documents, either auto-including them or hand-reviewing them.</p>
<p>Separately: <code>min_count=1</code> is almost always a mistake in word2vec &amp; related algorihtms, which do better if you <em>discard</em> words so rare they lack the variety of multiple usage examples the algorithm needs to generate good word-vectors.</p>
",1,0,290,2022-01-30 14:57:09,https://stackoverflow.com/questions/70915829/use-word2vec-to-expand-a-glossary-in-order-to-classify-texts
Convert grouped data in dataframe to documents in preparation for word2vec,"<p>I'm trying to replicate what the author of <a href=""https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9319034"" rel=""nofollow noreferrer"">this paper</a> has achieved with the publicly available Medicare dataset.</p>
<p>In summary, the author groups medical provider claims by the providers ID, their taxonomy and the HCPCS (codes of the procedures they performed) by most frequent to least frequent, see below image:</p>
<p><a href=""https://i.sstatic.net/vqvUv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vqvUv.png"" alt=""enter image description here"" /></a></p>
<p>Using the code below I have been able to recreate the top left hand table and also the bottom left table (I don't think it's necessary) but I don't know how to group each providers HCPCS codes by highest frequency to lowest frequency in preparation for feeding it into word2vec to train an embedding model.</p>
<p>If I could get some help preparing the data ready for word2vec training I would be very grateful.</p>
<pre><code>library(httr)
library(jsonlite)
library(tidyverse)

# CONNECT TO CMS DATA
res &lt;- GET(&quot;https://data.cms.gov/data-api/v1/dataset/5fccd951-9538-48a7-9075-6f02b9867868/data?size=5000&quot;)

# CONVERT TO DATA FRAME
data = fromJSON(rawToChar(res$content))

# GROUPING AND COUNTING OCCURANCES OF HCPCS PER PROVIDER ID
providerHCPCS &lt;- data %&gt;% 
  group_by(Rndrng_NPI,Rndrng_Prvdr_Type,HCPCS_Cd) %&gt;% 
  count(HCPCS_Cd, name = &quot;Line_Srvc_Cnt&quot;) %&gt;% 
  group_by(Rndrng_NPI) %&gt;% 
  arrange(desc(Line_Srvc_Cnt), .by_group = TRUE)
</code></pre>
","r, tidyverse, word2vec, word-embedding","<p>Is this what you want as a result?</p>
<pre><code>table2 &lt;- providerHCPCS %&gt;% group_by(Rndrng_NPI, Rndrng_Prvdr_Type) %&gt;% summarise(HCPCS_sequence = (paste(HCPCS_Cd, collapse=&quot;, &quot;)))

</code></pre>
",1,0,45,2022-02-03 23:55:27,https://stackoverflow.com/questions/70979931/convert-grouped-data-in-dataframe-to-documents-in-preparation-for-word2vec
"When applying word2vec, should I standardize the cosine values within each year, when comparing them across years?","<p>I'm a researcher, and I'm trying to apply NPL to understand the temporal changes of the meaning of some words.
So far I have obtained the trained embeddings (word2vec, sgn) of several years with identical parameters in the training.
For example, if I want to test the change of cosine similarity of word A and word B over 5 years, should I just compute them and plot the cosine values?</p>
<p>The reason I'm asking this is that I found the overall cosine values (mean of all possible pairs within that year) differ across the 5 years. **For example, 1990:0.21, 1991:0.19, 1992:0.31, 1993:0.22, 1994:0.31. Does it mean in some years, all words are more similar to each other than other years??</p>
<p>Base on my limited understanding, I think the vectors are odds in logistic functions, so they shouldn't be significantly affected by the size of the corpus? Is it necessary for me to standardize the cosine values (of all pairs within each year) so I can compare the relative ranking change across years? Or just trust the raw cosine values and compare them across years?</p>
","word2vec, word-embedding, cosine-similarity","<p>In general you should not think of cosine-similarities as an <em>absolute</em> measure that'd be comparable between models. That is, you should not think of &quot;0.7&quot; cosine-similarity as anything like &quot;70%&quot; similar, and choose some arbitrary &quot;70%&quot; threshold to be used across models.</p>
<p>Instead, it's only a measure within a single model's induced space - with its effective 'scale' affected by all the parameters &amp; the training data.</p>
<p>One small exercise that may help illustrate this: with the exact same data, train a 100d model, then a 200d model. Then look at some word pairs, or words alongside their nearest-neighbors ranked by cosine-similarity.</p>
<p>With enough training/data, generally the same highly-related words will be nearest-neighbors of each other. But the effective ranges of cosine-similarity values will be very different. If you chose a specific threshold in one model as meaning, &quot;close enough to feed some other analysis&quot;, the same threshold would <em>not</em> be sufficient in the other. Every model is its own world, induced by the training data &amp; parameters, as well as some sources of explicit or implicit randomness during training. (Several parts of the word2vec algorithm use random sampling, but also any efficient multi-threaded training will encounter arbitray differences in training-order via host OS thread-scheduling vagaries.)</p>
<p>If your parameters are identical, &amp; the corpora very-alike in every measurable internal proportion, these effects might be minimized, but never eliminated.</p>
<p>For example, even if people's intended word meanings were perfectly identical, one year's training data might include more discussion of 'war' or 'politics' or some medical-topic, than another. In that case, the iterative, interleaved tug-of-war in training updates will mean words from that overrepresented domain have far more push-pull influence on the final model word positions – essentially warping subregions of the final space for <em>finer</em> distinctions some places, and thus *coarser distinctions in the less-updated zones.</p>
<p>That is, you shouldn't expect any global-per-model scaling factor (as you've implied might apply) to correct for any model-to-model differences. The influences of different data &amp; training runs are far more subtle, and might affect different 'neighborhoods' of words differently.</p>
<p>Instead, when comparing different models, a more stable grounds for comparison is <em>relative rankings</em> or <em>relative-proportions</em> of words with respect to their closeness-to-others. Did words move into, or out of, each others' top-N neighbors? Did A move more closely to B than C did to D? etc.</p>
<p>Even there, you might want to be careful about differences in the full vocabulary: if A &amp; B were each others' closest neighbors year 1, but 5 other words squeeze between them in year 2, did any word's meaning <em>really</em> change? Or might it simply be because those other words weren't even suitably represented in year 1 to receive any position, or previously had somewhat 'noisier' positions nearby? (As words get rarer their positions from run to run will be more idiosyncratic, based on their few usage examples, and the influences of those other sources of run-to-run 'noise'.)</p>
<p>Limiting all such analyses to very-well-represented words will minimize misinterpreting noise-in-the-models as something meaningful. Re-running models more than once, either with same parameters or slightly-different ones, or slightly-different training data subsets, and seeing which comparisons hold up across such changes, may also help determine which observed changes are robust, versus methodological artifacts such as jitter from run-to-run, or other sampling effects.</p>
<p>A few previous answers on similar questions about comparing word-vectors across different source corpora may have other useful ideas or caveats for you:</p>
<p><a href=""https://stackoverflow.com/questions/59084092/how-calculate-distance-between-2-node2vec-model/59095246#59095246"">how calculate distance between 2 node2vec model</a></p>
<p><a href=""https://stackoverflow.com/questions/57392103/word-embeddings-for-the-same-word-from-two-different-texts/57400356#57400356"">Word embeddings for the same word from two different texts</a></p>
<p><a href=""https://stackoverflow.com/questions/61736874/how-to-compare-cosine-similarities-across-three-pretrained-models/61741677#61741677"">How to compare cosine similarities across three pretrained models?</a></p>
",1,1,487,2022-02-08 12:13:45,https://stackoverflow.com/questions/71033726/when-applying-word2vec-should-i-standardize-the-cosine-values-within-each-year
Combine similar elements in N*N matrix without duplicates,"<p>I have a list of sentences, and I want to find all the sentences similar to it and put them together in a list/tuple.</p>
<p>I formed sentence embeddings for them, then computed an N*N cosine similarity matrix for N sentences. I then iterated through the elements, and picked the ones higher than a threshold.</p>
<p>If <code>sentences[x]</code> is similar to <code>sentences[y]</code> and <code>sentences[z]</code>, if I combine <code>sentences[x]</code> and <code>sentences[y]</code>, <code>sentences[x]</code> should not combine with <code>sentences[z]</code> as the loop iterates further</p>
<p>I went with the intuition that since we are comparing cosine similarities, if X is similar to Y, and Y is similar to Z, X will be similar to Z as well, so I should not have to worry about it. My goal is to not have duplicates, but I am stuck.</p>
<p>Is there a better way / what's the best way to do this?</p>
<p>Here is my code:</p>
<pre><code>import pandas as pd
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim, pytorch_cos_sim

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
threshold = 0.75

def form_embeddings(sentences, embedding_model=embedding_model):

    if isinstance(sentences, str):
        sentences = [sentences]

    return embedding_model.encode(sentences, convert_to_tensor=True)

df = pd.read_csv('sample_file.csv')
sentences = df['sentences'].tolist()

#form embeddings
sentence_embeddings = form_embeddings(sentences=sentences)

#form similarity matrix
sim_matrix = pytorch_cos_sim(sentence_embeddings, sentence_embeddings)

#set similarity with itself as zero
sim_matrix.fill_diagonal_(0)

#iterate through and find pairs of similarity
pairs = []
for i in range(len(sentences)):
    for j in range(i, len(sentences)):
        if sim_matrix[i,j] &gt;= threshold:
            pairs.append({'index':[i,j], 'score': sim_matrix[i,j], 'original_sentence':sentences[i], 'similar_sentence':sentences[j]})
</code></pre>
","python, nlp, word-embedding, sentence-similarity","<p>I figured out a better way to do this.</p>
<p>This is solved by a <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/fast_clustering.py"" rel=""nofollow noreferrer"">fast clustering implementation</a>.</p>
",0,0,334,2022-02-18 10:18:56,https://stackoverflow.com/questions/71171821/combine-similar-elements-in-nn-matrix-without-duplicates
TensorFlow word embedding model + LDA Negative values in data passed to LatentDirichletAllocation.fit,"<p>I am trying to use a pre-trained <a href=""https://tfhub.dev/google/nnlm-en-dim128/2"" rel=""nofollow noreferrer"">model</a> from TensorFlow hub instead of frequency vectorization techniques for word embedding before passing the resultant feature vector to the LDA model.</p>
<p>I followed the steps for the TensorFlow model, but I got this error upon passing the resultant feature vector to the LDA model:</p>
<pre><code>Negative values in data passed to LatentDirichletAllocation.fit
</code></pre>
<p>Here's what I have implemented so far:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import tensorflow_hub as hub

from sklearn.decomposition import LatentDirichletAllocation

embed = hub.load(&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim50-with-normalization/1&quot;)
embeddings = embed([&quot;cat is on the mat&quot;, &quot;dog is in the fog&quot;])
lda_model = LatentDirichletAllocation(n_components=2, max_iter=50)
lda = lda_model.fit_transform(embeddings)
</code></pre>
<p>I realized that <code>print(embeddings)</code> prints some negative values as shown below:</p>
<pre><code>tf.Tensor(
[[ 0.16589954  0.0254965   0.1574857   0.17688066  0.02911299 -0.03092718
   0.19445257 -0.05709129 -0.08631689 -0.04391516  0.13032274  0.10905275
  -0.08515751  0.01056632 -0.17220995 -0.17925954  0.19556305  0.0802278
  -0.03247919 -0.49176937 -0.07767699 -0.03160921 -0.13952136  0.05959712
   0.06858718  0.22386682 -0.16653948  0.19412343 -0.05491862  0.10997339
  -0.15811177 -0.02576607 -0.07910853 -0.258499   -0.04206644 -0.20052543
   0.1705603  -0.15314153  0.0039225  -0.28694248  0.02468278  0.11069503
   0.03733957  0.01433943 -0.11048374  0.11931834 -0.11552787 -0.11110869
   0.02384969 -0.07074881]
</code></pre>
<p>But, is there a solution to this?</p>
","tensorflow, scikit-learn, lda, word-embedding, topic-modeling","<p>As the <code>fit</code> function of <code>LatentDirichletAllocation</code> does not allow a negative array, I will recommend you to apply <a href=""https://en.wikipedia.org/wiki/Activation_function#:%7E:text=displaystyle%20C%5E%7B%5Cinfty%20%7D%7D-,Softplus%5B8%5D,-%7B%5Cdisplaystyle%20%5Cln%20%5Cleft"" rel=""nofollow noreferrer"">softplus</a> on the <code>embeddings</code>.</p>
<p>Here is the code snippet:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import tensorflow_hub as hub
from tensorflow.math import softplus

from sklearn.decomposition import LatentDirichletAllocation

embed = hub.load(&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim50-with-normalization/1&quot;)
embeddings = softplus(embed([&quot;cat is on the mat&quot;, &quot;dog is in the fog&quot;]))

lda_model = LatentDirichletAllocation(n_components=2, max_iter=50)
lda = lda_model.fit_transform(embeddings)
</code></pre>
",1,0,321,2022-02-24 08:30:27,https://stackoverflow.com/questions/71249109/tensorflow-word-embedding-model-lda-negative-values-in-data-passed-to-latentdi
Can I use a different corpus for fasttext build_vocab than train in Gensim Fasttext?,"<p>I am curious to know if there are any implications of using a different source while calling the <code>build_vocab</code> and <code>train</code> of Gensim <code>FastText</code> model. Will this impact the contextual representation of the word embedding?</p>
<p>My intention for doing this is that there is a specific set of words I am interested to get the vector representation for and when calling <code>model.wv.most_similar</code>. I only want words defined in this vocab list to get returned rather than all possible words in the training corpus. I would use the result of this to decide if I want to group those words to be relevant to each other based on similarity threshold.</p>
<p>Following is the code snippet that I am using, appreciate your thoughts if there are any concerns or implication with this approach.</p>
<ul>
<li>vocab.txt contains a list of unique words of interest</li>
<li>corpus.txt contains full conversation text (i.e. chat messages) where each line represents a paragraph/sentence per chat</li>
</ul>
<p>A follow up question to this is what values should I set for <code>total_examples</code> &amp; <code>total_words</code> during training in this case?</p>
<pre><code>from gensim.models.fasttext import FastText

model = FastText(min_count=1, vector_size=300,)

corpus_path = f'data/{client}-corpus.txt'
vocab_path = f'data/{client}-vocab.txt'
# Unsure if below counts should be based on the training corpus or vocab
corpus_count = get_lines_count(corpus_path)
total_words = get_words_count(corpus_path)

# build the vocabulary
model.build_vocab(corpus_file=vocab_path)

# train the model
model.train(corpus_file=corpus.corpus_path, epochs=100, 
    total_examples=corpus_count, total_words=total_words,
)

# save the model
model.save(f'models/gensim-fastext-model-{client}')
</code></pre>
","python, nlp, gensim, word-embedding, fasttext","<p>Incase someone has similar question, I'll paste the reply I got when asking this question in the <a href=""https://groups.google.com/d/msgid/gensim/e1330c28-e5df-4494-9bdb-bc1eaccd18dfn%40googlegroups.com?utm_medium=email&amp;utm_source=footer"" rel=""nofollow noreferrer"">Gensim Disussion Group</a> for reference:</p>
<blockquote>
<p>You can try it, but I wouldn't expect it to work well for most
purposes.</p>
<p>The <code>build_vocab()</code> call establishes the known vocabulary of the
model, &amp; caches some stats about the corpus.</p>
<p>If you then supply another corpus – &amp; especially one with <em>more</em> words
– then:</p>
<ul>
<li>You'll want your <code>train()</code> parameters to reflect the actual size of your training corpus. You'll want to provide a true <code>total_examples</code> and <code>total_words</code> count that are accurate for the training-corpus.</li>
<li>Every word in the training corpus that's not in the know vocabulary is ignored completely, as if it wasn't even there. So you might as
well filter your corpus down to just the words-of-interest first, then
use that same filtered corpus for both steps. Will the example texts
still make sense? Will that be enough data to train meaningful,
generalizable word-vectors for just the words-of-interest, alongside
other words-of-interest, without the full texts? (You could look at
your pref-filtered corpus to get a sense of that.) I'm not sure - it
could depend on how severely trimming to just the words-of-interest
changed the corpus. In particular, to train high-dimensional dense
vectors – as with <code>vector_size=300</code> – you need a lot of varied data.
Such pre-trimming might thin the corpus so much as to make the
word-vectors for the words-of-interest far less useful.</li>
</ul>
<p>You could certainly try it both ways – pre-filtered to just your
words-of-interest, or with the full original corpus – and see which
works better on downstream evaluations.</p>
<p>More generally, if the concern is training time with the full corpus,
there are likely other ways to get an adequate model in an acceptable
amount of time.</p>
<p>If using <code>corpus_file</code> mode, you can increase <code>workers</code> to equal the
local CPU core count for a nearly-linear speedup from number of cores.
(In traditional <code>corpus_iterable</code> mode, max throughput is usually
somewhere in the 6-12 <code>workers</code> threads, as long as you ahve that many
cores.)</p>
<p><code>min_count=1</code> is usually a bad idea for these algorithms: they tend to
train faster, in less memory, leaving better vectors for the remaining
words when you discard the lowest-frequency words, as the default
<code>min_count=5</code> does. (It's possible <code>FastText</code> can eke a little bit of
benefit out of lower-frequency words via their contribution to
character-n-gram-training, but I'd only ever lower the default
<code>min_count</code> if I could confirm it was actually improving relevant
results.</p>
<p>If your corpus is so large that training time is a concern, often a
more-aggressive (smaller) <code>sample</code> parameter value not only speeds
training (by dropping many redundant high-frequency words), but ofthen
improves final word-vector quality for downstream purposes as well (by
letting the rarer words have relatively more influence on the model in
the absense of the downsampled words).</p>
<p>And again if the corpus is so large that training time is a concern,
than <code>epochs=100</code> is likely overkill. I believe the <code>GoogleNews</code>
vectors were trained using only 3 passes – over a gigantic corpus. A
sufficiently large &amp; varied corpus, with plenty of examples of all
words all throughout, could potentially train in 1 pass – because each
word-vector can then get more total training-updates than many epochs
with a small corpus. (In general larger <code>epochs</code> values are more often
used when the corpus is thin, to eke out something – not on a corpus
so large you're considering non-standard shortcuts to speed the
steps.)</p>
<p>-- Gordon</p>
</blockquote>
",1,2,425,2022-02-28 01:06:02,https://stackoverflow.com/questions/71289683/can-i-use-a-different-corpus-for-fasttext-build-vocab-than-train-in-gensim-fastt
How does word2vec learn word relations?,"<p>Which part of the algorithm specifically makes the embeddings to have the <code>king - boy + girl = queen</code> ability? Did they just did this by accident?</p>
<p>Edit :</p>
<p>Take the CBOW as an example. I know about they use embeddings instead of one-hot vectors to encode the words and made the embeddings trainable instead of how we do when using one hot vectors that the data itself is not trainable. Then the output is a one-hot vector for target word. They just average all the surrounding word embeddings at some point then put some lego layers afterwards. So at the end they find the mentioned property by surprise, or is there a training procedure or network structure that gave the embeddings that property?</p>
","word2vec, embedding, word-embedding","<p>The algorithm simply works to train (optimize) a shallow neural-network model that's good at predicting words, from other nearby words.</p>
<p>That's the only internal training goal – subject to the neural network's constraints on how the words are represented (N floating-point dimensions), or combined with the model's internal weights to render an interpretable prediction (forward propagation rules).</p>
<p>There's no other 'coaching' about what words 'should' do in relation to each other. All words are still just opaque tokens to word2vec. It doesn't even consider their letters: the whole-token is just a lookup key for a whole-vector. (Though, the word2vec variant FastText varies that somewhat by also training vectors for subwords – &amp; thus can vaguely simulate the same intuitions that people have for word-roots/suffixes/etc.)</p>
<p>The interesting 'neighborhoods' of nearby words, and relative orientations that align human-interpretable aspects to vague directions in the high-dimensional coordinate space, fall out of the prediction task. And those relative orientations are what gives rise to the surprising &quot;analogical arithmetic&quot; you're asking about.</p>
<p>Internally, there's a tiny internal training cycle applied over and over: &quot;nudge this word-vector to be slightly better at predicting these neighboring words&quot;. Then, repeat with another word, and other neighbors. And again &amp; again, millions of times, each time only looking at a tiny subset of the data.</p>
<p>But the updates that contradict each other cancel out, and those that represent reliable patterns in the source training texts reinforce each other.</p>
<p>From one perspective, it's essentially trying to &quot;compress&quot; some giant vocabulary – tens of thousands, to millions, of unique words – into a smaller N-dimensional representation - usually 100-400 dimensions when you have enough training data. The dimensional-values that become as-good-as-possible (but never necessary great) at predicting neighbors turn out to exhibit the other desirable positionings, too.</p>
",0,0,218,2022-03-02 09:46:30,https://stackoverflow.com/questions/71320529/how-does-word2vec-learn-word-relations
Update an element in faiss index,"<p>I am using <code>faiss indexflatIP</code> to store vectors related to some words. I also use another list to store words (the vector of the nth element in the list is nth vector in faiss index). I have two questions:</p>
<ol>
<li>Is there a better way to relate words to their vectors?</li>
<li>Can I update the nth element in the faiss?</li>
</ol>
","python, word-embedding, faiss","<p>You can do both.</p>
<blockquote>
<ol>
<li>Is there a better way to relate words to their vectors?</li>
</ol>
</blockquote>
<p>Call <code>index.add_with_ids(vectors, ids)</code></p>
<p>Some index types support the method <code>add_with_ids</code>, but flat indexes don't.</p>
<p>If you call the method on a flat index, you will receive the error <code>add_with_ids not implemented for this type of index</code></p>
<p>If you want to use IDs with a flat index, you must use <code>index2 = faiss.IndexIDMap(index)</code></p>
<blockquote>
<ol start=""2"">
<li>Can I update the nth element in the faiss?</li>
</ol>
</blockquote>
<p>If you want to update some encodings, first remove them, then add them again with <code>add_with_ids</code></p>
<p>If you don't remove the original IDs first, you will have duplicates and search results will be messed up.</p>
<p>To remove an array of IDs, call <code>index.remove_ids(ids_to_replace)</code></p>
<p>Nota bene: IDs must be of <code>np.int64</code> type.</p>
",9,8,11676,2022-03-26 12:10:28,https://stackoverflow.com/questions/71627943/update-an-element-in-faiss-index
tensorflow2.x keras Embedding layer process tf.dataset error,"<p>This question is a follow-up of <a href=""https://stackoverflow.com/questions/71714299/tensorflow-2-textvectorization-process-tensor-and-dataset-error"">tensorflow 2 TextVectorization process tensor and dataset error</a></p>
<p>I would like to make do a word embedding for the processed text with tnesorflow 2.8 on Jupyter.</p>
<pre><code>def standardize(input_data):

    input_data = tf.strings.lower(input_data)
    input_data = tf.strings.regex_replace(input_data, f&quot;[{re.escape(string.punctuation)}]&quot;, &quot; &quot;)
    return input_data

# the input data loaded from text files by TfRecordDataset(file_paths, &quot;GZIP&quot;)
# each file can be 200+MB, totally about 300 files
# each file hold the data with multiple columns
# some columns are text
# after loading, the dataset will be accessed by column name 
# e.g. one column is &quot;sports&quot;, so the input_dataset[&quot;sports&quot;] 
# return a tensor, which is like the following example

input_data = tf.constant([[&quot;SWIM 2008-07 Baseball&quot;], [&quot;Football&quot;]], shape=(2, 1), dtype=tf.string)

text_layer = tf.keras.layers.TextVectorization( standardize = standardize, max_tokens = 10, output_mode = 'int', output_sequence_length=10 )

dataset = tf.data.Dataset.from_tensors( input_data )

dataset = dataset.batch(2)

text_layer.adapt(dataset)

process_text = dataset.map(text_layer)

emb_layer = layers.Embedding(10, 10)

emb_layer(process_text) # error 
</code></pre>
<p>error:</p>
<pre><code> AttributeError: Exception encountered when calling layer &quot;embedding_7&quot; (type Embedding).

'MapDataset' object has no attribute 'dtype'

Call arguments received:

 • inputs=&lt;MapDataset element_spec=TensorSpec(shape=(None, 2, 10), dtype=tf.int64, name=None)&gt;
</code></pre>
<p>How can I convert a tf.dataset to tf.tensor ?</p>
<p>This <a href=""https://stackoverflow.com/questions/64497977/tensorflow-convert-tf-dataset-to-tf-tensor"">TensorFlow: convert tf.Dataset to tf.Tensor</a> does not help me.</p>
<p>The above layers will be implemented in a machine learning neural network model.</p>
<pre><code>loading data --&gt; processing features (multiple text columns) --&gt; tokens --&gt; embedding --&gt; average pooling --&gt; some dense layers --&gt; output layer
</code></pre>
<p>thanks</p>
","python, tensorflow, keras, text-processing, word-embedding","<p>You cannot feed a <code>tf.data.Dataset</code> directly to an <code>Embedding</code> layer, you can either use <code>.map(...)</code>:</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import re
import string 
def standardize(input_data):

    input_data = tf.strings.lower(input_data)
    input_data = tf.strings.regex_replace(input_data, f&quot;[{re.escape(string.punctuation)}]&quot;, &quot; &quot;)
    return input_data

input_data = tf.constant([[&quot;SWIM 2008-07 Baseball&quot;], [&quot;Football&quot;]], shape=(2, 1), dtype=tf.string)

text_layer = tf.keras.layers.TextVectorization( standardize = standardize, max_tokens = 10, output_mode = 'int', output_sequence_length=10 )

dataset = tf.data.Dataset.from_tensors( input_data )

dataset = dataset.batch(2).map(lambda x: tf.squeeze(x, axis=0))

text_layer.adapt(dataset)

process_text = dataset.map(text_layer)

emb_layer = layers.Embedding(10, 10)
process_text = process_text.map(emb_layer)
</code></pre>
<p>Or define your model and feed your dataset through <code>model.fit(...)</code>:</p>
<pre><code>import tensorflow as tf
import re
import string 
def standardize(input_data):

    input_data = tf.strings.lower(input_data)
    input_data = tf.strings.regex_replace(input_data, f&quot;[{re.escape(string.punctuation)}]&quot;, &quot; &quot;)
    return input_data

input_data = tf.constant([[&quot;SWIM 2008-07 Baseball&quot;], [&quot;Football&quot;]], shape=(2, 1), dtype=tf.string)

text_layer = tf.keras.layers.TextVectorization( standardize = standardize, max_tokens = 10, output_mode = 'int', output_sequence_length=10 )

dataset = tf.data.Dataset.from_tensors( input_data )

dataset = dataset.batch(2)

text_layer.adapt(dataset)

process_text = dataset.map(lambda x: (text_layer(tf.squeeze(x, axis=0)), tf.random.uniform((2, ), maxval=2, dtype=tf.int32))) # add random label to each entry

inputs = tf.keras.layers.Input((10, ))
emb_layer = tf.keras.layers.Embedding(10, 10)
x = emb_layer(inputs)
x = tf.keras.layers.GlobalAveragePooling1D()(x)
outputs = tf.keras.layers.Dense(1, 'sigmoid')(x)
model = tf.keras.Model(inputs, outputs)
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(process_text)
</code></pre>
",1,1,529,2022-04-02 22:45:07,https://stackoverflow.com/questions/71721694/tensorflow2-x-keras-embedding-layer-process-tf-dataset-error
Keras Semantic Similarity model from pre-trained embeddings,"<p>I want to implement a Keras model to predict the similarity between two sentences from words embeddings as follows (I included my full script at the end):</p>
<ol>
<li>Load words embeddings models, e.g., Word2Vec and fastText.</li>
<li>Generate samples (<code>X1</code> and <code>X2</code>) by computing the average word vectors for all words in a sentence. If two or more models are used, calculate the arithmetic mean of all embeddings (<a href=""https://arxiv.org/abs/1804.05262"" rel=""nofollow noreferrer""><em>Frustratingly Easy Meta-Embedding -- Computing Meta-Embeddings by Averaging Source Word Embeddings</em></a>).</li>
<li>Concatenate <code>X1</code> and <code>X2</code> into one array before feeding them to the network.</li>
<li>Compile (and evaluate) the Keras model.</li>
</ol>
<p>The entire script is as follows:</p>
<pre><code>import numpy as np
from gensim.models import Word2Vec
from keras.layers import Dense
from keras.models import Sequential
from sklearn.model_selection import train_test_split


def encoder_vector(v: str, model: Word2Vec) -&gt; np.array:
    wv_dim = model.vector_size
    if v in model.wv:
        return model.wv[v]
    else:
        return np.zeros(wv_dim)


def encoder_words_avg(words: list[str], model: Word2Vec) -&gt; np.array:
    dim = model.vector_size
    words = [word for word in words if word in model.wv]
    if len(words) &gt;= 1:
        return np.mean(model.wv[words], axis=0)
    else:
        return np.zeros(dim)


def load_samples(mappings, w2v_model, fast_model):
    dim = w2v_model.vector_size
    num = len(mappings)

    X1 = np.zeros((num, dim))
    X2 = np.zeros((num, dim))
    y = np.zeros((num, 1))

    for i in range(num):
        mapping = mappings[i].split(&quot;|&quot;)
        sentence_1, sentence_2 = mapping[1:]

        e = np.zeros((2, dim))

        # Compute meta-embedding by averaging all embeddings.
        e[0, :] = encoder_words_avg(words=sentence_1.split(), model=w2v_model)
        e[1, :] = encoder_words_avg(words=sentence_1.split(), model=fast_model)
        X1[i] = e.mean(axis=0)

        e[0, :] = encoder_words_avg(words=sentence_2.split(), model=w2v_model)
        e[1, :] = encoder_words_avg(words=sentence_2.split(), model=fast_model)
        X2[i] = e.mean(axis=0)

        y[i] = 0.0 if mapping[0].startswith(&quot;-&quot;) else 1.0

    return X1, X2, y


def baseline_model(X_train, X_test, y_train, y_test):
    model = Sequential()
    model.add(
        Dense(
            200,
            input_shape=(X_train.shape[1],),
            activation=&quot;relu&quot;,
            kernel_initializer=&quot;he_uniform&quot;,
        )
    )
    model.add(Dense(1, activation=&quot;sigmoid&quot;))
    model.compile(optimizer=&quot;sgd&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
    model.fit(X_train, y_train, batch_size=8, epochs=14)

    # Evaluate the trained model, using the train and test data
    _, train_acc = model.evaluate(X_train, y_train, verbose=0)
    _, test_acc = model.evaluate(X_test, y_test, verbose=0)

    print(&quot;Train: %.3f, Test: %.3f\n&quot; % (train_acc, test_acc))

    return model


def main():
    w2v_model = Word2Vec.load(&quot;&quot;)
    fast_model = Word2Vec.load(&quot;&quot;)

    mappings = [
        &quot;1|boiled chicken egg|hen egg whole boiled&quot;,
        &quot;2|tomato|tomato substance&quot;,
        &quot;3|sweet potatoes|potato chip&quot;,
        &quot;-1|watering plants|cornsalad plant&quot;,
        &quot;-2|butter|butane&quot;,
        &quot;-3|olive plant|black olives&quot;,
    ]

    X1, X2, y = load_samples(mappings, w2v_model=w2v_model, fast_model=fast_model)

    # Concatenate both arrays into one before feeding to the network.
    X = np.concatenate([X1, X2], axis=1)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    model = baseline_model(X_train, X_test, y_train, y_test)

    model.summary()
</code></pre>
<p>The above script seems to work, but the prediction result is very poor even when using only Word2Vec (which makes me think there could be an issue with the Keras model...). Any ideas on how to improve the outcome? Am I doing something wrong?</p>
<p>Thank you.</p>
","keras, neural-network, word2vec, similarity, word-embedding","<p>It's unclear what you're intending to predict.</p>
<p>Do you want your Keras NN to report the <em>same</em> value as the precise cosine-similarity calculation, between the two text summary vectors, would report? If so, why not just... do the calculation? It's not something I'd necessarily expect a neural-architecture to approxmate better.</p>
<p>Alternatively, if your tiny 6-pair dataset is the target:</p>
<ol>
<li><p>Your existing 'gold standard' answers don't seem obviously correct to me. Superficially, 'olive plant' &amp; 'black olives' seem nearly as 'similar' as 'tomato' &amp; 'tomato substance'. Similarly, 'watering plants' &amp; 'cornsalad plant' about-as-similar as 'sweet potatoes' &amp; 'potato chip'.</p>
</li>
<li><p>A mere 6 examples (maybe 5 after train/test split?) is both inadequate to usefully train a larger neural classifier, <em>and</em> to the extent the classifer might be easily trained (indeed 'overfit') to the 5 training examples, it won't necessarily have learned anything generalizable to the one hold-out example (which is using vectors quite far from the training texts). (With such a paucity of training data, &amp; testing using inputs that might be arbitrarily different than the training data, &quot;very poor&quot; performance would be expected. Neural nets require lots of varied training examples!)</p>
</li>
</ol>
<p>Finally, the strategy of creating combined-embeddings-by-averaging, as investigated by your linked paper, is another atypical practice that seems fishy to me. Even if it could offer some benefits, there's no reason to mix that atypical, somewhat non-intuitive extra practice into your experiment before even having things work with a more typical and simple baseline approach, for comparison, to be sure the extra 'meta'/averaging is worth the complication.</p>
<p>The paper itself doesn't really show any advantage over concatenation, which has a stronger theoretical basis (preserving each model's full independent spaces) than averaging, except by a tiny amount in 1-of-6 tests. Further, average of GLoVe &amp; CBOW performs <em>the same or worse</em> than GLoVe alone on 3 on their 6 evaluations – and pretty minimally better on the 3 other evaluations. That implies to me the outperformance might be mainly random jitter introduced by the extra steps, and the averaging is – at best – a cheap option to consider as a tiny boost, not a generally-better approach.</p>
<p>The paper also leaves many natural related questions unaddressed:</p>
<ul>
<li>Is averaging better than, say, just picking a random half of each models' dimensions for concatenation? That'd be even cheaper!</li>
<li>Might some of the <em>slight</em> lift in <em>some</em> tasks be due not to the averaging, but the other transformations they've applied – the l2-normalization applied to each source model, or across the whole of each dimension for the GLoVE model? (It's unclear if this model-postprocessing was only applied before dual-model averaging, or also to GLoVe in its solo evaluation.)</li>
</ul>
<p>There's other work suggesting post-training transformations of word-vector spaces may improve performance on downstream tasks – see for example <a href=""https://arxiv.org/abs/1702.01417"" rel=""nofollow noreferrer"">'All But The Top'</a> – so which steps, exactly, get which advantages is important to distinguish.</p>
",1,1,458,2022-04-08 20:15:55,https://stackoverflow.com/questions/71802729/keras-semantic-similarity-model-from-pre-trained-embeddings
how to replace keras embedding with pre-trained word embedding to CNN,"<p>I am currently studying how CNNs can be used in text classification and found some code on stack overflow that had worked with the use of a keras embedding layer.</p>
<p>I ran the code with the keras embedding but now want to test out what would happen with a pre-trained embedding, I have downloaded the word2vec api from gensim but dont know how to adapt the code from there?</p>
<p>My question is how can I replace the keras embedding layer with a pre-trained embedding like the word2vec model or Glove?</p>
<p>heres is the code</p>
<pre><code>from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, Convolution1D, Flatten, Dropout
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.callbacks import TensorBoard

# Using keras to load the dataset with the top_words
top_words = 10000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)

# Pad the sequence to the same length
max_review_length = 1600
X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)
X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)

# Using embedding from Keras
embedding_vecor_length = 300
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))

# Convolutional model (3x conv, flatten, 2x dense)
model.add(Convolution1D(64, 3, padding='same'))
model.add(Convolution1D(32, 3, padding='same'))
model.add(Convolution1D(16, 3, padding='same'))
model.add(Flatten())
model.add(Dropout(0.2))
model.add(Dense(180,activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(1,activation='sigmoid'))

# Log to tensorboard
tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=3, callbacks=[tensorBoardCallback], batch_size=64)

# Evaluation on the test set
scores = model.evaluate(X_test, y_test, verbose=0)
print(&quot;Accuracy: %.2f%%&quot; % (scores[1]*100))

</code></pre>
","tensorflow, keras, deep-learning, word-embedding","<p>This reads the text file containing the weights, stores the words and their weights in a dictionary, then maps them into a new matrix using the vocabulary of your fit tokenizer.</p>
<pre><code>from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, Convolution1D, Flatten, Dropout
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.callbacks import TensorBoard
from tensorflow import keras
import itertools
import numpy as np


# Using keras to load the dataset with the top_words
top_words = 10000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)

word_index = keras.datasets.imdb.get_word_index()

embedding_vecor_length = 300  # same as the embeds to be loaded below
embeddings_dictionary = dict()
glove_file = open('./embeds/glove.6B.300d.txt', 'rb')

for line in glove_file:
    records = line.split()  # seperates each line by a white space
    word = records[0]  # the first element is the word
    vector_dimensions = np.asarray(
        records[1:], dtype='float32')  # the rest are the weights
    # storing in dictionary
    embeddings_dictionary[word] = vector_dimensions
    
glove_file.close()

# len_of_vocab = len(word_index)
embeddings_matrix = np.zeros((top_words, embedding_vecor_length))
# mapping to a new matrix, using only the words in your tokenizer's vocabulary
for word, index in word_index.items():
    if index&gt;=top_words:
        continue
    # the weights of the individual words in your vocabulary
    embedding_vector = embeddings_dictionary.get(bytes(word, 'utf-8'))
    if embedding_vector is not None:
        embeddings_matrix[index] = embedding_vector

# Pad the sequence to the same length
max_review_length = 1600
X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)
X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)


# Using embedding from Keras
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length,
          input_length=max_review_length, name=&quot;embeddinglayer&quot;, weights=[embeddings_matrix], trainable=True))


# Convolutional model (3x conv, flatten, 2x dense)
model.add(Convolution1D(64, 3, padding='same'))
model.add(Convolution1D(32, 3, padding='same'))
model.add(Convolution1D(16, 3, padding='same'))
model.add(Flatten())
model.add(Dropout(0.2))
model.add(Dense(180, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

# Log to tensorboard
tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)
model.compile(loss='binary_crossentropy',
              optimizer='adam', metrics=['accuracy'])


model.fit(X_train, y_train, epochs=3, callbacks=[
          tensorBoardCallback], batch_size=64)


# Evaluation on the test set
scores = model.evaluate(X_test, y_test, verbose=0)
print(&quot;Accuracy: %.2f%%&quot; % (scores[1]*100))

</code></pre>
",1,2,775,2022-04-13 20:00:56,https://stackoverflow.com/questions/71863257/how-to-replace-keras-embedding-with-pre-trained-word-embedding-to-cnn
word2vec/gensim — RuntimeError: you must first build vocabulary before training the model,"<p>I am having trouble training my own <code>word2vec</code> model on the <code>.txt</code> files.</p>
<p>The code:</p>
<pre><code>import gensim
import json
import pandas as pd
import glob
import gensim.downloader as api
import matplotlib.pyplot as plt
from gensim.models import KeyedVectors


# loading the .txt files

sentences = []
sentence = []
for doc in glob.glob('./data/*.txt'): 
     with(open(doc, 'r')) as f:
        for line in f:
            line = line.rstrip()
            if line == &quot;&quot;:
                if len(sentence) &gt; 0:
                    sentences.append(sentence)
                    sentence = []
            else:
                cols = line.split(&quot;\t&quot;)
                if len(cols) &gt; 4:
                    form = cols[1]
                    lemma = cols[2]
                    pos = cols[3]
                    if pos != &quot;PONCT&quot;:
                        sentence.append(form.lower())


# trying to train the model

from gensim.models import Word2Vec
model_hugo = Word2Vec(sentences, vector_size=200, window=5, epochs=10, sg=1, workers=4)
</code></pre>
<p>Message error:</p>
<pre><code>RuntimeError: you must first build vocabulary before training the model
</code></pre>
<p>How do I build the vocabulary?</p>
<p>The code works with the sample <code>.conll</code> files, but I want to train the model on my own data.</p>
","python, gensim, word2vec, word-embedding","<p>Thanks to the @gojomo's suggestion and to <a href=""https://stackoverflow.com/a/55091252/9737944"">this answer</a>, I resolved the empty <code>sentences</code> issue. I needed the following block of code:</p>
<pre><code># make an iterator that reads your file one line at a time instead of reading everything in memory at once
# reads all the sentences

class SentenceIterator: 
    def __init__(self, filepath): 
        self.filepath = filepath 

    def __iter__(self): 
        for line in open(self.filepath): 
            yield line.split() 

</code></pre>
<p>before training the model:</p>
<pre><code># training the model

sentences = SentenceIterator('/content/drive/MyDrive/rousseau/rousseau_corpus.txt') 
model = gensim.models.Word2Vec(sentences, min_count=2) # min_count is for pruning 
                                                       # the internal dictionary. 
                                                       # Words that appear only once 
                                                       # in the corpus are probably 
                                                       # uninteresting typos and garbage. 
                                                       # In addition, there’s not enough 
                                                       # data to make any meaningful 
                                                       # training on those words, so it’s
                                                       # best to ignore them
</code></pre>
",1,0,1323,2022-04-13 21:11:20,https://stackoverflow.com/questions/71863897/word2vec-gensim-runtimeerror-you-must-first-build-vocabulary-before-training
pytorch loading pretrained weights from json file,"<p>I downloaded a json file of word-embeddings. The file contents look like this:</p>
<pre><code>{
&quot;in&quot;:[0.052956,0.065460,0.066195,0.047072,0.052221,-0.082009,-0.061415,-0.116210,0.015629,0.099293,-0.085686,-0.028133,0.052221,0.058840,-0.077596,-0.073550,0.033282,0.077228,-0.045785,-0.027214,-0.034201,0.035672,-0.090835,-0.048175,0.001701,0.027949,-0.002195,0.088628,0.046521,0.048175,0.061047,-0.051853,-0.016089,0.041556,-0.064357,0.051853,-0.096351,-0.025007,0.074286,0.132391,0.083480,-0.026110,-0.035488,-0.006390,0.027030,0.077596,0.020318,-0.021605,-0.003861,0.080170,0.045050,0.070976,0.025375,-0.020410,-0.070976,0.000776,-0.036407,0.025926,0.061047,-0.085318,-0.066931,0.027030,-0.109590,-0.183876,-0.046337,0.039901,0.042843,0.135333,0.045969,0.065460,0.093409,-0.030340,0.017009,0.133862,-0.022341,-0.022341,0.088260,0.023444,-0.072447,0.050014,0.003540,-0.060311,0.047440,-0.015538,-0.041188,-0.102235,-0.047808,0.062886,-0.048175,0.016181,0.058105,-0.027949,-0.025375,-0.138275,-0.054795,0.011952,0.070241,-0.046337,-0.010711,-0.002597,0.008366,-0.119152,-0.012871,0.004666,-0.006574,-0.060679,-0.011492,-0.066195,0.002620,-0.012136,-0.009286,0.073550,-0.105177,-0.064724,-0.020226,0.040637,0.100028,0.084951,0.091202,0.064357,-0.005355,0.033649,-0.109590,-0.002413,-0.088628,-0.049279,0.053692,-0.070976,-0.022801,0.090467,0.060311,-0.071344,-0.122094,-0.058473,0.015997,-0.061415,0.002965,-0.118416,-0.073918,0.029972,0.029604,-0.006849,0.077596,0.051117,-0.032178,0.047808,-0.036959,0.015721,-0.125771,0.070241,0.070608,0.005172,0.040453,0.039533,-0.018388,-0.024455,-0.046337,-0.004183,0.072447,0.028501,0.009194,-0.033098,-0.005631,0.079434,0.015354,0.109590,0.061782,0.004344,0.003448,-0.069873,-0.104441,-0.043211,-0.038798,-0.098557,-0.105177,-0.015446,-0.020410,0.024639,0.079067,-0.001758,-0.017009,0.000379,-0.083480,0.063989,-0.097822,-0.013147,-0.000270,0.081273,0.066931,0.033649,0.018939,0.017928,0.061047,0.017836,-0.082744,0.004045,-0.013331,-0.025559,-0.024823,-0.123565,0.072079,-0.013791,0.003999,-0.025926,-0.033282,-0.050014,-0.013515,-0.022341,-0.005723,-0.038614,-0.040820,0.067299,-0.054059,0.011492,-0.062150,-0.023904,0.026846,-0.015997,-0.044682,-0.009837,0.035304,0.017376,0.015813,-0.059208,-0.006068,0.014710,-0.004183,0.031259,0.020962,0.010251,0.026110,-0.137539,0.090467,0.055898,-0.030891,-0.007493,0.032362,-0.005493,0.092673,0.043395,-0.040269,-0.024272,-0.006849,-0.035120,0.033098,-0.038246,0.051853,0.002252,-0.003149,-0.033282,0.055530,-0.009608,0.050750,0.004735,0.056634,-0.028501,0.003678,0.033649,-0.050750,0.007309,0.003563,0.015446,0.053692,0.128713,0.130920,0.041924,0.068770,-0.028133,0.037511,-0.029604,0.033282,0.047072,0.036591,-0.040085,0.036775,-0.098557,-0.021789,-0.027214,-0.045785,-0.043211,0.092673,-0.062150,-0.008964,0.094144,0.001023,0.048175,-0.080170,-0.108119,-0.031811,0.018112,-0.127242,-0.066931,-0.060679,0.048911,0.046153,-0.035672,-0.044314,-0.035856,0.010895,-0.047072],
&quot;for&quot;:[-0.008512,-0.034224,0.032284,0.045868,-0.013143,-0.046221,-0.000948,-0.052219,0.046574,0.062451,-0.122785,-0.028756,0.051513,-0.018700,0.013143,0.098792,0.104438,-0.024345,-0.070566,-0.086796,-0.057511,0.045162,-0.048338,0.053630,0.016407,0.024169,-0.130547,0.037576,0.010012,0.067038,0.002536,-0.006571,-0.070213,0.049043,-0.006351,0.031931,-0.096675,-0.071977,0.023992,0.020200,0.112200,-0.012790,0.010320,-0.079387,-0.061745,-0.052924,-0.017818,0.124902,0.044633,0.064568,-0.017553,0.102321,-0.023816,0.019847,-0.112200,0.005689,-0.051160,0.031578,0.004344,-0.040399,-0.106555,0.020552,-0.095970,-0.127724,-0.065979,-0.036694,-0.018788,-0.107260,-0.058217,0.108672,-0.031402,0.057158,0.023992,0.065274,0.016407,-0.045162,0.118551,0.062098,-0.008953,0.141838,-0.044986,0.016230,-0.021787,0.015348,0.002404,-0.040046,-0.052924,0.021523,0.035989,0.012614,0.075506,0.028050,0.061392,-0.179238,0.050102,-0.107966,0.042163,0.069155,-0.024169,0.045515,0.015436,-0.105143,0.038811,-0.065626,-0.018347,0.032813,0.003837,-0.083621,-0.014113,0.087502,0.023287,0.068449,-0.046574,0.016407,0.087149,0.043574,0.087149,0.035283,0.067391,0.048338,0.021170,-0.024698,-0.080445,0.038635,-0.018524,0.012878,0.044986,-0.018700,0.105143,0.045162,0.077975,-0.117845,-0.070566,-0.076564,-0.061745,-0.064215,0.073036,-0.057511,0.006086,0.017377,0.094558,0.037047,0.058923,0.067743,-0.042340,-0.069860,-0.020464,-0.105143,-0.106555,0.105143,-0.012702,0.023816,-0.061745,-0.007939,-0.026815,-0.009879,0.025933,-0.005954,0.036341,-0.068449,0.034577,0.014995,0.022140,0.093853,0.038106,0.013584,-0.012702,0.025227,0.013231,-0.007145,-0.133370,-0.064921,-0.020993,-0.043927,-0.037047,-0.001709,0.047985,-0.059628,-0.028932,0.069507,-0.111494,-0.110789,0.020464,0.009482,0.021611,-0.008777,-0.069860,0.017906,0.139721,0.009394,0.017465,-0.025933,0.071272,-0.069860,-0.144660,-0.009967,0.062098,-0.057864,-0.127724,-0.126313,0.003705,-0.025227,-0.039517,0.067743,-0.067391,-0.008644,-0.000408,0.070566,0.017906,-0.028756,0.007057,0.085385,0.018612,0.088913,0.046574,0.051160,0.021170,-0.035812,-0.056453,0.020905,0.032990,-0.031049,0.018700,-0.037400,0.101615,0.003087,-0.027344,0.019847,0.043398,0.020464,0.020288,-0.026462,0.094558,-0.000070,-0.050102,-0.015966,0.049043,-0.016848,-0.011070,-0.042163,0.044104,0.000466,0.002889,-0.051513,0.066332,0.018965,0.014466,0.025580,-0.041810,-0.021434,0.019758,0.018171,0.043574,0.095264,-0.003153,0.001974,0.043222,0.071272,-0.066332,-0.033166,-0.012614,0.027697,-0.013849,0.033519,0.034577,0.070919,-0.029108,0.068096,-0.025051,-0.030520,0.050807,-0.009879,0.076917,0.011908,0.095264,-0.001224,-0.006130,-0.103026,-0.033695,-0.079387,0.059275,-0.029638,-0.013672,0.063509,-0.002029,0.172181,-0.034048,-0.016583,0.029461,0.021170,-0.016318,0.002690,-0.059628,0.058923,0.005733,0.000345,0.013319,0.051513,-0.025227,0.017465],
&quot;that&quot;:[-0.012361,-0.022230,0.065540,0.039477,-0.086620,0.024913,-0.011163,-0.070522,0.092369,0.092752,-0.056341,-0.060557,-0.054042,0.060557,-0.108850,0.005102,0.008624,-0.011881,-0.000755,-0.023763,-0.000124,0.030087,-0.018972,-0.036028,0.074355,-0.043310,-0.050975,0.004791,0.000671,0.048676,-0.042735,0.011067,0.017439,-0.035261,0.087386,-0.030279,0.040244,0.019739,0.013319,0.049442,0.108083,0.106550,0.051359,-0.050592,-0.018876,-0.010492,-0.029129,0.003378,-0.012361,0.014948,0.085087,0.035070,-0.035261,-0.074738,0.068223,0.064390,0.005366,-0.103484,0.002144,-0.059407,0.017631,0.134912,-0.038136,0.030087,-0.069373,-0.013510,0.017152,0.105017,0.008384,0.039094,0.029895,-0.004120,0.048101,-0.039286,-0.083170,0.043693,0.121115,0.134146,0.037752,0.099651,0.064007,-0.079721,0.034495,-0.010636,-0.105017,-0.123414,0.019068,0.164041,-0.080104,-0.073589,0.038136,0.059024,0.002767,-0.096968,-0.018972,-0.001036,0.030087,0.005965,0.013894,0.034303,-0.077038,-0.045610,0.011067,0.032195,-0.027787,-0.018014,-0.102717,-0.113449,0.022709,-0.096202,-0.055958,-0.005605,-0.075888,0.045993,0.081637,0.020697,0.005941,0.028362,0.031620,0.041394,-0.160208,-0.026254,-0.022805,0.024913,-0.096968,-0.052892,0.012456,-0.067839,0.009821,-0.049442,-0.094669,0.018397,-0.103484,-0.092752,-0.009534,-0.086237,0.074738,-0.032962,0.014373,0.040627,0.011738,-0.124947,-0.017056,-0.004024,0.028171,-0.002383,-0.061324,-0.040244,-0.005821,0.068606,-0.018780,0.034686,-0.089303,0.016864,-0.003006,-0.034111,-0.081637,-0.145644,-0.035261,0.035261,-0.034878,0.014948,-0.016481,0.010588,0.011977,-0.023859,0.036603,0.080487,-0.010875,0.006468,-0.041394,0.015427,-0.059791,-0.070522,0.034495,0.006228,0.009917,-0.085087,-0.014564,-0.082021,-0.119581,-0.062090,-0.022613,-0.014660,0.076271,-0.006564,-0.027787,0.005917,0.045610,0.064390,0.022613,0.040052,0.002491,-0.014564,0.011738,-0.057108,-0.026829,0.034495,-0.038327,-0.126480,0.020122,0.028746,-0.000121,-0.000988,-0.031237,-0.025296,-0.012361,0.047718,0.076271,-0.011786,-0.026446,-0.012025,0.003665,0.025871,-0.064390,0.083554,0.121115,0.006899,-0.094285,0.048101,0.045993,0.030470,-0.012552,-0.034495,0.094285,-0.059024,0.098118,0.027596,0.057108,0.068606,0.016577,-0.057874,0.027021,-0.073972,0.009103,-0.044843,-0.061707,0.012552,0.059407,0.023955,0.003617,-0.114216,-0.019451,-0.084704,0.054042,0.045610,0.098118,-0.051359,0.004144,0.009294,0.054808,0.099651,0.051359,-0.013606,0.093519,-0.025488,0.113449,0.060174,-0.025296,-0.051742,0.049442,-0.049059,-0.075505,0.083554,-0.031237,0.091219,-0.007618,-0.027787,-0.051359,0.046184,0.127247,0.040244,0.124947,0.074738,0.059791,-0.072055,0.019739,-0.061707,0.070139,-0.045993,-0.031428,0.036028,0.024338,0.030662,0.027979,-0.083170,-0.029129,-0.126480,0.016768,0.000958,-0.008863,-0.012265,-0.026254,-0.016193,-0.015235,0.050209,0.015810,0.005390,0.047909,-0.116515],
...
</code></pre>
<p>I found this function to load pre-trained embeddings into pytorch:</p>
<pre><code>self.embeds = torch.nn.Embedding.from_pretrained(weights)
</code></pre>
<p>My question is, how to load the <code>.json</code> file into the above function? I don't find the documentation helpful. From the docs:</p>
<pre><code>CLASSMETHOD from_pretrained(
    embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, 
    scale_grad_by_freq=False, sparse=False
)

embeddings (Tensor) – FloatTensor containing weights for the Embedding. 
First dimension is being passed to Embedding as num_embeddings, second as embedding_dim.
</code></pre>
<p>How do I convert this json file to a &quot;FloatTensor&quot; in the proper format for this function?</p>
<p>Thanks!</p>
","pytorch, word-embedding","<pre><code>weights = torch.stack([torch.Tensor(value) for _, value in in_json.items()], dim=0)
</code></pre>
",0,0,250,2022-04-22 13:24:01,https://stackoverflow.com/questions/71969499/pytorch-loading-pretrained-weights-from-json-file
Predict numeric variable from a text variable using word embeddings in R,"<p>I have a text variable with reviews of movies and another variables with ratings – I want to try to use the text reviews to predict the ratings.</p>
<p>Here are some example data:</p>
<pre><code>movie_reviews &lt;- c(&quot;I really loved the movie plot&quot;, &quot;This movie really sucked&quot;, &quot;I really found this movie thought provoking&quot;, &quot;ahh what a boring movie&quot;, &quot;A wonderful movie, with a wonderful end&quot;, &quot;Great action movie: Very thrilling&quot;, &quot;Worst movie ever, it never stopped being cheesy&quot;, &quot;Enjoying, feelgood movie for the entire family&quot;, &quot;I will definitely watch this movie again&quot;)

movie_ratings &lt;- c(8, 2, 6, 3, 9, 8.5, 3.5, 9.5, 7.5)  
  
movie_df &lt;- tibble(movie_reviews, movie_ratings) 

</code></pre>
<p>Thank you.</p>
","r, nlp, word-embedding, r-text","<p>For this you can use the <code>text</code>-package</p>
<pre><code># Create word embedding representations of your text
help(textEmbed)
reviews_embeddings &lt;- textEmbed(movie_df, 
                                model = &quot;bert-base-uncased&quot;, # Select model you want from huggingface
                                layers = 11:12) # Select which layers you want to use

# Train the word embeddings to the numeric variable using ridge regression 
reviews_rating_model &lt;- textTrain(reviews_embeddings$movie_reviews, 
                                  movie_df$movie_ratings) 
# See the results
reviews_rating_model
</code></pre>
<p>Result</p>
<pre><code>$results

    Pearson's product-moment correlation

data:  predy_y$predictions and predy_y$y
t = 5.621, df = 7, p-value = 0.0003991
alternative hypothesis: true correlation is greater than 0
95 percent confidence interval:
 0.6785761 1.0000000
sample estimates:
      cor 
0.9047823 
</code></pre>
",1,1,197,2022-05-01 14:18:12,https://stackoverflow.com/questions/72077504/predict-numeric-variable-from-a-text-variable-using-word-embeddings-in-r
Can we deduce the relationship b/w a dimension of a word vector with the linguistic characteristic it represents?,"<p>Let's imagine we generated a 200 dimension word vector using any pre-trained model of the word ('hello') as shown in the below image.</p>
<p><a href=""https://i.sstatic.net/5TfJp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5TfJp.png"" alt=""Word_Vector"" /></a></p>
<p>So, by any means can we tell which linguistic feature is represented by each d_i of this vector?</p>
<p>For example, d1 might be looking at whether the word is a noun; d2 might tell whether the word is a named entity or not and so on.</p>
","nlp, stanford-nlp, word2vec, word-embedding","<p>Because these word vectors are dense distributional representations, it is often difficult / impossible to interpret individual neurons, and such models often do not localize interpretable features to a single neuron (though this is an active area of research). For example, see <a href=""https://arxiv.org/abs/2010.02695"" rel=""nofollow noreferrer"">Analyzing Individual Neurons in Pre-trained Language Models
</a> for a discussion of this with respect to pre-trained language models).</p>
<p>A common method for studying how individual dimensions contribute to a particular phenomenon / task of interest is to train a linear model (i.e., logistic regression if the task is classification) to perform the task from fixed vectors, and then analyze the weights of the trained linear model.</p>
<p>For example, if you're interested in part of speech, you can train a linear model to map from the word vector to the POS [1]. Then, the weights of the linear model represent a linear combination of the dimensions that are predictive of the feature. For example, if the weight on the 5th neuron has large magnitude (very positive or very negative), you might expect that neuron to be somewhat correlated with the phenomenon of interest.</p>
<p>[1]: Note that defining a POS for a particular word is nontrivial, since the POS often depends on context. For example, &quot;play&quot; can be a noun (&quot;he saw a play&quot;) or a verb (&quot;I will play in the grass&quot;).</p>
",2,0,54,2022-05-03 05:25:50,https://stackoverflow.com/questions/72095099/can-we-deduce-the-relationship-b-w-a-dimension-of-a-word-vector-with-the-linguis
fasttext: why do aligned vectors contain only one value per word?,"<p>I was taking a look at the <a href=""https://fasttext.cc/docs/en/aligned-vectors.html"" rel=""nofollow noreferrer"">Fasttext aligned vectors</a> of some languages and was surprised to find that each vectors consisted of one value only. I was expecting a Matrix witch multidimensional vectors belonging to each word, instead there is only one column of numbers. I'm very new to this field and was wondering if somebody could explain to me, how this single number belongig to each word came to be and wether I'm looking at a semantic space as I was expecting or something different (if so what is it and are alingend multidimensional semantic spaces available somewhere?)</p>
","nlp, word-embedding, fasttext","<p>I think you may be misinterpreting those files.</p>
<p>When I look at one of those files – for example <a href=""https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.en.align.vec"" rel=""nofollow noreferrer""><code>wiki.en.align.vec</code></a> – each line is a word-token, then 300 different values (to provide a 300-dimensional word-vector).</p>
<p>For example, the 4th line of the file is:</p>
<pre><code>the -0.0324 -0.0462 -0.0087 0.0994 0.0147 -0.0198 -0.0811 -0.0362 0.0445 0.0402 -0.0199 -0.1173 0.0906 -0.0304 -0.0320 -0.0374 -0.0249 -0.0099 0.0017 0.0719 -0.0834 0.0382 -0.1141 -0.0288 -0.0666 -0.0365 -0.0006 0.0098 0.0282 0.0310 -0.0773 0.0755 -0.0528 0.1225 -0.0138 -0.0879 0.0036 -0.0593 0.0416 -0.0588 0.0266 -0.0011 -0.0419 0.0141 0.0388 -0.0597 -0.0203 0.0444 0.0253 -0.0316 0.0352 -0.0318 -0.0473 0.0347 -0.0250 0.0289 0.0426 0.0218 -0.0254 0.0486 -0.0252 -0.0904 0.1607 -0.0379 0.0231 -0.0988 -0.1213 -0.0926 -0.1116 0.0345 -0.1856 -0.0409 0.0306 -0.0653 -0.0377 -0.0301 0.0361 0.1212 0.0105 -0.0354 0.0552 0.0363 -0.0427 0.0555 -0.0031 -0.0830 -0.0325 0.0415 -0.0461 -0.0615 -0.0412 0.0060 0.1680 -0.1347 0.0271 -0.0438 0.0364 0.0121 0.0018 -0.0138 -0.0625 -0.0161 -0.0009 -0.0373 -0.1009 -0.0583 0.0038 0.0109 -0.0068 0.0319 -0.0043 -0.0412 -0.0506 -0.0674 0.0426 -0.0031 0.0788 0.0924 0.0559 0.0449 0.1364 0.1132 -0.0378 0.1060 0.0130 0.0349 0.0638 0.1020 0.0459 0.0634 -0.0870 0.0447 -0.0124 0.0167 -0.0603 0.0297 -0.0298 0.0691 -0.0280 0.0749 0.0474 0.0275 0.0255 0.0184 0.0085 0.1116 0.0233 0.0176 0.0327 0.0471 0.0662 -0.0353 -0.0387 -0.0336 -0.0354 -0.0348 0.0157 -0.0294 0.0710 0.0299 -0.0602 0.0732 -0.0344 0.0419 0.0773 0.0119 -0.0550 0.0377 0.0808 -0.0424 -0.0977 -0.0386 -0.0334 -0.0384 -0.0520 0.0641 0.0049 0.1226 -0.0011 -0.0131 0.0224 0.0138 -0.0243 0.0544 -0.0164 0.1194 0.0916 -0.0755 0.0565 0.0235 -0.0009 -0.0818 0.0953 0.0873 -0.0215 0.0240 -0.0271 0.0134 -0.0870 0.0597 -0.0073 -0.0230 -0.0220 0.0562 -0.0069 -0.0796 -0.0118 0.0059 0.0221 0.0509 0.1175 0.0508 -0.0044 -0.0265 0.0328 -0.0525 0.0493 -0.1309 -0.0674 0.0148 -0.0024 -0.0163 -0.0241 0.0726 -0.0165 0.0368 -0.0914 0.0197 0.0018 -0.0149 0.0654 0.0912 -0.0638 -0.0135 -0.0277 -0.0078 0.0092 -0.0477 0.0054 -0.0153 -0.0411 -0.0177 0.0874 0.0221 0.1040 0.1004 0.0595 -0.0610 0.0650 -0.0235 0.0257 0.1208 0.0129 -0.0086 -0.0846 0.1102 -0.0338 -0.0553 0.0166 -0.0602 0.0128 0.0792 -0.0181 0.0046 -0.0548 -0.0394 -0.0546 0.0425 0.0048 -0.1172 -0.0925 -0.0357 -0.0123 0.0371 -0.0142 0.0157 0.0442 0.1186 0.0834 -0.0293 0.0313 -0.0287 0.0095 0.0080 0.0566 -0.0370 0.0257 0.1032 -0.0431 0.0544 0.0323 -0.1076 -0.0187 0.0407 -0.0198 -0.0255 -0.0505 0.0827 -0.0650 0.0176
</code></pre>
<p>Thus every one of the 2,519,370 word-tokens has a 300-dimensional vector.</p>
<p>If this isn't what you're seeing, you should explain further. If this is what you're seeing and you were expecting something else, you should explain further what you were expecting.</p>
",1,0,272,2022-05-05 09:03:27,https://stackoverflow.com/questions/72124590/fasttext-why-do-aligned-vectors-contain-only-one-value-per-word
&quot;`select()` doesn&#39;t handle lists&quot; when computing textSimilarity between two word embeddings in R,"<p>How many words in word embedding variables do you need to compute semantic similarity in r-package <em>text</em>? I’m trying to run:</p>
<pre><code>library(text)
WEhello&lt;-textEmbed(&quot;hello&quot;)
WEgoodbye&lt;-textEmbed(&quot;goodbye&quot;)
textSimilarity(WEhello, WEgoodbye)
</code></pre>
<p>But I get this error:</p>
<pre><code>Error in `dplyr::select()`:
! `select()` doesn't handle lists.
</code></pre>
","r, nlp, word-embedding, r-text","<p>To get this to work you have to select the word embedding (and avoid also including the $singlewords_we). Try this:</p>
<pre><code>textSimilarity(WEhello$x, WEgoodbye$x)
</code></pre>
",1,1,315,2022-05-05 14:20:03,https://stackoverflow.com/questions/72128768/select-doesnt-handle-lists-when-computing-textsimilarity-between-two-word
Why in Keras embedding layer&#39;s matrix is a size of vocab_size + 1?,"<p>I have below toy example where my vocabulary size is 7, embedding size is 8 BUT weights output of Keras Embedding layer is 8x8. (?) How is that? This seems to be connected to other questions related to Keras embedding layer being &quot;maximum integer index + 1&quot; and I've read all the other stackoverflow queries on this, but all of them suggest it's not vocab_size + 1 while my code tells me it is.
I'm asking this as I'd need to know which exactly embeding vector relates to which word.</p>
<pre><code>docs = ['Well done!',
            'Good work',
            'Great effort',
            'nice work']
labels = np.array([1,1,1,1])
tokenizer = Tokenizer()
tokenizer.fit_on_texts(docs)
encoded_docs = tokenizer.texts_to_sequences(docs)
max_seq_len = max(len(x) for x in encoded_docs) # max len is 2
padded_seq = pad_sequences(sequences=encoded_docs,maxlen=max_seq_len,padding='post')
embedding_size = 8
tokenizer.index_word
</code></pre>
<p>{1: 'work',
2: 'well',
3: 'done',
4: 'good',
5: 'great',
6: 'effort',
7: 'nice'}</p>
<pre><code>    len(tokenizer.index_word) # 7
    vocab_size = len(tokenizer.index_word)+1 
    model = Sequential()
    model.add(Embedding(input_dim=vocab_size,output_dim=embedding_size,input_length=max_seq_len, name='embedding_lay'))
    model.add(Flatten())
    model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])
model.summary()

Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_lay (Embedding)    (None, 2, 8)              64        
_________________________________________________________________
flatten_1 (Flatten)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 81
Trainable params: 81
Non-trainable params: 0

model.fit(padded_seq,labels, verbose=1,epochs=20)
model.get_layer('embedding_lay').get_weights()

[array([[-0.0389936 , -0.0294274 ,  0.02361362,  0.01885288, -0.01246006,
         -0.01004354,  0.01321061, -0.02298149],
        [-0.01264734, -0.02058442,  0.0114141 , -0.02725944, -0.06267354,
          0.05148344, -0.02335678, -0.06039589],
        [ 0.0582506 ,  0.00020944, -0.04691287,  0.02985037,  0.02437406,
         -0.02782   ,  0.00378997,  0.01849808],
        [-0.01667434, -0.00078654, -0.04029636, -0.04981862,  0.01762467,
          0.06667487,  0.00302309,  0.02881355],
        [ 0.04509508, -0.01994639,  0.01837089, -0.00047283,  0.01141069,
         -0.06225454,  0.01198813,  0.02102971],
        [ 0.05014603,  0.04591557, -0.03119368,  0.04181939,  0.02837115,
         -0.01640332,  0.0577693 ,  0.01364574],
        [ 0.01948108, -0.04200416, -0.06589368, -0.05397511,  0.02729052,
          0.04164972, -0.03795817, -0.06763416],
        [ 0.01284658,  0.05563928, -0.026766  ,  0.03231764, -0.0441488 ,
         -0.02879154,  0.02092744,  0.01947528]], dtype=float32)]
</code></pre>
<p>So how do I get my 7 words vectors for instance for {1: 'work'...} from 8th vectors (rows) matrix and what does that 8th vector mean ?
If I change vocab_size = len(tokenizer.index_word) - not adding (+1)
then when trying to fit the model I'm getting size errors etc.</p>
","python, tensorflow, keras, embedding, word-embedding","<p>The <code>Embedding</code> layer uses <code>tf.nn.embedding_lookup</code> under the <a href=""https://github.com/keras-team/keras/blob/v2.8.0/keras/layers/embeddings.py#L197"" rel=""nofollow noreferrer"">hood</a>, which is zero-based by default. For example:</p>
<pre><code>import tensorflow as tf
import numpy as np

docs = ['Well done!',
            'Good work',
            'Great effort',
            'nice work']
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(docs)
encoded_docs = tokenizer.texts_to_sequences(docs)
max_seq_len = max(len(x) for x in encoded_docs) # max len is 2
padded_seq = tf.keras.preprocessing.sequence.pad_sequences(sequences=encoded_docs,maxlen=max_seq_len,padding='post')
embedding_size = 8

tf.random.set_seed(111)

# Create integer embeddings for demonstration purposes.
embeddings = tf.cast(tf.random.uniform((7, embedding_size), minval=10,  maxval=20, dtype=tf.int32), dtype=tf.float32)

print(padded_seq)

tf.nn.embedding_lookup(embeddings, padded_seq)
</code></pre>
<pre><code>[[2 3]
 [4 1]
 [5 6]
 [7 1]]
&lt;tf.Tensor: shape=(4, 2, 8), dtype=float32, numpy=
array([[[17., 11., 10., 16., 17., 16., 16., 17.],
        [18., 15., 13., 13., 18., 18., 10., 16.]],

       [[17., 16., 13., 12., 13., 15., 19., 14.],
        [12., 15., 12., 15., 10., 19., 15., 12.]],

       [[18., 15., 11., 13., 13., 13., 16., 10.],
        [18., 18., 11., 12., 10., 13., 14., 10.]],

    --&gt; [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.] &lt;--,
        [12., 15., 12., 15., 10., 19., 15., 12.]]], dtype=float32)&gt;
</code></pre>
<p>Notice how the integer 7 is mapped to zero, because the <code>tf.nn.embedding_lookup</code> only knows how to map values from 0 to 6. That is the reason, you should use <code>vocab_size = len(tokenizer.index_word)+1</code>, since you want a meaningful vector for the integer 7:</p>
<pre><code>embeddings = tf.cast(tf.random.uniform((8, embedding_size), minval=10,  maxval=20, dtype=tf.int32), dtype=tf.float32)

tf.nn.embedding_lookup(embeddings, padded_seq)
</code></pre>
<p>The index 0 could then be reserved for unknown tokens, since your vocabulary starts from 1.</p>
",2,1,890,2022-05-16 17:37:50,https://stackoverflow.com/questions/72263400/why-in-keras-embedding-layers-matrix-is-a-size-of-vocab-size-1
How to create word embedding using Word2Vec on Python?,"<p>I have seen many tutorials online on how to use Word2Vec (gensim).</p>
<p>Most tutorials are showing on how to find the <code>.most_similar</code> word or similarity between two words.</p>
<p>But, how if I have text data <code>X</code> and I want to produce the word embedding vector <code>X_vector</code>?</p>
<p>So that, this <code>X_vector</code> can be used for classification algorithms?</p>
","python, gensim, word2vec, text-classification, word-embedding","<p>If <code>X</code> is a word (string token), you can look up its vector with <code>word_model[X]</code>.</p>
<p>If <code>X</code> is a text - say, a list-of-words – well, a <code>Word2Vec</code> model only has vectors for words, not texts.</p>
<p>If you have some desired way to use a list-of-words plus per-word-vectors to create a text-vector, you should apply that yourself. There are many potential approaches, some simple, some complicated, but no one 'official' or 'best' way.</p>
<p>One easy popular baseline (a fair starting point especially on very small texts like titles) is to average together all the word vectors. That can be as simple as (assuming <code>numpy</code> is imported as <code>np</code>):</p>
<pre><code>np.mean([word_model[word] for word in word_list], axis=0)
</code></pre>
<p>But, recent versions of Gensim also have a convenience <code>.get_mean_vector()</code> method for averaging together sets of vectors (specified as their word-keys, or raw vectors), with some other options:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector</a></p>
",2,0,1877,2022-05-23 09:42:48,https://stackoverflow.com/questions/72346412/how-to-create-word-embedding-using-word2vec-on-python
All words used to used to train Doc2Vec model appear as unknown,"<p>I'm trying to make an unsupervised text classifier using Doc2Vec. I am getting the error message of &quot;The following keywords from the 'keywords_list' are unknown to the Doc2Vec model and therefore not used to train the model: sport&quot;. The rest of the error message is super long but eventually ends with &quot;cannot compute similarity with no input&quot; which leads me to believe that all of my keywords are not accepted.</p>
<p>The portion of code where I create the Lbl2Vec model is</p>
<pre><code> Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels[&quot;keywords&quot;]), tagged_documents=df_pdfs['tagged_docs'], label_names=list(labels[&quot;class_name&quot;]), similarity_threshold=0.43, min_num_docs=10, epochs=10)
</code></pre>
<p>I've been following this tutorial <a href=""https://towardsdatascience.com/unsupervised-text-classification-with-lbl2vec-6c5e040354de"" rel=""nofollow noreferrer"">https://towardsdatascience.com/unsupervised-text-classification-with-lbl2vec-6c5e040354de</a> but using my own dataset that I load from JSON.</p>
<p>The entire code is posted below:</p>
<pre><code>#imports
from ast import keyword
import tkinter as tk
from tkinter import filedialog
import json

import pandas as pd

import numpy as np

from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import strip_tags
from gensim.models.doc2vec import TaggedDocument
from lbl2vec import Lbl2Vec


class DocumentVectorize:
    def __init__(self):
        pass
    
    #imports data from json file
    def import_from_json (self):
        root = tk.Tk()
        root.withdraw()
        json_file_path = filedialog.askopenfile().name
        with open(json_file_path, &quot;r&quot;) as json_file:
                try:
                    json_load = json.load(json_file)
                except:
                    raise ValueError(&quot;No PDFs to convert to JSON&quot;)
        self.pdfs = json_load


if __name__ == &quot;__main__&quot;:

#tokenizes documents
    def tokenize(doc):
        return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=1000000)

    #initializes document vectorization class and imports the data from json
    vectorizer = DocumentVectorize()
    vectorizer.import_from_json()

    #converts json data to dataframe
    df_pdfs = pd.DataFrame.from_dict(vectorizer.pdfs)
    
    #creates data frame that contains the keywords and their class names for the training
    labels =  {&quot;keywords&quot;: [[&quot;sport&quot;], [&quot;physics&quot;]], &quot;class_name&quot;: [&quot;rec.sport&quot;, &quot;rec.physics&quot;]}
    labels = pd.DataFrame.from_dict(labels)

    #applies tokenization to documents
    df_pdfs['tagged_docs'] = df_pdfs.apply(lambda row: TaggedDocument(tokenize(row['text_clean']), [str(row.name)]), axis=1)
    #creates key for documents
    df_pdfs['doc_key'] = df_pdfs.index.astype(str)
    print(df_pdfs.head())

    #Initializes Lbl2vec model
    Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels[&quot;keywords&quot;]), tagged_documents=df_pdfs['tagged_docs'], label_names=list(labels[&quot;class_name&quot;]), similarity_threshold=0.43, min_num_docs=10, epochs=10)

    #Fits Lbl2vec model to data
    Lbl2Vec_model.fit()
</code></pre>
","python, nlp, artificial-intelligence, word-embedding","<p>The problem that was occurring is that the default initialization of the Lbl2Vec model has the min_count for words = 50. This means that a word needs to occur 50 times for it to be included in the word vectorization set. This wasn't occurring for any words in any of my documents and thus all of my keywords would be rejected and that was causing the error message I was receiving. This is what the updated code for that line would look like:</p>
<pre><code> Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels[&quot;keywords&quot;]), tagged_documents=df_pdfs['tagged_docs'], label_names=list(labels[&quot;class_name&quot;]), min_count = 2, similarity_threshold=0.43, min_num_docs=10, epochs=10)

</code></pre>
",4,1,761,2022-05-31 20:29:40,https://stackoverflow.com/questions/72453796/all-words-used-to-used-to-train-doc2vec-model-appear-as-unknown
Cannot download GloVe embeddings. Have they been moved or is downloads.cs.stanford.edu down temporarily?,"<p>I am attempting to download glove.840B.300d.zip. I used the link at <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a> and also ran <code>wget https://nlp.stanford.edu/data/glove.840B.300d.zip</code>. The output from wget looks as follows:</p>
<pre><code>--2022-06-23 15:50:30--  (try: 2)  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip
Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... failed: Connection timed out.
Retrying.
</code></pre>
<p>Does anyone know if this is a temporary issue? Thank you!</p>
","nlp, stanford-nlp, word-embedding","<p>Just found that someone opened an issue for this: <a href=""https://github.com/stanfordnlp/GloVe/issues/206"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/issues/206</a></p>
<p>Downloading from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a> is not currently possible.</p>
<p>However, Huggingface has mirrors for all of the GloVe sets that can be downloaded. Links to these are provided by a comment made on the GitHub issue by joelsewhere on June 22nd.</p>
",1,2,630,2022-06-23 20:07:26,https://stackoverflow.com/questions/72736034/cannot-download-glove-embeddings-have-they-been-moved-or-is-downloads-cs-stanfo
resize_token_embeddings on the a pertrained model with different embedding size,"<p>I would like to ask about the way to change the embedding size of the trained model.</p>
<p>I have a trained model <code>models/BERT-pretrain-1-step-5000.pkl</code>.
Now I am adding a new token <code>[TRA]</code>to the tokeniser and try to use the <code>resize_token_embeddings</code> to the pertained one.</p>
<pre class=""lang-py prettyprint-override""><code>from pytorch_pretrained_bert_inset import BertModel #BertTokenizer 
from transformers import AutoTokenizer
from torch.nn.utils.rnn import pad_sequence
import tqdm

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model_bert = BertModel.from_pretrained('bert-base-uncased', state_dict=torch.load('models/BERT-pretrain-1-step-5000.pkl', map_location=torch.device('cpu')))

#print(tokenizer.all_special_tokens) #--&gt; ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
#print(tokenizer.all_special_ids)    #--&gt; [100, 102, 0, 101, 103]

num_added_toks = tokenizer.add_tokens(['[TRA]'], special_tokens=True)
model_bert.resize_token_embeddings(len(tokenizer))  # --&gt; Embedding(30523, 768)
print('[TRA] token id: ', tokenizer.convert_tokens_to_ids('[TRA]'))  # --&gt; 30522
</code></pre>
<p>But I encountered the error:</p>
<pre><code>AttributeError: 'BertModel' object has no attribute 'resize_token_embeddings'
</code></pre>
<p>I assume that it is because the <code>model_bert(BERT-pretrain-1-step-5000.pkl)</code> I had has the different embedding size.
I would like to know if there is any way to fit the embedding size of my modified tokeniser and the model I would like to use as the initial weights.</p>
<p>Thanks a lot!!</p>
","pytorch, huggingface-transformers, bert-language-model, word-embedding, huggingface-tokenizers","<p><a href=""https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings"" rel=""noreferrer"">resize_token_embeddings</a> is a huggingface transformer method. You are using the BERTModel class from <code>pytorch_pretrained_bert_inset</code> which does not provide such a method. Looking at the <a href=""https://github.com/dreasysnail/INSET/blob/2dee48ef6045c8987cef399ecbb3d443b69c1c5e/pytorch_pretrained_bert_inset/modeling.py"" rel=""noreferrer"">code</a>, it seems like they have copied the BERT code from huggingface some time ago.</p>
<p>You can either wait for an update from INSET (maybe create a github issue) or write your own code to extend the word_embedding layer:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn 

embedding_layer = model.embeddings.word_embeddings

old_num_tokens, old_embedding_dim = embedding_layer.weight.shape

num_new_tokens = 1

# Creating new embedding layer with more entries
new_embeddings = nn.Embedding(
        old_num_tokens + num_new_tokens, old_embedding_dim
)

# Setting device and type accordingly
new_embeddings.to(
    embedding_layer.weight.device,
    dtype=embedding_layer.weight.dtype,
)

# Copying the old entries
new_embeddings.weight.data[:old_num_tokens, :] = embedding_layer.weight.data[
    :old_num_tokens, :
]

model.embeddings.word_embeddings = new_embeddings
</code></pre>
",7,3,14230,2022-06-27 16:38:00,https://stackoverflow.com/questions/72775559/resize-token-embeddings-on-the-a-pertrained-model-with-different-embedding-size
Using a Word2Vec Model to Extract Data,"<p>I've used gensim Word2Vec to learn the embedding of monetary amounts and other numeric data in bank transaction memos. The goal is to use this to be able to extract these amounts and currencies from future input strings.</p>
<p><strong>Design</strong>
Our input strings are something like</p>
<pre><code>&quot;AMAZON.COM TXNw98e7r3347 USD 49.00 @ 1.283&quot;
</code></pre>
<p>During preprocessing, I tokenize and also replace all tokens that have the possibility of being a monetary amount (string consisting only of digits, commas, and &lt;= 1 decimal point/period) with a special VALUE_TOKEN. And I also manually replace exchange rates with RATE_TOKEN. The result would be</p>
<pre><code>[&quot;AMAZON&quot;, &quot;.COM&quot;, &quot;TXNw&quot;, &quot;98&quot;, &quot;e&quot;, &quot;7&quot;, &quot;r&quot;, &quot;3347&quot;, &quot;USD&quot;, &quot;VALUE_TOKEN&quot;, &quot;@&quot;, &quot;RATE_TOKEN&quot;]
</code></pre>
<p>With all my preprocessed lists of strings in list <code>data</code>, I generate model</p>
<pre><code>model = Word2Vec(data, window=3, min_count=3)
</code></pre>
<p>The embeddings of model that I'm most interested in are that of VALUE_TOKEN, RATE_TOKEN, as well as any currencies (USD, EUR, CAD, etc.). Now that I generated the model, I'm not sure what to do with it.</p>
<p><strong>Problem</strong>
Say I have a new string that the model has never seen before,</p>
<pre><code>new_string = &quot;EUR 299.99 RATE 1.3289 WITH FEE 5.00&quot;
</code></pre>
<p>I would like to use <code>model</code> to identify which tokens of <code>new_string</code> is most contextually similar to VALUE_TOKEN (which should return [&quot;299.99&quot;, &quot;5.00&quot;]), which is closest to RATE_TOKEN (&quot;1.3289&quot;). It should be able to classify these based on the learned embedding. I can preprocess <code>new_string</code> the way I do with the training data, but because I don't know the exchange rate before hand, all three tokens of [&quot;299.99&quot;, &quot;5.00&quot;, &quot;1.3289&quot;] will be tagged the same (either with VALUE_TOKEN or a new UNIDENTIFIED_TOKEN).</p>
<p>I've looked into methods like <code>most_similar</code> and <code>similarity</code> but don't think they work for tokens that are not necessarily in the vocabulary. What methods should I use to do this? Is this the right approach?</p>
","machine-learning, nlp, data-science, word2vec, word-embedding","<p>Word2vec's fuzzy, dense embedded token representations don't strike me as the right tool for what you're doing, though they might perhaps be an indirect contributor to a hybrid approach.</p>
<p>In particular:</p>
<ul>
<li>The word2vec algorithm originated from, &amp; has the most consistent public results, when applied to natural-language texts, with their particular patterns of relative token frequences, and varied co-occurrences. Certainly, many ahave applied it, with success, to other kinds of text/record data, but such uses may require a lot more preprocessing/parameter-tuning, and to the extent the underlying data has some fixed, highly-repetitive scheme, might be more amenable to other approaches.</li>
<li>If you replace all <em>known</em> values with <code>'VALUE_TOKEN'</code>, &amp; all known rates with <code>'RATE_TOKEN'</code>, then the model is only going to learn token-vectors for <code>'VALUE_TOKEN'</code> &amp; <code>'RATE_TOKEN'</code>. Such a model won't be able to supply any vector for non-replaced tokens it's never seen like <code>'$1.2345'</code> or <code>'299.99'</code>. Even collapsing all those to <code>'UNIDENTIFIED_TOKEN'</code> just limits the model to whatever it learned earlier was the vector for <code>'UNIDENTIFIED_TOKEN'</code> (if any, in the training data).</li>
<li>I've not noticed existing word2vec implementations offering an interface for inferring the word-vector for new unknown-vectors, from just one or several new examples of its appearance in-context. They <em>could</em>, in the same style of new-document-vector inference used by 'Paragraph Vectors'/<code>Doc2Vec</code>, but just don't.) The closest I've seen is <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.predict_output_word"" rel=""nofollow noreferrer"">Gensim's <code>predict_output_word()</code></a>, which does a CBOW-like forward-propagation on negative-sampling models, to every 'output node' (one per known word), to give a ranked list of the known-words most-likely to appear given some context words.</li>
</ul>
<p>That <code>predict_output_word()</code> <em>might</em>, if fed surrounding known-tokens, contribute to your needs by whether it says your <code>'VALUE_TOKEN'</code> or <code>'RATE_TOKEN'</code> is a more-likely model-prediction. You could adapt its code to <em>only</em> evaluate those two candidates, if you're always sure the right answer is one or the other, for a speed-up. A simple comparison of the average-of-context-word-vectors, and the candidate-answer vectors, <em>might</em> be as effective as the full forward-propagation.</p>
<p>Alternatively, you might want use the word2vec model solely as a source of features (via context-words) for some other classifier, which is trained to answer <code>VALUE</code> or <code>TOKEN</code>. This other classifier's input might include things like:</p>
<ul>
<li>some average of the vectors of all nearby tokens</li>
<li>the full vectors of closest neighbors</li>
<li>a one-hot encoding ('bag-of-words') of all nearby (or 'preceding') or 'following) known-tokens, assuming the vocabulary of non-numerical tokens is fairly short &amp; highly indicative</li>
<li>?</li>
</ul>
<p>If the data streams might include arbitrary new or corrupted tokens whose meaning might be inferrable from substrings, you could consider a <code>FastText</code> model as well.</p>
",1,0,262,2022-07-08 17:21:37,https://stackoverflow.com/questions/72914889/using-a-word2vec-model-to-extract-data
What is the meaning of size(embedding_model)?,"<p>I want to be sure I understand correctly:</p>
<p>Using the length of embedding model means number of different tokens it contains?</p>
<p>i.e:</p>
<pre><code>from gensim import downloader
embedding_model = downloader.load('glove-wiki-gigaword-50')
print(len(embedding_model))
</code></pre>
<p>output:</p>
<pre><code>400000 
</code></pre>
<p>means: <code>glove-wiki-gigaword-50</code> has 400000 different tokens (words) and each token (word) has the size of 50 bytes ?</p>
","gensim, word2vec, word-embedding","<p>Yes, <code>len(model)</code> in this case gives you the count of words inside it.</p>
<p><code>model.vector_size</code> will give you the number of dimensions (not bytes) per vector. (The actual size of the vector in bytes will be 4 times the count of dimensions, as each <code>float32</code>-sized value takes 4 bytes.)</p>
<p>I generally recommend against ever using the Gensim <code>api.downloader</code> functionality: if you instead find &amp; manually download from the original source of the files, you'll better understand their contents, formats, &amp; limitations – and where the file has landed in your local filesystem. And, by then using a specific class/method to load the file, you'll better understand what kinds of classes/objects you're using, rather than whatever mystery-object <code>downloader.load()</code> might have given you.</p>
",1,0,62,2022-07-09 02:59:25,https://stackoverflow.com/questions/72918624/what-is-the-meaning-of-sizeembedding-model
Use word2vec in tokenized sentences,"<p>I am trying to create a emotion recognition model resorting to SVM. I have a big dataset of sentences each one with a labeled emotion. After text pre-processing, I have a pandas data frame containing the tokenized sentences, like it can be seen in [1.] <a href=""https://i.sstatic.net/tbRqB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tbRqB.png"" alt=""Dataframe adter pre-processing"" /></a>.</p>
<p>My objective is to turn all this tokenized sentences to word embeddings so that I can train models such as SVM. The problem is how to use this date frame as input to word2vec or any other word embedding model.</p>
","python, machine-learning, nlp, word-embedding","<p>You need one vector per input instance if you want to use SVM. This means that you need to get embeddings for the words and do some operation, typically pooling, that will shrink the sequence of word embeddings into a single vector.</p>
<p>The most frequently used methods are mean-pooling and max-pooling, simply taking the average or the maximum of the embeddings.</p>
<p>Assuming, you pandas data frames in variable <code>data</code> and you have the word embeddings in a dictionary <code>embedding_table</code> with string keys and NumPy array value, you can do something like this (mean pooling), assuming that at least one word is covered by the word embeddings:</p>
<pre class=""lang-py prettyprint-override""><code>def embed(word_sequence):
    embeddings = []
    for word in word_sequence:
        if word in embedding_table:
            embeddings.append(word)
    return np.mean(embeddings, axis=0)

data[&quot;vector&quot;] = data.Utterance.map(embed)
</code></pre>
",0,1,1409,2022-07-17 18:58:51,https://stackoverflow.com/questions/73014558/use-word2vec-in-tokenized-sentences
Prediction with keras embedding leads to indices not in list,"<p>I have a model that I trained with</p>
<pre><code>common_embed = Embedding(
    name=&quot;synopsis_embedd&quot;,
    input_dim =len(t.word_index)+1,
    output_dim=len(embeddings_index['no']),
    weights=[embedding_matrix],
    input_length=len(X_train['asset_text_seq_pad'].tolist()[0]),
    trainable=True
)

lstm_1 = common_embed(input_1)
common_lstm = LSTM(64, input_shape=(100,2))
...
</code></pre>
<p>For the embedding I use Glove as a pre-trained embedding dictionary. Where I first build the tokenizer and text sequence with:
t = Tokenizer()
t.fit_on_texts(all_text)</p>
<pre><code>text_seq= pad_sequences(t.texts_to_sequences(data['example_texts'].astype(str).values))
</code></pre>
<p>and then I'm calculating the embedding matrix with:</p>
<pre><code>embeddings_index = {}
for line in new_byte_string.decode('utf-8').split('\n'):
  if line:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs


embedding_vector = None
not_present_list = []
vocab_size = len(t.word_index) + 1
print('Loaded %s word vectors.' % len(embeddings_index))
embedding_matrix = np.zeros((vocab_size, len(embeddings_index['no'])))
for word, i in t.word_index.items():
    if word in embeddings_index.keys():
        embedding_vector = embeddings_index.get(word)
    else:
        not_present_list.append(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
    else:
        embedding_matrix[i] = np.zeros(300)
</code></pre>
<p>now I'm using a new dataset for the prediction. This leads to an error:</p>
<blockquote>
<p>Node: 'model/synopsis_embedd/embedding_lookup'
indices[38666,63] = 136482 is not in [0, 129872)
[[{{node model/synopsis_embedd/embedding_lookup}}]] [Op:__inference_predict_function_12452]</p>
</blockquote>
<p>I do all of the steps for the prediction step again. Is that wrong and do I have to reuse the tokenizer from the training? Or why are the indices during prediction not existing?</p>
","python, tensorflow, keras, word-embedding","<p>You are probably getting this error because you are not using the same <code>tokenizer</code> and <code>embedding_matrix</code> during inference. Here is an example:</p>
<pre><code>import tensorflow as tf

vocab_size = 50
embedding_layer = tf.keras.layers.Embedding(vocab_size, 64, input_length=10)

sequence1 = tf.constant([[1, 2, 5, 10, 32]])
embedding_layer(sequence1) # This works

sequence2 = tf.constant([[51, 2, 5, 10, 32]])
embedding_layer(sequence2) # This throws an error because 51 is larger than the vocab_size=50
</code></pre>
",1,1,485,2022-07-19 07:38:08,https://stackoverflow.com/questions/73032904/prediction-with-keras-embedding-leads-to-indices-not-in-list
How to find closest embedding vectors?,"<p>I have 100K known embedding i.e.</p>
<pre><code>[emb_1, emb_2, ..., emb_100000]
</code></pre>
<p>Each of this embedding is derived from GPT-3 sentence embedding with dimension 2048.</p>
<p>My task is given an embedding(<code>embedding_new</code>) find the closest 10 embedding from the above <code>100k</code> embedding.</p>
<p>The way I am approaching this problem is brute force.</p>
<p>Every time a query asks to find the closest embeddings, I compare <code>embedding_new</code> with <code>[emb_1, emb_2, ..., emb_100000]</code> and get the similarity score.</p>
<p>Then I do quicksort of the similarity score to get the top <code>10</code> closest embedding.</p>
<p>Alternatively, I have also thought about using Faiss.</p>
<p>Is there a better way to achieve this?</p>
","deep-learning, embedding, word-embedding, vector-database","<p>I found a solution using Vector Database Lite (VDBLITE)</p>
<p>VDBLITE here: <a href=""https://pypi.org/project/vdblite/"" rel=""nofollow noreferrer"">https://pypi.org/project/vdblite/</a></p>
<pre><code>import vdblite
from time import time
from uuid import uuid4
import sys
from pprint import pprint as pp


if __name__ == '__main__':
    vdb = vdblite.Vdb()
    dimension = 12    # dimensions of each vector                         
    n = 200    # number of vectors                   
    np.random.seed(1)             
    db_vectors = np.random.random((n, dimension)).astype('float32')
    print(db_vectors[0])
    for vector in db_vectors:
        info = {'vector': vector, 'time': time(), 'uuid': str(uuid4())}
        vdb.add(info)
    vdb.details()
    results = vdb.search(db_vectors[10])
    pp(results)
</code></pre>
<p>Looks like it uses FAISS behind the scene.</p>
",0,3,3366,2022-07-22 02:22:42,https://stackoverflow.com/questions/73074462/how-to-find-closest-embedding-vectors
Correct keras LSTM input shape after text-embedding,"<p>I'm trying to understand the keras LSTM layer a bit better in regards to timesteps, but am still struggling a bit.</p>
<p>I want to create a model that is able to compare 2 inputs (siamese network). So my input is twice a preprocessed text. The preprocessing is done as followed:</p>
<pre><code>max_len = 64
data['cleaned_text_1'] = assets.apply(lambda x: clean_string(data[]), axis=1)
data['text_1_seq'] = t.texts_to_sequences(cleaned_text_1.astype(str).values)
data['text_1_seq_pad'] = [list(x) for x in pad_sequences(assets['text_1_seq'], maxlen=max_len, padding='post')]
</code></pre>
<p>same is being done for the second text input. T is from <code>keras.preprocessing.text.Tokenizer</code>.</p>
<p>I defined the model with:</p>
<pre><code>common_embed = Embedding(
    name=&quot;synopsis_embedd&quot;,
    input_dim=len(t.word_index)+1,
    output_dim=300,
    input_length=len(data['text_1_seq_pad'].tolist()[0]),
    trainable=True
)

lstm_layer = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2)
)

input1 = tf.keras.Input(shape=(len(data['text_1_seq_pad'].tolist()[0]),))
e1 = common_embed(input1)
x1 = lstm_layer(e1)

input2 = tf.keras.Input(shape=(len(data['text_1_seq_pad'].tolist()[0]),))
e2 = common_embed(input2)
x2 = lstm_layer(e2)

merged = tf.keras.layers.Lambda(
    function=l1_distance, output_shape=l1_dist_output_shape, name='L1_distance'
)([x1, x2])

conc = Concatenate(axis=-1)([merged, x1, x2])

x = Dropout(0.01)(conc)
preds = tf.keras.layers.Dense(1, activation='sigmoid')(x)
model = tf.keras.Model(inputs=[input1, input2], outputs=preds)
</code></pre>
<p>that seems to work if I feed the numpy data with the fit method:</p>
<pre><code>model.fit(
    x = [np.array(data['text_1_seq_pad'].tolist()), np.array(data['text_2_seq_pad'].tolist())],
    y = y_train.values.reshape(-1,1), 
    epochs=epochs,
    batch_size=batch_size,
    validation_data=([np.array(val['text_1_seq_pad'].tolist()), np.array(val['text_2_seq_pad'].tolist())], y_val.values.reshape(-1,1)),
)
</code></pre>
<p>What I'm trying to understand at the moment is what is the shape in my case for the LSTM layer for:</p>
<ul>
<li>samples</li>
<li>time_steps</li>
<li>features</li>
</ul>
<p>Is it correct that the input_shape for the LSTM layer would be <code>input_shape=(300,1)</code> because I set the embedding output dim to 300 and I have only 1 input feature per LSTM?</p>
<p>And do I need to reshape the embedding output or can I just set</p>
<pre><code>lstm_layer = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(32, input_shape=(300,1), dropout=0.2, recurrent_dropout=0.2)
)
</code></pre>
<p>from the embedding output?</p>
<p>Example notebook can be found in <a href=""https://github.com/fsulser/lstm_example/blob/main/Untitled0.ipynb"" rel=""nofollow noreferrer"">Github</a> or as <a href=""https://colab.research.google.com/drive/1SzoQThQAbBY0xuMlTecYnhT7gusXGwSI?authuser=1"" rel=""nofollow noreferrer"">Colab</a></p>
","python, tensorflow, keras, lstm, word-embedding","<p><strong>In general, an LSTM layer needs 3D inputs shaped this way :</strong> <code>(batch_size, lenght of an input sequence , number of features )</code>. (Batch size is not really important, so you can just consider that one input need to have this shape <code>(lenght of sequence, number of features par item)</code> )</p>
<p>In your case, the output dim of your embedding layer is 300. So your LSTM have 300 features.</p>
<p>Then, using LSTM on sentences requires a constant number of tokens. LSTM works with constant input dimension, you can not pass it a text with 12 tokens following by another one with 68 tokens. Indeed, you need to fix a limit and pad the sequence if needed.
So, if your sentence is 20 tokens long and that your limit is 50, you need to pad (add at the end of your sequence) the sequence with 30 “neutral” tokens (often zeros).</p>
<p>After all, your LSTM input dimension must be <code>(number of token per text, dimension of your embedding outputs)</code> -&gt; <code>(50, 300)</code> in my example.</p>
<p><em>To learn more about it, it suggest you to take a look to this : (but in your case, you can replace time_steps by number_of_tokens)</em></p>
<p><a href=""https://shiva-verma.medium.com/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e"" rel=""nofollow noreferrer"">https://shiva-verma.medium.com/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e</a></p>
<p>Share
Edit
Delete
Flag</p>
",1,0,1216,2022-07-28 13:17:13,https://stackoverflow.com/questions/73153633/correct-keras-lstm-input-shape-after-text-embedding
Do weights of the [PAD] token have a function?,"<p>When looking at the weights of a transformer model, I noticed that the embedding weights for the padding token <code>[PAD]</code> are nonzero. I was wondering whether these weights have a function, since they are ignored in the multi-head attention layers.</p>
<p>Would it make sense to set these weights to zeros? The weights can be seen using <code>model.base_model.embeddings.word_embeddings.weight[PAD_ID]</code> where typically <code>PAD_ID=0</code>.</p>
","huggingface-transformers, word-embedding, transformer-model, huggingface-tokenizers, huggingface","<blockquote>
<p>Would it make sense to set these weights to zeros?</p>
</blockquote>
<p>As you said, these tokens are ignored during the self_attention calculation, therefore, it doesn't make a difference to make them zero.</p>
<p>Let's have a look at the relevant code of Bert as an example:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertModel, BertConfig
import torch

sample = &quot;This is&quot;
model_id = 'bert-base-uncased'

t = BertTokenizer.from_pretrained(model_id)
c = BertConfig.from_pretrained(model_id)
c.num_attention_heads = 1
c.num_hidden_layers=1
m = BertModel.from_pretrained(model_id,config=c)

encoded_input = t(sample, padding='max_length', max_length=5, return_tensors='pt')

print(encoded_input)
</code></pre>
<p>The model input consists of 5 tokens (BOS token, two text tokens, EOS token, and padding token):</p>
<pre class=""lang-py prettyprint-override""><code>{'input_ids': tensor([[ 101, 2023, 2003,  102,    0]]), 
'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 
'attention_mask': tensor([[1, 1, 1, 1, 0]])}
</code></pre>
<p>The attention_mask tells our model, that the first 4 tokens should attend to each other and the fifth token should be ignored. Bert does not use the attention mask as it is, it converts it to an <a href=""https://github.com/huggingface/transformers/blob/9c336657a92d14c906ed252b5bd251aabd0650fc/src/transformers/models/bert/modeling_bert.py#L991"" rel=""nofollow noreferrer"">extended_attention_mask</a>:</p>
<pre class=""lang-py prettyprint-override""><code>extended_attention_mask = m.get_extended_attention_mask(encoded_input['attention_mask'], encoded_input['input_ids'].shape)
print(extended_attention_mask)
</code></pre>
<p><code>extended_attention_mask</code> has negative infinite (e.g. float32.min) for every token which should not be taken into account during self-attention calculation and zero otherwise (<a href=""https://github.com/huggingface/transformers/blob/a2586795e577b29c526569b115f0cb4b002db968/src/transformers/modeling_utils.py#L741"" rel=""nofollow noreferrer"">code</a>):</p>
<pre><code># Please note the values depend on your machine you might see different numbers for negative infinite
tensor([[[[    -0.,     -0.,     -0.,     -0., -10000.]]]])
</code></pre>
<p>It is applied before the softmax is calculated from the <code>QK^T</code>-product (<a href=""https://github.com/huggingface/transformers/blob/9c336657a92d14c906ed252b5bd251aabd0650fc/src/transformers/models/bert/modeling_bert.py#L348https://"" rel=""nofollow noreferrer"">code</a>) and adds negative infinite to padding attention scores. Due to the huge difference in the individual values, the following softmax will assign zero to the padding attention scores:</p>
<pre class=""lang-py prettyprint-override""><code>attention_scores = torch.tensor([[[[ 9.5116e+00,  2.4427e-01, -1.1232e+00,  1.2221e+00, -1.0003e+04],
          [ 6.4593e+00,  5.6316e+00,  6.7172e+00,  7.7484e+00, -9.9928e+03],
          [ 4.6683e+00,  8.1287e+00,  6.1758e+00,  7.5101e+00, -9.9916e+03],
          [ 1.0366e+01,  8.0461e+00,  7.5019e+00,  9.2650e+00, -9.9944e+03],
          [ 1.2470e+01,  4.6752e+00,  5.9156e+00,  9.9091e+00, -9.9891e+03]]]])

attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)
print(attention_probs)
</code></pre>
<p>Output:</p>
<pre><code>tensor([[[[9.9963e-01, 9.4426e-05, 2.4055e-05, 2.5105e-04, 0.0000e+00],
          [1.5721e-01, 6.8711e-02, 2.0347e-01, 5.7061e-01, 0.0000e+00],
          [1.8351e-02, 5.8412e-01, 8.2864e-02, 3.1466e-01, 0.0000e+00],
          [6.7211e-01, 6.6057e-02, 3.8333e-02, 2.2350e-01, 0.0000e+00],
          [9.2672e-01, 3.8169e-04, 1.3195e-03, 7.1576e-02, 0.0000e+00]]]])
</code></pre>
<p>Even when you set the padding embedding tensor to zero, the difference to the other values is still so high, that it won't make a difference.</p>
",3,2,1436,2022-07-28 15:52:21,https://stackoverflow.com/questions/73155719/do-weights-of-the-pad-token-have-a-function
Clustering based on semantic similarity returning no values,"<p>I have 'Key_Phrases' as a column in pandas dataframe df. The objective is to cluster them on semantic similarity. I am using SentenceTransformer model.</p>
<pre><code> df['Key Phrases'] is as follows

                'Key_Phrases'

0              ['BYD' 'Daiwa Capital Markets analyst' 'NIO' 'Order flows'\n 'consumer preferences' 'cost pressures' 'raw materials'\n 'regulatory pressure' 'sales cannibalization' 'sales volume growth'\n 'vehicle batteries']
1              ['CANADA' 'Canada' 'Global Carbon Pricing Challenge'\n 'Major Economies Forum' 'climate finance commitment'\n 'developing countries' 'energy security' 'food security'\n 'international shipping' 'pollution pricing']
2              ['Clean Power Plan' 'EPA' 'Environmental Protection Agency'\n 'Supreme Court' 'Supreme Court decision' 'Virginia' 'West Virginia'\n 'renewable energy' 'tax subsidies']
3              ['BlueOvalSK' 'Ford' 'Ford Motor' 'Kathleen Valley' 'LG Energy' 'Liontown'\n 'Liontown Resources' 'SK Innovation' 'SK On' 'Tesla' 'battery metals'\n 'joint venture' 'lithium spodumene concentrate'\n 'lithium supply agreement']
4              ['Emissions Trading System' 'European Commission' 'European Parliament'\n 'ICIS' 'carbon border adjustment mechanism' 'carbon leakage']
5              ['Digital Industries' 'MG Motor India' 'MindSphere'\n 'Plant Simulation software' 'Siemens' 'carbon footprints'\n 'digitalisation' 'experience' 'intelligent manufacturing'\n 'production efficiency' 'strategic collaborations']
6              ['Malaysia' 'Mosti' 'NTIS' 'National Technology and Innovation Sandbox'\n 'National Urbanisation Policy' 'Sunway Innovation Labs'\n 'Sunway iLabs Super Accelerator' 'economic growth'\n 'memorandum of understanding' 'quality of life' 'safe environment'\n 'smart cities' 'smart city sandbox' 'urban management' 'urban population']
7              ['Artificial Intelligence' 'Electricity and Water Authority'\n 'Green Mobility' 'Grid Automation' 'Internet of Things' 'Smart Dubai'\n 'Smart Energy Solutions' 'Smart Grid' 'Smart Water'\n 'artificial intelligence' 'blockchain' 'connected services'\n 'energy storage' 'integrated systems' 'interoperability' 'smart city'\n 'smart grid' 'sustainability' 'water network']
8              ['Artificial Intelligence' 'Clean Energy Strategy 2050'\n 'Dubai Electricity and Water Authority' 'Green Mobility'\n 'Grid Automation' 'Internet of Things' 'Smart Dubai'\n 'Smart Energy Solutions' 'Smart Grid' 'Smart Water'\n 'Zero Carbon Emissions Strategy' 'artificial intelligence' 'blockchain'\n 'clean energy sources' 'connected services' 'energy storage'\n 'integrated systems' 'interoperability' 'smart city' 'smart grid'\n 'sustainability']

Key_Phrases_list_1 = df['Key Phrases'].tolist()
from sentence_transformers import SentenceTransformer, util
import numpy as np

model = SentenceTransformer('distilbert-base-nli-stsb-quora-ranking')    
#Encoding is done with one simple step
embeddings = model.encode(Key_Phrases_list_1, show_progress_bar=True, convert_to_numpy=True)
</code></pre>
<p>Then the following function is created:</p>
<pre><code>def detect_clusters(embeddings, threshold=0.90, min_community_size=20):
    # Compute cosine similarity scores
    cos_scores = util.pytorch_cos_sim(embeddings, embeddings)

    #we filter those scores according to the minimum community size we specified earlier
    # Minimum size for a community
    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)
    # Filter for rows &gt;= min_threshold
    extracted_communities = []
    for i in range(len(top_k_values)):
        if top_k_values[i][-1] &gt;= threshold:
            new_cluster = []

    # Only check top k most similar entries
            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=True)
            top_idx_large = top_idx_large.tolist()
            top_val_large = top_val_large.tolist()
            
            if top_val_large[-1] &lt; threshold:
                for idx, val in zip(top_idx_large, top_val_large):
                    if val &lt; threshold:
                        break
                        new_cluster.append(idx)
            else:
                # Iterate over all entries (slow)
                for idx, val in enumerate(cos_scores[i].tolist()):
                    if val &gt;= threshold:
                        new_cluster.append(idx)
                        
            extracted_communities.append(new_cluster)

    unique_communities = []
    extracted_ids = set()
        
    for community in extracted_communities:
        add_cluster = True
        for idx in community:
            if idx in extracted_ids:
                add_cluster = False
                break
        if add_cluster:
            unique_communities.append(community)
            for idx in community:
                extracted_ids.add(idx)
    return unique_communities
</code></pre>
<p>Then the function is called:</p>
<pre><code>clusters = detect_clusters(embeddings, min_community_size=6, threshold=0.75)
</code></pre>
<p>I am getting no values in return. Am I missing anything in the detect_clusters function.</p>
","python-3.x, pandas, numpy, word-embedding, transformer-model","<p>As the OP asked for a solution where the number of clusters would be automatic selected it is easier to use something more robust like sklearn:</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer, util
import numpy as np
model = SentenceTransformer('distilbert-base-nli-stsb-quora-ranking')
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
def choose_classifier(X):
    X1 = X / (X**2).sum(axis=-1, keepdims=True)
    vv = []
    cc = np.arange(2, len(X))
    for nclusters in cc:
        km_model = KMeans(nclusters).fit(X1)
        labels = km_model.labels_
        v = silhouette_score(X1, labels)
        vv.append(v)
    nclusters = cc[np.argmax(vv)]
    return KMeans(nclusters).fit(X1)

</code></pre>
<p>Use it like this</p>
<pre><code>phrases = [
    'I like ice cream',
    'I like cake',
    'You are so kind',
    'You are very intelligent'
]
embeddings = model.encode(phrases, show_progress_bar=True, convert_to_numpy=True)

classifier = choose_classifier(embeddings)

for i, (v, s) in enumerate(zip(embeddings, phrases)):
    print(classifier.predict(v[np.newaxis]), s)
</code></pre>
<pre class=""lang-txt prettyprint-override""><code>[1] I like ice cream
[1] I like cake
[0] You are so kind
[0] You are very intelligent
</code></pre>
<h2>GPU capable solution</h2>
<p>At a first sight I couldn't grasp all you are doing in your code, but let me suggest you some simplified method. I use <code>pytorch_kmeans</code>, and I explore the fact that the squared euclidean distance is <code>dot(A-B,A-B) = dot(A,A) + dot(B,B) - 2 * dot(A, B)</code>, and that cosine similarity is <code>dot(A, B) / sqrt(dot(A,A) * dot(B,B))</code>. So (1)
multiplying <code>A</code> or <code>B</code> by a scalar does not change cosine similarity, (2) if <code>A</code> and <code>B</code> have the same length, minimizing euclidean maximizes cosine similarity. Given the set of vectors you want to cluster you can (1) normalize all of them, making them the same length, (2) compute the clusters that minimize euclidean distance. Then you have the clusters that maximize cosine similarity.</p>
<pre class=""lang-bash prettyprint-override""><code>pip install kmeans_pytorch
</code></pre>
<h3>Setup</h3>
<p>Since you didnt' give data I will generate an example myself</p>
<pre class=""lang-py prettyprint-override""><code>import torch;
# 2D Example Data
# Generate some random data in three clusters
NPC=10
X = torch.cat([
  (torch.randn((NPC, 2)) + c) * (torch.rand((NPC,1))**2+1)/2 
      for c in torch.tensor([[5,3], [-7,0], [-0, -7]])])
</code></pre>
<h2>Solution</h2>
<p>This is the code</p>
<pre class=""lang-py prettyprint-override""><code>from kmeans_pytorch import kmeans
import torch
def detect_clusters(X, nclusters, tol=1e-6):
  X = torch.as_tensor(X)
  assert X.ndim == 2
  # Project the points in a hypersphere
  X1 = X / torch.sqrt(torch.sum(X**2, axis=-1, keepdims=True))

  # Run kmeans on the normalized points with euclidean distance
  cluster_ID, C = kmeans(X1, nclusters, distance='euclidean', tol=tol)
  return cluster_ID, C
</code></pre>
<h3>Example visualization</h3>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt
import numpy as np
import torch;

#### THE RESUTLS ####
cluster_ID, C = detect_clusters(X, 3)
# Avoid distortion of the angles
plt.axes().set_aspect('equal')
# Initial points
plt.plot(X[:,0], X[:,1], '.')
# Reference circle
theta = torch.linspace(0, 2*np.pi, 1000)
plt.plot(torch.cos(theta), torch.sin(theta), '--k')
plt.plot(X1[:,0], X1[:,1], '.')
xlim = plt.xlim()
ylim = plt.ylim()
plt.xlim(xlim)
plt.ylim(ylim)

# Draw lines in the directions given by the centroids
R = 20
for c in C:
    plt.plot([0, c[0]*R], [0, c[1]*R]);

plt.grid();
</code></pre>
<p><a href=""https://i.sstatic.net/cieGO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cieGO.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/ywveY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ywveY.png"" alt=""enter image description here"" /></a></p>
<h2>Using with sentence embeddings</h2>
<p>Some example embeddings</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer, util
import numpy as np

model = SentenceTransformer('distilbert-base-nli-stsb-quora-ranking')   


phrases = [
    'I like ice cream',
    'I like cake',
    'You are so kind',
    'You are very intelligent'
]
embeddings = model.encode(phrases, show_progress_bar=True, convert_to_numpy=True)
</code></pre>
<p>Then you can pass the embeddings to the <code>detect_cluster</code> function I provided above</p>
<pre class=""lang-py prettyprint-override""><code>label, center = detect_clusters(torch.as_tensor(embeddings), 2)
for c, s in zip(label, phrases):
    print(f'[{c}] {s}')
</code></pre>
<p>That should give give you the sentences with the corresponding cluster</p>
<pre class=""lang-txt prettyprint-override""><code>[0] I like ice cream
[0] I like cake
[1] You are so kind
[1] You are very intelligent
</code></pre>
",4,3,989,2022-07-31 11:56:17,https://stackoverflow.com/questions/73183103/clustering-based-on-semantic-similarity-returning-no-values
Using Word2Vec for word embedding of sentences,"<p>I am trying to create an emotion recognition model and for that I am using Word2Vec. I have a tokenized pandas data frame <code>x_train['Utterance']</code> and I have used</p>
<pre><code>model = gensim.models.Word2Vec(x_train['Utterance'], min_count = 1, vector_size = 100)
</code></pre>
<p>to create a vocabulary. Then, I created a dictionary embeddings_index that has as key the words and as value the vector embedding. I created a new column in my data frame where every word is replaced by the respective vector.</p>
<pre><code>x_train['vector'] = x_train['Utterance'].explode().map(embeddings_index).groupby(level=0).agg(list)
</code></pre>
<p>Finally, I used pad_sequences so that each instance of the data set is padded to the size of the instance with biggest length (because the data set initially was made of sentences of different sizes):</p>
<pre><code>x_train['vector'] = tf.keras.utils.pad_sequences(x_train.vector, maxlen = 30, dtype='float64', padding='post', truncating='post', value=0).tolist()
</code></pre>
<p>If <code>min_count = 1</code>, one of the parameters of Word2Vec, everything is alright and <code>x_train['vector']</code> is what I pretend, a column of the embeddings vectors of the tokenized sentences in <code>x_train['Utterance']</code>. However, when <code>min_count != 1</code>, the created vocabulary only has the words which appears more than the <code>min_count</code> value in <code>x_train['Utterance']</code>. Because of this, when creating <code>x_train['vector']</code> mapping the dictionary <code>embeddings_index</code>, the new column will contain lists like the following <code>[nan, [0.20900646, 0.76452744, 2.3117824], [0....</code>, where <code>nan</code> corresponds to words that are not in the dictionary. Because of this <code>nan</code>, when using the <code>tf.keras.utils.pad_sequences</code> I get the following error message: ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.</p>
<p>I would like to remove the <code>nan</code> from each list but I am not being able. Tried the <code>fillna('')</code> however it just removes the <code>nan</code> but keeps an empty index on the list. Any idea?</p>
","python, pandas, nlp, word2vec, word-embedding","<p>It seems the problem may that <code>x_train['Utterance']</code> includes a bunch of words that (after <code>min_count</code> trimming) aren't in the model. As a result you may be both miscalculating the true longest-text (because you're counting with unknown words), and get some nonsense values (where no word-vector was available for a low-frequency word)</p>
<p>The most simple fix would be to stop using the original <code>x_train['Utterance']</code> as your texts for steps that will be limited to a smaller vocabulary of only those words with word-vectors. Instead, pre-filter those text to eliminate words not present in the word-vector model. For example:</p>
<pre class=""lang-py prettyprint-override""><code>cleaned_texts = [[word for word in text if word in model.wv] 
                 for text in x_train['Utterance']]
</code></pre>
<p>Then, only use <code>cleaned_texts</code> for anything driving word-vector lookups, including your calculation of the longest text.</p>
<p>Other notes:</p>
<ul>
<li><p>you probably don't need to create your own <code>embeddings_index</code> dict-like object: the <code>Word2Vec</code> model already offers a dict-like interface, returning a word-vector per lookup key, via the instance of <code>KeyedVectors</code> in its <code>.wv</code> property.</p>
</li>
<li><p>if your other libraries or hardware considerations don't require <code>float64</code> values, you might just want to stick with <code>float32</code>-width values – that's what the <code>Word2Vec</code> model will train into word-vectors, they take half as much memory, and results from these kinds of models are rarely improved, and sometimes slowed, by using higher-precisions.</p>
</li>
<li><p>you could also consider creating a <code>FastText</code> model instead of plain <code>Word2Vec</code> - such a model will always return a vector, even for unknown words, synthesized from word-fragment-vectors that it learns while training.</p>
</li>
</ul>
",1,0,997,2022-08-06 18:23:47,https://stackoverflow.com/questions/73262309/using-word2vec-for-word-embedding-of-sentences
How to use word embedding and feature for text classification,"<p>I have a bunch of sentences that I am trying to classify. For each sentence, I generated a word embedding using word2vec. I also performed a cluster analysis which clustered the sentences into 3 separate clusters.</p>
<p>What I want to do is use the cluster id (1-3) as a feature for my model. However, I am just not entirely sure how to do this? I can't seem to find a good article that clearly states how to do this.</p>
<p>I was thinking I could create a one hot embedding for the cluster id and then somehow combine the one hot to the word embedding? I am really not sure what to do here.</p>
<p>I already have a model that will take the word embedding and classify the sentence:</p>
<pre><code>X=Data[word_embedding].values
y=Data[category].values

indices = filtered_products.index.values
X_train, X_test, y_train, y_test, indices_train, indices_test, = train_test_split(X, y, indices, test_size=0.3, random_state=428)

clf = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')

DSVM = clf.fit(X_train,y_train)
prediction = DSVM.predict(X_test)

print(metrics.classification_report(y_test, prediction))
</code></pre>
<p>Where <code>X</code> is the word embedding and <code>y</code> is the category. Just not sure how to add in the cluster id as a feature</p>
","python, tensorflow, machine-learning, scikit-learn, word-embedding","<p>Assuming, you want to use Tensorflow. You can either one-hot encode the ids or map them to n-dimensional random vectors using an <code>Embedding</code> layer. Here is an example with an <code>Embedding</code> layer, where I am mapping each id to a 10-dimensional vector and then repeating this vector 50 times to correspond to the max length of a sentence (So, each word has the same 10-dimensional vector for a given input). Afterwards, I just concatenate:</p>
<pre><code>import tensorflow as tf

word_embedding_dim = 300
max_sentence_length = 50

word_embedding_input = tf.keras.layers.Input((max_sentence_length, word_embedding_dim))

id_input = tf.keras.layers.Input((1, ))
embedding_layer = tf.keras.layers.Embedding(1, 10) # or one-hot encode
x = embedding_layer(id_input)
x = tf.keras.layers.RepeatVector(max_sentence_length)(x[:, 0, :])

output = tf.keras.layers.Concatenate()([word_embedding_input, x])
model = tf.keras.Model([word_embedding_input, id_input], output)

print(model.summary())
</code></pre>
<pre><code>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_17 (InputLayer)          [(None, 1)]          0           []                               
                                                                                                  
 embedding_3 (Embedding)        (None, 1, 10)        10          ['input_17[0][0]']               
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 10)          0           ['embedding_3[0][0]']            
 ingOpLambda)                                                                                     
                                                                                                  
 input_16 (InputLayer)          [(None, 50, 300)]    0           []                               
                                                                                                  
 repeat_vector_1 (RepeatVector)  (None, 50, 10)      0           ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 concatenate (Concatenate)      (None, 50, 310)      0           ['input_16[0][0]',               
                                                                  'repeat_vector_1[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10
Trainable params: 10
Non-trainable params: 0
__________________________________________________________________________________________________
None
</code></pre>
<p>If you do not have a 2D input, but actually sentence embeddings, it is even easier:</p>
<pre><code>import tensorflow as tf

sentence_embedding_dim = 300

sentence_embedding_input = tf.keras.layers.Input((sentence_embedding_dim,))
id_input = tf.keras.layers.Input((1, ))
embedding_layer = tf.keras.layers.Embedding(1, 10) # or one-hot encode
x = embedding_layer(id_input)

output = tf.keras.layers.Concatenate()([sentence_embedding_input, x[:, 0, :]])
model = tf.keras.Model([sentence_embedding_input, id_input], output)
</code></pre>
<p>Here is a solution with <code>numpy</code> and <code>sklearn</code> for reference:</p>
<pre><code>import numpy as np
from sklearn.preprocessing import OneHotEncoder

samples = 10
word_embedding_dim = 300
max_sentence_length = 50

ids = np.random.randint(low=1, high=4, size=(10,)).reshape(-1, 1)
enc = OneHotEncoder(handle_unknown='ignore')
ids = enc.fit_transform(ids).toarray()[:, None, :]

X_train = np.random.random((samples, max_sentence_length, word_embedding_dim))

ids = np.repeat(ids, max_sentence_length, axis=1)
X_train = np.concatenate([X_train, ids], axis=-1)
print(X_train.shape)
# (10, 50, 303)
</code></pre>
",2,1,1175,2022-08-24 15:59:56,https://stackoverflow.com/questions/73476302/how-to-use-word-embedding-and-feature-for-text-classification
Spacy models with different word2vec embeddings give same results,"<p>I am trying to improve the performance of my spacy NER model by implementing my pretrained vectors. I have created my own vectors with word2vec using different texts and I have saved them in .txt files. However I get the exact same scores and this doesn't seem right.</p>
<p>Here are the steps I have been following for one file with custom pretrained embeddings:</p>
<pre><code>!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_uncleaned_sents_model.txt ./uncl_txt --name JSTOR_unlceaned_sents_model

nlp = spacy.load(&quot;./uncl_txt&quot;)
nlp.add_pipe(&quot;ner&quot;)
nlp.to_disk(&quot;./uncl_txt&quot;)

!python -m spacy train /content/uncl_txt/config.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./uncl_txt --paths.vectors ./uncl_txt

!python -m spacy evaluate /content/uncl_txt/model-best ./Spacy/eval.spacy --output ner_with_uncleaned_sents_vectors.jsonl
</code></pre>
<p>Here are the steps for the other embeddings file:</p>
<pre><code>!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_abs_model.txt ./abs --name JSTOR_abs_model

nlp = spacy.load(&quot;./abs&quot;)
nlp.add_pipe(&quot;ner&quot;)
nlp.to_disk(&quot;./abs&quot;)

!python -m spacy train /content/abs/config.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./abs/ --paths.vectors ./abs

!python -m spacy evaluate ./abs/model-best ./Spacy/eval.spacy --output ner_with_abs_vectors.jsonl
</code></pre>
<p>Am I doing something wrong? Should I add something in the config file?</p>
","python, spacy, word2vec, named-entity-recognition, word-embedding","<p>The model created using <code>nlp.add_pipe(&quot;ner&quot;)</code> does not have embeddings enabled by default.</p>
<p>The easiest way to create a config for <code>ner</code> with embeddings enabled is to use <code>spacy init config</code> with <code>-o accuracy</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>spacy init config -p ner -o accuracy ner.cfg
</code></pre>
<p>And then train with:</p>
<pre class=""lang-bash prettyprint-override""><code>spacy train ner.cfg --paths.train train.spacy --paths.dev dev.spacy --paths.vectors ./vectors
</code></pre>
<p>(You can also enable it using custom config settings with <code>nlp.add_pipe(&quot;ner&quot;, config=...)</code>, but this requires digging into the details about the internal default model config, which might also change depending on the version of spacy, so <code>spacy init config</code> is easier to use.)</p>
",1,1,198,2022-09-01 11:20:26,https://stackoverflow.com/questions/73568510/spacy-models-with-different-word2vec-embeddings-give-same-results
Calculating similarities of text embeddings using CLIP,"<p>I am trying to use <a href=""https://github.com/openai/CLIP"" rel=""noreferrer"">CLIP</a> to calculate the similarities between strings. (I know that CLIP is usually used with text and images but it should work with only strings as well.)</p>
<p>I provide a list of simple text prompts and calculate the similarity between their embeddings. The similarities are off but I can't figure what I'm doing wrong.</p>
<pre><code>import torch
import clip
from torch.nn import CosineSimilarity

cos = CosineSimilarity(dim=1, eps=1e-6)

def gen_features(model, text):
    tokens = clip.tokenize([text]).to(device)
    text_features = model.encode_text(tokens)

    return text_features

def dist(v1, v2):
    #return torch.dist(normalize(v1), normalize(v2)) # euclidean distance
    #return cos(normalize(v1), normalize(v2)).item() # cosine similarity

    similarity = (normalize(v1) @ normalize(v2).T)

    return similarity.item()



device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
model_name = &quot;ViT-B/32&quot;
model, _ = clip.load(model_name, device=device)

sentences = [&quot;A cat&quot;, &quot;A dog&quot;, &quot;A labrador&quot;, &quot;A poodle&quot;, &quot;A wolf&quot;, &quot;A lion&quot;, &quot;A house&quot;]

with torch.no_grad():
    embeddings = [(sentence, gen_features(model, sentence)) for sentence in sentences]
    for label1, embedding1 in embeddings:
        for label2, embedding2 in embeddings:
            print(f&quot;{label1} -&gt; {label2}: {dist(embedding1, embedding2)}&quot;)


Output


    A cat -&gt; A cat: 0.9999998211860657
    A cat -&gt; A dog: 0.9361147880554199
    A cat -&gt; A labrador: 0.8170720934867859
    A cat -&gt; A poodle: 0.8438302278518677
    A cat -&gt; A wolf: 0.9086413979530334
    A cat -&gt; A lion: 0.8914517164230347
    A cat -&gt; A house: 0.8724125027656555
    A dog -&gt; A cat: 0.9361147880554199
    A dog -&gt; A dog: 1.0000004768371582
    A dog -&gt; A labrador: 0.8481228351593018
    A dog -&gt; A poodle: 0.9010260105133057
    A dog -&gt; A wolf: 0.9260395169258118
    A dog -&gt; A lion: 0.886112630367279
    A dog -&gt; A house: 0.8852840662002563
    A labrador -&gt; A cat: 0.8170720934867859
    A labrador -&gt; A dog: 0.8481228351593018
    A labrador -&gt; A labrador: 1.000000238418579
    A labrador -&gt; A poodle: 0.7722526788711548
    A labrador -&gt; A wolf: 0.8111101984977722
    A labrador -&gt; A lion: 0.783727765083313
    A labrador -&gt; A house: 0.7569846510887146
    A poodle -&gt; A cat: 0.8438302278518677
    A poodle -&gt; A dog: 0.9010260105133057
    A poodle -&gt; A labrador: 0.7722526788711548
    A poodle -&gt; A poodle: 0.999999463558197
    A poodle -&gt; A wolf: 0.8539597988128662
    A poodle -&gt; A lion: 0.8460092544555664
    A poodle -&gt; A house: 0.8119628429412842
    A wolf -&gt; A cat: 0.9086413979530334
    A wolf -&gt; A dog: 0.9260395169258118
    A wolf -&gt; A labrador: 0.8111101984977722
    A wolf -&gt; A poodle: 0.8539597988128662
    A wolf -&gt; A wolf: 1.000000238418579
    A wolf -&gt; A lion: 0.9043934941291809
    A wolf -&gt; A house: 0.860664427280426
    A lion -&gt; A cat: 0.8914517164230347
    A lion -&gt; A dog: 0.886112630367279
    A lion -&gt; A labrador: 0.783727765083313
    A lion -&gt; A poodle: 0.8460092544555664
    A lion -&gt; A wolf: 0.9043934941291809
    A lion -&gt; A lion: 1.0000004768371582
    A lion -&gt; A house: 0.8402873873710632
    A house -&gt; A cat: 0.8724125027656555
    A house -&gt; A dog: 0.8852840662002563
    A house -&gt; A labrador: 0.7569846510887146
    A house -&gt; A poodle: 0.8119628429412842
    A house -&gt; A wolf: 0.860664427280426
    A house -&gt; A lion: 0.8402873873710632
    A house -&gt; A house: 0.9999997615814209


The results show that a dog is closer to a house than it is for a labrador 0.885 vs 0.848 which doesn't make sense. I've tried cosine similarity and euclidean distance to check whether the distance measure was wrong, but the results are similar. Where am I going wrong?

</code></pre>
","python, word-embedding, cosine-similarity","<p>If you use the text embeddings from the output of <code>CLIPTextModel</code> ([number of prompts, 77, 512]), flatten them ([number of prompts, 39424]) and the apply cosine similarity, you'll get improved results.</p>
<p>This code lets you test both solutions ([1,512] and [77,512]). I'm running it on Google Colab.</p>
<pre><code>    !pip install -U torch transformers 
    
    import torch
    from torch.nn import CosineSimilarity
    from transformers import CLIPTokenizer, CLIPModel, CLIPTextModel
    cossim = CosineSimilarity(dim=0, eps=1e-6)
    
    def dist(v1, v2):
      return cossim(v1, v2)
    
    torch_device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
    
    models = [
        'openai/clip-vit-base-patch16',
        'openai/clip-vit-base-patch32',
        'openai/clip-vit-large-patch14',
    ]
    
    model_id = models[1]
    
    tokenizer = CLIPTokenizer.from_pretrained(model_id)
    text_encoder = CLIPTextModel.from_pretrained(model_id).to(torch_device)
    model = CLIPModel.from_pretrained(model_id).to(torch_device)
    
    prompts = [
      &quot;A cat&quot;, &quot;A dog&quot;, &quot;A labrador&quot;, &quot;A poodle&quot;, &quot;A wolf&quot;, &quot;A lion&quot;, &quot;A house&quot;,
    ] 
    
    text_inputs = tokenizer(
        prompts, 
        padding=&quot;max_length&quot;, 
        return_tensors=&quot;pt&quot;,
        ).to(torch_device)
    text_features = model.get_text_features(**text_inputs)
    text_embeddings = torch.flatten(text_encoder(text_inputs.input_ids.to(torch_device))['last_hidden_state'],1,-1)
    
    print(&quot;\n\nusing text_features&quot;)
    for i1, label1 in enumerate(prompts):
      for i2, label2 in enumerate(prompts):
        if (i2&gt;=i1):
          print(f&quot;{label1} &lt;-&gt; {label2} = {dist(text_features[i1], text_features[i2]):.4f}&quot;)
    
    print(&quot;\n\nusing text_embeddings&quot;)
    for i1, label1 in enumerate(prompts):
      for i2, label2 in enumerate(prompts):
        if (i2&gt;=i1):
          print(f&quot;{label1} &lt;-&gt; {label2} = {dist(text_embeddings[i1], text_embeddings[i2]):.4f}&quot;)

</code></pre>
<p>You'll get the same values for the [1,512] embedding</p>
<pre><code>A cat &lt;-&gt; A cat = 1.0000
A cat &lt;-&gt; A dog = 0.9361
A cat &lt;-&gt; A labrador = 0.8171
A cat &lt;-&gt; A poodle = 0.8438
A cat &lt;-&gt; A wolf = 0.9086
A cat &lt;-&gt; A lion = 0.8915
A cat &lt;-&gt; A house = 0.8724
A dog &lt;-&gt; A dog = 1.0000
**A dog &lt;-&gt; A labrador = 0.8481**
A dog &lt;-&gt; A poodle = 0.9010
A dog &lt;-&gt; A wolf = 0.9260
A dog &lt;-&gt; A lion = 0.8861
**A dog &lt;-&gt; A house = 0.8853**
A labrador &lt;-&gt; A labrador = 1.0000
A labrador &lt;-&gt; A poodle = 0.7723
A labrador &lt;-&gt; A wolf = 0.8111
A labrador &lt;-&gt; A lion = 0.7837
A labrador &lt;-&gt; A house = 0.7570
A poodle &lt;-&gt; A poodle = 1.0000
A poodle &lt;-&gt; A wolf = 0.8540
A poodle &lt;-&gt; A lion = 0.8460
A poodle &lt;-&gt; A house = 0.8120
A wolf &lt;-&gt; A wolf = 1.0000
A wolf &lt;-&gt; A lion = 0.9044
A wolf &lt;-&gt; A house = 0.8607
A lion &lt;-&gt; A lion = 1.0000
A lion &lt;-&gt; A house = 0.8403
A house &lt;-&gt; A house = 1.0000
</code></pre>
<p>But the results have improved with the [1,77,512] embedding, and now the dog is closer to the labrador than to the house. Still, you'll get funny results such as the cat being more similar to a house than to a poodle.</p>
<pre><code>A cat &lt;-&gt; A cat = 1.0000
A cat &lt;-&gt; A dog = 0.8880
A cat &lt;-&gt; A labrador = 0.8057
A cat &lt;-&gt; A poodle = 0.7579
A cat &lt;-&gt; A wolf = 0.8558
A cat &lt;-&gt; A lion = 0.8358
A cat &lt;-&gt; A house = 0.8024
A dog &lt;-&gt; A dog = 1.0000
**A dog &lt;-&gt; A labrador = 0.8794**
A dog &lt;-&gt; A poodle = 0.8583
A dog &lt;-&gt; A wolf = 0.8888
A dog &lt;-&gt; A lion = 0.8265
**A dog &lt;-&gt; A house = 0.8294**
A labrador &lt;-&gt; A labrador = 1.0000
A labrador &lt;-&gt; A poodle = 0.8006
A labrador &lt;-&gt; A wolf = 0.8182
A labrador &lt;-&gt; A lion = 0.7958
A labrador &lt;-&gt; A house = 0.7608
A poodle &lt;-&gt; A poodle = 1.0000
A poodle &lt;-&gt; A wolf = 0.7928
A poodle &lt;-&gt; A lion = 0.7735
A poodle &lt;-&gt; A house = 0.7623
A wolf &lt;-&gt; A wolf = 1.0000
A wolf &lt;-&gt; A lion = 0.8496
A wolf &lt;-&gt; A house = 0.8063
A lion &lt;-&gt; A lion = 1.0000
A lion &lt;-&gt; A house = 0.7671
A house &lt;-&gt; A house = 1.0000
</code></pre>
",9,10,7988,2022-09-03 16:13:34,https://stackoverflow.com/questions/73593712/calculating-similarities-of-text-embeddings-using-clip
Embedding inside the model vs outside the model,"<p>What is the difference between using the embedding layer inside the model and outside the model? I can build the embedding layer into the model:</p>
<pre><code>model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(input_dim=1000, output_dim=64, input_length=10))
...
model.fit(features, target ...)
</code></pre>
<p>I can also use embdedding outside the model to generate embedded data and then feed it into the model:</p>
<pre><code>embedding_encoder = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)
embedded_features = embedding_encoder(features)
...

model.fit(embedded_features, target ...)
</code></pre>
<p>Does it mean that if I use embedding outside the model, embedding parameters are not learned during the training?</p>
","python, tensorflow, keras, neural-network, word-embedding","<blockquote>
<p>Does it mean that if I use embedding outside the model, embedding
parameters are not learned during the training?</p>
</blockquote>
<p>The dense vector representations assigned from an <code>Embedding</code> layer are generally only trainable when setting <code>trainable=True</code>. It's entirely up to you how you want to preprocess your data yourself and how much you want to leave to the <code>Embedding</code> layer. Usually, if you are working on a NLP task, you can add a <code>StringLookup</code> or <code>TextVectorization</code> layer prior to adding an <code>Embedding</code> layer that allows you to preprocess your texts and train them in an elegant way without any &quot;manual&quot; steps.</p>
<p><strong>Generally</strong>:</p>
<p>Each integer value fed to an <code>Embedding</code> layer is mapped to a unique N-dimensional vector representation, where N is chosen by you. These vector representations are, by default, drawn from a uniform distribution. The <code>Embedding</code> layer inherits from <code>tf.keras.layers.Layer</code> which contains the trainable parameter.</p>
<p>I think it could make sense to generate embedding data outside your model if you are, for example, using pretrained context-sensitive vectors and you do not want to update their values during training. But again, it’s all up to you.</p>
",1,1,679,2022-09-06 16:01:25,https://stackoverflow.com/questions/73624865/embedding-inside-the-model-vs-outside-the-model
How to model with NLP when the token is not relevant (by itself) but its type is?,"<p>I would like to build an NLP classification model.
My input is a paragraph or a sentence. Ideally, my output is a score or probability (between 0 and 1).</p>
<p>I have defined specific entities ex-ante, each entity belongs to a single group.</p>
<p>Based on business insights, we know that the output to predict does not depend on the entities by themselves, but depends on their groups. For example, the phrase “Max barks” would return 1 because “Max” belongs to the group “Dogs”, but “Kitty barks” would return 0 (because Kitty is not a dog). If “Max” was a cat, the phrase would return 0.
One way to do so would be to generate all the sentences with all the permutations of dogs and cats (in my example) but that is very cumbersome!
Another way would be to replace the entity with the name of the group (the phrase “Max” becomes “” for example) but that looks weird to me!</p>
<p>I don't have any other idea how to tackle this problem.</p>
<p>Could you please help me, ideally with code?</p>
<p>Thanks a lot.</p>
","nlp, tokenize, bert-language-model, word-embedding","<p>If I understand your question correctly, you are to classify the text into &quot;dog activities&quot; vs. &quot;non-dog activities&quot; and in the text you are referencing dogs, cats (and maybe other animals) by their names but you know which name is related with which species.</p>
<p>In such a case I would suggest introducing a named entity token replacing each name of an animal with its species. In your example <code>&quot;Max barks&quot;</code> could be replaced with <code>&quot;%DOG% barks&quot;</code> and <code>&quot;Kitty barks&quot;</code> with <code>&quot;%CAT% barks&quot;</code>.</p>
<p>This would form a strong signal for the model to pick up and train correctly.</p>
<p>Otherwise, you could also go with your approach of generating all of the potential examples of dogs and cats where the name would be loosely linked with a one or the other group by the label of the training / testing example. Even though it is a bit cumbersome it can be more practical that introducing another step to the processing pipeline - Name Entity Recognition - which translates the names of the animals to their species. And such a step would be necessary both in the training and during inference.</p>
",0,0,49,2022-09-17 21:54:59,https://stackoverflow.com/questions/73758805/how-to-model-with-nlp-when-the-token-is-not-relevant-by-itself-but-its-type-is
BERT without positional embeddings,"<p>I am trying to build a pipeline in HuggingFace which will not use the positional embeddings in BERT, in order to study the role of the embeddings for a particular use case. I have looked through the documentation and the code, but I have not been able to find a way to implement a model like that. Will I need to modify BERT source code, or is there a configuration I can fiddle around with?</p>
","huggingface-transformers, bert-language-model, word-embedding","<p>You can do a workaround by setting the position embedding layer to zeros. When you check, the embeddings part of BERT, you can see that the position embeddings are there as a separate PyTorch module:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel
bert = AutoModel.from_pretrained(&quot;bert-base-cased&quot;)
print(bert.embeddings)
</code></pre>
<pre><code>BertEmbeddings(
  (word_embeddings): Embedding(28996, 768, padding_idx=0)
  (position_embeddings): Embedding(512, 768)
  (token_type_embeddings): Embedding(2, 768)
  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
</code></pre>
<p>You can assign the position embedding parameters whatever value you want, including zeros, which will effectively disable the position embeddings:</p>
<pre class=""lang-py prettyprint-override""><code>bert.embeddings.position_embeddings.weight.data = torch.zeros((512, 768))
</code></pre>
<p>If you plan to fine-tune the modified model, make sure the zeroed parameters do not get updated by setting:</p>
<pre class=""lang-py prettyprint-override""><code>bert.embeddings.position_embeddings.requires_grad_ = False
</code></pre>
<p>This sort of bypassing the position embeddings might work well when you train a model from scratch. When you work with a pre-trained model, such removal of some parameters might confuse the models quite a bit, so more fine-tuning data might be needed. In this case, there might be better strategies on how to replace the position embeddings, e.g., using the average value for all positions.</p>
",2,0,1259,2022-10-10 23:25:43,https://stackoverflow.com/questions/74021562/bert-without-positional-embeddings
tensorflow UnknownError: Graph execution error: JIT compilation failed. [Op:__inference_restored_function_body_9127],"<p>I was trying to use UNIVERSAL SENTENCE ENCODER from tensorflow hub.
Downloaded and extracted universal sentence encoder from hub
and when i tried to predict a senetence it showed an Error saying</p>
<p>UnknownError: Graph execution error:</p>
<p>JIT compilation failed.</p>
<pre><code>import tensorflow_hub as hub

#loading downloaded and untarred universal sentence encoder
embed = hub.load(&quot;./universal-sentence-encoder_4/&quot;)
  
# passed as an array in embed()
Sentences = [
    &quot;How old are you&quot;
]
embeddings = embed(Sentences)
  
print(embeddings)
  
</code></pre>
<p>and got error</p>
<pre><code>2022-11-25 06:29:46.006767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2630 MB memory:  -&gt; device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
2022-11-25 06:29:50.652156: W tensorflow/core/framework/op_kernel.cc:1768] UNKNOWN: JIT compilation failed.
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
Input In [1], in &lt;cell line: 25&gt;()
     17 # Load pre-trained universal sentence encoder model
     18 # embed = hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;)
     19   
     20 # Sentences for which you want to create embeddings,
     21 # passed as an array in embed()
     22 Sentences = [
     23     &quot;How old are you&quot;
     24 ]
---&gt; 25 embeddings = embed(Sentences)
     27 # Printing embeddings of each sentence
     28 print(embeddings)

File ~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:704, in _call_attribute(instance, *args, **kwargs)
    703 def _call_attribute(instance, *args, **kwargs):
--&gt; 704   return instance.__call__(*args, **kwargs)

File ~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--&gt; 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File ~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---&gt; 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

UnknownError: Graph execution error:

JIT compilation failed.
     [[{{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/mod}}]] [Op:__inference_restored_function_body_4561]

</code></pre>
<p>how to i fix it?
I just want it working.</p>
","tensorflow, deep-learning, word-embedding","<p>I had the error too and just did it with my CPU and it worked.</p>
<pre><code>with tf.device('/CPU:0'):
    embeddings = embed(Sentences)
</code></pre>
",3,1,5235,2022-11-25 01:13:14,https://stackoverflow.com/questions/74567500/tensorflow-unknownerror-graph-execution-error-jit-compilation-failed-op-in
Classic king - man + woman = queen example with pretrained word-embedding and word2vec package in R,"<p>I am really desperate, I just cannot reproduce the allegedly classic example of <code>king - man + woman = queen</code> with the <code>word2vec</code> package in R and any (!) pre-trained embedding model (as a <code>bin</code> file).</p>
<p>I would be very grateful if anybody could provide working code to reproduce this example... including a link to the necessary pre-trained model which is also downloadable (many are not!).</p>
<p>Thank you very much!</p>
","r, nlp, word2vec, word-embedding","<p>An overview of using word2vec with R is available at <a href=""https://www.bnosac.be/index.php/blog/100-word2vec-in-r"" rel=""nofollow noreferrer"">https://www.bnosac.be/index.php/blog/100-word2vec-in-r</a> which even shows an example of king - man + woman = queen.</p>
<p>Just following the instructions there and downloading the first English 300-dim embedding word2vec model from <a href=""http://vectors.nlpl.eu/repository"" rel=""nofollow noreferrer"">http://vectors.nlpl.eu/repository</a> ran on the British National Corpus which I encountered, downloaded and unzipped the model.bin on my drive and next inspecting the terms in the model (words are there apparently appended with pos tags), getting the word vectors, displaying the vectors, getting the king - man + woman and finding the closest vector to that vector gives ... queen.</p>
<pre><code>&gt; library(word2vec)
&gt; model &lt;- read.word2vec(&quot;C:/Users/jwijf/OneDrive/Bureaublad/model.bin&quot;, normalize = TRUE)
&gt; head(summary(model, type = &quot;vocabulary&quot;), n = 10)
 [1] &quot;vintage-style_ADJ&quot; &quot;Sinopoli_PROPN&quot;    &quot;Yarrell_PROPN&quot;     &quot;en-1_NUM&quot;          &quot;74°–78°F_X&quot;       
 [6] &quot;bursa_NOUN&quot;        &quot;uni-male_ADJ&quot;      &quot;37541_NUM&quot;         &quot;Menuetto_PROPN&quot;    &quot;Saxena_PROPN&quot;     
&gt; wv &lt;- predict(model, newdata = c(&quot;king_NOUN&quot;, &quot;man_NOUN&quot;, &quot;woman_NOUN&quot;), type = &quot;embedding&quot;)
&gt; head(t(wv), n = 10)
       king_NOUN    man_NOUN  woman_NOUN
 [1,] -0.4536242 -0.47802860 -1.03320265
 [2,]  0.7096733  1.40374041 -0.91597748
 [3,]  1.1509652  2.35536361  1.57869458
 [4,] -0.2882653 -0.59587735 -0.59021348
 [5,] -0.2110678 -1.05059254 -0.64248675
 [6,]  0.1846713 -0.05871651 -1.01818573
 [7,]  0.5493720  0.13456300  0.38765019
 [8,] -0.9401053  0.56237948  0.02383301
 [9,]  0.1140556 -0.38569298 -0.43408644
[10,]  0.3657919  0.92853492 -2.56553030
&gt; wv &lt;- wv[&quot;king_NOUN&quot;, ] - wv[&quot;man_NOUN&quot;, ] + wv[&quot;woman_NOUN&quot;, ]
&gt; predict(model, newdata = wv, type = &quot;nearest&quot;, top_n = 4)
             term similarity rank
1       king_NOUN  0.9332663    1
2      queen_NOUN  0.7813236    2
3 coronation_NOUN  0.7663506    3
4   kingship_NOUN  0.7626975    4
</code></pre>
<p>Do you prefer to build your own model based on your own text or a more larger corpus e.g. the text8 file. Follow the instructions shown at <a href=""https://www.bnosac.be/index.php/blog/100-word2vec-in-r"" rel=""nofollow noreferrer"">https://www.bnosac.be/index.php/blog/100-word2vec-in-r</a>.
Get a text file and use R package word2vec to build the model, wait untill the model finished training and next interact with it.</p>
<pre><code>download.file(&quot;http://mattmahoney.net/dc/text8.zip&quot;, &quot;text8.zip&quot;)
unzip(&quot;text8.zip&quot;, files = &quot;text8&quot;)

&gt; library(word2vec)
&gt; set.seed(123456789)
&gt; model &lt;- word2vec(x = &quot;text8&quot;, type = &quot;cbow&quot;, dim = 100, window = 10, lr = 0.05, iter = 5, hs = FALSE, threads = 2)
&gt; wv    &lt;- predict(model, newdata = c(&quot;king&quot;, &quot;man&quot;, &quot;woman&quot;), type = &quot;embedding&quot;)
&gt; wv    &lt;- wv[&quot;king&quot;, ] - wv[&quot;man&quot;, ] + wv[&quot;woman&quot;, ]
&gt; predict(model, newdata = wv, type = &quot;nearest&quot;, top_n = 4)
      term similarity rank
1     king  0.9743692    1
2    queen  0.8295941    2
</code></pre>
",3,4,1867,2022-12-12 10:15:29,https://stackoverflow.com/questions/74769552/classic-king-man-woman-queen-example-with-pretrained-word-embedding-and-wo
Subword vector in fastText?,"<p>I can't figure out what a subword input vector is. I read in the newspaper that the subword is hashed, the subword is the hash code, hash code is a number, not a vector</p>
<p><strong>Ex:</strong> Input vector of word eating is [0,0,0,1,0,0,0,0,0]
So what is the input vector of subwords &quot;eat&quot;, &quot;ati&quot;, &quot;ing&quot;,...?</p>
<p>Link paper: <a href=""https://arxiv.org/pdf/1607.04606.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1607.04606.pdf</a></p>
<p><a href=""https://i.sstatic.net/afcFL.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>the subword is the hash code, hash code is a number, not a vector</p>
","nlp, word-embedding, fasttext","<p>The FastText subwords are, as you've suggested, fragments of the full word. For the purposes of subword creation, FastText will <em>also</em> prepend/append special start-of-word and end-of-word characters. (If I recall correctly, it uses <code>&lt;</code> &amp; <code>&gt;</code>.)</p>
<p>So, for the full word token <code>'eating'</code>, it is considered as <code>'&lt;eating&gt;'</code>.</p>
<p>All the 3-character subwords would be <code>'&lt;ea'</code>, <code>'eat'</code>, <code>'ati'</code>, <code>'tin'</code>, <code>'ing'</code>, <code>'ng&gt;'</code>.</p>
<p>All the 4-character subwords would be <code>'&lt;eat'</code>, <code>'atin'</code>, <code>'ting'</code>, <code>'ing&gt;'</code>.</p>
<p>All the 5-character subwords would be <code>'&lt;eati'</code>, <code>'ating'</code>, <code>'ting&gt;'</code>.</p>
<p>I see you've written out a &quot;one-hot&quot; representation of the full word <code>'eating'</code> – <code>[0,0,0,1,0,0,0,0,0]</code> – as if <code>'eating'</code> is the 4th word in a 9-word vocabulary. While diagrams &amp; certain ways of thnking about the underlying model may consider such a one-hot vector, it's useful to realize that in actual code implementations, such a sparse one-hot vector for words is never actually created.</p>
<p>Instead, it's just represented as a single number – the index to the non-zero number. That's used as a lookup into an array of vectors of the configured 'dense' size, returning one input word-vector of that size for the word.</p>
<p>For example, imagine you have a model with a 1-million word known vocabulary, which offers 100-dimensional 'dense embedding' word-vectors. The word <code>'eating'</code> is the 543,210th word.</p>
<p>That model will have an array of input-vectors that's has one million slots, and each slot has a 100-dimensional vector in it. We could call it <code>word_vectors_in</code>. The word <code>'eating'</code>'s vector will be at <code>word_vectors_in[543209]</code> (beccause the 1st vector is at <code>word_vectors_in[0]</code>).</p>
<p>At no point during the creation/training/use of this model will an actual 1-million-long one-hot vector for <code>'eating'</code> be created. Most often, it'll just be referred-to inside the code as the word-index <code>543209</code>. The model will have a helper lookup dictionary/hashmap, let's call it <code>word_index</code> that lets code find the right slot for a word. So <code>word_index['eating']</code> will be 543209.</p>
<p>OK, now to your actual question, about the subwords. I've detailed how the the single vectors per one known full word are stored, above, in order to contrast it with the different way subwords are handled.</p>
<p>Subwords are also stored in a big array of vectors, but that array is treated as a <em>collision-oblivious</em> hashtable. That is, by design, many subwords can and do all reuse the same slot.</p>
<p>Let's call that big array of subword vectors <code>subword_vector_in</code>. Let's also make it 1 million slots long, where each slot has a 100-dimensional vector.</p>
<p>But now, there is no dictionary that remembers which subwords are in which slots - for example, remembering that subword <code>'&lt;eat'</code> is in arbitrary slot 78789.</p>
<p>Instead, the string <code>'&lt;eat'</code> is hashed to a number, that number is restricted to the possible indexes into the subwords, and the vector at that index, let's say it's 12344, is used for the subword.</p>
<p>And then when some other subword comes along, maybe <code>'&lt;dri'</code>, it <em>might</em> hash to the exact-same 12344 slot. And that same vector then gets adjusted for that other subword (during training), or returned for both those subwords (and possibly many others) during later FastText-vector synthesis from the finali model.</p>
<p>Notably, now even if there are far <em>more</em> than 1-million unique subwords, they can <em>all</em> be represented inside that single 1-million slot array, albeit with collisions/interference.</p>
<p>In practice, the collisions are tolerable because many collisions from very-rare subwords essentially just fuzz slots with lots of random noise that mostly cancels out. For the <em>most-common</em> subwords, that tend to carry any <em>unique meaning</em> because of the way word-roots/prefixes/suffixes hint at word meaning in English &amp; similar langauges, those very-common examples overpower the other noise, and ensure that slot, for at least one or more of its most-common subwords, carries at least some hint of the subword's implied meaning(s).</p>
<p>So when FastText assembles its final word-vector, by adding:</p>
<pre><code>word_vector_in[word_index['eating']]  # learned known-word vector
+ subword_vector_in[slot_hash('&lt;ea')]  # 1st 3-char subword
+ subword_vector_in[slot_hash('eat')]
+ subword_vector_in[slot_hash('ati')]
... # other 3-char subwords
... # every 4-char subword
... # other 5-char subwords
+ subword_vector_in[slot_hash('ting&gt;')]  # last 5-char subword
</code></pre>
<p>…it gets something that's dominated by the (likely stronger-in-magnitude) known full-word vector, with some useful hints of meaning also contributed by the (probably lower-magnitude) many noisy subword vectors.</p>
<p>And then if we were to imagine that some other word that's <em>not</em> part of the known 1-million word vocabulary comes along, say <code>'eatery'</code>, it has nothing from <code>word_vector_in</code> for the full word, but it can still do:</p>
<pre><code>subword_vector_in[slot_hash('&lt;ea')]  # 1st 3-char subword
+ subword_vector_in[slot_hash('eat')]  
+ subword_vector_in[slot_hash('ate')]
... # other 3-char subwords
... # every 4-char subword
... # other 5-char subwords
+ subword_vector_in[slot_hash('tery&gt;')]  # last 5-char subword
</code></pre>
<p>Because at least a few of those subwords likely include some meaningful hints of the meaning of the word <code>'eatery'</code> – especially meanings around <code>'eat'</code> or even the venue/vendor aspects of the suffix <em>-tery</em>, this synthesized guess for an out-of-vocabulary (OOV) word will be better than a random vector, &amp; often better than ignoring the word entirely in whatever upper-level process is using the FastText vectors.</p>
",2,-1,490,2022-12-14 12:18:45,https://stackoverflow.com/questions/74798182/subword-vector-in-fasttext
"Use three transformations (average, max, min) of pretrained embeddings to a single output layer in Pytorch","<p>I have developed a trivial Feed Forward neural network with Pytorch.</p>
<p>The neural network uses GloVe pre-trained embeddings in a freezed <code>nn.Embeddings</code> layer.</p>
<p>Next, the embedding layer splits into three embeddings. Each split is a different transformation applied to the initial embedding layer. Then the embeddings layer feed three <code>nn.Linear</code> layers. And finally I have a single output layer for a binary classification target.</p>
<p>The shape of the embedding tensor is [64,150,50]<br>
-&gt; 64: sentences in the batch,<br>
-&gt; 150: words per sentence,<br>
-&gt; 50: vector-size of a single word (pre-trained GloVe vector)</p>
<p>So after the transformation, the embedding layer splits into three layers with shape [64,50], where 50 = either the <code>torch.mean()</code>,  <code>torch.max()</code> or <code>torch.min()</code> of the 150 words per sentence.</p>
<p>My questions are:</p>
<ol>
<li><p>How could I feed the output layer from three different <code>nn.Linear</code> layers to predict a single target value [0,1].</p>
</li>
<li><p>Is this efficient and helpful to the total predictive power of the model? Or just selecting the average of the embeddings is sufficient and no improvement will be observed.</p>
</li>
</ol>
<p>The <code>forward()</code> method of my PyTorch model is:</p>
<pre class=""lang-py prettyprint-override""><code>  def forward(self, text):

    embedded = self.embedding(text)
    if self.use_pretrained_embeddings:
      embedded_average = torch.mean(embedded, dim=1)
      embedded_max = torch.max(embedded, dim=1)[0]
      embedded_min = torch.min(embedded, dim=1)[0]
    else:
      embedded = self.flatten_layer(embedded)

    input_layer = self.input_layer(embedded_average) #each Linear layer has the same value of hidden unit
    input_layer = self.activation(input_layer)

    input_layer_max = self.input_layer(embedded_max)
    input_layer_max = self.activation(input_layer_max)

    input_layer_min = self.input_layer(embedded_min)
    input_layer_min = self.activation(input_layer_min)
    
    #What should I do here? to exploit the weights of the 3 hidden layers
    output_layer = self.output_layer(input_layer)
    output_layer = self.activation_output(output_layer) #Sigmoid()
    
    return output_layer
</code></pre>
<p>After the proposed answer the function is:</p>
<pre class=""lang-py prettyprint-override""><code>  def forward(self, text):

    embedded = self.embedding(text)
    if self.use_pretrained_embeddings:
      embedded_average = torch.mean(embedded, dim=1)
      embedded_max = torch.max(embedded, dim=1)[0]
      embedded_min = torch.min(embedded, dim=1)[0]

      #use of average embeddings transformation
      input_layer_average = self.input_layer(embedded_average)
      input_layer_average = self.activation(input_layer_average)
      
      #use of max embeddings transformation
      input_layer_max = self.input_layer(embedded_max)
      input_layer_max = self.activation(input_layer_max)

      #use of min embeddings transformation
      input_layer_min = self.input_layer(embedded_min)
      input_layer_min = self.activation(input_layer_min)

    else:
      embedded = self.flatten_layer(embedded)

    input_layer = torch.concat([input_layer_average, input_layer_max, input_layer_min], dim=1)
    input_layer = self.activation(input_layer)

    print(&quot;3&quot;,input_layer.shape) #[192,1] vs [64,1] -&gt; output layer

    if self.n_layers !=0:
      for layer in self.layers:
          input_layer = layer(input_layer)

    output_layer = self.output_layer(input_layer)
    output_layer = self.activation_output(output_layer)
    
    return output_layer
</code></pre>
<p>This generates the following error:</p>
<blockquote>
<p>ValueError: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([192, 1])) is deprecated. Please ensure they have the same size.</p>
</blockquote>
<p>Expected outcome since the concatenated layer is 3x the size of the sentences (64). Any fix that could resolve it?</p>
","python, machine-learning, pytorch, neural-network, word-embedding","<p>Regarding 1: You can use <a href=""https://pytorch.org/docs/stable/generated/torch.concat.html"" rel=""nofollow noreferrer"">torch.concat</a> to concatenate the outputs along the appropriate dimension, and then e.g. map them to a single output using another linear layer.</p>
<p>Regarding 2: You will have to try it yourself and see whether this is useful.</p>
",2,1,530,2022-12-20 08:40:29,https://stackoverflow.com/questions/74860397/use-three-transformations-average-max-min-of-pretrained-embeddings-to-a-sing
How do I introduce a new subject without extra training in Stable Diffusion?,"<p>Suppose I have a dataset of 1000 pokemon, I have 10 images of Pikachu, 10 images of Bulbasaur, etc.
I also have a metadata specifying the name of each pokemon exactly. So from the metadata, I can know which image is Pikachu and which image is not.
I want to fine-tune a Stable Diffusion model to draw pokemons with prompt like this &quot;a drawing of [name]&quot; where name is the name of the pokemon I want to draw. It should be fine to draw any Pokemon with well-known name in the dataset. I should probably even be able to draw Donald Trump in the style of pokemon because the base model already knows about Donald Trump.</p>
<p>The problem is when I want to draw a completely new-made up pokemon, the model doesn't know its name. Let's say my Pokemon is called &quot;Megachu&quot; which is basically a thick Pikachu with red body and wings.
I want to introduce the model to Megachu by drawing Megachu myself and show the image to the model somehow. There are common ways of doing this which are Dreambooth, Textual Inversion, DreamArtist, etc but they all require me to train the model which takes long time and is costly.</p>
<p>So what I want is to somehow feed the model Pokemon embedding vectors so that the model knows how to draw any pokemon based on its embedding instead of its name.
Given a new Pokemon like Megachu, I want to just run the Megachu image through an embedding extraction process and feed the embedding to the model so that it can draw my Megachu.
I think it should be roughly similar to face embedding training process.</p>
<p>I am very new to Stable Diffusion architecture in general. Please suggest me a way</p>
<ol>
<li>to train the embedding vectors for Pokemon from 1000 images (preferably using existing weights from Stable Diffusion to assist the process so that it can be accurate with low amount of data). Which model layer should I modify? Should I just represent this vector as tokens?</li>
<li>to extract the Pokemon embedding from any image. Should the model that does the embedding be separated from the Diffusion model itself?</li>
</ol>
<p>I tried <a href=""https://huggingface.co/spaces/lambdalabs/stable-diffusion-image-variations"" rel=""nofollow noreferrer"">Stable Diffusion Variations</a> and it doesn't preserve the character. For example, if I give it Megachu, it will change my pokemon's color, change the wing shape, or body thickness.</p>
","deep-learning, computer-vision, embedding, word-embedding, stable-diffusion","<p>I have already studied Stable Diffusion architecture in detail so let me answer it myself:</p>
<ol>
<li>Train embedding models separately similar to the face embedding models or CLIP models. Then concatenate the embedding vector to every layer of UNet and train it.</li>
<li>Yes. Study the model architecture of Stable Diffusion and you will see how to inject this special embedding as guidance for image generation. It's called conditioning the model.</li>
</ol>
",0,-1,384,2022-12-23 23:09:26,https://stackoverflow.com/questions/74904651/how-do-i-introduce-a-new-subject-without-extra-training-in-stable-diffusion
&#39;Word2Vec&#39; object has no attribute &#39;infer_vector&#39;,"<p>This is the version of gensim I am using:</p>
<pre><code>Name: gensim
Version: 4.3.0
Summary: Python framework for fast Vector Space Modelling
Home-page: http://radimrehurek.com/gensim
Author: Radim Rehurek
Author-email: me@radimr
</code></pre>
<p>I want to convert sentences into vectors using <code>Word2Vec</code>. So is there any other method than <code>infer_vector</code> that converts a sentence into a vector. [Using <code>Word2Vec</code> is a compulsion]</p>
<p><strong>Current code:</strong></p>
<pre><code>In:clean_data[:3]
Out:[['good'],
 ['nice'],
 ['its',
  'ok',
  'but',
  'still',
  'not',
  'work',
  'some',
  'times',
  'please',
  'upgrade',
  'a',
  'valuable',
  'process']]
In:from gensim.models import Word2Vec

In:model= Word2Vec(clean_data, vector_size=100, min_count=2, sg=1)

In:model.train(clean_data,total_examples=model.corpus_count,epochs=model.epochs)

In:model.infer_vector(['its','ok','but','still','not','work','some','times','please','upgrade','a','valuable','process'])

</code></pre>
<p><strong>Error:</strong></p>
<pre><code>AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_11408/92733804.py in &lt;module&gt;
----&gt; 1 model.infer_vector(['its','ok','but','still','not','work','some','times','please','upgrade','a','valuable','process'])

AttributeError: 'Word2Vec' object has no attribute 'infer_vector'
</code></pre>
","python, vectorization, word2vec, word-embedding","<p><code>.infer_vector()</code> is only available on the <code>Doc2Vec</code> model, Its underlying algorithm, &quot;Paragraph Vectors&quot;, describes a standard way to learn fixed-length vectors associated with multi-word texts. The <code>Doc2Vec</code> class follows that algorithm, first during bulk training, than as an option in the frozen trained model via the <code>.infer_vector()</code> method.</p>
<p><code>Word2Vec</code>, on the other hand, is a model only for learning vectors for individual words. As an algorithm, word2vec says nothing about what a vector for a multi-word text should be.</p>
<p>Many people choose to use the average of all a multi-word text's individual words as a simple vector for the text as a whole. It's quick &amp; easy to calculate, but fairly limited in its power. Still, for some applications, especially broad topical-classifications that don't rely on any sort of grammatical/ordering understanding, such text-vectors work OK – especially as a starting baseline against which to compare additional techniques.</p>
<p>Gensim's <code>KeyedVectors</code> class, which is how the <code>Word2Vec</code> model stores its learned word-vectors inside its <code>.wv</code> property, has a utility method to help calculation the mean (aka average) of multiple word-vectors. Its documentation is here:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector</a></p>
<p>You could use it with a list-of-words like so:</p>
<pre><code>multiword_average_vector = model.wv.get_mean_vector([
        'its','ok','but','still','not','work','some',
        'times','please','upgrade','a','valuable','process'
    ])
</code></pre>
<p>Note that it will by default ignore any words not present in the model, but if you'd prefer it to raise an error, you can use the optional <code>ignore_missing=True</code> parameter.</p>
<p>Separately: note that tiny toy-sized uses of <code>Word2Vec</code> generally <em>won't</em> show any useful properties &amp; may mislead you about how the algorithm works on the larger datasets for which it is most valuable. You generally will want to train on corpuses of at least hundreds-of-thousands (if not millions) of words, to create vocabularies with at least tens-of-thousands of known words (each with many contrasting realistic usage examples in your training data), in order to see the real behavior/value of this algorithm.</p>
",0,0,560,2023-01-05 19:42:32,https://stackoverflow.com/questions/75023586/word2vec-object-has-no-attribute-infer-vector
Gensim Word2Vec produces different most_similar results through final epoch than end of training,"<p>I'm using gensim's Word2Vec for a recommendation-like task with part of my evaluation being the use of callbacks and the <code>most_similar()</code> method. However, I am noticing a huge disparity between the final few epoch callbacks and that of immediately post-training. In fact, the last epoch callback may often appear worthless, while the post training result is as best as could be desired.</p>
<p>My during-training tracking of most similar entries utilizes gensim's <code>CallbackAny2Vec</code> class. It follows the <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">doc example</a> fairly directly and roughly looks like:</p>
<pre><code>class EpochTracker(CallbackAny2Vec):

  def __init__(self):
    self.epoch = 0

  def on_epoch_begin(self, model):
    print(&quot;Epoch #{} start&quot;.format(self.epoch))

  def on_epoch_end(self, model):
    
    print('Some diagnostics')
    # Multiple terms used in the below
    e = model.wv
    print(e.most_similar(positive=['some term'])[0:3]) # grab the top 3 examples for some term

    print(&quot;Epoch #{} end&quot;.format(self.epoch))
    self.epoch += 1
</code></pre>
<p>As the epochs progress, the <code>most_similar()</code> results given by the callbacks to not seem to indicate an advancement of learning and seem erratic. In fact, often the callback from the first epoch shows the best result.</p>
<p>Counterintuitively, I also have an additional process (not shown) built into the callback that does indicate gradual learning. Following the similarity step, I take the current model's vectors and evaluate them against a down-stream task. In brief, this process is a sklearn <code>GridSearchCV</code> logistic regression check against some known labels.</p>
<p>I find that often the last <code>on_epoch_end</code> callback appears to be garbage. Or perhaps some multi-threading shenanigans. However, if directly after training the model I try the similarity call again:</p>
<pre><code>e = e_model.wv # e_model was the variable assignment of the model overall
print(e.most_similar(positive=['some term'])[0:3])
</code></pre>
<p>I tend to get beautiful results that are in agreement with the downstream evaluation task also used in the callbacks, or are at least vastly different than that of the final epoch end.</p>
<p>I suspect I am missing something painfully apparent or <code>most_similar()</code> has an unusual behavior with epoch-end callbacks. Is this a known issue or is my approach flawed?</p>
","nlp, gensim, word2vec, word-embedding","<p>What version of Gensim are you using?</p>
<p>In older versions – pre 4.0 if I remember correctly? – the <code>most_similar()</code> operation relies on a cached pre-computed set of unit-normalized word-vectors that in some cases will be frozen when you first try a <code>most_similar()</code>.</p>
<p>Thus, incremental updates to vectors won't be reflected in results, unless something happens to flush that cache - which happens at then <em>end</em> of training. But, since mid-training checks weren't an originally-envisioned usage, more-frequent flushing doesn't happen unless forced.</p>
<p>I think if you're sure to use the latest Gensim, the problem may go away - or reviewing this older project issue may provide ideas if you're stuck on an older version: <a href=""https://github.com/RaRe-Technologies/gensim/issues/2260"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2260</a></p>
<p>(Your other mid-training learning process – if it's accessing the non-normalized per-word vectors directly, rather than via <code>most_similar()</code> – is likely succeeding because it's skipping that normed-vector cache.)</p>
",1,0,122,2023-01-13 03:03:18,https://stackoverflow.com/questions/75104456/gensim-word2vec-produces-different-most-similar-results-through-final-epoch-than
Embedding version control of the weaviate,"<p>In <a href=""https://weaviate.io/"" rel=""nofollow noreferrer"">Weaviate</a>, the vector engine, I wonder how this can handle version issue of embedding model.</p>
<p>For instance, considering the (trained) word2vec model, embedded vectors from different models must be seperated.</p>
<p>One option might think is that make distinct multiple classes representing model version.<br />
Custom script may useful. If new model available, create new class and import accorded data. After that, change (GET) entrypoints (used for searching nearest vectors) to the new class.</p>
<p>Or maybe weaviate have other fancy way to handle this issue, but I couldn't find.</p>
","version-control, word-embedding, weaviate, approximate-nn-searching","<p>As at version 1.17.3, you have to manage this yourself because weaviate only supports one embedding per object.</p>
<p>There is a feature request to allow multiple embeddings per object <a href=""https://github.com/weaviate/weaviate/issues/2465"" rel=""nofollow noreferrer"">here</a>. But it sounds like your request is closer to this <a href=""https://github.com/weaviate/weaviate/issues/2570"" rel=""nofollow noreferrer"">one</a>. In any case, have a look at them and upvote the one that addresses your need so the engineering team can prioritize accordingly. Also, feel free to raise a new feature request if neither of these addresses your needs.</p>
",2,3,397,2023-02-14 06:22:45,https://stackoverflow.com/questions/75444138/embedding-version-control-of-the-weaviate
What does the embedding elements stand for in huggingFace bert model?,"<p>Prior to passing my tokens through encoder in BERT model, I would like to perform some processing on their embeddings. I extracted the embedding weight using:</p>
<pre><code>from transformers import TFBertModel

# Load a pre-trained BERT model
model = TFBertModel.from_pretrained('bert-base-uncased')

# Get the embedding layer of the model
embedding_layer = model.get_layer('bert').get_input_embeddings()

# Extract the embedding weights
embedding_weights = embedding_layer.get_weights()
</code></pre>
<p>I found it contains 5 elements as shown in Figure.
<a href=""https://i.sstatic.net/sSCfy.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>In my understanding, the first three elements are the word embedding weights, token type embedding weights, and positional embedding weights. My question is what does the last two elements stand for?</p>
<p>I dive deep into the source code of bert model. But I cannot figure out the meaning of the last two elements.</p>
","tensorflow, nlp, huggingface-transformers, bert-language-model, word-embedding","<p>In bert model, there is a post-processing of the embedding tensor that uses layer normalization followed by dropout ,
<a href=""https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L362"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L362</a></p>
<p>I think that those two arrays are the gamma and beta of the normalization layer, <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization</a>
They are learned parameters, and will span the axes of inputs specified in param &quot;axis&quot; which defaults to -1 (corresponding to 768 in embedding tensor).</p>
",0,1,353,2023-02-18 06:06:46,https://stackoverflow.com/questions/75491528/what-does-the-embedding-elements-stand-for-in-huggingface-bert-model
How does gensim calculate sentence embeddings when using a pretrained fasttext model?,"<p>According to <a href=""https://github.com/facebookresearch/fastText/issues/323#issuecomment-353167113"" rel=""nofollow noreferrer"">this</a> answer, sentence similarity for FastText is calculated with one of two ways (depending if the embeddings are created superviser or unsupervised)</p>
<ol>
<li>The mean of the normalized word vectors (unsupervised)</li>
<li>The mean of the word vectors (supervised)</li>
</ol>
<p>But I cannot make either of those give the same answer as the sentence embedding</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models import fasttext
import numpy as np

wv = fasttext.load_facebook_vectors(&quot;transtotag/cc.da.300.bin&quot;)

w1 = wv[&quot;til&quot;]
norm_w1 = np.linalg.norm(wv[&quot;til&quot;], ord=2)
s1 = w1/norm_w1

w2 = wv[&quot;skat&quot;]
norm_w2 = np.linalg.norm(wv[&quot;skat&quot;], ord=2)
s2 = w2/norm_w2

w3 = wv[&quot;til skat&quot;]

# Using &quot;raw&quot; embeddings
((w1+w2)/2-w3).max() #0.25
((w1+w2)-w3).max() # 0.5

# using normalized embeddings
((s1+s2)/2-w3).max() # 0.18
((s1+s2)-w3).max() # 0.37

</code></pre>
<p>I even tried to add the EOS (as stated in the answer) aswell</p>
<pre class=""lang-py prettyprint-override""><code>nl = wv[&quot;&lt;/s&gt;&quot;]
norm_nl = np.linalg.norm(wv[&quot;&lt;/s&gt;&quot;],2)
snl = nl/norm_nl

w3 = wv[&quot;til skat&quot;]

((s1+s2+snl)/3-w3).max() #0.12
</code></pre>
<p>If we look in the source code, then <code>wv[]</code> just returns <code>vstack([self.get_vector(key) for key in key_or_keys])</code> i.e it treats <code>til skat</code> a single word.</p>
<p>I cannot find anyting about how sentence embeddings are created in the docs aswell.</p>
","gensim, word-embedding, fasttext","<p>In Gensim, you should use <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastTextKeyedVectors.get_sentence_vector"" rel=""nofollow noreferrer"">get_sentence_vector</a> method, which was recently added.</p>
<p>Please read the docs and notice that this method expects a list of words specified by string or int ids.</p>
",0,0,899,2023-03-13 14:19:19,https://stackoverflow.com/questions/75723102/how-does-gensim-calculate-sentence-embeddings-when-using-a-pretrained-fasttext-m
Why does the loss of Gensim Word2Vec model deteriorate in every epoch?,"<p>I'm training a Word2vec model using Gensim Word2Vec on twitter data. The loss of the model deteriorates in every epoch. The first epoch gives the lowest loss. Why is it so? Code is shared below:</p>
<pre><code>loss_list = []
class callback(CallbackAny2Vec):
     
    def __init__(self):
        self.epoch = 0
          
    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_list.append(loss)
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch = self.epoch + 1

model = Word2Vec(df['tweet_text'], vector_size=300, window=10, epochs=30, hs=0, negative = 1, compute_loss=True, callbacks=[callback()])
embedding_size = model.wv.vectors.shape[1]
print(&quot;embedding size---&gt;&quot;, embedding_size)
vocab = model.wv.index_to_key
print(&quot;minimum loss {} at epoch {}&quot;.format(min(loss_list), loss_list.index(min(loss_list))))
</code></pre>
<p>The output is:</p>
<pre><code>Loss after epoch 0: 527066.375
Loss after epoch 1: 1038087.0625
Loss after epoch 2: 1510719.75
Loss after epoch 3: 1936163.875
Loss after epoch 4: 2364015.5
Loss after epoch 5: 2779299.75
Loss after epoch 6: 3183956.25
Loss after epoch 7: 3570054.5
Loss after epoch 8: 3966524.75
Loss after epoch 9: 4335994.5
Loss after epoch 10: 4706316.0
Loss after epoch 11: 5046213.0
Loss after epoch 12: 5410604.5
Loss after epoch 13: 5754962.0
Loss after epoch 14: 6080469.0
Loss after epoch 15: 6428622.5
Loss after epoch 16: 6771707.0
Loss after epoch 17: 7105302.0
Loss after epoch 18: 7400089.0
Loss after epoch 19: 7732032.0
Loss after epoch 20: 8059942.5
Loss after epoch 21: 8408386.0
Loss after epoch 22: 8685176.0
Loss after epoch 23: 8959723.0
Loss after epoch 24: 9242788.0
Loss after epoch 25: 9506676.0
Loss after epoch 26: 9752588.0
Loss after epoch 27: 10013168.0
Loss after epoch 28: 10288152.0
Loss after epoch 29: 10550915.0
embedding size---&gt; 300
minimum loss 527066.375 at epoch 0
</code></pre>
","nlp, gensim, word2vec, word-embedding","<p>Unfortunately, the code which totals-up loss for reporting in the Gensim <code>Word2Vec</code> model has a number of known bugs &amp; deviations from reasonable user expectations. You can see an overview of the problems, with links to a number of more-specific bugs, in the project's bug tracking issue [#2617][1].</p>
<p>Among other problems, the default loss reported is a running tally across all epochs – you'd have to do extra comparisons, or resets-to-<code>0.0</code>, to get per-epoch loss. And, insufficient precision in the running tally variables means other inaccuracies that become noticeable in large epochs or large runs.</p>
<p>These bugs <em>don't</em> affect the effectiveness of training, only the accuracy of <code>get_latest_training_loss()</code>. reporting.</p>
<p>Manually resetting the internal tally to <code>0.0</code> at the start of each epoch, from your own callback, may improve the reporting enough for your purposes, if your jobs aren't especially large.</p>
<p>However, other things to note about your apparent setup:</p>
<ul>
<li><p>Keep in mind that a full epoch's loss can hint about whether more SGD training will be beneficial on the model's internal training goals, but is <em>not</em> a reliable indicator of the quality of the final word-vectors for other downstream uses. A model with more loss might give better vectors, a model with less loss might (through overfitting) give word-vector that are less generally-useful for typical purposes. So don't rely on loss as a guide to other meta-optimization, only the choice of <code>epochs</code>/<code>alpha</code> or potential early-stopping.</p>
</li>
<li><p><code>min_count=1</code> is essentially always a mistake with <code>Word2Vec</code>, giving you not just bad vectors for the words that only appear 1 (or a few) times, but also making the other word-vectors, for more common words, worse than they'd be with a more sensible <code>min_count</code> choice. This is especially the case if you truly have enough data to justify large <code>vector_size=300</code> vectors.</p>
</li>
<li><p>The atypical parameter <code>negative=1</code> is also almost certainly sub-optimal, and <code>window=10</code> is another deviation from defaults that will usually only make sense if you've got some repeatable quantitative quality evaluation that can assure you it's an improvement over the default.</p>
</li>
</ul>
",1,0,127,2023-04-01 18:14:07,https://stackoverflow.com/questions/75908046/why-does-the-loss-of-gensim-word2vec-model-deteriorate-in-every-epoch
How to store n-dimensional vector in Microsoft SQL Server?,"<p>I want to store a large <code>n</code>-dimensional vector (<em>e.g.</em> an <a href=""https://huggingface.co/blog/getting-started-with-embeddings"" rel=""noreferrer"">embedding vector</a>) in SQL Server as a piece of metadata associated with another row.</p>
<p>In this example, it will be a 384-dimensional vector, for example:</p>
<pre><code>[0.161391481757164,   -0.23294533789157867, -0.5648667216300964,  -0.3210797905921936,  -0.03274689242243767,  0.011770576238632202, -0.06612513959407806,
-0.14662186801433563, -0.17081189155578613,  0.2879514992237091,  -0.1932784765958786,   0.009713868610560894, 0.23330552875995636,   0.03551964834332466,
-0.20526213943958282,  0.06445703655481339, -0.3146169185638428,   0.5788811445236206,   0.09118294715881348, -0.0048667509108781815,-0.16503077745437622,
 0.25162017345428467, -0.36395764350891113, -0.34742429852485657,  0.0526515394449234,   0.08912508934736252,  0.48464590311050415,  -0.04224267974495888,
 0.32445403933525085, -0.6847451329231262,  -0.20959551632404327, -0.027657458558678627, 0.20439794659614563,  0.6859520077705383,   -0.4988805055618286,
-0.26204171776771545, -0.18842612206935883,  0.07067661732435226,  0.02633148804306984,  0.03182782977819443,  0.28935596346855164,  -0.0016041728667914867,
 0.14609676599502563, -0.36272501945495605,  0.10288259387016296, -0.3651926815509796,  -0.3823530375957489,   0.14052163064479828,   0.006418740376830101,
 0.11741586774587631, -0.6509529948234558,  -0.15997739136219025, -0.42837604880332947,  0.12351743131875992,  0.0485026054084301,    0.24820692837238312,
 0.46972623467445374, -0.47954055666923523, -0.5238635540008545,  -0.3543052673339844,   0.22626525163650513,  0.18406584858894348,   0.6463921070098877,
 0.11894208937883377, -0.07143554836511612,  0.004256516695022583, 0.10088140517473221,  0.3335645794868469,   0.16905969381332397,   0.056856121867895126,
 0.11355260014533997,  0.3708053231239319,  -0.7484591603279114,   0.17503942549228668, -0.3249044418334961,   0.5901510715484619,    0.41506800055503845,
 0.05852462351322174,  0.5119204521179199,   0.2750142216682434,  -0.2058306783437729,   0.8199670314788818,   0.16698679327964783,  -0.1572146713733673,
 0.014733579009771347 ,0.0168467964977026,   0.4688740372657776,  -0.07839230448007584,  0.49326324462890625, -0.29934313893318176,   0.21525822579860687,
 0.1396997570991516,  -0.3420834243297577,  -0.5197309851646423,   0.10842061042785645, -0.0338996984064579,   0.35846689343452454,  -0.1660442352294922,
 0.15579357743263245,  0.015674782916903496,-0.8510578870773315,  -0.07501569390296936, -0.1791406124830246,   0.14926102757453918,  -0.2269722819328308,
 0.42619261145591736,  0.09489753842353821, -0.13341256976127625,  0.3312526345252991,   0.22534190118312836,  0.0679713636636734,    0.17042726278305054,
 0.14300595223903656, -0.06654901057481766, -0.2170567661523819,  -0.454984188079834,   -0.5516679286956787,  -0.10752955824136734,  -0.05743071809411049,
 0.32108309864997864, -0.5445901155471802,  -0.43162357807159424,  0.08207866549491882,  0.0664522647857666,   0.4478979706764221,    0.2190810590982437,
-0.05722910910844803, -0.0932786613702774,   0.01758035272359848,  0.16166797280311584,  0.44004616141319275, -0.21601708233356476,   0.43121641874313354,
 0.32022470235824585, -0.014045504853129387,-0.24948528409004211, -0.4389941990375519,   0.3816317319869995,  -0.5687862038612366,    0.1088542640209198,
-0.403241366147995,    0.08174201846122742,  0.21350793540477753,  0.2396722435951233,   0.4973253607749939,   0.31202447414398193,  -0.5260801315307617,
-0.3351263403892517,  -0.04100760444998741,  0.6609364151954651,  -0.2047063261270523,   0.19385716319084167, -0.5661329627037048,   -0.27058693766593933,
-0.1637117713689804,   0.30641692876815796, -0.08894442766904831, -0.052735116332769394,-0.13839660584926605, -0.6741533875465393,    0.05569711700081825,
-0.04354270175099373,  0.20251914858818054,  0.24813368916511536,  0.1719648838043213,   0.26782000064849854,  0.3137670159339905,    0.18599936366081238,
 0.23953016102313995,  0.17769533395767212,  0.46293920278549194, -0.19122551381587982, -0.5595004558563232,   0.09755659103393555,   0.3125424385070801,
-0.5813230276107788,  -1.0698442459106445,  -0.09045401215553284, -0.08948248624801636, -0.051830895245075226,-0.0001317809073952958,-0.08400193601846695,
 0.25725823640823364, -0.10135184973478317,  0.07884480804204941,  0.2091679722070694,   0.3950233459472656,   0.2745698094367981,   -0.872776448726654,
-0.16590780019760132,  0.4308463931083679,  -0.24375642836093903, -0.02120584435760975,  0.05213866010308266, -0.19898287951946259,  -0.5506985187530518,
 0.40167248249053955,  0.1640072464942932,  -0.010167916305363178, 0.14038121700286865,  0.4958030879497528,  -0.7259818315505981,   -0.24387206137180328,
 0.08528701961040497,  0.03415993973612785, -0.16687284409999847,  0.3804749548435211,  -0.08561687171459198, -0.2752263844013214,    0.5883951783180237,
-0.3283255994319916,  -0.12724250555038452,  0.08751262724399567, -0.44206979870796204, -0.11079336702823639, -0.16302113234996796,   0.11022322624921799,
-0.09404750168323517, -0.256179541349411,    0.20473307371139526,  0.41829538345336914, -0.1095203086733818,   0.02342342585325241,  -0.18814104795455933,
-0.2540932893753052,   0.48397907614707947,  0.03593514859676361, -0.089835524559021,   -0.6478171944618225,  -0.1757517009973526,    0.0672023594379425,
 0.0695127546787262,  -0.6398074626922607,  -0.03958022966980934, -0.10351496934890747,  0.22433893382549286,  0.6756673455238342,   -0.2924160957336426,
 0.17503827810287476,  0.12915058434009552, -0.239552840590477,    0.15498916804790497, -0.4730042815208435,  -0.12289212644100189,  -0.004052990116178989,
 0.11593572050333023, -0.1965983510017395,   0.5210273265838623,  -0.18184830248355865,  0.2579534947872162,  -0.1920309066772461,   -0.389960378408432,
 0.04139290377497673, -0.11638019979000092, -0.10620912909507751, -0.5321099162101746,   0.13135096430778503, -0.07761876285076141,  -0.0830138698220253,
-0.01572849042713642,  0.31080499291419983, -0.41445496678352356,  0.1609737128019333,   0.5787453651428223,  -0.05459209159016609,   0.1318219006061554,
-0.06957206130027771,  0.15152350068092346, -0.07094550132751465, -0.196294367313385,    0.12644843757152557,  0.23419199883937836,   0.5845456719398499,
-0.19989481568336487, -0.19607964158058167, -0.19692276418209076, -0.08633144199848175, -0.004551170393824577, 0.09362921118736267,  -0.14167727530002594,
-0.14917594194412231,  0.31781134009361267,  0.18779256939888,     0.42154577374458313, -0.20578211545944214,  0.14142100512981415,  -0.5664211511611938,
 0.18177354335784912,  0.14776530861854553,  0.29254236817359924,  0.17831481993198395, -0.1894354224205017,  -0.2836195230484009,   -0.4065170884132385,
-0.14325398206710815,  0.17800962924957275,  0.7763587832450867,   0.5497004389762878,  -0.00946379080414772, -0.48568078875541687,  -0.022227048873901367,
-0.005903944373130798, 0.4351034462451935,   0.05010621249675751, -0.12799566984176636, -0.06675072759389877,  0.167253315448761,    -0.1653994619846344,
 0.21004730463027954,  0.2765181362628937,   0.5885812640190125,  -0.326379656791687,   -0.007390940561890602, 0.27159956097602844,  -0.043763305991888046,
-0.39229199290275574, -0.19412016868591309,  0.4250912666320801,   0.6105153560638428,  -0.06168382614850998, -0.5341082811355591,   -0.611929714679718,
 0.08125612139701843, -0.1779184639453888,   0.5319408774375916,  -0.23601730167865753,  0.22285249829292297, -0.32505497336387634,   0.2152460366487503,
 0.4679816663265228,   0.048206135630607605,-0.24099768698215485, -0.30208054184913635,  0.13667792081832886,  0.3552468717098236,   -0.12280546128749847,
-0.006191314198076725,-0.10851636528968811,  0.08330328017473221, -0.09545236080884933, -0.02249046228826046,  0.0003346469602547586,-0.12273653596639633,
-0.05594412609934807,  0.027804357931017876,-0.4045255482196808,  -0.18987023830413818, -0.0027474926318973303,0.30244430899620056,   0.2323288917541504,
-0.2729185223579407,   0.12836921215057373,  0.27967774868011475,  0.3031359016895294,   0.41273725032806396, -0.06173351779580116,   0.33845168352127075,
 0.26775869727134705, -0.2933143079280853,  -0.0485006645321846,   0.11777450144290924,  0.6205862760543823,  -0.07637807726860046,  -0.19466432929039001,
-0.3994691073894501,   0.15689416229724884, -0.11139731854200363, -0.2333720475435257,   0.2364773154258728,   0.30898618698120117,  -0.1263875812292099,
-0.231489360332489,    0.34536853432655334,  0.6001318097114563,  -0.44741731882095337,  0.07382357120513916, -0.019649405032396317, -0.1029537245631218,
 0.369470477104187,   -0.032077688723802567,-0.13972929120063782,  0.24549521505832672, -0.13091856241226196, -0.029257331043481827]
</code></pre>
<h2>Attempt#1 - n-dimensional vector → n-columns</h2>
<p>My first thought was to store the 384 <code>real</code> values in a separate table, with a key to the original row (vertical partitioning):</p>
<pre><code>CREATE TABLE Embeddings (
   RowGUID uniquedientifier NOT NULL PRIMARY KEY,
   f1 real NOT NULL, 
   f2 real NOT NULL,
   f3 real NOT NULL,
   f4 real NOT NULL,
   f5 real NOT NULL,
   f6 real NOT NULL,
   f7 real NOT NULL,
   f8 real NOT NULL,
   f9 real NOT NULL,
   f10 real NOT NULL,
   ...snip...
   f384 real NOT NULL)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>RowGUID</th>
<th>f1</th>
<th>f2</th>
<th>f3</th>
<th>f4</th>
<th>f5</th>
<th>f6</th>
<th>f7</th>
<th>...</th>
<th>f384</th>
</tr>
</thead>
<tbody>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>0.161391481757164</td>
<td>-0.23294533789157867</td>
<td>-0.5648667216300964</td>
<td>-0.3210797905921936</td>
<td>-0.03274689242243767</td>
<td>0.011770576238632202</td>
<td>-0.06612513959407806</td>
<td>...</td>
<td>-0.029257331043481827</td>
</tr>
</tbody>
</table>
</div>
<p>This...sorta...works. But it is unwieldy. Plus, my vectors today <em>happen</em> to be 385-dimensional; but they may soon be <a href=""https://platform.openai.com/docs/guides/embeddings/what-are-embeddings"" rel=""noreferrer"">1556-dimensional</a>, which exceeds the SQL Server maximum of <a href=""https://learn.microsoft.com/en-us/sql/sql-server/maximum-capacity-specifications-for-sql-server?view=sql-server-ver16"" rel=""noreferrer"">1,024 columns per table.</a></p>
<h2>Attempt#2 - n-dimensional → n-IEEE 32-bit floats → varbinary(n*4)</h2>
<p>The next idea was to pack the 4-byte (32-bit) floats into a <code>varbinary</code> column:</p>
<pre><code>CREATE TABLE Embeddings (
   RowGUID uniquedientifier NOT NULL PRIMARY KEY,
   PackedVector varbinary(1516) NOT NULL -- 384 floats * 4 bytes = 1540 bytes
)
</code></pre>
<pre><code>0x0000000100000002000000030000000400000005000000060000000700000008...0000017F
  \______/\______/\______/\______/\______/\______/\______/\______/   \______/
     f1      f2      f3      f4      f5      f6      f7      f8        f384
</code></pre>
<p>And then when it comes time to read each <code>Single</code>, use <code>SUBSTRING</code> to rip the 4-byte float out of the varbinary, and then convert it to a <code>real</code>:</p>
<pre><code>DECLARE @f1 real = CAST(SUBSTRING(PackedVector, 0*4, 4) AS real);
</code></pre>
<p>Except two down-sides:</p>
<ul>
<li><p><strong>Downside#1</strong>: You cannot convert a <a href=""https://learn.microsoft.com/en-us/sql/t-sql/data-types/data-type-conversion-database-engine?view=sql-server-ver16"" rel=""noreferrer""><code>binary(4)</code> to a <code>real</code></a> (even though you can convert a <code>real</code> to a <code>binary(4)</code>; just not the other way:</p>
<p><a href=""https://i.sstatic.net/Ti086.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Ti086.png"" alt=""enter image description here"" /></a></p>
<p>May be able to workarond it with <code>decimal</code> or <code>numeric</code>).</p>
</li>
<li><p><strong>Downside#2</strong>: The math of computing the euclidian distance between two vectors is conceptually valid:</p>
<pre><code>DECLARE @target VARBINARY(1536) -- packed 384-dimensional vector

SELECT TOP(10) RowGUID, SUM(POWER(CAST(SUBSTRING(Embedding, i*4+1, 4) AS real) - CAST(SUBSTRING(@target, i*4 + 1, 4) AS real), 2)) as distance
FROM Embeddings
CROSS APPLY (VALUES (0), (1), (2), ..., (383)) AS sequence(i) -- Fill in the values from 0 to 383
GROUP BY RowGUID
ORDER BY distance ASC
</code></pre>
<p>But that will be pretty poorly performing (even if issue #1 didn't exist).</p>
</li>
</ul>
<h2>Attempt 3 - Do what Joe Celko does</h2>
<p>Many years ago, someone on the Microsoft newsgroups had <a href=""https://microsoft.public.sqlserver.programming.narkive.com/G679BC53/best-way-to-store-vectors#post1"" rel=""noreferrer"">the same question</a>:</p>
<blockquote>
<p>Can anyone point me to a reference or discuss the best way to store a
vector of 120 to 480 numbers in the database? Rows seem to be out
since we would quickly top the billion row mark. A table with 480
columns is too unnormalized. A single varchar(max) column? This
seems the best answer for now unless there is a more efficiant way of
storing it.</p>
<p>Thanks for any help or opinions,</p>
</blockquote>
<p>And then <strong>--CELKO--</strong> responded:</p>
<blockquote>
<p>I think of a vector as a particular kind of mathematical structure and
you seem to be talking about a list of some kind. Vectors have a fixed
number of dimensions, etc. Here is a guess:</p>
<pre><code>CREATE TABLE Vectors (
   vector_id CHAR(3) NOT NULL, --whatever
   dim_nbr   INTEGER NOT NULL,
   CHECK (dim_nbr BETWEEN 1 AND 480),
   PRIMARY KEY (vector_id, dim_nbr),
   dim_val   INTEGER NOT NULL
);
</code></pre>
</blockquote>
<p>Making the values of a the vector into rows:</p>
<p><strong>Embeddings</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>RowGUID</th>
<th>dimNumber</th>
<th>dimValue</th>
</tr>
</thead>
<tbody>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>1</td>
<td>0.161391481757164</td>
</tr>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>2</td>
<td>-0.23294533789157867</td>
</tr>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>3</td>
<td>-0.5648667216300964</td>
</tr>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>4</td>
<td>-0.3210797905921936</td>
</tr>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>5</td>
<td>-0.03274689242243767</td>
</tr>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>6</td>
<td>0.011770576238632202</td>
</tr>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>7</td>
<td>-0.06612513959407806</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>6ba7b814-9dad-11d1-80b4-00c04fd430c8</td>
<td>384</td>
<td>-0.029257331043481827</td>
</tr>
</tbody>
</table>
</div>
<p>This is <strong>probably</strong> the best approach.</p>
<h2>Nothing better?</h2>
<p>Doesn't SQL Server has better support for vectors? I know there is GEOSPATIAL/GEOGRAPHY types, but i gather those only work for 2-dimensional vectors (e.g. lattuitude+logitude)? Can't they be <a href=""https://stackoverflow.com/a/1622964/12597"">abused to solve the problem?</a></p>
<p>And since the goal is to compute euclidian distance between two vectors, is there a data structure that does a better job of allowing math? (varchar? xml? json? varbinary? variant?)</p>
<h1>Bonus Reading</h1>
<ul>
<li><a href=""https://stackoverflow.com/questions/58719399/indexing-n-dimensional-vectors"">Indexing N-dimensional vectors</a></li>
<li>microsoft.public.sqlserver.programming: <a href=""https://microsoft.public.sqlserver.programming.narkive.com/G679BC53/best-way-to-store-vectors"" rel=""noreferrer"">Best way to store vectors?</a></li>
<li><a href=""https://huggingface.co/blog/getting-started-with-embeddings"" rel=""noreferrer"">🤗Getting Started With Embeddings</a></li>
<li><a href=""https://bdtechtalks.com/2023/05/01/customize-chatgpt-llm-embeddings/"" rel=""noreferrer"">How to customize LLMs like ChatGPT with your own data and documents</a></li>
<li>FAISS - Facebook open-source vector database: <a href=""https://github.com/facebookresearch/faiss"" rel=""noreferrer"">https://github.com/facebookresearch/faiss</a></li>
<li>Pinecone, an online vector database system: <a href=""https://www.pinecone.io/"" rel=""noreferrer"">https://www.pinecone.io/</a></li>
<li>Microsoft: <a href=""https://learn.microsoft.com/en-us/semantic-kernel/concepts-ai/vectordb"" rel=""noreferrer"">What is a Vector Database?</a> (<a href=""https://archive.ph/15kXF"" rel=""noreferrer"">https://archive.ph/15kXF</a>)</li>
<li>SQLServerCentral: <a href=""https://www.sqlservercentral.com/editorials/the-rise-of-vector-databases"" rel=""noreferrer"">The Rise of Vector Databases</a></li>
<li><a href=""https://stackoverflow.com/questions/68161189/optimized-approach-for-calculating-cosine-similarity-in-sql-server"">Optimized approach for calculating cosine similarity in SQL Server</a> <em>(tips for using cosine angle distane)</em></li>
</ul>
","arrays, sql-server, vector, word-embedding","<p>You may also try something like explained <a href=""https://devblogs.microsoft.com/azure-sql/vector-similarity-search-with-azure-sql-database-and-openai/"" rel=""noreferrer"">here</a></p>
<blockquote>
<p>There is no specific data type available to store a vector in Azure SQL database, but we can use some human ingenuity to realize that a vector is just a list of numbers. As a result, we can store a vector in a table very easily by creating a column to contain vector data. One row per vector element. We can then use a columnstore index to efficiently store and search for vectors.</p>
</blockquote>
<p>So create a table to hold the vectors:</p>
<pre><code>CREATE TABLE [dbo].[embeddings]
(
    [article_id] [int] NOT NULL,
    [vector_value_id] [int] NOT NULL,
    [vector_value] [float] NOT NULL
) 
</code></pre>
<p>And then we can use T-SQL to efficiently compute distances:</p>
<blockquote>
<p>On that table we can create a column store index to efficiently store and search for vectors. Then it is just a matter of calculating the distance between vectors to find the closest. Thanks to the internal optimization of the columnstore (that uses SIMD AVX-512 instructions to speed up vector operations) the distance calculation is extremely fast.</p>
<p>The most common distance is the cosine similarity, which can be calculated quite easily in SQL.</p>
</blockquote>
<pre><code>SELECT 
    SUM(a.value * b.value) / (  
        SQRT(SUM(a.value * a.value)) * SQRT(SUM(b.value * b.value))   
    ) AS cosine_similarity
FROM
    vectors_values
</code></pre>
<p>The SQL query is calculating the cosine similarity between two vectors, represented by the columns <code>a.value</code> and <code>b.value</code>. Cosine similarity is defined as follows:</p>
<p><img src=""https://latex.codecogs.com/svg.image?%7BCosine&amp;space;Similarity%7D=%5Cfrac%7B%7BA%5Ccdot&amp;space;B%7D%7D%7B%7B%7CA%7C%5Ctimes%7CB%7C%7D%7D"" alt=""LaTeX"" /></p>
<p>Where:</p>
<ul>
<li><img src=""https://latex.codecogs.com/svg.image?(A%5Ccdot&amp;space;B)"" alt=""LaTeX"" /> is the dot product of vectors <img src=""https://latex.codecogs.com/svg.image?(A)"" alt=""a"" /> and <img src=""https://latex.codecogs.com/svg.image?(B)"" alt=""asdf"" /></li>
<li>(|A|) and (|B|) are the magnitudes of vectors (A) and (B), respectively</li>
</ul>
<p>The important parts of the query are:</p>
<ol>
<li><code>SUM(a.value * b.value)</code>: Calculates the dot product of vectors (A) and (B).</li>
<li><code>SQRT(SUM(a.value * a.value))</code>: Calculates the magnitude of vector (A).</li>
<li><code>SQRT(SUM(b.value * b.value))</code>: Calculates the magnitude of vector (B).</li>
</ol>
<p>Finally, the entire expression divides the dot product by the product of the magnitudes to find the cosine similarity, which is returned as <code>cosine_similarity</code>.</p>
",6,8,6431,2023-05-03 18:44:21,https://stackoverflow.com/questions/76167117/how-to-store-n-dimensional-vector-in-microsoft-sql-server
What is the best way to save fastText word vectors in a dataframe as numeric values?,"<p><strong>How to save fastText word vectors in dataframe better in order to use them for further calculations?</strong></p>
<p>Hello everyone!</p>
<p>I have a question about <em>fastText</em> word vectors, namely, I'd like to know, how to save them in my <em>dataframe</em> as vectors, but not objects. I want the column with word vectors be a numeric value as my next step is to calculate the average between different word forms.</p>
<p>Right now I use the following line to save word vectors into my dataframe:</p>
<pre><code>full_forms[&quot;word_vec&quot;] = full_forms.apply(lambda row: ft.get_word_vector(row[&quot;word_sg&quot;]), axis=1)
</code></pre>
<p>After getting word vectors I try to calculate the average but it does not work:</p>
<pre><code>full_forms[&quot;average&quot;] = full_forms.apply(lambda row: row[&quot;word_vec&quot;:&quot;word_vec_pl&quot;].mean(axis=0))
</code></pre>
<p>One of the ideas is to save word vectors to a <em>list</em>, then this list will be <em>numpy.ndarray</em>. But I am not sure, whether it is a good choice. I expect this array to have 300 dimentions as fastText word vectors have 300 dimentions, but, when I check the number of dim with arr.ndim attribute, I get 1. Shouldn't it be 300?</p>
<p>That's me first time asking for help here, so sorry if it is too messy.
Thank you for help in advance!
Have a nice day!
Ana</p>
","python, dataframe, vector, word-embedding, fasttext","<p>For further calculations, usually the best approach is to <em>not</em> move the vectors into a <code>DataFrame</code> at all - which brings up these sorts of type/size issues, and adds more indirection &amp; data-structure overhead from the <code>DataFrame</code>'s table/cells model.</p>
<p>Rather, leave them as the <code>numpy.ndarray</code> objects they are – either the individual 300-dimension arrays, or in some cases the giant <code>(number_of_words, vector_size)</code> matrix used by the FastText model itself to store all the words.</p>
<p>Using <code>numpy</code> functions directly on those will generally lead to the most concise &amp; efficient code, with the least memory overhead.</p>
<p>For example, if <code>word_list</code> is a Python list of the words whose vectors you want to average:</p>
<pre><code>average_vector = np.mean([ft.get_word_vector(word) for word in word_list], axis=0)
</code></pre>
",0,0,195,2023-05-21 12:41:57,https://stackoverflow.com/questions/76299956/what-is-the-best-way-to-save-fasttext-word-vectors-in-a-dataframe-as-numeric-val
Should I Pass Word2Vec and FastText Vectors Separately or Concatenate Them for Deep Learning Model in Smart Contract Vulnerability Detection?,"<p>i have been working with word embedding latly, i have a question. So, here consider taking vulnerability detection in smart contract. So the input is smart contract files labeled with 0 or 1 stating vulnerable or not. Now i m performing 2 different word embedding such as word2vec and fasttext with same input. My question is, it is right to concatenate the vectors of word2vec and fasttext and then fed as input to deep learning model. or should i pass the vectors of the word embedding models separately to the deep learning model for feature extraction and then concatenate the extracted features for classification.</p>
<p>So far, i have performed word embedding using word2vec and passed the vectors obtained to cnn and performed fasttext and passed it to BiGRU model and concatenated the extracted features. My question is can i concatenate the vectors before performing feature extraction using deep learning ? But i m afraid that the concatenation of 2 word embedding models with same input will cause confusion ? that is the same input words will have two different vectors when concatenated. i m so confused. If anybody have insight kindly help. Thanks in advance.</p>
","python, deep-learning, word2vec, word-embedding, fasttext","<p>The generic answer for when you don't know which of multiple different ideas is better, youy try them each separately &amp; see which evaluates as better on your robust, repeatable evaluations.</p>
<p>(If you don't have a way to evaluate which is better, that's a bigger &amp; more foundational thing to address than any other choices.)</p>
<p>Given what you've said, other observations:</p>
<p>The word2vec &amp; FastText algorithms are very similar, with the most experience supporting their use being in the fuzzy sorts of menaings inherent in natural-language text. And, the main advantage of FastText is in being able to synthesize better-than-nothing guess-vectors for words that weren't seen during training, but might be similar in substrings that hint their meaning to other known words.</p>
<p>Smart contract source code (or bytecode) is sufficiently <em>unlike</em> natural language, in its narrow vocabulary, token frequencies, purposes, &amp; rigorous execution model that it's not immediately clear word-vectors could help. Word-vectors often <em>have</em> been useful with language-like token-sets that aren't natural-language, but even there, usually for discovering gradations of meaning. With smart contracts, the difference between &quot;Works as hoped&quot; and &quot;fatally vulnerable&quot; may just be a tiny matter of a single misplaced operation, or subtle missed error case. Those are the kind of highly contextual, ordering-based outcomes that word-vectors simply do not model. (At best, I think you might discover that competent coders tend to use mroe of certain kinds of operations or names than incompetent ones.)</p>
<p>Further, the main advantage of FastText – synthesizing vectors for unknown but morphologically-similar tokens – may be far less relevant for bytecode-analysis, where unknown tokens are rare or even impossible. (Maybe, if you're analyzing source-code including freely chosen variable names, new unknown variable names will have hints of relations to previously-trained names.)</p>
<p>So: word-vectors may the an improper or underpowered tool for doing the sort of high-stakes, subtle classification you're attempting. But, as with the topmost answer: the only way to know, &amp; test ideas of what works or not, is to try each approach, &amp; evaluate it in some fair, repeatable way. (This even includes testing different ways of training the word-vectors from a single algorithm like just word2vec itself: different modes, parameters, preprocessing, etc.)</p>
",0,-1,155,2023-05-23 09:14:30,https://stackoverflow.com/questions/76313091/should-i-pass-word2vec-and-fasttext-vectors-separately-or-concatenate-them-for-d
How embedding lookup works in Word2Vec,"<p>I am trying to understand Word2Vec. For word input of 5x1 (one hot encoding) and hidden layer of 3 units.I have came across following information from famous sources. The first (monochrome) image says the first column will become the embedding vector when multiplying  3x 5 to 5 x 1 matrix while in 2nd image, 4th row is taken against 1 x 5 one hot encoding. This is confusing. The embedding lookup pics row or column is not understandable. Please help.</p>
<p><a href=""https://i.sstatic.net/xiNeO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xiNeO.jpg"" alt=""enter image description here"" /></a></p>
","machine-learning, nlp, word2vec, word-embedding","<p>There's a lot of custom intros to the word2vec algorithm online that, in my opinion, are quite unhelpful in what they choose to highlight.</p>
<p>So, if you're struggling with a particular one, I'd suggest moving on to some other. And if you have to go someplace else (like StackOverflow) to explain some <em>external</em> writeup, try to provide a link to the full original source as context for understanding what that particular author has adopted as their mental-model.</p>
<p>Further, if your true interest is in understanding actual word2vec implementation, studying working source code may be a better path than more abstract write-ups, for seeing what actually happens.</p>
<p>Although abstractly, you <em>can</em> think of a word-vector lookup as being the multiplication of a one-hot vector times a (count-of-vocabulary x count-of-dimensions) matrix, I've not seen popular implementations (like Google's original <code>word2vec.c</code> release, or the Python Gensim library) do exactly that. So learning that form can mislead when later using, or reviewing the source code of, or implementing real code.</p>
<p>Instead, implementations tend to use the word-token as a key to lookup a row-number inside some sort of <code>dict</code>/hashtable. (No one-hot vector is ever created – except abstractly, in the sense that a simple <code>int</code> can be thought of as representing the one-hot vector with a single one at that int index.)</p>
<p>Then, they use that row-number to access the word's vector, from a matrix that's better considered the &quot;input weights&quot; that lead to a hidden-layer, rather than any &quot;hidden layer&quot; itself. (The &quot;hidden layer&quot; activations, at least in 1:1 modes like skip gram, is then that vector itself.)</p>
<p>That is: despite the abstract description, in implementations, no multiplication occurs. An index-lookup occurs, then a simple row-access-by-that-index occurs, and then you've got a word's vector. (And yes, it's the same result as-f there'd been a one-hot multiplication - at least in a simple skip-gram mode, where the 'input' to the network is a single context word.)</p>
<p>To try to map to that diagram's top (monochrome) half: you have a 5-word vocabulary, where each word has 3 dimensions. The <em>columns</em> of that <code>w00 .. w24</code> table, with 0-based indexes {0,1,2,3,4} are the individual word-vectors. (This varies from most implementations I know, where individual word-vectors are the <em>rows</em> of the model's matrix.)</p>
<p>So, per the top (monochrome) half, you get the 3-dimensional word-vector for the 1st of 5 words by pulling the <em>column</em> that's <code>w[0][0] .. w[2][0]</code>: rows 0-2, column 0</p>
<p>In contrast, per the bottom (multicolor) half, you get the 3-dimensional word-vector for the 4th of 5 words by pulling the <em>row</em> that's <code>cell[3][0] to cell[3][2]</code>: rows 3, columns 0-2</p>
<p>The bottom diagram better fits the implementations I know. There, learned word-vectors – both the in-progress vectors grabbed for adjustment during training, &amp; what's accessed at the end as final word-vectors – are more often stored as the <em>rows</em> of an &quot;input weights&quot; matrix.</p>
<p>That matrix <em>can be thought of</em> as a mapping from one-hot vectors to hidden-layer activations - but really isn't a &quot;hidden layer&quot; itself. And further: in a mode like &quot;CBOW with averaging&quot;, the actual &quot;hidden layer&quot; activations are an <em>average</em> of multiple rows' values.</p>
<p>Of your 2 diagrams, the bottom better represents usual implementations – though again, usually no actual &quot;multiplication by a one-hot&quot; occurs.</p>
<p>Hope this helps!</p>
",2,0,227,2023-06-14 09:39:53,https://stackoverflow.com/questions/76472136/how-embedding-lookup-works-in-word2vec
Chroma database embeddings = none when using get(),"<p>I am a brand new user of Chroma database (and the associate python libraries).</p>
<p>When I call <code>get</code> on a <code>collection</code>, embeddings is always <code>none</code>, even if embeddings are explicitly set/defined when adding documents to a collection (so it can't be an issue with generating the embeddings - I don't think).</p>
<p>For the following code (Python 3.10, chromadb 0.3.26), I expected to see a list of embeddings in the returned dictionary, but it is <code>none</code>.</p>
<pre><code>import chromadb

chroma_client = chromadb.Client()
collection = chroma_client.create_collection(name=&quot;my_collection&quot;)
collection.add(
    embeddings=[[1.2, 2.3, 4.5], [6.7, 8.2, 9.2]],
    documents=[&quot;This is a document&quot;, &quot;This is another document&quot;],
    metadatas=[{&quot;source&quot;: &quot;my_source&quot;}, {&quot;source&quot;: &quot;my_source&quot;}],
    ids=[&quot;id1&quot;, &quot;id2&quot;]
)

print(collection.get())
</code></pre>
<p>Output:</p>
<pre><code>{'ids': ['id1', 'id2'], 'embeddings': None, 'documents': ['This is a document', 'This is another document'], 'metadatas': [{'source': 'my_source'}, {'source': 'my_source'}]}
</code></pre>
<p>The same issue does not occur when using <code>query</code> instead of <code>get</code>:</p>
<pre><code>print(collection.query(query_embeddings=[[1.2, 2.3, 4.4]], include=[&quot;embeddings&quot;]))
</code></pre>
<p>Output:</p>
<pre><code>{'ids': [['id1', 'id2']], 'embeddings': [[[1.2, 2.3, 4.5], [6.7, 8.2, 9.2]]], 'documents': None, 'metadatas': None, 'distances': None}
</code></pre>
<p>The same issue occurs when using <code>langchain</code> wrappers.</p>
<p>Any ideas, friends? :-)</p>
","word-embedding, langchain, chromadb","<p>According to the documentation <a href=""https://docs.trychroma.com/usage-guide"" rel=""noreferrer"">https://docs.trychroma.com/usage-guide</a> embeddings are excluded by default for performance:</p>
<blockquote>
<p>When using get or query you can use the include parameter to specify which data you want returned - any of embeddings, documents, metadatas, and for query, distances. By default, Chroma will return the documents, metadatas and in the case of query, the distances of the results. embeddings are excluded by default for performance and the ids are always returned.</p>
</blockquote>
<p>You can include the embeddings when using <code>get</code> as followed:</p>
<pre><code>print(collection.get(include=['embeddings', 'documents', 'metadatas']))
</code></pre>
",23,10,12271,2023-06-15 13:27:36,https://stackoverflow.com/questions/76482987/chroma-database-embeddings-none-when-using-get
ML.Net stuck on pretrained model Fit() method,"<p>there is some code example which works with Pretrained model (link to whole example page <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.ml.textcatalog.applywordembedding?view=ml-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.ml.textcatalog.applywordembedding?view=ml-dotnet</a>):</p>
<pre><code>using System;
using System.Collections.Generic;
using Microsoft.ML;
using Microsoft.ML.Transforms.Text;

namespace Samples.Dynamic
{
    public static class ApplyWordEmbedding
    {
        public static void Example()
        {
        // Create a new ML context, for ML.NET operations. It can be used for
        // exception tracking and logging, as well as the source of randomness.
        var mlContext = new MLContext();

        // Create an empty list as the dataset. The 'ApplyWordEmbedding' does
        // not require training data as the estimator ('WordEmbeddingEstimator')
        // created by 'ApplyWordEmbedding' API is not a trainable estimator.
        // The empty list is only needed to pass input schema to the pipeline.
        var emptySamples = new List&lt;TextData&gt;();

        // Convert sample list to an empty IDataView.
        var emptyDataView = mlContext.Data.LoadFromEnumerable(emptySamples);

        // A pipeline for converting text into a 150-dimension embedding vector
        // using pretrained 'SentimentSpecificWordEmbedding' model. The
        // 'ApplyWordEmbedding' computes the minimum, average and maximum values
        // for each token's embedding vector. Tokens in 
        // 'SentimentSpecificWordEmbedding' model are represented as
        // 50 -dimension vector. Therefore, the output is of 150-dimension [min,
        // avg, max].
        //
        // The 'ApplyWordEmbedding' API requires vector of text as input.
        // The pipeline first normalizes and tokenizes text then applies word
        // embedding transformation.
        var textPipeline = mlContext.Transforms.Text.NormalizeText(&quot;Text&quot;)
            .Append(mlContext.Transforms.Text.TokenizeIntoWords(&quot;Tokens&quot;,
                &quot;Text&quot;))
            .Append(mlContext.Transforms.Text.ApplyWordEmbedding(&quot;Features&quot;,
                &quot;Tokens&quot;, WordEmbeddingEstimator.PretrainedModelKind
                .FastTextWikipedia300D));

        // Fit to data.
        var textTransformer = textPipeline.Fit(emptyDataView);

        // Create the prediction engine to get the embedding vector from the
        // input text/string.
        var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;TextData,
            TransformedTextData&gt;(textTransformer);

        // Call the prediction API to convert the text into embedding vector.
        var data = new TextData()
        {
            Text = &quot;This is a great product. I would &quot; +
            &quot;like to buy it again.&quot;
        };
        var prediction = predictionEngine.Predict(data);

        // Print the length of the embedding vector.
        Console.WriteLine($&quot;Number of Features: {prediction.Features.Length}&quot;);

        // Print the embedding vector.
        Console.Write(&quot;Features: &quot;);
        foreach (var f in prediction.Features)
            Console.Write($&quot;{f:F4} &quot;);

        //  Expected output:
        //   Number of Features: 150
        //   Features: -1.2489 0.2384 -1.3034 -0.9135 -3.4978 -0.1784 -1.3823 -0.3863 -2.5262 -0.8950 ...
    }

    private class TextData
    {
        public string Text { get; set; }
    }

    private class TransformedTextData : TextData
    {
        public float[] Features { get; set; }
    }
}
</code></pre>
<p>}</p>
<p>So, in my case if I use FastTextWikipedia300D OR Glove200D OR Glove 100D pretrained models there is a stuck process which not ends even after 10 mins while run there:</p>
<blockquote>
<p>var textTransformer = textPipeline.Fit(emptyDataView);</p>
</blockquote>
<p>I tried use this resolution: <a href=""https://stackoverflow.com/a/54561423/5168936"">https://stackoverflow.com/a/54561423/5168936</a>
But adding doesn't have any effect</p>
<blockquote>
<p>AppendCacheCheckpoint(mlContext)</p>
</blockquote>
<p>are there any ways to understand why this happened? or I use it wrong; I will be happy for any idea. thank you!</p>
<p>Microsoft.ML package version in Nuget is: 2.0.1</p>
","c#, vectorization, word-embedding, pre-trained-model, microsoft.ml","<p>I'm an idiot :)
to resolve this &quot;issue&quot; you should download word embeddings to your &quot;..AppData\Local\mlnet-resources\WordVectors&quot; folder;
as example adding downloaded file: glove.6B.300d.txt to that folder succeed the debug completely and Fit() works correctly without stuck process;
so, I close this question with my own answer</p>
",0,0,496,2023-06-28 16:16:08,https://stackoverflow.com/questions/76575021/ml-net-stuck-on-pretrained-model-fit-method
why nn.Embedding layer is used for positional encoding in bert?,"<p>In the huggingface implementation of <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L186"" rel=""nofollow noreferrer"">bert model</a>, for positional embedding nn.Embedding is used.
Why it is used instead of traditional sin/cos positional embedding described in the transformer paper? how this two things are same?</p>
<p>Also I am confused about the nn.Embedding layer? there are many word embedding like word2vec, glove. among them which is actually nn.Embedding layer? Can you please explain the inner structure of nn.Embedding in detail?
<a href=""https://stackoverflow.com/questions/75646273/what-is-the-difference-nn-embedding-and-nn-linear"">This question</a> also comes in my mind.</p>
","pytorch, nlp, huggingface-transformers, bert-language-model, word-embedding","<p><code>nn.Embedding</code> is just a table of vectors. Its input are indices to the table. Its output are the vectors associated to the indices from the input. Conceptually, it is equivalent to having one-hot vectors multiplied by a matrix, because the result is just the vector within the matrix selected by the one-hot input.</p>
<p>BERT is based on the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Transformer architecture</a>. The Transformer architecture needs positional information to be added to the normal tokens for it to distinguish where each token is at. The positional information in the original formulation of the Transformer architecture can be incorporated in 2 different ways (both with equal performance numbers):</p>
<ul>
<li>Static sinusoidal vectors: the positional embeddings are precomputed.</li>
<li>Trained positional embeddings: the positional embeddings are learned.</li>
</ul>
<p>The authors of the BERT article decided to go with trained positional embeddings. Anyway, in both cases the positional encodings are implemented with a normal embedding layer, where each vector of the table is associated with a different position in the input sequence.</p>
<p><strong>Update</strong>:</p>
<p>Positional embeddings are not essentially different from word embeddings. The only difference is how they are trained.</p>
<p>In word embeddings, you obtain the vectors so that they can be used to predict other words that appear close to the vector's word in the training data.</p>
<p>In positional embeddings, each vector of the table is associated with an index representing a token position, and you train the embeddings so that the vector associated with a specific position, when added to the token embedding at that position, is helpful for the task the model is trained on (masked LM for BERT, machine translation for the original Transformer).</p>
<p>Therefore, the positional embeddings end up with information that depends on the position, because the positional embedding vector is selected based on the position of the token it will be used for, and which has been trained to be useful for the task.</p>
<p>Later, the authors of the Transformer article discovered that they could simply devise a &quot;static&quot; (not trained) version of the embeddings (i.e. the sinusoidal embeddings), which reduced the total size of the model to be stored. In this case, the information in the precomputed positional vectors, together with the learned token embeddings is enough for the model to reach the same level of performance (in the machine translation task at least).</p>
",3,4,4693,2023-08-03 05:11:31,https://stackoverflow.com/questions/76825022/why-nn-embedding-layer-is-used-for-positional-encoding-in-bert
How to get Attentions Part from the output of a Bert model?,"<h4>I am using <code>Bert-Model</code> for  Query Expansion and I am trying to extract the keywords from the Document I have</h4>
<pre><code>tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
sentence=&quot;This is a sentence&quot;
tokens = tokenizer.tokenize(sentence)
print(tokens,&quot;-tokens&quot;)
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(\[input_ids\])
print(input_ids)
with torch.no_grad():
output = model(input_ids)
attention_scores = output.attentions
print(attention_scores)   #this prints None
</code></pre>
<p>this is my code I am using a Simple Sentence and trying to extract keywords from it In order to Do that I need attention of the output part of <code>Bert-Model</code></p>
<p>I tried with Different tokenizer methods(tokenize,encode,encode_plus) to tokenize and tried with
different bert variants (bert-large-uncased)</p>
<p>I want to extract the attention Part form the model output but I am not able to do that</p>
<p>I get None in that place I am not able to get any value in the attention part of  the output</p>
","python, nlp, huggingface-transformers, bert-language-model, word-embedding","<p>you can't really extract keywords from the code you provided. Your <code>output</code> has no attribute called <code>attentions</code>, that's why <code>output.attentions</code> is returning <code>None</code>, hence the error you're facing.</p>
<p>I benchmarked some transformer models for keyword extraction last year for my university project. I will provide you with a solution from there. This script will return you the extracted keywords with their score as a dictionary.</p>
<h4>Solution</h4>
<ul>
<li>Install the dependencies</li>
</ul>
<pre><code>pip install nltk
pip install spacy
pip install torch
pip install transformers
</code></pre>
<ul>
<li>Necessary imports</li>
</ul>
<pre><code>from transformers import AutoModel, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.stem.wordnet import WordNetLemmatizer
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
import spacy

nlp = spacy.load('en_core_web_sm')
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
</code></pre>
<ul>
<li>Utility functions</li>
</ul>
<pre><code>def pre_process(text):
    # lowercase
    text=text.lower()
    #remove tags
    text=re.sub(&quot;&amp;lt;/?.*?&amp;gt;&quot;,&quot; &amp;lt;&amp;gt; &quot;,text)
    # remove special characters and digits
    text=re.sub(&quot;(\\d|\\W)+&quot;,&quot; &quot;,text)
    ##Convert to list from string
    text = text.split()
    # remove stopwords
    text = [word for word in text if word not in stop_words]
    # remove words less than three letters
    text = [word for word in text if len(word) &gt;= 3]
    #lemmatize
    lmtzr = WordNetLemmatizer()
    text = [lmtzr.lemmatize(word) for word in text]
    return ' '.join(text)

def get_candidates(text):
    # group of words
    n_gram_range = (1, 1)

    # Extract candidate words/phrases
    count = CountVectorizer(ngram_range=n_gram_range, stop_words=&quot;english&quot;).fit([text])
    all_candidates = count.get_feature_names_out()
    
    doc = nlp(text)
    noun_phrases = set(chunk.text.strip() for chunk in doc.noun_chunks)

    # Only pass the Noun/Adjective keywords for extraction
    noun_adj = set()
    for token in doc:
        if token.pos_ == &quot;NOUN&quot;:
            noun_adj.add(token.text)
        if token.pos_ == &quot;ADJ&quot;:
            noun_adj.add(token.text)

    all_words = noun_adj.union(noun_phrases)
    candidates = list(filter(lambda candidate: candidate in all_words, all_candidates))
    return candidates
</code></pre>
<ul>
<li>Keyword Extraction Method</li>
</ul>
<pre><code># The keyword extraction method
def keyword_extract(string, model_name):

    # Obtaining candidate keywords and document representations
    model = AutoModel.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    text=pre_process(string)
    candidates=get_candidates(text)
    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=&quot;pt&quot;)
    candidate_embeddings = model(**candidate_tokens)[&quot;pooler_output&quot;]

    # Determination of keywords:
    text_tokens = tokenizer([text], padding=True, return_tensors=&quot;pt&quot;, truncation=True, max_length=512)
    text_embedding = model(**text_tokens)[&quot;pooler_output&quot;]

    candidate_embeddings = candidate_embeddings.detach().numpy()
    text_embedding = text_embedding.detach().numpy()

    distances = cosine_similarity(text_embedding, candidate_embeddings)
    keywords = {i:j for i,j in zip(candidates, distances[0])}

    # sort based on attention score and return the dictionary
    return dict(sorted(keywords.items(), key=lambda x:x[1], reverse=True))

# put the model name here
model_name = &quot;bert-base-uncased&quot;

my_sentence = &quot;Hello there! This is an example sentence and this is the code that I'm using to extract keywords from a sentence&quot;

keywords = keyword_extract(my_sentence, model_name)
print(keywords)
</code></pre>
<h4>Output</h4>
<p>Output returned the keywords sorted according to their score</p>
<pre><code>{'keywords': 0.97554314, 'example': 0.94142365, 'extract': 0.885766, 'code': 0.83722556, 'sentence': 0.76782566}
</code></pre>
",0,0,546,2023-08-26 11:00:44,https://stackoverflow.com/questions/76982659/how-to-get-attentions-part-from-the-output-of-a-bert-model
How to view Vespa Embedding?,"<p>I tried the following block of code to implement nearest neighbor search algorithm in Vespa.
<a href=""https://docs.vespa.ai/en/nearest-neighbor-search-guide.html"" rel=""nofollow noreferrer"">https://docs.vespa.ai/en/nearest-neighbor-search-guide.html</a>
I was able to run it successfully but was unable to identify where this Vector DB/Embedding is getting saved.</p>
<p>I want to query the embedding column, which I was unable to see, however I am able to see the Title, track and other columns.</p>
<p>![Here I was able to view title and artist (https://i.sstatic.net/pCBOx.png)</p>
<p><img src=""https://i.sstatic.net/1wrwc.png"" alt=""Here I was unable to see the Embedding"" /></p>
","nearest-neighbor, word-embedding, vespa, vector-database, semantic-search","<p>Just add &quot;summary&quot; to the indexing statement in your schema:</p>
<pre><code>field embedding type tensor&lt;float&gt;(x[384]) {
        indexing: input title | embed e5 | attribute | index | summary
</code></pre>
<p>You can configure different sets of fields to be returned for different queries by configuring multiple <a href=""https://docs.vespa.ai/en/document-summaries.html"" rel=""nofollow noreferrer"">document summaries</a>.</p>
",3,0,218,2023-09-11 13:59:18,https://stackoverflow.com/questions/77082292/how-to-view-vespa-embedding
How to get a progess bar for gensim.models.FastText.train()?,"<p>I have the following code to train a FastText embedding model.</p>
<pre><code>embed_model = FastText(vector_size=meta_hyper['vector_size'],
                       window=meta_hyper['window'],
                       alpha= meta_hyper['alpha'],
                       workers=meta_hyper['CPU'])

embed_model.build_vocab(data)

start = time.time()

embed_model.train(data, total_examples=len(data), epochs=meta_hyper['epochs'])
</code></pre>
<p>I have a fairly large dataset (some million tokens), and I need to understand, how close the model is to the end of training. What can I do?</p>
<p>I have tried to use tqdm and search in the official documentation, it did not help.</p>
","python, python-3.x, google-colaboratory, word-embedding, fasttext","<p>For the goal of achieving an accurate estimate of the time remaining, the easiest thing would be to enable logging at the INFO level. For example, a simple 2-liner that does this globally is:</p>
<pre class=""lang-py prettyprint-override""><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
</code></pre>
<p>Then, many Gensim functions (including <code>.train()</code>) will log their internal steps to the console, including progress reports during long operations.</p>
<p>As training is essentially a uniformly-costly operation through all ranges/epochs of your corpus, even just a few minutes' progress will generally be representative of the overall rate, and thus be sufficient to (manualy) project when a training-session will end.</p>
<p>For example, if it takes 5 minutes to get through 8% of your your 1st training-epoch, and you've requested 10 epochs, then training should complete in about (5 minutes * 100%/8% * 10 epochs =) 625 minutes.</p>
<p>(The main thing that might ruin such a linear-projection might be if your corpus is wildly-different in text-size or token-diversity in different ranges – for example, the first 8% of your corpus is all short docs with common words, while other ranges are all long docs with rarer words. But that'd also be bad for other reasons – the model optimization works best if the full variety of training data is equally mised throughout the full corpus. So if your data has any potential 'clumping' of texts by length, vocabulary, etc, a single pre-shuffle before training is helpful for both training efficiency and predictable progress.)</p>
<p>If there's too much logging, <code>.train()</code> has an optional <code>report_delay</code> parameter to specify the number of seconds to wait before each new progress-report.</p>
<p>Getting a true progress-bar leveraging other tools would be a bit trickier, as <code>.train()</code> generally needs a multiply-re-iterable Python sequence to run its configurable number of epochs, and the most straightforward use of <code>tqdm</code> expects to show progress over 1 single iterator's iteration. (It might be possible to hack-around with some corpus/parameter changes &amp; custom iterable-wrapping, but I'm not sure such an approach wouldn't hit other gotchas.)</p>
",3,0,427,2023-09-13 10:50:01,https://stackoverflow.com/questions/77096387/how-to-get-a-progess-bar-for-gensim-models-fasttext-train
Convert vertex ai text embedding response to vector representation,"<p>I am trying to generate a vector representation of a text to then put it into my search database to perform operations, such as semantic search or recommendations.</p>
<p>To do this I first use the vertex AI (<code>textembedding-gecko-multilingual@latest</code>) to request the data. Unfortunately, I am unable to convert this response to a readable array of floats.</p>
<pre class=""lang-js prettyprint-override""><code>...
// Predict request
  const [response] = await predictionServiceClient.predict(request);
  const predictions = response.predictions;
  console.log(&quot;\tPredictions:&quot;);
  for (const prediction of predictions) {
    console.log(`\t\tPrediction : ${JSON.stringify(prediction)}`);
  }
</code></pre>
<p>Which results in a quite unusable format, that I cannot put into my search database.
Note that there is the vector inside and each field is written as <code>{&quot;numberValue&quot;:-0.08402...,&quot;kind&quot;:&quot;numberValue&quot;}</code>:</p>
<pre><code>{&quot;predictions&quot;:[{&quot;structValue&quot;:{&quot;fields&quot;:{&quot;embeddings&quot;:{&quot;structValue&quot;:{&quot;fields&quot;:{&quot;statistics&quot;:{&quot;structValue&quot;:{&quot;fields&quot;:{&quot;token_count&quot;:{&quot;numberValue&quot;:1,&quot;kind&quot;:&quot;numberValue&quot;},&quot;truncated&quot;:{&quot;boolValue&quot;:false,&quot;kind&quot;:&quot;boolValue&quot;}}},&quot;kind&quot;:&quot;structValue&quot;},&quot;values&quot;:{&quot;listValue&quot;:{&quot;values&quot;:[{&quot;numberValue&quot;:0.016404684633016586,&quot;kind&quot;:&quot;numberValue&quot;},{&quot;numberValue&quot;:-0.08402395248413086,&quot;kind&quot;:&quot;numberValue&quot;} ....
</code></pre>
<p>Sure I could write a JS code to decode the response, but I believe this is not the best practice (as syntax might change). So, How can I retrieve the vector inside and make it readable - possibly with a function provided by the API?</p>
","node.js, word-embedding, google-cloud-vertex-ai","<p>I found the answer. In the node.js code they use a function to convert an. javascript object to an <code>ÌValue</code> with toValue(). You can use the same to decode the response of the prediction:</p>
<pre class=""lang-js prettyprint-override""><code>const aiplatform = require('@google-cloud/aiplatform');
const {PredictionServiceClient} = aiplatform.v1;
const {helpers} = aiplatform;


helpers.fromValue(prediction); // Note: this cannot be a list of predictions
</code></pre>
",2,0,712,2023-10-05 16:57:30,https://stackoverflow.com/questions/77239170/convert-vertex-ai-text-embedding-response-to-vector-representation
translation invariance of Rotary Embedding,"<p>RoPE (rotary position encoding), the positional encoding used in Llama, is a relative position encoding. The attention scores are bound to be decided by the <strong>relative distance</strong> between tokens only. Therefore, the position indices in the Llama model should have <strong>shift invariance</strong>.</p>
<p>But when I add all the position indices by a given number (e.g. 1000), the model's performance will actually be affected (which can be told from the performance difference on a downstream task). But why? Shouldn't it be <strong>shift invariance</strong>?</p>
","word-embedding, large-language-model, llama","<p>In theory, it should be shift invariant, but in practice it is very dependent on the numerical precision as it involves multiple nonlinear operations involving sines and cosines.</p>
<p>Lets look at a standard RoPE implementation in NumPy (unbatched for simplicity):</p>
<pre class=""lang-py prettyprint-override""><code>sequence_length = 5

positions = np.arange(sequence_length)

# Construct some dummy query and key arrays of shape [S, H].
hidden_size = 4
q = np.arange(hidden_size)[None].repeat(sequence_length, axis=0)
k = np.arange(hidden_size)[None].repeat(sequence_length, axis=0)

frequencies = 1e4 ** (np.arange(0, hidden_size, 2.0) / hidden_size)
inputs = positions[:, None] / frequencies[None, :]
sin, cos = np.sin(inputs), np.cos(inputs)

q1, q2 = np.split(q, 2, axis=-1)
q_rot = np.concatenate([q1 * cos - q2 * sin, q1 * sin + q2 * cos], axis=-1)

k1, k2 = np.split(k, 2, axis=-1)
k_rot = np.concatenate([k1 * cos - k2 * sin, k1 * sin + k2 * cos], axis=-1)

np.einsum('td,Td-&gt;tT', q_rot, k_rot)

&gt;&gt;&gt; array([[14.        , 12.16070923,  8.33341272],
          [12.16070923, 14.        , 12.16070923],
          [ 8.33341272, 12.16070923, 14.        ]])
</code></pre>
<p>The above code runs in double precision which is NumPy default and shifting positions (e.g. <code>positions = 100 + np.arange(sequence_length)</code>) gives the exact same result.</p>
<p>After reducing precision to float32, the differences are still negligible (within 1e-7), but going down to float16 the difference becomes relatively large (~1e-2).</p>
<p>Since large models such as LLaMA typically run with mixed precision, this would explain your observation.</p>
",1,3,156,2023-10-17 16:06:20,https://stackoverflow.com/questions/77310576/translation-invariance-of-rotary-embedding
How to Load the pre-trained word embeddings in .npy files,"<p>I'm trying to use the word embeddings pre-trained in <a href=""https://nlp.stanford.edu/projects/histwords/"" rel=""nofollow noreferrer"">HistWords Project</a> by the Stanford NLP team. But when I ran the document <strong>example.py</strong> from the <a href=""https://github.com/williamleif/histwords"" rel=""nofollow noreferrer"">GitHub website</a>, there was an error: <strong>ModuleNotFoundError: No module named 'representations.sequentialembedding'</strong>.
How can I solve this problem?</p>
<p>I've installed the &quot;representations&quot; module, but it doesn't work. The pre-trained word embeddings are of &quot;.npy&quot; format, is there any Python-based method for uploading them?</p>
","python, nlp, word-embedding","<p>To set it up you can do the following in your shell:</p>
<pre><code>cd &lt;path you want to store your project&gt;
git clone https://github.com/williamleif/histwords.git
cd histwords

# ----- Set up Python2.7 -----
## Python2.7 via conda is quite easy
conda create -y -n &lt;env_name&gt; python=2.7
conda activate &lt;env_name&gt;

## Else install/locate a python 2 version
### Check python -V (maybe its python2.7)
### Maybe you have python2 or python2.7 executables
### On linux you could look for ls /usr/bin/python* 
python2 -m venv &lt;env_name&gt;
source &lt;env_name&gt;/bin/activate # if you use linux
bin/Scripts/activate # on Windows

# ---- Now with Python2.7 ----
pip install -r requirements.txt

# ---- Download &amp; Move your embedding from https://nlp.stanford.edu/projects/histwords/
# to embeddings/&lt;category&gt; subfolders

python examples.py
# Outputs the similarities
</code></pre>
",0,0,359,2023-12-13 07:34:20,https://stackoverflow.com/questions/77651770/how-to-load-the-pre-trained-word-embeddings-in-npy-files
Sentence Similarity between a phrase with 2-3 words and documents with multiple sentences,"<p><strong>What I want to achieve:</strong> I have thousands of documents (descriptions of incidents) and I would like to find the documents which match a phrase or are similar to the words in the phrase. An example, for an input phrase, &quot;electric vehicle&quot;, I would like to find all the documents that has any discussion related to anything happening with any type of electric vehicle or conveyance, the documents in the corpus might not have the word &quot;vehicle&quot;, but may have the specific vehicle type mentioned, like &quot;scooter&quot;, &quot;bicycle&quot;, &quot;hoverboard&quot; etc,. and document may have the word &quot;electrical&quot; or even something like &quot;lithium battery of a &quot;. So, from an input phrase like &quot;an electric vehicle&quot; or &quot;an electric automobile&quot; or &quot;vehicle powered by a lithium-ion battery&quot;, I need to find out all the documents that has related mentions to that term. But, I don't want to capture the documents with &quot;automobile&quot;, &quot;scooter&quot; that doesn't have any mention of &quot;electric&quot; or &quot;lithium-ion&quot;. So, from a phrase with 1 to 4 words, I must find matching documents containing anywhere from 2 to 100 words used for 1 to 7 sentences in each document.</p>
<p>And the list of input phrases (that are used to find matching documents) will vary, hence something like Siamese-networks or even training a classification model can't be done I suppose.
And the count of documents will also keep increasing by day and each of the document is independent of each other.</p>
<p><strong>Here's what I have done till now:</strong>
I have used <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">sentence-transformers</a> (tried the <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">pre-trained models</a>, <code>multi-qa-mpnet-base-dot-v1</code>, <code>all-MiniLM-L12-v2</code>, <code>all-MiniLM-L16-v2</code> and <code>all-mpnet-base-v2</code>), to get normalized embeddings for all the documents, then my input phrase. and then computed cosine-similarity between my input phrase's embeddings with all the documents, then get the top 20 sentences with highest values.</p>
<p>The matched documents were barely relevant. For ex, for input phrase &quot;an electrical vehicle&quot; matches documents, with highest cosine-similarity, containing nothing but the word &quot;electrical&quot;, followed by documents with only &quot;vehicle&quot;, then documents with only &quot;electrical vehicle&quot; or a bit more words or the same 2 words in different forms, followed by documents just a bit more words but having mentions only of &quot;vehicle&quot; without &quot;electrical&quot; and vice-versa. I presume, because of the less count of words in the input phrase.</p>
<p>How do I counter this and find documents that actually mention all the words in my input phrase instead of just using one word to find the matching documents?</p>
","nlp, word-embedding, sentence-similarity, sentence-transformers","<p>In general your approach so far seem sensible and you should see more relevant search results. I suggest these improvements:</p>
<ul>
<li>Use models for <strong>asymmetric semantic search</strong>. Have a look at <a href=""https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"" rel=""nofollow noreferrer"">this part of Sentence Transformer's documentation</a>. Here you'll find <a href=""https://www.sbert.net/docs/pretrained-models/msmarco-v3.html"" rel=""nofollow noreferrer"">a selection of MS Marco models</a> specifically trained for short queries (e.g. few words) and search for larger text passages (e.g. your documents). At the moment you seem to use suboptimal pretrained models.</li>
<li>Have a look at <strong>hybrid search</strong>. Combining lexical with semantic search might yield better results for your use case. <a href=""https://weaviate.io/developers/weaviate/search/hybrid"" rel=""nofollow noreferrer"">Weaviate has a nice implementation</a> that you quickly can whip up and try out.</li>
</ul>
<p>You could also provide a minimal reproducible example. This might help to give more detailed recommendations.</p>
",2,1,300,2023-12-14 15:09:37,https://stackoverflow.com/questions/77661152/sentence-similarity-between-a-phrase-with-2-3-words-and-documents-with-multiple
Efficient many-to-many embedding comparisons,"<p>I am trying to recommend a user the top &quot;articles&quot; given embeddings of &quot;interests&quot; they have.</p>
<p>Each &quot;user&quot; will have 5-10 embeddings associated with their profile, represented as arrays of doubles.</p>
<p>Each &quot;article&quot; will also have 5-10 embeddings associated with it (each embedding represents a distinct topic).</p>
<p>I want to write a PostgreSQL query that returns the top 20 &quot;articles&quot; that are most aligned to a users interests. Since each user 5-10 embeddings representing their interests and each article has 5-10 embeddings representing the content it covers, I can't trivially apply an extension like <code>pgvector</code> to solve this issue.</p>
<p>I wrote an algorithm in SQL where I compute pairwise similarities between user embeddings and article embeddings and then take a max along each row and then average those values. It helps to imagine a UxT matrix (where U represents number of user embeddings and T is article embeddings) and fill each entry by the cosine similarity between the corresponding user embedding and articles embedding.</p>
<p>I wrote helper functions to compute products between two arrays, another to compute cosine similarity, and a third to compute &quot;vectors_similarity&quot; -- which computes the similarity between a set of user vectors and a set of article vectors.</p>
<p>The query itself applies a few joins to get the required information, filters out for articles in the last ten days and articles that have already been &quot;read&quot; by the user, and returns the top 20 most similar articles using this methodology.</p>
<p><strong>This takes over 30sec to search through 1000 articles.</strong> I am NOT a SQL expert, and am struggling to debug this. Below, I have posted my SQL query and the results of &quot;explain analysis.</p>
<p>Is this just computationally intractable, or am I missing some obvious optimization opportunities?</p>
<pre><code>CREATE OR REPLACE FUNCTION array_product(arr double precision[])
RETURNS double precision AS
$$
DECLARE
    result double precision := 1;
    i integer;
BEGIN
    FOR i IN array_lower(arr, 1) .. array_upper(arr, 1)
    LOOP
        result := result * arr[i];
    END LOOP;
    RETURN result;
END;
$$
LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION cosine_similarity(a double precision[], b double precision[])
RETURNS double precision AS $$
DECLARE
    dot_product double precision;
    norm_a double precision;
    norm_b double precision;
    a_length int;
    b_length int;
BEGIN
    a_length := array_length(a, 1);
    b_length := array_length(b, 1);

    dot_product := 0;
    norm_a := 0;
    norm_b := 0;
    
    FOR i IN 1..a_length LOOP
        dot_product := dot_product + a[i] * b[i];
        norm_a := norm_a + a[i] * a[i];
        norm_b := norm_b + b[i] * b[i];
    END LOOP;

    norm_a := sqrt(norm_a);
    norm_b := sqrt(norm_b);

    IF norm_a = 0 OR norm_b = 0 THEN
        RETURN 0;
    ELSE
        RETURN dot_product / (norm_a * norm_b);
    END IF;
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION vectors_similarity(
    user_vectors FLOAT[][],
    article_vectors FLOAT[][]
) RETURNS FLOAT AS $$
DECLARE
    num_user_vectors INT;
    num_article_vectors INT;
    scores FLOAT[][];
    row_weights FLOAT[];
    row_values FLOAT[];
    col_weights FLOAT[];
    similarity FLOAT;
    article_vector FLOAT[][];
    user_vector FLOAT[][];
    i int;
    j int;
BEGIN
    num_user_vectors := array_length(user_vectors, 1);
    num_article_vectors := array_length(article_vectors, 1);

    scores := ARRAY(SELECT ARRAY(SELECT 0.0 FROM generate_series(1, num_article_vectors)) FROM generate_series(1, num_user_vectors));
    
    i := 1;
    FOREACH user_vector SLICE 1 IN ARRAY user_vectors
    LOOP
        j := 1;
        FOREACH article_vector SLICE 1 IN ARRAY article_vectors
        LOOP
            scores[i][j] := cosine_similarity(user_vector, article_vector);
            scores[i][j] := exp(scores[i][j] * 7);
        j := j+1;
        END LOOP;
    i := i + 1;
    END LOOP;
        
    SELECT 
      AVG(
        (SELECT MAX(row_val) FROM unnest(row_array) AS row_val)
      ) INTO similarity
    FROM 
      (
        SELECT scores[row_index][:] AS row_array
        FROM generate_series(1, array_length(scores, 1)) AS row_index
      ) AS subquery;
    
    RETURN similarity;
END;
$$ LANGUAGE plpgsql;

EXPLAIN ANALYZE
SELECT
        ART.*,
        vectors_similarity(array_agg(TOPIC.vector), ARRAY[ARRAY[ -0.0026961329858750105,0.004657252691686153, -0.011298391036689281, ...], ARRAY[...]]) AS similatory_score
    FROM
        article ART     
    JOIN
        article_topic ART_TOP ON ART.id = ART_TOP.article_id
    JOIN
        topic TOPIC ON ART_TOP.topic_id = TOPIC.id
    WHERE
        ART.date_published &gt; CURRENT_DATE - INTERVAL '5' DAY
        AND NOT EXISTS (
        SELECT 1
        FROM user_article_read USR_ART_READ
        WHERE USR_ART_READ.article_id = ART.id
        AND USR_ART_READ.profile_id = 1 -- :user_id to be inputted by the user with the actual user_id
        )
    GROUP BY
        ART.id
    ORDER BY
        similatory_score DESC, ART.date_published DESC, ART.id DESC
    LIMIT 20;
</code></pre>
<p>And here is the analysis:</p>
<pre><code>&quot;Limit  (cost=945.53..945.55 rows=5 width=518) (actual time=27873.197..27873.227 rows=5 loops=1)&quot;
&quot;  Output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type, (scaled_geometric_similarity_vectors(array_agg(topic.vector), '{{-0.0026961329858750105,...}, {...}, ...}'::double precision[])&quot;
&quot;              Group Key: art.id&quot;
&quot;              Batches: 25  Memory Usage: 8524kB  Disk Usage: 3400kB&quot;
&quot;              Buffers: shared hit=14535 read=19, temp read=401 written=750&quot;
&quot;              -&gt;  Hash Join  (cost=395.19..687.79 rows=4491 width=528) (actual time=6.746..20.875 rows=4638 loops=1)&quot;
&quot;                    Output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type, topic.vector&quot;
&quot;                    Inner Unique: true&quot;
&quot;                    Hash Cond: (art_top.topic_id = topic.id)&quot;
&quot;                    Buffers: shared hit=289&quot;
&quot;                    -&gt;  Hash Anti Join  (cost=202.53..483.33 rows=4491 width=518) (actual time=3.190..15.589 rows=4638 loops=1)&quot;
&quot;                          Output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type, art_top.topic_id&quot;
&quot;                          Hash Cond: (art.id = usr_art_read.article_id)&quot;
&quot;                          Buffers: shared hit=229&quot;
&quot;                          -&gt;  Hash Join  (cost=188.09..412.13 rows=4506 width=518) (actual time=3.106..14.853 rows=4638 loops=1)&quot;
&quot;                                Output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type, art_top.topic_id&quot;
&quot;                                Inner Unique: true&quot;
&quot;                                Hash Cond: (art_top.article_id = art.id)&quot;
&quot;                                Buffers: shared hit=224&quot;
&quot;                                -&gt;  Seq Scan on public.article_topic art_top  (cost=0.00..194.67 rows=11167 width=16) (actual time=0.018..7.589 rows=11178 loops=1)&quot;
&quot;                                      Output: art_top.id, art_top.created_timestamp, art_top.article_id, art_top.topic_id&quot;
&quot;                                      Buffers: shared hit=83&quot;
&quot;                                -&gt;  Hash  (cost=177.56..177.56 rows=843 width=510) (actual time=3.005..3.011 rows=818 loops=1)&quot;
&quot;                                      Output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type&quot;
&quot;                                      Buckets: 1024  Batches: 1  Memory Usage: 433kB&quot;
&quot;                                      Buffers: shared hit=141&quot;
&quot;                                      -&gt;  Seq Scan on public.article art  (cost=0.00..177.56 rows=843 width=510) (actual time=0.082..1.585 rows=818 loops=1)&quot;
&quot;                                            Output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type&quot;
&quot;                                            Filter: (art.date_published &gt; (CURRENT_DATE - '5 days'::interval day))&quot;
&quot;                                            Rows Removed by Filter: 1191&quot;
&quot;                                            Buffers: shared hit=141&quot;
&quot;                          -&gt;  Hash  (cost=14.35..14.35 rows=7 width=8) (actual time=0.052..0.052 rows=0 loops=1)&quot;
&quot;                                Output: usr_art_read.article_id&quot;
&quot;                                Buckets: 1024  Batches: 1  Memory Usage: 8kB&quot;
&quot;                                Buffers: shared hit=5&quot;
&quot;                                -&gt;  Bitmap Heap Scan on public.user_article_read usr_art_read  (cost=4.21..14.35 rows=7 width=8) (actual time=0.051..0.052 rows=0 loops=1)&quot;
&quot;                                      Output: usr_art_read.article_id&quot;
&quot;                                      Recheck Cond: (usr_art_read.profile_id = 1)&quot;
&quot;                                      Buffers: shared hit=5&quot;
&quot;                                      -&gt;  Bitmap Index Scan on user_article_read_profile_id_d4edd4f6  (cost=0.00..4.21 rows=7 width=0) (actual time=0.050..0.050 rows=0 loops=1)&quot;
&quot;                                            Index Cond: (usr_art_read.profile_id = 1)&quot;
&quot;                                            Buffers: shared hit=5&quot;
&quot;                    -&gt;  Hash  (cost=118.96..118.96 rows=5896 width=26) (actual time=3.436..3.440 rows=5918 loops=1)&quot;
&quot;                          Output: topic.vector, topic.id&quot;
&quot;                          Buckets: 8192  Batches: 1  Memory Usage: 434kB&quot;
&quot;                          Buffers: shared hit=60&quot;
&quot;                          -&gt;  Seq Scan on public.topic  (cost=0.00..118.96 rows=5896 width=26) (actual time=0.009..2.100 rows=5918 loops=1)&quot;
&quot;                                Output: topic.vector, topic.id&quot;
&quot;                                Buffers: shared hit=60&quot;
&quot;Planning:&quot;
&quot;  Buffers: shared hit=406 read=7&quot;
&quot;Planning Time: 52.507 ms&quot;
&quot;Execution Time: 27875.522 ms&quot;
</code></pre>
","sql, postgresql, search, sql-execution-plan, word-embedding","<p>Currently, almost all cost is accrued in the outer <code>SELECT</code>, and that is not due to sorting. It's the hugely expensive function <code>vectors_similarity()</code> which calls the nested function <code>cosine_similarity()</code> many times, and that nested function is as inefficient as the first.</p>
<p>(You also show the function <code>array_product()</code>, but that's unused in the query, so just a distraction. Also inefficient, btw.)</p>
<p>This part in your query plan indicates you need more <a href=""https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-WORK-MEM"" rel=""nofollow noreferrer""><code>work_mem</code></a>:</p>
<blockquote>
<pre><code>Memory Usage: 8,524kB Disk Usage: 3,400kB
</code></pre>
</blockquote>
<p>Indeed, your server seems to be at default settings, or else <code>EXPLAIN(ANALYZE, VERBOSE, BUFFERS, SETTINGS)</code> (like you claim to have used) would report custom settings. That won't do for a non-trivial workload.</p>
<p>I started out with &quot;currently&quot;, because this is only getting worse. You filter the past 5 days or your data, but don't have an index on <code>article.date_published</code>. Currently, almost half of your 2000 articles qualify, but that ratio is bound to change dramatically. Then you need an index on <code>article (date_published)</code>.</p>
<p>Also, your <code>LIMIT</code> is combined with an <code>ORDER BY</code> on the computed similarity. So there is no way around computing the similarity score for all qualifying rows. A small limit barely helps.<br />
(Aside: your query plan reports <code>rows=5</code> though there are more than enough candidate rows, which disagrees with <code>LIMIT 20</code> in your query.)</p>
<p>Your <strong>course of action</strong> should be:</p>
<p>1.) Optimize both &quot;similarity&quot; functions, most importantly <strong><code>cosine_similarity()</code></strong>.</p>
<p>2.) Reduce the number of rows for which to do the expensive calculation, maybe by pre-filtering rows with a much cheaper filter.</p>
<p>3.) Optimize server configuration.</p>
<p>4.) Optimize indexes.</p>
",0,1,213,2024-01-28 22:38:36,https://stackoverflow.com/questions/77896681/efficient-many-to-many-embedding-comparisons
Mapping embeddings to labels in PyTorch/Huggingface,"<p>I am currently working on a project where I am using a pre-trained transformer model to generate embeddings for DNA sequences (some have a '1' label and some have a '0' label). I'm trying to map these embeddings back to their corresponding labels in my dataset, but I'm encountering an IndexError when attempting to do so. I think it has to do with the fact that I am batching since I'm running out of memory.</p>
<p>Here is the code I'm working with:</p>
<pre><code>from datasets import Dataset
from transformers import AutoTokenizer, AutoModel
import torch

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;)
model = AutoModel.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;)

# Load the dataset
ds1 = Dataset.from_file('training.arrow') #this is already tokenized

# Convert tokenized sequences to tensor
inputs = torch.tensor(ds1['input_ids']).to(torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;))

# Reduce batch size
batch_size = 4

# Pass tokenized sequences through the model with reduced batch size
with torch.no_grad():
    outputs = model(input_ids=inputs[:batch_size], output_hidden_states=True)

# Extract embeddings
hidden_states = outputs.hidden_states
embeddings1 = hidden_states[-1]

</code></pre>
<p>Here is the information about the size of the output embeddings and the original dataset:</p>
<pre><code>embeddings1.shape
torch.Size([4, 86, 1280])


ds1
Dataset({
    features: ['labels', 'input_ids', 'attention_mask'],
    num_rows: 22535512
})
</code></pre>
<p>I'm having a hard time figuring out how to map the labels back to the output embeddings, especially with the big discrepancy with the sizes. As you can see, I have 22million sequences, I would like a an embedding for each sequence.</p>
<p>My plan is to use these embeddings for downstream prediction using another model.
I have already split my data into train, test, and val, but does it make more sense to get the embeddings for a label1 dataset and label0 dataset and then combine and then split into train/test, so I don't have to worry about the mapping of the labels?</p>
","python, tensorflow, pytorch, huggingface-transformers, word-embedding","<p>You can use the <strong>.map</strong> function in the dataset to append the embeddings. I suggest you run this on <strong>GPU</strong> instead of <strong>CPU</strong> since nos of rows is very high.</p>
<p>Please try running the code below.</p>
<pre><code>import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModel

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;CPU&quot;)

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;)
model = AutoModel.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;, device_map = device)

# Load the dataset
ds = Dataset.from_file('training.arrow') #this is already tokenized

# Convert tokenized sequences to tensor
inputs = torch.tensor(ds['input_ids']).to(device)

# Reduce batch size
batch_size = 4

def get_embeddings(data):

    # Convert tokenized sequences to tensor
    input_ids =  torch.tensor(data['input_ids']).to(device)

    # Pass tokenized sequences through the model with reduced batch size
    with torch.no_grad():
        outputs = model(input_ids, output_hidden_states=True)
    
    hidden_states = outputs.hidden_states
    embeddings = hidden_states[-1]

    return {'embeddings' : embeddings.detach().cpu()}

# Extract embeddings
ds = ds.map(get_embeddings, batched=True, batch_size=batch_size)
ds
</code></pre>
",1,0,776,2024-02-04 17:06:58,https://stackoverflow.com/questions/77936766/mapping-embeddings-to-labels-in-pytorch-huggingface
How can I build an embedding encoder with FastAPI,"<p>I just want to use a pre-trained open source embedding model from SentenceTransformer for encoding plain text.</p>
<p>The goal is to use swagger as GUI - put in a sentence and get out embeddings.</p>
<pre><code>from fastapi import Depends, FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer(&quot;./assets/BAAI/bge-small-en&quot;)

app = FastAPI()

class EmbeddingRequest(BaseModel):
    text: str
    
class EmbeddingResponse(BaseModel):
    embeddings: float

@app.post(&quot;/embeddings&quot;, response_model=EmbeddingResponse)
async def get_embeddings(request: EmbeddingRequest, model: embedding_model):
    embeddings_result = model.encode(request.text)
    return EmbeddingResponse(embeddings=embeddings_result)
</code></pre>
","python, fastapi, word-embedding","<p>Note that I don't have access to your <code>&quot;./assets/BAAI/bge-small-en</code> model, so I used <code>all-mpnet-base-v2</code> instead.</p>
<p>That said, there are two issues with your implementation:</p>
<ol>
<li>You're trying to use the <code>model</code> as an input parameter which is not necessary. Just use the global <code>embedding_model</code> directly.</li>
<li>The return type for your <code>embeddings</code> is wrong. (Unless your model really outputs a single <code>float</code>). The output of <code>embedding_model.encode</code>is <code>np.ndarray</code>, which you can convert to a list using <code>embeddings_result.tolist()</code>.</li>
</ol>
<p>The following works for me:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import List

from fastapi import FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)

app = FastAPI()


class EmbeddingRequest(BaseModel):
    text: str


class EmbeddingResponse(BaseModel):
    embeddings: List[float]


@app.post(&quot;/embeddings&quot;, response_model=EmbeddingResponse)
async def get_embeddings(request: EmbeddingRequest):
    embeddings_result = embedding_model.encode(request.text)
    return EmbeddingResponse(embeddings=embeddings_result.tolist())

</code></pre>
",1,0,887,2024-02-13 10:42:23,https://stackoverflow.com/questions/77987277/how-can-i-build-an-embedding-encoder-with-fastapi
ModuleNotFoundError: No module named &#39;llama_index.embeddings.langchain&#39;,"<p>I am trying to use LangChain embeddings, using the following code in Google colab:</p>
<p>These are the installations:</p>
<pre><code>pip install pypdf
pip install -q transformers einops accelerate langchain bitsandbytes
pip install install sentence_transformers
pip3 install llama-index --upgrade
pip install llama-index-llms-huggingface
huggingface-cli login
pip install -U llama-index-core llama-index-llms-openai llama-index-embeddings-openai


</code></pre>
<p>Then I ran this code in the google colab:</p>
<pre><code>from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.prompts.prompts import SimpleInputPrompt


# Reading pdf
documents=SimpleDirectoryReader(&quot;/content/sample_data/Data&quot;).load_data()

#Prompt
query_wrapper_prompt=SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;)

import torch
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={&quot;temperature&quot;: 0.0, &quot;do_sample&quot;: False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name=&quot;meta-llama/Llama-2-7b-chat-hf&quot;,
    model_name=&quot;meta-llama/Llama-2-7b-chat-hf&quot;,
    device_map=&quot;auto&quot;,
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={&quot;torch_dtype&quot;: torch.float16 , &quot;load_in_8bit&quot;:True}
)

# Embeddings
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index.core import ServiceContext
from llama_index.embeddings.langchain import LangchainEmbedding

embed_model=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-mpnet-base-v2&quot;))


</code></pre>
<p>Then I got this error:
<code>ModuleNotFoundError: No module named 'llama_index.embeddings.langchain'</code></p>
<p>I am using latest version of llama-index
Version: 0.10.26</p>
<p>Can someone suggest, how to resolve this error.</p>
","python, langchain, word-embedding, llama-index","<p>You have <code>pip install llama-index-embeddings-openai</code><br />
and <a href=""https://github.com/run-llama/llama_index"" rel=""nofollow noreferrer"">official documentation</a> has <code>pip install llama-index-embeddings-huggingface</code></p>
<p>And similar way you need</p>
<pre><code>pip install llama-index-embeddings-langchain
</code></pre>
",4,1,9082,2024-04-03 20:05:10,https://stackoverflow.com/questions/78270250/modulenotfounderror-no-module-named-llama-index-embeddings-langchain
Tensor size error when generating embeddings for documents using HuggingFace pre-trained models,"<p>I am trying to get document embeddings using pre-trained models in the HuggingFace Transformer library. The input is a document, the output is an embedding for this document using a pre-trained model. But I got an error as below and don't know how to fix it.</p>
<p>Code:</p>
<pre><code>from transformers import pipeline, AutoTokenizer, AutoModel
from transformers import RobertaTokenizer, RobertaModel
import fitz
from openpyxl import load_workbook
import os
from tqdm import tqdm

PRETRAIN_MODEL = 'distilbert-base-cased'
DIR = &quot;dataset&quot;

# Load and process the text
all_files = os.listdir(DIR)
pdf_texts = {}
for filename in all_files:
    if filename.lower().endswith('.pdf'):
        pdf_path = os.path.join(DIR, filename)
        with fitz.open(pdf_path) as doc:
            text_content = &quot;&quot;
            for page in doc:
                text_content += page.get_text()
            text = text_content.split(&quot;PUBLIC CONSULTATION&quot;)[0]
            project_code = os.path.splitext(filename)[0]
            pdf_texts[project_code] = text 

# Generate embeddings for the documents
tokenizer = AutoTokenizer.from_pretrained(PRETRAIN_MODEL)
model = AutoModel.from_pretrained(PRETRAIN_MODEL)
pipe = pipeline('feature-extraction', model=model, tokenizer=tokenizer)

embeddings = {}
for project_code, text in tqdm(pdf_texts.items(), desc=&quot;Generating embeddings&quot;, unit=&quot;doc&quot;):
    embedding = pipe(text, return_tensors=&quot;pt&quot;)
    embeddings[project_code] = embedding[0][0].numpy()
</code></pre>
<p>Error:</p>
<p>The error happens to the line <code>embedding = pipe(text, return_tensors=&quot;pt&quot;)</code>. The output is as follows:</p>
<pre><code>Generating embeddings:   0%|          | 0/58 [00:00&lt;?, ?doc/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3619 &gt; 512). Running this sequence through the model will result in indexing errors
Generating embeddings:   0%|          | 0/58 [00:00&lt;?, ?doc/s]
RuntimeError: The size of tensor a (3619) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>The input documents: <a href=""https://drive.google.com/file/d/17yFOR0dQ8UMbefFed5QPZUXqU0vzifUw/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/17yFOR0dQ8UMbefFed5QPZUXqU0vzifUw/view?usp=sharing</a></p>
<p>Thank you!</p>
","huggingface-transformers, large-language-model, word-embedding, huggingface, pre-trained-model","<p>The length of the <code>text</code> variable is 3619, while the pipeline accepts a maximum length of 512. You can fix this by splitting your text into chunks of 512, or you can use a model that accepts larger sequences.</p>
",1,1,169,2024-04-04 05:23:42,https://stackoverflow.com/questions/78271828/tensor-size-error-when-generating-embeddings-for-documents-using-huggingface-pre
Is it possible to fine-tune a pretrained word embedding model like vec2word?,"<p>I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data.</p>
<p>However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model.</p>
<p>This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?</p>
","python, nlp, artificial-intelligence, word2vec, word-embedding","<p>As has been pointed out <a href=""https://stackoverflow.com/questions/76161758/fine-tune-a-custom-word2vec-model-with-gensim-4?rq=2"">before</a>, there is no &quot;go-to&quot; way for fine-tuning Word2Vec type models.</p>
<p>I would suggest training your own model from scratch, combining your data with other available data from a similar domain. Word2vec models are fairly quick to train and this would probably give you the best results. If you do not need static word-level embeddings, I would recommend considering contextualized embeddings, for example through the use of <a href=""https://sbert.net/"" rel=""nofollow noreferrer"">sentence-transformers</a> or similar frameworks, which has a wide selection of already pre-trained models you can choose from. You can fine-tune these types of models on your specific data rather easily, and there are tons of resources online on how to do that.</p>
<p>For your use case, you can embed all the documents into dense vector representations using the abovementioned library, and then construct a searchable index over this semantic space. In order to match queries, all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product, often referred to as a MIPS search. An example library to take a look at would be <a href=""https://github.com/facebookresearch/faiss"" rel=""nofollow noreferrer"">faiss</a>.</p>
",1,0,1332,2024-04-12 17:57:56,https://stackoverflow.com/questions/78317989/is-it-possible-to-fine-tune-a-pretrained-word-embedding-model-like-vec2word
&quot;Deadline&quot; error when embedding video with Google Vertex AI multimodal embedding modal,"<p>I am currently using Vertex AI's Multimodal Embedding Model (<a href=""https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings"" rel=""nofollow noreferrer"">https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings</a>)</p>
<p>I was able to get the Image and Text examples running just fine using the Python SDK and POST, however, when I try to run the following Video example (from the above link), I get the following error</p>
<pre class=""lang-py prettyprint-override""><code>    import vertexai
    
    from vertexai.vision_models import MultiModalEmbeddingModel, Video
    
    project_id = 'project_name'
    location = 'us-central1'
    
    vertexai.init(project=project_id, location=location)
    
    # Document metadata
    video_path = 'gs://test-public-bucket-123/dog_jumping_short.mp4' # public small 1 MB 7 second video file
    description = 'dogs jumping'
    
    model = MultiModalEmbeddingModel.from_pretrained(&quot;multimodalembedding@001&quot;)
    
    video = Video.load_from_file(video_path)
    
    embeddings = model.get_embeddings(
        video=video
    )
    
    # Video Embeddings are segmented based on the video_segment_config.
    print(&quot;Video Embeddings:&quot;)
    for video_embedding in embeddings.video_embeddings:
        print(
            f&quot;Video Segment: {video_embedding.start_offset_sec} - {video_embedding.end_offset_sec}&quot;
        )
        print(f&quot;Embedding: {video_embedding.embedding}&quot;)
    
    print(f&quot;Text Embedding: {embeddings.text_embedding}&quot;)
</code></pre>
<p>The error I get when running this sample code is related to <code>Deadline</code></p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/me/Code/multi-modal-test/multi-modal/lib/python3.12/site-packages/google/api_core/grpc_helpers.py&quot;, line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/me/Code/multi-modal-test/multi-modal/lib/python3.12/site-packages/grpc/_channel.py&quot;, line 1181, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/me/Code/multi-modal-test/multi-modal/lib/python3.12/site-packages/grpc/_channel.py&quot;, line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:
        status = StatusCode.UNAUTHENTICATED
        details = &quot;Video embedding failed with the following error: Deadline&quot;
        debug_error_string = &quot;UNKNOWN:Error received from peer ipv4:142.250.81.234:443 {grpc_message:&quot;Video embedding failed with the following error: Deadline&quot;, grpc_status:16, created_time:&quot;2024-05-23T15:53:15.429704-04:00&quot;}&quot;
</code></pre>
<p>Note that I have been able to embed <code>contextual_text</code> and <code>image</code> just fine with the same authentication, so I am fairly certain this has nothing to do with authentication even though the error says so.</p>
<p>I have also tried to POST using the following cURL, but I get the same error response <code>Deadline</code></p>
<pre class=""lang-bash prettyprint-override""><code>    curl -X POST \ 
     -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \
     -H &quot;Content-Type: application/json; charset=utf-8&quot; \
     -d @request.json \
     &quot;https://us-central1-aiplatform.googleapis.com/v1/projects/multi/locations/us-central1/publishers/google/models/multimodalembedding@001:predict&quot;
</code></pre>
<p>I have the following APIs enabled (note that according to docs, I only need to have Vertex AI API enabled)</p>
<p><a href=""https://i.sstatic.net/mLZRMFFD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mLZRMFFD.png"" alt=""enter image description here"" /></a></p>
<p>These are my IAM permissions</p>
<pre><code>- members:
  - user:me@gmail.com
  role: roles/aiplatform.admin
- members:
  - user:me@gmail.com
  role: roles/aiplatform.user
- members:
  - user:me@gmail.com
  role: roles/ml.admin
- members:
  - user:me@gmail.com
  role: roles/owner
- members:
  - user:me@gmail.com
  role: roles/storage.admin
- members:
  - user:me@gmail.com
  role: roles/visionai.admin
etag: BwYaJsGtzOk=
version: 1
</code></pre>
<p>I am not using a Service Account, but a User Account.</p>
<p>Anyone know what I can do here?</p>
","google-cloud-platform, word-embedding, google-cloud-vertex-ai, video-embedding","<p>This might come from a discrepancy between the &quot;current project&quot; of gcloud, and the &quot;Application Defaults Credentials&quot;. This can be confusing because the exact order of these commands matter.</p>
<p>To make sure, could you try in this order:</p>
<p>1.</p>
<pre><code>gcloud config set project MY_PROJECT_ID
</code></pre>
<p>(<a href=""https://cloud.google.com/sdk/gcloud/reference/config/set"" rel=""nofollow noreferrer"">doc</a>)</p>
<p>2.</p>
<pre><code>gcloud auth application-default login
</code></pre>
<p>(<a href=""https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login"" rel=""nofollow noreferrer"">doc</a>)</p>
<ol start=""3"">
<li>Run your python program (or your cuRL command)</li>
</ol>
",0,2,469,2024-05-23 20:27:15,https://stackoverflow.com/questions/78525417/deadline-error-when-embedding-video-with-google-vertex-ai-multimodal-embedding
How to get multimodal embeddings from CLIP model?,"<p>I'm hoping to use CLIP to get a single embedding for rows of multimodal (image and text) data.</p>
<p>Say I have the following model:</p>
<pre><code>from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import torchvision.transforms as transforms

model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

def convert_image_data_to_tensor(image_data):
    return torch.tensor(image_data)

dataset = df[['image_data', 'text_data']].to_dict('records')

embeddings = []
for data in dataset:
    image_tensor = convert_image_data_to_tensor(data['image_data'])
    text = data['text_data']

    inputs = processor(text=text, images=image_tensor, return_tensors=True)
    with torch.no_grad():
        output = model(**inputs)
</code></pre>
<p>I want to get the embeddings calculated in <code>output</code>. I know that <code>output</code> has the addtributes <code>text_embeddings</code> and <code>image_embeddings</code>, but I'm not sure how they interact later on. If I want to get a single embedding for each record, should I just be concatenating these attributes together? Is there another attribute that combines the two in some other way?</p>
<p>These are the attributes stored in output:</p>
<pre><code>print(dir(output))

['__annotations__', '__class__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'image_embeds', 'items', 'keys', 'logits_per_image', 'logits_per_text', 'loss', 'move_to_end', 'pop', 'popitem', 'setdefault', 'text_embeds', 'text_model_output', 'to_tuple', 'update', 'values', 'vision_model_output']
</code></pre>
<p>Also, is there a way to specify the size of the embedding that CLIP outputs? Similar to how you can specify the embedding size in BERT configs?</p>
<p>Thanks in advance for any help here. Feel free to correct me if I'm misunderstanding anything critical here.</p>
","machine-learning, conv-neural-network, openai-api, large-language-model, word-embedding","<p>CLIP is trained such that the text and image embeddings are projected on to a shared latent space. In fact, image-text similarity is what the model is trained to optimise.</p>
<p>So a very typical use case of CLIP is to compare and match images and text based on similarity. In your case, you don't seem to be interested in any measure of similarity. You already have an image and the text and want some joint embedding representation. So concatenation of the two embeddings the way you described it is fine. An alternative would be take their mean (since they are in the same embedding space, it's fine to do this).</p>
<p>As for the size of the embedding, I don't think there is a way to change it as it's hardwired into the architecture of the model when it's trained. You can perhaps employ a dimensionality reduction technique, or fine tune the model after stacking another fully connected layer with the dimensionality of your choice.</p>
",0,0,941,2024-07-15 19:53:18,https://stackoverflow.com/questions/78751682/how-to-get-multimodal-embeddings-from-clip-model
Why am I not seeing any return results for my firestore vector embedding search?,"<p>I have made a composite index as per the <a href=""https://firebase.google.com/docs/firestore/vector-search"" rel=""nofollow noreferrer"">firebase docs</a></p>
<pre><code> gcloud alpha firestore indexes composite create \
--project=[Insert Project Name Here]
--collection-group=elements --query-scope=COLLECTION \
--field-config=vector-config='{&quot;dimension&quot;:&quot;1536&quot;,&quot;flat&quot;: &quot;{}&quot;}',field-path=embedding
</code></pre>
<p>That works fine and I can see it clearly under <code>gcloud firestore indexes composite describe [Insert Index Name Here]</code></p>
<p>Then I try to query the database using an onCall function.</p>
<pre><code>export const vectorSearch = functions.https.onCall(async (data, context) =&gt; {
  const { query, quantity } = data;

  if (!query) {
    throw new functions.https.HttpsError('failed-precondition', 'query is required');
  }

  if (!quantity) {
    throw new functions.https.HttpsError('failed-precondition', 'quantity is required');
  }

  try {
    console.log(`Creating embedding for ${query}`)
    const embeddingResponse = await openai.embeddings.create({
      model: &quot;text-embedding-3-small&quot;,
      input: query,
      encoding_format: &quot;float&quot;,
    });

    const embedding = embeddingResponse.data[0].embedding;
    console.log(`Embedding:`, embedding)

    const vectorQuery: VectorQuery = firestore.collection('elements').findNearest('embedding', FieldValue.vector(embedding), {
      limit: quantity,
      distanceMeasure: 'COSINE'
    });

    const snapshot: VectorQuerySnapshot = await vectorQuery.get();
    console.log(`Snapshot:`, snapshot)

    const formattedData: any = {};
    snapshot.docs.forEach((doc) =&gt; {
      formattedData[doc.id] = doc.data();
    });

    return { docs: formattedData };
  } catch (error) {
    console.error('Error querying database:', error);
    throw new functions.https.HttpsError('internal', 'Error querying database');
  }
});
</code></pre>
<h1>The Problem</h1>
<p>When I call this function, I get no entries despite having documents under the relevant collection. I don't get any errors - just no documents appear in the response. However I should be seeing the number of documents I specified in the <code>quantity</code> field. Why is this?</p>
","node.js, google-cloud-firestore, vector, openai-api, word-embedding","<p>I was storing each embedding field as an array in my firestore database.</p>
<p><code>[0.1, 0.2, 0.3]</code></p>
<p>I should have been storing each embedding as a type of <code>vector</code>.</p>
<p><code>FieldValue.vector([0.1, 0.2, 0.3])</code></p>
<p>Firebase ignored the fields that weren't a vector, so the index for the search was empty and therefore no documents were returned.</p>
",3,0,184,2024-08-23 09:21:46,https://stackoverflow.com/questions/78905188/why-am-i-not-seeing-any-return-results-for-my-firestore-vector-embedding-search
Can not load the safetensors huggingface model in DJL in Java,"<p>I tried a lot, but I want to read embeddings from the <a href=""https://huggingface.co/jinaai/jina-embeddings-v2-base-de/tree/main"" rel=""nofollow noreferrer"">jina embeddings</a></p>
<p>this is my java code:</p>
<pre class=""lang-java prettyprint-override""><code>public static float[] getTextEmbedding(String text) throws ModelNotFoundException, MalformedModelException, IOException, TranslateException {
        Criteria&lt;String, float[]&gt; criteria = Criteria.builder()
                .setTypes(String.class, float[].class)
                .optEngine(&quot;PyTorch&quot;)
                .optModelPath(Paths.get(&quot;/jina-embeddings-v2-base-de&quot;))
                .optTranslator(
                    TextEmbeddingTranslator
                        .builder(HuggingFaceTokenizer.newInstance(Paths.get(&quot;/jina-embeddings-v2-base-de&quot;)))
                        .build()
                )
                .optProgress(new ProgressBar())
                .optOption(&quot;trust_remote_code&quot;, &quot;true&quot;)
                .build();

        try (Predictor&lt;String, float[]&gt; predictor = ModelZoo.loadModel(criteria).newPredictor()) {
            return predictor.predict(text);
        }
    }
</code></pre>
<p>but I keept getting this error:</p>
<pre><code>java.io.FileNotFoundException: jina-embeddings-v2-base-de.pt file not found in: /jina-embeddings-v2-base-de
</code></pre>
","java, huggingface-transformers, word-embedding, huggingface-tokenizers, djl","<p><code>djl-convert</code> can convert local model into DJL format as well:</p>
<pre><code>djl-convert -m /jina-embeddings-v2-base-de
</code></pre>
<p>Then you can point to <code>model/jina-embeddings-v2-base-de</code> folder in Java code.
You will find the following files in the folder:</p>
<ul>
<li>jina-embeddings-v2-base-de.pt</li>
<li>config.json</li>
<li>serving.properties</li>
<li>tokenizer_config.json</li>
<li>tokenizer.json</li>
</ul>
",0,0,201,2024-11-16 00:04:27,https://stackoverflow.com/questions/79194273/can-not-load-the-safetensors-huggingface-model-in-djl-in-java
Do I fit t-SNE the two sets of embeddings from two different models at the same time or do I fit each separately then visualize and compare?,"<p>I have two sets of embeddings from two different GNNs. I want to compare the embeddings by visualization and I want to know which way is the most appropriate way for comparison. Do I fit t-SNE separately for each set of embedding then use the scatter plot? Or do I fit them all into one t-SNE?</p>
","graph, neural-network, visualization, word-embedding, tsne","<p>If you fit t-SNE separately for each embedding set, you’ll preserve the internal structure of each embedding space, as the optimization is independent. However, scatter plots from this method are not directly comparable because t-SNE outputs are non-deterministic and embedding spaces can be arbitrarily rotated, flipped, or scaled. While you can examine clusters in isolation, you won't gain insight into the relationship between the two embedding sets.</p>
<p>If you fit t-SNE on both embedding sets together, the embeddings are projected into a shared low-dimensional space. This allows direct visual comparison, making it easier to observe whether embeddings from both GNNs cluster similarly. However, this approach risks distorting the internal structure of individual embedding spaces, especially if the embeddings differ significantly. The t-SNE algorithm will balance the relationships between and within the two sets, potentially <strong>introducing artifacts</strong>.</p>
<p>The choice depends on your objective. For independent analysis of internal structures, fit t-SNE separately. For relational comparisons in a shared space, fit t-SNE jointly.</p>
<p>To complement the visualization, you'd better include quantitative metrics like cosine similarity or cluster purity to reinforce your observations, as visualizations should be treated as exploratory tools rather than conclusive evidence.</p>
",1,1,62,2024-11-29 20:03:37,https://stackoverflow.com/questions/79238246/do-i-fit-t-sne-the-two-sets-of-embeddings-from-two-different-models-at-the-same
Create native picture embeddings and then make a similarity search with text,"<p>Is it generally possible to create image embeddings directly (without additional text) and store them in a database? The aim is to make the content of the images findable later via a text input in the front end using a similarity search. Is this feasible?</p>
<p>In best case I dont want to use any OCR and natively embed the images.</p>
","python, openai-api, large-language-model, word-embedding","<p>Have you looked into multimodal embedding models?</p>
<p>A commercial option would be <a href=""https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html"" rel=""nofollow noreferrer"">Amazons Titan Multimodal Embeddings G1 model</a>. Another one is <a href=""https://docs.cohere.com/docs/multimodal-embeddings"" rel=""nofollow noreferrer"">Coheres Embed</a> which is multimodal too.</p>
<p>There are also Open Source options on Huggingface - see e.g. <a href=""https://huggingface.co/learn/cookbook/en/faiss_with_hf_datasets_and_clip"" rel=""nofollow noreferrer"">here</a>.</p>
",0,1,54,2025-01-02 12:50:32,https://stackoverflow.com/questions/79323765/create-native-picture-embeddings-and-then-make-a-similarity-search-with-text
