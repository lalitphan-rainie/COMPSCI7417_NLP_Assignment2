Methods for Geotagging or Geolabelling Text Content,"<p>What are some good algorithms for automatically labeling text with the city / region  or origin?  That is, if a blog is about New York, how can I tell programatically.  Are there packages / papers that claim to do this with any degree of certainty?  </p>

<p>I have looked at some tfidf based approaches, proper noun intersections, but so far, no spectacular successes, and I'd appreciate ideas!  </p>

<p>The more general question is about assigning texts to topics, given some list of topics.</p>

<p>Simple / naive approaches preferred to full on Bayesian approaches, but I'm open.</p>
","algorithm, statistics, nlp, named-entity-recognition","<p>You're looking for a <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Named_entity_recognition"" rel=""nofollow noreferrer"">named entity recognition</a> system, or short NER. There are <a href=""http://nltk.sourceforge.net/index.php/Main_Page"" rel=""nofollow noreferrer"">several</a> <a href=""http://opennlp.sourceforge.net/index.html"" rel=""nofollow noreferrer"">good</a> <a href=""http://alias-i.com/lingpipe"" rel=""nofollow noreferrer"">toolkits</a> available to help you out. LingPipe in particular has a very <a href=""http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html"" rel=""nofollow noreferrer"">decent tutorial</a>. <a href=""http://cageclass.sourceforge.net/"" rel=""nofollow noreferrer"">CAGEclass</a> seems to be oriented around NER on geographical place names, but I haven't used it yet.</p>
<p>If you're going with Java, I'd recommend using the LingPipe NER classes. OpenNLP also has some, but the former has a better documentation.</p>
<p>If you're looking for some theoretical background, <a href=""http://www.linguateca.pt/documentos/gkb_technical_report.pdf"" rel=""nofollow noreferrer"">Chavez et al. (2005)</a> have constructed an interesting system and documented it.</p>
",13,9,6591,2008-10-02 18:44:32,https://stackoverflow.com/questions/163923/methods-for-geotagging-or-geolabelling-text-content
Named Entity Recognition Libraries for Java,"<p>I am looking for a simple but ""good enough"" Named Entity Recognition library (and dictionary) for java, I am looking to process emails and documents and extract some ""basic information"" like:
Names, places, Address and Dates</p>

<p>I've been looking around, and most seems to be on the heavy side and full NLP kind of projects. </p>

<p>Any recommendations ?</p>
","java, nlp, named-entity-recognition","<p>BTW, I recently ran across <a href=""http://www.opencalais.com/"" rel=""nofollow noreferrer"">OpenCalais</a> which seems to havethe functionality I was looking after.</p>
",1,27,23784,2008-10-09 16:54:54,https://stackoverflow.com/questions/188176/named-entity-recognition-libraries-for-java
Algorithms recognizing physical address on a webpage,"<p>What are the best algorithms for recognizing structured data on an HTML page?</p>

<p>For example Google will recognize the address of home/company in an email, and offers a map to this address.</p>
","algorithm, screen-scraping, nlp, pattern-matching, named-entity-recognition","<p>A named-entity extraction framework such as <a href=""http://gate.ac.uk/"" rel=""noreferrer"">GATE</a> has at least tackled the <a href=""http://gate.ac.uk/ie/index.html"" rel=""noreferrer"">information extraction problem</a> for locations, assisted by a gazetteer of known places to help resolve common issues.  Unless the pages were machine generated from a common source, you're going to find regular expressions a bit weak for the job.</p>
",12,7,4568,2008-12-08 09:06:36,https://stackoverflow.com/questions/348958/algorithms-recognizing-physical-address-on-a-webpage
Strategies for recognizing proper nouns in NLP,"<p>I'm interested in learning more about <a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""noreferrer"">Natural Language Processing</a> (NLP) and am curious if there are currently any strategies for recognizing proper nouns in a text that aren't based on dictionary recognition? Also, could anyone explain or link to resources that explain the current dictionary-based methods? Who are the authoritative experts on NLP or what are the definitive resources on the subject?</p>
","nlp, named-entity-recognition, part-of-speech","<p>The task of determining the proper part of speech for a word in a text is called <a href=""http://en.wikipedia.org/wiki/POS_tagging"" rel=""noreferrer"">Part of Speech Tagging</a>. The <a href=""http://en.wikipedia.org/wiki/Brill_tagger"" rel=""noreferrer"">Brill tagger</a>, for example, uses a mixture of dictionary(vocabulary) words and contextual rules. I believe that some of the important initial dictionary words for this task are the stop words. 
Once you have (mostly correct) parts of speech for your words, you can start building larger structures. <a href=""http://books.google.com/books?id=jkkoj7U5g4kC&amp;dq=Natural+Language+Processing+for+Online+Applications:+Text+Retrieval,+Extraction+and+Categorization&amp;printsec=frontcover&amp;source=bn&amp;hl=en&amp;ei=GVGuSdebCJDRjAf16p2kBg&amp;sa=X&amp;oi=book_result&amp;resnum=7&amp;ct=result#PPP1,M1"" rel=""noreferrer"">This industry-oriented book</a> differentiates between recognizing noun phrases (NPs) and recognizing named entities. 
About textbooks: <a href=""https://rads.stackoverflow.com/amzn/click/com/0805303340"" rel=""noreferrer"" rel=""nofollow noreferrer"">Allen's Natural Language Understanding</a> is a good, but a bit dated, book. <a href=""https://rads.stackoverflow.com/amzn/click/com/0262133601"" rel=""noreferrer"" rel=""nofollow noreferrer"">Foundations of Statistical Natural Language Processing</a> is a nice introduction to statistical NLP. <a href=""https://rads.stackoverflow.com/amzn/click/com/0131873210"" rel=""noreferrer"" rel=""nofollow noreferrer"">Speech and Language Processing</a> is a bit more rigorous and maybe more authoritative. <a href=""http://www.aclweb.org/"" rel=""noreferrer"">The Association for Computational Linguistics</a> is a leading scientific community on computational linguistics.</p>
",12,14,9096,2009-03-03 23:56:45,https://stackoverflow.com/questions/608743/strategies-for-recognizing-proper-nouns-in-nlp
Algorithms for named entity recognition,"<p>I would like to use named entity recognition (NER) to find adequate tags for texts in a database.</p>

<p>I know there is a Wikipedia article about this and lots of other pages describing NER, I would preferably hear something about this topic from you:</p>

<ul>
<li>What experiences did you make with the various algorithms?</li>
<li>Which algorithm would you recommend?</li>
<li>Which algorithm is the easiest to implement (PHP/Python)?</li>
<li>How to the algorithms work? Is manual training necessary?</li>
</ul>

<p>Example:</p>

<p>""Last year, I was in London where I saw Barack Obama."" => Tags: London, Barack Obama</p>

<p>I hope you can help me. Thank you very much in advance!</p>
","php, python, extract, analysis, named-entity-recognition","<p>To start with check out <a href=""http://www.nltk.org/"" rel=""noreferrer"">http://www.nltk.org/</a> if you plan working with python although as far as I know the code isn't ""industrial strength"" but it will get you started.</p>

<p>Check out section 7.5 from <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""noreferrer"">http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html</a> but to understand the algorithms you probably will have to read through a lot of the book.</p>

<p>Also check this out <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/CRF-NER.shtml</a>. It's done with java, </p>

<p>NER isn't an easy subject and probably nobody will tell you ""this is the best algorithm"", most of them have their pro/cons.</p>

<p>My 0.05 of a dollar.</p>

<p>Cheers,</p>
",12,22,10003,2009-06-22 12:26:33,https://stackoverflow.com/questions/1026925/algorithms-for-named-entity-recognition
Entity extraction web services,"<p>Are there any paid or free named entity recognition web services available.</p>
<p>Basically I'm looking for something - where if I pass a text like:</p>
<blockquote>
<p>&quot;John had french fries at Burger King&quot;</p>
</blockquote>
<p>It should be identify - something along the lines:</p>
<blockquote>
<p>Person: John</p>
<p>Organization: Burger King</p>
</blockquote>
<p>I've heard of <a href=""https://en.wikipedia.org/wiki/General_Architecture_for_Text_Engineering"" rel=""nofollow noreferrer"">Annie from GATE</a> - but I don't think it has a web service available.</p>
","named-entity-recognition, named-entity-extraction","<p><a href=""http://viewer.opencalais.com/"" rel=""nofollow noreferrer"">OpenCalais</a> by Reuters - pretty awesome at detecting companies, political entities etc. but not, say, food items.</p>

<p><img src=""https://i.sstatic.net/RtxUc.png"" alt=""enter image description here""></p>

<p>Free for 50,000 requests per day even for commercial, <a href=""http://www.opencalais.com/APIKey"" rel=""nofollow noreferrer"">as long as you display their logo</a>.</p>

<p>Totally tripped by certain types of input though. As of now, this results in nothing being detected, despite <a href=""https://www.google.com/finance?q=OTCMKTS%3AWACMY"" rel=""nofollow noreferrer"">Wacom being a relatively well-known company</a>.</p>

<blockquote>
  <p>Wacom introduces WILL - the Wacom Ink Layer Language as a Leading Standard in Digital Inking</p>
</blockquote>
",6,8,2402,2010-05-21 01:55:18,https://stackoverflow.com/questions/2879134/entity-extraction-web-services
Disambiguating Named Entities in Java,"<p>I have a list of strings (company names, in this case), and a Java program that extracts a list of things that look like company names out of mostly-unstructured text.  I need to match each element of extracted text to a string in the list. Caveat: the unstructured text has typos, things like ""Blah, Inc."" referred to as ""Blah,"" etc.  I've tried Levenshtein Edit Distance, but that fails for predictable reasons.  Are there known best-practices ways of tackling this problem? Or am I back to manual data-entry?</p>
","java, named-entity-recognition","<p>This is not a simple problem, and there are entire companies built around trying to solve it (even for reduced matching sets like company names versus the general case). </p>

<p>If you can identify a discrete number of patterns that valid company names fall into, and that noise does not fall into, then you could tackle this with a series of regular expression matches.</p>

<p>If the patterns are difficult or too numerous, then you could try developing a probabilistic model, perhaps something like a Bayesian network. You would take a subset of your data for training, and perhaps a second subset for a quick validation, and grow the network. Techniques might include genetic programming or setting up a neural network. This approach is obviously not lightweight, and you'd probably want to consider your need carefully before going down this road.</p>
",3,3,738,2010-06-09 15:18:07,https://stackoverflow.com/questions/3007186/disambiguating-named-entities-in-java
Is NER necessary for Coreference resolution?,"<p>... or is gender information enough?
More specifically, I'm interested in knowing if I can reduce the number of models loaded by the Stanford Core NLP to extract coreferences. I am not interested in actual named entity recognition.</p>

<p>Thank you</p>
","nlp, stanford-nlp, named-entity-recognition","<p>According to the EMNLP paper that describes the coref system packaged with Stanford CoreNLP, named entities tags are just used in the following coref annotation passes: <em>precise constructs</em>, <em>relaxed head matching</em>, and <em>pronouns</em> <a href=""http://cs.stanford.edu/people/nc/pubs/emnlp2010-sieve-coref.pdf"">(Raghunathan et al. 2010)</a>. </p>

<p>You can specify what passes to use with the <strong>dcoref.sievePasses</strong> configuration property. If you want coreference but you don't want to do NER, you should be able to just run the pipeline without NER and specify that the coref system should only use the annotation passes that don't require NER labels. </p>

<p>However, the resulting coref annotations will take a hit on <a href=""http://en.wikipedia.org/wiki/Recall_%28information_retrieval%29"">recall</a>. So, you might want to do some experiments to determine whether the degraded quality of the annotations is problem for whatever your are using them for downstream.</p>
",5,4,1511,2010-12-15 19:33:08,https://stackoverflow.com/questions/4454029/is-ner-necessary-for-coreference-resolution
Understanding Relevance Score of OpenCalais,"<p>I am trying to understand what is the relevance score that opencalais returns associated with each entity? What does it signify and how is it to be interpreted? I would be thankful for insights into this.</p>
","nlp, information-extraction, named-entity-recognition, opencalais","<p>Their <a href=""http://www.opencalais.com/documentation/calais-web-service-api/api-metadata/entity-relevance-score"" rel=""noreferrer"">documentation</a> states: The relevance capability detects the importance of each unique entity and assigns a relevance score in the range 0-1 (1 being the most relevant and important).</p>

<p>While they do not explain what 'relevance' means exactly, one would expect it to quantify the centrality of the entity to the discourse of the document.  It's likely influenced by factors such as the entities mention frequency in this document as compared to its expected frequency in a random document (cf. TF-IDF), but could also involve more sophisticated discourse analysis.</p>
",5,5,720,2011-01-08 12:15:07,https://stackoverflow.com/questions/4633794/understanding-relevance-score-of-opencalais
"How do I loop over several files, keeping the base name for further processing?","<p>I have multiple text files that need to be tokenised, POS and NER. I am using <a href=""http://svn.ask.it.usyd.edu.au/trac/candc"" rel=""nofollow"">C&amp;C</a> taggers and have run their tutorial, but I am wondering if there is a way to tag multiple files rather than one by one.</p>

<p>At the moment I am tokenising the files:</p>

<pre><code>bin/tokkie --input working/tutorial/example.txt--quotes delete --output working/tutorial/example.tok
</code></pre>

<p>as follows and then Part of Speech tagging:</p>

<pre><code>bin/pos --input working/tutorial/example.tok --model models/pos --output working/tutorial/example.pos
</code></pre>

<p>and lastly Named Entity Recognition:</p>

<pre><code>bin/ner --input working/tutorial/example.pos --model models/ner --output working/tutorial/example.ner
</code></pre>

<p>I am not sure how I would go about creating a loop to do this and keep the file name the same as the input but with the extension representing the tagging it has. I was thinking of a bash script or perhaps Perl to open the directory but I am not sure on how to enter the C&amp;C commands in order for the script to understand.</p>

<p>At the moment I am doing it manually and it's pretty time consuming to say the least!</p>
","perl, bash, tokenize, named-entity-recognition, part-of-speech","<p>Untested, likely needs some directory mangling.</p>

<pre><code>use autodie qw(:all);
use File::Basename qw(basename);

for my $text_file (glob 'working/tutorial/*.txt') {
    my $base_name = basename($text_file, '.txt');
    system 'bin/tokkie',
        '--input'  =&gt; ""working/tutorial/$base_name.txt"",
        '--quotes' =&gt; 'delete',
        '--output' =&gt; ""working/tutorial/$base_name.tok"";
    system 'bin/pos',
        '--input'  =&gt; ""working/tutorial/$base_name.tok"",
        '--model'  =&gt; 'models/pos',
        '--output' =&gt; ""working/tutorial/$base_name.pos"";
    system 'bin/ner',
        '--input'  =&gt; ""working/tutorial/$base_name.pos"",
        '--model'  =&gt; 'models/ner',
        '--output' =&gt; ""working/tutorial/$base_name.ner"";
}
</code></pre>
",3,3,288,2011-03-01 12:52:20,https://stackoverflow.com/questions/5154748/how-do-i-loop-over-several-files-keeping-the-base-name-for-further-processing
Linking Named-Entity Recognition tagged files to Google maps using Google Geocoding API,"<p>I have text files which are tagged using NER and I need to link them to a Google map.</p>

<pre><code>&lt;p&gt;Gardai|NNP|O fear|VBP|O a|DT|O 28-year-old|JJ|O man|NN|O ,|,|O missing|VBG|O from|IN|O his|PRP$|O Dublin|NNP|I-PER home|NN|O for|IN|O a|DT|I-DAT week|NN|I-DAT
</code></pre>

<p>Although locations are not tagged correctly ie. Dublin is tagged as a person, I want to use the Google Geocoding API, to feed in the location which is identified as being NER tagged and find the location!</p>

<p>Is this possible?</p>

<p>I was thinking of creating a regex to extract any information which is tagged as a location, organisation or person and give it to Google and see if it has a latitude and longitude co-ordinatea that corresponds to it.  Or take the 2-3 words in a row that are tagged as NER and add them as an entire address.</p>

<p>I'm just not sure about how I actually give this information to Google!?</p>

<p>Then I'm going to use the Json response to link the text file to the map using the address Google Geocoder matched.</p>

<p>Any insight or ideas would be greatly appreciated!
Thanks</p>
","javascript, html, google-maps, geocoding, named-entity-recognition","<p>You can use <a href=""http://www.tizag.com/javascriptT/javascript-string-split.php"" rel=""nofollow"">JavaScript's Split function</a> to get the data with delimiter being <code>|</code> and turn the data into an array.  Let's say if your data is this html format:</p>

<pre><code>&lt;p id='data'&gt;Gardai|NNP|O fear|VBP|O a|DT|O 28-year-old|JJ|O man|NN|O ,|,|O missing|VBG|O from|IN|O his|PRP$|O Dublin|NNP|I-PER home|NN|O for|IN|O a|DT|I-DAT week|NN|I-DAT&lt;/p&gt;
</code></pre>

<p>To grab the data w/ delimiter set to <code>|</code> you can do the following with JavaScript:</p>

<pre><code>function parseDataGeocode() {
    var mydata = document.getElementById('data').innerHTML;
    mydata = mydata.split('|');
    var locationIndexMod = 3; //this determines which element to pull from the array
    for (var i = 0; i &lt; mydata.length; i++) {
        if (i % locationIndexMod == 0) {
            console.log(mydata[i]); //hopefully it's a location
            //do your marker creation after fetch Geocode Lat Lng
            //with mydata[i] as the location/address
        }
    }
}

window.onload = parseDataGeocode;
</code></pre>

<p>Here is the <a href=""http://jsfiddle.net/kjy112/ejvT7/"" rel=""nofollow"">JSFiddle Demo</a>.  Using firebug on firefox or google chrome's console tool you can see the data output, and if you need help w/ creation of markers and Geocode please let me know.</p>
",1,2,537,2011-03-22 12:17:56,https://stackoverflow.com/questions/5391067/linking-named-entity-recognition-tagged-files-to-google-maps-using-google-geocod
Stanford NER - Extract Multi word entities,"<p>How can I tag collocations in Stanford NER? Currently it tags <code>Federal Reserve Bank of New York</code> as   </p>

<pre><code>&lt;wi num=""11"" entity=""ORGANIZATION""&gt;Federal&lt;/wi&gt; &lt;wi num=""12"" entity=""ORGANIZATION""&gt;Reserve&lt;/wi&gt; &lt;wi num=""13"" entity=""ORGANIZATION""&gt;Bank&lt;/wi&gt; &lt;wi num=""14"" entity=""ORGANIZATION""&gt;of&lt;/wi&gt; &lt;wi num=""15"" entity=""ORGANIZATION""&gt;New&lt;/wi&gt; &lt;wi num=""16"" entity=""ORGANIZATION""&gt;York&lt;/wi&gt;
</code></pre>

<p>I want it to be recognized as </p>

<pre><code>&lt;wi num=""11"" entity=""ORGANIZATION""&gt;Federal Reserve Bank of New York&lt;/wi&gt;
</code></pre>

<p>Is this possible?</p>
","java, stanford-nlp, named-entity-recognition","<p>Something similar is, yes.  If you give the flag </p>

<pre><code>-outputFormat inlineXML
</code></pre>

<p>then you'll get:</p>

<pre><code>&lt;ORGANIZATION&gt;Federal Reserve Bank of New York&lt;/ORGANIZATION&gt;
</code></pre>

<p>(Note that this isn't really changing how Stanford NER works but just the formatting of output.  If you don't like any of the provided output formats, it is fairly simple to write your own.)</p>
",2,1,1716,2011-04-18 07:54:27,https://stackoverflow.com/questions/5700002/stanford-ner-extract-multi-word-entities
Named Entity Recognition for NLTK in Python. Identifying the NE,"<p>I need to classify words into their parts of speech. Like a verb, a noun, an adverb etc..
I used the </p>

<pre><code>nltk.word_tokenize() #to identify word in a sentence 
nltk.pos_tag()       #to identify the parts of speech
nltk.ne_chunk()      #to identify Named entities. 
</code></pre>

<p>The out put of  this is a tree.
 Eg </p>

<pre><code>&gt;&gt;&gt; sentence = ""I am Jhon from America""
&gt;&gt;&gt; sent1 = nltk.word_tokenize(sentence )
&gt;&gt;&gt; sent2 = nltk.pos_tag(sent1)
&gt;&gt;&gt; sent3 =  nltk.ne_chunk(sent2, binary=True)
&gt;&gt;&gt; sent3
Tree('S', [('I', 'PRP'), ('am', 'VBP'), Tree('NE', [('Jhon', 'NNP')]), ('from', 'IN'), Tree('NE', [('America', 'NNP')])])
</code></pre>

<p>When accessing the element in this tree, i did it as follows:</p>

<pre><code>&gt;&gt;&gt; sent3[0]
('I', 'PRP')
&gt;&gt;&gt; sent3[0][0]
'I'
&gt;&gt;&gt; sent3[0][1]
'PRP'
</code></pre>

<p>But when accessing a Named Entity:</p>

<pre><code>&gt;&gt;&gt; sent3[2]
Tree('NE', [('Jhon', 'NNP')])
&gt;&gt;&gt; sent3[2][0]
('Jhon', 'NNP')
&gt;&gt;&gt; sent3[2][1]    
Traceback (most recent call last):
  File ""&lt;pyshell#121&gt;"", line 1, in &lt;module&gt;
    sent3[2][1]
  File ""C:\Python26\lib\site-packages\nltk\tree.py"", line 139, in __getitem__
    return list.__getitem__(self, index)
IndexError: list index out of range
</code></pre>

<p>I got the above error.</p>

<p>What i want is to get the output  as 'NE' similar to the previous 'PRP' so i cant identify which word is a Named Entity.
Is there any way of doing this with NLTK in python?? If so please post the command. Or is there a function in the tree library to do this? I need the node value 'NE'</p>
","python, nlp, nltk, named-entity-recognition","<p>This answer may be off base, and in which case I'll delete it, as I don't have NLTK installed here to try it, but I think you can just do:</p>

<pre><code>   &gt;&gt;&gt; sent3[2].node
   'NE'
</code></pre>

<p><code>sent3[2][0]</code> returns the first child of the tree, not the node itself</p>

<p><strong>Edit:</strong> I tried this when I got home, and it does indeed work.</p>
",14,20,15277,2011-04-18 20:14:16,https://stackoverflow.com/questions/5708352/named-entity-recognition-for-nltk-in-python-identifying-the-ne
Information extraction. Counting mentions to measure relevance,"<p>Is it possible to count how many times an entity has been mentioned in an article? For example</p>

<blockquote>
  <p><strong><code>ABC Company</code></strong> is one of the largest car manufacturers in the
  world. <strong><code>It</code></strong> is also the largest
  company in terms of annual production.
  <strong><code>It</code></strong> is also the second largest exporter of luxury cars, after XYZ
  company. Both <strong><code>ABC</code></strong> and XYZ
  together produces over n% of total car
  production in the country.</p>
</blockquote>

<p>mentions ABC company 4 times.</p>
","nlp, information-extraction, named-entity-recognition","<p>Yes, this is possible. It's a combination of</p>

<ul>
<li>named-entity recognition (NER), which for English is practically a <a href=""http://nlp.stanford.edu/ner/index.shtml"" rel=""nofollow"">solved problem</a>, and</li>
<li>coreference resolution, which is the subject of ongoing research (but give <a href=""http://cogcomp.cs.illinois.edu/page/software_view/18"" rel=""nofollow"">this package</a> a try)</li>
</ul>
",3,0,169,2011-04-19 05:56:14,https://stackoverflow.com/questions/5712170/information-extraction-counting-mentions-to-measure-relevance
Does OpenNLP use WordNet under the hood for the Named Entity Recognition,"<p>I have tried using OpenNLP Tools 1.5 from SourceForge for getting the Named Entites from a text. I did find a JWNL file in the OpenNLP download. Does that imply that OpenNLP in turn is using WordNet for the Named Entity Recognition. (Specifically, does it mean that the Name Model files were generated with WordNet) ?</p>
","java, nlp, wordnet, named-entity-recognition, opennlp","<p>No, OpenNLP only uses WordNet for co-reference resolution, such as correlating ""He"" to ""Steve Jones"" in separate sentences.</p>

<p>WordNet is not used in Named Entity Recognition. The named entity training files are manually annotated fields from various conferences/tests, such as CoNLL. </p>

<p>Source: <a href=""http://opennlp.sourceforge.net/README.html"" rel=""nofollow"">http://opennlp.sourceforge.net/README.html</a></p>
",2,1,2058,2011-04-27 20:39:36,https://stackoverflow.com/questions/5810292/does-opennlp-use-wordnet-under-the-hood-for-the-named-entity-recognition
Part-Of-Speech tagging and Named Entity Recognition for C/C++/Obj-C,"<p>need some help!</p>

<p>I'm trying to write some code in objective-c that requires part-of-speech tagging, and ideally also named entity recognition.  I don't have much interest in ""rolling my own"", so I'm looking for a decent library to use for this purpose.  Obviously the more accurate the better, but we're not talking anything critical here -- so as long as it's generally pretty accurate that's good enough.</p>

<p>It's going to be English-only, at least for the time being, but I don't want to have to do any training of models myself.  So whatever the solution, it has to have an English language model already built.</p>

<p>And finally, it has to be available via a commercial-friendly license (e.g. BSD/Berkeley, LGPL).  Can't do GPL or anything restrictive like that, though I'm open to paying a small amount for a commercial license if that's the only option.</p>

<p>C, C++ or Obj-C code is all fine.</p>

<p>So: Anyone familiar with something that'd do the trick here?  Thanks!!</p>
","objective-c, ios, nlp, named-entity-recognition, part-of-speech","<p>I suggest you check out the iOS 5 beta release notes.</p>
",3,3,2004,2011-06-28 13:53:47,https://stackoverflow.com/questions/6507556/part-of-speech-tagging-and-named-entity-recognition-for-c-c-obj-c
Finding words from a dictionary in a string of text,"<p>How would you go about parsing a string of free form text to detect things like locations and names based on a dictionary of location and names? In my particular application there will be tens of thousands if not more entries in my dictionaries so I'm pretty sure just running through them all is out of the question. Also, is there any way to add ""fuzzy"" matching so that you can also detect substrings that are within <code>x</code> edits of a dictionary word? If I'm not mistaken this falls within the field of natural language processing and more specifically named entity recognition (NER); however, my attempt to find information about the algorithms and processes behind NER have come up empty. I'd prefer to use Python for this as I'm most familiar with that although I'm open to looking at other solutions.</p>
","search, full-text-search, nlp, named-entity-recognition","<p>You might try downloading the Stanford Named Entity Recognizer:
<a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/CRF-NER.shtml</a></p>

<p>If you don't want to use someone else's code and you want to do it yourself, I'd suggest taking a look at the algorithm in their associated paper, because the Conditional Random Field model that they use for this has become a fairly common approach to NER. </p>

<p>I'm not sure exactly how to answer the second part of your question on looking for substrings without more details.  You could modify the Stanford program, or you could use a part-of-speech tagger to mark proper nouns in the text.  That wouldn't distinguish locations from names, but it would make it very simple to find words that are x words away from each proper noun.</p>
",1,1,574,2011-07-14 14:49:33,https://stackoverflow.com/questions/6695181/finding-words-from-a-dictionary-in-a-string-of-text
"How I classify a word of a text in things like names, number, money, date,etc?","<p>I did some questions about text-mining a week ago, but I was a bit confused and still, but now I know wgat I want to do.</p>

<p><strong>The situation:</strong> I have a lot of download pages with HTML content. Some of then can bean be a text from a blog, for example. They are not structured and came from different sites.</p>

<p><strong>What I want to do:</strong> I will split all the words with whitespace and I want to classify each one or a group of ones in some pre-defined itens like names, numbers, phone, email, url, date, money, temperature, etc.</p>

<p><strong>What I know:</strong> I know the concepts/heard about about Natural Language Processing, Named Entity Reconigzer, POSTagging, NayveBayesian, HMM, training and a lot of things to do classification, etc., but there is some different NLP libraries with differents classifiers and ways to do this and I don't know what use or what do.</p>

<p><strong>WHAT I NEED:</strong> I need some code example from a classifier, NLP, whatever, that can classify each word from a text separetely, and not a entire text. Something like this:</p>

<pre><code>//This is pseudo-code for what I want, and not a implementation

classifier.trainFromFile(""file-with-train-words.txt"");
words = text.split("" "");
for(String word: words){
    classifiedWord = classifier.classify(word);
    System.out.println(classifiedWord.getType());
}
</code></pre>

<p>Somebody can help me? I'm confused with various APIs, classifiers and algorithms.</p>
","java, nlp, classification, text-mining, named-entity-recognition","<p>You should try <a href=""http://incubator.apache.org/opennlp/"" rel=""nofollow"">Apache OpenNLP</a>. It is easy to use and customize. </p>

<p>If you are doing it for Portuguese there are information on how to do it on the project documentation using Amazonia Corpus. The types supported are:</p>

<p>Person, Organization, Group, Place, Event, ArtProd, Abstract, Thing, Time and Numeric.</p>

<ol>
<li><p>Download the OpenNLP and the <a href=""http://www.linguateca.pt/floresta/ficheiros/gz/amazonia.ad.gz"" rel=""nofollow"">Amazonia Corpus</a>. Extract both and copy the file <code>amazonia.ad</code> to the <code>apache-opennlp-1.5.1-incubating</code> folder.</p></li>
<li><p>Execute the TokenNameFinderConverter tool to convert the Amazonia corpus to the OpenNLP format:</p>

<pre><code>bin/opennlp TokenNameFinderConverter ad -encoding ISO-8859-1 -data amazonia.ad -lang pt &gt; corpus.txt
</code></pre></li>
<li><p>Train you model (Change the encoding to the encoding of the corpus.txt file, that should be your system default encoding. This command can take several minutes):</p>

<pre><code>bin/opennlp TokenNameFinderTrainer -lang pt -encoding UTF-8 -data corpus.txt -model pt-ner.bin -cutoff 20
</code></pre></li>
<li><p>Executing it from command line (You should execute only one sentence and the tokens should be separated):</p>

<pre><code>$ bin/opennlp TokenNameFinder pt-ner.bin 
Loading Token Name Finder model ... done (1,112s)
Meu nome é João da Silva , moro no Brasil . Trabalho na Petrobras e tenho 50 anos .
Meu nome é &lt;START:person&gt; João da Silva &lt;END&gt; , moro no &lt;START:place&gt; Brasil &lt;END&gt; . &lt;START:abstract&gt; Trabalho &lt;END&gt; na &lt;START:abstract&gt; Petrobras &lt;END&gt; e tenho &lt;START:numeric&gt; 50 anos &lt;END&gt; .
</code></pre></li>
<li><p>Executing it using the API:</p>

<pre><code>InputStream modelIn = new FileInputStream(""pt-ner.bin"");

try {
  TokenNameFinderModel model = new TokenNameFinderModel(modelIn);
}
catch (IOException e) {
  e.printStackTrace();
}
finally {
  if (modelIn != null) {
    try {
       modelIn.close();
    }
    catch (IOException e) {
    }
  }
}

// load the name finder
NameFinderME nameFinder = new NameFinderME(model);

// pass the token array to the name finder
String[] toks = {""Meu"",""nome"",""é"",""João"",""da"",""Silva"","","",""moro"",""no"",""Brasil"",""."",""Trabalho"",""na"",""Petrobras"",""e"",""tenho"",""50"",""anos"","".""};

// the Span objects will show the start and end of each name, also the type
Span[] nameSpans = nameFinder.find(toks);
</code></pre></li>
<li><p>To evaluate your model you can use 10-fold cross validation: (only available in 1.5.2-INCUBATOR, to use it today you need to use the SVN trunk) (it can take several hours)</p>

<pre><code>bin/opennlp TokenNameFinderCrossValidator -lang pt -encoding UTF-8 -data corpus.txt -cutoff 20
</code></pre></li>
<li><p>Improve the precision/recall by using the Custom Feature Generation (check documentation), for example by adding a name dictionary.</p></li>
</ol>
",5,1,8058,2011-08-01 02:55:44,https://stackoverflow.com/questions/6893858/how-i-classify-a-word-of-a-text-in-things-like-names-number-money-date-etc
How I train an Named Entity Recognizer identifier in OpenNLP?,"<p>Ok, I have the following code to train the NER Identifier from OpenNLP</p>

<pre><code>FileReader fileReader = new FileReader(""train.txt"");
ObjectStream fileStream = new PlainTextByLineStream(fileReader);
ObjectStream sampleStream = new NameSampleDataStream(fileStream);
TokenNameFinderModel model = NameFinderME.train(""pt-br"", ""train"", sampleStream, Collections.&lt;String, Object&gt;emptyMap());
nfm = new NameFinderME(model); 
</code></pre>

<p>I don't know if I'm doing something wrong of if something is missing, but the classifying is not working. I'm supposing that the train.txt is wrong.</p>

<p><strong>The error</strong> that occurs is that all tokens are classified to only one type.</p>

<p>My train.txt data is something like the following example, but with a lot more of variation and quantity of entries. Another thing is that I'm classifind word by word from a text per time, and not all tokens.</p>

<pre><code>&lt;START:distance&gt; 8000m &lt;END&gt;
&lt;START:temperature&gt; 100ºC &lt;END&gt;
&lt;START:weight&gt; 50kg &lt;END&gt;
&lt;START:name&gt; Renato &lt;END&gt;
</code></pre>

<p>Somebody can show what I doing wrong?</p>
","java, nlp, opennlp, named-entity-recognition","<p>Your training data is not OK.</p>

<p>You should put all entities in a context inside a sentence:</p>

<pre><code>At an altitude of &lt;START:distance&gt; 8000m &lt;END&gt; the temperature of boiling water is less than &lt;START:temperature&gt; 100ºC &lt;END&gt; .
The climber &lt;START:name&gt; Renato &lt;END&gt; is carrying &lt;START:weight&gt; 50kg &lt;END&gt; of equipment.
</code></pre>

<p>You will have better results if your training data derives from real world sentences and have the same style of the sentences you are classifying. For example you should train using a newspaper corpus if you will process news.</p>

<p>Also you will need thousands of sentences to build your model! Maybe you can start with a hundred to bootstrap and use the poor model to improve your corpus and train your model again.</p>

<p>And of course you should classify all tokens of a sentence, otherwise there will be no context to decide the type of an entity.</p>
",23,10,10197,2011-08-05 06:51:18,https://stackoverflow.com/questions/6952512/how-i-train-an-named-entity-recognizer-identifier-in-opennlp
Expand an HTML string to have named character entities,"<p>Is there a way in the .NET class library to encode a string so that any character for which a named entity exists is replaced to use this named entity? HttpUtility.HtmlEncode doesn't appear to help:</p>

<pre><code>// Result is ""$"", would like ""&amp;euro;"" instead
var encoded = HttpUtility.HtmlEncode(""€"");
</code></pre>
",".net, html-encode, named-entity-recognition","<p>I think you would have to get your hands dirty and just grab the canonical list of named entities from the specification and then create a loop that, for every named entity, simply replaces every occurrence of that character with the corresponding entity reference.</p>
",1,2,188,2011-09-16 16:02:33,https://stackoverflow.com/questions/7447479/expand-an-html-string-to-have-named-character-entities
unsupervised Named entity recognition (NER) with custom controlled vocabulary for crosslink-suggestions in Java,"<p>I'm looking for a Java library that can do Named entity recognition (NER) with a custom controlled vocabulary, without needing labeled training data first. I searched some on SE, but most questions are rather unspecific.</p>

<p>Consider the following use-case:</p>

<ul>
<li>an editor is inputting articles in a CMS (about 500 words).</li>
<li>the text may contain references (in plain text) to entities of a specific domain.  e.g: 
<ul>
<li>names of points of interest, like bars, restaurants, as well as neighborhoods, etc. </li>
</ul></li>
<li>a controlled vocabulary of these entities exist (about 5.000 entities) . 
<ul>
<li>I imagine an entity to be a -tuple in the vocabulary</li>
</ul></li>
<li>after finishing the text, the user should be able to save the document. </li>
<li>This triggers the workflow to scan the piece of text against the vocabulary, by comparing against the name of the entity. It's not required to have a 100% match: 97% on Jarao-winkler or whatever (I'm not familiar with what algo's NER uses) may be enough, I need this to be configurable. </li>
<li>Hits are returned to the controller server-side. This in return returns JSON to the client containing  of the entities, which are represented as suggested crosslinks to the editor. </li>
</ul>

<p>Ideally, I'm looking for a project that uses NRE to suggests crosslinks within a CMS-environment to piggyback on. (I'm sure plugins for wordpress exist for example) not so sure if something similar exists in Java. </p>

<p>All other more general pointers to NRE-libraries which work with controlled custom vocabularies are welcome as well.  </p>
","java, information-retrieval, text-mining, named-entity-recognition","<p>For people looking this up in the future: </p>

<p>""Approximate Dictionary-Based Chunking""
see: <a href=""http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html"" rel=""nofollow"">http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html</a></p>

<p>(URL edited.)</p>
",3,6,1681,2011-10-05 15:02:39,https://stackoverflow.com/questions/7663428/unsupervised-named-entity-recognition-ner-with-custom-controlled-vocabulary-fo
Maximum Entropy for Natural Language Processing,"<p>Can anyone explain simply how how maximum entropy models work when used in Natural Language Processing. I need to statistically parse simple words and phrases to try to figure out the likelihood of specific words and what objects they refer to or what phrases they are contained within.</p>
","statistics, nlp, named-entity-recognition","<p>I recommend the NLTK python package. You can also use MALLET or WEKA.
For a theoretical background, you should ask at <a href=""https://stats.stackexchange.com/"">https://stats.stackexchange.com/</a> or <a href=""http://metaoptimize.com/qa/"" rel=""nofollow noreferrer"">http://metaoptimize.com/qa/</a> . </p>
",1,0,290,2011-10-22 08:01:18,https://stackoverflow.com/questions/7858181/maximum-entropy-for-natural-language-processing
Difference between named entity recognition and resolution?,"<p>What is the difference between named entity recognition and named entity resolution? Would appreciate a practical example.</p>
","nlp, named-entity-recognition, named-entity-extraction","<p>Named entity recognition is picking up the names and classifying them in running text. E.g., given (<a href=""http://www.guardian.co.uk/football/2011/dec/21/john-terry-racism-case-cps"">1</a>)</p>

<pre><code>John Terry to face criminal charges over alleged racist abuse
</code></pre>

<p>an NE recognizer will output</p>

<pre><code>[PER John Terry] to face criminal charges over alleged racist abuse
</code></pre>

<p>NE resolution or normalization means finding out which entity in the outside world a name refers to. E.g., in the above example, the output would be annotated with a unique identifier for the footballer John Terry, like his Wikipedia URL:</p>

<pre><code>[https://en.wikipedia.org/wiki/John_Terry John Terry] to face criminal charges
over alleged racist abuse
</code></pre>

<p>as opposed to, e.g.</p>

<pre><code>https://en.wikipedia.org/wiki/John_Terry_%28actor%29
https://en.wikipedia.org/wiki/John_Terry_%28baseball%29
</code></pre>

<p>or any of the other John Terry's the Wikipedia knows.</p>
",17,9,2615,2011-12-21 11:22:14,https://stackoverflow.com/questions/8589005/difference-between-named-entity-recognition-and-resolution
"How does Apple find dates, times and addresses in emails?","<p>In the iOS email client, when an email contains a date, time or location, the text becomes a hyperlink and it is possible to create an appointment or look at a map simply by tapping the link. It not only works for emails in English, but in other languages also. I love this feature and would like to understand how they do it. </p>

<p>The naive way to do this would be to have many regular expressions and run them all. However I  this is not going to scale very well and will work for only a specific language or date format, etc. I think that Apple must be using some concept of machine learning to extract entities (8:00PM, 8PM, 8:00, 0800, 20:00, 20h, 20h00, 2000 etc.).</p>

<p>Any idea how Apple is able to extract entities so quickly in its email client? What machine learning algorithm would you to apply accomplish such task? </p>
","machine-learning, nlp, information-extraction, named-entity-recognition","<p>They likely use <a href=""http://en.wikipedia.org/wiki/Information_extraction"" rel=""nofollow noreferrer"">Information Extraction</a> techniques for this.</p>
<p>Here is a demo of Stanford's <a href=""https://nlp.stanford.edu/software/sutime.html"" rel=""nofollow noreferrer"">SUTime</a> tool:</p>
<p><a href=""http://nlp.stanford.edu:8080/sutime/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/sutime/process</a></p>
<p>You would extract attributes about n-grams (consecutive words) in a document:</p>
<ul>
<li>numberOfLetters</li>
<li>numberOfSymbols</li>
<li>length</li>
<li>previousWord</li>
<li>nextWord</li>
<li>nextWordNumberOfSymbols<br />
...</li>
</ul>
<p>And then use a classification algorithm, and feed it positive and negative examples:</p>
<pre><code>Observation  nLetters  nSymbols  length  prevWord  nextWord isPartOfDate  
&quot;Feb.&quot;       3         1         4       &quot;Wed&quot;     &quot;29th&quot;   TRUE  
&quot;DEC&quot;        3         0         3       &quot;company&quot; &quot;went&quot;   FALSE  
...
</code></pre>
<p>You might get away with 50 examples of each, but the more the merrier. Then, the algorithm learns based on those examples, and can apply to future examples that it hasn't seen before.</p>
<p>It might learn rules such as</p>
<ul>
<li>if previous word is only characters and maybe periods...</li>
<li>and current word is in &quot;february&quot;, &quot;mar.&quot;, &quot;the&quot; ...</li>
<li>and next word is in &quot;twelfth&quot;, any_number ...</li>
<li>then is date</li>
</ul>
<p>Here is a <a href=""http://videolectures.net/mlas06_nigam_tie/"" rel=""nofollow noreferrer"">decent video</a> by a Google engineer on the subject</p>
",155,133,25926,2012-02-15 14:12:06,https://stackoverflow.com/questions/9294926/how-does-apple-find-dates-times-and-addresses-in-emails
How to find references to dates in natural text?,"<p>What I want to do is to parse raw natural text and find all the phrases that describe dates.</p>

<p>I've got a fairly big corpus with all the references to dates marked up:</p>

<pre><code>I met him &lt;date&gt;yesterday&lt;/date&gt;.
Roger Zelazny was born &lt;date&gt;in 1937&lt;/date&gt;
He'll have a hell of a hangover &lt;date&gt;tomorrow morning&lt;/date&gt;
</code></pre>

<p>I don't want to interpret the date phrases, just locate them. The fact that they're dates is irrelevant (in real life they're not even dates but I don't want to bore you with the details), basically it's just an open-ended set of possible values. The grammar of the values themselves can be approximated as context-free, however it's quite complicated to build manually and with increasing complexity it gets increasingly hard to avoid false positives.</p>

<p>I know this is a bit of a long shot so I'm not expecting an out-of-the-box solution to exist out there, but what technology or research can I potentially use?</p>
","nlp, machine-learning, information-extraction, named-entity-recognition, temporal","<p>One of the generic approaches used in academia and in industry is based on Conditional Random Fields. Basically, it is a special probabilistic model, you train it first with your marked up data and then it can label certain types of entities in a given text.</p>

<p>You can even try one of the systems from Stanford Natural Language Processing Group: <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">Stanford Named Entity Recognizer</a></p>

<p>When you download the tool, note there are several models, you need the last one:</p>

<blockquote>
  <p>Included with the Stanford NER are a 4 class model trained for CoNLL,
  a 7 class model trained for MUC, and a 3 class model trained on both
  data sets for the intersection of those class sets.</p>
  
  <p>3 class   Location, Person, Organization</p>
  
  <p>4 class   Location, Person, Organization, Misc</p>
  
  <p><strong>7 class Time, Location, Organization, Person, Money, Percent, Date</strong></p>
</blockquote>

<p><strong>Update.</strong> You can actually try that tool <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow noreferrer"">online here</a>. Select the <code>muc.7class.distsim.crf.ser.gz</code> classifier and try some text with dates. It doesn't seem to recognize ""yesterday"", but it recognizes ""20th century"", for example. In the end, this is a matter of CRF training.</p>

<hr>

<p><img src=""https://i.sstatic.net/BHqSF.png"" alt=""Stanford NER screenshot""></p>
",7,2,5426,2012-03-12 22:21:06,https://stackoverflow.com/questions/9675633/how-to-find-references-to-dates-in-natural-text
ML based domain specific named enitty recognition (NER)?,"<p>I need to build a classifier which identifies NEs in a specific domain. So for instance if my domain is Hockey or Football, the classifier should go accept NEs in that domain but NOT all pronouns it sees on web pages. My ultimate goal is to improve text classification through NER. </p>

<p>For people working in this area please suggest me how should I build such a classifier?
thanks!</p>
","text, nlp, machine-learning, classification, named-entity-recognition","<p>If all you want is to ignore pronouns, you can run any POS tagger followed by any NER algorithm ( the Stanford package is a popular implementation) and then ignore any named entities which are pronouns. However, the pronouns might refer to named entities, which may or may not turn out to be important for the performance of your classifier. The only way to tell for sure it to try.</p>

<p>A slightly unrelated comment- a NER system trained on domain-specific data (e.g. hockey) is more likely to pick up entities from that domain because it will have seen some of the contexts entities appear in. Depending on the system, it might also pick up entities from other domains (which you do not want, if I understand your question correctly) because of syntax, word shape patterns, etc.</p>
",1,4,1310,2012-04-03 05:41:57,https://stackoverflow.com/questions/9987681/ml-based-domain-specific-named-enitty-recognition-ner
Focused Named Entity Recognition (NER)?,"<p>I want to recognize named entities in a specific field (e.g. baseball). I know there are  tools available like StanfordNER, LingPipe, AlchemyAPI and I have done a little testing with them. But what I want them to be is field specific as I mentioned earlier. How this is possible? </p>
","nlp, machine-learning, named-entity-recognition","<p>One approach may be to</p>

<ol>
<li><p>Use a <strong>general (non-domain specific) tool</strong> to detect people's names</p></li>
<li><p>Use a <strong>subject classifier</strong> to filter out texts that are not in the domain</p></li>
</ol>

<p>If the total size of the data set is sufficient and the accuracy of the extractor and classifier good enough, you can use the result to obtain a list of <strong>people's names that are closely related to the domain</strong> in question (e.g. by restricting the results to those that are mentioned <em>significantly more often in domain-specific texts than in other texts</em>).</p>

<p>In the case of baseball, this should be a fairly good way of getting a list of people <em>related to baseball</em>. It would, however, not be a good way to obtain a list of baseball <em>players</em> only. For the latter it would be necessary to analyse the precise context in which the names are mentioned and the things said about them; but perhaps that is not required.</p>

<p><strong>Edit:</strong> By <em>subject classifier</em> I mean the same as what other people might refer to simply as <em>categorization</em>, <em>document classification</em>, <em>domain classification</em>, or similar. Examples of ready-to-use tools include the classifier in Python-NLTK (see <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html"" rel=""nofollow"">here</a> for examples) and the one in LingPipe (see <a href=""http://alias-i.com/lingpipe/demos/tutorial/classify/read-me.html"" rel=""nofollow"">here</a>).</p>
",3,3,1830,2012-04-06 12:09:28,https://stackoverflow.com/questions/10043293/focused-named-entity-recognition-ner
Named entities as a feature in text categorization?,"<p>With existing text categorization (supervised) techniques why don't we consider Named Entities (NE) in the text as a feature in training and testing? Do you think we can improve precision with using NEs as a feature?</p>
","text, machine-learning, classification, named-entity-recognition","<p>It depends a lot on the domain you are working in. You have to define the features based on the domain. Say in a search engine you are working on learning to rank problem, generating a dynamic rank, the NE's wont give you any benefit here. It largerly depends on the domain that you are working and also the output categorization labels (supervised learning) defined. </p>

<p>Now say you are working on classifying documents pertaining to Soccer or Movie or Polictics and so on. In this case Named Entities can work. I will give you an example here, say you are using a Neural Network which categorizes documents into Soccer, Movie, Politics etc. Now say a document comes in ""Lionel Messi was invited to attend the premier of ""The Social Network"", also present were the cast and crew including Jesse Eisenberg, Andrew Garfield and Justin Timberlake"" Here the connection between named entities (input features) and movie (output defined) will be stronger and hence it will be classified as a document on Movie. </p>

<p>Another example, say our document is ""Tom Cruise is portraying the character of Lionel Messi in the movie ""The last soccer game"". Here comes the benefit say your neural network has learnt that when an actor and footballer comes together in one document there is high probability of it being a movie. Again it depends on the data and training it may be other way round too (but that is what is learning all about; seeing the past data)</p>

<p>So my answer would be try it out, nobody is stopping you to have named entities as features. It might help for the domain that you are working in.</p>
",3,8,1353,2012-04-09 18:10:09,https://stackoverflow.com/questions/10077647/named-entities-as-a-feature-in-text-categorization
Can OpenNLP use HTML tags as part of the training?,"<p>I'm creating a training set for the TokenNameFinder using html documents converted into plain text, but my precision is low and I want to use the HTML tags as part of the training. Like words in bold, and sentences in differents margin sizes. 
Will OpenNLP accept and use those tags to create rules?
Is there another way to make use of those tags to improve precision?</p>
","html, nlp, pattern-matching, named-entity-recognition, opennlp","<p>It is not clear what you mean with using HTML tags to train OpenNLP.
The train input is an annotated tokenized sentence:</p>

<pre><code>&lt;START:person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mr . &lt;START:person&gt; Vinken &lt;END&gt; is chairman of &lt;START:company&gt; Elsevier N.V. &lt;END&gt; , the Dutch publishing group .
</code></pre>

<p>To train an OpenNLP model using the standard tooling you need annotations follows this convention. Note that the annotations does not follow the XML standard.</p>

<p>You can embed annotations directly to the HTML documents you will use for training. It might even help the classifier with the extra context, but I've never read any experimental results about it.</p>

<p>You should keep in mind that the training data should be tokenized. It means that you should include white spaces between words and punctuation, as well as between text elements and html:</p>

<pre><code>&lt;p&gt; &lt;i&gt; Mr . &lt;START:person&gt; Vinken &lt;END&gt; &lt;/i&gt; is chairman of &lt;b&gt; &lt;START:company&gt; Elsevier N.V. &lt;END&gt; &lt;/b&gt;, the Dutch publishing group .
</code></pre>
",0,1,514,2012-04-10 17:39:01,https://stackoverflow.com/questions/10093677/can-opennlp-use-html-tags-as-part-of-the-training
Stanford NER: extracting separate lists of entities?,"<p>I can get a string annotated with Named Entities with the following code.</p>

<pre><code>String NEString =  classifier.classifyWithInlineXML(fileContents)
</code></pre>

<p>I'm wondering if there is any method to call so that I can get separate entities (PERSON, ORGANIZATION, LOCATIOIN) lists in the file, that way I don't have to parse the retrieved string with the above method to get the entity lists?</p>
","java, stanford-nlp, named-entity-recognition","<p>In my opinion, the cleanes way to run the classification is:</p>

<pre><code>List&lt;Triple&lt;String,Integer,Integer&gt;&gt; out = classifier.classifyToCharacterOffsets(text);
triple.first(): entity type
triple.second(): start position
triple.third(): end position
</code></pre>

<p>It groups consequent entities and returns the start and end position of entities.</p>
",3,4,1229,2012-04-16 12:31:37,https://stackoverflow.com/questions/10174122/stanford-ner-extracting-separate-lists-of-entities
Using the nltk to recognise dates as named entities?,"<p>I'm trying to use the NLTK Named Entity Tagger to identify various named entities. In the book <em>Natural Language Processing with Python</em> they provide a list of commonly used named entitities, (Table 7.4, if anyone is curious) which include: DATE   June, 2008-06-29 and TIME   two fifty a m, 1:30 p.m. So I got the impresssion that this could be done with the NLTK's named entity tagger.</p>

<p>However, when I've run the tagger, it doesn't seem to pick up dates or times at all, as it does people or organizations. Does the NLTK named entity tagger not handle these date/time cases, or does it only pick up a specific date/time format? If it doesn't handle these cases, does anybody know of a system that does? Or is creating my own the only solution?</p>

<p>Thanks!</p>
","nltk, named-entity-recognition","<p>You should check out the contrib repository of NLTK - contains a module called timex.py or download it here:
<a href=""https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/timex.py"" rel=""nofollow"">https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/timex.py</a></p>

<p>From the first line of the module:</p>

<pre>
# Code for tagging temporal expressions in text
</pre>
",10,4,6187,2012-04-26 19:58:43,https://stackoverflow.com/questions/10340540/using-the-nltk-to-recognise-dates-as-named-entities
Semi-automatic annotation tool - How to find RDF Triplets,"<p>I'm developing a semi-automatic annotation tool for medical texts and I am completely lost in finding the RDF triplets for annotation.</p>

<p>I am currently trying to use an NLP based approach. I have already looked into Stanford NER and OpenNLP and they both do not have models for extracting disease names.</p>

<p>My question is:
* How can I create a new NER model for extracting disease names? and can I get any help from the OpenNLP or Standford NERs?
* Is there another approach all-together - other than NLP - to extracting the RDF triplets from a text?</p>

<p>Any help would be appreciated! Thanks.</p>
","annotations, rdf, named-entity-recognition, named-entity-extraction","<p>I have done something similar to what you need both with OpenNLP and LingPipe.
I found the exact dictionary-based chunking of LingPipe good enough for my use case and used that. Documentation available here: <a href=""http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html"" rel=""nofollow"">http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html</a></p>

<p>You can find a small demo here: </p>

<ul>
<li><a href=""https://github.com/castagna/nerdf"" rel=""nofollow"">https://github.com/castagna/nerdf</a></li>
</ul>

<p>If a gazetteer/dictionary approach isn't good enough for you, you can try creating your own model, OpenNLP has API for training models as well. Documentation is here: <a href=""http://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.namefind.training"" rel=""nofollow"">http://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.namefind.training</a></p>

<p>Extracting RDF triples from natural language is a different problem than identify named entities. NER is a related and perhaps necessary step, but not enough. To extract an RDF statement from natural language not only you need to identify entities such as the subject and the object of a statement. But you also need to identify the verb and/or relationship of those entities and also you need to map those to URIs.</p>
",4,6,1505,2012-04-28 21:44:41,https://stackoverflow.com/questions/10367815/semi-automatic-annotation-tool-how-to-find-rdf-triplets
NLTK named entity recognition in dutch,"<p>I am trying to extract named entities from dutch text. I used <a href=""https://github.com/japerk/nltk-trainer/"" rel=""noreferrer"">nltk-trainer</a> to train a tagger and a chunker on the conll2002 dutch corpus. However, the parse method from the chunker is not detecting any named entities. Here is my code:</p>

<pre><code>str = 'Christiane heeft een lam.'

tagger = nltk.data.load('taggers/dutch.pickle')
chunker = nltk.data.load('chunkers/dutch.pickle')

str_tags = tagger.tag(nltk.word_tokenize(str))
print str_tags

str_chunks = chunker.parse(str_tags)
print str_chunks
</code></pre>

<p>And the output of this program:</p>

<pre><code>[('Christiane', u'N'), ('heeft', u'V'), ('een', u'Art'), ('lam', u'Adj'), ('.', u'Punc')]
(S Christiane/N heeft/V een/Art lam/Adj ./Punc)
</code></pre>

<p>I was expecting Christiane to be detected as a named entity.
Any help?</p>
","python, nlp, nltk, named-entity-recognition","<p>The <code>conll2002</code> corpus has both spanish and dutch text, so you should make sure to use the <code>fileids</code> parameter, as in <code>python train_chunker.py conll2002 --fileids ned.train</code>. Training on both spanish and dutch will have poor results.</p>

<p>The default algorithm is a Tagger based Chunker, which does not work well on conll2002. Instead, use a classifier based chunker like NaiveBayes, so the full command might look like this (and I've confirmed that the resulting chunker does recognize ""Christiane"" as a ""PER""):</p>

<p><code>python train_chunker.py conll2002 --fileids ned.train --classifier NaiveBayes --filename ~/nltk_data/chunkers/conll2002_ned_NaiveBayes.pickle</code></p>
",7,8,5985,2012-07-02 11:54:10,https://stackoverflow.com/questions/11293149/nltk-named-entity-recognition-in-dutch
Extract a person&#39;s full name from a block of text in Perl?,"<p>I need to extract names (including uncommon names) from blocks of text using Perl. I've looked into <a href=""http://search.cpan.org/~dbourget/Text-Names-0.19/lib/Text/Names.pm"" rel=""nofollow"">this</a> module for extracting names, but it only has the top 1000 popular names and surnames in the US dating back to 1990; I need something a bit more comprehensive.</p>

<p>I've considered using the Social Security Index to make a database for comparison, but this seems very tedious and processing intensive. Is there a way to pull names from Perl using another method?</p>

<p>Example of text to parse:</p>

<blockquote>
  <p><b>LADNIER<br/> </b>  Louis Anthony Ladnier, [Louie] age 48, of Mobile, Alabama died at home Friday, November 16, 2012.<br/>   Louie was born January 9, 1964 in Mobile, Alabama. He was the son of John E. Ladnier, Sr. and Gloria Bosarge Ladnier.  He was a graduate of McGill-Toolen High School and attended University of South Alabama.  He was employed up until his medical retirement as Communi-cations Supervisor with the Bayou La Batre Police Department.  <br/>    He is preceded in death by his father, John. Survived by his mother, Gloria, nephews, Dominic Ladnier and Christian Rubio, whom he loved and help raise as his own sons, sisters, Marj Ladnier and Morgan Gordy [Julian], and brother Eddie Ladnier [Cindy], and nephews, Jamie, Joey, Eddie, Will, Ben and nieces, Anna and Elisabeth.<br/>   Memorial service will be held at St. Dominic's Catholic Church in Mobile on Wednesday at 1pm.   <br/>   Serenity Funeral Home is in charge of arrangements. <br/>  In lieu of flowers, memorials may be sent to St. Dominic School, 4160 Burma Road  Mobile, AL 36693, education fund for Christian Rubio and McGill-Toolen High School, 1501 Old Shell Road  Mobile, AL 36604, education Fund for Dominic Ladnier. <br/>   The family is grateful for all the prayers and support during this time.  Louie was a rock and a joy to us all.  <p> </p></p>
</blockquote>
","perl, parsing, nlp, cpan, named-entity-recognition","<p>There is no sure fire way to do this due to the nature of the English language. You either need lists to (fuzzy)compare with, or will have to settle for significant accuracy penalties.</p>
",1,3,1766,2012-11-30 18:12:59,https://stackoverflow.com/questions/13650795/extract-a-persons-full-name-from-a-block-of-text-in-perl
Multi-term named entities in Stanford Named Entity Recognizer,"<p>I'm using the Stanford Named Entity Recognizer <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"">http://nlp.stanford.edu/software/CRF-NER.shtml</a> and it's working fine. This is</p>

<pre><code>    List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(text);
    for (List&lt;CoreLabel&gt; sentence : out) {
        for (CoreLabel word : sentence) {
            if (!StringUtils.equals(word.get(AnswerAnnotation.class), ""O"")) {
                namedEntities.add(word.word().trim());           
            }
        }
    }
</code></pre>

<p>However the problem I'm finding is identifying names and surnames. If the recognizer encounters ""Joe Smith"", it is returning ""Joe"" and ""Smith"" separately. I'd really like it to return ""Joe Smith"" as one term. </p>

<p>Could this be achieved through the recognizer maybe through a configuration? I didn't find anything in the javadoc till now. </p>

<p>Thanks!</p>
","nlp, stanford-nlp, named-entity-recognition","<p>This is because your inner for loop is iterating over individual tokens (words) and adding them separately. You need to change things to add whole names at once.</p>

<p>One way is to replace the inner for loop with a regular for loop with a while loop inside it which takes adjacent non-O things of the same class and adds them as a single entity.*</p>

<p>Another way would be to use the CRFClassifier method call:</p>

<pre><code>List&lt;Triple&lt;String,Integer,Integer&gt;&gt; classifyToCharacterOffsets(String sentences)
</code></pre>

<p>which will give you whole entities, which you can extract the String form of by using <code>substring</code> on the original input.</p>

<p>*The models that we distribute use a simple raw IO label scheme, where things are labeled PERSON or LOCATION, and the appropriate thing to do is simply to coalesce adjacent tokens with the same label. Many NER systems use more complex labels such as IOB labels, where codes like B-PERS indicates where a person entity starts. The CRFClassifier class and feature factories support such labels, but they're not used in the models we currently distribute (as of 2012).</p>
",20,19,10254,2012-12-07 14:45:42,https://stackoverflow.com/questions/13765349/multi-term-named-entities-in-stanford-named-entity-recognizer
Is it possible to get a set of a specific named entity tokens that comprise a phrase,"<p>I'm using the Stanford CoreNLP parsers to run through some text and there are date phrases, such as 'the second Monday in October' and 'the past year'.  The library will appropriately tag each token as a DATE named entity, but is there a way to programmatically get this whole date phrase?  And it's not just dates, ORGANIZATION named entities will do the same (""The International Olympic Committee"", for example, could be one identified in a given text example).</p>

<pre><code>String content = ""Thanksgiving, or Thanksgiving Day (Canadian French: Jour de""
        + "" l'Action de grâce), occurring on the second Monday in October, is""
        + "" an annual Canadian holiday which celebrates the harvest and other""
        + "" blessings of the past year."";

Properties p = new Properties();
p.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(p);

Annotation document = new Annotation(content);
pipeline.annotate(document);

for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
    for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {

        String word = token.get(CoreAnnotations.TextAnnotation.class);
        String ne = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);

        if (ne.equals(""DATE"")) {
            System.out.println(""DATE: "" + word);
        }

    }
}
</code></pre>

<p>Which, after the Stanford annotator and classifier loading, will yield the output:</p>

<pre><code>DATE: Thanksgiving
DATE: Thanksgiving
DATE: the
DATE: second
DATE: Monday
DATE: in
DATE: October
DATE: the
DATE: past
DATE: year
</code></pre>

<p>I feel like the library has to be recognizing the phrases and using them for the named entity tagging, so the question would be is that data kept and available somehow through the api?</p>

<p>Thanks,
Kevin</p>
","stanford-nlp, named-entity-recognition","<p>After discussions on the mailing list I've found that the api does not support this.  My solution was to just keep the state of the last NE, and build a string if necessary.  John B. from the nlp mailing lists was helpful in answering my question.</p>
",1,1,1522,2013-02-04 15:16:47,https://stackoverflow.com/questions/14689717/is-it-possible-to-get-a-set-of-a-specific-named-entity-tokens-that-comprise-a-ph
Named Entity recognition with openNLP (default model),"<p>Can anyone point out the algorithm(s) used by openNLP NameFinder module?
The code is complex and only sparsely documented and playing with it as a black box (with the default model provided) gives me the impression that it is mostly heuristic. 
Here are some examples for input and output:</p>

<p>Input:</p>

<blockquote>
  <p>John Smith is frustrated. </p>
  
  <p>john smith is frustrated.</p>
  
  <p>Barak Obama is frustrated.</p>
  
  <p>Hugo Chavez is frustrated. (no more)</p>
  
  <p>Jeff Atwood is frustrated.</p>
  
  <p>Bing Liu is frustrated with openNLP NER module.</p>
  
  <p>Noam Chomsky is frustrated with the world.</p>
  
  <p>Jayden Smith is frustrated.</p>
  
  <p>Smith Jayden is frustrated.</p>
  
  <p>Lady Gaga is frustrated. </p>
  
  <p>Ms. Gaga is frustrated. </p>
  
  <p>Mrs. Gaga is frustrated. </p>
  
  <p>Jayden is frustrated.</p>
  
  <p>Mr. Liu is frustrated.</p>
</blockquote>

<p>Output (I changed diamonds to square brackets) :</p>

<blockquote>
  <p>[START:person] John Smith [END] is frustrated.</p>
  
  <p>john smith is frustrated.</p>
  
  <p>[START:person] Barak Obama [END] is frustrated.</p>
  
  <p>Hugo Chavez is frustrated. (no more)</p>
  
  <p>[START:person] Jeff Atwood [END] is frustrated.</p>
  
  <p>Bing Liu is frustrated with openNLP NER module.</p>
  
  <p>[START:person] Noam Chomsky [END] is frustrated with the world.</p>
  
  <p>Jayden [START:person] Smith [END] is frustrated.</p>
  
  <p>[START:person] Smith [END] [START:person] Jayden [END] is frustrated.</p>
  
  <p>Lady Gaga is frustrated.</p>
  
  <p>Ms. Gaga is frustrated.</p>
  
  <p>Mrs. Gaga is frustrated.</p>
  
  <p>Jayden is frustrated.</p>
  
  <p>Mr. Liu is frustrated.</p>
</blockquote>

<p>It seems that the model simply learns a fixed list of names that are annotated in the training data and allows some tiling and combinations. 
Two notable (FN) examples are:</p>

<ol>
<li>Strong name indicators such as Mr. and Mrs. are ignored.  </li>
<li>Jayden (#4 most popular name in the US in 2011) wasn't identified while the following 'Smith' (in ""Jayden Smith..."") was identified. I suspect that the model ""thinks"" that the capitalized Jayden in the beginning of the sentence is due the beginning of sentence and not due being a NE. Reversing the order, ""Smith Jayden"" as a hint (assuming 1), openNLP identifies it as two distinctive NEs, unlike other full names such as ""John Smith"", maybe suggesting that 'Smith' is in the last-names list...</li>
</ol>

<p>-> I'm puzzled and frustrated and if anyone could point me to the algorithm (or verify it sucks) I'll be thankful. </p>

<p>p.s. both Stanford and UIUC NER systems perform much better with some subtle differences that are interesting but off topic (this question is too long as is)</p>
","opennlp, named-entity-recognition","<p>As the name implies, <a href=""http://opennlp.sourceforge.net/api/opennlp/tools/namefind/NameFinderME.html"" rel=""noreferrer"">NameFinderME</a> uses a Maximum Entropy model. <a href=""http://dl.acm.org/citation.cfm?id=234289"" rel=""noreferrer"">Here</a> is the seminal paper on ME.</p>

<p>If OpenNLP's performance does not meets your requirements and you can not use Stanford or UIUC NERs, I recommend to try <a href=""http://mallet.cs.umass.edu/"" rel=""noreferrer"">Mallet</a>, using a <a href=""http://www.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf"" rel=""noreferrer"">CRF</a>. <a href=""https://github.com/chbrown/nlp/blob/master/src/main/java/cc/mallet/share/weili/ner/enron/TUI.java"" rel=""noreferrer"">This sample code</a> should get you started.</p>
",5,6,4724,2013-03-06 15:18:42,https://stackoverflow.com/questions/15251132/named-entity-recognition-with-opennlp-default-model
Registering and resolving named instances in Castle.Windsor,"<p>I can't seem to be able to get the proper instance injected into a class ctor. Here is what I am trying to do:</p>

<pre><code>class Program
{
    static void Main(string[] args)
    {
        WindsorContainer container = new WindsorContainer();
        container.Register(
            Component.For&lt;ISessionFactory&gt;()
                .UsingFactoryMethod(() =&gt; GetSessionFactory(""1""))
                .Named(""1""),
            Component.For&lt;ISessionFactory&gt;()
                .UsingFactoryMethod(() =&gt; GetSessionFactory(""2""))
                .Named(""2""));

        container.Register(
            Component.For&lt;IRepository&gt;()
                .ImplementedBy&lt;Repository&gt;()
                .DependsOn(container.Resolve&lt;ISessionFactory&gt;(""1"")),
            Component.For&lt;IReadOnlyRepository&gt;()
                .ImplementedBy&lt;ReadOnlyRepository&gt;()
                .DependsOn(container.Resolve&lt;ISessionFactory&gt;(""2"")));

        var connectionString1 = container.Resolve&lt;IRepository&gt;().Factory.ConnectionString;
        var connectionString2 = container.Resolve&lt;IReadOnlyRepository&gt;().Factory.ConnectionString;

        //These should not be equal!!!
        Console.WriteLine(connectionString1);
        Console.WriteLine(connectionString2);
    }

    public static SessionFactory GetSessionFactory(string connectionString)
    {
        return new SessionFactory { ConnectionString = connectionString };
    }

    public static bool Blah(Type accepted)
    {
        int d = 3;
        return true;
    }
}

public interface ISessionFactory
{
    string ConnectionString { get; set; }
}

public class SessionFactory : ISessionFactory
{
    public string ConnectionString { get; set; }
}

public interface IRepository
{
    ISessionFactory Factory { get; set; }
}

public class Repository : IRepository
{
    public ISessionFactory Factory { get; set; }

    public Repository(ISessionFactory factory)
    {
        this.Factory = factory;
    }
}

public interface IReadOnlyRepository
{
    ISessionFactory Factory { get; set; }
}

public class ReadOnlyRepository : IReadOnlyRepository
{
    public ISessionFactory Factory { get; set; }

    public ReadOnlyRepository(ISessionFactory factory)
    {
        this.Factory = factory;
    }
}
</code></pre>

<p>Can anyone spot the problem?</p>
","c#-4.0, dependency-injection, castle-windsor, named-entity-recognition","<p>try this:</p>

<pre><code>container.Register(
   Component.For&lt;ISessionFactory&gt;()
       .UsingFactoryMethod(() =&gt; GetSessionFactory(""1""))
       .Named(""1""),
   Component.For&lt;ISessionFactory&gt;()
       .UsingFactoryMethod(() =&gt; GetSessionFactory(""2""))
       .Named(""2""),
   Component.For&lt;IRepository&gt;()
       .ImplementedBy&lt;Repository&gt;()
       .DependsOn(Dependency.OnComponent(typeof(ISessionFactory),""1"")),
   Component.For&lt;IReadOnlyRepository&gt;()
       .ImplementedBy&lt;ReadOnlyRepository&gt;()
       .DependsOn(Dependency.OnComponent(typeof(ISessionFactory), ""2"")));
</code></pre>
",9,5,6961,2013-03-08 12:03:38,https://stackoverflow.com/questions/15293598/registering-and-resolving-named-instances-in-castle-windsor
Training n-gram NER with Stanford NLP,"<p>Recently I have been trying to train n-gram entities with Stanford Core NLP. I have followed the following tutorials - <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#b"" rel=""noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#b</a></p>

<p>With this, I am able to specify only unigram tokens and the class it belongs to. Can any one guide me through so that I can extend it to n-grams. I am trying to extract known entities like movie names from chat data set.  </p>

<p>Please guide me through in case I have mis-interpretted the Stanford Tutorials and the same can be used for the n-gram training. </p>

<p>What I am stuck with is the following property</p>

<pre><code>#structure of your training file; this tells the classifier
#that the word is in column 0 and the correct answer is in
#column 1
map = word=0,answer=1
</code></pre>

<p>Here the first column is the word (unigram) and the second column is the entity, for example </p>

<pre><code>CHAPTER O
I   O
Emma    PERS
Woodhouse   PERS
</code></pre>

<p>Now that I need to train known entities (say movie names) like <strong>Hulk</strong>, <strong>Titanic</strong> etc as movies, it would be easy with this approach. But in case I need to train <strong>I know what you did last summer</strong> or <strong>Baby's day out</strong>, what is the best approach ? </p>
","nlp, stanford-nlp, opennlp, named-entity-recognition, named-entity-extraction","<p>It had been a long wait here for an answer. I have not been able to figure out the way to get it done using Stanford Core. However mission accomplished. I have used the LingPipe NLP libraries for the same. Just quoting the answer here because, I think someone else could benefit from it. </p>

<p>Please check out the <a href=""http://alias-i.com/lingpipe/web/licensing.html"" rel=""noreferrer"">Lingpipe licencing</a> before diving in for an implementation in case you are a developer or researcher or what ever.</p>

<p>Lingpipe provides various NER methods. </p>

<p>1) Dictionary Based NER</p>

<p>2) Statistical NER (HMM Based)</p>

<p>3) Rule Based NER etc.</p>

<p>I have used the Dictionary as well as the statistical approaches. </p>

<p>First one is a direct look up methodology and the second one being a training based. </p>

<p>An example for the dictionary based NER can be found <a href=""http://alias-i.com/lingpipe/demos/tutorial/ne/src/DictionaryChunker.java"" rel=""noreferrer"">here</a></p>

<p>The statstical approach requires a training file. I have used the file with the following format -</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;root&gt;
&lt;s&gt; data line with the &lt;ENAMEX TYPE=""myentity""&gt;entity1&lt;/ENAMEX&gt;  to be trained&lt;/s&gt;
...
&lt;s&gt; with the &lt;ENAMEX TYPE=""myentity""&gt;entity2&lt;/ENAMEX&gt;  annotated &lt;/s&gt;
&lt;/root&gt;
</code></pre>

<p>I then used the following code to train the entities. </p>

<pre class=""lang-java prettyprint-override""><code>import java.io.File;
import java.io.IOException;

import com.aliasi.chunk.CharLmHmmChunker;
import com.aliasi.corpus.parsers.Muc6ChunkParser;
import com.aliasi.hmm.HmmCharLmEstimator;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;

@SuppressWarnings(""deprecation"")
public class TrainEntities {

    static final int MAX_N_GRAM = 50;
    static final int NUM_CHARS = 300;
    static final double LM_INTERPOLATION = MAX_N_GRAM; // default behavior

    public static void main(String[] args) throws IOException {
        File corpusFile = new File(""inputfile.txt"");// my annotated file
        File modelFile = new File(""outputmodelfile.model""); 

        System.out.println(""Setting up Chunker Estimator"");
        TokenizerFactory factory
            = IndoEuropeanTokenizerFactory.INSTANCE;
        HmmCharLmEstimator hmmEstimator
            = new HmmCharLmEstimator(MAX_N_GRAM,NUM_CHARS,LM_INTERPOLATION);
        CharLmHmmChunker chunkerEstimator
            = new CharLmHmmChunker(factory,hmmEstimator);

        System.out.println(""Setting up Data Parser"");
        Muc6ChunkParser parser = new Muc6ChunkParser();  
        parser.setHandler( chunkerEstimator);

        System.out.println(""Training with Data from File="" + corpusFile);
        parser.parse(corpusFile);

        System.out.println(""Compiling and Writing Model to File="" + modelFile);
        AbstractExternalizable.compileTo(chunkerEstimator,modelFile);
    }

}
</code></pre>

<p>And to test the NER I used the following class</p>

<pre class=""lang-java prettyprint-override""><code>import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.util.ArrayList;
import java.util.Set;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.util.AbstractExternalizable;

public class Recognition {
    public static void main(String[] args) throws Exception {
        File modelFile = new File(""outputmodelfile.model"");
        Chunker chunker = (Chunker) AbstractExternalizable
                .readObject(modelFile);
        String testString=""my test string"";
            Chunking chunking = chunker.chunk(testString);
            Set&lt;Chunk&gt; test = chunking.chunkSet();
            for (Chunk c : test) {
                System.out.println(testString + "" : ""
                        + testString.substring(c.start(), c.end()) + "" &gt;&gt; ""
                        + c.type());

        }
    }
}
</code></pre>

<p>Code Courtesy : Google :)</p>
",26,23,16769,2013-03-25 06:59:22,https://stackoverflow.com/questions/15609324/training-n-gram-ner-with-stanford-nlp
How do I use python interface of Stanford NER(named entity recogniser)?,"<p>I want to use Stanford NER in python using pyner library. Here is one basic code snippet.</p>

<pre><code>import ner 
tagger = ner.HttpNER(host='localhost', port=80)
tagger.get_entities(""University of California is located in California, United States"")
</code></pre>

<p>When I run this on my local python console(IDLE). It should have given me an output like this</p>

<pre><code>  {'LOCATION': ['California', 'United States'],
 'ORGANIZATION': ['University of California']}
</code></pre>

<p>but when I execut this, it showed empty brackets. I am actually new to all this.</p>
","python-2.7, nlp, stanford-nlp, named-entity-recognition","<p>I am able to run the stanford-ner server in socket mode using: </p>

<pre><code>java -mx1000m -cp stanford-ner.jar edu.stanford.nlp.ie.NERServer \
    -loadClassifier classifiers/english.muc.7class.distsim.crf.ser.gz \
    -port 8080 -outputFormat inlineXML
</code></pre>

<p>and receive the following output from the command line:</p>

<pre><code>Loading classifier from 
/Users/roneill/stanford-ner-2012-11-11/classifiers/english.muc.7class.distsim.crf.ser.gz 
... done [1.7 sec].
</code></pre>

<p>Then in python repl:</p>

<pre><code>Python 2.7.2 (default, Jun 20 2012, 16:23:33) 
[GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import ner
&gt;&gt;&gt; tagger = ner.SocketNER(host='localhost', port=8080)
&gt;&gt;&gt; tagger.get_entities(""University of California is located in California, United States"")
{'ORGANIZATION': ['University of California'], 'LOCATION': ['California', 'United States']}
</code></pre>
",27,14,9039,2013-03-30 19:48:35,https://stackoverflow.com/questions/15722802/how-do-i-use-python-interface-of-stanford-nernamed-entity-recogniser
How can one resolve synonyms in named-entity recognition?,"<p>In natural language processing, named-entity recognition is the challenge of, well, recognizing named entities such as organizations, places, and most importantly <em>names</em>.</p>

<p>There is a major challenge in this though that I call that of <em>synonymy</em>: <em>The Count</em> and <em>Dracula</em> are in fact referring to the same person, but it it possible that this is never discussed directly in the text.</p>

<p>What would be the best algorithm to resolve these synonyms?</p>

<hr>

<p>If there is a feature for this in any Python-based library, I'm eager to be educated.  I'm using NLTK.</p>
","nlp, nltk, named-entity-recognition","<p>You are describing a problem of <a href=""http://en.wikipedia.org/wiki/Coreference#Coreference_resolution"" rel=""nofollow noreferrer"">coreference resolution</a> and named entity linking. I'm providing separate links as I am not entirely sure which one you meant.</p>

<ul>
<li>Coreference: <a href=""http://nlp.stanford.edu/software/dcoref.shtml"" rel=""nofollow noreferrer"">Stanford CoreNLP</a> currently has one of the best implementations, but is in Java. I have used the <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""nofollow noreferrer"">python bindings</a> and I wasn't too happy- I ended up running all my data through the Stanford pipeline just once, and then loading the processed XML files in python. Obviously, that doesn't work if you have to be processing in real time.</li>
<li>Named entity linking: Check out <a href=""http://stanbol.apache.org/docs/trunk/components/enhancer/engines/namedentitytaggingengine.html"" rel=""nofollow noreferrer"">Apache Stanbol</a> and the links in the following <a href=""https://stackoverflow.com/questions/4747990/how-to-use-dbpedia-to-extract-tags-keywords-from-content"">Stackoverflow post</a>.</li>
</ul>
",6,5,3096,2013-04-05 13:43:41,https://stackoverflow.com/questions/15835563/how-can-one-resolve-synonyms-in-named-entity-recognition
Which is the best way to obtain data similar to Google Knowldge Graph,"<p>I need data which is similar to GKG primarily for Named Entity Recognition. Basically I want to use the context(extra information) behind an entity for entity recognition. What is the suggested way of using the data?</p>

<p>I found out that DBPedia provides what is needed but I felt the data is incomplete. For some entities like ""Larry_Page"", in instance_types dump of 3.8 version, the content seems to be </p>

<pre><code>    &lt;http://dbpedia.org/resource/Larry_Page&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://dbpedia.org/ontology/Person&gt; .
&lt;http://dbpedia.org/resource/Larry_Page&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://xmlns.com/foaf/0.1/Person&gt; .
&lt;http://dbpedia.org/resource/Larry_Page&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://www.w3.org/2002/07/owl#Thing&gt; .
&lt;http://dbpedia.org/resource/Larry_Page&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://schema.org/Person&gt; .
&lt;http://dbpedia.org/resource/Larry_Page&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://dbpedia.org/ontology/Agent&gt; .
</code></pre>

<p>So, for ""Larry_Page"" the information restricts until ""Person"" type. 
   Is there any way I can get complete hierarchy of entities with their types?  </p>
","wikipedia, dbpedia, named-entity-recognition","<p>Have you tried Freebase? In Freebase they have a /common/topic/notable_for Computer Scientist for Larry Page. The problem with these databases is that you have to do a lot of engineering (magic) to pick the right predicates. And you always have some noise.</p>

<p>P.S. Sorry for posting a response, I don't have enough reputation to add comments :)</p>
",0,0,77,2013-05-16 13:53:31,https://stackoverflow.com/questions/16589352/which-is-the-best-way-to-obtain-data-similar-to-google-knowldge-graph
What do the BILOU tags mean in Named Entity Recognition?,"<p>Title pretty much sums up the question.  I've noticed that in some papers people have referred to a BILOU encoding scheme for NER as opposed to the typical BIO tagging scheme (Such as this paper by Ratinov and Roth in 2009 <a href=""http://cogcomp.cs.illinois.edu/page/publication_view/199"" rel=""noreferrer"">http://cogcomp.cs.illinois.edu/page/publication_view/199</a>)</p>

<p>From working with the 2003 CoNLL data I know that</p>

<pre><code>B stands for 'beginning' (signifies beginning of an NE)
I stands for 'inside' (signifies that the word is inside an NE)
O stands for 'outside' (signifies that the word is just a regular word outside of an NE)
</code></pre>

<p>While I've been told that the words in BILOU stand for</p>

<pre><code>B - 'beginning'
I - 'inside'
L - 'last'
O - 'outside'
U - 'unit'
</code></pre>

<p>I've also seen people reference another tag </p>

<pre><code>E - 'end', use it concurrently with the 'last' tag
S - 'singleton', use it concurrently with the 'unit' tag
</code></pre>

<p>I'm pretty new to the NER literature, but I've been unable to find something clearly explaining these tags.  My questions in particular relates to what the difference between 'last' and 'end' tags are, and what 'unit' tag stands for.</p>
","nlp, named-entity-recognition","<p>Based on an <a href=""http://code.google.com/p/cleartk/issues/detail?id=311"" rel=""noreferrer"">issue</a> and a <a href=""http://code.google.com/p/cleartk/issues/attachmentText?id=311&amp;aid=3110003000&amp;name=bilou.patch&amp;token=be5BXdd4q7kFCHnKlHX_Xksl32Y:1366250844964"" rel=""noreferrer"">patch</a> in Clear TK, it seems like BILOU stands for ""Beginning, Inside and Last tokens of <strong>multi-token chunks</strong>, <strong>Unit-length chunks</strong> and Outside"" (emphasis added). For instance, the chunking denoted by brackets</p>

<pre><code>(foo foo foo) (bar) no no no (bar bar)
</code></pre>

<p>can be encoded with BILOU as </p>

<pre><code>B-foo, I-foo, L-foo, U-bar, O, O, O, B-bar, L-bar
</code></pre>
",54,60,29140,2013-06-14 20:05:35,https://stackoverflow.com/questions/17116446/what-do-the-bilou-tags-mean-in-named-entity-recognition
Difference between IOB Accuracy and Precision,"<p>I'm doing some works on NLTK with named entity recognition and chunkers. I retrained a classifier using <code>nltk/chunk/named_entity.py</code> for that and I got the following mesures:</p>

<pre><code>ChunkParse score:
    IOB Accuracy:  96.5%
    Precision:     78.0%
    Recall:        91.9%
    F-Measure:     84.4%
</code></pre>

<p>But I don't understand what is the exact difference between IOB Accuracy and Precision in this case. Actually, I found on the docs (<a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""nofollow"">here</a>) the following for an specific example:</p>

<blockquote>
  <p>The IOB tag accuracy indicates that more than a third of the words are
  tagged with O, i.e. not in an NP chunk. However, since our tagger did
  not find any chunks, its precision, recall, and f-measure are all
  zero.</p>
</blockquote>

<p>So, if IOB accuracy is just the number of O labels, how come we don't have chunks and IOB accuracy is not 100% at the same time, in that example?</p>

<p>Thank you in advance</p>
","python, nlp, nltk, precision, named-entity-recognition","<p>There is a very detailed explanation of the difference between precision and accuracy on wikipedia (see <a href=""https://en.wikipedia.org/wiki/Accuracy_and_precision"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Accuracy_and_precision</a>), in brief:</p>

<pre><code>accuracy = (tp + tn) / (tp + tn + fp + fn)
precision = tp / tp + fp
</code></pre>

<p>Back to NLTK, there is a module call <a href=""http://nltk.googlecode.com/svn-/trunk/doc/api/nltk.chunk.util-pysrc.html#ChunkScore"" rel=""noreferrer"">ChunkScore</a> that computes the <code>accuracy</code>, <code>precision</code> and <code>recall</code> of your system. And here's the funny part the way NLTK calculates the <code>tp,fp,tn,fn</code> for <code>accuracy</code> and <code>precision</code>, it does at different granularity. </p>

<p>For <strong>accuracy</strong>, NLTK calculates the total number of tokens (<strong>NOT CHUNKS!!</strong>) that are guessed correctly with the POS tags and IOB tags, then divided by the total number of tokens in the gold sentence.</p>

<pre><code>accuracy = num_tokens_correct / total_num_tokens_from_gold
</code></pre>

<p>For <strong>precision</strong> and <strong>recall</strong>, NLTK calculates the:</p>

<ul>
<li><code>True Positives</code> by counting the number of chunks (<strong>NOT TOKENS!!!</strong>) that are guessed correctly</li>
<li><code>False Positives</code> by counting the number of chunks (<strong>NOT TOKENS!!!</strong>) that are guessed but they are wrong.</li>
<li><code>True Negatives</code> by counting the number of chunks (<strong>NOT TOKENS!!!</strong>) that are not guessed by the system.</li>
</ul>

<p>And then calculates the precision and recall as such:</p>

<pre><code>precision = tp / fp + tp
recall = tp / fn + tp
</code></pre>

<p>To prove the above points, try this script:</p>

<pre><code>from nltk.chunk import *
from nltk.chunk.util import *
from nltk.chunk.regexp import *
from nltk import Tree
from nltk.tag import pos_tag

# Let's say we give it a rule that says anything with a [DT NN] is an NP
chunk_rule = ChunkRule(""&lt;DT&gt;?&lt;NN.*&gt;"", ""DT+NN* or NN* chunk"")
chunk_parser = RegexpChunkParser([chunk_rule], chunk_node='NP')

# Let's say our test sentence is:
# ""The cat sat on the mat the big dog chewed.""
gold = tagstr2tree(""[ The/DT cat/NN ] sat/VBD on/IN [ the/DT mat/NN ] [ the/DT big/JJ dog/NN ] chewed/VBD ./."")

# We POS tag the sentence and then chunk with our rule-based chunker.
test = pos_tag('The cat sat on the mat the big dog chewed .'.split())
chunked = chunk_parser.parse(test)

# Then we calculate the score.
chunkscore = ChunkScore()
chunkscore.score(gold, chunked)
chunkscore._updateMeasures()

# Our rule-based chunker says these are chunks.
chunkscore.guessed()

# Total number of tokens from test sentence. i.e.
# The/DT , cat/NN , on/IN , sat/VBD, the/DT , mat/NN , 
# the/DT , big/JJ , dog/NN , chewed/VBD , ./.
total = chunkscore._tags_total
# Number of tokens that are guessed correctly, i.e.
# The/DT , cat/NN , on/IN , the/DT , mat/NN , chewed/VBD , ./.
correct = chunkscore._tags_correct
print ""Is correct/total == accuracy ?"", chunkscore.accuracy() == (correct/total)
print correct, '/', total, '=', chunkscore.accuracy()
print ""##############""

print ""Correct chunk(s):"" # i.e. True Positive.
correct_chunks = set(chunkscore.correct()).intersection(set(chunkscore.guessed()))
##print correct_chunks
print ""Number of correct chunks = tp = "", len(correct_chunks)
assert len(correct_chunks) == chunkscore._tp_num
print

print ""Missed chunk(s):"" # i.e. False Negative.
##print chunkscore.missed()
print ""Number of missed chunks = fn = "", len(chunkscore.missed())
assert len(chunkscore.missed()) == chunkscore._fn_num
print 

print ""Wrongly guessed chunk(s):"" # i.e. False positive.
wrong_chunks = set(chunkscore.guessed()).difference(set(chunkscore.correct()))
##print wrong_chunks
print ""Number of wrong chunks = fp ="", len(wrong_chunks)
print chunkscore._fp_num
assert len(wrong_chunks) == chunkscore._fp_num
print 

print ""Recall = "", ""tp/fn+tp ="", len(correct_chunks), '/', len(correct_chunks)+len(chunkscore.missed()),'=', chunkscore.recall()

print ""Precision ="", ""tp/fp+tp ="", len(correct_chunks), '/', len(correct_chunks)+len(wrong_chunks), '=', chunkscore.precision()
</code></pre>
",8,4,3255,2013-06-26 16:27:49,https://stackoverflow.com/questions/17325554/difference-between-iob-accuracy-and-precision
Stanford NER prop file meaning of DistSim,"<p>In one of the example .prop files coming with the Stanford NER software there are two options I do not understand:</p>

<pre><code>useDistSim = true
distSimLexicon = /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters
</code></pre>

<p>Does anyone have a hint what DistSim stands for and where I can find any more documentation on how to use these options?</p>

<p>UPDATE: I just found out that DistSim means distributional similarity. I still wonder what that means in this context.</p>
","nlp, stanford-nlp, named-entity-recognition","<p>""DistSim"" refers to using features based on word classes/clusters, built using distributional similarity clustering methods (e.g., Brown clustering, exchange clustering). Word classes group words which are similar, semantically and/or syntactically, and allow an NER system to generalize better, including handling words not in the training data of the NER system better. Many of our distributed models use a distributional similarity clustering features as well as word identity features, and gain significantly from doing so.  In Stanford NER, there are a whole bunch of flags/properties that affect how distributional similarity is interpreted/used: <code>useDistSim</code>, <code>distSimLexicon</code>, <code>distSimFileFormat</code>, <code>distSimMaxBits</code>, <code>casedDistSim</code>, <code>numberEquivalenceDistSim</code>, <code>unknownWordDistSimClass</code>, and you need to look at the code in <code>NERFeatureFactory.java</code> to decode the details, but in the simple case, you just need the first two, and they need to be used while training the model, as well as at test time.  The default format of the lexicon is just a text file with a series of lines with two tab separated columns of <code>word clusterName</code>. The cluster names are arbitrary.</p>
",9,4,1689,2013-07-18 12:59:03,https://stackoverflow.com/questions/17724164/stanford-ner-prop-file-meaning-of-distsim
Methods for extracting locations from text?,"<p>What are the recommended methods for extracting locations from free text? </p>

<p>What I can think of is to use regex rules like ""words ... in location"". But are there better approaches than this?</p>

<p>Also I can think of having a lookup hash table table with names for countries and cities and then compare every extracted token from the text to that of the hash table.</p>

<p>Does anybody know of better approaches?</p>

<p>Edit: I'm trying to extract locations from tweets text. So the issue of high number of tweets might also affect my choice for a method.</p>
","nlp, text-mining, information-extraction, named-entity-recognition, named-entity-extraction","<p>All rule-based approaches will fail (if your text is really ""free""). That includes regex, context-free grammars, any kind of lookup... Believe me, I've been there before :-)</p>

<p>This problem is called <strong>Named Entity Recognition</strong>. Location is one of the 3 most studied classes (with Person and Organization). Stanford NLP has an open source Java implementation that is extremely powerful: <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"">http://nlp.stanford.edu/software/CRF-NER.shtml</a></p>

<p>You can easily find implementations in other programming languages.</p>
",11,10,6636,2013-07-20 12:58:47,https://stackoverflow.com/questions/17762516/methods-for-extracting-locations-from-text
how to use Entity Recognition with Apache solr and LingPipe or similar tools,"<p>I would like to use NLP while indexing the data with Apache Solr.</p>

<ol>
<li><p>Identify the synonyms of the words and index that also.</p></li>
<li><p>Identify thenamed entity and label it while indexing. </p></li>
<li><p>when some one query the Solr Index, I should able to extract the
named entity and intention from the query and form the query string,
so that it can effectively search the indexed file.</p></li>
</ol>

<p>Is there any tools / plugins available to satisfy my requirements? I believe it is a common use cases for most of the content based websites. How people handling it?  </p>
","solr, nlp, named-entity-recognition, stanford-nlp","<p>Here's <a href=""http://www.searchbox.com/named-entity-recognition-ner-in-solr/"" rel=""nofollow"">a tutorial on using Stanford NER with SOLR</a>.</p>
",4,1,5904,2013-08-04 09:46:15,https://stackoverflow.com/questions/18041663/how-to-use-entity-recognition-with-apache-solr-and-lingpipe-or-similar-tools
Is it good to use stanford temporal tagger for big data?,"<p>I'm exploring Stanford Temporal Tagger for my project to extract date entity from the text. The demo from <a href=""http://nlp.stanford.edu:8080/sutime/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/sutime/process</a> seems to be promising. I would like to understand whether this library is matured. And also somebody help me understanding how this library performs with big data. It would be also helpful if you can guide me about other java based  temporal tagger libraries especially for big data requirements. Is there any apache project which does temporal tagging? </p>

<p>I found some of the libraries like </p>

<p><a href=""https://code.google.com/p/heideltime/"" rel=""nofollow"">https://code.google.com/p/heideltime/</a></p>

<p><a href=""https://code.google.com/p/stemptag/"" rel=""nofollow"">https://code.google.com/p/stemptag/</a></p>
","nlp, stanford-nlp, information-extraction, named-entity-recognition","<p>Yes, the SUTime library is mature and quite accurate, and has been run over tens of millions of words of text. (Just make sure you are not invoking the more expensive and much slower parts of Stanford CoreNLP - parsing and dcoref - which are not needed for temporal tagging.)</p>

<p>Heideltime is another very good Java library for temporal tagging. It has the advantage of supporting several languages, whereas SUTime at present only supports English. It has the disadvantage of coming configured to use TreeTagger as its part-of-speech tagger, which means you either need to deal with using this non-open source, non-Java component, or you need to write stuff to get it configured to use some other POS tagger. I'm not familiar with stemptag; I don't think there is any apache project for this.</p>
",4,2,1253,2013-09-03 13:20:58,https://stackoverflow.com/questions/18593443/is-it-good-to-use-stanford-temporal-tagger-for-big-data
Training Named Entity in OpenNLP,"<p>I want to train a corpus for Indian names:</p>

<pre><code>class NameTraining
{
    public static void TrainNames() throws IOException 
    {
        Charset charset = Charset.forName(""UTF-8"");         
        FileReader fileReader = new FileReader(""train.txt"");
        ObjectStream fileStream = new PlainTextByLineStream(fileReader);
        ObjectStream sampleStream = new NameSampleDataStream(fileStream);
        TokenNameFinderModel model = NameFinderME.train(""pt-br"", ""train"", sampleStream, Collections.&lt;String, Object&gt;emptyMap());
        NameFinderME nfm = new NameFinderME(model); 
    }

    public static void main(String args[]) throws IOException
    {
        NameTraining det = new NameTraining();
        det.TrainNames();
    }
}
</code></pre>

<p>I compile this using the command:</p>

<pre><code>javac -cp $(echo lib/*.jar | tr ' ' ':') NameTraining.java -Xlint:unchecked
</code></pre>

<p>However I get these error  messages</p>

<pre><code>NameTraining.java:35: warning: [unchecked] unchecked conversion
found   : opennlp.tools.util.ObjectStream
required: opennlp.tools.util.ObjectStream&lt;java.lang.String&gt;
        ObjectStream sampleStream = new NameSampleDataStream(fileStream);
                                                             ^
NameTraining.java:36: warning: [unchecked] unchecked conversion
found   : opennlp.tools.util.ObjectStream
required: opennlp.tools.util.ObjectStream&lt;opennlp.tools.namefind.NameSample&gt;
        TokenNameFinderModel model = NameFinderME.train(""pt-br"", ""train"", sampleStream, Collections.&lt;String, Object&gt;emptyMap());
                                                                          ^
2 warnings
</code></pre>

<p>I want to know two things</p>

<ol>
<li>Is the above code correct for training, and if yes, then how do I check the results after training?</li>
<li>What do the warnings mean?</li>
</ol>
","java, bash, opennlp, named-entity-recognition","<p>Hi I got a brief successful training data set</p>

<pre><code>public static void TrainNames() throws IOException
    {
        Charset charset = Charset.forName(""UTF-8"");
        ObjectStream&lt;String&gt; lineStream =new PlainTextByLineStream(new FileInputStream(""/home/yogi.singh/dev/java/nlp/data/en-ner-person.train""), charset);
        ObjectStream&lt;NameSample&gt; sampleStream = new NameSampleDataStream(lineStream);       
        //FileReader fileReader = new FileReader(""train.txt"");
        //ObjectStream fileStream = new PlainTextByLineStream(fileReader);
        //ObjectStream sampleStream = new NameSampleDataStream(fileStream);
        TokenNameFinderModel model = NameFinderME.train(""en"", ""person"", sampleStream, Collections.&lt;String, Object&gt;emptyMap());
        NameFinderME nfm = new NameFinderME(model);
        String sentence = """";


        BufferedReader br = new BufferedReader(new FileReader(""/home/yogi.singh/dev/java/nlp/train.txt""));
        try
         {
            StringBuilder sb = new StringBuilder();
            String line = br.readLine();

            while (line != null)
            {
                sb.append(line);
                sb.append('\n');
                line = br.readLine();
            }
            sentence = sb.toString();
         } 
        finally
        {
            br.close();
        }

        InputStream is1 = new FileInputStream(""/home/yogi.singh/dev/java/nlp/data/en-token.bin"");
        TokenizerModel model1 = new TokenizerModel(is1);

        Tokenizer tokenizer = new TokenizerME(model1);

        String tokens[] = tokenizer.tokenize(sentence);

        for (String a : tokens)
            System.out.println(a);

        Span nameSpans[] = nfm.find(tokens);
        for(Span s: nameSpans)
        {
            System.out.print(s.toString());
            System.out.print("" "");
            for(int index = s.getStart();index &lt; s.getEnd();index++)
            {
                System.out.print(tokens[index] + "" "");
            }
            System.out.println("" "");
        }
    }
</code></pre>
",2,0,1911,2013-10-16 07:14:31,https://stackoverflow.com/questions/19397291/training-named-entity-in-opennlp
"OpenNLP NameFinder training, &quot;Found unexpected annotation&quot;","<p>In training my NameFinderME, I get the following error message:</p>

<p>My data looks as follows:
<code>&lt;START someTag&gt; some text &lt;END&gt;</code></p>

<p><code>Computing event counts...  java.io.IOException: Found unexpected annotation:</code></p>

<p>In everything else Google has found me for this error message, it's always a simple error in the spacing of the training data (e.g., change <code>&lt;START:entity&gt;some text&lt;END&gt;</code> to  <code>&lt;START:entity&gt; some text &lt;END&gt;</code> .  This isn't applicable to me (it's all correctly spaced.)  It's all UTF-16, and specified to be so when I set up the objects to do the training.  Any ideas on what could be wrong?</p>

<p>Thank you,</p>

<p>WalrusTheCat</p>
","opennlp, named-entity-recognition, training-data","<p>The data is incorrect.  Instead of <code>&lt;START someTag&gt;</code> I need <code>&lt;START:someTag&gt;</code>.  I was using a space instead of a colon.</p>
",5,2,664,2013-11-20 00:15:04,https://stackoverflow.com/questions/20084946/opennlp-namefinder-training-found-unexpected-annotation
What is the best way to match substring from a big string to a huge list of keywords,"<p>Imagine you have millions of records containing text with average 2000 words (each), and also you have an other list with about 100000 items.</p>

<p>e.g: In the keywords list you a have item like ""president Obama"" and in one of the text records you have some thing like this: ""..... president Obama ...."", so i want to find this keyword in the text and replace it with some thing like this: ""..... {president Obama} ...."" to highlight the keyword in the text, the keywords list contains multi-noun word like the example.</p>

<p>What is the fastest way to this in such a huge list with millions of text records?</p>

<p>Notes:</p>

<ol>
<li><p>Now I do this work in a greedy way, check word by word and match them, but it takes about 2 seconds for each text record, and I want some thing near zero time.</p></li>
<li><p>Also I know this is something like named-entity-recognition and I worked with many of the NER framework such as Gate and ..., but because I want this for a language which is not supported by the frameworks I want to to this manually.</p></li>
</ol>
","c#, regex, lookup, string-matching, named-entity-recognition","<p>Assumptions: Most keywords are single words, but there are som multi word keywords.</p>

<p>My suggestion.</p>

<p>Hash the keywords based on the first word.  So ""President"",""President Obama"" and ""President Clinton"" will all hash to the same value.</p>

<p>Then search word-by-word by computing the hashes.  On hash matches implement logic to check if you have a match on a multi word keyword.</p>

<p>Calculating the hashes will be the most expensive operation of this solution and should be linear in the length of the input string.</p>
",2,8,284,2013-11-26 07:55:49,https://stackoverflow.com/questions/20211380/what-is-the-best-way-to-match-substring-from-a-big-string-to-a-huge-list-of-keyw
java.lang.NoClassDefFoundError CRFClassifier in a Rails app,"<p>I'm trying to run the CRFClassifier on a string to extract entities from the string. I'm using the Ruby bindings for the Stanford NLP entity recognizer from here: <a href=""https://github.com/tiendung/ruby-nlp"" rel=""nofollow"">https://github.com/tiendung/ruby-nlp</a></p>

<p>It works perfectly fine on its own class say (nlp.rb). When I run <code>ruby nlp.rb</code> it works fine. However, I've tried to create an object of this class inside one of my controllers in my rails app and for some reason I'm getting the following error: </p>

<p><strong><code>java.lang.NoClassDefFoundError: edu/stanford/nlp/ie/crf/CRFClassifier</code></strong></p>

<p>Here is the code that works fine on its own but not inside a controller.</p>

<pre><code>    def initialize
        Rjb::load('stanford-postagger.jar:stanford-ner.jar', ['-Xmx200m'])
        crfclassifier = Rjb::import('edu.stanford.nlp.ie.crf.CRFClassifier')
        maxentTagger = Rjb::import('edu.stanford.nlp.tagger.maxent.MaxentTagger')
        maxentTagger.init(""left3words-wsj-0-18.tagger"")
        sentence = Rjb::import('edu.stanford.nlp.ling.Sentence')
        @classifier = crfclassifier.getClassifierNoExceptions(""ner-eng-ie.crf-4-conll.ser.gz"")


    end


    def get_entities(sentence)
        sent = sentence
        @classifier.testStringInlineXML( sent )

    end
</code></pre>

<p>It's the same exact code in both cases. Anyone has any idea of what's happening here!?</p>

<p>Thanks in advance!</p>
","ruby, nlp, stanford-nlp, named-entity-recognition","<p>I think you need this:</p>

<p>Rjb::load('/path/to/jar/stanford-postagger.jar:/path/to/jar/stanford-ner.jar', ['-Xmx200m'])</p>

<p>I just tried this and it works. Create a dir in lib called nlp. Put the jars there and then create a class which loads the jars using the full path:</p>

<p>So you end up with:</p>

<pre><code>├── lib
│   ├── nlp
│   │   ├── stanford-ner.jar
│   │   └── stanford-postagger.jar
│   └── nlp.rb



require 'rjb'

class NLP
  def initialize
    pos_tagger = File.expand_path('../nlp/stanford-postagger.jar', __FILE__)
    ner = File.expand_path('../nlp/stanford-ner.jar', __FILE__)
    Rjb::load(""#{pos_tagger}:#{ner}"", ['-Xmx200m'])
    crfclassifier = Rjb::import('edu.stanford.nlp.ie.crf.CRFClassifier')
    maxentTagger = Rjb::import('edu.stanford.nlp.tagger.maxent.MaxentTagger')
    maxentTagger.init(""left3words-wsj-0-18.tagger"")
    sentence = Rjb::import('edu.stanford.nlp.ling.Sentence')
    @classifier = crfclassifier.getClassifierNoExceptions(""ner-eng-ie.crf-4-conll.ser.gz"")
  end


  def get_entities(sentence)
    sent = sentence
    @classifier.testStringInlineXML( sent )
  end
end
</code></pre>

<p>Little test class:</p>

<pre><code>require_relative 'lib/nlp'

n = NLP.new
n.get_entities(""Good afternoon Rajat Raina, how are you today?"")
</code></pre>

<p>output:</p>

<pre><code>ruby t.rb
Loading classifier from /Users/brendan/code/ruby/ruby-nlp/ner-eng-ie.crf-4-conll.ser.gz ... done [1.2 sec].
Getting data from Good afternoon Rajat Raina, how are you today? (default encoding)
Good afternoon &lt;PERSON&gt;Rajat Raina&lt;/PERSON&gt;, how are you today?
</code></pre>
",1,1,608,2014-01-24 21:29:21,https://stackoverflow.com/questions/21342811/java-lang-noclassdeffounderror-crfclassifier-in-a-rails-app
Custom NER and POS tagging,"<p>I was checking out Stanford CoreNLP in order to understand NER and POS tagging. But what if I want to create custom tags for entities like<code>&lt;title&gt;Nights&lt;/title&gt;, &lt;genre&gt;Jazz&lt;/genre&gt;, &lt;year&gt;1992&lt;/year&gt;</code> How can I do it? is CoreNLP useful in this case?</p>
","nlp, stanford-nlp, named-entity-recognition, pos-tagger","<p>CoreNLP out-of-the-box will be restricted to types they mention : PERSON, LOCATION, ORGANIZATION, MISC, DATE, TIME, MONEY, NUMBER. No, you won't be able to recognize other entities just by assuming it could ""intuitively"" do it :)</p>

<p>In practice, you'll have to choose, either:</p>

<ol>
<li>Find another NER systems that tags those types</li>
<li>Address this tagging task using knowledge-based / unsupervised approaches.</li>
<li>Search for extra resources (corpora) that contain types you want recognize, and re-train a supervised NER system (CoreNLP or other)</li>
<li>Build (and possibly annotate) your own resources - then you'll have to define an annotation scheme, rules, etc. - quite an interesting part of the work!</li>
</ol>

<p>Indeed, unless you find an existing system that fulfills your needs, some effort will be required! Unsupervised approaches may help you bootstrapping a system, so as to see if you need to find / annotate a dedicated corpus. In the latter case, it would be better to separate data as train/dev/test parts, so as to be able to assess how much the resulting system performs on unseen data.</p>
",4,4,2497,2014-01-26 00:03:02,https://stackoverflow.com/questions/21357881/custom-ner-and-pos-tagging
How to run multiple classifiers with Stanford NER?,"<p>I'd like to run one of the built-in classifiers on a file, then run my own classifier, merging the results.</p>

<p>How do I do so with Stanford NER, in particular, via the command line?</p>

<p>I am aware of <a href=""https://stackoverflow.com/questions/15746695/how-do-i-include-more-than-one-classifiers-when-using-stanford-named-entity-reco"">How do I include more than one classifiers when using Stanford named entity recogniser?</a> , but this is slightly different, as that questions asks about multiple classifiers with <strong>NERServer</strong>.</p>

<p>Looks like I need to use CoreNLP to run multiple NER models in sequence...can I do it without CoreNLP?</p>

<p>Say I had a file with contents ""the quick brown fox jumped over the lazy dog in America"". I run the one of the built-in classifiers, and it finds ""America"" as a location, then I run my own, and it finds ""fox"" and ""dog"", the result should be:</p>

<pre><code>the quick brown &lt;animal&gt;fox&lt;/animal&gt; jumped over the lazy &lt;animal&gt;dog&lt;/animal&gt; in &lt;location&gt;America&lt;/location
</code></pre>
","nlp, stanford-nlp, named-entity-recognition","<p>So, a place to get started if you're dead set on doing this in a single command from the command line: </p>

<pre><code>cat corpus.txt | tee `stanfordNER -options here &gt; out1.xml` | myNERTagger -options here &gt; out2.xml &amp;&amp; diff out1.xml out2.xml | awk to do whatever merging you want here...
</code></pre>

<p>But what you'll likely find is that this is not a solution. You're going to want to go sentence-by-sentence in a little script, calling <a href=""https://github.com/dat/pyner"" rel=""nofollow"">pyner</a> or similar to hook into the Stanford tagger and then whatever custom tagger you've built, merging the differences as you go along. The output formatting of your taggers will change how this looks pretty dramatically. </p>
",0,2,1164,2014-01-30 19:46:03,https://stackoverflow.com/questions/21466083/how-to-run-multiple-classifiers-with-stanford-ner
How do I use IOB tags with Stanford NER?,"<p>There seem to be a few different settings:</p>

<pre><code>iobtags
iobTags
entitySubclassification (IOB1 or IOB2?)
evaluateIOB
</code></pre>

<p>Which setting do I use, and how do I use it correctly?</p>

<p>I tried labelling like this:</p>

<pre><code>1997    B-DATE
volvo   B-BRAND
wia64t  B-MODEL
highway B-TYPE
tractor I-TYPE
</code></pre>

<p>But on the training output, it seemed to think that B-TYPE and I-TYPE were different classes. </p>

<p>I am using the 2013-11-12 release. </p>
","stanford-nlp, named-entity-recognition","<p>How this can be done is currently (2013 releases) a bit of a mess, since there are two different sets of flags for two different <code>DocumentReaderAndWriter</code> implementations. Sorry.</p>

<p>The most flexible support for different IOB styles is found in <code>CoNLLDocumentReaderAndWriter</code>. You can have it map any IOB/IOE/... annotation done by hyphenated prefixes like your examples (B-BRAND) to any other while it is reading files with the flag:</p>

<pre><code>-entitySubclassification IOB2
</code></pre>

<p>The resulting label set is then used for training and classification. The options are documented in the <code>entitySubclassify()</code> method of <code>CoNLLDocumentReaderAndWriter</code>: IOB1, IOB2, IOE1, IOE2, SBIEO, IO. You can find a discussion of IOB1 vs. IOB2 in <a href=""http://acl.ldc.upenn.edu/E/E99/E99-1023.pdf"">Tjong Kim Sang and Veenstra 1999</a>. By default the representation is mapped back to IOB1 on output, since that is the default used in the CoNLL <code>conlleval</code> program, but you can keep it as what you mapped it to with the flag: </p>

<pre><code>-retainEntitySubclassification
</code></pre>

<p>To use this <code>DocumentReaderAndWriter</code>, you can give a training command like:</p>

<pre><code>java8 -mx6g edu.stanford.nlp.ie.crf.CRFClassifier -prop conll.crf.chris2009.prop -readerAndWriter edu.stanford.nlp.sequences.CoNLLDocumentReaderAndWriter -entitySubclassification iob2
</code></pre>

<p>Alternatively, <code>ColumnDocumentReaderAndWriter</code> is the default <code>DocumentReaderAndWriter</code> which we use in the distributed models. The options you get with it are different and slightly more limited. You have these two flags:</p>

<ul>
<li><code>-mergeTags</code> will take either plain (""BRAND"") or CoNLL-like (""I-BRAND"") labels and map them down to a prefix-less IO label (""BRAND"") and use that for training and classifying.</li>
<li><code>-iobTags</code> can take either plain (""BRAND"") or CoNLL-like (""I-BRAND"") labels and maps them to IOB2.</li>
</ul>

<p>In a sequence model, for any of the labeling schemes like IOB2, the labels <strong>are</strong> different classes. That is how these labeling schemes work. The special interpretation of ""I-"", ""B-"", etc. is left to the human observer and entity-level evaluation software. The included evaluation software will work with IOB1, IOB2, or prefixless IO encoding only. </p>
",14,7,7411,2014-01-30 22:48:22,https://stackoverflow.com/questions/21469082/how-do-i-use-iob-tags-with-stanford-ner
Adjectives used with named entities,"<p>I have used the python code given below to extract named entities present in the text. Now i need to get the adjectives from those sentences in the text where there is a named entity . i.e the adjective used with named entities. Can i alter my code to check whether the tree has 'JJ' if there is 'NE', or is there any other approach??</p>

<pre><code>def tokenize(text): 
sentences = nltk.sent_tokenize(text) 
sentences = [nltk.word_tokenize(sent) for sent in sentences] 
sentences = [nltk.pos_tag(sent) for sent in sentences] 
return sentences 

text=open(""file.txt"",""r"").read() 
sentences=tokenize(text) 
chunk_sent=nltk.batch_ne_chunk(sentences,binary=True)
print chunk_sent[1]
</code></pre>

<p>The output:</p>

<p>Tree('S', [(""'"", 'POS'), ('Accomplished', 'NNP'), ('in', 'IN'), ('speech', 'NN'), (',', ','), Tree('NE', [('Gautam', 'NNP')]), (',', '
,'), ('thus', 'RB'), ('questioned', 'VBD'), (',', ','), ('gave', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('midst', 'NN'), ('of', 'IN'),
('that', 'DT'), ('big', 'JJ'), ('assemblage', 'NN'), ('of', 'IN'), ('contemplative', 'JJ'), ('sages' 'NNP'), ('a', 'DT'), ('full', '
JJ'), ('and', 'CC'), ('proper', 'NN'), ('answer', 'NN'), ('in', 'IN'), ('words', 'NNS'), ('consonant', 'JJ'), ('with', 'IN'), ('their
', 'PRP$'), ('mode', 'NN'), ('of', 'IN'), ('life', 'NN'), ('.', '.')])</p>

<p>Though this sentence doesnt have a JJ before NE.How can i get the JJ used with NE?</p>

<pre><code>def ne(tree):
    names = []
    if hasattr(tree, 'node') and tree.node:
      if tree.node == 'NE':
        names.append(' '.join([child[0] for child in tree]))
    else:
        for child in tree:
            names.extend(ne(child))

return names

names = []
for item in chunk_sent:
   names.extend(ne(item))
print names
</code></pre>
","python, entity-framework, nltk, named-entity-recognition","<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from nltk import batch_ne_chunk as bnc
&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; sentences = brown.tagged_sents()[0:5]
&gt;&gt;&gt; chunk_sents = bnc(sentences)
&gt;&gt;&gt; 
&gt;&gt;&gt; for sent in chunk_sents:
...     for i,j in zip(sent[:-1], sent[1:]):
...             if type(j) is Tree and i[1].startswith(""JJ""):
...                     print i,j
... 
('Grand', 'JJ-TL') (PERSON Jury/NN-TL)
('Executive', 'JJ-TL') (ORGANIZATION Committee/NN-TL)
</code></pre>
",0,0,700,2014-01-31 13:50:40,https://stackoverflow.com/questions/21481705/adjectives-used-with-named-entities
grouping all Named entities in a Document,"<p>I would like to group all named entities in a given document.
For Example,</p>

<pre><code>**Barack Hussein Obama** II  is the 44th and current President of the United States, and the first African American to hold the office. 
</code></pre>

<p>I do not want to use OpenNLP APIs as it might not be able to recognize all named entities.
Is there any way to generate such n-grams using other services or may be a way to group all noun terms together.</p>
","n-gram, named-entity-recognition, part-of-speech","<p>If you want to avoid using NER, you could use a sentence chunker or parser. This will extract noun phrases generically. OpenNLP has a sentence chunker and parser, but if you are for some reason adverse to using OpenNLP, you can try others.
If you are interested in using the OpenNLP chunker i will post some code that extracts noun phrases using OpenNLP.</p>

<p>Here is the code. You will need to download the models from sourceforge here</p>

<p><a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">http://opennlp.sourceforge.net/models-1.5/</a></p>

<pre><code>import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import opennlp.tools.chunker.ChunkerME;
import opennlp.tools.chunker.ChunkerModel;
import opennlp.tools.postag.POSModel;
import opennlp.tools.postag.POSTaggerME;
import opennlp.tools.tokenize.TokenizerME;
import opennlp.tools.tokenize.TokenizerModel;
import opennlp.tools.util.Span;

/**
 *
 * Extracts noun phrases from a sentence. To create sentences using OpenNLP use
 * the SentenceDetector classes.
 */
public class OpenNLPNounPhraseExtractor {

  static final int N = 2;

  public static void main(String[] args) {

    try {
      String modelPath = ""c:\\temp\\opennlpmodels\\"";
      TokenizerModel tm = new TokenizerModel(new FileInputStream(new File(modelPath + ""en-token.zip"")));
      TokenizerME wordBreaker = new TokenizerME(tm);
      POSModel pm = new POSModel(new FileInputStream(new File(modelPath + ""en-pos-maxent.zip"")));
      POSTaggerME posme = new POSTaggerME(pm);
      InputStream modelIn = new FileInputStream(modelPath + ""en-chunker.zip"");
      ChunkerModel chunkerModel = new ChunkerModel(modelIn);
      ChunkerME chunkerME = new ChunkerME(chunkerModel);
      //this is your sentence
      String sentence = ""Barack Hussein Obama II  is the 44th and current President of the United States, and the first African American to hold the office."";
      //words is the tokenized sentence
      String[] words = wordBreaker.tokenize(sentence);
      //posTags are the parts of speech of every word in the sentence (The chunker needs this info of course)
      String[] posTags = posme.tag(words);
      //chunks are the start end ""spans"" indices to the chunks in the words array
      Span[] chunks = chunkerME.chunkAsSpans(words, posTags);
      //chunkStrings are the actual chunks
      String[] chunkStrings = Span.spansToStrings(chunks, words);
      for (int i = 0; i &lt; chunks.length; i++) {
        if (chunks[i].getType().equals(""NP"")) {
          System.out.println(""NP: \n\t"" + chunkStrings[i]);
          String[] split = chunkStrings[i].split("" "");

          List&lt;String&gt; ngrams = ngram(Arrays.asList(split), N, "" "");
          System.out.println(""ngrams:"");
          for (String gram : ngrams) {
            System.out.println(""\t"" + gram);
          }

        }
      }


    } catch (IOException e) {
    }
  }

  public static List&lt;String&gt; ngram(List&lt;String&gt; input, int n, String separator) {
    if (input.size() &lt;= n) {
      return input;
    }
    List&lt;String&gt; outGrams = new ArrayList&lt;String&gt;();
    for (int i = 0; i &lt; input.size() - (n - 2); i++) {
      String gram = """";
      if ((i + n) &lt;= input.size()) {
        for (int x = i; x &lt; (n + i); x++) {
          gram += input.get(x) + separator;
        }
        gram = gram.substring(0, gram.lastIndexOf(separator));
        outGrams.add(gram);
      }
    }
    return outGrams;
  }
}
</code></pre>

<p>the output I get with your sentence is this (with N set to 2 (bigram)</p>

<pre><code>NP: 
    Barack Hussein Obama II
ngrams:
    Barack Hussein
    Hussein Obama
    Obama II
NP: 
    the 44th and current President
ngrams:
    the 44th
    44th and
    and current
    current President
NP: 
    the United States
ngrams:
    the United
    United States
NP: 
    the first African American
ngrams:
    the first
    first African
    African American
NP: 
    the office
ngrams:
    the
    office
</code></pre>

<p>this does not explicitly handle the case of when an adjective falls outside of the NP... if so you can get this info from the POS tags and integrate it. What I gave you should send you in the right direction.</p>
",4,0,1748,2014-02-04 12:30:40,https://stackoverflow.com/questions/21552647/grouping-all-named-entities-in-a-document
How can I extract Name and Type of all the entities in Freebase?,"<p>i am trying to extract the all the Named entities and its type in Freebase dump.</p>

<p>I have looked at 
<a href=""https://stackoverflow.com/questions/16594646/how-to-extract-freebase-data-dump-for-a-particular-topic"">How to extract Freebase Data Dump for a particular topic</a> 
But i didn't get actual named entities in Freebase dump.</p>

<p>I'am trying to get the information something like this,
Name: Barak Obama type: Person
Name: New York type: City etc..</p>

<p>Freebase is already having all these entities in its database but i didn't find any of Freebase API or Google API to list all the entities and its type.</p>

<p>Could you please help me.</p>

<p>Thanks</p>
","google-api, freebase, named-entity-recognition, freebase-acre","<p>The data dump format is described <a href=""https://developers.google.com/freebase/data"" rel=""nofollow"">here</a>.  It's changed some since I wrote the answer that you reference, but the basic principals are still the same.</p>

<p>The <a href=""https://developers.google.com/freebase/v1/mql-overview"" rel=""nofollow"">MQLread</a> and <a href=""https://developers.google.com/freebase/v1/search-overview"" rel=""nofollow"">Search</a> APIs can both be used to filter Freebase topics to a specific type (e.g. the /people/person type).  Topics can have multiple types, but if what you want is the classic NER types, then the first thing to do is probably filter by the desired types since there are ""only"" about 3 million people in Freebase out of the tens of millions of topics.</p>

<p>Freebase contains a lot more information than just the name which would be useful in an NER task, so you might want to take a step back and think about what other properties you might want to extract as well.</p>
",3,3,1720,2014-02-10 09:41:32,https://stackoverflow.com/questions/21673586/how-can-i-extract-name-and-type-of-all-the-entities-in-freebase
"Efficient way to find 200,000 product names in 20 million articles?","<p>We have two (MySQL) databases, one with about 200.000 products (like ""Samsung Galaxy S4"", db-size 200 MB) and one with about 10 million articles (plain text, db-size 20GB) which can contain zero, one or many of the product names from the product database. Now we want to find product names in the article texts and store them as facets of the articles while indexing them in elasticsearch. Using regular expressions to find the products is pretty slow, we looked at Apache OpenNLP and Stanford Named Entity Recognizer, for both we have to train our own models and there are some projects at github for integrating those NER tools into elasticsearch, but they don't seem to be ready for production use.</p>

<p>Products and articles are added every day, so we have to run a complete recognition every day. Is NER the way to go? Or any other ideas? We don't have to understand the grammer etc. of the text, we only have to find the product name strings as fast as possible. We can't do the calculation in realtime because that's way to slow, so we have to pre-calculate the connection between articles and products and store them as facets, so we can query them pretty fast in our application. </p>

<p><strong>So what's your recommendation to find so many product names in so many articles?</strong></p>
","mysql, indexing, elasticsearch, faceted-search, named-entity-recognition","<p>One of the issues you'll run into the most is consistency... new articles and new product names are always coming in and you'll have an ""eventual consistency"" problem. So there are three approaches that come to mind that I have used to tackle this kind of problem.</p>

<ol>
<li><p>As suggested, use a full text search in MySQL, basically create a loop over your products table, and for each product name do a MATCH AGAIST query and insert productkey, and article key into a tie table. THis is fast, I used to run a system in SQL Server with over 90000 items being searched against 1B sentences. If you had a multithreaded java program that chunked up the categories and exectured the full text query, you may be surpised how fast this will be. Also, this can hammer your DB server.</p></li>
<li><p>Use Regex. Put all the products in a collection in memory, and regex find with that list against every document. This CAN be fast if you have your docs in something like hadoop, where it can be parallelized. You could run the job at night, and have it populate a MySQL table... This approach means you will have to start storing your docs in HDFS or some NOSQL solution, or import from MySQL to hadoop daily etc etc.</p></li>
<li><p>You can try doing it ""at index time"", so when a record is indexed in ElasticSearch the extraction will happen then and your facets will be built. I have only used SOLR for stuff like this... problem here is that when you add new products you will have to process in batch again anyway because the previously index docs will not have had the new products extracted from them.</p></li>
</ol>

<p>so there may be better options, but the one that scales infinitely (if you can afford the machines) is option 2... the hadoop job.... but this means big change.</p>

<p>These are just my thoughts, so I hope others come up with more clever ideas</p>

<p>EDIT:
As for using NER, I have used NER extensively, mainly OpenNLP, and the problem with this is that what it extracts will not be normalized, or to put it another way, it may extract pieces and parts of a product name, and you will be left dealing with things like fuzzy string matching to align the NER Results to the table of products. OpenNLP 1.6 trunk has a component called the EntityLinker, which is designed for this type of thing (linking NER results to authoritative databases). Also, NER/NLP will not solve the consistency problem, because every time you change your NER model, you will have to reprocess.</p>
",1,1,544,2014-02-18 09:31:48,https://stackoverflow.com/questions/21849398/efficient-way-to-find-200-000-product-names-in-20-million-articles
Is it possible to train Stanford NER system to recognize more named entities types?,"<p>I'm using some NLP libraries now, (stanford and nltk) 
Stanford I saw the demo part but just want to ask if it possible to use it to identify more entity types.</p>

<p>So currently stanford NER system (as the demo shows) can recognize entities as person(name), organization or location. But the organizations recognized are limited to universities or some, big organizations. I'm wondering if I can use its API to write program for more entity types, like if my input is ""Apple"" or  ""Square"" it can recognize it as a company.</p>

<p>Do I have to make my own training dataset?</p>

<p>Further more, if I ever want to extract entities and their relationships between each other, I feel I should use the stanford dependency parser.
I mean, extract first the named entities and other parts tagged as ""noun"" and find relations between them.</p>

<p>Am I correct.</p>

<p>Thanks.</p>
","nlp, stanford-nlp, named-entity-recognition","<p>Yes, you need your own training set. The pre-trained Stanford models only recognise the word ""Stanford"" as a named entity because they have been trained on data that had that word (or very similar words according to the feature set they use, I don't know what that is) marked as a named entity. </p>

<p>Once you have more data, you need to put it in the right format described in <a href=""https://stackoverflow.com/questions/15609324/training-n-gram-ner-with-stanford-nlp"">this question</a> and the Stanford tutorial.</p>
",13,28,18510,2014-03-03 22:07:18,https://stackoverflow.com/questions/22158530/is-it-possible-to-train-stanford-ner-system-to-recognize-more-named-entities-typ
Extracting multi word named entities using CoreNLP,"<p>I'm using CoreNLP for named entity extraction and have run into a bit of an issue.
The issue is that whenever a named entity is composed of more than one token, such as ""Han Solo"", the annotator does not return ""Han Solo"" as a single named entity, but as two separate entities, ""Han"" ""Solo"".</p>

<p>Is it possible to get the named entity as one token? I know I can make use of the CRFClassifier with classifyWithInlineXML to this extent, but my solution requires that I use CoreNLP, since I need to know the word number as well.</p>

<p>The following is the code that I have so far:</p>

<pre><code>    Properties props = new Properties();
    props.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse"");
    props.setProperty(""ner.model"", ""edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz"");
    pipeline = new StanfordCoreNLP(props);
    Annotation document = new Annotation(text);
    pipeline.annotate(document);
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    for (CoreMap sentence : sentences) {
        for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
                System.out.println(token.get(NamedEntityTagAnnotation.class));
        }
    }
</code></pre>

<blockquote>
  <p>Help me Obi-Wan Kenobi. You're my only hope.</p>
</blockquote>
","stanford-nlp, named-entity-recognition","<pre><code>PrintWriter writer = null;
 try {  
     String inputLine = ""Several possible plans emerged from the talks, held at the Federal Reserve Bank of New York"" + "" and led by Timothy R. Geithner, the president of the New York Fed, and Treasury Secretary Henry M. Paulson Jr."";

     String serializedClassifier = ""english.all.3class.distsim.crf.ser.gz"";
     AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifierNoExceptions(serializedClassifier);

     writer = new PrintWriter(new File(""output.xml""));
     writer.println(""&lt;Sentences&gt;"");
     writer.flush();
     String output =""&lt;Sentence&gt;""+classifier.classifyToString(inputLine, ""xml"", true)+""&lt;/Sentence&gt;""; 
     writer.println(output);
     writer.flush();
     writer.println(""&lt;/Sentences&gt;"");
     writer.flush(); 
 } catch (FileNotFoundException ex) {
     ex.printStackTrace();
 } finally {
     writer.close();
 }
</code></pre>

<p>I was able to come up with this solution. I am writing the output to an XML file ""output.xml"". From the obtained output, you can merge continuous nodes in xml with ""PERSON"" or ""ORGANIZATION"" or ""LOCATION"" attributes in to one entity. And this format produces the word count by default.</p>

<p>Here is a snapshot of xml output.</p>

<pre><code>&lt;wi num=""11"" entity=""ORGANIZATION""&gt;Federal&lt;/wi&gt;
&lt;wi num=""12"" entity=""ORGANIZATION""&gt;Reserve&lt;/wi&gt;
&lt;wi num=""13"" entity=""ORGANIZATION""&gt;Bank&lt;/wi&gt;
&lt;wi num=""14"" entity=""ORGANIZATION""&gt;of&lt;/wi&gt;
&lt;wi num=""15"" entity=""ORGANIZATION""&gt;New&lt;/wi&gt;
&lt;wi num=""16"" entity=""ORGANIZATION""&gt;Yorkand&lt;/wi&gt;
</code></pre>

<p>From the above output you can see that continuously words are recognized as ""ORGANIZATION"". So these words could be combined to one entity.</p>
",3,15,3826,2014-03-09 11:59:08,https://stackoverflow.com/questions/22281889/extracting-multi-word-named-entities-using-corenlp
How does OpenNLP treat spanish names that get complex,"<p>I'm seeking to tokenise Peruvian names in order to separate the name fields from them. The problem I'm worried about is how exactly OpenNLP treats the complicated names that can occur in Peru, e.g Rafael de la Fuente Benavides. Would OpenNLP take that whole string as a name or will the de/la/del/los connectors mess it up? Also, how would OpenNLP determine when a name is ""done""? Since some peruvian names indeed go quite long (e.g Jose Manuel de los Reyes Gonzalez de Prada y Ulloa) I would wonder if OpenNLP would split that into 2 or 3 ""names"".</p>

<p>The goal of this is to be able to use the tokenization to seperate it into the correct fields for a person database e.g</p>

<pre><code>Rafael de la Fuente Benavides   ----&gt; First: Rafael Paternal Last:Benavides
Jose Carlos Mariategui La Chira ----&gt; First: Jose Carlos Paternal Last: Mariategui  Maternal Last: Chira
</code></pre>
","tokenize, opennlp, named-entity-recognition","<p>In order to properly recognize names, OpenNLP must be trained: you should provide a file formatted like this (also other formats are supported):</p>

<p><code>Sé &lt;START:person&gt; Rafael de la Fuente Benavides &lt;END &gt;, que trabajan en España</code></p>

<p>The training file must contain one sentence per line and each sentence may contain one or more names: to perform well, you'd provide the model with at least 15000 (according to the documentation).</p>

<p>Pay attention to the whitespace that precedes and follow each tag: this tag <code>&lt;START:person&gt;Rafael&lt;END&gt;</code> would be rejected.</p>
",0,0,125,2014-06-04 20:45:55,https://stackoverflow.com/questions/24047256/how-does-opennlp-treat-spanish-names-that-get-complex
Looking to Reason / Extract Information from Entity and Part of Speech Tagged Text,"<p>Let us say I start with the following text:</p>

<pre><code>I love Toyota Camrys and hate Ferraris
</code></pre>

<p>I use a POS tagger like Stanford CoreNLP and get the following Annotations:</p>

<pre><code>I_PRP love_VBP Toyota_NNP Camrys_NNPS and_CC hate_VB Ferraris_NNP
</code></pre>

<p>Let us assume I have a Named Entity Recognizer and am able to identify a Camry and Ferrari from the above notation.</p>

<p>I want to be able to reason about the above sentence where for example I deduce the following:</p>

<ul>
<li>I hate Camrys    </li>
<li>I love Ferraris</li>
</ul>

<p>possibly even:</p>

<ul>
<li>I hate something manufactured by Toyota</li>
<li>I hate something manufactured by Ferrari</li>
</ul>

<p>I am currently doing the above using manually coded heuristics and slot matching.</p>

<p>Question: Is there a more standard way to accomplish this?</p>

<p>For example I ran in to JAPE Java Annotation Patterns Engine from Gate -- is that part of the tool chain do something like this.</p>
","nlp, nltk, named-entity-recognition, gate","<p>There are 2 ways to do that:</p>

<p>1) Write your own JAPE grammars. This is not as hard as it appears to be. There are many JAPE manuals on the web. First <a href=""https://www.google.ru/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CBsQFjAA&amp;url=http://gate.ac.uk/sale/thakker-jape-tutorial/GATE%2520JAPE%2520manual.pdf&amp;ei=rjSXU7OOH6P9ygPLkoL4BA&amp;usg=AFQjCNGu-IKe_oL3HiFen2UrpXtfvrfBuQ&amp;sig2=QMepv5bd8dSKHFH3T_7qBQ&amp;bvm=bv.68693194,d.bGQ&amp;cad=rjt"" rel=""nofollow"">google link</a> for <code>""gate jape manual""</code> seem to be ok for startup. Additionally, existing JAPE grammars from GATE ANNIE can provide good examples and ideas for your task.</p>

<p>At the beginning you would try to create your own dictionary for GATE Gazetteer with entries for names of brands (Toyota, Ferrary, e.t.c.) to create ""Lookup"" annotations. Then your JAPE rules would contain rules like</p>

<p><code>Rule: LoveBrand
(
{Token.kind == word, Token.string = ""I""}
{Token.kind == word, Token.string = ""love""}
{Lookup.majorType == ""brand""}
): label
--&gt;
:label.Prefererence = {rule= ""LoveBrand"" }</code></p>

<p>2) Use <a href=""http://gate.ac.uk/sale/tao/splitch18.html#x23-45000018.4"" rel=""nofollow"">Parser_Stanford plugin</a> in GATE. It will create two types of annotations for  Dependencies and TreeNodes. Dependencies are typed links between couples of words, TreeNodes are dependencies collapsed into trees. Just try to play with Parser_Stanford plugin in GATE Developer GUI and you will get idea how to use it for your task.</p>

<p>You can process your <code>""I love Toyota Camrys and hate Ferraris.""</code> on <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"">this demo page</a> to see what Stanford parser can do. Particularly you need dependencies of type <code>dobj</code>. There is a <a href=""https://www.google.ru/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CB0QFjAA&amp;url=http://nlp.stanford.edu/software/dependencies_manual.pdf&amp;ei=SDKXU63oNqvOygOX54LoBQ&amp;usg=AFQjCNFvNTtNhYCa9IkZMIaIUvKnzka1nA&amp;sig2=Ie8tCG-96CxdvqyfOVF8Pw&amp;bvm=bv.68693194,d.bGQ"" rel=""nofollow"">Stanford dependencies manual</a> with description of all possible dependencies if you want use other Stanford dependencies.</p>

<p>Parser_Stanford plugin for GATE just adds annotations for Stanford dependencies to your document. You can add GATE transducer processing resource with your JAPE grammars and add it to your sequence of processing resources in GATE Developer after Parser_Stanford to process annotations created for Stanford dependencies.</p>
",3,2,322,2014-06-09 21:07:52,https://stackoverflow.com/questions/24129081/looking-to-reason-extract-information-from-entity-and-part-of-speech-tagged-te
JAVA: How use Gazettes with Stanford NLP?,"<p>I read this <a href=""http://www-nlp.stanford.edu/software/crf-faq.shtml#gazette"" rel=""nofollow"">faq</a> but i not understand. I try with this code:</p>

<pre><code>   Properties pp=new Properties();  
   pp.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
   pp.put(""ner.useSUTime"",""false"");

   pp.put(""useGazettes"",""true"");
   pp.put(""gazette"",""C:\\gaz.txt"");

   StanfordCoreNLP s=new StanfordCoreNLP(pp);
</code></pre>

<p>This is String: ""Dan became a member of the Music friends association in 2008""</p>

<p>the gazette file is:</p>

<pre><code>  CLASS Music friends association 
</code></pre>

<p>But ""Music friends association"" is not recognized by NER.</p>

<p>Where am I wrong?</p>
","java, nlp, stanford-nlp, named-entity-recognition","<p>The answer is given there:</p>

<blockquote>
  <p>If a gazette is used, this does not guarantee that words in the gazette are always used as a member of the intended class, and it does not guarantee that words outside the gazette will not be chosen. It simply provides another feature for the CRF to train against. If the CRF has higher weights for other features, the gazette features may be overwhelmed.</p>
</blockquote>

<p>So there is not guarantee that your phrase will be tagged in any way.  The alternative is </p>

<blockquote>
  <p>either the regexner or the tokensregex tools included in Stanford CoreNLP</p>
</blockquote>
",0,3,1063,2014-06-11 13:28:08,https://stackoverflow.com/questions/24164186/java-how-use-gazettes-with-stanford-nlp
NLTK NER: Continuous Learning,"<p>I have been trying to use NER feature of NLTK. I want to extract such entities from the articles. I know that it can not be perfect in doing so but I wonder if there is human intervention in between to manually tag NEs, will it improve?</p>

<p>If yes, is it possible with present model in NLTK to continually train the model. (Semi-Supervised Training)</p>
","nlp, nltk, named-entity-recognition, reinforcement-learning","<p>The plain vanilla NER chunker provided in nltk internally uses maximum entropy chunker trained on the ACE corpus. Hence it is not possible to identify dates or time, unless you train it with your own classifier and data(which is quite a meticulous job).</p>

<p>You could refer this <a href=""http://mattshomepage.com/#/blog/feb2013/liftingthehood"" rel=""nofollow"">link</a> for performing he same.</p>

<p>Also, there is a module called <a href=""https://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py"" rel=""nofollow"">timex</a> in nltk_contrib which might help you with your needs. </p>

<p>If you are interested to perform the same in Java better look into <a href=""http://nlp.stanford.edu/software/sutime.shtml"" rel=""nofollow"">Stanford SUTime</a>, it is a part of Stanford CoreNLP. </p>
",1,0,412,2014-06-24 16:55:35,https://stackoverflow.com/questions/24392268/nltk-ner-continuous-learning
Named Entity Recognition with Regular Expression: NLTK,"<p>I have been playing with NLTK toolkit. I come across this problem a lot and searched for solution online but nowhere I got a satisfying answer. So I am putting my query here. </p>

<p>Many times NER doesn't tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.</p>

<p>Example:</p>

<p>Input: </p>

<blockquote>
  <p>Barack Obama is a great person.</p>
</blockquote>

<p>Output:  </p>

<blockquote>
  <p>Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('.', '.')])</p>
</blockquote>

<p>where as </p>

<p>input: </p>

<blockquote>
  <p>Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he ""was honored"" to be compared to Darth Vader while in office.</p>
</blockquote>

<p>Output: </p>

<blockquote>
  <p>Tree('S', [('Former', 'JJ'), ('Vice', 'NNP'), ('President', 'NNP'), Tree('NE', [('Dick', 'NNP'), ('Cheney', 'NNP')]), ('told', 'VBD'), ('conservative', 'JJ'), ('radio', 'NN'), ('host', 'NN'), Tree('NE', [('Laura', 'NNP'), ('Ingraham', 'NNP')]), ('that', 'IN'), ('he', 'PRP'), ('<code>', '</code>'), ('was', 'VBD'), ('honored', 'VBN'), (""''"", ""''""), ('to', 'TO'), ('be', 'VB'), ('compared', 'VBN'), ('to', 'TO'), Tree('NE', [('Darth', 'NNP'), ('Vader', 'NNP')]), ('while', 'IN'), ('in', 'IN'), ('office', 'NN'), ('.', '.')])</p>
</blockquote>

<p>Here Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP) , is correctly extracted. </p>

<p>So I think if nltk.ne_chunk is used first and then if two consecutive trees are NNP there are high chances that both refers to one entity. </p>

<p>Any suggestion will be really appreciated. I am looking for flaws in my approach.</p>

<p>Thanks.</p>
","regex, nlp, nltk, named-entity-recognition","<pre><code>from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.tree import Tree

def get_continuous_chunks(text):
    chunked = ne_chunk(pos_tag(word_tokenize(text)))
    prev = None
    continuous_chunk = []
    current_chunk = []

    for i in chunked:
        if type(i) == Tree:
            current_chunk.append("" "".join([token for token, pos in i.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    if continuous_chunk:
        named_entity = "" "".join(current_chunk)
        if named_entity not in continuous_chunk:
            continuous_chunk.append(named_entity)

    return continuous_chunk

txt = ""Barack Obama is a great person."" 
print get_continuous_chunks(txt)
</code></pre>

<p>[out]:</p>

<pre><code>['Barack Obama']
</code></pre>

<p>But do note that if the continuous chunk are not supposed to be a single NE, then you would be combining multiple NEs into one. I can't think of such an example off my head but i'm sure it would happen. But if they not continuous, the script above works fine:</p>

<pre><code>&gt;&gt;&gt; txt = ""Barack Obama is the husband of Michelle Obama.""  
&gt;&gt;&gt; get_continuous_chunks(txt)
['Barack Obama', 'Michelle Obama']
</code></pre>
",17,10,15330,2014-06-25 00:45:05,https://stackoverflow.com/questions/24398536/named-entity-recognition-with-regular-expression-nltk
Pass array of floats when training Stanford CRFClassifier,"<p>For every word in a document, I'm looking to add in a series of floating point numbers as features for Stanford NER's <code>CRFClassifier</code> to train on.  Unfortunately, the documentation on Stanford NER's <code>.prop</code> files hasn't made it clear how to pass in custom features. In general, how does one go about adding custom features to a Stanford NER training set?</p>
","java, nlp, stanford-nlp, named-entity-recognition","<p>You will find your answer inside the 'NERFeatureFactory' class. </p>
",1,1,185,2014-06-27 19:20:38,https://stackoverflow.com/questions/24459351/pass-array-of-floats-when-training-stanford-crfclassifier
Why do I get a NullPointerException while trying to begin transaction?,"<p>Before I was getting NullPointerException in this line </p>

<pre><code>EntityManager manager = emf.createEntityManager();
</code></pre>

<p>but I fixed it by adding one line above</p>

<pre><code>emf =  Persistence.createEntityManagerFactory(""main"");
</code></pre>

<p>Now I am getting NullPointerException in this line</p>

<pre><code>transaction.begin();
</code></pre>

<p>I can't understand why. I was trying to take it from the try catch block but was getting the same result. Also in the finally I tried to add emf.close(); but the error doesn't get fixed. Also, I include @Named annotation because someone suggested me to do that. </p>

<pre><code>import java.io.Serializable;
import javax.inject.Named; 
import javax.inject.Inject; 
import java.sql.*;
import javax.annotation.Resource;
import javax.persistence.EntityManager;
import javax.persistence.EntityManagerFactory;
import javax.persistence.Persistence;
import javax.persistence.PersistenceUnit;
import javax.persistence.TypedQuery ; 
import javax.transaction.UserTransaction;
import javax.transaction.RollbackException; 
import javax.persistence.criteria.*;
import java.util.List;
import java.util.*;

@Named
public class UserManager implements Serializable{

   @PersistenceUnit(unitName=""main"")
   private EntityManagerFactory emf;

   @Resource
   private UserTransaction transaction;

   public UserManager() {}

   public UserTransaction getTransaction() { return transaction; }
   public void setTransaction(UserTransaction transaction) { this.transaction = transaction; }

   public void addUser(User v) throws Exception {
    emf =  Persistence.createEntityManagerFactory(""main"");
        EntityManager manager = emf.createEntityManager();
        boolean commit;
        try {   
    transaction.begin();
        manager.joinTransaction();
    commit = false;
        try {
        manager.persist(v);
        transaction.commit();
        commit = true;
        }finally {
                if(commit==false){
            transaction.rollback();
                }
         }
        }finally {     
         manager.close();
             //emf.close();
    }
   }



@Named(""subBean"")
@RequestScoped
public class UserBean implements Serializable{

   private List&lt;User&gt; users;
   private UserManager vm;
   private User v;
   private int total;

   public UserBean() {
      this.total= 0;
      users = new ArrayList&lt;User&gt;();
      vm = new UserManager();
      v = new User();
   }

   public String addUserAction() throws Exception {
      vm.addUser(v);
      users = getUsers();
      return ""submit1"";

   }

   //getters and setters 
}
</code></pre>
","java, jpa, nullpointerexception, cdi, named-entity-recognition","<p>As noted in my comment, the problem is that you're creating an instance of <code>UserManager</code> bean manually and CDI cannot inject the variables. This is noted here:</p>

<pre><code>@Named(""subBean"")
@RequestScoped
public class UserBean implements Serializable{
   //variable is just declared...
   private UserManager vm;

   public UserBean() {
      //and here you're initializing it manually...
      vm = new UserManager();
   }

   //rest of your code...
}
</code></pre>

<p>Let CDI do its work by injecting the variable:</p>

<pre><code>@Named(""subBean"")
@RequestScoped
public class UserBean implements Serializable{
   //variable is just declared
   //and notify CDI it should inject an instance when creating an instance of the client class
   @Inject
   private UserManager vm;

   public UserBean() {
      //no need to initialize it ever
      //vm = new UserManager();
   }

   //rest of your code...
}
</code></pre>

<p>By doing this, you won't need to create any field that should be injected in your other bean either, which means you don't need to initialize <code>emf</code> field manually:</p>

<pre><code>//this line should be commented as well...
//emf =  Persistence.createEntityManagerFactory(""main"");
</code></pre>
",3,1,2046,2014-06-29 21:12:47,https://stackoverflow.com/questions/24480328/why-do-i-get-a-nullpointerexception-while-trying-to-begin-transaction
Inverted index in C# Generic Collections,"<p>(Sorry if the title is a complete red herring by the way)</p>

<p><strong>Background:</strong></p>

<p>I am developing a map of all of the tweets in the world in real-time using the Twitter Streaming API and ASP.NET SignalR. I am using the Tweetinvi C# Twitter library to asynchronously push tweets to the browser using SignalR. Everything is working as expected - see <a href=""http://dev.wherelionsroam.co.uk"" rel=""nofollow noreferrer"">http://dev.wherelionsroam.co.uk</a> to get an idea of it.</p>

<p>The next step of the development involves parsing each tweet's text data using the Stanford Natural Language Parsing library (<a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml</a>), in particular the Named Entity Recognizer (also called the CRFClassifier) so that I can extract meaningful metadata from each tweet (i.e. People, Places and Organisations mentioned). The desired result is that I will be able to determine People, Places and Organisations that lots of people are talking about (similar to the concept ""Trending""), and broadcast them to all clients using SignalR. I am aware that the Twitter API has the <code>GET trends</code> methods, but that wouldn't be any fun would it?!</p>

<p>Here are the main classes in my app:</p>

<p><strong>Main classes:</strong></p>

<p>TweetModel.cs (holds all of the information regarding a tweet as broadcast to it from the Streaming API):</p>

<pre><code>public class TweetModel
{
    public string User { get; set; }
    public string Text { get; set; }
    public DateTime CreatedAt { get; set; }
    public string ImageUrl { get; set; }
    public double Longitude { get; set; }
    public double Latitude { get; set; }
    public string ProfileUrl { get; set; }

    // This field is set later during Tokenization / Named Entity Recognition
    public List&lt;NamedEntity&gt; entities = new List&lt;NamedEntity&gt;();
}
</code></pre>

<p>The Abstract NamedEntity class:</p>

<pre><code>public abstract class NamedEntity
{
    /// &lt;summary&gt;
    /// Abstract modelling class for NER tagging - overridden by specific named entities. Used here so that all classes inherit from a single base class - polymorphic list
    /// &lt;/summary&gt;
    protected string _name;
    public abstract string Name { get; set; }
}
</code></pre>

<p>The Person class, an example of a class that overrides the abstract NamedEntity class:</p>

<pre><code>public class Person : NamedEntity
{
    public override string Name
    {
        get
        {
            return _name;
        }
        set
        {
            _name = value;
        }
    }
    public string entityType = ""Person"";
}
</code></pre>

<p>The TweetParser class:</p>

<pre><code> public class TweetParser
    {
        // Static List to hold all of tweets (and their entities) - tweets older than 20 minutes are cleared out
        public static List&lt;TweetModel&gt; tweets = new List&lt;TweetModel&gt;();
        public TweetParser(TweetModel tweet)
        {
            ProcessTweet(tweet);
            // Removed all of NER logic from this class
        }
}
</code></pre>

<p><strong>Explanation of the Named Entity Recognizer:</strong></p>

<p>The way that the NER recognition library works is that it classifies words in a sentence with a tag such as ""PERSON"" for 'Luis Suarez' or ""PLACE"" for ""New York"". This information is stored in the sub classes of the NamedEntity class, depending on what type of tag has been attributed to the word by the NER library (choice of <code>PERSON</code>, <code>LOCATION</code>, <code>ORGANISATION</code>)</p>

<p><strong>The problem:</strong></p>

<p>My question is, considering it is likely that there may be multiple versions of the term ""Luis Suarez"" coming in (i.e. Luis Suarez, Luis Suárez), which will both be defined in their own distinct NamedEntity instance (inside of the <code>List&lt;NamedEntity&gt;</code> instance, in turn inside of a <code>TweetModel</code> instance), what would be the best way of grouping matching instances of the term ""Luis Suarez"" together from all tweets whilst still preserving the <code>TweetModel</code> > <code>List&lt;NamedEntity&gt;</code> parent-child relationship. I have been informed that this is effectively an inverted index but I'm not sure how well informed this person was!</p>

<p><strong>Visualization of structure:</strong></p>

<p><img src=""https://i.sstatic.net/il2JF.png"" alt=""enter image description here""></p>

<p>I'm really sorry if this question is unclear; I can't really express it with much more brevity than this! For full src so far, please see <a href=""https://github.com/adaam2/FinalUniProject"" rel=""nofollow noreferrer"">https://github.com/adaam2/FinalUniProject</a></p>
","c#, oop, generics, named-entity-recognition, inverted-index","<p>1- add <code>List&lt;TweetModel&gt;</code> property to your <code>NamedEntity</code>.</p>

<pre><code>public abstract List&lt;TweetModel&gt; Tweets { get; set; }
</code></pre>

<p>2- guaranty that your Tokenization function always returns the same <code>NamedEntity</code> object for the same tag.</p>

<p>3- when you add a <code>NamedEntity</code> to the entities list also add the <code>TweetModel</code> to the list on the <code>NamedEntity</code>.</p>

<pre><code>Person p = this is the result of the Tokenization;
entities.Add(p);
p.Tweets.Add(this);
</code></pre>

<p>Basically the only difficult part is to have the function that generates the Named entities return the same object when it finds the text 'Luis Suarez' and 'Luis Suárez' on different Tweets.</p>
",1,4,630,2014-07-04 09:52:00,https://stackoverflow.com/questions/24571499/inverted-index-in-c-generic-collections
Stanford Named Entity Tagger - Inconsistency ?,"<p>I have a strange problem.</p>

<p>I have a list of sentences (around 0.1 million) which is want to tag using the stanford named entity recognition(ner) tagging. I was tagging using the following line of code that is provided from the stanford ner demo website (Java Demo Code).</p>

<pre><code>for (String str : List&lt;sentences&gt;) {
   System.out.print(classifier.classifyToString(str, ""slashTags"", false));
}
</code></pre>

<p>I thought everything is going right until I manually checked for some of the sentences that were not tagged at all which are supposed to be tagged. But when these sentences which are not tagged are hand picked into some sample list and tested with the above code they are getting tagged then. So I am confused where I am going wrong. The sentences which are not tagged correctly are like in the range of 1000 - 1500 sentences. so when I ran these incorrectly tagged sentences in a separate list then they are getting tagged. Is the size of the dataset (0.1 million) having any impact on the classifier ?</p>

<p>For example:
consider the following sentence - ""IBM Corporation Introduction""
Sentences like above are present in considerable number in my 0.1 million dataset. So when I do the tagging using the above code on the 0.1 million dataset, many of sentences like these have got no tagging at all. But When I hand pick those and put in in a list and then do the tagging then they are getting tagged. </p>

<p>I have tried all the approaches and I end up in the same result of no tagging for the sentences like above when tagging on the entire dataset.</p>

<p>I tried the following 3 different ways
1. classifier.classifyToString(inputString, ""slashTags"", false)
2. classifier.classify(inputString)
3. classifier.classifyToCharacterOffsets(inputString)  </p>

<p>Any ideas or suggestions where I am going wrong ?</p>

<p>Thanks</p>
","stanford-nlp, named-entity-recognition, named-entity-extraction","<p>I think you got answer from the below link:</p>

<p><a href=""https://mailman.stanford.edu/pipermail/java-nlp-user/2014-July/006045.html"" rel=""nofollow"">https://mailman.stanford.edu/pipermail/java-nlp-user/2014-July/006045.html</a></p>
",0,0,357,2014-07-31 02:43:12,https://stackoverflow.com/questions/25050041/stanford-named-entity-tagger-inconsistency
Named entities in encapsulated XML cause parsing errors,"<p>I have XML docs that contain other XML docs encapsulated as CDATA, like this:</p>

<pre><code>    &lt;mds&gt;
      &lt;md&gt;
        &lt;value&gt;
          &lt;![CDATA[&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&lt;record xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:dc=""http://purl.org/dc/elements/1.1/"" xmlns:dcterms=""http://purl.org/dc/terms/""&gt;
             &lt;dc:title&gt;some text containing &amp;amp;&lt;/dc:title&gt;&lt;/record&gt;]]&gt;
        &lt;/value&gt;
      &lt;/md&gt;
    &lt;/mds&gt;
</code></pre>

<p>I extract this XML and the dc:title from it using LibXML:</p>

<pre><code>$dcrawData = &lt;get the CDATA from above&gt;;
$dcDOM = $::PRSR-&gt;load_xml(expand_entities =&gt; 0, string =&gt; $dcRawData);
$dcTitle = $dcDOM-&gt;findvalue(""//dc:title"");
</code></pre>

<p>Then I insert it into another XML section by doing a string replace:</p>

<pre><code>&lt;mods:titleInfo&gt;
    &lt;mods:title&gt;some text containing &amp;&lt;/mods:title&gt;
&lt;/mods:titleInfo&gt;
</code></pre>

<p>As you can see, the &amp;amp entity gets expanded and becomes a single &amp;. Which is a problem because now, the resulting XML generates a parsing error because any parser expects a named entity here. </p>

<p>Is there a way to prevent LibXML from expanding named entities when using findvalue or to reencode them before using the value? There might be others in other records. The expand_entities option does not make a difference.</p>
","xml, perl, named-entity-recognition","<p>Ok, I think I found a solution. <a href=""https://metacpan.org/pod/XML::Entities"" rel=""nofollow""><code>XML::Entities</code></a> will do the job of re-encoding the entities in the string.</p>

<p>However, I need to limit the characters that are encoded to only a few, otherwise the encoded string will have entities that the xml parser does not recognize.</p>

<p>So at the moment I use</p>

<pre><code>$dcTitle = encode_entities($dcDOM-&gt;findvalue(""//dc:title""),'&amp;&lt;&gt;""');
</code></pre>

<p>to only encode the ampersand and a few special xml chars.</p>
",2,1,150,2014-08-26 11:01:18,https://stackoverflow.com/questions/25504216/named-entities-in-encapsulated-xml-cause-parsing-errors
Display Stanford NER confidence score,"<p>I'm extracting named-entities from news articles with the use of Stanford NER CRFClassifier and in order to implement active learning, I would like to know what are the confidence scores of the classes for each labelled entity.</p>

<p>Exemple of display :</p>

<blockquote>
  <p>LOCATION(0.20) PERSON(0.10) ORGANIZATION(0.60) MISC(0.10)</p>
</blockquote>

<p>Here is my code for extracting named-entities from a text :</p>

<pre><code>AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifierNoExceptions(classifier_path);
String annnotatedText = classifier.classifyWithInlineXML(text);
</code></pre>

<p>Is there a workaround to get thoses values along with the annotations ?</p>
","java, stanford-nlp, named-entity-recognition","<p>I've found it out by myself, in CRFClassifier's doc it is written :</p>

<blockquote>
  <p>Probabilities assigned by the CRF can be interrogated using either the
  <code>printProbsDocument()</code> or <code>getCliqueTrees()</code> methods.</p>
</blockquote>

<p>The first method is not useful since it only prints what I want on the console, but I want to be able to access this data, so I have read how this method is coded and copied a bit its behaviour like this :</p>

<pre><code>List&lt;CoreLabel&gt; classifiedLabels = classifier.classify(sentences);
CRFCliqueTree&lt;String&gt; cliqueTree = classifier.getCliqueTree(classifiedLabels);

for (int i = 0; i &lt; cliqueTree.length(); i++) {
    CoreLabel wi = classifiedLabels.get(i);
    for (Iterator&lt;String&gt; iter = classifier.classIndex.iterator(); iter.hasNext();) {
        String label = iter.next();
        int index = classifier.classIndex.indexOf(label);
        double prob = cliqueTree.prob(i, index);
        System.out.println(""\t"" + label + ""("" + prob + "")"");
    }
    String tag = StringUtils.getNotNullString(wi.get(CoreAnnotations.AnswerAnnotation.class));
    System.out.println(""Class : "" + tag);
}   
</code></pre>
",6,1,2066,2014-10-28 16:03:50,https://stackoverflow.com/questions/26612999/display-stanford-ner-confidence-score
Recognize partial/complete address with NLP framework,"<p>I was wondering the amount of work on NLP framework to get partial (without city) or complete postal address extraction with NLP frameworks from unstructured text? Are NLP frameworks efficient to do this? Also, how difficult is it to ""train"" Named Entity Recognition modules to match new locations ?</p>
","location, nlp, named-entity-recognition","<p>As long as most addresses are correctly formatted and regular, i.e. contain contact name, street number, street name, separated by commas, you may find rule-based frameworks.</p>

<p>Using unstructured or partially structured text will require more preprocessing and statistics e.g. morpho-syntax and CRF. <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">Stanford tools</a> are the most popular for this purpose. It may also be an interresting direction to search for corpus containing intermediary annotations: not only ""LOC"", but also ""NUMBER"", ""STREETNAME"", ""CITY"", etc. so as to be able to extract location even if they are not complete. For this kind of annotation, you may have a look at <a href=""http://www.aclweb.org/anthology/I11-1142"" rel=""noreferrer"">tree-structured approaches</a>.</p>

<p>So the amount of work mostly depends on how much regular are expressions you are looking for.</p>
",10,10,5490,2014-11-16 08:50:33,https://stackoverflow.com/questions/26955305/recognize-partial-complete-address-with-nlp-framework
problems traversing ne chunks in NLTK3,"<p>Hi Im trying this code in NLTK3:-
Somehow I managed to fix line-6 to work with version 3 of NLTK. But stil the for loop doesnt return anything at all. </p>

<pre><code>import nltk
sample = """""" some random text content with names and countries etc""""""     
sentences = nltk.sent_tokenize(sample)
tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]
tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]
chunked_sentences=nltk.chunk.ne_chunk_sents(tagged_sentences) #Managed to fix this to work with version_3

for i in chunked_sentences:
    if hasattr(i,'label'):
        if i.label()=='NE':
            print i
</code></pre>

<p>Also if I try to debug , I see this output :</p>

<pre><code>for i in chunked_sentences:
    if hasattr(i,'label') and i.label:
        print i.label
S
S
S
S
S
S
S
S
</code></pre>

<p>Then how do I check it for ""NE"". Theres something wrong with NLTK-3 that Im really not able to figure out.Pls help</p>
","python, nltk, named-entity-recognition","<p>It seems you are iterating over sentences. I assume you want to iterate over the individual nodes contained in sentences.</p>

<p>It should work like this:</p>

<pre><code>for sentence in chunked_sentences:
    for token in sentence: 
       if hasattr(token,'label') and token.label() == 'NE':
           print token
</code></pre>

<p>Edit: For future reference, what tipped me off to the fact that you are iterating over sentences is simply that the root node for a sentence is commonly labeled 'S'.</p>
",2,2,1025,2014-11-30 10:02:25,https://stackoverflow.com/questions/27212050/problems-traversing-ne-chunks-in-nltk3
Chunking Stanford Named Entity Recognizer (NER) outputs from NLTK format,"<p>I am using NER in NLTK to find persons, locations, and organizations in sentences. I am able to produce the results like this:</p>

<pre><code>[(u'Remaking', u'O'), (u'The', u'O'), (u'Republican', u'ORGANIZATION'), (u'Party', u'ORGANIZATION')]
</code></pre>

<p>Is that possible to chunk things together by using it?
What I want is like this:</p>

<pre><code>u'Remaking'/ u'O', u'The'/u'O', (u'Republican', u'Party')/u'ORGANIZATION'
</code></pre>

<p>Thanks!</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>You can use the standard NLTK way of representing chunks using <strong>nltk.Tree</strong>. This might mean that you have to change your representation a bit.</p>

<p>What I usually do is represent <strong>NER-tagged</strong> sentences as <strong>lists of triplets</strong>:</p>

<pre><code>sentence = [('Andrew', 'NNP', 'PERSON'), ('is', 'VBZ', 'O'), ('part', 'NN', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('Republican', 'NNP', 'ORGANIZATION'), ('Party', 'NNP', 'ORGANIZATION'), ('in', 'IN', 'O'), ('Dallas', 'NNP', 'LOCATION')]
</code></pre>

<p>I do this when I use an external tool for NER tagging a sentence. Now you can transform this sentence the NLTK representation:</p>

<pre><code>from nltk import Tree


def IOB_to_tree(iob_tagged):
    root = Tree('S', [])
    for token in iob_tagged:
        if token[2] == 'O':
            root.append((token[0], token[1]))
        else:
            try:
                if root[-1].label() == token[2]:
                    root[-1].append((token[0], token[1]))
                else:
                    root.append(Tree(token[2], [(token[0], token[1])]))
            except:
                root.append(Tree(token[2], [(token[0], token[1])]))

    return root


sentence = [('Andrew', 'NNP', 'PERSON'), ('is', 'VBZ', 'O'), ('part', 'NN', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('Republican', 'NNP', 'ORGANIZATION'), ('Party', 'NNP', 'ORGANIZATION'), ('in', 'IN', 'O'), ('Dallas', 'NNP', 'LOCATION')]
print IOB_to_tree(sentence)
</code></pre>

<p>The change in representation kind of makes sense because you certainly need POS tags for NER tagging.</p>

<p>The end result should look like:</p>

<pre><code>(S
  (PERSON Andrew/NNP)
  is/VBZ
  part/NN
  of/IN
  the/DT
  (ORGANIZATION Republican/NNP Party/NNP)
  in/IN
  (LOCATION Dallas/NNP))
</code></pre>
",2,11,2548,2014-12-23 22:46:15,https://stackoverflow.com/questions/27629130/chunking-stanford-named-entity-recognizer-ner-outputs-from-nltk-format
Freebase to find alternative company names,"<p>I am attempting to use Freebase to find all the alternative names of a specified company. I currently have the following code, which works when you specify the languages you want the name to be in. I would like it to return all alias, regardless of the language. However, if I do not specify the languages, no results are returned. Also, if I include all languages in the query as one long list, it returns results for the wrong company. When I did this in the below code, it returned results for Bank of America. Is there a way of specifying all languages? The main purpose of the code is to find common acronyms of companies, though select full names are useful too.   </p>

<pre><code>        HttpTransport httpTransport = new NetHttpTransport();
        HttpRequestFactory requestFactory = httpTransport.createRequestFactory();
        JSONParser parser = new JSONParser();

        GenericUrl url = new GenericUrl(""https://www.googleapis.com/freebase/v1/search"");

        url.put(""query"", ""Royal Bank of Scotland"");
        url.put(""filter"", ""(all type:/organization/organization )"");
        url.put(""limit"", ""1"");
        url.put(""output"", ""( /common/topic/alias )"");
        url.put(""lang"", ""cs"");//Specify Laungauges
        url.put(""key"", properties.get(""API_KEY""));

        HttpRequest request = requestFactory.buildGetRequest(url);
        HttpResponse httpResponse = request.execute();
        JSONObject response = (JSONObject)parser.parse(httpResponse.parseAsString());
        JSONArray results = (JSONArray)response.get(""result"");
        System.out.println(results);
</code></pre>
","java, freebase, named-entity-recognition","<p>The <code>lang</code> parameter takes a comma separated list of languages.  If you really want <em>all</em>, you can just use the entire list which currently is: </p>

<p>""en,es,fr,de,it,pt,zh,ja,ko,ru,sv,fi,da,nl,el,ro,tr,hu,th,pl,cs,id,bg,uk,ca,eu,no,sl,sk,hr,sr,ar,hi,vi,fa,ga,iw,lv,lt,gl,is,hy,lo,km,sq,fil,zxx""</p>

<p>From <a href=""https://www.googleapis.com/freebase/v1/search?help=langs&amp;indent=true"" rel=""nofollow"">https://www.googleapis.com/freebase/v1/search?help=langs&amp;indent=true</a> as documented on <a href=""https://developers.google.com/freebase/v1/search-cookbook#language-constraints"" rel=""nofollow"">https://developers.google.com/freebase/v1/search-cookbook#language-constraints</a> </p>

<p>This query works for me:</p>

<p><a href=""https://www.googleapis.com/freebase/v1/search?query="" rel=""nofollow"">https://www.googleapis.com/freebase/v1/search?query=</a>""Royal%20Bank%20of%20Scotland""&amp;filter=(all%20type:/organization/organization%20)&amp;limit=1&amp;output=(/common/topic/alias%20)&amp;lang=en,es,fr,de,it,pt,zh,ja,ko,ru,sv,fi,da,nl,el,ro,tr,hu,th,pl,cs,id,bg,uk,ca,eu,no,sl,sk,hr,sr,ar,hi,vi,fa,ga,iw,lv,lt,gl,is,hy,lo,km,sq,fil,zxx</p>
",1,0,275,2015-01-14 12:52:20,https://stackoverflow.com/questions/27943417/freebase-to-find-alternative-company-names
Named Entity Recognition using WEKA,"<p>I am new to WEKA and I want to ask you few questions regarding WEKA.
I had follow this tutorial (<a href=""http://www.linguisticsweb.org/doku.php?id=linguisticsweb:tutorials:linguistics_tutorials_linguistic_processing_scenarios:named_entity_recognition"" rel=""nofollow"">Named Entity Recognition using WEKA</a>).</p>

<p>But I am really confusing and have no idea at all.</p>

<ol>
<li>Is it possible if I want to filter the string by phrase not word/token?</li>
</ol>

<p>For example in my .ARFF file:</p>

<pre><code>  @attribute text string
  @attribute tag {CC, CD, DT, EX, FW, IN, JJ, JJR, JJS, LS, MD, NN, NNS, NNP, NNPS, PDT, POS, PRP, PRP$, RB, RBR, RBS, RP, SYM, TO, UH, VB, VBD , VBG, VBN , VBP, VBZ, WDT, WP, WP$, WRB, ,, ., :}
  @attribute capital {Y, N}
  @attribute chunked {B-NP, I-NP, B-VP, I-VP, B-PP, I-PP, B-ADJP, B-ADVP , B-SBAR, B-PRT, O-Punctuation}
  @attribute @@class@@ {B-PER, I-PER, B-ORG, I-ORG, B-NUM, I-NUM, O, B-LOC, I-LOC}

  @data
  'Wanna',NNP,Y,B-NP,O
  'be',VB,N,B-VP,O
  'like',IN,N,B-PP,O
  'New',NNP,Y,B-NP,B-LOC
  'York',NNP,Y,I-NP,I-LOC
   '?',.,N,O-Punctuation,O
</code></pre>

<p>So, when I filtered the String, it tokenized the string into word but what I want is, I want to tokenize/filter the string according to the phrase. For example extract the phrase ""New York"" not ""New"" and ""York"" according to the chunked attributes.</p>

<p>""B-NP"" means start phrase and ""I-NP"" means next phrase (the middle or  end of the phrase).</p>

<ol start=""2"">
<li>How can i show the result for the classify class for example:</li>
</ol>

<p>B-PER and I-PER to the class name PERSON?</p>

<pre><code>                 TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                    0         0.021      0         0         0          0.768    B-PER
                    1         0.084      0.333     1         0.5        0.963    I-PER
                  0.167     0.054      0.167     0.167     0.167      0.313    B-ORG
                    0         0          0         0         0          0.964    I-ORG
                    0         0          0         0         0          0.281    B-NUM
                    0         0          0         0         0          0.148    I-NUM
                    0.972     0.074      0.972     0.972     0.972      0.949    O
                    0.875     0          1         0.875     0.933      0.977    B-LOC
                    0         0          0         0         0          0.907    I-LOC
</code></pre>

<p>Weighted Avg.    0.828     0.061      0.811     0.828     0.813      0.894</p>
","weka, named-entity-recognition","<p>In my opinion, WEKA won't (currently) be the best machine learning software to do NER... as far as I know, WEKA does classify sets of examples, for NER it may be done either:</p>

<ol>
<li><strong>By tokenizing sentences in tokens</strong>: in that case sequence (i.e. contiguity) will be lost... ""New"" and ""York"" are two separate examples, the fact that those words are contiguous won't be taken into account in any way.</li>
<li><strong>By keeping chunks / sentences as examples</strong>: sequences can then be kept as a whole and filtered (StringToWordVector for instance), but one class has to be associated for each chunk/sentence (for instance O+O+O+B-LOC+I-LOC+O is the class of the whole sentence in your example).</li>
</ol>

<p>In both cases, contiguity is not taken into account, which is really disturbing. Also, as far as I know, this is the same for R (?). This why ""sequence labelling"" (NER, morpho-syntax, syntax and dependencies) are usually done using software that determines a token category using current word, but also previous, next word, etc. and can output single tokens but also multitoken expressions or more complicated structures.</p>

<p>For NER, currently, CRF are usually used for that, see:</p>

<ul>
<li>CRF++</li>
<li>CRFSuite</li>
<li>Wapiti</li>
<li>Mallet</li>
<li>...</li>
</ul>
",3,2,1477,2015-04-07 08:32:35,https://stackoverflow.com/questions/29487186/named-entity-recognition-using-weka
How to find Named Entity for any Unicode Value,"<p>How to find named entity for any Unicode value, or any ISO characters, for <code>egrave</code> the html named entity is <code>&amp;amp;egrave;</code>, Unicode is <code>&amp;amp;#x00C8;</code>, likewise I am looking for named entity for this symbol ờ (its not <code>ograve</code>, its a symbol in Vietnam language) and its Unicode value is <code>&amp;amp;#x7901;</code>. Can anyone suggest this and help to find any symbol.</p>
","html, unicode, entity, named-entity-recognition","<blockquote>
  <p>I am looking for named entity for this symbol <code>ờ</code></p>
</blockquote>

<p>There isn't one. HTML only provides named entities for a small selection of characters, not all of Unicode. The characters available are largely Western and mathematical characters that were commonly in use in the early days of the web back when people's text editors couldn't do Unicode.</p>

<p>These days, other than:</p>

<ul>
<li>the characters you have to escape to stop them acting as delimiters (<code>&amp;amp;</code>, <code>&amp;lt;</code>, <code>&amp;quot;</code>), and maybe</li>
<li>the otherwise-invisible characters (<code>&amp;nbsp;</code>, <code>&amp;shy;</code>...),</li>
</ul>

<p>there isn't much good reason to use named entities. Instead just type <code>ờ</code> as-is and make sure to save your file with a suitable encoding and declaration (eg UTF-8).</p>

<p>The full list of named entities in HTML5 is <a href=""http://dev.w3.org/html5/html-author/charref"" rel=""nofollow"">here</a>; for any other character you must either type it normally or, if you have to work with unfortunate tools that can't support Unicode, use a numeric character reference such as <code>&amp;#x1EDD;</code> for U+1EDD Latin Small Letter O With Horn And Grave (<code>ờ</code>).</p>
",2,0,227,2015-04-20 12:31:59,https://stackoverflow.com/questions/29747980/how-to-find-named-entity-for-any-unicode-value
Train model using Named entity,"<p>I am looking on standford corenlp using the Named Entity REcognizer.I have different kinds of input text and i need to tag it into my own Entity.So i  started training my own model and it doesnt seems to be working.</p>

<p>For eg: my input text string is ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio <a href=""http://t.co/EqxmY1VmLg"">http://t.co/EqxmY1VmLg</a> <a href=""http://t.co/F0Vefuoj9Q"">http://t.co/F0Vefuoj9Q</a>""</p>

<p>I go through the examples to train my own models and and look for only some words that I am interested in.</p>

<p>My jane-austen-emma-ch1.tsv looks like this</p>

<pre><code>Toyota  PERS
Land Cruiser    PERS
</code></pre>

<p>From the above input text i am only interested in those two words. The one is 
Toyota and the other word is Land Cruiser.</p>

<p>The austin.prop look like this</p>

<pre><code>trainFile = jane-austen-emma-ch1.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Run the following command to generate the ner-model.ser.gz file </p>

<p>java -cp stanford-corenlp-3.4.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop</p>

<pre><code>public static void main(String[] args) {
        String serializedClassifier = ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz"";
        String serializedClassifier2 = ""C:/standford-ner/ner-model.ser.gz"";
        try {
            NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, 
                    serializedClassifier2,serializedClassifier);
            String ss = ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio http://t.co/EqxmY1VmLg http://t.co/F0Vefuoj9Q"";
            System.out.println(""---"");
            List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(ss);
            for (List&lt;CoreLabel&gt; sentence : out) {
              for (CoreLabel word : sentence) {
                System.out.print(word.word() + '/' + word.get(AnswerAnnotation.class) + ' ');
              }
              System.out.println();
            }

        } catch (ClassCastException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }  catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

    }
</code></pre>

<p>Here is the output I am getting</p>

<pre><code>Book/PERS of/PERS 49/O Magazine/PERS Articles/PERS on/O Toyota/PERS Land/PERS Cruiser/PERS 1956-1987/PERS Gold/O Portfolio/PERS http://t.co/EqxmY1VmLg/PERS http://t.co/F0Vefuoj9Q/PERS
</code></pre>

<p>which i think its wrong.I am looking for Toyota/PERS and Land Cruiser/PERS(Which is a multi valued fied.</p>

<p>Thanks for the Help.Any help is really appreciated.</p>
","nlp, stanford-nlp, sentiment-analysis, named-entity-recognition, pos-tagger","<p>The NERClassifier* is word level, that is, it labels words, not phrases. Given that, the classifier seems to be performing fine. If you want, you can hyphenate words that form phrases. So in your labeled examples and in your test examples, you would make ""Land Cruiser"" to ""Land_Cruiser"".</p>
",1,2,2456,2015-04-20 18:43:00,https://stackoverflow.com/questions/29755910/train-model-using-named-entity
How to NER and POS tag a pre-tokenized text with Stanford CoreNLP?,"<p>I'm using the Stanford's CoreNLP Named Entity Recognizer (NER) and Part-of-Speech (POS) tagger in my application. The problem is that my code tokenizes the text beforehand and then I need to NER and POS tag each token. However I was only able to find out how to do that using the command line options but not programmatically.</p>

<p>Can someone please tell me how programmatically can I NER and POS tag pretokenized text using Stanford's CoreNLP?</p>

<p>Edit: </p>

<p>I'm actually using the individual NER and POS instructions. So my code was written as instructed in the tutorials given in the Stanford's NER and POS packages. But I have CoreNLP in my classpath. So I have the CoreNLP in my classpath but using the tutorials in the NER and POS packages. </p>

<p>Edit:</p>

<p>I just found that there are instructions as how one can set the properties for CoreNLP here <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml</a> but I wish if there was a quick way to do what I want with Stanford NER and POS taggers so I don't have to recode everything!</p>
","nlp, stanford-nlp, named-entity-recognition, pos-tagger","<p>If you set the property:</p>

<pre><code>tokenize.whitespace = true
</code></pre>

<p>then the CoreNLP pipeline will tokenize on whitespace rather than the default PTB tokenization. You may also want to set:</p>

<pre><code>ssplit.eolonly = true
</code></pre>

<p>so that you only split sentences on newline characters.</p>
",5,5,1712,2015-04-22 19:33:27,https://stackoverflow.com/questions/29807175/how-to-ner-and-pos-tag-a-pre-tokenized-text-with-stanford-corenlp
How to suppress unmatched words in Stanford NER classifiers?,"<p>I am new to Stanford NLP and NER and trying to train a custom classifier with a data sets of currencies and countries.</p>

<p>My training data in training-data-currency.tsv looks like -</p>

<pre><code>USD CURRENCY
GBP CURRENCY
</code></pre>

<p>And, training data in training-data-countries.tsv looks like -</p>

<pre><code>USA COUNTRY
UK  COUNTRY
</code></pre>

<p>And, classifiers properties look like -</p>

<pre><code>trainFileList = classifiers/training-data-currency.tsv,classifiers/training-data-countries.tsv
ner.model=classifiers/english.conll.4class.distsim.crf.ser.gz,classifiers/english.muc.7class.distsim.crf.ser.gz,classifiers/english.all.3class.distsim.crf.ser.gz
serializeTo = classifiers/my-classification-model.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
#no ngrams will be included that do not contain either the
#beginning or end of the word
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
#the next 4 deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Java code to find the categories is -</p>

<pre><code>LinkedHashMap&lt;String, LinkedHashSet&lt;String&gt;&gt; map = new&lt;String, LinkedHashSet&lt;String&gt;&gt; LinkedHashMap();
NERClassifierCombiner classifier = null;
try {
    classifier = new NERClassifierCombiner(true, true, 
            ""C:\\Users\\perso\\Downloads\\stanford-ner-2015-04-20\\stanford-ner-2015-04-20\\classifiers\\my-classification-model.ser.gz""
            );
} catch (IOException e) {
    // TODO Auto-generated catch block
    e.printStackTrace();
}
List&lt;List&lt;CoreLabel&gt;&gt; classify = classifier.classify(""Zambia"");
for (List&lt;CoreLabel&gt; coreLabels : classify) {
    for (CoreLabel coreLabel : coreLabels) {

        String word = coreLabel.word();
        String category = coreLabel
                .get(CoreAnnotations.AnswerAnnotation.class);
        if (!""O"".equals(category)) {
            if (map.containsKey(category)) {
                map.get(category).add(word);
            } else {
                LinkedHashSet&lt;String&gt; temp = new LinkedHashSet&lt;String&gt;();
                temp.add(word);
                map.put(category, temp);
            }
            System.out.println(word + "":"" + category);
        }

    }

}
</code></pre>

<p>When I run the above code with input as ""USD"" or ""UK"", I get expected result as ""CURRENCY"" or ""COUNTRY"". But, when I input something like ""Russia"", return value is ""CURRENCY"" which is from the first train file in the properties. I am expecting 'O' would be returned for these values which is not present in my training dat.</p>

<p>How can I achieve this behavior? Any pointers where I am going wrong would be really helpful.</p>
","nlp, stanford-nlp, named-entity-recognition","<p>Hi I'll try to help out!</p>

<p>So it sounds to me like you have a list of strings that should be called ""CURRENCY"", and you have a list of strings that should be called ""COUNTRY"", etc...</p>

<p>And you want something to tag strings based off of your list.  So when you see ""RUSSIA"", you want it to be tagged ""COUNTRY"", when you see ""USD"", you want it to be tagged ""CURRENCY"".</p>

<p>I think these tools will be more helpful for you (particularly the first one):</p>

<p><a href=""http://nlp.stanford.edu/software/regexner/"" rel=""nofollow"">http://nlp.stanford.edu/software/regexner/</a></p>

<p><a href=""http://nlp.stanford.edu/software/tokensregex.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokensregex.shtml</a></p>

<p>The NERClassifierCombiner is designed to train on large volumes of tagged sentences and look at a variety of features including the capitalization and the surrounding words to make a guess about a given word's NER label.</p>

<p>But it sounds to me in your case you just want to explicitly tag certain sequences based off of your pre-defined list.  So I would explore the links I provided above.</p>

<p>Please let me know if you need any more help and I will be happy to follow up!</p>
",2,0,991,2015-05-09 15:35:31,https://stackoverflow.com/questions/30141835/how-to-suppress-unmatched-words-in-stanford-ner-classifiers
NLTK : combining stanford tagger and personal tagger,"<p>The goal of my project is to answer queries such as, for example:
""I am looking for American women between 20 and 30 years old who work in Google""
I then have to process the query and to look into a DB to find the answer.</p>

<p>For this, I would need to combine the Stanford 3-class NERTagger and my own tagger. Indeed, my NER tagger can tag ages, nationalities and gender. But I need the Stanford tagger to tag organizations as I don't have any training file for this.</p>

<p>Right now, I have a code like this:</p>

<pre><code>def __init__(self, q):
    self.userQuery = q
def get_tagged_tokens(self):
    st = NERTagger('C:\stanford-ner-2015-01-30\my-ner-model.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    result = st.tag(self.userQuery.split())[0]
    return result
</code></pre>

<p>And I would like to have something like this:</p>

<pre><code>def get_tagged_tokens(self):
    st = NERTagger('C:\stanford-ner-2015-01-30\my-ner-model.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    st_def = NERTagger('C:\stanford-ner-2015-01-30\classifiers\english.all.3class.distsim.crf.ser.gz','C:\stanford-ner-2015-01-30\stanford-ner.jar')
    tagger = BackoffTagger([st, st_def])
    result = st.tag(self.userQuery.split())[0]
    return result
</code></pre>

<p>This would mean that the tagger first uses my tagger and then the stanford one to tag untagged words. </p>

<p>Is it possible to combine my model with the Stanford model just to tag organizations? If yes, what is the best way to perform this?</p>

<p>Thank you!</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>The new NERClassifierCombiner with Stanford CoreNLP 3.5.2 or the new Stanford NER 3.5.2 has added command line functionality that makes it easy to get this effect with NLTK.</p>

<p>When you provide a list of serialized classifiers, NERClassifierCombiner will run them in sequence.  After one tagger tags the sentence, no other taggers will tag tokens that have already been tagged.  So note in my demo code I provide 2 classifiers as an example.  They are run in the order you place them.  I believe you can put as many as 10 in there if I recall correctly!</p>

<p>First, make sure that you have the latest copy of Stanford CoreNLP 3.5.2 or Stanford NER 3.5.2 , so that you have the right .jar file with this new functionality.</p>

<p>Second, make sure your custom NER model was built with Stanford CoreNLP or Stanford NER, this won't work otherwise!  It should be ok if you used older versions.</p>

<p>Third, I have provided some sample code that should work, the main gist of this is to subclass NERTagger:</p>

<p>If people would like I could look into pushing this to NLTK so it is in there by default!</p>

<p>Here is some sample code (it is a little hacky since I was just rushing this out the door, for instance in NERComboTagger's constructor there is no point to the first argument being classifier_path1, but the code would crash if I didn't put a valid file there):</p>

<pre><code>#!/usr/bin/python

from nltk.tag.stanford import NERTagger

class NERComboTagger(NERTagger):

  def __init__(self, *args, **kwargs):
    self.stanford_ner_models = kwargs['stanford_ner_models']
    kwargs.pop(""stanford_ner_models"")
    super(NERComboTagger,self).__init__(*args, **kwargs)

  @property
  def _cmd(self):
    return ['edu.stanford.nlp.ie.NERClassifierCombiner',
            '-ner.model',
            self.stanford_ner_models,
            '-textFile',
            self._input_file_path,
            '-outputFormat',
            self._FORMAT,
            '-tokenizerFactory',
            'edu.stanford.nlp.process.WhitespaceTokenizer',
            '-tokenizerOptions',
            '\""tokenizeNLs=false\""']

classifier_path1 = ""classifiers/english.conll.4class.distsim.crf.ser.gz""
classifier_path2 = ""classifiers/english.muc.7class.distsim.crf.ser.gz""

ner_jar_path = ""stanford-ner.jar""

st = NERComboTagger(classifier_path1,ner_jar_path,stanford_ner_models=classifier_path1+"",""+classifier_path2)

print st.tag(""Barack Obama is from Hawaii ."".split("" ""))  
</code></pre>

<p>Note the major change in the subclass is what is returned by _cmd .</p>

<p>Also note that I ran this in the unzipped folder stanford-ner-2015-04-20 , so the paths are relative to that.</p>

<p>I get this output:</p>

<pre><code>[('Barack','PERSON'), ('Obama', 'PERSON'), ('is','O'), ('from', 'O'), ('Hawaii', 'LOCATION'), ('.', 'O')]
</code></pre>

<p>Here is a link to the Stanford NER page: </p>

<p><a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/CRF-NER.shtml</a></p>

<p>Please let me know if you need any more help or if there are any errors in my code, I may have made a mistake while transcribing, but it works on my laptop!</p>
",8,1,1274,2015-05-11 08:24:46,https://stackoverflow.com/questions/30162809/nltk-combining-stanford-tagger-and-personal-tagger
Relation extraction via chunking using NLTK,"<p>I am trying to figure out how to use NLTK's cascading chunker as per <a href=""http://www.nltk.org/book/ch07.html"" rel=""nofollow"">Chapter 7 of the NLTK book</a>. Unfortunately, I'm running into a few issues when performing non-trivial chunking measures.</p>

<p>Let's start with this phrase:</p>

<p><code>""adventure movies between 2000 and 2015 featuring performances by daniel craig""</code></p>

<p>I am able to find all the relevant NPs when I use the following grammar:</p>

<p><code>grammar = ""NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}""</code></p>

<p>However, I am not sure how to build nested structures with NLTK. The book gives the following format, but there are clearly a few things missing (e.g. How does one actually specify multiple rules?):</p>

<pre><code>grammar = r""""""
  NP: {&lt;DT|JJ|NN.*&gt;+}          # Chunk sequences of DT, JJ, NN
  PP: {&lt;IN&gt;&lt;NP&gt;}               # Chunk prepositions followed by NP
  VP: {&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$} # Chunk verbs and their arguments
  CLAUSE: {&lt;NP&gt;&lt;VP&gt;}           # Chunk NP, VP
  """"""
</code></pre>

<p>In my case, I'd like to do something like the following:</p>

<pre><code>grammar = r""""""
          MEDIA: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
          RELATION: {&lt;V.*&gt;}{&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
          ENTITY: {&lt;NN.*&gt;}
          """"""
</code></pre>

<p>Assuming that I'd like to use a cascaded chunker for my task, what syntax would I need to use? Additionally, is it possible for me to specify specific words (e.g. ""directed"" or ""acted"") when using a chunker?</p>
","python, nltk, named-entity-recognition, chunking","<p>I can't comment on the relationship extraction part, not least because you don't give any details on what you want to do and what kind of data you have. So this is a rather partial answer.</p>

<blockquote>
  <p>a.) How does cascading chunking work in NLTK b.) Is it possible to treat the chunker like a context-free grammar, and if so, how?</p>
</blockquote>

<p>As I understand section  <a href=""http://www.nltk.org/book/ch07.html#building-nested-structure-with-cascaded-chunkers"" rel=""nofollow"">""Building nested structure with cascaded chunkers""</a> in the NLTK book, you can use it with a context free grammar but you have to apply it repeatedly to get the recursive structure. Chunkers are flat, but you can add chunks on top of chunks.</p>

<blockquote>
  <p>c.) How can I use chunking to perform relation extraction?</p>
</blockquote>

<p>I can't really speak to that, and anyway as I said you don't give any specifics; but if you're dealing with real text, my understanding is is that hand-written rulesets for <em>any</em> task are useless unless you have a large team and a lot of time. Look into the probabilistic tools that come with the NLTK. It'll be a whole lot easier if you have an annotated training corpus.</p>

<p>Anyway, a couple more comments about the RegexpParser.</p>

<ol>
<li><p>You'll find a lot more use examples on <a href=""http://www.nltk.org/howto/chunk.html"" rel=""nofollow"">http://www.nltk.org/howto/chunk.html</a>. (Unfortunately it's not a real how-to, but a test suite.)</p></li>
<li><p>According to <a href=""http://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/dongqing-chunking.pdf"" rel=""nofollow"">this,</a> you can specify multiple expansion rules like this:</p>

<pre><code>patterns = """"""NP: {&lt;DT|PP\$&gt;?&lt;JJ&gt;*&lt;NN&gt;}
    {&lt;NNP&gt;+}
    {&lt;NN&gt;+}
""""""
</code></pre>

<p>I should add that grammars can have multiple rules with the same left side. That should add some flexibility with grouping related rules, etc.</p></li>
</ol>
",3,9,2220,2015-05-16 00:16:24,https://stackoverflow.com/questions/30270502/relation-extraction-via-chunking-using-nltk
Extract list of Persons and Organizations using Stanford NER Tagger in NLTK,"<p>I am trying to extract list of persons and organizations using Stanford Named Entity Recognizer (NER) in Python NLTK.
When I run:</p>

<pre><code>from nltk.tag.stanford import NERTagger
st = NERTagger('/usr/share/stanford-ner/classifiers/all.3class.distsim.crf.ser.gz',
               '/usr/share/stanford-ner/stanford-ner.jar') 
r=st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
print(r) 
</code></pre>

<p>the output is:</p>

<pre><code>[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),
('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),
('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]
</code></pre>

<p>what I want is to extract from this list all persons and organizations in this form:</p>

<pre><code>Rami Eid
Sony Brook University
</code></pre>

<p>I tried to loop through the list of tuples:</p>

<pre><code>for x,y in i:
        if y == 'ORGANIZATION':
            print(x)
</code></pre>

<p>But this code only prints every entity one per line:</p>

<pre><code>Sony 
Brook 
University
</code></pre>

<p>With real data there can be more than one organizations, persons in one sentence, how can I put the limits between different entities?</p>
","python, nltk, stanford-nlp, named-entity-recognition","<p>Thanks to the <a href=""https://stackoverflow.com/questions/13765349/multi-term-named-entities-in-stanford-named-entity-recognizer"">link</a> discovered by @Vaulstein, it is clear that the trained Stanford tagger, as distributed (at least in 2012) <strong>does not chunk named entities</strong>. From <a href=""https://stackoverflow.com/a/13781588/699305"">the accepted answer</a>:</p>
<blockquote>
<p>Many NER systems use more complex labels such as IOB labels, where codes like B-PERS indicates where a person entity starts. The CRFClassifier class and feature factories support such labels, <strong>but they're not used in the models we currently distribute (as of 2012)</strong></p>
</blockquote>
<p>You have the following options:</p>
<ol>
<li><p>Collect runs of identically tagged words; e.g., all adjacent words tagged <code>PERSON</code> should be taken together as one named entity. That's very easy, but of course it will sometimes combine different named entities. (E.g. <code>New York, Boston [and] Baltimore</code> is about three cities, not one.)  <strong>Edit:</strong> This is what Alvas's code does in the accepted anwser. See below for a simpler implementation.</p>
</li>
<li><p>Use <code>nltk.ne_chunk()</code>. It doesn't use the Stanford recognizer but it does chunk entities. (It's a wrapper around an IOB named entity tagger).</p>
</li>
<li><p>Figure out a way to do your own chunking on top of the results that the Stanford tagger returns.</p>
</li>
<li><p>Train your own IOB named entity chunker (using the Stanford tools, or the NLTK's framework) for the domain you are interested in. If you have the time and resources to do this right, it will probably give you the best results.</p>
</li>
</ol>
<p><strong>Edit:</strong> If all you want is to pull out runs of continuous named entities (option 1 above), you should use <code>itertools.groupby</code>:</p>
<pre><code>from itertools import groupby
for tag, chunk in groupby(netagged_words, lambda x:x[1]):
    if tag != &quot;O&quot;:
        print(&quot;%-12s&quot;%tag, &quot; &quot;.join(w for w, t in chunk))
</code></pre>
<p>If <code>netagged_words</code> is the list of <code>(word, type)</code> tuples in your question, this produces:</p>
<pre class=""lang-none prettyprint-override""><code>PERSON       Rami Eid
ORGANIZATION Stony Brook University
LOCATION     NY
</code></pre>
<p>Note again that if two named entities of the same type occur right next to each other, this approach will combine them. E.g. <code>New York, Boston [and] Baltimore</code> is about three cities, not one.</p>
",30,27,27744,2015-06-05 10:49:58,https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk
Named Entity Recognition - Do we need an external list to match results?,"<p>I am not an expert in Machine Learning, so I will try to be as accurate as possible...</p>

<p>I am currently analyzing financial documents that are giving information on a specific fund. What I would like to do is to be able to extract the fund name.</p>

<p>For this, I am using Named Entity Recognition (NER) in Azure Machine Learning platform. After analyzing approx. 100 documents, I get results classified as Organizations. In most cases, they are really organizations. This is great, but my problem is that the fund name is also categorized as an organization. I am not able to distinguish between a company name and a fund name.</p>

<p>From some readings on Internet, I could discover that Gazette system could help so that we can match the recognized organizations against a list of funds, and therefore make sure that we have a fund name.</p>

<p>Do you think this would be a good approach? Or is there any other algorithm that I should try to improve the results?</p>

<p>Thanks for any suggestion!</p>
","azure, machine-learning, nlp, named-entity-recognition","<p>NER has its origins in identifying text identifying broad semantic categories, like the names of people or organizations (companies) in your case. Reading the description of question, I don't think this is the problem you really want to solve. Specifically you mention:</p>

<blockquote>
  <p>that Gazette system could help so that we can match the recognized organizations against a list of funds</p>
</blockquote>

<p>I suspect the problem you really want to solve is one of semantic interoperability - you want text from your NLP program to match a list you have that is part of another system. In that case, the only accepted way you are going to solve your problem is to map all of the input text to a list/common standard - ie) use the gazetteer. So you are on the right path.</p>

<p>The only caveat is that if you <strong>only</strong> need to distinguish between funds and other types of organizations - without the need to match the results against a list. If that is the case, you write a classifier to distinguish funds from everything else and you can avoid mapping to your list entirely. Otherwise use a gazetteer.</p>
",1,0,375,2015-06-18 20:50:26,https://stackoverflow.com/questions/30925579/named-entity-recognition-do-we-need-an-external-list-to-match-results
Java named entity recognition library for Persons Name &quot;Parts&quot;,"<p>My current project needs to improve the data quality of our customers details.</p>

<p>One issue we have is that customers names have seperate data capture input fields for First, Middle names and surnames, however in many cases each part of the name was entered incorrectly.</p>

<p>We need to clean up the data we hold.</p>

<p>This data quality issue impacts when we contact our customers in correspondance, because we do not know their first name, middle names and surnames we offend some customers by using an inappropriate salutation</p>

<p>We need a named entity recognition library that can not only detect PERSONS names, but also detct First, Middle and Surnames.</p>

<p>What makes this data quality task harder is that we have almost 100 million customers, our customer base is world wide so we need to be able to identify first , middle and surnames, e.g. Given name, patronymic, and different ordr of parts. what will help is that we also know the customers nationallity.</p>

<p>Does a named entity recognition exist specific for Person Name PARTS?</p>

<p>I realise that a ""Perfect"" solution is impossible, however I am sure I can improve the data quality we currently have.</p>

<p>I just mentioned First, Middle and surnames as thats name structure I am most familiar with, however i do understand the following are examples of what I am facing</p>

<pre><code>In many parts of the world, parts of names are derived from titles, locations, genealogical information, caste, religious references, and so on. Here are a few examples:

    the Indian name Kogaddu Birappa Timappa Nair follows the order villageName-fathersName-givenName-lastName.
    the Rajasthani name Aditya Pratap Singh Chauhan is composed of givenName-fathersName-surname-casteName.

    in another part of India the name Madurai Mani Iyer represents townName-givenName-casteName.

    the Arabic Abu Karim Muhammad al-Jamil ibn Nidal ibn Abdulaziz al-Filistini translates as ""Father of Karim, Muhammad (given name), The beautiful, Son of Nidal, Son of Abdulaziz, the Palestinian"". Karim is Muhammad's first-born son.
</code></pre>
","java, named-entity-recognition","<p>There is a simple, universal solution that companies seem surprisingly unwilling to apply:</p>

<p>Include a salutation if, and only if, the communication really is from a human being who is preparing that communication specifically for the recipient. In that case, part of paying attention to the recipient is writing a correct salutation taking into account the recipient's culture.</p>

<p>If you are computer-generating a communication using names from a database, be honest about what you are doing. Simply show the name as it was supplied to you on whatever form it came from. Do not attempt to use it to construct a formal salutation. Do not change it in any way. Communications that are obviously computer-generated but that try to pretend individual attention just look silly, even if they are not sufficiently incorrect to cause actual annoyance.</p>
",0,0,544,2015-06-20 08:51:54,https://stackoverflow.com/questions/30952137/java-named-entity-recognition-library-for-persons-name-parts
What&#39;s meaning of BOS and EOS in CRFSuite feature list and what is the role of them?,"<p>In NER(Named Entity Recognition) example in python-crf package website we see this function as feature generator:</p>

<pre><code>def word2features(sent, i):
word = sent[i][0]
postag = sent[i][1]
features = [
    'bias',
    'word.lower=' + word.lower(),
    'word[-3:]=' + word[-3:],
    'word[-2:]=' + word[-2:],
    'word.isupper=%s' % word.isupper(),
    'word.istitle=%s' % word.istitle(),
    'word.isdigit=%s' % word.isdigit(),
    'postag=' + postag,
    'postag[:2]=' + postag[:2],
]
if i &gt; 0:
    word1 = sent[i-1][0]
    postag1 = sent[i-1][1]
    features.extend([
        '-1:word.lower=' + word1.lower(),
        '-1:word.istitle=%s' % word1.istitle(),
        '-1:word.isupper=%s' % word1.isupper(),
        '-1:postag=' + postag1,
        '-1:postag[:2]=' + postag1[:2],
    ])
else:
    features.append('BOS')

if i &lt; len(sent)-1:
    word1 = sent[i+1][0]
    postag1 = sent[i+1][1]
    features.extend([
        '+1:word.lower=' + word1.lower(),
        '+1:word.istitle=%s' % word1.istitle(),
        '+1:word.isupper=%s' % word1.isupper(),
        '+1:postag=' + postag1,
        '+1:postag[:2]=' + postag1[:2],
    ])
else:
    features.append('EOS')

return features
</code></pre>

<p>You can see the completed tutorial there:
<a href=""http://nbviewer.ipython.org/github/tpeng/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb#Features"" rel=""nofollow"">python-crfsuite NER example</a></p>

<p>As you see after appending meaningful features - like <strong>word.lower</strong> and ...- two features has appended.</p>

<pre><code>features.append('EOS')
</code></pre>

<p>and</p>

<pre><code>features.append('BOS')
</code></pre>

<p>My question is ""What's meaning of BOS and EOS and what is the role of them?""</p>
","python, named-entity-recognition, crf","<p>These stand for ""Beginning of Sentence"" and ""End of Sentence"".  They are used in place of the ""previous word"" and ""next word"" features for words that do not have previous/next words.</p>
",3,3,1216,2015-07-13 14:43:24,https://stackoverflow.com/questions/31386459/whats-meaning-of-bos-and-eos-in-crfsuite-feature-list-and-what-is-the-role-of-t
Output &quot;file--token--entity&quot; using Stanford NER,"<p>I want to use Stanford NER in C# to read all files in a folder and output the result into one file in the format ""file  token  entity""</p>

<p>Here is what I have:</p>

<pre class=""lang-c# prettyprint-override""><code>namespace stanfordNER
{
    class Program
    {
        public static CRFClassifier Classifier = CRFClassifier.getClassifierNoExceptions(@""english.all.3class.distsim.crf.ser.gz"");

        static void Main(string[] args)
        {
            Console.WriteLine(""directory address?"");
            string dir = Console.ReadLine();

            //Reads all files in directory
            string[] files = System.IO.Directory.GetFiles(dir);
            foreach (string f in files)
            {
                //Get the document name
                string docNo = Path.GetFileName(Path.GetFullPath(f).TrimEnd(Path.DirectorySeparatorChar));
                Console.WriteLine(docNo);

                string docText = System.IO.File.ReadAllText(f); 

                var classified = Classifier.classifyFile(f).toArray();

                //Error here when running
                //Should output the entities,**this part is the work of Stewart Whiting (STEWH)
                for (int i = 0; i &lt; classified.Length; i++)
                {
                    Triple triple = (Triple)classified[i];

                    int second = Convert.ToInt32(triple.second().ToString());
                    int third = Convert.ToInt32(triple.third().ToString());

                    Console.WriteLine(docNo + '\t' + triple.first().ToString() + '\t' +                              docText.Substring(second, third - second));
                }
            }
        }
    }
}
</code></pre>

<p>I get a invalid cast exception error at ""triple"".  I don't understand how to use the triple function.</p>

<p>example of the output I want:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""true"">
<div class=""snippet-code snippet-currently-hidden"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>wiki-ms      ORGANIZATION    Microsoft Corporation
wiki-ms      LOCATION        Redmond
wiki-ms      LOCATION        Washington
wiki-ms      ORGANIZATION    Microsoft
wiki-ms      ORGANIZATION    Microsoft Office
wiki-ms      ORGANIZATION    Microsoft
wiki-ms      PERSON          Bill Gates
wiki-ms      PERSON          Paul Allen
wiki-ms      ORGANIZATION    Microsoft
wiki-ms      ORGANIZATION    Microsoft</code></pre>
</div>
</div>
</p>

<p>Thanks in advance!  I'm a manufacturing engineer so my programming knowledge is pretty bad.</p>

<p>If you have a way to filter duplicates and/or similar entities that would be an added bonus!</p>

<p>Thanks to Stewart Whiting. <a href=""http://www.stewh.com/"" rel=""nofollow"">His Site</a></p>
","c#, stanford-nlp, named-entity-recognition","<p>I figured it out, just had to change</p>

<p><code>var classified = Classifier.classifyFile(f).toArray();</code></p>

<p>to</p>

<pre><code>var classified = Classifier.classifyToCharacterOffsets(docText).toArray();
</code></pre>

<p>thanks.</p>
",0,0,282,2015-07-24 16:44:04,https://stackoverflow.com/questions/31615731/output-file-token-entity-using-stanford-ner
NLTK Named Entity recognition to a Python list,"<p>I used NLTK's <code>ne_chunk</code> to extract named entities from a text:</p>

<pre><code>my_sent = ""WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.""


nltk.ne_chunk(my_sent, binary=True)
</code></pre>

<p>But I can't figure out how to save these entities to a list? E.g. –</p>

<pre><code>print Entity_list
('WASHINGTON', 'New York', 'Loretta', 'Brooklyn', 'African')
</code></pre>

<p>Thanks.</p>
","python, nlp, nltk, named-entity-recognition","<p><code>nltk.ne_chunk</code> returns a nested <code>nltk.tree.Tree</code> object so you would have to traverse the <code>Tree</code> object to get to the NEs.</p>

<p>Take a look at <a href=""https://stackoverflow.com/questions/24398536/named-entity-recognition-with-regular-expression-nltk"">Named Entity Recognition with Regular Expression: NLTK</a></p>

<pre><code>&gt;&gt;&gt; from nltk import ne_chunk, pos_tag, word_tokenize
&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; 
&gt;&gt;&gt; def get_continuous_chunks(text):
...     chunked = ne_chunk(pos_tag(word_tokenize(text)))
...     continuous_chunk = []
...     current_chunk = []
...     for i in chunked:
...             if type(i) == Tree:
...                     current_chunk.append("" "".join([token for token, pos in i.leaves()]))
...             if current_chunk:
...                     named_entity = "" "".join(current_chunk)
...                     if named_entity not in continuous_chunk:
...                             continuous_chunk.append(named_entity)
...                             current_chunk = []
...             else:
...                     continue
...     return continuous_chunk
... 
&gt;&gt;&gt; my_sent = ""WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.""
&gt;&gt;&gt; get_continuous_chunks(my_sent)
['WASHINGTON', 'New York', 'Loretta E. Lynch', 'Brooklyn']


&gt;&gt;&gt; my_sent = ""How's the weather in New York and Brooklyn""
&gt;&gt;&gt; get_continuous_chunks(my_sent)
['New York', 'Brooklyn']
</code></pre>
",36,26,66525,2015-08-05 14:58:33,https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list
How to create a good NER training model in OpenNLP?,"<p>I just have started with OpenNLP. I need to create a simple training model to recognize name entities. </p>

<p>Reading the doc here <a href=""https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/namefind"" rel=""nofollow noreferrer"">https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/namefind</a> I see this simple text to train the model:</p>

<pre><code>&lt;START:person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mr . &lt;START:person&gt; Vinken &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group .
&lt;START:person&gt; Rudolph Agnew &lt;END&gt; , 55 years old and former chairman of Consolidated Gold Fields PLC ,
    was named a director of this British industrial conglomerate .
</code></pre>

<p>The questions are two:</p>

<ul>
<li><p>Why should i have to put the names of the persons in a text (phrase) context ? Why not write person's name one for each line? like:</p>

<pre><code>&lt;START:person&gt; Robert &lt;END&gt;

&lt;START:person&gt; Maria &lt;END&gt;

&lt;START:person&gt; John &lt;END&gt;
</code></pre></li>
<li><p>How can I also add extra information to that name?
For example I would like to save the information Male/Female for each name.</p></li>
</ul>

<p>(I know there are systems that try to understand it reading the last letter, like the ""a"" for <strong>Female</strong> etc but i would like to add it myself)</p>

<p>Thanks.</p>
","java, nlp, text-mining, opennlp, named-entity-recognition","<p>The answer to your first question is that the algorithm works on surrounding context(tokens) within a sentence; it's not just a simple lookup mechanism. OpenNLP uses maximum entropy, which is a form of multinomial logistic regression to build its model. The reason for this is to reduce ""word sense ambiguity,"" and find entities in context. For instance, if my name is April, I can easily get confused with the month of April, and if my name is May, then I would get confused with the month of May as well as the verb may. For your second part of the first question, you could make a list of names that are known, and use those names in a program that looks at your sentences and automatically annotates them to help you create a training set, however making a list of names alone without context will not train the model sufficiently or at all. In fact, there is an OpenNLP addon called the ""modelbuilder addon"" designed for this: you give it a file of names, and it uses the names and some of your data (sentences) to train a model. If you are looking for particular names of generally non ambiguous entities, you may be better off just using a list and something like regex to discover names rather than NER.</p>

<p>As for your second question there are a few options, but in general, I don't think NER is a great tool for delineating something like gender, however with enough training sentences you may get decent results. Since NER uses a model based on surrounding tokens in your sentence training set to establish the existence of a named entity, it can't do much in terms of identifying gender. You may be better off finding all person names, then referencing an index of names that you know are male or female to get a match. Also, some names, like Pat, are both male and female, and in most textual data there will be no indication of which it is to neither human nor machine. That being said, you could create a male and female model separately, or you could create different entity types within the same model. You could use an annotation like this (using different entity type names of male.person and female.person). I've never tried this but it might do ok, you'd have to test it on your data.</p>

<pre><code>&lt;START:male.person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mrs . &lt;START:female.person&gt; Maria &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group
</code></pre>

<p>NER= Named Entity Recognition</p>

<p>HTH</p>
",20,18,7754,2015-08-14 13:43:46,https://stackoverflow.com/questions/32011615/how-to-create-a-good-ner-training-model-in-opennlp
NER model to recognize Indian names,"<p>I am planning to use Named Entity Recognition (NER) technique to identify person names (most of which are Indian names) from a given text. I have already explored the CRF-based NER model from Stanford NLP, however it is not quite accurate in recognizing Indian names. Hence I decided to create my own custom NER model via supervised training. I have a fair idea of how to create own NER model using the Stanford NER CRF, but creating a large training corpus with manual annotation is something I would like to avoid, as it is a humongous effort for an individual and secondly obtaining diverse people names from different states of India is also a challenge. Could anybody suggest any automation/programmatic way to prepare a labelled training corpus with at least 100k Indian names?<br>
I have already looked into Facebook and LinkedIn API, but did not find a way to extract 100k number of user's full name from a given location (e.g. India).</p>
","facebook-graph-api, nlp, stanford-nlp, named-entity-recognition, linkedin-api","<p>I ended up doing the following to create NER model to identify Indian names. This may be useful for anybody looking for creating a custom NER model to recognize non-English person names, since most of the publicly available NER models such as the ones from Stanford NLP were trained with English names and hence are more accurate in identifying English (British/American) names. </p>

<ol>
<li>Find an Indian celebrity with Twitter account and having a huge number of followers in Twitter (for my case, I chose Sachin Tendulkar).</li>
<li>Create a program in the language of your choice to call the Twitter REST API (GET followers/list) to get the names of all the followers of the celebrity and save to a file. We can safely assume most of the followers would be Indians. Note that there is an API Rate Limit in place (30 requests per 15 minute window), so the program should be built in to handle that. For our case, we developed the program as a Windows Service which runs every 15 minutes.</li>
<li>Since some Twitter users' names may not be valid person names, it is advisable to add some rule-based logic (like RegEx) to filter seemingly real names and add only those to the file.</li>
<li>Once the file with real names is generated, create another program to create the training data file containing these names labelled/annotated as PERSON as well as non-entity names annotated as OTHER. If you are using Stanford NER CRF Classifier, the program should generate a training (TSV) file  having two columns - one containing the word (token) and the second column mentioning the label.</li>
<li>Once the training corpus is generated programmatically, you can follow the below link to create your custom NER model to recognize Indian names:
<a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""noreferrer"">http://nlp.stanford.edu/software/crf-faq.shtml#a</a></li>
</ol>
",11,6,11012,2015-08-18 12:53:50,https://stackoverflow.com/questions/32073018/ner-model-to-recognize-indian-names
Best method to confirm an entity,"<p>I would like to understand the best approach to the following problem.</p>

<p>I have documents really similar to resume/cv and I have to extract entities (Name, Surname, Birthday, Cities, zipcode etc).</p>

<p>To extract those entities I am combining different finders (Regex, Dictionary etc)</p>

<p>There are no problems with those finders, but, I am looking for a method / algorithm or something like that to confirm the entities.</p>

<p>With ""confirm"" I mean that I have to find specific term (or entities) in proximities (closer to the entities I have found).</p>

<p>Example:</p>

<pre><code>My name is &lt;name&gt;
Name: &lt;name&gt;
Name and Surname: &lt;name&gt;
</code></pre>

<p>I can confirm the entity <code>&lt;name&gt;</code> because it is closer to specific term that let me understand the ""context"". If i have ""name"" or ""surname"" words near the entity  so i can say that i have found the <code>&lt;name&gt;</code> with a good probability.</p>

<p>So the goal is write those kind of rules to confirm entities. Another example should be:</p>

<blockquote>
  <p>My address is ......, 00143 Rome</p>
</blockquote>

<p>Italian zipcodes are 5 digits long (numeric only), it is easy to find a 5 digits number inside my document (i use regex as i wrote above), and i also check it by querying a database to understand if the number exists. The problem here is that i need one more check to confirm (definitely) it.</p>

<p>I must see if that number is near the entity <code>&lt;city&gt;</code>, if yes, ok... I have good probabilities.</p>

<p>I also tried to train a model but i do not really have a ""context"" (sentences).
Training the model with:</p>

<pre><code>My name is: &lt;name&gt;John&lt;/name&gt;
Name: &lt;name&gt;John&lt;/name&gt;
Name/Surname: &lt;name&gt;John&lt;/name&gt;
&lt;name&gt;John&lt;/name&gt; is my name
</code></pre>

<p>does not sound good to me because:</p>

<ol>
<li>I have read we need many sentences to train a good model</li>
<li>Those are not ""sentences"" i do not have a ""context"" (remember where I said the document is similar to resume/cv)</li>
<li>Maybe those phrases are too short</li>
</ol>

<p>I do not know how many different ways i could find to say the exact thing, but surely I can not find 15000 ways :)</p>

<p>What method should I use to try to confirm my entities?</p>

<p>Thank you so much!</p>
","java, nlp, text-mining, opennlp, named-entity-recognition","<h2>Problem statement</h2>

<p>First of all, I don't think that your decomposition of the task into 2 steps (extract and confirm) is the best, if only I don't miss some specifics in the problem. If I understand correctly, you goal is to extract structured info like Name/City/etc from the set of docs with maximum precision and recall; either metric can be more important, but usually they are considered with equal weights - e.g. by using F1-measure.</p>

<h2>Evaluate first</h2>

<p>'You can't control what you can't measure' <a href=""https://en.wikiquote.org/wiki/Tom_DeMarco"" rel=""nofollow noreferrer"">Tom DeMarco</a> </p>

<p>I'd propose to firstly prepare evaluation system and marked up dataset: for each document find correct Name/City/etc - it can be done fully manually (which is more 'true', but more hard way) or semi-automatically, e.g. by applying some method, including that under development, and correcting its errors if any.
Evaluation system should be able to compute Precision and Recall (see <a href=""https://en.wikipedia.org/wiki/Confusion_matrix"" rel=""nofollow noreferrer"">Confusion matrix</a> in order to easily implement them by yourself).</p>

<p>As for its size, I wouldn't be so afraid by necessity of preparing too big dataset: sure, more is better, but it is crucial for the case with complex (significantly non-linear) tasks and a lot of features. I believe 100-200 docs to be enough for start in your case - and it would take several hours to prepare.</p>

<p>Then you can evaluate your simple extractors based on RegExps and Dictionaries - best if different aspects (Name or City) would have separate metrics. Depending on results, your actions may differ.</p>

<h2>Low precision - add more specific features</h2>

<p>If the method shows too low precision, i.e. extract too many wrong items, you should add specificity, or specific features; I'd search for them in scientific papers devoted to Information extraction concerning to those aiming at the specific information type, be it Name/Surname, or Address, or something more vague like skills if you're interested in such info. For instance, many papers (like [<a href=""https://en.wikipedia.org/wiki/Confusion_matrix"" rel=""nofollow noreferrer"">2</a>] and [<a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">3</a>]) devoted to Resume parsing note that Name/Surname are usually placed at the very beginning of the text; or that Cities are usually preceded by 'at'. 
I don't know specifics of you documents, but I doubt they violate such patterns.</p>

<p>Also it may be useful and easy to treat output of Named Entity Recognizer, e.g. <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">Standord NLP</a>, as a feature (see also <a href=""https://stackoverflow.com/questions/30664399/name-extraction-cv-resume-stanford-ner-opennlp"">relevant question</a>)</p>

<p>Again, harder but better is to analyze approaches used by NERC and to adapt them to specifics of your task and docs.</p>

<p>These features can be aggregated by any Supervised machine learning (start with Logistic Regression and Random Forest if you have not much experience): you know positive and negative (all but not positive) answers from your evaluation dataset, just transform them into feature space and feed to some ML lib like <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow noreferrer"">Weka</a>.</p>

<h2>Low recall - extract more candidates</h2>

<p>If the method shows too low recall, i.e. misses a lot of items, then you should extend set of candidates - for example, develop less restrictive patterns, add fuzzy matching (look at <a href=""https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"" rel=""nofollow noreferrer"">Jaro-Winkler</a> or <a href=""https://en.wikipedia.org/wiki/Soundex"" rel=""nofollow noreferrer"">Soundex</a> string metric) to Dictionary lookup. </p>

<p>Another option is to apply Part-of-Speech tagging and take each noun as a candidate - maybe each Proper noun for some info items, or take noun bigrams, or add other weak restrictions. In this case, most probably, your precision will degrade, so the paragraph above would have to be considered.</p>

<p>NB: If your data comes from Web (e.g. profiles from LinkedIn), try to search by keywords 'Web data extraction' or take a look at import.io</p>

<h2>Literature</h2>

<p><em>just a few random, try to search at <a href=""https://scholar.google.ru/scholar?q=information%20extraction%20survey%20&amp;btnG=&amp;hl=ru&amp;as_sdt=0%2C5"" rel=""nofollow noreferrer"">Google scholar</a>, preferably start from surveys</em></p>

<ol>
<li><p>Renuka S. Anami, Gauri R. Rao. Automated Profile Extraction and
Classification with Stanford Algorithm. International Journal of
Innovative Technology and Exploring Engineering (IJITEE) ISSN:
2278-3075, Volume-4 Issue-7, December 2014 (<a href=""http://www.ijitee.org/attachments/File/v4i7/G1916124714.pdf"" rel=""nofollow noreferrer"">link</a>)</p></li>
<li><p>Swapnil Sonar. Resume Parsing with Named Entity Clustering
Algorithm. 2015 (<a href=""http://www.slideshare.net/swapnilmsonar/resume-parsing-with-named-entity-clustering-algorithm"" rel=""nofollow noreferrer"">link</a>)</p></li>
</ol>
",6,9,793,2015-09-04 14:16:05,https://stackoverflow.com/questions/32400289/best-method-to-confirm-an-entity
Part of speech tagging and entity recognition - python,"<p>I want to perform part of speech tagging and entity recognition in python similar to Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R.  I would prefer a code in python which takes input as textual sentence and gives output as different features- like number of ""CC"", number of ""CD"", number of ""DT"" etc.. CC, CD, DT are POS tags as used in Penn Treebank. So there should be 36 columns/features for POS tagging corresponding to 36 POS tags as in <a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow"">Penn Treebank POS</a>. I want to implement this on Azure ML ""Execute Python Script"" module and Azure ML supports python 2.7.7. I heard nltk in python may does the job, but I am a beginner on python. Any help would be appreciated. </p>
","python, azure, named-entity-recognition, part-of-speech, azure-machine-learning-service","<p>Take a look at <a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow"">NTLK book</a>, Categorizing and Tagging Words section.</p>

<p>Simple example, it uses the Penn Treebank tagset:</p>

<pre><code>from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
pos_tag(word_tokenize(""John's big idea isn't all that bad."")) 

[('John', 'NNP'),
(""'s"", 'POS'),
 ('big', 'JJ'),
 ('idea', 'NN'),
 ('is', 'VBZ'),
 (""n't"", 'RB'),
 ('all', 'DT'),
 ('that', 'DT'),
 ('bad', 'JJ'),
 ('.', '.')]
</code></pre>

<p>Then you can use</p>

<pre><code>from collections import defaultdict
counts = defaultdict(int)
for (word, tag) in pos_tag(word_tokenize(""John's big idea isn't all that bad."")):
    counts[tag] += 1
</code></pre>

<p>to get frequencies:</p>

<pre><code>defaultdict(&lt;type 'int'&gt;, {'JJ': 2, 'NN': 1, 'POS': 1, '.': 1, 'RB': 1, 'VBZ': 1, 'DT': 2, 'NNP': 1})
</code></pre>
",3,0,1078,2015-09-06 10:36:34,https://stackoverflow.com/questions/32422626/part-of-speech-tagging-and-entity-recognition-python
NER interfere with REGEXNER,"<p>I use regexner to find named entities that are not in the default set of Stanford NLP and it works fine. However, when I add ner annotator, it annotates tokens that match my regular expression with default tags. How can I overwrite default annotations?</p>

<pre><code>def createNLPPipelineRegex(): StanfordCoreNLP = {
     val props = new Properties()
     props.put(""regexner.mapping"", ""regex.txt"")
     props.put(""annotators"", ""tokenize, ssplit, regexner, pos, lemma, ner"")
     props.put(""tokenize.options"", ""untokenizable=noneKeep,normalizeParentheses=false"")
     new StanfordCoreNLP(props)
</code></pre>

<p>}</p>
","scala, stanford-nlp, named-entity-recognition","<p>If you add regexner after the ner annotator it should work:</p>

<pre><code>props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner"")
</code></pre>
",2,2,396,2015-09-18 00:23:57,https://stackoverflow.com/questions/32642008/ner-interfere-with-regexner
Scores for tagged NER results in the StandfordCore NLP.net library,"<p>I'm using the Sandford CoreNLP.NET module and its CRFClassifier to find Named Entities in a document. I am able to get the entities by using classifyWithInlineXML, but does anyone know how to get the entities along with their relevance/confidence scores (0-1)? </p>

<p>Would love an example in C# on how to do this.</p>
","stanford-nlp, named-entity-recognition","<p>Looks like a duplicate of <a href=""https://stackoverflow.com/questions/26612999/display-stanford-ner-confidence-score"">Display Stanford NER confidence score</a> question.</p>

<p>All you need is to rewrite provided sample in C#.</p>
",1,0,626,2015-09-26 10:10:46,https://stackoverflow.com/questions/32796011/scores-for-tagged-ner-results-in-the-standfordcore-nlp-net-library
US state resolution from unstructured text,"<p>I have a database with a ""location"" field that contains unconstrained user input in the form of a string.  I would like to map each entry to either a US state or NULL.</p>

<p>For example:</p>

<pre><code>'Southeastern Massachusetts' -&gt; MA
'Brookhaven, NY' -&gt; NY
'Manitowoc' -&gt; WI
'Blue Springs, MO' -&gt; MO
'A Damp &amp; Cold Corner Of The World.' -&gt; NULL
'Baltimore, Maryland' -&gt; MD
'Indiana' -&gt; IN
</code></pre>

<p>I can tolerate some errors but fewer would obviously be better. What's is the best way to go about this?</p>
","nlp, pattern-recognition, named-entity-recognition","<p>For posterity: I just threw a bunch of regexps at it, which worked 'pretty alright'.</p>
",0,0,33,2015-10-09 19:44:47,https://stackoverflow.com/questions/33046008/us-state-resolution-from-unstructured-text
issue recognizing NEs with StanfordNER in python NLTK,"<p>This happens when there is a potential NE followed by a comma, for example if my strings are something like,</p>

<blockquote>
  <p>""These names  Praveen Kumar,,  David Harrison,  Paul Harrison, blah ""</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>""California, United States""</p>
</blockquote>

<p>my output is something as follows, respectively.</p>

<blockquote>
  <p>[[(u'These', u'O'), (u'names', u'O'), (u'Praveen', u'O'), (u'Kumar,,', u'O'), (u'David', u'PERSON'), (u'Harrison,', u'O'), (u'Paul', u'PERSON'), (u'Harrison,', u'O'), (u'blah', u'O')]]</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>[[(u'California,', u'O'), (u'United', u'LOCATION'), (u'States', u'LOCATION')]] </p>
</blockquote>

<p>why it doesn't recognize potential NEs such as ""Praveen Kumar"", ""Harrison"" and ""California""? </p>

<p>Here is how is use it in the code:</p>

<pre><code>from nltk.tag.stanford import NERTagger
st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')

tags = st.tag(""California, United States"".split())
</code></pre>

<p>Is it because I tokenize the input stirng with <code>split()</code> ? How can I resolve this as it's working fine when tried in Java?</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>Since you are doing this through the nltk, use its tokenizers to split your input:</p>

<pre><code>alltext = myfile.read()
tokenized_text = nltk.word_tokenize(alltext)
</code></pre>

<p><strong>Edit:</strong> You're probably better off with the stanford toolkit's own tokenizer, as recommended by the other answer.  So if you'll be feeding the tokens to one of the Stanford tools, tokenize your text like this to get exactly the tokenization that the tools expect:</p>

<pre><code>from nltk.tokenize.stanford import StanfordTokenizer
tokenize = StanfordTokenizer().tokenize

alltext = myfile.read()
tokenized_text = tokenize(alltext)
</code></pre>

<p>To use this method you'll need to have the Stanford tools installed, and the nltk must be able to find them. I assume you have already taken care of this, since you're using the Stanford NER tool.</p>
",2,1,156,2015-11-09 06:26:03,https://stackoverflow.com/questions/33603534/issue-recognizing-nes-with-stanfordner-in-python-nltk
Named entities: guidelines that pertain to titles of persons,"<p>I'm working on an annotation task of named entities in a text corpus. I found guidelines in the document <a href=""http://www.dessem.com/sites/default/files/ne99_taskdef_v1_4.pdf"" rel=""nofollow"">1999 Named Entity Recognition Task Definition</a>. In that document, there are guidelines that pertain to titles of persons, in particular the following one: <em>Titles such as “Mr.” and role names such as “President” are not considered part of a person name.</em> For example, in “Mr. Harry Schearer” or “President Harry Schearer”, only Harry Schearer should be tagged as person.</p>

<p>In the Stanford NER though, there are many examples of including titles in the person tag (Captain Weston, Mr. Perry, etc). See <a href=""http://nlp.stanford.edu/software/ner-example/austen.gaz.txt"" rel=""nofollow"">here</a> an example of gazette that they give. In their view of person tags, it seems that even “Mrs. and Miss Bates” should be tagged as a person.</p>

<p>Question: what is the most generally accepted guideline?</p>
","stanford-nlp, named-entity-recognition","<p>If you download Stanford CoreNLP 3.5.2 from here: <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>

<p>and run this command:</p>

<pre><code>java -Xmx6g -cp ""*:."" edu.stanford.nlp.pipeline.StanfordCoreNLP -ssplit.eolonly -annotators tokenize,ssplit,pos,lemma,ner -file ner_examples.txt -outputFormat text
</code></pre>

<p>(assuming you put some sample sentences, one sentence per line in ner_examples.txt)</p>

<p>the tagged tokens will be shown in: ner_examples.txt.out</p>

<p>You can try out some sentences and see how our current NER system handles different situations.  This system is trained on data that does not have titles tagged as PERSON, so our current system in general does not tag the titles as PERSON.</p>
",1,0,534,2015-11-10 15:43:13,https://stackoverflow.com/questions/33633874/named-entities-guidelines-that-pertain-to-titles-of-persons
how to speed up NE recognition with stanford NER with python nltk,"<p>First I tokenize the file content into sentences and then call Stanford NER on each of the sentences. But this process is really slow. I know if I call it on the whole file content if would be faster, but I'm calling it on each sentence as I want to index each sentence before and after NE recognition. </p>

<pre><code>st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
for filename in filelist:
    sentences = sent_tokenize(filecontent) #break file content into sentences
    for j,sent in enumerate(sentences): 
        words = word_tokenize(sent) #tokenize sentences into words
        ne_tags = st.tag(words) #get tagged NEs from Stanford NER
</code></pre>

<p>This is probably due to calling <code>st.tag()</code> for each sentence, but is there any way to make it run faster?</p>

<p><strong>EDIT</strong></p>

<p>The reason that I want to tag sentences separate is that I want to write sentences to a file (like sentence indexing) so that given the ne tagged sentence at a later stage, i can get the unprocessed sentence (i'm also doing lemmatizing here)</p>

<p>file format:</p>

<blockquote>
  <p>(sent_number, orig_sentence, NE_and_lemmatized_sentence)</p>
</blockquote>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>From <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L139"" rel=""noreferrer"">StanfordNERTagger</a>, there is the <code>tag_sents()</code> function, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L68"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L68</a></p>

<pre><code>&gt;&gt;&gt; st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
&gt;&gt;&gt; tokenized_sents = [[word_tokenize(sent) for sent in sent_tokenize(filecontent)] for filename in filelist]
&gt;&gt;&gt; st.tag_sents(tokenized_sents)
</code></pre>
",8,10,5955,2015-11-17 03:17:44,https://stackoverflow.com/questions/33748554/how-to-speed-up-ne-recognition-with-stanford-ner-with-python-nltk
"How can Stanford CoreNLP Named Entity Recognition capture measurements like 5 inches, 5&quot;, 5 in., 5 in","<p>I'm looking to capture measurements using <a href=""http://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""noreferrer"">Stanford CoreNLP</a>. (If you can suggest a different extractor, that is fine too.)</p>

<p>For example, I want to find <strong>15kg</strong>, <strong>15 kg</strong>, <strong>15.0 kg</strong>, <strong>15 kilogram</strong>, <strong>15 lbs</strong>, <strong>15 pounds</strong>, etc. But among CoreNLPs extraction rules, I don't see one for measurements.</p>

<p>Of course, I can do this with pure regexes, but toolkits can run more quickly, and they offer the opportunity to chunk at a higher level, e.g.  to treat <strong>gb</strong> and <strong>gigabytes</strong> together, and <strong>RAM</strong> and <strong>memory</strong> as building blocks--even without full syntactic parsing--as they build  bigger units  like <strong>128 gb RAM</strong> and <strong>8 gigabytes memory</strong>.</p>

<p>I want an  extractor for this that is  rule-based, not machine-learning-based), but don't see one as part of <a href=""http://nlp.stanford.edu/software/regexner/"" rel=""noreferrer"">RegexNer</a> or elsewhere. How do I go about this?</p>

<p><a href=""https://www-01.ibm.com/support/knowledgecenter/SSPT3X_2.1.1/com.ibm.swg.im.infosphere.biginsights.text.doc/doc/ana_txtan_NamedEntities.html"" rel=""noreferrer"">IBM Named Entity Extraction</a> can do this. The regexes are run in an efficient way rather than passing the text through each one. And the regexes are bundled to express meaningful entities, as for example one that unites all the measurement units into a single concept.</p>
","nlp, stanford-nlp, named-entity-recognition, named-entity-extraction","<p>I don't think a rule-based system exists for this particular task. However, it shouldn't be hard to make with TokensregexNER. For example, a mapping like:</p>

<pre><code>[{ner:NUMBER}]+ /(k|m|g|t)b/ memory?   MEMORY
[{ner:NUMBER}]+ /""|''|in(ches)?/       LENGTH
...
</code></pre>

<p>You could try using vanilla TokensRegex as well, and then just extract out the relevant value with a capture group:</p>

<pre><code>(?$group_name [{ner:NUMBER}]+) /(k|m|g|t)b/ memory?
</code></pre>
",6,7,1433,2015-12-13 14:30:17,https://stackoverflow.com/questions/34252170/how-can-stanford-corenlp-named-entity-recognition-capture-measurements-like-5-in
Is DBPedia Spotlight still available?,"<p>I was playing around with <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow"">NER</a> of some texts, and came along <a href=""https://github.com/dbpedia-spotlight/dbpedia-spotlight"" rel=""nofollow"">DBPedia Spotlight</a>.  </p>

<p>However,</p>

<ul>
<li>the website: <a href=""http://spotlight.dbpedia.org/"" rel=""nofollow"">http://spotlight.dbpedia.org/</a> is not accessable</li>
<li>the 2 demo: <a href=""http://dbpedia-spotlight.github.io/demo/"" rel=""nofollow"">http://dbpedia-spotlight.github.io/demo/</a> and <a href=""http://spotlight.dbpedia.org/demo/"" rel=""nofollow"">http://spotlight.dbpedia.org/demo/</a> seem not to work</li>
<li>the provided example url: <a href=""http://spotlight.dbpedia.org/rest/spot/?text=Berlin&amp;spotter=LingPipeSpotter%E2%80%99"" rel=""nofollow"">http://spotlight.dbpedia.org/rest/spot/?text=Berlin&amp;spotter=LingPipeSpotter%E2%80%99</a> doesn't work either</li>
</ul>

<p>So is this service still available?<br>
Are there any alternatives (I want to access it from PHP)?</p>
","semantic-web, dbpedia, named-entity-recognition, part-of-speech, spotlight-dbpedia","<p>I just learned that this machine is down. You can use the one under <a href=""http://spotlight.sztaki.hu:2222/rest"" rel=""nofollow"">http://spotlight.sztaki.hu:2222/rest</a></p>

<p>You can curl it like this:</p>

<p>curl ""<a href=""http://spotlight.sztaki.hu:2222/rest/annotate?text=Michelle%20Obama%20called%20Thursday%20on%20Congress%20to%20extend%20a%20tax%20break%20for%20students%20included%20in%20last%20year%27s%20economic%20stimulus%20package,%20arguing%20that%20the%20policy%20provides%20more%20generous%20assistance.&amp;confidence=0.2&amp;support=20"" rel=""nofollow"">http://spotlight.sztaki.hu:2222/rest/annotate?text=Michelle%20Obama%20called%20Thursday%20on%20Congress%20to%20extend%20a%20tax%20break%20for%20students%20included%20in%20last%20year%27s%20economic%20stimulus%20package,%20arguing%20that%20the%20policy%20provides%20more%20generous%20assistance.&amp;confidence=0.2&amp;support=20</a>"" -H ""Accept:application/json"" </p>
",0,1,547,2015-12-22 10:26:04,https://stackoverflow.com/questions/34413586/is-dbpedia-spotlight-still-available
Python PyNER library doesn&#39;t give any output,"<p>I want to use pyNER library in order to extract names from a sentence.</p>

<p>I have installed ner on my ubuntu machine, then I have written the following script for the test.</p>

<pre><code>&gt;&gt;&gt; import ner
&gt;&gt;&gt; tagger = ner.HttpNER(host='localhost', port=80)
&gt;&gt;&gt; tagger.json_entities(""Alice went to the Museum of Natural History."")
</code></pre>

<p>Normally, I have to get this output:</p>

<pre><code>'{""ORGANIZATION"": [""Museum of Natural History""], ""PERSON"": [""Alice""]}'
</code></pre>

<p>But I get nothing:</p>

<pre><code>{}
</code></pre>

<p>How can I fix this issue?</p>

<p>Thanks,</p>
","python, named-entity-recognition","<p>Looks like there is an issue (<a href=""https://github.com/dat/pyner/issues/2"" rel=""nofollow"">https://github.com/dat/pyner/issues/2</a>)</p>

<p>To get it working, you have to specify the output format (slashTags): </p>

<pre><code>tagger = ner.SocketNER(host='localhost',port=80, output_format='slashTags')
</code></pre>

<p>Also, I would consider using another port besides 80 as that is usually reserved for web traffic.</p>

<p>Also if that doesn't work, use <strong>SocketNER</strong> instead of <strong>HttpNER</strong> and follow the instructions per NER's FAQ</p>

<p><a href=""http://nlp.stanford.edu/software/crf-faq.shtml#cc"" rel=""nofollow"">http://nlp.stanford.edu/software/crf-faq.shtml#cc</a></p>

<pre><code>cp stanford-ner.jar stanford-ner-with-classifier.jar 
jar -uf stanford-ner-with-classifier.jar classifiers/english.all.3class.distsim.crf.ser.gz 
java -mx500m -cp stanford-ner-with-classifier.jar edu.stanford.nlp.ie.NERServer -port 9191 -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz &amp;
</code></pre>

<p>And then in your python script</p>

<pre><code>import ner
tagger = ner.SocketNER(host='localhost',port=9191, output_format='slashTags')
print tagger.get_entities(""University of California is located in California, United States"")
</code></pre>
",1,1,431,2015-12-23 10:03:24,https://stackoverflow.com/questions/34433067/python-pyner-library-doesnt-give-any-output
What does NER model to find person names inside a resume/CV?,"<p>i just have started with Stanford CoreNLP, I would like to build a custom NER model to find <strong>persons</strong>.</p>

<p>Unfortunately, I did not find a good ner model for italian. I need to find these entities inside a resume/CV document.</p>

<p>The problem here is that document like those can have different structure, for example i can have:</p>

<p><strong>CASE 1</strong></p>

<pre><code>- Name: John

- Surname: Travolta

- Last name: Travolta

- Full name: John Travolta

(so many labels that can represent the entity of the person i need to extract)
</code></pre>

<p><strong>CASE 2</strong></p>

<pre><code>My name is John Travolta and I was born ...
</code></pre>

<p>Basically, i can have structured data (with different labels) or a context where i should find these entities.</p>

<p>What is the best approach for this kind of documents? Can a maxent model work in this case?</p>

<hr>

<h2><strong>EDIT @vihari-piratla</strong></h2>

<p>At the moment, i adopt the strategy to find a pattern that has something on the left and something on the right, following this method i have 80/85% to find the entity.</p>

<p>Example:</p>

<pre><code>Name: John
Birthdate: 2000-01-01
</code></pre>

<p>It means that i have ""Name:"" on the left of the pattern and a <strong>\n</strong> on the right (until it finds the <strong>\n</strong>).
I can create a very long list of patterns like those. I thought about patterns because i do not need names inside ""other"" context. </p>

<p>For example, if the user writes other names inside a <strong>job experience</strong> i do not need them. Because i am looking for the personal name, not others. With this method i can reduce false positives because i will look at specific patterns not ""general names"".</p>

<p>A problem with this method is that i have a big list of patterns (1 pattern = 1 regex), so it does not scale so well if i add others.</p>

<p>If i can train a NER model with all those patterns it will be awesome, but i should use tons of documents to train it well.</p>
","nlp, stanford-nlp, named-entity-recognition","<p>The first case could be trivial, and I agree with Ozborn's suggestion.</p>

<p>I would like to make a few suggestions for case-2.<br>
Stanford NLP provides an excellent English name recognizer, but may not be able to find all the person names. OpenNLP also gives a decent performance, but much lesser than Stanford. There are many other entity recognizers available for English. I will focus here on StanfordNLP, here are a few things to consider.</p>

<ol>
<li><p>Gazettes. You can provide the model with a list of names and also customize how the Gazette entries are matched. Stanford also provides a sloppy match option when set, will allow partial matches with the Gazette entries. Partial matches should work well with the person names. </p></li>
<li><p>Stanford recognizes entities constructively. If in a document, a name like ""John Travolta"" is recognized, then it would also get ""Travolta"" in the same document even if it had no prior idea about ""Travolta"". So, append as much information to the document as possible. Add the names recognized in case-1, in a familiar context like ""My name is John Travolta."" if ""John Travolta"" is recognized by the rules employed in case-1. Adding dummy sentences can improve the recall.</p></li>
</ol>

<p>Making a benchmark for training is a very costly and boring process; you should annotate in the order of tens of thousands of sentences for decent test performance. I am sure that even if you have a model trained on annotated training data, the performance won't be any better than when you have the two steps above implemented. </p>

<p><strong>@edit</strong></p>

<p>Since the asker of this question is interested in unsupervised pattern-based approaches, I am expanding my answer to discuss these.</p>

<p>When supervised data is not available, a method called bootstrapped pattern-learning approach is generally used. The algorithm starts with a small set of seed instances of interest (like a list of books) and outputs more instances of the same type.<br>
Refer the following resources for more information<br></p>

<ul>
<li><a href=""http://nlp.stanford.edu/software/patternslearning.shtml"" rel=""nofollow"">SPIED</a> is a software that uses the above-described technique and is available for download and use.</li>
<li><a href=""http://www.cs.stanford.edu/people/sonal/"" rel=""nofollow"">Sonal Gupta</a> received Ph.D. on this topic, her dissertation is available <a href=""http://www.cs.stanford.edu/people/sonal/sonalgupta_dissertation.pdf"" rel=""nofollow"">here</a>.</li>
<li>For a light introduction on this topic, see these <a href=""http://nlp.stanford.edu/pubs/Gupta_Manning_CoNLL14_slides.pdf"" rel=""nofollow"">slides</a>.</li>
</ul>

<p>Thanks</p>
",7,10,4275,2015-12-28 23:54:43,https://stackoverflow.com/questions/34502517/what-does-ner-model-to-find-person-names-inside-a-resume-cv
Result Difference in Stanford NER tagger NLTK (python) vs JAVA,"<p>I am using both python and java to run the Stanford NER tagger but I am seeing the difference in the results.</p>

<p>For example, when I input the sentence ""Involved in all aspects of data modeling using ERwin as the primary software for this."",</p>

<p>JAVA Result:</p>

<pre><code>""ERwin"": ""PERSON""
</code></pre>

<p>Python Result:</p>

<pre><code>In [6]: NERTagger.tag(""Involved in all aspects of data modeling using ERwin as the primary software for this."".split())
Out [6]:[(u'Involved', u'O'),
 (u'in', u'O'),
 (u'all', u'O'),
 (u'aspects', u'O'),
 (u'of', u'O'),
 (u'data', u'O'),
 (u'modeling', u'O'),
 (u'using', u'O'),
 (u'ERwin', u'O'),
 (u'as', u'O'),
 (u'the', u'O'),
 (u'primary', u'O'),
 (u'software', u'O'),
 (u'for', u'O'),
 (u'this.', u'O')]
</code></pre>

<p>Python nltk wrapper can't catch ""ERwin"" as PERSON.</p>

<p>What's interesting here is both Python and Java uses the same trained data (english.all.3class.caseless.distsim.crf.ser.gz) released in 2015-04-20.</p>

<p>My ultimate goal is to make python work in the same way Java does.</p>

<p>I'm looking at StanfordNERTagger in nltk.tag to see if there's anything I can modify. Below is the wrapper code:</p>

<pre><code>class StanfordNERTagger(StanfordTagger):
""""""
A class for Named-Entity Tagging with Stanford Tagger. The input is the paths to:

- a model trained on training data
- (optionally) the path to the stanford tagger jar file. If not specified here,
  then this jar file must be specified in the CLASSPATH envinroment variable.
- (optionally) the encoding of the training data (default: UTF-8)

Example:

    &gt;&gt;&gt; from nltk.tag import StanfordNERTagger
    &gt;&gt;&gt; st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') # doctest: +SKIP
    &gt;&gt;&gt; st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) # doctest: +SKIP
    [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),
     ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),
     ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]
""""""

_SEPARATOR = '/'
_JAR = 'stanford-ner.jar'
_FORMAT = 'slashTags'

def __init__(self, *args, **kwargs):
    super(StanfordNERTagger, self).__init__(*args, **kwargs)

@property
def _cmd(self):
    # Adding -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions tokenizeNLs=false for not using stanford Tokenizer  
    return ['edu.stanford.nlp.ie.crf.CRFClassifier',
            '-loadClassifier', self._stanford_model, '-textFile',
            self._input_file_path, '-outputFormat', self._FORMAT, '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions','\""tokenizeNLs=false\""']

def parse_output(self, text, sentences):
    if self._FORMAT == 'slashTags':
        # Joint together to a big list    
        tagged_sentences = []
        for tagged_sentence in text.strip().split(""\n""):
            for tagged_word in tagged_sentence.strip().split():
                word_tags = tagged_word.strip().split(self._SEPARATOR)
                tagged_sentences.append((''.join(word_tags[:-1]), word_tags[-1]))

        # Separate it according to the input
        result = []
        start = 0 
        for sent in sentences:
            result.append(tagged_sentences[start:start + len(sent)])
            start += len(sent);
        return result 

    raise NotImplementedError
</code></pre>

<p>Or, if it's because of using different Classifier (In java code, it seems to use AbstractSequenceClassifier, on the other hand, python nltk wrapper uses the CRFClassifier.) is there a way that I can use AbstractSequenceClassifier in python wrapper?</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>Try setting <code>maxAdditionalKnownLCWords</code> to 0 in the properties file (or command line) for CoreNLP, and if possible for NLTK as well. This disables an option which allows the NER system to learn from test-time data a little bit, which could cause occasional mildly different results.</p>
",5,6,1070,2016-01-06 05:56:47,https://stackoverflow.com/questions/34626555/result-difference-in-stanford-ner-tagger-nltk-python-vs-java
"Train Stanford NER with big gazette, memory issue","<p>I have previously trained a german classifier using the Stanford NER and a training-file with 450.000 tokens. Because I had almost 20 classes, this took about 8 hours and I had to cut a lot of features short in the prop file.</p>

<p>I now have a gazette-file with 16.000.000 unique tagged tokens. I want to retrain my classifier under use of those tokens, but I keep running into memory issues. The gazette-txt is 386mb and mostly contains two-token objects (first + second name), all unique.</p>

<p>I have reduced the amount of classes to 5, reduced the amount of tokens in the gazette by 4 million and I've removed all the features listed on the Stanford NER FAQ-site from the prop-file but I still run into the out of memory: java heap space error. I have 16gb of ram and start the jvm with -mx15g -Xmx14g.</p>

<p>The error occurs about 5 hours into the process.</p>

<p>My problem is that I don't know how to further reduce the memory usage without arbitrarily deleting entries from the gazette. Does someone have further suggestions on how I could reduce my memory-usage?</p>

<p>My prop-file looks like this:</p>

<pre><code>trainFile = ....tsv
serializeTo = ...ser.gz
map = word=0,answer=1

useWordPairs=false
useNGrams=false
useClassFeature=true
useWord=true
noMidNGrams=true
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
saveFeatureIndexToDisk=true
qnSize=2
printFeatures=true
useObservedSequencesOnly=true

cleanGazette=true
gazette=....txt
</code></pre>

<p>Hopefully this isnt too troublesome. Thank you in advance!</p>
","java, memory, nlp, stanford-nlp, named-entity-recognition","<p>RegexNER could help you with this:</p>

<p><a href=""http://nlp.stanford.edu/static/software/regexner/"" rel=""nofollow"">http://nlp.stanford.edu/static/software/regexner/</a></p>

<p>Some thoughts:</p>

<ol>
<li><p>start with 1,000,000 entries and see how big of a gazetteer you can handle, or if 1,000,000 is too large shrink it down more.</p></li>
<li><p>sort the entries by how frequent they are in a large corpus and eliminate the infrequent ones</p></li>
<li><p>Hopefully a lot of the rarer entries in your gazetteer aren't ambiguous, so you can just use RegexNER and have a rule based layer in your system that automatically tags them as PERSON</p></li>
</ol>
",1,2,865,2016-01-22 06:33:39,https://stackoverflow.com/questions/34940417/train-stanford-ner-with-big-gazette-memory-issue
How are StanfordNER Classifiers built,"<p>I am working with StanfordNER classifiers. There are 4 classifiers as</p>

<pre><code>english.all.3class.distsim.crf.ser.gz
english.muc.7class.distsim.crf.ser.gz
english.conll.4class.distsim.crf.ser.gz
example.serialized.ncc.ncc.ser.gz
</code></pre>

<p>How are these classifiers built? Since each of them is based on a different corpus, here is my guess</p>

<ol>
<li><p>Train a machine learning classifier like <code>SVM</code> coupled with <code>OVR</code> (for multi label case) on the corpus to detect entities like <code>ORGANIZATION</code>,<code>PERSON</code>,<code>LOCATION</code> etc. This means that the training data would be the entire text of a document in the corpus. For that piece of text we explicitly indicate the <code>ORGANIZATION</code>s,<code>PERSON</code>s and <code>LOCATION</code>s. Thus the classifiers would be able to predict those entities.</p></li>
<li><p>Train a machine learning classifier to link POS tags with entities like <code>ORGANIZATION</code>,<code>PERSON</code>,<code>LOCATION</code>. For example, a classifier can be trained to predict which proper nouns should be <code>ORGANIZATION</code></p></li>
</ol>

<p>Is this the correct big picture? I am just trying to work out how to build my own NER.</p>
","machine-learning, nlp, classification, stanford-nlp, named-entity-recognition","<p>Yes, the models are trained on supervised data. They're 1st order CRFs which do multi-class probabilistic sequence classification (so not OVR, not SVM). You can find an introduction to NER and Stanford NER in particular on <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">the Stanford NER page</a>.</p>
",5,3,216,2016-01-22 14:50:20,https://stackoverflow.com/questions/34949472/how-are-stanfordner-classifiers-built
NLP : Is Gazetteer a cheat,"<p>In NLP there is a concept of <code>Gazetteer</code> which can be quite useful for creating annotations. As far as i understand, </p>

<p><code>A gazetteer consists of a set of lists containing names of entities such as cities, organisations, days of the week, etc. These lists are used to ﬁnd occurrences of these names in text, e.g. for the task of named entity recognition.</code></p>

<p>So it is essentially a lookup. Isn't this kind of a cheat? If we use a <code>Gazetteer</code> for detecting named entities, then there is not much <code>Natural Language Processing</code> going on. Ideally, i would want to detect named entities using <code>NLP</code> techniques. Otherwise how is it any better than a regex pattern matcher.</p>

<p>Does that make sense?</p>
","nlp, named-entity-recognition","<p>Depends on how you built/use your gazetteer. If you are presenting experiments in a closed domain and you custom picked your gazetteer, then yes, you are cheating.
If you are using some openly available gazetteer and performing experiments on a large dataset or using it in an application in the wild where you don't control the input then you are fine.
We found ourselves in a similar situation. We partition our dataset and use the training data to automatically build our gazetteers. As long as you report your methodology you should not feel like cheating (let the reviewers complain).</p>
",7,15,6629,2016-01-25 14:35:43,https://stackoverflow.com/questions/34995139/nlp-is-gazetteer-a-cheat
How to clean sentences for StanfordNER,"<p>I want to use <code>StanfordNER</code> in python to detect named entities. How should i clean up the sentences?</p>

<p>for example, consider</p>

<p><code>qry=""In the UK, the class is relatively crowded with Zacc competing with Abc's Popol (market leader) and  Xyz's Abcvd.""</code></p>

<p>if i do</p>

<pre><code>st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
print st.tag(qry.split())
</code></pre>

<p>i get</p>

<pre><code>[
    (u'In', u'O'), (u'the', u'O'), (u'UK,', u'O'), (u'the', u'O'), 
    (u'class', u'O'), (u'is', u'O'), (u'relatively', u'O'), (u'crowded', u'O'), 
    (u'with', u'O'), (u'Zacc', u'PERSON'), (u'competing', u'O'), (u'with', u'O'), 
    (u""Abc's"", u'O'), (u'Popol', u'O'), (u'(market', u'O'), (u'leader)', u'O'), 
    (u'and', u'O'), (u""Xyz's"", u'O'), (u'Abcvd.', u'O')
]
</code></pre>

<p>`</p>

<p>so only 1 named entities was detected. However, if i do some cleanup by replacing all special characters with spaces</p>

<p><code>qry=""In the UK the class is relatively crowded with Zacc competing with Abc s Popol  market leader and  Xyz s Abcvd""</code></p>

<p>i get</p>

<pre><code>[
    (u'In', u'O'), (u'the', u'O'), (u'UK', u'LOCATION'), (u'the', u'O'), 
    (u'class', u'O'), (u'is', u'O'), (u'relatively', u'O'), (u'crowded', u'O'), 
    (u'with', u'O'), (u'Zacc', u'PERSON'), (u'competing', u'O'), (u'with', u'O'), 
    (u'Abc', u'ORGANIZATION'), (u's', u'O'), (u'Popol', u'PERSON'), (u'market', u'O'), 
    (u'leader', u'O'), (u'and', u'O'), (u'Xyz', u'ORGANIZATION'), (u's', u'O'), (u'Abcvd', u'PERSON')]
</code></pre>

<p>`</p>

<p>so clearly, this is more appropriate. Are there any general rules on how to clean up sentences for <code>StanfordNER</code>? Initially i thought that there is no cleanup required at all!</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>You can use Stanford Tokenizer for your purpose. 
You could use the code below.</p>

<pre><code>from nltk.tokenize.stanford import StanfordTokenizer
token = StanfordTokenizer('stanford-ner-2014-06-16/stanford-ner.jar')
qry=""In the UK, the class is relatively crowded with Zacc competing with Abc's Popol (market leader) and  Xyz's Abcvd.""
tok = token.tokenize(qry)
print tok
</code></pre>

<p>You will get the tokens as you require them.</p>

<blockquote>
  <p>[u'In',<br>
   u'the',<br>
   u'UK',<br>
   u',',<br>
   u'the',<br>
   u'class',<br>
   u'is',<br>
   u'relatively',<br>
   u'crowded',<br>
   u'with',<br>
   u'Zacc',<br>
   u'competing',<br>
   u'with',<br>
   u'Abc',<br>
   u""'s"",<br>
   u'Popol',<br>
   u'-LRB-',<br>
   u'market',<br>
   u'leader',<br>
   u'-RRB-',<br>
   u'and',<br>
   u'Xyz',<br>
   u""'s"",<br>
   u'Abcvd',<br>
   u'.'] </p>
</blockquote>
",4,0,565,2016-01-26 15:07:49,https://stackoverflow.com/questions/35017041/how-to-clean-sentences-for-stanfordner
error loading NER .bin file as model argument for openNLP::Maxent_Entity_Annotator(),"<p>I created a model using Apache OpenNLP's command line tool to recognize named entities.  The below code created the model using the file <code>sentences4OpenNLP.txt</code> as a training set.</p>

<pre><code>opennlp TokenNameFinderTrainer -type maxent -model C:\Users\Documents\en-ner-org.bin -lang en -data C:\Users\Documents\apache-opennlp-1.6.0\sentences4OpenNLP.txt -encoding UTF-8
</code></pre>

<p>I tested the model from the command line by passing it sentences to tag, and the model seemed to be working well.  However, I am unable to successfully use the model from R.  I am using the below lines in attempts to create an organization annotating function.  Using the same code to load a model downloaded from OpenNLP works fine.</p>

<pre><code>modelNER &lt;- ""C:/Users/Documents/en-ner-org.bin""
oa &lt;- openNLP::Maxent_Entity_Annotator(language = ""en"",
                                  kind = ""organization"",
                                  probs = TRUE,
                                  model = modelNER)
</code></pre>

<p>When the above code is run I get an error saying:</p>

<pre><code>Could not instantiate the opennlp.tools.namefind.TokenNameFinderFactory. The initialization throw an exception.
opennlp.tools.util.ext.ExtensionNotLoadedException: Unable to find implementation for opennlp.tools.util.BaseToolFactory, the class or service opennlp.tools.namefind.TokenNameFinderFactory could not be located!
     at opennlp.tools.util.ext.ExtensionLoader.instantiateExtension(ExtensionLoader.java:97)
     at opennlp.tools.util.BaseToolFactory.create(BaseToolFactory.java:106)
     at opennlp.tools.util.model.BaseModel.initializeFactory(BaseModel.java:254)
Error in .jnew(""opennlp.tools.namefind.TokenNameFinderModel"", .jcast(.jnew(""java.io.FileInputStream"",  : 
java.lang.IllegalArgumentException: opennlp.tools.util.InvalidFormatException: Could not instantiate the opennlp.tools.namefind.TokenNameFinderFactory. The initialization throw an exception.
     at opennlp.tools.util.model.BaseModel.loadModel(BaseModel.java:237)
     at opennlp.tools.util.model.BaseModel.&lt;init&gt;(BaseModel.java:181)
     at opennlp.tools.namefind.TokenNameFinderModel.&lt;init&gt;(TokenNameFinderModel.java:110)
</code></pre>

<p>Any advice on how to fix the error would be a big help.  Thanks in advance.</p>
","r, apache, opennlp, named-entity-recognition","<p>Resolved the error.  The R function <code>openNLP::Maxent_Entity_Annotator</code> was not compatible with the named entity recognition (NER) model being produced by OpenNLP 1.6.0.  Building the NER model using OpenNLP 1.5.3 resulted in <code>openNLP::Maxent_Entity_Annotator</code> running without error.</p>
",0,0,743,2016-02-23 21:27:21,https://stackoverflow.com/questions/35588529/error-loading-ner-bin-file-as-model-argument-for-opennlpmaxent-entity-annotat
Python: Encoding characters but still work with the list,"<p>So for a text mining assignment we try to collect tweets (especially the texts) and run the <strong>stanford NER tagger</strong> to find out if there are persons or locations mentioned. This could also be done by checking the hashtags, but the idea is to use some text mining tools.</p>

<p>so let's say that we have data loaded from a cPickle file which is saved, loaded and split on white space.</p>

<pre><code>hil_text = [[u'Man', u'is', u'not', u'a', u'issue', u'cah', u'me', u'pum', u'pum', u'tun', u'up', u'#InternationalWomensDay', u'#cham', u'#empowerment', u'#Clinton2016', u'#PiDay2016'], [u'Shonda', u'Land', u'came', u'out', u'with', u'a', u'great', u'ad', u'for', u'Clinton:https://t.co/Vfg9lAKNaH#Clinton2016'], [u'RT', u'@BeaverforBernie:', u'Trump', u'and', u'the', u""#Clinton's"", u'are', u'the', u'same.', u'They', u'worship', u'$$$$$.', u'https://t.co/yUXoJaL6mJ'], [u'.@GloriaLaRiva', u'on', u'#Clinton,', u'Reagans', u'&amp;amp;', u'#AIDS:', u'\u201cClinton', u'just', u're-wrote', u'history\u201d', u'https://t.co/L3YuIyFjxo', u'Clinton', u'incapable', u'of', u'telling', u'truth.'], [u'#KKK', u'Leader', u'Gets', u'Behind', u'This', u'Democratic', u'Candidate', u'https://t.co/p9yTQ2sXmV', u'How', u'fitting!', u'#Hillary2016', u'#HillaryClinton', u'#Hillary', u'#Killary', u'#tcot'], [u'#KKK', u'Leader', u'Gets', u'Behind', u'This', u'Democratic', u'Candidate', u'https://t.co/p9yTQ2sXmV', u'How', u'fitting!', u'#Hillary2016', u'#HillaryClinton', u'#Hillary', u'#Killary', u'#tcot'], [u'RT', u'@jvlibrarylady:', u'President', u'Clinton', u'at', u'rally', u'for', u'Hillary', u'at', u'Teamsters', u'Local', u'245', u'in', u'Springfield,', u'Mo.', u'#HillaryClintonForPresident', u'https://t.\u2026'], [u'RT', u'@jvlibrarylady:', u'President', u'Clinton', u'at', u'rally', u'for', u'Hillary', u'at', u'Teamsters', u'Local', u'245', u'in', u'Springfield,', u'Mo.', u'#HillaryClintonForPresident', u'https://t.\u2026']]
</code></pre>

<p>The tagger doesn't accept the unicode, so in trying to get it to work we tried to do the following.</p>

<pre><code>for word in hil_text:
    for x in word:
        print x.encode('utf-8',errors='ignore')
        print tagger.tag(x.encode('utf-8',errors='ignore')
</code></pre>

<p>This results in x being the word printed, but the tagger tagging each letter separately.</p>

<p>Is there a way to encode it and send it through the tagger as a word? Or in other words to encode parts of a list but still keep that part in a list?</p>

<p>And why does the tagger tag each letter and not just the whole x?</p>
","python, unicode, text-mining, named-entity-recognition","<p>It looks like <code>tagger.tag</code> is expecting a <em>sequence</em> of strings. But you are passing in a <em>single string</em>, which python will treats as <em>sequence of characters</em>. To fix that, try this:</p>

<pre><code>for section in hil_text:
    # encode each word in the section, and put them in a new list
    words = [word.encode('utf-8') for word in section]
    # pass the list of encoded words to the tagger
    print tagger.tag(words)
</code></pre>
",0,-2,43,2016-03-14 22:00:14,https://stackoverflow.com/questions/35998911/python-encoding-characters-but-still-work-with-the-list
Named Entity Recognition (Ner) - Organization Name Database,"<p>I'm Working on my current graduating project which is Named Entity Recognition For Turkish. The recognizer should catch Turkish words when i work with Person Names and Locations (Sometimes locations can be in different Languages, for example Hilton Hotels in Taksim/Istanbul) all i need add ""Hotel"" in my dataset which is Full of specific location tags like Hotel , Restaurant or Mall. But when its come to Organization Name Tag. I need to find a good dataset of bands , products , company names, But cant figure out how to find or collect this dataset</p>

<p>In stanford nlp tool : <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/ner/process</a></p>

<p>When i type Facebook , Nike , Adidas etc it can find it's organization. So is there any way to have that organization name Dataset ? </p>
","nlp, nltk, stanford-nlp, opennlp, named-entity-recognition","<p>If you are interested in a data resource with these organizations names. You can use one of the knowledge bases KBs available such as </p>

<ul>
<li><a href=""http://wiki.dbpedia.org/OnlineAccess"" rel=""nofollow"">DBpedia</a></li>
<li><a href=""https://www.mpi-inf.mpg.de/de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/"" rel=""nofollow"">YAGO</a></li>
<li><a href=""http://babelnet.org/guide"" rel=""nofollow"">BabelNet</a> It cannot be downloaded only online access.</li>
<li><a href=""https://www.freebase.com/"" rel=""nofollow"">FreeBase</a></li>
</ul>

<p>All of them have names of these organizations and more, you will need some effort to extract the organizations only using their types. For example, <strong>YAGO</strong> has downloadable file with possible entities and their types. You can filter it on  and then you can use hasMeaning data to get all possible names.</p>

<p>Yago and BabelNet have been used to NER or Named Entity Disambiguation system AIDA and Babelfy.</p>

<p><a href=""https://github.com/yago-naga/aida"" rel=""nofollow"">AIDA</a> offers a robust dataset of possible Entity Names, that can be used for NER. </p>
",4,1,5361,2016-04-01 08:10:46,https://stackoverflow.com/questions/36351251/named-entity-recognition-ner-organization-name-database
Lowest value and its associated key in JavaScript,"<p>I am trying identify artists from twitter. So I have a tweet and I am using the <a href=""https://github.com/NaturalNode/natural"" rel=""nofollow"" title=""natural"">natural for node</a> to tokenise the tweet and compare it to an array of artists using Levenshtein distance to match the token with the artists. My problem is that I am having difficulty in the logic of actually comparing each token to the list of artists and matching the one the tweet is referring to.</p>

<p>The following example should get Clean Bandit as the artist.</p>

<pre><code>var saturday = [""Kanye West"", ""Pharrell Williams"", ""Paloma Faith"", ""Burt Bacharach"", ""Clean Bandit""];

var tweet = ""My queen @graciechatto about to go on The Other Stage at Glastonbury #cleanbandit #glastonbury…""

tokenizer = new natural.WordTokenizer(); //new tokeniser

var tweetTokenised = tokenizer.tokenize(tweet); //tokenise the tweet and store it in tweetTokenised

var i , j;

//loop through tokenised tweet    
for(i=0;i&lt;tweetTokenised.length;i++){
    console.log(tweetTokenised[i] + ""--------------------------"");
    var temp = [];

    //compare token with list of artists performing on saturday    
    for(j=0;j&lt;saturday.length;j++){

        //remove whitespace from the tweet tokens
        console.log(tweetTokenised[i]+ ""---&gt;""+saturday[j]); //testing purposes
        var score = natural.LevenshteinDistance(tweetTokenised[i].replace(/\s+/g, '').toLowerCase(),saturday[j].toLowerCase());

        //store score for each token vs artists in a temp dictionary 
        temp.push({
            key:   saturday[j],
            value: score
        });
    }
}
</code></pre>
","javascript, arrays, node.js, twitter, named-entity-recognition","<pre><code>        //sort array from lowest to biggest

        temp.sort(function(a, b) {

            return parseFloat(a.value) - parseFloat(b.value);

        });





        //console.log(temp);

        //get the first object (the smallest in this instance as its been sorted)

        lowest = temp.shift();

        console.log(lowest);

        if(lowest.value &lt; 2){

            distances.push(lowest);

        }

    }

    console.log(""printing distances"");

    console.log(distances[0].key); //get artist name 

}
</code></pre>
",0,0,53,2016-04-01 19:40:42,https://stackoverflow.com/questions/36364596/lowest-value-and-its-associated-key-in-javascript
Named Entity Recognition from personal dictionary in python,"<p>I have a large database with a lot of entries (most of them movies) which has only description as information. A description of the entry with ID 1 (for example) may be like:</p>

<blockquote>
  <p>'Forrest Gump is a 1994 American epic romantic-comedy-drama film based
  on the 1986 novel of the same name by Winston Groom. The film was
  directed by Robert Zemeckis and stars Tom Hanks, Robin Wright, Gary
  Sinise, Mykelti Williamson, and Sally Field.'</p>
</blockquote>

<p>Now I have also some txt documents that are basically dictionaries, and are structured like this:</p>

<pre><code>actors.txt

Mickey Mouse
Tom Hanks
...

directors.txt

Donald Duck
Robert Zemeckis
...
</code></pre>

<p>What I want to do is to analyse the description of every entry and parse named entities from my dictionary. So if the text contain 'Tom Hanks' I want to recognize that the entry with ID 1 has Tom Hanks as actor and so on. An output should be something like that:</p>

<pre><code>Actor: Tom Hanks, Actor: Robin Wright, Director: Robert Zemeckis, Distributor: Paramount Pictures.
</code></pre>

<p>or whatever format easy to manipulate.</p>
","python, parsing, dictionary, named-entity-recognition","<p>All you got to do is use SOLR, setup a few new fieldtypes(like text_actors) in its schema which are linked to appropriate dictionaries, write the appropriate schema, and then import the database. From what I know, this can help you develop a searchable database from which you can query all the results and populate your own database.</p>
",1,1,628,2016-04-06 15:16:41,https://stackoverflow.com/questions/36455354/named-entity-recognition-from-personal-dictionary-in-python
Can NER Stanford called in java file?,"<p>I've tried to give NER and POS tag from texts with <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">Stanford Named Entity Recognizer (NER)</a> and <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford Log-linear Part-Of-Speech Tagger</a>.</p>

<p>Let me give you an example of POStag</p>

<pre><code>import edu.stanford.nlp.tagger.maxent.MaxentTagger;

public class POSTag {
    static MaxentTagger tagger =  new MaxentTagger(""english_language_pos.tagger"");

    public static void main(String[] args) throws IOException{
        String text = ""Sarah and I go to Samsung office in Korea.""
        String tagged = tagger.tagString(text);

        System.out.println(tagged);
    }
}
</code></pre>

<p>The output: <br></p>

<pre><code>Sarah_NNP and_CC I_PRP go_VBP to_TO Samsung_NNP office_NN in_IN Korea_NNP ._.  
</code></pre>

<p>And for NER Stanford, I have to run in command line:</p>

<pre><code>java -mx500m -cp stanford-ner-3.5.2.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier indonesian_language_ner.gz -textFile E:/file.txt &gt; E:/file_ner.txt
</code></pre>

<p>My question is how can I call Stanford NER in file with java extension(file.java)? Is there any way to call Stanford NER besides from command line?
Or any other tool or library that I can use to give NER that called from java file?</p>

<p>*****edit:<br>
I need the output of NER become like this:</p>

<pre><code>Sarah_PERSON and_O I_O go_O to_O Samsung_ORGANIZATION office_O in_O Korea_PLACE ._O 
</code></pre>

<p>*****edit</p>

<p>**fyi: I use Windows 8.1</p>
","java, stanford-nlp, named-entity-recognition, pos-tagger","<p>You could do something like this:</p>

<pre><code>    String model = ""english.all.3class.distsim.crf.ser.gz"";
    String text = ""file.txt"";

    AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier.getClassifier(model);
    classifier.classifyAndWriteAnswers(text);
</code></pre>

<p>I hope it helps.</p>
",1,0,1746,2016-04-13 09:36:59,https://stackoverflow.com/questions/36594369/can-ner-stanford-called-in-java-file
Unable to use Stanford NER in python module,"<p>I want to use Python Stanford NER module but keep getting an error,I searched it on internet but got nothing. Here is the basic usage with error.</p>

<pre><code>import ner
tagger = ner.HttpNER(host='localhost', port=8080)
tagger.get_entities(""University of California is located in California,   

United States"")
</code></pre>

<p>Error</p>

<pre><code>Traceback (most recent call last):
File ""&lt;pyshell#3&gt;"", line 1, in &lt;module&gt;
tagger.get_entities(""University of California is located in California, United States"")
File ""C:\Python27\lib\site-packages\ner\client.py"", line 81, in get_entities
tagged_text = self.tag_text(text)
File ""C:\Python27\lib\site-packages\ner\client.py"", line 165, in tag_text
c.request('POST', self.location, params, headers)
File ""C:\Python27\lib\httplib.py"", line 1057, in request
self._send_request(method, url, body, headers)
File ""C:\Python27\lib\httplib.py"", line 1097, in _send_request
self.endheaders(body)
File ""C:\Python27\lib\httplib.py"", line 1053, in endheaders
self._send_output(message_body)
File ""C:\Python27\lib\httplib.py"", line 897, in _send_output
self.send(msg)
File ""C:\Python27\lib\httplib.py"", line 859, in send
self.connect()
File ""C:\Python27\lib\httplib.py"", line 836, in connect
self.timeout, self.source_address)
File ""C:\Python27\lib\socket.py"", line 575, in create_connection
raise err
error: [Errno 10061] No connection could be made because the target machine actively refused it
</code></pre>

<p>Using windows 10 with latest Java installed</p>
","python, python-2.7, nlp, stanford-nlp, named-entity-recognition","<ul>
<li>The Python Stanford NER module is a wrapper for the Stanford NER that
allows you to run python commands to use the NER service.     </li>
<li>The NER
service is a separate entity to the Python module. It is a Java
program. To access this service, via python, or any other way, you
first need to start the service.   </li>
<li>Details on how to start the Java
Program/service can be found here -
<a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/CRF-NER.shtml</a>    </li>
<li><p>The NER comes with
a <code>.bat</code> file for windows and a <code>.sh</code> file for unix/linux. I think
these files start the <code>GUI</code></p></li>
<li><p>To start the service without the <code>GUI</code> you should run a command similar to this:<br>
<code>java -mx600m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz</code><br>
This runs the NER jar, sets the memory, and sets the classifier you want to use. (I think youll have to be in the Stanford NER directory to run this)  </p></li>
<li><p>Once the NER program is running then you will be able to run your python code and query the NER.  </p></li>
</ul>
",1,1,1269,2016-04-16 18:54:59,https://stackoverflow.com/questions/36668340/unable-to-use-stanford-ner-in-python-module
Why does my NamedEntityAnnotator for date mentions differ from CoreNLP demo&#39;s output?,"<p>The date detected from my following program gets split into two separate mentions whereas the detected date in the NER output of <a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">CoreNLP demo</a> is single as it should be. What should I edit in my program to correct this.</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, entitymentions"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

String text =  ""This software was released on Februrary 5, 2015."";
Annotation document = new Annotation(text);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
      List&lt;CoreMap&gt; mentions = sentence.get(MentionsAnnotation.class);
      if (mentions != null) {
              for (CoreMap mention : mentions) {
                     System.out.println(""== Token="" + mention.get(TextAnnotation.class));
                     System.out.println(""NER="" + mention.get(NamedEntityTagAnnotation.class));
                     System.out.println(""Normalized NER="" + mention.get(NormalizedNamedEntityTagAnnotation.class));
              }
       }
}
</code></pre>

<p>Output from this program:</p>

<pre><code>== Token=Februrary 5,
NER=DATE
Normalized NER=****0205
== Token=2015
NER=DATE
Normalized NER=2015  
</code></pre>

<p>Output from CoreNLP online demo:
<a href=""https://i.sstatic.net/50oef.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/50oef.png"" alt=""enter image description here""></a></p>
","nlp, stanford-nlp, named-entity-recognition","<p>Note that the online demo is showing any sequence of consecutive tokens with the same NER tag as belonging to the same unit.  Consider this sentence:</p>

<pre><code>The event happened on February 5th January 9th.
</code></pre>

<p>This example yields ""February 5th January 9th"" as a single DATE in the online demo.</p>

<p>Yet it recognizes ""February 5th"" and ""January 9th"" as separate entity mentions.</p>

<p>Your sample code is looking at mentions, not NER chunks.  Mentions are not being shown by the online demo.</p>

<p>That being said, I am not sure why SUTime is not joining February 5th and 2015 together in your example.  Thanks for bringing this up, I will look into improving the module to fix this issue in future releases.</p>
",2,0,441,2016-04-25 06:25:39,https://stackoverflow.com/questions/36833373/why-does-my-namedentityannotator-for-date-mentions-differ-from-corenlp-demos-ou
How to get Named Entity Extraction using GATE Annie in Java?,"<p>I am newbie in <strong>GATE ANNIE</strong>. I tried <strong>GATE GUI interface</strong> and got experience to do task on it. I wanted to know how can I implement <strong>Named Entity Extraction</strong> in Java? </p>

<p>I did R&amp;D but unable to find any tutorial regarding <strong>Named Entity Extraction</strong>. </p>

<p>Is there any code available to find out <strong>Named Entity Extraction</strong> in <strong>GATE ANNIE</strong> in <strong>Java</strong>?</p>
","java, nlp, named-entity-recognition, gate","<pre><code>import gate.*;
import gate.creole.ANNIEConstants;
import gate.util.persistence.PersistenceManager;
import java.io.File;
import java.util.*;

public class AnnieNerExample {

    public static void main(String[] args) throws Exception {
        Gate.setGateHome(new File(""C:\\Program Files\\GATE_Developer_8.1""));
        Gate.init();

        LanguageAnalyser controller = (LanguageAnalyser) PersistenceManager
                .loadObjectFromFile(new File(new File(Gate.getPluginsHome(),
                        ANNIEConstants.PLUGIN_DIR), ANNIEConstants.DEFAULT_FILE));

        Corpus corpus = Factory.newCorpus(""corpus"");
        Document document = Factory.newDocument(
                ""Michael Jordan is a professor at the University of California, Berkeley."");
        corpus.add(document); controller.setCorpus(corpus); 
        controller.execute();

        document.getAnnotations().get(new HashSet&lt;&gt;(Arrays.asList(""Person"", ""Organization"", ""Location"")))
            .forEach(a -&gt; System.err.format(""%s - \""%s\"" [%d to %d]\n"", 
                    a.getType(), Utils.stringFor(document, a),
                    a.getStartNode().getOffset(), a.getEndNode().getOffset()));

        //Don't forget to release GATE resources 
        Factory.deleteResource(document); Factory.deleteResource(corpus); Factory.deleteResource(controller);
    }
}
</code></pre>

<p>The output:</p>

<pre><code>Person - ""Michael Jordan"" [0 to 14]
Organization - ""University of California"" [37 to 61]
Location - ""Berkeley"" [63 to 71]
</code></pre>

<hr>

<h2>Jars</h2>

<p>two possibilities:</p>

<ol>
<li>Manual</li>
</ol>

<p><a href=""https://gate.ac.uk/sale/tao/splitch7.html#sec:api:embed"" rel=""nofollow noreferrer"" title=""Quick Start with GATE Embedded"">Quick Start with GATE Embedded</a>:</p>

<blockquote>
  <p>add <code>$GATE_HOME/bin/gate.jar</code> and the JAR ﬁles in <code>$GATE_HOME/lib</code> to the Java CLASSPATH (<code>$GATE_HOME</code> is the GATE root directory)</p>
</blockquote>

<ol start=""2"">
<li><p>Maven</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;uk.ac.gate&lt;/groupId&gt;
    &lt;artifactId&gt;gate-core&lt;/artifactId&gt;
    &lt;version&gt;8.4&lt;/version&gt;
&lt;/dependency&gt;
</code></pre></li>
</ol>
",6,4,2833,2016-05-17 10:09:20,https://stackoverflow.com/questions/37273126/how-to-get-named-entity-extraction-using-gate-annie-in-java
Annotated Training data for NER corpus,"<p>It is mentioned in the documentation of opennlp that we've to train our model with 15000 line for a good performance. 
now, I've to extract different entities from the document which means I've to add different tags for many tokens in the training data(15000 lines) which will take a lot of time. Is there any other way to do this? which will reduce the time or any other method which I can proceed.</p>

<p>Thanks.</p>
","nlp, opennlp, corpus, training-data, named-entity-recognition","<p>Here are some tools:</p>

<p>GATE <a href=""http://gate.ac.uk/"" rel=""nofollow"">http://gate.ac.uk/</a></p>

<p>GATE Teamware (web-based) <a href=""http://gate.ac.uk/teamware/"" rel=""nofollow"">http://gate.ac.uk/teamware/</a></p>

<p>XConc Suite <a href=""http://www-tsujii.is.s.u-tokyo.a"" rel=""nofollow"">http://www-tsujii.is.s.u-tokyo.a</a>...</p>

<p>Sapient (sentence-based) <a href=""http://www.aber.ac.uk/en/cs/rese"" rel=""nofollow"">http://www.aber.ac.uk/en/cs/rese</a>...</p>

<p>Knowtator (Protégé plug-in) <a href=""http://knowtator.sourceforge.net/"" rel=""nofollow"">http://knowtator.sourceforge.net/</a></p>

<p>CorpusTool <a href=""http://www.wagsoft.com/CorpusToo"" rel=""nofollow"">http://www.wagsoft.com/CorpusToo</a>...</p>

<p>UIMA CAS Editor <a href=""http://uima.apache.org/"" rel=""nofollow"">http://uima.apache.org/</a></p>

<p>Callisto <a href=""http://callisto.mitre.org/"" rel=""nofollow"">http://callisto.mitre.org/</a></p>

<p>Wordfreak <a href=""http://wordfreak.sourceforge.net/"" rel=""nofollow"">http://wordfreak.sourceforge.net/</a></p>

<p>MMax2 <a href=""http://mmax2.sourceforge.net/"" rel=""nofollow"">http://mmax2.sourceforge.net/</a></p>

<p>reference: <a href=""https://www.quora.com/Natural-Language-Processing-What-are-the-best-tools-for-manually-annotating-a-text-corpus-with-entities-and-relationships"" rel=""nofollow"">https://www.quora.com/Natural-Language-Processing-What-are-the-best-tools-for-manually-annotating-a-text-corpus-with-entities-and-relationships</a></p>
",3,4,3391,2016-05-23 12:48:00,https://stackoverflow.com/questions/37391443/annotated-training-data-for-ner-corpus
Exact Dictionary based Named Entity Recognition with Stanford,"<p>I have a dictionary of named entities, extracted from Wikipedia. I want to use it as the dictionary of an NER. I wanted to know how can I use Stanford-NER with this data of mine.
I have also downloaded Lingpipe, although I have no idea how can I use it. I would appreciate all kinds of information.</p>

<p>Thanks for your helps.</p>
","java, stanford-nlp, named-entity-recognition, named-entity-extraction, lingpipe","<p>You can use dictionary (or regular expression-based) named entity recognition with Stanford CoreNLP. See the <a href=""http://stanfordnlp.github.io/CoreNLP/regexner.html"" rel=""nofollow"">RegexNER annotator</a>. For some applications, we run this with quite large dictionaries of entities. Nevertheless, for us this is typically a secondary tool to using statistical (CRF-based) NER.</p>
",3,3,2492,2016-06-11 11:54:27,https://stackoverflow.com/questions/37763404/exact-dictionary-based-named-entity-recognition-with-stanford
Training NER model in stanford-nlp,"<p>I have been trying to play around with stanford Core NLP. I would wish to train the my own NER model. From the forums on SO and the official website describes to use a property file to do so. How would I do it via API?.</p>

<pre><code>Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, sentiment, regexner"");
props.setProperty(""regexner.mapping"", ""resources/customRegexNER.txt"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);      

String processedQuestion = ""Who is the prime minister of Australia?""

//Annotation annotation = pipeline.process(processedQuestion);
Annotation document = new Annotation(processedQuestion);
pipeline.annotate(document);
List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
for (CoreMap sentence : sentences) {

    // To get the tokens for the parsed sentence
    for (CoreMap tokens : sentence.get(TokensAnnotation.class)) {           
        String token = tokens.get(TextAnnotation.class);
        String POS = tokens.get(PartOfSpeechAnnotation.class);      
        String NER = tokens.get(NamedEntityTagAnnotation.class);            
        String Sentiment = tokens.get(SentimentClass.class);            
        String lemma = tokens.get(LemmaAnnotation.class);
</code></pre>

<ol>
<li>How &amp; Where do I add the Prop file? </li>
<li>N-gram tokenization (E.g. prime minister to be considered as a single token, later this token is passed for the POS, NER instead of two tokens being passed (prime and minister))?</li>
</ol>
","java, stanford-nlp, tokenize, named-entity-recognition","<p>I think it could work with that code :</p>

<pre><code>val props = new Properties()
  props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner"")
  props.put(""ner.model"", ""/your/path/ner-model.ser.gz"");
  val pipeline = new StanfordCoreNLP(props)
</code></pre>
",1,0,786,2016-06-24 12:08:02,https://stackoverflow.com/questions/38013103/training-ner-model-in-stanford-nlp
How do I generate an xml output from standfordner classifier?,"<p>I have used standfordNER classifier to classify text. 
Here is the code.</p>

<pre><code>string docText = fileContent;
        string txt = """";
        var classified = Classifier.classifyToCharacterOffsets(docText).toArray();

        for (int i = 0; i &lt; classified.Length; i++)
        {
            Triple triple = (Triple)classified[i];

            int second = Convert.ToInt32(triple.second().ToString());
            int third = Convert.ToInt32(triple.third().ToString());
            txt = txt + ('\t' + triple.first().ToString() + '\t' + docText.Substring(second, third - second));

            string s = Classifier.classifyWithInlineXML(txt);
            string s1 = Classifier.classifyToString(s, ""xml"", true);
            Panel1.GroupingText = s1;

        }


        Panel1.Visible = true;
</code></pre>

<p>and this is the out put:</p>

<pre><code>LOCATION    Lanka LOCATION  colombo ORGANIZATION microsoft
</code></pre>

<p>But i need an out put in xml format like this</p>

<pre><code>&lt;LOCATION&gt;  Lanka &lt;/LOCATION&gt;   &lt;LOCATION&gt;colombo&lt;/LOCATION&gt;    &lt;ORGANIZATION&gt; microsoft&lt;/ORGANIZATION&gt; 
</code></pre>

<p>In my code i have used ,</p>

<pre><code> string s = Classifier.classifyWithInlineXML(txt);
            string s1 = Classifier.classifyToString(s, ""xml"", true);
</code></pre>

<p>to get the xml ,but its not working. since i m new to this field please do a help for me to resolve this.
Thanks a lot</p>
","xml, stanford-nlp, named-entity-recognition, information-extraction","<p>This sample code should be helpful:</p>

<pre><code>   String content = ""..."";
   String classifierPath = ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"";
   AbstractSequenceClassifier&lt;CoreLabel&gt; asc  = CRFClassifier.getClassifierNoExceptions(classifierPath);
   String result = asc.classifyWithInlineXML(content);
</code></pre>
",1,0,153,2016-06-26 15:08:48,https://stackoverflow.com/questions/38039874/how-do-i-generate-an-xml-output-from-standfordner-classifier
Programmatically training NER Model using .prop file,"<p>I have been to train my ner model using a property file as shown in the tutorial here <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow noreferrer"">LINK</a>. I am using the same prop file but, when I fail to understand as to how to do it programmatically. </p>

<pre><code>props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, sentiment, regexner"");
props.setProperty(""ner.model"", ""resources/NER.prop"");
</code></pre>

<p>the prop file is as shown below :</p>

<pre><code># location of the training file
trainFile = nerTEST.tsv
# location where you would like to save (serialize) your
# classifier; adding .gz at the end automatically gzips the file,
# making it smaller, and faster to load
serializeTo = resources/ner-model.ser.gz

# structure of your training file; this tells the classifier that
# the word is in column 0 and the correct answer is in column 1
map = word=0,answer=1

# This specifies the order of the CRF: order 1 means that features
# apply at most to a class pair of previous class and current class
# or current class and next class.
maxLeft=1

# these are the features we'd like to train with
# some are discussed below, the rest can be
# understood by looking at NERFeatureFactory
useClassFeature=true
useWord=true
# word character ngrams will be included up to length 6 as prefixes
# and suffixes only
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
# the last 4 properties deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Error :</p>

<pre><code> java.io.StreamCorruptedException: invalid stream header: 23206C6F
....
..
Caused by: java.io.IOException: Couldn't load classifier from resources/NER.prop
</code></pre>

<p>From another question on <a href=""https://stackoverflow.com/questions/30601875/how-to-use-serialized-crfclassifier-with-stanfordcorenlp-prop-ner"">SO</a>, I understand you provide the model file directly. But, how can we do that with the help of a property file?</p>
","java, named-entity-recognition, stanford-nlp","<p>You should run this command from the command line:</p>

<pre><code>java -cp ""*"" edu.stanford.nlp.ie.crf.CRFClassifier -prop NER.prop
</code></pre>

<p>If you want to run this in Java code, you could do something like this:</p>

<pre><code>String[] args = new String[]{""-props"", ""NER.prop""};
CRFClassifier.main(args);
</code></pre>

<p>The .prop file is a file specifying the settings for training your model.  Your code is attempting to load the .prop file as a model itself, which is causing the error.</p>

<p>Doing either will generate the final model at resources/ner-model.ser.gz</p>
",2,4,831,2016-06-28 10:00:05,https://stackoverflow.com/questions/38073043/programmatically-training-ner-model-using-prop-file
use polyglot package for Named Entity Recognition in hebrew,"<p>I am trying to use the polyglot package for Named Entity Recognition in hebrew. <br>
this is my code:</p>

<pre><code># -*- coding: utf8 -*-
import polyglot
from polyglot.text import Text, Word
from polyglot.downloader import downloader
downloader.download(""embeddings2.iw"")
text = Text(u""in france and in germany"")
print(type(text))
text2 = Text(u""נסעתי מירושלים לתל אביב"")
print(type(text2))
print(text.entities)
print(text2.entities)
</code></pre>

<p>this is the output:</p>

<pre><code>&lt;class 'polyglot.text.Text'&gt;
&lt;class 'polyglot.text.Text'&gt;
[I-LOC([u'france']), I-LOC([u'germany'])]
Traceback (most recent call last):
  File ""C:/Python27/Lib/site-packages/IPython/core/pyglot.py"", line 15, in &lt;module&gt;
    print(text2.entities)
  File ""C:\Python27\lib\site-packages\polyglot\decorators.py"", line 20, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File ""C:\Python27\lib\site-packages\polyglot\text.py"", line 132, in entities
    for i, (w, tag) in enumerate(self.ne_chunker.annotate(self.words)):
  File ""C:\Python27\lib\site-packages\polyglot\decorators.py"", line 20, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File ""C:\Python27\lib\site-packages\polyglot\text.py"", line 100, in ne_chunker
    return get_ner_tagger(lang=self.language.code)
  File ""C:\Python27\lib\site-packages\polyglot\decorators.py"", line 30, in memoizer
    cache[key] = obj(*args, **kwargs)
  File ""C:\Python27\lib\site-packages\polyglot\tag\base.py"", line 191, in get_ner_tagger
    return NEChunker(lang=lang)
  File ""C:\Python27\lib\site-packages\polyglot\tag\base.py"", line 104, in __init__
    super(NEChunker, self).__init__(lang=lang)
  File ""C:\Python27\lib\site-packages\polyglot\tag\base.py"", line 40, in __init__
    self.predictor = self._load_network()
  File ""C:\Python27\lib\site-packages\polyglot\tag\base.py"", line 109, in _load_network
    self.embeddings = load_embeddings(self.lang, type='cw', normalize=True)
  File ""C:\Python27\lib\site-packages\polyglot\decorators.py"", line 30, in memoizer
    cache[key] = obj(*args, **kwargs)
  File ""C:\Python27\lib\site-packages\polyglot\load.py"", line 61, in load_embeddings
    p = locate_resource(src_dir, lang)
  File ""C:\Python27\lib\site-packages\polyglot\load.py"", line 43, in locate_resource
    if downloader.status(package_id) != downloader.INSTALLED:
  File ""C:\Python27\lib\site-packages\polyglot\downloader.py"", line 738, in status
    info = self._info_or_id(info_or_id)
  File ""C:\Python27\lib\site-packages\polyglot\downloader.py"", line 508, in _info_or_id
    return self.info(info_or_id)
  File ""C:\Python27\lib\site-packages\polyglot\downloader.py"", line 934, in info
    raise ValueError('Package %r not found in index' % id)
ValueError: Package u'embeddings2.iw' not found in index
</code></pre>

<p>The english worked but not the hebrew.<br>
Whether I try to download the package <code>u'embeddings2.iw'</code>  or not I get: <br></p>

<pre><code>ValueError: Package u'embeddings2.iw' not found in index
</code></pre>
","python, nlp, named-entity-recognition, polyglot","<p>I got it! <br>
It seems like a bug to me.<br>
The language detection defined the language as <code>'iw'</code> which is the The former ISO 639 language code for Hebrew, and was changed to <code>'he'</code>.
The <code>text.entities</code> did not recognize the <code>iw</code> code, so i changes it like so:</p>

<pre><code>text2.hint_language_code = 'he'
</code></pre>
",6,3,2692,2016-07-10 21:25:38,https://stackoverflow.com/questions/38296602/use-polyglot-package-for-named-entity-recognition-in-hebrew
Different result in StanfordNERTagger in python3.5 - Stanford-ner-2015-12-09,"<p>I tried to run a sample sentence:</p>

<pre><code>from nltk.tag import StanfordNERTagger
_model_filename = r'D:/standford/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz'

_path_to_jar = r'D:/standford/stanford-ner-2015-12-09/stanford-ner.jar'

st = StanfordNERTagger(model_filename=_model_filename, path_to_jar=_path_to_jar)

st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) 
</code></pre>

<p>My output was as below in python:</p>

<blockquote>
  <p>[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying',
  'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook',
  'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY',
  'O')]</p>
</blockquote>

<p>while I was expected NY also selected as location based on this <a href=""https://stackoverflow.com/questions/23861355/how-to-install-and-invoke-stanford-nertagger"">reference</a>.</p>

<p>I tried another example as below:</p>

<pre><code>st.tag('Ali is living in London.'.split())
</code></pre>

<p>the result was as below which was correct.</p>

<blockquote>
  <p>[('Ali', 'PERSON'), ('is', 'O'), ('living', 'O'), ('in', 'O'),
  ('London.', 'LOCATION')]</p>
</blockquote>

<p>Do you have any idea why it didn't recognize NY as location in first sentence?</p>

<p>I am using visual studio 2015, Python 3.5, Stanford-ner-2015-12-09</p>
","python-3.x, nlp, nltk, stanford-nlp, named-entity-recognition","<p>Stanford NER tool is trained on properly formatted news text so punctuation is quite important. From the <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">docs</a>:</p>

<blockquote>
  <p>Stanford NER is a Java implementation of a Named Entity Recognizer.
  Named Entity Recognition (NER) labels sequences of words in a text
  which are the names of things, such as person and company names, or
  gene and protein names. It comes with well-engineered feature
  extractors for Named Entity Recognition, and many options for defining
  feature extractors. Included with the download are good named entity
  recognizers for English, particularly for the 3 classes (PERSON,
  ORGANIZATION, LOCATION), and we also make available on this page
  various other models for different languages and circumstances,
  including models trained on just the CoNLL 2003 English training data.</p>
</blockquote>

<p>From the <a href=""http://www.cnts.ua.ac.be/conll2003/ner/"" rel=""nofollow"">CoNLL 2003 doc</a>:</p>

<blockquote>
  <p>The English data is a collection of news wire articles from the
  Reuters Corpus. The annotation has been done by people of the
  University of Antwerp. Because of copyright reasons we only make
  available the annotations. In order to build the complete data sets
  you will need access to the Reuters Corpus. It can be obtained for
  research purposes without any charge from NIST.</p>
</blockquote>

<p>By adding the fullstop to the example sentence, you should get your desired output, but still no model is perfect =)</p>

<pre><code>alvas@ubi:~$ export STANFORDTOOLSDIR=$HOME
alvas@ubi:~$ export CLASSPATH=$STANFORDTOOLSDIR/stanford-ner-2015-12-09/stanford-ner.jar
alvas@ubi:~$ export STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-ner-2015-12-09/classifiers
alvas@ubi:~$ python3
Python 3.5.2 (default, Jul  5 2016, 12:43:10) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from nltk.tag import StanfordNERTagger
&gt;&gt;&gt; st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')
&gt;&gt;&gt; sent = 'Rami Eid is studying at Stony Brook University in NY .'.split()
&gt;&gt;&gt; st.tag(sent)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION'), ('.', 'O')]
&gt;&gt;&gt; sent = 'Rami Eid is studying at Stony Brook University in NY'.split()
&gt;&gt;&gt; st.tag(sent)
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]
</code></pre>
",1,0,334,2016-07-22 08:42:59,https://stackoverflow.com/questions/38521971/different-result-in-stanfordnertagger-in-python3-5-stanford-ner-2015-12-09
Entities on my gazette are not recognized,"<p>I would like to create a custom NER model. That's what i did:</p>

<p><strong>TRAINING DATA</strong> (stanford-ner.tsv):</p>

<pre><code>Hello    O
!    O
My    O
name    O
is    O
Damiano    PERSON
.    O
</code></pre>

<p><strong>PROPERTIES</strong> (stanford-ner.prop):</p>

<pre><code>trainFile = stanford-ner.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
maxLeft=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useGazettes=true
gazette=gazzetta.txt
cleanGazette=true
</code></pre>

<p><strong>GAZZETTE</strong> gazzetta.txt):</p>

<pre><code>PERSON John
PERSON Andrea
</code></pre>

<p>I build the model via command line with:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier  -prop stanford-ner.prop
</code></pre>

<p>And test with:</p>

<pre><code>java -classpath ""stanford-ner.jar:lib/*"" edu.stanford.nlp.ie.crf.CRFClassifier  -loadClassifier ner-model.ser.gz -textFile test.txt
</code></pre>

<p>I did two tests with the following texts:</p>

<p><strong>>>> TEST 1 &lt;&lt;&lt;</strong></p>

<ul>
<li><p>TEXT:
Hello! My name is Damiano and this is a fake text to test.</p></li>
<li><p>OUTPUT
<em>Hello/O !/O
My/O name/O is/O Damiano/PERSON and/O this/O is/O a/O fake/O text/O to/O test/O ./O</em></p></li>
</ul>

<p><strong>>>> TEST 2 &lt;&lt;&lt;</strong></p>

<ul>
<li><p>TEXT:
Hello! My name is John and this is a fake text to test.</p></li>
<li><p>OUTPUT
<em>Hello/O !/O
My/O name/O is/O John/O and/O this/O is/O a/O fake/O text/O to/O test/O ./O</em></p></li>
</ul>

<p>As you can see only ""Damiano"" entity is found. This entity is in my training data but ""John"" (second test) is inside the gazzette. So the question is.</p>

<p>Why does John entity is not recognized ?</p>

<p>Thank you so much in advance.</p>
","machine-learning, nlp, stanford-nlp, named-entity-recognition","<p>As <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#gazette"" rel=""nofollow"">Stanford FAQ</a> says, </p>

<blockquote>
  <p>If a gazette is used, this does not guarantee that words in the
  gazette are always used as a member of the intended class, and it does
  not guarantee that words outside the gazette will not be chosen. It
  simply provides another feature for the CRF to train against. If the
  CRF has higher weights for other features, the gazette features may be
  overwhelmed.</p>
  
  <p>If you want something that will recognize text as a member of a class
  if and only if it is in a list of words, you might prefer either the
  regexner or the tokensregex tools included in Stanford CoreNLP. The
  CRF NER is not guaranteed to accept all words in the gazette as part
  of the expected class, and it may also accept words outside the
  gazette as part of the class.</p>
</blockquote>

<p>Btw, it is not a good practice to test machine learning pipelines in a 'unit-test'-way, i.e. with only one or two examples, because it is supposed to work on much greater volume of data and, more importantly, it is probabilistic by nature.</p>

<p>If you want to check if your gazette file is actually used, it may be better to take existent examples (see the bottom of the page linked above for <code>austen.gaz.prop</code> and <code>austen.gaz.txt</code> examples) and replace multiple names by your own ones, then check. If it fails, firstly try to change your test, e.g. add more names, reformulate text and so on.</p>
",3,5,920,2016-08-13 11:36:59,https://stackoverflow.com/questions/38932299/entities-on-my-gazette-are-not-recognized
Training caseless NER models with Stanford corenlp,"<p>I know how to train an NER model as specified <a href=""http://nlp.stanford.edu/software/crf-faq.html"" rel=""nofollow"">here</a> and have a very successful one in fact. I also know about the 3 provided caseless models as talked about <a href=""http://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""nofollow"">here</a>.  But what if I want to train my own caseless model, what is the trick there?  I have a bunch of all uppercase documents for training.  Do I use the same training process or are there special/different features for the caseless models or are there properties that need to be set?  I can't find a description as to how the provided caseless models were created.</p>
","stanford-nlp, named-entity-recognition","<p>There is only one property change in our models, which is that you want to have it invoke a function that removes case information before words are processed for classification. We do that with this property value (which also maps some words to American spelling):</p>

<p><code>wordFunction = edu.stanford.nlp.process.LowercaseAndAmericanizeFunction</code></p>

<p>but there is also simply:</p>

<p><code>wordFunction = edu.stanford.nlp.process.LowercaseFunction</code></p>

<p>Having more automatic stuff for deciding document format (hard/soft line breaks), case, or even language would be nice, but at present we don't have any of those....</p>
",2,0,513,2016-09-08 19:17:03,https://stackoverflow.com/questions/39398623/training-caseless-ner-models-with-stanford-corenlp
Human language based searches in elasticsearch,"<p>Is it possible to make elasticsearch understand human languages?</p>

<p>user types ""need a laptop for less than $800 with 8 gb ram"" in the searchbox, elasticsearch understands that and filter laptops that have 8gb ram and less than $800?</p>

<p>Are there any packages for this or elasticsearch supports it naturally? Or if it's theoretically possible, any basic idea to achieve this</p>
","elasticsearch, nlp, named-entity-recognition, information-extraction","<p>Machine understanding of natural languages is an unsolved problem and is an active area of research, so <strong>the short answer is no, elasticsearch can't be made to understand human languages.</strong></p>

<p>NLP typically does semantics (understanding) through information extraction which is a sub-field within NLP. Elasticsearch is a great tool, but it not designed for information extraction - it is better thought of as an indexing tool with some extra features. You are likely more interested in named entity recognition (NER) and concept recognition to answer the kind of query you have described of which there is minimal support in Elasticsearch. The problem you are posing is actually a very hard one to answer, which is why Amazon makes available terms like GB on the left hand side of the screen when searching for laptops.</p>

<p>I would start with something other than Elasticsearch, maybe the Stanford NLP toolkit (<a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/</a>)</p>

<p>Good luck!</p>
",4,4,1660,2016-09-21 13:46:20,https://stackoverflow.com/questions/39618418/human-language-based-searches-in-elasticsearch
Custom NER model - FAIL,"<p>I am new to the NLP scene and am using <code>OpenNLP 1.5</code> for getting started.</p>

<p>I went through some the commands given in the documentation here:
<a href=""https://opennlp.apache.org/documentation/manual/opennlp.html"" rel=""nofollow"">https://opennlp.apache.org/documentation/manual/opennlp.html</a> <br>
(I am using the command line interface to get started)</p>

<p>I used the already available sample models for experimenting with the different tools and finally decided to create a <i>custom NER model</i>.</p>

<p>I followed the instruction given in the aforementioned link.</p>

<p>Copied the sample sentences given into a <code>.train</code> file (I simply created a new file with that extension and pasted the contents into it):</p>

<pre><code>&lt;START:person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mr . &lt;START:person&gt; Vinken &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group .
</code></pre>

<p>I used the following command to make the model:</p>

<pre><code>bin/opennlp TokenNameFinderTrainer -model en-ner-person2.bin -lang en -data en-ner-person2.train -encoding UTF-8
</code></pre>

<p>The problem is that even though the model is getting created, it seems to be not working properly. Tested this by using the newly created model:
<code>bin/opennlp TokenNameFinder en-ner-person2.bin</code></p>

<p>But when I input <code>Pierre Vinken</code>, it's not getting recognised as a person. I also tried creating the model from a <code>.txt</code> file with the exact same content, but that too failed.</p>

<p>What am I doing wrong?</p>

<p>TIA.</p>
","nlp, opennlp, named-entity-recognition","<p>In short - you <strong>cannot</strong> expect statistical model to learn from just <strong>two</strong> sentences. Add 14,998 more and you are good to go. </p>

<blockquote>
  <p>The training data should contain at least 15000 sentences to create a model which performs wel</p>
</blockquote>

<p>CRF (Conditional Random Fields) are such statistical models, they do need <strong>a lot</strong> of data to figure out the rules of the game, they are not simply ""remembering"" what they seen during training phase, so even if you ask for something from the trianing set- they can fail to provide the answer. </p>
",4,1,564,2016-09-21 18:23:46,https://stackoverflow.com/questions/39623896/custom-ner-model-fail
Using Python to create a (random) sample of n words from text files,"<p>For my PhD project I am evaluating all existing Named Entity Recogition Taggers for Dutch. In order to check the precision and recall for those taggers I want to manually annotate all Named Entities in a random sample from my corpus. That manually annotated sample will function as the 'gold standard' to which I will compare the results of the different taggers. </p>

<p>My corpus consists of 170 Dutch novels. I am writing a Python script to generate a random sample of a specific amount of words for each novel (which I will use to annotate afterwards). All novels will be stored in the same directory. The following script is meant to generate for each novel in that directory a random sample of n-lines:</p>

<pre><code>import random
import os
import glob
import sys
import errno

path = '/Users/roelsmeets/Desktop/libris_corpus_clean/*.txt'
files = glob.glob(path)  

for text in files:
    try:
        with open(text, 'rt', encoding='utf-8') as f:
             # number of lines from txt file
             random_sample_input = random.sample(f.readlines(),100) 

    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR: 
            raise 


# This block of code writes the result of the previous to a new file
random_sample_output = open(""randomsample"", ""w"", encoding='utf-8') 
random_sample_input = map(lambda x: x+""\n"", random_sample_input)
random_sample_output.writelines(random_sample_input)
random_sample_output.close()
</code></pre>

<p>There are two problems with this code:</p>

<ol>
<li><p>Currently, I have put two novels (.txt files) in the directory. But the code only outputs a random sample for one of each novels. </p></li>
<li><p>Currently, the code samples a random amount of LINES from each .txt file, but I prefer to generate a random amount of WORDS for each .txt file. Ideally, I would like to generate a sample of, say, the first or last 100 words of each of the 170 .txt-files. In that case, the sample won't be random at all; but thus far, I couldn't find a way to create a sample without using the random library.</p></li>
</ol>

<p>Could anyone give a suggestion how to solve both problems? I am still new to Python and programming in general (I am a literary scholar), so I would be pleased to learn different approaches. Many thanks in advance!</p>
","python, text, random, nlp, named-entity-recognition","<p>You just have to split your lines into words, store them somewhere, and then, after having read all of your files and stored their words, pick 100 with <code>random.sample</code>. It it what I did in the code below. However, I am not quite sure if it is able to deal with 170 novels, since it will likely result in a lot of memory usage.</p>

<pre><code>import random
import os
import glob
import sys
import errno

path = '/Users/roelsmeets/Desktop/libris_corpus_clean/*.txt'
files = glob.glob(path)
words = []

for text in files:
    try:
        with open(text, 'rt', encoding='utf-8') as f:
             # number of lines from txt file
             for line in f:
                 for word in line.split():
                     words.append(word)

    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR: 
            raise 

random_sample_input = random.sample(words, 100)

# This block of code writes the result of the previous to a new file
random_sample_output = open(""randomsample"", ""w"", encoding='utf-8') 
random_sample_input = map(lambda x: x+""\n"", random_sample_input)
random_sample_output.writelines(random_sample_input)
random_sample_output.close()
</code></pre>

<p>In the above code, the more words a novel has, the more likely is to be represented in the output sample. That may or may not be the desired behaviour. If you want each novel to have the same ponderation, you can select, let's say, 100 words from it to add in the <code>words</code> variable, and then select 100 hundred words from there at the end. It will also have the side effect of using a lot less memory, since only one novel will be stored at a time.</p>

<pre><code>import random
import os
import glob
import sys
import errno

path = '/Users/roelsmeets/Desktop/libris_corpus_clean/*.txt'
files = glob.glob(path)
words = []

for text in files:
    try:
        novel = []
        with open(text, 'rt', encoding='utf-8') as f:
             # number of lines from txt file
             for line in f:
                 for word in line.split():
                     novel.append(word)
             words.append(random.sample(novel, 100))


    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR: 
            raise 


random_sample_input = random.sample(words, 100)

# This block of code writes the result of the previous to a new file
random_sample_output = open(""randomsample"", ""w"", encoding='utf-8') 
random_sample_input = map(lambda x: x+""\n"", random_sample_input)
random_sample_output.writelines(random_sample_input)
random_sample_output.close()
</code></pre>

<p>Third version, this one will deal with sentences instead of words, and keep the punctuation. Also, each book has the same ""weight"" on the final sentences kept, regardless of its size. Keep in mind that the sentence detection is done by an algorithm that is quite clever, but not infallible.</p>

<pre><code>import random
import os
import glob
import sys
import errno
import nltk.data

path = '/home/clement/Documents/randomPythonScripts/data/*.txt'
files = glob.glob(path)

sentence_detector = nltk.data.load('tokenizers/punkt/dutch.pickle')
listOfSentences = []

for text in files:
    try:
        with open(text, 'rt', encoding='utf-8') as f:
            fullText = f.read()
        listOfSentences += [x.replace(""\n"", "" "").replace(""  "","" "").strip() for x in random.sample(sentence_detector.tokenize(fullText), 30)]

    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR:
            raise

random_sample_input = random.sample(listOfSentences, 15)
print(random_sample_input)

# This block of code writes the result of the previous to a new file
random_sample_output = open(""randomsample"", ""w"", encoding='utf-8')
random_sample_input = map(lambda x: x+""\n"", random_sample_input)
random_sample_output.writelines(random_sample_input)
random_sample_output.close()
</code></pre>
",2,4,2731,2016-10-14 10:05:02,https://stackoverflow.com/questions/40040453/using-python-to-create-a-random-sample-of-n-words-from-text-files
pattens in regexNER in core-nlp,"<p>I would like to have a regex pattenr for regexner inside the core-nlp pipeline. my entity/token is </p>

<pre><code>Machine_DS2302
</code></pre>

<p>Where the second part is <code>alphanumeric</code>.</p>

<p>What I have currently is </p>

<pre><code>Machine_.*  MachineNumber
</code></pre>

<p>But, this annotates everything (this is being a wildcard). I would like to add the tag as <code>MachineNumber</code> based on the the <strong>regex</strong> in the second part i.e. if the second part after _ is a number, then assign it the said tag. </p>

<p>The regex pattern </p>

<pre><code>^[a-zA-Z0-9]*$
</code></pre>

<p>But even </p>

<pre><code>Machine_^[a-zA-Z0-9]*$
</code></pre>

<p>Does not work</p>

<p>How would such a pattern look like for the regexNER?</p>
","regex, named-entity-recognition, stanford-nlp","<p>The anchors are redundant, they actually prevent the pattern from matching because <code>^</code> matches a string start location and <code>$</code> matches the string end location.</p>

<p>Since you need to have access to the part after <code>_</code>, you need to also capture, so use a capturing group:</p>

<pre><code>Machine_([a-zA-Z0-9]*)
</code></pre>

<p>The <code>(...)</code> will create a submatch with the alphanumeric value.</p>

<p>Note that you might want to replace <code>*</code> with <code>+</code> if the alphanumeric part should consist of at least 1 char.</p>
",1,1,469,2016-10-21 10:48:10,https://stackoverflow.com/questions/40174663/pattens-in-regexner-in-core-nlp
Text mining and Machine learning,"<p>I have a DataSet of words and texts and I want to make clusters (by K-means )or any other unsupervised/supervised learning method to distinguish words for example , the word 'John' will be classified as a name(and will be clustered with other person names) , 'brazil' as a place and etc...
Is there a model that I can use to solve the problem.
I have Heard of N-grams but I dont know how to plot the Ngrams probability on a x,y plot or such<br>
P.S if you have any examples that will be wonderful  </p>
","machine-learning, nlp, cluster-analysis, text-mining, named-entity-recognition","<p>How about word2vec and embeddings?<br>
<a href=""https://deeplearning4j.org/word2vec"" rel=""nofollow noreferrer"">https://deeplearning4j.org/word2vec</a></p>
",0,-5,216,2016-11-06 18:40:06,https://stackoverflow.com/questions/40453077/text-mining-and-machine-learning
How to do Wikification / Entity Linking on the basis of the EntityMentionsAnnotator?,"<p>I am trying to link entity mentions to a knowledge base, for instance DBpedia or Wikidata.</p>

<p>In the end I want to enrich the JSON output with an arbitrary ontology and hereby provide some kind of semantics. But as a first step it should just look something like this:</p>

<pre><code>{

    ""index"": 1,
    ""mention"": ""Barack Obama"",
    ""characterOffsetBegin"": 0,
    ""characterOffsetEnd"": 12,
    ""ner"": ""PERSON"",
    ""before"": """",
    ""after"": "" ""
    ""uri"": ""http://dbpedia.org/page/Barack_Obama""

}
</code></pre>

<p>Is there a way of doing so with the already provided tools of Stanford CoreNLP? I have seen a WikidictAnnotator on GitHub, but unfortunately there is no documentation or whatsoever on what it is and how to use it respectively.</p>

<p>Apart form that, what other possibilities are there? Do I have to use a third-party tool like DBpedia Spotlight in conjunction with the Stanford NE Recognizer in order to realize entity linking?</p>

<p>Thanks in advance!</p>
","java, stanford-nlp, dbpedia, named-entity-recognition, wikidata","<p>After some research I am going to answer my own question. Maybe it helps somebody in the future.</p>

<p>I found a framework called AGDISTIS which provides a mentions to knowledge base functionality. It accepts mentions from the EntityMentionsAnnotator and matches them with the according dbpedia resources. You can even use another index apart from dbpedia.</p>

<p>Nevertheless, I would be happy if someone could explain whether and how wikification is possible just with CoreNLP (see WikidictAnnotator).</p>
",0,1,1034,2016-11-29 12:57:10,https://stackoverflow.com/questions/40866504/how-to-do-wikification-entity-linking-on-the-basis-of-the-entitymentionsannota
Extract recommendations/suggestions from text,"<p>My documents often include sentences like:</p>

<blockquote>
  <p>Had I known about this, I would have prevented this problem</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>If John was informed, this wouldn't happen</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>this wouldn't be a problem if Jason was smart </p>
</blockquote>

<p>I'm interested in extracting these sort of information (not sure what they are called, linguistically). So I would like to extract either the whole sentence, or ideally, a summary like:</p>

<blockquote>
  <p>(inform John) (prevent)</p>
</blockquote>

<p>Most, if not all, the examples of relation extraction, and information extraction that I've come across, follow fairly standard flow:
do NER, then relation extraction looks for relations like ""in"" or ""at"", etc (ch7 of nltk book for example). </p>

<p>Do these type of sentences fall under a certain category in NLP? Are there any papers/tutorials on something like this?</p>
","nlp, nltk, named-entity-recognition","<p>When you are asking for a suggestion on a topic which is pretty open, give more examples. I mean to say, if you just give one example and explain what are you targeting doesn't give enough information. For example, if you have sentences which following specific patterns, then it becomes easier to extract information (in your desired format) from them. Otherwise, it becomes broad and complex research problem!</p>

<p>From your example, it looks like you want to extract the <code>head words</code> of a sentence and other words which modify those heads. You can use dependency parsing for this task. Look at <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow noreferrer"">Stanford Neural Network Dependency Parser</a>. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between ""head"" words and words which modify those heads. So, i believe it should help you in your desired task.</p>

<p>If you want to make it more general, then this problem aligns well with Open Information Extraction. You may consider looking into <a href=""http://nlp.stanford.edu/software/openie.html"" rel=""nofollow noreferrer"">Stanford OpenIE</a> api.</p>

<p>You may also consider <a href=""http://nlp.stanford.edu/software/relationExtractor.html"" rel=""nofollow noreferrer"">Stanford Relation Extractor</a> api in your task. But i strongly believe relation extraction through dependency parsing best suits your problem definition. You can read this <a href=""http://semanticweb.kaist.ac.kr/home/images/5/58/Automatic_Relation_Triple_Extraction_by_Dependency_Parse_Tree_Traversing.pdf"" rel=""nofollow noreferrer"">paper</a> to get some idea and utilize them in your task.</p>
",2,2,917,2016-12-08 04:35:55,https://stackoverflow.com/questions/41031853/extract-recommendations-suggestions-from-text
Customised tokens annotation in R,"<p>Currently I'm working on an NLP project. It's totally new for me that's why i'm really struggling with implementation of NLP techniques in R. 
Generally speaking, I need to extract machines entities from descriptions. I have a dictionary of machines which contains 2 columns: Manufacturer and Model. </p>

<p>To train the extraction model, I have to have an annotated corpus. That's where I'm stuck. How to annotate machines in text? Here is an example of the text:</p>

<p><em>The <strong>Skyjack 3219E</strong> electric scissor lift is a self-propelled device powered by 4 x 6 V batteries. The machine is easy to charge, just plug it into the mains. This unit can be used in construction, manufacturing and maintenance operations as a working installation on any flat paved surface. You can use it both indoors and outdoors. Thanks to its non-marking tyres, the machine does not leave any visible tracks on floors. The machine can be driven at full height and is very easy to operate. The S3219E has a 250 kg platform payload capacity. It can handle two people when operating indoors and one outdoors. Discover our trainings via Heli Safety Academy.</em></p>

<p><strong>Skyjack 3219E</strong> - this is a machine which has to be identified and tagged.
I wanna have results similar to POS tagging but instead of nouns and verbs - manufacturer and model. All the other words might be tagged as irrelevant.</p>

<p>Manual annotation is very expensive and not an option as usually descriptions are really long and messy.</p>

<p>Is there a way to adapt POS tagger and use a customised dictionary for tagging? Any help is appreciated!</p>
","r, dictionary, nlp, tagging, named-entity-recognition","<p><strong>Edit: ( At the end of writing this I realized you plan on using R, all my algorithmic suggestions are based on python implementations but I hope you can still get some ideas from the answer )</strong></p>

<p>In general this is considered an <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">NER</a> (named entity recognition) problem. I am doing work on a similar problem at my job.</p>

<h2><strong>Is there any general structure to the text?</strong></h2>

<p>For example does the entity name generally occur in the first sentence? This maybe a way to simplify a heuristic search or a search based a dictionary (of Known products for instance).</p>

<h2><strong>Is annotation that prohibitive?</strong></h2>

<p>A weeks worth of tagging could be all you need given that you essentially have to one label that you care about. I was working on discovering brand names in a unstructured sentences, we did quite well with a week's work of annotation and training a <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">CRF</a> ( Conditional Random Fields ) model. see <a href=""https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb"" rel=""nofollow noreferrer""><code>pycrfsuite</code></a> a good python wrapper of a fast c++ implementation of CRF</p>

<p>[<strong>EDIT</strong>] </p>

<p>For annotation I used a variant BIO tagging scheme.</p>

<p>This what typical sentence like: ""We would love a victoria's secret in our neighborhood"", would look like when tagged.</p>

<pre><code>We O
would O
love O
a O
victoria B-ENT
's I-ENT
secret I-ENT
</code></pre>

<p>O represented words that are <strong>O</strong>utside of the entities I cared about (brands). B represented the <strong>B</strong>eginning of entity phrases and I represents <strong>I</strong>nside of entity phrases.</p>

<p>In your case you seem to want to separate the manufacturer and the model item. So you can use tags like B-MAN, I-MAN, B-MOD, I-MOD. Here is an example of annotating:</p>

<pre><code>The O 
Skyjack B-MAN
3219E B-MOD
electric O
scissor O
lift O
etc..
</code></pre>

<p>of course a manufacture of a model can have multiple words in their names so use the I-MOD, I-MAN tags to capture that (see the example from my work above)</p>

<p>See <a href=""https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb"" rel=""nofollow noreferrer"">this link</a> ( ipython notebook) for a full example of how tagged sequences look for me. I based my work on this.</p>

<h2><strong>Build A big dictionary</strong></h2>

<p>We scrapped the internet, used or own data got databases from partners. And build a huge dictionary that we used as features in our CRF and for general searches. see <a href=""https://github.com/JDonner/NoAho"" rel=""nofollow noreferrer"">ahocorosick</a>  for a fast trie based keyword search in python.</p>

<p>Hope some of this helps!</p>
",0,0,287,2016-12-18 12:44:10,https://stackoverflow.com/questions/41208447/customised-tokens-annotation-in-r
How to create a custom model with my own entities,"<p>I have been trying to find some reference material on how to create custom models with my own entities , like if I want to recognize the name of sports from a text.How do I do it?</p>
","nlp, stanford-nlp, named-entity-recognition","<pre><code>    try {
        propFile = new File(System.getProperty(""user.dir"") + ""/src/edu/stanford/nlp/ie/crf/propfile.prop"");
        properties = new Properties();
        properties.load(new FileInputStream(propFile));

        String to = properties.getProperty(""serializeTo"");

        properties.setProperty(""serializeTo"", ""ner-customModel.ser.gz"");
        properties.setProperty(""trainFile"",System.getProperty(""user.dir"") + ""/src/edu/stanford/nlp/ie/crf/outputTokenized.tsv"");
        CRFClassifier crf = new CRFClassifier(properties);
        crf.train();
        String s2 = ""apples are apples"";

        System.out.println(crf.classifyToString(s2));

        crf.serializeClassifier(System.getProperty(""user.dir"") + ""/src/edu/stanford/nlp/ie/crf/ner-customModel.ser.gz"");

    } catch (IOException e) {
        e.printStackTrace();
    }
</code></pre>

<p>and declare the training file and other properties in the properties file.
This worked for me :)</p>
",1,1,792,2016-12-26 16:56:36,https://stackoverflow.com/questions/41334082/how-to-create-a-custom-model-with-my-own-entities
Error loading list when adding a list to Arabic plugin gazetteer,"<p>I tried to add a new list to the arabic plugin gazetteer. 
I followed the following steps: </p>

<ol>
<li>create a new file ""tags.lst"" to the directory ""GATE_Developer_8.1\plugins\Lang_Arabic\resources\gazetteer\"" </li>
<li>append the ""lists.def"" file with: ""tags.lst:tags::arabic""</li>
</ol>

<p>When launching the gate software, a window pop-up with the following message:</p>

<blockquote>
  <p>Resource could not be created!</p>
  
  <p>gate.creole.ResourceInstantiationException:
  gate.util.GateRuntimeException: Error loading list: tags.lst:
  java.io.IOException: The system cannot find the path specified.</p>
</blockquote>

<p>Here is the full exception:</p>

<pre><code>gate.creole.ResourceInstantiationException: gate.util.GateRuntimeException: Error loading list: tags.lst: java.io.IOException: The system cannot find the path specified
    at gate.creole.gazetteer.LinearDefinition.load(LinearDefinition.java:281)
    at gate.creole.gazetteer.DefaultGazetteer.init(DefaultGazetteer.java:119)
    at gate.Factory.createResource(Factory.java:432)
    at gate.gui.NewResourceDialog$4.run(NewResourceDialog.java:257)
    at java.lang.Thread.run(Thread.java:745)
Caused by: gate.util.GateRuntimeException: Error loading list: tags.lst: java.io.IOException: The system cannot find the path specified
    at gate.creole.gazetteer.LinearDefinition.add(LinearDefinition.java:527)
    at gate.creole.gazetteer.LinearDefinition.load(LinearDefinition.java:276)
    ... 4 more
Caused by: gate.creole.ResourceInstantiationException: java.io.IOException: The system cannot find the path specified
    at gate.creole.gazetteer.LinearDefinition.loadSingleList(LinearDefinition.java:199)
    at gate.creole.gazetteer.LinearDefinition.loadSingleList(LinearDefinition.java:158)
    at gate.creole.gazetteer.LinearDefinition.add(LinearDefinition.java:520)
    ... 5 more
Caused by: java.io.IOException: The system cannot find the path specified
    at java.io.WinNTFileSystem.createFileExclusively(Native Method)
    at java.io.File.createNewFile(File.java:1012)
    at gate.creole.gazetteer.LinearDefinition.loadSingleList(LinearDefinition.java:188)
    ... 7 more
</code></pre>

<p>I will appreciate any help ?? </p>
","nlp, named-entity-recognition, gate","<p>The Problem was due to two major issues which are: </p>

<ol>
<li><p>The file was not saved correctly as utf-8 encoding which was resolved by using online converter: <a href=""http://www.motobit.com/util/charset-codepage-conversion.asp"" rel=""nofollow noreferrer"">http://www.motobit.com/util/charset-codepage-conversion.asp</a></p></li>
<li><p>The file contains special characters which were resolved by using the following replaceAll regular expression [#|""|:]: </p></li>
</ol>

<blockquote>
  <p><code>line = line.replaceAll(""[#|\""|:]"", "" "");</code></p>
</blockquote>
",1,0,160,2016-12-30 15:31:33,https://stackoverflow.com/questions/41398680/error-loading-list-when-adding-a-list-to-arabic-plugin-gazetteer
NLTK Named Entity recognition for a column in a dataset,"<p>Thanks to ""alvas"" code from here , <a href=""https://stackoverflow.com/questions/24398536/named-entity-recognition-with-regular-expression-nltk"">Named Entity Recognition with Regular Expression: NLTK</a> and as an example:</p>

<pre><code>from nltk import ne_chunk, pos_tag
from nltk.tokenize import word_tokenize
from nltk.tree import Tree

def get_continuous_chunks(text):
    chunked = ne_chunk(pos_tag(word_tokenize(text)))
    prev = None
    continuous_chunk = []
    current_chunk = []

    for i in chunked:
        if type(i) == Tree:
            current_chunk.append("" "".join([token for token, pos in i.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk

txt = 'The new GOP era in Washington got off to a messy start Tuesday as House Republicans,under pressure from President-elect Donald Trump.'
print (get_continuous_chunks(txt))
</code></pre>

<p>the output is :<br/></p>

<blockquote>
  <p>['GOP', 'Washington', 'House Republicans', 'Donald Trump']</p>
</blockquote>

<p>I replaced this text with this : <code>txt = df['content'][38]</code> from my dataset and I get this result : </p>

<blockquote>
  <p>['Ina', 'Tori K.', 'Martin Cuilla', 'Phillip K', 'John J Lavorato']</p>
</blockquote>

<p>This dataset has many rows and one column named 'content'.My question is how can I use this code to extract names from this column for each row and store that names in another column and corresponding rows?</p>

<pre><code>import os
from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize
from nltk.tree import Tree
st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')
text = df['content']
tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)
print (classified_text)
</code></pre>
","python, nlp, nltk, named-entity-recognition","<p>Try <code>apply</code>:</p>

<pre><code>df['ne'] = df['content'].apply(get_continuous_chunks)
</code></pre>

<p>For the code in your second example, create a function and apply it the same way:</p>

<pre><code>def my_st(text):
    tokenized_text = word_tokenize(text)
    return st.tag(tokenized_text)

df['st'] = df['content'].apply(my_st)
</code></pre>
",3,1,2337,2017-01-04 04:32:43,https://stackoverflow.com/questions/41456250/nltk-named-entity-recognition-for-a-column-in-a-dataset
machine learning - check &amp; parse sentence related to previous sentence,"<p>During natural language processing, what should be the logic to determine if the current sentence has ""cues"" which needs to be taken from the previous sentence while parsing.</p>

<p>I am not sure how to aptly describe this issue or what sub-topic of NLP it is referred as, so I would present an example.</p>

<p>If <em>previous sentence</em> was <code>Find me an Italian restaurant in New York city</code><br>
and the <em>current sentence</em> is <code>What's the weather there tomorrow</code></p>

<p>How should the parser proceed to understand that <code>there</code> in the current sentence refers to <code>New york</code> which was mentioned in the previous sentence.</p>

<p><em>PS - I am fairly new to NLP, so please be kind here with your explanations.</em></p>
","machine-learning, nlp, artificial-intelligence, named-entity-recognition","<p>An anaphor is an expression that refers back to a previous expression in a natural language discourse. For example: ""Mary died. She was very old."" The word <strong>she</strong> refers to <strong>Mary</strong>, and is described as an <strong>anaphoric reference</strong> to Mary. Mary is described as the <strong>antecedent</strong> of she. Anaphoric references are frequently pronouns, as in the example, but may also be definite noun phrases, as in: ""Ronald Reagan frowned. The President was clearly worried by this issue."" Here The President is an anaphoric reference to Ronald Reagan. Anaphors may in some cases not be explicitly mentioned in a previous sentence - as in ""John got out his pencil. He found that the lead was broken."" The lead here refers to a subpart of his pencil. Anaphors need not be in the immediately preceding sentence, they could be further back, or in the same sentence, as in ""John got out his pencil, but found that the lead was broken."" In all our examples so far the anaphor and the antecedent are noun phrases, but VP and sentence-anaphora is also possible, as in ""I have today dismissed the prime minister. It was my duty in the circumstances."" Here It is an anaphoric reference to the VP dismissed the prime minister.
For a fairly complete and quite entertaining treatment of anaphora, see <a href=""http://www.cs.toronto.edu/pub/gh/Hirst-Anaphora-0.pdf"" rel=""nofollow noreferrer"">Hirst, G. Anaphora in Natural Language Understanding: A Survey Springer Lecture Notes in Computer Science 119, Berlin: Springer, 1981</a>.</p>

<p>You can also find an algorithm to solve the problem in this <a href=""http://www.aclweb.org/anthology/Y99-1027"" rel=""nofollow noreferrer"">Paper</a>.</p>
",1,3,398,2017-01-21 14:43:19,https://stackoverflow.com/questions/41780649/machine-learning-check-parse-sentence-related-to-previous-sentence
Which settings should be used for TokensregexNER,"<p>When I try regexner it works as expected with the following settings and data;</p>

<pre><code>props.setProperty(""annotators"", ""tokenize, cleanxml, ssplit, pos, lemma, regexner"");
</code></pre>

<blockquote>
  <p>Bachelor of Laws  DEGREE <br>
  Bachelor of (Arts|Laws|Science|Engineering|Divinity)    DEGREE</p>
</blockquote>

<p>What I would like to do is that using TokenRegex. For example </p>

<blockquote>
  <p>Bachelor of Laws  DEGREE<br>
  Bachelor of ([{tag:NNS}] [{tag:NNP}])   DEGREE</p>
</blockquote>

<p>I read that to do this, I should use TokensregexNERAnnotator.</p>

<p>I tried to use it as follows, but it did not work.</p>

<pre><code>Pipeline.addAnnotator(new TokensRegexNERAnnotator(""expressions.txt"", true));
</code></pre>

<p>Or I tried setting annotator in another way,</p>

<pre><code>props.setProperty(""annotators"", ""tokenize, cleanxml, ssplit, pos, lemma, tokenregexner"");    
props.setProperty(""customAnnotatorClass.tokenregexner"", ""edu.stanford.nlp.pipeline.TokensRegexNERAnnotator"");
</code></pre>

<p>I tried to different TokenRegex formats but either annotator could not find the expression or I got SyntaxException.</p>

<p>What is the proper way to use TokenRegex (query on tokens with tags) on NER data file ?</p>

<p>BTW I just see a comment in TokensRegexNERAnnotator.java file. Not sure if it is related pos tags does not work with RegexNerAnnotator.</p>

<pre><code>if (entry.tokensRegex != null) {
    // TODO: posTagPatterns...
    pattern = TokenSequencePattern.compile(env, entry.tokensRegex);
  }
</code></pre>
","named-entity-recognition, stanford-nlp","<p>First you need to make a TokensRegex rule file (sample_degree.rules).  Here is an example:</p>

<pre><code>ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }

{ pattern: (/Bachelor/ /of/ [{tag:NNP}]), action: Annotate($0, ner, ""DEGREE"") }
</code></pre>

<p>To explain the rule a bit, the <code>pattern</code> field is specifying what type of pattern to match.  The <code>action</code> field is saying to annotate every token in the overall match (that is what <code>$0</code> represents), annotate the <code>ner</code> field (note that we specified ner = ... in the rule file as well, and the third parameter is saying set the field to the String ""DEGREE"").</p>

<p>Then make this .props file (degree_example.props) for the command:</p>

<pre><code>customAnnotatorClass.tokensregex = edu.stanford.nlp.pipeline.TokensRegexAnnotator

tokensregex.rules = sample_degree.rules

annotators = tokenize,ssplit,pos,lemma,ner,tokensregex
</code></pre>

<p>Then run this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -props degree_example.props -file sample-degree-sentence.txt -outputFormat text
</code></pre>

<p>You should see that the three tokens you wanted tagged as ""DEGREE"" will be tagged.</p>

<p>I think I will push a change to the code to make <code>tokensregex</code> link to the TokensRegexAnnotator so you won't have to specify it as a custom annotator.
But for now you need to add that line in the .props file.</p>

<p>This example should help in implementing this.  Here are some more resources if you want to learn more:</p>

<p><a href=""http://nlp.stanford.edu/software/tokensregex.shtml#TokensRegexRules"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/tokensregex.shtml#TokensRegexRules</a></p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/SequenceMatchRules.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/SequenceMatchRules.html</a></p>

<p><a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/types/Expressions.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/tokensregex/types/Expressions.html</a></p>
",2,3,668,2017-02-26 16:09:13,https://stackoverflow.com/questions/42470843/which-settings-should-be-used-for-tokensregexner
Tagging and Training NER dataset,"<p>I have a data set and I want to tag it for Named Entity Recognition. My dataset is in Persian.
I want to know how should I tag expressions like :</p>

<p>*** آقای مهدی کاظمی  = Mr Mehdi Kazemi / Mr will Smith. >>> (names with titles) should I tag all as a person or just the first name and last name should be tagged? (I mean should i also tag ""Mr"")</p>

<p>Mr  >>  b_per     ||     Mr >> O</p>

<p>Mehdi  >>   i_per || Mehdi >> b_per</p>

<p>Kazemi  >> i_per || Kazemi >> i_per</p>

<p>*** بیمارستان نور = Noor hospital >>> Should I tag the name only or the name and hospital both as Named Entity?</p>

<p>*** Eiffel tower / The Ministry of Defense (I mean the us DOD for example) >>> in Persian it is called :
وزارت دفاع    (vezarate defa)
should I only tag Defense ? or all together?</p>

<p>There are many more examples for schools, movies, cities, countries and.... since we use the entity class before the named entity.</p>

<p>I would appreciate if you can help me with tagging this dataset.</p>
","tags, nlp, stanford-nlp, named-entity-recognition, named-entity-extraction","<p>I'll give you some examples from the CoNLL 2003 training data:</p>

<p>""Mr."" is not tagged as part of the person, so titles are ignored.</p>

<p>""Columbia Presbyterian Hospital"" is tagged as (LOC, LOC, LOC)</p>

<p>""a New York hospital"" (O, LOC, LOC, O)</p>

<p>""Ministry of Commerce"" is (ORG, ORG, ORG)</p>

<p>I think ""Eiffel Tower"" should be (LOC, LOC)</p>
",0,-1,998,2017-03-28 12:42:34,https://stackoverflow.com/questions/43069935/tagging-and-training-ner-dataset
Training own model and adding new entities with spacy,"<p>I have been trying to train a model with the same method as #887 is using, just for a test case.
I have a question, what would be the best format for a training corpus to import in spacy. I have a text-file with a list of of entities that requires new entities for tagging.
Let me explain my case, I follow the update.training script like this:</p>

<pre><code>nlp = spacy.load('en_core_web_md', entity=False, parser=False)

ner= EntityRecognizer(nlp.vocab, entity_types=['FINANCE'])

for itn in range(5):
    random.shuffle(train_data)
    for raw_text, entity_offsets in train_data:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)

        nlp.tagger(doc)
        ner.update(doc, gold)
ner.model.end_training()
</code></pre>

<p>I add my training data as entity_offsets:</p>

<pre><code>train_data = [
    ('Monetary contracts are financial instruments between parties', [(23, 44, 'FINANCE')])
]
</code></pre>

<p>This is working fine for the one example and new entity tag. Obviously I want to be able to add more than one example. The Idea is to create a text file with tagged sentences, the question is what format does spacy needs for training data, should I keep with entity_offset from the examples (this will be a very tedious task for 1000's of sentences) or is there another method to prepare the file, like:</p>

<pre><code>financial instruments   FINANCE
contracts   FINANCE
Product OBJ
of O
Microsoft ORG
etc ...
</code></pre>

<p>And how can I pass the corpus in spcay using the mentioned method? Do I have to use the new created model or can I add the new entities to the old model, how can this be achieved?</p>

<p><strong>UPDATE</strong>
I managed to import a file with training data that would be recognized by the training method described above.
The list will look like this:</p>

<pre><code>Financial instruments can be real or virtual documents, 0 21 FINANCE
The number of units of the financial instrument, 27 47 FINANCE
or the number of derivative contracts in the transaction, 17 37 BANKING
Date and time when the transaction was executed, 23 34 ORDER
...
</code></pre>

<p>But the training is not performing well, I supposed this is due to the small training data. I get all entries in the test corpus tagged as FINANCE or all tagged by BANKING. How big does my train data need to be to get a better performance?</p>

<p>I guess I will have to annotate a bigger corpus for may training data. Can this be done in a different way?</p>

<p>What algorithm is behind the spacy Named Entity Recognizer?</p>

<p>Thanks for any help.</p>

<p>My Environment</p>

<p>spaCy version: 1.7.3
Platform: Windows-7-6.1.7601-SP1
Python version: 3.6.0
Installed models: en, en_core_web_md</p>
","python, named-entity-recognition, spacy","<p>To provide training examples to the entity recogniser, you'll first need to create an instance of the GoldParse class. You can specify your annotations in a stand-off format or as token tags.</p>

<pre><code>import spacy
import random
from spacy.gold import GoldParse
from spacy.language import EntityRecognizer

train_data = [
    ('Who is Chaka Khan?', [(7, 17, 'PERSON')]),
    ('I like London and Berlin.', [(7, 13, 'LOC'), (18, 24, 'LOC')])
]

nlp = spacy.load('en', entity=False, parser=False)
ner = EntityRecognizer(nlp.vocab, entity_types=['PERSON', 'LOC'])

for itn in range(5):
    random.shuffle(train_data)
    for raw_text, entity_offsets in train_data:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)

        nlp.tagger(doc)
        ner.update(doc, gold)
ner.model.end_training()
</code></pre>

<p>Or to simplify this you can try this code</p>

<pre><code>doc = Doc(nlp.vocab, [u'rats', u'make', u'good', u'pets'])
gold = GoldParse(doc, [u'U-ANIMAL', u'O', u'O', u'O'])
ner = EntityRecognizer(nlp.vocab, entity_types=['ANIMAL'])
ner.update(doc, gold)
</code></pre>
",5,3,7014,2017-04-11 08:56:09,https://stackoverflow.com/questions/43341148/training-own-model-and-adding-new-entities-with-spacy
Train corpus for NER with NLTK ieer or conll2000 corpus,"<p>I have been trying to train a model for Named Entity Recognition for a specific domain, and with new entities. It seems there is not a completed suitable pipeline for this, and there is the need to use different packages.</p>

<p>I would like to give a chance to NLTK. My question is, how can I train a the NLTK NER to classify and match new entities using the ieer corpus?</p>

<p>I will of course provide training data with the IOB-Format like:</p>

<pre><code>We PRP B-NP
saw VBD O
the DT B-NP
yellow JJ I-NP
dog NN I-NP
</code></pre>

<p>I guess I will have to tag the tokens by myself.</p>

<p>What do I do next when I have a text file in this format, what are the steps to train my data with the ieer corpus, or with a better one, conll2000?</p>

<p>I know there is some documentation out there, but it is not clear for me what to do after you have a training corpus tagged.</p>

<p>I want to go for NLTK because I then want to use the <em>relextract()</em> function.</p>

<p>Please any advise.</p>

<p>Thanks</p>
","python, nltk, named-entity-recognition","<p>The nltk provides everything you need. Read the nltk book's chapter 6, on <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">Learning to Classify Text</a>. It gives you a worked example of classification. Then study sections 2 and 3 from <a href=""http://www.nltk.org/book/ch07.html"" rel=""nofollow noreferrer"">Chapter 7</a>, which show you how to work with IOB text and write a chunking classifier. Although the example application is not named entity recognition, the code examples should need almost no changes to work (although of course you'll need a custom feature function to get decent performance.)</p>

<p>You can also use the nltk's tagger (or another tagger) to add POS tags to your corpus, or you could take your chances and try to train a classifier on data without part-of-speech tags (just the IOB named entity categories). My guess is that POS tagging will improve performance, and you're actually much better off if the same POS tagger is used on the training data as for evaluation (and eventually production use).</p>
",1,1,902,2017-04-19 07:54:21,https://stackoverflow.com/questions/43489724/train-corpus-for-ner-with-nltk-ieer-or-conll2000-corpus
Stanford NER not tagging date and time,"<p>I am using Stanford NER tagger in python. It is not tagging dates and time. Rather returns O on every word.
My sentence was:</p>

<p>""What sum of money will earn an interest of $ 162 in 3 years at the rate of 12% per annum""</p>

<p>The result I got after tagging was-</p>

<pre><code>[('What', 'O'), ('sum', 'O'), ('of', 'O'), ('money', 'O'), ('will', 'O'), ('earn', 'O'), ('an', 'O'), ('interest', 'O'), ('of', 'O'), ('$', 'O'), ('162', 'O'), ('in', 'O'), ('3', 'O'), ('years', 'O'), ('at', 'O'), ('the', 'O'), ('rate', 'O'), ('of', 'O'), ('12%', 'O'), ('per', 'O'), ('annum', 'O')]
</code></pre>

<p>How to fix this?</p>
","python, stanford-nlp, named-entity-recognition","<ol>
<li><p>Download and install Stanford NLP Group's Python library <code>stanza</code> .</p>

<p>GitHub: <a href=""https://github.com/stanfordnlp/stanza"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/stanza</a></p></li>
<li><p>With Stanford CoreNLP 3.7.0, start a server:</p>

<p>command: <code>java -Xmx4g edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000</code></p>

<p>Stanford CoreNLP 3.7.0: <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>(Note: make sure CLASSPATH contains all of the jars in the download folder)</p></li>
<li><p>Issue a request to the Java Stanford CoreNLP server started in Step 2:</p>

<pre><code>from stanza.nlp.corenlp import CoreNLPClient

client = CoreNLPClient(server='http://localhost:9000', default_annotators=['ssplit', 'tokenize', 'lemma', 'pos', 'ner'])

annotated = client.annotate(""..text to annotate..."")

for sentence in annotated.sentences:
  print ""---""
  print sentence.tokens
  print sentence.ner_tags
</code></pre>

<p>We are working on having the Python library handle starting and stopping the server for Stanford CoreNLP 3.8.0.</p></li>
</ol>
",3,1,1262,2017-04-21 03:55:25,https://stackoverflow.com/questions/43533701/stanford-ner-not-tagging-date-and-time
How to call the ClassifierBasedTagger() in NLTK,"<p>I have followed in the documentation from nltk book (chapter 6 and 7) and other ideas to train my own model for named entity recognition. After building a  feature function and ClassifierBasedTagger like this:</p>

<pre><code>class NamedEntityChunker(ChunkParserI):
    def __init__(self, train_sents, feature_detector=features, **kwargs):
        assert isinstance(train_sents, Iterable)
        tagged_sents = [[((w,t),c) for (w,t,c) in
                         tree2conlltags(sent)]
                        for sent in train_sents]

        #other possible option: self.feature_detector = features
        self.tagger = ClassifierBasedTagger(tagged_sents, feature_detector=feature_detector, **kwargs)

    def parse(self, tagged_sent):
        chunks = self.tagger.tag(tagged_sent)

        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]

        # Transform the list of triplets to nltk.Tree format
        return conlltags2tree(iob_triplets)
</code></pre>

<p>I am having problems when caling the classifiertagger from another script where I load my traning and test data. I call the classifier using a portion from my training data for testing purpose from:</p>

<pre><code>chunker = NamedEntityChunker(training_samples[:500])
</code></pre>

<p>No matter what I change in my classifier I keept getting the error:</p>

<pre><code>   self.tagger = ClassifierBasedTagger(tagged_sents, feature_detector=feature_detector, **kwargs)
TypeError: __init__() got multiple values for argument 'feature_detector'
</code></pre>

<p>What am I doing wrong here, I supossed the feature function is working fine and I don't have to pass anything else when calling NamedEntityChunker().</p>

<p>my second question, is there a way to save the model being trained and reuse it lataer, how can I approach this?
This is a follow up of my <a href=""https://stackoverflow.com/questions/43489724/train-corpus-for-ner-with-nltk-ieer-or-conll2000-corpus"">last question on training data</a></p>

<p>Thanks for any advise</p>
","python, nltk, named-entity-recognition","<p>I finally realised what I was missing:
when defining BasedTagger you have to pass an argument for ""tagged_sents"", like this:</p>

<pre><code>#self.tagger = ClassifierBasedTagger(train=train_sents, feature_detector=features, **kwargs) 
</code></pre>

<p>now when I call the chunker NamedEntityChunker() everything is working.</p>
",1,1,493,2017-04-27 16:08:10,https://stackoverflow.com/questions/43662829/how-to-call-the-classifierbasedtagger-in-nltk
Named entity recognition including context in Python with nltk,"<p>I have a text and want to extract the named entities including the context using nltk with Python 3.6. 
The following example should illustrate what I mean:</p>

<pre><code>import nltk

text = 'the Iraqi people'
tokens = nltk.word_tokenize(text)
tag_tokens = nltk.pos_tag(tokens)
named_entities = nltk.ne_chunk(tag_tokens, binary=True)

print(named_entities)
</code></pre>

<p>Running this code gives me</p>

<pre><code>(S the/DT (NE Iraqi/NNP) people/NNS)
</code></pre>

<p>However, I want to my algorithm to recognise 'Iraqi people' as one named entity, i.e.</p>

<pre><code>(S the/DT (NE Iraqi/NNP people/NNS))
</code></pre>

<p>Right now my best guess about how to go about this is to look through the list of pos-tagged tokens and make chunks out of nouns, that follow proper nouns. 
Is there a better way of going about this, or maybe even a library I don't know of?</p>
","python, nltk, named-entity-recognition","<p><code>ne_chunk</code> stands for NLTK’s currently recommended named entity chunker which is some statistical model. That means</p>

<ul>
<li>sometimes it might be wrong</li>
<li>it might be trained to predict different things than you would like it to predict</li>
</ul>

<p>Unfortunately you can't modify the model. So you are left with two options:
Either you train your own model, which is going to be a lot of work. Or you use some heuristics, for example the one you are proposing.</p>

<p>As to which heuristic to use, it depends on your application. However, generally speaking, errors shouldn't surprise you...</p>
",0,0,558,2017-04-28 15:42:47,https://stackoverflow.com/questions/43684098/named-entity-recognition-including-context-in-python-with-nltk
Re-Training spaCy&#39;s NER v1.8.2 - Training Volume and Mix of Entity Types,"<p>I'm in the process of (re-) training spaCy's Named Entity Recognizer and have a couple of doubts that I hope a more experienced researcher/practitioner can help me figure out:</p>
<ol>
<li>If a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? Is 100 000 entity/label excessive?</li>
<li>If I introduce a new label, is it best if the number of the entities of that labeled are roughly the same (balanced) during training?</li>
<li>Regarding the mixing in 'examples of other entity types':
<ul>
<li><p>do I just add random known categories/labels to my training set eg: <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'ORG')], )</code>?</p>
</li>
<li><p>can I use the same text for various labels? e.g. <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(55,64, 'COMMODITY')], )</code>?</p>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>on a similar note let's assume I want spaCyto also recognize a second <code>COMMODITY</code> could I then just use the same sentence and label a different region e.g. <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(69,80, 'COMMODITY')], )</code>? Is that how it's supposed to be done?</p>
<ul>
<li>what ratio between new and other (old) labels is considered reasonable</li>
</ul>
</li>
</ul>
<p>I'm working with Python2.7 in Ubuntu 16.04 using spaCy 1.8.2</p>
","python-2.7, nlp, named-entity-recognition, spacy","<p>For a full answer by <a href=""https://github.com/explosion/spaCy/issues/1054"" rel=""nofollow noreferrer"">Matthew Honnibal check out issue 1054 on spaCy's github page</a>. Below are the most important points as they relate to my questions:</p>
<blockquote>
<p><strong>Question(Q) 1:</strong> If a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? Is 100 000 entity/label excessive?</p>
<blockquote>
<p><strong>Answer(A):</strong> Every machine learning problem will have a different examples/accuracy curve. You can get an idea for this by training with less data than you have, and seeing what the curve looks like. If you have 1,000 examples, then try training with 500, 750, etc, and see how that affects your accuracy.</p>
</blockquote>
<p><strong>Q 2:</strong> If I introduce a new label, is it best if the number of the entities of that label are roughly the same (balanced) during training?</p>
<blockquote>
<p><strong>A:</strong> There's trade-off between making the gradients too sparse, and making the learning problem too unrepresentative of what the actual examples will look like.</p>
</blockquote>
<p><strong>Q 3:</strong> Regarding the mixing in 'examples of other entity types':</p>
<ul>
<li>do I just add random known categories/labels to my training set:</li>
</ul>
</blockquote>
<p><strong>A:</strong> No, one should annotate all the entities in that text, so the example above: <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'ORG')], )</code> should be <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'ORG'), (55,64, 'COMMODITY'), (69,80, 'COMMODITY')], )</code></p>
<blockquote>
<ul>
<li>can I use the same text for various labels?:</li>
</ul>
</blockquote>
<p><strong>A:</strong> Not in the way the examples were given. See previous answer.</p>
<blockquote>
<ul>
<li>what ratio between new and other (old) labels is considered reasonable?:</li>
</ul>
</blockquote>
<p><strong>A:</strong> See answer <strong>Q 2</strong>.</p>
<hr />
<p><em>PS: Double citations are direct quotes from the github issue answer.</em></p>
",1,3,820,2017-05-11 09:58:06,https://stackoverflow.com/questions/43912195/re-training-spacys-ner-v1-8-2-training-volume-and-mix-of-entity-types
Custom NER model extracts substring of keyword used for training,"<p>I trained a custom NER model with more than a million sentences using the training API of OpenNlp for identifying the skill that I taught. During the testing I have found that the model identifying the skill and a substring of skill. For example, I have taught a skill 'Core Java', if the sentence have  word 'Core' then my model will identify it as a skill. I want to avoid such situation. I want only identify the whole word 'Core Java' if the whole word is there in the test sentences.</p>

<p>How can improve my custom model for above mentioned result?</p>
","nlp, opennlp, named-entity-recognition","<p>Sounds like you need to provide more training examples. </p>

<p>If the only time the word <code>Core</code> appears in your training data is as part of the phrase <code>Core Java</code>, your model might learn that <code>Core</code> is part of a skill name with 100% probability, and based on what it knows that isn't wrong. To fix it, add more training data where it's used in an unrelated way. Some examples:</p>

<pre><code>He threw away the apple core.
Core skills in math include addition and subtraction.
The core of the application is implemented in C for speed.
</code></pre>
",1,1,311,2017-07-04 06:59:30,https://stackoverflow.com/questions/44899007/custom-ner-model-extracts-substring-of-keyword-used-for-training
Linear CRF Versus Word2Vec for NER,"<p>I have done lots of reading around Linear CRF and Word2Vec and wanted to know which one is the best to do Named Entity Recognition. I trained my model using Stanford NER(Which is a Linear CRF Implementation) and got a precision of 85%. I know that Word2vec groups similar words together but is it a good model to do NER?</p>
","nlp, named-entity-recognition","<p>CRFs and word2vec are apples and oranges, so comparing them doesn't really make sense.</p>

<p>CRFs are used for sequence labelling problems like NER. Given a sequence of items, represented as features and paired with labels, they'll learn a model to predict labels for new sequences.</p>

<p>Word2vec's word embeddings are representations of words as vectors of floating point numbers. They don't predict anything by themselves. You can even use the word vectors to build features in a CRF, though it's more typical to use them with a neural model like an LSTM.</p>

<p>Some people have used word vectors with CRFs with success. For some discussion of using word vectors in a CRF see <a href=""https://datascience.stackexchange.com/questions/853/unsupervised-feature-learning-for-ner"">here</a> and <a href=""https://web.archive.org/web/20180602231633/http://www.sersc.org/journals/IJSEIA/vol10_no2_2016/8.pdf"" rel=""nofollow noreferrer"">here</a>. </p>

<p>Do note that with many standard CRF implementations features are expected to be binary or categorical, not continuous, so you typically can't just shove word vectors in as you would another feature. </p>

<p>If you want to know which is better for your use case, the only way to find out is to try both.</p>
",6,2,2876,2017-07-13 21:35:31,https://stackoverflow.com/questions/45091281/linear-crf-versus-word2vec-for-ner
Extracting fields from an emails based on values in a database as training set,"<p>Ive got 480 emails and each of them consist of one or all of these values :-</p>

<p>[person, degree, working/not working, role]</p>

<p>So for example one of the email looks like this :-</p>

<pre><code>    Hi Amy,

    I wanted to discuss about Bob. I can see that he has a degree in 
    Computer Science which he got three years ago but hes still unemployed. 
    I dont know whetehr he'll be fit for the role of junior programmer at 
    our institute.
    Will get back to you on this.

    Thanks
</code></pre>

<p>The corresponding database entry for this email looks like this</p>

<pre><code>Email_123 | Bob | Computer Science | Unemployed | Junior Programmer
</code></pre>

<p>Now even though the data hasnt been labelled but we still have somewhat of a database to lookup which values were extracted into the 4 fields from each of the email.
Now my question is that how can I use this corpus of 480 emails to learn and extract these 4 fields using Machine Learning/NLP.
Do I need to manually tag all these 480 emails like..</p>

<pre><code>I wanted to discuss about &lt;person&gt;Bob&lt;/person&gt;. I can see that he has a degree in 
    &lt;degree&gt;Computer Science&lt;/degree&gt; which he got....
</code></pre>

<p>Or is there a better way. Something like this (MarI/O - Machine Learning for Video Games) <a href=""https://www.youtube.com/watch?v=qv6UVOQ0F44&amp;t=149s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=qv6UVOQ0F44&amp;t=149s</a> </p>
","machine-learning, nlp, named-entity-recognition","<p>Assuming that each email has only one value for each field, and that the value is always reproduced verbatim from the email, you can use something like <a href=""http://www.aclweb.org/anthology/P/P16/P16-1145.pdf"" rel=""nofollow noreferrer"">WikiReading</a>.</p>

<p><a href=""https://i.sstatic.net/KIJx0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KIJx0.png"" alt=""WikiReading Extraction Example""></a></p>

<p>The problem is that WikiReading was trained on 4.7 <em>million</em> examples, so if you only have 480 that's nowhere near enough to train a good model.</p>

<p>What I would suggest is preprocessing your dataset to automatically add tags like in your example. Something like this, in pseudo-python:</p>

<pre><code>entity = ""Junior Programmer""
entity_type = ""role""
mail = ""...[text of email]...""

ind = mail.index(entity)
tagged = ""{front}&lt;{tag}&gt;{ent}&lt;/{tag}&gt;{back}"".format(
  front=mail[0:ind],
  back=mail[ind+len(entity):],
  tag=entity_type,
  ent=entity)
</code></pre>

<p>You'll need to adjust for case issues, multiple matches, and so on.</p>

<p>With tagged data you can use a conventional NER system like a CRF. <a href=""https://spacy.io/docs/usage/entity-recognition"" rel=""nofollow noreferrer"">Here</a>'s a tutorial using spaCy in Python.</p>
",1,1,89,2017-08-02 03:37:53,https://stackoverflow.com/questions/45451032/extracting-fields-from-an-emails-based-on-values-in-a-database-as-training-set
"Why does the Stanford NER demo convert &#39;this year&#39; to 2017, whereas my CoreNLP server does not?","<p>I have set up a CoreNLP server and am using Stanford NER to extract time periods from sentences.</p>

<p>If I use the online interactive demo at corenlp.run to parse the sentence </p>

<blockquote>
  <p>'Last year something happened.'</p>
</blockquote>

<p><a href=""https://i.sstatic.net/dxdB9.png"" rel=""nofollow noreferrer"">it shows 'DATE' and '2016'</a>.
However, my own server, set up with the latest release of CoreNLP, <a href=""https://i.sstatic.net/JL9qw.png"" rel=""nofollow noreferrer"">only shows 'DATE'</a>. What's more, when I use Python Requests to query my server's API with the same sentence, the first two tokens in the response contain the fields <code>'timex': {'type': 'DATE','tid': 't1', 'altValue': 'THIS P1Y OFFSET P-1Y'}</code> and <code>'normalizedNER': 'THIS P1Y OFFSET P-1Y'</code>.</p>

<p>If I just have to deal with the fact that my output is not as good as the demo's, then where is the Stanford NER or timex3 documentation explaining what <code>THIS P1Y OFFSET P-1Y</code> means or describing what other possible responses I might get in the <code>normalizedNER</code> field?</p>

<p>Here is the entire API response</p>

<pre><code>[
{'word': 'Last', 'after': ' ', 'originalText': 'Last', 'timex': {'type': 'DATE', 'tid': 't1', 'altValue': 'THIS P1Y OFFSET P-1Y'}, 'pos': 'JJ', 'ner': 'DATE', 'lemma': 'last', 'normalizedNER': 'THIS P1Y OFFSET P-1Y', 'before': '', 'index': 1, 'characterOffsetBegin': 0, 'characterOffsetEnd': 4},
{'word': 'year', 'after': ' ', 'originalText': 'year', 'timex': {'type': 'DATE', 'tid': 't1', 'altValue': 'THIS P1Y OFFSET P-1Y'}, 'pos': 'NN', 'ner': 'DATE', 'lemma': 'year', 'normalizedNER': 'THIS P1Y OFFSET P-1Y', 'before': ' ', 'index': 2, 'characterOffsetBegin': 5, 'characterOffsetEnd': 9},
{'word': 'something', 'before': ' ', 'originalText': 'something', 'ner': 'O', 'lemma': 'something', 'after': ' ', 'characterOffsetEnd': 19, 'index': 3, 'characterOffsetBegin': 10, 'pos': 'NN'},
{'word': 'happened', 'before': ' ', 'originalText': 'happened', 'ner': 'O', 'lemma': 'happen', 'after': '', 'characterOffsetEnd': 28, 'index': 4, 'characterOffsetBegin': 20, 'pos': 'VBD'}, 
{'word': '.', 'before': '', 'originalText': '.', 'ner': 'O', 'lemma': '.', 'after': '', 'characterOffsetEnd': 29, 'index': 5, 'characterOffsetBegin': 28, 'pos': '.'}
]
</code></pre>
","stanford-nlp, named-entity-recognition","<p>If you closely look at the request made to corenlp server in the interactive demo, then you will see that current date is also sent as ""date"" parameter with the request. 
For eg. If your sentence is ""I went to school today."", then ""today"" has normalized ner is ""2017-19-09"" (current date).
If you dont pass the ""date"" parameter, ""today"" won't have the exact date as normalized ner.</p>

<p>Hope it makes sense.
<a href=""https://i.sstatic.net/qsGPu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qsGPu.png"" alt=""enter image description here""></a></p>
",0,0,313,2017-08-15 11:28:41,https://stackoverflow.com/questions/45691933/why-does-the-stanford-ner-demo-convert-this-year-to-2017-whereas-my-corenlp-s
NLTK tokenize but don&#39;t split named entities,"<p>I am working on a simple grammar based parser. For this I need to first tokenize the input. In my texts lots of cities appear (e.g., New York, San Francisco, etc.). When I just use the standard nltk word_tokenizer, all these cities are split.</p>

<pre><code>from nltk import word_tokenize
word_tokenize('What are we going to do in San Francisco?')
</code></pre>

<p>Current output:</p>

<pre><code>['What', 'are', 'we', 'going', 'to', 'do', 'in', 'San', 'Francisco', '?']
</code></pre>

<p>Desired output:</p>

<pre><code>['What', 'are', 'we', 'going', 'to', 'do', 'in', 'San Francisco', '?']
</code></pre>

<p>How can I tokenize such sentences without splitting named entities?</p>
","nltk, tokenize, named-entity-recognition","<p>Identify the named entities, then walk the result and join the chunked tokens together:</p>

<pre><code>&gt;&gt;&gt; from nltk import ne_chunk, pos_tag, word_tokenize
&gt;&gt;&gt; toks = word_tokenize('What are we going to do in San Francisco?')
&gt;&gt;&gt; chunks = ne_chunk(pos_tag(toks))
&gt;&gt;&gt; [ w[0] if isinstance(w, tuple) else "" "".join(t[0] for t in w) for w in chunks ]
['What', 'are', 'we', 'going', 'to', 'do', 'in', 'San Francisco', '?']
</code></pre>

<p>Each element of <code>chunks</code> is either a <code>(word, pos)</code> tuple or a <code>Tree()</code> containing the parts of the chunk.</p>
",3,2,958,2017-08-30 18:33:49,https://stackoverflow.com/questions/45967533/nltk-tokenize-but-dont-split-named-entities
Named Entity Extraction of dates,"<p>I am absolutely new to the NER and Extraction and programming in general. I am trying to figure out a way where I can extract due dates and start date of certain documents. Is there a way to do this? A place where I can start? I have been looking around but the problem  I run into is the same. Can extract dates but not whether the date is due or post. If it only has 1 date, is it post or due. Stuff like that. Any help would be appreciated.</p>

<p>Example: </p>

<p>""Essay on Medieval Asia was due on September 3rd.""</p>

<p>""Your last assignment that was given on April 6th was supposed to be submitted in 10 days.""</p>

<p>""The bid is due no later than a month from the date it was posted(today).""</p>
","nlp, named-entity-recognition","<p>The amount of possibilities to express dates in free text is huge. There are a few solutions:</p>

<ul>
<li><p>You can come with a set of regular expressions and try to parse them for yourself. </p></li>
<li><p>Another option is to train a supervised sequence classifier like CRF, if you have a document with dates annotated. </p></li>
<li><p>A third option, which can have quick results is to use this framework from Facebook research <a href=""https://github.com/facebookincubator/duckling"" rel=""noreferrer"" title=""Duckling"">https://github.com/facebookincubator/duckling</a>, it will identify expressions which are dates or time expressions, and it will even normalise them into a single unique date.</p></li>
<li><p>Yet another options is <a href=""https://github.com/comtravo/ctparse"" rel=""noreferrer"">ct-parse</a>, based on Duckling but a pure python package to parse time expressions from natural language in German and English.</p></li>
</ul>
",15,10,5649,2017-09-13 08:05:18,https://stackoverflow.com/questions/46192144/named-entity-extraction-of-dates
Dataset to train MITIE ner model,"<p>Is there any existing dataset with tagged entities to train MITIE ner model? 
I checked the link, <a href=""https://github.com/mit-nlp/MITIE/blob/master/examples/python/train_ner.py"" rel=""nofollow noreferrer"">https://github.com/mit-nlp/MITIE/blob/master/examples/python/train_ner.py</a> which trains the model with just two samples. Is there any existing dataset with tagged entities to train ?</p>
","python, named-entity-recognition, rasa-nlu","<p>I've been looking for something like this, too. Simply for a ""generic"" (and hence not very useful) NLU backend. The only thing I've found so far is a trained model with 9 news categories (not very generic). See blog post here: <a href=""http://eric-yuan.me/ner_1/"" rel=""nofollow noreferrer"">http://eric-yuan.me/ner_1/</a></p>

<p>If you have the option to switch NERs, spaCy has a trained model available by default. Its visualisation front end can be found by google ""displacy""</p>

<p>If you find anything else, let me know!</p>

<p><strong>EDIT:</strong> Spent the day looking into this and I think I've found what you're after. If you go to <a href=""https://github.com/mit-nlp/MITIE/releases"" rel=""nofollow noreferrer"">https://github.com/mit-nlp/MITIE/releases</a> there you'll find MITIE's own NER model trained on Wikipedia, Freebase, etc. The actual training dataset is there too. The README on their github page provides example on how to use the pre-trained model. You can also investigate the ner.py file in the examples folder to see how to use the pre-trained model in python code. </p>
",2,1,804,2017-10-06 09:23:28,https://stackoverflow.com/questions/46602495/dataset-to-train-mitie-ner-model
MITIE ner model,"<p>I have been exploring on using pretrained MITIE models for named entity extraction. Is there anyway I can look at their actual ner model rather than using a pretrained model? Is the model available as open source?</p>
","python, model, named-entity-recognition, rasa-nlu","<blockquote>
  <p><strong>Setting things up:</strong></p>
  
  <p>For starters, you can download the <a href=""https://github.com/mit-nlp/MITIE/releases/download/v0.4/MITIE-models-v0.2.tar.bz2"" rel=""noreferrer"">English Language Model</a> which
  contains Corpus of annotated text from a huge dump in a file called
  <strong>total_word_feature_extractor.dat</strong>.</p>
  
  <p>After that, download/clone the <a href=""https://github.com/mit-nlp/MITIE"" rel=""noreferrer"">MITIE-Master Project</a> from their
  official Git.</p>
  
  <p>If you are running Windows O.S then download <a href=""https://cmake.org/files/v3.10/cmake-3.10.0-win64-x64.msi"" rel=""noreferrer"">CMake</a>.</p>
  
  <p>If you are running a x64 based Windows O.S, then install Visual Studio
  2015 Community edition for the C++ compiler.</p>
  
  <p>After downloading, the above, extract all of them into a folder.</p>
</blockquote>

<p><a href=""https://i.sstatic.net/dvnt8.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/dvnt8.png"" alt=""The project structure will look something like this""></a></p>

<p>Open Developer Command Prompt for VS 2015 from Start > All Apps > Visual Studio, and navigate to the tools folder, you will see 5 sub-folders inside.</p>

<p><a href=""https://i.sstatic.net/x2wgU.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/x2wgU.png"" alt=""enter image description here""></a></p>

<p>The next step is to build ner_conll, ner_stream, train_freebase_relation_detector and wordrep packages, by using following Cmake commands in the Visual Studio Developer Command Prompt.</p>

<p>Something like this:</p>

<p><a href=""https://i.sstatic.net/5hKVs.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/5hKVs.png"" alt=""enter image description here""></a></p>

<p>For ner_conll:</p>

<pre><code>cd ""C:\Users\xyz\Documents\MITIE-master\tools\ner_conll""
</code></pre>

<p>i) <code>mkdir build</code>
ii) <code>cd build</code>
iii) <code>cmake -G ""Visual Studio 14 2015 Win64"" ..</code>
iv) <code>cmake --build . --config Release --target install</code> </p>

<p>For ner_stream:</p>

<pre><code>cd ""C:\Users\xyz\Documents\MITIE-master\tools\ner_stream""
</code></pre>

<p>i) <code>mkdir build</code>
ii) <code>cd build</code>
iii) <code>cmake -G ""Visual Studio 14 2015 Win64"" ..</code>
iv) <code>cmake --build . --config Release --target install</code> </p>

<p>For train_freebase_relation_detector:</p>

<pre><code>cd ""C:\Users\xyz\Documents\MITIE-master\tools\train_freebase_relation_detector""
</code></pre>

<p>i) <code>mkdir build</code>
ii) <code>cd build</code>
iii) <code>cmake -G ""Visual Studio 14 2015 Win64"" ..</code>
iv) <code>cmake --build . --config Release --target install</code> </p>

<p>For wordrep:</p>

<pre><code>cd ""C:\Users\xyz\Documents\MITIE-master\tools\wordrep""
</code></pre>

<p>i) <code>mkdir build</code>
ii) <code>cd build</code>
iii) <code>cmake -G ""Visual Studio 14 2015 Win64"" ..</code>
iv) <code>cmake --build . --config Release --target install</code></p>

<p>After you build them you will get some 150-160 warnings, don't worry.</p>

<p>Now, navigate to the <code>""C:\Users\xyz\Documents\MITIE-master\examples\cpp\train_ner""</code></p>

<p>Make a JSON file ""data.json"" using Visual Studio Code for annotating text manually, something like this:</p>

<pre><code>{
  ""AnnotatedTextList"": [
    {
      ""text"": ""I want to travel from New Delhi to Bangalore tomorrow."",
      ""entities"": [
        {
          ""type"": ""FromCity"",
          ""startPos"": 5,
          ""length"": 2
        },
        {
          ""type"": ""ToCity"",
          ""startPos"": 8,
          ""length"": 1
        },
        {
          ""type"": ""TimeOfTravel"",
          ""startPos"": 9,
          ""length"": 1
        }
      ]
    }
  ]
}
</code></pre>

<p>You can add more utterances and annotate them, the more the training data the better is the prediction accuracy. </p>

<p>This annotated JSON can also be created via front-end tools like jQuery or Angular. But for brevity, I have created them by hand.</p>

<p>Now, to parse the our Annotated JSON file and pass it to ner_training_instance's add_entity method.</p>

<p>But C++ doesn't support reflection to deserialize JSON, that's why you can use this library <a href=""https://github.com/Tencent/rapidjson"" rel=""noreferrer"">Rapid JSON Parser</a>. Download the package from their Git page and place it under <code>""C:\Users\xyz\Documents\MITIE-master\mitielib\include\mitie""</code>.</p>

<p>Now we have to customize the train_ner_example.cpp file so as to parse our annotated custom entities JSON and pass it to MITIE to train.</p>

<pre><code>#include ""mitie\rapidjson\document.h""
#include ""mitie\ner_trainer.h""

#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;list&gt;
#include &lt;tuple&gt;
#include &lt;string&gt;
#include &lt;map&gt;
#include &lt;sstream&gt;
#include &lt;fstream&gt;

using namespace mitie;
using namespace dlib;
using namespace std;
using namespace rapidjson;

string ReadJSONFile(string FilePath)
{
    ifstream file(FilePath);
    string test;
    cout &lt;&lt; ""path: "" &lt;&lt; FilePath;
    try
    {
        std::stringstream buffer;
        buffer &lt;&lt; file.rdbuf();
        test = buffer.str();
        cout &lt;&lt; test;
        return test;
    }
    catch (exception &amp;e)
    {
        throw std::exception(e.what());
    }
}

//Helper function to tokenize a string based on multiple delimiters such as ,.;:- or whitspace
std::vector&lt;string&gt; SplitStringIntoMultipleParameters(string input, string delimiter)
{
    std::stringstream stringStream(input);
    std::string line;

    std::vector&lt;string&gt; TokenizedStringVector;

    while (std::getline(stringStream, line))
    {
        size_t prev = 0, pos;
        while ((pos = line.find_first_of(delimiter, prev)) != string::npos)
        {
            if (pos &gt; prev)
                TokenizedStringVector.push_back(line.substr(prev, pos - prev));
            prev = pos + 1;
        }
        if (prev &lt; line.length())
            TokenizedStringVector.push_back(line.substr(prev, string::npos));
    }
    return TokenizedStringVector;
}

//Parse the JSON and store into appropriate C++ containers to process it.
std::map&lt;string, list&lt;tuple&lt;string, int, int&gt;&gt;&gt; FindUtteranceTuple(string stringifiedJSONFromFile)
{
    Document document;
    cout &lt;&lt; ""stringifiedjson : "" &lt;&lt; stringifiedJSONFromFile;
    document.Parse(stringifiedJSONFromFile.c_str());

    const Value&amp; a = document[""AnnotatedTextList""];
    assert(a.IsArray());

    std::map&lt;string, list&lt;tuple&lt;string, int, int&gt;&gt;&gt; annotatedUtterancesMap;

    for (int outerIndex = 0; outerIndex &lt; a.Size(); outerIndex++)
    {
        assert(a[outerIndex].IsObject());
        assert(a[outerIndex][""entities""].IsArray());
        const Value &amp;entitiesArray = a[outerIndex][""entities""];

        list&lt;tuple&lt;string, int, int&gt;&gt; entitiesTuple;

        for (int innerIndex = 0; innerIndex &lt; entitiesArray.Size(); innerIndex++)
        {
            entitiesTuple.push_back(make_tuple(entitiesArray[innerIndex][""type""].GetString(), entitiesArray[innerIndex][""startPos""].GetInt(), entitiesArray[innerIndex][""length""].GetInt()));
        }

        annotatedUtterancesMap.insert(pair&lt;string, list&lt;tuple&lt;string, int, int&gt;&gt;&gt;(a[outerIndex][""text""].GetString(), entitiesTuple));
    }

    return annotatedUtterancesMap;
}

int main(int argc, char **argv)
{

    try {

        if (argc != 3)
        {
            cout &lt;&lt; ""You must give the path to the MITIE English total_word_feature_extractor.dat file."" &lt;&lt; endl;
            cout &lt;&lt; ""So run this program with a command like: "" &lt;&lt; endl;
            cout &lt;&lt; ""./train_ner_example ../../../MITIE-models/english/total_word_feature_extractor.dat"" &lt;&lt; endl;
            return 1;
        }

        else
        {
            string filePath = argv[2];
            string stringifiedJSONFromFile = ReadJSONFile(filePath);

            map&lt;string, list&lt;tuple&lt;string, int, int&gt;&gt;&gt; annotatedUtterancesMap = FindUtteranceTuple(stringifiedJSONFromFile);


            std::vector&lt;string&gt; tokenizedUtterances;
            ner_trainer trainer(argv[1]);

            for each (auto item in annotatedUtterancesMap)
            {
                tokenizedUtterances = SplitStringIntoMultipleParameters(item.first, "" "");
                mitie::ner_training_instance *currentInstance = new mitie::ner_training_instance(tokenizedUtterances);
                for each (auto entity in item.second)
                {
                    currentInstance -&gt; add_entity(get&lt;1&gt;(entity), get&lt;2&gt;(entity), get&lt;0&gt;(entity).c_str());
                }
                // trainingInstancesList.push_back(currentInstance);
                trainer.add(*currentInstance);
                delete currentInstance;
            }


            trainer.set_num_threads(4);

            named_entity_extractor ner = trainer.train();

            serialize(""new_ner_model.dat"") &lt;&lt; ""mitie::named_entity_extractor"" &lt;&lt; ner;

            const std::vector&lt;std::string&gt; tagstr = ner.get_tag_name_strings();
            cout &lt;&lt; ""The tagger supports "" &lt;&lt; tagstr.size() &lt;&lt; "" tags:"" &lt;&lt; endl;
            for (unsigned int i = 0; i &lt; tagstr.size(); ++i)
                cout &lt;&lt; ""\t"" &lt;&lt; tagstr[i] &lt;&lt; endl;
            return 0;
        }
    }

    catch (exception &amp;e)
    {
        cerr &lt;&lt; ""Failed because: "" &lt;&lt; e.what();
    }
}
</code></pre>

<p>The add_entity accepts 3 parameters, the tokenized string which can be a vector, the custom entity type name ,the start index of a word in a sentence and the range of the word.</p>

<p>Now we have to build the ner_train_example.cpp by using following commands in Developer Command Prompt Visual Studio.</p>

<p>1) <code>cd ""C:\Users\xyz\Documents\MITIE-master\examples\cpp\train_ner""</code>
2) <code>mkdir build</code>
3) <code>cd build</code>
4) <code>cmake -G ""Visual Studio 14 2015 Win64"" ..</code>
5) <code>cmake --build . --config Release --target install</code>
6) <code>cd Release</code></p>

<p>7) <code>train_ner_example ""C:\\Users\\xyz\\Documents\\MITIE-master\\MITIE-models\\english\\total_word_feature_extractor.dat"" ""C:\\Users\\xyz\\Documents\\MITIE-master\\examples\\cpp\\train_ner\\data.json""</code></p>

<p>On successfully executing the above we will get a new_ner_model.dat file which is a serialized and trained version of our utterances.</p>

<p>Now, that .dat file can be passed to RASA or used standalone.</p>

<p>For passing it to RASA:</p>

<p>Make the config.json file as follows:</p>

<pre><code>{
    ""project"": ""demo"",
    ""path"": ""C:\\Users\\xyz\\Desktop\\RASA\\models"",
    ""response_log"": ""C:\\Users\\xyz\\Desktop\\RASA\\logs"",
    ""pipeline"": [""nlp_mitie"", ""tokenizer_mitie"", ""ner_mitie"", ""ner_synonyms"", ""intent_entity_featurizer_regex"", ""intent_classifier_mitie""], 
    ""data"": ""C:\\Users\\xyz\\Desktop\\RASA\\data\\examples\\rasa.json"",
    ""mitie_file"" : ""C:\\Users\\xyz\\Documents\\MITIE-master\\examples\\cpp\\train_ner\\Release\\new_ner_model.dat"",
    ""fixed_model_name"": ""demo"",
    ""cors_origins"": [""*""],
    ""aws_endpoint_url"": null,
    ""token"": null,
    ""num_threads"": 2,
    ""port"": 5000
}
</code></pre>
",5,1,1903,2017-10-11 09:29:34,https://stackoverflow.com/questions/46684633/mitie-ner-model
Calculating confidence score for Entity in NLP Named-entity recognition,"<p>I am working on named-entity extraction from documents(pdfs). Each pdf contains set of entities (nearly 16 different type entities)</p>

<p>Here are my steps to build the NLP and ML models:</p>

<ul>
<li><code>Step 1</code> : Parsed documents. Got nearly 2 Million tokens (words). Used these words and CBOW method for building word2vec model.</li>
<li><code>Step 2</code> : By used word2vec model, generated vectors for words in douments.</li>
<li><code>Step 3</code> : As per the domain, i labeled words(vectors) for training, validation and testing.</li>
<li><code>Step 4</code> : With labeled data, train the Neural Network model.</li>
<li><code>Step 5</code>: Once model got build, given testing data (words) to the model. Got 85% accuracy.</li>
</ul>

<p>Till now everything going good. But problem is in next step. :(</p>

<ul>
<li><code>Step 6</code> : Now i want to make entities with confidence score from words which are classified from the trained model. </li>
</ul>

<p>Neural network model using <code>SOFTMAX</code> to classify input. From this model getting score for each word.</p>

<p>But my question is, my entities contains minimum 3 words. How can i calculate confidence score for generated entity.</p>

<p>right now i am using <code>P(entity) = P(w1)*P(w2)*(w3)</code>  if entity has three words.</p>

<p>Kindly help me. this approach wont make sense all the time.</p>

<p>suppose, if model predict only two words in entity then entity confidence will be  <code>P(entity) = P(w1)*P(w2)</code>.</p>

<p>And if model predict only one word in a entity then <code>P(entity) = P(w1)</code>. :(</p>
","machine-learning, nlp, named-entity-recognition","<p>Why not P(entity) = P(w1)+P(w2)+P(w3) ?
if you need a normalized number (0-1) and assuming that P(w) has a 0-1 range make it: P(entity) = (P(w1)+P(w2)+P(w3)) / 3</p>

<p>For a better score, you should calculate the information content of each word. A common word should contribute less: <a href=""https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-6-S1-S21"" rel=""nofollow noreferrer"">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-6-S1-S21</a> </p>
",0,0,1987,2017-10-14 11:33:08,https://stackoverflow.com/questions/46744058/calculating-confidence-score-for-entity-in-nlp-named-entity-recognition
OpenNLP NameFinder Custom Feature generation,"<p>I would like to train a own model to use in <code>OpenNLP NameFinder</code>.</p>

<p>Searched a lot and finally came up with a working code to train a model, but its not accurate. The documentation also says you need 15000 sentences to have a accurate model. </p>

<p>So you need to have 15000 sentences with a good context and at the name every time <code>&lt;START&gt; &lt;END&gt;</code>. To do this you would have to spend hours
days/months writing a <code>.txt file.</code></p>

<p>I did some futher search to train a model without writing all by your self and came on <code>Custom Feature generation of OpenNLP</code>.
<a href=""http://opennlp.apache.org/docs/1.8.1/manual/opennlp.html#tools.namefind.training.tool"" rel=""nofollow noreferrer"">http://opennlp.apache.org/docs/1.8.1/manual/opennlp.html#tools.namefind.training.tool</a> </p>

<p>But it says not much about how to use it. The documentation gives this :</p>

<pre><code>AdaptiveFeatureGenerator featureGenerator = new CachedFeatureGenerator(
     new AdaptiveFeatureGenerator[]{
       new WindowFeatureGenerator(new TokenFeatureGenerator(), 2, 2),
       new WindowFeatureGenerator(new TokenClassFeatureGenerator(true), 2, 2),
       new OutcomePriorFeatureGenerator(),
       new PreviousMapFeatureGenerator(),
       new BigramNameFeatureGenerator(),
       new SentenceFeatureGenerator(true, false)
       });
</code></pre>

<p>Does somebody knows how to use this? Or much better, is someone already successful to train his own model for <code>OpenNLP NER</code> without typing him self the data set?</p>

<p>Thanks in advance.</p>
","java, machine-learning, opennlp, named-entity-recognition","<p>You still need annotated training text. The feature generators are used during the training to create a better model. Unfortunately, there is no substitute for annotated training text.</p>
",0,2,481,2017-10-25 07:45:13,https://stackoverflow.com/questions/46926729/opennlp-namefinder-custom-feature-generation
How to get spaCy NER probability,"<p>I want to combine spaCy's NER engine with a separate NER engine (a BoW model). I'm currently comparing outputs from the two engines, trying to figure out what the optimal combination of the two would be. Both perform decently, but quite often spaCy finds entities that the BoW engine misses, and vice versa. What I would like is to access a probability score (or something similar) from spaCy whenever it finds an entity that is not found by the BoW engine. Can I get spaCy to print out its own probability score for a given entity it has found? As in, ""Hi, I'm spaCy. I've found this token (or combination of tokens) that I'm X% certain is an entity of type BLAH."" I want to know that number X every time spaCy finds an entity. I imagine there must be such a number somewhere internally in spaCy's NER engine, plus a threshold value below which the possible entity is not flagged as an entity, and I'd like to know how to get my hands on that number. Thanks in advance.</p>
","named-entity-recognition, spacy","<p>Actually, there is an <a href=""https://github.com/explosion/spaCy/issues/881"" rel=""noreferrer"">issue</a> for that.</p>

<p>The author of the library, suggests there (among others) the following solution:</p>

<blockquote>
  <ol>
  <li>Beam search with global objective.
  This is the standard solution: use a global objective, so that the parser model is trained to prefer parses that are better overall. Keep N different candidates, and output the best one. This can be used to support confidence by looking at the alternate analyses in the beam. If an entity occurs in every analysis, the NER is more confident it's correct.</li>
  </ol>
</blockquote>

<p>Code:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
import sys
from collections import defaultdict

nlp = spacy.load('en')
text = u'Will Japan join the European Union? If yes, we should \ 
move to United States. Fasten your belts, America we are coming'


with nlp.disable_pipes('ner'):
    doc = nlp(text)

threshold = 0.2
(beams, somethingelse) = nlp.entity.beam_parse([ doc ], beam_width = 16, beam_density = 0.0001)

entity_scores = defaultdict(float)
for beam in beams:
    for score, ents in nlp.entity.moves.get_beam_parses(beam):
        for start, end, label in ents:
            entity_scores[(start, end, label)] += score

print ('Entities and scores (detected with beam search)')
for key in entity_scores:
    start, end, label = key
    score = entity_scores[key]
    if ( score &gt; threshold):
        print ('Label: {}, Text: {}, Score: {}'.format(label, doc[start:end], score))
</code></pre>

<p>Sample output: </p>

<blockquote>
  <p>Entities and scores (detected with beam search) </p>
  
  <p>Label: GPE, Text: Japan, Score: 0.9999999999999997 </p>
  
  <p>Label: GPE, Text: America, Score: 0.9991664575947963 </p>
</blockquote>

<p><strong>Important note:</strong> The outputs you will get here are probably different from the outputs you would get using the Standard NER and not the beam search alternative. However, the beam search alternative provides you a metric of confidence that as I understand from your question is useful for your case.</p>

<p>Outputs with Standard NER for this example:</p>

<blockquote>
  <p>Label: GPE, Text: Japan</p>
  
  <p>Label: ORG, Text: the European Union </p>
  
  <p>Label: GPE, Text: United States </p>
  
  <p>Label: GPE, Text: America</p>
</blockquote>
",18,21,9457,2017-10-25 14:02:27,https://stackoverflow.com/questions/46934523/how-to-get-spacy-ner-probability
Stanford CRFClassifier performance evaluation output,"<p>I'm following this FAQ <a href=""https://nlp.stanford.edu/software/crf-faq.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/crf-faq.shtml</a> for training my own classifier and I noticed that the performance evaluation output does not match the results (or at least not in the way I expect).
Specifically this section</p>

<p><code>CRFClassifier tagged 16119 words in 1 documents at 13824.19 words per second.
         Entity P       R       F1      TP  FP  FN
       MYLABEL  1.0000  0.9961  0.9980  255 0   1
         Totals 1.0000  0.9961  0.9980  255 0   1
</code></p>

<p>I expect <code>TP</code> to be all instances where the predicted label matched the golden label, <code>FP</code> to be all instances where <code>MYLABEL</code> was predicted but the golden label was <code>O</code>, <code>FN</code> to be all instances where <code>O</code> was predicted but the golden was <code>MYLABEL</code>.</p>

<p>If I calculate those numbers myself from the output of the program, I get completely different numbers with no relation to what the program prints. I've tried this with various test files. 
I'm using <code>Stanford NER - v3.7.0 - 2016-10-31</code></p>

<p>Am I missing something?</p>
","stanford-nlp, named-entity-recognition, crf","<p>The F1 scores are over entities not labels.</p>

<p>Example:</p>

<pre><code>(Joe, PERSON) (Smith, PERSON) (went, O) (to, O) (Hawaii, LOCATION) (., O).
</code></pre>

<p>In this example there are two possible entities:</p>

<pre><code>Joe Smith   PERSON
Hawaii      LOCATION
</code></pre>

<p>Entities are created by taking all adjacent tokens with the same label.  (Unless you use a more complicated BIO labeling scheme ; BIO schemes have tags like I-PERSON and B-PERSON to indicate whether a token is the beginning of an entity, etc...).</p>
",1,0,343,2017-10-25 19:03:00,https://stackoverflow.com/questions/46940195/stanford-crfclassifier-performance-evaluation-output
OpenNLP model builder addon doesnt continue,"<p>I'm using the <code>model builder addon</code> for <code>OpenNLP</code> to create a better NER model.
According to this <a href=""https://stackoverflow.com/questions/21617540/how-to-update-an-existing-named-entity-recognition-model-rather-than-creatin"">post</a>, I have used the code posted by <a href=""https://stackoverflow.com/users/3059517/markg"">markg</a> :</p>

<pre><code>public class ModelBuilderAddonUse {

  private static List&lt;String&gt; getSentencesFromSomewhere() throws Exception 
  {
      List&lt;String&gt; list = new ArrayList&lt;String&gt;();
      BufferedReader reader = new BufferedReader(new FileReader(""D:\\Work\\workspaces\\default\\UpdateModel\\documentrequirements.docx""));
      String line;
      while ((line = reader.readLine()) != null) 
      {
          list.add(line);
      }
      reader.close();
      return list;

    }

  public static void main(String[] args) throws Exception {
    /**
     * establish a file to put sentences in
     */
    File sentences = new File(""D:\\Work\\workspaces\\default\\UpdateModel\\sentences.text"");

    /**
     * establish a file to put your NER hits in (the ones you want to keep based
     * on prob)
     */
    File knownEntities = new File(""D:\\Work\\workspaces\\default\\UpdateModel\\knownentities.txt"");

    /**
     * establish a BLACKLIST file to put your bad NER hits in (also can be based
     * on prob)
     */
    File blacklistedentities = new File(""D:\\Work\\workspaces\\default\\UpdateModel\\blentities.txt"");

    /**
     * establish a file to write your annotated sentences to
     */
    File annotatedSentences = new File(""D:\\Work\\workspaces\\default\\UpdateModel\\annotatedSentences.txt"");

    /**
     * establish a file to write your model to
     */
    File theModel = new File(""D:\\Work\\workspaces\\default\\UpdateModel\\nl-ner-person.bin"");


//------------create a bunch of file writers to write your results and sentences to a file

    FileWriter sentenceWriter = new FileWriter(sentences, true);
    FileWriter blacklistWriter = new FileWriter(blacklistedentities, true);
    FileWriter knownEntityWriter = new FileWriter(knownEntities, true);

//set some thresholds to decide where to write hits, you don't have to use these at all...
    double keeperThresh = .95;
    double blacklistThresh = .7;


    /**
     * Load your model as normal
     */
    TokenNameFinderModel personModel = new TokenNameFinderModel(new File(""D:\\Work\\workspaces\\default\\UpdateModel\\nl-ner-person.bin""));
    NameFinderME personFinder = new NameFinderME(personModel);
    /**
     * do your normal NER on the sentences you have
     */
   for (String s : getSentencesFromSomewhere()) {
      sentenceWriter.write(s.trim() + ""\n"");
      sentenceWriter.flush();

      String[] tokens = s.split("" "");//better to use a tokenizer really
      Span[] find = personFinder.find(tokens);
      double[] probs = personFinder.probs();
      String[] names = Span.spansToStrings(find, tokens);
      for (int i = 0; i &lt; names.length; i++) {
        //YOU PROBABLY HAVE BETTER HEURISTICS THAN THIS TO MAKE SURE YOU GET GOOD HITS OUT OF THE DEFAULT MODEL
        if (probs[i] &gt; keeperThresh) {
          knownEntityWriter.write(names[i].trim() + ""\n"");
        }
        if (probs[i] &lt; blacklistThresh) {
          blacklistWriter.write(names[i].trim() + ""\n"");
        }
      }
      personFinder.clearAdaptiveData();
      blacklistWriter.flush();
      knownEntityWriter.flush();
    }
    //flush and close all the writers
    knownEntityWriter.flush();
    knownEntityWriter.close();
    sentenceWriter.flush();
    sentenceWriter.close();
    blacklistWriter.flush();
    blacklistWriter.close();

    /**
     * THIS IS WHERE THE ADDON IS GOING TO USE THE FILES (AS IS) TO CREATE A NEW MODEL. YOU SHOULD NOT HAVE TO RUN THE FIRST PART AGAIN AFTER THIS RUNS, JUST NOW PLAY WITH THE
     * KNOWN ENTITIES AND BLACKLIST FILES AND RUN THE METHOD BELOW AGAIN UNTIL YOU GET SOME DECENT RESULTS (A DECENT MODEL OUT OF IT).
     */
    DefaultModelBuilderUtil.generateModel(sentences, knownEntities, blacklistedentities, theModel, annotatedSentences, ""person"", 3);


  }
}
</code></pre>

<p>It also runs, but my output quits at :</p>

<pre><code>    annotated sentences: 1862
    knowns: 58
    Building Model using 1862 annotations
    reading training data...
</code></pre>

<p>But in the example in the <a href=""https://stackoverflow.com/questions/21617540/how-to-update-an-existing-named-entity-recognition-model-rather-than-creatin"">post</a> it should go futher like this :</p>

<pre><code>Indexing events using cutoff of 5

    Computing event counts...  done. 561755 events
    Indexing...  done.
Sorting and merging events... done. Reduced 561755 events to 127362.
Done indexing.
Incorporating indexed data for training...  
done.
    Number of Event Tokens: 127362
        Number of Outcomes: 3
      Number of Predicates: 106490
...done.
</code></pre>

<p>Can anyone help me to fix this problem, so I can generate a model?
I have searched realy a lot but cant find any good documutation about it.
Would really appreciat it, thanks.</p>
","java, machine-learning, opennlp, named-entity-recognition","<p>Correct the path to your training data file like this:</p>

<pre><code>File sentences = new File(""D:/Work/workspaces/default/UpdateModel/sentences.text"");
</code></pre>

<p>instead of </p>

<pre><code>File sentences = new File(""D:\\Work\\workspaces\\default\\UpdateModel\\sentences.text"");
</code></pre>

<p><strong>Update</strong></p>

<p>This is how is used, by adding the files to the project folder. Try it like this -</p>

<pre><code>File sentences = new File(""src/training/resources/CreateModel/sentences.txt"");
</code></pre>

<p><a href=""https://github.com/caffeinator13/srae/blob/master/src/training/CreateModel.java"" rel=""nofollow noreferrer"">Check my respository for reference on Github</a></p>

<p>This should help.</p>
",0,1,387,2017-10-26 09:02:36,https://stackoverflow.com/questions/46950036/opennlp-model-builder-addon-doesnt-continue
"Stanford NLP NER, Sentiment, SUTime Performance Issue","<p>The text in the main method seem to be taking more than 2 seconds to return NER. I am not an expert in NLP and this code is not at all scalable. I have added comments in 2 places where the bottleneck i have identified. Can you please suggest improvements to improve the performance of the program. </p>

<p>Thanks.</p>

<pre><code> public class NERSentimentUtil
{
private static final Logger logger = Logger.getLogger(NERSentimentUtil.class);

private static final String serializedClassifier7 = ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz"";
private static final String serializedClassifier4 = ""edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz"";
private static final String serializedClassifier3 = ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"";

private static NERClassifierCombiner ncc;
private static StanfordCoreNLP pipeline;

static
{       
    try
    {
        ncc = new NERClassifierCombiner(serializedClassifier3,serializedClassifier4,serializedClassifier7);
    } catch (IOException e) {
        e.printStackTrace();
        logger.error(e);
    }
}

static
{               
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment, sutime"");
    /*props.setProperty(""ner.useSUTime"", ""0"");*/

    String defs_sutime = ""/edu/stanford/nlp/models/sutime/defs.sutime.txt"";
    String holiday_sutime = ""/edu/stanford/nlp/models/sutime/english.holidays.sutime.txt"";
    String _sutime = ""/edu/stanford/nlp/models/sutime/english.sutime.txt"";

    String sutimeRules = defs_sutime + "","" + holiday_sutime + "","" + _sutime;
    props.setProperty(""ner.useSUTime"", ""true"");
    props.setProperty(""-sutime.rules"", sutimeRules);
    props.setProperty(""sutime.binders"", ""0"");
    props.setProperty(""sutime.markTimeRanges"", ""false"");
    props.setProperty(""sutime.includeRange"", ""false"");
    props.setProperty(""customAnnotatorClass.sutime"", ""edu.stanford.nlp.time.TimeAnnotator"");
    props.setProperty(""parse.maxlen"", ""20"");
    //props.setProperty(""ner.applyNumericClassifiers"", ""false"");
    //props.setProperty(""nthreads"", ""16"");
    //props.setProperty(""threads"", ""16"");
    //props.setProperty(""parse.nthreads"",""16"");
    //props.setProperty(""ssplit.eolonly"",""true"");

    props.setProperty(""-parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    RedwoodConfiguration.current().clear().apply();
    pipeline = new StanfordCoreNLP(props);
    //RedwoodConfiguration.empty().capture(System.err).apply();
}

//A sentiment score of 0 or 1 is negative, 2 neutral and 3 or 4 positive.
private static int getScore(int score)
{
    if(score&lt;2)
        return -1;
    else if(score==2)
        return 0;
    else
        return 1;       
}

public static HashMap&lt;String,Object&gt; getStanford(String s, long dateString)//""2013-07-14""
{   
    int finalScore =0;

    HashMap&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();

    HashMap&lt;String, Integer&gt; dateMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; dateCountMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, String&gt; dateSentenceMap = new HashMap&lt;String, String&gt;();

    HashMap&lt;String, Integer&gt; personMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; personCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Integer&gt; orgMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; orgCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Integer&gt; locationMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; locationCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Article_Location&gt; locationArticleMap = new HashMap&lt;String, Article_Location&gt;();

    ArrayList&lt;Articel_Ner&gt; organisationlist = new ArrayList&lt;Articel_Ner&gt;();
    ArrayList&lt;Articel_Ner&gt; personlist = new ArrayList&lt;Articel_Ner&gt;();
    ArrayList&lt;Artilcle_Ner_Date&gt; datelist = new ArrayList&lt;Artilcle_Ner_Date&gt;();
    ArrayList&lt;Article_NerLocation&gt; locationList = new ArrayList&lt;Article_NerLocation&gt;();     

    try
    {
        Annotation annotation = pipeline.process(s);//1/3 rd time is taken up by this line

        List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);

        for (CoreMap sentence : sentences)
        {
             String str = sentence.toString();

             int score = getSentiment(sentence);

             finalScore+=score;
             boolean dFlag = true;

             List&lt;Triple&lt;String,Integer,Integer&gt;&gt; triples = ncc.classifyToCharacterOffsets(str);

             for (Triple&lt;String,Integer,Integer&gt; trip : triples)
             {
                 String ne = trip.first();
                 String word = str.substring(trip.second(), trip.third).toLowerCase();

                 switch(ne)
                 {
                    case ""LOCATION"":                        
                        extractLocation(locationMap, locationCountMap, locationArticleMap, score, word);
                        break;

                    case ""ORGANIZATION"":                        
                        extractOrg(orgMap, orgCountMap, score, word);                       
                        break;

                    case ""PERSON"":                      
                        extractPerson(personMap, personCountMap, score, word);
                        break;

                    case ""DATE"":
                        if(dFlag)
                        {
                         extractSUDate(dateString, dateMap, dateCountMap, dateSentenceMap, str, score);
                         dFlag = false;
                        }
                        break;

                    default:
                        break;
                 }
             }
        }
            //2/3rd of the time taken by these 4 methods:: can be obtimized
        mapDate(dateMap, dateCountMap, dateSentenceMap, datelist);
        mapLocation(locationMap, locationCountMap, locationArticleMap, locationList);   
        mapOrg(orgMap, orgCountMap, organisationlist);  
        mapPerson(personMap, personCountMap, personlist);
        //
    }
    catch(Exception e)
    {
        logger.error(e);
        logger.error(s);
        e.printStackTrace();
    }

    if(finalScore&gt;0)
        finalScore = 1;
    else if(finalScore&lt;0)
        finalScore = -1;
    else
        finalScore = 0;

    map.put(""ORGANISATION"", organisationlist);
    map.put(""PERSON"", personlist);
    map.put(""DATE"", datelist);
    map.put(""LOCATION"", locationList);
    map.put(""SENTIMENT"", finalScore);

    return map;
}

private static void extractPerson(HashMap&lt;String, Integer&gt; personMap, HashMap&lt;String, Integer&gt; personCountMap,
        int score, String word)
{       
    if(personMap.get(word)!=null)
    {
        personMap.put(word, personMap.get(word)+score);
        personCountMap.put(word, personCountMap.get(word)+1);
    }
    else
    {
        personMap.put(word, score);
        personCountMap.put(word, 1);
        //personSentenceMap.put(pname, str);
    }   
}

private static void extractOrg(HashMap&lt;String, Integer&gt; orgMap, HashMap&lt;String, Integer&gt; orgCountMap,
        int score, String word)
{
    if(orgMap.get(word)!=null)
    {
        orgMap.put(word, orgMap.get(word)+score);
        orgCountMap.put(word, orgCountMap.get(word)+1);                             
    }
    else
    {
        orgMap.put(word, score);
        orgCountMap.put(word, 1);
        //orgSentenceMap.put(oname, str);
    }
}

private static void extractLocation(HashMap&lt;String, Integer&gt; locationMap,
        HashMap&lt;String, Integer&gt; locationCountMap,
        HashMap&lt;String, Article_Location&gt; locationArticleMap,
        int score,
        String word)
{
    if(locationMap.get(word)!=null)
    {
        locationMap.put(word, locationMap.get(word)+score);
        locationCountMap.put(word, locationCountMap.get(word)+1);                               
    }
    else
    {
        Article_Location articleLocation = LocationUtil.getLocation(word);

        locationMap.put(word, score);
        locationCountMap.put(word, 1);
        locationArticleMap.put(word, articleLocation);
    }   
}

private static void extractSUDate(long dateString,
        HashMap&lt;String, Integer&gt; dateMap,
        HashMap&lt;String, Integer&gt; dateCountMap,
        HashMap&lt;String, String&gt; dateSentenceMap, 
        String str,
        int score) {

    Annotation dateAnnotation = new Annotation(str);
    dateAnnotation.set(CoreAnnotations.DocDateAnnotation.class, FormatUtil.getDate(dateString));
    pipeline.annotate(dateAnnotation);

    for(CoreMap timex:dateAnnotation.get(TimeAnnotations.TimexAnnotations.class))
    {
        TimeExpression timeExpression = timex.get(TimeExpression.Annotation.class);

         if(timeExpression!=null &amp;&amp; timeExpression.getTemporal()!=null &amp;&amp;
            timeExpression.getTemporal().getTimexValue()!=null)
         {           
             String word = checkDate(timeExpression.getTemporal().getTimexValue());

             if(word!=null)
             {
                 if(dateMap.get(word)!=null)
                 {
                     dateMap.put(word, dateMap.get(word)+score);
                     dateCountMap.put(word, dateCountMap.get(word)+1);
                     dateSentenceMap.put(word, dateSentenceMap.get(word)+"" ""+str);
                 }
                 else
                 {
                     dateMap.put(word, score);
                     dateCountMap.put(word, 1);
                     dateSentenceMap.put(word, str);
                 }                       
             }
         }
    }
}

private static int getSentiment(CoreMap sentence) {
    Tree annotatedTree = sentence.get(SentimentAnnotatedTree.class);
     int localScore = RNNCoreAnnotations.getPredictedClass(annotatedTree);
     int score = getScore(localScore);       
    return score;
}   


private static void mapLocation(HashMap&lt;String, Integer&gt; locationMap,
        HashMap&lt;String, Integer&gt; locationCountMap,
        HashMap&lt;String, Article_Location&gt; locationArticleMap,
        ArrayList&lt;Article_NerLocation&gt; locationList)
{       
    for(Map.Entry&lt;String, Integer&gt; entry : locationMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Article_Location articleLocation = locationArticleMap.get(key);

        Article_NerLocation l1 = new Article_NerLocation();
        if(value&gt;=1)
            l1.setNerSentiment(1);
        else if(value&lt;=-1)
            l1.setNerSentiment(-1);
        else
            l1.setNerSentiment(0);            

        l1.setKeyword(key);
        l1.setCount(locationCountMap.get(key));

        if(articleLocation!=null)
        {                   
            l1.setNerCountry(articleLocation.getCountryCode());
            l1.setNerLatLong(articleLocation.getLatitude()+"",""+articleLocation.getLongitude());
            l1.setTimeZone(articleLocation.getTimeZone());
            l1.setCountryName(articleLocation.getCountryName());
        }

        locationList.add(l1);
    }
}

private static void mapDate(HashMap&lt;String, Integer&gt; dateMap,
        HashMap&lt;String, Integer&gt; dateCountMap,
        HashMap&lt;String, String&gt; dateSentenceMap,
        ArrayList&lt;Artilcle_Ner_Date&gt; datelist)
{               
    for(Map.Entry&lt;String, Integer&gt; entry : dateMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Artilcle_Ner_Date d1 = new Artilcle_Ner_Date();

        if(value&gt;=1)
            d1.setNerSentiment(1);
        else if(value&lt;=-1)
            d1.setNerSentiment(-1);
        else
            d1.setNerSentiment(0);

        d1.setKeyword(key);
        d1.setCount(dateCountMap.get(key));
        d1.setSentence(dateSentenceMap.get(key));
        d1.setNerDateTheme1(SummaryThemeUtil.getSTByDate(dateSentenceMap.get(key)));
        datelist.add(d1);
    }   
}   

private static void mapOrg(HashMap&lt;String, Integer&gt; orgMap,
        HashMap&lt;String, Integer&gt; orgCountMap,
        ArrayList&lt;Articel_Ner&gt; organisationlist) 
{
    for(Map.Entry&lt;String, Integer&gt; entry : orgMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Articel_Ner o1 = new Articel_Ner();
        if(value&gt;=1)
            o1.setNerSentiment(1);
        else if(value&lt;=-1)
            o1.setNerSentiment(-1); 
        else
            o1.setNerSentiment(0);            


        o1.setKeyword(key);
        o1.setCount(orgCountMap.get(key));
        organisationlist.add(o1);            
    }       
}

private static void mapPerson(HashMap&lt;String, Integer&gt; personMap,
        HashMap&lt;String, Integer&gt; personCountMap,
        ArrayList&lt;Articel_Ner&gt; personlist)
{
    for(Map.Entry&lt;String, Integer&gt; entry : personMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Articel_Ner p1 = new Articel_Ner();
        if(value&gt;=1)
            p1.setNerSentiment(1);
        else if(value&lt;=-1)
            p1.setNerSentiment(-1);
        else
            p1.setNerSentiment(0);            

        p1.setKeyword(key);
        p1.setCount(personCountMap.get(key));
        personlist.add(p1);      
    }
}   

private static String checkDate(String date)
{               
    if(date.length()&lt;10)
        return null;
    else if(date.length()&gt;10)
        date = date.substring(0,10);

    if (date.matches(""\\d{4}-\\d{2}-\\d{2}""))
        return date; 
    else
        return null;
}

public static void main(String args[])
{
    String text = ""Lets meet on every 2nd week. Night is young. Happy new Year. The festival will be held on the following dates are 18 Feb 1997, the 20th of july and 4 days from today."";
    long pre = System.currentTimeMillis();
    HashMap&lt;String, Object&gt; map = getStanford(text, 1508745558);
    long post = System.currentTimeMillis();
    long diff = post-pre;

    System.out.println(diff);
    System.out.println(map);
}
}
</code></pre>
","stanford-nlp, sentiment-analysis, named-entity-recognition, sutime","<p>After days and days of sore black eyes. Here is where the problem is:</p>

<ul>
<li><p>Stanford ""<strong>parse</strong>"" model whether <strong>PCFG</strong> or <strong>SRparser</strong> both are CPU killers. You will never be able to scale. At best i was doing 70 docs/second. This is with 15 threads that i was able to manage on tomcat. The docs where being consumed from RabbitMQ. Machine Intel Xeon 8Core VM with 15 GB RAM. The CPU was always 90%.</p></li>
<li><p>So if you want to do <strong>NER</strong>,<strong>sentiment</strong>,<strong>sutime</strong>. Its better to use separate libraries and not use stanford for all 3. For NER you can use <strong>NERClassifierCombiner</strong> from stanford. For sentiment you can use <strong>weka</strong>. For extracting dates you can use <strong>natty</strong>. </p></li>
<li><p>Now we are able to do <strong>2,000 docs/second</strong>. </p></li>
</ul>
",0,-1,539,2017-11-09 09:35:26,https://stackoverflow.com/questions/47198333/stanford-nlp-ner-sentiment-sutime-performance-issue
Spacy 2.0 NER Training,"<p>In SpacyV1 it was possible to train the NER model by providing a document and a list of entity annotations in BILOU format.</p>

<p>However it seems as if in V2 training is only possible by providing entity annotation like this (7, 13, 'LOC'), so with enity offsets and entity tag.</p>

<p>Is the old way of providing the list of tokens and another list of entity tags in BILOU format still valid? </p>

<p>From what I gather from the documentation it looks like the nlp.update method accepts a list of GoldParse objects so I could create a GoldParse Object for each doc and pass the BILOU tags to its entities attribute. However would I loose important information by ignoring the other attributes of the GoldParse class (e.g. heads or tags <a href=""https://spacy.io/api/goldparse"" rel=""nofollow noreferrer"">https://spacy.io/api/goldparse</a> ) or are the other attributes not needed for training the NER?</p>

<p>Thanks!</p>
","nlp, training-data, named-entity-recognition, spacy","<p>Yes, you can still create <code>GoldParse</code> objects with the BILUO tags. The main reason the usage examples show the ""simpler"" offset format is that it makes them slightly easier to read and understand.</p>

<p>If you only want to train the NER, you can now also use the <a href=""https://spacy.io/api/language#disable_pipes"" rel=""nofollow noreferrer""><code>nlp.disable_pipes()</code> context manager</a> and disable all other pipeline components (e.g. the <code>'tagger'</code> and <code>'parser'</code>) during training. After the block, the components will be restored, so when you save out the model, it will include the whole pipeline. You can see this in action in the <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">NER training examples</a>.</p>
",3,3,2542,2017-11-10 09:35:50,https://stackoverflow.com/questions/47219639/spacy-2-0-ner-training
how to use pos tag as feature in Stanford NER training?,"<p>I am trying to use the useTags and related features in training Stanford NER CRF model. However, although I have specified in the .prop file that I will use this feature, CoreAnnotations.PartOfSpeechAnnotation.class does not seem to return anything and hence the training does not use this feature at all. Is there something I did wrong that it wasn't using this feature? Thanks!</p>
","nlp, stanford-nlp, named-entity-recognition","<p>You need to specify which column in your training/test data has the pos tag and add the pos tags to the CoNLL.</p>

<p>You specify that column in this part of the properties:</p>

<p><code>map = word=0,answer=1,tag=2</code></p>

<p>(for example if you added the tags in the 3rd column)</p>
",1,0,868,2017-11-16 20:13:06,https://stackoverflow.com/questions/47338317/how-to-use-pos-tag-as-feature-in-stanford-ner-training
Named Entity Recognition upper case issue,"<p>I recently switched the model I use for NER in spacy from <strong>en_core_web_md</strong> to <strong>xx_ent_wiki_sm</strong>.</p>

<p>I noticed that the new model always recognises full upper case words such as NEW JERSEY or NEW YORK as organisations. I would be able to provide training data to retrain the model, although it would be very time consuming. However I am uncertain if the model would loose the assumption that upper case words are organisations or if it would instead keep the assumption and create some exceptions for it. Does it maybe even learn that every all upper case with word with less than 5 letter is likely to be an organisation and everything with more letters not? I just dont know how exactly the training will affect the model</p>

<p>en_core_web_md seems to deal fine with acronyms, while ignoring words like NEW JERSEY. However the overall performance of xx_ent_wiki_sm is better for my use case</p>

<p>I ask because the assumption as such is still pretty useful, as it allows us to identify acronyms such as IBM as an organisation.</p>
","named-entity-recognition, spacy","<p>The <code>xx_ent_wiki_sm</code> model was trained on Wikipedia, so it's very biased towards what Wikipedia considers and entity, and what's common in the data. (It also tends to frequently recognise ""I"" as an entity, since sentences in the first person are so rare on Wikipedia.) So post-training with more examples is definitely a good strategy, and what you're trying to do sounds feasible.</p>

<p>The best way to prevent the model from ""forgetting"" about the uppercase entities is to always include examples of entities that the model previously recognised correctly in the training data (see: the <a href=""https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"" rel=""nofollow noreferrer"">""catastrophic forgetting problem""</a>). The nice thing is that you can create those programmatically by running spaCy over a bunch of text and extracting uppercase entities:</p>

<pre><code>uppercase_ents = [ent for ent in doc.ents if all(t.is_upper for t in ent)]
</code></pre>

<p>See <a href=""https://spacy.io/usage/training#training-data"" rel=""nofollow noreferrer"">this section</a> for more examples of how to create training data using spaCy. You can also use spaCy to generate the lowercase and titlecase variations of the selected entities to bootstrap your training data, which should hopefully save you a lot of time and work.</p>
",4,1,2812,2017-11-20 09:12:37,https://stackoverflow.com/questions/47388438/named-entity-recognition-upper-case-issue
Formatting training dataset for SpaCy NER,"<p>I want to train a blank model for NER with my own entities. To do this, I need to use a dataset, which is currently in .csv form and features entity tags in the following format (I'll provide one example row for each relevant column):</p>

<hr>

<p>Column: sentence</p>

<p>Value:  I want apples</p>

<hr>

<p>Column: data</p>

<p>Value:  ['want;@command;2;6','apples';@fruit;7;13']</p>

<hr>

<p>Column: entity</p>

<p>Value:  I @command @fruit</p>

<hr>

<p>Column: entity_types</p>

<p>Value: @bot/@command;@bot/@food/@fruit</p>

<hr>

<p>In order to train SpaCy's NER, I need the training data as json in the following form:</p>

<pre><code>    TRAIN_DATA = [
    ('Who is Shaka Khan?', {
        'entities': [(7, 17, 'PERSON')]
    }),
    ('I like London and Berlin.', {
        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]
    })
]
</code></pre>

<p><a href=""https://spacy.io/usage/training#ner"" rel=""noreferrer"">Link to the relevant part in the SpaCy Docs</a></p>

<p>I've tried to find a solution for how I could re-format the data from the csv to the format required by SpaCy, but I was unsuccessful as of yet. The dataset does contain all the necessary information - text string, entity names, entity types, entity offsets - but I simply don't know how to get them in the correct form.</p>

<p>I would appreciate any and all help concerning how I would accomplish this!</p>
","json, format, training-data, named-entity-recognition, spacy","<p>It wasn't 100% clear from your question whether you're also asking about the CSV extraction – so I'll just assume this is not the problem. (If it is, this should be pretty easy to achieve using the <code>csv</code> module. If the CSV data is messy and contains a bunch of stuff combined in one string, you might have to call <code>split</code> on it and do it the hacky way.)</p>

<p>If you're able to extract the ""sentence"" and ""data"" column in a format like this, you're actually very close to spaCy's training format already:</p>

<pre><code>[{ 
    'sentence': 'I want apples'
    'data': [('want', '@command', 2, 6) ('apples', '@fruit', 7, 13)]
}]
</code></pre>

<p>It seems like your data counts the end character differently and with an offset of <code>+1</code> compared to spaCy. So you'll have to adjust this by subtracting <code>1</code>. I'm probably making this a lot more verbose than it should be, but I hope this makes it easier to follow:</p>

<pre><code>TRAIN_DATA = []

for example in your_extracted_data:  # see example above
    entities = []
    for entity in example['data']:  # iterate over the entities
        text, label, start, end = entity  # ('want', '@command', 2, 6)
        label = label.split('@')[1].upper()  # not necessary, but nicer
        end = end - 1  # correct the end character index
        entities.append((start, end, label))
    # add training example of (text, annotations) tuple
    TRAIN_DATA.append((example['sentence'], {'entities': entities}))
</code></pre>

<p>This should give you training data that looks like this:</p>

<pre><code>[
    ('I want apples', {'entities': [(2, 5, 'COMMAND'), (7, 12, 'FRUIT')]})
]
</code></pre>
",11,7,10459,2017-11-22 21:14:11,https://stackoverflow.com/questions/47443976/formatting-training-dataset-for-spacy-ner
AssertionError on trying to add new entity using matcher on spaCy,"<p>I'm trying to match all e-mail like looking text in a bunch of documents and add it to custom NER label called 'EMAIL'. 
Here is the code for a test case.</p>

<pre><code>nlp = spacy.load('en_core_web_sm')
matcher = Matcher(nlp.vocab)

EMAIL = nlp.vocab.strings['EMAIL']

def add_email_ent(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    doc.ents += ((EMAIL, start, end),)

matcher.add('EmailPII', add_email_ent, [{'LIKE_EMAIL': True}])

text = u""Hi, this is John. My email is john@ymail.com and an alternate is john@gmail.com""
doc = nlp(text)

matches = matcher(doc)
for i,[match_id, start, end] in enumerate(matches):
    print (i+1, doc[start:end])

for ent in doc.ents:
    print (ent.text, ent.label_)
</code></pre>

<p>Here's what I get when I run this code.</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Python27/emailpii.py"", line 26, in &lt;module&gt;
    matches = matcher(doc)
  File ""matcher.pyx"", line 407, in spacy.matcher.Matcher.__call__
  File ""C:/Python27/emailpii.py"", line 19, in add_event_ent
    doc.ents += ((EMAIL, start, end),)
  File ""doc.pyx"", line 415, in spacy.tokens.doc.Doc.ents.__get__
  File ""span.pyx"", line 61, in spacy.tokens.span.Span.__cinit__
AssertionError: 17587345535198158200
</code></pre>

<p>However, on running a similar example</p>

<pre><code>import spacy


print ""*****************""
print(spacy.__version__)
print ""*****************""


from spacy.matcher import Matcher
#from spacy import displacy

nlp = spacy.load('en_core_web_sm')
matcher = Matcher(nlp.vocab)

EVENT = nlp.vocab.strings['EVENT']

def add_event_ent(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    doc.ents += ((EVENT, start, end),)

matcher.add('GoogleIO', add_event_ent,
            [{'ORTH': 'Google'}, {'ORTH': 'I'}, {'ORTH': '/'}, {'ORTH': 'O'}],
            [{'ORTH': 'Google'}, {'ORTH': 'I'}, {'ORTH': '/'}, {'ORTH': 'O'}, {'IS_DIGIT': True}])

text = u""Google I/O was great this year. See you all again in Google I/O 2018""
doc = nlp(text)

matches = matcher(doc)
for i,[match_id, start, end] in enumerate(matches):
    print (i, doc[start:end])

for ent in doc.ents:
    print (ent.text, ent.label_)

#displacy.serve(doc, style = 'ent')
</code></pre>

<p>I get the output as desired:</p>

<hr>

<p>2.0.1</p>

<hr>

<p>(0, Google I/O)</p>

<p>(1, Google I/O)</p>

<p>(2, Google I/O 2018)</p>

<p>(u'Google I/O', u'EVENT')</p>

<p>(u'this year', u'DATE')</p>

<p>(u'Google I/O 2018', u'EVENT')</p>

<p>Am I missing something here?</p>
","named-entity-recognition, spacy","<p>I believe your first code fails because you have not added an Entity label for 'EMAIL'. The second code works because EVENT is a pre-existing Entity type.</p>

<p>The documentation is not very clear on what the first argument of the <code>matcher.add()</code> method actually does, but it adds an Entity label for you. Here are two alternatives that should work and clear up the confusion:</p>

<p>Alternative 1:</p>

<pre><code>import spacy
from spacy.matcher import Matcher

nlp = spacy.load('en_core_web_sm')
matcher = Matcher(nlp.vocab)

#EMAIL = nlp.vocab.strings['EMAIL'] #Not needed

def add_email_ent(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    doc.ents += ((match_id, start, end),)

matcher.add('EMAIL', add_email_ent, [{'LIKE_EMAIL': True}])

text = u""Hi, this is John. My email is john@ymail.com and an alternate is john@gmail.com""
doc = nlp(text)

matches = matcher(doc)
for i,[match_id, start, end] in enumerate(matches):
    print (i+1, doc[start:end])

for ent in doc.ents:
    print (ent.text, ent.label_)
</code></pre>

<p>Alternative 2 (I'm not sure why you'd want to do it this way because you end up with two entity labels serving essentially the same purpose, but provided just for illustration purposes):</p>

<pre><code>import spacy
from spacy.matcher import Matcher
from spacy.pipeline import EntityRecognizer

nlp = spacy.load('en_core_web_sm')
matcher = Matcher(nlp.vocab)
ner = EntityRecognizer(nlp.vocab)

ner.add_label('EMAIL')

EMAIL = nlp.vocab.strings['EMAIL']

def add_email_ent(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    doc.ents += ((EMAIL, start, end),)

matcher.add('EmailPII', add_email_ent, [{'LIKE_EMAIL': True}])

text = u""Hi, this is John. My email is john@ymail.com and an alternate is john@gmail.com""
doc = nlp(text)

matches = matcher(doc)
for i,[match_id, start, end] in enumerate(matches):
    print (i+1, doc[start:end])

for ent in doc.ents:
    print (ent.text, ent.label_)
</code></pre>
",1,1,480,2017-11-29 20:49:14,https://stackoverflow.com/questions/47561572/assertionerror-on-trying-to-add-new-entity-using-matcher-on-spacy
NER types for &#39;en&#39; model?,"<p>Is there a way to get all the built-in and added custom NER types in spaCy? The method suggested <a href=""https://github.com/explosion/spaCy/issues/441"" rel=""nofollow noreferrer"">here</a> doesn't seem to work anymore. On trying </p>

<pre><code>nlp.entity.cfg 
</code></pre>

<p>I get back,</p>

<pre><code>{u'hist_size': 0, u'pretrained_dims': 0L, u'hist_width': 0, u'beam_density': 
0.0, u'cnn_maxout_pieces': 3, u'maxout_pieces': 2, u'hidden_depth': 1, 
u'token_vector_width': 128, u'nr_class': 73, u'beam_width': 1, 
u'hidden_width': 200}
</code></pre>

<p>There are no 'actions' and 'extra_labels' keys in this dictionary.</p>
","python, named-entity-recognition, spacy","<p>I've found another way. Works for spacy v. 3.0.5:</p>
<pre><code>ner = nlp.get_pipe('ner')
print(ner.labels)
</code></pre>
<p>it gives the result:</p>
<pre><code>('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')
</code></pre>
",4,3,1341,2017-12-04 04:22:57,https://stackoverflow.com/questions/47626464/ner-types-for-en-model
Why does my custom spaCy entity type get detected?,"<p>I am writing a <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spaCy</a> program for which I want to define a custom named entity tag. Following the example <a href=""https://spacy.io/usage/training#example-new-entity-type"" rel=""nofollow noreferrer"">here</a>, I add a label called <code>MY_NEW_LABEL</code> to the pipeline.</p>

<pre><code>import spacy

nlp = spacy.load(""en_core_web_lg"")

ner = nlp.get_pipe(""ner"")
new_label = ""MY_NEW_LABEL""
ner.add_label(new_label)

documents_path = ""my_document.txt""
document = nlp(open(documents_path).read())
print([e for e in document.ents if e.label_ == new_label])
</code></pre>

<p>When I run the above program it prints out a list of entities labeled with <code>MY_NEW_LABEL</code>. I don't see how this is possible because I never do anything with the label.</p>

<p>Clearly I'm misunderstanding how to work with custom entity tags, but I can't figure out why this would be happening from the documentation. Can anyone tell me why my program doesn't print out an empty list?</p>
","named-entity-recognition, spacy","<p>This is unexpected behavior. I opened it as spaCy issue <a href=""https://github.com/explosion/spaCy/issues/1697"" rel=""nofollow noreferrer"">1697: Custom Entity Labels Are Erroneously Detected</a>.</p>
",2,0,313,2017-12-04 23:05:23,https://stackoverflow.com/questions/47643438/why-does-my-custom-spacy-entity-type-get-detected
TypeError: &#39;&lt;&#39; not supported between instances of &#39;NoneType&#39; and &#39;str&#39; using Pyner for Name entity recognition,"<p>I am trying to pass an email string to Pyner to pull out all the entities into a dictionary. I can verify my setup works with this returning two PERSON entities </p>

<pre><code>import ner
tagger = ner.SocketNER(port=9191, output_format='slashTags')
t = ""My daughter Sophia goes to the university of California. James also goes there""
print(type(t))
test = tagger.get_entities(t)
person_ents = test['PERSON']
for i in person_ents:
    print(i)
</code></pre>

<p>This outputs as expected</p>

<pre><code>Sophia
James
</code></pre>

<p>The only difference is here that I have email text here instead I can verify it's a string </p>

<pre><code>print(type(firstEmail))

test = tagger.get_entities(firstEmail)
person_ents = test['PERSON']
print (type(person_ents))
for i in person_ents:
    print(i)
</code></pre>

<p>This returns the following error</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-79-ff847452c8df&gt; in &lt;module&gt;()
      3 
      4 
----&gt; 5 test = tagger.get_entities(firstEmail)
      6 person_ents = test['PERSON']
      7 print (type(person_ents))

~/anaconda3/envs/nlp/lib/python3.6/site-packages/ner-0.1-py3.6.egg/ner/client.py in get_entities(self, text)
     90         else: #inlineXML
     91             entities = self.__inlineXML_parse_entities(tagged_text)
---&gt; 92         return self.__collapse_to_dict(entities)
     93 
     94     def json_entities(self, text):

~/anaconda3/envs/nlp/lib/python3.6/site-packages/ner-0.1-py3.6.egg/ner/client.py in __collapse_to_dict(self, pairs)
     71         """"""
     72         return dict((first, list(map(itemgetter(1), second))) for (first, second)
---&gt; 73             in groupby(sorted(pairs, key=itemgetter(0)), key=itemgetter(0)))
     74 
     75     def get_entities(self, text):

TypeError: '&lt;' not supported between instances of 'NoneType' and 'str'
</code></pre>

<p>Any idea how what's wrong</p>
","string, python-3.x, stanford-nlp, named-entity-recognition","<p>The issue here is that NER is setup so that when the output is set to SlashTags it output a dictionary format. However the text is parsed with slash characters where a named entity occurs and this character is then used to separate dictionary entity before the dictionary is generated. As a result if any slashes occur in your text data you need to parse this out. </p>

<p>Something like</p>

<pre><code>#text is your string
text = text.replace('/', '-')
</code></pre>

<p>This shouldn't be an issue in NLP terms as dates should still be picked out with this format. But if some key part of your analysis requires this tag to be there this solution might not be suitable. I can't verify if this issue exists in the java implementation but it's possible</p>
",0,1,820,2018-01-04 12:00:09,https://stackoverflow.com/questions/48094827/typeerror-not-supported-between-instances-of-nonetype-and-str-using-pyn
Ambiguous Entity in stanfors NER,"<p>I am working on Stanford NER, My question is regarding ambiguous entities.
For example, I have 2 sentences:</p>

<blockquote>
  <ol>
  <li>I love oranges.</li>
  <li>Orange is my dress code for tomorrow.</li>
  </ol>
</blockquote>

<p>How can i train these 2 sentences to give out, </p>

<blockquote>
  <p>first orange as Fruit,  second orange as Color.</p>
</blockquote>

<p>Thanks</p>
","python-3.x, stanford-nlp, named-entity-recognition","<p>Scrape data from sites like wikipedia,etc. and create a scoring model and then use it for context prediction.</p>
",0,1,210,2018-01-08 11:21:56,https://stackoverflow.com/questions/48149281/ambiguous-entity-in-stanfors-ner
"Training Stanford-NER-CRF, control number of iterations and regularisation (L1,L2) parameters","<p>I was looking through StanfordNER documentation/FAQ but I can't find anything related to specifying the maximum number of iterations in training and also the value of the regularisation parameters L1 and L2.</p>

<p>I saw an answer on which is suggested to set, for instance: </p>

<p><code>maxIterations=10</code> </p>

<p>in the properties file, but that did not gave any results.</p>

<p>Is it possible to set these parameters?</p>
","nlp, stanford-nlp, crf, named-entity-recognition","<p>I had to dig in the code but found it, so basically StanfordNER supports many different numerical optimization algorithms. One can see which ones are implemented and can be used to train the CRF by looking into the 
 <code>getMinimizer()</code> method in the <code>CRFClassifier.java</code> file.</p>

<p>I configured my properties file to use the Orthant-Wise Limited-memory Quasi-Newton, by setting: </p>

<p><code>useOWLQN = true</code></p>

<p>The L1-prior can be set with:</p>

<p><code>priorLambda = 10</code></p>

<p>An useful trick is to play with the convergence tolerance parameter TOL, which is checked at each iteration: <code>|newest_val - previous_val| / |newestVal| &lt; TOL</code>, the <code>TOL</code> is controlled by:</p>

<p><code>tolerance = 0.01</code></p>

<p>Yet another useful parameter is to explicitly control the maximum number of iterations for which the learning algorithm should run:</p>

<p><code>maxQNItr = 100</code></p>
",0,0,479,2018-01-09 10:27:28,https://stackoverflow.com/questions/48166110/training-stanford-ner-crf-control-number-of-iterations-and-regularisation-l1-l
Named entity recognition in Spacy,"<p>I am trying to find Named entities for a sentence as below </p>

<pre><code>import spacy.lang.en
parser = spacy.lang.en.English()
ParsedSentence = parser(u""Alphabet is a new startup in China"")
for Entity in  ParsedSentence.ents:  
    print (Entity.label, Entity.label_, ' '.join(t.orth_ for t in Entity))
</code></pre>

<p>I am expecting to get the result ""Alphabet"",""China"" but I am getting an empty set as result. What am I doing wrong here</p>
","python, named-entity-recognition, spacy","<p>As per spacy <a href=""https://spacy.io/usage/linguistic-features#101"" rel=""nofollow noreferrer"">documentation</a> for Name Entity Recognition here is the way to extract name entity</p>

<pre><code>import spacy
nlp = spacy.load('en') # install 'en' model (python3 -m spacy download en)
doc = nlp(""Alphabet is a new startup in China"")
print('Name Entity: {0}'.format(doc.ents))
</code></pre>

<p><strong>Result</strong><br>
<code>Name Entity:  (China,)</code>
<br></p>

<p>To make ""Alphabet"" a 'Noun' append it with ""The"". </p>

<pre><code>doc = nlp(""The Alphabet is a new startup in China"")
print('Name Entity: {0}'.format(doc.ents))
</code></pre>

<p><code>Name Entity:  (Alphabet, China)</code></p>
",24,12,14392,2018-01-11 05:48:10,https://stackoverflow.com/questions/48200524/named-entity-recognition-in-spacy
JAPE check for numeric only input,"<p>I have the below JAPE file  for ANNIE in which I am trying to ensure that establishments names which contain only numeric characters do not get annotated.</p>

<p>I have tried many variations of the below but do not seem to be able to get it to work. </p>

<pre><code>Phase:Establishment
Input: Lookup Token
Options: control = appelt

Rule: EstablishmentNonNumeric
Priority: 100
//do not tag establishment names which contain only numbers as this is just weird
(
({Lookup.majorType == ""establishment"", Lookup.kind == number})
): temp
--&gt;
{ }

Rule: Establishment
(
{Lookup.majorType == ""establishment""}
): temp
--&gt;
:temp.Establishment ={rule= ""EstablishmentRule"" } 
</code></pre>

<p>I have a feeling it may be to do with the control setting but I am not really sure.</p>

<p>Many thanks!</p>
","nlp, gate, named-entity-recognition","<p>My guess is that you meant: </p>

<pre><code>Token.kind == number
</code></pre>

<p>instead of: </p>

<pre><code>Lookup.kind == number
</code></pre>
",1,1,111,2018-01-14 02:42:59,https://stackoverflow.com/questions/48246236/jape-check-for-numeric-only-input
Named Entity Recognition in practice,"<p>I am a NLP novice trying to learn, and would like to better understand how Named Entity Recognition (NER) is implemented in practice, for example in popular python libraries such as <a href=""https://spacy.io"" rel=""nofollow noreferrer"">spaCy</a>.</p>

<p>I understand the basic concept behind it, but I suspect I am missing some details.
From the documentation, it is not clear to me for example how much preprocessing is done on the text and annotation data; and what statistical model is used.</p>

<p>Do you know if:</p>

<ul>
<li>In order to work, the text has to go through chunking before the model is trained, right? Otherwise it wouldn't be able to perform anything useful?</li>
<li>Are the text and annotations typically normalized prior to the training of the model? So that if a named entity is at the beginning or middle of a sentence it can still work?</li>
<li>Specifically in spaCy, how are things implemented concretely? Is it a HMM, CRF or something else that is used to build the model?</li>
</ul>

<p>Apologies if this is all trivial, I am having some trouble finding easy to read documentation on NER implementations.</p>
","python, entity-framework, nlp, named-entity-recognition, spacy","<p>In <a href=""https://spacy.io/models/en#en_core_web_md"" rel=""nofollow noreferrer"">https://spacy.io/models/en#en_core_web_md</a> they say <code>English multi-task CNN trained on OntoNotes</code>. So I imagine that's how they obtain the NEs. You can see that the pipeline is</p>

<p>tagger, parser, ner</p>

<p>and read more here: <a href=""https://spacy.io/usage/processing-pipelines"" rel=""nofollow noreferrer"">https://spacy.io/usage/processing-pipelines</a>. I would try to remove the different components and see what happens. This way you could see what depends on what. I'm pretty sure NER depends on tagger, but not sure whether requires the parser. All of them of course require the tokenizer</p>

<p>I don't understand your second point. If an entity is at the beginning or middle of a sentence is just fine, the NER system should be able to catch it. I don't see how you're using the word <code>normalize</code> in a position of text context.</p>

<p>Regarding the model, they mention multi-task CNN, so I guess the CNN is the model for NER. Sometimes people use a CRF on top, but they don't mention it so probably is just that. According to their performance figures, it's good enough</p>
",1,0,1044,2018-01-16 23:13:26,https://stackoverflow.com/questions/48291313/named-entity-recognition-in-practice
Stanford NLP 3.9.0: Does using CoreEntityMention combine adjacent entity mentions?,"<p>I am testing out getting entity mentions the new 3.9.0 way with CoreEntityMention.  I do something like:</p>

<pre><code>    CoreDocument document = new CoreDocument(text);
    stanfordPipe = createNerPipeline();
    stanfordPipe.annotate(document);

    for (CoreSentence sentence : document.sentences()) {
        logger.debug(""Found sentence {}"", sentence);
        if (sentence.entityMentions() == null) continue;
        for (CoreEntityMention cem : sentence.entityMentions()) {
            logger.debug(""Found em {}"", stringify(cem));            
        }
    }
</code></pre>

<p>When I iterate through entity mentions using <code>sentence.entityMentions()</code> I see that some of the entity mentions produced are multi-token entity mentions.  The old way of getting entity mentions, and correct me if I am wrong, is that you have to iterate over CoreLabel and therefore have to combine the multi-token entity mentions yourself.</p>

<p>So is there some new method that did not exist before to combine adjacent tokens with the same ner label?  Or have I missed older ways to combine multi-token entity mentions?</p>
","stanford-nlp, named-entity-recognition","<p>Hi thanks for using the new interface!</p>

<p>Yes, the CoreEntityMention is supposed to represent a full entity mention.  This was some new syntax added to help make it easier to work with our code.</p>

<p>Traditionally there has been a need for things like sentence.get(CoreAnnotations.TokensAnnotation.class)...etc...so we tried to add some wrapper classes so people could use the pipeline interface but not have the cumbersome syntax.</p>

<p>With this newly debuted syntax, you can write:</p>

<pre><code>sentence.tokens();
</code></pre>

<p>Regarding entity mentions, if the sentence is ""Joe Smith went to Hawaii."" you would get two entity mentions:</p>

<p>Joe Smith  (2 tokens)
Hawaii (1 token)</p>

<p>Traditionally the <code>ner</code> annotator would tag every token in the sentence with it's named entity type.  Then a separate <code>entitymentions</code> annotator would build <code>Mention</code> annotations which were <code>CoreMap</code> representations of full entity mentions (e.g. Joe Smith).</p>

<p>I've seen a lot of people over the years ask ""How do I go from a tagged sequence of tokens to the full entity mentions?""  So in response to this we tried to make it a lot easier to just extract the full entity's referred to in the sentence.</p>

<p>I should also note that for the most part the older ways should still work.  Updated documentation is on the way as we work on finalizing the 3.9.0 release!</p>
",2,0,329,2018-02-05 22:04:36,https://stackoverflow.com/questions/48632256/stanford-nlp-3-9-0-does-using-coreentitymention-combine-adjacent-entity-mention
How can I extract GPE(location) using NLTK ne_chunk?,"<p>I am trying to implement a code to check for the weather condition of a particular area using OpenWeatherMap API and NLTK to find entity name recognition. But I am not able to find the method of passing the entity present in GPE(that gives the location), in this case, Chicago, to my API request. Kindly help me with the syntax.The code to given below. </p>

<p>Thank you for your assistance</p>

<pre><code>import nltk
from nltk import load_parser
import requests
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords

sentence = ""What is the weather in Chicago today? ""
tokens = word_tokenize(sentence)

stop_words = set(stopwords.words('english'))

clean_tokens = [w for w in tokens if not w in stop_words]

tagged = nltk.pos_tag(clean_tokens)

print(nltk.ne_chunk(tagged))
</code></pre>
","python, geolocation, nlp, nltk, named-entity-recognition","<p>The <code>GPE</code> is a <code>Tree</code> object's label from the pre-trained <code>ne_chunk</code> model.</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, ne_chunk
&gt;&gt;&gt; sent = ""What is the weather in Chicago today?""
&gt;&gt;&gt; ne_chunk(pos_tag(word_tokenize(sent)))
Tree('S', [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('weather', 'NN'), ('in', 'IN'), Tree('GPE', [('Chicago', 'NNP')]), ('today', 'NN'), ('?', '.')])
</code></pre>

<p>To traverse the tree, see <a href=""https://stackoverflow.com/questions/31689621/how-to-traverse-an-nltk-tree-object"">How to Traverse an NLTK Tree object?</a></p>

<p>Perhaps, you're looking for something that's a slight modification to <a href=""https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list"">NLTK Named Entity recognition to a Python list</a> </p>

<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import Tree

def get_continuous_chunks(text, label):
    chunked = ne_chunk(pos_tag(word_tokenize(text)))
    prev = None
    continuous_chunk = []
    current_chunk = []

    for subtree in chunked:
        if type(subtree) == Tree and subtree.label() == label:
            current_chunk.append("" "".join([token for token, pos in subtree.leaves()]))
        if current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk
</code></pre>

<p>[out]:</p>

<pre><code>&gt;&gt;&gt; sent = ""What is the weather in New York today?""
&gt;&gt;&gt; get_continuous_chunks(sent, 'GPE')
['New York']

&gt;&gt;&gt; sent = ""What is the weather in New York and Chicago today?""
&gt;&gt;&gt; get_continuous_chunks(sent, 'GPE')
['New York', 'Chicago']

&gt;&gt;&gt; sent = ""What is the weather in New York""
&gt;&gt;&gt; get_continuous_chunks(sent, 'GPE')
['New York']

&gt;&gt;&gt; sent = ""What is the weather in New York and Chicago""
&gt;&gt;&gt; get_continuous_chunks(sent, 'GPE')
['New York', 'Chicago']
</code></pre>
",5,1,8586,2018-02-07 09:46:01,https://stackoverflow.com/questions/48660547/how-can-i-extract-gpelocation-using-nltk-ne-chunk
Named Entity Annotation read &#39;in-context&#39; as RDF?,"<p>I'm rather new to RDF, and have the basics nailed down, but I have a question about possible data sources (in addition to ontologies): is it possible to 'read' an (appropriately) Named Entity tag as an RDF subject-predicate-object(which would be the tagged text itself)... in its original context, that is to say, in its original text document?</p>

<p>I would not only like a query to pull a text excerpt as data itself (only linked to the original document), but allow an interface/query to include also the context in which it appeared (or: the text around it).</p>
","xml, rdf, refs, named-entity-recognition","<p>The answer is: RDFa. The ontology-source-abbreviation is declared as an 'xmlins' in the <code>&lt;body&gt;</code> tag of an <code>&lt;html&gt;</code> page, and the tags containing the text (<code>&lt;span&gt;</code>, <code>&lt;p&gt;</code>, etc.) annotated accordingly...</p>

<p>I was overthinking the problem while the solution was right in front of me.</p>
",1,1,78,2018-02-08 13:03:59,https://stackoverflow.com/questions/48686182/named-entity-annotation-read-in-context-as-rdf
How to group up NER tags in order to get data from sentence as a whole?,"<p>Through the CoreNLP library, upon calling <code>ner()</code> on a <code>CoreLabel</code> I receive a string indicating its named entity tag (such as <code>PERSON</code> or <code>DATE</code>).</p>

<p>However, I know of no way of comparing tokens in a sentence against each other. For example: (text of tokens surrounded in backticks)</p>

<pre><code>`Ellen` PERSON
`Wexler `PERSON
`,` O
`February` DATE
`9` DATE
`,` DATE
`2016` DATE
</code></pre>

<p><strong>Through CoreNLP, How do I group up the person tags in order to get the name <code>Ellen Wexler</code>? Or the date tags in order to get <code>February 9, 2016</code>, or another representation that I could eventually turn into a Date/Calendar object in Java?</strong> I have looked at the example given <a href=""https://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow noreferrer"">here</a>, however that only finds the ner tags for each individual core label. It does not provide me a way to group consecutive, identical ner tags together.</p>

<p><strong>What I have tried:</strong>
I have written a for loop that iterates over the sentence and finds X number of consecutive, identical ner tags (so if X is 2 and the ner tag is <code>PERSON</code>, it will find 2 consecutive PERSONs). In this scenario, that is <code>Ellen Wexler</code>. However, this breaks down when punctuation comes into play, as punctuation, depending on context, is given the ner tag of its adjacent tokens. In addition, there must be some way to do this through CoreNLP.</p>

<p><strong>My Resarch</strong>:
<a href=""https://stackoverflow.com/questions/45349957/merge-consecutive-tokens-with-same-ner-tag-in-corenlp-conll-output"">This</a> similar question has not been answered. The CoreNLP home page provides no answer, as it only provides an example regarding analysis of individual core labels/tokens. </p>
","java, nlp, stanford-nlp, named-entity-recognition","<p>More traditionally you want to use the <code>entitymentions</code> annotator.</p>

<p>In version 3.9.0 which has just been beta-released, the <code>ner</code> annotator will automatically create entity mentions which link tokens together that belong to the same entity mention.</p>

<p>You can see some example usage of a new API to see how to easily access the entity mentions.  Some of the features of this class aren't in the beta of 3.9.0 on the site, but will be added in an updated version very soon.</p>

<p>Helpful demo code:</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/examples/BasicPipelineExample.java"" rel=""noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/examples/BasicPipelineExample.java</a></p>
",6,1,1428,2018-02-10 20:00:53,https://stackoverflow.com/questions/48725027/how-to-group-up-ner-tags-in-order-to-get-data-from-sentence-as-a-whole
NLP - Speed of Named Entity Recognition (StanfordNER),"<p>I'm doing Natural Language Processing (NLP) in Python 3 and more specifically Named Entity Recognition (NER) on the Harry Potter set of books. I'm using StanfordNER, which works pretty well but takes incredible amounts of time...</p>

<p>I have done some research online on why it would be this slow but I can't seem to find anything that truly suits my code, and I honestly think the problem lays more in the (bad) way I have written the code.</p>

<p>So here's what I wrote for now :</p>

<pre><code>import string
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk.tag.stanford as st

tagger = st.StanfordNERTagger('_path_/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz', '_path_/stanford-ner-2017-06-09/stanford-ner.jar')

#this is just to read the file

hp = open(""books/hp1.txt"", 'r', encoding='utf8')
lhp = hp.readlines()

#a small function I wrote to divide the book in sentences

def get_sentences(lbook):
    sentences = []
    for k in lbook:
        j = sent_tokenize(k)
        for i in j:
            if bool(i):
                sentences.append(i)
    return sentences

#a function to divide a sentence into words

def get_words(sentence):
    words = word_tokenize(sentence)
    return words

sentences = get_sentences(lhp)

#and now the code I wrote to get all the words labeled as PERSON by the StanfordNER tagger

characters = []
    for i in sentence:
    characters = [tag[0] for tag in tagger.tag(get_words(sentences[i])) if tag[1]==""PERSON""]
    print(characters)
</code></pre>

<p>Now the problem, as I explained, is that the code takes huge amounts of time... So I'm wondering, is it normal or <strong>can I save time by rewriting the code in a better way ?</strong> If so, could you help me out ?</p>
","python, python-3.x, nlp, stanford-nlp, named-entity-recognition","<p>The bottleneck is the <code>tagger.tag</code> method, it has a big overhead. Therefore, calling it for every sentence results in a really slow program. Unless there's an additional need for splitting the book into sentences, I'd process the whole text at once:</p>

<pre><code>with open('books/hp1.txt', 'r') as content_file:
    all_text = content_file.read()
    tags = tagger.tag(word_tokenize(all_text))
    characters = [tag[0] for tag in tags if tag[1] == ""PERSON""]
    print(characters)
</code></pre>

<p>Now if what you wanted to know is, say, what sentence each character is mentioned in, then you could first get the characters' names in <code>characters</code> like in the code above, and then loop through the sentences checking if an element from <code>characters</code> exists there.</p>

<p>If file size is a concern (although a .txt file of most books shouldn't be a problem to load into memory), then instead of reading the whole book, you could read a number <code>n</code> of sentences at once. From your code, modify your for loop like so:</p>

<pre><code>n = 1000
for i in range(0, len(sentences), n):
    scs = '. '.join(sentences[i:i + n])
    characters = [tag[0] for tag in tagger.tag(get_words(scs)) if tag[1]==""PERSON""]
</code></pre>

<p>The general idea is to minimize the calls to <code>tagger.tag</code> for its big overhead.</p>
",3,0,606,2018-02-15 22:00:52,https://stackoverflow.com/questions/48817017/nlp-speed-of-named-entity-recognition-stanfordner
CoreNLP - NER and SUTime to only recognize absolute dates,"<p>I'm working with the Named Entity Recognition annotator of CoreNLP.</p>

<p>My problem is that I would like to not recognize as entities relative dates.
My goal is to connect dates with events</p>

<blockquote>
  <p>Some interesting dates are 18 Feb 1997, the 20th of july, the year 1992, 4 days from today and Monday the 13th.</p>
</blockquote>

<p>In this example I would like to highlight ""18 Feb 1997"", ""20th of july"" and ""1992"".
Even if some of these dates are not complete, they can still be used to search for events.</p>

<p>On the other hand ""4 days from today"" and ""Monday the 13th"" are not interesting for me: the reasons are that the first it is relative to the current date (or the date the text has been written), while the second one is too generic.</p>

<p>Is there a simple way to tell the NER annotator to discard relative dates? </p>

<p>Thank you</p>
","stanford-nlp, named-entity-recognition, sutime","<p>I found the following solution, which works very well in my case.</p>

<p>Each token representing a Time/Date Named Entity has an annotation field containing its normalized form.</p>

<p>The absolute dates that I want to recognize will have a normalized form which follows the following pattern:</p>

<ul>
<li>18 Feb 1997 -> 1997/02/18</li>
<li>20th of July -> XXXX/07/20</li>
<li>1992 -> 1992</li>
</ul>

<p>Using a REGEX it is possible to discard annotations which do not have a normalized form like this.</p>

<pre><code>(\d{4}|X{4})((\/\d{2}(\/\d{2})?)?)
</code></pre>
",1,1,624,2018-03-06 08:56:05,https://stackoverflow.com/questions/49126827/corenlp-ner-and-sutime-to-only-recognize-absolute-dates
Spacy NER entities postition,"<p>How can I get the entity position found by NER within spacy?</p>

<p>From the following example:</p>

<pre><code>doc = nlp('Rami Eid is studying at Stony Brook University in New York')
print(list([(ent for ent in doc.ents])
</code></pre>

<p>it results: </p>

<pre><code>['Rami Eid','Stony Brook University','New York']
</code></pre>

<p>but I need the position of each entity within the sentence, so that I can know which tokens belong to the respective entity.</p>

<p>If I need to search from these results, I may have some cases where single word entities match multiple words of other entities. </p>
","python, spacy, named-entity-recognition","<p>An entity is an object of the spacy.Span class, meaning it inherits methods such as <em>start</em>, <em>end</em> etc.</p>

<pre><code>&gt;&gt;&gt; doc = nlp('Rami Eid is studying at Stony Brook University in New York')
&gt;&gt;&gt; [(e.start, e.end) for e in doc.ents]
[(0, 2), (5, 8), (9, 11)]
</code></pre>
",7,2,2581,2018-03-10 12:40:01,https://stackoverflow.com/questions/49209163/spacy-ner-entities-postition
SpaCy doesn&#39;t recognize money and countries as expected,"<p>I'm working with SpaCy in order to extract the different named entities from a sample text. The problem is that SpaCy doesn't recognize all the expected entities. It has problems with money and some locations. This is my code:</p>

<pre><code># encoding: utf-8
import spacy
from spacy import displacy

nlp = spacy.load('es') #cargo el modelo en español.
text = u""Una vez un personaje le preguntó a Agustín Chirichigno si estaba en su casa. El nombre de este personaje es Lucas Picchi y es de Mar del Plata. Junto a SU PRIMO, DE ESTADOS UNIDOS hacen cosas re locas como por ejemplo comprar un Fernet Branca a AR$2.500 cuando en realidad está a $180.""
doc = nlp(text)
displacy.serve(doc, style='ent')
</code></pre>

<p>Note that I'm loading a spanish model. My configuration is as follows:
spacy info</p>

<pre><code>Info about spaCy

Python version     2.7.6          
Platform           Linux-4.4.0-112-generic-x86_64-with-Ubuntu-14.04-trusty
spaCy version      2.0.9          
Location           /usr/local/lib/python2.7/dist-packages/spacy
Models             es, en
</code></pre>

<p>So, the output is:
<a href=""https://i.sstatic.net/POi9y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/POi9y.png"" alt=""enter image description here""></a></p>

<p>What is expected is that SpaCy recognize <code>MONEY</code> correctly (AR$2500 is 2500 argentinian pesos and $180 is money too). The word ""Junto"" is not a <code>LOCATION</code> and neither ""SU PRIMO"". ""Junto a su primo"" would be like ""with his cousin"" in english. Also, Fernet Branca is a <code>BRAND</code>, and not a person.</p>

<p>So, my question is: what am I doing wrong? Should I use additional libraries?</p>

<p>Thanks in advance.</p>
","python, nlp, spacy, named-entity-recognition","<p>According to spacy documentation of spanish model it supports identification of PER, LOC, ORG and MISC entities. It is trained on AnCora and WikiNER corpus.</p>

<p>In the release note it was mentioned that :</p>

<p>""Because the model is trained on Wikipedia, it may perform inconsistently on many genres, such as social media text.""</p>

<p>As the results for your data are not satisfactory you need train it for your data. This can be done as suggested in <a href=""https://spacy.io/usage/training#section-ner"" rel=""nofollow noreferrer"">here</a></p>

<p>For money you can actually write a simple regular expression like this for american dollar money</p>

<pre><code>^\$?[0-9][0-9\,]*(\.\d{1,2})?$|^\$?[\.]([\d][\d]?)$
</code></pre>
",3,1,2245,2018-03-15 17:47:50,https://stackoverflow.com/questions/49306083/spacy-doesnt-recognize-money-and-countries-as-expected
Is there any word shape feature library for NER in python?,"<p>As a beginner in python, I am trying to build my own named entity recognizer and it is known that word shape features are particularly important in NER. Are there any known libraries where these features are defined? For example, one version of these features denotes lowers-case letters by x and upper-case letters by X, numbers by d and retaining punctuation, maps <b>DC10-30</b> to <b>XX-dd-dd</b> and <b>I.M.F</b> to <b>X.X.X</b>. <br><br>
So I look for a library which will improve my recognizer by applying these popularly known features. If there is no such library, how can I extract word shape features of a word like </p>

<pre><code>wordshape(""D-Day"") = X-Xxx
</code></pre>

<p>Thanks in advance.</p>
","python, string, nlp, named-entity-recognition","<p>You can solve this problem with regex (regular expressions). The Python standard library for regex is <code>re</code>.</p>

<p>The function below can achieve what you want</p>

<pre><code>def wordshape(text):
    import re
    t1 = re.sub('[A-Z]', 'X',text)
    t2 = re.sub('[a-z]', 'x', t1)
    return re.sub('[0-9]', 'd', t2)

&gt;&gt;&gt; wordshape(""DC10-30"")
'XXdd-dd'
&gt;&gt;&gt; wordshape(""D-Day"")
'X-Xxx'
&gt;&gt;&gt; wordshape('I.M.F')
'X.X.X'
</code></pre>
",2,0,2019,2018-04-20 16:13:30,https://stackoverflow.com/questions/49945812/is-there-any-word-shape-feature-library-for-ner-in-python
sequence tagging task in tensorflow using bidirectional lstm,"<p>I am little interested in sequence tagging for NER. I follow the code ""<a href=""https://github.com/monikkinom/ner-lstm/blob/master/model.py"" rel=""nofollow noreferrer"">https://github.com/monikkinom/ner-lstm/blob/master/model.py</a>"" to make my model like below:</p>

<pre><code>X = tf.placeholder(tf.float32, shape=[None, timesteps , num_input])
Y = tf.placeholder(""float"", [None, timesteps, num_classes])
y_true = tf.reshape(tf.stack(Y), [-1, num_classes])
</code></pre>

<p>the input is,<br>
X: (batch_size,max_sent_length,word_embed_dim) 
<br>and <br>
Y: (batch_size,max_sent_length,number_of_labels)</p>

<p>Then I pass the value to a Bi-direction LSTM unit:</p>

<pre><code>def BiRNN(x):
    x=tf.unstack(tf.transpose(x, perm=[1, 0, 2]))

    def rnn_cell():
        cell = tf.nn.rnn_cell.LSTMCell(rnn_size, forget_bias=1,state_is_tuple=True)
        return cell

    fw_cell=rnn_cell()
    bw_cell=rnn_cell()
    output,_, _ = tf.nn.static_bidirectional_rnn(fw_cell, bw_cell,x, dtype=tf.float32)
    weight, bias = weight_and_bias(2 * rnn_size, num_classes)
    output = tf.reshape(tf.transpose(tf.stack(output), perm=[1, 0, 2]), [-1, 2 * rnn_size])
return (tf.matmul(output, weight) + bias)
</code></pre>

<p>Where, rnn_size = 128</p>

<p>Then I am doing the below calculations:</p>

<pre><code>logits = BiRNN(X)
logits = tf.reshape(tf.stack(logits), [-1, timesteps,num_classes])
prediction = tf.reshape(logits, [-1, num_classes])
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_true))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
train_op = optimizer.minimize(cost)
</code></pre>

<p>I took, batch_size = 64 and 30 epochs. 
<br>But in my model only one label is detected every time. I am not able to point out the problem in my code. Please help.</p>
","python, tensorflow, lstm, sequence-to-sequence, named-entity-recognition","<p>Please check the dimensions of the tensors y_true, output(both the places), logits and prediction and check whether it comes as per your expectation. </p>
",0,0,299,2018-04-22 18:43:25,https://stackoverflow.com/questions/49969484/sequence-tagging-task-in-tensorflow-using-bidirectional-lstm
How to re-train an existing spacy NER model for currency,"<p>I am trying to update an existing spacy model ""en_core_web_sm"" with some different country currency such as ""euro"", ""rupees"", ""eu"", ""Rs."", ""INR"" etc. How can I achieve that ? The spacy tutorial didn't quite help me as training a fixed string such as ""horses"" as ""ANIMAL"" seems different than my requirements. The reason is I can have currency value indifferent formats : ""1 million euros"", ""Rs. 10,000"", ""INR 1 thousand"" etc. My sample dataset contains around  1000 samples with the following format :</p>

<pre><code>TRAIN_DATA = [      
 ("" You have activated International transaction limit for Debit Card ending XXXX1137 on 2017-07-05 12:48:20.0 via NetBanking. The new limit is Rs. 250,000.00"", {'entities' : [(140, 154, 'MONEY')] }),...
]
</code></pre>

<p>Can anyone please help me out with this with the data format, training size or any other relevant information ?</p>
","python, nlp, spacy, named-entity-recognition","<p>The example from the documentation should work for you. I altered it a little to match your variable name. </p>

<pre><code>optimizer = nlp.begin_training()

for itn in range(100):
    random.shuffle(train_data)
    for raw_text, entity_offsets in TRAIN_DATA:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)
        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)
nlp.to_disk('/model')
</code></pre>

<p><a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">Link to Documentation</a></p>
",4,3,3581,2018-05-05 12:06:26,https://stackoverflow.com/questions/50189238/how-to-re-train-an-existing-spacy-ner-model-for-currency
How to distinguish between two Different Named Entities of same name?,"<p>I have few articles, in which I am taking out name using NER Model (Named Entity Recognition). Since NER is classifying into four categories ( PERSON, LOCATION, ORGANISATION, MISCELLANEOUS ). Now I having two people of same name. How will I go about distinguishing between them? <br>
Kindly direct me towards some research available on this problem, if possible.</p>
","machine-learning, nlp, named-entity-recognition","<p>The task you need is called <a href=""https://en.wikipedia.org/wiki/Entity_linking"" rel=""nofollow noreferrer"">Entity Linking</a>, it is a harder problem than Named Entity Recognition.</p>

<p>A good way to start research on this problem is the <a href=""https://aclanthology.coli.uni-saarland.de/catalog?utf8=%E2%9C%93&amp;utf8=%E2%9C%93&amp;search_field=all_fields&amp;q=named+entity+linking&amp;range%5Bpublish_date%5D%5Bbegin%5D=2015&amp;range%5Bpublish_date%5D%5Bend%5D=2017&amp;range%5Bpublish_date%5D%5Bbegin%5D=2016&amp;range%5Bpublish_date%5D%5Bend%5D=2017&amp;commit=Limit"" rel=""nofollow noreferrer"">ACL anthology</a>.</p>
",1,2,649,2018-05-09 13:01:00,https://stackoverflow.com/questions/50254074/how-to-distinguish-between-two-different-named-entities-of-same-name
Stanford NER Tagger and NLTK - not working [OSError: Java command failed ],"<p>Trying to run Stanford NER Taggerand NLTK from a jupyter notebook. 
I am continuously getting </p>

<pre><code>OSError: Java command failed
</code></pre>

<p>I have already tried the hack at 
    <a href=""https://gist.github.com/alvations/e1df0ba227e542955a8a"" rel=""nofollow noreferrer"">https://gist.github.com/alvations/e1df0ba227e542955a8a</a>
and thread 
    <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk/34112695#34112695"">Stanford Parser and NLTK</a> </p>

<p>I am using </p>

<pre><code>NLTK==3.3
Ubuntu==16.04LTS 
</code></pre>

<p>Here is my python code:</p>

<pre><code>Sample_text = ""Google, headquartered in Mountain View, unveiled the new Android phone""

sentences = sent_tokenize(Sample_text)
tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]

PATH_TO_GZ = '/home/root/english.all.3class.caseless.distsim.crf.ser.gz'

PATH_TO_JAR = '/home/root/stanford-ner.jar'

sn_3class = StanfordNERTagger(PATH_TO_GZ,
                       path_to_jar=PATH_TO_JAR,
                              encoding='utf-8')

annotations = [sn_3class.tag(sent) for sent in tokenized_sentences]
</code></pre>

<p>I got these files using following commands:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip
wget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip
wget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip
# Extract the zip file.
unzip stanford-ner-2015-04-20.zip 
unzip stanford-parser-full-2015-04-20.zip 
unzip stanford-postagger-full-2015-04-20.zip
</code></pre>

<p>I am getting the following error:</p>

<pre><code>CRFClassifier invoked on Thu May 31 15:56:19 IST 2018 with arguments:
   -loadClassifier /home/root/english.all.3class.caseless.distsim.crf.ser.gz -textFile /tmp/tmpMDEpL3 -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions ""tokenizeNLs=false"" -encoding utf-8
tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer
Unknown property: |tokenizerFactory|
tokenizerOptions=""tokenizeNLs=false""
Unknown property: |tokenizerOptions|
loadClassifier=/home/root/english.all.3class.caseless.distsim.crf.ser.gz
encoding=utf-8
Unknown property: |encoding|
textFile=/tmp/tmpMDEpL3
outputFormat=slashTags
Loading classifier from /home/root/english.all.3class.caseless.distsim.crf.ser.gz ... Error deserializing /home/root/english.all.3class.caseless.distsim.crf.ser.gz
Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassCastException: java.util.ArrayList cannot be cast to [Ledu.stanford.nlp.util.Index;
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1380)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1331)
    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2315)
Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to [Ledu.stanford.nlp.util.Index;
    at edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2164)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1249)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1366)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1377)
    ... 2 more

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-15-5621d0f8177d&gt; in &lt;module&gt;()
----&gt; 1 ne_annot_sent_3c = [sn_3class.tag(sent) for sent in tokenized_sentences]

/home/root1/.virtualenv/demos/local/lib/python2.7/site-packages/nltk/tag/stanford.pyc in tag(self, tokens)
     79     def tag(self, tokens):
     80         # This function should return list of tuple rather than list of list
---&gt; 81         return sum(self.tag_sents([tokens]), [])
     82 
     83     def tag_sents(self, sentences):

/home/root1/.virtualenv/demos/local/lib/python2.7/site-packages/nltk/tag/stanford.pyc in tag_sents(self, sentences)
    102         # Run the tagger and get the output
    103         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,
--&gt; 104                                        stdout=PIPE, stderr=PIPE)
    105         stanpos_output = stanpos_output.decode(encoding)
    106 

/home/root1/.virtualenv/demos/local/lib/python2.7/site-packages/nltk/__init__.pyc in java(cmd, classpath, stdin, stdout, stderr, blocking)
    134     if p.returncode != 0:
    135         print(_decode_stdoutdata(stderr))
--&gt; 136         raise OSError('Java command failed : ' + str(cmd))
    137 
    138     return (stdout, stderr)

OSError: Java command failed : [u'/usr/bin/java', '-mx1000m', '-cp', '/home/root/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/home/root/english.all.3class.caseless.distsim.crf.ser.gz', '-textFile', '/tmp/tmpMDEpL3', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf-8']
</code></pre>
","nltk, stanford-nlp, named-entity-recognition","<p>Download Stanford <a href=""http://Download%20Stanford%20Named%20Entity%20Recognizer%20version%203.9.1"" rel=""nofollow noreferrer"">Named Entity Recognizer version 3.9.1</a>: see ‘Download’ section from <a href=""https://nlp.stanford.edu/software/CRF-NER.shtml#Download"" rel=""nofollow noreferrer"">The Stanford NLP website</a>.</p>

<p>Unzip it and move 2 files ""ner-tagger.jar"" and ""english.all.3class.distsim.crf.ser.gz"" to your folder</p>

<p>Open jupyter notebook or ipython prompt in your folder path and run the following python code:</p>

<pre><code>import nltk
from nltk.tag.stanford import StanfordNERTagger

sentence = u""Twenty miles east of Reno, Nev., "" \
    ""where packs of wild mustangs roam free through "" \
    ""the parched landscape, Tesla Gigafactory 1 "" \
    ""sprawls near Interstate 80.""

jar = './stanford-ner.jar'

model = './english.all.3class.distsim.crf.ser.gz'

ner_tagger = StanfordNERTagger(model, jar, encoding='utf8')

words = nltk.word_tokenize(sentence)

# Run NER tagger on words
print(ner_tagger.tag(words))
</code></pre>

<p>I tested this on NLTK==3.3 and Ubuntu==16.0.6LTS</p>
",1,1,1898,2018-05-31 11:07:28,https://stackoverflow.com/questions/50622897/stanford-ner-tagger-and-nltk-not-working-oserror-java-command-failed
Understanding Spacy&#39;s Scorer Output,"<p>I'm evaluating a custom NER model that I built using Spacy. I'm evaluating the training sets using Spacy's Scorer class.</p>

<pre><code>    def Eval(examples):
    # test the saved model
    print(""Loading from"", './model6/')
    ner_model = spacy.load('./model6/')

    scorer = Scorer()
    try:
        for input_, annot in examples:
            doc_gold_text = ner_model.make_doc(input_)
            gold = GoldParse(doc_gold_text, entities=annot['entities'])
            pred_value = ner_model(input_)
            scorer.score(pred_value, gold)
    except Exception as e: print(e)

    print(scorer.scores)
</code></pre>

<p>It works fine but I don't understand the output. Here's what I get for each training set.</p>

<pre><code>{'uas': 0.0, 'las': 0.0, 'ents_p': 90.14084507042254, 'ents_r': 92.7536231884058, 'ents_f': 91.42857142857143, 'tags_acc': 0.0, 'token_acc': 100.0}

{'uas': 0.0, 'las': 0.0, 'ents_p': 91.12227805695142, 'ents_r': 93.47079037800687, 'ents_f': 92.28159457167091, 'tags_acc': 0.0, 'token_acc': 100.0}

{'uas': 0.0, 'las': 0.0, 'ents_p': 92.45614035087719, 'ents_r': 92.9453262786596, 'ents_f': 92.70008795074759, 'tags_acc': 0.0, 'token_acc': 100.0}

{'uas': 0.0, 'las': 0.0, 'ents_p': 94.5993031358885, 'ents_r': 94.93006993006993, 'ents_f': 94.76439790575917, 'tags_acc': 0.0, 'token_acc': 100.0}

{'uas': 0.0, 'las': 0.0, 'ents_p': 92.07920792079209, 'ents_r': 93.15525876460768, 'ents_f': 92.61410788381743, 'tags_acc': 0.0, 'token_acc': 100.0}
</code></pre>

<p>Does anyone know what the keys are? I've looked over Spacy's documentation and could not find anything.</p>

<p>Thanks!</p>
","python, spacy, named-entity-recognition","<ul>
<li>UAS (Unlabelled Attachment Score) and LAS (Labelled Attachment Score) are standard metrics to evaluate dependency parsing. UAS is the proportion of tokens whose head has been correctly assigned, LAS is the proportion of tokens whose head has been correctly assigned with the right dependency label (subject, object, etc).</li>
<li><code>ents_p</code>, <code>ents_r</code>, <code>ents_f</code> are the precision, recall and <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""noreferrer"">fscore</a> for the NER task.</li>
<li><code>tags_acc</code> is the POS tagging accuracy.</li>
<li><code>token_acc</code> seems to be the precision for token segmentation.</li>
</ul>
",24,18,10505,2018-06-01 13:41:32,https://stackoverflow.com/questions/50644777/understanding-spacys-scorer-output
How to cluster Named Entity using StanfordNER using python,"<p>Stanford NER provides it NER jars to detect POS tags and NERs. But I am facing one issue with one of the sentences when trying to parse. The sentence is as follows:</p>

<pre><code>Joseph E. Seagram &amp; Sons, INC said on Thursday that it is merging its two United States based wine companies
</code></pre>

<p>Below is my code</p>

<pre><code>st = StanfordNERTagger('./stanford- ner/classifiers/english.all.3class.distsim.crf.ser.gz',
                       './stanford-ner/stanford-ner.jar',
                       encoding='utf-8')
ne_in_sent = []
with open(""./CCAT/2551newsML.txt"") as fd:
    lines = fd.readlines()
    for line in lines:
        print(line)
        tokenized_text = word_tokenize(line)
        classified_text = st.tag(tokenized_text)
        ne_tree = stanfordNE2tree(classified_text)
        for subtree in ne_tree:
            # If subtree is a noun chunk, i.e. NE != ""O""
            if type(subtree) == Tree:
                ne_label = subtree.label()
                ne_string = "" "".join([token for token, pos in subtree.leaves()])
                ne_in_sent.append((ne_string, ne_label))
                print(ne_in_sent)
</code></pre>

<p>when I parse it I get the following entities as the organization.
(Joseph E. Seagram &amp; Sons, Organization) and (Inc, Organization)</p>

<p>Also for some other texts in the file like</p>

<pre><code>TransCo has a very big plane. Transco is moving south.
</code></pre>

<p>It differentiates the organizations due to capitalization hence I get
2 entities (TransCo, organization) and (Transco, organization).</p>

<p>Is it possible to convert these into one entity?</p>
","python, nlp, nltk, stanford-nlp, named-entity-recognition","<p>Used Cosine similarity checker to check the similarity</p>

<p>ref: <a href=""https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings"">Calculate cosine similarity given 2 sentence strings</a></p>
",0,1,546,2018-06-07 08:47:04,https://stackoverflow.com/questions/50736830/how-to-cluster-named-entity-using-stanfordner-using-python
CleanNLP package in R: metadata data frame?,"<p>Let's assume my dataframe looks like this:</p>

<pre><code>bio_text &lt;- c(""Georg Aemilius, eigentlich Georg Oemler, andere Namensvariationen „Aemylius“ und „Emilius“ (* 25. Juni 1517 in Mansfeld; † 22. Mai 1569 in Stolberg (Harz))..."", ""Johannes Aepinus auch: Johann Hoeck, Huck, Hugk, Hoch oder Äpinus (* um 1499 in Ziesar; † 13. Mai 1553 in Hamburg) war ein deutscher evangelischer Theologe und Reformator.\nAepinus wurde als Sohn des Ratsherrn Hans Hoeck im brandenburgischen Ziesar 1499 geboren..."")
doc_id &lt;- c(""1"", ""2"")
url &lt;- c(""https://de.wikipedia.org/wiki/Georg_Aemilius"", ""https://de.wikipedia.org/wiki/Johannes_Aepinus"")
name &lt;- c(""Aemilius, Georg"", ""Aepinus, Johannes"")
place_of_birth &lt;- c(""Mansfeld"", ""Ziesar"")

full_wikidata &lt;- data.frame(bio_text, doc_id, url, name, place_of_birth)
</code></pre>

<p>I want to carry out Named Entity Recognition with the cleanNLP package in R. Therefore, I initialize the tokenizers and the spaCy backend, everything works fine:</p>

<pre><code>options(stringsAsFactors = FALSE)
library(cleanNLP)

cnlp_init_tokenizers()

require(reticulate)
cnlp_init_spacy(""de"")

wikidata &lt;- full_wikidata[,c(""doc_id"", ""bio_text"")]
wikimeta &lt;- full_wikidata[,c(""url"", ""name"", ""place_of_birth"")]

spacy_annotatedWikidata &lt;- cleanNLP::cnlp_annotate(wikidata, as_strings = TRUE, meta = wikimeta)
</code></pre>

<p>My only problem is the metadata. When I run it like this, I get the following warning message: <strong>In cleanNLP::cnlp_annotate(full_wikidata, as_strings = TRUE, meta = wikimeta) : data frame input given along with meta; ignoring the latter</strong>. To be honest, I don't get the documentation concerning <code>meta</code> in <code>cnlp_annotate</code>: ""an optional data frame to bind to the document table"". This means that I should deliver a data frame containing the metadata, right?! Later on, I want to be able to do something like this, e.g. filter out all person entities in document no. 3:</p>

<pre><code>cnlp_get_entity(spacy_annotatedWikidata) %&gt;%
  filter(doc_id == 3, entity_type == ""PER"") %&gt;%
  count(entity)
</code></pre>

<p>Therefore, I have to find a way to access the metadata. Any help would be highly appreciated!</p>
","r, spacy, named-entity-recognition","<p>Fortunatelly, in the meantime I got some help and the advice to take a closer look at the method code of <code>cnlp_annotate</code> on Github: <a href=""https://github.com/statsmaths/cleanNLP/blob/master/R/annotate.R"" rel=""nofollow noreferrer"">https://github.com/statsmaths/cleanNLP/blob/master/R/annotate.R</a>
It says that you only can pass in a metadata dataframe if the input itself is not a dataframe but a file path. So if you do want to pass in a dataframe, the first row has to be <code>doc_id</code>, the second <code>text</code> and the remaining ones are automatically considered as metadata! So in my example only the order in <code>full_wikidata</code> has to be changed: </p>

<pre><code>full_wikidata &lt;- data.frame(doc_id, bio_text, url, name, place_of_birth)
</code></pre>

<p>Like this, it can be directly used as an input in <code>clnp_annotate</code>:</p>

<pre><code>spacy_annotatedWikidata &lt;- cleanNLP::cnlp_annotate(full_wikidata, as_strings = TRUE)
</code></pre>
",1,1,333,2018-06-17 13:10:00,https://stackoverflow.com/questions/50896983/cleannlp-package-in-r-metadata-data-frame
Creating relations in sentence using chunk tags (not NER) with NLTK | NLP,"<p>I am trying to create custom chunk tags and to extract relations from them. Following is the code that takes me to the cascaded chunk tree.</p>

<pre><code>grammar = r""""""
  NPH: {&lt;DT|JJ|NN.*&gt;+}          # Chunk sequences of DT, JJ, NN
  PPH: {&lt;IN&gt;&lt;NP&gt;}               # Chunk prepositions followed by NP
  VPH: {&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$} # Chunk verbs and their arguments
  CLAUSE: {&lt;NP&gt;&lt;VP&gt;}           # Chunk NP, VP
  """"""
cp = nltk.RegexpParser(grammar)
sentence = [(""Mary"", ""NN""), (""saw"", ""VBD""), (""the"", ""DT""), (""cat"", ""NN""),
    (""sit"", ""VB""), (""on"", ""IN""), (""the"", ""DT""), (""mat"", ""NN"")]


chunked = cp.parse(sentence)
</code></pre>

<p><strong>Output -</strong> </p>

<p>(S
  (NPH Mary/NN)
  saw/VBD
  (NPH the/DT cat/NN)
  sit/VB
  on/IN
  (NPH the/DT mat/NN))</p>

<p>Now I am trying to extract relations between the NPH tag values with the text in between using the nltk.sem.extract_rels function, BUT it seems to work ONLY on named entities generated with the ne_chunk function. </p>

<pre><code>IN = re.compile(r'.*\bon\b')
for rel in nltk.sem.extract_rels('NPH', 'NPH', chunked,corpus='ieer',pattern = IN):
        print(nltk.sem.rtuple(rel))
</code></pre>

<p>This gives the following error - </p>

<p><strong>ValueError: your value for the subject type has not been recognized: NPH</strong></p>

<p>Is there an easy way to use only chunk tags to create relations as I don't really want to retrain the NER model to detect my chunk tags as respective named entities</p>

<p>Thank you!</p>
","python, nlp, nltk, named-entity-recognition, chunking","<ol>
<li><code>extract_rels</code> (<a href=""https://www.nltk.org/_modules/nltk/sem/relextract.html#extract_rels"" rel=""noreferrer"">doc</a>)
checks that arguments <code>subjclass</code> and <code>objclass</code> are known NE tags, hence the error with <code>NPH</code>.</li>
<li><p>The easy, ad hoc, way is to rewrite a customized <code>extract_rels</code> function (example below).</p>

<pre><code>import nltk
import re

grammar = r""""""
  NPH: {&lt;DT|JJ|NN.*&gt;+}          # Chunk sequences of DT, JJ, NN
  PPH: {&lt;IN&gt;&lt;NP&gt;}               # Chunk prepositions followed by NP
  VPH: {&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$} # Chunk verbs and their arguments
  CLAUSE: {&lt;NP&gt;&lt;VP&gt;}           # Chunk NP, VP
  """"""
cp = nltk.RegexpParser(grammar)
sentence = [(""Mary"", ""NN""), (""saw"", ""VBD""), (""the"", ""DT""), (""cat"", ""NN""),
    (""sit"", ""VB""), (""on"", ""IN""), (""the"", ""DT""), (""mat"", ""NN"")]

chunked = cp.parse(sentence)

IN = re.compile(r'.*\bon\b')

def extract_rels(subjclass, objclass, chunked, pattern):

    # padding because this function checks right context
    pairs = nltk.sem.relextract.tree2semi_rel(chunked) + [[[]]] 

    reldicts = nltk.sem.relextract.semi_rel2reldict(pairs)

    relfilter = lambda x: (x['subjclass'] == subjclass and
                           pattern.match(x['filler']) and
                           x['objclass'] == objclass)


    return list(filter(relfilter, reldicts))

for e in extract_rels('NPH', 'NPH', chunked, pattern=IN):
    print(nltk.sem.rtuple(e))
</code></pre>

<p>Output:</p>

<pre><code>[NPH: 'the/DT cat/NN'] 'sit/VB on/IN' [NPH: 'the/DT mat/NN']
</code></pre></li>
</ol>
",7,7,925,2018-07-17 21:38:58,https://stackoverflow.com/questions/51390568/creating-relations-in-sentence-using-chunk-tags-not-ner-with-nltk-nlp
Training a model to identify names appearing in a sentence,"<p>I have a dataset containing the names of about 238583 people. The names can contain more than one word for example:
 <code>Willie Enriquez , James J Johnson, D.J. Khaled</code>.
 My problem is to identify these names when it appears in a sentence. I am trying to create a machine learning model that can identify if the input is a name or not. My trouble is figuring the input and output of this model. Since I have a bunch of names I can train a model which can recognise a name when the input is a name, but what about the other words that are part of this sentence. The model should also be able to identify words that are not names. Assuming the sentences can have any other words in it, what would be the ideal dataset for this purpose? Does it make sense to train a model on a random bunch of words and tag it as NonNames?<br>
(The entire sentences in which the names appear is not available. The user can type absolutely anything he/she wants)   </p>

<p>Thankyou.</p>
","machine-learning, nlp, named-entity-recognition","<p>The specifics of the answer may vary according to which model you are using, but the general idea is more or less the following:</p>

<p>You are trying to solve a classification task, precisely a binary classification task where you want to distinguish between proper names (assuming from your example) from other expressions. </p>

<p>The input to your model, in the most general case, are the features of the example that you want to classify: you should decide what features you think are useful to distinguish such names (e.g., number of words, contains capital letter, every word is capitalized, contains dotted letters, contains any word that you already have in your dataset, etc...). The output is the class, that is 0/1 for non-names/names. </p>

<p>You then train your model with positive examples from the dataset that you have and negative examples (i.e. non-names) taken from random words for non-names.</p>

<p>If the use can enter full sentences then you will need to do a preprocessing step where you extract all sequences of length N (word n-grams) and classify each of them individually with your previously trained model.</p>
",1,1,2029,2018-07-23 10:30:30,https://stackoverflow.com/questions/51476682/training-a-model-to-identify-names-appearing-in-a-sentence
How to train NER to recognize that a word is not an entity?,"<p>I may have worded my question poorly, but basically I have been training new models using spaCy for NER.  I have trained some custom entities and it's doing a really great job when I test it.  However, when I send it something that shouldn't be recognized as an entity, it seems to guess one of the entities anyways.  I am guessing it's because I never trained it what would = O(I think that's how stanford does it).</p>

<p>Here is a sample of my training data, does this look right?  Do I need to just add trash values and set the entity as O?</p>

<pre><code>[ ""644663"" , {""entities"": [[0,6, ""CARDINAL""]]}],
[ ""871448"" , {""entities"": [[0,6, ""CARDINAL""]]}],
[ ""6/26/1967"" , {""entities"": [[0,9, ""DATE""]]}],
[ ""1/21/1969"" , {""entities"": [[0,9, ""DATE""]]}],
[ ""GORDON GARDIN"" , {""entities"": [[0,13, ""PERSON""]]}],
[ ""CANDRA CARDINAL"" , {""entities"": [[0,15, ""PERSON""]]}],
[ ""FIAT"" , {""entities"": [[0,4, ""CARMAKE""]]}],
[ ""FORD"" , {""entities"": [[0,4, ""CARMAKE""]]}]
</code></pre>
","machine-learning, nlp, stanford-nlp, spacy, named-entity-recognition","<p>You're correct in that the problem is that you haven't shown the system anything that's <em>not</em> an entity. You don't want to add ""trash values"" however. Spacy expects your training strings to be strings with entities in context, not just singular examples of entities. So one training example should look more like:</p>

<p><code>[ ""My uncle drives a Ford"" , {""entities"": [(18,22, ""CARMAKE"")]}]</code></p>

<p>This will allow your system to train to recognize entities in context, and recognize more entities than just the specific training examples you give it (e.g. a well trained system would be able to recognize ""Chrysler"" and ""Toyota"" as car makes in addition to Ford and Fiat). Spacy has more in-depth <a href=""https://spacy.io/usage/training#example-new-entity-type"" rel=""nofollow noreferrer"">examples</a> for training custom entities, so I'd recommend you check that out.</p>
",2,0,698,2018-07-25 14:28:18,https://stackoverflow.com/questions/51521429/how-to-train-ner-to-recognize-that-a-word-is-not-an-entity
Output result to csv file - TypeError writerow() takes 2 positional arguments but 5 were given,"<p>I'm trying to output my spaCy NER results to a csv file rather than a plain text file.</p>

<p>So far I have this code to try and achieve that:</p>

<pre><code>def spacy_ner():
    with open(""spacy_results.txt"", ""w"") as f:

        cf = csv.DictWriter(f, ['Character', 'begin', 'end'\
                                , 'Label'], extrasaction='ignore')
        cf.writeheader()

        nlp = spacy.load('en_core_web_md')
        doc = nlp(text)

        for ent in doc.ents:
            if ent.label_ == 'PERSON':
                cf.writerow(ent.text, ent.start_char, ent.end_char, ent.label_)
        print(""Processing done"")
</code></pre>

<p>I'm not sure how to create the csv file, while trying to display all the attributes mentioned above: <code>ent.text, ent.start_char, ent.end_char, ent.label_</code></p>

<p>Running the code above calls a <code>TypeError: writerow() takes 2 positional arguments but 5 were given</code></p>

<p>It works fine if I write the result to a .txt file, but it would be nice if it was a more structred and easy to access csv file.</p>

<p>What am I not seeing here that I need to change? Any help would be great!</p>
","python, csv, typeerror, spacy, named-entity-recognition","<p>A csv.DictWriter requires a dictionary for writing.  If your <code>ent</code> object doesn't have a <code>to_dict()</code> method, you will have to make one.</p>

<pre><code>    for ent in doc.ents:
        if ent.label_ == 'PERSON':
            d = {'Character':ent.text, 'begin':ent.start_char,
                 'end':ent.end_char,'Label':ent.label_}
            cf.writerow(d)
</code></pre>
",1,1,3105,2018-08-01 18:22:18,https://stackoverflow.com/questions/51639941/output-result-to-csv-file-typeerror-writerow-takes-2-positional-arguments-bu
Documentation for training a named entity recognizer model from an IOB annotated train set,"<p>Is there any documentation which can be used to find out which properties should be set for training a Stanford NER model from a train set with IOB annotation tags in Java? </p>

<p>Thanks in advance.</p>
","java, stanford-nlp, named-entity-recognition","<p>There is a detailed example located here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/ner.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/ner.html</a></p>

<p>see:
""Training or retraining new models""</p>
",1,0,82,2018-08-02 18:39:30,https://stackoverflow.com/questions/51660233/documentation-for-training-a-named-entity-recognizer-model-from-an-iob-annotated
Spacy - Chunk NE tokens,"<p>Let's say that I have a document, like so:</p>

<pre><code>import spacy

nlp = spacy.load('en')

doc = nlp('My name is John Smith')

[t for t in doc]
&gt; [My, name, is, John, Smith]
</code></pre>

<p>Spacy is intelligent enough to realize that 'John Smith' is a multi-token named entity:</p>

<pre><code>[e for e in doc.ents]
&gt; [John Smith]
</code></pre>

<p>How can I make it chunk named entities into discrete tokens, like so:</p>

<pre><code>&gt; [My, name, is, John Smith]
</code></pre>
","python, tokenize, spacy, named-entity-recognition","<p>Spacy documentation on NER says that you can access token entity annotations using the <code>token.ent_iob_</code> and <code>token.ent_type_</code> attributes.</p>

<p><a href=""https://spacy.io/usage/linguistic-features#accessing"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#accessing</a></p>

<p>Example:</p>

<pre><code>import spacy

nlp = spacy.load('en')
doc = nlp('My name is John Smith')


ne = []
merged = []
for t in doc:
    # ""O"" -&gt; current token is not part of the NE
    if t.ent_iob_ == ""O"":
        if len(ne) &gt; 0:
            merged.append("" "".join(ne))
            ne = []
        merged.append(t.text)
    else:
        ne.append(t.text)

if len(ne) &gt; 0:
    merged.append("" "".join(ne))

print(merged)
</code></pre>

<p>This will print:</p>

<pre><code>['My', 'name', 'is', 'John Smith']
</code></pre>
",1,0,726,2018-08-11 18:35:18,https://stackoverflow.com/questions/51802645/spacy-chunk-ne-tokens
Stanford OpenNLP extract only those names that are mentioned in relation to (identified) organisation,"<p>With the Stanford NER tagger, I am able to extract all PERSONs and ORGANISATIONS as expected. Here is a short snippet:</p>

<pre><code>    ss=tagger.get_entities(text)
    xorg=unique_list(ss.get('ORGANIZATION'))
    xper=unique_list(ss.get('PERSON'))
    out= (xorg,xperson)
    #out is written to database
</code></pre>

<p>My question is how do I extract only those PERSON names which have a relation to named ORGANISATION? Specifically, I want the output as a triplet: PERSON, RELATION, ORGANISATION.</p>

<p>For either ""Enron Chairman Kenneth Lay"" OR ""Kenneth Lay, Chairman, Enron"" I expect the output to read as (Kenneth Lay) (Chairman) (Enron).</p>

<p>Any help will be useful.</p>
","python-3.x, stanford-nlp, opennlp, named-entity-recognition","<p>Plain NER is just about finding (named) entities and label them correctly. Your task is called relation extraction. You should look at following links:</p>

<p><a href=""https://nlp.stanford.edu/software/relationExtractor.html"" rel=""nofollow noreferrer"" title=""Stanford Relation Extractor"">Stanford Relation Extractor</a> extracts relations between entities: <code>Live_In</code>, <code>Located_In</code>, <code>OrgBased_In</code>, <code>Work_For</code>, and <code>None</code>.</p>

<p><a href=""https://nlp.stanford.edu/software/openie.html"" rel=""nofollow noreferrer"" title=""Stanford OpenIE"">Stanford OpenIE</a> is able to extract arbitrary binary relations from text. Thus, doing NER isn't necessary beforehand.</p>

<p>Maybe one of these tools helps you with your task.</p>
",0,0,75,2018-08-24 15:46:14,https://stackoverflow.com/questions/52007847/stanford-opennlp-extract-only-those-names-that-are-mentioned-in-relation-to-ide
How to link NE with it&#39;s dependent?,"<p>I need to extract descriptions of locations from a text. For now, I am trying to get location with it's adjectival modifier.<br>
For example from  </p>

<blockquote>
  <p>In compact Durham you don't need transport to get around.  </p>
</blockquote>

<p>I want to get  </p>

<blockquote>
  <p>compact Durham  </p>
</blockquote>

<p>I have <code>CoreEntityMention</code> and <code>SemanticGraph</code> of my sentence. I can get index of NE's token to find <code>IndexedWord</code> in <code>SemanticGraph</code>, but NE may contain more than one token so I don't know hot to build the link. I saw <a href=""https://stackoverflow.com/questions/40479342/how-to-extract-named-entity-verb-from-text"">this similar question</a>, but didn't understand suggested solution. Do need I to check dependence for each token?<br>
Here is my approach written in Kotlin (no big difference from Java):  </p>

<pre><code>    val dependencies = mutableListOf&lt;String&gt;()
    val depGraph = entityMention.sentence().dependencyParse()

    for (token in entityMention.tokens()) {
        val node = depGraph.getNodeByIndex(token.index())
        for (dependence in depGraph.childPairs(node)) {
            if (dependence.first.shortName == ""amod"") {
                dependencies.add(dependence.second.toString())
            }
        }
    }
</code></pre>

<p>Is it correct and simplest way?</p>
","nlp, stanford-nlp, named-entity-recognition","<p>Yes, at the moment I think the best thing you can do is iterate through the tokens of the entity mention since dependencies exist between tokens.</p>

<p>I'll note this question, and maybe we can add some code to make this easier in the future.</p>
",0,0,49,2018-10-08 17:46:24,https://stackoverflow.com/questions/52707579/how-to-link-ne-with-its-dependent
Tabular data using spacy,"<p>I'm using Spacy and need some help to train our model with custom entities given in tabular format in a word/pdf document.  </p>

<p>I'm able to train it with a custom entity based on an example of ANIMAL and it's working fine. In this case, we are providing the start and the end index of the aforementioned custom entity in a given text. </p>

<pre><code>(""Horses are too tall and they pretend to care about your feelings"", {
    'entities': [(0, 6, 'ANIMAL')]
}),
</code></pre>

<p>My question comes in case of Tabular format:<br>
How can I give indexes like ANIMAL example?<br>
Can anyone please guide and assist?</p>

<p><a href=""https://i.sstatic.net/80LbU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/80LbU.png"" alt=""enter image description here""></a></p>
","nlp, spacy, named-entity-recognition","<p>After a lots of research and article, I found a way to pass it through.</p>

<ol>
<li>Convert this table as text.</li>
<li>As you convert this as text. this will add lots of white spaces etc.</li>
<li>Replace them with spaces. </li>
<li>This will convert you table as paragraph.</li>
<li>Now you can give indexes as sentences, and train your model.</li>
</ol>

<p>Further, you can use dependency parser algorithm to find correct values linked with head ( in case, a values belongs to multiple key) </p>
",1,2,1652,2018-10-16 13:22:42,https://stackoverflow.com/questions/52836481/tabular-data-using-spacy
Python NLTK: Stanford NER tagger error message: NLTK was unable to find the java file,"<p>Trying to get Stanford NER working with Python. Followed some instructions on the web, but got the error message: ""NLTK was unable to find the java file!
Use software specific configuration paramaters or set the JAVAHOME environment variable."" What was wrong? Thank you! </p>

<pre><code>from nltk.tag.stanford import StanfordNERTagger
from nltk.tokenize import word_tokenize

model = r'C:\Stanford\NER\classifiers\english.muc.7class.distsim.crf.ser.gz'
jar = r'C:\Stanford\NER\stanford-ner-3.9.1.jar'

ner_tagger = StanfordNERTagger(model, jar, encoding = 'utf-8')

text = 'While in France, Christine Lagarde discussed short-term stimulus ' \
       'efforts in a recent interview with the Wall Street Journal.'

words = word_tokenize(text)
classified_words = ner_tagger.tag(words)
</code></pre>
","python, nltk, stanford-nlp, named-entity-recognition","<p>Found the solution on the web. Replace the path with your own. </p>

<blockquote>
<pre><code> import os

 java_path = ""C:/../../jdk1.8.0_101/bin/java.exe""   
 os.environ['JAVAHOME'] = java_path
</code></pre>
</blockquote>

<p>or:</p>

<blockquote>
<pre><code>import nltk

nltk.internals.config_java('C:/../../jdk1.8.0_101/bin/java.exe')
</code></pre>
</blockquote>

<p>Source: <a href=""https://tianyouhu.wordpress.com/2016/09/01/problem-of-nltk-with-stanfordtokenizer/"" rel=""nofollow noreferrer"">https://tianyouhu.wordpress.com/2016/09/01/problem-of-nltk-with-stanfordtokenizer/</a></p>
",0,0,1316,2018-10-18 04:21:36,https://stackoverflow.com/questions/52866988/python-nltk-stanford-ner-tagger-error-message-nltk-was-unable-to-find-the-java
Rule based named entity recognizer without parts of speech label or any other information,"<p>I'm working on a project where I am trying to build a named entity recognizer from texts. So basically I want to build and experiment the NER in 3 different ways.</p>

<p>First, I want to build it using only segmented sentences-> tokenized words. To clarify, I want to input only split/tokenized words into the system. Once again, the NER system is rule-based. Hence, it can only use rules to conclude which is a named entity. In the first NER, it will not have any chunk information or part of speech label. Just the tokenized words. Here, the efficiency is not the concern. Rather the concern lies in comparing the 3 different NERs, how they perform. (The one I am asking about is the 1st one).</p>

<p>I thought of it for a while and could not figure out any rules or any idea of coming up with a solution to this problem. One naive approach would be to conclude all words beginning with an uppercase and that does not follow a period to be a named entity. </p>

<p>Am I missing anything? Any heads up or guidelines would help.</p>
","nlp, entity, named-entity-recognition, named, part-of-speech","<p>Typically NER relies on preprocessing such as part-of-speech tagging (named entities are typically nouns), so not having this basic information makes the task more difficult and therefore more prone to error. There will be certain patterns that you could look for, such as the one you suggest (although what do you do with sentence-initial named entities?). You could add certain regular expression patterns with prepositions, e.g. (Title_case_token)+ of (the)? (Title_case_token)+ would match ""Leader of the Free World"", ""Prime Minister of the United Kindom"", and ""Alexander the Great"". You might also want to consider patterns to match acronyms such as ""S.N.C.F."", ""IBM"", ""UN"", etc. A first step is probably to look for some lexical resources (i.e. word lists) like country names, first names, etc., and build from there.</p>

<p>You could use <a href=""http://spacy.io/"" rel=""nofollow noreferrer"">spaCy</a> (Python) or <a href=""https://nlp.stanford.edu/software/tokensregex.html"" rel=""nofollow noreferrer"">TokensRegex</a> (Java) to do token-based matching (and not use the linguistic features they add to the tokens).</p>
",0,0,242,2018-10-25 05:26:46,https://stackoverflow.com/questions/52981868/rule-based-named-entity-recognizer-without-parts-of-speech-label-or-any-other-in
How to find similar noun phrases in NLP?,"<p>Is there a way to identify similar noun phrases. Some suggest use pattern-based approaches, for example <code>X as Y</code> expressions:</p>
<blockquote>
<p>Usain Bolt as Sprint King</p>
<p>Liverpool as Reds</p>
</blockquote>
","nlp, text-classification, synonym, named-entity-recognition, pattern-synonyms","<p>There are many techniques to find alternative names for a given entity,
using patterns such as: </p>

<ul>
<li><code>X also known as Y</code> </li>
<li><code>X also titled as Y</code></li>
</ul>

<p>and scanning large collections of documents (e.g., Wikipedia or news papers articles) is one way to do it. </p>

<p>There are also other alternatives, one I remember is using Wikipedia inter-links structure, for instance, by exploring the redirect links between articles. You can download a file with a list of redirects from here: <a href=""https://wiki.dbpedia.org/Downloads2015-04"" rel=""nofollow noreferrer"">https://wiki.dbpedia.org/Downloads2015-04</a> and exploring the file you can find alternative names/synonyms for entities, e.g.:</p>

<ul>
<li><code>Kennedy_Centre -&gt; John_F._Kennedy_Center_for_the_Performing_Arts&gt;</code></li>
<li><code>Lord_Alton_of_Liverpool -&gt; David_Alton,_Baron_Alton_of_Liverpool</code></li>
<li><code>Indiana_jones_2 -&gt; Indiana_Jones_and_the_Temple_of_Doom</code></li>
</ul>

<p>Another thing you can do is combine these two techniques, for instance, look for text segments where both <code>Indiana Jones</code> and <code>Indiana_Jones_and_the_Temple_of_Doom</code> occur and are not further apart more than, let's say, 4 or 5 tokens. You might find patterns like <code>also titled as</code>, then you can use these patterns to find more synonyms/alternative names.</p>
",2,3,354,2018-10-27 20:10:17,https://stackoverflow.com/questions/53025861/how-to-find-similar-noun-phrases-in-nlp
Named Entity Recognition influence of previous sentence,"<p>In Named Entity Recognition (NER), does the previous sentence have any influence on the current sentence? Is the result the same if you apply NER on every sentence separately compared to applying NER on articles consisting of multiple sentences? </p>

<p>More specifically, I'm using Spacy NER. This is approach one:</p>

<pre><code>import spacy
nlp = spacy.load('en')

sentences = ""My name is Bruce and I come from New York. Pete is my best friend from Amsterdam.""

nlp_object = nlp(sentences)
print([(y.text, y.label_) for y in nlp_object.ents])
</code></pre>

<p>which yields this result:</p>

<pre><code>[('Bruce', 'PERSON'), ('New York', 'GPE'), ('Pete', 'PERSON'), ('Amsterdam', 'GPE')]
</code></pre>

<p>But instead of the last two lines, you could also split the sentences and apply NER on every sentence:</p>

<pre><code>for s in sentences.split("".""):
    nlp_object = nlp(s)
    print([(y.text, y.label_) for y in nlp_object.ents])
</code></pre>

<p>Which returns the same result (but instead in two separate lines):</p>

<pre><code>[('Bruce', 'PERSON'), ('New York', 'GPE')]
[('Pete', 'PERSON'), ('Amsterdam', 'GPE')]
</code></pre>

<p>In this example, running the code gives the same extracted entities. But how does the underlying model do this? Is there any influence of the previous sentence in NER? And is there any difference between different implementations of NER?</p>
","python, spacy, named-entity-recognition","<p>Spacy NER system uses a deep neural network to train millions of examples of word-entity pairs. The pairs are usually trained as separate sentences if you look at their sample training codes <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">here.</a>. </p>

<p>While I do not know how exactly the pre-trained model that spacy provides is trained, I can assume that they are also trained using single sentences. Even if they aren't, previous sentences should not have any influence, because the training data is not given to deep learning system as words, but as vector representations, learned from other samples of text. Take a look at <a href=""https://en.wikipedia.org/wiki/Word_embedding"" rel=""nofollow noreferrer"">this article</a> to understand how contextual words affect the prediction.</p>
",4,3,1124,2018-11-13 08:25:55,https://stackoverflow.com/questions/53276718/named-entity-recognition-influence-of-previous-sentence
"compare NER library from Stanford coreNLP, SpaCy And Google cloud","<p>I want to recognise <strong>person</strong> name from text. But i'm getting confused which NLP library I have to use for NER. I find out following best NLP library for NER
1. Stanford coreNLP
2. Spacy
3. Google cloud.</p>

<p>I unable to find out which library will give more accurate result and good performance. Please help me here.</p>
","nlp, stanford-nlp, spacy, named-entity-recognition, google-natural-language","<p>spaCy have the industrial-strength in terms of NLP and obviously faster and accurate in terms of NER. It is also bundled with multi-lingual models. Check <a href=""https://spacy.io/usage/linguistic-features#section-named-entities"" rel=""nofollow noreferrer"">spaCy</a></p>

<p>Also AllenNLP comes with state-of-the-art NER model but slightly complex to use. Check <a href=""http://demo.allennlp.org/named-entity-recognition"" rel=""nofollow noreferrer"">AllenNLP demo</a></p>

<p>If paywall is not the issue then I would suggest to go with Google's Cloud Natural Language (of course it is faster and accurate).</p>

<p>I have personally used spaCy and AllenNLP. I would say go with spaCy if you are seeking to just start with.</p>

<p>Hope this helps.</p>
",2,0,5023,2018-11-22 12:06:20,https://stackoverflow.com/questions/53430654/compare-ner-library-from-stanford-corenlp-spacy-and-google-cloud
I have a question regarding practical implementation of Named Entity Recognition in NLP,"<p>If we Consider two Named Entity Relation systems, one based on the use of word embedding and the other using both word and character embedding jointly. How can we intuitively conclude which model is better for NER task? Is any illustration possible for above case?</p>
","nlp, stanford-nlp, named-entity-recognition","<p>If I understand your question correctly, you may train (if they require training) your models on some annotated dataset like CONL-2003 <a href=""https://www.clips.uantwerpen.be/conll2003/ner/"" rel=""nofollow noreferrer"">https://www.clips.uantwerpen.be/conll2003/ner/</a> and compare their accuracies using the test-part of the dataset.</p>
",0,0,87,2018-11-26 09:48:45,https://stackoverflow.com/questions/53478429/i-have-a-question-regarding-practical-implementation-of-named-entity-recognition
Runnig DeepPavlov Named Entity Recognition,"<p>How I can run NER from DeepPavlov? </p>

<p>I have an input file with sentences ""sentences.txt"" and need to put results in ""result.txt"". </p>
","deep-learning, nlp, named-entity-recognition","<p>There are several choices to do named-entity recognition with DeepPavlov</p>

<p>The Pythonic way</p>

<pre><code>from deeppavlov import configs, build_model

ner_model = build_model(configs.ner.ner_ontonotes, download=True)
ner_model(['Computer Sciences Corp. is close to making final an agreement to buy Cleveland Consulting Associates'])
</code></pre>

<p>and from the command line</p>

<pre><code>python deeppavlov/deep.py interact ner_ontonotes [-d]
</code></pre>
",0,1,202,2018-11-26 14:19:47,https://stackoverflow.com/questions/53483101/runnig-deeppavlov-named-entity-recognition
Removing names from noun chunks in spacy,"<p>Is there a way to remove name of person in noun chunks ?</p>

<p>Here is the code </p>

<pre><code>import en_vectors_web_lg
nlp = en_vectors_web_lg.load()
text = ""John Smith is lookin for Apple ipod""
doc = nlp(text)
for chunk in doc.noun_chunks:
     print(chunk.text)
</code></pre>

<p>Current output</p>

<pre><code>John Smith
Apple ipod
</code></pre>

<p>I would like to have an output like below where name of the people is ignored. How to achieve this ?</p>

<pre><code>Apple ipod
</code></pre>
","python-3.x, nlp, spacy, named-entity-recognition","<p>Reference <a href=""https://spacy.io/usage/linguistic-features#101"" rel=""nofollow noreferrer"">spaCy ents</a></p>

<pre><code>import spacy
# loading the model
nlp = spacy.load('en_core_web_lg')
doc = nlp(u'""John Smith is lookin for Apple ipod""')
# creating the filter list for tokens that are identified as person
fil = [i for i in doc.ents if i.label_.lower() in [""person""]]
# looping through noun chunks
for chunk in doc.noun_chunks:
    # filtering the name of the person
    if chunk not in fil:
        print(chunk.text)
</code></pre>

<p>Output:</p>

<pre><code>Apple ipod
</code></pre>

<p>Hope this helps.</p>
",2,0,5181,2018-11-29 08:07:14,https://stackoverflow.com/questions/53534376/removing-names-from-noun-chunks-in-spacy
spaCy coreference resolution - named entity recognition (NER) to return unique entity ID&#39;s?,"<p>Perhaps I've skipped over a part of the docs, but what I am trying to determine is a unique ID for each entity in the standard NER toolset. For example:</p>

<pre><code>import spacy
from spacy import displacy
import en_core_web_sm
nlp = en_core_web_sm.load()

text = ""This is a text about Apple Inc based in San Fransisco. ""\
        ""And here is some text about Samsung Corp. ""\
        ""Now, here is some more text about Apple and its products for customers in Norway""

doc = nlp(text)

for ent in doc.ents:
    print('ID:{}\t{}\t""{}""\t'.format(ent.label,ent.label_,ent.text,))


displacy.render(doc, jupyter=True, style='ent')
</code></pre>

<p>returns:</p>

<blockquote>
<pre><code>ID:381    ORG ""Apple Inc"" 
ID:382    GPE ""San Fransisco"" 
ID:381    ORG ""Samsung Corp."" 
ID:381    ORG ""Apple"" 
ID:382    GPE ""Norway""
</code></pre>
</blockquote>

<p>I have been looking at <code>ent.ent_id</code> and <code>ent.ent_id_</code> but these are inactive according to the <a href=""https://spacy.io/api/token"" rel=""nofollow noreferrer"">docs</a>. I couldn't find anything in <code>ent.root</code> either. </p>

<p>For example, in <a href=""https://cloud.google.com/natural-language/"" rel=""nofollow noreferrer"">GCP NLP</a> each entity is returned with an ⟨entity⟩number that enables you to identify multiple instances of the same entity within a text.</p>

<blockquote>
  <p>This is a ⟨text⟩2 about ⟨Apple Inc⟩1 based in ⟨San Fransisco⟩4. And
  here is some ⟨text⟩3 about ⟨Samsung Corp⟩6. Now, here is some more
  ⟨text⟩8 about ⟨Apple⟩1 and its ⟨products⟩5 for ⟨customers⟩7 in
  ⟨Norway⟩9""</p>
</blockquote>

<p>Does spaCy support something similar? Or is there a way using NLTK or Stanford?</p>
","python, nlp, spacy, information-extraction, named-entity-recognition","<p>You can use neuralcoref library to get coreference resolution working with SpaCy's models as:</p>
<pre><code># Load your usual SpaCy model (one of SpaCy English models)
import spacy
nlp = spacy.load('en')

# Add neural coref to SpaCy's pipe
import neuralcoref
neuralcoref.add_to_pipe(nlp)

# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.
doc = nlp(u'My sister has a dog. She loves him.')

doc._.has_coref
doc._.coref_clusters
</code></pre>
<p>Find the installation and usage instructions here: <a href=""https://github.com/huggingface/neuralcoref"" rel=""nofollow noreferrer"">https://github.com/huggingface/neuralcoref</a></p>
",4,8,3292,2018-12-12 19:57:12,https://stackoverflow.com/questions/53750468/spacy-coreference-resolution-named-entity-recognition-ner-to-return-unique-e
Automatic Summarization using Named Entity Recognition,"<p>I would like to use <strong>Named Entity Recognition</strong> (NER) to auto summarize <strong>Airline ticket</strong> based on a given dataset.</p>

<p>So basically this is my dataset.</p>

<p><a href=""https://i.sstatic.net/ajUcG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ajUcG.png"" alt=""enter image description here""></a></p>

<p>Here i need to create a summary about the details of passenger in a pdf like :</p>

<blockquote>
  <p>The PNR Number ____(PNRNum) refers to the passenger name
  ____(Name) travelling from ____(Dep Airport),____(Start Country) to ____(Arr Airport),____(End Country) starting at ____(Start Time). The flight number is ____(Flight No) which is _____(Int Dom) using
  _____(Cabin Class) ticket of base fare _____(Base Fare).</p>
</blockquote>

<p>Here when the PNR Number should be given as input to enter in the first blank space and the corresponding data from dataset should be filled in remaining blank spaces.</p>

<pre><code>airline = pd.read_csv(""AIR-LINE.csv"")
def create_airline_ticket():
    c = canvas.Canvas('AIRlines.pdf')

    c.setFont(""Courier"", 20)
    c.drawCentredString(300, 700, 'Airline Ticket')
    c.setFont(""Courier"", 14)
    form = c.acroForm

    c.drawString(10, 650, 'The PNR Number')
    options = [('airline.loc[[0, 10], :]')]
    form.choice(name='choice1', tooltip='Field choice1',
                value='A',
                x=165, y=645, width=72, height=20,
                borderColor=magenta, fillColor=pink, 
                textColor=blue, forceBorder=True, options=options)

    c.save()
</code></pre>

<p>I thought of using ReportLabs module in order to use listbox available in it. But it didn't go accordingly. I have to do with some other way.</p>

<p>So could you suggest me a step by step procedure? Since i'm a beginner in python, i could learn easily. Thanks. </p>
","python, nlp, spacy, named-entity-recognition","<p>Yes, I would definitely recommend SpaCy with python. The other option is StanfordNER. </p>

<p>I don't understand what you mean by reference? You mean if somebody else tried to do the airline ticket summarization?</p>
",1,0,269,2018-12-18 06:30:47,https://stackoverflow.com/questions/53827517/automatic-summarization-using-named-entity-recognition
What is the list of possible tags with a description of CoNLL 2003 NER Task?,"<p>I need to do some NER. I've found <a href=""https://github.com/deepmipt/DeepPavlov"" rel=""nofollow noreferrer"">DeepPavlov</a> library that does this.</p>

<p>Here is an example from <a href=""http://docs.deeppavlov.ai/en/latest/components/ner.html"" rel=""nofollow noreferrer"">docs</a>:</p>

<pre class=""lang-py prettyprint-override""><code>from deeppavlov import configs, build_model

ner_model = build_model(configs.ner.ner_ontonotes, download=True)
ner_model(['Bob Ross lived in Florida'])
&gt;&gt;&gt; [[['Bob', 'Ross', 'lived', 'in', 'Florida']], [['B-PERSON', 'I-PERSON', 'O', 'O', 'B-GPE']]]
</code></pre>

<p>I don't understand what all those tags mean. As I understood from the documentation, they are in the CoNLL 2003 NER Task format.</p>

<p>Can somebody point me at the list of possible tags with a description of CoNLL 2003 NER Task?</p>
","tags, named-entity-recognition, conll","<p>For NER task there are some common types of entities used as tags:</p>

<ul>
<li>persons (PER)</li>
<li>organizations (ORG)</li>
<li>monetary values (MONEY)</li>
<li>Geopolitical entity, i.e. countries, cities, states (GPE)</li>
</ul>

<p>and many others</p>

<p>Furthermore, to distinguish adjacent entities with the same tag many applications use BIO tagging scheme. Here <strong>B</strong> denotes the beginning of an entity, <strong>I</strong> stands for ""inside"" and is used for all words comprising the entity except the first one, and <strong>O</strong> means the absence of entity.</p>

<p>So on the example above, <strong>B-PERSON</strong> means that the person name begins with the token <em>Bob</em>, the next tag <strong>I-PERSON</strong> says that <em>Ross</em> relates to the entity as the previous tag. Then goes <strong>O</strong> which means that <em>lived</em> doesn't belong to any entity, the same is with <em>in</em>, whereas <em>Florida</em> is the begginging of <strong>Geopolitical entity (GPE)</strong>.</p>

<p>Please let me know if this was helpful enough.</p>
",5,3,1444,2018-12-26 15:16:37,https://stackoverflow.com/questions/53933854/what-is-the-list-of-possible-tags-with-a-description-of-conll-2003-ner-task
NameError: name &#39;ne_chunk&#39; is not defined,"<p>I am currently learning named-entity recognition using NLTK. Here is my code:</p>

<pre><code>from nltk.chunk import conlltags2tree, tree2conlltags
from pprint import pprint
iob_tagged = tree2conlltags(cs)
pprint(iob_tagged)

ne_tree = ne_chunk(pos_tag(word_tokenize(ex)))
print(ne_tree)
</code></pre>

<p>and it's giving me an error:</p>

<blockquote>
  <p>NameError                                 Traceback (most recent call last)
   in 
  ----> 1 ne_tree = ne_chunk(pos_tag(word_tokenize(ex)))
        2 print(ne_tree)</p>
  
  <p>NameError: name 'ne_chunk' is not defined</p>
</blockquote>

<p>I have tried other example of NLTK, whenever it has a ne_chunk it gives an error too. Can you please help me? I am using Ubuntu 18.04, and python 3.7.1</p>
","python, named-entity-recognition","<p>You need to download the below packages:
The named entity chunker will give you a tree containing both chunks and tags.</p>

<pre><code> # nltk for NER-tagging
 import nltk
 from nltk.corpus import conll2000
 from nltk.chunk import conlltags2tree, tree2conlltags
 from nltk.chunk import ne_chunk
 from nltk import pos_tag

 sentence = ""Clement and Mathieu are working at Apple.""
 ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))
</code></pre>
",0,-1,4389,2019-01-16 06:19:56,https://stackoverflow.com/questions/54211431/nameerror-name-ne-chunk-is-not-defined
Named Entity Recognition failing to show lists,"<p>The idea is that I'm using Named Entity Recognition (NER) on a tokenised text which is also tagged.</p>

<pre><code>def make_tag_lists(sents):
    tokens=[]
    pos=[]
    ner=[]
    for sent in sents:
        for t in sent:
            tokens.append(t.text)
            pos.append(t.pos_)
            ner.append(t.ent_type_)
    return tokens,pos,ner

tokens,pos,ner = make_tag_lists(sample)

def extract_entities(tokenlist,taglist,tagtype):
    entities={}
    inentity=False
    for i,(token,tag) in enumerate(zip(tokenlist,taglist)):
        if tag==tagtype:
            if inentity:
                entity+="" ""+token
            else:
                entity=token
                inentity=True
        elif inentity:
            entities[entity]=entities.get(entity,0)+1
            inentity=False
    return entities

people=extract_entities(tokens,ner,""PERSON"")
top_people=sorted(people.items(),key=operator.itemgetter(1),reverse=True)[:20]
print(top_people)
</code></pre>

<p>What I should be receiving is the top 20 most commonly referred to people in a list, though my output is currently an empty list. There are no syntax errors and not sure where I've gone wrong.</p>
","python, tags, token, named-entity-recognition","<p>I suggest you trying to skip the first block of your code and check the remaining execution flow.</p>

<pre><code># tokens,pos,ner = make_tag_lists(sample)
tokens = ['Hi','FOO','BAR',""it's"",'ME']
ner =['MISC','PERSON','PERSON','MISC','PERSON']
def extract_entities(tokenlist,taglist,tagtype):
    entities={}
    inentity=False
    for i,(token,tag) in enumerate(zip(tokenlist,taglist)):
        if tag==tagtype:
            if inentity:
                entity+="" ""+token
            else:
                entity=token
                inentity=True
        elif inentity:
            entities[entity]=entities.get(entity,0)+1
            inentity=False
    return entities

people=extract_entities(tokens,ner,""PERSON"")
top_people=sorted(people.items(),key=operator.itemgetter(1),reverse=True)[:20]
print(top_people)
</code></pre>

<p>The outcome of this example is <code>[('FOO BAR', 1)]</code>.
Furthermore, please notice that you are missing the last PERSON entity because it is not added to the <code>entities</code> dictionary.</p>
",3,0,87,2019-01-21 10:36:07,https://stackoverflow.com/questions/54288097/named-entity-recognition-failing-to-show-lists
How to feed CoreNLP some pre-labeled Named Entities?,"<p>I want to use Standford CoreNLP to pull out Coreferences and start working on the Dependencies of pre-labeled text.  I eventually hope to build graph nodes and edges between related Named Entities.  I am working in python, but using nltk's java functions to call the ""edu.stanford.nlp.pipeline.StanfordCoreNLP"" jar directly (which is what nltk does behind the scenes anyway).</p>

<p>My pre-labeled text is in this format:</p>

<pre><code>PRE-LABELED:  During his youth, [PERSON: Alexander III of Macedon] was tutored by [PERSON: Aristotle] until age 16.  Following the conquest of [LOCATION: Anatolia], [PERSON: Alexander] broke the power of [LOCATION: Persia] in a series of decisive battles, most notably the battles of [LOCATION: Issus] and [LOCATION: Gaugamela].  He subsequently overthrew [PERSON: Persian King Darius III] and conquered the [ORGANIZATION: Achaemenid Empire] in its entirety.
</code></pre>

<p>What I tried to do is tokenize my sentences myself, building a list of tuples in IOB format: [ (""During"",""O""), (""his"",""O""), (""youth"",""O""), (""Alexander"",""B-PERSON""), (""III"",""I-PERSON""), ...]</p>

<p>However, I can't figure out how to tell CoreNLP to take this tuple list as a starting point, building additional Named Entities that weren't initially labeled and finding coreferences on these new, higher-quality tokenized sentences.  I obviously tried simply striping out my labels, and letting CoreNLP do this by itself, but CoreNLP is just not as good at finding the Named Entities as the human-tagged pre-labeled text.</p>

<p>I need an output as below.  I understand that it will be difficult to use Dependencies to get Edges in this way, but I need to see how far I can get.</p>

<pre><code>DESIRED OUTPUT:
[Person 1]:
Name: Alexander III of Macedon
Mentions:
* ""Alexander III of Macedon""; Sent1 [4,5,6,7] # List of tokens
* ""Alexander""; Sent2 [6]
* ""He""; Sent3 [1]
Edges:
* ""Person 2""; ""tutored by""; ""Aristotle""

[Person 2]:
Name: Aristotle
[....]
</code></pre>

<p><strong>How can I feed CoreNLP some pre-identified Named Entities, and still get help with additional Named Entities, with Coreference, and with Basic Dependencies?</strong></p>

<p>P.S. Note that this is not a duplicate of <a href=""https://stackoverflow.com/questions/11333903/nltk-named-entity-recognition-with-custom-data"">NLTK Named Entity Recognition with Custom Data</a>.  I'm not trying to train a new classifier with my pre-labeled NER, I'm only trying to add CoreNLP's to my own when running coreference (including mentions) and dependencies on a given sentence.</p>
","python, nltk, stanford-nlp, named-entity-recognition","<p>The answer is to make a Rules file with <a href=""https://stanfordnlp.github.io/CoreNLP/ner.html#additional-tokensregexner-rules"" rel=""nofollow noreferrer""><strong>Additional TokensRegexNER Rules</strong></a>.</p>

<p>I used a regex to group out the labeled names.  From this I built a rules tempfile which I passed to the corenlp jar with <code>-ner.additional.regexner.mapping mytemprulesfile</code>.</p>

<pre><code>Alexander III of Macedon    PERSON      PERSON,LOCATION,ORGANIZATION,MISC
Aristotle                   PERSON      PERSON,LOCATION,ORGANIZATION,MISC
Anatolia                    LOCATION    PERSON,LOCATION,ORGANIZATION,MISC
Alexander                   PERSON      PERSON,LOCATION,ORGANIZATION,MISC
Persia                      LOCATION    PERSON,LOCATION,ORGANIZATION,MISC
Issus                       LOCATION    PERSON,LOCATION,ORGANIZATION,MISC
Gaugamela                   LOCATION    PERSON,LOCATION,ORGANIZATION,MISC
Persian King Darius III     PERSON      PERSON,LOCATION,ORGANIZATION,MISC
Achaemenid Empire           ORGANIZATION    PERSON,LOCATION,ORGANIZATION,MISC
</code></pre>

<p><em>I have aligned this list for readability, but these are tab-separated values.</em></p>

<p>An interesting finding is that some multi-word pre-labeled entities stay multi-word as originally labeled, whereas running corenlp without the rules files will sometimes split these tokens into separate entities.</p>

<p>I had wanted to specifically identify the named-entity tokens, figuring it would make coreferences easier, but I guess this will do for now.  How often are entity names identical but unrelated within one document, anyway?</p>

<p><strong>Example</strong> <em>(execution takes ~70secs)</em></p>

<pre><code>import os, re, tempfile, json, nltk, pprint
from subprocess import PIPE
from nltk.internals import (
    find_jar_iter,
    config_java,
    java,
    _java_options,
    find_jars_within_path,
)

def ExtractLabeledEntitiesByRegex( text, regex ):
    rgx = re.compile(regex)
    nelist = []
    for mobj in rgx.finditer( text ):
        ne = mobj.group('ner')
        try:
            tag = mobj.group('tag')
        except IndexError:
            tag = 'PERSON'
        mstr = text[mobj.start():mobj.end()]
        nelist.append( (ne,tag,mstr) )
    cleantext = rgx.sub(""\g&lt;ner&gt;"", text)
    return (nelist, cleantext)

def GenerateTokensNERRules( nelist ):
    rules = """"
    for ne in nelist:
        rules += ne[0]+'\t'+ne[1]+'\tPERSON,LOCATION,ORGANIZATION,MISC\n'
    return rules

def GetEntities( origtext ):
    nelist, cleantext = ExtractLabeledEntitiesByRegex( origtext, '(\[(?P&lt;tag&gt;[a-zA-Z]+)\:\s*)(?P&lt;ner&gt;(\s*\w)+)(\s*\])' )

    origfile = tempfile.NamedTemporaryFile(mode='r+b', delete=False)
    origfile.write( cleantext.encode('utf-8') )
    origfile.flush()
    origfile.seek(0)
    nerrulefile = tempfile.NamedTemporaryFile(mode='r+b', delete=False)
    nerrulefile.write( GenerateTokensNERRules(nelist).encode('utf-8') )
    nerrulefile.flush()
    nerrulefile.seek(0)

    java_options='-mx4g'
    config_java(options=java_options, verbose=True)
    stanford_jar = '../stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar'
    stanford_dir = os.path.split(stanford_jar)[0]
    _classpath = tuple(find_jars_within_path(stanford_dir))

    cmd = ['edu.stanford.nlp.pipeline.StanfordCoreNLP',
        '-annotators','tokenize,ssplit,pos,lemma,ner,parse,coref,coref.mention,depparse,natlog,openie,relation',
        '-ner.combinationMode','HIGH_RECALL',
        '-ner.additional.regexner.mapping',nerrulefile.name,
        '-coref.algorithm','neural',
        '-outputFormat','json',
        '-file',origfile.name
        ]

    # java( cmd, classpath=_classpath, stdout=PIPE, stderr=PIPE )
    stdout, stderr = java( cmd, classpath=_classpath, stdout=PIPE, stderr=PIPE )    # Couldn't get working- stdin=textfile
    PrintJavaOutput( stdout, stderr )

    origfilenametuple = os.path.split(origfile.name)
    jsonfilename = origfilenametuple[len(origfilenametuple)-1] + '.json'

    os.unlink( origfile.name )
    os.unlink( nerrulefile.name )
    origfile.close()
    nerrulefile.close()

    with open( jsonfilename ) as jsonfile:
        jsondata = json.load(jsonfile)

    currentid = 0
    entities = []
    for sent in jsondata['sentences']:
        for thisentity in sent['entitymentions']:
            tag = thisentity['ner']
            if tag == 'PERSON' or tag == 'LOCATION' or tag == 'ORGANIZATION':
                entity = {
                    'id':currentid,
                    'label':thisentity['text'],
                    'tag':tag
                }
                entities.append( entity )
                currentid += 1

    return entities

#### RUN ####
corpustext = ""During his youth, [PERSON:Alexander III of Macedon] was tutored by [PERSON: Aristotle] until age 16.  Following the conquest of [LOCATION: Anatolia], [PERSON: Alexander] broke the power of [LOCATION: Persia] in a series of decisive battles, most notably the battles of [LOCATION: Issus] and [LOCATION: Gaugamela].  He subsequently overthrew [PERSON: Persian King Darius III] and conquered the [ORGANIZATION: Achaemenid Empire] in its entirety.""

entities = GetEntities( corpustext )
for thisent in entities:
    pprint.pprint( thisent )
</code></pre>

<p><strong>Output</strong></p>

<pre><code>{'id': 0, 'label': 'Alexander III of Macedon', 'tag': 'PERSON'}
{'id': 1, 'label': 'Aristotle', 'tag': 'PERSON'}
{'id': 2, 'label': 'his', 'tag': 'PERSON'}
{'id': 3, 'label': 'Anatolia', 'tag': 'LOCATION'}
{'id': 4, 'label': 'Alexander', 'tag': 'PERSON'}
{'id': 5, 'label': 'Persia', 'tag': 'LOCATION'}
{'id': 6, 'label': 'Issus', 'tag': 'LOCATION'}
{'id': 7, 'label': 'Gaugamela', 'tag': 'LOCATION'}
{'id': 8, 'label': 'Persian King Darius III', 'tag': 'PERSON'}
{'id': 9, 'label': 'Achaemenid Empire', 'tag': 'ORGANIZATION'}
{'id': 10, 'label': 'He', 'tag': 'PERSON'}
</code></pre>
",0,0,702,2019-01-30 15:08:06,https://stackoverflow.com/questions/54443634/how-to-feed-corenlp-some-pre-labeled-named-entities
Tokenizing Named Entities in Spacy,"<p>can anyone assist please.</p>

<p>I'm attempting to tokenize a document using Spacy whereby named entities are tokenised. For example:</p>

<p>'New York is a city in the United States of America'</p>

<p>would be tokenized as:</p>

<p>['New York', 'is', 'a', 'city', 'in', 'the', 'United States of America']</p>

<p>Any tips on how to do this are very welcome. Have looked at using span.merge(), but with no success, but I am new to coding so am likely to have missed something.</p>

<p>Thank you in advance</p>
","nlp, tokenize, spacy, named-entity-recognition","<p>Use the <a href=""https://spacy.io/api/doc#retokenize"" rel=""nofollow noreferrer""><code>doc.retokenize</code></a> context manager to merge entity spans into single tokens. Wrap this in a <a href=""https://spacy.io/usage/processing-pipelines#custom-components-attributes"" rel=""nofollow noreferrer"">custom pipeline component</a>, and add the component to your language model.</p>

<pre><code>import spacy

class EntityRetokenizeComponent:
  def __init__(self, nlp):
    pass
  def __call__(self, doc):
    with doc.retokenize() as retokenizer:
        for ent in doc.ents:
            retokenizer.merge(doc[ent.start:ent.end], attrs={""LEMMA"": str(doc[ent.start:ent.end])})
    return doc

nlp = spacy.load('en')
retokenizer = EntityRetokenizeComponent(nlp) 
nlp.add_pipe(retokenizer, name='merge_phrases', last=True)

doc = nlp(""German Chancellor Angela Merkel and US President Barack Obama ""
          ""converse in the Oval Office inside the White House in Washington, D.C."")

[tok for tok in doc]

#[German,
# Chancellor,
# Angela Merkel,
# and,
# US,
# President,
# Barack Obama,
# converse,
# in,
# the Oval Office,
# inside,
# the White House,
# in,
# Washington,
# ,,
# D.C.]
</code></pre>
",4,2,1856,2019-02-11 23:35:19,https://stackoverflow.com/questions/54640715/tokenizing-named-entities-in-spacy
How to recognize entities in text that is the output of optical character recognition (OCR)?,"<p>I am trying to do multi-class classification with textual data. Problem I am facing that I have unstructured textual data. I'll explain the problem with an example.
consider this image for example:</p>

<p><a href=""https://i.sstatic.net/mS1fx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mS1fx.jpg"" alt=""example data""></a></p>

<p>I want to extract and classify text information given in image. Problem is when I extract information OCR engine will give output something like this:</p>

<pre><code>18
EURO 46
KEEP AWAY
FROM FIRE
MADE IN CHINA
2226249917581
7412501
DOROTHY
PERKINS
</code></pre>

<p>Now target classes here are:</p>

<pre><code>18 -&gt; size
EURO 46 -&gt; price
KEEP AWAY FROM FIRE -&gt; usage_instructions
MADE IN CHINA -&gt; manufacturing_location
2226249917581 -&gt; product_id
7412501 -&gt; style_id
DOROTHY PERKINS -&gt; brand_name
</code></pre>

<p>Problem I am facing is that input text is not separable, meaning ""multiple lines can belong to same class"" and there can be cases where ""single line can have multiple classes"".</p>

<p>So I don't know how I can split/merge lines before passing it to classification model.<br> Is there any way using NLP I can split paragraph based on target class. In other words given input paragraph split it based on target labels.</p>
","nlp, recurrent-neural-network, text-classification, named-entity-recognition, named-entity-extraction","<p>If you only consider the text, this is a Named Entity Recognition (NER) task.</p>

<p>What you can do is <a href=""https://spacy.io/usage/training#section-ner"" rel=""noreferrer""><strong>train</strong> a Spacy model to NER for your particular problem</a>.</p>

<p>Here is what you will need to do:</p>

<ol>
<li>First gather a list of training text data</li>
<li>Label that data with corresponding entity types</li>
<li>Split the data into training set and testing set</li>
<li>Train a model with Spacy NER using training set</li>
<li>Score the model using the testing set</li>
<li>...</li>
<li>Profit!</li>
</ol>

<p>See <a href=""https://spacy.io/usage/training#section-ner"" rel=""noreferrer"">Spacy documentation on training specific NER models</a></p>

<p>Good luck!</p>
",5,6,1436,2019-03-03 10:52:46,https://stackoverflow.com/questions/54968055/how-to-recognize-entities-in-text-that-is-the-output-of-optical-character-recogn
Append list with NULLs to data frame,"<p>I am using an NER library (<a href=""https://github.com/trinker/entity"" rel=""nofollow noreferrer"">entity</a>) to extract person names from sentences in a data frame. </p>

<p>If I run:</p>

<pre><code>library(entity)
dat &lt;- data.frame(texts=c('Henry went home', 'Drive a car', 'Two snowmen'), stringsAsFactors=FALSE)
person_entity(dat$texts)
</code></pre>

<p>I get a list of extracted names: </p>

<pre><code>&gt; person_entity(dat$texts)
[[1]]
[1] ""Henry""

[[2]]
NULL

[[3]]
NULL
</code></pre>

<p>How can I append this list as an additional column to my data frame? The additional column could be a list of the extracted names, or even just the length of the list, e.g.:</p>

<pre><code>dat &lt;- data.frame(texts=c('Henry went home', 'Drive a car', 'Two snowmen'), person_count=c(1,0,0), stringsAsFactors=FALSE)
</code></pre>
","r, list, dataframe, named-entity-recognition","<p>One way would be to use <code>lengths</code> to get the length of individual elements in the list.</p>

<pre><code>dat$person_count &lt;- lengths(person_entity(dat$texts))
</code></pre>
",1,0,181,2019-03-08 01:53:44,https://stackoverflow.com/questions/55055633/append-list-with-nulls-to-data-frame
How to get a confidence score in Google Cloud NLU while analyzing named entities?,"<p>I have a text that can contain multiple named entities (say, for example, one person and two organizations).
I need to select one of the texts based on the confidence of the Named-Entity-Tagger. (Example at the end)</p>

<p>The project uses ""analyzeEntities"" <a href=""https://cloud.google.com/natural-language/docs/reference/rest/v1beta2/documents/analyzeEntities"" rel=""nofollow noreferrer"">details here</a> to tag the Named Entities. How can I get a confidence level of each of the tagged entities?</p>

<p><strong>Example:</strong>
Let the text be ""Homes and Joshua are two employees at StackOverflow.""</p>

<p>Let us assume that the tagger has the following confidences:</p>

<pre><code>Homes -&gt; PERSON -&gt; 0.3
Joshua -&gt; PERSON -&gt; 0.7
StackOverflow -&gt; ORGANIZATION -&gt; 0.4
</code></pre>

<p>I need to select ""Joshua"" as this entity has the highest confidence score.
One way to do this is to get the confidence scores along with the returned content from the service API. How can I do this? Is there any way to get the scores like above?</p>

<p>PS: I use Python 3.x, but solutions in other languages are also welcome.</p>
","python-3.x, named-entity-recognition, google-cloud-nl","<p>Currently, the API doesn't expose any confidence numbers for the entity predictions. It produces a ""salience"" score which associates a score to each entity to indicate how important they are in the text, not to be confused with the confidence which would indicate how confident the model is in its prediction.</p>
",1,1,230,2019-03-11 14:33:45,https://stackoverflow.com/questions/55104210/how-to-get-a-confidence-score-in-google-cloud-nlu-while-analyzing-named-entities
Text to word per line + named entity tag in Python,"<p>I’m making a Named Entity Recognizer and I’m struggling with putting data into the right format, using Python. What I have is a certain string and a list of the named entities in that text with belonging tags. For example:</p>

<pre><code>text = “Hidden Figures is a 2016 American biographical drama film directed by Theodore Melfi and written by Melfi and Allison Schroeder.”
</code></pre>

<p>This string can also be “[[Hidden Figures]] is a 2016 [[American]] biographical drama film directed by [[Theodore Melfi]] and written by [[Melfi]] and [[Allison Schroeder]].” if that makes it easier.<br></p>

<pre><code>listOfNEsAndTags = [‘Hidden Figures PRO’, 'American LOC’, 'Theodore Melfi PER’, 'Melfi PER’, 'Allison Schroeder PER’]
</code></pre>

<p>What I want as output is: </p>

<pre><code>Hidden PRO
Figures PRO
is O
a O
2016 O
American LOC
biographical O
drama O
film O
directed O
by O
Theodore PER
Melfi PER
and O
written O
by O
Melfi PER
and O 
Allison PER
Schroeder PER 
. O
</code></pre>

<p>So far I’ve only gotten as far as the following function:</p>

<pre><code>def wordPerLine(text, neplustags): 
    text = re.sub(r""([?!,.]+)"", r"" \1 "", text) 
    wpl = text.split() 
    output = [] 
    for line in wpl: 
        output.append(line + ” O"") 
    return output
</code></pre>

<p>Which gives every line the default tag O (which is the tag for non-named entities). How can I make it so that the named entities in the text get the right tag?</p>
","python, string, list, named-entity-recognition","<p>This could work, replacing the print with something else and refinement of the regex is needed, but it's a good start.</p>

<pre><code>text = ""[[Hidden test Figures]] is, a 2016 [[American]] biographical drama film directed by [[Theodore Melfi]] and written by [[Melfi]] and [[Allison Schroeder]].""

tags = {""Hidden test Figures"": ""PRO"", ""American"": ""LOC"", 'Theodore Melfi': ""PER"", 'Melfi': ""PER"", 'Allison Schroeder': ""PER""}

text = re.sub(r""([?!,.]+)"", r"" \1"", text)

search = """"
inTag = False

for w in text.split("" ""):
    outTag = False

    rest = w

    if rest[:2] == ""[["":
        rest = rest[2:]
        inTag = True
    if rest[-2:] == ""]]"":
        rest = rest[:-2]
        outTag = True

    if inTag:
        search += rest
        if outTag:
            val = tags[search]
            for word in search.split():
                print(word + "": "" + val)
            inTag = False
            search = """"
        else:
            search += "" ""
    else:
        print(rest + "": O"")
</code></pre>

<p>Input:</p>

<pre><code>[[Hidden test Figures]] is, a 2016 [[American]] biographical drama film directed by [[Theodore Melfi]] and written by [[Melfi]] and [[Allison Schroeder]].
</code></pre>

<p>Output:</p>

<pre><code>Hidden: PRO
test: PRO
Figures: PRO
is: O
,: O
a: O
2016: O
American: LOC
biographical: O
drama: O
film: O
directed: O
by: O
Theodore: PER
Melfi: PER
and: O
written: O
by: O
Melfi: PER
and: O
Allison: PER
Schroeder: PER
.: O
</code></pre>
",0,1,85,2019-03-13 21:08:46,https://stackoverflow.com/questions/55151240/text-to-word-per-line-named-entity-tag-in-python
Error with NLTK package and other dependencies,"<p>I have installed the NLTK package and other dependencies and set the environment variables as follows:</p>

<pre><code>STANFORD_MODELS=/mnt/d/stanford-ner/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz:/mnt/d/stanford-ner/stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz:/mnt/d/stanford-ner/stanford-ner-2018-10-16/classifiers/english.conll.4class.distsim.crf.ser.gz

CLASSPATH=/mnt/d/stanford-ner/stanford-ner-2018-10-16/stanford-ner.jar
</code></pre>

<p>When I try to access the classifier like below:</p>

<pre><code>stanford_classifier = os.environ.get('STANFORD_MODELS').split(':')[0]

stanford_ner_path = os.environ.get('CLASSPATH').split(':')[0]

st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')
</code></pre>

<p>I get the following error.  But I don't understand what is causing this error.</p>

<pre><code>Error: Could not find or load main class edu.stanford.nlp.ie.crf.CRFClassifier
OSError: Java command failed : ['/mnt/c/Program Files (x86)/Common 
Files/Oracle/Java/javapath_target_1133041234/java.exe', '-mx1000m', '-cp', '/mnt/d/stanford-ner/stanford-ner-2018-10-16/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/mnt/d/stanford-ner/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz', '-textFile', '/tmp/tmpaiqclf_d', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf8']
</code></pre>
","python, nlp, stanford-nlp, named-entity-recognition","<p>I found the answer for this issue. I am using NLTK == 3.4. From NLTK ==3.3 and above Stanford NLP (POS, NER , tokenizer) are not loaded as part of nltk.tag but from nltk.parse.corenlp.CoreNLPParser. The stackoverflow answer is available in stackoverflow.com/questions/13883277/stanford-parser-and-nltk/… and the github link for official documentation is github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK.</p>

<p>Additional information if you are facing timeout issue from the NER tagger or any other parser of coreNLP API, please increase the timeout limit as stated in <a href=""https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK/_compare/3d64e56bede5e6d93502360f2fcd286b633cbdb9...f33be8b06094dae21f1437a6cb634f86ad7d83f7"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK/_compare/3d64e56bede5e6d93502360f2fcd286b633cbdb9...f33be8b06094dae21f1437a6cb634f86ad7d83f7</a> by dimazest.</p>
",1,1,210,2019-03-15 14:38:15,https://stackoverflow.com/questions/55185021/error-with-nltk-package-and-other-dependencies
How to get back incorrect NER predictions in sklearn-crfsuite,"<p>I am performing NER using sklearn-crfsuite. I am trying to report back on an entity mention by entity mention case as a true positive (both prediction and expected correct even if no entity), false positive (prediction says yes, expected no) or false negative (prediction says no, expected yes).  </p>

<p>I cannot see how to get anything other than tag/token based summary statistics for NER performance.  </p>

<p>I would be OK with a different way of grouping entity mentions such as: correct, incorrect, partial, missing, spurious.  I can write a whole bunch of code around it myself to try to accomplish this (and might have to), but there has to be a single call to get this info?</p>

<p>Here are some of the calls that are being made to get the summary statistics:</p>

<pre><code>from sklearn import metrics
report = metrics.classification_report(targets, predictions,
                                       output_dict=output_dict)
precision = metrics.precision_score(targets, predictions,
                                    average='weighted')
f1 = metrics.f1_score(targets, predictions, average='weighted')
accuracy = metrics.accuracy_score(targets, predictions)
</code></pre>
","scikit-learn, nlp, named-entity-recognition, crf","<p>It's not so straightforward to get the metrics you mentioned (i.e., correct, incorrect, partial, missing, spurious) which I believe are the same ones as SemEval'13 challenge introduced.</p>

<p>I also needed to report some results based on these metrics and ended up coding it myself:</p>

<ul>
<li><a href=""http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/"" rel=""nofollow noreferrer"">detailed explanation of these metrics</a></li>
<li><a href=""https://github.com/davidsbatista/NER-Evaluation"" rel=""nofollow noreferrer"">my own code implementation</a> (it's really too much for a SO post)</li>
</ul>

<p>I'm working together with someone else and we are planning to release that as package that can be easily integrated with open-source NER systems and/or read standard formats like CoNLL. Feel free to join and help us out :)</p>
",2,0,382,2019-03-21 13:31:19,https://stackoverflow.com/questions/55281583/how-to-get-back-incorrect-ner-predictions-in-sklearn-crfsuite
Finding the word that corresponds to experience,"<p>Suppose i have a sentence like the following, how can i find what the experience corresponds to?</p>

<p>Ex: Programmer with 5 years of experience wanted.
I want to find what the experience (5 years) corresponds to, in this case programmer.</p>

<p>The code should also be recognize a corresponding verb, ex: 5 years of programming</p>

<p>So how should i go about it? I was thinking of making a pattern that finds the closest noun or verb.</p>
","python, spacy, named-entity-recognition, part-of-speech","<p>As I didn't get any meaningful help, I just came up with a temporary solution of my own. Find the sentence with the entity tag ""DATE"", and remove the stop words.</p>

<p>For example, the sentence 'The ideal candidate must have at least 6 years of experience in the field.' becomes into the following:</p>

<p>'ideal candidate 6 years experience field'</p>

<p>Obviously I can refine this further, but for now this will do.</p>
",0,-3,50,2019-04-05 04:43:50,https://stackoverflow.com/questions/55528385/finding-the-word-that-corresponds-to-experience
"Named Entity Recognition using NLTK: Extract Auditor name, address and organisation","<p>I am trying to use nltk to identify Person, Organization and Place from a sentence.</p>

<p>My Use Case is to basically extract Auditor name, organization and Place from an annual financial report</p>

<p>With nltk in python the results don't seem to be really satisfactory</p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

ex='Alastair John Richard Nuttall (Senior statutory auditor) for and on behalf of Ernst &amp; Young LLP (Statutory auditor) Leeds'

ne_tree = ne_chunk(pos_tag(word_tokenize(ex)))

print(ne_tree)

Tree('S', [Tree('PERSON', [('Alastair', 'NNP')]), Tree('PERSON', [('John', 'NNP'), ('Richard', 'NNP'), ('Nuttall', 'NNP')]), ('(', '('), Tree('ORGANIZATION', [('Senior', 'NNP')]), ('statutory', 'NNP'), ('auditor', 'NN'), (')', ')'), ('for', 'IN'), ('and', 'CC'), ('on', 'IN'), ('behalf', 'NN'), ('of', 'IN'), Tree('GPE', [('Ernst', 'NNP')]), ('&amp;', 'CC'), Tree('PERSON', [('Young', 'NNP'), ('LLP', 'NNP')]), ('(', '('), ('Statutory', 'NNP'), ('auditor', 'NN'), (')', ')'), ('Leeds', 'NNS')])

</code></pre>

<p>As seen above 'Leeds' is not identified as place nor is Ernst &amp; Young LLP recognized as Organization</p>

<p>Are there any better ways of achieving this in Python?</p>
","python-3.x, nlp, nltk, named-entity-recognition","<p>Try spacy instead of NLTK:</p>

<p><a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#named-entities</a></p>

<p>I think spacy's pretrained models are likely to perform better. The results (with spacy 2.1, en_core_web_lg) for your sentence are:</p>

<blockquote>
  <p>Alastair John Richard Nuttall PERSON<br>
  Ernst &amp; Young LLP ORG<br>
  Leeds GPE </p>
</blockquote>
",1,0,608,2019-04-09 07:56:10,https://stackoverflow.com/questions/55587735/named-entity-recognition-using-nltk-extract-auditor-name-address-and-organisat
ValueError: setting an array element with a sequence. On Keras model.fit,"<p>I'm getting ""ValueError: setting an array element with a sequence"" on keras model.fit </p>

<pre><code>model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=5, validation_split=0.1, verbose=1)



X_tr
Out[22]:
array([[4256, 1244, 4475, ..., 8766, 8766, 8766],
       [5443, 3401, 4709, ..., 8766, 8766, 8766],
       [3829,  543,  681, ..., 8766, 8766, 8766],
       ...,
       [2185, 7510, 8004, ..., 8766, 8766, 8766],
       [7562, 5842, 4742, ..., 8766, 8766, 8766],
       [2449, 6217, 2310, ..., 8766, 8766, 8766]], dtype=int32)

X_tr.shape
(2699, 75)

np.array(y_tr)
Out[37]:
array([array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0.],
   [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0.],
   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
    0.],
   ...,
   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
    0.],
   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
    0.],
   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
    0.]], dtype=float32)], dtype=object)

np.array(y_tr).shape
(2699,)
</code></pre>

<p>I am so confused, anyone able to help me out? Thanks in advance!</p>

<p>In case you need code: <a href=""https://github.com/sunsuntianyi/question/blob/master/LSTM.ipynb"" rel=""noreferrer"">https://github.com/sunsuntianyi/question/blob/master/LSTM.ipynb</a></p>
","python, tensorflow, keras, lstm, named-entity-recognition","<p>The error definitely originates from you passing in an object array as your predictive variable. Your <code>y_tr</code> should be of shape (2699,17) as far as I can tell from your snippet. Maybe some of your rows in <code>y_tr</code> is not 17 long or maybe you have specifically used an object array to generate the data. If the latter, you can try to convert it back like this:</p>

<pre><code>y_tr = np.asarray([np.asarray(row, dtype=float) for row in y_tr], dtype=float)
</code></pre>

<p>Replace <code>float</code> with whatever type suits your needs. This should give an error if the rows are of different size as well.</p>
",4,4,2593,2019-04-13 08:39:36,https://stackoverflow.com/questions/55663594/valueerror-setting-an-array-element-with-a-sequence-on-keras-model-fit
Fixing a custom OpenNLP NER model,"<p>We have a report writing tool that we're trying to add a search capability to.  Essentially a user would be able to type in a question and get a report back based on the criteria in the sentence.  We're trying to keep this as open ended as we can, not requiring a specific sentence structure, which is why we thought to try OpenNLP NER.</p>

<p>An example would be:</p>

<p>""what was Arts attendance last quarter""</p>

<p>Tagged as:</p>

<pre><code>what was &lt;START:dept&gt; Arts &lt;END&gt; &lt;START:filter&gt; attendance &lt;END&gt; last &lt;START:calc&gt; quarter &lt;END&gt;
</code></pre>

<p>We've tried to come up with many different variations of the questions with varying departments, filters, etc..  We're still not at 15k only at 14.6k, so still working towards that.</p>

<p>As far as analyzing the question this is the start of it:</p>

<pre><code>InputStream tokenStream = getClass().getResourceAsStream(""/en-token.bin""); //$NON-NLS
            TokenizerModel tokenModel = new TokenizerModel(tokenStream);
            Tokenizer tokenizer = new TokenizerME(tokenModel);
            for (String name : modelNames) {
                tokenizedQuestion = tokenizer.tokenize(question);

                String alteredQuestion = question;

                TokenNameFinderModel entityModel = new TokenNameFinderModel(getClass().getResourceAsStream(name));
                NameFinderME nameFinder = new NameFinderME(entityModel);
                Span[] nameSpans = nameFinder.find(tokenizedQuestion);
                for (Span span : nameSpans) {
                    if (span.getType().equals(""dept"")) { 
                        deptList.add(span);
                    } else if (span.getType().equals(""filter"")) { 
                        filterList.add(span);
                    } else if (span.getType().equals(""calculation""){ 
                        calculationList.add(span);
                    }
                }
</code></pre>

<p>The problem now is if you put in ""what was Bugs Bunny last cartoon""
you get 'Bugs' as a dept, 'Bunny' as a filter, and 'cartoon' as a calculation.</p>

<p><strong>I'm guessing our training questions are to similar to each other and now it's assuming whatever follows ""what was"" is a department.<br>
1. Is that a correct assumption and is there a better way of training these models?<br>
2. Is the best bet to break each entity into it's own model?</strong> I did try this and had 105 unit tests that failed afterwards so, hoping to try something simpler first, lol.</p>

<p>Also I have read multiple threads on here about custom NER models, but most of what I've found is how to start one.  There's also a thread about how multiple entity models don't work.  I forget where the post was I found that putting null in for the type allows you to tag multiple types in the same model and it seems to work fairly well.</p>

<pre><code> tokenNameFinderModel = NameFinderME.train(""en"", null, sampleStream, TrainingParameters.defaultParams(), new TokenNameFinderFactory()); 
 tokenNameFinderModel.serialize(modelOut);
</code></pre>

<p>Thanks in advance for any and all help!!</p>
","java, opennlp, named-entity-recognition","<p>Our end goal was to be able to train a model on certain words that we classified and have to correctly classify each word regardless of sentence structure.  In OpenNLP we weren't able to accomplish that.  </p>

<blockquote>
  <p>I'm guessing our training questions are to similar to each other and now it's assuming whatever follows ""what was"" is a department.<br>
  1. Is that a correct assumption and is there a better way of training these models?</p>
</blockquote>

<p>Based on my testing and results I'm concluding yes the sequence and pattern of the words plays a part.  I don't have any documentation to back that though.  Also I can't find anything to get around that with OpenNLP.</p>

<blockquote>
  <ol start=""2"">
  <li>Is the best bet to break each entity into it's own model?</li>
  </ol>
</blockquote>

<p>Based on experience and testing I'm resolving that separate models, as much as possible, is the best way to train.  Unfortunately we still haven't been able to accomplish our goals even with this approach.</p>

<p>Ultimately what we've done to switch to StanfordNLP NER models.  You can still do custom implementations around domain specific language, and have the option of turning off sequencing in the properties file:</p>

<pre><code>usePrev=false
useNext=false
useDisjunctive=false
useSequences=false
usePrevSequences=false
</code></pre>

<p>Reference for custom NER in StanfordNLP:
<a href=""https://medium.com/swlh/stanford-corenlp-training-your-own-custom-ner-tagger-8119cc7dfc06"" rel=""nofollow noreferrer"">Stanford CoreNLP: Training your own custom NER tagger</a></p>
",0,0,351,2019-05-14 23:33:05,https://stackoverflow.com/questions/56139932/fixing-a-custom-opennlp-ner-model
How to train a spaCy model with line number as a feature?,"<p>I'm a newbie to nlp and <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spaCy</a> and I'm working on a project for extracting person and company names from business cards.</p>

<p>In order to extract text I am using a decent OCR function that I've made which gives me something like this:</p>

<pre><code>Sunny J. Mistry
Product Design Engineer

Apple
5 Infinite Loop, MS 305-1PH
Cupertino, CA 95014

T 408 974-5339
M 925 548-4585
sjmistry@apple.com
www.apple.com
</code></pre>

<p>At first I was trying process line by line using the default English NER for the job and soon realized that it's not enough.  </p>

<p>Eventually I've decided to create my own custom NER that will be trained with information about the position of text.</p>

<p>I haven't found any information in the official documentation on how to add custom features for the training data like line numbers, but I've found this <a href=""https://support.prodi.gy/t/incorporating-custom-position-feature-into-ner/160"" rel=""nofollow noreferrer"">answer</a> and <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/ner_multitask_objective.py"" rel=""nofollow noreferrer"">example</a> of <strong>Matthew Honnibal</strong> which suggested to use a multi-task objective in order to train a model with a costume feature.</p>

<p>I'm still not sure:</p>

<ol>
<li><p>How the training data should look like?</p></li>
<li><p>How do I use spaCy's API to add a custom feature to the training process?</p></li>
<li><p>Is multi-task objective the right tool to train this kind of model?</p></li>
</ol>
","python, machine-learning, nlp, spacy, named-entity-recognition","<p>Answering my own question:</p>

<p>I didn't find an official way for implementing this kind of task, but in the end I decided on training a model on a normal business card data set containing 200 images. I've extracted the text from each image using Google OCR and annotated it using a tool described in <a href=""https://medium.com/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6"" rel=""nofollow noreferrer"">this</a> post.</p>

<p>It worked like a charm.</p>
",1,4,557,2019-05-25 10:47:27,https://stackoverflow.com/questions/56304109/how-to-train-a-spacy-model-with-line-number-as-a-feature
Case-sensitive entity recognition,"<p>I have keywords that are all stored in lower case, e.g. ""discount nike shoes"", that I am trying to perform entity extraction on. The issue I've run into is that spaCy seems to be case sensitive when it comes to NER. Mind you , I don't think that this is spaCy specific. </p>

<p>When I run...</p>

<pre><code>doc = nlp(u""i love nike shoes from the uk"")

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
</code></pre>

<p>... nothing is returned.</p>

<p>When I run...</p>

<pre><code>doc = nlp(u""i love Nike shoes from the Uk"")

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
</code></pre>

<p>I get the following results...</p>

<pre><code>Nike 7 11 ORG
Uk 25 27 GPE
</code></pre>

<p>Should I just title case everything? Is there another workaround that I could use?</p>
","python, spacy, named-entity-recognition","<p>spaCy's <a href=""https://spacy.io/models/en"" rel=""noreferrer"">pre-trained statistical models</a> were trained on a large corpus of general news and web text. This means that the entity recognizer has likely only seen very few all-lowercase examples, because that's much less common in those types of texts. In English, capitalisation is also a strong indicator for a named entitiy (unlike German, where all nouns are typically capitalised), so the model probably tends to pay more attention to that.</p>

<p>If you're working with text that doesn't have proper capitalisation, you probably want to fine-tune the model to be less sensitive here. See the docs on <a href=""https://spacy.io/usage/training#ner"" rel=""noreferrer"">updating the named entity recognizer</a> for more details and code examples.</p>

<p>Producing the training examples will hopefully not be very difficult, because you can use existing annotations and datasets, or create one using the pre-trained model, and then lowercase everything. For example, you could take text with proper capitalisation, run the model over it and extract all entitiy spans in the text. Next, you lowercase all the texts, and update the model with the new data. Make sure to also mix in text with proper capitalisation, because you don't want the model to learn something like ""Everything is lowercase now! Capitalisation doesn't exist anymore!"".</p>

<p>Btw, if you have entities that can be defined using a list or set of rules, you might also want to check out the <a href=""https://spacy.io/usage/rule-based-matching#entityruler"" rel=""noreferrer""><code>EntityRuler</code> component</a>. It can be combined with the statistical entity recognizer and will let you pass in a dictionary of exact matches or abstract token patterns that can be case-insensitive. For instance, <code>[{""lower"": ""nike""}]</code> would match one token whose lowercase form is ""nike"" – so ""NIKE"", ""Nike"", ""nike"", ""NiKe"" etc.</p>
",19,12,4174,2019-05-30 19:05:48,https://stackoverflow.com/questions/56384231/case-sensitive-entity-recognition
ValueError: [E024] Could not find an optimal move to supervise the parser,"<p>I am getting the following error while training <code>spacy</code> NER model with my custom training data.</p>

<pre><code>ValueError: [E024] Could not find an optimal move to supervise the parser. Usually, this means the GoldParse was not correct. For example, are all labels added to the model?
</code></pre>

<p>Can anyone help me with this?</p>
","python, python-3.x, nlp, spacy, named-entity-recognition","<p>passing the training data through this function below works fine without any error. </p>

<pre><code>def trim_entity_spans(data: list) -&gt; list:
    """"""Removes leading and trailing white spaces from entity spans.

    Args:
        data (list): The data to be cleaned in spaCy JSON format.

    Returns:
        list: The cleaned data.
    """"""
    invalid_span_tokens = re.compile(r'\s')

    cleaned_data = []
    for text, annotations in data:
        entities = annotations['entities']
        valid_entities = []
        for start, end, label in entities:
            valid_start = start
            valid_end = end
            while valid_start &lt; len(text) and invalid_span_tokens.match(
                    text[valid_start]):
                valid_start += 1
            while valid_end &gt; 1 and invalid_span_tokens.match(
                    text[valid_end - 1]):
                valid_end -= 1
            valid_entities.append([valid_start, valid_end, label])
        cleaned_data.append([text, {'entities': valid_entities}])

    return cleaned_data
</code></pre>
",14,6,4963,2019-06-18 06:21:14,https://stackoverflow.com/questions/56642816/valueerror-e024-could-not-find-an-optimal-move-to-supervise-the-parser
Spacy NER doesn&#39;t identify lowercase entities,"<p>I am facing problem to detect named entities which starts with lowercase letter. I have tried the solution provided on link <a href=""https://github.com/explosion/spaCy/issues/701"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/701</a>. It seems to be not working for me.</p>

<p>===== Info about spaCy=============</p>

<pre><code>spaCy version    2.1.4
Platform         Darwin-16.7.0-x86_64-i386-64bit
Python version   3.6.5
Models           en
</code></pre>

<pre><code>import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_sm')
sk = nlp.vocab[u'south korea']
SK = nlp.vocab[u'South Korea']
sk.is_lower = SK.is_lower
sk.shape = SK.shape
sk.shape_ = SK.shape_
sk.is_upper =SK.is_upper
sk.cluster = SK.cluster
sk.is_title = SK.is_title
doc = nlp(u'south korea is a country in asia')
for word in doc:
    print(word.text, word.tag_, word.ent_type_) 

</code></pre>

<p>The expected output is: </p>

<pre><code>south NNP GPE
korea NNP GPE
is VBZ 
a DT 
country NN 
in IN 
asia NNP 
</code></pre>

<p>But the output of above code is:</p>

<pre><code>south JJ 
korea NN 
is VBZ 
a DT 
country NN 
in IN 
asia NNP 
</code></pre>
","nlp, spacy, named-entity-recognition","<p>The NE recognizer is machine learned and thus relies on the strongest features it sees in the training data.</p>

<p>You can use a truecaser/recaser, a statical model that fixes casing in lowercased text and pass the output to spacy. You can use:</p>

<ul>
<li><a href=""https://github.com/alvations/sacremoses"" rel=""noreferrer"">sacremoses</a>, a preprocessing tool for machine translation </li>
<li><a href=""https://github.com/nreimers/truecaser"" rel=""noreferrer"">nreimers/truecaser</a>, a truecaser implementation using NLTK</li>
</ul>

<p>Alternatively, you might try to <a href=""https://medium.com/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6"" rel=""noreferrer"">train your recognizer</a> and modify your training data so it also has lower-cased entities, but it is rather a tedious process.</p>
",6,2,3086,2019-06-18 09:52:38,https://stackoverflow.com/questions/56646365/spacy-ner-doesnt-identify-lowercase-entities
Training spacy model not working: running the train_ner script has no effect,"<p>I am writing a program that uses the spacy model en_core_web_md for Named Entity Recognition. It was not identifying all my entities correctly: for instance, there were some names of people and organisations that were not being recognised as such.</p>

<p>I looked up how to train the model and found this script: <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py</a></p>

<p>I downloaded the script, put it in the same folder as my program, replaced their training data with my own (containing the names I wanted it to recognise) and ran it, with <code>model=""en_core_web_md""</code> and <code>output_dir=""model""</code> instead of <code>None</code>.</p>

<p>My project involves video game characters so my training data was:</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_DATA = [
    (""Who is Cave Johnson?"", {""entities"": [(7, 19, ""PERSON"")]}),
    (""I work for Aperture Science."", {""entities"":[(11, 27, ""ORG"")]}),
    (""Wallace Breen is CEO of Black Mesa."", {""entities"":[(0, 13, ""PERSON""), (25, 35, ""ORG"")]}),

]
</code></pre>

<p>The train_ner script outputs the expected results. However, when I run my other program, it still does not recognise ""Cave Johnson"" as a <code>PERSON</code> or ""Black Mesa"" as an <code>ORG</code>. Why is the script not working?</p>

<p>Update: still haven't got it working. I ran the script again, to no apparent effect.</p>
","nlp, spacy, training-data, named-entity-recognition","<p>Looking in more detail on the Github ""issues"", it turns out that even though the example script only gives it a couple of sentences to train it, when you run the script you are expected to actually uses hundreds of examples.</p>

<p>This is not clear for someone with no NLP experience from reading the documentation.
Hopefully now this question and answer will come up when people search for it so other people don't have to spend weeks wondering what they were doing wrong.</p>

<p>Basically, I just need more sentences.</p>
",0,0,413,2019-06-18 14:30:35,https://stackoverflow.com/questions/56651465/training-spacy-model-not-working-running-the-train-ner-script-has-no-effect
In spaCy is there a way to extract the sentence the entity has been extracted from?,"<p>considering the following sentence:</p>

<blockquote>
  <p>""the quick brown fox jumps over the lazy dog""</p>
</blockquote>

<p>and if I want to extract brown, extracting only brown is fairly easy to do however I want the following output:</p>

<blockquote>
  <p>""brown"" , ""the quick brown fox jumps over the lazy dog""</p>
</blockquote>
","python, spacy, named-entity-recognition","<p>Yes, there is a way to do this in Spacy.  You have to iterate over the entity <em>Span</em> objects and extract the sentence from each <em>Span</em> object. Here is an example:</p>

<pre><code>    doc = nlp(""John and Claire live in London. They have a dog. Claire walks her 
    dog everyday."")
    for entity in doc.ents:
       print('Entity extracted : ', entity.text)
       print('Sentence extracted from : ', entity.sent)

</code></pre>

<p>This should give you the following output :</p>

<pre><code>Entity extracted :  John
Sentence extracted from :  John and Claire live in London.

Entity extracted :  Claire
Sentence extracted from :  John and Claire live in London.

Entity extracted :  London
Sentence extracted from :  John and Claire live in London.

Entity extracted :  Claire
Sentence extracted from :  Claire walks her dog everyday.

Entity extracted :  everyday
Sentence extracted from :  Claire walks her dog everyday.
</code></pre>
",7,1,1652,2019-06-20 22:40:18,https://stackoverflow.com/questions/56694713/in-spacy-is-there-a-way-to-extract-the-sentence-the-entity-has-been-extracted-fr
How to tag named entities to prepare training data for custom named entity recognition with spacy?,"<p>I want to train spacy named entity recognizer on my custom dataset. I have prepared a python dictionary having <strong>key = entity_type and list of values = entity name</strong>, but i'm not getting any way using which I can tag the tokens in proper format.</p>

<p>I have tried normal string matching(find) and regular expression(search, compile) but not getting what I want.</p>

<p>for ex: my sentence and the dict I'm using are(this is the example)</p>

<pre><code>sentence = ""Machine learning and data mining often employ the same methods
and overlap significantly.""

dic = {'MLDM': ['machine learning and data mining'], 'ML': ['machine learning'],
 'DM': ['data mining']}

for k,v in dic.items():
  for val in v:
    if val in sentence:
      print(k, val, sentence.index(val)) #right now I'm just printing 
#the key, val and starting index

output:
MLDM machine learning and data mining 0
ML machine learning 0
DM data mining 21

expected output: MLDM 0 32

so I can further prepare training data to train Spacy NER : 
[{""content"":""machine learning and data mining often employ the same methods 
and overlap significantly."",""entities"":[[0,32,""MLDM""]]}
</code></pre>
","regex, python-3.x, spacy, named-entity-recognition","<p>You may build a regex from all values in your <code>dic</code> to match them as whole words and upon a match grab the key associated with the matched value. I assume the value items are unique in the dictionary, they can contain whitespaces and only contain ""word"" characters (no special ones like <code>+</code> or <code>(</code>).</p>

<pre><code>import re

sentence = ""Machine learning and data mining often employ the same methods and overlap significantly.""

dic = {'MLDM': ['machine learning and data mining'], 'ML': ['machine learning'],
 'DM': ['data mining']}

def get_key(val):
    for k,v in dic.items():
        if m.group().lower() in map(str.lower, v):
            return k
    return ''

# Flatten the lists in values and sort the list by length in descending order
l=sorted([v for x in dic.values() for v in x], key=len, reverse=True)
# Build the alternation based regex with \b to match each item as a whole word 
rx=r'\b(?:{})\b'.format(""|"".join(l))
for m in re.finditer(rx, sentence, re.I): # Search case insensitively
    key = get_key(m.group())
    if key:
        print(""{} {}"".format(key, m.start()))
</code></pre>

<p>See the <a href=""https://ideone.com/VRKHBW"" rel=""nofollow noreferrer"">Python demo</a></p>
",1,1,949,2019-06-25 06:30:37,https://stackoverflow.com/questions/56748048/how-to-tag-named-entities-to-prepare-training-data-for-custom-named-entity-recog
Perform Named Entity Recognition - NLP,"<p>I am trying to learn how to perform Named Entity Recognition.</p>

<p>I have a set of discharge summaries containing medical information about patients. I converted my unstructured data into structured data. Now, I have a <code>DataFrame</code> that looks like this:</p>

<pre><code>Text                        |   Target
normal coronary arteries...     R060
</code></pre>

<p>The <code>Text</code> column contains information about the diagnosis of a patient, and the <code>Target</code> column contains the code that will need to be predicted in a further task.</p>

<p>I have also constructed a dictionary that looks like this:</p>

<pre><code>Code (Key) | Term (Value)
A00          Cholera
</code></pre>

<p>This dictionary brings information about each diagnosis and the afferent code. The <code>term</code> column will be used to identify the clinical entities in the corpus.</p>

<p>I will need to train a classifier and predict the code in order to automate the process of assigning codes for the discharge summaries (I am explaining this to have an idea about the task I'm performing).</p>

<p>Until now I have converted my data into a structured one. I am trying to understand how I should perform Named Entity Recognition to label the medical terminology. I would like to try direct matching and fuzzy matching but I am not sure what are the previous steps. Should I perform tokenizing, stemming, lemmatizing before? Or firstly should I find the medical terminology as clinical named entities are often multi-token terms with nested structures that include other named entities inside them? Also what packages or tools are you recommending me to use in Python?</p>

<p>I am new in this field so any help will be appreciated! Thanks!</p>
","python, nlp, fuzzywuzzy, named-entity-recognition","<p>If you are asking for building a classification model, then you should go for deep learning. Deep learning is highly efficient in classification.  </p>

<p>While dealing with such type of language processing tasks, I recommend you to first tokenize your text and do padding. Basic tokenization should be enough, but you can go for more preprocessing like basic string processing because proper preprocessing can improve your model accuracy upto 3% or 4%. For basic string processing, you can use regex(built-in package called re) in python.</p>

<p><a href=""https://docs.python.org/3/library/re.html"" rel=""nofollow noreferrer"">https://docs.python.org/3/library/re.html</a></p>

<p>I think, you are doing mapping after preprocessing. Mapping should be enough for tasks like classification, but I recommend you to learn about word embeddings. Word embedding will improve your model.</p>

<p>For all these tasks, i recommend you to use tensorflow. Tensorflow is famous tool for machine learning, language processing, image processing, and much more. You can learn natural language processing from official tensorflow documentation. They have provided all learning material in tensorflow tutorial section.</p>

<p><a href=""https://www.tensorflow.org/tutorials/"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/</a></p>

<p>I think, this will help you. All the best for your work!!!!</p>

<p>Thank you.</p>
",1,0,252,2019-07-01 11:05:52,https://stackoverflow.com/questions/56834587/perform-named-entity-recognition-nlp
How to get information regarding population/Country in the text using NLTK package,"<p>I have text which has information about the population as well the country.I would like to get the NER for the population as well as the country. </p>

<p>My text is as follow:</p>

<p>text_sent = antigens in arterial occlusive diseases in japan.using a nih standard lymphocytotoxicity test, a possible japanese specific antigen, bjw 22.2 was identified in 17 out of 48 patients with thromboangiitis obliterans (35.4 per cent), in 5 out of 15 patients with takayasu's arteritis (33.3 per cent) and in 11 out of 113 normal controls (9.7 per cent). </p>

<p>I have tried using this </p>

<p><em>from nltk import word_tokenize, pos_tag, ne_chunk
ne_chunk(pos_tag(word_tokenize(text_sent )))</em></p>

<p>i got the tagging but <strong>didnt get any GPE tagged word</strong>.</p>

<p>(S
  antigens/NNS
  in/IN
  arterial/JJ
  occlusive/JJ
  diseases/NNS
  in/IN
  japan.using/VBG
  a/DT
  nih/JJ
  standard/JJ
  lymphocytotoxicity/NN
  test/NN
  ,/,
  a/DT
  possible/JJ
  japanese/JJ
  specific/JJ
  antigen/NN
  ,/,
  bjw/JJ
  22.2/CD
  was/VBD
  identified/VBN
  in/IN
  17/CD
  out/IN
  of/IN
  48/CD
  patients/NNS
  with/IN
  thromboangiitis/NN
  obliterans/NNS
  (/(
  35.4/CD
  per/IN
  cent/NN
  )/)
  ,/,
  in/IN
  5/CD
  out/IN
  of/IN
  15/CD
  patients/NNS
  with/IN
  takayasu/NN
  's/POS
  arteritis/NN
  (/(
  33.3/CD
  per/IN
  cent/NN
  )/)
  and/CC
  in/IN
  11/CD
  out/IN
  of/IN
  113/CD
  normal/JJ
  controls/NNS
  (/(
  9.7/CD
  per/IN
  cent/NN
  )/)
  ./.)</p>
","python-3.x, nltk, named-entity-recognition","<p>you are not getting GPE tagged because ""japan.using"" is not a name of geographical location instead it Should be Japan using </p>

<p>I Have tried this using trained spacy model  </p>

<pre><code>import spacy 
nlp = spacy.load(""en_core_web_sm"")

doc = nlp(u""antigens in arterial occlusive diseases in japan.using a nih standard lymphocytotoxicity test, a possible japanese specific antigen, bjw 22.2 was identified in 17 out of 48 patients with thromboangiitis obliterans (35.4 per cent), in 5 out of 15 patients with takayasu's arteritis (33.3 per cent) and in 11 out of 113 normal controls (9.7 per cent)."")

for ent in doc.ents:
print(ent.text, ent.start_char, ent.end_char, ent.label_)

#o/p
japanese 106 114 NORP
22.2 137 141 CARDINAL
17 160 162 CARDINAL
48 170 172 CARDINAL
35.4 per cent 215 228 MONEY
5 234 235 CARDINAL
15 243 245 CARDINAL
33.3 per cent 282 295 MONEY
11 304 306 CARDINAL
113 314 317 CARDINAL
9.7 per cent 335 347 MONEY
</code></pre>

<p>But when you modify  'japan.using' with 'Japan. using' you will get GPE tag</p>

<pre><code>Japan 43 48 GPE
japanese 107 115 NORP
22.2 138 142 CARDINAL
17 161 163 CARDINAL
48 171 173 CARDINAL
35.4 per cent 216 229 MONEY
5 235 236 CARDINAL
15 244 246 CARDINAL
33.3 per cent 283 296 MONEY
11 305 307 CARDINAL
113 315 318 CARDINAL
9.7 per cent 336 348 MONEY
</code></pre>
",1,0,33,2019-07-11 07:30:43,https://stackoverflow.com/questions/56983821/how-to-get-information-regarding-population-country-in-the-text-using-nltk-packa
"How to perform NER on true case, then lemmatization on lower case, with spaCy","<p>I try to lemmatize a text using spaCy 2.0.12 with the French model <code>fr_core_news_sm</code>. Morevoer, I want to replace people names by an arbitrary sequence of characters, detecting such names using <code>token.ent_type_ == 'PER'</code>. Example outcome would be ""Pierre aime les chiens"" -> ""~PER~ aimer chien"".</p>

<p>The problem is I can't find a way to do both. I only have these two partial options:</p>

<ul>
<li>I can feed the pipeline with the original text: <code>doc = nlp(text)</code>. Then, the NER will recognize most people names but the lemmas of words starting with a capital won't be correct. For example, the lemmas of the simple question ""Pouvons-nous faire ça?"" would be <code>['Pouvons', '-', 'se', 'faire', 'ça', '?']</code>, where ""Pouvons"" is still an inflected form.</li>
<li>I can feed the pipeline with the lower case text: <code>doc = nlp(text.lower())</code>. Then my previous example would correctly display <code>['pouvoir', '-', 'se', 'faire', 'ça', '?']</code>, but most people names wouldn't be recognized as entities by the NER, as I guess a starting capital is a useful indicator for finding entities.</li>
</ul>

<p>My idea would be to perform the standard pipeline (tagger, parser, NER), then lowercase, and then lemmatize only at the end.</p>

<p>However, lemmatization doesn't seem to have its own pipeline component and the documentation doesn't explain how and where it is performed. <a href=""https://stackoverflow.com/a/52560132/11774841"">This</a> answer seem to imply that lemmatization is performed independent of any pipeline component and possibly at different stages of it. </p>

<p>So my question is: how to choose when to perform the lemmatization and which input to give to it?</p>
","python, nlp, spacy, lemmatization, named-entity-recognition","<p>If you can, use the most recent version of spacy instead. The French lemmatizer has been improved a lot in 2.1.</p>

<p>If you have to use 2.0, consider using an alternate lemmatizer like this one: <a href=""https://spacy.io/universe/project/spacy-lefff"" rel=""nofollow noreferrer"">https://spacy.io/universe/project/spacy-lefff</a></p>
",1,0,1002,2019-07-12 13:59:23,https://stackoverflow.com/questions/57008528/how-to-perform-ner-on-true-case-then-lemmatization-on-lower-case-with-spacy
Meaning of &quot;drop&quot; in SpaCy custom NER model training?,"<p>Below code is an example training loop for SpaCy's named entity recognition(<code>NER</code>). </p>

<pre><code>for itn in range(100):
    random.shuffle(train_data)
    for raw_text, entity_offsets in train_data:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)
        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)
nlp.to_disk(""/model"")
</code></pre>

<p><code>drop</code>  as per <code>spacy</code> is the drop out rate. Can somebody explain the meaning of the same in detail?</p>
","python, nlp, spacy, named-entity-recognition","<p>According to the documentation <a href=""https://spacy.io/api/entityrecognizer"" rel=""noreferrer"">here</a>, the SpaCy <code>Entity Recognizer</code> is a neural network that should implement the <strong>thinc.neural.Model</strong> API. The <code>drop</code> argument that you are talking about is something called <a href=""https://en.wikipedia.org/wiki/Dropout_(neural_networks)"" rel=""noreferrer"">dropout</a> rate which is a way to optimize a neural network.</p>

<p>The recommended value is <code>0.2</code> based on my experience which means that about 20% of the neurons used in this model will be dropped randomly during training. </p>
",6,4,2298,2019-08-06 06:56:55,https://stackoverflow.com/questions/57370524/meaning-of-drop-in-spacy-custom-ner-model-training
How to prepare data for spacy&#39;s custom named entity recognition?,"<p>I'm trying to prepare a training dataset for custom named entity recognition using spacy. My data has a variable 'Text', which contains some sentences, a variable 'Names', which has names of people from the previous variable (sentences). After going through some examples and spacy's documentation, I realised that one has to pass index of the entity while preparing the dataset. I want to know if there's any way to pass the entity as a string directly while preparing the dataset ?</p>

<p>Reference: ""<a href=""https://medium.com/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6"" rel=""nofollow noreferrer"">https://medium.com/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6</a>""</p>
","python-3.x, nlp, spacy, named-entity-recognition","<p>No, spaCy will need exact start &amp; end indices for your entity strings, since the string by itself may not always be uniquely identified and resolved in the source text.  Examples:</p>

<ul>
<li><code>Apple</code> is usually an ORG, but can be a PERSON.</li>
<li><code>Ann</code> is a PERSON, but not in <code>Annotation tools are best for this purpose.</code></li>
</ul>

<p>In python, you can use the re module to grab the indices:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; [m.span() for m in re.finditer('Amazon', 'The Amazon is a river in South America.  Amazon Inc is a company.')]
[(4, 10), (41, 47)]

</code></pre>

<p>You will have to go through and verify the indices before creating your spaCy training set.</p>
",4,2,1862,2019-08-08 14:32:43,https://stackoverflow.com/questions/57415016/how-to-prepare-data-for-spacys-custom-named-entity-recognition
POS tagging and NER for Chinese Text with Spacy,"<ul>
<li>I am trying to print the entities and pos present in Chinese text. </li>
<li>I have installed # !pip3 install jieba and used Google colab for the below script.</li>
</ul>

<p>But I am getting empty tuples for the entities and no results for pos_.</p>

<pre class=""lang-py prettyprint-override""><code>from spacy.lang.zh import Chinese

nlp = Chinese()
doc = nlp(u""蘋果公司正考量用一億元買下英國的新創公司"")

doc.ents
# returns (), i.e. empty tuple


for word in doc:
    print(word.text, word.pos_)

''' returns
蘋果 
公司 
正 
考量 
用 
一 
億元 
買 
下 
英國 
的 
新創 
公司 
'''

</code></pre>

<p>I am new to NLP. I want to know what is the correct way to do ?</p>
","nlp, spacy, named-entity-recognition","<p><strong>EDIT 3/21: Spacy now supports NER and POS tagging for CN</strong></p>
<p>Find the SpaCy model here: <a href=""https://spacy.io/models/zh"" rel=""nofollow noreferrer"">https://spacy.io/models/zh</a></p>
<p><strong>OLD ANSWER:</strong></p>
<p>SpaCy is a fantastic package, but as of yet does not support Chinese, so I assume thats the reason you dont get POS results - even though your sentence is</p>
<blockquote>
<p>&quot;Apple is looking at buying U.K. startup for $1 billion&quot;</p>
</blockquote>
<p>in traditional Chinese and should therefore return &quot;Apple&quot; and &quot;U.K.&quot; as <code>ent</code>, among others.</p>
<p>For a more extensive NLP approach to traditional Chinese, you can try using the <a href=""https://nlp.stanford.edu/projects/chinese-nlp.shtml"" rel=""nofollow noreferrer"">Stanford Chinese NLP</a> package - you are using python, and there are versions available for python (see a <a href=""https://github.com/stanfordnlp/stanfordnlp/blob/master/demo/pipeline_demo.py"" rel=""nofollow noreferrer"">demo script</a> or an <a href=""https://medium.com/analytics-vidhya/introduction-to-stanfordnlp-an-nlp-library-for-53-languages-with-python-code-d7c3efdca118"" rel=""nofollow noreferrer"">intro on Medium</a>), but the original is Java, if you are more comfortable with that.</p>
",2,1,2802,2019-08-12 03:11:15,https://stackoverflow.com/questions/57455267/pos-tagging-and-ner-for-chinese-text-with-spacy
Clarification on the use of Vocab file in NER,"<p>I am learning Named Entity Recognition, and i see that the training script uses a variable called <code>vocab</code> which looks like this </p>

<pre><code>vocab = ""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\'-/\t \n\r\x0b\x0c:""
</code></pre>

<p>My Guess is that it is supposed to learn all these characters present in the text like abcd... etc, what i dont understand is the use of char like <code>/n /t</code>  what is the use of these char? and in general this variable?</p>

<p>Thanks in advance. </p>
","deep-learning, nlp, named-entity-recognition","<p>This string is the vocabulary. In the context of NLP, vocabulary is a list of all words or characters used in the training set. In your example the vocabulary is a list of characters. Specifically <code>\n</code> is a newline, and <code>\t</code> a tab.</p>

<p>For NER and other nlp tasks, we usually use a vocabulary to produce embeddings for each token (word or char), and these embeddings are fed to the machine learning model (nowadays, neural networks architectures such as LSTM are used to get the best results). Character based embeddings have an advantage over word based embeddings for OOV (Out-of-vocabulary) words, i.e. words that do not appear in the training set, but are encountered during inference.</p>
",1,1,222,2019-08-13 06:49:55,https://stackoverflow.com/questions/57472501/clarification-on-the-use-of-vocab-file-in-ner
Getting full names from NER,"<p>From reading through the docs and playing with the API, it looks like CoreNLP will tell me the NER tags per token, but it won't help me extract out full names from a sentence. For example:</p>

<pre><code>Input: John Wayne and Mary have coffee
CoreNLP Output: (John,PERSON) (Wayne,PERSON) (and,O) (Mary,PERSON) (have,O) (coffee,O)
Desired Result: list of PERSON ==&gt; [John Wayne, Mary]
</code></pre>

<p>Unless there is some flag I missed, I believe to do this I will need to parse the tokens and glue together successive tokens tagged PERSON.</p>

<p>Can someone confirm that this is indeed what I need to do? I mostly want to know if there is some flag or utility in CoreNLP that does something like this for me. Bonus points if someone has a utility (ideally Java, since I'm using the Java API) that does this and wants to share :)</p>

<p>Thanks!</p>

<p>PS: There was a very similar question <a href=""https://stackoverflow.com/questions/25842982/tagging-full-name-in-stanford-ner"">here</a>, which seems to suggest the answer is ""roll your own"", but it was never confirmed by anyone else.</p>
","java, nlp, stanford-nlp, named-entity-recognition","<p>Your are probably looking for <a href=""https://stanfordnlp.github.io/CoreNLP/ner.html#entity-mention-detection"" rel=""nofollow noreferrer"">entity mentions</a> instead of or as well as NER tags. For example with the <a href=""https://stanfordnlp.github.io/CoreNLP/simple.html"" rel=""nofollow noreferrer"">Simple API</a>:</p>

<pre><code>new Sentence(""Jimi Hendrix was the greatest"").nerTags()
[PERSON, PERSON, O, O, O]

new Sentence(""Jimi Hendrix was the greatest"").mentions()
[Jimi Hendrix]
</code></pre>

<p>The link above has an example with the traditional non-simple API using a good old <code>StanfordCoreNLP</code> pipeline</p>
",2,1,827,2019-08-20 15:09:37,https://stackoverflow.com/questions/57576640/getting-full-names-from-ner
Shoud i use Spacy Named Entity Recognition for this case?,"<p>I have a set of names, a fixed set of names which can extend up-to 50,000 names.</p>

<p>""John"",""Mike"",""Josh"",""Peter"",""Karl"".</p>

<p>And I have a document, this document is dynamic. I need to find whether this document has
the predefined name or not ?</p>

<p>Is defining everything as a entity in spacy nlp the right way to do it ?</p>
","nlp, nltk, spacy, opennlp, named-entity-recognition","<p>I understand that your purpose is to look for known names (from a list) in a document.</p>

<p>It seems that Named Entity Recognition may not be useful to you.</p>

<p>Instead, a scalable approach to this problem can be Flashtext (<a href=""https://github.com/vi3k6i5/flashtext"" rel=""nofollow noreferrer"">https://github.com/vi3k6i5/flashtext</a>).</p>
",1,-1,491,2019-08-22 11:28:50,https://stackoverflow.com/questions/57608346/shoud-i-use-spacy-named-entity-recognition-for-this-case
What is the good metric to evaluate NER model trained in Spacy,"<p>I have 3000 manually labeled data set, divided into train and test set I have trained the NER model using SpaCy, to extract 8 custom entities like ""ACTION"", HIRE-DATE, STATUS etc... To evaluate the model I am using SpaCy Scorer. </p>

<p>There is no Accuracy metrics in the output, I am not sure which metric should I consider to decide whether the model performance is Good or Bad?</p>

<p>There are couple of cases where precision is   low but the recall is 100 and f1 is also low eg:</p>

<pre><code>'LOCATION': {'p': 7.142857142857142, 'r': 100.0, 'f': 13.333333333333334},
</code></pre>

<p>in the above case what should be our conclusion?</p>

<p>Following is the full result of the Scorer, Where p=precision, r=recall and f=F1 score.... it has got overall performance and Entity wise performance. </p>

<pre><code>{
'uas': 0.0,
 'las': 0.0,
 'ents_p': 86.40850417615793,
 'ents_r': 97.93459552495698,
 'ents_f': 91.81121419927389,
 'ents_per_type': {'ACTION': {'p': 97.17682020802377,
   'r': 97.61194029850746,
   'f': 97.3938942665674},
  'STATUS': {'p': 83.33333333333334,
   'r': 96.3855421686747,
   'f': 89.3854748603352},
  'PED': {'p': 98.61751152073732,
   'r': 99.53488372093024,
   'f': 99.07407407407408},
  'TERM-DATE': {'p': 83.52272727272727,
   'r': 98.65771812080537,
   'f': 90.46153846153847},
  'LOCATION': {'p': 7.142857142857142, 'r': 100.0, 'f': 13.333333333333334},
  'DOB': {'p': 10.0, 'r': 100.0, 'f': 18.181818181818183},
  'RE-HIRE-DATE': {'p': 34.84848484848485,
   'r': 100.0,
   'f': 51.685393258426956},
  'HIRE-DATE': {'p': 18.96551724137931, 'r': 100.0, 'f': 31.88405797101449},
  'PED-CED': {'p': 100.0, 'r': 71.42857142857143, 'f': 83.33333333333333},
  'CED': {'p': 100.0, 'r': 100.0, 'f': 100.0}},
 'tags_acc': 0.0,
 'token_acc': 100.0}
</code></pre>

<p>Kindly Suggest.</p>
","machine-learning, spacy, named-entity-recognition","<p>It depends on your application. What's worse: missing an entity, or wrongly flagging something as an entity? If failing to label an entity (false negative) is bad, then you care about recall. If wrongly flagging a non-entity as an entity (false positive) is bad, you care about precision. If you care about both precision and recall the same, use F_1. If you care about precision (false positives) twice as much as recall (false negatives), use F_0.5. You can do F_b for any b to express what you care about. The formula is shown and explained on the <a href=""https://en.wikipedia.org/wiki/F1_score"" rel=""noreferrer"">Wikipedia page for F Score</a></p>

<p>Edit: answering the direct question from the original post:</p>

<p>The system does badly at LOCATION and the 3 date entities. The others look good. If it were me, I would try to use NER to extract all dates as one entity, then try to build a separate system, rule based or a classifier, for distinguishing between the different kinds of dates. For location, you could use a system that focuses on just geo-parsing, such as <a href=""https://github.com/openeventdata/mordecai"" rel=""noreferrer"">Mordecai</a>. </p>
",6,5,2484,2019-09-01 16:54:05,https://stackoverflow.com/questions/57747872/what-is-the-good-metric-to-evaluate-ner-model-trained-in-spacy
Add custom entity in addition to NER basic model,"<p>I am using spacy to train my own NER model. In addition to entities trained by spacy basic 'en_core_web_sm' model (ORG, PERSON, DATE, etc), I want to add my own entities. I trained my model with 'en_core_web_sm' as my base model, but then the model can only detect my own custom entities only, not the basic entities. Is there any way to do this? Thanks.</p>
","machine-learning, nlp, spacy, named-entity-recognition","<p>You can definitely do this with spaCy, cf the <a href=""https://spacy.io/usage/training#example-new-entity-type"" rel=""nofollow noreferrer"">docs</a> and also check <a href=""https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"" rel=""nofollow noreferrer"">Matt's blogpost</a>  around the problem of ""catastrophic forgetting"" (when your model ""forgets"" about the old types it knew before, which you obviously want to avoid).</p>
",1,1,521,2019-09-10 10:00:03,https://stackoverflow.com/questions/57868372/add-custom-entity-in-addition-to-ner-basic-model
SpaCy NER: Can a same word be part of two different entities?,"<p>For example:</p>
<p>Sentence: The best product in the world is Nestle Cookies.</p>
<blockquote>
<p>Entities:</p>
<p>BRAND: Nestle</p>
<p>PRODUCT: Nestle Cookie</p>
</blockquote>
<p>Are the above entities valid, or should I tag them as:</p>
<blockquote>
<p>Entities:</p>
<p>BRAND: Nestle</p>
<p>PRODUCT: Cookie</p>
</blockquote>
<p>And will it affect model performance?</p>
","nlp, stanford-nlp, spacy, feature-extraction, named-entity-recognition","<p>From the <a href=""https://support.prodi.gy/t/what-happens-if-your-annotation-has-overlapping-entity-spans/363"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>The entity recognizer is constrained to predict only non-overlapping, non-nested spans. The training data should obey the same constraint. If you like, you could have two sentences with the different annotations in your data. I’m not sure whether this would hurt or help your performance, though.</p>
<p>If you want spaCy to learn to recover both annotations, you could have two EntityRecognizer instances in the pipeline. You would need to move the entity annotations into an extension attribute, because you don’t want the second entity recogniser to overwrite the entities set by the first one.</p>
</blockquote>
<p>Consequence:</p>
<p>If you want to have a single NER tagger you must label as follows:<br />
Entities: BRAND: Nestle PRODUCT: Cookie</p>
<p>If you want to train two separate NER taggers (one for BRAND and one for PRODUCT)   then you can do:<br />
Entities: BRAND: Nestle PRODUCT: Nestle Cookie</p>
",4,3,2203,2019-09-11 09:27:13,https://stackoverflow.com/questions/57886043/spacy-ner-can-a-same-word-be-part-of-two-different-entities
Train Spacy NER model with &#39;en_core_web_sm&#39; as base model,"<p>I am using Spacy to train my NER model with new entities and I am using <code>en_core_web_sm</code> model as my base model because I also want to detect the basic entities (<code>ORG</code>, <code>PERSON</code>, <code>DATE</code>, etc). I ran <code>en_core_web_sm</code> model over unlabelled sentences, and adding their annotations to my training set.</p>

<p>After I finished with that, now I want to create the training data for the new entities. For example, I want to add a new entity called <code>FRUIT</code>. I have a bunch of sentences (in addition to those that were annotated using <code>en_core_web_sm</code> earlier) that I am going to annotate. The sentence example is:  </p>

<blockquote>
  <p>""James likes eating apples"".</p>
</blockquote>

<p><strong><em>My question is</em></strong>: Do I still need to annotate ""<em>James</em>"" as <code>PERSON</code> as well as annotating ""<em>apples</em>"" as <code>FRUIT</code>? Or whether I don't need to do it because I already have another bunch of sentences that were annotated with <code>PERSON</code> entity using <code>en_core_web_sm</code> model earlier.</p>
","machine-learning, nlp, spacy, named-entity-recognition","<p><strong>Short answer:</strong></p>

<p>Yes, if you want to keep your model precise.</p>

<p><strong>Long answer:</strong></p>

<p>NER is implemented using Machine Learning algorithms. These classify a token as a Entity based on learned distributions and surrounding tokens. </p>

<p>Therefore, if you provide several samples of annotated text without marking a word (token) as a specific Entity that it usually represents, you may affect your model precision by providing samples to your model where that token is unimportant.</p>
",0,0,548,2019-09-11 13:54:02,https://stackoverflow.com/questions/57890739/train-spacy-ner-model-with-en-core-web-sm-as-base-model
Need approach on building Custom NER for extracting below keywords from any format of payslips,"<p>I am trying to build a generic extraction of below parameters from any format of payslip:</p>

<ol>
<li>Name</li>
<li>His PostCode</li>
<li>Pay Date</li>
<li>Net Pay.</li>
</ol>

<p>Challenge I am facing is due to variety of format that may come, I want to apply NER (Spacy) to learn these under the entities</p>

<ol>
<li>Name - PERSON</li>
<li>His PostCode</li>
<li>Pay Date - DATE</li>
<li>Net Pay. - MONEY</li>
</ol>

<p>But I am unsuccess so far, I even tried to build a custom EntityMatcher for Postcode &amp; Date but to no success.</p>

<p>I seek any guideline and approach to make me take the right path in achieving the above ask, as to what is the right and best approach under the ML to achieve this.</p>

<p>A snippet of Custom NER I tried to build</p>

<pre><code>import spacy
import random
import threading
import time
from DateEntityMatcher import  DateEntityMatcher
from PostCodeEntityMatcher import PostCodeEntityMatcher


class IncomeValidatorModel(object):
    """""" Threading example class
    The run() method will be started and it will run in the background
    until the application exits.
    """"""

    def __init__(self, interval=1):
        """""" Constructor
        :type interval: int
        :param interval: Check interval, in seconds
        """"""
        self.interval = interval

        thread = threading.Thread(target=self.run, args=())
        thread.daemon = True                            # Daemonize thread
        thread.start()                                  # Start the execution

    def run(self):
        """""" Method that runs forever """"""
        while True:
            # Do something
            print('Doing something important in the background')
            DATA = [
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR K KHANA    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M MENON    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR F JAHAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR A JAHAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
                {'entities': [(203, 218, 'ORG'), (100, 106, 'PERSON'), (1097, 1103, 'MONEY')]}),
                (u""Sample Payslip    Matrix House    Basing View    Basingstoke    Hampshire    RG21 4FF    Advantage Resourcing    6th Floor, Matrix House, Basing View, Basingstoke, Hampshire, RG21 4FF    Registered Number 03341461    COMPANY    DIVISION    Advantage Resourcing UK    SWINDON    WORKER NO.    NAME    PERIOD    PAY DATE    IND    123456    Sample Payslip    14/2016    08/07/2016    W1    DEPARTMENT    TAX CODE    N.I. NO./TABLE LETTER    NAT    1100L    JA123456A/A    PAYMENTS    DEDUCTIONS    Wk Ending    Timesheet    Description    Units    Rate    Amount    Deduction    Amount    03/07/2016    GEN000499628 Hourly Rate    40.00    10.00    400.00    Tax    87.60    03/07/2016    GEN000499628 Week Day Overtime    10.00    15.00    150.00    NI    59.40    03/07/2016    GEN000499628 Saturday Overtime    5.00    20.00    100.00    TOTAL PAYMENTS    650.00    TOTAL DEDUCTIONS    147.00    CUMULATIVES    GROSS TO DATE    650.00    Current Holiday Entitlement: 0.00 Unit(s)    TAXABLE PAY TO DATE    650.00    EE PENSION TO DATE    0.00    ER PENSION TO DATE    0.00    TAX TO DATE    87.60     TO DATE    68.17    TO DATE    59.40    c Safe Computing Limited 2002    NET PAY    503.00"",
                {'entities': [(89, 109, 'ORG'), (0, 14, 'PERSON'), (1186, 1191, 'MONEY')]}),
                (u""Mubssar Hasan    Matrix House    Basing View    Basingstoke    Hampshire    RG21 4FF    Advantage Resourcing    6th Floor, Matrix House, Basing View, Basingstoke, Hampshire, RG21 4FF    Registered Number 03341461    COMPANY    DIVISION    Advantage Resourcing UK    SWINDON    WORKER NO.    NAME    PERIOD    PAY DATE    IND    123456    Sample Payslip    14/2016    08/07/2016    W1    DEPARTMENT    TAX CODE    N.I. NO./TABLE LETTER    NAT    1100L    JA123456A/A    PAYMENTS    DEDUCTIONS    Wk Ending    Timesheet    Description    Units    Rate    Amount    Deduction    Amount    03/07/2016    GEN000499628 Hourly Rate    40.00    10.00    400.00    Tax    87.60    03/07/2016    GEN000499628 Week Day Overtime    10.00    15.00    150.00    NI    59.40    03/07/2016    GEN000499628 Saturday Overtime    5.00    20.00    100.00    TOTAL PAYMENTS    650.00    TOTAL DEDUCTIONS    147.00    CUMULATIVES    GROSS TO DATE    650.00    Current Holiday Entitlement: 0.00 Unit(s)    TAXABLE PAY TO DATE    650.00    EE PENSION TO DATE    0.00    ER PENSION TO DATE    0.00    TAX TO DATE    87.60     TO DATE    68.17     TO DATE    59.40    c Safe Computing Limited 2002    NET PAY    503.00"",
                {'entities': [(88, 108, 'ORG'), (0, 13, 'PERSON'), (1186, 1191, 'MONEY')]}),
                (u""Oracle Corp Anil Menon Work Date 01/09/2019 PAYMENTS Tax 100 Net Pay 2000"",
                 {'entities': [(0, 10, 'ORG'), (12, 21, 'PERSON'), (69, 72, 'MONEY')]}),
                (u""Huawei Corp Anil Menon Work Date 01/06/2019 PAYMENTS Tax 100 Net Pay 1900"",
                 {'entities': [(0, 10, 'ORG'), (12, 21, 'PERSON'), (69, 72, 'MONEY')]}),
                (u""Tata Corp Nitin Garg Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1900"",
                 {'entities': [(0, 8, 'ORG'), (10, 19, 'PERSON'), (67, 70, 'MONEY')]}),
                (u""Accenture Corp Amol Joshi Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 900"",
                 {'entities': [(0, 15, 'ORG'), (17, 26, 'PERSON'), (72, 74, 'MONEY')]}),
                (u""Cognizant Corp Anup Nair Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 900"",
                 {'entities': [(0, 15, 'ORG'), (17, 25, 'PERSON'), (71, 73, 'MONEY')]}),
                (u""Cognizant Corp Sajit Kumar Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1900"",
                 {'entities': [(0, 15, 'ORG'), (17, 27, 'PERSON'), (73, 76, 'MONEY')]}),
                (u""Tata Corp Saurabh Dave Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1300"",
                 {'entities': [(0, 8, 'ORG'), (10, 21, 'PERSON'), (69, 72, 'MONEY')]}),
                (u""Capgemini PLC Mubashshir Hasan Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1700"",
                 {'entities': [(0, 12, 'ORG'), (14, 29, 'PERSON'), (77, 80, 'MONEY')]}),
                (u""Capgemini PLC Sagar Pande Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 1700"",
                 {'entities': [(0, 12, 'ORG'), (14, 24, 'PERSON'), (72, 75, 'MONEY')]}),
                (u""Capgemini PLC Sreeram Yegappan Work Date 20/04/2019 PAYMENTS Tax 100 Net Pay 2000"",
                 {'entities': [(0, 12, 'ORG'), (14, 29, 'PERSON'), (77, 80, 'MONEY')]})
            ]

            # nlp = spacy.blank('en')  # new, empty model. Let’s say it’s for the English language
            global nlp
            nlp = spacy.load('en_core_web_sm')
            nlp.entity.add_label('ORG')
            nlp.entity.add_label('PERSON')
            nlp.entity.add_label('MONEY')

            # add NER pipeline
            # ner = nlp.create_pipe('ner')  # our pipeline would just do NER
            # nlp.add_pipe(ner, last=True)  # we add the pipeline to the model
            postcde_entity_matcher = PostCodeEntityMatcher(nlp, ['NN1 3LE', 'NN2 8HF', 'IG3 8TH', 'NN4 7YH', 'RG21 5GH'], 'POSTCDE')
            nlp.entity.add_label('POSTCDE')
            nlp.add_pipe(postcde_entity_matcher, before='ner')

            date_entity_matcher = DateEntityMatcher(nlp, ['20/04/2019','20/04/2019', '25/04/2016', '20/04/2019', '20/07/2019', '20/12/2019'], 'DATE')
            nlp.entity.add_label('DATE')
            nlp.add_pipe(date_entity_matcher, before='ner')

            optimizer = nlp.begin_training()

            for i in range(11):
                random.shuffle(DATA)
                for text, annotations in DATA:
                    nlp.update([text], [annotations], sgd=optimizer)

            time.sleep(self.interval)

    def extractPayslipData(self, data):
        doc = nlp(data)
        for entity in doc.ents:
            print(entity.label_, ' | ', entity.text)
        return doc.ents
</code></pre>
","python-3.x, nlp, spacy, named-entity-recognition","<p><strong>Training json(x.json) should be like this:-</strong></p>

<pre><code>[{
    ""text"": ""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53"",
    ""entities"": [
        [
            191,
            198,
            ""PERSON""
        ],
        [
            202,
            211,
            ""ORG""
        ],
        [
            150,
            157,
            ""POST_CODE""
        ],
        [
            1096,
            1103,
            ""MONEY""
        ]]
},
{
    ""text"": ""Mubssar Hasan    Matrix House    Basing View    Basingstoke    Hampshire    RG21 4FF    Advantage Resourcing    6th Floor, Matrix House, Basing View, Basingstoke, Hampshire, RG21 4FF    Registered Number 03341461    COMPANY    DIVISION    Advantage Resourcing UK    SWINDON    WORKER NO.    NAME    PERIOD    PAY DATE    IND    123456    Sample Payslip    14/2016    08/07/2016    W1    DEPARTMENT    TAX CODE    N.I. NO./TABLE LETTER    NAT    1100L    JA123456A/A    PAYMENTS    DEDUCTIONS    Wk Ending    Timesheet    Description    Units    Rate    Amount    Deduction    Amount    03/07/2016    GEN000499628 Hourly Rate    40.00    10.00    400.00    Tax    87.60    03/07/2016    GEN000499628 Week Day Overtime    10.00    15.00    150.00    NI    59.40    03/07/2016    GEN000499628 Saturday Overtime    5.00    20.00    100.00    TOTAL PAYMENTS    650.00    TOTAL DEDUCTIONS    147.00    CUMULATIVES    GROSS TO DATE    650.00    Current Holiday Entitlement: 0.00 Unit(s)    TAXABLE PAY TO DATE    650.00    EE PENSION TO DATE    0.00    ER PENSION TO DATE    0.00    TAX TO DATE    87.60     TO DATE    68.17     TO DATE    59.40    c Safe Computing Limited 2002    NET PAY    503.00"",
    ""entities"": [
        [
            1,
            13,
            ""PERSON""
        ],
        [
            88,
            108,
            ""ORG""
        ],
        [
            150,
            157,
            ""POST_CODE""
        ],
        [
            1186,
            1192,
            ""MONEY""
        ]]
}

]
</code></pre>

<p><strong>code:-</strong></p>

<pre><code>with open(training_pickel_file) as input:
TRAIN_DATA = json.load(input)

def main(model=None, output_dir=""/home/NLP/model"", n_iter=50):
    if model is not None:
    nlp = spacy.load(model)
    print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print(""Created blank 'en' model"")

    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    # otherwise, get it so we can add labels
    else:
        ner = nlp.get_pipe('ner')

        for annotations in TRAIN_DATA:
            for ent in annotations[""entities""]:
                ner.add_label(ent[2])
        print(ner)
        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
        with nlp.disable_pipes(*other_pipes):  # only train NER
            optimizer = nlp.begin_training()
            for itn in range(n_iter):
                random.shuffle(TRAIN_DATA)
                losses = {}
                for a in TRAIN_DATA:`
                    doc = nlp.make_doc(a[""text""])
                    gold = GoldParse(doc, entities = a[""entities""])
                    nlp.update([doc], [gold], drop =0.5, sgd=optimizer, losses = losses)
                print('Losses', losses)
           if output_dir is not None:
               output_dir = Path(output_dir)
               if not output_dir.exists():
                   output_dir.mkdir()
               nlp.to_disk(output_dir)
</code></pre>

<p><strong>Model Testing :-</strong></p>

<pre><code>sen = [""""""PRIVATE &amp; CONFIDENTIAL    REF. No.    DEPT    SITE    PAY DATE    82521    002    31/07/2019    MR M HASAN    69 ALCOMBE ROAD    NORTHAMPTON    UK    NN1 3LE    CONFIDENTIAL PAY ADVICE    MR M HASAN    CAPGEMINI UK PLC    EMP REFERENCE    COME A TAXDISTRICT    TAXREFERENCE    D83/82521    475/VB53759    TAXABLE PAY    14297.14    AY DATE    31/07/2019    TAX PERIOD    2019-04    ANN. SALARY    49650.00    TAX PAID    1611.40    PAY METHOD    BACS    TAX CODE    1871L    PAY PERIOD    MONTHLY    N.I. EMPLOYEE    1365.96    N.I. NUMBER    SY095026C    CONTRACT HRS    40.00    PERIOD PAY    4137.50    N.I. EMPLOYER    1576.11    N.I. TABLE    A    O/TIME RATE    23.8702    HOURLY RATE    23.8702    PAYMENTS    DEDUCTIONS    DESCRIPTION    HRS/UNITS    RATE    VALUE    TO DATE    DESCRIPTION    VALUE    BAL ANCE    TO DATE    BENEFIT ALLOW    620.67    706.61    NAT.INS    385.84    1365.96    DISP NT    -353.08    -1253.08    P.A.Y.E.    474.80    1611.40    SALARY    4137.50    16514.38    ACCOM NT    -470.77    -1670.77    GROSS PAY    4758.17    TOTAL DEDUCTIONS    860.64    NET PAY    3897.53""""""]
for text in sen:
    doc = nlp(text)
    entity = {}
    for ent in doc.ents:
        list_of_ent = []
        list_of_ent.append(ent.text)
        entity.update({ent.label_: list_of_ent})
    print(entity)
</code></pre>

<p><strong>Result :-</strong>
<a href=""https://i.sstatic.net/IWzYX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IWzYX.png"" alt=""enter image description here""></a></p>
",5,2,757,2019-09-12 10:20:21,https://stackoverflow.com/questions/57904642/need-approach-on-building-custom-ner-for-extracting-below-keywords-from-any-form
Using spacy&#39;s Matcher without a model,"<p>I want to use spaCy's Matcher class on a new language (Hebrew) for which spaCy does not yet have a working model.</p>

<p>I found a working tokenizer + POS tagger (from Stanford NLP), yet I would prefer spaCy as its Matcher can help me do some rule-based NER.</p>

<p>Can the rule-based Matcher be fed with POS-tagged text instead of the standard NLP pipeline?</p>
","python, nlp, spacy, named-entity-recognition","<p>You can set the words and tags for a spacy document from another source by hand and then use the Matcher. Here's an example using English words/tags just to demonstrate:</p>

<pre><code>from spacy.lang.he import Hebrew
from spacy.tokens import Doc
from spacy.matcher import Matcher

words = [""my"", ""words""]
tags = [""PRP$"", ""NNS""]

nlp = Hebrew()
doc = Doc(nlp.vocab, words=words)
for i in range(len(doc)):
    doc[i].tag_ = tags[i]

# This is normally set by the tagger. The Matcher validates that
# the Doc has been tagged when you use the `""TAG""` attribute.
doc.is_tagged = True

matcher = Matcher(nlp.vocab)
pattern = [{""TAG"": ""PRP$""}]
matcher.add(""poss"", None, pattern)
print(matcher(doc))
# [(440, 0, 1)]
</code></pre>
",2,1,628,2019-09-18 21:50:52,https://stackoverflow.com/questions/58001184/using-spacys-matcher-without-a-model
Integrate custom trained NER model with existing default model in Stanford CoreNLP,"<p>I have trained corpus by following below link. </p>

<p><a href=""https://www.sicara.ai/blog/2018-04-25-python-train-model-NTLK-stanford-ner-tagger"" rel=""nofollow noreferrer"">https://www.sicara.ai/blog/2018-04-25-python-train-model-NTLK-stanford-ner-tagger</a></p>

<p>Dataset is of some health blog (in English language) on which I've trained. I am successfully able to run this model on my new unseen text. </p>

<p>Problem: The problem I am facing is that I want to run my custom English NER model along with default English model in Stanford CoreNLP. </p>

<p>Desired Outcome: I want Stanford default model to run in sequential manner just after my own custom model NER model to handle those entities in English that are missed by my own model. </p>
","python, stanford-nlp, named-entity-recognition","<pre><code> ner.model = /path/to/custom-model.ser.gz,edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz
</code></pre>

<p>There is more info here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/api.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/api.html</a></p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/cmdline.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/cmdline.html</a></p>
",0,1,392,2019-09-24 05:18:20,https://stackoverflow.com/questions/58073494/integrate-custom-trained-ner-model-with-existing-default-model-in-stanford-coren
Merging tags into my file using named entity annotation,"<p>While learning the basics of text mining i run into the following problem: I must use named entity annotation to find and locate named entities. However, when found, the tag must be included in the document. So for example: ""Hello I am Koen"" must result in ""Hello I am &lt; PERSON> Koen &lt; /PERSON>. </p>

<p>I figured out how to find and label the named entities but I am stuck on getting them in the file in the right way. I've tried comparing if the ent.orth_ is in the file and then replace it with the tag + ent.orth_ + closing tag. </p>

<pre><code>print([(X, X.ent_iob_, X.ent_type_) for X in doc])
</code></pre>

<p>I used this for locating where the entities are and where they start.</p>

<pre><code>for ent in doc.ents:
    entities.append(ent.orth_ + "", "" + ent.label_)
</code></pre>

<p>I used this for creating a variable with both the original form and the label.</p>

<p>Right now i have the variable with all original forms and labels and know where the entities start and end. However when trying to replace it somehow my knowledge runs short and can't find any similar examples.</p>
","tags, spacy, named-entity-recognition, mining","<p>Try this:</p>

<pre><code>import spacy

nlp = spacy.load(""en_core_web_sm"")
s =""Apple is looking at buying U.K. startup for $1 billion""
doc = nlp(s)

def replaceSubstring(s, replacement, position, length_of_replaced):
    s = s[:position] + replacement + s[position+length_of_replaced:]
    return(s)

for ent in reversed(doc.ents):
    #print(ent.text, ent.start_char, ent.end_char, ent.label_)
    replacement = ""&lt;{}&gt;{}&lt;/{}&gt;"".format(ent.label_,ent.text, ent.label_)
    position = ent.start_char
    length_of_replaced = ent.end_char - ent.start_char 
    s = replaceSubstring(s, replacement, position, length_of_replaced)

print(s)
#&lt;ORG&gt;Apple&lt;/ORG&gt; is looking at buying &lt;GPE&gt;U.K.&lt;/GPE&gt; startup for &lt;MONEY&gt;$1 billion&lt;/MONEY&gt;


</code></pre>
",0,0,308,2019-09-24 10:03:10,https://stackoverflow.com/questions/58077806/merging-tags-into-my-file-using-named-entity-annotation
Parse measurements (multiple dimensions) from a given string in Python 3,"<p>I'm aware of <a href=""https://stackoverflow.com/questions/15161012/parse-python-string-for-units"">this post</a> and <a href=""https://pypi.org/project/quantulum3/"" rel=""nofollow noreferrer"">this library</a> but they didn't help me with these specific cases below. How can I parse measurements like below:</p>

<p>I have strings like below;</p>

<pre><code>""Square 10 x 3 x 5 mm""
""Round 23/22; 24,9 x 12,2 x 12,3""
""Square 10x2""
""Straight 10x2mm""
</code></pre>

<p>I'm looking for a Python package or some way to get results like below;</p>

<pre><code>&gt;&gt;&gt; a = amazing_parser.parse(""Square 10 x 3 x 5 mm"")
&gt;&gt;&gt; print(a)
10 x 3 x 5 mm
</code></pre>

<p>Likewise;</p>

<pre><code>&gt;&gt;&gt; a = amazing_parser.parse(""Round 23/22; 24,9x12,2"")
&gt;&gt;&gt; print(a)
24,9 x 12,2
</code></pre>

<p>I also tried to use ""<a href=""http://docs.deeppavlov.ai/en/master/features/models/ner.html"" rel=""nofollow noreferrer"">named entity recognition</a>"" using ""ner_ontonotes_bert_mult"" model. But the results were like below:</p>

<pre><code>&gt;&gt;&gt; from deeppavlov import configs, build_model
&gt;&gt;&gt; ner_model = build_model(configs.ner.ner_ontonotes_bert_mult, download=True)
&gt;&gt;&gt; print(ner_model([""Round 23/22; 24,9 x 12,2 x 12,3""]))
&lt;class 'list'&gt;: [[['Round', '23', '/', '22', ';', '24', ',', '9', 'x', '12', ',', '2', 'x', '12', ',', '3']], [['O', 'B-CARDINAL', 'O', 'B-CARDINAL', 'O', 'B-CARDINAL', 'O', 'B-CARDINAL', 'O', 'B-CARDINAL', 'O', 'B-CARDINAL', 'O', 'B-CARDINAL', 'O', 'B-CARDINAL']]]
</code></pre>

<p>I have no idea how to extract those measurements from this list properly.</p>

<p>I also found <a href=""https://stackoverflow.com/a/39452278/10183880"">this</a> regex: </p>

<pre><code>&gt;&gt;&gt;re.findall(""(\d+(?:,\d+)?) x (\d+(?:,\d+)?)(?: x (\d+(?:,\d+)?))?"", ""Straight 10 x 2 mm"")
&lt;class 'list'&gt;: [('10', '2', '')]

</code></pre>

<p>But it does leave an empty value in the resulting list if the input contains 2 dimensions and it doesn't work if there is no whitespace between numbers and ""x""s. I'm not good with regex...</p>
","regex, python-3.x, parsing, units-of-measurement, named-entity-recognition","<p>For the given examples, you might use:</p>

<pre><code>(?&lt;!\S)\d+(?:,\d+)? ?x ?\d+(?:,\d+)?(?: ?x ?\d+(?:,\d+)?)*
</code></pre>

<p>In parts</p>

<ul>
<li><code>(?&lt;!\S)</code> Negative lookbehind, assert what is on the left is not a non whitespace char</li>
<li><code>\d+(?:,\d+)?</code> Match 1+ digits and optionally a <code>,</code> and 1+ digits</li>
<li><code> ?x ?</code> Match <code>x</code> between optional spaces</li>
<li><code>\d+(?:,\d+)?</code> Match 1+ digits and optionally a <code>,</code> and 1+ digits</li>
<li><code>(?:</code> Non capturing group

<ul>
<li><code> ?x ?\d+</code><code>Match</code>x` between optional spaces and 1+ digits</li>
<li><code>(?:,\d+)?</code> Optionally match a <code>,</code> and 1+ digits</li>
</ul></li>
<li><code>)*</code> Close non capturing group and repeat 0+ times</li>
</ul>

<p><a href=""https://regex101.com/r/HVzm1Q/1"" rel=""noreferrer"">Regex demo</a> | <a href=""https://ideone.com/SH17eM"" rel=""noreferrer"">Python demo</a></p>

<p>For example</p>

<pre><code>import re

regex = r""(?&lt;!\S)\d+(?:,\d+)? ?x ?\d+(?:,\d+)?(?: ?x ?\d+(?:,\d+)?)*""
test_str = (""Square 10 x 3 x 5 mm\n""
    ""Round 23/22; 24,9 x 12,2 x 12,3\n""
    ""Square 10x2\n""
    ""Straight 10x2mm\n""
    ""Round 23/22; 24,9x12,2"")
result = re.findall(regex, test_str)
print(result)
</code></pre>

<p>Output</p>

<pre><code>['10 x 3 x 5', '24,9 x 12,2 x 12,3', '10x2', '10x2', '24,9x12,2']
</code></pre>
",6,3,1017,2019-09-25 12:04:33,https://stackoverflow.com/questions/58097949/parse-measurements-multiple-dimensions-from-a-given-string-in-python-3
How to extract out dates in a dd/mm/yyyy format from an unstructured string?,"<p>I have few strings like below :</p>

<pre><code>'Thursday;60 days;Monday, days;the last two years;the six months;October 2017;March 2018;three days;Jan. 4;Last year;Dec. 21;'
</code></pre>

<p>expected result : <code>October 2017</code></p>

<pre><code>'January 7;30;39;24;46;1750;April 2017;April 30;February;'
</code></pre>

<p>expected result : <code>April 2017</code></p>

<pre><code>'Thursday;a day;another six days;the day;Tuesday;three days;mid-October;Wednesday;'
</code></pre>

<p>expected result : <code>mid-October</code></p>

<p>I know the string is completely unstructured but can we have a python code to get the dates even from these ?</p>

<p>This is a part of a NER model where I am trying to extract the data entities.</p>

<p>I have tried a few methods but those were not even close to the result as string doesn't have a proper pattern</p>
","python, regex, named-entity-recognition","<p>You may use <a href=""https://github.com/akoumjian/datefinder"" rel=""nofollow noreferrer""><code>datefinder</code></a> with a regex to check for month names in the found date time strings:</p>

<pre><code>import datefinder, re
from datetime import datetime

strs = ['Thursday;60 days;Monday, days;the last two years;the six months;October 2017;March 2018;three days;Jan. 4;Last year;Dec. 21;',
        'January 7;30;39;24;46;1750;April 2017;April 30;February;',
        'Thursday;a day;another six days;the day;Tuesday;three days;mid-October;Wednesday;']

day_of_week_rx = re.compile(r'(?:A(?:pr(?:il)?|ug(?:ust)?)|Dec(?:ember)?|Feb(?:ruary)?|J(?:an(?:uary)?|u(?:ly|ne|[ln]))|Ma(?:rch|[ry])|Nov(?:ember)?|Oct(?:ober)?|Sep(?:tember)?)', re.I)
for s in strs:
    raw_dates = list(datefinder.find_dates(s, source=True))
    print([y for x,y in raw_dates if day_of_week_rx.search(y)])
</code></pre>

<p>Output:</p>

<pre><code>['October 2017', 'March 2018', 'Jan. 4', 'Dec. 21']
['January 7', 'April 2017', 'April 30']
[]
</code></pre>

<p>Note that <code>mid-October</code> cannot be cast to a valid date time thus it is not extracted. You will need to apply some more specific regex like <code>re.search(r'\b(?:half|mid)-(?:A(?:pr(?:il)?|ug(?:ust)?)|Dec(?:ember)?|Feb(?:ruary)?|J(?:an(?:uary)?|u(?:ly|ne|[ln]))|Ma(?:rch|[ry])|Nov(?:ember)?|Oct(?:ober)?|Sep(?:tember)?)', text)</code>.</p>

<p>The <code>(?:A(?:pr(?:il)?|ug(?:ust)?)|Dec(?:ember)?|Feb(?:ruary)?|J(?:an(?:uary)?|u(?:ly|ne|[ln]))|Ma(?:rch|[ry])|Nov(?:ember)?|Oct(?:ober)?|Sep(?:tember)?)</code> matches English month full and abbreviated names.</p>
",2,-2,770,2019-09-26 05:16:13,https://stackoverflow.com/questions/58109978/how-to-extract-out-dates-in-a-dd-mm-yyyy-format-from-an-unstructured-string
NLP Named Entity Recognition using NLTK and Spacy,"<p>I used the NER for the following sentence on both NLTK and Spacy and below are the results:</p>

<pre><code>""Zoni I want to find a pencil, a eraser and a sharpener""
</code></pre>

<p>I ran the following code on Google Colab.</p>

<pre><code>import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

ex = ""Zoni I want to find a pencil, a eraser and a sharpener""

def preprocess(sent):
    sent = nltk.word_tokenize(sent)
    sent = nltk.pos_tag(sent)
    return sent

sent = preprocess(ex)
sent

#Output:
[('Zoni', 'NNP'),
 ('I', 'PRP'),
 ('want', 'VBP'),
 ('to', 'TO'),
 ('find', 'VB'),
 ('a', 'DT'),
 ('pencil', 'NN'),
 (',', ','),
 ('a', 'DT'),
 ('eraser', 'NN'),
 ('and', 'CC'),
 ('a', 'DT'),
 ('sharpener', 'NN')]
</code></pre>

<p>But when i used spacy on the same text, it didn't return me any result</p>

<pre><code>import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()

text = ""Zoni I want to find a pencil, a eraser and a sharpener""

doc = nlp(text)
doc.ents

#Output:
()
</code></pre>

<p>Its only working for some sentences.</p>

<pre><code>import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()

# text = ""Zoni I want to find a pencil, a eraser and a sharpener""

text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'

doc = nlp(text)
doc.ents

#Output:
(European, Google, $5.1 billion, Wednesday)
</code></pre>

<p>Please let me know if there is something wrong.</p>
","python-3.x, nlp, nltk, spacy, named-entity-recognition","<p><em>Spacy</em> models are statistical. So the named entities that these models recognize are dependent on the data sets that these models were trained on.</p>

<p>According to <em>spacy</em> documentation a named entity is a “<em>real-world object</em>” that’s assigned a name – for example, a person, a country, a product or a book title.</p>

<p>For example, the name <em>Zoni</em> is not common, so the model doesn't recognize the name as being a named entity (person). If I change the name <em>Zoni</em> to <em>William</em> in your sentence <em>spacy</em> recognize <em>William</em> as a person.</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

doc = nlp('William I want to find a pencil, a eraser and a sharpener')

for entity in doc.ents:
  print(entity.label_, ' | ', entity.text)
  #output
  PERSON  |  William
</code></pre>

<p>One would assume that <em>pencil</em>, <em>eraser</em> and <em>sharpener</em> are objects, so they would potentially be classified as products, because <em>spacy</em>  <a href=""https://spacy.io/api/annotation#named-entities"" rel=""nofollow noreferrer"">documentation</a> states 'objects' are products.  But that does not seem to be the case with the 3 objects in your sentence.  </p>

<p>I also noted that if no named entities are found in the input text then the output will be empty. </p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_lg"")

doc = nlp('Zoni I want to find a pencil, a eraser and a sharpener')
if not doc.ents:
  print ('No named entities were recognized in the input text.')
else:
  for entity in doc.ents:
    print(entity.label_, ' | ', entity.text)
</code></pre>
",3,2,1485,2019-10-02 23:33:32,https://stackoverflow.com/questions/58210582/nlp-named-entity-recognition-using-nltk-and-spacy
SpaCy NER differentiating numbers or entities,"<p>I am currently playing with SpaCy NER and wondering if SpaCy NER can do these 2 things:</p>

<p><strong>Case 1</strong></p>

<p>Let's say we have 2 sentences that we want to do NER with:</p>

<ol>
<li>Sugar level in his body is increasing.</li>
<li>His overall health quality is increasing.</li>
</ol>

<p>Can we tag ""increasing"" in the first sentence as ""symptoms"" entity, and tag ""increasing"" in the second one as ""good outcome"" entity? Will NER see the difference in those 2 ""increasing"" words?</p>

<p><strong>Case 2</strong></p>

<p>We also have 2 different sentences:</p>

<ol>
<li>My salary is USD 8000 per month</li>
<li>My spending is USD 5000 per month</li>
</ol>

<p>Can NER see the number in the first sentence as ""income"" entity and the number in the second sentence as ""spending""?</p>

<p>Thank you</p>
","machine-learning, nlp, spacy, named-entity-recognition","<p>These tasks go beyond what you would expect an NER model to be able to do in a number of ways. Spacy's NER algorithm could be used to find types of entities like <code>MONEY</code> (which is an entity type in its English models) or maybe something like <code>SYMPTOM</code>, but it doesn't look at a very large context to detect/classify entities, so it's not going to be able to differentiate these cases where the relevant context is fairly far away.</p>

<p>You probably want to combine NER (or another type of relevant span detection, which could also be rule-based) with another type of analysis that focuses more on the context. This could be some kind of text classification, you could examine the dependency parse, etc.</p>

<p>Here is a simple example from the spacy docs about extracting entity relations using NER (to find <code>MONEY</code>) followed by examining the dependency parse to try to figure out what the money element could be referring to:</p>

<p><a href=""https://spacy.io/usage/examples#entity-relations"" rel=""nofollow noreferrer"">https://spacy.io/usage/examples#entity-relations</a></p>
",2,3,3062,2019-10-09 02:14:12,https://stackoverflow.com/questions/58296163/spacy-ner-differentiating-numbers-or-entities
How can I keep multi-word names in tokenization together?,"<p>I want to classify documents using TF-IDF features. One way to do it:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import string
import re
import nltk


def tokenize(document):
    document = document.lower()
    for punct_char in string.punctuation:
        document = document.replace(punct_char, "" "")
    document = re.sub('\s+', ' ', document).strip()

    tokens = document.split("" "")

    # Contains more than I want:
    # from spacy.lang.de.stop_words import STOP_WORDS
    stopwords = nltk.corpus.stopwords.words('german')
    tokens = [token for token in tokens if token not in stopwords]
    return tokens

# How I intend to use it
transformer = TfidfVectorizer(tokenizer=tokenize)

example = ""Jochen Schweizer ist eines der interessantesten Unternehmen der Welt, hat den Sitz allerdings nicht in der Schweizerischen Eidgenossenschaft.""
transformer.fit([example])

# Example of the tokenizer
print(tokenize(example))
</code></pre>

<p>One flaw of this tokenizer is that it splits words that belong together: ""Jochen Schweizer"" and ""schweizerische Eidgenossenschaft"". Also lemmatization (word stemming) is missing. I would like to get the following tokens:</p>

<pre><code>[""Jochen Schweizer"", ""interessantesten"", ""unternehmen"", ""Welt"", ""Sitz"", ""allerdings"", ""nicht"", ""Schweizerische Eidgenossenschaft""]
</code></pre>

<p>I know that Spacy can identify those named entities (NER):</p>

<pre><code>import en_core_web_sm  # python -m spacy download en_core_web_sm --user
parser = en_core_web_sm.load()
doc = parser(example)
print(doc.ents)  # (Jochen Schweizer, Welt, Sitz)
</code></pre>

<p>Is there a good way to use spacy to tokenize in a way that keeps the named entity words together?</p>
","scikit-learn, nlp, nltk, spacy, named-entity-recognition","<p>How about this:</p>

<pre class=""lang-py prettyprint-override""><code>with doc.retokenize() as retokenizer:
    for ent in doc.ents:
        retokenizer.merge(doc[ent.start:ent.end])
</code></pre>

<p>In fact, you can use spacy to remove punctuations &amp; stop words, and perform lemmatization too!</p>

<pre class=""lang-py prettyprint-override""><code>parser = spacy.load('de_core_news_sm')
def tokenize(text):
    doc = parser(text)
    with doc.retokenize() as retokenizer:
        for ent in doc.ents:
            retokenizer.merge(doc[ent.start:ent.end], attrs={""LEMMA"": ent.text})
    return [x.lemma_ for x in doc if not x.is_punct and not x.is_stop]
</code></pre>

<p>Example:</p>

<pre><code>&gt;&gt;&gt; text = ""Jochen Schweizer ist eines der interessantesten Unternehmen der Welt, hat den Sitz allerdings nicht in der Schweizerischen Eidgenossenschaft.""
&gt;&gt;&gt; print(tokenize(text))
&gt;&gt;&gt; [u'Jochen Schweizer', u'interessant', u'Unternehmen', u'Welt', u'Sitz', u'Schweizerischen Eidgenossenschaft']
</code></pre>
",6,5,2820,2019-10-09 08:02:56,https://stackoverflow.com/questions/58299587/how-can-i-keep-multi-word-names-in-tokenization-together
spaCy: How to write named entities to an existing Doc object using some loaded model for this?,"<p>I created a <code>Doc</code> object from a custom list of tokens according to documentation like so:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.tokens import Doc

nlp = spacy.load(""my_ner_model"")
doc = Doc(nlp.vocab, words=[""Hello"", "","", ""world"", ""!""])
</code></pre>

<p>How do I write named entities tags to <code>doc</code> with my NER model now?</p>

<p>I tried to do <code>doc = nlp(doc)</code>, but that didn't work for me raising a <code>TypeError</code>.</p>

<p>I can't just join my list of words into a plain text to do <code>doc = nlp(text)</code> as usual because in this case <code>spaCy</code> splits some words in my texts into two tokens which I can not accept.</p>
","python, nlp, token, spacy, named-entity-recognition","<p>You can get the NER component from your loaded model and call it directly on the constructed <code>Doc</code>:</p>

<pre><code>doc = nlp.get_pipe(""ner"")(doc)
</code></pre>

<p>You can inspect a list of all the available components in the pipeline with <code>nlp.pipe_names</code> and call them individually this way. The tokenizer is always the first element of the pipeline when you call <code>nlp()</code> and it isn't included in this list, which only has the components that both take and return a <code>Doc</code>.</p>
",2,2,268,2019-10-13 13:10:35,https://stackoverflow.com/questions/58363886/spacy-how-to-write-named-entities-to-an-existing-doc-object-using-some-loaded-m
Distant Supervision: a rule-based labelling approach?,"<p>I am currently working on entity relations stuff and I found out that a lot of papers implemented distant supervision to label the data. What I understand about distant supervision is that we have an established Knowledge Base (KB) and we do kind of ""rule-based labeling"" by checking the extracted entity pairs whether they exist in the KB or not. If the entity pair exist in KB, it will be labelled as positive, otherwise it will be labelled as negative. </p>

<p>My questions are:</p>

<ol>
<li>Do I understand this distant supervision concept correctly?</li>
<li>If yes, I don't understand why do we train neural networks to classify rule-based system? For example, if in the future we get new sentences that contain entities and we want to check if they have relation to each other, why don't we just refer back to the KB? Why do we train entity relation instead?</li>
</ol>

<p>Thank you</p>
","machine-learning, nlp, data-science, named-entity-recognition","<p>Distant supervision is the approach of using rule based heuristics in order to produce labeled data, the labeled data produced being then used to train a model (generally a neural network).</p>

<p>The Knowledge Base (KB) can be used can be used as a rule based heuristic. As stated by Nathan McCoy, the KB will generally not be complete and the model will enable you to detect a relation between to entities which are not present in the knowledge base.</p>

<p><a href=""https://www.snorkel.org/"" rel=""nofollow noreferrer"">Snorkel</a> is an example of a tool which was developped for distant supervision</p>
",2,1,502,2019-10-18 04:03:12,https://stackoverflow.com/questions/58443783/distant-supervision-a-rule-based-labelling-approach
spaCy CLI debug shows 0 train/dev docs in CLI-formatted JSON converted by spacy.gold.docs_to_json,"<h1>Issue</h1>

<p>I am trying to run the spaCy CLI but my training data and dev data seem somehow to be incorrect as seen when I run debug:</p>

<pre><code>| =&gt; python3 -m spacy debug-data en 

./CLI_train_randsplit_anno191022.json ./CLI_dev_randsplit_anno191022.json --pipeline ner --verbose 




=========================== Data format validation =========================== 

✔ Corpus is loadable 




=============================== Training stats =============================== 

Training pipeline: ner 

Starting with blank model 'en' 

0 training docs 

0 evaluation docs 

✔ No overlap between training and evaluation data 

✘ Low number of examples to train from a blank model (0) 

It's recommended to use at least 2000 examples (minimum 100) 




============================== Vocab &amp; Vectors ============================== 

ℹ 0 total words in the data (0 unique) 

10 most common words: 

ℹ No word vectors present in the model 




========================== Named Entity Recognition ========================== 

ℹ 0 new labels, 0 existing labels 

0 missing values (tokens with '-' label) 

✔ Good amount of examples for all labels 

✔ Examples without occurrences available for all labels 

✔ No entities consisting of or starting/ending with whitespace 




================================== Summary ================================== 

✔ 5 checks passed 

✘ 1 error 
</code></pre>

<p>Trying to train anyway yields:</p>

<pre><code>| =&gt; python3 -m spacy train en ./models/CLI_1 ./CLI_train_randsplit_anno191022.json ./CLI_dev_randsplit_anno191022.json -n 150 -p 'ner' --verbose 

dropout_from = 0.2 by default 

dropout_to = 0.2 by default 

dropout_decay = 0.0 by default 

batch_from = 100.0 by default 

batch_to = 1000.0 by default 

batch_compound = 1.001 by default 

Training pipeline: ['ner'] 

Starting with blank model 'en' 

beam_width = 1 by default 

beam_density = 0.0 by default 

beam_update_prob = 1.0 by default 

Counting training words (limit=0) 

learn_rate = 0.001 by default 

optimizer_B1 = 0.9 by default 

optimizer_B2 = 0.999 by default 

optimizer_eps = 1e-08 by default 

L2_penalty = 1e-06 by default 

grad_norm_clip = 1.0 by default 

parser_hidden_depth = 1 by default 

subword_features = True by default 

conv_depth = 4 by default 

bilstm_depth = 0 by default 

parser_maxout_pieces = 2 by default 

token_vector_width = 96 by default 

hidden_width = 64 by default 

embed_size = 2000 by default 




Itn  NER Loss   NER P   NER R   NER F   Token %  CPU WPS 

---  ---------  ------  ------  ------  -------  ------- 

✔ Saved model to output directory 

models/CLI_1/model-final 




Traceback (most recent call last): 

  File ""/usr/local/lib/python3.7/site-packages/spacy/cli/train.py"", line 389, in train 

    scorer = nlp_loaded.evaluate(dev_docs, verbose=verbose) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/language.py"", line 673, in evaluate 

    docs, golds = zip(*docs_golds) 

ValueError: not enough values to unpack (expected 2, got 0) 




During handling of the above exception, another exception occurred: 




Traceback (most recent call last): 

  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 193, in _run_module_as_main 

    ""__main__"", mod_spec) 

  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 85, in _run_code 

    exec(code, run_globals) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/__main__.py"", line 35, in &lt;module&gt; 

    plac.call(commands[command], sys.argv[1:]) 

  File ""/usr/local/lib/python3.7/site-packages/plac_core.py"", line 328, in call 

    cmd, result = parser.consume(arglist) 

  File ""/usr/local/lib/python3.7/site-packages/plac_core.py"", line 207, in consume 

    return cmd, self.func(*(args + varargs + extraopts), **kwargs) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/cli/train.py"", line 486, in train 

    best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/cli/train.py"", line 548, in _collate_best_model 

    bests[component] = _find_best(output_path, component) 

  File ""/usr/local/lib/python3.7/site-packages/spacy/cli/train.py"", line 567, in _find_best 

    accs = srsly.read_json(epoch_model / ""accuracy.json"") 

  File ""/usr/local/lib/python3.7/site-packages/srsly/_json_api.py"", line 50, in read_json 

    file_path = force_path(location) 

  File ""/usr/local/lib/python3.7/site-packages/srsly/util.py"", line 21, in force_path 

    raise ValueError(""Can't read file: {}"".format(location)) 

ValueError: Can't read file: models/CLI_1/model0/accuracy.json 
</code></pre>

<p>My training and dev docs were generated using spacy.gold.docs_to_json(), saved as json files using the function: </p>

<pre><code>def make_CLI_json(mock_docs, CLI_out_file_path): 
    CLI_json = docs_to_json(mock_docs) 
    with open(CLI_out_file_path, 'w') as json_file: 
        json.dump(CLI_json, json_file) 
</code></pre>

<p>I verified them both to be valid json at <a href=""http://www.jsonlint.com"" rel=""nofollow noreferrer"">http://www.jsonlint.com</a>.</p>

<p>I created the docs from which these json originated using the function:  </p>

<pre><code>def import_from_doccano(jx_in_file_path, view=True): 
    annotations = load_jsonl(jx_in_file_path)     
    mock_nlp = English()     
    sentencizer = mock_nlp.create_pipe(""sentencizer"")     
    unlabeled = 0     
    DATA = []     
    mock_docs = [] 

    for anno in annotations:                      
        # get DATA (as used in spacy inline training)     
        if ""label"" in anno.keys():     
            ents = [tuple([label[0], label[1], label[2]])     
                               for label in anno[""labels""]] 
        else: 
            ents = [] 

        DATUM = (anno[""text""], {""entities"": ents}) 
        DATA.append(DATUM) 

        # mock a doc for viz in displacy 
        mock_doc = mock_nlp(anno[""text""]) 
        if ""labels"" in anno.keys():     
            entities = anno[""labels""]     
            if not entities:     
                unlabeled += 1     
            ents = [(e[0], e[1], e[2]) for e in entities]     
            spans = [mock_doc.char_span(s, e, label=L) for s, e, L in ents]     
            mock_doc.ents = _cleanup_spans(spans)     
            sentencizer(mock_doc) 

            if view:     
                displacy.render(mock_doc, style='ent') 

        mock_docs.append(mock_doc)                      
    print(f'Unlabeled: {unlabeled}')                      
    return DATA, mock_docs 
</code></pre>

<p>I wrote the function above to return the examples in both the format required for inline training (e.g. as shown at <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py</a>) as well as to form these kind of “mock” docs so that I can use displacy and/or the CLI. For the latter purpose, I followed the code shown at <a href=""https://github.com/explosion/spaCy/blob/master/spacy/cli/converters/jsonl2json.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/spacy/cli/converters/jsonl2json.py</a> with a couple of notable differences. The _cleanup_spans() function is identical to the one in the example. I did not use the minibatch() but made a separate doc for each of my labeled annotations. Also, (perhaps critically?) I found that using the sentencizer ruined many of my annotations, possibly because the spans get shifted in a way that the _cleanup_spans() function fails to repair properly. Removing the sentencizer causes the docs_to_json() function to throw an error. In my function (unlike in the linked example) I therefore run the sentencizer on each doc <em>after</em> the entities are written to them, which preserves my annotations properly and allows the docs_to_json() function to run without complaints.  </p>

<p>The function load_jsonl called within import_from_doccano() is defined as:</p>

<pre><code>def load_jsonl(input_path):     
    data = []     
    with open(input_path, 'r', encoding='utf-8') as f:     
        for line in f:     
            data.append(json.loads(line.replace('\n|\r',''), strict=False))     
    print('Loaded {} records from {}'.format(len(data), input_path))     
    print()     
    return data 
</code></pre>

<p>My annotations are each of length ~10000 characters or less. They are exported from doccano </p>

<p>(<a href=""https://doccano.herokuapp.com/"" rel=""nofollow noreferrer"">https://doccano.herokuapp.com/</a>) as JSONL using the format:</p>

<pre><code>{""id"": 1, ""text"": ""EU rejects ..."", ""labels"": [[0,2,""ORG""], [11,17, ""MISC""], [34,41,""ORG""]]}
{""id"": 2, ""text"": ""Peter Blackburn"", ""labels"": [[0, 15, ""PERSON""]]}
{""id"": 3, ""text"": ""President Obama"", ""labels"": [[10, 15, ""PERSON""]]}
...
</code></pre>

<p>The data are split into train and test sets using the function:</p>

<pre><code>def test_train_split(DATA, mock_docs, n_train):
    L = list(zip(DATA, mock_docs))
    random.shuffle(L)
    DATA, mock_docs = zip(*L)
    DATA = [i for i in DATA]
    mock_docs = [i for i in mock_docs]
    TRAIN_DATA = DATA[:n_train]
    train_docs = mock_docs[:n_train]
    TEST_DATA = DATA[n_train:] 
    test_docs = mock_docs[n_train:]
    return TRAIN_DATA, TEST_DATA, train_docs, test_docs
</code></pre>

<p>And finally each is written to json using the following function:</p>

<pre><code>def make_CLI_json(mock_docs, CLI_out_file_path):
    CLI_json = docs_to_json(mock_docs)
    with open(CLI_out_file_path, 'w') as json_file:
        json.dump(CLI_json, json_file)
</code></pre>

<p>I do not understand why the debug shows 0 training docs and 0 development docs, or why the train command fails. The JSON look correct as far as I can tell. <strong>Is my data formatted incorrectly, or is there something else going on?</strong> Any help or insights would be greatly appreciated.</p>

<p><em>This is my first question on SE- apologies in advance if I've failed to follow some or other guideline. There are a lot of components involved so I'm not sure how I might produce a minimal code example that would replicate my problem.</em></p>

<h1>Environment</h1>

<p>Mac OS 10.15 Catalina
Everything is pip3 installed into user path 
No virtual environment</p>

<pre><code>| =&gt; python3 -m spacy info --markdown

## Info about spaCy

* **spaCy version:** 2.2.1
* **Platform:** Darwin-19.0.0-x86_64-i386-64bit
* **Python version:** 3.7.4
</code></pre>
","python, nlp, spacy, named-entity-recognition, doccano","<p>This is a legitimately confusing aspect of the API. For internal/historical reasons, <code>spacy.gold.docs_to_json()</code> produces a dict that still needs to be wrapped in list to get to the final training format. Try:</p>

<pre><code>srsly.write_json(filename, [spacy.gold.docs_to_json(docs)])
</code></pre>

<p><code>spacy debug-data</code> doesn't have proper schema checks yet, so this is more frustrating/confusing than it should be.</p>
",3,0,977,2019-10-22 23:17:37,https://stackoverflow.com/questions/58513452/spacy-cli-debug-shows-0-train-dev-docs-in-cli-formatted-json-converted-by-spacy
How to use BERT just for ENTITY extraction from a Sequence without classification in the NER task?,"<p>My requirement here is given a sentence(sequence), I would like to just extract the entities present in the sequence without classifying them to a type in the NER task. I see that BertForTokenClassification for NER does the classification. Can this be adapted for just the extraction?</p>

<p>Can BERT just be used to do <strong>entity extraction/identification</strong>?</p>
","nlp, pytorch, named-entity-recognition, named-entity-extraction, bert-language-model","<p>Regardless BERT, NER tagging is usually done by tagging with the IOB format (inside, outside, beginning) or something similar (often the end is also explicitly tagged). The <em>inside</em> and <em>beggining</em> tags contain the entity type. Something like this:</p>

<pre><code>Alex B-PER
is O
going O
to O
Los B-LOC
Angeles I-LOC
</code></pre>

<p>If you modify your training data, such that there will be only one entity type, the model will only learn to detect the entities without knowing what type the entity is.</p>

<pre><code>Alex B
is O
going O
to O
Los B
Angeles I
</code></pre>
",0,0,613,2019-10-24 12:50:47,https://stackoverflow.com/questions/58541811/how-to-use-bert-just-for-entity-extraction-from-a-sequence-without-classificatio
limit EntityRecognitionSkill to confident &gt; .5,"<p>I'm using Microsoft.Skills.Text.EntityRecognitionSkill in my skillset which output ""Person"", ""Location"", ""Organization"".
however I want to only output Location that have a confident level > .5
is there a way to do that?
here is a snap of my code</p>

<pre><code>{
      ""@odata.type"": ""#Microsoft.Skills.Text.EntityRecognitionSkill"",
      ""categories"": [
        ""Person"",
        ""Location"",
        ""Organization""
      ],
      ""context"": ""/document/finalText/pages/*"",
      ""inputs"": [
        {
          ""name"": ""text"",
          ""source"": ""/document/finalText/pages/*""
        },
        {
          ""name"": ""languageCode"",
          ""source"": ""/document/languageCode""
        }
      ],
      ""outputs"": [
        {
          ""name"": ""persons"",
          ""targetName"": ""people""
        },
        {
          ""name"": ""locations""
        },
        {
          ""name"": ""namedEntities"",
          ""targetName"": ""entities""
        }
      ]
    },
</code></pre>
","azure, azure-web-app-service, azure-cognitive-search, named-entity-recognition, azure-cognitive-services","<p>[Edited based on Mick's comment]</p>

<p>Yes, this should be possible by setting the <code>minimumPrecision</code> parameter of the entity recognition skill to 0.5, which will result in entities whose confidence is >= 0.5 to be returned.</p>

<p>The documentation for entity recognition skill is here: <a href=""https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-entity-recognition"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-entity-recognition</a></p>

<p>As Mick points out, the documentation says <code>minimumPrecision</code> is unused, however that documentation is out of date and I will fix it soon.</p>
",3,1,81,2019-10-29 18:32:38,https://stackoverflow.com/questions/58613465/limit-entityrecognitionskill-to-confident-5
Replace entity with its label in SpaCy,"<p>Is there anyway by SpaCy to replace entity detected by SpaCy NER with its label?
For example:
<strong>I am eating an apple while playing with my Apple Macbook.</strong></p>

<p>I have trained NER model with SpaCy to detect ""FRUITS"" entity and the model successfully detects the first ""apple"" as ""FRUITS"", but not the second ""Apple"".</p>

<p>I want to do post-processing of my data by replacing each entity with its label, so I want to replace the first ""apple"" with ""FRUITS"". The sentence will be ""<strong>I am eating an FRUITS while playing with my Apple Macbook.</strong>""</p>

<p>If I simply use regex, it will replace the second ""Apple"" with ""FRUITS"" as well, which is incorrect. Is there any smart way to do this?</p>

<p>Thanks!</p>
","nlp, spacy, named-entity-recognition","<p>the entity label is an attribute of the token (see <a href=""https://spacy.io/api/token#attributes"" rel=""noreferrer"">here</a>)</p>
<pre><code>import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_lg')

s = &quot;His friend Nicolas is here.&quot;
doc = nlp(s)

print([t.text if not t.ent_type_ else t.ent_type_ for t in doc])
# ['His', 'friend', 'PERSON', 'is', 'here', '.']

print(&quot; &quot;.join([t.text if not t.ent_type_ else t.ent_type_ for t in doc]) )
# His friend PERSON is here .
</code></pre>
<p><strong>Edit:</strong></p>
<p>In order to handle cases were entities can span several words the following code can be used instead:</p>
<pre><code>s = &quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;
doc = nlp(s)
newString = s
for e in reversed(doc.ents): #reversed to not modify the offsets of other entities when substituting
    start = e.start_char
    end = start + len(e.text)
    newString = newString[:start] + e.label_ + newString[end:]
print(newString)
#His friend PERSON is here with PERSON and PERSON.
</code></pre>
<p><strong>Update:</strong></p>
<p>Jinhua Wang brought to my attention that there is now a more built-in and simpler way to do this using the merge_entities pipe.
See Jinhua's answer below.</p>
",22,15,7146,2019-11-05 13:31:11,https://stackoverflow.com/questions/58712418/replace-entity-with-its-label-in-spacy
Getting a Spacy error: No module named &#39;spacy.pipeline.pipes&#39;; &#39;spacy.pipeline&#39; is not a package,"<p>I'm trying to test a model that is working in another machine, but when I try to import it to my notebook, I get this error: 
ModuleNotFoundError: No module named 'spacy.pipeline.pipes'; 'spacy.pipeline' is not a package</p>

<p>We have installed:
Spacy 2.0.18 (Frozen version, Not updatable whatsoever) </p>

<p>And I'm importing:</p>

<pre><code>import spacy
import thinc
import unidecode
import nltk
from spacy.vocab    import Vocab
from spacy.language import Language
from spacy.lang.pt  import Portuguese
from spacy.lang.en  import English
from spacy.pipeline import EntityRecognizer
ner = EntityRecognizer(nlp.vocab)
nlp = Language(Vocab())
nlp = Portuguese()
# Load NER Model
NER_MODEL = pickle.load( open(""/ner_model_v022_epoch=706_loss=09o76364626.pkl"", ""rb"" ) )
</code></pre>

<p>And I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-12-83d4770d3e3e&gt; in &lt;module&gt;

---&gt; 40 NER_MODEL = pickle.load( open(""/ner_model_v022_epoch=706_loss=09o76364626.pkl"", ""rb"" ) )

ModuleNotFoundError: No module named 'spacy.pipeline.pipes'; 'spacy.pipeline' is not a package
</code></pre>

<p>Any ideas why this might be happening? Already installed everything again from 0 but keeps giving me the same error.</p>

<p>Any help will be greatly appreciated.</p>
","nlp, spacy, named-entity-recognition","<p>I had this problem come up and found that switching my spacy version from <code>spacy==2.0.18</code> to <code>spacy==2.1.4</code> worked! Went back through their releases and spacy.pipeline.pipes isn't present until <a href=""https://github.com/explosion/spaCy/tree/v2.1.0a8"" rel=""nofollow noreferrer"">v2.1.0a8</a></p>
",1,3,5708,2019-11-06 17:37:37,https://stackoverflow.com/questions/58735715/getting-a-spacy-error-no-module-named-spacy-pipeline-pipes-spacy-pipeline
How to train Stanford NLP NER Extraction model to skip the repeating words?,"<p>I am trying to extract the NER from the text using <strong>.NET Framework</strong> and <strong>StanFord NER Model</strong>.
I have a text like </p>

<p>Hello, I am John Doe. Body Mass index is 27. And Body Surface Area is 2.3m.</p>

<p>For this i did create tsv file to train the model. Which is as under:</p>

<pre><code>Hello   O
,   O
I   O
am  O
John    PERSON
Doe.    PERSON
Body    BMI
Mass    BMI
index   BMI
is  O
27. O
And O
Body    O
Surface O
Area    O
is  O
2.3m.   O
</code></pre>

<p>prop file is as under</p>

<pre><code>trainFileList = train/standford_train.tsv
serializeTo = dummy-ner-model-eng.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
</code></pre>

<p>and using below java command</p>

<pre><code>java -mx1g -cp stanford-ner.jar;lib/* edu.stanford.nlp.ie.crf.CRFClassifier -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -prop train/prop.txt
</code></pre>

<p>So, the problem i am facing is Body with tagging BMI is coming two times because of repetition in <strong>Body Mass Index</strong> and <strong>Body Surface Area</strong>.</p>

<p>Is there any way that i can omit this second body tag?</p>
","nlp, stanford-nlp, named-entity-recognition","<p>You'll need to produce more training data that has examples with <code>Body</code> not labeled as <code>BMI</code>.  If you are only looking for specific patterns, you might get better results with a rule-based approach.  There are tools for rule based NER building in Stanford CoreNLP.  </p>

<p>More info: <a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokensregex.html</a></p>
",0,0,272,2019-11-08 09:22:41,https://stackoverflow.com/questions/58763703/how-to-train-stanford-nlp-ner-extraction-model-to-skip-the-repeating-words
Train NER SpaCy using en_trf_bertbaseuncased_lg model,"<p>I am currently working on NER project and I would like to improve my NER performance by trying new SpaCy model <code>en_trf_bertbaseuncased_lg</code> but it gave me error <code>KeyError: ""[E001] No component 'trf_tok2vec' found in pipeline. Available names: ['ner']""</code>. Is it that SpaCy currently does not support NER for this language model? Thanks!</p>

<pre><code>   # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        for itn in tqdm(range(n_iter)):
            random.shuffle(train_data_list)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(train_data_list, size=compounding(8., 64., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                           losses=losses)
            tqdm.write('Iter: ' + str(itn + 1) + ' Losses: ' + str(losses['ner']))
            if itn == 30 or itn == 40:
                output_dir = Path(output_dir)
                if not output_dir.exists():
                    output_dir.mkdir()
                nlp.to_disk(Path(output_dir))
</code></pre>

<p>It gave error on </p>

<pre><code>nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                           losses=losses)
</code></pre>
","nlp, spacy, named-entity-recognition","<p>According to the documentation of this model on spaCy <a href=""https://spacy.io/models/en#en_trf_bertbaseuncased_lg"" rel=""nofollow noreferrer"">here</a>, this model doesn't support Named-Entity Recognition yet. It only supports:</p>

<ul>
<li><code>sentencizer</code></li>
<li><code>trf_wordpiecer</code></li>
<li><code>trf_tok2vec</code></li>
</ul>

<p>You can get the available pipe for a given model like so:</p>

<pre><code>&gt;&gt;&gt; import spacy

&gt;&gt;&gt; nlp = spacy.load(""en_trf_bertbaseuncased_lg"")
&gt;&gt;&gt; nlp.pipe_names
[sentencizer, trf_wordpiecer, trf_tok2vec]
</code></pre>
",2,0,1254,2019-11-11 01:49:08,https://stackoverflow.com/questions/58794613/train-ner-spacy-using-en-trf-bertbaseuncased-lg-model
spaCy - Most efficient way to sort entities by label,"<p>I'm using spaCy pipeline to extract all entities from articles. I need to save these entities on a variable depending on the label they have been tagged with. For now I have this solution, but I think this is not the most suitable one as I need to iterate over all the entities for each label:</p>

<pre><code>nlp = spacy.load(""es_core_news_md"")
text = # I upload my text here
doc = nlp(text)

personEntities = list(set([e.text for e in doc.ents if e.label_ == ""PER""]))
locationEntities = list(set([e.text for e in doc.ents if e.label_ == ""LOC""]))
organizationEntities = list(set([e.text for e in doc.ents if e.label_ == ""ORG""]))
</code></pre>

<p>Is there a direct method in spaCy in order to get all the entities for each label or would I need to do <code>for ent in ents: if... elif... elif...</code> to achieve that?</p>
","python, entity, spacy, named-entity-recognition","<p>I suggest using the <code>groupby</code> method from <code>itertools</code>:</p>

<pre><code>from itertools import *
#...
entities = {key: list(g) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}
</code></pre>

<p>Or, if you need to only extract unique values:</p>

<pre><code>entities = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}
</code></pre>

<p>Then, you may print known entities using</p>

<pre><code>print(entities['ORG'])
</code></pre>

<p>If you need to get unique occurrences of the entity <em>objects</em>, not just <em>strings</em>, you may use</p>

<pre><code>import spacy
from itertools import *

nlp = spacy.load(""en_core_web_sm"")
s = ""Hello, Mr. Wood! We are in New York. Mrs. Winston is not coming, John hasn't sent her any invite. They will meet in California next time. General Motors and Toyota are companies.""
doc = nlp(s * 2)

entities = dict()
for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_):
    seen = set()
    l = []
    for ent in list(g):
      if ent.text not in seen:
        seen.add(ent.text)
        l.append(ent)
    entities[key] = l
</code></pre>

<p>Output for <code>print(entities['GPE'][0].text)</code> is <code>New York</code> here.</p>
",5,3,2681,2019-11-27 11:01:48,https://stackoverflow.com/questions/59068687/spacy-most-efficient-way-to-sort-entities-by-label
Longest match only with Spacy Phrasematcher,"<p>I have created a <a href=""https://spacy.io/api/phrasematcher"" rel=""nofollow noreferrer"">Spacy Phrasematcher</a> to match names in a document, following the <a href=""https://spacy.io/usage/rule-based-matching#phrasematcher"" rel=""nofollow noreferrer"">tutorial</a>. I want to use the resulting matches as additional training data in order to train a Spacy NER model.
My patterns, however, contain both full names (e.g. 'Barack Obama') and last names ('Obama') separately.</p>

<p>Hence, in a sentence that contains 'Barack Obama', both patterns match, resulting in overlapping matches. This overlap, however, triggers an exception when I try to use the data for training, e.g.:</p>

<pre><code>ValueError: [E103] Trying to set conflicting doc.ents: '(19, 33, 'PERSON')' and '(29, 33, 'PERSON')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.
</code></pre>

<p>I've been considering to filter out overlapping matches before using the data for training, but this seems like a very inefficient approach, resulting in a significant increase in processing time for large data.</p>

<p>Is there a way to set up a <code>PhraseMatcher</code> so that it only matches the longest match for overlapping matches?</p>
","python, nlp, spacy, named-entity-recognition","<p>The <code>PhraseMatcher</code> doesn't have a built-in way to filter out overlapping matches while it's matching, but there is a utility function to filter overlapping matches afterwards: <code>spacy.util.filter_spans()</code>. It prefers the longest span and if two overlapping spans are the same length, the earlier span in the text.</p>
",8,5,1307,2019-11-29 13:01:21,https://stackoverflow.com/questions/59105346/longest-match-only-with-spacy-phrasematcher
Why spacy forgetting old trained data and how to solve,"<p>I am trying to train a spacy model for ner. I have a data set with 2940 rows and i trained a base model let it's name be <code>current_model</code> with these data and i got another 10 distinct dataset each have rows ranging from 200 to 530 rows so i loaded my current_model using spacy's <code>spacy.load(""current_model"")</code> then i trained using my each dataset. and i tried to predict ner using <code>test data</code> it recognises ner in new dataset but it seems forgetting ner in oldest dataset. i did this to reduce training time. please see my code below to see the what have i tried to do</p>

<p><strong>Code for base model training</strong></p>

<pre><code>import spacy
from spacy.util import minibatch,compounding
import random
from pathlib import Path
from spacy import displacy
import re
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
imporcytoolz import partition_all
import os
from os import path
import shutil
import json


df = pd.read_csv(""new_annotations/dataset_transfer_learning1.csv"")
def populate_train_data(df):
train_data = []
i =0
for d_index, row in df.iterrows():
    print(row[""annotations""])
    content = row[""annotations""].replace(""\\n"", ""\n"").replace(""\n"", "" "")
    content = re.sub(r""(?&lt;=[:])(?=[^\s])"", r"" "", content)

# Finding tags and entities and store values in a entity list-----
soup = BeautifulSoup(content, ""html.parser"")
text = soup.get_text()
entities = []
for tag in soup.find_all():
    if tag.string is None:
        # failing silently for invalid tag
        print(f'Tagging is invalid: {row[""_id""], tag.name}, on row {i+2}skipping..')
        continue

tag_index = content.split(str(tag))[0].count(tag.string)
try:

for index, match in enumerate(re.finditer(tag.string.replace(""*"", "" ""), text)):

if index == tag_index:

entities.append((match.start(), match.end(), tag.name))

except Exception as e:

print(e, f""at line no {i+2}"")

continue

i += 1

if entities:

train_data.append((text, {""entities"": entities}))

return train_data



def train(training_data,old_training_data=None,model_name=None):

nlp = """"

pretrained_weights = Path('weights/model999.bin')

if model_name is not None:

nlp = spacy.load(model_name,weights=pretrained_weights)

else:

print(""no model specified using default model"")

nlp = spacy.load(""en_core_web_sm"")

if ""ner"" not in nlp.pipe_names:

print(""there is no ner creating ner"")

ner = nlp.create_pipe(""ner"")

nlp.add_pipe(ner,last=True)

else:

print(""there is ner"")

ner = nlp.get_pipe(""ner"")

for _,annotations in training_data:

for ent in annotations.get(""entities""):

ner.add_label(ent[2])

start_time = time.time()

if model_name is not None:

# nlp.resume_training()

# TRAINING_DATA = populate_train_data(pd.read_csv(old_training_data))

TRAINING_DATA = old_training_data

revision_data =[]

for doc in nlp.pipe(list(zip(*TRAINING_DATA))[0]):

tags = [w.tag_ for w in doc]

heads = [w.head.i for w in doc]

deps = [w.dep_ for w in doc]

entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]

revision_data.append((doc, GoldParse(doc, entities=entities)))

fine_tune_data = []

for raw_text, entity_offsets in training_data:

doc = nlp.make_doc(raw_text)

try:

gold = GoldParse(doc,entities=entity_offsets['entities'])

except ValueError:

pass

fine_tune_data.append((doc,gold))

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

optimizer = nlp.entity.create_optimizer()

with nlp.disable_pipes(*other_pipes):

# pretrained_weights = Path('weights/model999.bin')

# with pretrained_weights.open(""rb"") as file_:

# ner.model.tok2vec.from_bytes(file_.read())

for i in range(20):

example_data = revision_data+fine_tune_data

# example_data = training_data

losses = {}

random.shuffle(example_data)

for batch in partition_all(2,example_data):

docs, golds = zip(*batch)

# print(docs, golds)

try:


nlp.update(docs,golds)

except ValueError:

pass

# print(losses)

else:

for i in range(20):

random.shuffle(training_data)

correct = 1

for text, annotations in training_data:

try:

nlp.update([text],[annotations])

print(correct)

correct +=1

except ValueError:

pass

# print(""skipping.."")

no_of_stars = i

print(""*""*no_of_stars)

end_time = time.time()

print(""this code took {}"".format(end_time - start_time))

return nlp


def save_to_directory(nlp,directory_name):

save_directory = directory_name

for directory in save_directory:

if directory is not None:

directory_full_path = Path(directory+""_""+datetime.today().strftime('%Y_%m_%d'))

if path.exists(directory_full_path):

shutil.rmtree(directory_full_path)

print(""folder already existed so removed"")

if not directory_full_path.exists():

directory_full_path.mkdir()

nlp.to_disk(directory_full_path)

print(""Saved model to output directory"",directory)



if __name__ == ""__main__"":

training_data = populate_train_data(df)


# training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

# print(training_data)

nlp = train(training_data)

save_to_directory(nlp,[""trained_model_with_transfer_learning""])cytoolz import partition_all

import os

from os import path

import shutil

import json



df = pd.read_csv(""new_annotations/dataset_transfer_learning1.csv"")

def populate_train_data(df):

train_data = []

i =0

for d_index, row in df.iterrows():

print(row[""annotations""])

content = row[""annotations""].replace(""\\n"", ""\n"").replace(""\n"", "" "")

content = re.sub(r""(?&lt;=[:])(?=[^\s])"", r"" "", content)


# Finding tags and entities and store values in a entity list-----

soup = BeautifulSoup(content, ""html.parser"")

text = soup.get_text()

entities = []

for tag in soup.find_all():

if tag.string is None:

# failing silently for invalid tag

print(f'Tagging is invalid: {row[""_id""], tag.name}, on row {i+2}skipping..')

continue


tag_index = content.split(str(tag))[0].count(tag.string)

try:

for index, match in enumerate(re.finditer(tag.string.replace(""*"", "" ""), text)):

if index == tag_index:

entities.append((match.start(), match.end(), tag.name))

except Exception as e:

print(e, f""at line no {i+2}"")

continue

i += 1

if entities:

train_data.append((text, {""entities"": entities}))

return train_data



def train(training_data,old_training_data=None,model_name=None):

nlp = """"

pretrained_weights = Path('weights/model999.bin')

if model_name is not None:

nlp = spacy.load(model_name,weights=pretrained_weights)

else:

print(""no model specified using default model"")

nlp = spacy.load(""en_core_web_sm"")

if ""ner"" not in nlp.pipe_names:

print(""there is no ner creating ner"")

ner = nlp.create_pipe(""ner"")

nlp.add_pipe(ner,last=True)

else:

print(""there is ner"")

ner = nlp.get_pipe(""ner"")

for _,annotations in training_data:

for ent in annotations.get(""entities""):

ner.add_label(ent[2])

start_time = time.time()

if model_name is not None:

# nlp.resume_training()

# TRAINING_DATA = populate_train_data(pd.read_csv(old_training_data))

TRAINING_DATA = old_training_data

revision_data =[]

for doc in nlp.pipe(list(zip(*TRAINING_DATA))[0]):

tags = [w.tag_ for w in doc]

heads = [w.head.i for w in doc]

deps = [w.dep_ for w in doc]

entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]

revision_data.append((doc, GoldParse(doc, entities=entities)))

fine_tune_data = []

for raw_text, entity_offsets in training_data:

doc = nlp.make_doc(raw_text)

try:

gold = GoldParse(doc,entities=entity_offsets['entities'])

except ValueError:

pass

fine_tune_data.append((doc,gold))

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

optimizer = nlp.entity.create_optimizer()

with nlp.disable_pipes(*other_pipes):

# pretrained_weights = Path('weights/model999.bin')

# with pretrained_weights.open(""rb"") as file_:

# ner.model.tok2vec.from_bytes(file_.read())

for i in range(20):

example_data = revision_data+fine_tune_data

# example_data = training_data

losses = {}

random.shuffle(example_data)

for batch in partition_all(2,example_data):

docs, golds = zip(*batch)

# print(docs, golds)

try:


nlp.update(docs,golds)

except ValueError:

pass

# print(losses)

else:

for i in range(20):

random.shuffle(training_data)

correct = 1

for text, annotations in training_data:

try:

nlp.update([text],[annotations])

print(correct)

correct +=1

except ValueError:

pass

# print(""skipping.."")

no_of_stars = i

print(""*""*no_of_stars)

end_time = time.time()

print(""this code took {}"".format(end_time - start_time))

return nlp


def save_to_directory(nlp,directory_name):

save_directory = directory_name

for directory in save_directory:

if directory is not None:

directory_full_path = Path(directory+""_""+datetime.today().strftime('%Y_%m_%d'))

if path.exists(directory_full_path):

shutil.rmtree(directory_full_path)

print(""folder already existed so removed"")

if not directory_full_path.exists():

directory_full_path.mkdir()

nlp.to_disk(directory_full_path)

print(""Saved model to output directory"",directory)



if __name__ == ""__main__"":

training_data = populate_train_data(df)


# training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

# print(training_data)

nlp = train(training_data)

save_to_directory(nlp,[""trained_model_with_transfer_learning""])


#to do train using batched

#add drop rate


</code></pre>

<p><strong>Code for training with new dataset and save to another directory</strong></p>

<p>note: below code is written new file.</p>

<pre><code>
import spacy

from spacy import displacy

import pandas as pd

from annotations_training_spacy_31_oct_2019 import populate_train_data,train,save_to_directory



# test_texts = ""I Like Today and Evening""

# base_training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

test_text = test_texts


# new_data_set = [

# (""Today is an Awsome Day"", {""entities"":[(1,5,""DAY"")]}),

# ]


nlp = train(training_data=new_data_set,old_training_data=base_training_data,model_name=""trained_model_with_transfer_learning_8_2019_12_05"")

save_to_directory(nlp,[""trained_model_with_transfer_learning_9""])


doc = nlp(test_text)

print(""ENTITIES in '%s'"" % test_text)

nlp.add_pipe(nlp.create_pipe('sentencizer'))

sentence = list(doc.sents)

for ent in doc.ents:

print(ent.label_,ent.text)



displacy.serve(sentence, style='ent')


</code></pre>

<p>as you can see i also tried to load old datasets tags. but still i have this problem</p>

<p>i know some peoples faced this problem please if anybody solved this problem help me.</p>

<p>thanks in advance for you help friends.</p>

<p>Hi,</p>

<p>I am trying to train a spacy model for ner. I have a data set with 2940 rows and i trained a base </p>

<p>model let it's name be <code>current_model</code> with these data and i got another 10 distinct dataset </p>

<p>each have rows ranging from 200 to 530 rows so i loaded my current_model using spacy's <code>spacy.load(""current_model"")</code> then i trained using my each dataset. and i tried to predict ner </p>

<p>using <code>test data</code> it recognises ner in new dataset but it seems forgetting ner in oldest dataset</p>

<p>i did this because to reduce training time. please see my code below to see the what have i tried </p>

<p>to do and i </p>

<p><strong>code for base model training</strong></p>

<pre><code>
import spacy

from spacy.util import minibatch,compounding

import random

from pathlib import Path

from spacy import displacy

import re

import pandas as pd

from bs4 import BeautifulSoup

from datetime import datetime

imporcytoolz import partition_all

import os

from os import path

import shutil

import json



df = pd.read_csv(""new_annotations/dataset_transfer_learning1.csv"")

def populate_train_data(df):

train_data = []

i =0

for d_index, row in df.iterrows():

print(row[""annotations""])

content = row[""annotations""].replace(""\\n"", ""\n"").replace(""\n"", "" "")

content = re.sub(r""(?&lt;=[:])(?=[^\s])"", r"" "", content)


# Finding tags and entities and store values in a entity list-----

soup = BeautifulSoup(content, ""html.parser"")

text = soup.get_text()

entities = []

for tag in soup.find_all():

if tag.string is None:

# failing silently for invalid tag

print(f'Tagging is invalid: {row[""_id""], tag.name}, on row {i+2}skipping..')

continue


tag_index = content.split(str(tag))[0].count(tag.string)

try:

for index, match in enumerate(re.finditer(tag.string.replace(""*"", "" ""), text)):

if index == tag_index:

entities.append((match.start(), match.end(), tag.name))

except Exception as e:

print(e, f""at line no {i+2}"")

continue

i += 1

if entities:

train_data.append((text, {""entities"": entities}))

return train_data



def train(training_data,old_training_data=None,model_name=None):

nlp = """"

pretrained_weights = Path('weights/model999.bin')

if model_name is not None:

nlp = spacy.load(model_name,weights=pretrained_weights)

else:

print(""no model specified using default model"")

nlp = spacy.load(""en_core_web_sm"")

if ""ner"" not in nlp.pipe_names:

print(""there is no ner creating ner"")

ner = nlp.create_pipe(""ner"")

nlp.add_pipe(ner,last=True)

else:

print(""there is ner"")

ner = nlp.get_pipe(""ner"")

for _,annotations in training_data:

for ent in annotations.get(""entities""):

ner.add_label(ent[2])

start_time = time.time()

if model_name is not None:

# nlp.resume_training()

# TRAINING_DATA = populate_train_data(pd.read_csv(old_training_data))

TRAINING_DATA = old_training_data

revision_data =[]

for doc in nlp.pipe(list(zip(*TRAINING_DATA))[0]):

tags = [w.tag_ for w in doc]

heads = [w.head.i for w in doc]

deps = [w.dep_ for w in doc]

entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]

revision_data.append((doc, GoldParse(doc, entities=entities)))

fine_tune_data = []

for raw_text, entity_offsets in training_data:

doc = nlp.make_doc(raw_text)

try:

gold = GoldParse(doc,entities=entity_offsets['entities'])

except ValueError:

pass

fine_tune_data.append((doc,gold))

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

optimizer = nlp.entity.create_optimizer()

with nlp.disable_pipes(*other_pipes):

# pretrained_weights = Path('weights/model999.bin')

# with pretrained_weights.open(""rb"") as file_:

# ner.model.tok2vec.from_bytes(file_.read())

for i in range(20):

example_data = revision_data+fine_tune_data

# example_data = training_data

losses = {}

random.shuffle(example_data)

for batch in partition_all(2,example_data):

docs, golds = zip(*batch)

# print(docs, golds)

try:


nlp.update(docs,golds)

except ValueError:

pass

# print(losses)

else:

for i in range(20):

random.shuffle(training_data)

correct = 1

for text, annotations in training_data:

try:

nlp.update([text],[annotations])

print(correct)

correct +=1

except ValueError:

pass

# print(""skipping.."")

no_of_stars = i

print(""*""*no_of_stars)

end_time = time.time()

print(""this code took {}"".format(end_time - start_time))

return nlp


def save_to_directory(nlp,directory_name):

save_directory = directory_name

for directory in save_directory:

if directory is not None:

directory_full_path = Path(directory+""_""+datetime.today().strftime('%Y_%m_%d'))

if path.exists(directory_full_path):

shutil.rmtree(directory_full_path)

print(""folder already existed so removed"")

if not directory_full_path.exists():

directory_full_path.mkdir()

nlp.to_disk(directory_full_path)

print(""Saved model to output directory"",directory)



if __name__ == ""__main__"":

training_data = populate_train_data(df)


# training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

# print(training_data)

nlp = train(training_data)

save_to_directory(nlp,[""trained_model_with_transfer_learning""])cytoolz import partition_all

import os

from os import path

import shutil

import json



df = pd.read_csv(""new_annotations/dataset_transfer_learning1.csv"")

def populate_train_data(df):

train_data = []

i =0

for d_index, row in df.iterrows():

print(row[""annotations""])

content = row[""annotations""].replace(""\\n"", ""\n"").replace(""\n"", "" "")

content = re.sub(r""(?&lt;=[:])(?=[^\s])"", r"" "", content)


# Finding tags and entities and store values in a entity list-----

soup = BeautifulSoup(content, ""html.parser"")

text = soup.get_text()

entities = []

for tag in soup.find_all():

if tag.string is None:

# failing silently for invalid tag

print(f'Tagging is invalid: {row[""_id""], tag.name}, on row {i+2}skipping..')

continue


tag_index = content.split(str(tag))[0].count(tag.string)

try:

for index, match in enumerate(re.finditer(tag.string.replace(""*"", "" ""), text)):

if index == tag_index:

entities.append((match.start(), match.end(), tag.name))

except Exception as e:

print(e, f""at line no {i+2}"")

continue

i += 1

if entities:

train_data.append((text, {""entities"": entities}))

return train_data



def train(training_data,old_training_data=None,model_name=None):

nlp = """"

pretrained_weights = Path('weights/model999.bin')

if model_name is not None:

nlp = spacy.load(model_name,weights=pretrained_weights)

else:

print(""no model specified using default model"")

nlp = spacy.load(""en_core_web_sm"")

if ""ner"" not in nlp.pipe_names:

print(""there is no ner creating ner"")

ner = nlp.create_pipe(""ner"")

nlp.add_pipe(ner,last=True)

else:

print(""there is ner"")

ner = nlp.get_pipe(""ner"")

for _,annotations in training_data:

for ent in annotations.get(""entities""):

ner.add_label(ent[2])

start_time = time.time()

if model_name is not None:

# nlp.resume_training()

# TRAINING_DATA = populate_train_data(pd.read_csv(old_training_data))

TRAINING_DATA = old_training_data

revision_data =[]

for doc in nlp.pipe(list(zip(*TRAINING_DATA))[0]):

tags = [w.tag_ for w in doc]

heads = [w.head.i for w in doc]

deps = [w.dep_ for w in doc]

entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]

revision_data.append((doc, GoldParse(doc, entities=entities)))

fine_tune_data = []

for raw_text, entity_offsets in training_data:

doc = nlp.make_doc(raw_text)

try:

gold = GoldParse(doc,entities=entity_offsets['entities'])

except ValueError:

pass

fine_tune_data.append((doc,gold))

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

optimizer = nlp.entity.create_optimizer()

with nlp.disable_pipes(*other_pipes):

# pretrained_weights = Path('weights/model999.bin')

# with pretrained_weights.open(""rb"") as file_:

# ner.model.tok2vec.from_bytes(file_.read())

for i in range(20):

example_data = revision_data+fine_tune_data

# example_data = training_data

losses = {}

random.shuffle(example_data)

for batch in partition_all(2,example_data):

docs, golds = zip(*batch)

# print(docs, golds)

try:


nlp.update(docs,golds)

except ValueError:

pass

# print(losses)

else:

for i in range(20):

random.shuffle(training_data)

correct = 1

for text, annotations in training_data:

try:

nlp.update([text],[annotations])

print(correct)

correct +=1

except ValueError:

pass

# print(""skipping.."")

no_of_stars = i

print(""*""*no_of_stars)

end_time = time.time()

print(""this code took {}"".format(end_time - start_time))

return nlp


def save_to_directory(nlp,directory_name):

save_directory = directory_name

for directory in save_directory:

if directory is not None:

directory_full_path = Path(directory+""_""+datetime.today().strftime('%Y_%m_%d'))

if path.exists(directory_full_path):

shutil.rmtree(directory_full_path)

print(""folder already existed so removed"")

if not directory_full_path.exists():

directory_full_path.mkdir()

nlp.to_disk(directory_full_path)

print(""Saved model to output directory"",directory)



if __name__ == ""__main__"":

training_data = populate_train_data(df)


# training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

# print(training_data)

nlp = train(training_data)

save_to_directory(nlp,[""trained_model_with_transfer_learning""])


#to do train using batched

#add drop rate


</code></pre>

<p><strong>Code for training with new dataset and save to another directory</strong></p>

<p>note: below code is written new file.</p>

<pre><code>
import spacy

from spacy import displacy

import pandas as pd

from annotations_training_spacy_31_oct_2019 import populate_train_data,train,save_to_directory



# test_texts = ""I Like Today and Evening""

# base_training_data = [

# (""I Like Today and Evening"", {""entities"":[(7,12,""DAY""),(17,24,""DAY"")]}),

# (""Today is my lucky day"", {""entities"":[(1,5,""DAY"")]}),

# (""Yesterday and Today are two same days of a month"",{""entities"":[(14,19,""DAY"")]}),

# (""May Today is Best Day"",{""entities"":[(4,9,""DAY"")]}),

# (""Have a Nice Today and Every Day"",{""entities"":[(12,17,""DAY"")]}),

# (""Hey How are feeling Today"",{""entities"":[(20,25,""DAY"")]}),

# ]

test_text = test_texts


# new_data_set = [

# (""Today is an Awsome Day"", {""entities"":[(1,5,""DAY"")]}),

# ]


nlp = train(training_data=new_data_set,old_training_data=base_training_data,model_name=""trained_model_with_transfer_learning_8_2019_12_05"")

save_to_directory(nlp,[""trained_model_with_transfer_learning_9""])


doc = nlp(test_text)

print(""ENTITIES in '%s'"" % test_text)

nlp.add_pipe(nlp.create_pipe('sentencizer'))

sentence = list(doc.sents)

for ent in doc.ents:

print(ent.label_,ent.text)



displacy.serve(sentence, style='ent')


</code></pre>

<p>as you can see i also tried to load old datasets tags. but still i have this problem</p>

<p>i know some peoples faced this problem please if anybody solved this problem help me.</p>

<p>thanks in advance for you help friends.</p>
","python-3.x, nlp, spacy, named-entity-recognition","<p>Are you training the new model or appending on to existing spacy model? If you are doing the later, all the NN's(learnt weight, features) will be unlearnt and misaligned resulting in accuracy loss. I am telling this on experience when I wanted to train Korean and Japanese names that spacy could not identify. You can also try FastText, Flair and Polyglot and see if it achieves your purpose. Try to get the set out of all these tools and you should have good output. thats the solution I used in the end.</p>
",2,1,701,2019-12-05 11:05:07,https://stackoverflow.com/questions/59193762/why-spacy-forgetting-old-trained-data-and-how-to-solve
How to convert XML NER data from the CRAFT corpus to spaCy&#39;s JSON format?,"<p><strong>How to build a named entity recognition(NER) model using spaCy for biomedical NER on <a href=""https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmcids=PMC6207735"" rel=""nofollow noreferrer"">CRAFT corpus</a>?</strong> </p>

<p>It is difficult for me to pre-process the <code>xml</code> files given in that corpus to any format used by <code>spacy</code>, any little help would be highly appreciated.
I first converted the <code>xml</code> files to <code>json</code> format but that was not accepted by <code>spacy</code>. <strong>What format of training data does <code>spacy</code> expect?</strong> I even tried to build my own <code>NER</code> model but was not able to pre-process the <code>xml</code> files as given in this  <a href=""https://towardsdatascience.com/parsing-xml-named-entity-recognition-in-one-shot-629a8b9846ee"" rel=""nofollow noreferrer"">article</a>.</p>

<p>Here is an example of training an NER model using spacy, including the expected format of training data (from <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">spacy's docs</a>):</p>

<pre class=""lang-py prettyprint-override""><code>import random

import spacy


TRAIN_DATA = [
        (""Uber blew through $1 million a week"", {""entities"": [(0, 4, ""ORG"")]}),
        (""Google rebrands its business apps"", {""entities"": [(0, 6, ""ORG"")]})]

nlp = spacy.blank(""en"")
optimizer = nlp.begin_training()
for i in range(20):
    random.shuffle(TRAIN_DATA)
    for text, annotations in TRAIN_DATA:
        nlp.update([text], [annotations], sgd=optimizer)
nlp.to_disk(""/model"")
</code></pre>

<p>The XML file I am using is available online <a href=""https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmcids=PMC6207735"" rel=""nofollow noreferrer"">here</a>. An example record looks like:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;passage&gt;
&lt;infon key=""section_type""&gt;ABSTRACT&lt;/infon&gt;
&lt;infon key=""type""&gt;abstract&lt;/infon&gt;
&lt;offset&gt;141&lt;/offset&gt;
&lt;text&gt;
Breast cancer is the most frequent tumor in women, and in nearly two-thirds of cases, the tumors express estrogen receptor alpha (ERalpha, encoded by ESR1). Here, we performed whole-exome sequencing of 16 breast cancer tissues classified according to ESR1 expression and 12 samples of whole blood, and detected 310 somatic mutations in cancer tissues with high levels of ESR1 expression. Of the somatic mutations validated by a different deep sequencer, a novel nonsense somatic mutation, c.2830 C&gt;T; p.Gln944*, in transcriptional regulator switch-independent 3 family member A (SIN3A) was detected in breast cancer of a patient. Part of the mutant protein localized in the cytoplasm in contrast to the nuclear localization of ERalpha, and induced a significant increase in ESR1 mRNA. The SIN3A mutation obviously enhanced MCF7 cell proliferation. In tissue sections from the breast cancer patient with the SIN3A c.2830 C&gt;T mutation, cytoplasmic SIN3A localization was detected within the tumor regions where nuclear enlargement was observed. The reduction in SIN3A mRNA correlates with the recurrence of ER-positive breast cancers on Kaplan-Meier plots. These observations reveal that the SIN3A mutation has lost its transcriptional repression function due to its cytoplasmic localization, and that this repression may contribute to the progression of breast cancer.
&lt;/text&gt;
&lt;annotation id=""38""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""246"" length=""23""/&gt;
&lt;text&gt;estrogen receptor alpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""39""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""271"" length=""7""/&gt;
&lt;text&gt;ERalpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""40""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""291"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""41""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""392"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""42""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""512"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""43""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""720"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""44""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""868"" length=""7""/&gt;
&lt;text&gt;ERalpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""45""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""915"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""46""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""930"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""47""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1048"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""48""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1087"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""49""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1201"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""50""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1331"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""51""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""185"" length=""5""/&gt;
&lt;text&gt;women&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""52""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""762"" length=""7""/&gt;
&lt;text&gt;patient&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""53""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""1031"" length=""7""/&gt;
&lt;text&gt;patient&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""54""&gt;
&lt;infon key=""identifier""&gt;29278&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""397"" length=""10""/&gt;
&lt;text&gt;expression&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""55""&gt;
&lt;infon key=""identifier""&gt;29278&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""517"" length=""10""/&gt;
&lt;text&gt;expression&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""56""&gt;
&lt;infon key=""identifier""&gt;c.2830C&gt;T&lt;/infon&gt;
&lt;infon key=""type""&gt;DNAMutation&lt;/infon&gt;
&lt;location offset=""1054"" length=""10""/&gt;
&lt;text&gt;c.2830 C&gt;T&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""57""&gt;
&lt;infon key=""identifier""&gt;CVCL:0031&lt;/infon&gt;
&lt;infon key=""type""&gt;CellLine&lt;/infon&gt;
&lt;location offset=""964"" length=""4""/&gt;
&lt;text&gt;MCF7&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""58""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1494"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""59""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""346"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""60""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""743"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""61""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1017"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""62""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""477"" length=""6""/&gt;
&lt;text&gt;cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""63""&gt;
&lt;infon key=""identifier""&gt;p.Q944*&lt;/infon&gt;
&lt;infon key=""type""&gt;ProteinMutation&lt;/infon&gt;
&lt;location offset=""642"" length=""9""/&gt;
&lt;text&gt;p.Gln944*&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""64""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1130"" length=""5""/&gt;
&lt;text&gt;tumor&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""65""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""176"" length=""5""/&gt;
&lt;text&gt;tumor&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""66""&gt;
&lt;infon key=""identifier""&gt;c.2830C&gt;T&lt;/infon&gt;
&lt;infon key=""type""&gt;DNAMutation&lt;/infon&gt;
&lt;location offset=""630"" length=""10""/&gt;
&lt;text&gt;c.2830 C&gt;T&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""67""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1258"" length=""14""/&gt;
&lt;text&gt;breast cancers&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""68""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""231"" length=""6""/&gt;
&lt;text&gt;tumors&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""69""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""141"" length=""13""/&gt;
&lt;text&gt;Breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;/passage&gt;
</code></pre>
","python, nlp, bioinformatics, spacy, named-entity-recognition","<p>Here is some code to get you going. It is not a complete solution, but the problem you posed is very hard, and you didn't have any starter code.</p>

<p>It does not track the <code>identifier</code> or <code>NCBI Homologene</code> properties, but I think those can be stored in a dictionary separately.</p>

<pre class=""lang-py prettyprint-override""><code>import xml.etree.cElementTree as ET

import spacy

nlp = spacy.load('en_core_web_sm')

# this is one child of the XML doc
# https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmcids=PMC6207735
passage_string = """"""
&lt;passage&gt;
&lt;infon key=""section_type""&gt;ABSTRACT&lt;/infon&gt;
&lt;infon key=""type""&gt;abstract&lt;/infon&gt;
&lt;offset&gt;141&lt;/offset&gt;
&lt;text&gt;
Breast cancer is the most frequent tumor in women, and in nearly two-thirds of cases, the tumors express estrogen receptor alpha (ERalpha, encoded by ESR1). Here, we performed whole-exome sequencing of 16 breast cancer tissues classified according to ESR1 expression and 12 samples of whole blood, and detected 310 somatic mutations in cancer tissues with high levels of ESR1 expression. Of the somatic mutations validated by a different deep sequencer, a novel nonsense somatic mutation, c.2830 C&gt;T; p.Gln944*, in transcriptional regulator switch-independent 3 family member A (SIN3A) was detected in breast cancer of a patient. Part of the mutant protein localized in the cytoplasm in contrast to the nuclear localization of ERalpha, and induced a significant increase in ESR1 mRNA. The SIN3A mutation obviously enhanced MCF7 cell proliferation. In tissue sections from the breast cancer patient with the SIN3A c.2830 C&gt;T mutation, cytoplasmic SIN3A localization was detected within the tumor regions where nuclear enlargement was observed. The reduction in SIN3A mRNA correlates with the recurrence of ER-positive breast cancers on Kaplan-Meier plots. These observations reveal that the SIN3A mutation has lost its transcriptional repression function due to its cytoplasmic localization, and that this repression may contribute to the progression of breast cancer.
&lt;/text&gt;
&lt;annotation id=""38""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""246"" length=""23""/&gt;
&lt;text&gt;estrogen receptor alpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""39""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""271"" length=""7""/&gt;
&lt;text&gt;ERalpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""40""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""291"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""41""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""392"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""42""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""512"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""43""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""720"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""44""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""868"" length=""7""/&gt;
&lt;text&gt;ERalpha&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""45""&gt;
&lt;infon key=""identifier""&gt;2099&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;47906&lt;/infon&gt;
&lt;location offset=""915"" length=""4""/&gt;
&lt;text&gt;ESR1&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""46""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""930"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""47""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1048"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""48""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1087"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""49""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1201"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""50""&gt;
&lt;infon key=""identifier""&gt;25942&lt;/infon&gt;
&lt;infon key=""type""&gt;Gene&lt;/infon&gt;
&lt;infon key=""NCBI Homologene""&gt;32124&lt;/infon&gt;
&lt;location offset=""1331"" length=""5""/&gt;
&lt;text&gt;SIN3A&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""51""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""185"" length=""5""/&gt;
&lt;text&gt;women&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""52""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""762"" length=""7""/&gt;
&lt;text&gt;patient&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""53""&gt;
&lt;infon key=""identifier""&gt;9606&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""1031"" length=""7""/&gt;
&lt;text&gt;patient&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""54""&gt;
&lt;infon key=""identifier""&gt;29278&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""397"" length=""10""/&gt;
&lt;text&gt;expression&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""55""&gt;
&lt;infon key=""identifier""&gt;29278&lt;/infon&gt;
&lt;infon key=""type""&gt;Species&lt;/infon&gt;
&lt;location offset=""517"" length=""10""/&gt;
&lt;text&gt;expression&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""56""&gt;
&lt;infon key=""identifier""&gt;c.2830C&gt;T&lt;/infon&gt;
&lt;infon key=""type""&gt;DNAMutation&lt;/infon&gt;
&lt;location offset=""1054"" length=""10""/&gt;
&lt;text&gt;c.2830 C&gt;T&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""57""&gt;
&lt;infon key=""identifier""&gt;CVCL:0031&lt;/infon&gt;
&lt;infon key=""type""&gt;CellLine&lt;/infon&gt;
&lt;location offset=""964"" length=""4""/&gt;
&lt;text&gt;MCF7&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""58""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1494"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""59""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""346"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""60""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""743"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""61""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1017"" length=""13""/&gt;
&lt;text&gt;breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""62""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""477"" length=""6""/&gt;
&lt;text&gt;cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""63""&gt;
&lt;infon key=""identifier""&gt;p.Q944*&lt;/infon&gt;
&lt;infon key=""type""&gt;ProteinMutation&lt;/infon&gt;
&lt;location offset=""642"" length=""9""/&gt;
&lt;text&gt;p.Gln944*&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""64""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1130"" length=""5""/&gt;
&lt;text&gt;tumor&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""65""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""176"" length=""5""/&gt;
&lt;text&gt;tumor&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""66""&gt;
&lt;infon key=""identifier""&gt;c.2830C&gt;T&lt;/infon&gt;
&lt;infon key=""type""&gt;DNAMutation&lt;/infon&gt;
&lt;location offset=""630"" length=""10""/&gt;
&lt;text&gt;c.2830 C&gt;T&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""67""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""1258"" length=""14""/&gt;
&lt;text&gt;breast cancers&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""68""&gt;
&lt;infon key=""identifier""&gt;MESH:D009369&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""231"" length=""6""/&gt;
&lt;text&gt;tumors&lt;/text&gt;
&lt;/annotation&gt;
&lt;annotation id=""69""&gt;
&lt;infon key=""identifier""&gt;MESH:D001943&lt;/infon&gt;
&lt;infon key=""type""&gt;Disease&lt;/infon&gt;
&lt;location offset=""141"" length=""13""/&gt;
&lt;text&gt;Breast cancer&lt;/text&gt;
&lt;/annotation&gt;
&lt;/passage&gt;""""""

# turn into an object
passage = ET.fromstring(passage_string)

# these 3 definitions are per-passage
passage_annotations = passage.findall('./annotation')
passage_offset = int(passage.find('offset').text)
passage_text = passage.find('text').text

def get_entity_offset(offset_dict, passage_offset):
    """"""
    XML given offset_dict gives offset relative to the start of the document
    So subtract the passage offset (where passage starts relative to document beginning)
    """"""
    start = int(offset_dict['offset']) - passage_offset
    end = int(offset_dict['offset']) + (int(offset_dict['length']) + 1) - passage_offset
    return start, end

# collect entities as a list of tuples of the form
# (start, end, entitiy_type)
passage_entities = []
for ann in passage_annotations:
    entity_type = ann.find('./infon[@key=""type""]').text
    od = ann.find('./location').attrib
    start, end = get_entity_offset(od, passage_offset)
    passage_entities.append((start, end, entity_type))

# this is one entry in the spacy NER format
# you would want many entries
spacyd_passage = (passage_text, {""entities"": passage_entities})

# prove this worked
for ent in passage_entities:
    print(ent, passage_text[ent[0]:ent[1]])

# prints:
# (105, 129, 'Gene')  estrogen receptor alpha
# (130, 138, 'Gene') (ERalpha
# (150, 155, 'Gene')  ESR1
# (251, 256, 'Gene')  ESR1
# (371, 376, 'Gene')  ESR1
# (579, 585, 'Gene') (SIN3A
# (727, 735, 'Gene')  ERalpha
# (774, 779, 'Gene')  ESR1
# (789, 795, 'Gene')  SIN3A
# (907, 913, 'Gene')  SIN3A
# (946, 952, 'Gene')  SIN3A
# (1060, 1066, 'Gene')  SIN3A
# (1190, 1196, 'Gene')  SIN3A
# (44, 50, 'Species')  women
# (621, 629, 'Species')  patient
# (890, 898, 'Species')  patient
# (256, 267, 'Species')  expression
# (376, 387, 'Species')  expression
# (913, 924, 'DNAMutation')  c.2830 C&gt;T
# (823, 828, 'CellLine')  MCF7
# (1353, 1367, 'Disease')  breast cancer
# (205, 219, 'Disease')  breast cancer
# (602, 616, 'Disease')  breast cancer
# (876, 890, 'Disease')  breast cancer
# (336, 343, 'Disease')  cancer
# (501, 511, 'ProteinMutation')  p.Gln944*
# (989, 995, 'Disease')  tumor
# (35, 41, 'Disease')  tumor
# (489, 500, 'DNAMutation')  c.2830 C&gt;T
# (1117, 1132, 'Disease')  breast cancers
# (90, 97, 'Disease')  tumors
# (0, 14, 'Disease')  Breast cancer
</code></pre>

<p>So, the first thing I notice is that some of the given offsets are slightly off, catching <code>(</code>. You could look for <code>if passage_text[ent[0]] == ""(""</code> and shift the start of the entity by 1 to clean that, or clean it manually.</p>

<p>Also, this code uses one child node, a <code>passage</code> of the linked doc. You will want to download that doc locally, and instead of <code>passage = ET.fromstring(passage_string)</code>, you will create <code>tree = ET.parse('path_to_file')</code>:</p>

<p>Something like</p>

<pre class=""lang-py prettyprint-override""><code>import xml.etree.cElementTree as ET

tree = ET.parse('path_to_file')
root = tree.getroot()
passages = root.findall('./passages')

spacy_data = []

for passage in passages:
    passage_annotations = passage.findall('./annotation')
    passage_offset = int(passage.find('offset').text)
    passage_text = passage.find('text').text

    passage_entities = []
    for ann in passage_annotations:
        entity_type = ann.find('./infon[@key=""type""]').text
        od = ann.find('./location').attrib
        start, end = get_entity_offset(od, passage_offset)
        passage_entities.append((start, end, entity_type))

        spacyd_passage = (passage_text, {""entities"": passage_entities})
        spacy_data.append(spacyd_package)
</code></pre>

<p>This can still be improved upon. You'll want to split those <code>passage.text</code> passages using</p>

<pre class=""lang-py prettyprint-override""><code>import spacy

nlp = spacy.load('en_core_web_sm')

doc = nlp(passage_text)
sents = list(doc.sents)
</code></pre>

<p>But the tricky part is you need to do arithmetic to keep the offset indices correct. And you will also want to look at the start and end of each entity to make sure it stays within one sentence - it conceivably could be split by a sentence boundary, though probably not.</p>
",3,2,1245,2019-12-10 06:30:57,https://stackoverflow.com/questions/59261444/how-to-convert-xml-ner-data-from-the-craft-corpus-to-spacys-json-format
Scispacy for biomedical named entitiy recognition(NER),"<p><strong>How to label entities using scispacy?</strong></p>

<p>When I tried to perform NER using <code>scispacy</code>, it identified the biomedical entities by labeling them as <code>Entity</code> but failed to label them as gene/protein, etc.. So how do I do that using <code>scispacy</code>? Or is <code>scispacy</code> not capable of labeling data? The image is attached for reference:
<a href=""https://i.sstatic.net/QySa5.png"" rel=""nofollow noreferrer"">jupyter notebook snippet</a></p>
","python, nlp, bioinformatics, named-entity-recognition","<p>The models <code>en_core_sci_sm</code>, <code>en_core_sci_md</code> and <code>en_core_sci_lg</code> do not name their entities. If you want labeled entities use the models </p>

<ul>
<li>en_ner_craft_md</li>
<li>en_ner_jnlpba_md</li>
<li>en_ner_bc5cdr_md</li>
<li>en_ner_bionlp13cg_md</li>
</ul>

<p>each of which has its own type of entities see:-</p>

<p><a href=""https://allenai.github.io/scispacy/"" rel=""nofollow noreferrer"">https://allenai.github.io/scispacy/</a></p>

<p>for more information</p>
",2,1,3757,2019-12-10 10:43:30,https://stackoverflow.com/questions/59265404/scispacy-for-biomedical-named-entitiy-recognitionner
What is the process to create an FAQ bot using Spacy?,"<p>I am beginner to Machine Learning and NLP, I have to create a bot based on FAQ dataset, Each FAQ dataset excel file contains 2 columns ""Questions"" and its ""Answers"".</p>

<p>Eg. A record from an excel file (A question &amp; it's answer). </p>

<p><strong>Question</strong> - <code>What is RASA-NLU?</code></p>

<p><strong>Answer</strong> - <code>Rasa NLU is trained to identify intent and entities. Better the training, better the identification...</code> </p>

<p>We have 3K+ excel files which has around 10K to 20K such records each excel. </p>

<p>To implement the bot, I would have followed exactly <a href=""https://www.analyticsvidhya.com/blog/2018/01/faq-chatbots-the-future-of-information-searching/"" rel=""nofollow noreferrer"">this FAQ bot approach</a> which uses <code>RASA-NLU</code>, but the <code>RASA</code>,<code>Chatterbot</code> also Microsoft's <code>QnA maker</code> are not allowed in my organization.</p>

<p>And <code>Spacy</code> does the <code>NER extraction</code> perfectly for me, so I am looking for a bot creation using Spacy. but I don't know how to proceed further after extracting the entities. (IMHO, I will have to predict the exact question from dataset (and its answer from <code>knowlwdge base</code>) from user query to the bot)</p>

<p>I don't know what NLP algorithm/ ML process to be used or is there any easiest way to create that FAQ bot using extracted NERs.</p>
","python-3.x, machine-learning, nlp, chatbot, named-entity-recognition","<p>One way to achieve your FAQ bot is to transform the problem into a classification problem. You have questions and the answers can be the ""labels"". I suppose that you always have multiple training questions which map to the same answer. You can encode each answer in order to get smaller labels (for instance, you can map the text of the answer to an id).
Then, you can use your training data (the questions) and your labels (the encoded answers) and feed a classifier. After the training your classifier can predict the label of unseen questions. 
Of course, this is a supervised approach, so you will need to extract features from your training sentences (the questions). In this case, you can use as a feature the bag-of-word representations and even include the named entities.
An example of how to do text classification in spacy is available here: <a href=""https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/"" rel=""nofollow noreferrer"">https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/</a></p>
",2,0,1049,2019-12-11 12:00:53,https://stackoverflow.com/questions/59285376/what-is-the-process-to-create-an-faq-bot-using-spacy
Displaying the description of entity from kb id in spacy entity linking,"<blockquote>
<p>I have successfully trained a spacy entity linking model(obviously by
limiting the data).</p>
<p>my question is how to display the description
of entity from kb as output?</p>
</blockquote>
<pre><code>
import spacy
nlp = spacy.load(r&quot;D:\el model\nlp&quot;)
doc = nlp(&quot;Amir Khan is a great boxer&quot;)
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]
print(ents) 
</code></pre>
","spacy, named-entity-recognition, entity-linking","<p>As said by Sofie Van Landeghem(Spacy Entity Linking Representative).
The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.</p>
",0,0,393,2019-12-13 05:53:24,https://stackoverflow.com/questions/59316859/displaying-the-description-of-entity-from-kb-id-in-spacy-entity-linking
NER - Entity Recognition - Country Filter,"<p>I want to extract Geo-relevant Info from an Excel file with spacy. It works to extract all Entities, but I just need the Geo-Data and don´t find a way to filter the entities.</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import spacy

sp = spacy.load(""en_core_web_sm"")
df = pd.read_excel(""test.xlsx"", usecols=[""Bio"", ""Author""])
df.head(1)
df=df.fillna('')
#df['Bio']
doc = df.values.tolist()
#print (doc)
#sp(', '.join(doc[0])).ents
for entry in doc:
    #print('Current entry\n {}'.format(entry))
    for entity in sp(', '.join(entry)).ents:
        print(entity.text, entity.label)
</code></pre>

<p>Currently, the output looks like:</p>

<pre><code>Munich 384

Germany 384

Venezuela 384

London 384

Portrait | 9191306739292312949

📍 ℍ𝕠𝕟𝕘 𝕂𝕠𝕟𝕘 ​ 383

🇰 🌏 𝕋𝕣𝕒𝕧𝕖𝕝𝕝𝕖𝕣​ 394

Visited:🇬🇧🇬 383

🇸 384

🇹 392

</code></pre>

<p>At the end I want to write the Geo-relevant Entities (if existing) back to the user´s row in a new column <strong>""Location""</strong> in the <code>csv</code>.</p>

<p>I would appreciate your help very much, with kind regards</p>
","python, entity, spacy, named-entity-recognition","<p>As mentioned, you can filter for the ""LOC"" or ""GPE"" entity provided by the spacy language model. However, be aware that the NER language model needs to have a sentence contex to be able to predict the location entities.</p>

<pre class=""lang-py prettyprint-override""><code>sp = spacy.load(""en_core_web_sm"")
# loop over every row in the 'Bio' column
for text in df['Bio'].tolist():
    # use spacy to extract the entities
    doc = sp(text)
    for ent in doc.ents:    
        # check if entity is equal 'LOC' or 'GPE'
        if ent.label_ in ['LOC', 'GPE']:
            print(ent.text, ent.label_)   
</code></pre>

<p>Here the link to the spacy NER documentation: <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#named-entities</a></p>

<p><strong>EDIT</strong></p>

<p>Here is the full list of English spacy entity types from the documentation: </p>

<ul>
<li>PERSON   People, including fictional.  NORP  Nationalities or religious or political groups.  </li>
<li>FAC  Buildings, airports, highways, bridges, etc.    </li>
<li>ORG  Companies, agencies, institutions, etc.  </li>
<li>GPE  Countries, cities, states.  </li>
<li>LOC  Non-GPE locations, mountain ranges, bodies of water.</li>
<li>PRODUCT  Objects, vehicles, foods, etc. (Not services.)  </li>
<li>EVENT    Named hurricanes, battles, wars, sports events, etc.</li>
<li>WORK_OF_ART  Titles of books, songs, etc. </li>
<li>LAW  Named documents made
into laws. </li>
<li>LANGUAGE Any named language.  </li>
<li>DATE Absolute or relative dates or periods.</li>
<li>TIME Times smaller than a day. </li>
<li>PERCENT  Percentage, including ”%“.</li>
<li>MONEY    Monetary values, including unit.  </li>
<li>QUANTITY Measurements, as of weight or distance. </li>
<li>ORDINAL  “first”, “second”, etc.  </li>
<li>CARDINAL Numerals that do not fall under another type.</li>
</ul>

<p>Source: <a href=""https://spacy.io/api/annotation#named-entities"" rel=""nofollow noreferrer"">https://spacy.io/api/annotation#named-entities</a></p>
",3,1,3887,2019-12-13 09:04:12,https://stackoverflow.com/questions/59319207/ner-entity-recognition-country-filter
Tagging words in sentences using user define dictionary,"<p>I have a corpus of more than 100k sentences and i have dictionary. i want to match the words in the corpus and tagged them in the sentences</p>

<p>corpus file ""testing.txt""</p>

<pre><code>Hello how are you doing. HiV is dangerous
Malaria can be cure
he has anxiety thats why he is behaving like that.
</code></pre>

<p>Dictionary file ""dict.csv""</p>

<pre><code>abc, anxiety, disorder
def, HIV, virus
hij, Malaria, virus
klm, headache, symptom
</code></pre>

<p>My python program</p>

<pre><code>import csv
from difflib import SequenceMatcher as SM
from nltk.util import ngrams

import codecs

with open('dictionary.csv','r') as csvFile:
    reader = csv.reader(csvFile)
    myfile = open(""testing.txt"", ""rt"")
    my2file = open(""match.txt"" ,""w"")
    hay = myfile.read()
    myfile.close()

for row in reader:
    needle = row[1]
    needle_length = len(needle.split())
    max_sim_val = 0.9
    max_sim_string = u""""
    for ngram in ngrams(hay.split(), needle_length + int(.2 * needle_length)):
        hay_ngram = u"" "".join(ngram)

        similarity = SM(None, hay_ngram, needle).ratio()
        if similarity &gt; max_sim_val:
            max_sim_val = similarity
            max_sim_string = hay_ngram
            str = [row[1] , ' ', max_sim_val.__str__(),' ', max_sim_string , '\n']
            my2file.writelines(str)
            print(str)

csvFile.close()
</code></pre>

<p>my ouput for now is </p>

<pre><code> disorder 0.9333333333333333 anxiety
 virus 0.9333333333333333 Malaria
</code></pre>

<p>I want my output as </p>

<pre><code> Hello how are you doing. HIV [virus] is dangerous
 Malaria [virus] can be cure.
 he has anxiety [disorder] thats why he is behaving like that
</code></pre>
","python, dictionary, named-entity-recognition","<p>You can iterate over the lines on your <code>testing.txt</code> and replace those values, something like this should work:</p>

<pre class=""lang-py prettyprint-override""><code>...
if similarity &gt; max_sim_val:
    max_sim_val = similarity
    max_sim_string = hay_ngram
    str = [row[1] , ' ', max_sim_val.__str__(),' ', max_sim_string , '\n']
    my2file.writelines(str)
    print(str)

    for line in hay.splitlines():
        if max_sim_string in line:
            print(line.replace(max_sim_string, f""{max_sim_string} [{row[1]}]""))
            break
</code></pre>
",0,0,151,2020-01-02 14:35:25,https://stackoverflow.com/questions/59565105/tagging-words-in-sentences-using-user-define-dictionary
Tagging words in sentences using dictionares,"<p>I have a corpus of more than 100k sentences and i have dictionary. i want to match the words in the corpus and tagged them in the sentences</p>

<p>corpus file ""sentences.txt""</p>

<pre><code>Hello how are you doing. Headache is dangerous
Malaria can be cure
he has anxiety thats why he is behaving like that.
she is doing well
he has psychological problems
</code></pre>

<p>Dictionary file ""dict.csv""</p>

<pre><code>abc, anxiety, disorder
def, Headache, symptom
hij, Malaria, virus
klm, headache, symptom
</code></pre>

<p>My python program</p>

<pre><code>import csv
from difflib import SequenceMatcher as SM
from nltk.util import ngrams

import codecs

with open('dictionary.csv','r') as csvFile:
    reader = csv.reader(csvFile)
    myfile = open(""sentences.txt"", ""rt"")
    my3file = open(""tagged_sentences.txt"", ""w"")
    hay = myfile.read()
    myfile.close()

for row in reader:
    needle = row[1]
    needle_length = len(needle.split())
    max_sim_val = 0.9
    max_sim_string = u""""
    for ngram in ngrams(hay.split(), needle_length + int(.2 * needle_length)):
        hay_ngram = u"" "".join(ngram)

        similarity = SM(None, hay_ngram, needle).ratio()
        if similarity &gt; max_sim_val:
            max_sim_val = similarity
            max_sim_string = hay_ngram
            str = [row[1] , ' ', max_sim_val.__str__(),' ', max_sim_string , '\n']
            str1 = max_sim_string , row[2]
            for line in hay.splitlines():
                if max_sim_string in line:
                    tag_sent = line.replace(max_sim_string, str1.__str__())
                    my3file.writelines(tag_sent + '\n')
                    print(tag_sent)
            break

csvFile.close()
</code></pre>

<p>my ouput for now is </p>

<pre><code> he has ('anxiety', ' disorder') thats why he is behaving like that.
 ('Malaria', ' virus') can be cure
 Hello how are you doing. ('Headache', ' symptom') is dangerous
</code></pre>

<p>I want my output as. i want it tags the words in the sentences in the same file ""sentences.txt"" or write it in new file ""myfile3.txt. without disturbing the order of sentences or totally ignore (not adding) it</p>

<pre><code> Hello how are you doing. ('Headache', 'symptom') is dangerous
 ('Malaria', ' virus') can be cure.
 he has ('anxiety', ' disorder') thats why he is behaving like that
 she is doing well
 he has psychological problems
</code></pre>
","python, dictionary, tagging, named-entity-recognition","<p>Without changing much in your code this should make it work:</p>

<pre class=""lang-py prettyprint-override""><code>...
phrases = []
for row in reader:
    needle = row[1]
    needle_length = len(needle.split())
    max_sim_val = 0.9
    max_sim_string = u""""
    for ngram in ngrams(hay.split(), needle_length + int(.2 * needle_length)):
        hay_ngram = u"" "".join(ngram)

        similarity = SM(None, hay_ngram, needle).ratio()
        if similarity &gt; max_sim_val:
            max_sim_val = similarity
            max_sim_string = hay_ngram
            str = [row[1] , ' ', max_sim_val.__str__(),' ', max_sim_string , '\n']
            str1 = max_sim_string , row[2]
            phrases.append((max_sim_string, row[2]))

for line in hay.splitlines():
    if any(max_sim_string in line for max_sim_string, _ in phrases):
        for phrase in phrases:
            max_sim_string, _ = phrase
            if max_sim_string in line:
                tag_sent = line.replace(max_sim_string, phrase.__str__())
                my3file.writelines(tag_sent + '\n')
                print(tag_sent)
                break        
    else:
        my3file.writelines(line + '\n')

csvFile.close()
</code></pre>
",0,0,120,2020-01-02 19:28:05,https://stackoverflow.com/questions/59568952/tagging-words-in-sentences-using-dictionares
Set validation data in SpaCy NER training,"<p>Is it possible to train SpaCy NER with validation data?
Or split some data to validation set like in Keras (validation_split in model.fit)? Thanks</p>

<pre><code>with nlp.disable_pipes(*other_pipes):  # only train NER
        for itn in tqdm(range(n_iter)):
            random.shuffle(train_data_list)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(train_data_list, size=compounding(8., 64., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                           losses=losses)
</code></pre>
","nlp, spacy, named-entity-recognition","<p>Use the <a href=""https://spacy.io/api/cli#train"" rel=""nofollow noreferrer""><code>spacy train</code> CLI</a> instead of the demo script:</p>

<pre><code>spacy train lang /path/to/output train.json dev.json
</code></pre>

<p>The validation data is used to choose the best model from the training iterations and optionally for early stopping.</p>

<p>The main task is converting your data to spacy's JSON training format, see: <a href=""https://stackoverflow.com/a/59209377/461847"">https://stackoverflow.com/a/59209377/461847</a></p>
",2,0,2124,2020-01-06 08:45:28,https://stackoverflow.com/questions/59609034/set-validation-data-in-spacy-ner-training
Why does Spacy&#39;s NER trainer return tokens but not entities?,"<p>Thanks for looking.  I am trying to train a custom Named Entity Recognizer, using code from Spacy's website.  My problem is that after I run my examples through the trainer, it returns the tokens, but no entities.  Here are my examples, saved in the variable <code>to_train_ents</code>:</p>

<pre class=""lang-py prettyprint-override""><code>[('""We’re at the beginning of what we could do with laser ultrasound,"" says Brian W. Anthony, a principal research scientist in MIT’s Department of Mechanical Engineering and Institute for Medical Engineering and Science (IMES), a senior author on the paper.',
  {'entities': [(72, 88, 'PERSON')]}),
 ('Early concepts for noncontact laser ultrasound for medical imaging originated from a Lincoln Laboratory program established by Rob Haupt of the Active Optical Systems Group and Chuck Wynn of the Advanced Capabilities and Technologies Group, who are co-authors on the new paper along with Matthew Johnson.',
  {'entities': [(126, 135, 'PERSON'),
    (176, 186, 'PERSON'),
    (287, 302, 'PERSON')]}),
 ('From there, the research grew via collaboration with Anthony and his students, Xiang (Shawn) Zhang, who is now an MIT postdoc and is the paper’s first author, and recent doctoral graduate Jonathan Fincke, who is also a co-author.',
  {'entities': [(78, 97, 'PERSON'), (187, 202, 'PERSON')]})]
</code></pre>

<p>From what I can tell, they are formatted correctly to pass into the trainer.  Here is the code used to train the NER model, from spacy.io:</p>

<pre class=""lang-py prettyprint-override""><code>def main(model = None, output_dir = None, n_iter = 100):
    # Load the model, set up the pipeline and train the entity recognizer
    if model is not None:   # If model was specified...
        nlp = spacy.load(model)   # ...load the existing spaCy model
        pprint(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")   # ...otherwise, create a blank language class
        print(""Created blank 'en' model"")

    # Create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:   # If Named Entity Recognition is not part of the pipeline...
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner, last = True)   # ...add it to the pipeline
    else:
        ner = nlp.get_pipe(""ner"")

    # Add labels
    for _, annotations in to_train_ents:
        for ent in annotations.get(""entities""):  # ""get"" is a way of retrieving items from dictionaries
            ner.add_label(ent[2])

    # Get names of other pipes to disable them during training (we want only NER)
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""] # other_pipes is any pipe that is not NER
    with nlp.disable_pipes(*other_pipes):  # Train only NER
        # Reset and initialize the weights randomly - but only if we're training a new model
        if model is None:
            nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(to_train_ents)
            losses = {}
            # Batch up the examples using spaCy's minibatch
            batches = minibatch(to_train_ents, size = compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                texts,  # Batch of texts
                annotations,  # Batch of annotations
                drop = 0.5,  # Dropout - make it harder to memorize data (adjustable variable)
                losses = losses,
                )
            print(""Losses"", losses)

    # Test the trained model
    for text, _ in to_train_ents:
        doc = nlp(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])

    # Save the model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

    # Test the saved model
    print(""Loading from"", output_dir)
    nlp2 = spacy.load(output_dir)
    for text, _ in to_train_ents:
        doc = nlp2(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])
</code></pre>

<p>I tell this function to use the english model and to save in output directory 'nih_ner':</p>

<pre class=""lang-py prettyprint-override""><code>main(model = 'en', output_dir = 'nih_ner')
</code></pre>

<p>Here is the result:</p>

<pre><code>""Loaded model 'en'""
Losses {'ner': 52.71057402440056}
Losses {'ner': 43.944127584481976}
Losses {'ner': 40.92080506101935}
~snip~
Losses {'ner': 8.647840025578502}
Losses {'ner': 0.001753763942560257}
Entities []
Tokens [('From', '', 2), ('there', '', 2), (',', '', 2), ('the', '', 2), ('research', '', 2), ('grew', '', 2), ('via', '', 2), ('collaboration', '', 2), ('with', '', 2), ('Anthony', '', 2), ('and', '', 2), ('his', '', 2), ('students', '', 2), (',', '', 2), ('Xiang', '', 2), ('(', '', 2), ('Shawn', '', 2), (')', '', 2), ('Zhang', '', 2), (',', '', 2), ('who', '', 2), ('is', '', 2), ('now', '', 2), ('an', '', 2), ('MIT', '', 2), ('postdoc', '', 2), ('and', '', 2), ('is', '', 2), ('the', '', 2), ('paper', '', 2), ('’s', '', 2), ('first', '', 2), ('author', '', 2), (',', '', 2), ('and', '', 2), ('recent', '', 2), ('doctoral', '', 2), ('graduate', '', 2), ('Jonathan', '', 2), ('Fincke', '', 2), (',', '', 2), ('who', '', 2), ('is', '', 2), ('also', '', 2), ('a', '', 2), ('co', '', 2), ('-', '', 2), ('author', '', 2), ('.', '', 2)]
Entities []
Tokens [('""', '', 2), ('We', '', 2), ('’re', '', 2), ('at', '', 2), ('the', '', 2), ('beginning', '', 2), ('of', '', 2), ('what', '', 2), ('we', '', 2), ('could', '', 2), ('do', '', 2), ('with', '', 2), ('laser', '', 2), ('ultrasound', '', 2), (',', '', 2), ('""', '', 2), ('says', '', 2), ('Brian', '', 2), ('W.', '', 2), ('Anthony', '', 2), (',', '', 2), ('a', '', 2), ('principal', '', 2), ('research', '', 2), ('scientist', '', 2), ('in', '', 2), ('MIT', '', 2), ('’s', '', 2), ('Department', '', 2), ('of', '', 2), ('Mechanical', '', 2), ('Engineering', '', 2), ('and', '', 2), ('Institute', '', 2), ('for', '', 2), ('Medical', '', 2), ('Engineering', '', 2), ('and', '', 2), ('Science', '', 2), ('(', '', 2), ('IMES', '', 2), (')', '', 2), (',', '', 2), ('a', '', 2), ('senior', '', 2), ('author', '', 2), ('on', '', 2), ('the', '', 2), ('paper', '', 2), ('.', '', 2)]
Entities []
Tokens [('Early', '', 2), ('concepts', '', 2), ('for', '', 2), ('noncontact', '', 2), ('laser', '', 2), ('ultrasound', '', 2), ('for', '', 2), ('medical', '', 2), ('imaging', '', 2), ('originated', '', 2), ('from', '', 2), ('a', '', 2), ('Lincoln', '', 2), ('Laboratory', '', 2), ('program', '', 2), ('established', '', 2), ('by', '', 2), ('Rob', '', 2), ('Haupt', '', 2), ('of', '', 2), ('the', '', 2), ('Active', '', 2), ('Optical', '', 2), ('Systems', '', 2), ('Group', '', 2), ('and', '', 2), ('Chuck', '', 2), ('Wynn', '', 2), ('of', '', 2), ('the', '', 2), ('Advanced', '', 2), ('Capabilities', '', 2), ('and', '', 2), ('Technologies', '', 2), ('Group', '', 2), (',', '', 2), ('who', '', 2), ('are', '', 2), ('co', '', 2), ('-', '', 2), ('authors', '', 2), ('on', '', 2), ('the', '', 2), ('new', '', 2), ('paper', '', 2), ('along', '', 2), ('with', '', 2), ('Matthew', '', 2), ('Johnson', '', 2), ('.', '', 2)]
Saved model to nih_ner
Loading from nih_ner
Entities []
Tokens [('From', '', 2), ('there', '', 2), (',', '', 2), ('the', '', 2), ('research', '', 2), ('grew', '', 2), ('via', '', 2), ('collaboration', '', 2), ('with', '', 2), ('Anthony', '', 2), ('and', '', 2), ('his', '', 2), ('students', '', 2), (',', '', 2), ('Xiang', '', 2), ('(', '', 2), ('Shawn', '', 2), (')', '', 2), ('Zhang', '', 2), (',', '', 2), ('who', '', 2), ('is', '', 2), ('now', '', 2), ('an', '', 2), ('MIT', '', 2), ('postdoc', '', 2), ('and', '', 2), ('is', '', 2), ('the', '', 2), ('paper', '', 2), ('’s', '', 2), ('first', '', 2), ('author', '', 2), (',', '', 2), ('and', '', 2), ('recent', '', 2), ('doctoral', '', 2), ('graduate', '', 2), ('Jonathan', '', 2), ('Fincke', '', 2), (',', '', 2), ('who', '', 2), ('is', '', 2), ('also', '', 2), ('a', '', 2), ('co', '', 2), ('-', '', 2), ('author', '', 2), ('.', '', 2)]
Entities []
Tokens [('""', '', 2), ('We', '', 2), ('’re', '', 2), ('at', '', 2), ('the', '', 2), ('beginning', '', 2), ('of', '', 2), ('what', '', 2), ('we', '', 2), ('could', '', 2), ('do', '', 2), ('with', '', 2), ('laser', '', 2), ('ultrasound', '', 2), (',', '', 2), ('""', '', 2), ('says', '', 2), ('Brian', '', 2), ('W.', '', 2), ('Anthony', '', 2), (',', '', 2), ('a', '', 2), ('principal', '', 2), ('research', '', 2), ('scientist', '', 2), ('in', '', 2), ('MIT', '', 2), ('’s', '', 2), ('Department', '', 2), ('of', '', 2), ('Mechanical', '', 2), ('Engineering', '', 2), ('and', '', 2), ('Institute', '', 2), ('for', '', 2), ('Medical', '', 2), ('Engineering', '', 2), ('and', '', 2), ('Science', '', 2), ('(', '', 2), ('IMES', '', 2), (')', '', 2), (',', '', 2), ('a', '', 2), ('senior', '', 2), ('author', '', 2), ('on', '', 2), ('the', '', 2), ('paper', '', 2), ('.', '', 2)]
Entities []
Tokens [('Early', '', 2), ('concepts', '', 2), ('for', '', 2), ('noncontact', '', 2), ('laser', '', 2), ('ultrasound', '', 2), ('for', '', 2), ('medical', '', 2), ('imaging', '', 2), ('originated', '', 2), ('from', '', 2), ('a', '', 2), ('Lincoln', '', 2), ('Laboratory', '', 2), ('program', '', 2), ('established', '', 2), ('by', '', 2), ('Rob', '', 2), ('Haupt', '', 2), ('of', '', 2), ('the', '', 2), ('Active', '', 2), ('Optical', '', 2), ('Systems', '', 2), ('Group', '', 2), ('and', '', 2), ('Chuck', '', 2), ('Wynn', '', 2), ('of', '', 2), ('the', '', 2), ('Advanced', '', 2), ('Capabilities', '', 2), ('and', '', 2), ('Technologies', '', 2), ('Group', '', 2), (',', '', 2), ('who', '', 2), ('are', '', 2), ('co', '', 2), ('-', '', 2), ('authors', '', 2), ('on', '', 2), ('the', '', 2), ('new', '', 2), ('paper', '', 2), ('along', '', 2), ('with', '', 2), ('Matthew', '', 2), ('Johnson', '', 2), ('.', '', 2)]
</code></pre>

<p>As you can see, the model returns the tokens to me, but there are empty lists, [], where the recognized entities should be.  Any suggestions as to why this is happening would be helpful.</p>

<p>Thanks again!</p>
","python, nlp, spacy, named-entity-recognition","<p>The problem is with the <em>start</em> and <em>end</em> character indices in your training data.
<a href=""https://en.wikipedia.org/wiki/Zero-based_numbering"" rel=""nofollow noreferrer"">Zero-based numbering</a> must be used and not <em>1-based numbering</em>.</p>

<p>With <em>zero-based numbering</em> the index of the first character in a string is 0, the index of the second character is 1, etc ..</p>

<p>The following code shows that you offsets are using <em>1-based numbering</em>  </p>

<pre><code>l = []
for a in to_train_ents:
    sentence = a[0]
    for b in a[1]['entities']:
        l.append( sentence[int(b[0]): int(b[1])])
print(l)
# [' Brian W. Anthon', ' Rob Haup', ' Chuck Wyn', ' Matthew Johnso', ' Xiang (Shawn) Zhan', ' Jonathan Finck']

</code></pre>

<p>Using <em>zero-based numbering</em> the training data becomes:</p>

<pre><code>to_train_ents = [('""We’re at the beginning of what we could do with laser ultrasound,"" says Brian W. Anthony, a principal research scientist in MIT’s Department of Mechanical Engineering and Institute for Medical Engineering and Science (IMES), a senior author on the paper.',
  {'entities': [(73, 89, 'PERSON')]}),
 ('Early concepts for noncontact laser ultrasound for medical imaging originated from a Lincoln Laboratory program established by Rob Haupt of the Active Optical Systems Group and Chuck Wynn of the Advanced Capabilities and Technologies Group, who are co-authors on the new paper along with Matthew Johnson.',
  {'entities': [(127, 136, 'PERSON'),
    (177, 187, 'PERSON'),
    (288, 303, 'PERSON')]}),
 ('From there, the research grew via collaboration with Anthony and his students, Xiang (Shawn) Zhang, who is now an MIT postdoc and is the paper’s first author, and recent doctoral graduate Jonathan Fincke, who is also a co-author.',
  {'entities': [(79, 98, 'PERSON'), (188, 203, 'PERSON')]})]
</code></pre>

<p>Now the model trains and predicts correctly:</p>

<pre><code>Losses {'ner': 124.16665458679199}
Losses {'ner': 118.29711055755615}
Losses {'ner': 110.27205085754395}
Losses {'ner': 102.67473244667053}
Losses {'ner': 93.6117731332779}
Losses {'ner': 80.32513558864594}
...
Losses {'ner': 1.56542471502621e-07}
Losses {'ner': 2.071446077606498e-09}
Losses {'ner': 3.4424366409273253e-13}
Losses {'ner': 5.749029666370928e-09}
...
Entities [('Brian W. Anthony', 'PERSON')]
Entities [('Xiang (Shawn) Zhang', 'PERSON'), ('Jonathan Fincke', 'PERSON')]
Entities [('Rob Haupt', 'PERSON'), ('Chuck Wynn', 'PERSON'), ('Matthew Johnson', 'PERSON')]

</code></pre>
",2,0,701,2020-01-09 18:13:11,https://stackoverflow.com/questions/59669913/why-does-spacys-ner-trainer-return-tokens-but-not-entities
Customized StanfordNER,"<p>I am trying to build a customized StanfordNer model, training data and properties file are ready.<br>
But when I am trying to run the following code :</p>

<pre><code>java -cp ""stanford-ner.jar:lib/*"" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop download.txt
</code></pre>

<p>This error is popping out :</p>

<blockquote>
  <p>Error: Could not find or load main class
  edu.stanford.nlp.ie.crf.CRFClassifier</p>
</blockquote>

<p><em>Steps followed:</em></p>

<ol>
<li>Downloaded and extracted stanford-ner-2018-10-16.zip file.<br></li>
<li>Java 8 installed and $JAVA_HOME has been set.<br></li>
<li>The properties file (download.txt) has been placed in the folder where stanford-ner-2018-10-16.zip is extracted.</li>
</ol>
","java, machine-learning, nlp, stanford-nlp, named-entity-recognition","<p>If you are seeing errors like that it means your CLASSPATH is not properly configured.</p>

<p>You need to run that command in the same folder as the NER download or it won't find the needed jars.  That command should be run in whatever directory has <code>stanford-ner.jar</code> and <code>lib</code> in it.  Alternatively you can just set the <code>CLASSPATH</code> environment variable and remove the <code>-cp</code> option from the command.</p>

<p>More info on Java <code>CLASSPATH</code> here: <a href=""https://docs.oracle.com/javase/tutorial/essential/environment/paths.html"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/tutorial/essential/environment/paths.html</a></p>
",1,2,99,2020-01-10 12:37:27,https://stackoverflow.com/questions/59681871/customized-stanfordner
StanfordNLP to detect compound entities with prepositions,"<p>Basically, in the sentence:</p>

<pre><code>&lt;Lord of the bracelets&gt; is a fantasy movie.
</code></pre>

<p>I would like to detect the compound <code>Lord of the bracelets</code> as one entity (that could be linked in the entitylink annotator as well). This means detecting structures with POS tags of a form like <code>NNP</code> <code>DT</code> <code>NNP</code> or <code>NN</code> <code>IN</code> <code>DT</code> <code>NNP</code>.</p>

<p>Is this possible with CoreNLP?</p>

<p>My current setup doesn't detect them, and I couldn't find a way to do it.</p>

<pre class=""lang-java prettyprint-override""><code>
  public NamedEntityRecognition() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitylink"");
    props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");

    pipeline = new StanfordCoreNLP(props);
  }


  public CoreDocument recogniseEntities(String text) {
    CoreDocument doc = new CoreDocument(text);
    pipeline.annotate(doc);
    return doc;
  }

</code></pre>

<p>Thanks!</p>
","java, stanford-nlp, named-entity-recognition","<p>While @StanfordNLPHelp's answer was helpful, I thought I would add some more details into what my final solution was.</p>

<h1>Option 1:</h1>

<p>Add a <a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer""><em>TokensRegex</em> annotator</a> as pointed out by the previous answer. This adds a more customisable annotator to the pipeline, and you can specify your own rules in a text file.</p>

<p>This is what my rules file (extended_ner.rules) looks like:</p>

<pre><code># these Java classes will be used by the rules
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

# rule for recognizing compound names
{ ruleType: ""tokens"", pattern: ([{tag:""NN""}] [{tag:""IN""}] [{tag:""DT""}] [{tag:""NNP""}]), action: Annotate($0, ner, ""COMPOUND""), result: ""COMPOUND_RESULT"" }
</code></pre>

<p>You can see a breakdown of the rules sintax <a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html#tokens-rules-patterns"" rel=""nofollow noreferrer"">here</a>.</p>

<p><strong>Note:</strong> The TokensRegex annotator <em>must</em> be added after the ner annotator. Otherwise, the results will be overwritten.</p>

<p>This is what the Java code would look like:</p>

<pre class=""lang-java prettyprint-override""><code> public NamedEntityRecognition() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,tokensregex,entitylink"");
    props.setProperty(""tokensregex.rules"", ""extended_ner.rules"");
    props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");

    pipeline = new StanfordCoreNLP(props);
  }
</code></pre>

<h1>Option 2 (Chosen one)</h1>

<p>Instead of adding another annotator, the rules file can be sent to the ner annotator via de <code>""ner.additional.tokensregex.rules""</code> property. <a href=""https://stanfordnlp.github.io/CoreNLP/ner.html#additional-tokensregex-rules"" rel=""nofollow noreferrer"">Here</a> are the docs.</p>

<p>I chose this option because it seems simpler, and adding another annotator to the pipeline seemed a bit overdone for my case.</p>

<p>The rules file is exactly the same as in option 1, the java code now is:</p>

<pre class=""lang-java prettyprint-override""><code> public NamedEntityRecognition() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,entitylink"");
    props.setProperty(""ner.additional.tokensregex.rules"", ""extended_ner.rules"");

    props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");

    pipeline = new StanfordCoreNLP(props);
  }
</code></pre>

<p><strong>Note:</strong> For this to work, the property <code>""ner.applyFineGrained""</code> must be true (default value).</p>
",0,3,209,2020-01-16 10:14:10,https://stackoverflow.com/questions/59767325/stanfordnlp-to-detect-compound-entities-with-prepositions
NER using Spacy library not giving correct result on resume parser,"<p>I am using SpaCY's named entity recognition to extract the Name, Organization etc from a resume.
Here is my python code.</p>

<pre><code>import spacy
import PyPDF2
mypdf = open('C:\\Users\\akjain\\Downloads\\Resume\\Al Mal Capital_Nader El Boustany_BD Manager.pdf', mode='rb')
pdf_document = PyPDF2.PdfFileReader(mypdf)
first_page = pdf_document.getPage(0)
nlp = spacy.load('en_core_web_sm') 
text = first_page.extractText()
doc = nlp(text)   
for ent in doc.ents: 
    print(ent.text, ent.label_) 
</code></pre>

<p>If I see the output, it does not look great. 
Name is not correctly identified. Last name is considered as Org name, Dubai is treated as Person and so on.</p>

<p><a href=""https://i.sstatic.net/fNCr0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fNCr0.jpg"" alt=""enter image description here""></a></p>

<p>Here is the snapshot of resume which I got it from a public dataset.</p>

<p>I want to extract candidate name, Organization, location etc from set of resumes. When I read the documentation it says accuracy is more than 95% using spaCy. However in my case it is not.
Is there any way to improve the accuracy of feature extraction?</p>

<p><a href=""https://i.sstatic.net/5naeV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5naeV.jpg"" alt=""enter image description here""></a></p>
","python, nlp, spacy, named-entity-recognition","<p>The spaCy NER <a href=""https://spacy.io/models/en#en_core_web_sm"" rel=""noreferrer"">model</a> is trained on the OntoNotes corpus, which is a collection of   telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, and weblogs. These type of texts all mainly contain full sentences, which is quite different than the resumes that you're training on. For instance, the entity ""Dubai"" has no grammatical context surrounding it, making it very difficult for this particular model to recognize it as a location. It is used to seeing sentences like ""... while he was traveling in Dubai, ..."". In general, Machine Learning performance is always bound to the specific problem domain you're training and evaluating your models on.</p>

<p>You could try running this with <code>en_core_web_md</code> or <code>en_core_web_lg</code> which are performing slightly better on OntoNotes, but will still not perform well on your specific domain texts.</p>

<p>To try and improve upon the accuracy, I would recommend further refining the existing model by annotating a set of resumes yourself, and feeding that training data back into the model. See the documentation <a href=""https://spacy.io/usage/training#example-train-ner"" rel=""noreferrer"">here</a>. I'm not certain how well this will work however, because like I said resumes are just harder because they have less context from sentences.</p>
",6,1,2606,2020-01-27 06:56:06,https://stackoverflow.com/questions/59926339/ner-using-spacy-library-not-giving-correct-result-on-resume-parser
Training Spacy NER on custom dataset gives error,"<p>I am trying to train spacy NER model on custom dataset. Basically I want to use this model to extract Name, Organization, Email, phone number etc from resume.</p>

<p>Below is the code I am using. </p>

<pre><code>import json
import random
import spacy
import sys
import logging
from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_fscore_support
from spacy.gold import GoldParse
from spacy.scorer import Scorer
from sklearn.metrics import accuracy_score
from spacy.gold import biluo_tags_from_offsets
def convert_dataturks_to_spacy(dataturks_JSON_FilePath):

    try:
        training_data = []
        lines=[]
        with open(dataturks_JSON_FilePath, encoding='utf-8') as f:
            lines = f.readlines()
        for line in lines:
            data = json.loads(line)
            text = data['content']
            entities = []
            for annotation in data['annotation']:
                #only a single point in text annotation.
                point = annotation['points'][0]
                labels = annotation['label']
                if not isinstance(labels, list):
                    labels = [labels]

                for label in labels:
                    entities.append((point['start'], point['end'] + 1 ,label))


            training_data.append((text, {""entities"" : entities}))
        return training_data
    except Exception as e:
        logging.exception(""Unable to process "" + dataturks_JSON_FilePath + ""\n"" + ""error = "" + str(e))
        return None

def reformat_train_data(tokenizer, examples):
    output = []
    for i, (text, entity_offsets) in enumerate(examples):
        doc = tokenizer(text.strip())
        ner_tags = biluo_tags_from_offsets(tokenizer(text), entity_offsets['entities'])
        words = [w.text for w in doc]
        tags = ['-'] * len(doc)
        heads = [0] * len(doc)
        deps = [''] * len(doc)
        sentence = (range(len(doc)), words, tags, heads, deps, ner_tags)
        output.append((text, [(sentence, [])]))
    print(""output"",output)
    return output
################### Train Spacy NER.###########
def train_spacy():
    TRAIN_DATA = convert_dataturks_to_spacy(""C:\\Users\\akjain\\Downloads\\Entity-Recognition-In-Resumes-SpaCy-master\\traindata.json"")
    nlp = spacy.blank(""en"")  
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)    
    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    def get_data(): return reformat_train_data(nlp.tokenizer, TRAIN_DATA)
    optimizer = nlp.begin_training(get_data)
    for itn in range(10):
        print(""Starting iteration "" + str(itn))
        random.shuffle(TRAIN_DATA)
        losses = {}
        for text, annotations in TRAIN_DATA:
            nlp.update(
                [text],  # batch of texts
                [annotations],  # batch of annotations
                drop=0.2,  # dropout - make it harder to memorise data
                sgd=optimizer,  # callable to update weights
                losses=losses)
        print(losses)
train_spacy()
</code></pre>

<p>I am getting the below error. Also, I came across a link (<a href=""https://github.com/explosion/spaCy/issues/3558"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/3558</a>)  with some suggestion to fix this code. But even after implementing that I am still getting error.</p>

<p>I am using Python 3.6.5 and Spacy 2.2.3</p>

<p><a href=""https://i.sstatic.net/58QRH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/58QRH.jpg"" alt=""enter image description here""></a></p>

<p>Dataset:</p>

<pre><code>{""content"": ""Nida Khan\nTech Support Executive - Teleperformance for Microsoft\n\nJaipur, Rajasthan - Email me on Indeed: indeed.com/r/Nida-Khan/6c9160696f57efd8\n\n• To be an integral part of the organization and enhance my knowledge to utilize it in a productive\nmanner for the growth of the company and the global.\n\nINDUSTRIAL TRAINING\n\n• BHEL, (HEEP) HARIDWAR\nOn CNC System&amp;amp; PLC Programming.\n\nWORK EXPERIENCE\n\nTech Support Executive\n\nTeleperformance for Microsoft -\n\nSeptember 2017 to Present\n\nprocess.\n• 21 months of experience in ADFC as Phone Banker.\n\nEDUCATION\n\nBachelor of Technology in Electronics &amp; communication Engg\n\nGNIT institute of Technology -  Lucknow, Uttar Pradesh\n\n2008 to 2012\n\nClass XII\n\nU.P. Board -  Bareilly, Uttar Pradesh\n\n2007\n\nClass X\n\nU.P. Board -  Bareilly, Uttar Pradesh\n\n2005\n\nSKILLS\n\nMicrosoft office, excel, cisco, c language, cbs. (4 years)\n\nhttps://www.indeed.com/r/Nida-Khan/6c9160696f57efd8?isid=rex-download&amp;ikw=download-top&amp;co=IN"",""annotation"":[{""label"":[""Email Address""],""points"":[{""start"":872,""end"":910,""text"":""indeed.com/r/Nida-Khan/6c9160696f57efd8""}]},{""label"":[""Skills""],""points"":[{""start"":800,""end"":857,""text"":""Microsoft office, excel, cisco, c language, cbs. (4 years)""}]},{""label"":[""Graduation Year""],""points"":[{""start"":676,""end"":679,""text"":""2012""}]},{""label"":[""College Name""],""points"":[{""start"":612,""end"":640,""text"":""GNIT institute of Technology ""}]},{""label"":[""Degree""],""points"":[{""start"":552,""end"":609,""text"":""Bachelor of Technology in Electronics &amp; communication Engg""}]},{""label"":[""Companies worked at""],""points"":[{""start"":420,""end"":448,""text"":""Teleperformance for Microsoft""}]},{""label"":[""Designation""],""points"":[{""start"":395,""end"":417,""text"":""\nTech Support Executive""}]},{""label"":[""Email Address""],""points"":[{""start"":106,""end"":144,""text"":""indeed.com/r/Nida-Khan/6c9160696f57efd8""}]},{""label"":[""Location""],""points"":[{""start"":66,""end"":71,""text"":""Jaipur""}]},{""label"":[""Companies worked at""],""points"":[{""start"":35,""end"":63,""text"":""Teleperformance for Microsoft""}]},{""label"":[""Designation""],""points"":[{""start"":10,""end"":32,""text"":""Tech Support Executive ""}]},{""label"":[""Designation""],""points"":[{""start"":9,""end"":31,""text"":""\nTech Support Executive""}]},{""label"":[""Name""],""points"":[{""start"":0,""end"":8,""text"":""Nida Khan""}]}]}
</code></pre>
","python, spacy, named-entity-recognition","<p>The problem is you are feeding training data to model optimizer.</p>

<p>As mentioned in <a href=""https://github.com/explosion/spaCy/issues/3558"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/3558</a>, use the following function to remove leading and trailing white spaces from entity spans.</p>

<pre><code>def trim_entity_spans(data: list) -&gt; list:
    """"""Removes leading and trailing white spaces from entity spans.

    Args:
    data (list): The data to be cleaned in spaCy JSON format.

    Returns:
    list: The cleaned data.
    """"""
    invalid_span_tokens = re.compile(r'\s')

    cleaned_data = []
    for text, annotations in data:
        entities = annotations['entities']
        valid_entities = []
        for start, end, label in entities:
            valid_start = start
            valid_end = end
            # if there's preceding spaces, move the start position to nearest character
            while valid_start &lt; len(text) and invalid_span_tokens.match(
                    text[valid_start]):
                valid_start += 1
            while valid_end &gt; 1 and invalid_span_tokens.match(
                    text[valid_end - 1]):
                valid_end -= 1
            valid_entities.append([valid_start, valid_end, label])
        cleaned_data.append([text, {'entities': valid_entities}])
    return cleaned_data
</code></pre>

<p>Then use the following function for training:</p>

<pre><code>def train_spacy():

    TRAIN_DATA = convert_dataturks_to_spacy(""C:\\Users\\akjain\\Downloads\\Entity-Recognition-In-Resumes-SpaCy-master\\traindata.json"")
    TRAIN_DATA = trim_entity_spans(TRAIN_DATA)
    nlp = spacy.blank('en')  # create blank Language class
    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)


    # add labels
    for _, annotations in TRAIN_DATA:
         for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(10):
            print(""Statring iteration "" + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                [text],  # batch of texts
                [annotations],  # batch of annotations
                drop=0.2,  # dropout - make it harder to memorise data
                sgd=optimizer,  # callable to update weights
                losses=losses)
            print(losses)
</code></pre>
",1,1,4334,2020-02-04 15:51:01,https://stackoverflow.com/questions/60061024/training-spacy-ner-on-custom-dataset-gives-error
How do I get the text after a date using python?,"<p>I have a string which might have a date of some form inside it. </p>

<p>I want to extract the portion appearing just after that date.</p>

<p>For example:</p>

<pre><code>s = ""The term starts on Jan 1, 2000 and terminates on""
</code></pre>

<p>should return</p>

<pre><code>output = ""and terminates on""
</code></pre>

<p>My suggestion would be </p>

<pre><code>s[s.find(', 2') + 6]
</code></pre>
","python, string, date, named-entity-recognition","<p>Try using <a href=""https://dateutil.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">dateutil</a> with <code>fuzzy_with_tokens=True</code></p>

<p><strong>Ex:</strong></p>

<pre><code>from dateutil.parser import parse
s = ""The term starts on Jan 1, 2000 and terminates on""
print(parse(s, fuzzy_with_tokens=True))
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>(datetime.datetime(2000, 1, 1, 0, 0), ('The term starts on ', ' ', ' ', 'and terminates on'))
</code></pre>
",1,0,63,2020-02-06 09:23:14,https://stackoverflow.com/questions/60091502/how-do-i-get-the-text-after-a-date-using-python
Spacy - Entity Linking using descriptions from Wikipedia,"<p>I am using the example from here: <a href=""https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking</a>.</p>

<p>There is a flag for using descriptions from Wikipedia instead of Wikidata. I set this to True (it should get descriptions from the Wikipedia data). But looking at the code under the Wikidata section, </p>

<pre><code>if not descr_from_wp:
    logger.info(""STEP 4c: Writing Wikidata entity descriptions to {}"".format(entity_descr_path))
    io.write_id_to_descr(entity_descr_path, id_to_descr)
</code></pre>

<p>This should not run because the <code>if</code> statement is <code>False</code>. But under the Wikipedia section,</p>

<pre><code>if descr_from_wp:
    logger.info(""STEP 5b: Parsing and writing Wikipedia descriptions to {}"".format(entity_descr_path))
</code></pre>

<p>It just logs something -- it doesn't actually seem to create the descriptions. And the output file has the headers: <code>WD_id|description</code>.</p>

<p>How can I get it to write the Wikipedia descriptions?</p>
","python, spacy, named-entity-recognition","<p>I believe all the action happens in the line before the one you quoted:</p>
<pre><code>wp.create_training_and_desc(wp_xml, entity_defs_path, entity_descr_path, 
training_entities_path, descr_from_wp, limit_train)
</code></pre>
<p>(this is [https://github.com/explosion/projects/blob/master/nel-wikipedia/wikidata_pretrain_kb.py#L142])</p>
<p>That funtion is one file over, at <a href=""https://github.com/explosion/projects/blob/master/nel-wikipedia/wikidata_processor.py#L176"" rel=""nofollow noreferrer"">https://github.com/explosion/projects/blob/master/nel-wikipedia/wikidata_processor.py#L176</a>:</p>
<pre><code>def create_training_and_desc(
    wp_input, def_input, desc_output, training_output, parse_desc, limit=None
):
    wp_to_id = io.read_title_to_id(def_input)
    _process_wikipedia_texts(
        wp_input, wp_to_id, desc_output, training_output, parse_desc, limit
    )
</code></pre>
<p>That being said, having gone through this process a few days ago, I did get the impression that it's all in flux and there may be a bit of a mismatch between descriptions, the actual code, and versions of spacy. You may have noticed that the Readme starts with the instruction &quot;Run wikipedia_pretrain_kb.py&quot;. And yet, such a file does not exist, only wikidata_pretrain_kb.py.</p>
<p>While the process did work (ventually), the final training progresses at a glacial speed of 10 seconds per example. For 300,000 examples in the training set, that would imply about a year of training at the default 10 epochs.</p>
<p>There are some instructions that suggest one isn't intended to run <em>all</em> the training data that's available. But in that case it seems strange to run 10 epochs on a repeating set of data with diminishing rates of return.</p>
<p>(Updated URLs Nov 2020. This example did not make it over from v2 -&gt; v3 (yet?))</p>
",3,1,824,2020-02-11 16:22:52,https://stackoverflow.com/questions/60173314/spacy-entity-linking-using-descriptions-from-wikipedia
Does the notation of a named entity label type in spacy have to match with the notation of the annotated label type in the training data?,"<p>I want to train the NER-Model by spaCy on my own corpus, which was annotated via WebAnno. Unfortunately, the notation of one NE category in spaCy does not match with the respective notation in WebAnno: In WebAnno, the label is ""OTH"" whereas spaCy labels it ""MISC"" (semantically, it's the same). Would this affect the training process or the test accuracy in a negative way? Is it necessary to train an additional NE type ""OTH"" in this case? Thank you for your help!</p>

<p>spaCy version used: 2.2.5</p>
","spacy, training-data, named-entity-recognition, webanno","<p>Yes, of course you want to keep annotations aligned. If it's a one-off operation, it might be easiest to brute-force the problem by replacing the string in your data. </p>

<p>The more canonical option would appear to be TagMap: <a href=""https://spacy.io/usage/adding-languages#tag-map"" rel=""nofollow noreferrer"">https://spacy.io/usage/adding-languages#tag-map</a>. Quote:</p>

<blockquote>
  <p>[...] you need to define how [your tags] map down to the Universal
  Dependencies tag set.</p>
</blockquote>

<p>Their example:</p>

<pre><code>from ..symbols import POS, NOUN, VERB, DET

TAG_MAP = {
    ""NNS"":  {POS: NOUN, ""Number"": ""plur""},
    ""VBG"":  {POS: VERB, ""VerbForm"": ""part"", ""Tense"": ""pres"", ""Aspect"": ""prog""},
    ""DT"":   {POS: DET}
}
</code></pre>
",0,0,183,2020-02-11 20:10:46,https://stackoverflow.com/questions/60176633/does-the-notation-of-a-named-entity-label-type-in-spacy-have-to-match-with-the-n
How can I run this Polyglot token/tag extractor in PyCharm?,"<p>I am evaluating various named entity recognition (NER) libraries, and I'm trying out <a href=""https://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html#example"" rel=""nofollow noreferrer"">Polyglot</a>. </p>

<p>Everything seems to be going well, but the instructions tell me to use this line in the command prompt:</p>

<pre><code>!polyglot --lang en tokenize --input testdata/cricket.txt |  polyglot --lang en ner | tail -n 20
</code></pre>

<p>...which should give (in the example) this output:</p>

<pre><code>,               O
which           O
was             O
equalled        O
five            O
days            O
ago             O
by              O
South           I-LOC
Africa          I-LOC
in              O
their           O
victory         O
over            O
West            I-ORG
Indies          I-ORG
in              O
Sydney          I-LOC
.               O
</code></pre>

<p>That's exactly the kind of output I need for my project, and it works exactly like I need it to work; however, I need to run that within my PyCharm interface, not the command line, and store the results in a pandas dataframe.  How do I translate that command?</p>
","command-line, pycharm, named-entity-recognition, polyglot","<p>Assuming polyglot is installed correctly and proper environment is selected in pycharm. If not install polyglot in a <code>new conda environment</code> with necessary requirements. Create a new project and select that existing conda environment in pycharm. If <code>language embeddings</code>, <code>ner</code> models are not <code>downloaded</code> then they should be downloaded.</p>

<p><strong>Code:</strong></p>

<pre><code>from polyglot.text import Text

blob = """""", which was equalled five days ago by South Africa in the victory over West Indies in Sydney.""""""
text = Text(blob)
text.language = ""en""


## As list all detected entities
print(""As list all detected entities"")
print(text.entities)

print()

## Separately shown detected entities
print(""Separately shown detected entities"")
for entity in text.entities:
    print(entity.tag, entity)

print()

## Tokenized words of sentence
print(""Tokenized words of sentence"")
print(text.words)

print()

## For each token try named entity recognition.
## Not very reliable it detects some words as not English and tries other languages.
## If other embeddings are not installed or text.language = ""en"" is commented then it may give error.
print(""For each token try named entity recognition"")
for word in text.words:
    text = Text(word)
    text.language = ""en""

    ## Separately
    for entity in text.entities:
        print(entity.tag, entity)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>As list all detected entities
[I-LOC(['South', 'Africa']), I-ORG(['West', 'Indies']), I-LOC(['Sydney'])]

Separately shown detected entities
I-LOC ['South', 'Africa']
I-ORG ['West', 'Indies']
I-LOC ['Sydney']

Tokenized words of sentence
[',', 'which', 'was', 'equalled', 'five', 'days', 'ago', 'by', 'South', 'Africa', 'in', 'the', 'victory', 'over', 'West', 'Indies', 'in', 'Sydney', '.']

For each token try named entity recognition
I-LOC ['Africa']
I-PER ['Sydney']
</code></pre>
",1,0,146,2020-02-13 17:30:54,https://stackoverflow.com/questions/60213501/how-can-i-run-this-polyglot-token-tag-extractor-in-pycharm
Remove Named Entities from the spacy object,"<p>I am trying to remove Named Entities from the document using Spacy. I didn't find any troubles to recognize the named entities. used this code:</p>

<pre><code>ne = [(ent.text, ent.label_) for ent in doc.ents]
print(ne)
persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']
print(persons)
</code></pre>

<p>Output:</p>

<pre><code>'Timothy D. Cook',
 'Peter',
 'Peter',
 'Benjamin A. Reitzes',
 'Timothy D. Cook',
 'Steve Milunovich',
 'Steven Mark Milunovich',
 'Peter',
 'Luca Maestri'
</code></pre>

<p>But then I am trying to use this chunk to actually remove them from the document:</p>

<pre><code>text_no_namedentities = []

ents = [e.text for e in doc.ents]
for item in doc:
    if item.text in ents:
        pass
    else:
        text_no_namedentities.append(item.text)
print("" "".join(text_no_namedentities))
</code></pre>

<p>It does not work, since the NE are n-grams. And if I just check the contents of a little chunk of spacy object it is as follows:</p>

<pre><code>for item in doc:
    print(item.text)

iPad
has
a
78
%
Steve
Milunovich
share
of
the
U.S.
commercial
tablet
market
</code></pre>

<p>So the spacy object is tokenized. Hence I can't remove the NEs with my code above. Any ideas on how I can remove all the named entities from the object? </p>
","python, nlp, spacy, named-entity-recognition","<p>The condition you want to check on is </p>

<pre><code>if item.ent_type:
</code></pre>

<p>This will evaluate to <code>True</code> if the <code>item</code> (""token"") is part of a named entity. <code>token.ent_type</code> will be a hash ID of the actual type of the entity, which you can query with <code>token.ent_type_</code> (note the _).</p>

<p>This would be the code I'd use:</p>

<pre><code>    text_no_namedentities = """"
    for token in doc:
        if not token.ent_type:
            text_no_namedentities += token.text
            if token.whitespace_:
                text_no_namedentities += "" ""
</code></pre>

<p>Note that you can use <code>token.whitespace_</code> to determine whether or not the original token in the original sentence was followed by a space or not.</p>

<p>For more information, see the docs on <code>Token</code> <a href=""https://spacy.io/api/token#attributes"" rel=""nofollow noreferrer"">here</a>.</p>

<p>FYI - for the future, it would be more convenient to include a working minimal snippet of your code, instead of just parts of it.</p>
",3,3,2540,2020-02-23 18:17:58,https://stackoverflow.com/questions/60365350/remove-named-entities-from-the-spacy-object
How to modify or retrain existing OpenNLP models?,"<p>Is there any way to retrain existing OpenNLP models?? i.e to append new items to the existing models from OpenNLP ?</p>

<p>Suppose I want to add few new entries to existing <strong>en-ner-date.bin</strong> because some of the words are not getting detected as date.</p>

<p><em>Note: I don't want to make new model. I just want to modify the existing one...</em></p>

<p>I have seen something like model builder-add on but there is no concrete example about how to use it.</p>

<p>Any help will be appreciated.</p>
","java, nlp, opennlp, named-entity-recognition","<p>You can <em>not</em> simple manipulate existing binary OpenNLP model files. You have to train your own model(s) with the specific capabilities, that is, detecting named entities seen in text samples from (your) training. See hint on the <a href=""https://opennlp.apache.org/models.html"" rel=""nofollow noreferrer"">OpenNLP model download page</a>:</p>
<blockquote>
<p>The models can be used for testing or getting started. Please train your own models for all other use cases.</p>
</blockquote>
<p>Moreover, quoting the <em>Apache OpenNLP</em> developer Manual:</p>
<blockquote>
<p>The pre-trained models might not be available for a desired language, can not detect important entities or the performance is not good enough outside the news domain. These are the typical reason to do <strong>custom</strong> training of the name finder on a new corpus or on a corpus which is extended by private training data taken from the data which should be analyzed.</p>
</blockquote>
<p>Further details see section <a href=""https://opennlp.apache.org/docs/2.3.1/manual/opennlp.html#tools.namefind.training"" rel=""nofollow noreferrer"">Name Finder Training</a>.</p>
",2,2,213,2020-02-24 12:34:09,https://stackoverflow.com/questions/60376067/how-to-modify-or-retrain-existing-opennlp-models
Which Deep Learning Algorithm does Spacy uses when we train Custom model?,"<p>When we train custom model, I do see we have dropout and n_iter parameters to tune, but which deep learning algorithm does Spacy Uses to train Custom Models? Also, when Adding new Entity type is it good to create blank or train it on existing model?</p>
","nlp, spacy, named-entity-recognition","<h2>Which learning algorithm does spaCy use?</h2>
<p>spaCy has its own deep learning library called <a href=""https://github.com/explosion/thinc"" rel=""noreferrer"">thinc</a> used under the hood for different NLP models. for most (if not all) tasks, spaCy uses a deep neural network based on CNN with a few tweaks. Specifically for Named Entity Recognition, spacy uses:</p>
<ol>
<li><p>A <strong>transition based approach</strong> borrowed from shift-reduce parsers, which is described in the paper <a href=""https://arxiv.org/pdf/1603.01360.pdf"" rel=""noreferrer"">Neural Architectures for Named Entity Recognition</a> by Lample et al.
Matthew Honnibal describes how spaCy uses this on a <a href=""https://youtu.be/sqDHBH9IjRU?t=975"" rel=""noreferrer"">YouTube video</a>.</p>
</li>
<li><p>A framework that's called <strong>&quot;Embed. Encode. Attend. Predict&quot;</strong> (Starting <a href=""https://youtu.be/sqDHBH9IjRU?t=1254"" rel=""noreferrer"">here</a> on the video), slides <a href=""https://github.com/explosion/talks/blob/master/2018-04-12_Embed-Encode-Attend-Predict.pdf"" rel=""noreferrer"">here</a>.</p>
<ul>
<li><p><strong>Embed</strong>: Words are embedded using a Bloom filter, which means that word hashes are kept as keys in the embedding dictionary, instead of the word itself. This maintains a more compact embeddings dictionary, with words potentially colliding and ending up with the same vector representations.</p>
</li>
<li><p><strong>Encode</strong>: List of words is encoded into a sentence matrix, to take context into account. spaCy uses CNN for encoding.</p>
</li>
<li><p><strong>Attend</strong>: Decide which parts are more informative given a query, and get problem specific representations.</p>
</li>
<li><p><strong>Predict</strong>: spaCy uses a multi layer perceptron for inference.</p>
</li>
</ul>
</li>
</ol>
<p>Advantages of this framework, per Honnibal are:</p>
<ol>
<li>Mostly equivalent to sequence tagging (another task spaCy offers models for)</li>
<li>Shares code with the parser</li>
<li>Easily excludes invalid sequences</li>
<li>Arbitrary features are easily defined</li>
</ol>
<p>For a full overview, Matthew Honnibal describes how the model works in <a href=""https://www.youtube.com/watch?v=sqDHBH9IjRU"" rel=""noreferrer"">this YouTube video</a>. Slides could be found <a href=""https://github.com/explosion/talks/blob/master/2017-11-02_Practical-and-Effective-Neural-NER.pdf"" rel=""noreferrer"">here</a>.</p>
<p><em>Note</em>: This information is based on slides from 2017. The engine might have changed since then.</p>
<h2>When adding a new entity type, should we create a blank model or train an existing one?</h2>
<p>Theoretically, when fine-tuning a spaCy model with new entities, you have to make sure the model doesn't forget representations for previously learned entities. The best thing, if possible, is to train a model from scratch, but that might not be easy or possible due to lack of data or resources.</p>
<p><strong>EDIT Feb 2021</strong>: spaCy version 3 now uses the Transformer architecture as its deep learning model.</p>
",23,9,10641,2020-02-24 17:33:52,https://stackoverflow.com/questions/60381170/which-deep-learning-algorithm-does-spacy-uses-when-we-train-custom-model
How to group items in list using lables inside items?,"<p>There is list like : </p>

<pre><code>list_a = [('B-DATE', '07'),('I-DATE', '/'),('I-DATE', '08'),('I-DATE', '/'),('I-DATE', '20'),('B-LAW', 'Abc'),('I-LAW', 'def'),('I-LAW', 'ghj'),('I-LAW', 'klm')]
</code></pre>

<p>I need to get joined <code>list_a[x][1]</code> items according to <code>list_a[x][0]</code> labels: ""start with letter B"" and all to the next ""B-started""-label (<code>list_a[x][0]</code>):</p>

<pre><code>list_b = ['07/08/20','Abcdefghjklm']
</code></pre>

<p>Like using stringagg + groupby in Oracle :) </p>
","python, list, named-entity-recognition","<h1>One Line Solution</h1>

<p>Here is a one line answer using <strong>list-comprehension</strong>. The <em>trick</em> is to use a distinctly identifiable separator (I used <code>'|||'</code>) prepended to the <em>value</em> that appears with each new occurrence of <code>'B'</code>.</p>

<pre class=""lang-py prettyprint-override""><code>str(''.join([f'|||{v}' if k.startswith(""B"") else v for (k, v) in list_a])).split('|||')[1:]
</code></pre>

<p><strong>Output</strong>:</p>

<pre><code>['07/08/20', 'Abcdefghjklm']
</code></pre>

<blockquote>
  <p><strong>Algorithm</strong></p>
  
  <ol>
  <li>Create a list of values where the values corresponding to each new occurrence of <code>'B'</code> are preceded by <code>'|||'</code>.</li>
  <li>Join all the items in the list into a single string.</li>
  <li>Split the string by the separator, <code>'|||'</code>.</li>
  <li>Keep all but the first element for the <code>str.split()</code>.</li>
  </ol>
</blockquote>
",1,0,64,2020-03-04 09:28:59,https://stackoverflow.com/questions/60522823/how-to-group-items-in-list-using-lables-inside-items
How to use spacy to do Name Entity recognition on CSV file,"<p>I have tried so many things to do name entity recognition on a column in my csv file, i tried ne_chunk but i am unable to get the result of my ne_chunk in columns like so</p>

<pre><code>ID  STORY                                       PERSON  NE   NP  NN VB  GE
1   Washington, a police officer James...        1      0    0   0   0   1
</code></pre>

<p>Instead after using this code, </p>

<pre><code>news=pd.read_csv(""news.csv"")

news['tokenize'] = news.apply(lambda row: nltk.word_tokenize(row['STORY']), axis=1)


news['pos_tags'] = news.apply(lambda row: nltk.pos_tag(row['tokenize']), axis=1)

news['entityrecog']=news.apply(lambda row: nltk.ne_chunk(row['pos_tags']), axis=1)

tag_count_df = pd.DataFrame(news['entityrecognition'].map(lambda x: Counter(tag[1] for tag in x)).to_list())

news=pd.concat([news, tag_count_df], axis=1).fillna(0).drop(['entityrecognition'], axis=1)

news.to_csv(""news.csv"")
</code></pre>

<p>i got this error</p>

<pre><code>IndexError : list index out of range
</code></pre>

<p>So, i am wondering if i could do this using spaCy which is another thing that i have no clue about. Can anyone help?</p>
","python, pandas, csv, nltk, named-entity-recognition","<p>It seems that you are checking the chunks incorrectly, that's why you get a key error. I'm guessing a little about what you want to do, but this creates new columns for each NER type returned by NLTK. It would be a little cleaner to predefined and zero each NER type column (as this gives you NaN if NERs don't exist). </p>

<pre><code>def extract_ner_count(tagged):
    entities = {}
    chunks = nltk.ne_chunk(tagged)
    for chunk in chunks:
        if type(chunk) is nltk.Tree:
          #if you don't need the entities, just add the label directly rather than this.
          t = ''.join(c[0] for c in chunk.leaves())
          entities[t] = chunk.label()
    return Counter(entities.values())

news=pd.read_csv(""news.csv"")
news['tokenize'] = news.apply(lambda row: nltk.word_tokenize(row['STORY']), axis=1)
news['pos_tags'] = news.apply(lambda row: nltk.pos_tag(row['tokenize']), axis=1)
news['entityrecognition']=news.apply(lambda row: extract_ner_count(row['pos_tags']), axis=1)
news = pd.concat([news, pd.DataFrame(list(news[""entityrecognition""]))], axis=1)

print(news.head())
</code></pre>

<p>If all you want is the counts the following is more performant and doesn't have NaNs:</p>

<pre><code>tagger = nltk.PerceptronTagger()
chunker = nltk.data.load(nltk.chunk._MULTICLASS_NE_CHUNKER)
NE_Types = {'GPE', 'ORGANIZATION', 'LOCATION', 'GSP', 'O', 'FACILITY', 'PERSON'}

def extract_ner_count(text):
    c = Counter()
    chunks = chunker.parse(tagger.tag(nltk.word_tokenize(text,preserve_line=True)))
    for chunk in chunks:
        if type(chunk) is nltk.Tree:
            c.update([chunk.label()])
    return c

news=pd.read_csv(""news.csv"")
for NE_Type in NE_Types:
    news[NE_Type] = 0
news.update(list(news[""STORY""].apply(extract_ner_count)))

print(news.head())
</code></pre>
",0,0,2201,2020-03-10 14:49:20,https://stackoverflow.com/questions/60620058/how-to-use-spacy-to-do-name-entity-recognition-on-csv-file
Nested Named Entity Recognition with Google Cloud NLP,"<p>We can perform Simple Named Entity Recognition by uploading pdf complete documents, tagging simple entities and training.</p>

<blockquote>
  <p>But, does Google Cloud AutoML platform support <strong>Nested Named Entity Recognitio</strong>n ?</p>
</blockquote>
","google-cloud-platform, named-entity-recognition, google-cloud-automl","<p>Not by default. From what I can tell, there isn't necessarily a standardized method to implement Nested Named Entity Recognition, either, which could be part of a reason why it isn't supported. I imagine to do this within a single process, each annotation would be required to have multiple annotations within it, which isn't possible:</p>

<blockquote>
  <p>Each annotation can cover up to ten tokens (words). They cannot overlap; the start_offset of an annotation cannot be between the start_offset and end_offset of an annotation in the same document. [<a href=""https://cloud.google.com/natural-language/automl/docs/prepare#entity-extraction"" rel=""nofollow noreferrer"">docs</a>]</p>
</blockquote>

<p>You could, however, probably implement this yourself based on your understanding of nested NER. Train a general model to extract primary entities (the larger containing entities). Then, train a secondary model to extract secondary entities (the entities inside the primary entity). Run the secondary model only on outputs of the primary model. Potentially you should also implement some conditions such as number of tokens as well.</p>
",2,1,364,2020-03-11 14:36:45,https://stackoverflow.com/questions/60638539/nested-named-entity-recognition-with-google-cloud-nlp
pretrained vectors not loading in spacy,"<p>I am training a custom NER model from scratch using the spacy.blank(""en"") model. I add custom word vectors to it. The vectors are loaded as follows: </p>

<pre><code>from gensim.models.word2vec import Word2Vec
from gensim.models import KeyedVectors
med_vec = KeyedVectors.load_word2vec_format('./wikipedia-pubmed-and-PMC-w2v.bin', binary=True, limit = 300000)
</code></pre>

<p>and I add it to the blank model in this code snippet here: </p>

<pre><code>def main(model=None, n_iter=3, output_dir=None):
    """"""Set up the pipeline and entity recognizer, and train the new entity.""""""
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model) # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        nlp.vocab.reset_vectors(width=200)
        for idx in range(len(med_vec.index2word)):
            word = med_vec.index2word[idx]
            vector = med_vec.vectors[idx]
            nlp.vocab.set_vector(word, vector)
        for key, vector in nlp.vocab.vectors.items():
            nlp.vocab.strings.add(nlp.vocab.strings[key])
        nlp.vocab.vectors.name = 'spacy_pretrained_vectors'
        print(""Created blank 'en' model"")
......Code for training the ner
</code></pre>

<p>I then save this model.</p>

<p>When I try to load the model,
<code>nlp = spacy.load(""./NDLA/vectorModel0"")</code></p>

<p>I get the following error: </p>

<pre><code>
`~\AppData\Local\Continuum\anaconda3\lib\site-packages\thinc\neural\_classes\static_vectors.py in __init__(self, lang, nO, drop_factor, column)
     47         if self.nM == 0:
     48             raise ValueError(
---&gt; 49                 ""Cannot create vectors table with dimension 0.\n""
     50                 ""If you're using pre-trained vectors, are the vectors loaded?""
     51             )

ValueError: Cannot create vectors table with dimension 0.
If you're using pre-trained vectors, are the vectors loaded?
</code></pre>

<p>I also get this warning: </p>

<pre><code> UserWarning: [W019] Changing vectors name from spacy_pretrained_vectors to spacy_pretrained_vectors_336876, to avoid clash with previously loaded vectors. See Issue #3853.
  ""__main__"", mod_spec)
</code></pre>

<p>The vocab directory in the model has a vectors file of size 270 MB. So I know it is not empty... What is causing this error? </p>
","spacy, named-entity-recognition","<p>You could try to pass all vectors at once instead of using a for loop. </p>

<pre><code>nlp.vocab.vectors = spacy.vocab.Vectors(data=med_vec.syn0, keys=med_vec.vocab.keys())
</code></pre>

<p>So you're else statement would become like this:</p>

<pre><code>else:
    nlp = spacy.blank(""en"")  # create blank Language class
    nlp.vocab.reset_vectors(width=200)
    nlp.vocab.vectors = spacy.vocab.Vectors(data=med_vec.syn0, keys=med_vec.vocab.keys()) 
    nlp.vocab.vectors.name = 'spacy_pretrained_vectors'
    print(""Created blank 'en' model"")
</code></pre>
",0,0,1237,2020-03-23 23:45:46,https://stackoverflow.com/questions/60823095/pretrained-vectors-not-loading-in-spacy
How to load BertforSequenceClassification models weights into BertforTokenClassification model?,"<p>Initially, I have a fine-tuned BERT base cased model using a text classification dataset and I have used BertforSequenceClassification class for this. </p>

<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = BertForSequenceClassification.from_pretrained(
    ""bert-base-uncased"", # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)
</code></pre>

<p>Now I want to use this fine-tuned BERT model weights for Named Entity Recognition and I have to use BertforTokenClassification class for this. I'm unable to figure out how to load the fine-tuned BERT model weights into the new model created using BertforTokenClassification.</p>

<p>Thanks in advance.......................</p>
","nlp, pytorch, named-entity-recognition, bert-language-model","<p>You can get weights from the bert inside the first model and load into the bert inside the second:</p>

<pre><code>new_model = BertForTokenClassification(config=config)
new_model.bert.load_state_dict(model.bert.state_dict())
</code></pre>
",4,2,3827,2020-03-28 05:15:26,https://stackoverflow.com/questions/60897514/how-to-load-bertforsequenceclassification-models-weights-into-bertfortokenclassi
How to reconstruct text entities with Hugging Face&#39;s transformers pipelines without IOB tags?,"<p>I've been looking to use Hugging Face's Pipelines for NER (named entity recognition). However, it is returning the entity labels in inside-outside-beginning (IOB) format but <a href=""https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"" rel=""noreferrer"">without the IOB labels</a>. So I'm not able to map the output of the pipeline back to my original text. Moreover, the outputs are masked in BERT tokenization format (the default model is BERT-large).</p>

<p>For example: </p>

<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
nlp_bert_lg = pipeline('ner')
print(nlp_bert_lg('Hugging Face is a French company based in New York.'))
</code></pre>

<p>The output is:</p>

<pre><code>[{'word': 'Hu', 'score': 0.9968873858451843, 'entity': 'I-ORG'},
{'word': '##gging', 'score': 0.9329522848129272, 'entity': 'I-ORG'},
{'word': 'Face', 'score': 0.9781811237335205, 'entity': 'I-ORG'},
{'word': 'French', 'score': 0.9981815814971924, 'entity': 'I-MISC'},
{'word': 'New', 'score': 0.9987512826919556, 'entity': 'I-LOC'},
{'word': 'York', 'score': 0.9976728558540344, 'entity': 'I-LOC'}]
</code></pre>

<p>As you can see, New York is broken up into two tags.</p>

<p>How can I map Hugging Face's NER Pipeline back to my original text?</p>

<p>Transformers version: 2.7</p>
","nlp, tokenize, transformer-model, named-entity-recognition, huggingface-transformers","<p>EDIT 12/2023:
As pointed out, the <code>grouped_entities</code> parameter has been deprecated. The correct way is to use the <code>aggregation_strategy</code> parameters as pointed in the <a href=""https://huggingface.co/transformers/v4.10.1/_modules/transformers/pipelines/token_classification.html"" rel=""noreferrer"">source code </a>.
For instance:</p>
<pre><code>text = 'Hugging Face is a French company based in New York.'
tagger = pipeline(task='ner', aggregation_strategy='simple')
named_ents = tagger(text)
pd.DataFrame(named_ents)
</code></pre>
<p>Gives the following output</p>
<pre><code>[
   {
      &quot;entity_group&quot;:&quot;ORG&quot;,
      &quot;score&quot;:0.96934015,
      &quot;word&quot;:&quot;Hugging Face&quot;,
      &quot;start&quot;:0,
      &quot;end&quot;:12
   },
   {
      &quot;entity_group&quot;:&quot;MISC&quot;,
      &quot;score&quot;:0.9981816,
      &quot;word&quot;:&quot;French&quot;,
      &quot;start&quot;:18,
      &quot;end&quot;:24
   },
   {
      &quot;entity_group&quot;:&quot;LOC&quot;,
      &quot;score&quot;:0.9982121,
      &quot;word&quot;:&quot;New York&quot;,
      &quot;start&quot;:42,
      &quot;end&quot;:50
   }
]
</code></pre>
<p>ORIGINAL ANSWER:
The 17th of May, a new pull request <a href=""https://github.com/huggingface/transformers/pull/3957"" rel=""noreferrer"">https://github.com/huggingface/transformers/pull/3957</a> with what you are asking for has been merged, therefore now our life is way easier, you can you it in the pipeline like</p>
<pre><code>ner = pipeline('ner', grouped_entities=True)
</code></pre>
<p>and your output will be as expected. At the moment you have to install from the master branch since there is no new release yet. You can do it via</p>
<pre><code>pip install git+git://github.com/huggingface/transformers.git@48c3a70b4eaedab1dd9ad49990cfaa4d6cb8f6a0
</code></pre>
",26,12,13172,2020-03-30 18:58:03,https://stackoverflow.com/questions/60937617/how-to-reconstruct-text-entities-with-hugging-faces-transformers-pipelines-with
Named Entity Recognition in aspect-opinion extraction using dependency rule matching,"<p>Using Spacy, I extract aspect-opinion pairs from a text, based on the grammar rules that I defined. Rules are based on POS tags and dependency tags, which is obtained by <code>token.pos_</code> and <code>token.dep_</code>. Below is an example of one of the grammar rules. If I pass the sentence <code>Japan is cool,</code> it returns <code>[('Japan', 'cool', 0.3182)]</code>, where the value represents the polarity of <code>cool</code>.</p>

<p>However I don't know how I can make it recognise the Named Entities. For example, if I pass <code>Air France is cool</code>, I want to get <code>[('Air France', 'cool', 0.3182)]</code> but what I currently get is <code>[('France', 'cool', 0.3182)]</code>. </p>

<p>I checked Spacy online documentation and I know how to extract NE(<code>doc.ents</code>). But I want to know what the possible workaround is to make my extractor work. Please note that I don't want a forced measure such as concatenating strings <code>AirFrance</code>, <code>Air_France</code> etc.</p>

<p>Thank you!</p>

<pre><code>import spacy

nlp = spacy.load(""en_core_web_lg-2.2.5"")
review_body = ""Air France is cool.""
doc=nlp(review_body)

rule3_pairs = []

for token in doc:

    children = token.children
    A = ""999999""
    M = ""999999""
    add_neg_pfx = False

    for child in children :
        if(child.dep_ == ""nsubj"" and not child.is_stop): # nsubj is nominal subject
            A = child.text

        if(child.dep_ == ""acomp"" and not child.is_stop): # acomp is adjectival complement
            M = child.text

        # example - 'this could have been better' -&gt; (this, not better)
        if(child.dep_ == ""aux"" and child.tag_ == ""MD""): # MD is modal auxiliary
            neg_prefix = ""not""
            add_neg_pfx = True

        if(child.dep_ == ""neg""): # neg is negation
            neg_prefix = child.text
            add_neg_pfx = True

    if (add_neg_pfx and M != ""999999""):
        M = neg_prefix + "" "" + M

    if(A != ""999999"" and M != ""999999""):
        rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))
</code></pre>

<p>Result</p>

<pre><code>rule3_pairs
&gt;&gt;&gt; [('France', 'cool', 0.3182)]
</code></pre>

<p>Desired output</p>

<pre><code>rule3_pairs
&gt;&gt;&gt; [('Air France', 'cool', 0.3182)]
</code></pre>
","python, nlp, spacy, named-entity-recognition, dependency-parsing","<p>It's very easy to integrate entities in your extractor. For every pair of children, you should check whether the ""A"" child is the head of some named entity, and if it is true, you use the whole entity as your object. </p>

<p>Here I provide the whole code</p>

<pre><code>!python -m spacy download en_core_web_lg
import nltk
nltk.download('vader_lexicon')

import spacy
nlp = spacy.load(""en_core_web_lg"")

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()


def find_sentiment(doc):
    # find roots of all entities in the text
    ner_heads = {ent.root.idx: ent for ent in doc.ents}
    rule3_pairs = []
    for token in doc:
        children = token.children
        A = ""999999""
        M = ""999999""
        add_neg_pfx = False
        for child in children:
            if(child.dep_ == ""nsubj"" and not child.is_stop): # nsubj is nominal subject
                if child.idx in ner_heads:
                    A = ner_heads[child.idx].text
                else:
                    A = child.text
            if(child.dep_ == ""acomp"" and not child.is_stop): # acomp is adjectival complement
                M = child.text
            # example - 'this could have been better' -&gt; (this, not better)
            if(child.dep_ == ""aux"" and child.tag_ == ""MD""): # MD is modal auxiliary
                neg_prefix = ""not""
                add_neg_pfx = True
            if(child.dep_ == ""neg""): # neg is negation
                neg_prefix = child.text
                add_neg_pfx = True
        if (add_neg_pfx and M != ""999999""):
            M = neg_prefix + "" "" + M
        if(A != ""999999"" and M != ""999999""):
            rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))
    return rule3_pairs

print(find_sentiment(nlp(""Air France is cool."")))
print(find_sentiment(nlp(""I think Gabriel García Márquez is not boring."")))
print(find_sentiment(nlp(""They say Central African Republic is really great. "")))
</code></pre>

<p>The output of this code will be what you need:</p>

<pre><code>[('Air France', 'cool', 0.3182)]
[('Gabriel García Márquez', 'not boring', 0.2411)]
[('Central African Republic', 'great', 0.6249)]
</code></pre>

<p>Enjoy!</p>
",10,7,1217,2020-04-01 08:59:23,https://stackoverflow.com/questions/60967134/named-entity-recognition-in-aspect-opinion-extraction-using-dependency-rule-matc
SpaCy NER doesn&#39;t seem to correctly recognize hyphenated names,"<p>1st thing 1st: I'm new to SpaCy and just started to test it. I have to say that I'm impressed by its simplicity and the doc quality. Thanks!</p>

<p>Now, I'm trying to identify PER in a French text. It seems to work pretty well for most but I saw a recurring incorrect pattern: names with a hyphen are not correctly recognized (ex: Pierre-Louis Durand will appear as two PER: ""Pierre"" and ""Louis Durand"").</p>

<p>See example:</p>

<pre><code>import spacy

# nlp = spacy.load('fr')
nlp = spacy.load('fr_core_news_md')

description = ('C\'est Jean-Sébastien Durand qui leur a dit. Pierre Dupond n\'est pas venu à Boston comme attendu. '
    'Louis-Jean s\'est trompé. Claire a bien choisi.')

text = nlp(description)
labels = set([w.label_ for w in text.ents])
for label in labels:
    entities = [e.string for e in text.ents if label==e.label_]
    entities = list(entities)
    print(label, entities)
</code></pre>

<p>output is: </p>

<pre><code>LOC ['Boston ']
PER ['Jean', 'Sébastien Durand ', 'Pierre Dupond ', 'Louis', 'Jean ', 'Claire ']
</code></pre>

<p>It should be: ""Jean-Sébastien Durand"" and ""Louis-Jean"".</p>

<p>I'm not sure what to do here: </p>

<ol>
<li>change the way tokens are extracted (I'm wondering about the side effect for non PER) - I don't think this is the issue as a PER can be an aggregation of multiple tokens</li>
<li>apply a magic setting somewhere so that hyphen can be used in NER for PER</li>
<li>train the model</li>
<li>go back to school ;-)</li>
</ol>

<p>Thanks for your help (and yes I'm investigating by reading more, I love it)!</p>

<p>-TC</p>
","spacy, named-entity-recognition","<p>I initially thought this would be a mismatch between the tokenizer and the training data, but it's actually a problem with how the regex that handles some words with hyphens is loaded from the saved model.</p>

<p>A temporary fix for spacy v2.2 models (which you have to do every time after loading a French model) is to replace the problematic tokenizer setting with the correct default setting:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.lang.fr import French

nlp = spacy.load(""fr_core_news_md"")
nlp.tokenizer.token_match = French.Defaults.token_match

description = ('C\'est Jean-Sébastien Durand qui leur a dit. Pierre Dupond n\'est pas venu à Boston comme attendu. '
    'Louis-Jean s\'est trompé. Claire a bien choisi.')

text = nlp(description)
labels = set([w.label_ for w in text.ents])
for label in labels:
    entities = [e.text for e in text.ents if label==e.label_]
    entities = list(entities)
    print(label, entities)
</code></pre>

<p>Output:</p>

<pre><code>PER ['Jean-Sébastien Durand', 'Pierre Dupond']
LOC ['Boston', 'Louis-Jean', 'Claire']
</code></pre>

<p>(The French NER model is trained on data from Wikipedia, so it still doesn't do very well on the entity types for this particular text.)</p>
",1,2,1163,2020-04-04 13:29:20,https://stackoverflow.com/questions/61028824/spacy-ner-doesnt-seem-to-correctly-recognize-hyphenated-names
Transformer Pipeline for NER returns partial words with ##s,"<p>How should I interpret the partial words with '##'s in them returned by the Transformer NER pipelines? Other tools like Flair and SpaCy return the word and their tag. I have worked with the CONLL dataset before and never noticed anything like this. Moreover, why are words being divided like this?</p>

<p>Example from the HuggingFace:</p>

<pre><code>from transformers import pipeline

nlp = pipeline(""ner"")

sequence = ""Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very"" \
           ""close to the Manhattan Bridge which is visible from the window.""

print(nlp(sequence))
</code></pre>

<p>Output:</p>

<pre><code>[
    {'word': 'Hu', 'score': 0.9995632767677307, 'entity': 'I-ORG'},
    {'word': '##gging', 'score': 0.9915938973426819, 'entity': 'I-ORG'},
    {'word': 'Face', 'score': 0.9982671737670898, 'entity': 'I-ORG'},
    {'word': 'Inc', 'score': 0.9994403719902039, 'entity': 'I-ORG'},
    {'word': 'New', 'score': 0.9994346499443054, 'entity': 'I-LOC'},
    {'word': 'York', 'score': 0.9993270635604858, 'entity': 'I-LOC'},
    {'word': 'City', 'score': 0.9993864893913269, 'entity': 'I-LOC'},
    {'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'},
    {'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'},
    {'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'},
    {'word': 'Manhattan', 'score': 0.9758241176605225, 'entity': 'I-LOC'},
    {'word': 'Bridge', 'score': 0.990249514579773, 'entity': 'I-LOC'}
]
</code></pre>
","python, pytorch, named-entity-recognition, huggingface-transformers","<p>Pytorch transformers and BERT make 2 tokens, the regular words as tokens and words + sub-words as tokens; which divide words by their base meaning + their complement, addin ""##"" at the start.</p>

<p>Let's say you have the phrease: <code>I like hugging animals</code></p>

<p>The first set of tokens would be:</p>

<pre><code>[""I"", ""like"", ""hugging"", ""animals""]
</code></pre>

<p>And the second list with the sub-words would be:</p>

<pre><code>[""I"", ""like"", ""hug"", ""##gging"", ""animal"", ""##s""]
</code></pre>

<p>You can learn more here:
<a href=""https://www.kaggle.com/funtowiczmo/hugging-face-tutorials-training-tokenizer"" rel=""nofollow noreferrer"">https://www.kaggle.com/funtowiczmo/hugging-face-tutorials-training-tokenizer</a></p>
",3,1,1967,2020-04-08 18:18:06,https://stackoverflow.com/questions/61107371/transformer-pipeline-for-ner-returns-partial-words-with-s
How can I train spaCy entity link model using GPU?,"<p>When I train <strong>spaCy</strong> entity linking model follow the document <a href=""https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking"" rel=""nofollow noreferrer"">wiki_entity_linking</a>, and I found that model was trained using cpu. It costs very long time to train epoch.
（About 3 days for 2 epochs in the environment: 16x cpu, 64GB mem）</p>

<p>The command is:
<code>python wikidata_train_entity_linker.py -t 50000 -d 10000 -o xxx</code>. So my question is that how could I do to use GPU for the train phase. </p>
","gpu, entity, spacy, named-entity-recognition, entity-linking","<p>You will need to refactor the code to use spacy.require_gpu() before initialising your NLP models - for more information refer to the docs: <a href=""https://spacy.io/api/top-level#spacy.require_gpu"" rel=""nofollow noreferrer"">https://spacy.io/api/top-level#spacy.require_gpu</a></p>

<p>Before doing this I would make sure your task is running on all cores. If you are not running on all cores you could use joblib for multiprocessing minibatch partitions of your job:</p>

<pre><code>    partitions = minibatch(texts, size=batch_size)
    executor = Parallel(n_jobs=n_jobs, backend=""multiprocessing"", prefer=""processes"")
    do = delayed(partial(transform_texts, nlp))
    tasks = (do(i, batch, output_dir) for i, batch in enumerate(partitions))
    executor(tasks)
</code></pre>

<p>For more information here's a joblib multiprocessing NER training example from the docs: <a href=""https://spacy.io/usage/examples#multi-processing"" rel=""nofollow noreferrer"">https://spacy.io/usage/examples#multi-processing</a></p>
",2,1,2451,2020-04-15 08:32:32,https://stackoverflow.com/questions/61224496/how-can-i-train-spacy-entity-link-model-using-gpu
List-based Named Entity Recognition for search engine: how to scale?,"<p>I'm working on a search engine for documents stored in Solr.</p>

<p><strong>In the user query, I want to detect Named Entitities</strong> (persons, organizations, cities...).</p>

<p>The example query is:</p>

<blockquote>
  <p>barack obama wife age</p>
</blockquote>

<p>In this query, I want to detect that ""barack obama"" is a person.</p>

<p>Since queries are not real phrases, it is difficult for classic NER (Spacy, Stanford NER...) to work properly. 
So, I adopted <strong>this approach</strong>: </p>

<ul>
<li>store in a dictionary all entities found in the documents (sorted by decreasing length)</li>
<li><p>loop the dictionary, to see if the user query contains entities</p>

<pre><code>def find_entities(query,entities_dict):

    entities=[]
    new_query=query.lower()

    for entity in entities_dict:
        if find_substring(entity,new_query):
            entities.append({entity:entities_dict[entity]})
            new_query = re.sub(r'\b{}\b'.format(entity), '', new_query)
    return(new_query,entities)
</code></pre></li>
</ul>

<p><strong>At the moment, I have about 200k entities in my Solr index</strong>: dictionary creation takes a few minutes; after the loading, this approach works well, is fast and not so memory consuming.</p>

<p><strong>In the near future, I will have 50-100 million entities</strong>.</p>

<p>I think that it will be impossible to store these entities in memory.</p>

<p><strong>How can I change my approach?</strong> 
I'm looking for advice for the <em>algorithm</em>, the <em>memory management</em> and <em>data structures</em> to be used.</p>
","search, memory, solr, nlp, named-entity-recognition","<p>One obvious brute-force solution is just to make your search index distributed: you create e.g. 100 nodes with a dictionary of 1 million entities in each one, you run them in parallel, and you merge the results. </p>

<p>Another solution (which may be complementary to splitting the index) is to keep your entities not in a simple list, but instead in a <a href=""https://en.wikipedia.org/wiki/Trie"" rel=""nofollow noreferrer"">prefix tree</a> (aka <em>trie</em>) or in a graph of <a href=""https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm"" rel=""nofollow noreferrer"">Aho-Corasick</a>. These data structures speed up substring search by a lot, because they try to match <em>all</em> the entities with your query in a <em>single</em> pass, exploiting the fact that many entities have identical substrings in them. </p>

<p>In fact, I have used <a href=""https://pypi.org/project/pyahocorasick/"" rel=""nofollow noreferrer"">pyahocorasick</a> to look up a few million entities (movies, songs, actors, etc.) in short queries, and it seemed to scale very well. Formally, the time complexity of Aho-Corasick does not depend on the total number of entities, only on the number of matched entities in the concrete query. Therefore, if the search gets slow (which is unlikely), it makes sense to see what entities generate lots of false positive matches and remove them from the index. In my case, after removing very common entities such as ""It"" (it's a movie name!), the matcher sped up even more. </p>

<p>Here is an example. First, we get the entities to search for (15K cities):</p>

<pre><code>pip install pyahocorasick
wget https://simplemaps.com/static/data/world-cities/basic/simplemaps_worldcities_basicv1.6.zip
unzip simplemaps_worldcities_basicv1.6.zip
</code></pre>

<p>Then we create the automaton that can match entities (cities):</p>

<pre><code>import pandas as pd
import re
import ahocorasick
cities = pd.read_csv('worldcities.csv')

def preprocess(text):
    """"""
    Add underscores instead of non-alphanumeric characters 
    and on the word boundaries in order to tell words from word substrings.
    """"""
    return '_{}_'.format(re.sub('[^a-z0-9]', '_', text.lower()))

index = ahocorasick.Automaton()
for city in cities.city:
    index.add_word(preprocess(city), city)
index.make_automaton()
# this object can be pickled to disk and then loaded back
</code></pre>

<p>And now actually apply this index to lookup entities in the text:</p>

<pre><code>def find_cities(text, searcher):
    result = dict()
    for end_index, city_name in searcher.iter(preprocess(text)):
        end = end_index - 1
        start = end - len(city_name)
        result[(start, end)] = city_name
    return result

print(find_cities( 'Tver’ is somewhere between Moscow and Saint Petersburg', index))
# {(0, 5): 'Tver’', (27, 33): 'Moscow', (38, 54): 'Saint Petersburg', (44, 54): 'Petersburg'} 
# the search takes about 0.1 ms
</code></pre>

<p>Naive search gives the same results but takes about 10 ms:</p>

<pre><code>for city in cities.city:
    idx = text.find(city)
    if idx &gt;=0:
        print(idx, city)
</code></pre>

<p>Here is <a href=""https://gist.github.com/avidale/b4680e66c2e75a4fe0edeeb27c2e0a68"" rel=""nofollow noreferrer"">the notebook</a> with my code.</p>
",3,1,704,2020-04-20 09:32:50,https://stackoverflow.com/questions/61319219/list-based-named-entity-recognition-for-search-engine-how-to-scale
Traing a Spacy NER model I get an exception: &quot;[E022] Could not find a transition with the name &#39;B-COMPANY&#39; in the NER model.&quot;,"<p>For a reproducible example, I will provide toy data and the code:</p>

<p>The training data are the following (19 records):</p>

<pre><code>to_train_ents  = [('  δηµοσίευσης  στο  διαδικτυακό  τόπο του Γ.Ε.ΜΗ. στοιχείων της ανώνυµης εταιρείας µε την επωνυµία «ΑΛΟΥΜΥΛ, ΒΙΟΜΗΧΑΝΙΑ ΑΛΟΥΜΙΝΙΟΥ ΑΝΩΝΥΜΗ ΕΤΑΙΡΙΑ».    Την 25/07/2019 καταχωρίσθηκε στο Γενικό Εµπορικό Μητρώο (Γ.Ε.ΜΗ.) µε Κωδικό Αριθµό Καταχώρισης',
  {'entities': [(100, 146, 'B-COMPANY')]}),
 ('ΑΚΟΙΝΩΣΗ   Καταχώρισης στο Γενικό Εμπορικό Μητρώο στοιχείων της ανώνυμης εταιρείας με την επωνυμία «ΑΝΕΚ ΤΟΥΡΙΣΤΙΚΗ - ΞΕΝΟΔΟΧΕΙΑΚΗ - ΣΥΜΜΕΤΟΧΩΝ ΑΝΩΝΥΜΗ ΕΤΑΙΡΕΙΑ».  Την 18/12/2017 καταχωρίσθηκαν στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.), τα κατωτέρω στοιχεία της Ανώ',
  {'entities': [(100, 160, 'B-COMPANY')]}),
 ('ς στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.) στοιχείων της ανώνυμης τραπεζικής εταιρείας με την επωνυμία «ATTICA BANK ΑΝΩΝΥΜΗ ΤΡΑΠΕΖΙΚΗ ΕΤΑΙΡΕΙΑ»     Ανακοινώνεται ότι την 91-2020 καταχωρίσθηκε στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.) με Κωδικό Αρι',
  {'entities': [(100, 138, 'B-COMPANY')]}),
 ('Καταχώρισης στο Γενικό Εμπορικό Μητρώο στοιχείων της ανώνυμης τραπεζικής εταιρείας με την επωνυμία «ATTICA BANK ΑΝΩΝΥΜΗ ΤΡΑΠΕΖΙΚΗ ΕΤΑΙΡΕΙΑ». Ανακοινώνεται ότι την 18112019 καταχωρίστηκε στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.)  με Κωδικό Αριθ',
  {'entities': [(100, 138, 'B-COMPANY')]}),
 ('ΝΑΚΟΙΝΩΣΗ  Καταχώρισης στο Γενικό Εμπορικό Μητρώο στοιχείων της ανώνυμης εταιρείας με την επωνυμία «ΔΕΗ SOLAR SOLUTIONS ΑΝΩΝΥΜΗ ΕΤΑΙΡΕΙΑ», το διακριτικό τίτλο «ΔΕΗ SOLAR SOLUTIONS AE» και Αριθμό Γ.Ε.ΜΗ  129809601000.   Ο ΠΡΟΕΔΡΟΣ  ΤΟΥ Ε',
  {'entities': [(100, 136, 'B-COMPANY')]}),
 (' και δημοσίευσης στο διαδικτυακό τόπο του Γ.Ε.ΜΗ. στοιχείων της ανώνυμης εταιρείας με την επωνυμία «Ελληνικά Πετρέλαια Ανώνυμη Εταιρεία».  Την 111-2019 καταχωρίσθηκε στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.) με Κωδικό Αριθμό Καταχώρησης 162',
  {'entities': [(100, 135, 'B-COMPANY')]}),
 ('6806                                                     ΚΑΤΑΣΤΑΤΙΚΟ ΤΗΣ ΕΤΑΙΡΕΙΑΣ ΜΕ ΤΗΝ ΕΠΩΝΥΜΙΑ «ΕΛΛΗΝΙΚΑ ΠΕΤΡΕΛΑΙΑ ΑΝΩΝΥΜΗ ΕΤΑΙΡΕΙΑ»                                                   ΚΕΦΑΛΑΙΟ Α΄ Επωνυμία – Έδρα – Διάρκεια – Σκοπός ',
  {'entities': [(100, 135, 'B-COMPANY')]}),
 ('ΑΝΑΚΟΙΝΩΣΗ  Καταχώρισης στο Γενικό Εμπορικό Μητρώο στοιχείων της ανώνυμης εταιρείας με την επωνυμία FORTHNET ΑΝΩΝΥΜΗ ΕΤΑΙΡΕΙΑ ΣΥΝΔΡΟΜΗΤΙΚΗΣ ΤΗΛΕΟΡΑΣΗΣ, ΠΑΡΟΧΗΣ ΤΗΛΕΠΙΚΟΙΝΩΝΙΑΚΩΝ ΥΠΗΡΕΣΙΩΝ ΚΑΙ ΣΥΜΜΕΤΟΧΩΝ και το διακριτικό τίτλο FORTHNET MEDIA Α.Ε.  Την 13/11/2018 καταχωρίσθηκε στο Γενικό Εμπορικό Μητρώ',
  {'entities': [(99, 201, 'B-COMPANY')]}),
 ('ΝΑΚΟΙΝΩΣΗ  Καταχώρισης στο Γενικό Εμπορικό Μητρώο, στοιχείων της Ανώνυμης Εταιρείας με την επωνυμία FORTHNET ΑΝΩΝΥΜΗ ΕΤΑΙΡΕΙΑ ΣΥΝΔΡΟΜΗΤΙΚΗΣ ΤΗΛΕΟΡΑΣΗΣ, ΠΑΡΟΧΗΣ ΤΗΛΕΠΙΚΟΙΝΩΝΙΑΚΩΝ ΥΠΗΡΕΣΙΩΝ ΚΑΙ ΣΥΜΜΕΤΟΧΩΝ, το διακριτικό τίτλο FORTHNET MEDIA Α.Ε και αριθμό ΓΕΜΗ 124012301000.  Ο ΠΡΟΕΔΡΟΣ  ΤΟΥ ΕΜΠΟΡΙΚΟΥ &amp; ',
  {'entities': [(99, 201, 'B-COMPANY')]}),
 ('και δημοσίευσης στο διαδικτυακό τόπο του Γ.Ε.ΜΗ., στοιχείων της ανώνυμης εταιρείας με την επωνυμία «JUMBO ΑΝΩΝΥΜΗ ΕΜΠΟΡΙΚΗ ΕΤΑΙΡΕΙΑ».  Ανακοινώνεται ότι την 11.12.2019 καταχωρίσθηκαν στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.), τα κατωτέρ',
  {'entities': [(100, 131, 'B-COMPANY')]}),
 ('ΝΑΚΟΙΝΩΣΗ  Καταχώρισης στο Γενικό Εμπορικό Μητρώο στοιχείων της Ανώνυμης Εταιρείας με την επωνυμία «JUMBO ΑΝΩΝΥΜΗ ΕΜΠΟΡΙΚΗ ΕΤΑΙΡΕΙΑ».      Ανακοινώνεται ότι την 11.12.2019 καταχωρίσθηκε στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.) με Κωδικ',
  {'entities': [(100, 131, 'B-COMPANY')]}),
 ('και δημοσίευσης στον διαδικτυακό τόπο του Γ.Ε.ΜΗ. στοιχείων της Ανώνυμης Εταιρείας με την επωνυμία «ΣΥΣΤΗΜΑΤΑ ΜΙΚΡΟΫΠΟΛΟΓΙΣΤΩΝ ΑΝΩΝΥΜΗ ΕΤΑΙΡΕΙΑ». Την 16.02.2016 καταχωρίστηκε στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.) με Κωδικό Αριθμό Καταχώρησης 56',
  {'entities': [(100, 143, 'B-COMPANY')]}),
 ('αι δημοσίευσης στον διαδικτυακό τόπο του Γ.Ε.ΜΗ., στοιχείων της Ανώνυμης Εταιρείας με την επωνυμία «ΠΛΑΙΣΙΟ COMPUTERS ΑΝΩΝΥΜΗ ΕΜΠΟΡΙΚΗ ΚΑΙ ΒΙΟΜΗΧΑΝΙΚΗ ΕΤΑΙΡΕΙΑ ΗΛΕΚΤΡΟΝΙΚΩΝ ΥΠΟΛΟΓΙΣΤΩΝ ΚΑΙ ΕΙΔΩΝ ΒΙΒΛΙΟΧΑΡΤΟΠΩΛΕΙΟΥ».      Ανακοινώνεται ότι την 07.08.2019 καταχωρίσθηκε, εκ νέου, στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ)',
  {'entities': [(100, 213, 'B-COMPANY')]}),
 ('  δημοσίευσης  στο  διαδικτυακό  τόπο του Γ.Ε.ΜΗ. στοιχείων της ανώνυμης εταιρείας με την επωνυμία «ΠΛΑΙΣΙΟ COMPUTERS ΑΝΩΝΥΜΗ ΕΜΠΟΡΙΚΗ ΚΑΙ ΒΙΟΜΗΧΑΝΙΚΗ ΕΤΑΙΡΕΙΑ ΗΛΕΚΤΡΟΝΙΚΩΝ ΥΠΟΛΟΓΙΣΤΩΝ ΚΑΙ ΕΙΔΩΝ ΒΙΒΛΙΟΧΑΡΤΟΠΩΛΕΙΟΥ». Την 18/06/2018 καταχωρίσθηκε στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.) µε Κωδικό Αριθµό Καταχώρισης 14',
  {'entities': [(100, 213, 'B-COMPANY')]}),
 ('µ. Πρωτ. :    1826463                           ΚΩΔΙΚΟΠΟΙΗΜΕΝΟ ΚΑΤΑΣΤΑΤΙΚΟ  ΤΗΣ ΑΝΩΝΥΜΗΣ ΕΤΑΙΡΕΙΑΣ «Quest Συμμετοχών Ανώνυμη Εταιρεία» Αρ. Γ.Ε.ΜΗ. 121763701000  ΚΕΦΑΛΑΙΟ  Α Σύσταση – Επωνυμία – ΄Εδρα – Διάρκεια Σκοπός  ΄Αρθρο 1 Η επω',
  {'entities': [(100, 133, 'B-COMPANY')]}),
 ('ΝΑΚΟΙΝΩΣΗ  Καταχώρισης στο Γενικό Εμπορικό Μητρώο στοιχείων της ανώνυμης εταιρείας με την επωνυμία «TITAN ΔΙΕΘΝΗΣ ΕΜΠΟΡΙΚΗ ΑΝΩΝΥΜΗ ΕΤΑΙΡΕΙΑ ΤΣΙΜΕΝΤΩΝ».  Ο ΠΡΟΕΔΡΟΣ ΤΟΥ ΕΜΠΟΡΙΚΟΥ &amp; ΒΙΟΜΗΧΑΝΙΚΟΥ ΕΠΙΜΕΛΗΤΗΡΙΟΥ ΑΘΗΝΩΝ Ανακοινώνει ότι:  Την 10/06/2019 κα',
  {'entities': [(100, 149, 'B-COMPANY')]}),
 ('Καταχώρισης στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.), στοιχείων της Ανώνυμης Εταιρείας με την επωνυμία «TITAN ΔΙΕΘΝΗΣ ΕΜΠΟΡΙΚΗ ΑΝΩΝΥΜΗ ΕΤΑΙΡΕΙΑ ΤΣΙΜΕΝΤΩΝ» και αριθμό Γ.Ε.ΜΗ. 1604901000, (που είχε Αρ.ΜΑΕ. 29226/001/Β/93/0346).  Ο ΠΡΟΕΔΡΟΣ  ΤΟΥ ΕΜΠΟΡΙΚΟΥ ',
  {'entities': [(100, 149, 'B-COMPANY')]}),
 (' ΑΝΑΚΟΙΝΩΣΗ Καταχώρισης στο Γενικό Εμπορικό Μητρώο στοιχείων της ανώνυμης εταιρείας με την επωνυμία ΒΙΑΝΕΞ Α.Ε. ΑΝΩΝΥΜΟΣ ΕΜΠΟΡΟΒΙΟΜΗΧΑΝΙΚΗ ΤΟΥΡΙΣΤΙΚΗ ΞΕΝΟΔΟΧΕΙΑΚΗ ΚΑΙ ΝΑΥΤΙΛΙΑΚΗ ΑΝΩΝΥΜΟΣ ΕΤΑΙΡΕΙΑ και το διακριτικό τίτλο ΒΙΑΝΕΞ ΑΕ. Την 03/04/2019 καταχωρίσθηκε στο Γενικό Εμπορικό Μητρώο (Γ.Ε.ΜΗ.',
  {'entities': [(99, 194, 'B-COMPANY')]})]
</code></pre>

<p>The code is the following:</p>

<pre><code>def train_model(to_train_ents, nlp):

    optimizer = nlp.begin_training()

    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']

    with nlp.disable_pipes(*other_pipes):

        for itn in range(20):

            losses = {}

            random.shuffle(to_train_ents)

            for item in to_train_ents:

                nlp.update([item[0]], 

                          [item[1]],

                          sgd = optimizer,

                        drop = 0.35,

                          losses = losses)


    return(nlp, losses)
</code></pre>

<p>I use the Greek Spacy model (md) which is installed and loaded with the following code:</p>

<pre><code>!python -m spacy download el_core_news_md
nlp_el = spacy.load('el_core_news_md')
</code></pre>

<p>Then I call the function train_model() previously defined and I get the following error:</p>

<pre><code>nlp_el , losses = train_model(to_train_ents, nlp_el)
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-484-a059cf596651&gt; in &lt;module&gt;
----&gt; 1 nlp_el , losses = train_model(to_train_ents, nlp_el)

&lt;ipython-input-471-56fdae8ff98f&gt; in train_model(to_train_ents, nlp)
     23                         drop = 0.35,
     24 
---&gt; 25                           losses = losses)
     26 
     27 

~\AppData\Roaming\Python\Python36\site-packages\spacy\language.py in update(self, docs, golds, drop, sgd, losses, component_cfg)
    450             kwargs = component_cfg.get(name, {})
    451             kwargs.setdefault(""drop"", drop)
--&gt; 452             proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)
    453             for key, (W, dW) in grads.items():
    454                 sgd(W, dW, key=key)

nn_parser.pyx in spacy.syntax.nn_parser.Parser.update()

nn_parser.pyx in spacy.syntax.nn_parser.Parser._init_gold_batch()

ner.pyx in spacy.syntax.ner.BiluoPushDown.preprocess_gold()

ner.pyx in spacy.syntax.ner.BiluoPushDown.lookup_transition()

KeyError: ""[E022] Could not find a transition with the name 'B-COMPANY' in the NER model.""
</code></pre>

<p>How you explain the error and how can I remedy it?</p>
","python-3.x, spacy, named-entity-recognition","<p><code>COMPANY</code> needs to be added to the NER model:</p>

<pre><code>ner = nlp.get_pipe(""ner"")
ner.add_label(""COMPANY"")
</code></pre>

<p>You need to do this for each entity type in your data that is not yet in the labelset of the <a href=""https://spacy.io/models/el"" rel=""nofollow noreferrer"">pretrained Greek model</a> (For 2.2.5, this is: EVENT, GPE, LOC, ORG, PERSON, PRODUCT)</p>

<p>Also, you redefine <code>losses</code> every iteration. Instead, you probably want to print it / store it each loop so you can check the loss curve.</p>
",2,0,832,2020-04-28 09:04:31,https://stackoverflow.com/questions/61476460/traing-a-spacy-ner-model-i-get-an-exception-e022-could-not-find-a-transition
Problem adding custom entities to SpaCy&#39;s NER,"<ul>
<li>I added a new entity called ""orgName"" to en_core_web_lg using <a href=""https://spacy.io/usage/training#example-new-entity-type"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#example-new-entity-type</a></li>
<li>All my training data (26k sentences) have the ""orgName"" labeled in them.</li>
<li>To deal with the catastrophic forgetting problem, I ran en_core_web_lg on those 26k raw sentences and added the ORG, PROD, FAC, etc. entities as labels and not face the colliding entities, I created duplicates.
So, for a sentence A which was labeled by ""orgName"", I created a duplicate A2 which has ORG, PROD, FAC, etc. ending up with about 52k sentences.</li>
<li>I trained using 100 iterations.</li>
</ul>

<p>Now, the problem is that testing the model even on the training sentences, it's not showing the ORG, PROD, FAC, etc. but only showing ""orgName"".</p>

<p>Where do you think the problem is?</p>
","nlp, spacy, named-entity-recognition","<p>In principle the way you're trying to solve the catastrophic forgetting problem, by retraining it on its old predictions, seems like a good approach to me.</p>

<p>However, if you are having duplicate versions of the same sentence, but annotated differently, and feeding that to the NER classifier, you may confuse the model. The reason is that it doesn't just look at the positive examples, but also explicitely sees non-annotated words as negative cases. </p>

<p>So if you have ""Bob lives in London"", and you only annotate ""London"", then it will think Bob is surely not an NE. If then you have a second sentence where you annotate only Bob, it will ""unlearn"" that London is an NE, because now it's not annotated as such. So consistency really is important.</p>

<p>I would suggest to implement a more advanced algorithm to resolve the conflicts.
One option is to always just take the annotated entity with the longest <code>Span</code>. But if the Spans are often exactly the same, you may need to reconsider your label scheme. Which entities collide most often? I would assume ORG and OrgName? Do you really need ORG? Perhaps the two can be ""merged"" as the same entity?</p>
",2,1,586,2020-04-28 17:45:58,https://stackoverflow.com/questions/61486629/problem-adding-custom-entities-to-spacys-ner
Spacy CLI Training Unable to Activate GPU,"<p>I am trying to train a NER Spacy model on the CLI. Following all the steps necessary I finally created a correct input file, however when trying to train on the GPU I get the message that spacy is unable to activate the GPU, other programs actually are able to use my GPU and cuda is set up correctly. Still it doesn't seem to work, I only have 1 GPU in my computer so I selected -g 0 on the CLI. I can't find any further information as why the GPU cannot be activated, searching the internet has led to nothing either.</p>

<pre><code>Training pipeline: ['ner']
⚠ Unable to activate GPU: 0
Using CPU only
</code></pre>

<ul>
<li>NVIDIA-driver-version: 440.64</li>
<li>CUDA-verion: 10.2</li>
<li>GPU GeForce RTX 2060</li>
</ul>

<p><a href=""https://i.sstatic.net/h1tLu.png"" rel=""nofollow noreferrer"">Image showing the nvidia-smi output</a></p>
","python-3.x, tensorflow, gpu, spacy, named-entity-recognition","<p>Check that you have the correct version of cupy (from your CUDA version above: <code>cupy-cuda102</code>) installed.</p>
",1,1,935,2020-04-29 09:18:28,https://stackoverflow.com/questions/61498301/spacy-cli-training-unable-to-activate-gpu
Python: Spacy NER and memory consumption,"<p>I use SPACY for named entity recognition. I have my own trained model on en_core_web_md. The size of my model is 223 megabytes. When the model is loaded into memory, it uses 800 megabytes. Is it possible somehow for NER purposes not to load everything (lexemes.bin, string.json, key2row), but only vectors and model (which weigh 4 and 24 megabytes respectively) to consume much less memory? or is it all necessary to load for NER?</p>
","python, memory, spacy, named-entity-recognition","<p>For spacy v2.2, it is necessary to load everything. There is one minor bug that affects <code>key2row</code> in <code>md</code> models: to improve the size and loading time of <code>key2row</code> in <code>md</code> models with versions <code>v2.2.0-v2.2.5</code>, see <a href=""https://stackoverflow.com/a/60541041/461847"">https://stackoverflow.com/a/60541041/461847</a>.</p>

<p>The bug related to <code>key2row</code> is fixed in v2.2.4 if you're training a model from scratch with your own custom vectors, but the provided v2.2 <code>md</code> models will still have this issue.</p>

<p>Planned for v2.3: removal of <code>lexemes.bin</code> with lexemes only created on demand. With these changes, the <code>md</code> models will be about 50% smaller on disk and the initial model loading is about 50% faster. The English <code>md</code> model looks like it's about 300MB smaller in memory when loaded initially, but memory usage will increase a bit in use as it builds a lexeme cache. See: <a href=""https://github.com/explosion/spaCy/pull/5238"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/pull/5238</a></p>
",2,1,1397,2020-05-01 11:19:34,https://stackoverflow.com/questions/61541472/python-spacy-ner-and-memory-consumption
How to find all Wikipedia pages related to a named entity?,"<p>Given a text, I am looking to find links to all Wikipedia pages related to named entities mentioned in the text. Is there a reliable way to do this?  </p>

<p>For example, consider the text,</p>

<blockquote>
  <p>Mark Elliot Zuckerberg is an American internet entrepreneur and
  philanthropist.</p>
</blockquote>

<p>"" Given this, I am looking at output with the following links:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Mark_Zuckerberg"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Mark_Zuckerberg</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Americans"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Americans</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Internet"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Internet</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Entrepreneurship"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Entrepreneurship</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Philanthropy"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Philanthropy</a></li>
</ul>

<p>Is this possible at all given the current state of NLP? 
Many thanks!</p>
","nlp, mediawiki, stanford-nlp, wikipedia, named-entity-recognition","<p>The problem you are trying to solve is called <a href=""https://en.wikipedia.org/wiki/Entity_linking"" rel=""nofollow noreferrer"">Entity Linking</a>. There are many academic papers discussing solutions to this problem, but only few of them provide an implementation.</p>

<p><a href=""https://arxiv.org/abs/1904.09131"" rel=""nofollow noreferrer"">OpenTapioka</a> from Oxford has an <a href=""https://github.com/wetneb/opentapioca"" rel=""nofollow noreferrer"">open source implementation</a> and an <a href=""https://opentapioca.org"" rel=""nofollow noreferrer"">online demo</a>.</p>

<p><a href=""https://arxiv.org/pdf/1804.03580.pdf"" rel=""nofollow noreferrer"">SWAT</a> from the University of Pisa has a  <a href=""https://sobigdata.d4science.org/web/tagme/swat-api"" rel=""nofollow noreferrer"">publically available API</a>.</p>
",3,1,1361,2020-05-04 20:17:56,https://stackoverflow.com/questions/61600865/how-to-find-all-wikipedia-pages-related-to-a-named-entity
Change Named Entity Recognition Format from ENAMEX to CoNLL,"<p>I have a dataset which is in ENAMEX format like this:</p>

<pre><code>&lt;ENAMEX TYPE=""LOCATION""&gt;Italy&lt;/ENAMEX&gt;'s business world was rocked by the announcement &lt;TIMEX TYPE=""DATE""&gt;last Thursday&lt;/TIMEX&gt; that Mr. &lt;ENAMEX TYPE=„PERSON""&gt;Verdi&lt;/ENAMEX&gt; would leave his job as vicepresident of &lt;ENAMEX TYPE=""ORGANIZATION""&gt;Music Masters of Milan, Inc&lt;/ENAMEX&gt; to become operations director of &lt;ENAMEX TYPE=""ORGANIZATION""&gt;Arthur Andersen&lt;/ENAMEX&gt;.
</code></pre>

<p>I want to change it into CoNLL format:</p>

<pre><code>Italy  LOCATION
's  O
business O
world  O
was  O
rocked  O
by  O
the  O
announcement  O
last  DATE
Thursday  DATE
...
.  O
</code></pre>

<p>How can I do that? Is there a standard script for such format conversion?</p>
","named-entity-recognition, conll","<p>I wrote one myself that worked for me though is not heavily tested <a href=""https://github.com/afshinrahimi/markup2bio4ner.git"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>from __future__ import unicode_literals
import os
from os import path
import re
import os
import re
import en_core_web_sm #spacy

# to convert formats such as &lt;ENAMEX type=""LOCATION""&gt;Italy&lt;/ENAMEX&gt; is experiencing an economic boom.

def xml_iter(file_):
    with open(file_, 'r') as fin:
        for line in fin:
            yield line.strip()



def markupline2bio(line):
            #print(line.split('\t')[0])
        record = line.split('\t')[0]
        #print(record)
        #print(parse(record))
        #print(record[35:40], record[81:90])
        #tags = re.findall(r'&lt;ENAMEX\s+TYPE=\""(.+?)\""&gt;(.+?)&lt;/ENAMEX&gt;', record)
        prev_start = 0
        prev_end = 0
        all_tokens = []
        all_tags = []
        for f in re.finditer(r'&lt;ENAMEX\s+TYPE=\""(.+?)\""&gt;(.+?)&lt;/ENAMEX&gt;', record):
            #print(record[f.start(0):f.end(0)], f.start(0), f.end(0))
            annotations = re.findall(r'&lt;ENAMEX\s+TYPE=\""(.+?)\""&gt;(.+?)&lt;/ENAMEX&gt;', record[f.start(0):f.end(0)])
            before_text = record[prev_end:f.start(0)]
            prev_start, prev_end = f.start(0), f.end(0)
            for tok in nlp(before_text):
                if str(tok).strip():
                    all_tokens.append(tok)
                    all_tags.append('O')
            for phrasetag in annotations:
                tag, phrase = annotations[0]
                tokens = nlp(phrase)
                for entity_tok_index, tok in enumerate(tokens):
                    if str(tok).strip():
                        all_tokens.append(tok)
                        if entity_tok_index == 0:
                            all_tags.append(""B-"" + tag)
                        else:
                            all_tags.append(""I-"" + tag)
                    else:
                        entity_tok_index -= 1

        after_text = record[prev_end:]
        for tok in nlp(after_text):
            if str(tok).strip():
                all_tokens.append(tok)
                all_tags.append('O')
        return all_tokens, all_tags

if __name__ == '__main__':
    data_dir = './data/indonesian_bert_all/Indonesian/ner/'
    xml_iterator = xml_iter(os.path.join(data_dir, 'data_train_ugm.txt'))
    output_file = os.path.join(data_dir, 'data_train_ugm.bio')
    #nlp = spacy.load(""en_core_web_sm"")
    nlp = en_core_web_sm.load()
    with open(output_file, 'w') as fout:
        for i, line in enumerate(xml_iterator):
            if i &gt; 10:
                #break
                pass
            all_tokens, all_tags = markupline2bio(line.strip())
            #print(all_tokens)
            #print(all_tags)
            #print(line)
            for tok, tag in zip(all_tokens, all_tags):
                #print(tok, tag)
                fout.write(str(tok) + '\t' + tag)
                fout.write('\n')
            fout.write('\n')
</code></pre>
",0,0,535,2020-05-06 01:51:44,https://stackoverflow.com/questions/61626008/change-named-entity-recognition-format-from-enamex-to-conll
Spacy english language model take too long to load,"<p>I am trying to make a chatbot using python and for that i am using Spacy for Entity recognition so i have installed pre build Spacy english language model(medium) to extract the entities from user utterance, but the problem is that when i load the model to extract entites from user utterance it takes 31 seconds to load the model, because i am making a chatbot time really matter in my case.
Need some guidance from all of you, any alternative? any help would be really appreciated</p>

<p>Here is the code that extracts entities from user utterance:</p>

<pre><code>import spacy
import time
def extractEntity(userUtterance):
    ''' This funtion returns a list of tuple a tuple contain 
        (entity Name, Entity Type)    
        We use pre build spacy english language model to extract entities
    '''
    start_time = time.process_time()
    nlp = spacy.load(""en"")
    print(time.process_time() - start_time, ""seconds"") # prints the time taken to load the model
    docx = nlp(userUtterance)
    listOfTyples = [(word.text, spacy.explain(word.label_)) for word in docx.ents]
    return listOfTyples

if __name__ == ""__main__"":
    print(extractEntity(""I want to go to London, can you book my flight for wednesday""))
</code></pre>

<p>Output:</p>

<pre><code>31.0 seconds
[('London', 'Countries, cities, states'), ('wednesday', 'Absolute or relative dates or periods')]
</code></pre>
","python, chatbot, spacy, named-entity-recognition","<p>This is really slow because it loads the model for every sentence:</p>

<pre><code>import spacy

def dostuff(text):
    nlp = spacy.load(""en"")
    return nlp(text)
</code></pre>

<p>This is not slow because it loads the model once and re-uses it for every function call:</p>

<pre><code>import spacy

nlp = spacy.load(""en"")

def dostuff(text):
    return nlp(text)
</code></pre>

<p>You should change your application to look like the second example. This is not specific to spaCy but will be the case with any kind of model you choose to use.</p>
",6,2,2553,2020-05-06 19:08:57,https://stackoverflow.com/questions/61643370/spacy-english-language-model-take-too-long-to-load
How to use k-means algorithm to do attribute clustering after NER?,"<p>I am reading <a href=""http://aircconline.com/ijnlc/V8N5/8519ijnlc03.pdf"" rel=""nofollow noreferrer"">this paper</a> and in 3.2.1 sub-section, first paragraph last three lines,</p>

<blockquote>
  <p>To map the named entity candidates to the
  standard attribute names, we employed the k-means algorithm to cluster the identified named
  entities by computing the cosine similarities between them based on Term Frequency–Inverse
  Document Frequency (TFIDF).""</p>
</blockquote>

<p>Can anyone explain what does that mean? If possible give an example about the implementation scenario.</p>
","machine-learning, deep-learning, nlp, named-entity-recognition","<p>I am not completely sure what they mean; the best solution is to directly ask the paper's authors about this. But it seems that the clustering has been performed to do something related to <a href=""https://en.wikipedia.org/wiki/Entity_linking"" rel=""nofollow noreferrer"">entity linking</a>.</p>

<p>Entity linking is the process of disambiguating the named entities discovered in the text by matching them with the unique identities (e.g. Wikipedia articles or database entries). For example, ""Washington"" can be linked to the city ""Washington, D.C"", the state ""Washington"", or the person ""George Washington"". On the other hand, the strings ""Stanford"", ""Stanford University"", ""Leland Stanford Junior University"", ""LSJU"", ""Stanford U."", ""Stanford uni"", ""University of Stanford"", Stanford.edu"", ""Stanfurd"", and <a href=""https://dispenser.info.tm/~dispenser/cgi-bin/rdcheck.py?page=Stanford_University"" rel=""nofollow noreferrer"">a few more</a> do refer to the same institution. This information is not provided by pure NER models, because they can tell you only that e.g. in <code>I graduated from Stanford U. in 2010</code>, <code>Stanford U</code> is a school - but not that it is some specific school.</p>

<p>You may want to use NEL, because NER model predicts only that ""Stanford U"" is the name of a educational institution, or that ""TeslaMotors"" is the name of a company. Then the NEL model predicts that ""Stanford U"" really means ""Stanford University"", and ""TeslaMotors"" really means ""Tesla, inc."". So you can think that named entity linking somehow ""refines"" the recognized entities. It is useful, for example, if you perform some downstream task (e.g. classification of resumes) using the found entities, and ""Tesla, inc."" is present in the training sample, whereas ""TeslaMotors"" isn't. In this situation, named entity linking will improve the generalizing ability of the downstream model, because after NEL both entities will be treated exactly the same way.</p>

<p>The authors of the paper, however, don't seem to have the database for all their domain-specific entities (schools, degrees, skills, job position etc.), or don't have a labeled dataset to train a model for entity linking. Therefore, instead of classical entity linking, they just merge similar occurrences of entites into clusters, hoping that the strings that end up in the same cluster do really refer to the same identity. </p>

<p>This approach may seem crude, but it is better than no linking at all, and it can provide a good starting point for manually labelling/linking the clusters and thus creating a dataset for training a supervised model for entity linking.</p>
",3,2,709,2020-05-07 23:43:20,https://stackoverflow.com/questions/61669621/how-to-use-k-means-algorithm-to-do-attribute-clustering-after-ner
NER combining BIO tokens to form original compound word,"<p>Any way to combine the BIO tokens into compound words.
I implemented this method to form words from BIO schema but this does not work well for words with punctuations. For eg: S.E.C using the below function will join it as S . E . C</p>

<pre><code>def collapse(ner_result):
    # List with the result
    collapsed_result = []


    current_entity_tokens = []
    current_entity = None

    # Iterate over the tagged tokens
    for token, tag in ner_result:

        if tag.startswith(""B-""):
            # ... if we have a previous entity in the buffer, store it in the result list
            if current_entity is not None:
                collapsed_result.append(["" "".join(current_entity_tokens), current_entity])

            current_entity = tag[2:]
            # The new entity has so far only one token
            current_entity_tokens = [token]

        # If the entity continues ...
        elif current_entity_tokens!= None and tag == ""I-"" + str(current_entity):
            # Just add the token buffer
            current_entity_tokens.append(token)
        else:
            collapsed_result.append(["" "".join(current_entity_tokens), current_entity])
            collapsed_result.append([token,tag[2:]])

            current_entity_tokens = []
            current_entity = None

            pass

    # The last entity is still in the buffer, so add it to the result
    # ... but only if there were some entity at all
    if current_entity is not None:
        collapsed_result.append(["" "".join(current_entity_tokens), current_entity])
        collapsed_result = sorted(collapsed_result)
        collapsed_result = list(k for k,_ in itertools.groupby(collapsed_result))


    return collapsed_result
</code></pre>

<p>Another method:-</p>

<p>I tried to detokenize using TreebankWordDetokenizer but it still did not form the original sentence. For eg: Orig: sentence -> <code>parties. \n \n IN WITNESS WHEREOF, the parties hereto</code>
tokenized and detokenized sentence -> <code>parties . IN WITNESS WHEREOF, the parties hereto</code></p>

<p>Another example: Orig: sentence -> <code>Group’s employment, Group shall be</code>
tokenized and detokenized sentence -> <code>Group ’ s employment, Group shall be</code></p>

<p>Note that period and newlines are stripped using the TreebankWordDetokenizer.</p>

<p>Any workaround to form compound words?</p>
","python, named-entity-recognition","<p>A really small fix should do the job:</p>

<pre><code>def join_tokens(tokens):
    res = ''
    if tokens:
        res = tokens[0]
        for token in tokens[1:]:
            if not (token.isalpha() and res[-1].isalpha()):
                res += token  # punctuation
            else:
                res += ' ' + token  # regular word
    return res

def collapse(ner_result):
    # List with the result
    collapsed_result = []


    current_entity_tokens = []
    current_entity = None

    # Iterate over the tagged tokens
    for token, tag in ner_result:

        if tag.startswith(""B-""):
            # ... if we have a previous entity in the buffer, store it in the result list
            if current_entity is not None:
                collapsed_result.append([join_tokens(current_entity_tokens), current_entity])

            current_entity = tag[2:]
            # The new entity has so far only one token
            current_entity_tokens = [token]

        # If the entity continues ...
        elif current_entity_tokens!= None and tag == ""I-"" + str(current_entity):
            # Just add the token buffer
            current_entity_tokens.append(token)
        else:
            collapsed_result.append([join_tokens(current_entity_tokens), current_entity])
            collapsed_result.append([token,tag[2:]])

            current_entity_tokens = []
            current_entity = None

            pass

    # The last entity is still in the buffer, so add it to the result
    # ... but only if there were some entity at all
    if current_entity is not None:
        collapsed_result.append([join_tokens(current_entity_tokens), current_entity])
        collapsed_result = sorted(collapsed_result)
        collapsed_result = list(k for k, _ in itertools.groupby(collapsed_result))

    return collapsed_result
</code></pre>

<h3>Update</h3>

<p>This will solve most of the cases, but as can be seen in comments below there always be outliers. So the complete solution is to track the identity of the word that created certain token. Thus</p>

<pre><code>text=""U.S. Securities and Exchange Commission""
lut = [(token, ix) for ix, word in enumerate(text.split()) for token in tokenize(w)]  

# lut = [(""U"",0), (""."",0), (""S"",0), (""."",0), (""Securities"",1), (""and"",2), (""Exchange"",3), (""Commision"",4)]
</code></pre>

<p>Now, given token index you can know exact word it came from, and simply concatenate tokens that belong to the same word, while adding space when a token belongs to a different word. So the NER result would be something like:</p>

<pre><code>[[""U"",""B-ORG"", 0], [""."",""I-ORG"", 0], [""S"", ""I-ORG"", 0], [""."",""I-ORG"", 0], ['Securities', 'I-ORG', 1], ['and', 'I-ORG', 2], ['Exchange', 'I-ORG',3], ['Commission', 'I-ORG', 4]]
</code></pre>
",1,2,1255,2020-05-11 16:52:10,https://stackoverflow.com/questions/61734999/ner-combining-bio-tokens-to-form-original-compound-word
Data Annotation for Machine Learning,"<p>I am going to develop a machine learning model. I have large data sets(Text). I need overall better accuracy F1 score etc. I am using data annotation tools(Dataturks). Which approach will be good to label the data as single label per entity or multiple label per entity (like there has been 5 times GUI so we have to label it 1 time or 5 times for better overall score). Your help will be highly appreciated.</p>
","machine-learning, nlp, data-annotations, spacy, named-entity-recognition","<p>If you have any duplicate examples where all the features are identical you need to remove them</p>
",0,0,165,2020-05-12 01:47:23,https://stackoverflow.com/questions/61742408/data-annotation-for-machine-learning
Python code for training Arabic spacy NER model not giving result or errors,"<p>This is the code to train the spacy model for NER.  My dataset is Arabic tweets JSON file. I tagged location manually labeled in my dataset by <a href=""https://dataturks.com"" rel=""nofollow noreferrer"">https://dataturks.com</a> machine learning tools but the code is not running. </p>

<p>I used code from this link
<a href=""https://dataturks.com/help/dataturks-ner-json-to-spacy-train.php"" rel=""nofollow noreferrer"">https://dataturks.com/help/dataturks-ner-json-to-spacy-train.php</a></p>

<pre><code>    ############################################  NOTE  ########################################################
#
#           Creates NER training data in Spacy format from JSON downloaded from Dataturks.
#
#           Outputs the Spacy training data which can be used for Spacy training.
#
############################################################################################################
############################################################################################################
def convert_dataturks_to_spacy(dataturks_JSON_FilePath):
    training_data = []
    lines=[]
    with open(dataturks_JSON_FilePath, 'r') as f:
        lines = f.readlines()

        for line in lines:
            data = json.loads(line)
            text = data['content']
            entities = []
            annotations = data['annotation']
            if annotations:
                for annotation in annotations:
                    #only a single point in text annotation.
                    point = annotation['points'][0]
                    labels = annotation['label']

                    # handle both list of labels or a single label.
                    if not isinstance(labels, list):
                        labels = [labels]
                    #print(labels)
                    for label in labels:
                        #dataturks indices are both inclusive [start, end] but spacy is not [start, end)
                        entities.append((point['start'], point['end'] + 1 ,label))


                training_data.append((text, {""entities"" : entities}))

    return training_data
</code></pre>

<blockquote>
  <p>train data</p>
</blockquote>

<pre><code>TRAIN_DATA = convert_dataturks_to_spacy(""/content/drive/My Drive/Colab Notebooks/Name Entity Recognition/NERTweets.json"")
TRAIN_DATA
</code></pre>

<blockquote>
  <p>output of the first three tweets</p>
</blockquote>

<pre><code>    [('طقس حضرموت صور اوليه سيول وادي رخيه',
  {'entities': [(26, 35, 'loc'), (4, 10, 'city')]}),
 ('سيول وادي العف قرية هدى بمديرية حبان بمحافظة شبوة جنوب اليمن اليوم الاحد مايو م تصوير عدنان القميشي',
  {'entities': [(55, 60, 'country'),
    (50, 54, 'pre'),
    (45, 49, 'city'),
    (32, 36, 'loc'),
    (20, 23, 'loc'),
    (5, 14, 'loc')]}),
 ('اول مرة قابلته جدة جاها سيول', {'entities': [(15, 18, 'city')]})]
</code></pre>

<blockquote>
  <p>then the train spacey NER model</p>
</blockquote>

<pre><code>import spacy
import random
################### Train Spacy NER.###########
def train_spacy():
    TRAIN_DATA = convert_dataturks_to_spacy(""/content/drive/My Drive/Colab Notebooks/Name Entity Recognition/NERTweets.json"");
    nlp = spacy.blank('ar')  # create blank Language class
    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(1):
            print(""Statring iteration "" + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.2,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)

    #do prediction
    doc = nlp(""Samsing mobiles below $100"")
    print (""Entities= "" + str(["""" + str(ent.text) + ""_"" + str(ent.label_) for ent in doc.ents]))

train_spacy
</code></pre>

<blockquote>
  <p>output error</p>
</blockquote>

<pre><code>Statring iteration 0
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-8-6b61c2d740cf&gt; in &lt;module&gt;()
----&gt; 1 train_spacy()

2 frames
/usr/local/lib/python3.6/dist-packages/spacy/language.py in _format_docs_and_golds(self, docs, golds)
    470                     err = Errors.E151.format(unexp=unexpected, exp=expected_keys)
    471                     raise ValueError(err)
--&gt; 472                 gold = GoldParse(doc, **gold)
    473             doc_objs.append(doc)
    474             gold_objs.append(gold)

gold.pyx in spacy.gold.GoldParse.__init__()

gold.pyx in spacy.gold.biluo_tags_from_offsets()

ValueError: [E103] Trying to set conflicting doc.ents: '(42, 47, 'loc')' and '(34, 47, 'loc')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.
</code></pre>

<blockquote>
  <p>my results upload on colab google in the link below. where is the
  problem?</p>
</blockquote>

<p><a href=""https://drive.google.com/drive/folders/19t33kW4Dwtbv6s4vfMpa2kNwoVNzSu5I"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/19t33kW4Dwtbv6s4vfMpa2kNwoVNzSu5I</a></p>
","machine-learning, spacy, training-data, named-entity-recognition","<p>spacy doesn't allow overlapping entities,you should remove the overlapping entities
your code it will be:</p>
<pre><code>def convert_dataturks_to_spacy(dataturks_JSON_FilePath):
training_data = []
lines=[]
with open(dataturks_JSON_FilePath, 'r') as f:
    lines = f.readlines()
    for line in lines:
        #line=lines[0]
        data = json.loads(line)
        text = data['content']
        entities = []                   
        annotations=[]
        for annotation in data['annotation']:
            point = annotation['points'][0]
            label = annotation['label']  
            annotations.append((point['start'], point['end'] ,label,point['end']-point['start']))
            
        annotations=sorted(annotations, key=lambda student: student[3],reverse=True) 
        seen_tokens = set() 
        for annotation in annotations:

            start=annotation[0]
            end=annotation[1]
            labels=annotation[2]
            if start not in seen_tokens and end - 1 not in seen_tokens:
     
                seen_tokens.update(range(start, end)) 
                if not isinstance(labels, list):
                    labels = [labels]

                for label in labels:
                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)
                    entities.append((start, end+1  ,label))


        training_data.append((text, {&quot;entities&quot; : entities})
</code></pre>
",0,0,2410,2020-05-13 07:14:15,https://stackoverflow.com/questions/61768494/python-code-for-training-arabic-spacy-ner-model-not-giving-result-or-errors
"Extract medical marker name, values and units from analysed image?","<p>I am using Amazon Textract to analyse anonymous blood tests.
It consists of markers, their values, units, ref interval.</p>

<p>I want to extract them into a dictionary like this:</p>

<pre><code>{""globulin"": [2.8, gidL, [1.0, 4.0]], ""cholesterol"": [161, mg/dL, [120, 240]], .... }
</code></pre>

<p>Here is an example of such OCR produced text: </p>

<pre><code>Name:
Date Perfermed
$/6/2010
DOBESevState:
Date Collected:
05/03/201004.00 PN
Date Lac Meat: 05/03/2010 10.45 A
Eraminer:
PTM
Date Received: $/7/2010 12:13.11A
Tukit No.
8028522035
Abeormal
Normal
Range
CARDLAC RISK
CHOLESTEROL
161.00
120.00 240.00 mg/dL
CHOLESTEROLHDL RATIO
2.39
1.250 5.00
HIGH DENSITY LIPOPROTEINCHDL)
67.30
35.00 75.00 me/dL
LOW DENSITY LIPOPROTEIN (LDL)
78.70
60.00 a 190.00 midI.
TRIGLYCERIDES
75.00
10.00 a 200.00 made
CHEMISTRIES
ALBUMIN
4.40
3.50 5.50 pidl
ALKALINE PHOSPHATASE
49.00
30.00 120.00 UAL
BLOOD UREA NITROGEN (BUN)
17.00
6.00 2500 meidL
CREATININE
0,85
060 1.50 matdL
FRUCTOSAMINE
182
1.20 1.79 mmoV/l
GAMMA GLUTAMYUTRANSFERASE
9.00
2.00 65.00 UIL
GLOBULIN
2.80
1.00 4.00 gidL.
GLUCOSE
61.00
70.00 125.00 me/dl.
HEMOGLOBIN AIC
5.10
3.00 6.00 %
SGOT (AST)
25.00
0.00 41.00 UM
SOPI (ALT)
22.00
0.00 45.00 IMI
TOTAL BILIRUBIN
0.52
0.10 1.20 mmeldi.
TOTAL PROTEIN
720
6.00 8.50 gidl.
1. This sample lab report shows both normal and abnormal results. as well as
acceptable reference ranges for each testing category.
</code></pre>

<p>Please advise what is the best way to extract this information, I have tried Amazon Comprehend medical - it does the job but not for all images.
Tried SpaCy: <a href=""https://github.com/NLPatVCU/medaCy"" rel=""nofollow noreferrer"">https://github.com/NLPatVCU/medaCy</a>,
<a href=""https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da"" rel=""nofollow noreferrer"">https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da</a></p>
","python-3.x, text-extraction, named-entity-recognition, amazon-textract, amazon-comprehend","<p>This might not be a good application of NLP as the text isn't any sort of natural language. Rather, they are structured data that can be extracted using rules. Writing rules is definitely one way to go about this.</p>

<ol>
<li><p>You can first try to do a fuzzy match of the categories on the OCR results, namely ""CARDIAC RISK"" and ""CHEMISTRIES"" to partition the string into their respective categories. </p></li>
<li><p>If you are sure that each entry will take only 3 lines, you can simply partition them by newline and extract the data from there.</p></li>
<li><p>Once you have them split into entries</p></li>
</ol>

<p>Here's some sample code I ran on the data you provided. It requires the <code>fuzzyset</code> package which you can get by running <code>python3 -m pip install fuzzyset</code>. Since some entries don't have units I modified your desired output format slightly and made units a list so it can easily be empty. It also stores random letters found in the third line.</p>

<pre class=""lang-py prettyprint-override""><code>from fuzzyset import FuzzySet

### Load data
with open(""ocr_result.txt"") as f:
    data = f.read()

lines = data.split(""\n"")


### Create fuzzy set
CATEGORIES = (""CARDIAC RISK"", ""chemistries"")
fs = FuzzySet(lines)


### Get the line ranges of each category
cat_ranges = [0] * (len(CATEGORIES) + 1)
for i, cat in enumerate(CATEGORIES):
    match = fs.get(cat)[0]
    match_idx = lines.index(match[1])
    cat_ranges[i] = match_idx

last_idx = lines.index(fs.get(""sample lab report"")[0][1])
cat_ranges[-1] = last_idx


### Read lines in each category
def _to_float(s: str) -&gt; float:
    """"""
    Attempt to convert a string value to float
    """"""
    try:
        f = float(s)
    except ValueError:
        if "","" in s:
            s = s.replace("","", ""."")
            f = float(s)
        else:
            raise ValueError(f""Cannot convert {s} to float."")
    return f


result = {}
for i, cat in enumerate(CATEGORIES):
    result[cat] = {}

    # Ignore the line of the category itself
    s = slice(cat_ranges[i] + 1, cat_ranges[i + 1])
    lines_in_cat = lines[s]

    if len(lines_in_cat) % 3 != 0:
        breakpoint()
        raise ValueError(""Something's wrong"")

    for i in range(0, len(lines_in_cat), 3):
        _name = lines_in_cat[i]
        _value = lines_in_cat[i + 1]
        _line_3 = lines_in_cat[i + 2].split("" "")

        # Convert value to float
        _value = _to_float(_value)

        # Process line 3 to get range and unit
        _range = []
        _unit = []
        for i, v in enumerate(_line_3):
            if v[0].isdigit() and len(_range) &lt; 2:
                _range.append(_to_float(v))
            else:
                _unit.append(v)

        _l = [_value, _unit, _range]
        result[cat][_name] = _l

print(result)
</code></pre>

<p>Output:</p>

<pre><code>{'CARDIAC RISK': {'CHOLESTEROL': [161.0, ['mg/dL'], [120.0, 240.0]], 'CHOLESTEROLHDL RATIO': [2.39, [], [1.25, 5.0]], 'HIGH DENSITY LIPOPROTEINCHDL)': [67.3, ['me/dL'], [35.0, 75.0]], 'LOW DENSITY LIPOPROTEIN (LDL)': [78.7, ['a', 'midI.'], [60.0, 190.0]], 'TRIGLYCERIDES': [75.0, ['a', 'made'], [10.0, 200.0]]}, 'chemistries': {'ALBUMIN': [4.4, ['pidl'], [3.5, 5.5]], 'ALKALINE PHOSPHATASE': [49.0, ['UAL'], [30.0, 120.0]], 'BLOOD UREA NITROGEN (BUN)': [17.0, ['meidL'], [6.0, 2500.0]], 'CREATININE': [0.85, ['matdL'], [60.0, 1.5]], 'FRUCTOSAMINE': [182.0, ['mmoV/l'], [1.2, 1.79]], 'GAMMA GLUTAMYUTRANSFERASE': [9.0, ['UIL'], [2.0, 65.0]], 'GLOBULIN': [2.8, ['gidL.'], [1.0, 4.0]], 'GLUCOSE': [61.0, ['me/dl.'], [70.0, 125.0]], 'HEMOGLOBIN AIC': [5.1, ['%'], [3.0, 6.0]], 'SGOT (AST)': [25.0, ['UM'], [0.0, 41.0]], 'SOPI (ALT)': [22.0, ['IMI'], [0.0, 45.0]], 'TOTAL BILIRUBIN': [0.52, ['mmeldi.'], [0.1, 1.2]], 'TOTAL PROTEIN': [720.0, ['gidl.'], [6.0, 8.5]]}}
</code></pre>
",2,1,532,2020-05-13 16:55:07,https://stackoverflow.com/questions/61780401/extract-medical-marker-name-values-and-units-from-analysed-image
Dictionary value Counting,"<p>Good day,
i'm working on an  project and creating a dic out of the found entities.</p>
<p>Maybe somebody  may help me. i  would be apreciate to learn more about it.
I was thinkof the Counting possibility using <code>counter</code>.</p>
<p>andGreets!!</p>
","dictionary, nlp, spacy, entities, named-entity-recognition","<p>If I understand you correctly, you can achieve this by making some additions to your code </p>

<p>All you gotta do it use a <code>Counter</code> on your <code>perlist</code> and <code>loclist</code>, and store the results in a dict.</p>

<pre class=""lang-py prettyprint-override""><code>...

final_dict = {}  # stores the desired final output in a singe dict
for filepath in files:

    with open(filepath, 'r', encoding='UTF8') as file_to_read:
        some_text = file_to_read.read()
        base_name = os.path.basename(filepath)
        print(base_name)
        doc = nlp(some_text)
        perlist=[]
        loclist=[]
        for ent in doc.ents:
             if ent.label_ == ""PER"":
                perlist.append(str(ent))
             elif ent.label_ == ""LOC"":
                loclist.append(str(ent))

        # Count the number of PER/LOC entities and store in final_dict
        final_list = []  # {""1.txt"": final_list}

        # Count PER entities
        c = Counter(perlist)
        for p, count in c.most_common():
            final_list.append({
                'name': p,
                'type': 'PER',
                'frequency': count
            })

        # Count LOC entities
        c = Counter(loclist)
        for l, count in c.most_common():
            final_list.append({
                'name': l,
                'type': 'LOC',
                'frequency': count
            })

        # store list of results in final_dict.
        # eg. final_dict['1.txt'] = [{'name': 'Englishmen', 'type': 'PER', 'frequency': 1}, ...]
        final_dict[base_name] = final_list

print('Final result', final_dict)
</code></pre>
",0,0,188,2020-05-15 07:25:58,https://stackoverflow.com/questions/61813931/dictionary-value-counting
Get the start and end position of found named entities,"<p>I am very new to ML and also Spacy in general. I am trying to show <strong>Named Entities</strong> from an input text.</p>

<p>This is my method:</p>

<pre class=""lang-py prettyprint-override""><code>def run():

    nlp = spacy.load('en_core_web_sm')
    sentence = ""Hi my name is Oliver!""
    doc = nlp(sentence)

    #Threshold for the confidence socres.
    threshold = 0.2
    beams = nlp.entity.beam_parse(
        [doc], beam_width=16, beam_density=0.0001)

    entity_scores = defaultdict(float)
    for beam in beams:
        for score, ents in nlp.entity.moves.get_beam_parses(beam):
            for start, end, label in ents:
                entity_scores[(start, end, label)] += score

    #Create a dict to store output.
    ners = defaultdict(list)
    ners['text'] = str(sentence)

    for key in entity_scores:
        start, end, label = key
        score = entity_scores[key]
        if (score &gt; threshold):
            ners['extractions'].append({
                ""label"": str(label),
                ""text"": str(doc[start:end]),
                ""confidence"": round(score, 2)
            })

    pprint(ners)
</code></pre>

<p>The above method works fine, and will print something like:</p>

<pre><code>'extractions': [{'confidence': 1.0,
                'label': 'PERSON',
                'text': 'Oliver'}],
'text': 'Hi my name is Oliver'})
</code></pre>

<p>So far so good. Now I am trying to get the actual position of the found named entity. In this case ""Oliver"".</p>

<p>Looking at the <a href=""https://spacy.io/usage/linguistic-features#named-entities-101"" rel=""nofollow noreferrer""><strong>documentation</strong></a>, there is: <code>ent.start_char, ent.end_char</code> available, but if I use that:</p>

<pre><code>""start_position"": doc.start_char,
""end_position"": doc.end_char
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>AttributeError: 'spacy.tokens.doc.Doc' object has no attribute 'start_char'</p>
</blockquote>

<p>Can someone guide me in the right direction?</p>
","python-3.x, nlp, spacy, named-entity-recognition","<p>So I actually found an answer right after posting this question (typical).</p>

<p>I found that I didn't need to save the information into <code>entity_scores</code>, but instead just iterate over the actual found entities <code>ent</code>:</p>

<p>I ended up adding <code>for ent in doc.ents:</code>  instead and this gives me access to all the standard Spacy <a href=""https://spacy.io/api/span#attributes"" rel=""nofollow noreferrer"">attributes</a>. See below:</p>

<pre class=""lang-py prettyprint-override""><code>ners = defaultdict(list)
ners['text'] = str(sentence)
for beam in beams:
    for score, ents in nlp.entity.moves.get_beam_parses(beam):
        for ent in doc.ents:
            if (score &gt; threshold):
                ners['extractions'].append({
                    ""label"": str(ent.label_),
                    ""text"": str(ent.text),
                    ""confidence"": round(score, 2),
                    ""start_position"": ent.start_char,
                    ""end_position"": ent.end_char
</code></pre>

<p>My entire method ends up looking like this:</p>

<pre class=""lang-py prettyprint-override""><code>def run():
    nlp = spacy.load('en_core_web_sm')
    sentence = ""Hi my name is Oliver!""
    doc = nlp(sentence)

    threshold = 0.2
    beams = nlp.entity.beam_parse(
        [doc], beam_width=16, beam_density=0.0001)

    ners = defaultdict(list)
    ners['text'] = str(sentence)
    for beam in beams:
        for score, ents in nlp.entity.moves.get_beam_parses(beam):
            for ent in doc.ents:
                if (score &gt; threshold):
                    ners['extractions'].append({
                        ""label"": str(ent.label_),
                        ""text"": str(ent.text),
                        ""confidence"": round(score, 2),
                        ""start_position"": ent.start_char,
                        ""end_position"": ent.end_char
                    })
</code></pre>
",-1,1,1760,2020-05-19 16:25:20,https://stackoverflow.com/questions/61895995/get-the-start-and-end-position-of-found-named-entities
Convert from Prodigy&#39;s JSONL format for labeled NER to spaCy&#39;s training format?,"<p>I am new to Prodigy and spaCy as well as CLI coding. I'd like to use Prodigy to label my data for an NER model, and then use spaCy in python to create models. </p>

<p>Prodigy outputs in SQLite format. SpaCy takes in this other kind of format, not sure what to call it: </p>

<pre><code>TRAIN_DATA = [
    (
        ""Horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, LABEL)]},
    ),
    (""Do they bite?"", {""entities"": []}),
    (
        ""horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, LABEL)]},
    ),
    (""horses pretend to care about your feelings"", {""entities"": [(0, 6, LABEL)]}),
    (
        ""they pretend to care about your feelings, those horses"",
        {""entities"": [(48, 54, LABEL)]},
    ),
    (""horses?"", {""entities"": [(0, 6, LABEL)]}),
]
</code></pre>

<p>How can I convert from one to the other? It seems like this should be easy, but I cannot find it anywhere. </p>

<p>I have no problem loading in the dataset, just converting. </p>
","sqlite, nlp, spacy, named-entity-recognition, prodigy","<p>Prodigy should export this training format with <code>data-to-spacy</code> as of version 1.9: <a href=""https://prodi.gy/docs/recipes#data-to-spacy"" rel=""nofollow noreferrer"">https://prodi.gy/docs/recipes#data-to-spacy</a></p>
",2,0,2455,2020-05-21 16:00:05,https://stackoverflow.com/questions/61938628/convert-from-prodigys-jsonl-format-for-labeled-ner-to-spacys-training-format
Numeric conversion of textual features in crfsuite,"<p>I was looking at the example code provided in the docs of crfsuite-python and it has the following code for feature defining.</p>
<pre><code>def word2features(sent, i):
word = sent[i][0]
postag = sent[i][1]

features = [
    'bias',
    'word.lower=' + word.lower(),
    'word[-3:]=' + word[-3:],
    'word[-2:]=' + word[-2:],
    'word.isupper=%s' % word.isupper(),
    'word.istitle=%s' % word.istitle(),
    'word.isdigit=%s' % word.isdigit(),
    'postag=' + postag,
    'postag[:2]=' + postag[:2],
]
if i &gt; 0:
    word1 = sent[i-1][0]
    postag1 = sent[i-1][1]
    features.extend([
        '-1:word.lower=' + word1.lower(),
        '-1:word.istitle=%s' % word1.istitle(),
        '-1:word.isupper=%s' % word1.isupper(),
        '-1:postag=' + postag1,
        '-1:postag[:2]=' + postag1[:2],
    ])
else:
    features.append('BOS')
    
if i &lt; len(sent)-1:
    word1 = sent[i+1][0]
    postag1 = sent[i+1][1]
    features.extend([
        '+1:word.lower=' + word1.lower(),
        '+1:word.istitle=%s' % word1.istitle(),
        '+1:word.isupper=%s' % word1.isupper(),
        '+1:postag=' + postag1,
        '+1:postag[:2]=' + postag1[:2],
    ])
else:
    features.append('EOS')
            
return features
</code></pre>
<p>I understand that features such as isupper() can be either 0 or 1 but for features such as word[-2:]  which are characters ,how are they converted to numeric terms?</p>
","python, machine-learning, nlp, named-entity-recognition, python-crfsuite","<p>CRF trains upon sequence of input data to learn transitions from one state (label) to another. To enable such an algorithm, we need to define features which take into account different transitions. In the function word2features() below, we transform each word into a feature dictionary depicting the following attributes or features:</p>
<pre><code>lower case of word
suffix containing last 3 characters
suffix containing last 2 characters
flags to determine upper-case, title-case, numeric data and POS tag
</code></pre>
<p>We also attach attributes related to previous and next words or tags to determine beginning of sentence (BOS) or end of sentence (EOS)</p>
",0,1,133,2020-05-23 13:13:55,https://stackoverflow.com/questions/61972579/numeric-conversion-of-textual-features-in-crfsuite
How to add explanation/description for a newly defined label in SpaCy&#39;s NER?,"<p>I'm creating a new label called GADGET to identify gadgets like Apple iPhone, Samsung TV etc. How do I add a custom description for the new label ?</p>

<p>For example, if label='ORG' &amp; we give spacy.explain(label), it gives a description for ORG. How can I add similarly for a new label?</p>
","nlp, spacy, named-entity-recognition","<p>Go to the spacy model in your project and you can find <strong>glossary.py</strong> file <code>spaCy/spacy/glossary.py</code> there You can define your label and save it. Then You can get explanation of your label using <code>spacy.explain(label)</code></p>
",1,0,125,2020-05-27 07:39:21,https://stackoverflow.com/questions/62037824/how-to-add-explanation-description-for-a-newly-defined-label-in-spacys-ner
SpaCy different Language Models,"<p>I'm making some progress:) developing my litle OCR Project.
I was wondering if my idea is possible in this case!</p>
<p>After extracting the Text from a Images (ocr), I use nlp (spacy) to identify two Entities (LOCation and PERson). I write to a Dictionary and later in a JSON Data. That works good.</p>
<p>Now I'm wondering if I can improve my identified Entities.
One way I can imagine is to use the right Language Model for the text.
I have varies Texts in German, English,Spanish and French.
At the moment I'm using the
But now I have no idea how to put langdetect into this</p>
<p>Have a great week!
Greets</p>
","spacy, detect, named-entity-recognition","<p>Here is a link that you might find useful when it comes to detecting a language (There are multiple options including langdetect) - <a href=""https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language"">How to detect language</a></p>

<p>You can create a dictionary with the languages you plan to detect and match it with langdetect's output. I guess you have the rest sorted out.</p>
",0,0,190,2020-05-27 08:03:02,https://stackoverflow.com/questions/62038216/spacy-different-language-models
Mapping entities between two disparate company datasets,"<p>I have several datasets containing data about companies: 
- entity_structure (columns: entity_id, parent_entity_id, ultimate_parent_id)
- entity_addresses (columns: address_id, entity_id, location_city, state, postal_code, zip, street, ...)
- vendor (columns: vendor_id, parent_vendor_id, top_vendor_id, cnt_children, orgtype_id, geo_id, name, email, ...)
- geo (columns: geo_id, zipcode, is_primary, latitude, longtitude, elevation, state, ...)
- entity_coverage (entity_id, name, proper_name, sic_code, industry_code, sector_code, iso, ...)</p>

<p>I need to automatically map entities between the datasets, for example, there may be a company named ""Google"" in one data set, and a company named ""Google 123"" in another. I need to be able to determine with a high enough confidence that those are the same entities. In most cases, the data does not share a unique key. In most cases, the data does not share a unique key.</p>

<p>Would named entity linking be the best approach here? Are there any Python examples on how to approach this problem? </p>
","python-3.x, nlp, named-entity-recognition","<p>Based on your example, <a href=""https://pypi.org/project/python-Levenshtein/"" rel=""nofollow noreferrer"">Levenshtein Distance</a> may help.</p>
",1,0,221,2020-05-29 03:08:33,https://stackoverflow.com/questions/62078036/mapping-entities-between-two-disparate-company-datasets
Is it possible to improve spaCy&#39;s similarity results with custom named entities?,"<p>I've found that spaCy's similarity does a decent job of comparing my documents using ""en_core_web_lg"" out-of-the-box.  </p>

<p>I'd like to tighten up relationships in some areas and thought adding custom NER labels to the model would help, but my results before and after show no improvements, even though I've been able to create a test set of custom entities.</p>

<p>Now I'm wondering, was my theory completely wrong, or could I simply be missing something in my pipeline?  </p>

<p>If I was wrong, what's the best approach to improve results?  Seems like some sort of custom labeling should help.</p>

<p>Here's an example of what I've tested so far:</p>

<pre><code>import spacy
from spacy.pipeline import EntityRuler
from spacy.tokens import Doc
from spacy.gold import GoldParse

nlp = spacy.load(""en_core_web_lg"")

docA = nlp(""Add fractions with like denominators."")
docB = nlp(""What does one-third plus one-third equal?"")

sim_before = docA.similarity(docB)
print(sim_before)
</code></pre>

<p><strong><em>0.5949629181460099</em></strong></p>

<p>^^ Not too shabby, but I'd like to see results closer to 0.85 in this example.<br>
So, I use EntityRuler and add some patterns to try and tighten up the relationships:</p>

<pre><code>ruler = EntityRuler(nlp)
patterns = [
    {""label"": ""ADDITION"", ""pattern"": ""Add""},
    {""label"": ""ADDITION"", ""pattern"": ""plus""},
    {""label"": ""FRACTION"", ""pattern"": ""one-third""},
    {""label"": ""FRACTION"", ""pattern"": ""fractions""},
    {""label"": ""FRACTION"", ""pattern"": ""denominators""},

]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler, before='ner')
print(nlp.pipe_names)
</code></pre>

<p><strong><em>['tagger', 'parser', 'entity_ruler', 'ner']</em></strong></p>

<p>Adding GoldParse seems to be important, so I added the following and updated NER:</p>

<pre><code>doc1 = Doc(nlp.vocab, [u'What', u'does', u'one-third', u'plus', u'one-third', u'equal'])
gold1 = GoldParse(doc1, [u'0', u'0', u'U-FRACTION', u'U-ADDITION', u'U-FRACTION', u'O'])

doc2 = Doc(nlp.vocab, [u'Add', u'fractions', u'with', u'like', u'denominators'])
gold2 = GoldParse(doc2, [u'U-ADDITION', u'U-FRACTION', u'O', u'O', u'U-FRACTION'])

ner = nlp.get_pipe(""ner"")
losses = {}
optimizer = nlp.begin_training()
ner.update([doc1, doc2], [gold1, gold2], losses=losses, sgd=optimizer)
</code></pre>

<p><strong><em>{'ner': 0.0}</em></strong></p>

<p>You can see my custom entities are working, but the test results show zero improvement:</p>

<pre><code>test1 = nlp(""Add fractions with like denominators."")
test2 = nlp(""What does one-third plus one-third equal?"")

print([(ent.text, ent.label_) for ent in test1.ents])
print([(ent.text, ent.label_) for ent in test2.ents])

sim = test1.similarity(test2)
print(sim)
</code></pre>

<p><strong><em>[('Add', 'ADDITION'), ('fractions', 'FRACTION'), ('denominators', 'FRACTION')]<br>
[('one-third', 'FRACTION'), ('plus', 'ADDITION'), ('one-third', 'FRACTION')]<br>
0.5949629181460099</em></strong></p>

<p>Any tips would be greatly appreciated!</p>
","spacy, similarity, named-entity-recognition","<p>I found my solution was nestled in this tutorial: <a href=""https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/"" rel=""nofollow noreferrer"">Text Classification in Python Using spaCy</a>, which generates a BoW matrix for spaCy's text data by using SciKit-Learn's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a>.</p>
<p>I avoided sentiment analysis tutorials, due to binary classification, since I need support for multiple categories.  The trick was to set <strong>multi_class='auto'</strong> on the LogisticRegression linear model, and to use <strong>average='micro'</strong> on the precision score and precision recall, so all my text data, like entities, were leveraged:</p>
<pre><code>classifier = LogisticRegression(solver='lbfgs', multi_class='auto')
</code></pre>
<p>and...</p>
<pre><code>print(&quot;Logistic Regression Accuracy:&quot;,metrics.accuracy_score(y_test, predicted))
print(&quot;Logistic Regression Precision:&quot;,metrics.precision_score(y_test, predicted,average='micro'))
print(&quot;Logistic Regression Recall:&quot;,metrics.recall_score(y_test, predicted,average='micro'))
</code></pre>
<p>Hope this helps save someone some time!</p>
",0,1,644,2020-05-29 18:24:56,https://stackoverflow.com/questions/62092445/is-it-possible-to-improve-spacys-similarity-results-with-custom-named-entities
ScispaCy in google colab,"<p>I am trying to build  <strong>NER</strong> model of clinical data using <strong>ScispaCy</strong> in <strong>colab</strong>. I have installed packages like this.</p>

<pre><code>!pip install spacy
!pip install scispacy
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz       #pip install &lt;Model URL&gt;```
</code></pre>

<p>Then I imported both using</p>

<pre><code>import scispacy
import spacy
import en_core_sci_md
</code></pre>

<p>then used following code to display sentences and entities</p>

<pre><code>nlp = spacy.load(""en_core_sci_md"")
text =""""""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity. They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC)"""""" 
doc = nlp(text)
print(list(doc.sents))
print(doc.ents)
</code></pre>

<p>I am getting the following error</p>

<pre><code>OSError: [E050] Can't find model 'en_core_sci_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>

<p>I don't know why this error is coming, I followed all codes from the official GitHub post of ScispaCy. Any help would be appreciated.
Thanks in advance.</p>
","python, nlp, spacy, named-entity-recognition","<p>I hope I am not too late... I believe you are very close to the correct approach.</p>
<p>I will write my answer in steps and you can choose where to stop.</p>
<p>Step 1)</p>
<pre><code>#Install en_core_sci_lg package from the website of spacy  (large corpus), but you can also use en_core_sci_md for the medium corpus.
       
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz 
</code></pre>
<p>Step 2)</p>
<pre><code># Import the large dataset
import en_core_sci_lg
</code></pre>
<p>Step 3)</p>
<pre><code># Identify entities
nlp = en_core_sci_lg.load()
doc = nlp(text)
displacy_image = displacy.render(doc, jupyter = True, style = &quot;ent&quot;)
</code></pre>
<p>Step 4)</p>
<pre><code>#Print only the entities
print(doc.ents)
</code></pre>
<p>Step 5)</p>
<pre><code># Save the result 
save_res = [doc.ents]
save_res
</code></pre>
<p>Step 6)</p>
<pre><code>#Save the results to a dataframe
df_save_res = pd.DataFrame(save_res)
df_save_res
</code></pre>
<p>Step 7)</p>
<pre><code># In case that you want to visualise the dependency parse
  displacy_image = displacy.render(doc, jupyter = True, style = &quot;dep&quot;)
</code></pre>
",3,3,2652,2020-05-31 04:33:28,https://stackoverflow.com/questions/62111614/scispacy-in-google-colab
how to use ktrain for NER Offline?,"<p>I have trained my English model following this notebook (<a href=""https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-06-sequence-tagging.ipynb"" rel=""nofollow noreferrer"">https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-06-sequence-tagging.ipynb</a>). I  am able to save my pretrained model and run it with no problem. </p>

<p>However, I need to run it again but OFFLINE and it is not working, I  understand that I need to download the file and do something similar to what is done here. </p>

<p><a href=""https://github.com/huggingface/transformers/issues/136"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/136</a></p>

<p>However, I  am not able to understand where do  I need to change the settings of ktrain.</p>

<p>I  run this:</p>

<pre><code>ktrain.load_predictor('Functions/my_english_nermodel')
</code></pre>

<p>and this is  the error I get: </p>

<pre><code>Traceback (most recent call last):
  File ""Z:\Functions\NER.py"", line 155, in load_bert
    reloaded_predictor= ktrain.load_predictor('Z:/Functions/my_english_nermodel')
  File ""C:\Program Files\Python37\lib\site-packages\ktrain\core.py"", line 1316, in load_predictor
    preproc = pickle.load(f)
  File ""C:\Program Files\Python37\lib\site-packages\ktrain\text\ner\anago\preprocessing.py"", line 76, in __setstate__
    if self.te_model is not None: self.activate_transformer(self.te_model, layers=self.te_layers)
  File ""C:\Program Files\Python37\lib\site-packages\ktrain\text\ner\anago\preprocessing.py"", line 100, in activate_transformer
    self.te = TransformerEmbedding(model_name, layers=layers)
  File ""C:\Program Files\Python37\lib\site-packages\ktrain\text\preprocessor.py"", line 1095, in __init__
    self.tokenizer = self.tokenizer_type.from_pretrained(model_name)
  File ""C:\Program Files\Python37\lib\site-packages\transformers\tokenization_utils.py"", line 903, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File ""C:\Program Files\Python37\lib\site-packages\transformers\tokenization_utils.py"", line 1008, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'bert-base-uncased' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'bert-base-dutch-cased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.

Process finished with exit code 1
</code></pre>
","python, tensorflow, offline, named-entity-recognition, bert-language-model","<p>More generally, the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">transformers</a>-based pretrained models are downloaded to <code>&lt;home_directory&gt;/.cache/torch/transformers</code>. For instance, on Linux, this will be <code>/home/&lt;user_name&gt;/.cache/torch/transformers</code>.  </p>

<p>As indicated in the answer above, to reload the <em>ktrain</em> <code>predictor</code> on a machine with no internet access (for <code>ktrain</code> models that utilize models from <code>transformers</code> library), you'll need copy the model files in that folder to the same location on the new machine.</p>
",1,0,552,2020-06-02 10:40:18,https://stackoverflow.com/questions/62150139/how-to-use-ktrain-for-ner-offline
How do I host CoreNLP server with caseless models?,"<p>I'm trying to host a CoreNLP server but with the caseless models but I don't think I was successful and the official site doesn't have example hosting such model.</p>

<p>I'm currently hosting with:</p>

<pre><code>java -mx4g \
           -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
           -port 9000 \
           -timeout 15000
</code></pre>

<p>but this is the default way of hosting which doesn't use the caseless models. I checked the app log and it was loading the standard models instead of caseless models:</p>

<pre><code>[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
</code></pre>

<p>According to <a href=""https://stanfordnlp.github.io/CoreNLP/caseless.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/caseless.html</a>, I have downloaded the english models jar file and put it under the corenlp module folder, but I don't know exactly how to specify and use those for server hosting.</p>

<p>In the client side, I'm doing the following:</p>

<pre><code>import requests

r = requests.post('http://[::]:9000/?properties={""annotators"":""tokenize,ssplit,truecase,pos,ner"",""outputFormat"":""json""}', 
                  data=""show me hotels in toronto for next weekend"")
print(r.text)
</code></pre>

<p>The truecase is working, but I don't see the caseless models being used.</p>

<p>Any help would be appreciated.</p>
","python, nlp, stanford-nlp, named-entity-recognition","<p>You need to pass the property <code>""ner.model"": ""edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz""</code></p>

<p>Also you may want to use Stanza for accessing the Stanford CoreNLP server.</p>

<p>Details here: <a href=""https://stanfordnlp.github.io/stanza/corenlp_client.html#overview"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/corenlp_client.html#overview</a></p>
",1,1,198,2020-06-02 14:24:24,https://stackoverflow.com/questions/62154244/how-do-i-host-corenlp-server-with-caseless-models
Merge Tuples in a list - Spacy Trainset related,"<p>I have a 'n'(10K or more) tuples in a list like the following below (SpaCy's training format) -</p>

<pre><code>[
('Apple are selling their products on Amazon', {'entities': [(0, 5, 'PROD-ORG')]}),
('Apple are selling their products on Amazon', {'entities': [(36, 42, 'ECOM-ORG')]})
]
</code></pre>

<p>The plan is to group same sentences and merge the dictionaries. I obviously went for the brute-force looping idea but that is very slow if I have 10-25K data. Any better/optimal ways to do this?</p>

<p>Desired output -</p>

<pre><code>[('Apple are selling their products on Amazon', {'entities': [(0, 5, 'PROD-ORG'), (36, 42, 'ECOM-ORG')]})]
</code></pre>
","python, python-3.x, tuples, spacy, named-entity-recognition","<p>Use the fact that <code>str</code> in python can be hashed/indexed.</p>

<p>Here I am using a dictionary with key as the string or 1st element of your tuple</p>

<p>If you have memory limitations, you can batch it out OR use open-source platform like Google Colab</p>

<pre><code>temp = [
('Apple are selling their products on Amazon', {'entities': [(0, 5, 'PROD-ORG')]}),
('Apple are selling their products on Amazon', {'entities': [(36, 42, 'ECOM-ORG')]})
]
data = {}
for i in temp:
    if i[0] in data:data[i[0]]['entities'].append(i[1]['entities'][0])
    else: data[i[0]]= i[1]
temp = [(k,v) for i,(k,v) in enumerate(data.items())]
print(temp)
</code></pre>

<pre><code>[('Apple are selling their products on Amazon', {'entities': [(0, 5, 'PROD-ORG'), (36, 42, 'ECOM-ORG')]})]
</code></pre>
",1,0,118,2020-06-05 03:29:55,https://stackoverflow.com/questions/62207662/merge-tuples-in-a-list-spacy-trainset-related
Removing compound worded named entities from a document using spacy,"<p>How can one use spaCy to remove named entities from a text if some of the named entities are compound words?</p>
<p>I am aware of the question at <a href=""https://stackoverflow.com/questions/59313461/removing-named-entities-from-a-document-using-spacy/62563019#62563019"">Removing named entities from a document using spacy</a>
I believe this is not a duplicate of that question, because the accepted answer posted there will fail if the Named Entities are compound words.
Example code for why the accepted answer to the linked question fails appears below.</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

nlp = spacy.load('en_core_web_sm')

text_data = 'This is a text document that speaks about entities like New York and Nokia'

document = nlp(text_data)

text_no_namedentities = []

ents = [e.text for e in document.ents]
for item in document:
    if item.text in ents:
        pass
    else:
        text_no_namedentities.append(item.text)
print(&quot; &quot;.join(text_no_namedentities))
</code></pre>
<p>Output:</p>
<pre><code>This is a text document that speaks about entities like New York and'
</code></pre>
<p>What is the best way to remove named entities from text, including compound word entities?
Thanks.</p>
<p>P.S. I would have posted this as a comment to the linked question, but as a new user I lack sufficient reputation to comment.  I tried posting it as an answer there, but since I don't know the solution (only that the accepted answer will fail with compound words) it wasn't a good answer and was deleted.  A new question seemed to be the last recourse left to me, but if this was not appropriate any advice as to the correct course of action in a situation like this would be appreciated.</p>
","python, nlp, spacy, named-entity-recognition","<p>If a token is part of a named entity, its <code>token.ent_type_</code> attribute will be of that entity type even if it is part of a compound named entity. You can achieve what you are looking for using the code below as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

nlp = spacy.load('en_core_web_sm')

text_data = 'This is a text document that speaks about entities like New York and Nokia. My name is John Smith'

doc = nlp(text_data)

# You can add other Named Entities types that you would want to remove to the list
text_without_ne = [token.text for token in doc if token.ent_type_ not in ['GPE', 'PERSON']]

print(&quot; &quot;.join(text_without_ne))
</code></pre>
<p><strong>Output</strong></p>
<pre><code>This is a text document that speaks about entities like and . My name is
</code></pre>
",0,0,886,2020-06-25 10:58:44,https://stackoverflow.com/questions/62573772/removing-compound-worded-named-entities-from-a-document-using-spacy
Training SpaCy NER with a custom dataset,"<p>I have followed <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">this</a> SpaCy tutorial for training a custom dataset. My dataset is a gazetteer. Therefore, I made my training data as the following.</p>
<pre><code>TRAIN_DATA = [
(&quot;Where is Abbess&quot;,{&quot;entities&quot;:[(9, 15,&quot;GPE&quot;)]}),
(&quot;Where is Abbey Pass&quot;,{&quot;entities&quot;:[(9, 19,&quot;LOC&quot;)]}),
(&quot;Where is Abbot&quot;,{&quot;entities&quot;:[(9, 14,&quot;GPE&quot;)]}),
(&quot;Where is Abners Head&quot;,{&quot;entities&quot;:[(9, 29,&quot;LOC&quot;)]}),
(&quot;Where is Acheron Flat&quot;,{&quot;entities&quot;:[(9, 21,&quot;LOC&quot;)]}),
(&quot;Where is Acheron River&quot;,{&quot;entities&quot;:[(9, 22,&quot;LOC&quot;)]})
]
</code></pre>
<p>I used <code>'en_core_web_sm'</code> for the training, not a blank model.</p>
<pre><code>model = 'en_core_web_sm'
output_dir=Path(path)
n_iter=20
</code></pre>
<p>After training for 20 epocs, I tried to make a prediction with the trained model. The following is the output that I get.</p>
<pre><code>test_text = &quot;Seven people, including teenagers, have been taken to hospital after their car crashed in the mid-Canterbury town of Rakaia.&quot;

Seven people, including teenagers 0 33 GPE
the mid-Canterbury town of Rakaia.. 90 125 GPE
</code></pre>
<p>I did a prediction using <code>'en_core_web_sm'</code> for the same test_text. The output is the following.</p>
<pre><code>Seven 0 5 CARDINAL
mid-Canterbury 94 108 DATE
Rakaia 117 123 GPE
</code></pre>
<p>Can someone please instruct me on the errors that I am making while training SpaCy?</p>
","python-3.x, nlp, spacy, named-entity-recognition","<p>The reason for the poor results is due to a concept called <code>catastrophic forgetting</code>. You can get more information <a href=""https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"" rel=""nofollow noreferrer"">here</a>.</p>
<p><strong>tl;dr</strong></p>
<p>As you are training your <code>en_core_web_sm</code> model with new entities, it is forgetting what it previously learnt.</p>
<p>In order to make sure that the old learnings are not forgotten, you need to feed the model examples of the other types of entities too during retraining. By doing this, you will ensure that the model does not self tune and skew itself to predict everything as the new entity being trained.</p>
<p>You can read about possible solutions that can be implemented <a href=""https://github.com/explosion/spaCy/issues/2204#issuecomment-382553159"" rel=""nofollow noreferrer"">here</a></p>
",0,0,485,2020-06-25 22:33:43,https://stackoverflow.com/questions/62585306/training-spacy-ner-with-a-custom-dataset
Is it possible to subclass a spacy entity type?,"<p>I'd like to subclass the existing GPE, so that it differentiates between GPE-Nation, USA, and GPE-City, New York. I see in the docs how to create new entity types, but not how to subclass what's already there. Can this be done, and if so, how? Thanks.</p>
","python, machine-learning, nlp, spacy, named-entity-recognition","<p>You cannot subclass an NER type. You have to train custom NER types for that. In my opinion, I would get the GPE entities, and then separate them into Nation and City based on a dictionary lookup. There are finite number of major cities and nations in the world, therefore a dictionary lookup would be more suitable here than creating a generalization.</p>
",0,-1,105,2020-06-26 17:41:03,https://stackoverflow.com/questions/62600140/is-it-possible-to-subclass-a-spacy-entity-type
Python named entity recognition (NER): Replace named entities with labels,"<p>I'm new to Python NER and am trying to replace named entities in text input with their labels.</p>
<pre><code>from nerd import ner
input_text = &quot;&quot;&quot;Stack Overflow is a question and answer site for professional and enthusiast programmers. It is a privately held website, the flagship site of the Stack Exchange Network,[5][6][7] created in 2008 by Jeff Atwood and Joel Spolsky.&quot;&quot;&quot;
doc = ner.name(input_text, language='en_core_web_sm')
text_label = [(X.text, X.label_) for X in doc]
print(text_label)
</code></pre>
<p>The output is: <code>[('2008', 'DATE'), ('Jeff Atwood', 'PERSON'), ('Joel Spolsky', 'PERSON')]</code></p>
<p>I can then extract the people, for example:</p>
<pre><code>people = [i for i,label in text_label if 'PERSON' in label] 
print(people)
</code></pre>
<p>to get <code>['Jeff Atwood', 'Joel Spolsky']</code>.</p>
<p>My question is how can I replace identified named entities in the original input text so that the result is:</p>
<p><code>Stack Overflow is a question and answer site for professional and enthusiast programmers. It is a privately held website, the flagship site of the Stack Exchange Network,[5][6][7] created in DATE by PERSON and PERSON.</code></p>
<p>Thanks so much!</p>
","python, nlp, spacy, named-entity-recognition","<p>You can loop over <code>text_label</code> and replace each text with the corresponding label</p>
<pre><code>for text, label in text_label:
    input_text = input_text.replace(text, label)

print(input_text)
</code></pre>
",1,2,1763,2020-07-14 22:01:03,https://stackoverflow.com/questions/62904623/python-named-entity-recognition-ner-replace-named-entities-with-labels
"In Stanford CoreNlp, why are not all proper nouns (NNP) also named entities","<p>I use Stanford CoreNlp for Names Entity Recognition (NER). I've noticed that in some cases that it's not 100% which is fine and not surprising. However, even if a, say, single-word named entity is not recognized (i.e., the label is <code>O</code>), it has the tag <code>NNP</code> (proper noun).</p>
<p>For example, given the example sentence &quot;The RestautantName in New York is the best outlet.&quot;, <code>nerTags()</code> yields <code>[O, O, O, LOCATION, LOCATION, O, O, O, O, O]</code> only correctly recognizing &quot;New York&quot;. The parse tree for this sentence looks like</p>
<pre><code>(ROOT
  (S
    (NP
      (NP (DT The) (NNP RestautantName))
      (PP (IN in)
        (NP (NNP New) (NNP York))))
    (VP (VBZ is)
      (NP (DT the) (JJS best) (NN outlet)))
    (. .)))
</code></pre>
<p>so &quot;RestaurantName&quot; is a proper noun (<code>NNP</code>)</p>
<p>When I look up the definition of a proper noun, it sounds very close to a named entity. What's the difference?</p>
","nlp, stanford-nlp, named-entity-recognition","<p>The parser is trained on parse treebank data and the named entity recognizer is trained on separate named entity data for PERSON, LOCATION, ORGANIZATION, MISC.</p>
<p>I would've thought that RestaurantName might get marked as MISC, but if it's not getting tagged it means that there are not really examples like that in the training data for named entities. The key point here is that the parse decisions and named entity decisions are made completely independently of each other by separate models trained on separate data.</p>
",2,1,1046,2020-07-31 04:44:05,https://stackoverflow.com/questions/63185929/in-stanford-corenlp-why-are-not-all-proper-nouns-nnp-also-named-entities
Split sentence into words pandas and keep tags,"<p>I have a Pandas dataframe like</p>
<pre><code>Text                  label      value
board members         A1          NaN
a really long sent    A2          B2
</code></pre>
<p><strong>Result:</strong> I would like to unnest the sentences and keep each label per word-split, like this</p>
<pre><code>Sentence    Text         label      value
   1        board          A1        NaN
   1        members        A1        NaN
   2          a            A2        B2
   2        really         A2        B2 
   2         long          A2        B2 
   2         sent          A2        B2
</code></pre>
<p>Extra: If possible, I would like to extract a POS (Part of Speech) tagging of each word in a new column_</p>
<pre><code>Sentence    Text         label      value    POS
   1        board          A1        NaN     Something
   1        members        A1        NaN     Something
   2          a            A2        B2      Something
   2        really         A2        B2      etc
   2         long          A2        B2 
   2         sent          A2        B2
</code></pre>
","python, pandas, named-entity-recognition, sentence","<p>You can convert <code>Text</code> to list then <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a>:</p>
<pre><code>df['Text'] = df['Text'].str.split()
df = df.explode(&quot;Text&quot;)

print(df)

      Text        label value
0    board  A1            NaN
0  members  A1            NaN
1        a  A2             B2
1   really  A2             B2
1     long  A2             B2
1     sent  A2             B2
</code></pre>
",1,0,1000,2020-08-08 09:03:54,https://stackoverflow.com/questions/63313590/split-sentence-into-words-pandas-and-keep-tags
find sentence among sentence of words,"<p>I have two Pandas data frames. One where all the sentences are split by words vertically and look like this</p>
<pre><code>Sentence | Text   
1            I
1          like
1          Cats
2          The
2          man
2         plays
2         soccer
2         today
</code></pre>
<p>And the other data frame looks like this:</p>
<pre><code>ID      | Text         | Tags
1         plays soccer   sport
2          man           human
3         like cats     interest
</code></pre>
<p>What I would like is to map the Tags to the first data frame for the words in the sentence that match both places, and those who does not match get an &quot;O&quot; to symbolize it.</p>
<pre><code>Sentence | Text        | Tags 
1          I             O
1          like          interest
1          Cats          interest
2          The           O
2          man           human
2          plays         sport
2          soccer        sport
2          today         O
</code></pre>
","python, pandas, dataframe, named-entity-recognition","<p>Assume <code>df1</code> is your first table and <code>df2</code> is your second (where <code>ID</code> is the index) then you can:</p>
<ul>
<li>Explode your second table (<code>str.split()</code> + <code>explode()</code>)</li>
<li><code>merge()</code> with a left join</li>
<li><code>fillna()</code> the <code>NaN</code> with <code>O</code></li>
</ul>
<p>E.g.:</p>
<pre><code>In []:
df1.merge(df2.assign(Text=df2.Text.str.split()).explode('Text'), 'left').fillna('O')

Out[]:
   Sentence    Text      Tags
0         1       I         O
1         1    like  interest
2         1    Cats         O
3         2     The         O
4         2     man     human
5         2   plays     sport
6         2  soccer     sport
7         2   today         O
</code></pre>
<p>This is currently case sensitive but it is an easy exercise to merge on a column that is <code>str.lower()</code> of <code>Text</code>.</p>
",0,0,52,2020-08-15 19:52:31,https://stackoverflow.com/questions/63430237/find-sentence-among-sentence-of-words
AWS costum entity recognition: Wrong ARN-Endpoint,"<p>I try to use the custom entity recognition I just trained on Amazon Web Service (AWS).
The training worked so far:
<a href=""https://i.sstatic.net/QTkyV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QTkyV.png"" alt=""The trained Entitiy Recognizer"" /></a></p>
<p>However, if i try to recognize my entities with AWS Lambda (code below) with the given ARN-Endpoint I get the following error (even tho AWS should use the latest Version of the botocore/boto3 framework &quot;EntpointArn&quot; is not available <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_entities"" rel=""nofollow noreferrer"">(Docs)</a>):</p>
<pre><code>Response:
{
  &quot;errorMessage&quot;: &quot;Parameter validation failed:\nUnknown parameter in input: \&quot;EndpointArn\&quot;, must be one of: Text, LanguageCode&quot;,
  &quot;errorType&quot;: &quot;ParamValidationError&quot;,
  &quot;stackTrace&quot;: [
    &quot;  File \&quot;/var/task/lambda_function.py\&quot;, line 21, in lambda_handler\n    entities = client.detect_entities(\n&quot;,
    &quot;  File \&quot;/var/runtime/botocore/client.py\&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n&quot;,
    &quot;  File \&quot;/var/runtime/botocore/client.py\&quot;, line 607, in _make_api_call\n    request_dict = self._convert_to_request_dict(\n&quot;,
    &quot;  File \&quot;/var/runtime/botocore/client.py\&quot;, line 655, in _convert_to_request_dict\n    request_dict = self._serializer.serialize_to_request(\n&quot;,
    &quot;  File \&quot;/var/runtime/botocore/validate.py\&quot;, line 297, in serialize_to_request\n    raise ParamValidationError(report=report.generate_report())\n&quot;
  ]
}
</code></pre>
<p>I fixed this error with the first 4 lines in my code:</p>
<pre class=""lang-py prettyprint-override""><code>#---The hack I found on stackoverflow----
import sys
from pip._internal import main

main(['install', '-I', '-q', 'boto3', '--target', '/tmp/', '--no-cache-dir', '--disable-pip-version-check'])
sys.path.insert(0,'/tmp/')

#----------------------------------------

import json
import boto3

client = boto3.client('comprehend', region_name='us-east-1')

text = &quot;Thats my nice text with different entities!&quot;

entities = client.detect_entities(
            Text = text,
            LanguageCode = &quot;de&quot;, #If you specify an endpoint, Amazon Comprehend uses the language of your custom model, and it ignores any language code that you provide in your request.
            EndpointArn = &quot;arn:aws:comprehend:us-east-1:215057830319:entity-recognizer/MyFirstRecognizer&quot;
)
</code></pre>
<p>However, I still get one more error I cannot fix:</p>
<pre><code>Response:
{
  &quot;errorMessage&quot;: &quot;An error occurred (ValidationException) when calling the DetectEntities operation: 1 validation error detected: Value 'arn:aws:comprehend:us-east-1:XXXXXXXXXXXX:entity-recognizer/MyFirstRecognizer' at 'endpointArn' failed to satisfy constraint: Member must satisfy regular expression pattern: arn:aws(-[^:]+)?:comprehend:[a-zA-Z0-9-]*:[0-9]{12}:entity-recognizer-endpoint/[a-zA-Z0-9](-*[a-zA-Z0-9])*&quot;,
  &quot;errorType&quot;: &quot;ClientError&quot;,
  &quot;stackTrace&quot;: [
    &quot;  File \&quot;/var/task/lambda_function.py\&quot;, line 25, in lambda_handler\n    entities = client.detect_entities(\n&quot;,
    &quot;  File \&quot;/tmp/botocore/client.py\&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n&quot;,
    &quot;  File \&quot;/tmp/botocore/client.py\&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n&quot;
  ]
}
</code></pre>
<p>This error also occurs if I use the NodeJS framework with the given endpoint. The funny thing I should mention is that every ARN-Endpoint I found (in tutorials) looks exactly like mine and do not match with the regex-pattern returned as error.</p>
<p>I'm not quite sure if I do something wrong here or if it is a bug on the AWS-Cloud (or SDK).. Maybe somebody can reproduce this error and/or find a solution (or even a hack) for this problem</p>
<p>Cheers</p>
","amazon-web-services, boto3, named-entity-recognition, amazon-comprehend","<p>Endpoint ARN are different AWS resource as compared to Model ARN. Model ARN refers to a custom model while endpoint hosts that model. In your code, your code you are passing in the modelARN instead of endpointARN which is causing the error to be raised.</p>
<p>You can differentiate between the two ARNs on the basis of the prefix.</p>
<p>endpoint arn - arn:aws:comprehend:us-east-1:XXXXXXXXXXXX:entity-recognizer-endpoint/xxxxxxxxxx</p>
<p>model arn - arn:aws:comprehend:us-east-1:XXXXXXXXXXXX:entity-recognizer/MyFirstRecognizer</p>
<p>You can read more about Comprehend custom endpoints and its pricing on the documentation page.
<a href=""https://docs.aws.amazon.com/comprehend/latest/dg/detecting-cer-real-time.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/comprehend/latest/dg/detecting-cer-real-time.html</a></p>
",2,0,471,2020-08-25 14:19:14,https://stackoverflow.com/questions/63580899/aws-costum-entity-recognition-wrong-arn-endpoint
"How are P, R, and F scores calculated in spaCy CLI train NER?","<p>I am using the spaCy CLI train command for NER with <code>train_path</code> set to the training dataset (train-set) and  <code>dev_path</code> set to the evaluation dataset (test-set). The printout in the console shows me NER Precision, Recall, and the F-score.</p>
<p>However, it is not clear to me how the scores were calculated. Are they the scores from the model predicting on the train-set (train-scores) or from the test-set (test-scores)?</p>
<p>I want to determine after which epoch to stop training to prevent overfitting. Currently after 60 epochs the Loss is still slightly decreasing and Precision, Recall, and F-score are still slightly increasing. It seems to me that the model might be memorizing the training data and that the P, R, and F-scores are calculated on the train-set and thus keep improving.</p>
<p>To my knowledge a good stopping point in training would be right before the test-scores start dropping again, even though the train-scores keep increasing. So I would like to compare them over time (epochs).</p>
<p>My questions are:</p>
<ol>
<li>Are the scores displayed in the console while training train-scores or test-scores?</li>
<li>And how to get access to the other one?</li>
<li>If it is the train-score, for what is the testset (<code>dev_path</code>) used?</li>
</ol>
","spacy, named-entity-recognition","<p>The <code>loss</code> is calculated from the training examples, as a side effect of calling <code>nlp.update()</code> in the <a href=""https://github.com/explosion/spaCy/blob/master/spacy/cli/train.py#L425"" rel=""nofollow noreferrer"">training loop</a>. However, all the other performance metrics are calculated on the dev set, by <a href=""https://github.com/explosion/spaCy/blob/master/spacy/cli/train.py#L463"" rel=""nofollow noreferrer"">calling</a> the <code>Scorer</code>.</p>
<blockquote>
<p>To my knowledge a good stopping point in training would be right before the test-scores start dropping again, even though the train-scores keep increasing</p>
</blockquote>
<p>Yep, I agree. So looking at the <code>spacy train</code> results, this would be when the (training) loss is still decreasing, while the (dev) F-score starts decreasing again.</p>
<blockquote>
<p>Currently after 60 epochs the Loss is still slightly decreasing and Precision, Recall, and F-score are still slightly increasing.</p>
</blockquote>
<p>So it looks like you can train for some epochs more :-)</p>
",2,1,1177,2020-08-25 16:32:05,https://stackoverflow.com/questions/63583290/how-are-p-r-and-f-scores-calculated-in-spacy-cli-train-ner
No module named &#39;ICE&#39;,"<p>I am using my code on jupyter notebook</p>
<pre><code>!pip install ICE
!pip install zeroc-ice
</code></pre>
<blockquote>
<p>Requirement already satisfied: ICE in c:\python3.7\lib\site-packages (0.0.2)</p>
</blockquote>
<blockquote>
<p>Requirement already satisfied: zeroc-ice in c:\python3.7\lib\site-package (3.7.4)</p>
</blockquote>
<pre><code>input=['he and Chazz duel with all keys on the line.']

from ICE import CollocationExtractor

extractor = CollocationExtractor.with_collocation_pipeline('T1' , bing_key = 'Temp',pos_check = False)

print(extractor.get_collocations_of_length(input, length = 3))
</code></pre>
<p>getting error as:</p>
<blockquote>
<p>ModuleNotFoundError                       Traceback (most recent call last)
 in 
3 input=['he and Chazz duel with all keys on the line.']
4
----&gt; 5 from ICE import CollocationExtractor
6
7 extractor = CollocationExtractor.with_collocation_pipeline('T1' , bing_key = 'Temp',pos_check = False)</p>
</blockquote>
<p>ModuleNotFoundError: No module named 'ICE'</p>
<p>Sorry if it's not properly readable..</p>
","python, named-entity-recognition","<p>you need to import it as 'ice' e.g.</p>
<pre><code>import ice
app = ice.cube()
if __name__ == '__main__':
   app.run()
</code></pre>
<p>and for zeroc-ice as</p>
<pre><code>import sys, Ice
with Ice.initialize(sys.argv) as communicator:
    obj = communicator.stringToProxy(&quot;hello:tcp -h myhost.mydomain.com -p 10000&quot;)
    obj.ice_ping()
</code></pre>
",0,0,2093,2020-09-07 12:28:19,https://stackoverflow.com/questions/63777649/no-module-named-ice
Error with &#39; &#39;.join() parsing txt for named entity recognition in NLP google API,"<p>I'm having a rough time in trying to construct a dataset for Named Entity Recognition in Google NLP API, via this script provided by Google <a href=""https://cloud.google.com/natural-language/automl/docs/scripts/input_helper_v2.py?hl=de"" rel=""nofollow noreferrer"">input_helper_v2.py</a></p>
<p>The problem comes with the function <strong>_DownloadGcsFile</strong>, as it throws this error:</p>
<pre><code>gsutil_cp_cmd = ' '.join(['gsutil', 'cp', gcs_file, local_filename])
TypeError: sequence item 2: expected str instance, bytes found
</code></pre>
<p>I've tried to put <code>b' '.join(['gsutil', 'cp', gcs_file, local_filename])</code>, but it yields to similar problems.</p>
<p>In searching for information, I noticed that it could be the script being developed in python 2.7 what is causing this.</p>
<p>I'll appreciate any help, as I'm a complete beginner. Thank you so much.</p>
","python, google-cloud-platform, named-entity-recognition, google-cloud-automl, google-natural-language","<p>Well it means that gcs_file has type <em>bytes</em>. So you need to make it a string (<em>str</em>) type. For example:</p>
<pre><code>gsutil_cp_cmd = ' '.join(['gsutil', 'cp', gcs_file.decode('utf-8'), local_filename])
</code></pre>
",1,0,84,2020-09-16 22:55:21,https://stackoverflow.com/questions/63928965/error-with-join-parsing-txt-for-named-entity-recognition-in-nlp-google-api
Extracting multi-word named entities using NLTK Stanford NER in Python,"<p>I am trying to extract named entities from text using Stanford-NER. I have read all related threads regarding chunking and did not find anything to solve the problem I am having.</p>
<p>Input:</p>
<blockquote>
<p>The united nations is holding a meeting in the united states of America.</p>
</blockquote>
<p>Expected Output:</p>
<blockquote>
<p>united nations/organization</p>
</blockquote>
<blockquote>
<p>united states of America/location</p>
</blockquote>
<p>I was able to get this output, but it doesn't combine tokens for multi-work named entities:</p>
<pre><code>[('The', 'O'), ('united', 'ORGANIZATION'), ('nations', 'ORGANIZATION'), ('is', 'O'), ('holding', 'O'), ('a', 'O'), ('meeting', 'O'), ('in', 'O'), ('the', 'O'), ('united', 'LOCATION'), ('states', 'LOCATION'), ('of', 'LOCATION'), ('America', 'LOCATION'), ('.', 'O')]
</code></pre>
<p>or in a tree format:</p>
<pre><code>(S
  The/O
  united/ORGANIZATION
  nations/ORGANIZATION
  is/O
  holding/O
  a/O
  meeting/O
  in/O
  the/O
  united/LOCATION
  states/LOCATION
  of/LOCATION
  America/LOCATION
  ./O)
</code></pre>
<p>I am looking for this output:</p>
<pre><code>[('The', 'O'), ('united nations', 'ORGANIZATION'), ('is', 'O'), ('holding', 'O'), ('a', 'O'), ('meeting', 'O'), ('in', 'O'), ('the', 'O'), ('united states of America', 'LOCATION'), ('.', 'O')]
</code></pre>
<p>When I tried some of the code I found in other threads to join named entities in the tree format, it returned an empty list.</p>
<pre><code>import nltk
from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize
import os
java_path = &quot;C:\Program Files (x86)\Java\jre1.8.0_251/java.exe&quot;
os.environ['JAVAHOME'] = java_path

st = StanfordNERTagger(r'stanford-ner-4.0.0/stanford-ner-4.0.0/classifiers/english.all.3class.distsim.crf.ser.gz',
                       r'stanford-ner-4.0.0/stanford-ner-4.0.0/stanford-ner.jar',
                       encoding='utf-8')

text = 'The united nations is holding a meeting in the united states of America.'
tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)
namedEnt = nltk.ne_chunk(classified_text, binary = True)

#this line makes the tree return an empty list
np = [' '.join([y[0] for y in x.leaves()]) for x in namedEnt.subtrees() if x.label() == &quot;NE&quot;]

print(np)

print(classified_text)
</code></pre>
","python-3.x, nltk, stanford-nlp, named-entity-recognition","<p>The StanfordNERTagger in nltk doesn't retain information on the boundaries of named entities. If you try to parse the output of the tagger, there is no way to tell whether two consecutive nouns with the same tag are part of the same entity or whether they are distinct.</p>
<p>Alternatively, <a href=""https://stanfordnlp.github.io/CoreNLP/other-languages.html#python"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/other-languages.html#python</a> indicates that the Stanford team is actively developing a python package called Stanza which uses the Stanford CoreNLP. It is slow, but really easy to use.</p>
<p>$ pip3 install stanza</p>
<pre><code>&gt;&gt;&gt; import stanza
&gt;&gt;&gt; stanza.download ('en')
&gt;&gt;&gt; nlp = stanza.Pipeline ('en')
&gt;&gt;&gt; results = nlp (&lt;insert your text string here&gt;)
</code></pre>
<p>The chunked entities are in <code>results.ents</code>.</p>
",1,0,1750,2020-09-19 06:31:52,https://stackoverflow.com/questions/63965957/extracting-multi-word-named-entities-using-nltk-stanford-ner-in-python
How to fix spaCy en_model incompatible with current spaCy version (2.3.2)?,"<p>When I am running my NER model I am getting:</p>
<pre><code>UserWarning: [W031] Model 'en_model' (0.0.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2)
</code></pre>
<p>Please advise how can I fix it?</p>
<p>Python 3.7.9, spaCy 2.3.2, Ubuntu 18.04.</p>
","python-3.x, model, spacy, named-entity-recognition","<p>Solved by downgrading spaCy to 2.2.4.</p>
<pre><code>pip3 install spacy==2.2.4
</code></pre>
",5,1,6936,2020-09-23 20:33:24,https://stackoverflow.com/questions/64035821/how-to-fix-spacy-en-model-incompatible-with-current-spacy-version-2-3-2
How to decide between NER and QA Model?,"<p>I am completing a task involving NLP and transformers. I would like to identify relevant features in a corpus of text. If i was to extract the relevant features from job description for instance the tools that would be used at the job (powerpoint, excel, java, etc..) and the level of proficiency required would this task be better suited for a Named Entity Recognition model or a Question Answering model.</p>
<p>If I was to approach it like a NER task I would attach a label to all the relevant tools in the training data and hope it would generalize well. I could approach the problem simialrly as a QA model and ask things like &quot;what tools does this job require&quot; and supply a description as context.</p>
<p>I plan to use the transformers library unless I am missing a better tool for this task. There are many features I am looking to extract so not all may be as simple as grabbing keywords from a list (programming languages, microsoft office etc...).</p>
<p>Would one of these approaches be a better fit or am I missing a better way to approach the proble.</p>
<p>Any help appreciated. Thank you!</p>
","python, nlp, extract, named-entity-recognition, transformer-model","<p>From what you say, it seems it an entity recognition task. However, the questions you should ask and answer yourself are:</p>
<ul>
<li><p>How will your user interact with the model?</p>
<ul>
<li>Structured information → Entity recognition.</li>
<li>Chatbot → QA.</li>
</ul>
</li>
<li><p>Is there a predefined set of entities that you are going to extract from the text?</p>
<ul>
<li>Yes → entity recognition.</li>
<li>No → QA.</li>
</ul>
</li>
<li><p>How do the training data you have for finetuning look like?</p>
<ul>
<li>Only a few of them → Entity recognition.</li>
<li>Plenty of data, question-answer pair → QA.</li>
</ul>
</li>
</ul>
",6,2,1375,2020-09-25 02:51:11,https://stackoverflow.com/questions/64057111/how-to-decide-between-ner-and-qa-model
Loading saved NER back into HuggingFace pipeline?,"<p>I am doing some research into HuggingFace's functionalities for transfer learning (specifically, for named entity recognition). To preface, I am a bit new to transformer architectures. I briefly walked through their example off of their website:</p>
<pre><code>from transformers import pipeline

nlp = pipeline(&quot;ner&quot;)

sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; \
       &quot;close to the Manhattan Bridge which is visible from the window.&quot;

print(nlp(sequence))
</code></pre>
<p>What I would like to do is save and run this locally without having to download the &quot;ner&quot; model every time (which is over 1 GB in size). In their documentation, I see that you can save the pipeline using the &quot;pipeline.save_pretrained()&quot; function to a local folder. The results of this are various files which I am storing into a specific folder.</p>
<p>My question would be how can I load this model back up into a script to continue classifying as in the example above after saving? The output of &quot;pipeline.save_pretrained()&quot; is multiple files.</p>
<p>Here is what I have tried so far:</p>
<p>1: Following the documentation about pipeline</p>
<pre><code>pipe = transformers.TokenClassificationPipeline(model=&quot;pytorch_model.bin&quot;, tokenizer='tokenizer_config.json')
</code></pre>
<p>The error I got was: 'str' object has no attribute &quot;config&quot;</p>
<p>2: Following HuggingFace example on ner:</p>
<pre><code>from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

model = AutoModelForTokenClassification.from_pretrained(&quot;path to folder following .save_pretrained()&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;path to folder following .save_pretrained()&quot;)

label_list = [
&quot;O&quot;,       # Outside of a named entity
&quot;B-MISC&quot;,  # Beginning of a miscellaneous entity right after another miscellaneous entity
&quot;I-MISC&quot;,  # Miscellaneous entity
&quot;B-PER&quot;,   # Beginning of a person's name right after another person's name
&quot;I-PER&quot;,   # Person's name
&quot;B-ORG&quot;,   # Beginning of an organisation right after another organisation
&quot;I-ORG&quot;,   # Organisation
&quot;B-LOC&quot;,   # Beginning of a location right after another location
&quot;I-LOC&quot;    # Location
]

sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; \
       &quot;close to the Manhattan Bridge.&quot;

# Bit of a hack to get the tokens with the special tokens
tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))
inputs = tokenizer.encode(sequence, return_tensors=&quot;pt&quot;)

outputs = model(inputs)[0]
predictions = torch.argmax(outputs, dim=2)

print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())])
</code></pre>
<p>This yields an error: list index out of range</p>
<p>I also tried printing out just predictions which is not returning the text format of the tokens along with their entities.</p>
<p>Any help would be much appreciated!</p>
","nlp, named-entity-recognition, huggingface-transformers, huggingface-tokenizers","<p>Loading a model like this has always worked for me:</p>
<pre><code>from transformers import pipeline

pipe = pipeline('token-classification', model=model_folder, tokenizer=model_folder)
</code></pre>
<p>Have a look at <a href=""https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html#pipelines"" rel=""nofollow noreferrer"">here</a> for further examples on how to use pipelines.</p>
",2,4,903,2020-09-28 17:18:42,https://stackoverflow.com/questions/64106747/loading-saved-ner-back-into-huggingface-pipeline
Are special tokens [CLS] [SEP] absolutely necessary while fine tuning BERT?,"<p>I am following the tutorial <a href=""https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/"" rel=""nofollow noreferrer"">https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/</a> to do Named Entity Recognition with BERT.</p>
<p>While fine-tuning, before feeding the tokens to the model, the author does:</p>
<pre><code>input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=MAX_LEN, dtype=&quot;long&quot;, value=0.0,
                          truncating=&quot;post&quot;, padding=&quot;post&quot;)
</code></pre>
<p>According to my tests, this doesn't add special tokens to the ids. So am I missing something or i it not always necessary to include [CLS] (101) [SEP] (102)?</p>
","bert-language-model, named-entity-recognition, cls","<p>I'm also following this tutorial. It worked for me without adding these tokens, however, I found in another tutorial (<a href=""https://vamvas.ch/bert-for-ner"" rel=""nofollow noreferrer"">https://vamvas.ch/bert-for-ner</a>) that it is better to add them, because the model was trained in this format.</p>
<p>[Update]
Actually just checked it, it turned out that the accuracy improved by 20% after adding the tokens. But note that I am using it on a different dataset</p>
",1,2,2650,2020-09-29 23:13:27,https://stackoverflow.com/questions/64128864/are-special-tokens-cls-sep-absolutely-necessary-while-fine-tuning-bert
Readlines causing error after many lines?,"<p>I'm working on a NRE task at the moment, with data from <code>wnut17train.conll</code> (<a href=""https://github.com/leondz/emerging_entities_17"" rel=""nofollow noreferrer"">https://github.com/leondz/emerging_entities_17</a>). It's basically a collection of tweets where each line is a single word from the tweet with an IOB tag attached (separated by a <code>\t</code>). Different tweets are separated by a blank line (actually, and weirdly enough if you ask me, a <code>'\t\n'</code> line).</p>
<p>So, for reference, a single tweet would look like this:</p>
<pre><code>@paulwalk    IOBtag
...          ...
foo          IOBtag
[\t\n]
@jerrybeam   IOBtag
...          ...
bar          IOBtag
</code></pre>
<p>The goal for this first step is to achieve a situation where I converted this data set into a training file looking like this:</p>
<pre><code>train[0] = [(first_word_of_first_tweet, POStag, IOBtag),
(second_word_of_first_tweet, POStag, IOBtag),
...,
last_word_of_first_tweet, POStag, IOBtag)]
</code></pre>
<p>This is what I came up so far:</p>
<pre><code>tmp = []
train = []
nlp = spacy.load(&quot;en_core_web_sm&quot;)
with open(&quot;wnut17train.conll&quot;) as f:
    for l in f.readlines():
        if l == '\t\n':
            train.append(tmp)
            tmp = []
        else:
            doc = nlp(l.split()[0])
            for token in doc:
                tmp.append((token.text, token.pos_, token.ent_iob_))
</code></pre>
<p>Everything works smoothly for a certain amount of tweets (or lines, not sure yet), but after that I get a</p>
<pre><code>IndexError: list index out of range
</code></pre>
<p>raised by</p>
<pre><code>doc = nlp(l.split()[0])
</code></pre>
<p>First time I got it around line 20'000 (20'533 to be precise), then after checking that this was not due to the file (maybe a different way of separating tweets, or something like this that might have tricked the parser) I removed the first 20'000 lines and tried again. Again, I got an error after around line 20'000 (20'260 - or 40'779 in the original file - to be precise).</p>
<p>I did some research on <code>readlines()</code> to see if this was a known problem but it looks like it's not. Am I missing something?</p>
","python, nlp, named-entity-recognition, readlines","<p>I used the wnut17train.conll file from <a href=""https://github.com/leondz/emerging_entities_17"" rel=""nofollow noreferrer"">https://github.com/leondz/emerging_entities_17</a> and I ran a similar code to generate your required output. I found that in some lines instead of &quot;\t\n&quot; as the blank Line we have only &quot;\n&quot;.</p>
<p>Due to this l.split() will give an IndexError: list index out of range. To handle this we can check if length is 1 and in that case also we add our tmp to train.</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
train = []
tmp = []
with open(&quot;wnut17train.conll&quot;) as fp:
    for l in fp.readlines():
        if l == &quot;\t\n&quot; or len(l) == 1:
            train.append(tmp)
            tmp = []
        else:
            doc = nlp(l.split(&quot;\t&quot;)[0])
            for token in doc:
                tmp.append((l.split(&quot;\t&quot;)[0], token.pos_, l.split(&quot;\t&quot;)[1]))
</code></pre>
<p>Hope your question is resolved.</p>
",1,1,188,2020-10-21 09:14:37,https://stackoverflow.com/questions/64460399/readlines-causing-error-after-many-lines
Training on deeppavlov for NER keeps failing,"<p>I have been trying to train a deeppavlov model for NER based on the train syntax given on their docs and it keeps failing with below error message:</p>
<pre><code>/opt/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/dataset_readers/conll2003_reader.py in parse_ner_file(self, file_name)
    104                     items = line.split()
    105                     if len(items) &lt; expected_items:
--&gt; 106                         raise Exception(f&quot;Input is not valid {line}&quot;)
    107                     tokens.append(items[0])
    108                     tags.append(items[-1])

Exception: Input is not valid aio-pika==6.4.1

</code></pre>
<p>Used the following code to train the deeppavlov model, it seems to be working on their sample dataset, but when I created my own dataset as per their training sample guide, I keep getting above error message.
Training ner code:</p>
<pre><code>from deeppavlov import configs, train_model, build_model
from deeppavlov.core.commands.utils import parse_config
import json


with configs.ner.ner_ontonotes_bert_mult.open(encoding='utf8') as f:
    ner_config = json.load(f)

ner_config['dataset_reader']['data_path'] = '/Users/smankari001/deeppavlov'  # directory with train.txt, valid.txt and test.txt files
ner_config['metadata']['variables']['NER_PATH'] = '/Users/smankari001/deeppavlov'
ner_config['metadata']['download'] = [ner_config['metadata']['download'][-1]]  # do not download the pretrained ontonotes model

ner_model = train_model(ner_config, download=True)
</code></pre>
<p>input train.txt file:</p>
<pre><code>What    O
kind    O
of  O
memory  O
?   O

We  O
respectfully    O
invite  O
you O
to  O
watch   O
a   O
special O
edition O
of  O
Across  B-ORG
China   I-ORG
.   O

WW  B-WORK_OF_ART
II  I-WORK_OF_ART
Landmarks   I-WORK_OF_ART
on  I-WORK_OF_ART
the I-WORK_OF_ART
Great   I-WORK_OF_ART
Earth   I-WORK_OF_ART
of  I-WORK_OF_ART
China   I-WORK_OF_ART
:   I-WORK_OF_ART
Eternal I-WORK_OF_ART
Memories    I-WORK_OF_ART
of  I-WORK_OF_ART
Taihang I-WORK_OF_ART
Mountain    I-WORK_OF_ART

Standing    O
tall    O
on  O
Taihang B-LOC
Mountain    I-LOC
is  O
the B-WORK_OF_ART
Monument    I-WORK_OF_ART
to  I-WORK_OF_ART
the I-WORK_OF_ART
Hundred I-WORK_OF_ART
Regiments   I-WORK_OF_ART
Offensive   I-WORK_OF_ART
.   O

It  O
is  O
composed    O
of  O
a   O
primary O
stele   O
,   O
secondary   O
steles  O
,   O
a   O
huge    O
round   O
sculpture   O
and O
beacon  O
tower   O
,   O
and O
the B-WORK_OF_ART
Great   I-WORK_OF_ART
Wall    I-WORK_OF_ART
,   O
among   O
other   O
things  O
.   O

A   O
primary O
stele   O
,   O
three   B-CARDINAL
secondary   O
steles  O
,   O
and O
two B-CARDINAL
inscribed   O
steles  O
.   O

The B-EVENT
Hundred I-EVENT
Regiments   I-EVENT
Offensive   I-EVENT
was O
the O
campaign    O
of  O
the O
largest O
scale   O
launched    O
by  O
the B-ORG
Eighth  I-ORG
Route   I-ORG
Army    I-ORG
during  O
the B-EVENT
War I-EVENT
of  I-EVENT
Resistance  I-EVENT
against I-EVENT
Japan   I-EVENT
.   O

This    O
campaign    O
broke   O
through O
the O
Japanese    B-NORP
army    O
's  O
blockade    O
to  O
reach   O
base    O
areas   O
behind  O
enemy   O
lines   O
,   O
stirring    O
up  O
anti-Japanese   B-NORP
spirit  O
throughout  O
the O
nation  O
and O
influencing O
the O
situation   O
of  O
the O
anti-fascist    O
war O
of  O
the O
people  O
worldwide   O
.   O

</code></pre>
","bert-language-model, named-entity-recognition, deeppavlov","<p>As <code>ner_config['dataset_reader']['data_path']</code> you need to specify path to folder with only dataset files (train/valid/test).</p>
<p>This error:</p>
<pre><code>Exception: Input is not valid aio-pika==6.4.1
</code></pre>
<p>says that DatasetReader started to read lines from <code>requirements.txt</code> file.</p>
",0,0,291,2020-10-23 12:30:21,https://stackoverflow.com/questions/64500038/training-on-deeppavlov-for-ner-keeps-failing
How to extract all possible noun phrases from text,"<p>I want to extract some desirable concepts (noun phrases) in the text automatically. My plan is to extract all noun phrases and then label them as two classifications (i.e., desirable phrases and non-desirable phrases). After that, train a classifier to classify them. What I am trying now is to extract all possible phrases as the training set first. For example, one sentence is <code>Where a shoulder of richer mix is required at these junctions, or at junctions of columns and beams, the items are so described.</code> I want to get all phrases like <code>shoulder</code>, <code>richer mix</code>, <code>shoulder of richer mix</code>,<code>junctions</code>,<code>junctions of columns and beams</code>, <code>columns and beams</code>, <code>columns</code>, <code>beams</code> or whatever possible. The desirable phrases are <code>shoulder</code>, <code>junctions</code>,  <code>junctions of columns and beams</code>. But I don't care the correctness at this step, I just want to get the training set first. Are there available tools for such task?</p>
<p>I tried Rake in rake_nltk, but the results failed to include my desirable phrases (i.e., it did not extract all possible phrases)</p>
<pre><code>from rake_nltk import Rake
data = 'Where a shoulder of richer mix is required at these junctions, or at junctions of columns and beams, the items are so described.'
r = Rake()
r.extract_keywords_from_text(data)
phrase = r.get_ranked_phrases()
print(phrase)enter code herenter code here
</code></pre>
<p>Result: <code>['richer mix', 'shoulder', 'required', 'junctions', 'items', 'described', 'columns', 'beams']</code>
(Missed <code>junctions of columns and beams</code> here)</p>
<p>I also tried phrasemachine, the results also missed some desirable ones.</p>
<pre><code>import spacy
import phrasemachine
matchedList=[]
doc = nlp(data)
tokens = [token.text for token in doc]
pos = [token.pos_ for token in doc]
out = phrasemachine.get_phrases(tokens=tokens, postags=pos, output=&quot;token_spans&quot;)
print(out['token_spans'])
while len(out['token_spans']):
    start,end = out['token_spans'].pop()
    print(tokens[start:end])
</code></pre>
<p>Result:</p>
<pre><code>[(2, 6), (4, 6), (14, 17)]
['junctions', 'of', 'columns']
['richer', 'mix']
['shoulder', 'of', 'richer', 'mix'] 
</code></pre>
<p>(Missed many noun phrases here)</p>
","python, nlp, spacy, named-entity-recognition, information-extraction","<p>You may wish to make use of <code>noun_chunks</code> attribute:</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp('Where a shoulder of richer mix is required at these junctions, or at junctions of columns and beams, the items are so described.')

phrases = set() 
for nc in doc.noun_chunks:
    phrases.add(nc.text)
    phrases.add(doc[nc.root.left_edge.i:nc.root.right_edge.i+1].text)
print(phrases)
{'junctions of columns and beams', 'junctions', 'the items', 'a shoulder', 'columns', 'richer mix', 'beams', 'columns and beams', 'a shoulder of richer mix', 'these junctions'}
</code></pre>
",2,1,1316,2020-10-28 04:31:38,https://stackoverflow.com/questions/64566475/how-to-extract-all-possible-noun-phrases-from-text
Measuring F1-score for NER,"<p>I am trying to evaluate a model of artificial intelligence for NER (Named Entity Recognition).<br />
In order to compare with other benchmarks, I need to calculate the model's F1-score. However, I am unsure how to code this.</p>
<p>My idea was:<br />
<strong>True-positives</strong>: equal tokens and equal tags, true-positive for the tag<br />
<strong>False-negative</strong>: equal tokens and unequal tags or token did not appear in the prediction, false-negative for the tag<br />
<strong>False-positive</strong>: token does not exist but has been assigned to a tag, example:</p>
<blockquote>
<p>Phrase: &quot;This is a test&quot;<br />
Predicted: {token: This is, tag: WHO}<br />
True pairs: {token: This, tag: WHO} {token: a test, tag: what}<br />
In this case, {token: This is, tag: WHO} is considered as a false positive of WHO.</p>
</blockquote>
<p>The code:</p>
<pre><code>       for val predicted tokens (pseudo-code) {   
       // val = struct { tokens, tags } from a phrase
           for (auto const &amp;j : val.tags) {
                if (j.first == current_tokens) {
                    if (j.second == tag) {
                        true_positives[tag_id]++;
                    } else {
                        false_negatives[tag_id]++;
                    }
                    current_token_exists = true;
                }
                
            }
            if (!current_token_exists) {
                false_positives[tag_id]++;
            }
        }

        for (auto const &amp;i : val.tags) {
            bool find = 0;
            for (auto const &amp;j : listed_tokens) {
                if (i.first == j) {find = 1; break;}
            }
            if (!find) {
                false_negatives[str2tag_id[i.second]]++;
            }
        }
</code></pre>
<p>After this, calculate the F-1:</p>
<pre><code>    float precision_total, recall_total, f_1_total;
    precision_total = total_true_positives / (total_true_positives + total_false_positives);
    recall_total = total_true_positives / (total_true_positives + total_false_negatives);
    f_1_total = (2 * precision_total * recall_total) / (precision_total + recall_total);
</code></pre>
<p>However, I believe that I am wrong in some concept. Does anyone have an opinion?</p>
","nlp, artificial-intelligence, named-entity-recognition, measurement","<p>This is not a complete answer.
Taking a look <a href=""http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/"" rel=""nofollow noreferrer"">here</a>
we can see that there are many possible ways of defining an F1 score for NER. There are consider at least 6 possible cases, a part of TP, TN, FN, and FP, since the tag can correspond to more than one token, and therefore we may consider the partial matches.
If you take a look there are different ways of defining the F1 score, some of them defining the TP like a weighted average of strict positive and partial positive, for example.
CoNLL, which is one of the most famous benchmarks for NER looks like they use an strict definition for recall and precission, which is enough to define the F1 score:</p>
<blockquote>
<p>precision is the percentage of named entities found by the learning
system that are correct. Recall is the percentage of named entities
present in the corpus that are found by the system. A named entity is
correct only if it is an exact match of the corresponding entity in
the data file.</p>
</blockquote>
",3,1,7423,2020-11-08 13:30:14,https://stackoverflow.com/questions/64738626/measuring-f1-score-for-ner
How can we use Spacy minibatch and GoldParse to train NER model using BILUO tagging scheme?,"<p>My input data to the spacy ner model is in the <code>BILUO</code> tagging scheme and I wish to use the same as a part of some requirement. When I try to train the model simply without a minibatch, it works fine (the commented part). But I am unable to figure out how to use minibatch and GoldParse here in order to raise the model's accuracy. Are my expectations valid here as I could not find a single example with this kind of combination? Also, I have already trained the model with the approach of start, end, label format. Please help me to figure out this section. My code is as below,</p>
<pre><code>import spacy
from spacy.gold import offsets_from_biluo_tags
from spacy.gold import biluo_tags_from_offsets
import random
from spacy.util import minibatch, compounding
from os import path
from tqdm import tqdm


def train_spacy(data, iterations, model=None):
    TRAIN_DATA = data
    print(f&quot;downloads = {model}&quot;)
    if model is not None and path.exists(model):
        print(f&quot;training existing model&quot;)
        nlp = spacy.load(model)
        print(&quot;Model is Loaded '%s'&quot; % model)
    else:
        print(f&quot;Creating new model&quot;)

        nlp = spacy.blank('en')  # create blank Language class

    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe('ner')

    # Based on template, get labels and save those for further training
    LABEL = [&quot;Name&quot;, &quot;ORG&quot;]

    for i in LABEL:
        # print(i)
        ner.add_label(i)

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        if model is None:
            optimizer = nlp.begin_training()
        else:
            optimizer = nlp.entity.create_optimizer()
        tags = dict()
        for itn in range(iterations):
            print(&quot;Starting iteration &quot; + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            # for text, annotations in tqdm(TRAIN_DATA):
            #     print(f&quot;text={text}, an={annotations}&quot;)
            #     tags['entities'] = offsets_from_biluo_tags(nlp(text), annotations)
            #     print(f&quot;a={tags}&quot;)
            #     nlp.update([text],  # batch of texts
            #                [tags],  # batch of annotations
            #                drop=0.5,  # dropout - make it harder to memorise data
            #                sgd=optimizer,  # callable to update weights
            #                losses=losses)
            # print(losses)
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 16.0, 1.001))
            # type 2 with mini batch
            for batch in batches:
                texts, annotations = zip(*batch)
                print(texts)
                tags = {'entities': annotations}
                nlp.update(
                    texts,  # batch of texts
                    [tags],  # batch of annotations
                    drop=0.4,  # dropout - make it harder to memorise data
                    losses=losses,
                    sgd=optimizer
                )
            print(losses)
    return nlp

data_biluo = [
    ('I am Shah Khan, I work in MS Co', ['O', 'O', 'B-Name', 'L-Name', 'O', 'O', 'O', 'B-ORG', 'L-ORG']),
    ('I am Tom Tomb, I work in Telecom Networks', ['O', 'O', 'B-Name', 'L-Name', 'O', 'O', 'O', 'B-ORG', 'L-ORG'])
]


model = train_spacy(data_biluo, 10)
model.to_disk('./Vectors/')
</code></pre>
","python, nlp, spacy, named-entity-recognition","<p>You have 2 problems with your minibatch:</p>
<ol>
<li><code>tags</code> should be an iterable of ner tags with offsets</li>
<li>your <code>data_biluo</code> doesn't account for a <code>,</code> in the middle of the sentences.</li>
</ol>
<p>As soon as you correct for those you'r fine to go:</p>
<pre><code>import spacy
from spacy.gold import offsets_from_biluo_tags, GoldParse
from spacy.util import minibatch, compounding
import random
from tqdm import tqdm

def train_spacy(data, iterations, model=None):
    TRAIN_DATA = data
    print(f&quot;downloads = {model}&quot;)
    if model is not None and path.exists(model):
        print(f&quot;training existing model&quot;)
        nlp = spacy.load(model)
        print(&quot;Model is Loaded '%s'&quot; % model)
    else:
        print(f&quot;Creating new model&quot;)

        nlp = spacy.blank('en')  # create blank Language class

    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe('ner')

    # Based on template, get labels and save those for further training
    LABEL = [&quot;Name&quot;, &quot;ORG&quot;]

    for i in LABEL:
        # print(i)
        ner.add_label(i)

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        if model is None:
            optimizer = nlp.begin_training()
        else:
            optimizer = nlp.entity.create_optimizer()
        tags = dict()
        for itn in range(iterations):
            print(&quot;Starting iteration &quot; + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 16.0, 1.001))
            # type 2 with mini batch
            for batch in batches:
                texts, _ = zip(*batch)
                golds = [GoldParse(nlp.make_doc(t),entities = a) for t,a in batch]
                nlp.update(
                    texts,  # batch of texts
                    golds,  # batch of annotations
                    drop=0.4,  # dropout - make it harder to memorise data
                    losses=losses,
                    sgd=optimizer
                )
            print(losses)
    return nlp

data_biluo = [
    ('I am Shah Khan, I work in MS Co', ['O', 'O', 'B-Name', 'L-Name', 'O', 'O', 'O', 'O', 'B-ORG', 'L-ORG']),
    ('I am Tom Tomb, I work in Telecom Networks', ['O', 'O', 'B-Name', 'L-Name', 'O', 'O', 'O', 'O', 'B-ORG', 'L-ORG'])
]


model = train_spacy(data_biluo, 10)

Starting iteration 0
{'ner': 17.999998331069946}
Starting iteration 1
{'ner': 16.6766300201416}
Starting iteration 2
{'ner': 16.997647166252136}
Starting iteration 3
{'ner': 16.486496448516846}
Starting iteration 4
{'ner': 15.695325374603271}
Starting iteration 5
{'ner': 14.312554001808167}
Starting iteration 6
{'ner': 12.099276185035706}
Starting iteration 7
{'ner': 11.473928153514862}
Starting iteration 8
{'ner': 8.814643770456314}
Starting iteration 9
{'ner': 7.233813941478729}
</code></pre>
",2,3,3197,2020-11-09 22:40:55,https://stackoverflow.com/questions/64760271/how-can-we-use-spacy-minibatch-and-goldparse-to-train-ner-model-using-biluo-tagg
Where is the trained NER model saved after training the Spacy model with new Entities,"<p>I'm still learning Python and creation of models and am very new to NLP using Spacy. I used <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#ner</a> to train Spacy's existing model - en_core_web_sm.</p>
<p>I've trained this model with my domain specific entities.</p>
<pre><code>def main(model=&quot;en_core_web_sm&quot;, new_model_name=&quot;new_ner_model&quot;, output_dir='/content/drive/My Drive/Data/new_model', n_iter=100):
.
.
(code to train the model)
.
.
    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta[&quot;name&quot;] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(&quot;Saved model to&quot;, output_dir)
</code></pre>
<p>Now I assumed that I would find a single model file within the output directory. Instead, what I have are 4 subfolders - <em>vocab, ner, tagger, parser</em>. And 2 files <em>meta.json and tokenizer</em>.
The <em>ner</em> subfolder has <em>cfg, moves, model</em>.</p>
<p>According to the website mentioned above, to load the new model, I need to use the entire folder (output directory), i.e.</p>
<p><code>nlp2 = spacy.load(output_dir)</code></p>
<p>Is the whole directory needed (is that the model) or is it the binary file named <em>model</em> within the <em>ner</em> subfolder?</p>
","python, model, nlp, spacy, named-entity-recognition","<p>In general we do advice to save the entire model as a folder, to make sure everything is loaded back in consistently. it won't work to just load the <code>model</code> file in by itself. It just contains the weights of the neural network. Some of the other files are needed to define the parameters and setup of your NLP pipeline &amp; its different components. For instance, you always need the vocab data, etc.</p>
<p>One thing you could do, is disable the components you're not interested in. This will decrease the folder size on your disk and remove the redundant folders you don't want. For instance, if you're only interested in the NER, you could do:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;, disable=[&quot;parser&quot;, &quot;tagger&quot;])`
</code></pre>
<p>Or, if you loaded the whole model, you could store just parts of it to disk:</p>
<pre><code>nlp.to_disk(output_dir, exclude=[&quot;parser&quot;, &quot;tagger&quot;])
</code></pre>
",1,1,1448,2020-11-13 10:32:04,https://stackoverflow.com/questions/64819343/where-is-the-trained-ner-model-saved-after-training-the-spacy-model-with-new-ent
How can I make SpaCy recognize all my given entities,"<p>I have quite a list of patterns in JSONL format that I loaded and added to the entity ruler</p>
<pre><code>new_ruler = EntityRuler(nlp).from_disk(project_path + &quot;data/skill_patterns.jsonl&quot;)
nlp.add_pipe(new_ruler)
</code></pre>
<p>When I print the results: <code>print([(ent.text, ent.label_) for ent in doc.ents])</code>
My output is:</p>
<pre><code>[('data science','SKILL|data-science'), ('CV', 'ORG'), ('Kandidaat', 'FAC'), ('één', 'CARDINAL'), ('LSTM',
 'ORG'), ('Parts', 'GPE'), ('Speech', 'GPE'), ('POS', 'ORG'), ('Entity Recognition', 'ORG'), 
('NER', 'ORG'), ('Word2vec', 'ORG'), ('GloVe', 'ORG'), ('Recursive', 'NORP'), ('Neural Networks', 'ORG'),
 ('Ensemble', 'PERSON'), ('Dynamic', 'NORP'), ('Intent detection', 'PERSON'), ('Phrase matching.-', 'ORG'),
 ('Microsoft', 'NORP'), ('Azure.-', 'ORG'), ('één', 'CARDINAL'), ('Python', 'WORK_OF_ART'),
 ('Pytorch', 'GPE'), ('Django', 'GPE'), ('GoLanguage.-', 'GPE'), ('Kandidaat', 'FAC'), ('1 november 2020', 'DATE')]
</code></pre>
<p>Now I know for a fact that for example <code>('Pytorch', 'GPE')</code> or <code>('Django', 'GPE')</code> are in my pattern list and should be recognized as <code>SKILL</code> instead of the entities they got assigned now. This goes for quite a few other 'skills' as well.</p>
<pre><code>{&quot;label&quot;:&quot;SKILL|django&quot;,&quot;pattern&quot;:[{&quot;LOWER&quot;:&quot;django&quot;}]}
{&quot;label&quot;:&quot;SKILL|pytorch&quot;,&quot;pattern&quot;:[{&quot;LOWER&quot;:&quot;pytorch&quot;}]}
</code></pre>
<p>Is there someone that knows why it does not adhere to my self created entities?</p>
<p>Is there a way that I can prioritize my entities above the ones already in the model?</p>
<p>Thanks!</p>
","python, spacy, named-entity-recognition","<p>I've found a solution.</p>
<p>By adding the <code>new_ruler</code> before the NER (after parser) in the pipeline, it gives the created entities priority</p>
<pre><code>nlp.add_pipe(new_ruler, after='parser')
</code></pre>
",1,1,326,2020-11-19 08:30:43,https://stackoverflow.com/questions/64907960/how-can-i-make-spacy-recognize-all-my-given-entities
Extracting full names with ne_chunks,"<p>Newbie here. I'm trying to extract full names of people and organisations using the following code.</p>
<pre><code>def get_continuous_chunks(text):
    chunked = ne_chunk(pos_tag(word_tokenize(text)))
    continuous_chunk = []
    current_chunk = []
    for i in chunked:
        if type(i) == Tree:
            current_chunk.append(' '.join([token for token, pos in i.leaves()]))
            if current_chunk:
                named_entity = ' '.join(current_chunk)
                if named_entity not in continuous_chunk:
                    continuous_chunk.append(named_entity)
                    current_chunk = []
                else:
                    continue
                return continuous_chunk

            
&gt;&gt;&gt; my_sent = &quot;Toni Morrison was the first black female editor in fiction at Random House in New York City.&quot;
&gt;&gt;&gt; get_continuous_chunks(my_sent)
['Toni']
</code></pre>
<p>As you can see it is returning only the first proper noun. Not the full name, and not any other proper nouns in the string.</p>
<p>What am I doing wrong?</p>
","python-3.x, chunks, named-entity-recognition","<p>Here is some working code.</p>
<p>The best thing to do is to step through your code and put a lot of print statements at different places. You will see where I printed the <code>type()</code> and the <code>str()</code> value of the items you are iterating on. I find this helps me to visualize and think more about the loops and conditionals I am writing if I can see them listed.</p>
<p>Also, oops, I inadvertently named all of the variables, &quot;contiguous&quot; instead of &quot;continuous&quot; ... not sure why ... contiguous might be more accurate</p>
<h1>Code:</h1>
<pre><code>from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.tree import Tree


def get_continuous_chunks(text):
    chunked = ne_chunk(pos_tag(word_tokenize(text)))
    current_chunk = []
    contiguous_chunk = []
    contiguous_chunks = []

    for i in chunked:
        print(f&quot;{type(i)}: {i}&quot;)
        if type(i) == Tree:
            current_chunk = ' '.join([token for token, pos in i.leaves()])
            # Apparently, Tony and Morrison are two separate items,
            # but &quot;Random House&quot; and &quot;New York City&quot; are single items.
            contiguous_chunk.append(current_chunk)
        else:
            # discontiguous, append to known contiguous chunks.
            if len(contiguous_chunk) &gt; 0:
                contiguous_chunks.append(' '.join(contiguous_chunk))
                contiguous_chunk = []
                current_chunk = []

    return contiguous_chunks

my_sent = &quot;Toni Morrison was the first black female editor in fiction at Random House in New York City.&quot;


print()
contig_chunks = get_continuous_chunks(my_sent)
print(f&quot;INPUT: My sentence: '{my_sent}'&quot;)
print(f&quot;ANSWER: My contiguous chunks: {contig_chunks}&quot;)
</code></pre>
<h1>Exection:</h1>
<pre><code>(venv) [ttucker@zim stackoverflow]$ python contig.py 

&lt;class 'nltk.tree.Tree'&gt;: (PERSON Toni/NNP)
&lt;class 'nltk.tree.Tree'&gt;: (PERSON Morrison/NNP)
&lt;class 'tuple'&gt;: ('was', 'VBD')
&lt;class 'tuple'&gt;: ('the', 'DT')
&lt;class 'tuple'&gt;: ('first', 'JJ')
&lt;class 'tuple'&gt;: ('black', 'JJ')
&lt;class 'tuple'&gt;: ('female', 'NN')
&lt;class 'tuple'&gt;: ('editor', 'NN')
&lt;class 'tuple'&gt;: ('in', 'IN')
&lt;class 'tuple'&gt;: ('fiction', 'NN')
&lt;class 'tuple'&gt;: ('at', 'IN')
&lt;class 'nltk.tree.Tree'&gt;: (ORGANIZATION Random/NNP House/NNP)
&lt;class 'tuple'&gt;: ('in', 'IN')
&lt;class 'nltk.tree.Tree'&gt;: (GPE New/NNP York/NNP City/NNP)
&lt;class 'tuple'&gt;: ('.', '.')
INPUT: My sentence: 'Toni Morrison was the first black female editor in fiction at Random House in New York City.'
ANSWER: My contiguous chunks: ['Toni Morrison', 'Random House', 'New York City']
</code></pre>
<p>I am also a little unclear as to exactly what you were looking for, but from the description, this seems like it.</p>
",1,1,398,2020-11-25 01:35:15,https://stackoverflow.com/questions/64997336/extracting-full-names-with-ne-chunks
NER - Should I include common prefixes in labeled entities,"<p>I am trying to recognize entities in a set of OCR texts from images of documents. Since the text is commonly in the form <code>some_label: value</code> in the document, it comes up often (but not always) in the OCR text as well.</p>
<p>My question is, say I am trying to annotate dates in my OCR text files, and 80% of times the date is in the format <code>Date: xx/xx/xxxx</code>; would it better if I ...</p>
<ol>
<li>Only marked <code>xx/xx/xxxx</code> as my date entity
<ul>
<li>Represents the true entity</li>
<li>Would be representative of 100% of the data</li>
</ul>
</li>
<li><em>OR</em> marked the entire <code>Date: xx/xx/xxxx</code> as my date entity
<ul>
<li>Would take advantage of the commonly occurring <code>Date: </code> prefix for better accuracy?</li>
</ul>
</li>
</ol>
<h3>Another example:</h3>
<p>Amounts are commonly represented as <code>$xxxxxx5.37</code> and <code>$ 63.75</code></p>
<ol>
<li>Choose <code>5.37</code> and <code>63.75</code></li>
<li>Choose <code>$xxxxxx5.37</code> and <code>$ 63.75</code> (taking advantage of $ sign)</li>
</ol>
<p>Which of these would be the better practice to follow / lead to a better model?</p>
<p>(P.S.: I'm using Prodigy to annotate my data)</p>
","spacy, named-entity-recognition","<p>It depends on the neural network architecture you use.</p>
<p>Let's assume you use spaCy v2 and its default neural architecture which is a CNN.
In this case, the architecture is going to slide through your text according to a specific window (i.e. x number of words before the <code>date</code> entity and x number of words after the <code>date</code> entity).</p>
<p>With this approach, every time the token <code>Date:</code> appears in the text, it is likely that the neural network will recognize that the entity <code>date</code> sits next to it.</p>
<p>In this case, my suggestion would be to include only the annotate the xx/xx/xxxx date as an entity. It will give the model more flexibility in determining what is a date <code>entity</code>. However, testing is always the best way to find out what's best. So, give it a try :)</p>
",1,0,143,2020-12-03 20:58:48,https://stackoverflow.com/questions/65134056/ner-should-i-include-common-prefixes-in-labeled-entities
Replacement entity with their entity label using spacy,"<p>I want to do for my data by replacing each entity with its label using Spacy and I have 3000 text rows needed to replace entities with their label entity,</p>
<p>for example:</p>
<blockquote>
<p>&quot;Georgia recently became the first U.S. state to &quot;ban Muslim culture.&quot;</p>
</blockquote>
<p>And want to become like this:</p>
<blockquote>
<p>&quot;GPE recently became the ORDINAL GPE state to &quot;ban NORP culture. &quot;</p>
</blockquote>
<p>I want code to replace more than rows of text.</p>
<p>Thanks very much.</p>
<p>For example these codes but for one sentence, I want to modify s (string) to column contains 3000 rows</p>
<p>First one: from (<a href=""https://stackoverflow.com/questions/58712418/replace-entity-with-its-label-in-spacy"">Replace entity with its label in SpaCy</a>)</p>
<pre><code>s= &quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;
doc = nlp(s)
newString = s
for e in reversed(doc.ents): #reversed to not modify the offsets of other entities when substituting
    start = e.start_char
    end = start + len(e.text)
    newString = newString[:start] + e.label_ + newString[end:]
print(newString)
#His friend PERSON is here with PERSON and PERSON.
</code></pre>
<p>Second one: from (<a href=""https://stackoverflow.com/questions/58077806/merging-tags-into-my-file-using-named-entity-annotation"">Merging tags into my file using named entity annotation</a>)</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)
s =&quot;Apple is looking at buying U.K. startup for $1 billion&quot;
doc = nlp(s)

def replaceSubstring(s, replacement, position, length_of_replaced):
    s = s[:position] + replacement + s[position+length_of_replaced:]
    return(s)

for ent in reversed(doc.ents):
    #print(ent.text, ent.start_char, ent.end_char, ent.label_)
    replacement = &quot;&lt;{}&gt;{}&lt;/{}&gt;&quot;.format(ent.label_,ent.text, ent.label_)
    position = ent.start_char
    length_of_replaced = ent.end_char - ent.start_char 
    s = replaceSubstring(s, replacement, position, length_of_replaced)

print(s)
#&lt;ORG&gt;Apple&lt;/ORG&gt; is looking at buying &lt;GPE&gt;U.K.&lt;/GPE&gt; startup for &lt;MONEY&gt;$1 billion&lt;/MONEY&gt;
</code></pre>
","python, nlp, spacy, named-entity-recognition","<p>IIUC, you may achieve what you want with:</p>
<ol>
<li>Reading your texts from file, each text on its own line</li>
<li>Processing results by substituting entities, if any, with their tags</li>
<li>Writing results to disc, each text on its own line</li>
</ol>
<p>Demo:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_md&quot;)

#read txt file, each string on its own line
with open(&quot;./try.txt&quot;,&quot;r&quot;) as f:
    texts = f.read().splitlines()

#substitute entities with their TAGS
docs = nlp.pipe(texts)
out = []
for doc in docs:
    out_ = &quot;&quot;
    for tok in doc:
        text = tok.text
        if tok.ent_type_:
            text = tok.ent_type_
        out_ += text + tok.whitespace_
    out.append(out_)

# write to file
with open(&quot;./out_try.txt&quot;,&quot;w&quot;) as f:
    f.write(&quot;\n&quot;.join(out))
</code></pre>
<p>Contents of input file:</p>
<blockquote>
<p>Georgia recently became the first U.S. state to &quot;ban Muslim culture.<br />
His friend Nicolas J. Smith is here with Bart Simpon and Fred.<br />
Apple is looking at buying U.K. startup for $1 billion</p>
</blockquote>
<p>Contents of output file:</p>
<blockquote>
<p>GPE recently became the ORDINAL GPE state to &quot;ban NORP culture.<br />
His friend PERSON PERSON PERSON is here with PERSON PERSON and PERSON.<br />
ORG is looking at buying GPE startup for MONEYMONEY MONEY</p>
</blockquote>
<p>Note the <code>MONEYMONEY</code> pattern.</p>
<p>This is because:</p>
<pre><code>doc = nlp(&quot;Apple is looking at buying U.K. startup for $1 billion&quot;)
for tok in doc:
    print(f&quot;{tok.text}, {tok.ent_type_}, whitespace='{tok.whitespace_}'&quot;)
</code></pre>
<hr />
<pre><code>Apple, ORG, whitespace=' '
is, , whitespace=' '
looking, , whitespace=' '
at, , whitespace=' '
buying, , whitespace=' '
U.K., GPE, whitespace=' '
startup, , whitespace=' '
for, , whitespace=' '
$, MONEY, whitespace='' # &lt;-- no whitespace between $ and 1
1, MONEY, whitespace=' '
billion, MONEY, whitespace=''
</code></pre>
",1,1,1409,2020-12-16 17:55:28,https://stackoverflow.com/questions/65328587/replacement-entity-with-their-entity-label-using-spacy
How to extract Named Entities from Pandas DataFrame using SpaCy,"<p>I am trying to extract Named Entities using first answer to <a href=""https://stackoverflow.com/questions/60116419/extract-entity-from-dataframe-using-spacy"">this</a> question and code is as following</p>
<pre><code>for i in df['Article'].to_list():
    doc = nlp(i)
    for entity in doc.ents:
        print((entity.text))
</code></pre>
<p>But it is not printing entities. I have tried <code>print(i)</code> and <code>print(doc)</code> both variables have values and <code>df['Article']</code> contains news text. Can someone help with why second loop is not extracting entities? Thank you</p>
<p>EDIT:<br />
This is <a href=""https://drive.google.com/file/d/1toRNl-UgBIm5-E6DRfkl7t-lkz5PakOD/view?usp=sharing"" rel=""nofollow noreferrer"">dataset</a> file, please run following code to form preprocessing that I have done.</p>
<pre><code>df.iloc[:,0].dropna(inplace=True)
df = df[df.iloc[:,0].notna()]
</code></pre>
<p>to remove special characters from <code>df['Articles']</code></p>
<pre><code>df['Article'] = df['Article'].map(lambda x: re.sub(r'\W+', '', x))
</code></pre>
","python, pandas, spacy, named-entity-recognition","<p>With <code>df['Article'].map(lambda x: re.sub(r'\W+', '', x))</code>, you remove all whitespace chars from your articles.</p>
<p>You need to use</p>
<pre class=""lang-py prettyprint-override""><code>df['Article'] = df['Article'].str.replace(r'(?:_|[^\w\s])+', '')
</code></pre>
<p>With that regex, you will only remove special chars other than whitespaces.</p>
",1,2,1405,2020-12-18 17:44:14,https://stackoverflow.com/questions/65361605/how-to-extract-named-entities-from-pandas-dataframe-using-spacy
How to append Named Entites extracted from DataFrame?,"<p>the following code to extract and then print entities from <code>df['Article']</code> is working just fine.</p>
<pre><code>for i in df['Article'].to_list():
    doc = nlp(i)
    for entity in doc.ents:
        print((entity.text))
</code></pre>
<p>But whenever I try to <code>append</code> these entities using <code>entities_list.append((entity.text))</code> I get <code>TypeError: object of type 'float' has no len()</code> error I have tried to create <code>entities_list=[]</code> using following way</p>
<pre><code>entities_list = []
for i in df['Article'].to_list():
    doc = nlp(i)
    for entity in doc.ents:
        print((entity.text))
</code></pre>
<p>As well as</p>
<pre><code>for i in df['Article'].to_list():
    entities_list = []
    doc = nlp(i)
    for entity in doc.ents:
        print((entity.text))
</code></pre>
<p>Also even if I try to create another DataFrame or add new column to <code>df</code> I get same error. Can someone help with what am I doing wrong here? Thank you</p>
<p>EDIT:<br />
data in <code>df['Articles']</code> is news text like</p>
<blockquote>
<p>Pence’s move comes as inoculation efforts are unfurling around the
world in the race to halt a pandemic that has claimed at least 1.66
million lives and infected more than 74 million people.</p>
</blockquote>
<p>very first code prints entities extracted from text but I need those entities to append in list like as following</p>
<pre><code>[entity1, entity2, entity3, entity4]
</code></pre>
","python, pandas, for-loop, spacy, named-entity-recognition","<p>It seems that the column <em>Article</em> has some missing values, do the following:</p>
<pre><code>entities_list = []
for i in df['Article'].fillna('').to_list():
    doc = nlp(i)
    for entity in doc.ents:
        entities_list.append((entity.text))
</code></pre>
",1,1,164,2020-12-19 08:32:22,https://stackoverflow.com/questions/65368006/how-to-append-named-entites-extracted-from-dataframe
Repeating entity in replacing entity with their entity label using spacy,"<p>Code:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_md&quot;)

#read txt file, each string on its own line
with open(&quot;./try.txt&quot;,&quot;r&quot;) as f:
    texts = f.read().splitlines()

#substitute entities with their TAGS
docs = nlp.pipe(texts)
out = []
for doc in docs:
    out_ = &quot;&quot;
    for tok in doc:
        text = tok.text
        if tok.ent_type_:
            text = tok.ent_type_
        out_ += text + tok.whitespace_
    out.append(out_)

# write to file
with open(&quot;./out_try.txt&quot;,&quot;w&quot;) as f:
    f.write(&quot;\n&quot;.join(out))
</code></pre>
<p>Contents of input file:</p>
<pre><code>Georgia recently became the first U.S. state to &quot;ban Muslim culture.
His friend Nicolas J. Smith is here with Bart Simpon and Fred.
Apple is looking at buying U.K. startup for $1 billion
</code></pre>
<p>Contents of output file:</p>
<pre><code>GPE recently became the ORDINAL GPE state to &quot;ban NORP culture.
His friend PERSON PERSON PERSON is here with PERSON PERSON and PERSON.
ORG is looking at buying GPE startup for MONEYMONEY MONEY
</code></pre>
<p>I need to avoid this problem in above sentences. for example in (in sentence 2 'PERSON PERSON PERSON' to become one entity PERSON.</p>
","python, nlp, spacy, named-entity-recognition","<p>Lets try:</p>
<pre><code>import spacy
from spacy.gold import biluo_tags_from_offsets, spans_from_biluo_tags
nlp = spacy.load(&quot;en_core_web_md&quot;)

#read txt file, each string on its own line
with open(&quot;./try.txt&quot;,&quot;r&quot;) as f:
    texts = f.read().splitlines()

docs = nlp.pipe(texts)
out_text = &quot;&quot;
for doc in docs:
    offsets = []
    for ent in doc.ents:
        offsets.append((ent.start_char, ent.end_char, ent.label_))
    tags = biluo_tags_from_offsets(doc, offsets)
    text = *zip([tok for tok in doc],tags),
    out = []
    for item in text:
        tag = item[1].split(&quot;-&quot;)
        if tag[0] == &quot;O&quot;:
            out.append(item[0].text+item[0].whitespace_)
        if tag[0] == &quot;U&quot;:
            out.append(item[0].ent_type_+item[0].whitespace_)
        elif tag[0] == &quot;L&quot;:
            out.append(item[0].ent_type_+item[0].whitespace_)
    out_text += &quot;&quot;.join(out)+&quot;\n&quot;

with open(&quot;out_try.txt&quot;,&quot;w&quot;) as f:
    f.write(out_text)
</code></pre>
<p>Contents of the output file:</p>
<pre><code>GPE recently became the ORDINAL GPE state to &quot;ban NORP culture.
His friend PERSON is here with PERSON and PERSON.
ORG is looking at buying GPE startup for MONEY
</code></pre>
",1,0,326,2020-12-22 12:14:59,https://stackoverflow.com/questions/65408563/repeating-entity-in-replacing-entity-with-their-entity-label-using-spacy
How I can define function to integrate lists using python?,"<p>I am trying to define a function that iterates through three kind of list of lists, each list has the following structure:</p>
<pre><code>sentiment_labels =  [['B_S', 'O', 'O', 'O'], ['O', 'O', 'B_S', 'O']]
aspect_labels =     [['O', 'B_A', 'O', 'O'], ['O', 'O', 'O', 'B_A']]
modifier_labels =   [['O', 'O', 'BM', 'O'], ['O', 'O', 'O', 'O']]

# those lists contain 'B_A', 'I_S', 'B_S', 'I_S', 'BM', 'IM', and 'O' labels (IOB Tagging)

</code></pre>
<p>the <em><strong>target result</strong></em> must be like:</p>
<pre><code>labels = [['B_S', 'B_A', 'BM', 'O'], ['O', 'O', 'B_S', 'B_A'] ]
</code></pre>
<p>For this purpose, I have defined the following function:</p>
<pre><code># define function to integrate sentiments, aspects, and modifiers lists

def integrate_labels(sentiments_lists, aspects_lists, modifiers_lists):
  all_labels = []
  integrated_label = []

  # iterate through each list, then iterate through each element 
  for sentiment_list, aspect_list, modifier_list in zip(sentiments_lists, aspects_lists, modifiers_lists):
    
    for sentiment, aspect, modifier in zip(sentiment_list, aspect_list, modifier_list):

      # if the element is a sentiment label append it to the integrated_label list
      if sentiment != 'O':
        integrated_label.append(sentiment)

      # if the element is an aspect label append it to the integrated_label list
      elif aspect != 'O':
        integrated_label.append(aspect)

      # if the element is a modifier label append it to the integrated_label list
      elif modifier != 'O':
        integrated_label.append(modifier)

      else:
        integrated_label.append('O')
        
    # now append each integrated_label list to all_labels list
    all_labels.append(integrated_label)
  
  return all_labels
</code></pre>
<p>But I have this result:</p>
<pre><code>integrate_labels(sentiment_labels, aspect_labels, modifier_labels)

[['B_S', 'B_A', 'BM', 'O', 'O', 'O', 'B_S', 'B_A'],
 ['B_S', 'B_A', 'BM', 'O', 'O', 'O', 'B_S', 'B_A']]
</code></pre>
<p>How I could change integrate_labels function to get the <strong>target result</strong> ?</p>
<p>Thanks in advance!</p>
","python, function, nested-lists, named-entity-recognition","<p>change to</p>
<pre><code># now append each integrated_label list to all_labels list
all_labels.append(integrated_label.copy())
integrated_label = []
</code></pre>
",0,0,60,2020-12-29 01:39:28,https://stackoverflow.com/questions/65486229/how-i-can-define-function-to-integrate-lists-using-python
Do features have to be float numbers for multiclass-classification by Decision Tree?,"<pre><code>X_train

------------------------------------------------------------------------------------------
   | bias | word.lower | word[-3:] | word.isupper | word.isdigit |  POS  |  BOS  |  EOS  |
------------------------------------------------------------------------------------------
0  |  1.0 | headache,  |      HE,  |         True |        False |   NNP |  True | False |
1  |  1.0 |    mostly  |      tly  |        False |        False |   NNP | False | False |
2  |  1.0 |       but  |      BUT  |         True |        False |   NNP | False | False |
...
...
...

y_train

------------
   |  OBI  |
------------
0  | B-ADR |
1  | O     |
2  | O     |
...
...
...
</code></pre>
<p>I'm trying to do Name Entity Recognition (<strong>NER</strong>) with <strong>Decision Tree</strong>. My features dataframe and label dataframe look like the above. When I run the following code, it returns <code>ValueError: could not convert string to float: 'headache,'</code>. Are my data in the proper form (I'm following <a href=""https://www.datacamp.com/community/tutorials/decision-tree-classification-python"" rel=""nofollow noreferrer"">this tutorial</a>)? Do features have to be <strong>float numbers</strong> for multiclass-classification by Decision Tree? If so, how should I proceed the OBI labeling, given that most token features, if not all, are either string or Boolean?</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from sklearn.tree import DecisionTreeClassifier

DT = DecisionTreeClassifier()
DT.fit(X_train, y_train)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-15-aa02be64ac27&gt; in &lt;module&gt;
      1 DT = DecisionTreeClassifier()
----&gt; 2 DT.fit(X_train, y_train)

d:\python\lib\site-packages\sklearn\tree\_classes.py in fit(self, X, y, sample_weight, check_input, X_idx_sorted)
    888         &quot;&quot;&quot;
    889 
--&gt; 890         super().fit(
    891             X, y,
    892             sample_weight=sample_weight,

d:\python\lib\site-packages\sklearn\tree\_classes.py in fit(self, X, y, sample_weight, check_input, X_idx_sorted)
    154             check_X_params = dict(dtype=DTYPE, accept_sparse=&quot;csc&quot;)
    155             check_y_params = dict(ensure_2d=False, dtype=None)
--&gt; 156             X, y = self._validate_data(X, y,
    157                                        validate_separately=(check_X_params,
    158                                                             check_y_params))

d:\python\lib\site-packages\sklearn\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
    427                 # :(
    428                 check_X_params, check_y_params = validate_separately
--&gt; 429                 X = check_array(X, **check_X_params)
    430                 y = check_array(y, **check_y_params)
    431             else:

d:\python\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     70                           FutureWarning)
     71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---&gt; 72         return f(**kwargs)
     73     return inner_f
     74 

d:\python\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    596                     array = array.astype(dtype, casting=&quot;unsafe&quot;, copy=False)
    597                 else:
--&gt; 598                     array = np.asarray(array, order=order, dtype=dtype)
    599             except ComplexWarning:
    600                 raise ValueError(&quot;Complex data not supported\n&quot;

d:\python\lib\site-packages\numpy\core\_asarray.py in asarray(a, dtype, order)
     83 
     84     &quot;&quot;&quot;
---&gt; 85     return array(a, dtype, copy=False, order=order)
     86 
     87 

ValueError: could not convert string to float: 'headache,'
</code></pre>
","python, pandas, decision-tree, multiclass-classification, named-entity-recognition","<p>Yes, they need to be <strong>numeric</strong> (not necessarily float). So if you have 4 distinct text labels in a column then you need to convert this to 4 numbers. To do this, use sklearn's labelencoder. If your data is in a pandas dataframe <code>df</code>,</p>
<pre><code>from sklearn import preprocessing
from collections import defaultdict

# select text columns
cat_cols = df.select_dtypes(include='object').columns

# this is a way to apply label_encoder to all category cols at once, returning a label encoder per categorical column, in a dict d 
d = defaultdict(preprocessing.LabelEncoder)

 # transform all text columns to numbers
df[cat_cols] = df[cat_cols].apply(lambda x: d[x.name].fit_transform(x.astype(str)))
</code></pre>
<p>Once you have converted all columns to numbers, you may also wish to <a href=""https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f"" rel=""nofollow noreferrer"">&quot;one-hot&quot;</a> encode. Do this for categorical and boolean columns (here I've shown it for your categorical columns only).</p>
<pre><code># you should probably also one-hot the categorical columns
df = pd.get_dummies(df, columns=cat_cols)
</code></pre>
<p>You can retrieve the names of the values from the label encoder afterwards using the dict <code>d</code> of label encoders.</p>
<pre><code>d[col_name].inverse_transform(value)
</code></pre>
<p><a href=""https://medium.com/vickdata/four-feature-types-and-how-to-transform-them-for-machine-learning-8693e1c24e80"" rel=""nofollow noreferrer"">This tutorial</a> is particularly useful for understanding these concepts.</p>
",2,1,604,2020-12-30 11:13:32,https://stackoverflow.com/questions/65506053/do-features-have-to-be-float-numbers-for-multiclass-classification-by-decision-t
Can I use pre-labeled data in AWS SageMaker Ground Truth NER?,"<p>Let's say I have some text data that has already been labeled in SageMaker. This data could have either been labeled by humans or an ner model. Then let's say I want to have a human go back over the dataset, either to label new entity class or correct existing labels. How would I set up a labeling job to allow this? I tried using an output manifest from another labeling job, but all of the documents that were already labeled cannot be accessed by workers to re-label.</p>
","amazon-web-services, machine-learning, amazon-sagemaker, named-entity-recognition, labeling","<p>Yes, this is possible you are looking for <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates.html"" rel=""nofollow noreferrer"">Custom Labelling worklflows</a> you can also apply either Majority Voting (MV) or MDS to evaluate the accuracy of the job</p>
",0,1,839,2021-01-05 23:16:48,https://stackoverflow.com/questions/65587939/can-i-use-pre-labeled-data-in-aws-sagemaker-ground-truth-ner
Spacy Custom Name Entity Recognition (NER) &#39;catastrophic forgetting&#39; issue,"<p>The model is unable to remember the previous labels on which it was trained
i know that its 'catastrophic forgetting', but no example or blog seems to help this issue.
the most common response for this is this blog is this <a href=""https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"" rel=""nofollow noreferrer"">https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting</a> but this is pretty old now and is not helping</p>
<p>Here is my code:</p>
<pre><code>from __future__ import unicode_literals, print_function
import json
labeled_data = []
with open(r&quot;/content/emails_labeled.jsonl&quot;, &quot;r&quot;) as read_file:
    for line in read_file:
        data = json.loads(line)
        labeled_data.append(data)

TRAIN_DATA = []
for entry in labeled_data:
    entities = []
    for e in entry['labels']:
        entities.append((e[0], e[1],e[2]))
    spacy_entry = (entry['text'], {&quot;entities&quot;: entities})
    TRAIN_DATA.append(spacy_entry)       
import plac
import random
import warnings
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# new entity label
LABEL = &quot;OIL&quot;

# training data
# Note: If you're using an existing model, make sure to mix in examples of
# other entity types that spaCy correctly recognized before. Otherwise, your
# model might learn the new type, but &quot;forget&quot; what it previously knew.
# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
'''
TRAIN_DATA = [
    (
        &quot;Horses are too tall and they pretend to care about your feelings&quot;,
        {&quot;entities&quot;: [(0, 6, LABEL)]},
    ),
    (&quot;Do they bite?&quot;, {&quot;entities&quot;: []}),
    (
        &quot;horses are too tall and they pretend to care about your feelings&quot;,
        {&quot;entities&quot;: [(0, 6, LABEL)]},
    ),
    (&quot;horses pretend to care about your feelings&quot;, {&quot;entities&quot;: [(0, 6, LABEL)]}),
    (
        &quot;they pretend to care about your feelings, those horses&quot;,
        {&quot;entities&quot;: [(48, 54, LABEL)]},
    ),
    (&quot;horses?&quot;, {&quot;entities&quot;: [(0, 6, LABEL)]}),
]
'''

@plac.annotations(
    model=(&quot;Model name. Defaults to blank 'en' model.&quot;, &quot;option&quot;, &quot;m&quot;, str),
    new_model_name=(&quot;New model name for model meta.&quot;, &quot;option&quot;, &quot;nm&quot;, str),
    output_dir=(&quot;Optional output directory&quot;, &quot;option&quot;, &quot;o&quot;, Path),
    n_iter=(&quot;Number of training iterations&quot;, &quot;option&quot;, &quot;n&quot;, int),
)
def main(model='/content/LinkModelOutput', new_model_name=&quot;Oil21&quot;, output_dir='/content/Last', n_iter=30):
    &quot;&quot;&quot;Set up the pipeline and entity recognizer, and train the new entity.&quot;&quot;&quot;
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(&quot;Loaded model '%s'&quot; % model)
    else:
        nlp = spacy.blank(&quot;en&quot;)  # create blank Language class
        print(&quot;Created blank 'en' model&quot;)
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if &quot;ner&quot; not in nlp.pipe_names:
        ner = nlp.create_pipe(&quot;ner&quot;)
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe(&quot;ner&quot;)

    ner.add_label(LABEL)  # add new entity label to entity recognizer
    # Adding extraneous labels shouldn't mess anything up
    #ner.add_label(&quot;VEGETABLE&quot;)
    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.resume_training()
    move_names = list(ner.move_names)
    # get names of other pipes to disable them during training
    pipe_exceptions = [&quot;ner&quot;, &quot;trf_wordpiecer&quot;, &quot;trf_tok2vec&quot;]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    # only train NER
    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():
        # show warnings for misaligned entity spans once
        warnings.filterwarnings(&quot;once&quot;, category=UserWarning, module='spacy')

        sizes = compounding(1.0, 4.0, 1.001)
        # batch up the examples using spaCy's minibatch
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            batches = minibatch(TRAIN_DATA, size=sizes)
            losses = {}
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.entity.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
            print(&quot;Losses&quot;, losses)

    # test the trained model
    test_text = &quot;Here is Hindustan petroleum's oil reserves coup in Australia. Details can be found at https://www.textfixer.com/tools/remove-line-breaks.php?&quot;
    doc = nlp(test_text)
    print(&quot;Entities in '%s'&quot; % test_text)
    for ent in doc.ents:
        print(ent.label_, ent.text)

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta[&quot;name&quot;] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(&quot;Saved model to&quot;, output_dir)

        # test the saved model
        print(&quot;Loading from&quot;, output_dir)
        nlp2 = spacy.load(output_dir)
        # Check the classes have loaded back consistently
        assert nlp2.get_pipe(&quot;ner&quot;).move_names == move_names
        doc2 = nlp2(test_text)
        for ent in doc2.ents:
            print(ent.label_, ent.text)


if __name__ == &quot;__main__&quot;:
    plac.call(main)
</code></pre>
<p>and the data annotation was done on 'Daccano'.
Here is a look at the data:</p>
<pre><code>{&quot;id&quot;: 174, &quot;text&quot;: &quot;service\tmarathon petroleum reduces service postings marathon petroleum co said it reduced the contract price it will pay for all grades of service oil one dlr a barrel effective today the decrease brings marathon s posted price for both west texas intermediate and west texas sour to dlrs a bbl the south louisiana sweet grade of service was reduced to dlrs a bbl the company last changed its service postings on jan reuter&quot;, &quot;meta&quot;: {}, &quot;annotation_approver&quot;: null, &quot;labels&quot;: [[61, 70, &quot;OIL&quot;], [147, 150, &quot;OIL&quot;]]}
{&quot;id&quot;: 175, &quot;text&quot;: &quot;mutual funds\tmunsingwear inc mun th qtr jan loss shr loss cts vs loss seven cts net loss vs loss revs mln vs mln year shr profit cts vs profit cts net profit vs profit revs mln vs mln avg shrs vs note per shr adjusted for for stock split july and for split may reuter&quot;, &quot;meta&quot;: {}, &quot;annotation_approver&quot;: null, &quot;labels&quot;: []}
</code></pre>
","python, nlp, spacy, named-entity-recognition, doccano","<p>I am not spacy expert, but I had the same problem. There are some points which are necessary: annotation tool, amount of train data, mixing of correct predicted entities.
First make sure, that your training data is correctly labeled by tool of your choice (you don't get userwarnings). For a good prediction your model needs a lot of data. It means at least 200 examples for each entity you want to train. I personally label as much data as possible. And spacy's maker reccomend to mix the entities which your model corretly predicted.</p>
",2,2,1470,2021-01-28 15:07:57,https://stackoverflow.com/questions/65939855/spacy-custom-name-entity-recognition-ner-catastrophic-forgetting-issue
Mapping entity IDs to strings in SpaCy 3.0,"<p>I have trained a simple NER pipeline using spacy 3.0. After training I want to get a list of predicted IOB tags, among other things from a <code>Doc</code> (<code>doc = nlp(text)</code>). For example, <code>[&quot;O&quot;, &quot;O&quot;, &quot;B&quot;, &quot;I&quot;, &quot;O&quot;]</code></p>
<p>I can easily get the IOB ids (integers) using</p>
<pre><code>&gt;&gt; doc.to_array(&quot;ENT_IOB&quot;)
array([2, 2, ..., 2], dtype=uint64)
</code></pre>
<p>But how can I get the mappings/lookup?</p>
<p>I didn't find any lookup tables in <code>doc.vocab.lookups.tables</code>.</p>
<p>I also understand that I can achieve the same effect by accessing the <code>ent_iob_</code> at each token (<code>[token.ent_iob_ for token in doc]</code>), but I was wondering if there is a better way?</p>
","python, python-3.x, spacy, named-entity-recognition","<p>Check the <a href=""https://spacy.io/api/token#attributes"" rel=""nofollow noreferrer""><code>token</code></a> documentation:</p>
<blockquote>
<ul>
<li><code>ent_iob</code>   IOB code of named entity tag. 3 means the token begins an entity, 2 means it is outside an entity, 1 means it is inside an entity, and 0 means no entity tag is set.</li>
<li><code>ent_iob_</code>  IOB code of named entity tag. “B” means the token begins an entity, “I” means it is inside an entity, “O” means it is outside an entity, and &quot;&quot; means no entity tag is set.</li>
</ul>
</blockquote>
<p>So, all you need is to map the ids to the names using a simple <code>iob_map = {0: &quot;&quot;, 1: &quot;I&quot;, 2: &quot;O&quot;, 3: &quot;B&quot;}</code> dictionary replacement:</p>
<pre><code>doc = nlp(&quot;John went to New York in 2010.&quot;)
print([x.text for x in doc.ents])
# =&gt; ['John', 'New York', '2010']
iob_map = {0: &quot;&quot;, 1: &quot;I&quot;, 2: &quot;O&quot;, 3: &quot;B&quot;}
print(list(map(iob_map.get, doc.to_array(&quot;ENT_IOB&quot;).tolist())))
# =&gt; ['B', 'O', 'O', 'B', 'I', 'O', 'B', 'O']
</code></pre>
",1,1,418,2021-02-07 19:23:21,https://stackoverflow.com/questions/66092141/mapping-entity-ids-to-strings-in-spacy-3-0
Difference between spaCy&#39;s (v3.0) `nlp.make_doc(text)` and `nlp(text)`? Why should we use `nlp.make_doc(text)` when training?,"<p>I understand that we should create <a href=""https://spacy.io/api/example"" rel=""nofollow noreferrer""><code>Example</code></a> objects and pass it to the <code>nlp.update()</code> method. According to the example in the <a href=""https://spacy.io/api/language#update"" rel=""nofollow noreferrer"">docs</a>, we have</p>
<pre class=""lang-py prettyprint-override""><code>for raw_text, entity_offsets in train_data:
    doc = nlp.make_doc(raw_text)
    example = Example.from_dict(doc, {&quot;entities&quot;: entity_offsets})
    nlp.update([example], sgd=optimizer)
</code></pre>
<p>And looking at the <a href=""https://github.com/explosion/spaCy/blob/6ed423c16c99206ff2b81176d9565d0e1c1b7071/spacy/language.py#L1049"" rel=""nofollow noreferrer"">source code</a> of the <code>make_doc()</code> method, it seems like we would be just tokenizing the input text and then annotating the tokens.</p>
<p>But the <code>Example</code> object should have the reference/&quot;gold-standard&quot; and the predicted values. How does the information ends up in the document when we call <code>nlp.make_doc()</code>?</p>
<p>Additionally, when trying to get the predicted entity tags (using a trained <code>nlp</code> pipeline) back from the <code>Example</code> object I get no entities (though I could if I had created the object with <code>nlp(text)</code>. And training crashes if I try using <code>nlp(text)</code> instead of <code>nlp.make_doc(text)</code> with</p>
<pre class=""lang-py prettyprint-override""><code>...
&gt;&gt;&gt; spacy.pipeline._parser_internals.ner.BiluoPushDown.set_costs()
ValueError()
</code></pre>
","python, spacy, named-entity-recognition","<p>You can feel free to ask this sort of question on the Github Discussions board as well. Thanks also for taking time to think about this and read some of the code before asking. I wish every question were like this.</p>
<p>Anyway. I think the <code>Example.from_dict()</code> constructor might be getting in the way of understanding how the class works. Does this make things clearer for you?</p>
<pre><code>from spacy.tokens import Doc, Span
from spacy.training import Example
import spacy
nlp = spacy.blank(&quot;en&quot;)

# Build a reference Doc object, representing the gold standard.
y = Doc(
    nlp.vocab,
    words=[&quot;I&quot;, &quot;work&quot;, &quot;at&quot;, &quot;Berlin!&quot;, &quot;.&quot;, &quot;It&quot;, &quot;'s&quot;, &quot;a&quot;, &quot;hipster&quot;, &quot;bar&quot;, &quot;.&quot;]
)
# There are other ways we could set up the Doc object, including just passing
# stuff into the constructor. I wanted to show modifying the Doc to set annotations.
ent_start = y.text.index(&quot;Berlin!&quot;)
assert ent_start != -1
ent_end = ent_start + len(&quot;Berlin!&quot;)
y.ents = [y.char_span(ent_start, ent_end, label=&quot;ORG&quot;)]
# Okay, so we have our gold-standard, aka reference aka y, Doc object.
# Now, at runtime we won't necessarily be tokenizing that input text that way.
# It's a weird entity. If we only learn from the gold tokens, we can never learn
# to tag this correctly, no matter how many examples we see, if the predicted tokens
# don't match this tokenization. Because we'll always be learning from &quot;Berlin!&quot; but
# seeing &quot;Berlin&quot;, &quot;!&quot; at runtime. We'll have train/test skew. Since spaCy cares how
# it does on actual text, not just on the benchmark (which is usually run with 
# gold tokens), we want to train from samples that have the runtime tokenization. So
# the Example object holds a pair (x, y), where the x is the input.
x = nlp.make_doc(y.text)
example = Example(x, y)
# Show the aligned gold-standard NER tags. These should have the entity as B-ORG L-ORG.
print(example.get_aligned_ner())
</code></pre>
<p>The other piece of information that might explain this is that the pipeline components try to deal with partial annotations, so that you can have rules which are presetting some entities. This is what's happening when you have a fully annotated <code>Doc</code> as the <code>x</code> --- it's taking those annotations as part of the input, and there's no valid action for the model when it tries to construct the best sequence of actions to learn from. The usability for this situation could be improved.</p>
",3,3,4226,2021-02-07 21:32:32,https://stackoverflow.com/questions/66093326/difference-between-spacys-v3-0-nlp-make-doctext-and-nlptext-why-shou
Convert column data from text file into nested lists in Python?,"<p>I have a <code>txt</code> file with column wise written sentences and labels that look like:</p>
<pre><code>O   are
O   there
O   any
O   good
B-GENRE romantic
I-GENRE comedies
O   out
B-YEAR  right
I-YEAR  now

O   show
O   me
O   a
O   movie
O   about
B-PLOT  cars
I-PLOT  that
I-PLOT  talk
</code></pre>
<p>I want to read data from this <code>txt</code> file into two nested lists.
The desired output should be like:</p>
<pre><code>labels = [['O','O','O','O','B-GENRE','I-GENRE','O','B-YEAR','I-YEAR'],['O','O','O','O','O','B-PLOT','I-PLOT','I-PLOT']]
sentences = [['are','there','any','good','romantic','comedies','out','right','now'],['show','me','a','movie','about','cars','that','talk']]
</code></pre>
<p>I have tried with the following:</p>
<pre><code>with open(&quot;engtrain.bio.txt&quot;, &quot;r&quot;) as f:
  lsta = []
  for line in f:
    lsta.append([x for x in line.replace(&quot;\n&quot;, &quot;&quot;).split()])
</code></pre>
<p>But I have the following output:</p>
<pre><code>[['O', 'are'],
 ['O', 'there'],
 ['O', 'any'],
 ['O', 'good'],
 ['B-GENRE', 'romantic'],
 ['I-GENRE', 'comedies'],
 ['O', 'out'],
 ['B-YEAR', 'right'],
 ['I-YEAR', 'now'],
 [],
 ['O', 'show'],
 ['O', 'me'],
 ['O', 'a'],
 ['O', 'movie'],
 ['O', 'about'],
 ['B-PLOT', 'cars'],
 ['I-PLOT', 'that'],
 ['I-PLOT', 'talk']]
</code></pre>
<p><strong>Update</strong>
I also tried the following:</p>
<pre><code>with open(&quot;engtest.bio.txt&quot;, &quot;r&quot;) as f:
  lines = f.readlines()
  labels = []
  sentences = []
  for l in lines:
    as_list = l.split(&quot;\t&quot;)
    labels.append(as_list[0])
    sentences.append(as_list[1].replace(&quot;\n&quot;, &quot;&quot;))
</code></pre>
<p>Unfortunately, still have an error:</p>
<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-66-63c266df6ace&gt; in &lt;module&gt;()
      6     as_list = l.strip().split(&quot;\t&quot;)
      7     labels.append(as_list[0])
----&gt; 8     sentences.append(as_list[1].replace(&quot;\n&quot;, &quot;&quot;))

IndexError: list index out of range
</code></pre>
<p>The original data are from this link (engtest.bio or entrain.bio): <a href=""https://groups.csail.mit.edu/sls/downloads/movie/"" rel=""nofollow noreferrer"">https://groups.csail.mit.edu/sls/downloads/movie/</a></p>
<p>Could you help me please?</p>
<p>Thanks in advance</p>
","python, type-conversion, nested-lists, named-entity-recognition","<p>Iterate over each line and split it by <code>tab</code>:</p>
<pre><code>labels = [[]]
sentences = [[]]
with open('engtrain.bio', 'r') as f:
    for line in f.readlines():
        line = line.rstrip()
        if line:
            label, sentence = line.split('\t')
            labels[-1].append(label)
            sentences[-1].append(sentence)
        else:
            labels.append([])
            sentences.append([])
</code></pre>
<p>Output <code>labels</code>:</p>
<pre><code>[['O', 'O', 'O', 'B-ACTOR', 'I-ACTOR'], ['O', 'O', 'O', 'O', 'B-ACTOR', 'I-ACTOR', 'O', 'O', 'B-YEAR'] ...
</code></pre>
<p>Output <code>sentences</code>:</p>
<pre><code>[['what', 'movies', 'star', 'bruce', 'willis'], ['show', 'me', 'films', 'with', 'drew', 'barrymore', 'from', 'the', '1980s'] ...
</code></pre>
",2,1,595,2021-02-08 07:56:39,https://stackoverflow.com/questions/66097734/convert-column-data-from-text-file-into-nested-lists-in-python
Spacy : Named entity Recognition on dates not working as expected,"<p>I am not sure that I understand exactly how spacy identifies named entites in a text, and in my case especially dates.</p>
<p>I am trying to extract the education + the respective date in a text document. I have something like this</p>
<pre><code>text = 'University of A  2019 - 2020
        University of B  2016 - 2019
        College A        2013 - 2016
        College B        2008 - 2013'
doc = nlp(text)
for ent in doc.ents:
     print(ent.text, ent.label_)
</code></pre>
<p>Which gives me as output :</p>
<pre><code>University of A  ORG
University of B  ORG
2016 - 2019      DATE
2013 - 2016      DATE
2008 - 2013      DATE
</code></pre>
<p>As expected the universities are recognized as organizations and I expected spacy not to recognize the colleges as it's less obvious than the university names. However I do not understand why I lost the first date but all the others work fine.</p>
<p>I tried on another text that was something like this :</p>
<pre><code>1997 : any text
1998 : any text
1999 : any text
...
2018 : any text
</code></pre>
<p>And here all dates where recognized except 2013 and 2018, although the format of the lines are the same as all the others.</p>
<p>Is there a way to train spacy to better recognize the dates or should I use another tool? I'm already using spacy for other parts of the same program. I'm not using regex right now cause the dates can be in so many different formats (only year, beginning year - end year, sometimes months and days too, etc.)</p>
","python, date, spacy, named-entity-recognition","<p>You need a more feature-rich model type, the one with <code>_md</code> or <code>_lg</code> suffix with spacy 2.x and <code>_trf</code> with spacy 3.x.</p>
<p>For example, you may install</p>
<pre class=""lang-py prettyprint-override""><code>python -m spacy download en_core_web_trf
</code></pre>
<p>Then, you may use</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('en_core_web_trf')
text = '''University of A  2019 - 2020
         University of B  2016 - 2019
         College A        2013 - 2016
         College B        2008 - 2013'''
doc = nlp(text)
for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p>Output:</p>
<pre><code>2019 - 2020 DATE
2016 - 2019 DATE
2013 - 2016 DATE
2008 - 2013 DATE
</code></pre>
",1,1,2220,2021-02-08 09:40:22,https://stackoverflow.com/questions/66099105/spacy-named-entity-recognition-on-dates-not-working-as-expected
How much data / context needed to train custom NER Spacy model?,"<p>I am trying to extract previous Job titles from a CV using spacy and named entity recognition.</p>
<p>I would like to train spacy to detect a custom named entity type : 'JOB'. For that I have around 800 job title names from <a href=""https://www.careerbuilder.com/browse/titles/"" rel=""nofollow noreferrer"">https://www.careerbuilder.com/browse/titles/</a> that I can use as training data.</p>
<p>In my training data for spacy, do I need to integrate these job titles in sentences added to provide context or not?
In general in the CV the job title kinda stands on it's own and is not really part of a full sentence.</p>
<p>Also, if I need to provide coherent context for each of the 800 titles, it will be too time-consuming for what I'm trying to do, so maybe there are other solutions than NER?</p>
","machine-learning, model, spacy, named-entity-recognition","<p>Generally, Named Entity Recognition relies on the context of words, otherwise the model would not be able to detect entities in previously unseen words. Consequently, the list of titles would not help you to train any model. You could rather run string matching to find any of those 800 titles in CV documents and you will even be guaranteed to find all of them - no unknown titles, though.</p>
<p>I you could find 800 (or less) real CVs and replace the Job names by those in your list (or others!), then you are all set to train a model capable of NER. This would be the way to go, I suppose. Just download as many freely available CVs from the web and see where this gets you. If it is not enough data, you can augment it, for example by exchanging the job titles in the data by some of the titles in your list.</p>
",3,1,1147,2021-02-11 16:14:45,https://stackoverflow.com/questions/66158457/how-much-data-context-needed-to-train-custom-ner-spacy-model
Finding the Start and End char indices in Spacy,"<p>I am training a custom model in Spacy to extract custom entities but while I need to provide an input train data that consists of my entities along with the index locations, I wanted to understand if there's a faster way to assign the index value for keywords I am looking for in a particular sentence in my training data.</p>
<p>An example of my traning data:</p>
<pre><code>TRAIN_DATA = [

('Behaviour Skills include Communication, Conflict Resolution, Work Life Balance,
 {'entities': [(25, 37, 'BS'),(40, ,60, 'BS'),(62, 79, 'BS')]
 })
            ]
</code></pre>
<p><strong>Now to pass the index location for specific keywords in my training data, I am presently counting it manually to give the location of my keyword.</strong></p>
<p>For example: in case of the first line where I am saying Behaviour skills include Communication etc, I am manually calculating the location of the index for the word &quot;Communication&quot; which is 25,37.</p>
<p>I am sure there must be another way to identify the location of these indices by some other methods instead counting it manually. Any ideas how can I achieve this?</p>
","python-3.x, nlp, spacy, indices, named-entity-recognition","<p>Using <code>str.find()</code> can help here. However, you have to loop through both sentences and keywords</p>
<pre><code>keywords = ['Communication', 'Conflict Resolution', 'Work Life Balance']
texts = ['Behaviour Skills include Communication, Conflict Resolution, Work Life Balance', 
        'Some sentence where lower case conflict resolution is included']

LABEL = 'BS'
TRAIN_DATA = []

for text in texts:
    entities = []
    t_low = text.lower()
    for keyword in keywords:
        k_low = keyword.lower()
        begin = t_low.find(k_low) # index if substring found and -1 otherwise
        if begin != -1:
            end = begin + len(keyword)
            entities.append((begin, end, LABEL))
    TRAIN_DATA.append((text, {'entities': entities}))
</code></pre>
<p>Output:</p>
<pre><code>[('Behaviour Skills include Communication, Conflict Resolution, Work Life Balance', 
{'entities': [(25, 38, 'BS'), (40, 59, 'BS'), (61, 78, 'BS')]}), 
('Some sentence where lower case conflict resolution is included', 
{'entities': [(31, 50, 'BS')]})]
</code></pre>
<p>I added <code>str.lower()</code> just in case you might need it.</p>
",1,0,674,2021-02-18 12:47:44,https://stackoverflow.com/questions/66260282/finding-the-start-and-end-char-indices-in-spacy
NER activation function in SPACY,"<p>I have searched the documentation, but I couldn't find the answer. Does SPACY uses ReLu, Softmax or both as activation function?</p>
<p>Thanks</p>
","spacy, named-entity-recognition, softmax, relu","<p>By default, SPACY uses both, as we can see in layers architectures page from SPACY 3.0:</p>
<p><a href=""https://spacy.io/usage/layers-architectures"" rel=""nofollow noreferrer"">https://spacy.io/usage/layers-architectures</a></p>
",1,0,116,2021-02-21 20:21:52,https://stackoverflow.com/questions/66306728/ner-activation-function-in-spacy
Custom NERs training with spaCy 3 throws ValueError,"<p>I am trying to add custom NER labels using spacy 3. I found tutorials for older versions and made adjustments for spacy 3. Here is the whole code I am using:</p>
<pre><code>import random
import spacy
from spacy.training import Example

LABEL = 'ANIMAL'
TRAIN_DATA = [
    (&quot;Horses are too tall and they pretend to care about your feelings&quot;, {'entities': [(0, 6, LABEL)]}),
    (&quot;Do they bite?&quot;, {'entities': []}),
    (&quot;horses are too tall and they pretend to care about your feelings&quot;, {'entities': [(0, 6, LABEL)]}),
    (&quot;horses pretend to care about your feelings&quot;, {'entities': [(0, 6, LABEL)]}),
    (&quot;they pretend to care about your feelings, those horses&quot;, {'entities': [(48, 54, LABEL)]}),
    (&quot;horses?&quot;, {'entities': [(0, 6, LABEL)]})
]
nlp = spacy.load('en_core_web_sm')  # load existing spaCy model
ner = nlp.get_pipe('ner')
ner.add_label(LABEL)
print(ner.move_names) # Here I see, that the new label was added
optimizer = nlp.create_optimizer()
# get names of other pipes to disable them during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != &quot;ner&quot;]
with nlp.disable_pipes(*other_pipes):  # only train NER
    for itn in range(20):
        random.shuffle(TRAIN_DATA)
        losses = {}
        for text, annotations in TRAIN_DATA:
            doc = nlp(text)
            example = Example.from_dict(doc, annotations)
            nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)
        print(losses)
# test the trained model # add some dummy sentences with many NERs

test_text = 'Do you like horses?'
doc = nlp(test_text)
print(&quot;Entities in '%s'&quot; % test_text)
for ent in doc.ents:
    print(ent.label_, &quot; -- &quot;, ent.text)
</code></pre>
<p>This code outputs the ValueError exception, but only after 2 iterations - notice the first 2 lines:</p>
<pre><code>{'ner': 9.862242701536594}
{'ner': 8.169456698315201}
Traceback (most recent call last):
  File &quot;.\custom_ner_training.py&quot;, line 46, in &lt;module&gt;
    nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)
  File &quot;C:\ogr\moje\python\spacy_pg\myvenv\lib\site-packages\spacy\language.py&quot;, line 1106, in update
    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])
  File &quot;spacy\pipeline\transition_parser.pyx&quot;, line 366, in spacy.pipeline.transition_parser.Parser.update
  File &quot;spacy\pipeline\transition_parser.pyx&quot;, line 478, in spacy.pipeline.transition_parser.Parser.get_batch_loss
  File &quot;spacy\pipeline\_parser_internals\ner.pyx&quot;, line 310, in spacy.pipeline._parser_internals.ner.BiluoPushDown.set_costs
ValueError
</code></pre>
<p>I see the <code>ANIMAL</code> label was added by calling <code>ner.move_names</code>.</p>
<p>When I change my the value <code>LABEL = 'PERSON</code>, the code runs successfully and recognizes horses as <code>PERSON</code> on the new data. This is why I am assuming, there is no error in the code itself.</p>
<p>Is there something I am missing? What am I doing wrong? Could someone reproduce, please?</p>
<p>NOTE: This is my first question ever here. I hope I provided all information. If not, let me know in the comments.</p>
","python, nlp, spacy, named-entity-recognition, spacy-3","<p>You need to change the following line in the <code>for</code> loop</p>
<pre><code>doc = nlp(text)
</code></pre>
<p>to</p>
<pre><code>doc = nlp.make_doc(text)
</code></pre>
<p>The code should work and produce the following results:</p>
<pre><code>{'ner': 9.60289144264557}
{'ner': 8.875474230820478}
{'ner': 6.370401408220459}
{'ner': 6.687456469517201}
... 
{'ner': 1.3796682589133492e-05}
{'ner': 1.7709562613218738e-05}

Entities in 'Do you like horses?'
ANIMAL  --  horses
</code></pre>
",2,1,1274,2021-02-22 07:03:34,https://stackoverflow.com/questions/66311315/custom-ners-training-with-spacy-3-throws-valueerror
Unexpected type of NER data when trying to train spacy ner pipe to add new named entity,"<p>I'm trying to add a new named entity to spacy but I couldn't have good examples of Example objects for ner training and I'm getting a value error.
Here is my code:</p>
<pre><code>import spacy
from spacy.util import minibatch, compounding
from pathlib import Path
from spacy.training import Example

nlp=spacy.load('en_core_web_lg')

ner=nlp.get_pipe(&quot;ner&quot;)
TRAIN_DATA=[('ABC is a worldwide organization',{'entities':[0,2,'CRORG']}),
           ('we stand with ABC',{'entities':[24,26,'CRORG']}),
           ('we supports ABC',{'entities':[15,17,'CRORG']})]
ner.add_label('CRORG')
# Disable pipeline components that dont need to change
pipe_exceptions = [&quot;ner&quot;]
unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

with nlp.disable_pipes(*unaffected_pipes):
    for iteration in range(30):
        random.shuffle(TRAIN_DATA)
        for raw_text,entity_offsets in TRAIN_DATA:
            doc=nlp.make_doc(raw_text)
            nlp.update([Example.from_dict(doc,entity_offsets)])
</code></pre>
<p><a href=""https://i.sstatic.net/tzSyj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tzSyj.png"" alt=""Here is the error message I'm getting"" /></a></p>
","nlp, spacy, named-entity-recognition","<p>The <code>'entitites'</code> in <code>TRAIN_DATA</code> are supposed to be a list of tuples. They have to be 2D, not just 1D.</p>
<p>So instead of:</p>
<pre><code>TRAIN_DATA=[('ABC is a worldwide organization',{'entities':[0,2,'CRORG']}),
           ('we stand with ABC',{'entities':[24,26,'CRORG']}),
           ('we supports ABC',{'entities':[15,17,'CRORG']})]
</code></pre>
<p>Use:</p>
<pre><code>TRAIN_DATA=[('ABC is a worldwide organization',{'entities':[(0,2,'CRORG')]}),
           ('we stand with ABC',{'entities':[(24,26,'CRORG')]}),
           ('we supports ABC',{'entities':[(15,17,'CRORG')]})]
</code></pre>
",3,1,853,2021-02-24 23:55:45,https://stackoverflow.com/questions/66360545/unexpected-type-of-ner-data-when-trying-to-train-spacy-ner-pipe-to-add-new-named
POS/NER able to differentiate between the same word being used in multiple contexts?,"<p>I have a collection of over 1 million bodies of text. Within those bodies are multiple entities whose names mimic common stop words and phrases.</p>
<p>This has created issues when tokenizing the data, as there are ~50 entities with the same problem. To counteract this, I've disabled the removal of the matched stop words before their removal. This is fine, but Ideally I'd have a way to differentiate when a token is actually meant to be a stop word vs an entity, since I only care for when it's used as an entity.</p>
<p>Here's a sample excerpt:</p>
<p><code>A determined somebody slept. Prior to this, A could never be comfortable with the idea of responsibility. It was foreign, something heard about through a story passed down by words of U. As slow as it could be, A began to find meaning in the words of a story.</code></p>
<p>A and U are entities/nouns in most of their usages here. POS tagging so far has only labelled A as a determiner, and NER either won't tag any instances of the word. Adding the target tags to the NER list will result in every instance being tagged as an entity, which is not the case.</p>
<p>So far I've primarily used the Stanford POS Tagger and SpaCY for NER.</p>
","pos-tagger, named-entity-recognition","<p>I think you should try to train your own NER model.<br />
You can do this in three steps, as follows:</p>
<ol>
<li>label a number of documents in your corpus.
You can do this using the <a href=""https://github.com/ieriii/spacy-annotator"" rel=""nofollow noreferrer"">spacy-annotator</a>.</li>
<li>train your spacy NER model from scratch.
You can follow the instructions in the <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">spacy docs</a>.</li>
<li>Use the trained model to predict entities in your corpus.</li>
</ol>
<p>By labelling a good amount of entities at step 1, the model will learn to differentiate between a determiner and an entity.</p>
",1,0,528,2021-02-26 00:41:44,https://stackoverflow.com/questions/66378553/pos-ner-able-to-differentiate-between-the-same-word-being-used-in-multiple-conte
Display custom entities using diSplacy,"<p>I have a text string with a set of <em>fixed</em> named entities (person, location, ...) as shown in the example below</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;My name is John Smith and I live in Paris&quot;
entities = [
    (&quot;Person&quot;, 11, 21),  # John Smith
    (&quot;Location&quot;, 36, 41),  # Paris
]
</code></pre>
<p>and I would like to display them using the very nice renderer from Spacy called DiSplacy [1].
If I understand well, the best way for me is to create a custom <code>Doc</code> object in Spacy using my custom entities but I didn't find the right way to do this.</p>
<p>[1] <a href=""https://spacy.io/usage/visualizers#ent"" rel=""nofollow noreferrer"">https://spacy.io/usage/visualizers#ent</a></p>
","python, spacy, named-entity-recognition","<p>I finally found a solution in Spacy forums (<a href=""https://github.com/explosion/spaCy/discussions/7239"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/discussions/7239</a>).</p>
<p>In a nutshell, Doc can be easily instantiated with entities in spacy 3.0+. In Spacy 2.0+ (my case) a solution is to use <code>char_span</code>:</p>
<pre><code>import spacy

nlp = spacy.blank(&quot;en&quot;)

text = &quot;My name is John Smith and I live in Paris&quot;
entities = [
    (&quot;Employee&quot;, 11, 21),  # John Smith
    (&quot;Location&quot;, 36, 41),  # Paris
]

doc = nlp(text)

ents = []
for ee in entities:
    ents.append(doc.char_span(ee[1], ee[2], ee[0]))

doc.ents = ents

for ent in doc.ents:
    print(ent, ent.label_, sep=&quot;\t&quot;)
</code></pre>
",0,3,2711,2021-03-01 16:47:06,https://stackoverflow.com/questions/66426177/display-custom-entities-using-displacy
Add a custom component to pipeline in Spacy 3,"<p>I trained a NER model with Spacy3. I would like to add a custom component (add_regex_match) to the pipeline for NER task. The aim is to improve the existing NER results.</p>
<p>This is the code I want to implement:</p>
<pre><code>import spacy
from spacy.language import Language
from spacy.tokens import Span
import re

nlp = spacy.load(r&quot;\src\Spacy3\ner_spacy3_hortisem\training\ml_rule_model&quot;)

@Language.component(&quot;add_regex_match&quot;)
def add_regex_entities(doc):   
    new_ents = []

    label_z = &quot;Zeit&quot;
    regex_expression_z = r&quot;^(?:(?:31(\/|-|\.)(?:0?[13578]|1[02]|(?:Januar|März|Mai|Juli|August|Oktober|Dezember)))\1|(?:(?:29|30)(\/|-|\.)(?:0?[1,3-9]|1[0-2]|(?:Januar|März|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember))\2))(?:(?:1[6-9]|[2-9]\d)?\d{2})$|^(?:29(\/|-|\.)(?:0?2|(?:Februar))\3(?:(?:(?:1[6-9]|[2-9]\d)?(?:0[48]|[2468][048]|[13579][26])|(?:(?:16|[2468][048]|[3579][26])00))))$|^(?:0?[1-9]|1\d|2[0-8])(\/|-|\.)(?:(?:0?[1-9]|(?:Januar|Februar|März|April|Mai|Juni|Juli|August|September))|(?:1[0-2]|(?:Oktober|November|Dezember)))\4(?:(?:1[6-9]|[2-9]\d)?\d{2})$&quot;
    for match in re.finditer(regex_expression_z, doc.text):  # find match in text
        start, end = match.span()  # get the matched token indices
        entity = Span(doc, start, end, label=label_z)
        new_ents.append(entity)
        
    label_b = &quot;BBCH_Stadium&quot;
    regex_expression_b = r&quot;BBCH(\s?\d+)\s?(\/|\-|(bis)?)\s?(\d+)?&quot;
    for match in re.finditer(regex_expression_b, doc.text):  # find match in text
        start, end = match.span()  # get the matched token indices
        entity = Span(doc, start, end, label=label_b)
        new_ents.append(entity)

    doc.ents = new_ents
    return doc
nlp.add_pipe(&quot;add_regex_match&quot;, after=&quot;ner&quot;)

nlp.to_disk(&quot;./training/ml_rule_regex_model&quot;)

doc = nlp(&quot;20/03/2021 8 März 2021 BBCH 15, Fliegen, Flugbrand . Brandenburg, in Berlin, Schnecken, BBCH 13-48, BBCH 3 bis 34&quot;)

print([(ent.text, ent.label_) for ent in doc.ents])
</code></pre>
<p>when I want to evaluate the saved model ml_rule_regex_model using the command line <code>python -m spacy project run evaluate</code>, I got the error:
'ValueError: [E002] Can't find factory for 'add_regex_match' for language German (de). This usually happens when spaCy calls <code>nlp.create_pipe</code> with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator <code>@Language.component</code> (for function components) or <code>@Language.factory</code> (for class components).'</p>
<p>How should I do it? Has anyone had experience? Thank you very much for your tips.</p>
","python, named-entity-recognition, spacy-3","<blockquote>
<p>when I want to evaluate the saved model ml_rule_regex_model using the command line <code>python -m spacy project run evaluate</code>, I got the error...</p>
</blockquote>
<p>You haven't included the <code>project.yml</code> of your <code>spacy project</code>, where the <code>evaluate</code> command is defined. I will assume it calls <a href=""https://spacy.io/api/cli#evaluate"" rel=""nofollow noreferrer""><code>spacy evaluate</code></a>? If so, that command has a <code>--code</code> or <code>-c</code> flag to provide a path to a Python file with additional code, such as registered functions. By providing this file and pointing it to the definition of your new <code>add_regex_match</code> component, spaCy will be able to parse the configuration file and use the model.</p>
",2,1,1844,2021-03-08 10:44:54,https://stackoverflow.com/questions/66528347/add-a-custom-component-to-pipeline-in-spacy-3
"How to optimize SpaCy pipe for NER only (using an existing model, no training)","<p>I am looking to use SpaCy v3 to extract named entities from a large list of sentences. What I have works, but it seems slower than it should be, and before investing in more machines, I'd like to know if I am doing more work than I need to in the pipe.</p>
<p>I've used ntlk to parse everything into sentences as an iterator, then process these using &quot;pipe&quot; to get the named entities. All of this appears to work well, and python appears to be hitting every cpu core on my machine fairly heavily, which is good.</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_trf&quot;)
for (doc, context) in nlp.pipe(lines, as_tuples=True, batch_size=1000):
    for ent in doc.ents:
        pass #handle each entity
</code></pre>
<p>I understand that I can use nlp.disable_pipes to disable certain elements. Is there anything I can disable that won't impact accuracy and that isn't required for NER?</p>
","spacy, named-entity-recognition","<p>For NER only with the transformer model <code>en_core_web_trf</code>, you can disable <code>[&quot;tagger&quot;, &quot;parser&quot;, &quot;attribute_ruler&quot;, &quot;lemmatizer&quot;]</code>.</p>
<p>If you want to use a non-transformer model like <code>en_core_web_lg</code> (much faster but slightly lower accuracy), you can disable <code>[&quot;tok2vec&quot;, &quot;tagger&quot;, &quot;parser&quot;, &quot;attribute_ruler&quot;, &quot;lemmatizer&quot;]</code> and use <code>nlp.pipe(n_process=-1)</code> for multiprocessing on all CPUs (or <code>n_process=N</code> to restrict to N CPUs).</p>
",6,3,2216,2021-03-13 12:55:53,https://stackoverflow.com/questions/66613770/how-to-optimize-spacy-pipe-for-ner-only-using-an-existing-model-no-training
"NLU Watson API - ApiException: Error: invalid request: content is empty, Code: 400","<p>I am trying to do Name Entity Recognition, using Python and IBM Watson API. I did it before, but now faced a weird problem. This is a <a href=""https://stackoverflow.com/questions/49977879/how-to-resolve-content-is-empty-error-in-nlu-watson-api"">similar question</a>.</p>
<p><code>ApiException: Error: invalid request: content is empty, Code: 400 , X-global-transaction-id: 9ba464a0-310b-4106-8044-0362ba1d5850</code></p>
<p>I managed to get results only on a very small sample (10 articles), but if I increase it, I get this error above.</p>
<p>My code:</p>
<pre><code># Pass credentials
authenticator = IAMAuthenticator('#######################')
natural_language_understanding = NaturalLanguageUnderstandingV1(version = '2020-08-01', authenticator = authenticator)

# Create a function for extracting information, using Nick's code
def get_company_list(text):
    response = natural_language_understanding.analyze(
           language = 'en', text = text,
           features = Features(
                        entities = EntitiesOptions(sentiment = False, emotion = False)
                        # keywords = KeywordsOptions(sentiment = True, emotion = True),
                        # concepts = ConceptsOptions(limit = 50),
                        # relations = RelationsOptions(),
                        # sentiment = SentimentOptions(),
                        # semantic_roles = SemanticRolesOptions(entities = True, keywords = True),
                        # emotion = EmotionOptions(),
                        # categories = CategoriesOptions()
            )).get_result()
  company_list = [entity['text'] for entity in response['entities'] if entity['type'] == 'Company']
  time.sleep(0.1)
  return(company_list)

# Apply the function and get a list of companies and titles
company_list_text = [get_company_list(text) for text in data1000['text']]
company_list_title = [get_company_list(text) for text in data1000['title']]
</code></pre>
<p>Then error occurs. I also use the university server to run it, but I am not sure it is a problem. Previously I did it successfully on Colab.</p>
","python, nlp, ibm-watson, named-entity-recognition","<p>The <code>ApiException: Error: invalid request: content is empty, Code: 400</code> error occurs when an empty string or invalid characters are passed as input to Watson NLU.</p>
<p>Check your input data for missing or invalid text values before dispatching the API request.</p>
",1,1,1014,2021-03-14 19:15:23,https://stackoverflow.com/questions/66628569/nlu-watson-api-apiexception-error-invalid-request-content-is-empty-code-4
NER tagging schema for non-contiguous tokens,"<p>The most common tagging procedure for NER is IOB. But it seems that this kind of tagging is limited to cases where tokens from the same entity are contiguous.</p>
<p>So for instance,</p>
<p><code>Jane Smith is walking in the park</code> would be tagged as: <code>B-PER I-PER O O O O O</code></p>
<p>And here my PER entity is the concatenation of <code>[Jane, Smith]</code></p>
<p>If we tweak the example:</p>
<p><code>Jane and James Smith are walking in the park</code></p>
<p><code>B-PER O B-PER I-PER O O O O O</code></p>
<p>Now the issue is that the entities we would get are <code>[Jane]</code> and <code>[James, Smith]</code> because the IOB tagging does not allow to link Jane to Smith.</p>
<p>Is there any tagging schema that would allow to mark as entities both <code>[Jane, Smith]</code> and <code>[James, Smith]</code>?</p>
","deep-learning, nlp, pytorch, named-entity-recognition","<p>First, about doing this without a new data format:</p>
<p>There are a paper and repo about doing this using <a href=""http://textae.pubannotation.org/"" rel=""nofollow noreferrer"">TextAE</a> for this:</p>
<p><a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7362949/#:%7E:text=However%2C%20some%20entities%20are%20represented,%2Dcontiguous%2C%20or%20discontiguous%20entities."" rel=""nofollow noreferrer"">paper</a></p>
<p><a href=""https://github.com/jakelever/textae"" rel=""nofollow noreferrer"">repo</a></p>
<p>However, looking at their examples and yours, it seems like you could improve on <a href=""https://github.com/jakelever/textae/blob/stable/4/analysis/findNonContigious.py#L14-L17"" rel=""nofollow noreferrer"">what they did</a> by using dependency parsing. If you look at <a href=""https://explosion.ai/demos/displacy?text=Jane%20and%20James%20Smith%20are%20walking%20in%20the%20park&amp;model=en_core_web_sm&amp;cpu=0&amp;cph=0"" rel=""nofollow noreferrer"">the dependency parse</a> of &quot;Jane and James Smith are walking in the park&quot;, you can see that spaCy understands that Jane is conjoined with Smith. So after running entity extraction, you could do a dependency parse step, then edit your entities based on that.</p>
<p><strong>Now, to answer the real question.</strong> I have seen multi-dimensional labels that work in the following way (assume you have a maximum of ten entities per sentence:</p>
<pre class=""lang-py prettyprint-override""><code>empty = [0,0,0,0,0,0,0,0,0]

tokens = [&quot;Jane&quot;, &quot;and&quot;, &quot;James&quot;, &quot;Smith&quot;, &quot;are&quot;, &quot;walking&quot;, &quot;in&quot;, &quot;the&quot;, &quot;park&quot;]
labels = [
    [1, 0, 0, 1, 0, 0, 0, 0, 0],
    [0, 0, 1, 1, 0, 0, 0, 0, 0],
]
labels = labels + [empty] * (10-len(labels))
</code></pre>
<p>If you have more than one entity type, you can use those instead of just <code>1</code>.</p>
<p>This format works better with BERT anyway, since the BIO format is a pain when you have to split up tokens into BPE anyway.</p>
",2,4,609,2021-03-16 13:06:02,https://stackoverflow.com/questions/66655836/ner-tagging-schema-for-non-contiguous-tokens
How can i work with Example for nlp.update problem with spacy3.0,"<p>i am trying to train my data with spacy v3.0 and appareantly the nlp.update do not accept any tuples. Here is the piece of code:</p>
<pre><code>import spacy
import random
import json
nlp = spacy.blank(&quot;en&quot;)
ner = nlp.create_pipe(&quot;ner&quot;)
nlp.add_pipe('ner')
ner.add_label(&quot;label&quot;)
# Start the training
nlp.begin_training()
# Loop for 40 iterations
for itn in range(40):
    # Shuffle the training data
    random.shuffle(TRAINING_DATA)
    losses = {}
# Batch the examples and iterate over them
    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):
        texts = [text for text, entities in batch]
        annotations = [entities for text, entities in batch]
# Update the model
        nlp.update(texts, annotations, losses=losses, drop=0.3)
    print(losses)
</code></pre>
<p>and i am receiving error</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-79-27d69961629b&gt; in &lt;module&gt;
     18         annotations = [entities for text, entities in batch]
     19 # Update the model
---&gt; 20         nlp.update(texts, annotations, losses=losses, drop=0.3)
     21     print(losses)

~\Anaconda3\lib\site-packages\spacy\language.py in update(self, examples, _, drop, sgd, losses, component_cfg, exclude)
   1086         &quot;&quot;&quot;
   1087         if _ is not None:
-&gt; 1088             raise ValueError(Errors.E989)
   1089         if losses is None:
   1090             losses = {}

ValueError: [E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. 
</code></pre>
<p>I set my train data format:</p>
<pre><code>TRAINING_DATA = []
for entry in labeled_data:
    entities = []
    for e in entry['labels']:
        entities.append((e[0], e[1],e[2]))
    spacy_entry = (entry['text'], {&quot;entities&quot;: entities})
    TRAINING_DATA.append(spacy_entry)
</code></pre>
<p>My train data looks like this:</p>
<pre><code>[('Part List', {'entities': []}), ('pending', {'entities': []}), ('3D Printing', {'entities': [(0, 11, 'Process')]}), ('Recommended to use a FDM 3D printer with PLA material.', {'entities': [(25, 36, 'Process'), (41, 44, 'Material')]}), ('ï»¿', {'entities': []}), ('No need supports or rafts.', {'entities': []}), ('Resolution: 0.20mm', {'entities': []}), ('Fill density 20%', {'entities': []}), ('As follows from the analysis, part of the project is devoted to 3D', {'entities': [(64, 66, 'Process')]}), ('printing, as all static components were created using 3D modelling and', {'entities': [(54, 66, 'Process')]}), ('subsequent printing.', {'entities': []}), ('ï»¿', {'entities': []}), ('In our project, we created several versions of the', {'entities': []}), ('model during modelling, which we will describe and document in the', {'entities': []}), ('following subchapters. As a tool for 3D modelling, we used the Sketchup', {'entities': [(37, 49, 'Process')]}), ('Make tool, version from 2017. The main reason was the high degree of', {'entities': []}), ('intuitiveness and simplicity of the tool, as we had not encountered 3D', {'entities': [(68, 70, 'Process')]}), ('modelling before and needed a relatively flexible and efficient tool to', {'entities': []}), ('guarantee the desired result. with zero previous experience.', {'entities': []}), ('In this version, which is shown in the figures Figure 13 - Version no. 2 side view and Figure 24 - Version no. 2 - front view, for the first time, the specific dimensions of the infuser were clarified and', {'entities': []}), ('modelled. The details of the lower servo attachment, the cable hole in', {'entities': []}), ('the main mast, the winding cylinder mounting, the protrusion on the', {'entities': [(36, 44, 'Process')]}), ('winding cylinder for holding the tea bag, the preparation for fitting', {'entities': []}), ('the wooden and aluminium plate and the shape of the cylinder end that', {'entities': [(15, 25, 'Material')]}), ('exactly fit the servo were also reworked.', {'entities': []}), ('After the creation of this', {'entities': []}), ('version of the model, this model was subsequently officially consulted', {'entities': []}), ('and commented on for the first time.', {'entities': []}), ('In this version, which is shown in the figures Figure 13 - Version no. 2 side view and Figure 24 - Version no. 2 - front view, for the first time, the specific dimensions of the infuser were clarified and', {'entities': []}), ('modelled. The details of the lower servo attachment, the cable hole in', {'entities': []}), ('the main mast, the winding cylinder mounting, the protrusion on the', {'entities': [(36, 44, 'Process')]})]
</code></pre>
<p>I would appreciate your help as a new contributor. Thanks a lot!</p>
","nlp, spacy, named-entity-recognition","<p>You didn't provide your <code>TRAIN_DATA</code>, so I cannot reproduce it. However, you should try something like this:</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.training.example import Example

for batch in spacy.util.minibatch(TRAINING_DATA, size=2):
    for text, annotations in batch:
        # create Example
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, annotations)
        # Update the model
        nlp.update([example], losses=losses, drop=0.3)
</code></pre>
",38,21,20723,2021-03-17 14:36:59,https://stackoverflow.com/questions/66675261/how-can-i-work-with-example-for-nlp-update-problem-with-spacy3-0
Is there a way to render spaCy&#39;s NER output on a Dash dashboard?,"<p>I am trying to incorporate <code>spaCy</code>'s NER pre-trained model into my <code>Dash</code> Dashboard. I know <code>Dash</code> currently does not have the ability to render raw html so I am looking for a solution. I have search the online extensively without finding a solution.
Currently, I have a dashboard that looks like this:</p>
<p>[Dashboard]
<a href=""https://i.sstatic.net/aA4tg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aA4tg.png"" alt=""enter image description here"" /></a></p>
<p>Where I would like the <code>SpaCy</code>'s NER output to show underneath if possible. As an example please see the image below:</p>
<p>[NER Example Output]
<a href=""https://i.sstatic.net/rvoAh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rvoAh.png"" alt=""enter image description here"" /></a></p>
<p>If anyone has managed to find a solution that works with <code>Dash</code>, please let me know. If it isn't possible then it isn't the end of the world. I know it can be done in <code>Flask</code> albeit it is harder to code in HTML!</p>
<p>Many thanks!</p>
","python, spacy, dashboard, plotly-dash, named-entity-recognition","<p>it's not possible to render it in one line through displacy. However, you should be able to abstract the html through python functions and manually render the results. Here's an example app:</p>
<pre class=""lang-py prettyprint-override""><code>import dash
import dash_html_components as html

import spacy
from spacy.displacy.render import DEFAULT_LABEL_COLORS


# Initialize the application
app = dash.Dash(__name__)


def entname(name):
    return html.Span(name, style={
        &quot;font-size&quot;: &quot;0.8em&quot;,
        &quot;font-weight&quot;: &quot;bold&quot;,
        &quot;line-height&quot;: &quot;1&quot;,
        &quot;border-radius&quot;: &quot;0.35em&quot;,
        &quot;text-transform&quot;: &quot;uppercase&quot;,
        &quot;vertical-align&quot;: &quot;middle&quot;,
        &quot;margin-left&quot;: &quot;0.5rem&quot;
    })


def entbox(children, color):
    return html.Mark(children, style={
        &quot;background&quot;: color,
        &quot;padding&quot;: &quot;0.45em 0.6em&quot;,
        &quot;margin&quot;: &quot;0 0.25em&quot;,
        &quot;line-height&quot;: &quot;1&quot;,
        &quot;border-radius&quot;: &quot;0.35em&quot;,
    })


def entity(children, name):
    if type(children) is str:
        children = [children]

    children.append(entname(name))
    color = DEFAULT_LABEL_COLORS[name]
    return entbox(children, color)


def render(doc):
    children = []
    last_idx = 0
    for ent in doc.ents:
        children.append(doc.text[last_idx:ent.start_char])
        children.append(
            entity(doc.text[ent.start_char:ent.end_char], ent.label_))
        last_idx = ent.end_char
    children.append(doc.text[last_idx:])
    return children


text = &quot;When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.&quot;
nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(text)
print(&quot;Entities:&quot;, doc.ents)

# define de app
app.layout = html.Div(
    children=render(doc)
)

# Run the app
if __name__ == &quot;__main__&quot;:
    app.run_server(debug=True)

</code></pre>
<p>This produces the following result:
<a href=""https://i.sstatic.net/dsXBU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dsXBU.png"" alt=""enter image description here"" /></a></p>
<p>In the example above, the <code>entname</code> and <code>entbox</code> functions will respectively generate an <code>html.Span</code> and <code>html.Mark</code> with the style copied from the output html in <code>displacy</code>. Then, the function <code>entity</code> abstracts the previous two functions to easily generate an entity box. Finally, the <code>render</code> function will take spacy's <code>Doc</code> object and convert it into a list of Dash <code>html</code> components, which can be used inside the Dash layout.</p>
",3,3,1693,2021-03-19 12:47:05,https://stackoverflow.com/questions/66708474/is-there-a-way-to-render-spacys-ner-output-on-a-dash-dashboard
"For integer/dates values annotated using Prodigy, does the spaCy model learn the range of values as well?","<p>I have a prodigy session set up to annotate certain numeric values in a document for age (ranges from 0 to 100). I am only annotating the number. My question is, suppose there is a corrupt value which crept in (age being 1000 or 22.7), will the model understand that even though it is close to the age text in the document, it should not be picked up?</p>
<p>In other words, can it learn the range of integer values, and if it does, will that work for date format as well?
For instance a date in the format dd/mm/yyyy which is DOB (all the annotated ones are &lt; 01/01/2000) and there is a date 31/12/2020, will that get picked up as well since all the annotated dates are nowhere close to this range?</p>
<p>Thank you</p>
","nlp, spacy, named-entity-recognition, prodigy","<p>Good question! spaCy does not internally represent numeric tokens as numbers, so it doesn't have an explicit concept of the values. In that sense it can't tell between valid and invalid values for age.</p>
<p>However, spaCy does use &quot;shape&quot; features when representing tokens that will help it recognize valid ages. There are different kinds of shape tokens, but the one spaCy uses will represent words by converting characters to a representation of the character type. It works like this:</p>
<ul>
<li>spaCy → xxxXx</li>
<li>fish → xxxx</li>
<li>Fish → Xxxx</li>
<li>23 → dd</li>
<li>1000 → dddd</li>
<li>22.7 → dd.d</li>
</ul>
<p>Because of this you could expect that spaCy learns that two-digit numbers are likely to be ages, but numbers with decimals or four digits aren't likely. On the other hand, this doesn't help it differentiate between 100 and 999.</p>
<p>For dates this will not help with determining valid or invalid birthdates. Shape is just one of spaCy's features, but other features like prefix and suffix aren't really going to help with this either.</p>
<p>Since it's easy to verify numeric values in code, what I would suggest is matching broadly in spaCy and then using your own function to check whether dates or ages are valid by parsing them.</p>
<hr />
<p>Outside of spaCy in particular, the question of how NLP models represent numeric values is actually an increasingly popular research topic - if you'd like to know more about it this is a recent article on the topic: <a href=""https://ai.stanford.edu/blog/scalar-probing/"" rel=""nofollow noreferrer"">Do Language Models Know How Heavy an Elephant Is?</a></p>
",0,1,167,2021-03-23 11:23:30,https://stackoverflow.com/questions/66762169/for-integer-dates-values-annotated-using-prodigy-does-the-spacy-model-learn-the
Address Splitting with NLP,"<p>I am working currently on a project that should identify each part of an address, for example from &quot;str. Jack London 121, Corvallis, ARAD, ap. 1603, 973130 &quot; the output should be like this:</p>
<pre><code>street name: Jack London; 
no: 121; city: Corvallis; 
state: ARAD; 
apartment: 1603; 
zip code: 973130
</code></pre>
<p>The problem is that not all of the input data are in the same format so some of the elements may be missing or in different order, but it is guaranteed to be an address.</p>
<p>I checked some sources on the internet, but a lot of them are adapted for US addresses only - like Google API Places, the thing is that I will use this for another country.</p>
<p>Regex is not an option since the address may variate too much.</p>
<p>I also thought about NLP to use Named Entity Recognition model but I'm not sure that will work.</p>
<p>Do you know what could a be a good way to start, and maybe help me with some tips?</p>
","python, nlp, street-address, named-entity-recognition","<p>There is a <a href=""https://datascience.stackexchange.com/q/73783/25849"">similar question</a> in Data Science Stack Exchange forum with only one answer suggesting using SpaCy.</p>
<p>Another question on <a href=""https://stackoverflow.com/q/34408340/6573902"">detecting addresses using Stanford NLP</a> details another approach to detecting addresses and its constituents.</p>
<p>There is a <a href=""https://lexpredict-lexnlp.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">LexNLP</a> library that has a feature to detect and split addresses this way (snippet borrowed from <a href=""https://towardsdatascience.com/lexnlp-library-for-automated-text-extraction-ner-with-bafd0014a3f8"" rel=""nofollow noreferrer"">TowardsDatascience article</a> on the library):</p>
<pre><code>from lexnlp.extract.en.addresses import addresses
for filename,text in d.items():
    print(list(lexnlp.extract.en.addresses.get_addresses(text)))
</code></pre>
<p>There is also a relatively new (2018) and &quot;researchy&quot; code <a href=""https://github.com/GRAAL-Research/deepparse"" rel=""nofollow noreferrer"">DeepParse</a> (and <a href=""https://deepparse.org/"" rel=""nofollow noreferrer"">documentation</a>) for deep learning address parsing accompanying an <a href=""https://ieeexplore.ieee.org/document/8615844"" rel=""nofollow noreferrer"">IEEE article</a> (paywall) or <a href=""https://www.semanticscholar.org/paper/DeepParse%3A-A-Trainable-Postal-Address-Parser-Abid-Ul-Hasan/94c5a4e6394d0c97b67684e5cf4453909e8ec751"" rel=""nofollow noreferrer"">Semantic Scholar</a>.</p>
<p>For the training you will need to use some large corpora of addresses or fake addresses generated using, e.g. <a href=""https://pypi.org/project/Faker/"" rel=""nofollow noreferrer"">Faker</a> library.</p>
",12,9,4808,2021-03-24 10:18:46,https://stackoverflow.com/questions/66778944/address-splitting-with-nlp
Name Entity Recognition (NER) for multiple languages,"<p>I am writing some code to perform Named Entity Recognition (NER), which is coming along quite nicely for English texts. However, I would like to be able to apply NER to <em>any</em> language. To do this, I would like to 1) identify the language of a text, and then 2) apply the NER for the identified language. For step 2, I'm doubting to A) translate the text to English, and then apply the NER (in English), or B) apply the NER in the language identified.</p>
<p>Below is the code I have so far. What I would like is for the NER to work for text2, or in any other language, after this language is first recognized:</p>
<pre><code>import spacy
from spacy_langdetect import LanguageDetector
from langdetect import DetectorFactory

text = 'In 1793, Alexander Hamilton recruited Webster to move to New York City and become an editor for a Federalist Party newspaper.'
text2 = 'Em 1793, Alexander Hamilton recrutou Webster para se mudar para a cidade de Nova York e se tornar editor de um jornal do Partido Federalista.'

# Step 1: Identify the language of a text
DetectorFactory.seed = 0
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)
doc = nlp(text)
print(doc._.language)

# Step 2: NER
Entities = [(str(x), x.label_) for x in nlp(str(text)).ents]
print(Entities)
</code></pre>
<p>Does anyone have any experience with this? Much appreciated!</p>
","python, nlp, spacy, named-entity-recognition","<p>Spacy needs to load the correct model for the right language.</p>
<p>See <a href=""https://spacy.io/usage/models"" rel=""nofollow noreferrer"">https://spacy.io/usage/models</a> for available models.</p>
<pre><code>import spacy
from langdetect import detect
nlp={}    
for lang in [&quot;en&quot;, &quot;es&quot;, &quot;pt&quot;, &quot;ru&quot;]: # Fill in the languages you want, hopefully they are supported by spacy.
    if lang == &quot;en&quot;:
        nlp[lang]=spacy.load(lang + '_core_web_lg')
    else: 
        nlp[lang]=spacy.load(lang + '_core_news_lg')

def entites(text):
     lang = detect(text)
     try:
         nlp2 =nlp[lang]
     except KeyError:
         return Exception(lang + &quot; model is not loaded&quot;)
     return [(str(x), x.label_) for x in nlp2(str(text)).ents]
</code></pre>
<p>Then, you could run the two steps together</p>
<pre><code>ents = entites(text)
print(ents)
</code></pre>
",4,3,2153,2021-03-31 13:16:53,https://stackoverflow.com/questions/66888668/name-entity-recognition-ner-for-multiple-languages
"Same test and prediction values gives 0 precision, recall, f1 score for NER","<p>I was using sklearns crfsuite to compute the f1, precision, and recall scores but there is an anomaly. For just testing purposes I gave the same test and prediction values.</p>
<pre><code>from sklearn_crfsuite import scorers
from sklearn_crfsuite import metrics

cls = [i for i, _ in enumerate(CLASSES)]
cls.append(7)
cls.append(8)

print(metrics.flat_classification_report(
    test[&quot;y&quot;], test[&quot;y&quot;], labels=cls, digits=3
))
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0      1.000     1.000     1.000       551
           1      0.000     0.000     0.000         0
           2      0.000     0.000     0.000         0
           3      1.000     1.000     1.000      1196
           4      1.000     1.000     1.000      2593
           5      1.000     1.000     1.000     95200
           6      1.000     1.000     1.000      1165
           7      1.000     1.000     1.000      9636
           8      1.000     1.000     1.000    506363

   micro avg      1.000     1.000     1.000    616704
   macro avg      0.778     0.778     0.778    616704
weighted avg      1.000     1.000     1.000    616704
</code></pre>
<p>Why 1 and 2 labels are giving all 0 scores.
It should give 1 as the rest of the data. Can anyone explain to me the reason?</p>
<p>Need help. Thanks in advance!</p>
","python, scikit-learn, named-entity-recognition, precision-recall","<p>It seems that you don't actually have classes 1 and 2 in your data as the support of these two classes is zero, but since you have included classes 1 and 2 in the list of labels passed to <code>flat_classification_report()</code> they are still considered in the calculation of the various metrics.</p>
<pre><code>from sklearn_crfsuite import metrics
import numpy as np
np.random.seed(0)

cmin = 0
cmax = 8

labels = np.arange(1 + cmax)
print(np.unique(labels))
# [0 1 2 3 4 5 6 7 8]

y = np.random.randint(cmin, 1 + cmax, 1000).reshape(-1, 1)
print(np.unique(y))
# [0 1 2 3 4 5 6 7 8]

# classification report when &quot;y&quot; takes on all the specified labels
print(metrics.flat_classification_report(y_true=y, y_pred=y, labels=labels, digits=3))
#               precision    recall  f1-score   support
#            0      1.000     1.000     1.000       117
#            1      1.000     1.000     1.000       106
#            2      1.000     1.000     1.000       106
#            3      1.000     1.000     1.000       132
#            4      1.000     1.000     1.000       110
#            5      1.000     1.000     1.000       115
#            6      1.000     1.000     1.000       104
#            7      1.000     1.000     1.000       109
#            8      1.000     1.000     1.000       101
#     accuracy                          1.000      1000
#    macro avg      1.000     1.000     1.000      1000
# weighted avg      1.000     1.000     1.000      1000

# classification report when &quot;y&quot; takes on all the specified labels apart from 1 and 2,
# but 1 and 2 are still included among the possible labels
y = y[np.logical_and(y != 1, y != 2)].reshape(-1, 1)
print(np.unique(y))
# [0 3 4 5 6 7 8]

print(metrics.flat_classification_report(y_true=y, y_pred=y, labels=labels, digits=3))
#               precision    recall  f1-score   support
#            0      1.000     1.000     1.000       117
#            1      0.000     0.000     0.000         0
#            2      0.000     0.000     0.000         0
#            3      1.000     1.000     1.000       132
#            4      1.000     1.000     1.000       110
#            5      1.000     1.000     1.000       115
#            6      1.000     1.000     1.000       104
#            7      1.000     1.000     1.000       109
#            8      1.000     1.000     1.000       101
#    micro avg      1.000     1.000     1.000       788
#    macro avg      0.778     0.778     0.778       788
# weighted avg      1.000     1.000     1.000       788

# classification report when &quot;y&quot; takes on all the specified labels apart from 1 and 2,
# and 1 and 2 are not included among the possible labels
labels = labels[np.logical_and(labels != 1, labels != 2)]
print(np.unique(labels))
# [0 3 4 5 6 7 8]

print(metrics.flat_classification_report(y_true=y, y_pred=y, labels=labels, digits=3))
#               precision    recall  f1-score   support
#            0      1.000     1.000     1.000       117
#            3      1.000     1.000     1.000       132
#            4      1.000     1.000     1.000       110
#            5      1.000     1.000     1.000       115
#            6      1.000     1.000     1.000       104
#            7      1.000     1.000     1.000       109
#            8      1.000     1.000     1.000       101
#     accuracy                          1.000       788
#    macro avg      1.000     1.000     1.000       788
# weighted avg      1.000     1.000     1.000       788
</code></pre>
",1,0,698,2021-04-18 15:25:31,https://stackoverflow.com/questions/67150235/same-test-and-prediction-values-gives-0-precision-recall-f1-score-for-ner
Can you integrate your pre-trained word embeddings in a custom spaCy model?,"<p>Currently I am trying to develop a spaCy model for NER in the romanian legal domain. I was suggested to use specific WE that are presented at the following link (the links to download the WE are on the last pages - slides 25, 26, 27):</p>
<p><a href=""https://www1.ids-mannheim.de/fileadmin/kl/CoRoLa_based_Word_Embeddings.pdf"" rel=""nofollow noreferrer"">https://www1.ids-mannheim.de/fileadmin/kl/CoRoLa_based_Word_Embeddings.pdf</a></p>
<p>I already trained and tested a model without &quot;touching&quot; the pre-implemented WE but I do not know how to use external WE in computing a new spaCy model. Any relevant advice is appreciated. Although, an example of code will be preferable.</p>
","spacy, word-embedding, named-entity-recognition","<p>Yes, convert your vectors from word2vec text format with <code>spacy init vectors</code> and then specify that model as <code>[initialize.vectors]</code> in your config along with <code>include_static_vectors = true</code> for the relevant tok2vec models.</p>
<p>A config excerpt:</p>
<pre><code>[components.tok2vec.model.embed]
@architectures = &quot;spacy.MultiHashEmbed.v1&quot;
width = ${components.tok2vec.model.encode.width}
attrs = [&quot;ORTH&quot;, &quot;SHAPE&quot;]
rows = [5000, 2500]
include_static_vectors = true

[initialize]
vectors = &quot;my_vector_model&quot;
</code></pre>
<p>You can also use <code>spacy init config -o accuracy config.cfg</code> to generate a sample config including vectors that you can edit and adjust as you need.</p>
<p>See:</p>
<ul>
<li><a href=""https://spacy.io/api/cli#init-vectors"" rel=""nofollow noreferrer"">https://spacy.io/api/cli#init-vectors</a></li>
<li><a href=""https://spacy.io/usage/embeddings-transformers#static-vectors"" rel=""nofollow noreferrer"">https://spacy.io/usage/embeddings-transformers#static-vectors</a></li>
</ul>
",2,1,990,2021-04-21 16:32:43,https://stackoverflow.com/questions/67199914/can-you-integrate-your-pre-trained-word-embeddings-in-a-custom-spacy-model
Convert .CSV data into CoNLL BIO format for NER,"<p>I have some data in a .csv file that looks like this</p>
<pre><code>sent_num = [0, 1, 2]
text = [['Jack', 'in', 'the', 'box'], ['Jack', 'in', 'the', 'box'], ['Jack', 'in', 'the', 'box']]
tags = [['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG'], ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG'], ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG']]

df = pd.DataFrame(zip(sent_num, text, tags), columns=['sent_num', 'text', 'tags'])
df
</code></pre>
<p>I want to transform that data into CoNLL format text file like below, where each column (text and tags) is separated by a tab, and the
end of each sentence (or document) is indicated by a blank line.</p>
<pre><code>text    tags
Jack    B-ORG
in  I-ORG
the I-ORG 
box I-ORG

Jack    B-ORG
in  I-ORG
the I-ORG 
box I-ORG

Jack    B-ORG
in  I-ORG
the I-ORG
box I-ORG
</code></pre>
<p>What I have tried, but failed to work, it counts the empty rows as valid data, instead of the end of a sentence.</p>
<pre><code># create a three-column dataset
DF = df.apply(pd.Series.explode)
DF.head()

# insert space between rows in the data frame
# find the indices where changes occur 
switch = DF['sent_num'].ne(DF['sent_num'].shift(-1))

# construct a new empty dataframe and shift index by .5
DF1 = pd.DataFrame('', index=switch.index[switch] + .1, columns=DF.columns)

# concatenate old and new dataframes and sort by index, reset index and remove row positions by iloc
DF2 = pd.concat([DF, DF1]).sort_index().reset_index(drop=True).iloc[:-1]
DF2.head()

group by tags
DF2[['text', 'tags']].groupby('tags').count()
</code></pre>
<p>I am looking for some help in modifying or improving the code I have.</p>
","python-3.x, pandas, text, named-entity-recognition, conll","<pre><code>with open(&quot;output.txt&quot;, &quot;w&quot;) as f_out:
    print(&quot;text\ttags&quot;, file=f_out)
    for _, line in df.iterrows():
        for txt, tag in zip(line[&quot;text&quot;], line[&quot;tags&quot;]):
            print(&quot;{}\t{}&quot;.format(txt, tag), file=f_out)
        print(file=f_out)
</code></pre>
<p>Creates <code>output.txt</code>:</p>
<pre class=""lang-none prettyprint-override""><code>text    tags
Jack    B-ORG
in  I-ORG
the I-ORG
box I-ORG

Jack    B-ORG
in  I-ORG
the I-ORG
box I-ORG

Jack    B-ORG
in  I-ORG
the I-ORG
box I-ORG

</code></pre>
",3,1,2026,2021-04-21 16:46:45,https://stackoverflow.com/questions/67200114/convert-csv-data-into-conll-bio-format-for-ner
Entities extraction based on customized list in R,"<p>I have list of texts and I also have a list of entities.</p>
<p>The list of texts is typically in vectorized string.</p>
<p>The list of entities is a bit more complexed.
Some entities, can be listed out exhaustively such as the list of main cities of the world.
Some entities, while impossible to be listed out exhaustively, can be captured by regex pattern.</p>
<pre><code>
list_of_text &lt;- c('Lorem ipsum 12-01-2021 eat, Copenhagen 133.001.00.00 ...', 'Lorem ipsum 12-01-2021, Copenhagen www.stackoverflow.com swimming', ...)

entity_city &lt;- c('Copenhagen', 'Paris', 'New York', ...)

entity_IP_address &lt;- c('regex code for IP address')

entity_IP_address &lt;- c('regex code for URL')

entity_verb &lt;- c('verbs')

</code></pre>
<p>Given the <code>list_of_text</code> and the list of <code>entities</code>, I want to find matching entities for each text.</p>
<p>For example <code>c('Lorem ipsum 12-01-2021 eat drink sleep, Copenhagen 133.001.00.00 ...')</code>, it has <code>c(eat, drink, sleep)</code> for <code>entity_verb</code>, <code>c(133.001.00.00)</code> for <code>entity_IP</code>, etc.</p>
<pre><code>
res &lt;- extract_entity(text = c('Lorem ipsum 12-01-2021 eat drink sleep, Copenhagen 133.001.00.00 ...')
                      ,entities &lt;- c(entity_verb, entity_IP_address, entity_city))

res[['verb']]
c('eat', 'drink', 'sleep')

res[['IP']]
c('133.001.00.00')

res[['city']]
c('Copenhagen')

</code></pre>
<p>Is there a <code>R package</code> I can leverage on?</p>
","r, nlp, text-mining, r-package, named-entity-recognition","<p>Please take a look at maps and qdapDictionaries.  For world cities, I subset for cities with greater than a population of 1M.  Otherwise, it error with 'regular expression is too large'.</p>
<pre><code>library(maps)
library(qdapDictionaries)

list_of_text  &lt;- c('Lorem ipsum 12-01-2021 eat, Copenhagen 192.41.196.888','192.41.199.888','Lorem ipsum 12-01-2021, Copenhagen www.stackoverflow.com swimming')
#regex needs adjusted. Not extracting the first IP Address
ipRegex   &lt;- &quot;(?(?=.*?(\\d+\\.\\d+\\.\\d+\\.\\d+).*?)(\\1|))&quot;

regmatches(x = list_of_text , m = regexpr(ipRegex ,list_of_text ,perl = TRUE))[
  regmatches(x = list_of_text , m = regexpr(ipRegex ,list_of_text ,perl = TRUE)) != '']

verbRegex &lt;- substr(paste0((unlist(action.verbs)),'|',collapse = &quot;&quot;),
                     start = 1,nchar(paste0((unlist(action.verbs)),'|',collapse = &quot;&quot;))-1)

unlist(regmatches(x = list_of_text , m = gregexpr(verbRegex,list_of_text ,perl = TRUE))[
  regmatches(x = list_of_text , m = gregexpr(verbRegex,list_of_text ,perl = TRUE)) != ''])

citiesRegex &lt;- substr(paste0((unlist(world.cities[world.cities$pop &gt;1000000,'name'])),'|',collapse = &quot;&quot;),
                    start = 1,nchar(paste0((unlist(world.cities[world.cities$pop &gt;1000000,'name'])),'|',collapse = &quot;&quot;))-1)

unlist(regmatches(x = list_of_text , m = gregexpr(citiesRegex,list_of_text ,perl = TRUE))[
  regmatches(x = list_of_text , m = gregexpr(citiesRegex,list_of_text ,perl = TRUE)) != ''])
</code></pre>
",1,0,118,2021-04-22 07:55:42,https://stackoverflow.com/questions/67209062/entities-extraction-based-on-customized-list-in-r
Train two consecutive NER pipes in Spacy,"<p>I am working on a project to train a classifier to identify citations in a text. The citations we are dealing with tend be very disorganized. Below are some example citations:</p>
<ul>
<li>See Book A Chapter 3 Paragraph 7</li>
<li>See Paragraph 7 in Chapter 3 of Book A</li>
<li>See Chapter &quot;Some Chapter Title&quot; of Book A, Paragraph 7</li>
</ul>
<p>We have identified a small number of entities that tend to appear in these citations. For example, &quot;book title&quot;, &quot;chapter number&quot;, &quot;chapter name&quot;, &quot;paragraph number&quot;.</p>
<p>The project has two stages:</p>
<ol>
<li>Binary classification of the citation in the text</li>
<li>Classification of citation entities within the citation</li>
</ol>
<p>Is it possible with Spacy (we're using v3) to have two consecutive NER pipes? I would want the classifier to first tag the citations and only then tag the entities within each citation.</p>
<p>I was able to instantiate a model with two NER pipes with the below code:</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.lang.en import English
nlp = English()
nlp.add_pipe(&quot;ner&quot;, name=&quot;ner1&quot;, last=True)
ner1 = nlp.get_pipe(&quot;ner1&quot;)
ner1.add_label(&quot;Citation&quot;)
nlp.add_pipe(&quot;ner&quot;, name=&quot;ner2&quot;, last=True)
ner2 = nlp.get_pipe(&quot;ner2&quot;)
for label in [&quot;Book Title&quot;, &quot;Chapter Number&quot;, &quot;Chapter Name&quot;, &quot;Paragraph Number&quot;]:
    ner2.add_label(label)
</code></pre>
<p>My question is how to train each NER pipe separately. Normally, Spacy requires data in the following shape to train NER:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;text&quot;: &lt;TEXT&gt;,
    &quot;spans&quot;: [&lt;LIST OF NAMED ENTITY SPANS&gt;]
}
</code></pre>
<p>How can I distinguish data for each pipe in my training data?</p>
","python, spacy, named-entity-recognition, spacy-3","<p>There are several parts to this.</p>
<ol>
<li>You can have two NER components in one spaCy pipeline, though because of issues 2 and 3 this isn't going to work the way you want it to.</li>
<li>Pipelines cannot set annotations during training for downstream components. This is a limitation that is being worked on and should be resolved soon.</li>
<li>NER annotations cannot be overlapping. This is a design decision and is not going to change soon. It can be worked around with a custom component but it's extra work.</li>
</ol>
<blockquote>
<p>I would want the classifier to first tag the citations and only then tag the entities within each citation.</p>
</blockquote>
<p>Do you actually need the whole citation tag separately or are you designing this as a two-stage process to improve performance for some reason? If it's the latter, I would just try training on the second-stage detailed annotations first and see if you actually have a problem; I'm doubtful a two-stage process would actually make things easier.</p>
<p>If you actually need the whole &quot;citation&quot; then you can just extract chains of the detailed entities into a single span, there's no need to have a separate model for that.</p>
<p>I recommend you take a good look at the section on <a href=""https://spacy.io/usage/rule-based-matching#models-rules"" rel=""nofollow noreferrer"">Combining Models and Rules</a> in the docs. It has examples like expanding personal names to include titles like Mr. or Dr., or using dependency parse info, that seem applicable to your problem.</p>
",2,1,560,2021-04-25 07:14:42,https://stackoverflow.com/questions/67250719/train-two-consecutive-ner-pipes-in-spacy
NLP in python for food,"<p>I need to make a script for NLP in python. Given a string, I need to identify the food and output the calories. I thought of using a csv dataset and creating a model with tensorflow to identify the food. it's correct? do you think we can do another way?</p>
<p>Do you have examples or suggestions?</p>
<p>thank you</p>
","python, tensorflow, keras, nlp, named-entity-recognition","<p>You will have to use NER model to identify food in the string, and then you use the identified food to check the repository which has food calories for respective food item and share the output.</p>
",1,0,198,2021-04-26 11:28:13,https://stackoverflow.com/questions/67265819/nlp-in-python-for-food
catalogue.RegistryError: [E893] Could not find function &#39;spacy.copy_from_base_model.v1&#39; in function registry &#39;callbacks&#39;,"<p>I'm a  Spacy's new user and I'm trying to run this <a href=""https://github.com/explosion/projects/tree/v3/pipelines/ner_demo_update"" rel=""nofollow noreferrer"">ner_demo_update project</a> and I got this error :
<em>catalogue.RegistryError: [E893] Could not find function 'spacy.copy_from_base_model.v1' in function registry 'callbacks'. If you're using a custom function, make sure the code is available. If the function is provided by a third-party package, e.g. spacy-transformers, make sure the package is installed in your environment.</em>
I'll like to know if someone has face the same  issue.</p>
","named-entity-recognition, spacy-3","<p><code>copy_from_base_model.v1</code> is a new function, introduced in spaCy v3.0.6. Are you perhaps running an older version of spaCy? If so, can you try updating it? This will likely resolve your error.</p>
<p>See also: <a href=""https://github.com/explosion/spaCy/discussions/7985"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/discussions/7985</a></p>
",1,1,3353,2021-05-03 14:10:57,https://stackoverflow.com/questions/67370416/catalogue-registryerror-e893-could-not-find-function-spacy-copy-from-base-mo
Spacy 3.0 Training custom NER --&gt; Validation of this custom NER model,"<p>I trained a custom SpaCy Named entity recognition model to detect biased words in job description. Now that I trained 8 variantions (using different base model, training model, and pipeline setting), I want to evaluate which model is performing best.</p>
<p>But.. I can't find any documentation on the validation of these models.
There are some numbers of recall, f1-score and precision on the meta.json file, in the output folder, but that is no sufficient.</p>
<p>Anyone knows how to validate or can link me to the correct documentation? The documentation seem nowhere to be found.</p>
<p>NOTE: Talking about SpaCy V3.x</p>
","python-3.x, validation, nlp, named-entity-recognition, spacy-3","<p>During training you should provide &quot;evaluation data&quot; that can be used for validation. This will be evaluated periodically during training and appropriate scores will be printed.</p>
<p>Note that there's a lot of different terminology in use, but in spaCy there's &quot;training data&quot; that you actually train on and &quot;evaluation data&quot; which is not training and just used for scoring during the training process. To evaluate on held-out test data you can use the cli <a href=""https://spacy.io/api/cli#evaluate"" rel=""nofollow noreferrer"">evaluate</a> command.</p>
<p>Take a look at <a href=""https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands"" rel=""nofollow noreferrer"">this fashion brands example project</a> to see how &quot;eval&quot; data is configured and used.</p>
",3,1,2700,2021-05-12 20:09:52,https://stackoverflow.com/questions/67510383/spacy-3-0-training-custom-ner-validation-of-this-custom-ner-model
Retrain the multi language NER model(ner_ontonotes_bert_mult) from DeepPavlov with a dataset in a different language,"<p>I have successfully installed the multi-language NER model from DeepPavlov(ner_ontonotes_bert_mult). I want to retrain this model with new data(in the same format as they suggest in the <a href=""http://docs.deeppavlov.ai/en/master/features/models/ner.html#training-data"" rel=""nofollow noreferrer"">documentation</a> page) that are in the Albanian language.Is this possible(to retrain the multi-language NER model from DeepPavlov with data in a different language), or the retrain works only if we have English data??</p>
","nlp, named-entity-recognition, pre-trained-model, deeppavlov","<p>Yes, you can fine-tune the model on any language that was used for Multilingual BERT training <a href=""https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages</a>.</p>
<p>It is also possible to fine-tune on languages that are not from the list above if multilingual vocabulary has a good coverage for your language.</p>
",0,2,518,2021-05-21 10:20:45,https://stackoverflow.com/questions/67634995/retrain-the-multi-language-ner-modelner-ontonotes-bert-mult-from-deeppavlov-wi
Spacy dependencymatcher pattern not returning matches,"<p>I am trying to create, add and get results from a pattern using spacy DependencyMatcher.</p>
<p>I created a pattern for the sentence: &quot;From Monday to Friday&quot;</p>
<p>The full pattern:</p>
<pre><code>pattern = [
    {
        &quot;RIGHT_ID&quot;: &quot;node0&quot;,
        &quot;RIGHT_ATTRS&quot;: {'DEP': 'ROOT', 'POS': 'ADP', 'TAG': 'IN'}
    },
    {
        &quot;LEFT_ID&quot;: &quot;node0&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node1&quot;,
        &quot;RIGHT_ATTRS&quot;: {'DEP': 'pobj', 'POS': 'PROPN', 'TAG': 'NNP'},
    },
    {
        &quot;LEFT_ID&quot;: &quot;node1&quot;,
        &quot;REL_OP&quot;: &quot;$--&quot;,
        &quot;RIGHT_ID&quot;: &quot;node2&quot;,
        &quot;RIGHT_ATTRS&quot;: {'DEP': 'prep', 'POS': 'ADP', 'TAG': 'IN'},
    },
       {
        &quot;LEFT_ID&quot;: &quot;node2&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node3&quot;,
        &quot;RIGHT_ATTRS&quot;:{'DEP': 'pobj', 'POS': 'PROPN', 'TAG': 'NNP'},
    },
    
]
</code></pre>
<p>The simpler pattern is :</p>
<pre><code>pattern = [
    {
        &quot;RIGHT_ID&quot;: &quot;node0&quot;,
        &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;ADP&quot;}
    },
    {
        &quot;LEFT_ID&quot;: &quot;node0&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node1&quot;,
        &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;PROPN&quot;},
    },
    {
        &quot;LEFT_ID&quot;: &quot;node1&quot;,
        &quot;REL_OP&quot;: &quot;$--&quot;,
        &quot;RIGHT_ID&quot;: &quot;node2&quot;,
        &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;ADP&quot;},
    },
       {
        &quot;LEFT_ID&quot;: &quot;node2&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node3&quot;,
        &quot;RIGHT_ATTRS&quot;:{'POS': 'PROPN'},
    },
    
]
</code></pre>
<p><a href=""https://i.sstatic.net/pp356.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pp356.png"" alt=""enter image description here"" /></a></p>
<p>My question is, why is this pattern not giving any matches, not on the full or simpler pattern?</p>
<pre><code>import spacy
from spacy.matcher import DependencyMatcher


nlp = spacy.load(&quot;en_core_web_sm&quot;)
matcher = DependencyMatcher(nlp.vocab)


text=&quot;From monday to friday&quot;
doc = nlp(text)
matcher.add(&quot;pattern1&quot;, [pattern])

matches = matcher(doc)

# Each token_id corresponds to one pattern dict
match_id, token_ids = matches[0]
</code></pre>
<p>spacy versions:</p>
<p>spaCy v3.0.6</p>
<p>NAME             SPACY            VERSION</p>
<p>en_core_web_sm   &gt;=3.0.0,&lt;3.1.0   3.0.0   ✔</p>
","python, nlp, spacy, matcher, named-entity-recognition","<p>Your <code>REL_OP</code> for <code>node2</code> is backwards. It should be <code>$++</code>.</p>
<hr />
<p>To give a full explanation, this code works for me.</p>
<pre><code>import spacy

from spacy.matcher import DependencyMatcher

nlp = spacy.load(&quot;en_core_web_sm&quot;)
matcher = DependencyMatcher(nlp.vocab)

text=&quot;From Monday to Friday&quot;
doc = nlp(text)

pattern = [
    {
        &quot;RIGHT_ID&quot;: &quot;node0&quot;,
        &quot;RIGHT_ATTRS&quot;: {'POS': 'ADP', 'TAG': 'IN'}
    },
    {
        &quot;LEFT_ID&quot;: &quot;node0&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node1&quot;,
        &quot;RIGHT_ATTRS&quot;: {'POS': 'PROPN'},
    },
    {
        &quot;LEFT_ID&quot;: &quot;node1&quot;,
        &quot;REL_OP&quot;: &quot;$++&quot;,
        &quot;RIGHT_ID&quot;: &quot;node2&quot;,
        &quot;RIGHT_ATTRS&quot;: {'POS': 'ADP'},
    },
       {
        &quot;LEFT_ID&quot;: &quot;node2&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node3&quot;,
        &quot;RIGHT_ATTRS&quot;:{'POS': 'PROPN'},
    },
    
]

matcher.add(&quot;pattern1&quot;, [pattern])

matches = matcher(doc)
print(matches)

print(&quot;-----&quot;)
# this part is just for reference
for word in doc:
    print(word.pos_, word.tag_, word.dep_, word, sep=&quot;\t&quot;)
</code></pre>
<p>Couple of points about this:</p>
<ul>
<li>your second pattern is better, you shouldn't need to specify tag and pos for English (tag determines pos)</li>
<li>In the v3 small model &quot;monday&quot; and &quot;friday&quot; are not proper nouns unless capitalized (it looks like your displaCy output is from the public demo, which uses v2)</li>
</ul>
",1,1,398,2021-05-22 15:49:16,https://stackoverflow.com/questions/67651456/spacy-dependencymatcher-pattern-not-returning-matches
NLP: Create spaCy Doc objects based on delimiters or combine multiple Doc objects to form a single object,"<p>I am trying to create a spaCy Doc object (spacy.tokens.doc.Doc) using the make_doc() function. This is what I have done:</p>
<pre><code>import spacy
nlp = spacy.load('en')

a = nlp.make_doc(&quot;Sam, Software Engineer&quot;)
print(list(a)) # [Sam, ,, Software, Engineer]
</code></pre>
<p>But my desired result is:</p>
<pre><code>print(list(a)) # [Sam, Software Engineer]
</code></pre>
<p>Is there a way to create a spacy Doc object based on delimiters (in my case, its a comma)? Or is there a way to combine two spaCy Doc objects into one object? For eg:</p>
<pre><code>a = nlp.make_doc(&quot;Sam&quot;)
b = nlp.make_doc(&quot;Software Engineer&quot;)
c = Combine a and b into single Doc object c
print(list(c)) # [Sam, Software Engineer]
</code></pre>
","pandas, nlp, spacy, named-entity-recognition","<p>You can build a document using the <code>Doc</code> class after splitting the string with a comma:</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)
text = &quot;Sam, Software Engineer&quot;

tokens = text.split(',')
words_t = [t.strip() for t in tokens]
whitespaces_t = [x[0].isspace() for x in tokens]
a = spacy.tokens.Doc(nlp.vocab, words=words_t, spaces=whitespaces_t)
print(list(a))
# =&gt; [Sam, Software Engineer]
</code></pre>
<p>The <code>words_t = [t.strip() for t in tokens]</code> part grabs words and <code>whitespaces_t = [x[0].isspace() for x in tokens]</code> creates a list of boolean values denoting the presence of whitespace before the words.</p>
",2,2,587,2021-05-24 21:15:21,https://stackoverflow.com/questions/67679041/nlp-create-spacy-doc-objects-based-on-delimiters-or-combine-multiple-doc-object
How to use seqeval classification_report after having performed NER with HuggingFace transformers?,"<p>I have followed <a href=""https://huggingface.co/transformers/custom_datasets.html#tok-ner"" rel=""nofollow noreferrer"">this tutorial</a> on a custom dataset of mine, and I have now a trained model.</p>
<p>I would like to use <code>seqeval</code>'s <code>classification_report</code> function so that I have a nice display of the performances of my model.</p>
<p>On which data should I call <code>classification_report</code>?</p>
","nlp, huggingface-transformers, named-entity-recognition","<p>You can call the <code>classification_report</code> on your training data first to check if the model trained correctly, after that call it on the test data to check how your model is dealing with data that it didn't see before.</p>
",1,1,549,2021-05-26 14:11:35,https://stackoverflow.com/questions/67706707/how-to-use-seqeval-classification-report-after-having-performed-ner-with-hugging
How to use Spacy nlp custom ner to identity 2 types of docs at once,"<p>I want to make a SPACY ner model that identifies and uses tags depending on what doc type it is.</p>
<p>The input is in json format. Example-</p>
<pre><code>{&quot;text&quot;:{&quot;a&quot;:&quot;ABC DEF.&quot;,&quot;b&quot;:&quot;CDE FG.&quot;},
  &quot;annotations&quot;:[
    {&quot;start&quot;:0,&quot;end&quot;:3,&quot;doc_type&quot;:&quot;a&quot;,&quot;label&quot;:{&quot;text&quot;:&quot;FIRST&quot;},&quot;text&quot;:&quot;ABC&quot;}, 
    {&quot;start&quot;:4,&quot;end&quot;:6,&quot;doc_type&quot;:&quot;b&quot;,&quot;label&quot;:{&quot;text&quot;:&quot;SECOND&quot;},&quot;text&quot;:&quot;FG&quot;}
  ]
}
</code></pre>
<p>In this I want the model to identify that the 1st text is of type &quot;a&quot; so the text should be tagged with tag FIRST. Similarly second text is of type &quot;b&quot; so it the ner must be SECOND</p>
<p>How can I go about this problem? Thanks!</p>
","python, nlp, spacy, named-entity-recognition","<p>The description of your data is a little vague but given these assumptions:</p>
<ol>
<li>You don't know if a document is type A or type B, you need to classify it.</li>
<li>The NER is completely different between type A and B documents.</li>
</ol>
<p>What you should do is use (up to) three separate spaCy pipelines. Use the first pipeline with a textcat model to classify docs into A and B types, and then have one pipeline for NER for type A docs and one pipeline for type B docs. After classification just pass the text to the appropriate NER pipeline.</p>
<p>This is not the most efficient possible pipeline, but it's very easy to set up - you just train three separate models and stick them together with a little glue code.</p>
<p>You could also train the models separately and combine them in one spaCy pipeline, with some kind of special component to make execution of the NER conditional, but that would be pretty tricky to set up so I'd recommend the separate pipelines approach first.</p>
<p>That said, depending on your problem it's possible that you don't need two NER models, and learning entities for both types of docs would be effective. So I would also recommend you try putting all your training data together, training just one NER model, and seeing how it goes. If that works then you can have a single pipeline with textcat and NER models that don't directly interact with each other.</p>
<hr />
<p>To respond to the comment, when I say &quot;pipeline&quot; I mean a Language object, which is what <code>spacy.load</code> returns. So you train models using the config and each of those is in a directory and then you do this:</p>
<pre><code>import spacy

classifier = spacy.load(&quot;classifier&quot;)
ner_a = spacy.load(&quot;ner_a&quot;)
ner_b = spacy.load(&quot;ner_b&quot;)

texts = [&quot;I like cheese&quot;, ... raw texts ... ]

for text in texts:
    doc = classifier(text)
    if doc.cats[&quot;a&quot;] &gt; doc.cats[&quot;b&quot;]:
        nerdoc = ner_a(text)
    else:
        nerdoc = ner_b(text)
    ... do something with the doc here ...
</code></pre>
",1,1,683,2021-05-27 07:19:50,https://stackoverflow.com/questions/67717406/how-to-use-spacy-nlp-custom-ner-to-identity-2-types-of-docs-at-once
How to apply a pretrained transformer model from huggingface?,"<p>I am interested in using pre-trained models from Hugging Face for named entity recognition (NER) tasks without further training or testing of the model.</p>
<p>On the <a href=""https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"" rel=""nofollow noreferrer"">model page of Hugging Face</a>, the only information for reusing the model are as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
</code></pre>
<p>I tried the following code, but I am getting a tensor output instead of class labels for each named entity.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

text = &quot;my text for named entity recognition here.&quot;

input_ids = torch.tensor(tokenizer.encode(text, padding=True, truncation=True,max_length=50, add_special_tokens = True)).unsqueeze(0)

with torch.no_grad():
  output = model(input_ids, output_attentions=True)
</code></pre>
<p>Any suggestions on how to apply the model on a text for NER?</p>
","huggingface-transformers, named-entity-recognition, transformer-model","<p>In <code>transformers</code> NER is done with the <a href=""https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TokenClassificationPipeline"" rel=""nofollow noreferrer"">TokenClassificationPipeLine</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, pipeline,  AutoModelForTokenClassification
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
nerpipeline = pipeline('ner', model=model, tokenizer=tokenizer)
text = &quot;my text for named entity recognition here.&quot;
nerpipeline(text)
</code></pre>
<p>Output:</p>
<pre><code>[{'word': 'my',
  'score': 0.5209763050079346,
  'entity': 'LABEL_0',
  'index': 1,
  'start': 0,
  'end': 2},
 {'word': 'text',
  'score': 0.5161970257759094,
  'entity': 'LABEL_0',
  'index': 2,
  'start': 3,
  'end': 7},
 {'word': 'for',
  'score': 0.5297629237174988,
  'entity': 'LABEL_1',
  'index': 3,
  'start': 8,
  'end': 11},
 {'word': 'named',
  'score': 0.5258920788764954,
  'entity': 'LABEL_1',
  'index': 4,
  'start': 12,
  'end': 17},
 {'word': 'entity',
  'score': 0.5415489673614502,
  'entity': 'LABEL_1',
  'index': 5,
  'start': 18,
  'end': 24},
 {'word': 'recognition',
  'score': 0.5396601557731628,
  'entity': 'LABEL_1',
  'index': 6,
  'start': 25,
  'end': 36},
 {'word': 'here',
  'score': 0.5165827870368958,
  'entity': 'LABEL_0',
  'index': 7,
  'start': 37,
  'end': 41},
 {'word': '.',
  'score': 0.5266348123550415,
  'entity': 'LABEL_0',
  'index': 8,
  'start': 41,
  'end': 42}]
</code></pre>
<p>Please note that you need to use <code>AutoModelForTokenClassification</code> instead of <code>AutoModel</code> and that <em>not all</em> models have a trained head for token classification, i.e. you will get random weights for the token classification head :)</p>
",2,4,12464,2021-05-28 14:27:04,https://stackoverflow.com/questions/67740759/how-to-apply-a-pretrained-transformer-model-from-huggingface
How to extract a custom list of entities from a text file?,"<p>I have a list of entities which look something like this:</p>
<pre><code>[&quot;Bluechoice HMO/POS&quot;, &quot;Pathway X HMO/PPO&quot;, &quot;HMO&quot;, &quot;Indemnity/Traditional Health Plan/Standard&quot;]
</code></pre>
<p>It's not the exhaustive list, there are other similar entries.</p>
<p>I want to extract these entities, if present, from a text file (with over 30 pages of information). The crunch here is that this text file is generated using OCR and thus might not contain the exact entries. That is, for example, it might have:</p>
<pre><code>&quot;Out of all the entries the user made, BIueChoise HMOIPOS is the most prominent&quot;
</code></pre>
<p>Notice the spelling mistake in &quot;BIueChoise HMOIPOS&quot; w.r.t. &quot;Bluechoice HMO/POS&quot;.</p>
<p>I want those entities which are present in the text file even if the corresponding words do not match perfectly.</p>
<p>Any help, be it an algorithm or an approach, is welcomed. Thanks a lot!</p>
","python, nlp, text-extraction, named-entity-recognition, edit-distance","<p>You can do this by using algorithms that can approximately match strings and determine how similar they are, like <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a>, <a href=""https://en.wikipedia.org/wiki/Hamming_distance"" rel=""nofollow noreferrer"">Hamming distance</a>, <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">Cosine similarity</a>, and many more.</p>
<p><code>textdistance</code> is a module that has a wide range of such algorithms present that you can use. Check about it <a href=""https://pypi.org/project/textdistance/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I had similar problem that I solved using <code>textdistance</code> by picking substrings from the textfile of length equal to the string I needed to search/extract, and then use one of the algorithms to see which one solves my problem.
For me it was the <strong>cosine similarity</strong> which gave me the best results when I filtered out strings that fuzzy matched above <strong>75%</strong>.</p>
<p>Taking &quot;Bluechoice HMO/POS&quot; from your question as an example to give you an idea, I applied it like below:</p>
<pre><code>&gt;&gt;&gt; import textdistance
&gt;&gt;&gt;
&gt;&gt;&gt; search_strg = &quot;Bluechoice HMO/POS&quot;
&gt;&gt;&gt; text_file_strg = &quot;Out of all the entries the user made, BIueChoise HMOIPOS is the most prominent&quot;
&gt;&gt;&gt;
&gt;&gt;&gt; extracted_strgs = []
&gt;&gt;&gt; for substr in [text_file_strg[i:i+len(search_strg)] for i in range(0,len(text_file_strg) - len(search_strg)+1)]:
...     if textdistance.cosine(substr, search_strg) &gt; 0.75:
...             extracted_strgs.append(substr)
... 
&gt;&gt;&gt; extracted_strgs
['BIueChoise HMOIPOS']
</code></pre>
",0,2,693,2021-05-28 17:34:56,https://stackoverflow.com/questions/67743328/how-to-extract-a-custom-list-of-entities-from-a-text-file
Create a NER dictionary from a given text,"<p>I have the following variable</p>
<pre class=""lang-py prettyprint-override""><code>data = (&quot;Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country. Many people have been killed that day.&quot;,
        {&quot;entities&quot;: [(48, 54, 'Category 1'), (77, 81, 'Category 1'), (111, 118, 'Category 2'), (150, 173, 'Category 3')]})
</code></pre>
<p><code>data[1]['entities'][0] = (48, 54, 'Category 1')</code> stands for <code>(start_offset, end_offset, entity)</code>.</p>
<p>I want to read each word of <code>data[0]</code> and tag it according to <code>data[1]</code> entities. I am expecting to have as final output,</p>
<pre class=""lang-py prettyprint-override""><code>{
'Thousands': 'O', 
'of': 'O',
'demonstrators': 'O',
'have': 'O',
'marched': 'O',
'through': 'O',
'London': 'S-1',
'to': 'O', 
'protest': 'O', 
'the': 'O', 
'war': 'O', 
'in': 'O', 
'Iraq': 'S-1',
'and': 'O' 
'demand': 'O', 
'the': 'O', 
'withdrawal': 'O', 
'of': 'O', 
'British': 'S-2', 
'troops': 'O', 
'from': 'O',
'that': 'O', 
'country': 'O',
'.': 'O',
'Many': 'O', 
'people': 'S-3', 
'have': 'B-3', 
'been': 'B-3', 
'killed': 'E-3', 
'that': 'O', 
'day': 'O',
'.': 'O'
}
</code></pre>
<p>Here, 'O' stands for 'OutOfEntity', 'S' stands for 'Start', 'B' stands for 'Between', and 'E' stands for 'End' and are unique for every given text.</p>
<hr />
<p>I tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>entities = {}
offsets = data[1]['entities']
for entity in offsets:
    entities[data[0][entity[0]:entity[1]]] = re.findall('[0-9]+', entity[2])[0]

tags = {}
for key, value in entities.items():
    entity = key.split()
    if len(entity) &gt; 1:
        bEntity = entity[1:-1]
        tags[entity[0]] = 'S-'+value
        tags[entity[-1]] = 'E-'+value
        for item in bEntity:
            tags[item] = 'B-'+value
    else:
        tags[entity[0]] = 'S-'+value
</code></pre>
<p>The output will be</p>
<pre class=""lang-py prettyprint-override""><code>{'London': 'S-1',
 'Iraq': 'S-1',
 'British': 'S-2',
 'people': 'S-3',
 'killed': 'E-3',
 'have': 'B-3',
 'been': 'B-3'}
</code></pre>
<p>From this point, I am stuck on how to deal with 'O' entities. Also, I want to build more efficient and readable code. I think dictionary data structure is not going to work more efficiently because I can have the same words which they'll be as keys.</p>
","python, dictionary, nlp, named-entity-recognition","<pre class=""lang-py prettyprint-override""><code>def ner(data):
    entities = {}
    offsets = data[1]['entities']
    for entity in offsets:
        entities[data[0][int(entity[0]):int(entity[1])]] = re.findall('[0-9]+', entity[2])[0]
    
    tags = []
    for key, value in entities.items():
        entity = key.split()
        if len(entity) &gt; 1:
            bEntity = entity[1:-1]
            tags.append((entity[0], 'S-'+value))
            for item in bEntity:
                tags.append((item, 'B-'+value))
            tags.append((entity[-1], 'E-'+value))
        else:
            tags.append((entity[0], 'S-'+value))
    
    tokens = nltk.word_tokenize(data[0])
    OTokens = [(token, 'O') for token in tokens if token not in [token[0] for token in tags]]
    for token in OTokens:
        tags.append(token)
    
    return tags
</code></pre>
",0,0,469,2021-06-06 16:49:37,https://stackoverflow.com/questions/67861522/create-a-ner-dictionary-from-a-given-text
AttributeError: &#39;list&#39; object has no attribute &#39;ents&#39;,"<p><strong>I am using this code and get the csv file as a list into doc [].</strong></p>
<pre><code>doc = []
with open(r'C:\Users\DELL\Desktop\Final project\Requirements1.csv') as csv_file:
csv_reader = csv.reader(csv_file, delimiter=';')
for riga in csv_reader:
    for campo in riga:
        print(campo)
        doc.append(nlp(campo))
</code></pre>
<p><strong>But, When I do the named entity recognition for this using this code below,</strong></p>
<pre><code>for entity in doc.ents:
print(entity.text, entity.label)
</code></pre>
<p><strong>I am getting this error.</strong></p>
<pre><code>AttributeError: 'list' object has no attribute 'ents'
</code></pre>
<p><strong>What should I do about this? Please help me.</strong><a href=""https://i.sstatic.net/ycJJT.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","python, list, nlp, attributeerror, named-entity-recognition","<p>Do this.</p>
<pre><code>docs = [] # NOTE THIS CHANGED
with open(r'C:\Users\DELL\Desktop\Final project\Requirements1.csv') as csv_file:
csv_reader = csv.reader(csv_file, delimiter=';')
for riga in csv_reader:
    for campo in riga:
        print(campo)
        docs.append(nlp(campo))

# now to get the ner results...

for doc in docs:
    for ent in doc.ents:
        print(ent.text, ent.label)
</code></pre>
",1,0,3464,2021-06-06 18:38:49,https://stackoverflow.com/questions/67862504/attributeerror-list-object-has-no-attribute-ents
ValueError with NERDA model import,"<p>I'm trying to import the NERDA library in order use it to engage in a Named-Entity Recognition task in Python. I initially tried importing the library in a jupyter notebook and got the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\NERDA\models.py&quot;, line 13, in &lt;module&gt;
    from .networks import NERDANetwork
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\NERDA\networks.py&quot;, line 4, in &lt;module&gt;
    from transformers import AutoConfig
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\__init__.py&quot;, line 43, in &lt;module&gt;
    from . import dependency_versions_check
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\dependency_versions_check.py&quot;, line 36, in &lt;module&gt;
    from .file_utils import is_tokenizers_available
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\file_utils.py&quot;, line 51, in &lt;module&gt;
    from huggingface_hub import HfApi, HfFolder, Repository
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\__init__.py&quot;, line 31, in &lt;module&gt;
    from .file_download import cached_download, hf_hub_url
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\file_download.py&quot;, line 37, in &lt;module&gt;
    if tuple(int(i) for i in _PY_VERSION.split(&quot;.&quot;)) &lt; (3, 8, 0):
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\file_download.py&quot;, line 37, in &lt;genexpr&gt;
    if tuple(int(i) for i in _PY_VERSION.split(&quot;.&quot;)) &lt; (3, 8, 0):
ValueError: invalid literal for int() with base 10: '6rc1'
</code></pre>
<p>I then tried globally installing using pip in gitbash and got the same error. The library appeared to install without error but when I try the following import, I get that same ValueError:</p>
<pre><code>from NERDA.models import NERDA
</code></pre>
<p>I've also tried some of the pre-cooked model imports and gotten the same ValueError.</p>
<pre><code>from NERDA.precooked import EN_ELECTRA_EN
from NERDA.precooked import EN_BERT_ML
</code></pre>
<p>I can't find anything on this error online and am hoping someone may be able to lend some insight? Thanks so much!</p>
","python, huggingface-transformers, named-entity-recognition","<p>Take a look at the <a href=""https://github.com/huggingface/huggingface_hub/blob/59ea9998ee2331acf1c50a9fe2f93e5606c5fefb/src/huggingface_hub/file_download.py#L35-L37"" rel=""nofollow noreferrer"">source code of the used huggingface_hub lib</a>. They comparing the version of your python version to do different imports.<br />
But you uses a <a href=""https://devguide.python.org/devcycle/#rc"" rel=""nofollow noreferrer"">release candidate</a> python version (this tells the value <code>'6rc1'</code>, that caused the error). Because they didn't expect/handle this, you get the int-parse-ValueError.</p>
<hr />
<p><strong>Solution 1:</strong><br />
Update your python version to a stable version. No release candidate. So you have an int-only version number.</p>
<p><strong>Solution 2:</strong><br />
Monkeypatch <a href=""https://docs.python.org/3/library/sys.html#sys.version"" rel=""nofollow noreferrer""><code>sys.version</code></a>, before you import the <code>NERDA</code> libs.</p>
<pre><code>sys.version = '3.8.0'
</code></pre>
",1,0,134,2021-06-08 21:13:48,https://stackoverflow.com/questions/67894649/valueerror-with-nerda-model-import
How to read a text and label each word of it in Python,"<pre class=""lang-py prettyprint-override""><code>data = (&quot;Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country. Many people have been killed that day.&quot;,
        {&quot;entities&quot;: [(48, 54, 'Category 1'), (77, 81, 'Category 1'), (111, 118, 'Category 2'), (150, 173, 'Category 3')]})
</code></pre>
<p><code>data[1]['entities'][0] = (48, 54, 'Category 1')</code> stands for <code>(start_offset, end_offset, entity)</code>.</p>
<p>I want to read each word of <code>data[0]</code> in a sequential manner and tag each word according to <code>data[1]</code> entities. I am expecting to have as final output,</p>
<pre class=""lang-py prettyprint-override""><code>{
'Thousands': 'O', 
'of': 'O',
'demonstrators': 'O',
'have': 'O',
'marched': 'O',
'through': 'O',
'London': 'S-1',
'to': 'O', 
'protest': 'O', 
'the': 'O', 
'war': 'O', 
'in': 'O', 
'Iraq': 'S-1',
'and': 'O' 
'demand': 'O', 
'the': 'O', 
'withdrawal': 'O', 
'of': 'O', 
'British': 'S-2', 
'troops': 'O', 
'from': 'O',
'that': 'O', 
'country': 'O',
'.': 'O',
'Many': 'O', 
'people': 'S-3', 
'have': 'B-3', 
'been': 'B-3', 
'killed': 'E-3', 
'that': 'O', 
'day': 'O',
'.': 'O'
}
</code></pre>
<p>Here, 'O' stands for 'OutOfEntity', 'S' stands for 'Start', 'B' stands for 'Between', and 'E' stands for 'End' and are unique for every given text.</p>
<hr />
<p>I tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>def ner(data):
    entities = {}
    offsets = data[1]['entities']
    for entity in offsets:
        entities[data[0][int(entity[0]):int(entity[1])]] = re.findall('[0-9]+', entity[2])[0]
    
    tags = []
    for key, value in entities.items():
        entity = key.split()
        if len(entity) &gt; 1:
            bEntity = entity[1:-1]
            tags.append((entity[0], 'S-'+value))
            for item in bEntity:
                tags.append((item, 'B-'+value))
            tags.append((entity[-1], 'E-'+value))
        else:
            tags.append((entity[0], 'S-'+value))
    
    tokens = nltk.word_tokenize(data[0])
    OTokens = [(token, 'O') for token in tokens if token not in [token[0] for token in tags]]
    for token in OTokens:
        tags.append(token)
    
    return tags
</code></pre>
<p>But the above function does not work properly in case I have some words that are the same as those in <code>data[1]['entities']</code> offsets but not part of the offsets will be ignored instead they should be labeled as 'O'.</p>
","python, text, nltk, named-entity-recognition","<p>Not sure if the final format is json, yet below is an example to process the data into the print format, i.e.</p>
<pre><code># sample output
'''
{
'Thousands': 'O',
'of': 'O',
'demonstrators': 'O',
'have': 'O',
'marched': 'O',
'through': 'O',
'London': 'S-1',
'to': 'O',
'protest': 'O',
'the': 'O',
'war': 'O',
'in': 'O',
'Iraq': 'S-1',
'and': 'O',
'demand': 'O',
'the': 'O',
'withdrawal': 'O',
'of': 'O',
'British': 'S-2',
'troops': 'O',
'from': 'O',
'that': 'O',
'country.': 'O',
'Many': 'O',
'people': 'S-3',
'have': 'B-3',
'been': 'B-3',
'killed': 'E-3',
'that': 'O',
'day.': 'O'
}
'''
# sample code
data = (&quot;Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country. Many people have been killed that day.&quot;,
        {&quot;entities&quot;: [(48, 54, 'Category 1'), (77, 81, 'Category 1'), (111, 118, 'Category 2'), (150, 173, 'Category 3')]})

print(&quot;{&quot;)
pre = 0
for i in (data[1].values())[0]:
        a = data[0][i[0]:i[1]].split()
        t = pre + i[1]
        #print(pre, i[0])
        b = data[0][pre:i[0]].split()
        for j in b:
                print(&quot;'%s': '%s',&quot; % (j, &quot;O&quot;))
        pre = i[1]
        for j in range(len(a)): 
                if j == 0:
                        print(&quot;'%s': '%s-%s',&quot; % (a[j], &quot;S&quot;, i[2][-1]))
                elif j == len(a) - 1:
                        print(&quot;'%s': '%s-%s',&quot; % (a[j], &quot;E&quot;, i[2][-1]))
                else:
                        print(&quot;'%s': '%s-%s',&quot; % (a[j], &quot;B&quot;, i[2][-1]))
#print(i[1], las)
las = len(data[0])
c = data[0][i[1]:las].split()
for j in range(len(c)):
        if j == len(c) - 1:
                print(&quot;'%s': '%s'&quot; % (c[j], &quot;O&quot;))
        else:
                print(&quot;'%s': '%s',&quot; % (c[j], &quot;O&quot;))
print(&quot;}&quot;)
</code></pre>
",1,0,288,2021-06-09 00:51:02,https://stackoverflow.com/questions/67896141/how-to-read-a-text-and-label-each-word-of-it-in-python
Spacy error in loading pretrained custom model with entity rulers and ner pipeline,"<p>I used a spacy blank model with Gensim custom word vectors. Then I trained the model to get the pipeline in the respective order-</p>
<p>entityruler1, ner1, entity ruler2, ner2</p>
<p>After training it, I saved it in a folder through</p>
<p><code>nlp.to_disk('path to folder')</code></p>
<p>However, if I try to load the same model using
<code>nlp1 = spacy.load('path to folder')</code>
It gives me this error-</p>
<p><code>ValueError: [E109] Model for component 'ner' not initialized. Did you forget to load a model, or forget to call begin_training()?</code></p>
<p>I cannot find any solution online. What might be the reason I am getting this? How do I successfully load and use my pretrained model?</p>
","python, machine-learning, nlp, spacy, named-entity-recognition","<p>Upgrading to spacy version 2.3.7 resolved this error. :)</p>
",0,1,103,2021-06-10 05:14:54,https://stackoverflow.com/questions/67915131/spacy-error-in-loading-pretrained-custom-model-with-entity-rulers-and-ner-pipeli
SpaCy custom NER training AttributeError: &#39;DocBin&#39; object has no attribute &#39;to_disk&#39;,"<p>I want to train a custom NER model using spaCy v3 I prepared my train data and I used this script</p>
<pre><code>import spacy
from spacy.tokens import DocBin

nlp = spacy.blank(&quot;en&quot;) # load a new spacy model
db = DocBin() # create a DocBin object

for text, annot in tqdm(TRAIN_DATA): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label)
        if span is None:
            pass
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(&quot;./train.spacy&quot;) # save the docbin object
</code></pre>
<p>then it prints this error:</p>
<pre><code>AttributeError: 'DocBin' object has no attribute 'to_disk'
</code></pre>
","python, named-entity-recognition, spacy-3","<p>Make sure you are really using spaCy 3, in case you haven't :)</p>
<p>You can check this from the console by running <code>python -c &quot;import spacy; print(spacy.__version__)&quot;</code></p>
<p>By issuing via command line <code>pip install spacy==3.0.6</code> in a python env, and then running in the python console</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.tokens import DocBin

nlp = spacy.blank(&quot;en&quot;) # load a new spacy model
db = DocBin() # create a DocBin object

# omitting code for debugging purposes

db.to_disk(&quot;./train.spacy&quot;) # save the docbin object
</code></pre>
<p>you should get no errors.</p>
",2,1,2875,2021-06-13 09:36:33,https://stackoverflow.com/questions/67956814/spacy-custom-ner-training-attributeerror-docbin-object-has-no-attribute-to-d
how can I pass table or dataframe instead of text with entity recognition using spacy,"<p>The following <a href=""https://stackoverflow.com/a/57546292/12496869"">link</a> shows how to add multiple EntityRuler with spaCy. The code to do that is below:</p>
<pre><code>import spacy
import pandas as pd

from spacy.pipeline import EntityRuler
nlp = spacy.load('en_core_web_sm', disable = ['ner'])
ruler = nlp.add_pipe(&quot;entity_ruler&quot;)


flowers = [&quot;rose&quot;, &quot;tulip&quot;, &quot;african daisy&quot;]
for f in flowers:
    ruler.add_patterns([{&quot;label&quot;: &quot;flower&quot;, &quot;pattern&quot;: f}])
animals = [&quot;cat&quot;, &quot;dog&quot;, &quot;artic fox&quot;]
for a in animals:
    ruler.add_patterns([{&quot;label&quot;: &quot;animal&quot;, &quot;pattern&quot;: a}])



result={}
doc = nlp(&quot;cat and artic fox, plant african daisy&quot;)
for ent in doc.ents:
        result[ent.label_]=ent.text
df = pd.DataFrame([result])
print(df)
</code></pre>
<p><strong>The output:</strong></p>
<pre><code>      animal         flower
0  artic fox  african daisy
</code></pre>
<p><strong>The problem is</strong>: How can i pass <em>dataframe or table</em> instead of the text:<strong>&quot;cat and artic fox, plant african daisy&quot;</strong></p>
","python, pandas, entity, spacy, named-entity-recognition","<p>Imagine that your dataframe is</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({'Text':[&quot;cat and artic fox, plant african daisy&quot;]})
</code></pre>
<p>You may define a custom method to extract the entities and then use it with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html"" rel=""nofollow noreferrer""><code>Series.apply</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>def get_entities(x):
    result = {}
    doc = nlp(x)
    for ent in doc.ents:
        result[ent.label_]=ent.text
    return result
</code></pre>
<p>and then</p>
<pre class=""lang-py prettyprint-override""><code>df['Matches'] = df['Text'].apply(get_entities)
&gt;&gt;&gt; df['Matches']
0    {'animal': 'artic fox', 'flower': 'african daisy'}
Name: Matches, dtype: object
</code></pre>
",3,1,509,2021-06-15 08:56:47,https://stackoverflow.com/questions/67983109/how-can-i-pass-table-or-dataframe-instead-of-text-with-entity-recognition-using
How can I getting NER (Named Entity Recognition) for one word,"<p><strong>This is the python code I have written to NER(named entity recognition) a use-case scenario that is input by the user as a given text using Jupiter notebook.</strong></p>
<p>First of all, I have written a code to input the scenario as text.</p>
<pre><code>text = &quot;customer must be registered if wants to buy the product.unregistered user can’t go to 
the shopping cart. Customer logins to the system by entering valid user id and password for 
the shopping.  customer  can make order or cancel order of the product from the shopping cart  
after login or registration. Customer has to logout after ordering or surfing for the product &quot;
</code></pre>
<p>As the next step, I have to get it into a string.</p>
<pre><code>text_combined = str(text)
</code></pre>
<p>Second I put it into a doc.</p>
<pre><code>doc = nlp(text_combined)
</code></pre>
<p>Then I have written the NER code. I have put a screenshot of the output.</p>
<pre><code>for ent in doc.ents:
print(ent.text,ent.label_)
</code></pre>
<p>Finally, I am expected the entities like the customer is a person. But the code is identified that as an organization. (screenshot is attached) Can you explain to me why is that? And is there anyone to solve this problem?</p>
<pre><code>spacy.displacy.render(doc, style='ent',jupyter=True)
</code></pre>
<p><a href=""https://i.sstatic.net/iiUja.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iiUja.png"" alt=""enter image description here"" /></a></p>
","python, spacy, named-entity-recognition, junit-jupiter","<p>The spaCy models are trained on newspaper-like texts. Some of the labels they have are things like PER (Person) and ORG (Organization). But it learns what these are based on newspaper articles. So if you have a news article like this...</p>
<blockquote>
<p>John Smith of Eggplant Limited reported a new product today...</p>
</blockquote>
<p>Then it would be labelled like this:</p>
<blockquote>
<p>[John Smith PER] of [Eggplant Limited ORG] reported a new product today...</p>
</blockquote>
<p>So the named entities are <em>proper nouns</em>.</p>
<p>In your example &quot;Customer&quot; is not a proper noun, so there's no reason it would be tagged as PER. It's a little weird that it's tagged as ORG, and I'd consider that an error. As to why it has an error there, it's hard to say exactly, but models aren't perfect and they do have errors, so you have to be able to deal with issues like that in your application.</p>
",1,1,1027,2021-06-18 13:18:14,https://stackoverflow.com/questions/68035891/how-can-i-getting-ner-named-entity-recognition-for-one-word
How to use spacy train to add entities to an existing custom NER model? (Spacy v3.0),"<p>I am currently implementing a custom NER model interface where a user can interact with a frontend application to add custom entities to train a spacy model.</p>
<p>I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.</p>
<p>For example, let's say I have a model that is already trained for a custom entity of FOOD. (Pizza, Pasta, Bread, etc…). Now I want to take this existing model, and train it for a new entity called DRINKS with keywords like Coca-Cola, Pepsi, Juice, etc… Using spacy train command for spacy v3.0.</p>
<p>The spacy train command that I am using currently is as follows:</p>
<pre><code>&gt; python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
</code></pre>
<p>I load the model for prediction using:</p>
<pre><code>&gt; nlp1 = spacy.load(R&quot;.\output\model-best&quot;)
</code></pre>
<p>As of now, I was training the model for new entities manually. Below is the code to find keywords in my training data and output a JSON format for training data (old format).</p>
<pre><code>import re

keyword = [&quot;outages&quot;,&quot;updates&quot;,&quot;negative star&quot;,&quot;worst&quot;]
entity = [&quot;PROBLEM&quot;,&quot;PROBLEM&quot;,&quot;COMPLAINT&quot;,&quot;COMPLAINT&quot;]

train = []

for text in df.text:

    for n in range(0,len(keyword)):
    
        start_index = []
        end_index = []

        start_index = [m.start() for m in re.finditer(keyword[n], str(text))]

        if(start_index):

            end_index = [m+len(keyword[n]) for m in start_index]

            for i in range(0,len(start_index)):

                train.append((text,{&quot;entities&quot;: [(start_index[i],end_index[i],entity[n])]}))

train
</code></pre>
<p>After this, I converted my json format into .spacy format with below code.</p>
<pre><code>from tqdm import tqdm
from spacy.tokens import DocBin

db = DocBin() # create a DocBin object

for text, annot in tqdm(train): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        if span is None:
            print(&quot;Skipping entity&quot;)
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(&quot;./train.spacy&quot;)
</code></pre>
","python, machine-learning, nlp, spacy, named-entity-recognition","<blockquote>
<p>I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.</p>
</blockquote>
<p>What you are describing is called &quot;online learning&quot; and the default spaCy models don't support it. Most modern neural NER methods, even outside of spaCy, have no support for it at all.</p>
<p>You cannot fix this by using a custom training loop.</p>
<p>Your options are to use rule-based matching, so you can only match things explicitly in a list, or to retrain models on the fly.</p>
<p>Rule-based matching should be easy to set up but has the obvious issue that it can't learn things not explicitly in the list.</p>
<p>Training things on the fly may sound like it'll take too long, but you can train a small model quite quickly. What you can do is train a small model for a small number of iterations while the user is working interactively, and after they've confirmed the model is more or less working correctly you can use the same training data for a larger model with longer training.</p>
",3,3,1699,2021-06-22 12:21:42,https://stackoverflow.com/questions/68083466/how-to-use-spacy-train-to-add-entities-to-an-existing-custom-ner-model-spacy-v
Pytorch with CUDA throws RuntimeError when using pack_padded_sequence,"<p>I am trying to train a BiLSTM-CRF on detecting new NER entities with Pytorch.
To do so, I am using a snippet of code derivated from the <a href=""https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html"" rel=""nofollow noreferrer"">Pytorch Advanced tutorial</a>. <a href=""https://github.com/jtlin-sync/batch_bilstm_crf"" rel=""nofollow noreferrer"">This snippet</a> implements batch training.</p>
<p>I followed the READ-ME in order to present data as required. Everything works great on CPU, but when I'm trying to get it to GPU, the following error occur :</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-23-794982510db6&gt; in &lt;module&gt;
      4         batch_input, batch_input_lens, batch_mask, batch_target = batch_info
      5 
----&gt; 6         loss_train = model.neg_log_likelihood(batch_input, batch_input_lens, batch_mask, batch_target)
      7         optimizer.zero_grad()
      8         loss_train.backward()

&lt;ipython-input-11-e44ffbf7d75f&gt; in neg_log_likelihood(self, batch_input, batch_input_lens, batch_mask, batch_target)
    185 
    186     def neg_log_likelihood(self, batch_input, batch_input_lens, batch_mask, batch_target):
--&gt; 187         feats = self.bilstm(batch_input, batch_input_lens, batch_mask)
    188         gold_score = self.CRF.score_sentence(feats, batch_target)
    189         forward_score = self.CRF.score_z(feats, batch_input_lens)

/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

&lt;ipython-input-11-e44ffbf7d75f&gt; in forward(self, batch_input, batch_input_lens, batch_mask)
     46         batch_input = self.word_embeds(batch_input)  # size: #batch * padding_length * embedding_dim
     47         batch_input = rnn_utils.pack_padded_sequence(
---&gt; 48             batch_input, batch_input_lens, batch_first=True)
     49         batch_output, self.hidden = self.lstm(batch_input, self.hidden)
     50         self.repackage_hidden(self.hidden)

/opt/conda/lib/python3.7/site-packages/torch/nn/utils/rnn.py in pack_padded_sequence(input, lengths, batch_first, enforce_sorted)
    247 
    248     data, batch_sizes = \
--&gt; 249         _VF._pack_padded_sequence(input, lengths, batch_first)
    250     return _packed_sequence_init(data, batch_sizes, sorted_indices, None)
    251 

RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor`

</code></pre>
<p>If I understand well, pack_padded_sequence need the tensor to be on CPU rather than GPU. Unfortunately the pack_padded_sequence is called by my forward function and I can't see any way to do so without going back to CPU for the whole training.</p>
<p>Here is the complete code.</p>
<p>Classes definitions :</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils


class BiLSTM(nn.Module):
    def __init__(self, vocab_size, tagset, embedding_dim, hidden_dim,
                 num_layers, bidirectional, dropout, pretrained=None):
        super(BiLSTM, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.tagset_size = len(tagset)
        self.bidirectional = bidirectional
        self.num_layers = num_layers
        self.word_embeds = nn.Embedding(vocab_size+2, embedding_dim)
        if pretrained is not None:
            self.word_embeds = nn.Embedding.from_pretrained(pretrained)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim // 2 if bidirectional else hidden_dim,
            num_layers=num_layers,
            dropout=dropout,
            bidirectional=bidirectional,
            batch_first=True,
        )
        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)
        self.hidden = None

    def init_hidden(self, batch_size, device):
        init_hidden_dim = self.hidden_dim // 2 if self.bidirectional else self.hidden_dim
        init_first_dim = self.num_layers * 2 if self.bidirectional else self.num_layers
        self.hidden = (
            torch.randn(init_first_dim, batch_size, init_hidden_dim).to(device),
            torch.randn(init_first_dim, batch_size, init_hidden_dim).to(device)
        )

    def repackage_hidden(self, hidden):
        &quot;&quot;&quot;Wraps hidden states in new Tensors, to detach them from their history.&quot;&quot;&quot;
        if isinstance(hidden, torch.Tensor):
            return hidden.detach_().to(device)
        else:
            return tuple(self.repackage_hidden(h) for h in hidden)

    def forward(self, batch_input, batch_input_lens, batch_mask):
        batch_size, padding_length = batch_input.size()
        batch_input = self.word_embeds(batch_input)  # size: #batch * padding_length * embedding_dim
        batch_input = rnn_utils.pack_padded_sequence(
            batch_input, batch_input_lens, batch_first=True)
        batch_output, self.hidden = self.lstm(batch_input, self.hidden)
        self.repackage_hidden(self.hidden)
        batch_output, _ = rnn_utils.pad_packed_sequence(batch_output, batch_first=True)
        batch_output = batch_output.contiguous().view(batch_size * padding_length, -1)
        batch_output = batch_output[batch_mask, ...]
        out = self.hidden2tag(batch_output)
        return out

    def neg_log_likelihood(self, batch_input, batch_input_lens, batch_mask, batch_target):
        loss = nn.CrossEntropyLoss(reduction='mean')
        feats = self(batch_input, batch_input_lens, batch_mask)
        batch_target = torch.cat(batch_target, 0).to(device)
        return loss(feats, batch_target)

    def predict(self, batch_input, batch_input_lens, batch_mask):
        feats = self(batch_input, batch_input_lens, batch_mask)
        val, pred = torch.max(feats, 1)
        return pred


class CRF(nn.Module):
    def __init__(self, tagset, start_tag, end_tag, device):
        super(CRF, self).__init__()
        self.tagset_size = len(tagset)
        self.START_TAG_IDX = tagset.index(start_tag)
        self.END_TAG_IDX = tagset.index(end_tag)
        self.START_TAG_TENSOR = torch.LongTensor([self.START_TAG_IDX]).to(device)
        self.END_TAG_TENSOR = torch.LongTensor([self.END_TAG_IDX]).to(device)
        # trans: (tagset_size, tagset_size) trans (i, j) means state_i -&gt; state_j
        self.trans = nn.Parameter(
            torch.randn(self.tagset_size, self.tagset_size)
        )
        # self.trans.data[...] = 1
        self.trans.data[:, self.START_TAG_IDX] = -10000
        self.trans.data[self.END_TAG_IDX, :] = -10000
        self.device = device

    def init_alpha(self, batch_size, tagset_size):
        return torch.full((batch_size, tagset_size, 1), -10000, dtype=torch.float, device=self.device)

    def init_path(self, size_shape):
        # Initialization Path - LongTensor + Device + Full_value=0
        return torch.full(size_shape, 0, dtype=torch.long, device=self.device)

    def _iter_legal_batch(self, batch_input_lens, reverse=False):
        index = torch.arange(0, batch_input_lens.sum(), dtype=torch.long)
        packed_index = rnn_utils.pack_sequence(
            torch.split(index, batch_input_lens.tolist())
        )
        batch_iter = torch.split(packed_index.data, packed_index.batch_sizes.tolist())
        batch_iter = reversed(batch_iter) if reverse else batch_iter
        for idx in batch_iter:
            yield idx, idx.size()[0]

    def score_z(self, feats, batch_input_lens):
        # 模拟packed pad过程
        tagset_size = feats.shape[1]
        batch_size = len(batch_input_lens)
        alpha = self.init_alpha(batch_size, tagset_size)
        alpha[:, self.START_TAG_IDX, :] = 0  # Initialization
        for legal_idx, legal_batch_size in self._iter_legal_batch(batch_input_lens):
            feat = feats[legal_idx, ].view(legal_batch_size, 1, tagset_size)  # 
            # #batch * 1 * |tag| + #batch * |tag| * 1 + |tag| * |tag| = #batch * |tag| * |tag|
            legal_batch_score = feat + alpha[:legal_batch_size, ] + self.trans
            alpha_new = torch.logsumexp(legal_batch_score, 1).unsqueeze(2).to(device)
            alpha[:legal_batch_size, ] = alpha_new
        alpha = alpha + self.trans[:, self.END_TAG_IDX].unsqueeze(1)
        score = torch.logsumexp(alpha, 1).sum().to(device)
        return score

    def score_sentence(self, feats, batch_target):
        # CRF Batched Sentence Score
        # feats: (#batch_state(#words), tagset_size)
        # batch_target: list&lt;torch.LongTensor&gt; At least One LongTensor
        # Warning: words order =  batch_target order
        def _add_start_tag(target):
            return torch.cat([self.START_TAG_TENSOR, target]).to(device)

        def _add_end_tag(target):
            return torch.cat([target, self.END_TAG_TENSOR]).to(device)

        from_state = [_add_start_tag(target) for target in batch_target]
        to_state = [_add_end_tag(target) for target in batch_target]
        from_state = torch.cat(from_state).to(device)  
        to_state = torch.cat(to_state).to(device)  
        trans_score = self.trans[from_state, to_state]

        gather_target = torch.cat(batch_target).view(-1, 1).to(device)
        emit_score = torch.gather(feats, 1, gather_target).to(device)  

        return trans_score.sum() + emit_score.sum()

    def viterbi(self, feats, batch_input_lens):
        word_size, tagset_size = feats.shape
        batch_size = len(batch_input_lens)
        viterbi_path = self.init_path(feats.shape)  # use feats.shape to init path.shape
        alpha = self.init_alpha(batch_size, tagset_size)
        alpha[:, self.START_TAG_IDX, :] = 0  # Initialization
        for legal_idx, legal_batch_size in self._iter_legal_batch(batch_input_lens):
            feat = feats[legal_idx, :].view(legal_batch_size, 1, tagset_size)
            legal_batch_score = feat + alpha[:legal_batch_size, ] + self.trans
            alpha_new, best_tag = torch.max(legal_batch_score, 1).to(device)
            alpha[:legal_batch_size, ] = alpha_new.unsqueeze(2)
            viterbi_path[legal_idx, ] = best_tag
        alpha = alpha + self.trans[:, self.END_TAG_IDX].unsqueeze(1)
        path_score, best_tag = torch.max(alpha, 1).to(device)
        path_score = path_score.squeeze()  # path_score=#batch

        best_paths = self.init_path((word_size, 1))
        for legal_idx, legal_batch_size in self._iter_legal_batch(batch_input_lens, reverse=True):
            best_paths[legal_idx, ] = best_tag[:legal_batch_size, ]  # 
            backword_path = viterbi_path[legal_idx, ]  # 1 * |Tag|
            this_tag = best_tag[:legal_batch_size, ]  # 1 * |legal_batch_size|
            backword_tag = torch.gather(backword_path, 1, this_tag).to(device)
            best_tag[:legal_batch_size, ] = backword_tag
            # never computing &lt;START&gt;

        # best_paths = #words
        return path_score.view(-1), best_paths.view(-1)


class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset, embedding_dim, hidden_dim,
                 num_layers, bidirectional, dropout, start_tag, end_tag, device, pretrained=None):
        super(BiLSTM_CRF, self).__init__()
        self.bilstm = BiLSTM(vocab_size, tagset, embedding_dim, hidden_dim,
                             num_layers, bidirectional, dropout, pretrained)
        self.CRF = CRF(tagset, start_tag, end_tag, device)

    def init_hidden(self, batch_size, device):
        self.bilstm.hidden = self.bilstm.init_hidden(batch_size, device)

    def forward(self, batch_input, batch_input_lens, batch_mask):
        feats = self.bilstm(batch_input, batch_input_lens, batch_mask)
        score, path = self.CRF.viterbi(feats, batch_input_lens)
        return path

    def neg_log_likelihood(self, batch_input, batch_input_lens, batch_mask, batch_target):
        feats = self.bilstm(batch_input, batch_input_lens, batch_mask)
        gold_score = self.CRF.score_sentence(feats, batch_target)
        forward_score = self.CRF.score_z(feats, batch_input_lens)
        return forward_score - gold_score

    def predict(self, batch_input, batch_input_lens, batch_mask):
        return self(batch_input, batch_input_lens, batch_mask)
</code></pre>
<p>Training cell :</p>
<pre><code>def prepare_sequence(seq, to_ix, device):
    idxs = [to_ix[w] for w in seq]
    return torch.tensor(idxs, dtype=torch.long).to(device)

def prepare_labels(lab, tag_to_ix, device):
    idxs = [tag_to_ix[w] for w in lab]
    return torch.tensor(idxs, dtype=torch.long).to(device)


class PadSequence:
    def __call__(self, batch):
        device = torch.device('cuda')
        # Let's assume that each element in &quot;batch&quot; is a tuple (data, label).
        # Sort the batch in the descending order
        sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)
        # Get each sequence and pad it
        sequences = [x[0] for x in sorted_batch]
        sentence_in =[prepare_sequence(x, word_to_ix, device) for x in sequences]
        sequences_padded = torch.nn.utils.rnn.pad_sequence(sentence_in, padding_value = len(word_to_ix) +1, batch_first=True).to(device)
        
        lengths = torch.LongTensor([len(x) for x in sequences]).to(device)
        
        masks = [True if index_word!=len(word_to_ix)+1 else False for sentence in sequences_padded for index_word in sentence ]
        
        labels = [x[1] for x in sorted_batch]
        labels_in = [prepare_sequence(x, tag_to_ix, device) for x in labels]
        return sequences_padded, lengths, masks, labels_in


{ .... code to get the data formatted...}


device = torch.device(&quot;cuda&quot;)
batch_size = 64


START_TAG = &quot;&lt;START&gt;&quot;
STOP_TAG = &quot;&lt;STOP&gt;&quot;
EMBEDDING_DIM = 200
HIDDEN_DIM = 20
NUM_LAYER = 3
BIDIRECTIONNAL = True
DROPOUT = 0.1

train_iter = DataLoader(dataset=training_data, collate_fn=PadSequence(), batch_size=64, shuffle=True) 




model = BiLSTM_CRF(len(word_to_ix), tagset, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYER, BIDIRECTIONNAL, DROPOUT, START_TAG, STOP_TAG, device ).to(device)
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)
model.init_hidden(batch_size, device)
with tqdm(total=len(train_iter)) as progress_bar:
    for batch_info in train_iter:
        batch_input, batch_input_lens, batch_mask, batch_target = batch_info

        loss_train = model.neg_log_likelihood(batch_input, batch_input_lens, batch_mask, batch_target)
        optimizer.zero_grad()
        loss_train.backward()
        optimizer.step()
        progress_bar.update(1) # update progress
</code></pre>
","python, pytorch, named-entity-recognition, custom-training","<p>Within <code>PadSequence</code> function (which acts as a <code>collate_fn</code> which gathers samples and makes a batch from them) you are explicitly casting to <code>cuda</code> device, namely:</p>
<pre><code>class PadSequence:
    def __call__(self, batch):
        device = torch.device('cuda')
        
        # Left rest of the code for brevity
        ...
        lengths = torch.LongTensor([len(x) for x in sequences]).to(device)
        ...
        return sequences_padded, lengths, masks, labels_in
</code></pre>
<p><strong>You don't need to cast your data when creating batch</strong>, we usually do that right before pushing the examples through neural network.</p>
<p>Also you should at least define the device like this:</p>
<pre><code>device = torch.device('cuda' if torch.cuda.is_available() else &quot;cpu&quot;)
</code></pre>
<p>or even better leave the choice of device for you/user in some part of the code where you setup everything.</p>
",3,2,1800,2021-06-22 15:31:41,https://stackoverflow.com/questions/68086528/pytorch-with-cuda-throws-runtimeerror-when-using-pack-padded-sequence
using tfa.layers.crf on top of biLSTM,"<p>I am trying to implement NER model based on CRF with tensorflow-addons library. The model gets sequence of words in word to index and char level format and the concatenates them and feeds them to the BiLSTM layer. Here is the code of implementation:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D
from tensorflow.keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D
from tensorflow_addons.layers import CRF

word_input = Input(shape=(max_sent_len,))
word_emb = Embedding(input_dim=n_words + 2, output_dim=dim_word_emb,
                     input_length=max_sent_len, mask_zero=True)(word_input)

char_input = Input(shape=(max_sent_len, max_word_len,))
char_emb = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=dim_char_emb,
                           input_length=max_word_len, mask_zero=True))(char_input)

char_emb = TimeDistributed(LSTM(units=20, return_sequences=False,
                                recurrent_dropout=0.5))(char_emb)

# main LSTM
main_input = concatenate([word_emb, char_emb])
main_input = SpatialDropout1D(0.3)(main_input)
main_lstm = Bidirectional(LSTM(units=50, return_sequences=True,
                               recurrent_dropout=0.6))(main_input)
kernel = TimeDistributed(Dense(50, activation=&quot;relu&quot;))(main_lstm)  
crf = CRF(n_tags+1)  # CRF layer
decoded_sequence, potentials, sequence_length, chain_kernel = crf(kernel)  # output

model = Model([word_input, char_input], potentials)
model.add_loss(tf.abs(tf.reduce_mean(kernel)))
model.compile(optimizer=&quot;rmsprop&quot;, loss='categorical_crossentropy')
</code></pre>
<p>When I start to fit the model, I get this Warnings:</p>
<pre><code>WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss.
</code></pre>
<p>And training process goes like this:</p>
<pre><code>438/438 [==============================] - 80s 163ms/step - loss: nan - val_loss: nan
Epoch 2/10
438/438 [==============================] - 71s 163ms/step - loss: nan - val_loss: nan
Epoch 3/10
438/438 [==============================] - 71s 162ms/step - loss: nan - val_loss: nan
Epoch 4/10
438/438 [==============================] - 71s 161ms/step - loss: nan - val_loss: nan
Epoch 5/10
438/438 [==============================] - 71s 162ms/step - loss: nan - val_loss: nan
Epoch 6/10
438/438 [==============================] - 70s 160ms/step - loss: nan - val_loss: nan
Epoch 7/10
438/438 [==============================] - 70s 161ms/step - loss: nan - val_loss: nan
Epoch 8/10
438/438 [==============================] - 70s 160ms/step - loss: nan - val_loss: nan
Epoch 9/10
438/438 [==============================] - 71s 161ms/step - loss: nan - val_loss: nan
Epoch 10/10
438/438 [==============================] - 70s 159ms/step - loss: nan - val_loss: nan
</code></pre>
<p>I am almost sure that the problem is with the way I am setting the loss functions, but I do not know how exactly I should set them. I also searched for my problem but I did not get any answer.
Also When I test my model, it can not predict labels correct and gives them same labels. Can anybody describe for me that how should I solve this problem?</p>
","python-3.x, tensorflow, tf.keras, named-entity-recognition, crf","<p>Ghange your loss function to tensorflow_addons.losses.SigmoidFocalCrossEntropy().I guess categorical crossentropy is not a good choice.</p>
",0,3,3456,2021-06-23 13:11:22,https://stackoverflow.com/questions/68100518/using-tfa-layers-crf-on-top-of-bilstm
How to evaluate trained spaCy version 3 model?,"<p>I want to evaluate my trained spaCy model with the build-in Scorer function with this code:</p>
<pre><code>def evaluate(ner_model, examples):
    scorer = Scorer()
    for input_, annot in examples:
        text = nlp.make_doc(input_)
        gold = Example.from_dict(text, annot)
        pred_value = ner_model(input_)
        scorer.score(gold)
    return scorer.scores

examples = [('Brief discussion about instument replcement and Product ...confirmation', {'entities': [(48, 55, 'PRODUCT')]})('Met with special chem lead. Did not yet move assays from immulite to produc. Follow up with PhD tomorrow.', {'entities': [(57, 68, 'PRODUCT'), (45, 51, 'DATE'), (97, 105, 'DATE')]}), ('Discuss new products for ...', {'entities': [(36, 51, 'PRODUCT')]})]

ner_model = spacy.load(r'D:\temp\model') # for spaCy's pretrained use 'en_core_web_sm'
results = evaluate(ner_model, examples)
</code></pre>
<p>When I run the function I'm receiving the following error message:</p>
<blockquote>
<p>TypeError: [E978] The Tokenizer.score method takes a list of Example objects, but got: &lt;class 'spacy.training.example.Example'&gt;</p>
</blockquote>
<p>I already tried feeding in the annotations like {&quot;entities&quot;: annot} and some other versions of it. I checked google but every article seems to be related to version 2.xx of spaCy.</p>
<p>What am I doing wrong? How can I calculate recall, accuracy, and the F1 score with spacy Score()?</p>
","python, named-entity-recognition, spacy-3","<p>The scores method is still supported in spaCy 3.0 (<a href=""https://spacy.io/api/scorer"" rel=""nofollow noreferrer"">https://spacy.io/api/scorer</a>) and I finally got it working with the following code:</p>
<pre><code>nlp = spacy.load(path_to_model)
examples = []
scorer = Scorer()
for text, annotations in TEST_REVISION_DATA:
    doc = nlp.make_doc(text)
    example = Example.from_dict(doc, annotations)
    example.predicted = nlp(str(example.predicted))
    examples.append(example)
scorer.score(examples)
</code></pre>
<p>I didn't find the command line tool easy to apply (I was struggling with the test data load) and for my needs, the code version is also much more convenient. This way I can covert the results into visuals easily.</p>
",3,4,6545,2021-07-01 15:44:27,https://stackoverflow.com/questions/68213223/how-to-evaluate-trained-spacy-version-3-model
Explanation/interpretation of the parameters in the spaCy config file,"<p>I have a couple of questions regarding the parameters that we define in the <code>config.cfg</code> file. Although spaCy's docs do try to explain them, I feel that the explanation isn't really descriptive enough and that lots of things are scattered around the docs, making it difficult to find exactly what you need, especially with spaCy v3, (unless I'm looking at wrong parts of the website) which is recent and hence has really less question/answers in the forums.
I'm basically building a Named Entity Recognition (NER) model along with a transformer component. My questions are as follows:</p>
<ol>
<li><p>In the following part (same question for <code>corpora.train</code> also), what is the difference between <code>max_length</code> and <code>limit</code>?</p>
<p>For <code>max_length</code> the docs say &quot;Limitations on training document length&quot;<br />
For <code>limit</code>, the docs say &quot;Limitation on number of training examples&quot;</p>
<p>Aren't they both more or less the same thing? I mean I can limit the number of training examples by limiting the document's length itself, right?</p>
</li>
</ol>
<pre><code>[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null
</code></pre>
<ol start=""2"">
<li>In the below snippet, what is the meaning of one 'step'? I understand <code>max_steps=0</code> means infinite steps. But how do I know how many such 'steps' make one epoch? Also how many example sentences are covered in 1 such step?</li>
</ol>
<pre><code>[training]
train_corpus = &quot;corpora.train&quot;
dev_corpus = &quot;corpora.dev&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 10
max_steps = 0
eval_frequency = 200
frozen_components = []
before_to_disk = null
</code></pre>
<ol start=""3"">
<li>How exactly is the <code>learn_rate</code> being modified in the below snippet of code, during the training process? More specifically, what do <code>total_steps</code> and <code>warmup_steps</code> mean?</li>
</ol>
<pre><code>[training.optimizer.learn_rate]
@schedules = &quot;warmup_linear.v1&quot;
warmup_steps = 250
total_steps = 200
initial_rate = 0.00005
</code></pre>
<ol start=""4"">
<li>Finally, in the CLI output of the training process, What exactly is this '#'? It was mentioned in one of <a href=""https://github.com/explosion/spaCy/discussions/7450#discussioncomment-487000"" rel=""nofollow noreferrer"">GitHub discussions</a> that <em>&quot;The # column is the number of optimization steps (= batches processed)&quot;</em> , but what exactly is this 1 batch or 'optimization step'? If the training process shows me the scores for after 200 such 'batches' how do I interpret it (as in how many example sentences have been processed till that point)?</li>
</ol>
","python, nlp, spacy, named-entity-recognition, spacy-3","<blockquote>
<p>In the following part (same question for corpora.train also), what is the difference between max_length and limit?
For max_length the docs say &quot;Limitations on training document length&quot;
For limit, the docs say &quot;Limitation on number of training examples&quot;
Aren't they both more or less the same thing? I mean I can limit the number of training examples by limiting the document's length itself, right?</p>
</blockquote>
<p>These are different things, you seem to be confused about what a &quot;document&quot; is. You can think of a &quot;doc&quot; as being a single object in spaCy. Different docs don't know anything about each other. A doc is based on a single string.  Using normal Python strings as an example:</p>
<pre><code>[&quot;cat&quot;, &quot;dog&quot;, &quot;fish&quot;] # this is three strings
[&quot;cat dog fish&quot;] # this is one string
</code></pre>
<p>You can see that &quot;take three strings from the list&quot; and &quot;take strings not more than three characters long&quot; are very different things. The values in spaCy are like that.</p>
<blockquote>
<p>In the below snippet, what is the meaning of one 'step'? I understand max_steps=0 means infinite steps. But how do I know how many such 'steps' make one epoch? Also how many example sentences are covered in 1 such step?</p>
</blockquote>
<p>A &quot;step&quot; is a &quot;batch&quot;. A &quot;batch&quot; is running training over some number of examples and updating the model weights once. You can control the size of a batch so it can be any number of examples. An &quot;epoch&quot; is how long it takes the training to see every example once, so if you have 5 documents per batch and 30 training documents then 6 steps would be one epoch.</p>
<p>spaCy doesn't necessarily know anything about &quot;sentences&quot; in training, docs are the basic unit of a batch. Your training examples might all be single sentences but that's not a requirement.</p>
<p>These terms are not spaCy-specific, they are widely used in machine learning.</p>
<blockquote>
<p>How exactly is the learn_rate being modified in the below snippet of code, during the training process? More specifically, what do total_steps and warmup_steps mean?</p>
</blockquote>
<p>This is from Thinc, <a href=""https://thinc.ai/docs/api-schedules#warmup_linear"" rel=""nofollow noreferrer"">see the docs there</a>.</p>
<p>To quote:</p>
<blockquote>
<p>Generate a series, starting from an initial rate, and then with a warmup period, and then a linear decline. Used for learning rates.</p>
</blockquote>
<p>At the end of <code>total_steps</code> the learning rate stops changing.</p>
<blockquote>
<p>Finally, in the CLI output of the training process, What exactly is this '#'? It was mentioned in one of GitHub discussions that &quot;The # column is the number of optimization steps (= batches processed)&quot; , but what exactly is this 1 batch or 'optimization step'? If the training process shows me the scores for after 200 such 'batches' how do I interpret it (as in how many example sentences have been processed till that point)?</p>
</blockquote>
<p>A step is the same thing as in #2, it's one batch. Batch size is expressed in docs, not in sentences.</p>
",3,3,1905,2021-07-02 13:20:40,https://stackoverflow.com/questions/68225948/explanation-interpretation-of-the-parameters-in-the-spacy-config-file
Training epochs interpretation during spaCy NER training,"<p>I Was training my NER model with transformers, and am not really sure why the training stopped at some point, or why did it even go with so many batches. This is how my configuration file looks like (relevant part):</p>
<pre><code>[training]
train_corpus = &quot;corpora.train&quot;
dev_corpus = &quot;corpora.dev&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 2
max_steps = 0
eval_frequency = 200
frozen_components = []
before_to_disk = null

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.00005
</code></pre>
<p>And this is the training log:</p>
<pre><code>============================= Training pipeline =============================
ℹ Pipeline: ['transformer', 'ner']
ℹ Initial learn rate: 5e-05
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0         398.75     40.97    2.84    3.36    2.46    0.03
  0     200         906.30   1861.38   94.51   94.00   95.03    0.95
  0     400         230.06   1028.51   98.10   97.32   98.89    0.98
  0     600          90.22   1013.38   98.99   98.40   99.58    0.99
  0     800          80.64   1131.73   99.02   98.25   99.81    0.99
  0    1000          98.50   1260.47   99.50   99.16   99.85    1.00
  0    1200          73.32   1414.91   99.49   99.25   99.73    0.99
  0    1400          84.94   1529.75   99.70   99.56   99.85    1.00
  0    1600          55.61   1697.55   99.75   99.63   99.87    1.00
  0    1800          80.41   1936.64   99.75   99.63   99.87    1.00
  0    2000         115.39   2125.54   99.78   99.69   99.87    1.00
  0    2200          63.06   2395.48   99.80   99.75   99.85    1.00
  0    2400         104.14   2574.36   99.87   99.79   99.96    1.00
  0    2600          86.07   2308.35   99.88   99.79   99.97    1.00
  0    2800          81.05   1853.15   99.90   99.87   99.93    1.00
  0    3000          52.67   1462.61   99.96   99.93   99.99    1.00
  0    3200          57.99   1154.62   99.94   99.91   99.97    1.00
  0    3400         110.74    847.50   99.90   99.85   99.96    1.00
  0    3600          90.49    621.99   99.90   99.91   99.90    1.00
  0    3800          51.03    378.93   99.87   99.78   99.97    1.00
  0    4000          93.40    274.80   99.95   99.93   99.97    1.00
  0    4200         138.98    203.28   99.91   99.87   99.96    1.00
  0    4400         106.16    127.60   99.70   99.75   99.64    1.00
  0    4600          70.28     87.25   99.95   99.94   99.96    1.00
✔ Saved pipeline to output directory
training/model-last

</code></pre>
<p>I was trying to train my model for 2 epochs (<code>max_epochs=2</code>), and my train file has around 123591 Examples, and dev file has 2522 Examples.</p>
<p>My question is:</p>
<ul>
<li><p>Since my minimum batch size is 100, I expect my training to end before the 2400th eval batch, right? Because 2400th batch evaluated implies I have a <em>minimum</em> of 2400*100 = 240000, and it would actually be even more than that, since my batch size is increasing. So why did it go all the way to # 4600?</p>
</li>
<li><p>The training ended automatically, but the E still reads the 0th epoch. Why is that?</p>
</li>
</ul>
<p>Edit: In continuation to my 2nd bullet point, I'm curious to know why did the training went all the way upto 4600 batches, because 4600 batches at minimum means 4600*100 = 460000 examples, and I gave 123591  examples for train, so I'm clearly well above and over the 1st epoch, but E still reads as 0.</p>
","nlp, spacy, named-entity-recognition, spacy-3","<p>There's an entry for this in <a href=""https://github.com/explosion/spaCy/discussions/8226"" rel=""nofollow noreferrer"">the FAQ</a>, but to summarize:</p>
<ul>
<li><code>max_steps</code> is the maximum iterations. (Not &quot;evaluation iterations&quot;, but batches.)</li>
<li><code>max_epochs</code> is the maximum number of epochs.</li>
<li>If training goes for <code>patience</code> batches without improvement it will stop. That is what stopped your training.</li>
</ul>
<p>It seems like your model has already gotten a perfect score so I'm not sure why early stopping is a problem in this case, but that's what's happening.</p>
",4,4,5777,2021-07-21 14:42:09,https://stackoverflow.com/questions/68471586/training-epochs-interpretation-during-spacy-ner-training
Huggingface NER with custom data,"<p>I have a csv data as below.</p>
<pre><code>**token**      **label**
0.45&quot;      length
1-12       size
2.6&quot;       length
8-9-78     size
6mm        length
</code></pre>
<p>Whenever I get the text as below</p>
<pre><code>6mm 8-9-78 silver head
</code></pre>
<p>I should be able to say <code>length = 6mm</code> and <code>size = 8-9-78</code>. I'm new to NLP world, I'm trying to solve this using Huggingface NER. I have gone through various articles. I'm not getting how to train with my own data. Which <code>model/tokeniser</code> should I make use of? Or should I build my own? Any help would be appreciated.</p>
","python, nlp, huggingface-transformers, named-entity-recognition","<p>I had two options one is <code>Spacy</code> (as suggested by @scarpacci) and other one is <code>SparkNLP</code>. I opted for <code>SparkNLP</code> and found a solution. I formatted the data in CoNLL format and trained using Spark's <code>NerDlApproach</code> and <code>GLOVE word embedding</code>.</p>
",0,1,1080,2021-07-22 18:16:27,https://stackoverflow.com/questions/68489759/huggingface-ner-with-custom-data
Visualizing customized NER tags with SpaCy Displacy,"<p>I am new to spaCy and Python and I want to visualize a NER using this library. This is the sample example that I found:</p>
<pre><code>import spacy
from spacy import displacy

NER = spacy.load(&quot;en_core_web_sm&quot;)

raw_text=&quot;The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.&quot;

text1= NER(raw_text)

displacy.render(text1,style=&quot;ent&quot;,jupyter=True)
</code></pre>
<p><a href=""https://i.sstatic.net/y1CRm.png"" rel=""nofollow noreferrer"">The Example of Visualization</a></p>
<p>However, I already have a list of customized tags and their positions:</p>
<pre><code> [812, 834, &quot;POS&quot;], [838, 853, &quot;ORG&quot;], [870, 888, &quot;POS&quot;], [892, 920, &quot;ORG&quot;], [925, 929, &quot;ENGLEVEL&quot;], [987, 1002, &quot;SKILL&quot;],...
</code></pre>
<p>I want my text to be visualized with my own customized tags and entities, instead of the default NER options of spaCy. How can I achieve this?</p>
","python, spacy, named-entity-recognition, spacy-3","<p>You will need to add char spans signifying entities and attach them to your doc object. Something like this:</p>
<pre><code>import spacy
from spacy import displacy

nlp = spacy.blank('en')
raw_text = &quot;The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.&quot;
doc = nlp.make_doc(raw_text)
spans = [[812, 834, &quot;POS&quot;], [838, 853, &quot;ORG&quot;], [870, 888, &quot;POS&quot;], [892, 920, &quot;ORG&quot;], [925, 929, &quot;ENGLEVEL&quot;],
         [987, 1002, &quot;SKILL&quot;]]
ents = []
for span_start, span_end, label in spans:
    ent = doc.char_span(span_start, span_end, label=label)
    if ent is None:
        continue

    ents.append(ent)

doc.ents = ents
displacy.render(doc, style=&quot;ent&quot;, jupyter=True)
</code></pre>
<p>Change your <code>raw_text</code> and <code>spans</code> accordingly. If you give a span that starts or ends beyond the length of your text <code>doc.char_span()</code> returns <code>None</code> so you need to handle that appropriately.</p>
",5,3,3462,2021-07-26 14:48:32,https://stackoverflow.com/questions/68531988/visualizing-customized-ner-tags-with-spacy-displacy
Saving the custom model,"<p>I have a built a custom  model and planned to store the model in \ S3 which suits my project.</p>
<p>We have 2 different ways to save a model</p>
<pre><code>nlp.to_bytes()
nlp.to_disk()
</code></pre>
<p><code>nlp.to_disk()</code> needs a path in the argument. So I choose to go with <code>nlp.to_bytes()</code>,</p>
<p>Code:</p>
<pre><code>to_bytes_model = custom_nlp.to_bytes()
In: type(to_bytes_model) 
Out: bytes

s3 = boto3.resource('s3')    
s3.Bucket('customregex').upload_file('to_bytes_model','new_folder')
</code></pre>
<p>The above code for boto3 gives me error saying there is no file <code>to_bytes_model</code></p>
<p>Do need help on saving the nlp model directly to S3. Thanks.</p>
","python, amazon-s3, boto3, spacy, named-entity-recognition","<p><code>to_bytes_model</code> is an in-memory object. It is not written to a file on disk. Using <code>upload_file()</code> which looks for a file on disk based on the name of the file you pass in. See the documentation <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file"" rel=""nofollow noreferrer"">here</a>.</p>
<p>One way to do this is to use <code>nlp.to_disk()</code> followed by using some library to create zip/tar and then use <code>upload_file()</code> to upload the zip/tar. If you wish to skip the zip/tar you will need iterate over all the files in the folder to upload them one by one. I personally prefer option 1.</p>
",2,1,533,2021-07-26 17:46:07,https://stackoverflow.com/questions/68534365/saving-the-custom-model
spacy how to add patterns to existing Entity ruler?,"<p>My spacy version is 2.3.7. I have an existing trained custom NER model with NER and Entity Ruler pipes.
I want to update and retrain this existing pipeline.</p>
<p>The code to create the entity ruler pipe was as follows-</p>
<pre><code>ruler = EntityRuler(nlp)
for i in patt_dict:
  ruler.add_patterns(i)
nlp.add_pipe(ruler, name = &quot;entity_ruler&quot;)
</code></pre>
<p>Where <code>patt_dict</code> is the original patterns dictionary I had made.</p>
<p>Now, after finishing the training, now I have more input data and want to train the model more with the new input data.</p>
<p>How can I modify the above code to add more of patterns dictionary to the entity ruler when I load the spacy model later and want to retrain it with more input data?</p>
","python, spacy, named-entity-recognition","<p>It is generally better to retrain from scratch. If you train only on new data you are likely to run into &quot;catastrophic forgetting&quot;, where the model forgets anything not in the new data.</p>
<p>This is covered in detail in <a href=""https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"" rel=""nofollow noreferrer"">this spaCy blog post</a>. As of v3 the approach outlined there is available in spaCy, but it's still experimental and needs some work. In any case, it's still kind of a workaround, and the best thing is to train from scratch with all data.</p>
",3,2,1564,2021-07-27 11:36:29,https://stackoverflow.com/questions/68544127/spacy-how-to-add-patterns-to-existing-entity-ruler
Named Entity Recognition: Splitting data into test and train sets,"<p>When fitting a named entity recognition model, is it important to make sure that the entities that are in you training data do not repeat in your testing data? For example, if we have a relatively small data set and the goal is to identify person names. Now let us say we have 300 unique person names but would like to generalize our extraction to future data that may contain person names not in the 300 unique names we have in our data. Is it important to make sure that when we split the data into training and testing sets, that any of the 300 unique names not be found both in the training set as well as the testing set?</p>
","training-data, named-entity-recognition","<p>It is important that you have entities not in the training set to check that your model is generalizing, but usually you should have enough data and different values that with a random split you get a decent split even without checking to make sure it happens.</p>
",1,1,576,2021-07-27 14:29:53,https://stackoverflow.com/questions/68546794/named-entity-recognition-splitting-data-into-test-and-train-sets
Meaning of output/training status of 256 in Stanford NLP NER?,"<p>I have a Python program where I am using os.sys to train the Stanford NER from the command line. This returns an output/training status which I save in the variable &quot;status&quot;, and it is usually 0. However, I just ran it and got an output of 256, as well as not creating a file for the trained model. This error is only occurring for larger sets of training data. I searched through the documentation on the Stanford NLP website and there doesn't seem to be info on the meanings of the outputs or why increasing training data might affect the training. Thanks in advance for any help and problem code is below.</p>
<pre><code>cmdToSys = &quot;java -mx20g -cp stanford-corenlp-4.2.2.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop &quot; + self.trainPropFileName + &quot; -ner.useSUTime false test -ner.applyNumericClassifiers false test &quot;

status = os.system(cmdToSys)
</code></pre>
<p>note: self.trainPropFileName is just the property file</p>
","python, nlp, stanford-nlp, named-entity-recognition","<p>Status is an exit code, and non-zero exit codes mean your program failed. This is not a Stanford NLP convention, it's how all programs work on Unix/Linux.</p>
<p>There should be an error somewhere, maybe you ran out of memory? You'll have to track that down to find out what's wrong.</p>
",1,2,77,2021-07-27 14:34:44,https://stackoverflow.com/questions/68546867/meaning-of-output-training-status-of-256-in-stanford-nlp-ner
"Why is my SpaCy v3 scorer returing 0 for precision, recall and f1?","<p>I have the following code (migration from SpaCy v2) where I would like to calculate the precision, recall and f1-score for a given model:</p>
<pre><code>nlp = spacy.load(&quot;my_model&quot;)
scorer = Scorer(nlp)
examples = []
for text, annotations in TEST_DATA:
    examples.append(Example.from_dict(nlp.make_doc(text), annotations))
results = scorer.score(examples)
print(
    &quot;Precision {:0.4f}\tRecall {:0.4f}\tF-score {:0.4f}&quot;.format(results['ents_p'], results['ents_r'], results['ents_f'])
)
</code></pre>
<p>The weird thing I try to understand is why it always returns</p>
<pre><code>Precision 0.0000    Recall 0.0000   F-score 0.0000
</code></pre>
<p>My TEST_DATA set is in the same form as the TRAIN_DATA set I was using to train the same model. Here is how it looks like:</p>
<pre><code>[
    (
        'Line 106 – for dilution times, the units should be specified', {'entities': [(51, 60, 'ACTION'), (41, 47, 'MODAL'), (11, 40, 'CONTENT'), (0, 8, 'LOCATION')]}
    ),
    (
        'It should be indicated what test was applied  to verify the normality of distribution.', {'entities': [(13, 22, 'ACTION'), (28, 85, 'CONTENT'), (3, 9, 'MODAL')]}
    )
]
</code></pre>
","spacy, named-entity-recognition, precision-recall, spacy-3","<p>The scorer does not run the pipeline on the predicted docs, so you're evaluating blank docs against your test cases.</p>
<p>The recommended way is to use <code>nlp.evaluate</code> instead:</p>
<pre class=""lang-py prettyprint-override""><code>scores = nlp.evaluate(examples)
</code></pre>
<p>If you want the call the scorer directly for some reason, the other alternative is to run the pipeline on the predicted docs (<code>nlp</code> instead of <code>nlp.make_doc</code>), so:</p>
<pre class=""lang-py prettyprint-override""><code>example = Example.from_dict(nlp(text), annots)
</code></pre>
",2,0,1021,2021-08-11 10:04:48,https://stackoverflow.com/questions/68739904/why-is-my-spacy-v3-scorer-returing-0-for-precision-recall-and-f1
Error while trying to fine-tune the ReformerModelWithLMHead (google/reformer-enwik8) for NER,"<p>I'm trying to fine-tune the ReformerModelWithLMHead (google/reformer-enwik8) for NER. I used the padding sequence length same as in the encode method (max_length = max([len(string) for string in list_of_strings])) along with attention_masks. And I got this error:</p>
<p><strong>ValueError: If training, make sure that config.axial_pos_shape factors: (128, 512) multiply to sequence length. Got prod((128, 512)) != sequence_length: 2248. You might want to consider padding your sequence length to 65536 or changing config.axial_pos_shape.</strong></p>
<ul>
<li>When I changed the sequence length to 65536, my colab session crashed by getting all the inputs of 65536 lengths.</li>
<li>According to the second option(changing config.axial_pos_shape), I cannot change it.</li>
</ul>
<p>I would like to know, Is there any chance to change config.axial_pos_shape while fine-tuning the model? Or I'm missing something in encoding the input strings for reformer-enwik8?</p>
<p>Thanks!</p>
<p><strong>Question Update: I have tried the following methods:</strong></p>
<ol>
<li>By giving paramteres at the time of model instantiation:</li>
</ol>
<blockquote>
<p>model = transformers.ReformerModelWithLMHead.from_pretrained(&quot;google/reformer-enwik8&quot;, num_labels=9, max_position_embeddings=1024, axial_pos_shape=[16,64], axial_pos_embds_dim=[32,96],hidden_size=128)</p>
</blockquote>
<p>It gives me the following error:</p>
<blockquote>
<p>RuntimeError: Error(s) in loading state_dict for ReformerModelWithLMHead:
size mismatch for reformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([258, 1024]) from checkpoint, the shape in current model is torch.Size([258, 128]).
size mismatch for reformer.embeddings.position_embeddings.weights.0: copying a param with shape torch.Size([128, 1, 256]) from checkpoint, the shape in current model is torch.Size([16, 1, 32]).</p>
</blockquote>
<p>This is quite a long error.</p>
<ol start=""2"">
<li>Then I tried this code to update the config:</li>
</ol>
<blockquote>
<p>model1 = transformers.ReformerModelWithLMHead.from_pretrained('google/reformer-enwik8', num_labels = 9)</p>
</blockquote>
<h4>Reshape Axial Position Embeddings layer to match desired max seq length</h4>
<pre><code>model1.reformer.embeddings.position_embeddings.weights[1] = torch.nn.Parameter(model1.reformer.embeddings.position_embeddings.weights[1][0][:128])
</code></pre>
<h4>Update the config file to match custom max seq length</h4>
<pre><code>model1.config.axial_pos_shape = 16,128
model1.config.max_position_embeddings = 16*128 #2048
model1.config.axial_pos_embds_dim= 32,96
model1.config.hidden_size = 128
output_model_path = &quot;model&quot;
model1.save_pretrained(output_model_path)
</code></pre>
<p>By this implementation, I am getting this error:</p>
<blockquote>
<p>RuntimeError: The expanded size of the tensor (512) must match the existing size (128) at non-singleton dimension 2.  Target sizes: [1, 128, 512, 768].  Tensor sizes: [128, 768]</p>
</blockquote>
<p>Because updated size/shape doesn't match with the original config parameters of pretrained model. The original parameters are: axial_pos_shape = 128,512 max_position_embeddings = 128*512 #65536 axial_pos_embds_dim= 256,768 hidden_size = 1024</p>
<p>Is it the right way I'm changing the config parameters or do I have to do something else?</p>
<p>Is there any example where ReformerModelWithLMHead('google/reformer-enwik8') model fine-tuned.</p>
<p>My main code implementation is as follow:</p>
<pre><code>class REFORMER(torch.nn.Module):
def __init__(self):
    super(REFORMER, self).__init__()
    self.l1 = transformers.ReformerModelWithLMHead.from_pretrained(&quot;google/reformer-enwik8&quot;, num_labels=9)

def forward(self, input_ids, attention_masks, labels):
    output_1= self.l1(input_ids, attention_masks, labels = labels)
    return output_1


model = REFORMER()

def train(epoch):
    model.train()
    for _, data in enumerate(training_loader,0):
        ids = data['input_ids'][0]   # input_ids from encode method of the model https://huggingface.co/google/reformer-enwik8#:~:text=import%20torch%0A%0A%23%20Encoding-,def%20encode,-(list_of_strings%2C%20pad_token_id%3D0
        input_shape = ids.size()
        targets = data['tags']
        print(&quot;tags: &quot;, targets, targets.size())
        least_common_mult_chunk_length = 65536 
        padding_length = least_common_mult_chunk_length - input_shape[-1] % least_common_mult_chunk_length
        #pad input 
        input_ids, inputs_embeds, attention_mask, position_ids, input_shape = _pad_to_mult_of_chunk_length(self=model.l1,
                input_ids=ids,
                inputs_embeds=None,
                attention_mask=None,
                position_ids=None,
                input_shape=input_shape,
                padding_length=padding_length,
                padded_seq_length=None,
                device=None,
            )
        outputs = model(input_ids, attention_mask, labels=targets) # sending inputs to the forward method
        print(outputs)
        loss = outputs.loss
        logits = outputs.logits
        if _%500==0:
           print(f'Epoch: {epoch}, Loss:  {loss}')

for epoch in range(1):
    train(epoch)
</code></pre>
","python, nlp, pytorch, huggingface-transformers, named-entity-recognition","<p>First of all, you should note that <code>google/reformer-enwik8</code> is not a properly trained language model and that you will probably not get decent results from fine-tuning it. enwik8 is a compression challenge and <a href=""https://arxiv.org/abs/2001.04451"" rel=""nofollow noreferrer"">the reformer authors</a> used this dataset for exactly that purpose:</p>
<blockquote>
<p>To verify that the Reformer can indeed fit large models on a single
core and train fast on long sequences, we train up to 20-layer big
Reformers on enwik8 and imagenet64...</p>
</blockquote>
<p>This is also the reason why they haven't trained a sub-word tokenizer and operate on character level.</p>
<p>You should also note that the <code>LMHead</code> is usually used for predicting the next token of a sequence (CLM). You probably want to use a token classification head (i.e. use an encoder ReformerModel and add a linear layer with 9 classes on top+maybe a dropout layer).</p>
<p>Anyway, in case you want to try it still, you can do the following to reduce the memory footprint of the <code>google/reformer-enwik8</code> reformer:</p>
<ol>
<li>Reduce the number of hashes during training:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from transformers import ReformerConfig, ReformerModel
conf = ReformerConfig.from_pretrained('google/reformer-enwik8')
conf.num_hashes = 2 # or maybe even to 1
model = transformers.ReformerModel.from_pretrained(&quot;google/reformer-enwik8&quot;, config =conf)
</code></pre>
<p>After you have finetuned your model, you can increase the number of hashes again to increase the performance (compare Table 2 of the reformer paper).</p>
<ol start=""2"">
<li>Replace axial-position embeddings:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from transformers import ReformerConfig, ReformerModel
conf = ReformerConfig.from_pretrained('google/reformer-enwik8')
conf.axial_pos_embds = False 
model = transformers.ReformerModel.from_pretrained(&quot;google/reformer-enwik8&quot;, config =conf)
</code></pre>
<p>This will replace the learned axial positional embeddings with learnable position embeddings like Bert's and do not require the full sequence length of 65536. They are untrained and randomly initialized (i.e. consider a longer training).</p>
",3,6,928,2021-08-11 13:20:44,https://stackoverflow.com/questions/68742863/error-while-trying-to-fine-tune-the-reformermodelwithlmhead-google-reformer-enw
How to define a prediction function in keras for NER system?,"<p>I am creating a NER system following some tutorials using Keras. After the training and the first prediction, I'd like to use it to identify the NEs in a single string or in a list of strings of unseen data.</p>
<p>I can't seem to find the way to pass such string, or list of strings to the <code>model.predict()</code> and get an appropriate prediction.</p>
<p>This is the prediction for the test data in my code, so I was trying to adjust it to accept strings of unseen data and print the token + prediction:</p>
<pre><code>i = np.random.randint(0, x_test.shape[0])
print(&quot;This is sentence:&quot;,i)
p = model.predict(np.array([x_test[i]]))
p = np.argmax(p, axis=-1)

print(&quot;{:15}{:5}\t {}\n&quot;.format(&quot;Word&quot;, &quot;True&quot;, &quot;Pred&quot;))
print(&quot;-&quot; *30)
for w, true, pred in zip(x_test[i], y_test[i], p[0]):
    print(&quot;{:15}{}\t{}&quot;.format(words[w-1], tags[true], tags[pred]))
</code></pre>
<p>This piece of code predicts and print each token with the NE Tag, but I don't really understand how it works</p>
<p>This code prints something like:</p>
<pre><code>Word           True      Pred
------------------------------
The            O        O
British        B-gpe    B-gpe
pharmaceutical O        O
company        O        O
GlaxoSmithKlineB-org    O
</code></pre>
<p>I'd like to pass for example:</p>
<pre><code>sentence = &quot;President Obama became the first sitting American president to visit Hiroshima&quot;
</code></pre>
<p>and being able to see the identified NEs. Any advise on how to do this?</p>
<p>A copy of the full code is <a href=""https://colab.research.google.com/drive/1Nc8U3B6K1NRthLixWEceoggjmFPi99cM?usp=sharing"" rel=""nofollow noreferrer"">here</a> and the dataset is used is <a href=""https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/"" rel=""nofollow noreferrer"">here</a>.</p>
","python, tensorflow, keras, named-entity-recognition","<p>You can make a prediction on a list of sentences like this:</p>
<pre><code>my_sentences = [&quot;President Obama became the first sitting American president to visit Hiroshima&quot;,
                &quot;Jack is a good person and living in Iran&quot;]

my_sentences_idx = [[word2idx[w] for w in s.split(&quot; &quot;)] for s in my_sentences]

my_sentences_padded = pad_sequences(maxlen=max_len, sequences=my_sentences_idx, padding=&quot;post&quot;, value=num_words-1)
preds = np.argmax(model.predict(np.array(my_sentences_padded)), axis=-1)

for idx, p in enumerate(preds):
    print(&quot;-&quot; *30)
    print(my_sentences[idx])
    print(&quot;-&quot; *30)
    for w, pred in zip(my_sentences[idx].split(&quot; &quot;), preds[idx]):
        if tags[pred]!=&quot;O&quot;:
            print(&quot;{:15} {} &quot;.format(w, tags[pred]))
    print()
</code></pre>
<p>Output:</p>
<pre><code>------------------------------
President Obama became the first sitting American president to visit Hiroshima
------------------------------
President       B-per 
Obama           I-per 
American        B-gpe 
Hiroshima       B-geo 

------------------------------
Jack is a good person and living in Iran
------------------------------
Jack            B-per 
Iran            B-geo 
</code></pre>
",1,2,288,2021-08-13 11:52:44,https://stackoverflow.com/questions/68771849/how-to-define-a-prediction-function-in-keras-for-ner-system
Attribute error creating a column of NER labels,"<p>I am trying to create columns in a dataframe that show the entities and labels from a spaCy model, so far by entering the following code I can produce a column of entities:</p>
<pre><code>df['new_col'] = df['Combined'].apply(lambda x: list(ner_model(x).ents))
</code></pre>
<p>However, if I try the same for labels:</p>
<pre><code>#df['new_col1'] = df['Combined'].apply(lambda x: list(nlp(x).label_))
</code></pre>
<p>I get 'AttributeError: 'spacy.tokens.doc.Doc' object has no attribute 'label_''</p>
<p>I suspect I might have to iterate over individual tokens but I am not sure how to do this?</p>
","nlp, spacy, named-entity-recognition","<p>You need to do something like this</p>
<pre><code>df['new_col1'] = df['Combined'].apply(lambda x: [ent.label_ for ent in nlp(x).ents])
</code></pre>
<p>The output of <code>nlp(x)</code> is a <code>Doc</code> object and there is no <code>label</code> attribute on the <code>Doc</code> object (as is explicitly stated in the error you get). You need the labels of the entities on the <code>Doc</code> object which is why you need to iterate over <code>nlp(x).ents</code> and get the <code>label_</code> of each entity.</p>
",0,0,151,2021-09-03 08:28:34,https://stackoverflow.com/questions/69041790/attribute-error-creating-a-column-of-ner-labels
Replace to entity tags to IOB format,"<p>I am trying to convert non-IOB tags to IOB in a conllu file.</p>
<p>Two sample lines of the file would be:</p>
<blockquote>
<p>2 Ute Ute PROPN   NE  Case=Nom|Gender=Fem|Number=Sing 1   appos   _   <em><strong>NE=PER_23</strong></em>|Morph=nsf</p>
</blockquote>
<blockquote>
<p>3 Wedemeier   Wedemeier   PROPN   NE  Case=Nom|Gender=Fem|Number=Sing 2   flat    _   SpaceAfter=No|<em><strong>NE=PER_23</strong></em>|Morph=nsf</p>
</blockquote>
<p>And I would like to have</p>
<blockquote>
<p>2 Ute Ute PROPN   NE  Case=Nom|Gender=Fem|Number=Sing 1   appos   _   <em><strong>NE=B-PER</strong></em>|Morph=nsf</p>
</blockquote>
<blockquote>
<p>3 Wedemeier   Wedemeier   PROPN   NE  Case=Nom|Gender=Fem|Number=Sing 2   flat    _   SpaceAfter=No|<em><strong>NE=I-PER</strong></em>|Morph=nsf</p>
</blockquote>
<p>I now want to parse over the file, changing all occurring &quot;NE=NamedEntityTag_Number&quot; to IOB (the type isn't important, just each &quot;NE=field_type_number (in the example &quot;NE=PER_23&quot;) to (NE=B-PER and NE=I-PER). PER could be any field in in list_of_fields. Therefore, I created a list_of_fields with all named entity tags occurring. Since the conllu file is saved as a text file, I am parsing over a text file. Since not all lines contain named entity tags, I first check, whether a named entity tag is in the line, if so, I check, if the same tag (including the same number) is in the next line, and the line after that etc. This is important: when the next line contains the same annotation with the same number id, it belongs to the same entity, and therefore, the first must be B-PER, whereas the following of that row must be I-PER.</p>
<p>I am trying to use fileinput, just to change the part of the NE's.</p>
<p>Hope someone can help, thanks!</p>
<p>`</p>
<pre><code>import fileinput

import re

list_of_fields = [&quot;PER&quot;, &quot;ORG&quot;, &quot;LOC&quot;, &quot;GPE&quot;, &quot;OTH&quot;]

with fileinput.FileInput(file, inplace=True, backup=&quot;.bak&quot;) as file:
    for line in file:
        ne = [annotation for annotation in list_of_fields if (annotation in line)]
        if re.compile(r&quot;^NE=&quot;+ne+&quot;\_\d+$&quot;) in line:
            if re.compile(r&quot;^NE=&quot;+ne+&quot;\_\d+$&quot;) in next(line) == re.compile(r&quot;^NE=&quot;+ne+&quot;\_\d+$&quot;) in line:
                re.sub(r&quot;^NE=&quot;+ne+&quot;\_\d+$&quot;, r&quot;NE=B-&quot;+ne, line)
                re.sub(r&quot;^NE=&quot;+ne+&quot;\_\d+$&quot;, r&quot;NE=I-&quot;+ne, next(line))
            else:
                re.sub(r&quot;^NE=&quot; + ne + &quot;\_\d+$&quot;, r&quot;NE=B-&quot; + ne, line)`
</code></pre>
","python-3.x, regex, text-parsing, named-entity-recognition","<p>You have to save the last field and last value to compare it across multiple lines. If either differs with the next one, you do the replacement with <code>B-&lt;field&gt;</code> and otherwise with <code>I-&lt;field&gt;</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import fileinput
import re

list_of_fields = [&quot;PER&quot;, &quot;ORG&quot;, &quot;LOC&quot;, &quot;GPE&quot;, &quot;OTH&quot;]
joined_fields = f'({&quot;|&quot;.join(list_of_fields)})'
field_pattern = re.compile(f'NE={joined_fields}')
last_field = last_value = None

with fileinput.FileInput(file, inplace=True, backup=&quot;.bak&quot;) as in_file,
     open('output.txt', 'wt') as out_file:

    for line in in_file:
        matches = re.findall(field_pattern, line)
        if not matches:
            # keep input
            out_file.write(line)
            continue
        field = matches[0] # assuming only one field per line
        start_index = line.find(f'NE={field}')
        end_index = line.find('|', start_index)
        value = re.findall(rf'{field}_(\d+)', line[start_index:end_index])[0]
        if field != last_field or value != last_value:
            replacement = f'B-{field}'
        else:
            replacement = f'I-{field}'
        last_field = field
        last_value = value
        new_line = re.sub(rf'{field}_{value}(-{joined_fields}_\d+)*', replacement, line)
        out_file.write(new_line)
</code></pre>
<p>EDIT: allowed for multiple fields, using only the first one</p>
",1,1,791,2021-09-03 12:23:39,https://stackoverflow.com/questions/69044909/replace-to-entity-tags-to-iob-format
SpaCy 3: how to get the raw data used to train en_core_web_sm?,"<p>I am new to SpaCy. I noticed that there are a number of NER categories listed in the documentation of all <code>en_core_web</code> models:</p>
<pre><code>'CARDINAL', 
'DATE', 
'EVENT', 
'FAC', 
'GPE', 
'LANGUAGE', 
'LAW', 
'LOC', 
'MONEY', 
'NORP', 
'ORDINAL', 
'ORG', 
'PERCENT', 
'PERSON', 
'PRODUCT', 
'QUANTITY', 
'TIME', 
'WORK_OF_ART'
</code></pre>
<p>I need to access the raw data used to assign each word the correct category. In other words, what's the list of words labelled as <code>'WORK_OF_ART'</code>, and is this list available?</p>
<p>The reason I ask this question is that I want to build a custom model that uses some of the default NER categories, as well as my own.</p>
","python, nlp, spacy, named-entity-recognition","<p>Depending on which variant of <code>en_core_web</code>, the data varies,</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Dataset</th>
<th style=""text-align: left;"">License</th>
<th style=""text-align: center;"">URL</th>
<th style=""text-align: center;"">web_sm</th>
<th style=""text-align: center;"">web_md</th>
<th style=""text-align: center;"">eweb_lg</th>
<th style=""text-align: center;"">web_trf</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">OntoNotes 5</td>
<td style=""text-align: left;"">LDC Non-Members</td>
<td style=""text-align: center;"">https://catalog.ldc.upenn.edu/LDC2013T19</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
</tr>
<tr>
<td style=""text-align: left;"">Wordnet 3.0</td>
<td style=""text-align: left;"">WordNet License</td>
<td style=""text-align: center;""><a href=""https://wordnet.princeton.edu/download"" rel=""nofollow noreferrer"">https://wordnet.princeton.edu/download</a></td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
</tr>
<tr>
<td style=""text-align: left;"">ClearNLP Constituent-to-Dependency Conversion</td>
<td style=""text-align: left;"">Apache 2.0</td>
<td style=""text-align: center;""><a href=""https://github.com/clir/clearnlp-guidelines/blob/master/md/components/dependency_conversion.md"" rel=""nofollow noreferrer"">dependency_conversion.md</a></td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
</tr>
<tr>
<td style=""text-align: left;"">GloVe Common Crawl</td>
<td style=""text-align: left;"">Apache 2.0</td>
<td style=""text-align: center;""><a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a></td>
<td style=""text-align: center;"">✕</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✓</td>
<td style=""text-align: center;"">✕</td>
</tr>
<tr>
<td style=""text-align: left;"">Roberta Base</td>
<td style=""text-align: left;"">???</td>
<td style=""text-align: center;""><a href=""https://github.com/pytorch/fairseq/tree/master/examples/roberta"" rel=""nofollow noreferrer"">Fairseq Roberta</a></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
</tr>
</tbody>
</table>
</div>
<p>The NER labelling scheme as described from <a href=""https://spacy.io/models/en"" rel=""nofollow noreferrer"">https://spacy.io/models/en</a> is from OntoNotes that contains NER tags, see Section 2.6 of <a href=""https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf"" rel=""nofollow noreferrer"">https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf</a></p>
<p>The NER tags adopts the CONLL BIO format, see <a href=""https://github.com/yuchenlin/OntoNotes-5.0-NER-BIO"" rel=""nofollow noreferrer"">https://github.com/yuchenlin/OntoNotes-5.0-NER-BIO</a> and when read properly, each sentence should be a list of tuples, e.g. <a href=""https://stackoverflow.com/questions/56274996/get-stanford-ner-result-through-nltk-with-iob-format"">Get Stanford NER result through NLTK with IOB format</a></p>
<p>Also take a look at <a href=""https://github.com/flairNLP/flair/"" rel=""nofollow noreferrer"">https://github.com/flairNLP/flair/</a> when it comes to training NER using Ontonotes, it might help.</p>
",1,1,707,2021-09-07 10:57:02,https://stackoverflow.com/questions/69086957/spacy-3-how-to-get-the-raw-data-used-to-train-en-core-web-sm
looping over unique entries,"<p>I have some labeled entities with text and am trying to get them into something SpaCy to use them to make an ner model.  I am having trouble making a for loop to get entities within the same text to be in the same entry.</p>
<p>Example data: (df)</p>
<pre><code>Text                              start     end     ent
Sara and Sam went to the park     0         4       Person
Sara and Sam went to the park     9         12      Person
Jake played on the swings         0         4       Person
The dog played with Tom           20        23      Person
</code></pre>
<p>My attempt at this is:</p>
<pre><code>TRIAN = []
ENTS = []
for i in len(np.unique(df['Text'])[i]):
    text = df['Text'][i]
    for ii in range(len(df[df['Text'] == np.unique(df['Text'])[i]]]):
        Ent = [(df['start'][i + ii],[df['end'][i + ii],df['ent'][i + ii])]
        ENTS.append(Ent[i + ii])
        Results = [text[i], {'entities': ENTS.append(Ent[i + ii])}]
        TRAIN.append(Results)
print(TRAIN)
</code></pre>
<p>The desired output is:
[[ &quot;Sara and Sam went to the park&quot;, {&quot;entities&quot;: [[0,4,&quot;Person&quot;], [9,12, &quot;Person&quot;]]}], [&quot;Jake played on the swings&quot;, {&quot;entities&quot;: [[0,4,&quot;Person&quot;]]}], [&quot;The dog played with Tom&quot;, {entities&quot;: [[20,23,&quot;Person&quot;]]}]]</p>
<p>Any suggestions on how to fix my code to produce the desired output would be much appreciated.</p>
","python, spacy, named-entity-recognition","<p>The way your data is formatted is kind of weird and it's going to be kind of awkward to work with. You can do something like this. (I'm going to leave out dataframe manipulation because it's not relevant.)</p>
<pre><code>docs = []
ents = []
old = None # prior sentence
for row in data:
    text, start, end, label = ... # split it somehow
    if text != old:
        # new doc, reset the ent buffer
        if old is not None:
            docs.append( [old, ents] )
        ents = []
        old = text
    ents.append( (start, end, label) )
# clean up after the loop
docs.append( [text, ents] )
</code></pre>
",1,0,58,2021-09-10 16:20:38,https://stackoverflow.com/questions/69135202/looping-over-unique-entries
SpaCy: how do you add custom NER labels to a pre-trained model?,"<p>I am new to SpaCy and NLP. I am using SpaCy v 3.1 and Python 3.9.7 64-bit.</p>
<p><strong>My objective</strong>: to use a pre-trained SpaCy model (<code>en_core_web_sm</code>) and add a set of custom labels to the existing NER labels (<code>GPE</code>, <code>PERSON</code>, <code>MONEY</code>, etc.) so that the model can recognize both the default AND the custom entities.</p>
<p>I've looked at the SpaCy documentation and what I need seems to be an <a href=""https://spacy.io/api/entityrecognizer"" rel=""noreferrer"">EntityRecogniser</a>, specifically a new pipe.</p>
<p>However, it is not really clear to me at what point in my workflow I should add this new pipe, since in SpaCy 3 the training happens in CLI, and from the docs it's not even clear to me where the pre-trained model is called.</p>
<p>Any tutorials or pointers you might have are highly appreciated.</p>
<p>This is what I think should be done, but I am not sure how:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy import displacy
from spacy_langdetect import LanguageDetector
from spacy.language import Language
from spacy.pipeline import EntityRecognizer

# Load model
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# Register custom component and turn a simple function into a pipeline component
@Language.factory('new-ner')
def create_bespoke_ner(nlp, name):
    
    # Train the new pipeline with custom labels here??
    
    return LanguageDetector()

# Add custom pipe
custom = nlp.add_pipe(&quot;new-ner&quot;)
</code></pre>
<p>This is what my config file looks like so far. I suspect my new pipe needs to go next to &quot;tok2vec&quot; and &quot;ner&quot;.</p>
<pre><code>[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
update_with_oracle_cut_size = 100
</code></pre>
","python, nlp, spacy, named-entity-recognition","<p>For Spacy 3.2 I did it this way:</p>
<pre><code>import spacy
import random
from spacy import util
from spacy.tokens import Doc
from spacy.training import Example
from spacy.language import Language

def print_doc_entities(_doc: Doc):
    if _doc.ents:
        for _ent in _doc.ents:
            print(f&quot;     {_ent.text} {_ent.label_}&quot;)
    else:
        print(&quot;     NONE&quot;)

def customizing_pipeline_component(nlp: Language):
    # NOTE: Starting from Spacy 3.0, training via Python API was changed. For information see - https://spacy.io/usage/v3#migrating-training-python
    train_data = [
        ('We need to deliver it to Festy.', [(25, 30, 'DISTRICT')]),
        ('I like red oranges', [])
    ]

    # Result before training
    print(f&quot;\nResult BEFORE training:&quot;)
    doc = nlp(u'I need a taxi to Festy.')
    print_doc_entities(doc)

    # Disable all pipe components except 'ner'
    disabled_pipes = []
    for pipe_name in nlp.pipe_names:
        if pipe_name != 'ner':
            nlp.disable_pipes(pipe_name)
            disabled_pipes.append(pipe_name)

    print(&quot;   Training ...&quot;)
    optimizer = nlp.create_optimizer()
    for _ in range(25):
        random.shuffle(train_data)
        for raw_text, entity_offsets in train_data:
            doc = nlp.make_doc(raw_text)
            example = Example.from_dict(doc, {&quot;entities&quot;: entity_offsets})
            nlp.update([example], sgd=optimizer)

    # Enable all previously disabled pipe components
    for pipe_name in disabled_pipes:
        nlp.enable_pipe(pipe_name)

    # Result after training
    print(f&quot;Result AFTER training:&quot;)
    doc = nlp(u'I need a taxi to Festy.')
    print_doc_entities(doc)

def main():
    nlp = spacy.load('en_core_web_sm')
    customizing_pipeline_component(nlp)


if __name__ == '__main__':
    main()
</code></pre>
",12,12,8060,2021-09-14 16:02:47,https://stackoverflow.com/questions/69181078/spacy-how-do-you-add-custom-ner-labels-to-a-pre-trained-model
How to update NER model using BILUO schema in Spacy v3?,"<p>I want to add some PERSON vocab to the sm NER spacy model. To do this, I tagged names of Congressional officials taken from the ProPublica API using the BILUO schema. I have seen a lot of posts about using the dictionary w/ entities format. But so far can't find any where how to do it w/ the BILUO schema in v3. I have my data as a list of tuples. With the first value being the string and the second the labeled entities.</p>
<pre><code>data = [('Nanette Barragán', list(['B-PERSON', 'L-PERSON'])),
 ('Jack Bergman', list(['B-PERSON', 'L-PERSON'])),
 ('Andy Biggs', list(['B-PERSON', 'L-PERSON'])),
 ('Lisa Blunt Rochester', list(['B-PERSON', 'I-PERSON', 'L-PERSON'])),
 ('Anthony Brown', list(['B-PERSON', 'L-PERSON'])),
 ('Ted Budd', list(['B-PERSON', 'L-PERSON'])),
 ('Troy Balderson', list(['B-PERSON', 'L-PERSON'])),
 ('James Baird', list(['B-PERSON', 'L-PERSON'])),
 ('Tim Burchett', list(['B-PERSON', 'L-PERSON']))]
</code></pre>
<p>I want to add these names to the already loaded NER model. I have been following steps from: <a href=""https://stackoverflow.com/questions/64760271/how-can-we-use-spacy-minibatch-and-goldparse-to-train-ner-model-using-biluo-tagg"">How can we use Spacy minibatch and GoldParse to train NER model using BILUO tagging scheme?</a>. This referred to v2, however v3 no longer uses GoldParse as shown in the text taken from Spacy v3 documentation below:</p>
<pre><code>  The Language.update, Language.evaluate and 
  TrainablePipe.update methods now all take batches of Example 
  objects instead of Doc and GoldParse objects, or raw text and a 
  dictionary of annotations.
</code></pre>
<p>The documentation for Example is shown here: <a href=""https://spacy.io/api/example"" rel=""nofollow noreferrer"">https://spacy.io/api/example</a></p>
<p>Below is my updated script</p>
<pre><code>import spacy
from spacy import displacy
import en_core_web_sm
import random 
from spacy.util import minibatch, compounding
from spacy.tokens import Doc
from spacy.training import Example

nlp = en_core_web_sm.load()


if 'ner' not in nlp.pipe_names:
    nlp.add_pipe('ner', last=True)
else:
    ner = nlp.get_pipe('ner')

ner.add_label('PERSON')

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
with nlp.disable_pipes(*other_pipes):  # only train NER
    optimizer = nlp.create_optimizer()
    tags = dict()
    for itn in range(10):
        print(&quot;Starting iteration &quot; + str(itn))
        random.shuffle(data)
        losses = {}
        batches = minibatch(data, size=compounding(4.0, 16.0, 1.001))
        # type 2 with mini batch
        for batch in batches:
            texts, _ = zip(*batch)
            golds = [Example(Doc(nlp.vocab, words = t), references = a) for t,a in batch]
            nlp.update(
                texts,  # batch of texts
                golds,  # batch of annotations
                drop=0.4,  # dropout - make it harder to memorise data
                losses=losses,
                sgd=optimizer)
</code></pre>
<p>However, I get the error:</p>
<pre><code>   Starting iteration 0
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/var/folders/36/j_203fcj42q9bvnlt1sl3j640000gp/T/ipykernel_162/1201951981.py in &lt;module&gt;
     14         for batch in batches:
     15             texts, _ = zip(*batch)
---&gt; 16             golds = [Example(Doc(nlp.vocab, words = t),references = a) for t,a in batch]
     17             nlp.update(
     18                 texts,  # batch of texts

/var/folders/36/j_203fcj42q9bvnlt1sl3j640000gp/T/ipykernel_162/1201951981.py in &lt;listcomp&gt;(.0)
     14         for batch in batches:
     15             texts, _ = zip(*batch)
---&gt; 16             golds = [Example(Doc(nlp.vocab, words = t),references = a) for t,a in batch]
     17             nlp.update(
     18                 texts,  # batch of texts

~/.pyenv/versions/3.8.3/lib/python3.8/site-packages/spacy/training/example.pyx in spacy.training.example.Example.__init__()

TypeError: __init__() takes exactly 2 positional arguments (1 given)
</code></pre>
<p>I also tried replacing the Doc var below as in the original code but still same error.</p>
<pre><code> golds = [Example(nlp.make_doc(t),references = a) for t,a in batch]
</code></pre>
<p>I can't figure out how to get past this error. Any help is greatly appreciated!</p>
","python, spacy, named-entity-recognition, spacy-3","<p>I'm not addressing your specific error here, rather you seem to have some misconceptions, so I want to be clear about how things work.</p>
<p>First, the NER model doesn't just use a list of words. You cannot just add a list of words to it. If you want to add those specific names, you need <strong>actual sentences</strong> that contain them to use as training data. But you also want a full suite of training data so just adding those names isn't a good idea anyway.</p>
<p>If you want to use the NER model <strong>plus</strong> you want to always recognize those names, then I would put those names in an <a href=""https://spacy.io/usage/rule-based-matching#entityruler"" rel=""nofollow noreferrer"">EntityRuler</a> and put it before the NER component in the pipeline.</p>
",1,1,441,2021-09-24 14:18:10,https://stackoverflow.com/questions/69316534/how-to-update-ner-model-using-biluo-schema-in-spacy-v3
NER using spaCy &amp; Transformers - different result when running inside and outside of a loop,"<p>I am using NER (spacy &amp; Transformer) for finding and anonymizing personal information. I noticed that the output I get when giving an input line directly is different than when the input line is read from a file (see screenshot below). Does anyone have suggestions on how to fix this?</p>
<p><a href=""https://i.sstatic.net/DZbaQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DZbaQ.png"" alt=""enter image description here"" /></a></p>
<p>Here is my code:</p>
<pre><code>import pandas as pd
import csv
import spacy
from spacy import displacy
from transformers import pipeline
import re

!python -m spacy download en_core_web_trf
nlp = spacy.load('en_core_web_trf')

sent = nlp('Yesterday I went out with Andrew, johanna and Jonathan Sparow.')
displacy.render(sent, style = 'ent')

with open('Synth_dataset_raw.txt', 'r') as fd:
    reader = csv.reader(fd)
    for row in reader:
        sent = nlp(str(row))
        displacy.render(sent, style = 'ent')
</code></pre>
","python, spacy, huggingface-transformers, named-entity-recognition","<p>You are using the csv module to read your file and then trying to convert each row (aka line) of the file to a string with <code>str(row)</code>.</p>
<p>If your file just has one sentence per line, then you do not need the csv module at all. You could just do</p>
<pre class=""lang-py prettyprint-override""><code>with open('Synth_dataset_raw.txt', 'r') as fd:
    for line in fd:
        # Remove the trailing newline
        line = line.rstrip()
        sent = nlp(line)
        displacy.render(sent, style = 'ent')
</code></pre>
<p>If you in fact have a csv (with presumably multiple columns and a header) you could do</p>
<pre class=""lang-py prettyprint-override""><code>open('Synth_dataset_raw.txt', 'r') as fd:
    reader = csv.reader(fd)
    header = next(reader)
    text_column_index = 0
    for row in reader:
        sent = nlp(row[text_column_index])
        displacy.render(sent, style = 'ent')
</code></pre>
",3,0,383,2021-10-06 02:55:20,https://stackoverflow.com/questions/69459301/ner-using-spacy-transformers-different-result-when-running-inside-and-outsid
SparkNLP&#39;s NerCrfApproach with custom labels,"<p>I am trying to train a SparkNLP <code>NerCrfApproach</code> model with a dataset in CoNLL format that has custom labels for product entities (like I-Prod, B-Prod etc.). However, when using the trained model to make predictions, I get only &quot;O&quot; as the assigned label for all tokens. When using the same model trained on the CoNLL data from the SparkNLP workshop example, the classification works fine.
(cf. <a href=""https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/jupyter/training/english/crf-ner"" rel=""nofollow noreferrer"">https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/jupyter/training/english/crf-ner</a>)</p>
<p>So, the question is: Does <code>NerCrfApproach</code> rely on the standard tag set for NER labels used by the CoNLL data? Or can I use it for any custom labels and, if yes, do I need to specify these somehow? My assumption was that the labels are inferred from the training data.</p>
<p>Cheers,
Martin</p>
<p>Update: The issue might not be related to the labels after all. I tried to replace my custom labels with CoNLL standard labels and I am still not getting the expected classification results.</p>
","named-entity-recognition, johnsnowlabs-spark-nlp","<p>As it turns out, this issue was not caused by the labels, but rather by the size of the dataset. I was using a rather small dataset for development purposes. Not only was this dataset quite small, but also heavily imbalanced, with a lot more &quot;O&quot; labels than the other labels. Fixing this by using a dataset of 10x the original size (in terms of sentences), I am able to get meaningful results, even for my custom labels.</p>
",0,0,366,2021-10-13 07:33:51,https://stackoverflow.com/questions/69551405/sparknlps-nercrfapproach-with-custom-labels
Confidence Score of Predicted NER entities using Spacy,"<p>I am trying to predict entities using a custom trained NER model using spacy. I read <a href=""https://github.com/explosion/spaCy/pull/8855"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/pull/8855</a> that confidence scores of each entity can be obtained using spancat. But I have a little confusion regarding to make that work. According to my understanding, we have to train a pipeline using spancat component. So while training, within the config file there is a segment,</p>
<pre><code>[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]
batch_size = 1000
</code></pre>
<p>Should we have to change this to</p>
<pre><code>[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;,&quot;spancat&quot;]
batch_size = 1000
</code></pre>
<p>for the spancat to work.</p>
<p>Then after training, while predicting the entities from unknown text, should we have to use</p>
<pre><code>doc = nlp(data_to_be_predicted)
spans = doc.spans[&quot;spancat&quot;] # SpanGroup
print(spans.attrs[&quot;scores&quot;]) # list of numbers, span length as SpanGroup
</code></pre>
<p>to get the confidence scores.</p>
<p>I am using spacy 3.1.3. I believe according to the documentation, this feature is rolled out by now.</p>
","python, nlp, spacy, named-entity-recognition","<p>I'm not really sure there's a question in your post, but yes, the spancat is available and you can get entity scores from it.</p>
<p>The spancat is a different component from the ner component. So if you do this:</p>
<pre><code>pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;,&quot;spancat&quot;]
</code></pre>
<p>The spancat will not add scores for things your ner component predicted. You probably want to remove the ner component.</p>
<hr />
<p>About usage, please see <a href=""https://spacy.io/api/spancategorizer"" rel=""nofollow noreferrer"">the docs</a> and <a href=""https://github.com/explosion/projects/tree/v3/experimental/ner_spancat"" rel=""nofollow noreferrer"">the example project</a>. This is how you get the score:</p>
<pre><code>doc = nlp(text)
span_group = doc.spans[&quot;spans&quot;] # default key, can be changed
scores = span_group.attrs[&quot;scores&quot;]

# Note that `scores` is an array with one score for each span in the group
for span, score in zip(span_group, scores):
    print(score, span)
</code></pre>
",4,1,3535,2021-10-23 09:42:51,https://stackoverflow.com/questions/69686930/confidence-score-of-predicted-ner-entities-using-spacy
Could not find function &#39;spacy-transformers.TransformerModel.v3&#39; in function registry &#39;architectures&#39;,"<p>I was trying to create a custom NER model. I used spacy library to create the model. And this line of code is to create the config file from the <code>base.config</code> file.
<strong>My code is :</strong></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>!python -m spacy init fill-config /content/drive/MyDrive/NER_RE_New/NER/base_config.cfg /content/drive/MyDrive/NER_RE_New/NER/config.cfg</code></pre>
</div>
</div>
</p>
<p><strong>Error</strong> :</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>catalogue.RegistryError: [E893] Could not find function 'spacy-transformers.TransformerModel.v3' in function registry 'architectures'. If you're using a custom function, make sure the code is available. If the function is provided by a third-party package, e.g. spacy-transformers, make sure the package is installed in your environment.</code></pre>
</div>
</div>
</p>
<p><strong>Available names:</strong></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>spacy-legacy.CharacterEmbed.v1, 
spacy-legacy.HashEmbedCNN.v1, 
spacy-legacy.MaxoutWindowEncoder.v1, 
spacy-legacy.MishWindowEncoder.v1, 
spacy-legacy.MultiHashEmbed.v1, 
spacy-legacy.TextCatBOW.v1, 
spacy-legacy.TextCatCNN.v1, 
spacy-legacy.TextCatEnsemble.v1, 
spacy-legacy.Tok2Vec.v1, 
spacy-legacy.TransitionBasedParser.v1, 
spacy-transformers.Tok2VecTransformer.v1,
spacy-transformers.TransformerListener.v1, 
spacy-transformers.TransformerModel.v1, 
spacy.CharacterEmbed.v1, 
spacy.EntityLinker.v1, 
spacy.HashEmbedCNN.v1, 
spacy.MaxoutWindowEncoder.v2, 
spacy.MishWindowEncoder.v2, 
spacy.MultiHashEmbed.v1, 
spacy.PretrainCharacters.v1, 
spacy.PretrainVectors.v1, 
spacy.Tagger.v1, 
spacy.TextCatBOW.v1, 
spacy.TextCatCNN.v1, 
spacy.TextCatEnsemble.v2, 
spacy.TextCatLowData.v1, 
spacy.Tok2Vec.v2, 
spacy.Tok2VecListener.v1, 
spacy.TorchBiLSTMEncoder.v1, 
spacy.TransitionBasedParser.v1, 
spacy.TransitionBasedParser.v2</code></pre>
</div>
</div>
</p>
","named-entity-recognition, bert-language-model, spacy-3, spacy-transformers","<p>This happened since spacy had a new update 3.1 recently. And the base_config file have the architecture mentioned as &quot;spacy-transformers.TransformerModel.v3&quot;. Change it into &quot;spacy-transformers.TransformerModel.v1&quot;</p>
<pre><code>[components.transformer.model]
@architectures = &quot;spacy-transformers.TransformerModel.v1&quot;
name = &quot;roberta-base&quot;
tokenizer_config = {&quot;use_fast&quot;: true}
</code></pre>
",2,2,5514,2021-10-24 06:10:49,https://stackoverflow.com/questions/69694277/could-not-find-function-spacy-transformers-transformermodel-v3-in-function-reg
What is the most accurate way to detect dates in text?,"<p>I'm working on a sensitive data recognition (NER) task. Faced with the fact that I can not accurately detect dates in texts. I've tried almost everything...</p>
<p>For example I have this type of dates in my text:</p>
<pre><code>date_list = ['23 octbr', '08/10/1975', '2/20/1961', 'December 23', '2021', '1/10/1980', ...]
</code></pre>
<p>But I must say that there is also a lot of numerical information in the text, for example, IP addresses, house addresses, bank card numbers, etc.</p>
<p>This is an example of how <code>Spacy</code> works:</p>
<pre><code>'08/10/1975' -&gt; Entityt type: No Entity
'2/20/1961' -&gt; Entityt type: DATE
'1/10/1980' -&gt; Entityt type: CARDINAL
</code></pre>
<p>Or for example I have phone number <code>&quot;(150) 224-2215&quot;</code> and it <code>Spacy</code> marks the part <code>&quot;24-2215&quot;</code> as a Date. It often happens with adresses and credit card numbers too.</p>
<p>Then I have tried <code>datefinder</code> and <code>dateparser.search</code>, but they detected completely incorrect parts of the sentence or those that contained the word &quot;to&quot;.</p>
<p>Can you please share your experience, what could work better? What is the best way to get high accuracy of date detection?</p>
","python, spacy, named-entity-recognition, dateparser, datefinder","<p>What does your corpus include, does it include full sentences?</p>
<ul>
<li><p>First of all you can try spaCy NER with <strong>context</strong>. NER algorithms work on full sentences.</p>
</li>
<li><p>If you look for a more token/shape oriented solution, I suggest context free parsing. A context free grammar is great for describing dates. Basically you define some grammar rules such as:</p>
</li>
</ul>
<pre><code>calendar_year -&gt; full_year | year
year -&gt; 19\d{,2} | 20\d{,2}
full_year -&gt; day/month/year | day.month.year
day -&gt; digit_num | two_digit_num
month -&gt; digit_num | two_digit_num
digit_num -&gt; 0 | 1 | 2 ... |9
</code></pre>
<p>Regex is not a good idea here, because it has no &quot;context&quot; i.e. parsed characters are not aware of what have been parsed before, there is no memory. Context free grammars offer a structured way to parse strings and offer parse trees as well.</p>
<p>This is how I did it with Lark, dates are in German:
<a href=""https://duygua.github.io/blog/2018/03/28/chatbot-nlu-series-datetimeparser/"" rel=""nofollow noreferrer"">https://duygua.github.io/blog/2018/03/28/chatbot-nlu-series-datetimeparser/</a></p>
",4,2,1274,2021-10-28 16:19:39,https://stackoverflow.com/questions/69757652/what-is-the-most-accurate-way-to-detect-dates-in-text
Label custom NER in pandas dataframe,"<p>I have a dataframe with 3 columns: <code>'text', 'in', 'tar'</code> of <code>type(str, list, list)</code> respectively.</p>
<pre><code>                   text                                       in       tar
0  This is an example text that I use in order to  ...       [2]       [6]
1  Discussion: We are examining the possibility of ...       [3]     [6, 7]
</code></pre>
<p><code>in</code> and <code>tar</code> represent specific entities that I want to tag into the text, and they return the position of each found entity term in the text.</p>
<p>For example, at the 2nd row of the dataframe where <code>in = [3]</code>, I want to take the 3rd word from <code>text</code> column (i.e.: <em>&quot;are&quot;</em>) and label it as <code>&lt;IN&gt;are&lt;/IN&gt;</code>.</p>
<p>Similarly, for the same row, since <code>tar = [6,7]</code>, I also want to take the 6th and 7th word from <code>text</code> column (i.e. <em>&quot;possibility&quot;</em>, <em>&quot;of&quot;</em>) and label them as  <code>&lt;TAR&gt;possibility&lt;/TAR&gt;</code>, <code>&lt;TAR&gt;of&lt;/TAR&gt;</code>.</p>
<p>Can someone help me how to do this?</p>
","python, pandas, nlp, spacy, named-entity-recognition","<p>This is not the most optimal implementation but is worth getting inspiration.</p>
<pre><code>data = {'text': ['This is an example text that I use in order to',
                 'Discussion: We are examining the possibility of the'],
        'in': [[2], [3]],
        'tar': [[6], [6, 7]]}
df = pd.DataFrame(data)
cols = list(df.columns)[1:]
new_text = []
for idx, row in df.iterrows():
    temp = list(row['text'].split())
    for pos, word in enumerate(temp):
        for col in cols:
            if pos in row[col]:
                temp[pos] = f'&lt;{col.upper()}&gt;{word}&lt;/{col.upper()}&gt;'
    new_text.append(' '.join(temp))
df['text'] = new_text
print(df.text.to_list())
</code></pre>
<p>output:</p>
<pre><code>['This is &lt;IN&gt;an&lt;/IN&gt; example text that &lt;TAR&gt;I&lt;/TAR&gt; use in order to', 
 'Discussion: We are &lt;IN&gt;examining&lt;/IN&gt; the possibility &lt;TAR&gt;of&lt;/TAR&gt; &lt;TAR&gt;the&lt;/TAR&gt;']
</code></pre>
<p><strong>UPDATE 1</strong></p>
<p>Merging consecutive occurrence of the similar tags can be done like below:</p>
<pre><code>data = {'text': ['This is an example text that I use in order to',
                 'Discussion: We are examining the possibility of the'],
        'in': [[2], [3, 4, 5]],
        'tar': [[6], [6, 7]]}
df = pd.DataFrame(data)
cols = list(df.columns)[1:]
new_text = []
for idx, row in df.iterrows():
    temp = list(row['text'].split())
    for pos, word in enumerate(temp):
        for col in cols:
            if pos in row[col]:
                temp[pos] = f'&lt;{col.upper()}&gt;{word}&lt;/{col.upper()}&gt;'
    new_text.append(' '.join(temp))
    
df['text'] = new_text
for col in cols:
    df['text'] = df['text'].apply(lambda text:text.replace(&quot;&lt;/&quot;+col.upper()+&quot;&gt; &lt;&quot;+col.upper()+&quot;&gt;&quot;, &quot; &quot;))
print(df.text.to_list())
</code></pre>
<p>output:</p>
<pre><code>['This is &lt;IN&gt;an&lt;/IN&gt; example text that &lt;TAR&gt;I&lt;/TAR&gt; use in order to', 'Discussion: We are &lt;IN&gt;examining the possibility&lt;/IN&gt; &lt;TAR&gt;of the&lt;/TAR&gt;']
</code></pre>
",1,0,427,2021-11-02 11:14:05,https://stackoverflow.com/questions/69809450/label-custom-ner-in-pandas-dataframe
Reference other entities in entity_ruler,"<p>I'm tring to build a custom list of &quot;named entities&quot; using the entity_ruler, following also the <a href=""https://spacy.io/api/entityruler"" rel=""nofollow noreferrer"">APIs</a></p>
<p>However I'm facing a problem: can I build a named entity that reference another one also defined in the entity_ruler?</p>
<p>To make an example, let's say I want to build the entity <code>Agreement</code> as some fixed expressions, and the entity <code>AgreementDate</code> as an <code>Agreement</code> followed by another expression:
can the following snipped correctly set spacy? Because the output is not what I was expecting.</p>
<pre class=""lang-py prettyprint-override""><code>patterns = [
    {'label': 'Agreement', 'pattern': [{'LOWER': 'license agreement'}]},
    {'label': 'Agreement', 'pattern': [{'LOWER': 'agreement'}]},
    {'label': 'Agreement', 'pattern': [{'LOWER': 'commencement'}]},
    {'label': 'Agreement', 'pattern': [{'LOWER': 'parties'}]},
    {'label': 'AgreementDate', 'pattern': [{'ENT_TYPE': 'Agreement'}, {'LOWER': 'date'}]},
]
nlp = spacy.load('en_core_web_sm')
entity_ruler = nlp.add_pipe('entity_ruler', config={
    'validate': True,
    'overwrite_ents': True
})
entity_ruler.initialize(lambda: [], nlp=nlp, patterns=patterns)
for ent in nlp('''Commencement Date
license agreement date''').ents:
    print(f'{ent.text:40} {ent.label_:40}')
</code></pre>
<pre><code>Commencement                             Agreement                               
agreement                                Agreement                               
</code></pre>
","spacy, named-entity-recognition, named-entity-extraction","<p>The entity ruler patterns only match against the annotation that is set before the entity ruler component starts running, but you can do this if you move the final pattern into a second entity ruler (use a custom component name).</p>
",1,0,297,2021-11-11 09:22:29,https://stackoverflow.com/questions/69925876/reference-other-entities-in-entity-ruler
"spaCy, preparing training data: doc.char_span returning &#39;None&#39;","<p>I'm following the instructions in spaCy's documentation to prepare my own training data (<a href=""https://spacy.io/usage/training#training-data"" rel=""nofollow noreferrer"">here</a>).</p>
<p>My problem begins at this line:</p>
<pre><code>span = doc.char_span(start, end, label=label)
</code></pre>
<p>For entities which I'm labelling as an organization ('ORG'), it seems to work fine i.e. it returns a span object. However, for entities which I'm labelling as money ('MONEY'), it returns a None object.</p>
<p>Here's two examples from my training set:</p>
<pre><code>('Payments from the Guardian, Kings Place, 90 York Way, London N1 9GU, for articles:', [(18, 26, 'ORG')]) // Returns a span object for 'Guardian'

('24 July 2020, received Â£100. Hours: 1 hr. (Registered 02 February 2021)', [(24, 28, 'MONEY')]) // Returns None for '£100'
</code></pre>
<p>Note: the Â appears in the console, but it's not in the original json text file. Leaving it in in case it's somehow part of the issue</p>
<p>Does anyone please have any suggestions where I'm going wrong?</p>
<p>[I'm very new to spacy (started learning last week), so please ELI5!]</p>
<p><strong>UPDATE: As it seems the Â could be the problem, below is how I'm loading the data. How do I get rid of the Â's? (which aren't visible in the original file)</strong></p>
<pre><code>with open('training_data.json') as train_data:
    train_data_json = json.load(train_data)
</code></pre>
","python, nlp, spacy, named-entity-recognition, spacy-3","<p>You have an encoding problem when opening the file. The context for information extraction on <code>tags</code> of type <code>MONEY</code> is not working most likely do to this issue since the start of the token is not <code>£</code>.</p>
<p>It is not clear what encoding the file is using so try some of the most common ones first which are <code>utf-8</code>, <code>iso-8859-1</code>, <code>latin1</code></p>
<pre><code>with open('training_data.json', encoding='utf-8')
    # your logic here
</code></pre>
<p>replace the <code>encoding</code> with other potential candidates</p>
",2,3,5061,2021-11-15 15:06:00,https://stackoverflow.com/questions/69976538/spacy-preparing-training-data-doc-char-span-returning-none
Efficient way to find an approximate string match and replacing with predefined string,"<p>I need to build a <code>NER</code> system (<a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a>). For simplicity, I am doing it by using approximate string matching as input can contain typos and other minor modifications. I have come across some great libraries like: <a href=""https://pypi.org/project/fuzzywuzzy/"" rel=""nofollow noreferrer"">fuzzywuzzy</a> or even faster <a href=""https://github.com/maxbachmann/RapidFuzz"" rel=""nofollow noreferrer"">RapidFuzz</a>. But unfortunately I didn't find a way to return the position where the match occurs. As, for my purpose I not only need to find the match, but also I need to know where the match happened. As for <code>NER</code>, I need to replace those matches with some predefined string.</p>
<p>For example, If any one of the line is found in input string I want to replace them with the string <code>COMPANY_NAME</code>:</p>
<pre><code>google
microsoft
facebook
International Business Machine
</code></pre>
<p>Like, input: <code>S/he works at Google</code> will be transformed to <code>S/he works at COMPANY_NAME</code>.
You can safely assume that, all the input and the pattern to match are already preprocessed and most importantly they are in lower-case now. So, there is no problem with case-sensitivity.</p>
<p>Currently, I have approached with a sliding window technique. And a sliding window is passed over the input string from left to right and this window has exactly the size of the pattern we want to match. For example, when I want to match with <code>International Business Machine</code>, I run a sliding window of size <code>3</code> from left to right and try to find the best match by observing each <code>3</code> consecutive tokens at the same time with a stride of <code>1</code>. I do believe, it is not the best way to do it, also it cannot find the <em>best</em> match.</p>
<p>So, what is the efficient way to find the <strong>best</strong> possible match along with the quantification on the found match (how much they are similar) and the position of the match(es), such that we can replace them with a given fixed string (if the calculated similarity is not less than a threshold)? Obviously, a single input may contain multiple portions to be replaced, each of them will be replaced separately, like: <code>Google and Microsoft are big companies</code> will become <code>COMPANY_NAME and COMPANY_NAME are big companies</code> etc.</p>
<p>EDIT: fixed link to RapidFuzz</p>
","python, nlp, named-entity-recognition, fuzzy-search, fuzzywuzzy","<p>It seems modules <code>fuzzywuzzy</code> and <code>RapidFuzz</code> don't have function for this. You could try to use <code>process.extract()</code> or <code>process.extractOne()</code> but it would need to split text in smaller parts (ie. words) and check every part separatelly. For longer words like <code>International Business Machine</code> it would need to split in part with 3 words - so it would need even more work.</p>
<hr />
<p>I think you need rather module <a href=""https://github.com/taleinat/fuzzysearch"" rel=""nofollow noreferrer"">fuzzysearch</a></p>
<pre><code>import fuzzysearch

words = ['google', 'microsoft', 'facebook', 'International Business Machine']

text = 'Google and Microsoft are big companies like International Business Machine'

print(' text:', text)
print('---')
    
for word in sorted(words, key=len, reverse=True):
    print(' word:', word)
    
    results = fuzzysearch.find_near_matches(word, text, max_l_dist=1)
    print('found:', results)
    
    for item in reversed(results):
        text = text[:item.start] + 'COMPANY' + text[item.end:]
    print(' text:', text)
    
    print('---')
</code></pre>
<p>Result:</p>
<pre><code> text: Google and Microsoft are big companies like facebook International Business Machine
---
 word: International Business Machine
found: [Match(start=53, end=83, dist=0, matched='International Business Machine')]
 text: Google and Microsoft are big companies like facebook COMPANY
---
 word: microsoft
found: [Match(start=11, end=20, dist=1, matched='Microsoft')]
 text: Google and COMPANY are big companies like facebook COMPANY
---
 word: facebook
found: [Match(start=42, end=50, dist=0, matched='facebook')]
 text: Google and COMPANY are big companies like COMPANY COMPANY
---
 word: google
found: [Match(start=0, end=6, dist=1, matched='Google')]
 text: COMPANY and COMPANY are big companies like COMPANY COMPANY
</code></pre>
<p>If it finds many results for one word then it is better to start replacing at last position to keep other words in the same place. And this is why I use <code>reversed()</code>.</p>
<p>I would start also with the longest word/name so later it still can search shorter words like <code>Business</code>. And this is why I use <code>sorted(..., key=len, reverse=True)</code></p>
<p>But I'm not sure if it works exactly as you want. Maybe it will have problem when words are more incorrect.</p>
<hr />
<p><strong>EDIT:</strong></p>
<p>I tried to use <code>fuzzywuzzy</code> for this and created this version but only for names with single word. For <code>International Business Machine</code> it would need some other idea.</p>
<p>It split full text into words and compare words. Later replace word wich have ration <code>&gt; 80</code></p>
<pre><code>words = ['google', 'microsoft', 'facebook', 'International Business Machine']

text = 'Google and Microsoft are big companies like International Business Machine'

# ---

import fuzzywuzzy.fuzz as fuzz
#import fuzzywuzzy.process

new_words = []

for part in text.split():

    matches = []

    for word in words:
        result = fuzz.token_sort_ratio(part, word)
        matches.append([result, part, word])
        #print([result, part, word])

    matches = sorted(matches, reverse=True)

    if matches and matches[0][0] &gt; 80:
        new_words.append('COMPANY')
    else:
        new_words.append(matches[0][1])
        
print(&quot; &quot;.join(new_words))
</code></pre>
<p>Result:</p>
<pre><code>[100, 'Google', 'google']
[27, 'Google', 'microsoft']
[29, 'Google', 'facebook']
[17, 'Google', 'International Business Machine']
[0, 'and', 'google']
[0, 'and', 'microsoft']
[18, 'and', 'facebook']
[12, 'and', 'International Business Machine']
[27, 'Microsoft', 'google']
[100, 'Microsoft', 'microsoft']
[35, 'Microsoft', 'facebook']
[15, 'Microsoft', 'International Business Machine']
[22, 'are', 'google']
[17, 'are', 'microsoft']
[36, 'are', 'facebook']
[12, 'are', 'International Business Machine']
[22, 'big', 'google']
[17, 'big', 'microsoft']
[18, 'big', 'facebook']
[12, 'big', 'International Business Machine']
[27, 'companies', 'google']
[33, 'companies', 'microsoft']
[24, 'companies', 'facebook']
[26, 'companies', 'International Business Machine']
[40, 'like', 'google']
[15, 'like', 'microsoft']
[17, 'like', 'facebook']
[18, 'like', 'International Business Machine']
[21, 'International', 'google']
[27, 'International', 'microsoft']
[19, 'International', 'facebook']
[60, 'International', 'International Business Machine']
[14, 'Business', 'google']
[24, 'Business', 'microsoft']
[12, 'Business', 'facebook']
[42, 'Business', 'International Business Machine']
[15, 'Machine', 'google']
[25, 'Machine', 'microsoft']
[40, 'Machine', 'facebook']
[38, 'Machine', 'International Business Machine']
COMPANY and COMPANY are big companies like International Business Machine
</code></pre>
<hr />
<p><strong>EDIT:</strong></p>
<p>Second version which check also names with many words</p>
<pre><code>all_names = ['google', 'microsoft', 'facebook', 'International Business Machine']

text = 'Google and Microsoft are big companies like International Business Machine'

# ---

import fuzzywuzzy.fuzz as fuzz


for name in all_names:

    length = len(name.split(' ')) # how many words has name 
    print('name length:', length, '|', name)

    words = text.split()  # split text into words

    # compare name with all words in text
    
    matches = []
    
    for index in range(0, len(words)-length+1):
        # join words if name has more then 1 word
        part = &quot; &quot;.join(words[index:index+length])
        #print('part:', part)
        
        result = fuzz.token_sort_ratio(part, name)
        matches.append([result, name, part, [index, index+length]])

        print([result, name, part, [index, index+length]])
        
    # reverse to start at last position
    matches = list(reversed(matches))

    max_match = max(x[0] for x in matches)
    print('max match:', max_match)

    # replace
    if max_match &gt; 80:
        for match in matches:
            if  match[0] == max_match:
                idx = match[3]  
                words = words[:idx[0]] + ['COMPANY'] + words[idx[1]:]

    text = &quot; &quot;.join(words)
    print('text:', text)
    print('---')
</code></pre>
<p>Result:</p>
<pre><code>ame length: 1 | google
[100, 'google', 'Google', [0, 1]]
[0, 'google', 'and', [1, 2]]
[27, 'google', 'Microsoft', [2, 3]]
[22, 'google', 'are', [3, 4]]
[22, 'google', 'big', [4, 5]]
[27, 'google', 'companies', [5, 6]]
[40, 'google', 'like', [6, 7]]
[21, 'google', 'International', [7, 8]]
[14, 'google', 'Business', [8, 9]]
[15, 'google', 'Machine', [9, 10]]
max match: 100
text: COMPANY and Microsoft are big companies like International Business Machine
---
name length: 1 | microsoft
[25, 'microsoft', 'COMPANY', [0, 1]]
[0, 'microsoft', 'and', [1, 2]]
[100, 'microsoft', 'Microsoft', [2, 3]]
[17, 'microsoft', 'are', [3, 4]]
[17, 'microsoft', 'big', [4, 5]]
[33, 'microsoft', 'companies', [5, 6]]
[15, 'microsoft', 'like', [6, 7]]
[27, 'microsoft', 'International', [7, 8]]
[24, 'microsoft', 'Business', [8, 9]]
[25, 'microsoft', 'Machine', [9, 10]]
max match: 100
text: COMPANY and COMPANY are big companies like International Business Machine
---
name length: 1 | facebook
[27, 'facebook', 'COMPANY', [0, 1]]
[18, 'facebook', 'and', [1, 2]]
[27, 'facebook', 'COMPANY', [2, 3]]
[36, 'facebook', 'are', [3, 4]]
[18, 'facebook', 'big', [4, 5]]
[24, 'facebook', 'companies', [5, 6]]
[17, 'facebook', 'like', [6, 7]]
[19, 'facebook', 'International', [7, 8]]
[12, 'facebook', 'Business', [8, 9]]
[40, 'facebook', 'Machine', [9, 10]]
max match: 40
text: COMPANY and COMPANY are big companies like International Business Machine
---
name length: 3 | International Business Machine
[33, 'International Business Machine', 'COMPANY and COMPANY', [0, 3]]
[31, 'International Business Machine', 'and COMPANY are', [1, 4]]
[31, 'International Business Machine', 'COMPANY are big', [2, 5]]
[34, 'International Business Machine', 'are big companies', [3, 6]]
[38, 'International Business Machine', 'big companies like', [4, 7]]
[69, 'International Business Machine', 'companies like International', [5, 8]]
[88, 'International Business Machine', 'like International Business', [6, 9]]
[100, 'International Business Machine', 'International Business Machine', [7, 10]]
max match: 100
text: COMPANY and COMPANY are big companies like COMPANY
</code></pre>
<hr />
<p><strong>EDIT:</strong></p>
<p>Version with <code>fuzzywuzzy.process</code></p>
<p>This time I don't have positions and I simply use standard <code>text.replace(item[0], 'COMPANY')</code>.</p>
<p>I think in most situations it will work correctly and it doesn't need better method.</p>
<p>This time I check it on text with mistakes:</p>
<pre><code>'Gogle and Mikro-Soft are big companies like Fasebok and Internat. Businnes Machin'
</code></pre>
<pre><code>
all_names = ['google', 'microsoft', 'facebook', 'International Business Machine']

text = 'Google and Microsoft are big companies like Facebook and International Business Machine'

# text with mistakes
text = 'Gogle and Mikro-Soft are big companies like Fasebok and Internat. Businnes Machin'

# ---

import fuzzywuzzy.process
#import fuzzywuzzy.fuzz

for name in sorted(all_names, key=len, reverse=True):
    lenght = len(name.split())

    words = text.split()
    words = [&quot; &quot;.join(words[i:i+lenght]) for i in range(0, len(words)-lenght+1)]
    #print(words)

    #result = fuzzywuzzy.process.extractBests(name, words, scorer=fuzzywuzzy.fuzz.token_sort_ratio, score_cutoff=80)
    result = fuzzywuzzy.process.extractBests(name, words, score_cutoff=80)
    print(name, result)

    for item in result:
        text = text.replace(item[0], 'COMPANY')

print(text)
</code></pre>
",3,1,7504,2021-11-21 03:54:17,https://stackoverflow.com/questions/70051704/efficient-way-to-find-an-approximate-string-match-and-replacing-with-predefined
What is the difference between 2 strings for SpaCy NER?,"<p>I need to find russian names and surnames in english text. I tried SpaCy NER, but it matches only english names (for instance, John Brandon), but not russian (like Vitaliy Ivanov).</p>
<p>I use the transliterate python library to translit english text to russian and then apply russian Spacy nlp model to get names by NER.</p>
<p>But i get different result for the string 'Ivanov Vitaliy' and the same string gotten by transliteration.</p>
<p>The code is as follows:</p>
<pre><code>from transliterate import translit, get_available_language_codes #pip install transliterate
from transliterate.discover import autodiscover
autodiscover()

from transliterate.base import TranslitLanguagePack, registry

class ExampleLanguagePack(TranslitLanguagePack):
    language_code = &quot;example&quot;
    language_name = &quot;Example&quot;
    mapping = (
             u'abvgdeziyklmnprstufhABVGDEZIYKLMNPRSTUFH', 
             u'абвгдезийклмнпрстуфхАБВГДЕЗИЙКЛМНПРСТУФХ',
             )
             
    pre_processor_mapping = {
             u&quot;kh&quot;: u&quot;x&quot;,
             u'ye': u'е',
             u'yo': u'ё',
             u'zh': u'ж', 
             u'ts': u'ц',
             u'ch': u'ч',
             u'sh': u'ш',
             u'shch': u'щ',
             u'sch': u'щ',
             #u'y': u'ы',
             u'e': u'э',
             u'kh': u'х',
             u'yu': u'ю',
             u'iu':u'ю',
             u'ya': u'я',
             u'ia': u'я',
             }     

registry.register(ExampleLanguagePack)
print(get_available_language_codes())

ru_trans=translit('Ivanov Vitaliy','example')
print (ru_trans)

name_ru=nlp_ru(ru_trans) 
#use NER to extract names
 
person=[entity.text for entity in name_ru.ents if entity.label_==&quot;PER&quot;] 
print(person)

</code></pre>
<p>Got ['Виталий']</p>
<p>Another case:</p>
<pre><code>name_ru=nlp_ru('Иванов Виталий')

#name_ru=nlp_ru(ru_trans) 
#use NER to extract names
 
person=[entity.text for entity in name_ru.ents if entity.label_==&quot;PER&quot;] 
print(person)
</code></pre>
<p>The result is ['Иванов Виталий']</p>
<p>I checked the type of the object gotten after translit:</p>
<pre><code>type(ru_trans) 
</code></pre>
<p>str</p>
<p>The ASCII codes of the letter in these two strings are the same.</p>
<p>What can be the reason for getting different results of NER on these two strings?</p>
","python, spacy, named-entity-recognition, transliteration","<p>You forgot to define the <code>o</code> letter mapping.</p>
<p>Here is the fix:</p>
<pre class=""lang-py prettyprint-override""><code>mapping = (
    'abvgdeziyklmnoprstufhABVGDEZIYKLMNOPRSTUFH', 
    'абвгдезийклмнопрстуфхАБВГДЕЗИЙКЛМНОПРСТУФХ',
)
</code></pre>
<p>Note I added both upper- and lowercase <code>o</code> letter mappings.</p>
",1,2,133,2021-11-22 18:30:20,https://stackoverflow.com/questions/70070719/what-is-the-difference-between-2-strings-for-spacy-ner
how to train spacy model which treats &amp; and &#39;and&#39; similar for accurate prediction,"<p>i've trained a spacy NER model which has text mapped to Company entity during training as -</p>
<pre><code>John &amp; Doe &amp; One pvt ltd -&gt; Company
</code></pre>
<p>Now in some cases I find, if giving sentence as below during prediction is being categorized as Others-</p>
<pre><code>John and Doe and One pvt ltd -&gt; Other
</code></pre>
<p>What should be done to overcome this problem where we have cases of &quot;&amp; == and&quot; and &quot;v == vs == versus&quot; etc cases to be understood by the model has same meaning ?</p>
","python, nlp, spacy, named-entity-recognition","<p>For these kinds of cases you want to add lexeme norms or token norms.</p>
<pre class=""lang-py prettyprint-override""><code># lexeme norm
nlp.vocab[&quot;and&quot;].norm_ = &quot;&amp;&quot;
# token norm
doc[1].norm_ = &quot;&amp;&quot;
</code></pre>
<p>The statistical models all use <code>token.norm</code> instead of <code>token.orth</code> as a feature by default. You can set <code>token.norm_</code> for an individual token in a doc (sometimes you might want normalizations that depend on the context), or set <code>nlp.vocab[&quot;word&quot;].norm_</code> as the default for any token that doesn't have an individual <code>token.norm</code> set.</p>
<p>If you add lexeme norms to the vocab and save the model with <code>nlp.to_disk</code>, the lexeme norms are included in the saved model.</p>
",2,0,182,2021-11-24 13:26:59,https://stackoverflow.com/questions/70096946/how-to-train-spacy-model-which-treats-and-and-similar-for-accurate-predictio
Why are non-appearing classes shown in the classification report?,"<p>I' m working on NER and using <code>sklearn.metrics.classification_report</code> to caculate micro and macro f1 score. It printed a table like:</p>
<pre><code>              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000         0
           3     0.0000    0.0000    0.0000         0
           4     0.8788    0.9027    0.8906       257
           5     0.9748    0.9555    0.9650      1617
           6     0.9862    0.9888    0.9875      1156
           7     0.9339    0.9138    0.9237       835
           8     0.8542    0.7593    0.8039       216
           9     0.8945    0.8575    0.8756       702
          10     0.9428    0.9382    0.9405      1668
          11     0.9234    0.9139    0.9186      1661

    accuracy                         0.9285      8112
   macro avg     0.7388    0.7230    0.7305      8112
weighted avg     0.9419    0.9285    0.9350      8112
</code></pre>
<p>It's obvious that the predicted labels have '0' or '3', but there's no '0' or '3' in true labels. Why the classification report will show these two classes which don't have any samples? And how to do to prevent &quot;0-support&quot; classes from being shown. It seems that these two classes have a great impact to macro f1 score.</p>
","scikit-learn, named-entity-recognition","<p>You can use the following snippet to ensure that all labels in the classification report are present in <code>y_true</code> labels:</p>
<pre><code>from sklearn.metrics import classification_report
y_true = [0, 1, 2, 2, 2, 2]
y_pred = [0, 0, 2, 2, 1, 42]
print(classification_report(y_true, y_pred, labels=np.unique(y_true)))
</code></pre>
<p>Which output:</p>
<pre><code>              precision    recall  f1-score   support

           0       0.50      1.00      0.67         1
           1       0.00      0.00      0.00         1
           2       1.00      0.50      0.67         4

   micro avg       0.60      0.50      0.55         6
   macro avg       0.50      0.50      0.44         6
weighted avg       0.75      0.50      0.56         6
</code></pre>
<p>As you see the label <code>42</code> present in the prediction is not shown as it has no support in <code>y_true</code>.</p>
",2,1,861,2021-12-01 08:16:09,https://stackoverflow.com/questions/70180929/why-are-non-appearing-classes-shown-in-the-classification-report
Spacy Doc Bin Creation for NER,"<p>I am trying to create doc bin object for Custom NER. I have around 100 tagged datas for training (Just as a start)</p>
<p>I am getting skipping entity message while creation.</p>
<pre><code> 54%|██████████████████████▊                   | 43/79 [00:00&lt;00:00, 216.47it/s]

Skipping entity
Skipping entity
Skipping entity
Skipping entity
Skipping entity
Skipping entity
Skipping entity
Skipping entity

100%|██████████████████████████████████████████| 79/79 [00:00&lt;00:00, 251.36it/s]
</code></pre>
<p>My Doubts are :</p>
<ol>
<li>What is the meaning of this skipping entity (How can the span be None) .</li>
<li>Is this a serious issue.</li>
<li>How this can affect the performance and how we can overcome this ?</li>
<li>If 100 datas are totally available, what ratio we can take for training and evaluating purpose ?</li>
</ol>
<p><strong>Code</strong></p>
<pre><code>import pandas as pd
import os
from tqdm import tqdm
import spacy
from spacy.tokens import DocBin
nlp = spacy.blank(&quot;en&quot;) # load a new spacy model

db = DocBin() # create a DocBin object

for text, annot in tqdm(train_data): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        if span is None:
            print(&quot;Skipping entity&quot;)
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(&quot;./train.spacy&quot;) # save the docbin object
</code></pre>
","python, spacy, named-entity-recognition","<p>The span can be <code>None</code> if <code>alignment_mode=&quot;contract&quot;</code> results in no marked tokens. So if you had a token <code>good</code> and tried to mark <code>oo</code> as a span with <code>contract</code>, then it would return <code>None</code>. With <code>expand</code>, you should always end up with at least one token.</p>
",3,2,1072,2021-12-01 12:14:28,https://stackoverflow.com/questions/70184009/spacy-doc-bin-creation-for-ner
Return all possible entity types from spaCy model?,"<p>Is there a method to extract all possible named entity types from a model in spaCy? You can manually figure it out by running on sample text, but I imagine there is a more programmatic way to do this?
For example:</p>
<pre><code>import spacy
model=spacy.load(&quot;en_core_web_sm&quot;)
model.*returns_entity_types*
</code></pre>
","python, nlp, spacy, named-entity-recognition","<p>The statistical pipeline components like <code>ner</code> provide their labels under <code>.labels</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
nlp.get_pipe(&quot;ner&quot;).labels
</code></pre>
",23,12,10092,2021-12-01 13:33:40,https://stackoverflow.com/questions/70185150/return-all-possible-entity-types-from-spacy-model
Is there a way to add each result to a row of the dataframe?,"<p>I'm working on a method to annotate a text and currently building a function to add each text and its pos to a row on the dataframe.</p>
<p>Text:   pos :</p>
<p>apple   PROPN
be      AUX
look    VERB</p>
<pre><code>import spacy
import pandas as pd

df = pd.DataFrame(columns = ['Text', 'pos'])

def annotate(text):
    nlp = spacy.load(&quot;en_core_web_sm&quot;)
    doc = nlp(text)

    for token in doc:
        print(token.text, token.pos_) 
        df = df.append({'Text' : 'token.text', 'pos' : 'token.pos_'},  ignore_index = True)

annotate('Apple is looking at buying U.K. startup for $1 billion')
</code></pre>
","python, pandas, nlp, spacy, named-entity-recognition","<p>Try collecting the data, THEN creating the dataframe.  In general that will run more efficiently than appending rows to an existing dataframe:</p>
<pre><code>def annotate(text):
    nlp = spacy.load(&quot;en_core_web_sm&quot;)
    doc = nlp(text)

    rows = []
    for token in doc:
        print(token.text, token.pos_)
        rows.append([token.text, token.pos])
    df = pd.DataFrame(rows, columns=['Text', 'pos'])
    return df
</code></pre>
<p>then call it using:</p>
<pre><code>df = annotate('Apple is looking at buying U.K. startup for $1 billion')
</code></pre>
",0,0,122,2021-12-10 22:06:10,https://stackoverflow.com/questions/70310883/is-there-a-way-to-add-each-result-to-a-row-of-the-dataframe
Train Spacy model with larger-than-RAM dataset,"<p>I asked <a href=""https://stackoverflow.com/questions/70367348/spacy-alignment-differences-when-training-with-docbin-vs-custom-data-reading-and"">this question</a> to better understand some of the nuances between training Spacy models with <code>DocBin</code>s serialized to disk, versus loading <code>Example</code> instances via custom data loading function. The goal was to train a Spacy NER model with more data that can fit into RAM (or at least some way to avoid loading the entire file into RAM). Though the custom data loader seemed like one specific way to accomplish this, I am writing this question to ask more generally:</p>
<p>How can one train a Spacy model without loading the entire training data set file during training?</p>
","python-3.x, spacy, named-entity-recognition, spacy-3","<p>Your only options are using a custom data loader or setting <code>max_epochs = -1</code>. See <a href=""https://spacy.io/usage/training#custom-code-readers-batchers"" rel=""nofollow noreferrer"">the docs</a>.</p>
",2,0,707,2021-12-19 17:17:38,https://stackoverflow.com/questions/70413682/train-spacy-model-with-larger-than-ram-dataset
BILOU Tagging scheme for multi-word entities in Spacy&#39;s NER,"<p>I am working on building a custom NER using spacy for recognizing new entities apart from spacy's NER. Now I have my training data to be tagged and added using spacy.Example. I am using the BILOU scheme. My doubt is that I have entities which have more than 3 words. For example:</p>
<pre><code>Housing Development Finance Corporation reported heavy losses in the past quarter.
</code></pre>
<p>I want to tag Housing Development Finance Corporation as a single Entity using the BILOU scheme. Something like</p>
<pre><code>'Housing'     B-Entity
'Development' I-Entity
'Finance'     I-Entity
'Corporation' L-Entity
</code></pre>
<p>Is this tagging correct?How will the model interpret the order within each entity?Any guidance would be much appreciated.</p>
","python, nlp, named-entity-recognition, spacy-3","<p>The tagging you have is correct while all outside words which are not entities would be marked with <code>O</code>.</p>
<p>The model will be depending on the same order within the entity to match it towards a previous entity of the same name, ex:</p>
<pre><code>'Housing'     B-Entity
'Development' I-Entity
'Finance'     I-Entity
'Corporation' L-Entity
</code></pre>
<p>and</p>
<pre><code>'Housing'     B-Entity
'Finance'     I-Entity
'Development' I-Entity
'Corporation' L-Entity
</code></pre>
<p>will not be linked as the same entity, although if you want this to be the case, you could look into a classification model to classify your foud entities towards your previously known entities and work from there.</p>
",1,2,472,2021-12-27 06:45:36,https://stackoverflow.com/questions/70492407/bilou-tagging-scheme-for-multi-word-entities-in-spacys-ner
Do I need to do any text cleaning for Spacy NER?,"<p>I am new to <code>NER</code> and <code>Spacy</code>. Trying to figure out what, if any, text cleaning needs to be done. Seems like some examples I've found trim the leading and trailing whitespace and then muck with the start/stop indexes. I saw one example where the guy did a bunch of cleaning and his accuracy was really bad because all the indexes were messed up.</p>
<p>Just to clarify, the dataset was annotated with DataTurks, so you get json like this:</p>
<pre><code>        &quot;Content&quot;: &lt;original text&gt;
        &quot;label&quot;: [
            &quot;Skills&quot;
        ],
        &quot;points&quot;: [
            {
                &quot;start&quot;: 1295,
                &quot;end&quot;: 1621,
                &quot;text&quot;: &quot;\n• Programming language...
</code></pre>
<p>So by &quot;mucking with the indexes&quot;, I mean, if you strip off the leading <code>\n</code>, you need to update the start index, so it's still aligned properly.</p>
<p>So that's really the question, if I start removing characters from the beginning, end or middle, I need to apply the rule to the content attribute and adjust start/end indexes to match, no? I'm guessing an obvious &quot;yes&quot; :), so I was wondering how much cleaning needs to be done.</p>
<p>So you would remove the <code>\n</code>s, bullets, leading / trailing whitespace, but leave standard punctuation like commas, periods, etc?</p>
<p>What about stuff like lowercasing, stop words, lemmatizing, etc?</p>
<p>One concern I'm seeing with a few samples I've looked at, is the start/stop indexes do get thrown off by the cleaning they do because you kind of need to update EVERY annotation as you remove characters to keep them in sync.</p>
<p>I.e.</p>
<pre><code>A 0 -&gt; 100
B 101 -&gt; 150
</code></pre>
<p>if I remove a <code>char</code> at <code>position 50</code>, then I need to adjust <code>B to 100 -&gt; 149</code>.</p>
","python, spacy, named-entity-recognition","<p>First, spaCy does no transformation of the input - it takes it literally as-is and preserves the format. So you don't lose any information when you provide text to spaCy.</p>
<p>That said, input to spaCy with the pretrained pipelines will work best if it is in natural sentences with no weird punctuation, like a newspaper article, because that's what spaCy's training data looks like.</p>
<p>To that end, you should remove meaningless white space (like newlines, leading and trailing spaces) or formatting characters (maybe a line of <code>----</code>?), but that's about all the cleanup you have to do. The spaCy training data won't have bullets, so they might get some weird results, but I would leave them in to start. (Also, bullets are obviously printable characters - maybe you mean non-ASCII?)</p>
<p>I have no idea what you mean by &quot;muck with the indexes&quot;, but for some older NLP methods it was common to do more extensive preprocessing, like removing stop words and lowercasing everything. Doing that will make things worse with spaCy because it uses the information you are removing for clues, just like a human reader would.</p>
<p>Note that you can train your own models, in which case they'll learn about the kind of text you show them. In that case you can get rid of preprocessing entirely, though for actually meaningless things like newlines / leading and following spaces you might as well remove them anyway.</p>
<hr />
<p>To address your new info briefly...</p>
<p>Yes, character indexes for NER labels must be updated if you do preprocessing. If they aren't updated they aren't usable.</p>
<p>It looks like you're trying to extract &quot;skills&quot; from a resume. That has many bullet point lists. The spaCy training data is newspaper articles, which don't contain any lists like that, so it's hard to say what the right thing to do is. I don't think the bullets matter much, but you can try removing or not removing them.</p>
<blockquote>
<p>What about stuff like lowercasing, stop words, lemmatizing, etc?</p>
</blockquote>
<p>I already addressed this, but <strong>do not do this</strong>. This was historically common practice for NLP models, but for modern neural models, including spaCy, it is actively unhelpful.</p>
",5,7,2595,2021-12-28 03:15:39,https://stackoverflow.com/questions/70502457/do-i-need-to-do-any-text-cleaning-for-spacy-ner
"Spacy - problem during the training of my model, it seems to block at epoch 0","<p>I am training my named entity recogniser but I have the impression that it blocks at epoch 0. I have already done several trainings and I have never had this problem. Does anyone have any tips? I am attaching a screenshot of my terminal. Many thanks!!</p>
<p><a href=""https://i.sstatic.net/WmpiD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WmpiD.png"" alt=""enter image description here"" /></a></p>
","python-3.x, spacy, named-entity-recognition","<p>It's likely you just have too much data and your training is slow.</p>
<p>How much data do you have? How much RAM? What does <code>spacy debug data</code> show?</p>
",3,1,696,2022-01-04 12:50:12,https://stackoverflow.com/questions/70579115/spacy-problem-during-the-training-of-my-model-it-seems-to-block-at-epoch-0
NLP Update cannot be used with tuples after spacy 3 update,"<p>here's my code for training a pre existing model. I'm receiving this error message due to updates in SpaCy but I couldn't solve the problem.</p>
<p>ValueError: [E989] <code>nlp.update()</code> was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of <code>(text, annotation)</code> tuples.</p>
<pre><code>def train_spacy(train_data, labels, iterations, dropout = 0.5, display_freq = 1):
    
 
    valid_f1scores=[]
    test_f1scores=[]
    nlp = spacy.load(&quot;en_core_web_md&quot;)
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner)
    else:
        ner = nlp.get_pipe(&quot;ner&quot;)
        
    #add entity labels to the NER pipeline
    for i in labels:
        ner.add_label(i)
        
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):
        optimizer = nlp.create_optimizer()
        for itr in range(iterations):
            random.shuffle(train_data) #shuffle the train data before each iteration
            losses = {}
            batches = minibatch(train_data, size = compounding(16.0, 64.0, 1.5))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                texts,
                annotations,
                drop = dropout,
                sgd = optimizer,
                losses = losses)
            #if itr % display_freq == 0:
            # print(&quot;Iteration {} Loss: {}&quot;.format(itr + 1, losses))
            scores = evaluate(nlp, VALID_DATA)
            valid_f1scores.append(scores[&quot;textcat_f&quot;])
            print('====================================')
            print('Iteration = ' +str(itr))
            print('Losses = ' +str(losses))
            print('====================VALID DATA====================')
            
            print('F1-score = ' +str(scores[&quot;textcat_f&quot;]))
            print('Precision = ' +str(scores[&quot;textcat_p&quot;]))
            print('Recall = ' +str(scores[&quot;textcat_r&quot;]))
            scores = evaluate(nlp,TEST_DATA)
            test_f1scores.append(scores[&quot;textcat_f&quot;])
            print('====================TEST DATA====================')
            print('F1-score = ' +str(scores[&quot;textcat_f&quot;]))
            print('Precision = ' +str(scores[&quot;textcat_p&quot;]))
            print('Recall = ' +str(scores[&quot;textcat_r&quot;]))
            print('====================================')
        
        return nlp,valid_f1scores,test_f1scores

#train and save the NER model
ner,valid_f1scores,test_f1scores = train_spacy(TRAIN_DATA, LABELS, 20)
ner.to_disk(&quot;C:\\NERdata\\spacy_example&quot;)
</code></pre>
","python, nlp, spacy, named-entity-recognition","<p>Migration from v2 to v3 for this kind of training loop is documented here: <a href=""https://spacy.io/usage/v3#migrating-training-python"" rel=""nofollow noreferrer"">https://spacy.io/usage/v3#migrating-training-python</a>.</p>
<p>Here's what an updated loop looks like (copied from the link above):</p>
<pre class=""lang-py prettyprint-override""><code>TRAIN_DATA = [
    (&quot;Who is Shaka Khan?&quot;, {&quot;entities&quot;: [(7, 17, &quot;PERSON&quot;)]}),
    (&quot;I like London.&quot;, {&quot;entities&quot;: [(7, 13, &quot;LOC&quot;)]}),
]
examples = []
for text, annots in TRAIN_DATA:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
for i in range(20):
    random.shuffle(examples)
    for batch in minibatch(examples, size=8):
        nlp.update(batch)
</code></pre>
<p>Note that it's not recommended to use this kind of training loop in v3, but <code>spacy train</code> with a config instead.</p>
",3,2,2124,2022-01-07 17:07:01,https://stackoverflow.com/questions/70624780/nlp-update-cannot-be-used-with-tuples-after-spacy-3-update
How to resume training in spacy transformers for NER,"<p>I have created a spacy transformer model for named entity recognition. Last time I trained till it reached 90% accuracy and I also have a <code>model-best</code> directory from where I can load my trained model for predictions. But now I have some more data samples and I wish to resume training this spacy transformer. I saw that we can do it by changing the <code>config.cfg</code> but clueless about 'what to change?'</p>
<p>This is my <code>config.cfg</code> after running <code>python -m spacy init fill-config ./base_config.cfg ./config.cfg</code>:</p>
<pre><code>[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = &quot;pytorch&quot;
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;transformer&quot;,&quot;ner&quot;]
batch_size = 128
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.ner_scorer.v1&quot;}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = false
nO = null

[components.ner.model.tok2vec]
@architectures = &quot;spacy-transformers.TransformerListener.v1&quot;
grad_factor = 1.0
pooling = {&quot;@layers&quot;:&quot;reduce_mean.v1&quot;}
upstream = &quot;*&quot;

[components.transformer]
factory = &quot;transformer&quot;
max_batch_items = 4096
set_extra_annotations = {&quot;@annotation_setters&quot;:&quot;spacy-transformers.null_annotation_setter.v1&quot;}

[components.transformer.model]
@architectures = &quot;spacy-transformers.TransformerModel.v3&quot;
name = &quot;roberta-base&quot;
mixed_precision = false

[components.transformer.model.get_spans]
@span_getters = &quot;spacy-transformers.strided_spans.v1&quot;
window = 128
stride = 96

[components.transformer.model.grad_scaler_config]

[components.transformer.model.tokenizer_config]
use_fast = true

[components.transformer.model.transformer_config]

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
accumulate_gradient = 3
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null

[training.batcher]
@batchers = &quot;spacy.batch_by_padded.v1&quot;
discard_oversize = true
size = 2000
buffer = 256
get_length = null

[training.logger]
@loggers = &quot;spacy.ConsoleLogger.v1&quot;
progress_bar = false

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001

[training.optimizer.learn_rate]
@schedules = &quot;warmup_linear.v1&quot;
warmup_steps = 250
total_steps = 20000
initial_rate = 0.00005

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>As you can see there is a 'vectors' parameter under <code>[initialize]</code> so I tried giving vectors from 'model-best' like this:</p>
<p><a href=""https://i.sstatic.net/efucM.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/efucM.png"" alt=""enter image description here"" /></a></p>
<p>But it gave me this error</p>
<pre><code>OSError: [E884] The pipeline could not be initialized because the vectors could not be found at './model-best/ner'. If your pipeline was already initialized/trained before, call 'resume_training' instead of 'initialize', or initialize only the components that are new.
</code></pre>
<p>For those who are wondering that I have been given the wrong path. No, that directory exists. You can see directory structure,</p>
<p><a href=""https://i.sstatic.net/BoOiX.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/BoOiX.png"" alt=""enter image description here"" /></a></p>
<p>So, please guide me on how I can successfully resume the training from previous weights.</p>
<p>Thank you!</p>
","deep-learning, spacy, named-entity-recognition, transformer-model","<p>The vectors setting is not related to the <code>transformer</code> or what you're trying to do.</p>
<p>In the new config, you want to use the <code>source</code> option to load the components from the existing pipeline. You would modify the <code>[component]</code> blocks to contain only the <code>source</code> setting and no other settings:</p>
<pre><code>[components.ner]
source = &quot;/path/to/model-best&quot;

[components.transformer]
source = &quot;/path/to/model-best&quot;
</code></pre>
<p>See: <a href=""https://spacy.io/usage/training#config-components"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#config-components</a></p>
",4,5,2658,2022-01-19 14:44:33,https://stackoverflow.com/questions/70772641/how-to-resume-training-in-spacy-transformers-for-ner
NER Classification Deberta Tokenizer error : You need to instantiate DebertaTokenizerFast,"<p>I'm trying to perform a NER Classification task using Deberta, but I'm stacked with a Tokenizer error. This is my code (my input sentence must be splitted word by word by &quot;,:):</p>
<pre><code>from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

import transformers
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)

tokenizer([&quot;Hello&quot;, &quot;,&quot;, &quot;this&quot;, &quot;is&quot;, &quot;one&quot;, &quot;sentence&quot;, &quot;split&quot;, &quot;into&quot;, &quot;words&quot;, &quot;.&quot;])
</code></pre>
<p>I have this results:</p>
<pre><code>{'input_ids': [[1, 31414, 2], [1, 6, 2], [1, 9226, 2], [1, 354, 2], [1, 1264, 2], [1, 19530, 4086, 2], [1, 44154, 2], [1, 12473, 2], [1, 30938, 2], [1, 4, 2]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]} 
</code></pre>
<p>Then I proceed but I have this error:</p>
<pre><code>tokenized_input = tokenizer(example[&quot;tokens&quot;])
tokens = tokenizer.convert_ids_to_tokens(tokenized_input[&quot;input_ids&quot;])
print(tokens)
</code></pre>
<pre><code>TypeError: int() argument must be a string, a bytes-like object or a number, not 'list'
</code></pre>
<p>And I think the reasons is that I need to have the results of the token in the following format(that is not possible because I have the sentence splitted by &quot;,&quot;:</p>
<pre><code>tokenizer(&quot;Hello, this is one sentence!&quot;)

{'input_ids': [1, 31414, 6, 42, 16, 65, 3645, 328, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
<p>So I tried in this both way but I'm stack and don't know how to do. There are very few documentation online about Deberta.</p>
<pre><code>tokenizer([&quot;Hello&quot;, &quot;,&quot;, &quot;this&quot;, &quot;is&quot;, &quot;one&quot;, &quot;sentence&quot;, &quot;split&quot;, &quot;into&quot;, &quot;words&quot;, &quot;.&quot;], is_split_into_words=True)

AssertionError: You need to instantiate DebertaTokenizerFast with add_prefix_space=True to use it with pretokenized inputs.
</code></pre>
<pre><code>tokenizer([&quot;Hello&quot;, &quot;,&quot;, &quot;this&quot;, &quot;is&quot;, &quot;one&quot;, &quot;sentence&quot;, &quot;split&quot;, &quot;into&quot;, &quot;words&quot;, &quot;.&quot;], is_split_into_words=True,add_prefix_space=True)
</code></pre>
<p>And the error is still the same.
Thank you so much !</p>
","python, tokenize, bert-language-model, named-entity-recognition, roberta","<p>Lets try this:</p>
<pre><code>input_ids = [1, 31414, 6, 42, 16, 65, 3645, 328, 2]
input_ids  = ','.join(map(str, input_ids ))


input_ids = [&quot;Hello&quot;, &quot;,&quot;, &quot;this&quot;, &quot;is&quot;, &quot;one&quot;, &quot;sentence&quot;, &quot;split&quot;, &quot;into&quot;, &quot;words&quot;, &quot;.&quot;]
input_ids  = ','.join(map(str, input_ids ))
input_ids
</code></pre>
",1,1,633,2022-01-21 09:42:10,https://stackoverflow.com/questions/70799226/ner-classification-deberta-tokenizer-error-you-need-to-instantiate-debertatoke
How to get a description for each Spacy NER entity?,"<p>I am using <a href=""https://spacy.io/api/entityrecognizer"" rel=""noreferrer"">Spacy NER model</a> to extract from a text, some named entities relevant to my problem, such us DATE, TIME, GPE among others.</p>
<p>For example, I need to recognize the Time Zone in the following sentence:</p>
<pre><code>&quot;Australian Central Time&quot;
</code></pre>
<p>With Spacy model <code>en_core_web_lg</code>, I got the following result:</p>
<pre><code>doc = nlp(&quot;Australian Central Time&quot;)
print([(ent.label_, ent.text) for ent in doc.ents])
    
&gt;&gt; [('NORP', 'Australian')]
</code></pre>
<p><strong>My problem is</strong>: I don't have a clear idea about what exactly means entity <code>NORP</code> and more general what exactly means each Spacy NER entity (leaving aside the intuitive values of course).</p>
<p>I found the following snippet to get the complete entities list, but after that I'm blocked:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_lg&quot;)
nlp.get_pipe(&quot;ner&quot;).labels
</code></pre>
<p>I'm pretty new to using Spacy NLP and didn't find what I'm looking for on the official documentation, so any help will be appreciated!</p>
<p>BTW, I'm using Spacy version <code>3.2.1</code>.</p>
","spacy, named-entity-recognition, spacy-3","<p>Most labels have definitions you can access using <code>spacy.explain(label)</code>.</p>
<p>For <code>NORP</code>: &quot;Nationalities or religious or political groups&quot;</p>
<p>For more details you would need to look into the annotation guidelines for the resources listed in the model documentation under <a href=""https://spacy.io/models/"" rel=""noreferrer"">https://spacy.io/models/</a>.</p>
",7,7,8868,2022-01-24 15:02:33,https://stackoverflow.com/questions/70835924/how-to-get-a-description-for-each-spacy-ner-entity
How to extract sentences from one text with only 1 named entity using spaCy?,"<p>I have a list of sentences and I want to be able to append only the sentences with <strong>1 &quot;PERSON&quot; named entity</strong> using spaCy. The code I used was as follows:</p>
<pre><code>test_list = []
for item in sentences: #for each sentence in 'sentences' list
  for ent in item.ents: #for each entity in the sentence's entities 
    if len(ent in item.ents) == 1: #if there is only one entity
      if ent.label_ == &quot;PERSON&quot;: #and if the entity is a &quot;PERSON&quot;
        test_list.append(item) #put the sentence into 'test_list'
</code></pre>
<p>But then I get:</p>
<pre><code>TypeError: object of type 'bool' has no len()
</code></pre>
<p>Am I doing this wrong? How exactly would I complete this task?</p>
","python, nlp, spacy, named-entity-recognition","<p>You get the error because <code>ent in item.ents</code> returns a boolean result, and you can't get its length.</p>
<p>What you want is</p>
<pre class=""lang-py prettyprint-override""><code>test_list = []
for item in sentences: #for each sentence in 'sentences' list
    if len(item.ents) == 1 and item.ents[0].label_ == &quot;PERSON&quot;: #if there is only one entity and if the entity is a &quot;PERSON&quot;
        test_list.append(item) #put the sentence into 'test_list'
</code></pre>
<p>The <code>len(item.ents) == 1</code> checks if there is only one entity detected in the sentence, and <code>item.ents[0].label_ == &quot;PERSON&quot;</code> makes sure the first entity lable text is <code>PERSON</code>.</p>
<p>Note the <code>and</code> operator, both conditions must be met.</p>
",1,1,1129,2022-01-27 19:08:15,https://stackoverflow.com/questions/70884361/how-to-extract-sentences-from-one-text-with-only-1-named-entity-using-spacy
how to get mentions in pytorch NER instead of toknes?,"<p>I am using PyTorch and a pre-trained model.</p>
<p>Here is my code:</p>
<pre><code>class NER(object):
    def __init__(self, model_name_or_path, tokenizer_name_or_path):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
        self.model = AutoModelForTokenClassification.from_pretrained(
            model_name_or_path)
        self.nlp = pipeline(&quot;ner&quot;, model=self.model, tokenizer=self.tokenizer)

    def get_mention_entities(self, query):
        return self.nlp(query)

</code></pre>
<p>when I call <code>get_mention_entities</code> and print its output for &quot;اینجا دانشگاه صنعتی امیرکبیر است.&quot;</p>
<p>it gives:</p>
<pre><code>[{'entity': 'B-FAC', 'score': 0.9454591, 'index': 2, 'word': 'دانشگاه', 'start': 6, 'end': 13}, {'entity': 'I-FAC', 'score': 0.9713519, 'index': 3, 'word': 'صنعتی', 'start': 14, 'end': 19}, {'entity': 'I-FAC', 'score': 0.9860724, 'index': 4, 'word': 'امیرکبیر', 'start': 20, 'end': 28}]
</code></pre>
<p>As you can see, it can recognize the university name, but there are three tokens in the list.</p>
<p>Is there any standard way to combine these tokens based on the &quot;entity&quot; attribute?</p>
<p>desired output is something like:</p>
<pre><code>[{'entity': 'FAC', 'word': 'دانشگاه صنعتی امیرکبیر', 'start': 6, 'end': 28}]
</code></pre>
<p>Finally, I can write a function to iterate, compare, and merge the tokens based on the &quot;entity&quot; attribute, but I want a standard way like an internal PyTorch function or something like this.</p>
<p>my question is similar to <a href=""https://stackoverflow.com/questions/57576640/getting-full-names-from-ner"">this question</a>.</p>
<p>PS: &quot;دانشگاه صنعتی امیرکبیر&quot; is a university name.</p>
","python, pytorch, huggingface-transformers, named-entity-recognition, mention","<p>Huggingface's NER pipeline has an argument <code>grouped_entities=True</code> which will do exactly what you seek: group BI into unified entities.</p>
<p>Adding</p>
<pre><code>self.nlp = pipeline(&quot;ner&quot;, model=self.model, tokenizer=self.tokenizer, grouped_entities=True)
</code></pre>
<p>should do the trick</p>
",0,0,101,2022-02-03 21:21:32,https://stackoverflow.com/questions/70978468/how-to-get-mentions-in-pytorch-ner-instead-of-toknes
Why FLAIR does&#39;t recognize the entire location name of simple sentence?,"<p>I'm tying to to detect simple location with NER algorithm, and I'm getting semi-correct results:</p>
<pre><code>from flair.data   import Sentence
from flair.models import SequenceTagger

tagger   = SequenceTagger.load('ner')
text     = 'Jackson leaves at north Carolina'
sentence = Sentence(text)

tagger.predict(sentence)
for entity in sentence.get_spans('ner'):
    print(entity)
</code></pre>
<p>Output:</p>
<pre><code>Span [1]: &quot;Jackson&quot;   [− Labels: PER (0.9996)]
Span [5]: &quot;Carolina&quot;   [− Labels: LOC (0.7363)]
</code></pre>
<p>I was expecting to receive <code>&quot;north Carolina&quot;</code>.</p>
<ol>
<li>Can FLAIR detect full location description? What do we need for it?</li>
<li>Is there any NER algorithm that cat detect full location description?</li>
</ol>
","deep-learning, nlp, named-entity-recognition, flair","<p>FLAIR CAN detect full location description. The reason for your issue is that the 'north' is not capitalized.</p>
<p>If you run</p>
<pre class=""lang-py prettyprint-override""><code>from flair.data   import Sentence
from flair.models import SequenceTagger

tagger   = SequenceTagger.load('ner')
text     = 'Jackson leaves at North Carolina'
sentence = Sentence(text)

tagger.predict(sentence)
for entity in sentence.get_spans('ner'):
    print(entity)
</code></pre>
<p>You'll get</p>
<pre><code>Span[0:1]: &quot;Jackson&quot; → PER (0.9997)
Span[3:5]: &quot;North Carolina&quot; → LOC (0.9246)
</code></pre>
",2,0,501,2022-02-05 08:09:29,https://stackoverflow.com/questions/70996277/why-flair-doest-recognize-the-entire-location-name-of-simple-sentence
Python: Merge the string in the list (first element) based on the second element,"<p>I have a nested list:</p>
<pre><code>&quot;Add changes &amp; things to hot 50 playlist&quot;
&quot;add Madchild to Electro Latino&quot;
&quot;Add artist to my 80'S PARTY&quot;

slot_list = [[['changes', 'entity_name'], ['&amp;', 'entity_name'], ['things', 'entity_name'], ['hot', 'playlist'], ['50', 'playlist']], 
[['Madchild', 'artist'], ['Electro', 'playlist'], ['Latino', 'playlist']],
[['artist', 'music_item'], ['my', 'playlist_owner'], [&quot;80'S&quot;, 'playlist'], ['PARTY', 'playlist']]]
</code></pre>
<p>I want to merge the string in the [0] position together when their [1] position (slot) elements are the same. And still keep the same nested structure, since that they belong to the same sentence.</p>
<p>the expected output:</p>
<pre><code>output = [[['entity_name', 'changes &amp; things'], ['playlist', 'hot 50']],
 [['artist', 'Madchild'], ['playlist', 'Electro Latino']], [['music_item', 'artist'], 
 ['playlist_owner', 'my'], ['playlist', &quot;80's PARTY&quot;]]]
</code></pre>
<p>This is the code I used:</p>
<pre><code>dic = defaultdict(str)
for element in slot_list:
    for word, slot in element:
        dic[slot] += ' ' + str(word)
print([[word, slot] for word, slot in dic.items()])
</code></pre>
<p>and I got:</p>
<pre><code>[['entity_name', ' changes &amp; things'], ['playlist', &quot; hot 50 Electro Latino 80'S PARTY&quot;], ['artist', ' Madchild'], ['music_item', ' artist'], ['playlist_owner', ' my']]
</code></pre>
<p>, which combine the <em>words</em> with same <em>slot</em> together because of the key-value pair in dict.
I also tried <strong>groupby</strong> but it also does not work out.</p>
<p>Hope someone can give me some guidance! Thanks!</p>
","python, pandas-groupby, nested-lists, named-entity-recognition, defaultdict","<p>Some denomination:</p>
<ul>
<li><p>A pair is a list containing two string elements: the
first one (value) is the value represented by the second
one (key), so the <code>['changes', 'entity_name']</code> pair represents
a entity name of value &quot;changes&quot;, and the <code>['hot', 'playlist']</code>
pair represents a playlist of value &quot;hot&quot;.</p>
</li>
<li><p>A slot is a list of pairs.</p>
</li>
</ul>
<p>Assuming their [1] position are sorted and a slot is</p>
<pre class=""lang-py prettyprint-override""><code>[
    ['changes', 'entity_name'],
    ['&amp;', 'entity_name'],
    ['things', 'entity_name'],
    ['hot', 'playlist'],
    ['50', 'playlist'],
]
</code></pre>
<p>you can group the slot using each pair's second element</p>
<pre class=""lang-py prettyprint-override""><code># itertools.groupby(slot, key=lambda x: x[1])
[
    ['entity_name', [
        ['changes', 'entity_name'],
        ['&amp;', 'entity_name'],
        ['things', 'entity_name'],
    ],
    ['playlist', [
        ['hot', 'playlist'],
        ['50', 'playlist']
    ],
]
</code></pre>
<p>For each grouped pairs, join all the first elements using a space:</p>
<pre class=""lang-py prettyprint-override""><code>import itertools

def group_slots(slots):
    # For each slot in the list of slots, group it
    return [group_slot(slot) for slot in slots]

def group_slot(slot): 
    return [[key, ' '.join(pair[0] for pair in pairs)] 
            for key, pairs in itertools.groupby(slot, key=lambda x: x[1])]

</code></pre>
<p>Then</p>
<pre><code>slots = [
    [
        ['changes', 'entity_name'],
        ['&amp;', 'entity_name'],
        ['things', 'entity_name'],
        ['hot', 'playlist'],
        ['50', 'playlist'],
    ],
    [
        ['Madchild', 'artist'],
        ['Electro', 'playlist'],
        ['Latino', 'playlist'],
    ],
    [
        ['artist', 'music_item'],
        ['my', 'playlist_owner'],
        [&quot;80'S&quot;, 'playlist'],
        ['PARTY', 'playlist'],
    ],
]
result = group_slots(slots)
print(result)
</code></pre>
<p>outputs</p>
<pre class=""lang-none prettyprint-override""><code>[
    [
        ['entity_name', 'changes &amp; things'],
        ['playlist', 'hot 50'],
    ],
    [
        ['artist', 'Madchild'], 
        ['playlist', 'Electro Latino'],
    ],
    [
        ['music_item', 'artist'], 
        ['playlist_owner', 'my'], 
        ['playlist', &quot;80'S PARTY&quot;],
    ],
]
</code></pre>
",0,0,210,2022-02-05 18:48:59,https://stackoverflow.com/questions/71001107/python-merge-the-string-in-the-list-first-element-based-on-the-second-element
ValueError: No gradients provided for any variable (TFCamemBERT),"<p>Currently I am working on Named Entity Recognition in the medical domain using Camembert, precisely using the model: <a href=""https://huggingface.co/jplu/tf-camembert-base"" rel=""nofollow noreferrer"">TFCamembert</a>.</p>
<p>However I have some problems with the fine-tuning of the model for my task as I am using a private dataset not available on Hugging Face.</p>
<p>The data is divided into text files and annotation files. The text file contains for example:</p>
<pre><code>Le cas présenté concerne un homme âgé de 61 ans (71 kg, 172 cm, soit un indice de masse corporelle de 23,9 kg/m²) admissible à une transplantation pulmonaire en raison d’une insuffisance respiratoire chronique terminale sur emphysème post-tabagique, sous oxygénothérapie continue (1 L/min) et ventilation non invasive nocturne. Il présente, comme principaux antécédents, une dyslipidémie, une hypertension artérielle et un tabagisme sevré estimé à 21 paquets-années (facteurs de risque cardiovasculaires). Le bilan préopératoire a révélé une hypertension artérielle pulmonaire essentiellement postcapillaire conduisant à l’ajout du périndopril (2 mg par jour) et du furosémide (40 mg par jour). La mise en évidence d’un Elispot (enzyme-linked immunospot) positif pour la tuberculose a motivé l’introduction d’un traitement prophylactique par l’association rifampicine-isoniazide (600-300 mg par jour) pour une durée de trois mois.
Deux mois après le bilan préopératoire, le patient a bénéficié d’une transplantation mono-pulmonaire gauche sans dysfonction primaire du greffon5,6. Le donneur et le receveur présentaient tous deux un statut sérologique positif pour cytomegalovirus (CMV) et Epstein Barr Virus (EBV). Une sérologie positive de la toxoplasmose a été mise en évidence uniquement chez le receveur. Le traitement immunosuppresseur d’induction associait la méthylprednisolone (500 mg à jour 0 et 375 mg à jour +1 post-transplantation) et le basiliximab, anticorps monoclonal dirigé contre l’interleukine-2 (20 mg à jour 0 et jour +4 posttransplantation). À partir de jour +2 post-transplantation, l’immunosuppression a été maintenue par une trithérapie par voie orale comprenant le tacrolimus à une posologie initiale de 5 mg par jour, le mofétil mycophénolate (MMF) 2000 mg par jour et la prednisone 20 mg par jour. Les traitements associés sont présentés dans le tableau I.
L’évolution est marquée par la survenue, au jour +5 posttransplantation, d’une dégradation respiratoire sur œdème pulmonaire gauche de reperfusion, avec possible participation cardiogénique. Le rejet aigu de grade III, évoqué par la présence d’infiltrats lymphocytaires aux biopsies transbronchiques, a été confirmé par l’anatomopathologie.
</code></pre>
<p>While the annotation file looks like:</p>
<pre><code>T1 genre 28 33 homme
T2 age 41 47 61 ans
A1 genre T1 masculin
T3 origine 127 326 une transplantation pulmonaire en raison d’une insuffisance respiratoire chronique terminale sur emphysème post-tabagique, sous oxygénothérapie continue (1 L/min) et ventilation non invasive nocturne
T4 issue 1962 2104 une dégradation respiratoire sur œdème pulmonaire gauche de reperfusion, avec possible participation cardiogénique. Le rejet aigu de grade III
A2 issue T4 détérioration
</code></pre>
<p>More details about the prepossessing of <a href=""https://drive.google.com/file/d/1Odq6eTexLg9ZXCjbWbnZiWMGAqg9Ut45/view?usp=sharing"" rel=""nofollow noreferrer"">the data</a> can be found in this <a href=""https://colab.research.google.com/drive/1oqCRFFvzSjDBpfCk5nS2KjBvzCr34Rqz?usp=sharing"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>The things is that the internal loss of my model does not work, if I run the training of the model without declaring a loss, it does not work, I have to define a loss to be able to run the training!</p>
<p>Here is my train_data converted to Tensor slice dataset.</p>
<pre><code>train_label_encodings = tf.convert_to_tensor(train_label_encodings, dtype=tf.int32)
train_label_encodings.data

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_text_encodings.data),
    train_label_encodings.data
))
train_dataset
</code></pre>
<pre><code>&lt;TensorSliceDataset shapes: ({input_ids: (512,), offset_mapping: (512, 2)}, (512,)), types: ({input_ids: tf.int32, offset_mapping: tf.int32}, tf.int32)&gt;
</code></pre>
<p>I define the model:</p>
<pre><code># Import the model and define an optimizer
from transformers import TFAutoModelForTokenClassification, TFCamembertModel, create_optimizer
import tensorflow as tf

num_train_steps = len(train_dataset) * 5
optimizer, lr_schedule = create_optimizer(
    init_lr = 5e-6,
    num_train_steps = num_train_steps,
    weight_decay_rate = 0.01,
    num_warmup_steps = 0
)

metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model = TFAutoModelForTokenClassification.from_pretrained(model_id, num_labels=len(unique_labels), label2id=label2id, id2label=id2label)

model.compile(optimizer=optimizer, metrics=['accuracy'])
</code></pre>
<p>Here is the summary of the model:</p>
<pre><code>Model: &quot;tf_camembert_for_token_classification_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 roberta (TFRobertaMainLayer  multiple                 110031360 
 )                                                               
                                                                 
 dropout_113 (Dropout)       multiple                  0         
                                                                 
 classifier (Dense)          multiple                  25377     
                                                                 
=================================================================
Total params: 110,056,737
Trainable params: 110,056,737
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>When I try to launch the training, I get the following error:</p>
<pre><code>import os
from tensorflow.keras.callbacks import TensorBoard

callbacks = []
callbacks.append(TensorBoard(log_dir=os.path.join(output_dir,&quot;logs&quot;)))

model.fit(
    train_dataset,
    callbacks = callbacks,
    epochs = 3,
)
</code></pre>
<pre><code>Epoch 1/3
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-71-54e2d25b9415&gt; in &lt;module&gt;()
      8     train_dataset,
      9     callbacks = callbacks,
---&gt; 10     epochs = 3,
     11 )

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 878, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 867, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 860, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 911, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py&quot;, line 532, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/optimization_tf.py&quot;, line 232, in apply_gradients
        return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars), name=name, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py&quot;, line 633, in apply_gradients
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/utils.py&quot;, line 73, in filter_empty_gradients
        raise ValueError(f&quot;No gradients provided for any variable: {variable}. &quot;

    ValueError: No gradients provided for any variable: (['tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/La...
</code></pre>
<p>Any clues to solve this gradient issue? Thanks in advance!</p>
","python, tensorflow, nlp, huggingface-transformers, named-entity-recognition","<p>Try transforming your data into the correct format, before feeding it to <code>model.fit</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def map_func(x, y):
  return {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask'], 'labels':y}

train_dataset = train_dataset.map(map_func)
</code></pre>
<p>The model seems to run after this step.</p>
",1,1,431,2022-02-11 10:02:44,https://stackoverflow.com/questions/71078218/valueerror-no-gradients-provided-for-any-variable-tfcamembert
How to load data for only certain label of Spacy&#39;s NER entities?,"<p>I just started to explore spaCy and need it only for GPE (Global political entities) of the name entity recognition (NER) component.</p>
<p>So, to save time on loading I keep only 'ner':</p>
<pre><code>    nlp = spacy.load('en_core_web_sm', disable=['tok2vec','tagger','parser', 'senter', 'attribute_ruler', 'lemmatizer'])
</code></pre>
<p>Then I create a set of cities / states / countries that exist in the text by running:</p>
<pre><code>doc = nlp(txt) 
geo_ents = {str(word) for word in doc.ents if word.label_=='GPE'}
</code></pre>
<p>That means I only need a small subset of the entities with the label_=='GPE'.
I didn't find a way yet to iterate only within that component of the whole model to reduce runtime on big loads of texts.</p>
<p>Would you please guide me to how to load only certain label of Spacy's NER entities? That might be helpful for others in order to get only selected types of entities.</p>
<p>Thank you very much!</p>
","python, nlp, spacy, named-entity-recognition","<p>It isn't possible to do this. The NER model is classifying each token/span between all the labels it knows about, and the knowledge is not separable.</p>
<p>Additionally, the NER component requires a tok2vec. Depending on the pipeline architecture you may be able to disable the top-level tok2vec. (EDIT: I incorrectly stated the top-level tok2vec was required for the small English model; it is not. See <a href=""https://spacy.io/models#design-cnn"" rel=""nofollow noreferrer"">here</a> for details.)</p>
<p>It may be possible to train a smaller model that only recognizes GPEs with similar accuracy, but I wouldn't be too optimistic about it. It also wouldn't be faster.</p>
",2,1,1184,2022-02-25 17:18:14,https://stackoverflow.com/questions/71269432/how-to-load-data-for-only-certain-label-of-spacys-ner-entities
Extracting SpaCy DATE entities and adding to new pandas column,"<p>I have a collection of social media comments that I want to explore based on their reference to dates. For this purpose, I am using SpaCy's Named Entity Recognizer to search for <code>DATE</code> entities. I have the comments in a pandas dataframe called <code>df_test</code> under the column <code>comment</code>. I would like to add a new column <code>dates</code> to this dataframe consisting of all the date entities found in each comment. Some comments are not going to have any date entities in which case <code>None</code> should be added here instead.
So for example:</p>
<pre><code>comment
'bla bla 21st century'
'bla 1999 bla bla 2022'
'bla bla bla'
</code></pre>
<p>Should be:</p>
<pre><code>comment                        dates
'bla bla 21st century'         '21st century'
'bla 1999 bla bla 2022'        '1999', '2022'
'bla bla bla'                  'None'
</code></pre>
<p>Based on <a href=""https://stackoverflow.com/questions/69028097/is-their-a-way-to-add-the-new-ner-tag-found-in-a-new-column"">Is their a way to add the new NER tag found in a new column?</a> I have tried a list approach:</p>
<pre><code>date_label = ['DATE']
dates_list = []

def get_dates(row):
    comment = str(df_test.comment.tolist())
    doc = nlp(comment)
    for ent in doc.ents:
        if ent.label_ in date_label:
            dates_list.append([ent.text])
        else:
            dates_list.append(['None'])

df_test.apply(lambda row: get_dates(row))
date_df_test = pd.DataFrame(dates_list, columns=['dates'])
</code></pre>
<p>However, this then produces a column that would be longer than the original dataframe, like:</p>
<pre><code>comment                        dates
'bla bla 21st century'         '21st century'
'bla 1999 bla bla 2022'        '1999'
'bla bla bla'                  '2022'
                               'None'
</code></pre>
<p>Which doesn't work, since the entries of dates no longer matches with their corresponding comments. I understand that it is because I am for-looping across all entities, but I don't know how to work around this. Is there any way to solve this, so that I can extract all date entities and connect them in some way to the comment their were found in for the purpose of later analysis? Any help is much appreciated!</p>
","python, pandas, spacy, named-entity-recognition, named-entity-extraction","<p>I managed to find a solution to my own problem by using this function.</p>
<pre><code>date_label = ['DATE']

def extract_dates(text):
    doc = nlp(text)
    results = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in date_label]
    return results

df_test['dates'] = df_test['comment'].apply(extract_dates)
</code></pre>
<p>I hope this may help anyone who face a similar issue.</p>
",2,1,3494,2022-03-02 17:36:16,https://stackoverflow.com/questions/71326920/extracting-spacy-date-entities-and-adding-to-new-pandas-column
Extracting important entities from unstructured data,"<p>I am working on a NLP problem where I am completely stuck at certain point. I am new to these so pardon if the question is dumb.
I have got a completely unstructured text let's say: &quot;<code>a person named x y is travelling to country ab, he spent xyz (alpha/currency/beta/gamma), ate a b c d e f food items and many more.</code>&quot;
now I have to extract</p>
<pre><code>|name of person| country's name | amount spent and the currency | food items he ate | place of              
stay|
</code></pre>
<p><strong>Constraint on this is, the text contains some false information, for example: the food b and c cannot be found in a particular country, and thus it should not be extracted.</strong>
I have a nested dictionary which looks like this:</p>
<pre><code>{country_name: {place 1: {name of hotels:[hotel1, hotel2, hotel3....],
                          eatables: [food1, food2, food3, food4.....],
                          currency_accepted: [c1, c2, c3, c4.......],
                          }
                }
} 
</code></pre>
<p><strong>I want to use this dictionary in the unstructured text so that I can parse the data and extract entities which are relevant in separate columns of dataframe.</strong>
I have seen NER based approaches, but I guess it requires tagging of words, and I have got huge data.</p>
<p><strong>I have tried regex based approach for pattern matching, but that doesn't give all the results, further to that I have tried to match all the entities stored in a list, but this creates the problem of many false entities being extracted and accuracy is quite important here.</strong></p>
<p>I am looking for more improve parsing based approaches, also if there is any way a certain model is trained on this dictionary such that it looks for values of nested dictionary only if a key is found in the unstructured text.</p>
","python, parsing, nlp, named-entity-recognition, text-extraction","<p>Before you go to machine learning, you could try using <a href=""https://pypi.org/project/fuzzywuzzy/"" rel=""nofollow noreferrer"">fuzzywuzzy</a>. I had a similar problem at work and was able to achieve high accuracy by adjusting the ratio attribute. So, for each extracted entity, you would have to run it through fuzzywuzzy and your dictionary.</p>
<p>For the issue of</p>
<blockquote>
<p>but this creates the problem of many false entities being extracted</p>
</blockquote>
<p>I would implement a filter: if the extracted &amp; matched entity is not in the list, leave the extracted entity out, otherwise, continue with the logic.</p>
",0,1,955,2022-03-02 18:18:30,https://stackoverflow.com/questions/71327407/extracting-important-entities-from-unstructured-data
Spacy NER not recognising NAME,"<p>Can anyone please help me understand why Spacy NER refuses to recognize the last NAME 'Hagrid' in the sentence, no matter the model used (sm, md, lg)?:</p>
<p><strong>&quot;Hermione bought a car, then both Hermione and Hagrid raced it on the track. Tom Brady was very happy with Hagrid this year.&quot;</strong></p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_md')

test_data = &quot;Hermione bought a car, then both Hermione and Hagrid raced it on the track. Tom Brady was very happy with Hagrid this year.&quot;

doc = nlp(test_data)
for ent in doc.ents:
        print(ent.text, ent.start_char, ent.end_char, ent.label_)
</code></pre>
<p><a href=""https://i.sstatic.net/ehJ15.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ehJ15.png"" alt=""enter image description here"" /></a></p>
","python, nlp, artificial-intelligence, spacy, named-entity-recognition","<p>Well, Neural Network Models are basically a black box, so there is no way to know this for sure.</p>
<p>I could imagine that the grammar in last sentence is a bit too &quot;fancy&quot;/literature-like if the model was trained on news or web data and might be throwing the model off. This difficulty of seeing the sentence context as something that would be followed up by a name as well as the fact that &quot;Hagrid&quot; is a kind of unusual name could be the reason.</p>
<p>You can try some other models such as the one integrated in Flair:</p>
<p><a href=""https://huggingface.co/flair/ner-english-large?text=Hermione+bought+a+car%2C+then+both+Hermione+and+Hagrid+raced+it+on+the+track.+Tom+Brady+was+very+happy+with+Hagrid+this+year"" rel=""nofollow noreferrer"">https://huggingface.co/flair/ner-english-large?text=Hermione+bought+a+car%2C+then+both+Hermione+and+Hagrid+raced+it+on+the+track.+Tom+Brady+was+very+happy+with+Hagrid+this+year</a>.</p>
<p>or this fine-tuned BERT model:</p>
<p><a href=""https://huggingface.co/dslim/bert-large-NER?text=Hermione+bought+a+car%2C+then+both+Hermione+and+Hagrid+raced+it+on+the+track.+Tom+Brady+was+very+happy+with+Hagrid+this+year"" rel=""nofollow noreferrer"">https://huggingface.co/dslim/bert-large-NER?text=Hermione+bought+a+car%2C+then+both+Hermione+and+Hagrid+raced+it+on+the+track.+Tom+Brady+was+very+happy+with+Hagrid+this+year</a>.</p>
<p>They are more powerful and get it right, from my experience SpaCy is a nice tool and quite fast, but not the most precise for NER.</p>
",1,1,1657,2022-03-03 16:05:02,https://stackoverflow.com/questions/71340177/spacy-ner-not-recognising-name
Grouping NLTK entities,"<p>I have the following code:</p>
<pre><code>import nltk
 
page = '
EDUCATION   
University
Won first prize for the best second year group project, focused on software engineering.
Sixth Form
Mathematics, Economics, French
UK, London
'


for sent in nltk.sent_tokenize(page):
  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
     if hasattr(chunk, 'label'):
        print(''.join(c[0] for c in chunk), ' ',chunk.label())
</code></pre>
<p>Returns:</p>
<pre><code>EDUCATION   ORGANIZATION
UniversityWon   ORGANIZATION
Sixth   PERSON
FormMathematics   ORGANIZATION
Economics   PERSON
FrenchUK   GPE
London   GPE
</code></pre>
<p>Which i'd like to be grouped into some data-structure based on the entity label, maybe a list: ORGANIZATION=[EDUCATION,UniversityWon,FormMathematics] PERSON=[Sixth,Economics] GPE=[FrenchUK,London]</p>
<p>Or maybe a dictionary with the keys: ORGANIZATION, PERSON, GPE then the associated values are as the lists above</p>
","python, nltk, named-entity-recognition","<p>A dictionary makes more sense, perhaps something like this.</p>
<pre class=""lang-py prettyprint-override""><code>from collections import defaultdict

entities = defaultdict(list)

for sent in nltk.sent_tokenize(page):
    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
        if hasattr(chunk, 'label'):
            entities[chunk.label()].append(''.join(c[0] for c in chunk))
</code></pre>
",0,0,147,2022-03-09 17:59:56,https://stackoverflow.com/questions/71413974/grouping-nltk-entities
How to label multi-word entities?,"<p>I'm quite new to data analysis (and Python in general), and I'm currently a bit stuck in my project.</p>
<p>For my NLP-task I need to create training data, i.e. find specific entities in sentences and label them. I have multiple csv files containing the entities I am trying to find, many of them consisting of multiple words. I have tokenized and lemmatized the unlabeled sentences with spaCy and loaded them into a <code>pandas.DataFrame</code>.</p>
<p>My main problem is: how do I now compare the tokenized sentences with the entity-lists and label the (often multi-word) entities? Having around 0.5 GB of sentences, I don't think it is feasible to just for-loop every sentence and then for-loop every entity in every class-list and do a simple substring-search. Is there any smart way to use pandas.Series or DataFrame to do this labeling?</p>
<p>As mentioned, I don't really have any experience regarding pandas/numpy etc. and after a lot of web searching I still haven't seemed to find the answer to my problem</p>
<p>Say that this is a sample of finance.csv, one of my entity lists:</p>
<pre><code>&quot;Frontwave Credit Union&quot;,
&quot;St. Mary's Bank&quot;,
&quot;Center for Financial Services Innovation&quot;,
...
</code></pre>
<p>And that this is a sample of sport.csv, another one of my entity lists:</p>
<pre><code>&quot;Christiano Ronaldo&quot;,
&quot;Lewis Hamilton&quot;,
...
</code></pre>
<p>And an example (dumb) sentence:</p>
<pre><code>&quot;Dear members of Frontwave Credit Union, any credit demanded by Lewis Hamilton is invalid, said Ronaldo&quot;
</code></pre>
<p>The result I'd like would be something like a table of tokens with the matching entity labels (with IOB labeling):</p>
<pre><code>&quot;Dear &quot;- O
&quot;members&quot; - O
&quot;of&quot; - O
&quot;Frontwave&quot; - B-FINANCE
&quot;Credit&quot; - I-FINANCE
&quot;Union&quot; - I-FINANCE
&quot;,&quot; - O
&quot;any&quot; - O
...
&quot;Lewis&quot; - B-SPORT
&quot;Hamilton&quot; - I-SPORT
...
&quot;said&quot; - O
&quot;Ronaldo&quot; - O
</code></pre>
","python, pandas, nlp, training-data, named-entity-recognition","<p>Use:</p>
<pre><code>FINANCE = [&quot;Frontwave Credit Union&quot;,
&quot;St. Mary's Bank&quot;,
&quot;Center for Financial Services Innovation&quot;]

SPORT = [
    &quot;Christiano Ronaldo&quot;,
    &quot;Lewis Hamilton&quot;,
]

FINANCE = '|'.join(FINANCE)
sent = pd.DataFrame({'sent': [&quot;Dear members of Frontwave Credit Union, any credit demanded by Lewis Hamilton is invalid, said Ronaldo&quot;]})
home = sent['sent'].str.extractall(f'({FINANCE})')

def labeler(row, group):
    l = len(row.split())
    return [f'I-{group}' if i !=0 else f'B-{group}' for i in range(l)]

home[0].apply(labeler, group='FINANCE').explode()
</code></pre>
<p><a href=""https://i.sstatic.net/LucfK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LucfK.png"" alt=""enter image description here"" /></a></p>
",0,0,575,2022-03-12 11:04:57,https://stackoverflow.com/questions/71449153/how-to-label-multi-word-entities
Extract Only Certain Named Entities From Tokens,"<p>Quick question (hopefully). Is it possible for me to get the named entities of the tokens except for the ones with CARDINAL label (The label is 397). Here is my code below:</p>
<pre><code>spacy_model = spacy.load('en-core-web-lg')
f = open('temp.txt')
tokens = spacy_model(f.read())
named_entities = tokens.ents #Except where named_entities.label = 397
</code></pre>
<p>Is this possible? Any help would be greatly appreciated.</p>
","python, nlp, token, spacy, named-entity-recognition","<p>You can filter out the entities using list comprehension:</p>
<pre class=""lang-py prettyprint-override""><code>named_entities = [t for t in tokens.ents if t.label_ != 'CARDINAL']
</code></pre>
<p>Here is a test:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
tokens = nlp('The basket costs $10. I bought 6.')
print([(ent.text, ent.label_) for ent in tokens.ents])
# =&gt; [('10', 'MONEY'), ('6', 'CARDINAL')]
print([t for t in tokens.ents if t.label_ != 'CARDINAL'])
# =&gt; [10]
</code></pre>
",2,1,1137,2022-03-13 20:49:17,https://stackoverflow.com/questions/71460724/extract-only-certain-named-entities-from-tokens
Training CamelBERT model for token classification,"<p>I am trying to use a huggingface model (<a href=""https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca"" rel=""nofollow noreferrer"">CamelBERT</a>) for token classification using <a href=""https://camel.abudhabi.nyu.edu/anercorp/"" rel=""nofollow noreferrer"">ANERCorp</a> Dataset. I fed the training set from <a href=""https://camel.abudhabi.nyu.edu/anercorp/"" rel=""nofollow noreferrer"">ANERCorp</a> to train the model, but I am getting the following error.</p>
<p>Error:</p>
<pre><code>Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/16/2022 07:31:01 - INFO - utils -   Creating features from dataset file at /content/drive/MyDrive/ANERcorp-CamelLabSplits
03/16/2022 07:31:01 - INFO - utils -   Writing example 0 of 3973
Traceback (most recent call last):
  File &quot;/content/CAMeLBERT/token-classification/run_token_classification.py&quot;, line 381, in &lt;module&gt;
    main()
  File &quot;/content/CAMeLBERT/token-classification/run_token_classification.py&quot;, line 226, in main
    if training_args.do_train
  File &quot;/content/CAMeLBERT/token-classification/utils.py&quot;, line 132, in __init__
    pad_token_label_id=self.pad_token_label_id,
  File &quot;/content/CAMeLBERT/token-classification/utils.py&quot;, line 210, in convert_examples_to_features
    label_ids.extend([label_map[label]] +
KeyError: 'B-LOC'
</code></pre>
<p>Please note: I am using Google Colab to train the model.
Code:</p>
<pre><code>DATA_DIR=&quot;/content/drive/MyDrive/ANERcorp-CamelLabSplits&quot;
MAX_LENGTH=512
BERT_MODEL=&quot;CAMeL-Lab/bert-base-arabic-camelbert-ca&quot;
OUTPUT_DIR=&quot;/content/Output&quot;
BATCH_SIZE=32
NUM_EPOCHS=3
SAVE_STEPS=750
SEED=12345

!python /content/CAMeLBERT/token-classification/run_token_classification.py \
--data_dir $DATA_DIR \
--task_type ner \
--labels $DATA_DIR/train.txt \
--model_name_or_path $BERT_MODEL \
--output_dir $OUTPUT_DIR \
--max_seq_length $MAX_LENGTH \
--num_train_epochs $NUM_EPOCHS \
--per_device_train_batch_size $BATCH_SIZE \
--save_steps $SAVE_STEPS \
--seed $SEED \
--overwrite_output_dir \
--overwrite_cache \
--do_train \
--do_predict
</code></pre>
","deep-learning, nlp, bert-language-model, named-entity-recognition","<p>The script you are using loads the labels from <code>$DATA_DIR/train.txt</code>.</p>
<p>See <a href=""https://github.com/CAMeL-Lab/CAMeLBERT/blob/master/token-classification/run_token_classification.py#L105"" rel=""nofollow noreferrer"">https://github.com/CAMeL-Lab/CAMeLBERT/blob/master/token-classification/run_token_classification.py#L105</a> for what the model expects.</p>
<p>It then tries to load the label list as first file file from the corpus (even before loading the training data), see <a href=""https://github.com/CAMeL-Lab/CAMeLBERT/blob/master/token-classification/run_token_classification.py#L183"" rel=""nofollow noreferrer"">https://github.com/CAMeL-Lab/CAMeLBERT/blob/master/token-classification/run_token_classification.py#L183</a> and put it into label_map.</p>
<p>But that fails for some reason. My assumption would be that it doensnt find anything and label_map is an empty dict, so the first attempt to get the labels from it fails with KeyError. Probably either your input data is not there or not in the path as expected (check if you have the right files and the right value for <code>$DATA_DIR</code>). From my experience relative paths in Google Drive can be tricky. Try something simple to see if it works, like <code>os.listdir($DATA_DIR)</code> to see if that is actually the directly you expect it to be.</p>
<p>If that is not the problem then probably something about the labels is actually wrong. Does ANERCorp use this exact way of writing labels (<code>B-LOC</code> etc.)? If it is different (e.g. <code>B-Location</code> or something) it would fail too.</p>
",0,0,276,2022-03-16 08:37:55,https://stackoverflow.com/questions/71493915/training-camelbert-model-for-token-classification
python Spacy custom NER – how to prepare multi-words entities?,"<p>:) Please help :)</p>
<p>I`m preparing custom Name Entity Recognition using Spacy (blank) model. I use only one entity: Brand (we can name it 'ORG' as Organisation). I have short texts with ORGs &amp; have prepared data like this (but I can change it):</p>
<pre><code>train_data = [ 
    (‘First text in string with Name I want’, {'entities': [(START, END, ‘ORG')]}),
    (‘Second text with Name and Name2’, {'entities': [(START, END, ‘ORG'), (START2, END2, ‘ORG')]})
 ]
</code></pre>
<p>START, END – are the start and end indexes of the brand name in text , of course.</p>
<p>This is working well, but...</p>
<p><strong>The problem I have is how to prepare entities for Brands that are made of 2 (or more) words.</strong>
Lets say Brand Name is a full name of a company. How to prepare an entity?</p>
<p>Consider the tuple itself for a single text:</p>
<p>text = 'Third text with Brand Name'</p>
<p>company = 'Brand Name'</p>
<ul>
<li>Can I treat company as a one word?</li>
</ul>
<pre><code>(‘Third text with Brand Name', {“entities”: [(16, 26, 'ORG')]})
</code></pre>
<ul>
<li>Or 2 separated brands ‘Brand’ &amp; ‘Name’ ? (will not be useful in my case while using :( the model later)</li>
</ul>
<pre><code>(‘Third text with Brand Name', {“entities”: [(16, 21, 'ORG'), (22, 26, 'ORG')]})
</code></pre>
<ul>
<li>Or I should use a different format of labeling eg. <strong>BIO</strong> ?
So <strong>Brand</strong> will be <strong>B-ORG</strong> and <strong>Name</strong> will be <strong>I-ORG</strong> ?
<ul>
<li>IF so can I prepare it like this for Spacy:</li>
</ul>
</li>
</ul>
<pre><code>(‘Third text with Brand Name', {“entities”: [(16, 21, 'B-ORG'), (22, 26, 'I-ORG')]})
</code></pre>
<ul>
<li>or should I change the format of <strong>train_data</strong> because I also need the ‘O’ from BIO?<br />
How? Like this? :</li>
</ul>
<pre><code>(‘Third text with Brand Name', {&quot;entities&quot;: [&quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;B-ORG&quot;, &quot;I-ORG&quot;]})
</code></pre>
<p><strong>The question is on the format of the train_data for ‘Third text with Brand Name'  - how to label the entity</strong>. If I have the format, I will handle the code. :)</p>
<p>The same question for 3 or more words entities. :)</p>
","python-3.x, spacy, named-entity-recognition","<p>You can just provide the start and end offsets for the whole entity. You describe this as &quot;treating it as one word&quot;, but the character offsets don't have any direct relation to tokenization - they won't affect tokenizer output.</p>
<p>You will get an error if the start and end of your entity don't match token boundaries, but it doesn't matter if the entity is one token or many.</p>
<p>I recommend you take a look at the <a href=""https://spacy.io/usage/training#training-data"" rel=""nofollow noreferrer"">training data section</a> in the spaCy docs. Your specific question isn't answered explicitly, but that's only because multi-token entries don't require special treatment. Examples include multi-token entities.</p>
<p>Regarding BIO tagging, for details on how to use it with spaCy you can see the docs for <a href=""https://spacy.io/api/cli#convert"" rel=""nofollow noreferrer""><code>spacy convert</code></a>.</p>
",2,1,1722,2022-03-17 15:36:01,https://stackoverflow.com/questions/71515090/python-spacy-custom-ner-how-to-prepare-multi-words-entities
"How to generate Precision, Recall and F-score in Named Entity Recognition using Spacy v3? Seeking ents_p, ents_r, ents_f for a small custom NER model","<p>The example code is given below, you may add one or more entities in this example for training purposes (You may also use a blank model with small examples for demonstration). I am seeking a complete working solution for custom NER model evaluation (precision, recall, f-score), Thanks in advance to all NLP experts.</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
example_text = &quot;Agra is famous for Tajmahal, The CEO of Facebook will visit India shortly to meet Murari Mahaseth and to visit Tajmahal&quot;
ner = nlp.get_pipe(&quot;ner&quot;)
doc = nlp(example_text)
for e in doc.ents:
    print(e.text + ' - ' + str(e.start_char) + ' - ' + str(e.end_char) + ' - ' + e.label_ + ' - ' + str(
        spacy.explain(e.label_)))
</code></pre>
","python, named-entity-recognition, precision-recall, spacy-3","<p>I will give a brief example :</p>
<pre><code>import spacy
from spacy.scorer import Scorer
from spacy.tokens import Doc
from spacy.training.example import Example

examples = [
    ('Who is Talha Tayyab?',
     {(7, 19, 'PERSON')}),
    ('I like London and Berlin.',
     {(7, 13, 'LOC'), (18, 24, 'LOC')}),
     ('Agra is famous for Tajmahal, The CEO of Facebook will visit India shortly to meet Murari Mahaseth and to visit Tajmahal.',
     {(0, 4, 'LOC'), (40, 48, 'ORG'), (60, 65, 'GPE'), (82, 97, 'PERSON'), (111, 119, 'GPE')})
]

def my_evaluate(ner_model, examples):
    scorer = Scorer()
    example = []
    for input_, annotations in examples:
        pred = ner_model(input_)
        print(pred,annotations)
        temp = Example.from_dict(pred, dict.fromkeys(annotations))
        example.append(temp)
    scores = scorer.score(example)
    return scores

ner_model = spacy.load('en_core_web_sm') # for spaCy's pretrained use 'en_core_web_sm'
results = my_evaluate(ner_model, examples)
print(results)
</code></pre>
<p>As, I said this is just an example you can make changes according to your needs.</p>
",0,1,2864,2022-03-23 19:57:29,https://stackoverflow.com/questions/71593295/how-to-generate-precision-recall-and-f-score-in-named-entity-recognition-using
ValueError: [E143] Labels for component &#39;tagger&#39; not initialized,"<p>I've been following this <a href=""https://newscatcherapi.com/blog/train-custom-named-entity-recognition-ner-model-with-spacy-v3"" rel=""noreferrer"">tutorial</a> to create a custom NER. However, I keep getting this error:
<code>ValueError: [E143] Labels for component 'tagger' not initialized. This can be fixed by calling add_label, or by providing a representative batch of examples to the component's </code>initialize<code> method.</code></p>
<p>This is how I defined the spacy model:</p>
<pre><code>import spacy
from spacy.tokens import DocBin
from tqdm import tqdm

nlp = spacy.blank(&quot;ro&quot;) # load a new spacy model
source_nlp = spacy.load(&quot;ro_core_news_lg&quot;)
nlp.tokenizer.from_bytes(source_nlp.tokenizer.to_bytes())
nlp.add_pipe(&quot;tagger&quot;, source=source_nlp)

doc_bin = DocBin() # create a DocBin object
</code></pre>
","named-entity-recognition, spacy-3","<p>I just meet the same problem. The picture of setting the config file is misleading you.
If you just want to run through the tutrital, you can set the config file like this.
<a href=""https://i.sstatic.net/VxJFd.png"" rel=""noreferrer"">only click the check box on ner</a></p>
",12,6,2128,2022-03-26 15:03:15,https://stackoverflow.com/questions/71629167/valueerror-e143-labels-for-component-tagger-not-initialized
Combine Camembert &amp; CRF for token classification,"<p>I want to combine Camembert and CRF in order to perform named entity recognition on French medical data.
I am following this <a href=""https://github.com/shushanxingzhe/transformers_ner/blob/main/models.py"" rel=""nofollow noreferrer"">code</a> combining Bert and CRF, but I can't reproduce the same thing with Camembert as I didn't find a <code>PreTrainedCamembert</code> class to pass and use instead of the <code>BertPreTrainedModel</code> used in the shared code.
I have tried to use the <code>CamembertModel</code> but it gave me a model in which the camembert layers are duplicated as shown below.</p>
<pre><code>BertCRF(
  (embeddings): RobertaEmbeddings(
    (word_embeddings): Embedding(32005, 768, padding_idx=1)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (token_type_embeddings): Embedding(1, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): RobertaEncoder(
    (layer): ModuleList(
      (0): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): RobertaPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
  (cmbert): CamembertModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(32005, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=21, bias=True)
  (crf): CRF(num_tags=21)
)
</code></pre>
<p>Any clues on how to fix this issue? I want to get a model similar to the BERT &amp; CRF one.</p>
<pre><code>BertCRF(
  (bert): BertPreTrainedModel()
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=21, bias=True)
  (crf): CRF(num_tags=21)
)
</code></pre>
","python, nlp, huggingface-transformers, named-entity-recognition, crf","<p>You can ignore <code>BertPreTrainedModel</code> and initialize it as torch module:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

from torchcrf import CRF
from transformers import CamembertModel, CamembertTokenizerFast

class CamemBERTCRF(nn.Module):
  def __init__(self, num_labels):
    super(CamemBERTCRF, self).__init__()
    
    self.encoder = CamembertModel.from_pretrained(&quot;camembert-base&quot;)
    
    self.config = self.encoder.config
    self.dropout = nn.Dropout(self.config.hidden_dropout_prob)
    self.classifier = nn.Linear(self.config.hidden_size, num_labels)
    self.crf = CRF(num_tags=num_labels, batch_first=True)

  def forward(
      self,
      input_ids=None,
      attention_mask=None,
      token_type_ids=None,
      position_ids=None,
      head_mask=None,
      inputs_embeds=None,
      labels=None,
      output_attentions=None,
      output_hidden_states=None,
  ):
      r&quot;&quot;&quot;
      labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
          Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -
          1]``.
      &quot;&quot;&quot;
      outputs = self.encoder(
          input_ids,
          attention_mask=attention_mask,
          token_type_ids=token_type_ids,
          position_ids=position_ids,
          head_mask=head_mask,
          inputs_embeds=inputs_embeds,
          output_attentions=output_attentions,
          output_hidden_states=output_hidden_states,
      )

      sequence_output = outputs.last_hidden_state
      sequence_output = self.dropout(sequence_output)
      logits = self.classifier(sequence_output)

      loss = None
      if labels is not None:
          log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)
          loss = 0 - log_likelihood
      else:
          tags = self.crf.decode(logits)
      tags = torch.Tensor(tags)

      output = (tags,) + outputs[2:]
      return ((loss,) + output) if loss is not None else output


m = CamemBERTCRF(4)
t = CamembertTokenizerFast.from_pretrained(&quot;camembert-base&quot;)

print(m(**t(&quot;this is a test&quot;, return_tensors=&quot;pt&quot;), labels=torch.tensor([[1,2,3,2,3,1]])))
print(m(**t(&quot;this is a test&quot;, return_tensors=&quot;pt&quot;)))
</code></pre>
<p>Output:</p>
<pre><code>(tensor(8.0685, grad_fn=&lt;RsubBackward1&gt;), tensor([[2., 2., 2., 2., 2., 2.]]))
(tensor([[2., 2., 2., 2., 2., 2.]]),)
</code></pre>
",2,1,870,2022-04-01 09:05:01,https://stackoverflow.com/questions/71704422/combine-camembert-crf-for-token-classification
Is possible to get dependency/pos information for entities in Spacy?,"<p>I am working on extracting entities from scientific text (I am using <strong>scispacy</strong>) and later I will want to extract relations using hand-written rules. I have extracted entities and their character span successfully, and I can also get the pos and dependency tags for tokens and noun chunks. So I am comfortable with the two tasks separately, but I want to bring the two together and I have been stuck for a while.</p>
<p>The idea is that I want to be able to write rules such as: (just an example) if in a sentence/clause there are two entities where the first one is a 'DRUG/CHEMICAL' + is the <em>subject</em>, and the second one is a 'DISEASE' + is an <em>object</em> --&gt; (then) infer 'treatment' relation between the two.</p>
<p>If anyone has any hints on how to approach this task, I would really appreciate it. Thank you!</p>
<p>S.</p>
<p>What I am doing to <strong>extract entities</strong>:</p>
<p><code>doc = nlp(text-with-more-than-one-sent)</code></p>
<p><code>for ent in doc.ents:</code></p>
<pre><code>`... (get information about the ent e.g. its character span)`
</code></pre>
<p><strong>Getting dependency information (for noun chunks and for tokens):</strong></p>
<p><code>for chunk in doc.noun_chunks:</code></p>
<p><code>    print(f&quot;Text: {chunk.text}, Root text: {chunk.root.text}, Root dep: {chunk.root.dep_}, Root head text: {chunk.root.head.text}, POS: {chunk.root.head.pos_}&quot;)</code></p>
<p>_</p>
<p><code>for token in doc:</code></p>
<p><code>    print(f&quot;Text: {token.text}, DEP label: {token.dep_}, Head text: {token.head.text}, Head POS: {token.head.pos_}, Children: {[child for child in token.children]}&quot;)</code></p>
","nlp, spacy, named-entity-recognition, dependency-parsing","<p>You can use the <a href=""https://spacy.io/api/pipeline-functions#merge_entities"" rel=""nofollow noreferrer""><code>merge_entities</code></a> mini-component to convert entities to single tokens, which would simplify what you're trying to do. There's also a component to merge noun chunks similarly.</p>
",1,1,766,2022-04-03 13:11:50,https://stackoverflow.com/questions/71726244/is-possible-to-get-dependency-pos-information-for-entities-in-spacy
Add custom NER to Spacy 3 pipeline,"<p>I am trying to build a custom Spacy pipeline based off the en_core_web_sm pipeline. From what I can tell the ner has been added correctly as it is displayed in the pipe names when printed(see below). For some reason when the model is tested on text I am not getting any results but when the custom ner is used by itself the correct entities are extracted and labelled. I am using Spacy 3.0.8 and en_core_web_sm pipeline 3.0.0.</p>
<pre><code>import spacy


crypto_nlp = spacy.load('model-best')
nlp = spacy.load('en_core_web_sm')

nlp.add_pipe('ner', source=crypto_nlp, name=&quot;crypto_ner&quot;, before=&quot;ner&quot;)

print(nlp.pipe_names)

text = 'Ethereum'

doc = nlp(text)
for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p>Output: '['tok2vec', 'tagger', 'parser', 'crypto_ner', 'ner', 'attribute_ruler', 'lemmatizer']'</p>
<p>But when I use my ner model:</p>
<pre><code>doc = crypto_nlp(text)
for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p>Output: 'Ethereum ETH'</p>
","python, named-entity-recognition, spacy-3","<p>It's not clear from the details in the question, but my guess is that your <code>crypto_nlp</code> <code>ner</code> depends on a separate <code>tok2vec</code> component that's not being included when you source.</p>
<p>Since this <code>tok2vec</code> won't be shared, it's easiest to modify the <code>ner</code> component to include a standalone copy of the <code>tok2vec</code>, which is called &quot;replacing listeners&quot;: <a href=""https://spacy.io/api/language#replace_listeners"" rel=""nofollow noreferrer"">https://spacy.io/api/language#replace_listeners</a></p>
<p>If <code>crypto_nlp</code> has <code>nlp.pipe_names</code> as <code>['tok2vec', 'ner']</code>, then this should replace the listener before loading it into the second pipeline, so it's now a standalone component:</p>
<pre class=""lang-py prettyprint-override""><code>crypto_nlp.replace_listeners(&quot;tok2vec&quot;, &quot;ner&quot;, [&quot;model.tok2vec&quot;])
nlp.add_pipe('ner', source=crypto_nlp, name=&quot;crypto_ner&quot;, before=&quot;ner&quot;)
</code></pre>
",2,0,1661,2022-05-06 03:30:21,https://stackoverflow.com/questions/72135860/add-custom-ner-to-spacy-3-pipeline
"Encoding IOB format, entity nested inside other entity","<p>I have a dataset and I have to do named entity recognition with it. I would convert the dataset which is a json to IOB format but i have an issue:
The dataset contains entity nested in other entity, for example</p>
<p>Sentence:</p>
<pre><code>Traitement des manifestations neurologiques progressives des patients adultes et des enfants atteints de maladie de Niemann-Pick type C
</code></pre>
<p>Entity:</p>
<pre><code>&quot;manifestations neurologiques progressives des patients  atteints de maladie de Niemann-Pick type C&quot;    and     &quot;adultes&quot;    and    &quot;enfants&quot; 
</code></pre>
<p>How should I encode the bigger one with nested entity inside?</p>
<p>I thought about:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">word</th>
<th style=""text-align: left;"">tag</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Traitement</td>
<td style=""text-align: left;"">O</td>
</tr>
<tr>
<td style=""text-align: left;"">des</td>
<td style=""text-align: left;"">O</td>
</tr>
<tr>
<td style=""text-align: left;"">manifestations</td>
<td style=""text-align: left;"">B-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">neurologiques</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">progressives</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">des</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">patients</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">adultes</td>
<td style=""text-align: left;"">B-Caracteristique_du_sujet</td>
</tr>
<tr>
<td style=""text-align: left;"">et</td>
<td style=""text-align: left;"">O</td>
</tr>
<tr>
<td style=""text-align: left;"">des</td>
<td style=""text-align: left;"">O</td>
</tr>
<tr>
<td style=""text-align: left;"">enfants</td>
<td style=""text-align: left;"">B-Caracteristique_du_sujet</td>
</tr>
<tr>
<td style=""text-align: left;"">atteints</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">de</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">maladie</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">de</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">Niemann-Pick</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">type</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
<tr>
<td style=""text-align: left;"">C</td>
<td style=""text-align: left;"">I-Cible</td>
</tr>
</tbody>
</table>
</div>
<p>But i'm not sure it's correct and comprehensible for an algorithm then.</p>
","python, nlp, nltk, spacy, named-entity-recognition","<p>You can either take just the outermost layer and use that as NER training data, or you can use all your labels to create spans (including nested spans) and train a <a href=""https://spacy.io/api/spancategorizer/"" rel=""nofollow noreferrer"">span categorizer</a>. You might also want to look at the <a href=""https://github.com/explosion/projects/tree/v3/experimental/ner_spancat_compare"" rel=""nofollow noreferrer"">spancat example project</a>.</p>
<p>You can't represent nested spans with IOB format, so if you go that route, you'll need to manually create Doc objects with spans saved in <code>Doc.spans</code>.</p>
",1,1,473,2022-05-11 14:32:31,https://stackoverflow.com/questions/72202895/encoding-iob-format-entity-nested-inside-other-entity
How to use custom named enitities dataset in spacy&#39;s DependecyMatcher?,"<p>Suppose I have created a spacy model or dataset with all named entities, tagged as a PERSON, from a certain text. How can I apply it in DependencyMatcher, if I need to extract pairs &quot;person&quot; - &quot;root verb&quot;?
In other words I want DependencyMatcher to use not its custom model of identifying people's names, but my, already made, dataset of names.</p>
<pre><code>import spacy
from spacy.matcher import DependencyMatcher

nlp = spacy.load(&quot;en_core_web_lg&quot;)
def on_match(matcher, doc, id, matches):
    return matches

patterns = [
        [#pattern1 (sur)name Jack lived
        {
            &quot;RIGHT_ID&quot;: &quot;person&quot;,
            &quot;RIGHT_ATTRS&quot;: {&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;DEP&quot;: &quot;nsubj&quot;}
        },
        {
            &quot;LEFT_ID&quot;: &quot;person&quot;,
            &quot;REL_OP&quot;: &quot;&lt;&quot;,
            &quot;RIGHT_ID&quot;: &quot;verb&quot;,
            &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;VERB&quot;}
        }
        ]
matcher = DependencyMatcher(nlp.vocab)
matcher.add(&quot;PERVERB&quot;, patterns, on_match=on_match)
</code></pre>
","python, dependencies, spacy, named-entity-recognition, spacy-3","<p>The DependencyMatcher does not have a &quot;custom model of identifying people's names&quot; - that's the NER component in the pipeline you loaded. In this case you should:</p>
<ol>
<li>disable the NER component</li>
<li>Use an EntityRuler to label names</li>
<li>Use the DependencyMatcher as usual</li>
</ol>
<p>To disable a component you can just do this:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_lg&quot;, disable=[&quot;ner&quot;])
</code></pre>
<p>To match names from your list with an EntityRuler, see <a href=""https://spacy.io/usage/rule-based-matching#entityruler"" rel=""nofollow noreferrer"">the rule-based matching docs</a>.</p>
<hr />
<p>Note that the above assumes you have a list of names, rather than annotations in sentences on exactly what is a name. If you have explicitly annotated names, then you can skip step 2 - disabling the NER component will be enough to leave only your existing annotations.</p>
",0,1,33,2022-05-22 11:23:51,https://stackoverflow.com/questions/72337123/how-to-use-custom-named-enitities-dataset-in-spacys-dependecymatcher
Is there a way to exclude an apostrophe “s” from entities in spaCy’s NER component?,"<p>I am performing named entity recognition (NER) in the context of a custom entity linking component using spaCy 3.2 (I am running into the same issue with the latest 3.3 version as well).</p>
<p>When performing named entity recognition on texts that contain an apostrophe “s” (e.g. <code>Apple's</code>), I would like to exclude the apostrophe “s” from the named entity the component returns.</p>
<p>In instances where the named entity includes a single token in addition to the apostrophe “s”, the NER component correctly returns only the named entity token (e.g. <code>Apple</code> instead of <code>Apple's</code>):</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)

s = u&quot;Apple's looking at buying U.K. startup for $1 billion&quot;
doc = nlp(s)

for ent in doc.ents:
    print(f&quot;{ent.text} (lemma={ent.lemma_}): {ent.label_}\n&quot;)

Apple (lemma=Apple): ORG
U.K. (lemma=U.K.): GPE
$1 billion (lemma=$1 billion): MONEY
</code></pre>
<pre class=""lang-py prettyprint-override""><code>for token in doc:
    print(f&quot;{token.text} (IOB={token.ent_iob_}): {token.ent_type_}&quot;)

Apple (IOB=B): ORG
's (IOB=O): 
...
</code></pre>
<p>However, when I perform named entity recognition on texts where the named entity spans multiple tokens (e.g. <code>Apple Inc.</code> instead of <code>Apple</code>), the apostrophe “s” is included as a part of the returned named entity:</p>
<pre class=""lang-py prettyprint-override""><code>s = u&quot;Apple Inc.'s looking at buying U.K. startup for $1 billion&quot;
doc = nlp(s)

for ent in doc.ents:
    print(f&quot;{ent.text} (lemma={ent.lemma_}): {ent.label_}\n&quot;)

Apple Inc.'s (lemma=Apple Inc.'s): ORG
U.K. (lemma=U.K.): GPE
$1 billion (lemma=$1 billion): MONEY
</code></pre>
<pre class=""lang-py prettyprint-override""><code>for token in doc:
    print(f&quot;{token.text} (IOB={token.ent_iob_}): {token.ent_type_}&quot;)

Apple (IOB=B): ORG
Inc. (IOB=I): ORG
's (IOB=I): ORG
...
</code></pre>
<p>There is no difference in tokenization between these texts (i.e. the <code>'s</code> portion is split out as its own token in both cases). I do not want the apostrophe “s” to be included in the named entity for entities like <code>Apple Inc.'s</code> and would like the NER component to return only the <code>Apple Inc.</code> portion of this named entity.</p>
<p>Is there a way to configure the NER component to prevent this behavior with multi-token named entities and exclude the apostrophe “s”?</p>
","spacy, named-entity-recognition","<p>There is no way to simply configure the component not to do this. What you can do is use a small custom component to remove <code>'s</code> from any entities.</p>
<pre><code>def my_component(doc):
    out = []
    for ent in doc.ents:
        if ent[-1].text == &quot;'s&quot;:
            out.append(ent[0:-1])
        else:
            out.append(ent)
    doc.ents = out
    return doc
</code></pre>
<p>See <a href=""https://spacy.io/usage/processing-pipelines#custom-components"" rel=""nofollow noreferrer"">the docs</a> for info on how to use it.</p>
",0,2,346,2022-06-06 16:41:48,https://stackoverflow.com/questions/72520863/is-there-a-way-to-exclude-an-apostrophe-s-from-entities-in-spacy-s-ner-compone
NER - how to check if a common noun indicates a place (subcategorization),"<p>I am looking for a way to find, in a sentence, if a common noun refers to places. This is easy for proper nouns, but I didn't find any straightforward solution for common nouns.</p>
<p>For example, given the sentence <em>&quot;After a violent and brutal attack, a group of college students travel into the countryside to find refuge from the town they fled, but soon discover that the small village is also home to a coven of serial killers&quot;</em> I would like to mark the following nouns as referred to places: <em>countryside</em>, <em>town</em>, <em>small village</em>, <em>home</em>.</p>
<p>Here is the code I'm using:</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

# Process whole documents
text = (&quot;After a violent and brutal attack, a group of college students travel into the countryside to find refuge from the town they fled, but soon discover that the small village is also home to a coven of satanic serial killers&quot;)
doc = nlp(text)

# Analyze syntax
print(&quot;Noun phrases:&quot;, [chunk.text for chunk in doc.noun_chunks])
print(&quot;Verbs:&quot;, [token.lemma_ for token in doc if token.pos_ == &quot;VERB&quot;])

# Find named entities, phrases and concepts
for entity in doc.ents:
    print(entity.text, entity.label_)
</code></pre>
<p>Which gives as output the following:</p>
<pre><code>Noun phrases: ['a violent and brutal attack', 'a group', 'college students', 'the countryside', 'refuge', 'the town', 'they', 'the small village', 'a coven', 'serial killers']
Verbs: ['travel', 'find', 'flee', 'discover']
</code></pre>
","python, nlp, spacy, named-entity-recognition","<p>You can use <a href=""https://www.nltk.org/howto/wordnet.html"" rel=""nofollow noreferrer"">WordNet</a> for this.</p>
<pre><code>from nltk.corpus import wordnet as wn

loc = wn.synsets(&quot;location&quot;)[0]

def is_location(candidate):
    for ss in wn.synsets(candidate):
        # only get those where the synset matches exactly
        name = ss.name().split(&quot;.&quot;, 1)[0]
        if name != candidate:
            continue
        hit = loc.lowest_common_hypernyms(ss)
        if hit and hit[0] == loc:
            return True
    return False

# true things
for word in (&quot;countryside&quot;, &quot;town&quot;, &quot;village&quot;, &quot;home&quot;):
    print(is_location(word), word, sep=&quot;\t&quot;)

# false things
for word in (&quot;cat&quot;, &quot;dog&quot;, &quot;fish&quot;, &quot;cabbage&quot;, &quot;knife&quot;):
    print(is_location(word), word, sep=&quot;\t&quot;)

</code></pre>
<p>Note that sometimes the synsets are wonky, so be sure to double-check everything.</p>
<p>Also, for things like &quot;small village&quot;, you'll have to pull out the head noun, but it'll just be the last word.</p>
",3,1,480,2022-06-26 10:16:44,https://stackoverflow.com/questions/72760674/ner-how-to-check-if-a-common-noun-indicates-a-place-subcategorization
Spacy - adding multiple patterns to a single NER using entity ruler,"<p>so this is my problem in Spacy Rule based matching.</p>
<p>I have a txt group say</p>
<p><strong>text = ('Wan, Flex, Havelock St, WAN, premium, Fibre, 15a,  UK, Fletcher inc, Fletcher, Princeton Street, Fendalton road, Bealey avenue)</strong></p>
<p><strong>doc = nlp3(text)</strong></p>
<p><strong><strong>for ent in doc.ents:<br />
print(ent, '|', ent.label_)</strong></strong></p>
<p>#This provides me a result where : <strong>Wan, WAN are classified as persons and Fibre as an ORG</strong></p>
<p>#Now when I build my custom pattern using entity ruler</p>
<p>**nlp3 = spacy.load(&quot;en_core_web_sm&quot;)</p>
<p>ruler = nlp3.add_pipe(&quot;entity_ruler&quot;, before=&quot;ner&quot;)**</p>
<p>#List of Entities and Patterns</p>
<p><strong>patterns = [{&quot;label&quot;: &quot;PRODUCT&quot;, &quot;pattern&quot;: [{&quot;LOWER&quot;: &quot;wan&quot;}, {&quot;LOWER&quot;: &quot;fibre&quot;}, {&quot;LOWER&quot;: &quot;flex&quot;},{&quot;LOWER&quot;: &quot;premium&quot;},{&quot;LOWER&quot;: &quot;standard&quot;},{&quot;LOWER&quot;: &quot;service&quot;}]}]</strong></p>
<p>ruler.add_patterns(patterns)</p>
<p>nlp3.pipe_names</p>
<p><strong>Even after this</strong> when I run I get <strong>Wan classified as person (while I wish to see WAN, wan, Fibre classified as Product)</strong>. What am I doing wrong in adding patterns here. And is there a way I can add multiple patterns in a single dictionary to a label. Any help in this regard is appreciated.</p>
","pattern-matching, spacy, named-entity-recognition","<p>Each pattern you add to the Ruler is one sequence of tokens. So you aren't matching each of those terms individually, you're matching all of them in a row, without punctuation. You should add them as separate patterns, something like this:</p>
<pre><code>words = (&quot;wan&quot;, &quot;fibre&quot;, ...)
patterns = []
for word in words:
    patterns.append({&quot;label&quot;:&quot;PRODUCT&quot;, &quot;pattern&quot;:[{&quot;LOWER&quot;:word}]})
</code></pre>
<p>Couple of other things:</p>
<ul>
<li>you may need to set <code>overwrite_ents = True</code> to get the results you want, see <a href=""https://spacy.io/api/entityruler/#section-config"" rel=""nofollow noreferrer"">here</a>.</li>
<li>if your actual input looks like &quot;Wan, Flex, Havelock St, WAN, premium, ...&quot;, that's not the normal prose the spaCy models were trained on, and they may not work very well.</li>
</ul>
",1,1,1514,2022-06-27 12:54:00,https://stackoverflow.com/questions/72772448/spacy-adding-multiple-patterns-to-a-single-ner-using-entity-ruler
spacy v3.3:- Getting zero loss as well as other metric during training using cli,"<p>I have used the commands which are provided in the <a href=""https://spacy.io/usage/training#quickstart"" rel=""nofollow noreferrer"">spacy document</a>. I followed all the below steps:-</p>
<ol>
<li>Using the spacy format for creating the model
<code>TRAIN_DATA =[ (&quot;Pizza is a common fast food.&quot;, {&quot;entities&quot;: [(0, 5, &quot;FOOD&quot;)]}), (&quot;Pasta is an italian recipe&quot;, {&quot;entities&quot;: [(0, 5, &quot;FOOD&quot;)]})]</code></li>
<li>Converted the train and dev data in .spacy files using below code:-</li>
</ol>
<pre><code>import os
from tqdm import tqdm
import spacy
from spacy.tokens import DocBin

nlp = spacy.load(&quot;en_core_web_sm&quot;) # load other spacy model

db = DocBin() # create a DocBin object

for text, annot in tqdm(TRAIN_DATA): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        if span is None:
            print(&quot;Skipping entity&quot;)
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)
db.to_disk(&quot;./train.spacy&quot;) # save the docbin object```
Similarly I converted for dev.spacy.

3.Using base spacy configuration file converted it to config.cfg
```python -m spacy init fill-config base_config.cfg config.cfg```
4. Training the model
```python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy```
5. Getting the below output:-[![spacy training output][1]][1]

Please let me know if there is anything I am doing wrong here. Thanks in advance.

  [1]: https://i.sstatic.net/FfrBX.png
</code></pre>
","python-3.x, spacy, named-entity-recognition","<p>It looks like your data is NER annotations, but your pipeline contains only a tok2vec and parser component. It should contain an NER component. Use the <a href=""https://spacy.io/usage/training#quickstart"" rel=""nofollow noreferrer"">quickstart</a> to generate an NER config and start over from step 3 in your list.</p>
",1,1,554,2022-06-27 18:39:22,https://stackoverflow.com/questions/72776930/spacy-v3-3-getting-zero-loss-as-well-as-other-metric-during-training-using-cli
How do I view the spacy NER softmax values?,"<p>I'm trying to obtain the softmax predictions for each output class from the spacy NER model. When I place a break point at 'preds' in the code below and skip through the pipeline until the predict method is being called on the NER model pipeline component I can see that object returned from the self._func call is a 'ParserStepModel' object.</p>
<pre><code>import spacy
from thinc.model import Model, InT, OutT

def predict(self, X:InT) -&gt; OutT:

    preds = self._func(self, X, is_train=False)[0]

    return preds

Model.predict = predict

nlp = spacy.load('en_core_web_sm')

def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text + ' - ' + str(ent.start_char) + ' - ' + str(ent.end) + ' - ' +
                  ent.label_ + ' - ' + str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')

doc = nlp('Apple is looking at buying U.K. startup for $1 billion')

show_ents(doc)
</code></pre>
<p>I assume that the 'ParserStepModel' object contains the results of processing the input text as I can see the object contains the properties 'tokvec' and the model 'vec2scores'. I was therefore assuming that if were to run the model and the vectorised input i.e.</p>
<pre><code>preds.vec2scores(preds.tokvecs, is_train = False)
</code></pre>
<p>The resulting array would be a softmax prediction for each of the entities. However the outputs don't appear to change if I set is_train = True. I was hoping someone could explain how I can view the softmax predictions from the NER model and which entities the softmax predictions relate to?</p>
","python, spacy, named-entity-recognition, spacy-3","<p>The NER component uses a transition-based parsing model that doesn't really provide useful scores for individual entity predictions.</p>
<p>If you need meaningful confidence scores for entity predictions, train a <a href=""https://spacy.io/api/spancategorizer"" rel=""noreferrer""><code>spancat</code></a> component instead of <code>ner</code>. The scores are saved under <code>doc[spans_key].attrs[&quot;scores&quot;]</code>.</p>
<p>Some related threads:</p>
<ul>
<li><a href=""https://github.com/explosion/spaCy/issues/831"" rel=""noreferrer"">https://github.com/explosion/spaCy/issues/831</a></li>
<li><a href=""https://github.com/explosion/spaCy/discussions/9189"" rel=""noreferrer"">https://github.com/explosion/spaCy/discussions/9189</a></li>
</ul>
",5,2,595,2022-07-05 19:55:33,https://stackoverflow.com/questions/72874877/how-do-i-view-the-spacy-ner-softmax-values
How make multi word entities a single word?,"<p>I have a pandas column with multiple word entities. I want to make the entities labeled as PERSON a single word. My input looks like this:</p>
<pre><code>    Text
 Vote for Donald Trump
 Vote for Barack Obama
 Vote for Bernie Sanders
 Move to another location
 Support LaVaughn Robinson
 Support Michelle LaVaughn Robinson
 Support Sanders
</code></pre>
<p>I need my output to look like this:</p>
<pre><code>   Text
Vote for Donald_Trump
Vote for Barack_Obama
Vote for Bernie_Sanders
Move to another location
Support LaVaughn_Robinson
Support Michelle_LaVaughn_Robinson
Support Sanders
</code></pre>
<p>My first though was to use Spacy NER, return the PERSON and later combined the words returned, but I'm getting words that are not NER.  Can I do it using BILOU? Is there any other way to do it?</p>
<p>Updated question</p>
<p>I have a new dataset that have 2 columns. Spacy NER seems to performe best, but it only identifies the NER(persons) on the column complete_text. When I run it on the column incomplete_text, it cannot identify the NER(persons). Is there a way to map the person identify on the column complete_text and matched  it with the person on column incomplete_text. I'm sorry if this sounds confusing. This is how my dataset looks like:</p>
<pre><code>complete_text                             incomplete_text                            
everyone to vote for Marine Le Pen     vote for Marine Le Pen
</code></pre>
<p>When I use spacy to get the person on both the complete_text column and the incomplete_text column, It only returns the person on the complete_text not on the incomplete_text. I want  want to match the person identified on the complete_text column with the person on the incomplete_text column and return the person identified as a single word.</p>
<pre><code>complete_text                           spacy_complete_text_person   incomplete_text                spacy_incomplete_text__person      result                                         
everyone to vote for Marine Le Pen     [Marine Le Pen]                 vote for Marine Le Pen                  []                      vote for Marine_Le_Pen
</code></pre>
","python, named-entity-recognition","<p>You can use <code>BERT</code> model from <a href=""https://huggingface.co/dslim/bert-base-NER"" rel=""nofollow noreferrer""><code>dslim/bert-base-NER</code></a> using <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer""><code>transformers</code></a>:</p>
<pre><code># Python env: pip install transformers
# Anaconda env: conda install transformers
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

def process_person(txt):
    # Currently not optimized for Pandas
    l = nlp(txt)
    for d in l:
        if d['entity_group'] == 'PER':
            s = d['start']
            e = d['end']
            txt = txt[:s] + txt[s:e+1].replace(' ', '_') + txt[e:]
    return txt

tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-large-NER&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-large-NER&quot;)

nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer, aggregation_strategy='max')
df['Text2'] = df['Text'].apply(process_person)
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; df
                                 Text                               Text2
0               Vote for Donald Trump               Vote for Donald_Trump
1               Vote for Barack Obama               Vote for Barack_Obama
2             Vote for Bernie Sanders             Vote for Bernie_Sanders
3            Move to another location            Move to another location
4           Support LaVaughn Robinson           Support LaVaughn_Robinson
5  Support Michelle LaVaughn Robinson  Support Michelle_LaVaughn_Robinson
6                     Support Sanders                     Support Sanders
7                           RT Please                           RT Please
8                            STOP luc                            STOP luc
9                           Kick Some                           Kick Some
</code></pre>
",1,1,724,2022-07-10 04:09:16,https://stackoverflow.com/questions/72926089/how-make-multi-word-entities-a-single-word
Use Spacy NER to identify person and make person one word?,"<p>I want to use Spacy NER to identify the PERSON and make it one word.</p>
<p>My dataset looks like this:</p>
<pre><code>text     
use your superpowers
vote for Barack Obama
vote for Marine Le Pen
play with Michael Jordan
support the supporters
</code></pre>
<p>I want my final output to look like this:</p>
<pre><code>text     
use your superpowers
vote for Barack_Obama
vote for Marine_Le_Pen
play with Michael_Jordan
support the supporters
</code></pre>
<p>This is the code I have so far:</p>
<pre><code> def get_ner (string):
     nlp = spacy.load(&quot;en_core_web_trf&quot;)
     doc = nlp(string)
     for token.text in doc:
         if token.ents==&quot;Person&quot;:
         s= ent['start']
         e= ent['end']
         txt = txt[:s] + txt[s:e+1].replace(' ', '_') + txt[e:]
     return txt

 df['text']= df.text.apply(get_ner)
</code></pre>
<p>When I use the code above, I'm getting an error message.</p>
<pre><code>AttributeError: name 'token' is not defined
</code></pre>
","pandas, spacy, named-entity-recognition","<p>If you use <code>Spacy</code>, you code should be:</p>
<pre><code>nlp = spacy.load('en_core_web_trf')

def get_ner(txt):
    doc = nlp(txt)
    for ent in doc.ents:
        if ent.label_ == 'PERSON':
            s = ent.start_char
            e = ent.end_char
            txt = txt[:s] + txt[s:e+1].replace(' ', '_') + txt[e:]
    return txt

df['text'] = df['text'].apply(get_ner)
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; df
                       text
0      use your superpowers
1     vote for Barack_Obama
2    vote for Marine_Le_Pen
3  play with Michael_Jordan
4    support the supporters
</code></pre>
",1,1,763,2022-07-15 14:15:34,https://stackoverflow.com/questions/72995392/use-spacy-ner-to-identify-person-and-make-person-one-word
Why don&#39;t spacy transformer models do NER for non-english models?,"<p>Why is it that spacy transformer models for languages like spanish (<code>es_dep_news_trf</code>) don't do named entity recognition.</p>
<p>However, for english (<code>en_core_web_trf</code>) it does.</p>
<p>In code:</p>
<pre><code>import spacy    
nlp=spacy.load(&quot;en_core_web_trf&quot;)
doc=nlp(&quot;my name is John Smith and I work at Apple and I like visiting the Eiffel Tower&quot;)
print(doc.ents)
(John Smith, Apple, the Eiffel Tower)
    
nlp=spacy.load(&quot;es_dep_news_trf&quot;)
doc=nlp(&quot;mi nombre es John Smith y trabajo en Apple y me gusta visitar la Torre Eiffel&quot;)
print(doc.ents)
()
</code></pre>
<p>Why doesn't spanish extract entities but english does?</p>
","spacy, named-entity-recognition, spacy-transformers","<p>It has to do with the available training data. <code>ner</code> is only included for the <code>trf</code> models if the training data has NER annotation on the exact same data as for tagging and parsing.</p>
<p>Training <code>trf</code> models on partial annotation does not work well in practice and an independent NER component (as in the CNN pipelines) would mean including an additional <code>transformer</code> component in the pipeline, which would make the pipeline a lot larger and slower.</p>
",1,0,905,2022-07-18 15:06:38,https://stackoverflow.com/questions/73024546/why-dont-spacy-transformer-models-do-ner-for-non-english-models
How do I extract full entity names from a hugging face model without IO tags,"<p>I am using a model from hugging face, specifically <code>Davlan/distilbert-base-multilingual-cased-ner-hrl</code>. However, I am not able to extract full entity names from the result.</p>
<p>If I run the following code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;Davlan/distilbert-base-multilingual-cased-ner-hrl&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;Davlan/distilbert-base-multilingual-cased-ner-hrl&quot;)
nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)

example = &quot;My name is Johnathan Smith and I work at Apple&quot;
ner_results = nlp(example, aggregation_strategy=&quot;max&quot;)
print(ner_results)
</code></pre>
<p>Then I get output:</p>
<pre><code>[{'entity': 'B-PER', 'score': 0.9998949, 'index': 4, 'word': 'Johna', 'start': 11, 'end': 16}, {'entity': 'I-PER', 'score': 0.999726, 'index': 5, 'word': '##tha', 'start': 16, 'end': 19}, {'entity': 'I-PER', 'score': 0.9997751, 'index': 6, 'word': '##n', 'start': 19, 'end': 20}, {'entity': 'I-PER', 'score': 0.99974835, 'index': 7, 'word': 'Smith', 'start': 21, 'end': 26}, {'entity': 'B-ORG', 'score': 0.99870986, 'index': 12, 'word': 'Apple', 'start': 41, 'end': 46}]
</code></pre>
<p>It looks like I might be able to post process this so <code>Jonathan Smith</code> is all one word. But ideally I would like this to be done for me and have no partial words identified.</p>
","nlp, huggingface-transformers, named-entity-recognition","<p>There is a bug in the code. The aggregation strategy is in the wrong place. It should read:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;Davlan/distilbert-base-multilingual-cased-ner-hrl&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;Davlan/distilbert-base-multilingual-cased-ner-hrl&quot;)
nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer, aggregation_strategy=&quot;max&quot;)

example = &quot;My name is Johnathan Smith and I work at Apple&quot;
ner_results = nlp(example)
print(ner_results)
</code></pre>
<p>Which gives:</p>
<pre><code>[{'entity_group': 'PER', 'score': 0.99982166, 'word': 'Johnathan Smith', 'start': 11, 'end': 26}, {'entity_group': 'ORG', 'score': 0.99870986, 'word': 'Apple', 'start': 41, 'end': 46}]
</code></pre>
",6,5,1710,2022-07-19 08:30:30,https://stackoverflow.com/questions/73033651/how-do-i-extract-full-entity-names-from-a-hugging-face-model-without-io-tags
How to get a graph with the best performing runs via Sweeps (Weights &amp; Biases)?,"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?</p>
","machine-learning, nlp, named-entity-recognition, wandb","<p>In the sweep view, you can filter runs by certain criteria by clicking this button:
<a href=""https://i.sstatic.net/kVFUC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kVFUC.png"" alt=""enter image description here"" /></a></p>
<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:
<a href=""https://i.sstatic.net/tAVJF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tAVJF.png"" alt=""enter image description here"" /></a></p>
<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).</p>
",3,0,266,2022-07-21 07:39:22,https://stackoverflow.com/questions/73062370/how-to-get-a-graph-with-the-best-performing-runs-via-sweeps-weights-biases
Apply name-entity recognition on specific dataframe columns,"<p>I have the following dataframe:</p>
<pre><code>df = pd.DataFrame({'source': ['Paul', 'Paul'],
                   'target': ['GOOGLE', 'Ferrari'],
                   'edge': ['works at', 'drive']
                   })

df
    source  target  edge
0   Paul    GOOGLE  works at
1   Paul    Ferrari drive
</code></pre>
<p>I want to apply <code>Name-Entity Recognition(NER)</code> on the columns.</p>
<p>Expected outcome:</p>
<pre><code>    source  target        edge
0   PERSON  ORGANIZATION  works at
1   PERSON  CAR           drive
</code></pre>
<p>I tried the following function:</p>
<pre><code>!python -m spacy download en_core_web_sm

import spacy
nlp = spacy.load('en_core_web_sm')

def ner(df):
    df['source_entities'] = df['source'].apply(lambda x: nlp(x).label_)
    df['target_entities'] = df['target'].apply(lambda x: nlp(x).label_)
    return df
</code></pre>
<p>But when I call the function <code>ner(df)</code> I get back an error:</p>
<pre><code>AttributeError: 'spacy.tokens.doc.Doc' object has no attribute 'label_'
</code></pre>
<p>Any ideas on how to reach the expected outcome?</p>
","python, python-3.x, dataframe, nlp, named-entity-recognition","<p>You are trying to get label_ attribute from list as nlp(x) return list of object. Because of which you are getting that error.</p>
<p>Replace</p>
<pre><code>def ner(df):
  df['source_entities'] = df['source'].apply(lambda x: nlp(x).label_)
  df['target_entities'] = df['target'].apply(lambda x: nlp(x).label_)
  return df
</code></pre>
<p>With</p>
<pre><code>def ner(df):
  df['source_entities'] = df['source'].apply(lambda x: [ent.label_ for ent in nlp(x).ents])
  df['target_entities'] = df['target'].apply(lambda x: [ent.label_ for ent in nlp(x).ents])
  return df
</code></pre>
",2,2,1292,2022-07-25 16:03:52,https://stackoverflow.com/questions/73112216/apply-name-entity-recognition-on-specific-dataframe-columns
How to identify body part names in a text with python,"<p>I am trying to specify whether an entity is a body part. For example in &quot;Other specified disorders of the right ear,&quot; I want to be able to identify the right ear as an entity. I tried some named entity recognition methods but they identify all entities, not just the body parts. I tried using scispacy to do so but I have not managed so far. I tried concise_concepts from spacy to create a separate entity for body parts but that didn't work either. Please guide me through how I can do that and a snippet code would be appreciated.</p>
","python, nlp, spacy, named-entity-recognition","<p>So this code based on <a href=""https://stackoverflow.com/questions/72760674/ner-how-to-check-if-a-common-noun-indicates-a-place-subcategorization/72761290#72761290"">this asnwer</a> almost detects everything correctly but it needs lemmatization.</p>
<pre><code>from nltk.corpus import wordnet as wn
import nltk 
nltk.download('wordnet')
part = wn.synsets('body_part')[0]

def is_body_part(candidate):
    for ss in wn.synsets(candidate):
        # only get those where the synset matches exactly
        name = ss.name().split(&quot;.&quot;, 1)[0]
        if name != candidate:
            continue
        hit = part.lowest_common_hypernyms(ss)
        if hit and hit[0] == part:
            return True
    return False

# true things
for word in (&quot;finger&quot;, &quot;hand&quot;, &quot;head&quot;, &quot;feet&quot;, 'foot', 'hair'):
    print(is_body_part(word), word, sep=&quot;\t&quot;)

# false things
for word in (&quot;cat&quot;, &quot;dog&quot;, &quot;fish&quot;, &quot;cabbage&quot;, &quot;knife&quot;):
    print(is_body_part(word), word, sep=&quot;\t&quot;)
</code></pre>
<p>Output:</p>
<pre><code>True    finger
True    hand
True    head
False   feet # have to lemmatize it to foot
True    foot
True    hair
False   cat
False   dog
False   fish
False   cabbage
False   knife
</code></pre>
",1,1,992,2022-07-26 02:22:13,https://stackoverflow.com/questions/73117109/how-to-identify-body-part-names-in-a-text-with-python
How to keep structure of text after feeding it to a pipeline for NER,"<p>I've build an NER (named entity recognition) model, based on a HuggingFace existing model and that I fine-tuned to recognize my custom entities. The text I want to run my model on is in a <code>txt</code> file.</p>
<p>The code of how I use the model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

# loading the fine-tuned model
ner_pipeline =  pipeline('token-classification', model=&quot;./my-model.model/&quot;, tokenizer=&quot;./my-model.model/&quot;, ignore_labels=[])

with open(my_file, 'r', encoding=&quot;utf8&quot;) as f:
  lines = f.readlines()
  joined_lines = ' '.join(lines)

  result = ner_pipeline(joined_lines, aggregation_strategy='first')
  text = &quot;&quot;
      
  for group in result:
     if group[&quot;entity_group&quot;] != 'O':
        # substitute the entity with its tag
        text += group[&quot;entity_group&quot;]+ &quot; &quot;
     else:
        text += group[&quot;word&quot;] + &quot; &quot;
</code></pre>
<p>Basically what I do is substituting the entities recognized with the entity tag, and leave the rest of the text as is.</p>
<p>With my code, the final <code>text</code> is filled with the content exactly as I want it, but the structure is lost. While doing <code>' '.join(lines)</code> I'm basically throwing away the <code>\n</code>s inside the text, that however I would like to keep in my reconstructed text.</p>
<p>I've tried feeding the pipeline with single sentences (each of the <code>f.readlines()</code>) end not the full joined text, but the results are far worse. The model works a lot better predicting on the whole text.</p>
<p>Does anyone knows a way how I could keep or retrieve the structure of the original text? Thanks.</p>
","python, nlp, tokenize, huggingface-transformers, named-entity-recognition","<p>The <code>group</code>s have a <code>start</code> and <code>end</code> index that tell you which part of the input string each label corresponds to. I.e., you can pass the text <em>as a whole</em>, with the newlines intact (<code>ner_pipeline(f.read(), ...)</code>) and subsequently replace substrings.</p>
<p>Here's a working, minimal reproducible example. The only thing to note here is that we replace from right to left (<code>result[::-1]</code>) so we don't mess up the indices of subsequent labels by changing the length of the string when replacing.</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.corpus import brown # for example data
from transformers import pipeline

ner_pipeline =  pipeline('token-classification')

# equivalent to f.read()
text = '\n'.join(' '.join(sent) for sent in brown.sents()[:100])

result = ner_pipeline(lines_joined, aggregation_strategy='first')

def replace_at(label, start, end, txt):
    &quot;&quot;&quot;Replace substring of txt from start to end with label&quot;&quot;&quot;
    return ''.join((txt[:start], label, txt[end:]))

# Substitution
for group in result[::-1]:
    ent = group[&quot;entity_group&quot;]
    if ent != 'ORG': # for testing since there's no 'O' in the default model
        text = replace_at(ent, group['start'], group['end'], text)

sentences = text.split('\n')
</code></pre>
<p>Example input/output (first line):</p>
<pre class=""lang-py prettyprint-override""><code>&quot;The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .&quot;
</code></pre>
<p>After processing:</p>
<pre class=""lang-py prettyprint-override""><code>&quot;The Fulton County Grand Jury said Friday an investigation of LOC's recent primary election produced `` no evidence '' that any irregularities took place .&quot;
                                                              ^^^
</code></pre>
",3,1,646,2022-08-04 10:51:25,https://stackoverflow.com/questions/73234626/how-to-keep-structure-of-text-after-feeding-it-to-a-pipeline-for-ner
Custom Name Entity Regognition,"<p>I have the following sentence:</p>
<pre><code>text=&quot;The weather is extremely severe in England&quot;
</code></pre>
<p>I want to perform a custom <code>Name Entity Recognition (NER)</code> procedure</p>
<p>First a normal <code>NER</code> procedure will output <code>England</code> with a <code>GPE</code> label</p>
<pre><code>pip install spacy

!python -m spacy download en_core_web_lg

import spacy
nlp = spacy.load('en_core_web_lg')

doc = nlp(text)

for ent in doc.ents:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))

Result: England - GPE - Countries, cities, states
</code></pre>
<p>However, I want the whole sentence to take the tag <code>High-Severity</code>.</p>
<p>So I am doing the following procedure:</p>
<pre><code>from spacy.strings import StringStore

new_hash = StringStore([u'High_Severity']) # &lt;-- match id
nlp.vocab.strings.add('High_Severity')

from spacy.tokens import Span

# Get the hash value of the ORG entity label
High_Severity = doc.vocab.strings[u'High_Severity']  

# Create a Span for the new entity
new_ent = Span(doc, 0, 7, label=High_Severity)

# Add the entity to the existing Doc object
doc.ents = list(doc.ents) + [new_ent]
</code></pre>
<p>I am taking the following error:</p>
<pre><code>ValueError: [E1010] Unable to set entity information for token 6 which is included in more than one span in entities, blocked, missing or outside.
</code></pre>
<p>From my understanding, this is happening because <code>NER</code> has already recognised <code>England</code> as <code>GRE</code> and cannot add a label over the existing label.</p>
<p>I tried to execute the custom <code>NER</code> code (i.e, without first running the normal <code>NER</code> code) but this did not solve my problem.</p>
<p>Any ideas on how to Solve this problem?</p>
","python, python-3.x, nlp, spacy, named-entity-recognition","<p>Indeed it looks like NER do not allow overlapping, and that is your problem, your second part of the code tries to create a ner containing another ner, hence, it fails.
see in:</p>
<p><a href=""https://github.com/explosion/spaCy/discussions/10885"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/discussions/10885</a></p>
<p>and therefore spacy has spans categorization.</p>
<p>I did not find yet the way to characterized a predefined span (not coming from a trained model)</p>
",1,3,240,2022-08-08 13:59:47,https://stackoverflow.com/questions/73279102/custom-name-entity-regognition
In spacy: Add a span (doc[a:b]) as entity in a spacy doc (python),"<p>I am using regex over a whole document to catch the spans in which such regex occurs:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
import re

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;The United States of America (USA) are commonly known as the United States (U.S. or US) or America.&quot;)

expression = r&quot;[Uu](nited|\.?) ?[Ss](tates|\.?)&quot;
for match in re.finditer(expression, doc.text):
    start, end = match.span()
    span = doc.char_span(start, end)
    # This is a Span object or None 
    # if match doesn't map to valid token sequence
    if span is not None:
        print(&quot;Found match:&quot;, span.text)
</code></pre>
<p>There is a way to get the span (list of tokens) corresponding to the regex match on the doc even if the boundaries of the regex match do not correspond to token boundaries.
See:
How can I expand the match to a valid token sequence? In <a href=""https://spacy.io/usage/rule-based-matching"" rel=""nofollow noreferrer"">https://spacy.io/usage/rule-based-matching</a></p>
<p>So far so good.</p>
<p>Now that I have a collectuon of spans how do I convert them into entities?
I am aware of the entity ruler:
The EntityRuler is a pipeline component (see also the link above) but that entityruler takes patterns as inputs to search in the doc and not spans.</p>
<p>If I want to use regex over the whole document to get the collection os spans I want to convert into ents what is the next step here? Entityruler? How? Or something else?</p>
<p>Put simpler:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;The aplicable law is article 102 section b sentence 6 that deals with robery&quot;)
</code></pre>
<p>I would like to generate an spacy ent (entity) out of doc[5,10] with label &quot;law&quot; in order to be able to:
A) loop over all the law entities in the texts
B) use the visualizer to display the different entities contained in the doc</p>
","python, nlp, spacy, named-entity-recognition","<p>The most flexible way to add spans as entities to a doc is to use <code>Doc.set_ents</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.tokens import Span

span = doc.char_span(start, end, label=&quot;ENT&quot;)
doc.set_ents(entities=[span], default=&quot;unmodified&quot;)
</code></pre>
<p>Use the <code>default</code> option to specify how to set all the other tokens in the doc. By default the other tokens are set to <code>O</code>, but you can use <code>default=&quot;unmodified&quot;</code> to leave them untouched, e.g. if you're adding entities incrementally.</p>
<p><a href=""https://spacy.io/api/doc#set_ents"" rel=""noreferrer"">https://spacy.io/api/doc#set_ents</a></p>
",5,4,1958,2022-08-11 04:45:47,https://stackoverflow.com/questions/73315383/in-spacy-add-a-span-docab-as-entity-in-a-spacy-doc-python
How to make spaCy NER ignore comparisons,"<p>I am trying to parse sentences using spaCy and I want to ignore comparisons, but I have no idea how that is done.</p>
<p>For example if I have the sentence <code>Java, unlike C, has a garbage collector</code>.</p>
<p>I only want &quot;Java&quot; to be recognized as the NER that is being addressed in the sentence based on its context.</p>
<p>I am sorry if my title is off, I am currently trying with NER, but maybe I need something else?</p>
","spacy, named-entity-recognition, spacy-3","<p>Using only NER will not allow you to do that.</p>
<p>By using a parser in combination with NER you should be able to identify the subject (<code>nsubj</code>) of your sentences, which seem to be the words of interest to you.</p>
<p>You will need to use a good model, though. I got good results on the example you gave in comment using <code>en_core_web_trf</code>.</p>
",1,1,257,2022-08-14 22:40:58,https://stackoverflow.com/questions/73355551/how-to-make-spacy-ner-ignore-comparisons
How to resolve the error name:_name = &quot;label&quot; if &quot;label&quot; in features[0].keys() else &quot;labels&quot; in Hugging face NER,"<p>I am trying to run custom NER on my data using offset values. I tried to replicate using this link &lt;&lt; <a href=""https://huggingface.co/course/chapter7/2"" rel=""nofollow noreferrer"">https://huggingface.co/course/chapter7/2</a> &gt;&gt;</p>
<p>I keep getting the error</p>
<pre><code>variable name:_name = &quot;label&quot; if &quot;label&quot; in features[0].keys() else &quot;labels&quot;
</code></pre>
<p><strong>DATA BEFORE  tokenize_and_align_labels FUNCTIONS</strong></p>
<pre><code>{'texts': ['WASHINGTON USA WA DRIVER LICENSE BESSETTE Lamma 4d DL 73235766 9 Class AM to Iss 22/03/2021 Ab Exp 07130/2021 DOB 2/28/21 1 BESSETTE 2 GERALD 8 6930 NE Grandview Blvd, keyport, WA 86494 073076 12 Restrictions A 9a End P 16 Hgt 5\'-04&quot; 15 Sex F 18 Eyes BLU 5 DD 73235766900000000000 Gerald Bessette', ] }
tag_names': [
    
[
{'start': 281, 'end': 296, 'tag': 'PERSON_NAME', 'text': 'Gerald Bessette'}, 
{'start': 135, 'end': 141, 'tag': 'FIRST_NAME', 'text': 'GERALD'}, 
{'start': 124, 'end': 122, 'tag': 'LAST_NAME', 'text': 'BESSETTE'}, 
{'start': 81, 'end': 81, 'tag': 'ISSUE_DATE', 'text': '22/03/2021'}, 
{'start': 99, 'end': 109, 'tag': 'EXPIRY_DATE', 'text': '07130/2021'}, 
{'start': 114, 'end': 121, 'tag': 'DATE_OF_BIRTH', 'text': '2/28/21'}, 
 {'start': 51, 'end': 59, 'tag': 'DRIVER_LICENSE_NUMBER', 'text': '73235766'}, 
{'start': 144, 'end': 185, 'tag': 'ADDRESS', 'text': '6930 NE Grandview Blvd, keyport, WA 86494'}
 ], 
    

</code></pre>
<p><strong>DATA AFTER  tokenize_and_align_labels FUNCTIONS</strong></p>
<pre><code>{'input_ids': 
 [[0, 305, 8684, 2805, 9342, 10994, 26994, 42560, 39951, 163, 12147, 3935, 6433, 6887, 1916, 204, 417, 13925, 6521, 1922, 4390, 4280, 361, 
   4210, 3326, 7, 19285, 820, 73, 3933, 73, 844, 2146, 2060, 12806, 321, 5339, 541, 73, 844, 2146, 14010, 387, 132, 73, 2517, 73, 2146, 112, 
   163, 12147, 3935, 6433, 132, 272, 39243, 495, 290, 5913, 541, 12462, 2374, 5877, 12543, 6, 762, 3427, 6, 9342, 290, 4027, 6405, 13470, 541, 
   5067, 316, 40950, 2485, 83, 361, 102, 4680, 221, 545, 289, 19377, 195, 32269, 3387, 113, 379, 15516, 274, 504, 26945, 12413, 791, 195, 27932,
   6521, 1922, 4390, 36400, 45947, 151, 14651, 163, 3361, 3398, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 


'attention_mask': 
    [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
    
'offset_mapping': [[(0, 0), (0, 1), (1, 10), (11, 14), (15, 17), (18, 20), (20, 24), (25, 28), (28, 32), (33, 34), (34, 37), (37, 39), (39, 41), 
                    (42, 45), (45, 47), (48, 49), (49, 50), (51, 53), (54, 56), (56, 58), (58, 60), (60, 62), (63, 64), (65, 70), (71, 73), 
                    (74, 76), (77, 80), (81, 83), (83, 84), (84, 86), (86, 87), (87, 89), (89, 91), (92, 94), (95, 98), (99, 100), (100, 102), 
                    (102, 104), (104, 105), (105, 107), (107, 109), (110, 112), (112, 113), (114, 115), (115, 116), (116, 118), (118, 119), 
                    (119, 121), (122, 123), (124, 125), (125, 128), (128, 130), (130, 132), (133, 134), (135, 136), (136, 140), (140, 141), 
                    (142, 143), (144, 146), (146, 148), (149, 151), (152, 157), (157, 161), (162, 166), (166, 167), (168, 171), (171, 175), 
                    (175, 176), (177, 179), (180, 181), (181, 183), (183, 185), (186, 188), (188, 190), (190, 192), (193, 195), (196, 204), 
                    (204, 208), (209, 210), (211, 212), (212, 213), (214, 217), (218, 219), (220, 222), (223, 224), (224, 226), (227, 228), 
                    (228, 230), (230, 232), (232, 233), (234, 236), (237, 240), (241, 242), (243, 245), (246, 250), (251, 253), (253, 254), 
                    (255, 256), (257, 259), (260, 262), (262, 264), (264, 266), (266, 269), (269, 277), (277, 280), (281, 287), (288, 289), 
                    (289, 292), (292, 296), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), 
                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]

'labels': [[24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2, 10, 10, 18, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 
            24, 24, 3, 11, 11, 11, 11, 19, 24, 24, 1, 9, 9, 9, 17, 24, 24, 24, 24, 24, 24, 4, 12, 20, 24, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
            16, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 
            24, 7, 15, 15, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 
            24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 
            24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 
            24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 
            24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24], 

</code></pre>
<p>My Code:</p>
<pre><code>import transformers
from transformers import AutoTokenizer
from transformers import AutoTokenizer,BertModel,BertTokenizer
from transformers import RobertaModel,RobertaConfig,RobertaForTokenClassification
from transformers import TrainingArguments, Trainer
# from transformers.trainer import get_tpu_sampler
from transformers.trainer_pt_utils import get_tpu_sampler
from transformers.data.data_collator import DataCollator, InputDataClass
from transformers import DataCollatorForTokenClassification
from transformers import AutoModelForTokenClassification

import torch
from torch.nn import CrossEntropyLoss, MSELoss
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data.dataloader import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data.sampler import RandomSampler

from torchcrf import CRF

import dataclasses
import logging
import warnings
import tqdm
import os
import numpy as np
from typing import List, Union, Dict
os.environ[&quot;WANDB_DISABLED&quot;] = &quot;true&quot;
print(transformers.__version__)

import evaluate
metric = evaluate.load(&quot;seqeval&quot;)

model_checkpoint = &quot;bert-base-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) #add_prefix_space=True


def isin(a, b):
    return a[1] &gt; b[0] and a[0] &lt; b[1]


def tokenize_and_align_labels(examples, label2id, max_length=256):
    tokenized_inputs = tokenizer(examples[&quot;texts&quot;], truncation=True, padding='max_length', max_length=max_length,return_offsets_mapping=True)
    print(&quot;tokenization done&quot;)

    labels = []
    for i, label_idx_for_single_input in enumerate(tqdm.tqdm(examples[&quot;tag_names&quot;])):

        #         print(i,label_idx_for_single_input)

        labels_for_single_input = ['O' for _ in range(max_length)]
        #         print(labels_for_single_input)
        text_offsets = tokenized_inputs['offset_mapping'][i]
        #         print(&quot;text_offsets&quot;,text_offsets)
        for entity in label_idx_for_single_input:
            #             print(&quot;entity&quot;,entity)
            tag = entity['tag']
            #             print(&quot;tag&quot;,tag)
            tag_offset = [entity['start'], entity['end']]
            #             print(&quot;tag_offset&quot;,tag_offset)

            #             text_offsets [(0, 0), (0, 1), (1, 10), (11, 14), (15, 17), (18, 20), (20, 24), (25, 28), (28, 32), (33, 34), (34, 37), (37, 39), (39, 41), (42, 45), (45, 47), (48, 49), (49, 50), (51, 53), (54, 56), (56, 58), (58, 60), (60, 62), (63, 64), (65, 70), (71, 73), (74, 76), (77, 80), (81, 83), (83, 84), (84, 86), (86, 87), (87, 89), (89, 91), (92, 94), (95, 98), (99, 100), (100, 102), (102, 104), (104, 105), (105, 107), (107, 109), (110, 112), (112, 113), (114, 115), (115, 116), (116, 118), (118, 119), (119, 121), (122, 123), (124, 125), (125, 128), (128, 130), (130, 132), (133, 134), (135, 136), (136, 140), (140, 141), (142, 143), (144, 146), (146, 148), (149, 151), (152, 157), (157, 161), (162, 166), (166, 167), (168, 171), (171, 175), (175, 176), (177, 179), (180, 181), (181, 183), (183, 185), (186, 188), (188, 190), (190, 192), (193, 195), (196, 204), (204, 208), (209, 210), (211, 212), (212, 213), (214, 217), (218, 219), (220, 222), (223, 224), (224, 226), (227, 228), (228, 230), (230, 232), (232, 233), (234, 236), (237, 240), (241, 242), (243, 245), (246, 250), (251, 253), (253, 254), (255, 256), (257, 259), (260, 262), (262, 264), (264, 266), (266, 269), (269, 277), (277, 280), (281, 287), (288, 289), (289, 292), (292, 296), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]
            #             entity {'start': 281, 'end': 296, 'tag': 'PERSON_NAME', 'text': 'Gerald Bessette'}
            #             tag PERSON_NAME
            #             tag_offset [281, 296]

            affected_token_ids = [j for j in range(max_length) if isin(tag_offset, text_offsets[j])]
            #             print(&quot;affected_token_ids&quot;,affected_token_ids)

            if len(affected_token_ids) &lt; 1:
                #                 print('affected_token_ids)&lt;1')
                continue
            if any(labels_for_single_input[j] != 'O' for j in affected_token_ids):
                #                 print('entity orverlap! skipping')
                continue

            for j in affected_token_ids:
                labels_for_single_input[j] = 'I_' + tag
            labels_for_single_input[affected_token_ids[-1]] = 'L_' + tag
            labels_for_single_input[affected_token_ids[0]] = 'B_' + tag

        label_ids = [label2id[x] for x in labels_for_single_input]
        labels.append(label_ids)

    tokenized_inputs[&quot;labels&quot;] = labels
    # print(tokenized_inputs.keys())
    return tokenized_inputs


import json

data = []
with open('data.json', 'r') as f:
    for line in f:
        data.append(json.loads(line))

l = []
for k, v in data[0].items():
    l.append({'text': k, 'spans': v})

train_set = [
    [
        x['text'],
        [{'start': y[&quot;start&quot;], 'end': y[&quot;end&quot;], 'tag': y[&quot;label&quot;], 'text': y[&quot;ngram&quot;]} for y in x['spans']]
    ] for x in l
]

## count labels in dataset
from collections import Counter
e = []
for x in train_set:
    for y in x[1]:
        e.append(y['tag'])
Counter(e).most_common()

## get label list
ori_label_list = []
for line in train_set:
    ori_label_list += [entity['tag'] for entity in line[1]]

ori_label_list = sorted(list(set(ori_label_list)))

label_list = []
for prefix in 'BIL':
    label_list += [prefix + '_' + x for x in ori_label_list]
label_list += ['O']
label_list = sorted(list(set(label_list)))
print(label_list)

print(len(label_list))

label2id = {n:i for i,n in enumerate(label_list)}
id2label= {str(i):n for i,n in enumerate(label_list)}

# id2label = {str(i): label for i, label in enumerate(label_names)}
# label2id = {v: k for k, v in id2label.items()}

train_examples ={'texts':[x[0] for x in train_set],'tag_names':[x[1] for x in train_set]}

train_examples = tokenize_and_align_labels(train_examples,label2id)
# train_examples = train_examples.map(tokenize_and_align_labels(label2id),batched=True)
print(&quot;here&quot;)
print(train_examples.keys())
print(len(train_examples['labels']))

# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'])
# 775
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
# collator=data_collator(train_examples)

# def compute_metrics(eval_preds):
#     logits, labels = eval_preds
#     predictions = np.argmax(logits, axis=-1)
#
#     # Remove ignored index (special tokens) and convert to labels
#     true_labels = [[label_list[l] for l in label if l != -100] for label in labels]
#     true_predictions = [
#         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
#         for prediction, label in zip(predictions, labels)
#     ]
#     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
#     return {
#         &quot;precision&quot;: all_metrics[&quot;overall_precision&quot;],
#         &quot;recall&quot;: all_metrics[&quot;overall_recall&quot;],
#         &quot;f1&quot;: all_metrics[&quot;overall_f1&quot;],
#         &quot;accuracy&quot;: all_metrics[&quot;overall_accuracy&quot;],
#     }

model = AutoModelForTokenClassification.from_pretrained(model_checkpoint,id2label=id2label,label2id=label2id,)

print(model.config.num_labels)



args = TrainingArguments(
    &quot;bert-finetuned-ner&quot;,
    # evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=2,
    weight_decay=0.01,
    # push_to_hub=True,
)


trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_examples,
    # eval_dataset=train_examples,
    data_collator=data_collator,
    # compute_metrics=compute_metrics,
    tokenizer=tokenizer)
trainer.train()


</code></pre>
<p><strong>ERROR</strong></p>
<pre><code> _name = &quot;label&quot; if &quot;label&quot; in features[0].keys() else &quot;labels&quot;
AttributeError: 'tokenizers.Encoding' object has no attribute 'keys'
</code></pre>
","python-3.x, token, huggingface-transformers, named-entity-recognition","<p>I think the object <code>tokenized_inputs</code> that you create and return in <code>tokenize_and_align_labels</code> is likely to be a <code>tokenizers.Encoding</code> object, not a <code>dict</code> or <code>Dataset</code> object (check this by printing <code>type(myobject)</code> when in doubt), and therefore it won't have keys.</p>
<p>You should apply your Tokenizer to your examples using the <code>map</code> function of Dataset, as in <a href=""https://huggingface.co/docs/datasets/use_dataset#tokenize-text"" rel=""nofollow noreferrer"">this example from the documentation</a>.</p>
",1,0,788,2022-08-15 08:11:45,https://stackoverflow.com/questions/73358347/how-to-resolve-the-error-name-name-label-if-label-in-features0-keys-e
"For spacy&#39;s NER, do I need to label the entire word as an entity?","<p>I'm fairly new to spacy and NER. I am dealing with a problem where I want to label many examples of short-form text data. I want to map company names to a custom entity CUSTOM.</p>
<p>Example descriptions:</p>
<pre><code>Amazon1337XS324, Amazon4357YT322, *Google, Just *Eat
</code></pre>
<p>I am currently labeling the training data. My doubt is whether I should label the entire word as an entity or not e.g. &quot;Amazon1337XS324&quot; or &quot;Amazon&quot;, &quot;*Google&quot; or &quot;Google&quot;, and &quot;Just *Eat&quot; or &quot;Just Eat&quot;.</p>
<p><a href=""https://stackoverflow.com/questions/70502457/do-i-need-to-do-any-text-cleaning-for-spacy-ner"">From this previous post</a> it seems I shouldn't try to remove information that the NER model would find useful. Also, in many labeling tutorials the entire word is always labeled. However, in my use case, the &quot;non-descriptive&quot; subsection of the word could always change, like in the Amazon example, and could end up being noise for the model.</p>
<p>I think I also don't understand if I only provide the entities &quot;Amazon&quot; or &quot;Google&quot; to the spacy's NER model, and new examples come in where there are many new characters next to it in the same word (e.g. Amazon1337XS325, Amazon1337XS326) , will the NER model still be able to identify &quot;Amazon&quot; or &quot;Google&quot; as CUSTOM?</p>
","python, spacy, named-entity-recognition","<p>You can't put an NER label on half a token. The tokenizer is run before NER and the NER component attempts to give a label to each whole token, so if you're only interested in part of a token, the NER component wont' be able to figure that out.</p>
<p>If you don't have some way to separate the tokens in preprocessing, it seems like the only thing you can do is label the whole token. You're right that will make it harder for the model to learn.</p>
<p>One alternative is to try training a character-level NER component - basically, split your input into individual characters before training.</p>
",0,1,517,2022-08-18 11:05:34,https://stackoverflow.com/questions/73401971/for-spacys-ner-do-i-need-to-label-the-entire-word-as-an-entity
"spaCy, NER, documentation about the different label types of a particular LM","<p>I am using spaCy for named entity recognition (NER). According to <a href=""https://spacy.io/models/en"" rel=""nofollow noreferrer"">the spaCy docs</a>, the language model <code>en_core_web_sm</code> is able to recognize 18 different entity types, i. e., it provides 18 labels such as <code>DATE</code>, <code>PERSON</code> or <code>ORG</code>.</p>
<p>I am particularly interested in the labels <code>LOC</code> (location), <code>FAC</code> (facilities) and <code>GPE</code> (gepolitical entities). Is there documentation about which objects are typically labelled with those labels? Have the guidelines used for labelling the entities been published?</p>
<p>I am asking, because sometimes it is not clear to me why a particular object is labelled as say <code>FAC</code> and not as <code>GPE</code>, or why an object is not labelled at all. Let's have a look at an example:</p>
<pre><code>#spacy.cli.download('en_core_web_sm')
nlp = spacy.load('en_core_web_sm')
text = 'Alice Miller went to the Empire State Building. Next she went to Times Square. Finally she went to the train station.'
doc = nlp_en(text)
displacy.render(doc, style='ent')
</code></pre>
<p>The output is:</p>
<p><a href=""https://i.sstatic.net/zxft7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zxft7.png"" alt=""enter image description here"" /></a></p>
<p>To my mind, the Empire State Building is correctly labelled as <code>GPE</code>. The Times Square, however, is labelled as <code>FAC</code>; I expected <code>GPE</code>. And &quot;train station&quot; is not recognized at all; I expected <code>FAC</code>.</p>
","python, nlp, spacy, named-entity-recognition","<p>If you check the page for a pipeline you'll see the data sources listed. For the NER data in the English pipelines OntoNotes is used. The schema is documented in the <a href=""https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf"" rel=""nofollow noreferrer"">OntoNotes Manual</a>, for example:</p>
<pre><code>PERSON People, including fictional
NORP Nationalities or religious or political groups
FACILITY Buildings, airports, highways, bridges, etc.
ORGANIZATION Companies, agencies, institutions, etc.
GPE Countries, cities, states
LOCATION Non-GPE locations, mountain ranges, bodies of water
PRODUCT Vehicles, weapons, foods, etc. (Not services)
EVENT Named hurricanes, battles, wars, sports events, etc.
WORK OF ART Titles of books, songs, etc.
LAW Named documents made into laws 
LANGUAGE Any named language 
</code></pre>
<p>In spacy you can get these definitions using <a href=""https://spacy.io/api/top-level/#spacy.explain"" rel=""nofollow noreferrer""><code>spacy.explain</code></a>, like <code>spacy.explain(&quot;FACILITY&quot;)</code>. Sometimes the official documentation has more detailed explanations, though in this case it seems not to.</p>
<p>&quot;train station&quot; is not picked up because it is not a named entity - named entities are typically proper nouns, not common nouns.</p>
<p>Also note the model is not perfect and it will make mistakes, and it is hard to explain individual mistakes (see <a href=""https://github.com/explosion/spaCy/issues/3052"" rel=""nofollow noreferrer"">here</a>).</p>
",3,3,3582,2022-08-24 10:13:31,https://stackoverflow.com/questions/73471328/spacy-ner-documentation-about-the-different-label-types-of-a-particular-lm
No module named &#39;spacy&#39; in PySpark,"<p>I am attempting to perform some entity extraction, using a custom NER spaCy model. The extraction will be done over a Spark Dataframe, and everything is being orchestrated in a <a href=""https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-console#create_a_cluster"" rel=""nofollow noreferrer"">Dataproc cluster</a> (using a Jupyter Notebook, available in the <em>&quot;Workbench&quot;</em>). The code I am using, looks like follows:</p>
<pre class=""lang-py prettyprint-override""><code># IMPORTANT: NOTICE THIS CODE WAS RUN FROM A JUPYTER NOTEBOOK (!)

import pandas as pd
import numpy as np
import time

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, pandas_udf
from pyspark.sql.types import ArrayType, StringType

spark = SparkSession.builder.appName('SpacyOverPySpark') \
                    .getOrCreate()


# FUNCTIONS DEFINITION

def load_spacy_model():
    import spacy
    print(&quot;Loading spacy model...&quot;)
    return spacy.load(&quot;./spacy_model&quot;)  # This model exists locally


@pandas_udf(ArrayType(StringType()))
def entities(list_of_text: pd.Series) -&gt; pd.Series:
    # retrieving the shared nlp object
    nlp = broadcasted_nlp.value
    # batch processing our list of text
    docs = nlp.pipe(list_of_text)
    # entity extraction (`ents` is a list[list[str]])
    ents=[
        [ent.text for ent in doc.ents]
        for doc in docs
    ]
    return pd.Series(ents)


# DUMMY DATA FOR THIS TEST

pdf = pd.DataFrame(
    [
        &quot;Pyhton and Pandas are very important for Automation&quot;,
        &quot;Tony Stark is a Electrical Engineer&quot;,
        &quot;Pipe welding is a very dangerous task in Oil mining&quot;,
        &quot;Nursing is often underwhelmed, but it's very interesting&quot;,
        &quot;Software Engineering now opens a lot of doors for you&quot;,
        &quot;Civil Engineering can get exiting, as you travel very often&quot;,
        &quot;I am a Java Programmer, and I think I'm quite good at what I do&quot;,
        &quot;Diane is never bored of doing the same thing all day&quot;,
        &quot;My father is a Doctor, and he supports people in condition of poverty&quot;,
        &quot;A janitor is required as soon as possible&quot;
    ],
    columns=['postings']
)
sdf=spark.createDataFrame(pdf)


# MAIN CODE

# loading spaCy model and broadcasting it
broadcasted_nlp = spark.sparkContext.broadcast(load_spacy_model())
# Extracting entities
df_new = sdf.withColumn('skills',entities('postings'))
# Displaying results
df_new.show(10, truncate=20)
</code></pre>
<p>The error code I am getting, looks similar to <a href=""https://github.com/explosion/spaCy/discussions/5351#discussion-58639"" rel=""nofollow noreferrer"">this</a>, but the <a href=""https://stackoverflow.com/questions/32336498/pyspark-module-not-found"">answer</a> does not apply for my case, because it deals with <em>&quot;executing a Pyspark job in Yarn&quot;</em> which is different (or so I think, feel free to correct me). Plus, I have also found <a href=""https://stackoverflow.com/questions/61383523/running-spacy-in-pyspark-but-getting-modulenotfounderror-no-module-named-spac"">this</a>, but the answer is rather vague (I gotta be honest here: the only thing I have done to &quot;restart the spark session&quot; is to run <code>spark.stop()</code> in the last cell of my Jupyter Notebook, and then run the cells above again, feel free to correct me here too).</p>
<p>The code used was heavily inspired by <a href=""https://stackoverflow.com/q/63681625/"">&quot;Answer 2 of 2&quot; in this forum</a>, which makes me wonder if some missing setting is still eluding me (BTW, &quot;Answer 1 of 2&quot; was already tested but did not work). And regarding my specific software versions, they can be found <a href=""https://github.com/explosion/spaCy/discussions/5351#discussioncomment-3460976"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Thank you.</p>
<p><strong>CLARIFICATIONS:</strong></p>
<p>Because some queries or hints generated in the comment section can be lengthy, I have decided to include them here:</p>
<ul>
<li><em>No. 1: &quot;Which command did you use to create your cluster?&quot;</em> : I used <a href=""https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-console#create_a_cluster"" rel=""nofollow noreferrer"">this</a> method, so the command was not visible &quot;at plain sight&quot;; I have just realized however that, when you are about to create the cluster, you have an <em>&quot;EQUIVALENT COMMAND LINE&quot;</em> button, that grants access to such command:</li>
</ul>
<p><a href=""https://i.sstatic.net/5x4uY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5x4uY.png"" alt=""enter image description here"" /></a></p>
<p>In my case, the Dataproc cluster creation code (automatically generated by GCP) is:</p>
<pre class=""lang-bash prettyprint-override""><code>gcloud dataproc clusters create my-cluster \
--enable-component-gateway \
--region us-central1 \
--zone us-central1-c \
--master-machine-type n1-standard-4 \
--master-boot-disk-size 500 \
--num-workers 2 \
--worker-machine-type n1-standard-4 \
--worker-boot-disk-size 500 \
--image-version 2.0-debian10 \
--optional-components JUPYTER \
--metadata PIP_PACKAGES=spacy==3.2.1 \
--project hidden-project-name
</code></pre>
<p>Notice how <code>spaCy</code> is installed in the metadata (following <a href=""https://cloud.google.com/dataproc/docs/tutorials/python-configuration"" rel=""nofollow noreferrer"">these</a> recommendations); however running <code>pip freeze | grep spacy</code> command, right after the Dataproc cluster creation, does not display any result (i.e., <strong>spaCy does NOT get installed successfully</strong>). To enable it, the <a href=""https://spacy.io/usage"" rel=""nofollow noreferrer"">official method</a> is used afterwards.</p>
<ul>
<li><em>No. 2: &quot;Wrong path as possible cause&quot;</em> : Not my case, it actually looks similar to <a href=""https://stackoverflow.com/questions/69716018/modulenotfounderror-no-module-named-spacy-even-though-spacy-and-python-are-in"">this case</a> (even when I can't say the root case is the same for both):
<ul>
<li>Running <code>which python</code> shows <code>/opt/conda/miniconda3/bin/python</code> as result.</li>
<li>Running <code>which spacy</code> (read &quot;Clarification No. 1&quot;) shows <code>/opt/conda/miniconda3/bin/spacy</code> as result.</li>
</ul>
</li>
</ul>
","pyspark, user-defined-functions, google-cloud-dataproc, named-entity-recognition, spacy-3","<p>I managed to solve this issue, by combining 2 pieces of information:</p>
<ul>
<li><em>&quot;Configure Dataproc Python environment&quot;</em>, <em>&quot;Dataproc image version 2.0&quot;</em> (as that is the version I am using): available <a href=""https://cloud.google.com/dataproc/docs/tutorials/python-configuration#image_version_20"" rel=""nofollow noreferrer"">here</a> (special thanks to @Dagang in the comment section).</li>
<li><em>&quot;Create a</em> (Dataproc) <em>cluster&quot;</em>: available <a href=""https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-console#create_a_cluster"" rel=""nofollow noreferrer"">here</a>.</li>
</ul>
<p>In specific, during the Dataproc cluster setup via Google Console, I &quot;installed&quot; spaCy by doing:</p>
<p><a href=""https://i.sstatic.net/vFdmh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vFdmh.png"" alt=""enter image description here"" /></a></p>
<p>And when the cluster was already created, I ran the code mentioned in my original post (NO modifications) with the following result:</p>
<p><a href=""https://i.sstatic.net/mQKPB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mQKPB.png"" alt=""enter image description here"" /></a></p>
<p>That solves my original question. I am planning to apply my solution on a larger dataset, but I think whatever happen there, is subject of a different thread.</p>
",1,1,664,2022-08-24 23:05:11,https://stackoverflow.com/questions/73480315/no-module-named-spacy-in-pyspark
Spacy models with different word2vec embeddings give same results,"<p>I am trying to improve the performance of my spacy NER model by implementing my pretrained vectors. I have created my own vectors with word2vec using different texts and I have saved them in .txt files. However I get the exact same scores and this doesn't seem right.</p>
<p>Here are the steps I have been following for one file with custom pretrained embeddings:</p>
<pre><code>!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_uncleaned_sents_model.txt ./uncl_txt --name JSTOR_unlceaned_sents_model

nlp = spacy.load(&quot;./uncl_txt&quot;)
nlp.add_pipe(&quot;ner&quot;)
nlp.to_disk(&quot;./uncl_txt&quot;)

!python -m spacy train /content/uncl_txt/config.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./uncl_txt --paths.vectors ./uncl_txt

!python -m spacy evaluate /content/uncl_txt/model-best ./Spacy/eval.spacy --output ner_with_uncleaned_sents_vectors.jsonl
</code></pre>
<p>Here are the steps for the other embeddings file:</p>
<pre><code>!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_abs_model.txt ./abs --name JSTOR_abs_model

nlp = spacy.load(&quot;./abs&quot;)
nlp.add_pipe(&quot;ner&quot;)
nlp.to_disk(&quot;./abs&quot;)

!python -m spacy train /content/abs/config.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./abs/ --paths.vectors ./abs

!python -m spacy evaluate ./abs/model-best ./Spacy/eval.spacy --output ner_with_abs_vectors.jsonl
</code></pre>
<p>Am I doing something wrong? Should I add something in the config file?</p>
","python, spacy, word2vec, named-entity-recognition, word-embedding","<p>The model created using <code>nlp.add_pipe(&quot;ner&quot;)</code> does not have embeddings enabled by default.</p>
<p>The easiest way to create a config for <code>ner</code> with embeddings enabled is to use <code>spacy init config</code> with <code>-o accuracy</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>spacy init config -p ner -o accuracy ner.cfg
</code></pre>
<p>And then train with:</p>
<pre class=""lang-bash prettyprint-override""><code>spacy train ner.cfg --paths.train train.spacy --paths.dev dev.spacy --paths.vectors ./vectors
</code></pre>
<p>(You can also enable it using custom config settings with <code>nlp.add_pipe(&quot;ner&quot;, config=...)</code>, but this requires digging into the details about the internal default model config, which might also change depending on the version of spacy, so <code>spacy init config</code> is easier to use.)</p>
",1,1,200,2022-09-01 11:20:26,https://stackoverflow.com/questions/73568510/spacy-models-with-different-word2vec-embeddings-give-same-results
Meaning of NER Training values using Spacy,"<p>Please explain the meaning of the columns when training Spacy NER model:</p>
<pre><code>E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  ------------  --------  ------  ------  ------  ------
  0       0          0.00     78.11   26.82   22.88   32.41    0.27
 26     200         82.40   3935.97   94.44   94.44   94.44    0.94
 59     400         50.37   2338.60   94.91   94.91   94.91    0.95
 98     600         66.31   2646.82   92.13   92.13   92.13    0.92
146     800         85.11   3097.20   94.91   94.91   94.91    0.95
205    1000         92.20   3472.80   94.91   94.91   94.91    0.95
271    1200        124.10   3604.98   94.91   94.91   94.91    0.95
</code></pre>
<p>I know that <code>ENTS_F</code>  <code>ENTS_P</code> and  <code>ENTS_R</code> represent the F-score, precision, and recall respectively and the SCORE is the overall model score.
What is the formula for SCORE?
Where can I see the documentation about these columns?
What are the <code>#</code> and <code>E</code> columns stand for?
Please guide or send me to the relevant docs, I didn't find a proper documentation about the columns except <a href=""https://spacy.io/models/en"" rel=""nofollow noreferrer"">here</a>.</p>
","python-3.x, spacy, named-entity-recognition, spacy-3","<p><code>#</code> refers to iterations (or batches), and E refers to epochs.</p>
<p>The score is calculated as a weighted average of other metrics, as designated in your config file. This is documented <a href=""https://spacy.io/usage/training#metrics"" rel=""nofollow noreferrer"">here</a>.</p>
",2,0,1103,2022-09-14 20:14:44,https://stackoverflow.com/questions/73722706/meaning-of-ner-training-values-using-spacy
How can I iterate on a column with spacy to get named entities?,"<p>I got a dataframe with a column named &quot;categories&quot;.  Some data of this column looks like this <code>{[], [], [amazon], [clothes], [telecommunication],[],...}</code>. Every row has only one of this values. My task is now to give this values their entities. I tried a lot but it didn't go well. This was my first attempt</p>
<pre><code>import spacy
nlp = spacy.load(&quot;de_core_news_sm&quot;)
doc=list(nlp.pipe(df.categories))
print([(X.text, X.label_) for X in doc.ents])
AttributeError 'list' object has no attribute 'ents'
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
in ----&gt; 1 print([(X.text, X.label_) for X in doc.ents])
AttributeError: 'list' object has no attribute 'ents'
</code></pre>
<p>My second attempt:</p>
<pre><code>for token in doc:
print(token.doc, token.pos_, token.dep_)
AttributeError 'spacy.tokens.doc.Doc' object has no attribute 'pos_'
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
in 1 for token in doc: ----&gt; 2 print(token.doc, token.pos_, token.dep_)
AttributeError 'spacy.tokens.doc.Doc' object has no attribute 'pos_'
</code></pre>
<p>Third attempt:</p>
<pre><code>docs = df[&quot;categories&quot;].apply(nlp)
for token in docs:
    print(token.text, token.pos_, token.dep_)
AttributeError 'spacy.tokens.doc.Doc' object has no attribute 'docs'
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
in 1 docs = df[&quot;categories&quot;].apply(nlp) 2 for token in docs: ----&gt; 3              print(token.docs, token.pos_, token.dep_) 
AttributeError: 'spacy.tokens.doc.Doc' object has no attribute 'docs'
</code></pre>
<p>I just want to iterate spacy on this column to give me for the values an entity. For the empty values it should give me no entity. The column is a string. Thanks for help.</p>
","python, nlp, spacy, named-entity-recognition","<p>You have list with many <code>doc</code> and you have to use extra <code>for</code>-loop to work with every doc separatelly.</p>
<pre><code>docs = list(nlp.pipe(df.categories))   # variable `docs` instead of `doc`

for doc in docs:   
    print([(X.text, X.label_) for X in doc.ents])
</code></pre>
<p>and</p>
<pre><code>docs = list(nlp.pipe(df.categories))   # variable `docs` instead of `doc`

for doc in docs:   
    for token in doc:
        print(token.text, token.pos_, token.dep_)
</code></pre>
<p>Documentations <a href=""https://spacy.io/usage/processing-pipelines"" rel=""nofollow noreferrer"">Language Processing Pipelines</a> shows it like</p>
<pre><code>for doc in nlp.pipe(df.categories):   
    print([(X.text, X.label_) for X in doc.ents])
    for token in doc:
        print(token.text, token.pos_, token.dep_)
</code></pre>
<p>And the same problem is with <code>apply(nlp)</code></p>
<pre><code>docs = df[&quot;categories&quot;].apply(nlp)

for doc in docs:
    for token in doc:
        print(token.text, token.pos_, token.dep_)
</code></pre>
<hr />
<p>Full working example:</p>
<pre><code>import spacy
import pandas as pd

df = pd.DataFrame({
    'categories': ['amazon', 'clothes', 'telecommunication']
})

nlp = spacy.load(&quot;de_core_news_sm&quot;)

print('\n--- version 1 ---\n')

docs = list(nlp.pipe(df.categories))

for doc in docs:
    print([(X.text, X.label_) for X in doc.ents])
    
    for token in doc:
        print(token.text, token.pos_, token.dep_)

print('\n--- version 2 ---\n')

docs = df[&quot;categories&quot;].apply(nlp)

for doc in docs:
    for token in doc:
        print(token.text, token.pos_, token.dep_)

</code></pre>
",0,0,700,2022-09-16 11:09:12,https://stackoverflow.com/questions/73743904/how-can-i-iterate-on-a-column-with-spacy-to-get-named-entities
How to pass arguments to HuggingFace TokenClassificationPipeline&#39;s tokenizer,"<p>I've finetuned a Huggingface BERT model for Named Entity Recognition. Everything is working as it should. Now I've setup a pipeline for token classification in order to predict entities out the text I provide. Even this is working fine.</p>
<p>I know that BERT models are supposed to be fed with sentences less than 512 tokens long. Since I have texts longer than that, I split the sentences in shorter chunks and I store the chunks in a list <code>chunked_sentences</code>. To make it brief my tokenizer for training looks like this:</p>
<pre><code>from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
tokenized_inputs = tokenizer(chunked_sentences, is_split_into_words=True, padding='longest')
</code></pre>
<p>I pad everything to the longest sequence and avoid truncation so that if a sentence is tokenized and goes beyond 512 tokens I receive a warning that I won't be able to train. This way I know that I have to split the sentences in smaller chunks.</p>
<p>During inference I wanted to achieve the same thing, but I haven't found a way to pass arguments to the pipeline's tokenizer. The code looks like this:</p>
<pre><code>from transformers import pipeline
ner_pipeline = pipeline('token-classification', model=model_folder, tokenizer=model_folder)
out = ner_pipeline(text, aggregation_strategy='simple')
</code></pre>
<p>I'm pretty sure that if a sentence is tokenized and surpasses the 512 tokens, the extra tokens will be truncated and I'll get no warning. I want to avoid this.</p>
<p>I tried passing arguments to the tokenizer like this:</p>
<pre><code>tokenizer_kwargs = {'padding': 'longest'}
out = ner_pipeline(text, aggregation_strategy='simple', **tokenizer_kwargs)
</code></pre>
<p>I got that idea from <a href=""https://stackoverflow.com/a/70729850/14774959"">this answer</a>, but it seems not to be working, since I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;...\inference.py&quot;, line 42, in &lt;module&gt;
    out = ner_pipeline(text, aggregation_strategy='simple', **tokenizer_kwargs)
  File &quot;...\venv\lib\site-packages\transformers\pipelines\token_classification.py&quot;, line 191, in __call__
    return super().__call__(inputs, **kwargs)
  File &quot;...\venv\lib\site-packages\transformers\pipelines\base.py&quot;, line 1027, in __call__
    preprocess_params, forward_params, postprocess_params = self._sanitize_parameters(**kwargs)
TypeError: TokenClassificationPipeline._sanitize_parameters() got an unexpected keyword argument 'padding'

Process finished with exit code 1
</code></pre>
<p>Any ideas? Thanks.</p>
","python, huggingface-transformers, named-entity-recognition, huggingface-tokenizers, huggingface","<p>I took a closer look at <a href=""https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/pipelines/token_classification.py#L86"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/pipelines/token_classification.py#L86</a>. It seems you can override <code>preprocess()</code> to disable truncation and add padding to longest.</p>
<pre><code>from transformers import TokenClassificationPipeline

class MyTokenClassificationPipeline(TokenClassificationPipeline):
    def preprocess(self, sentence, offset_mapping=None):
        truncation = False
        padding = 'longest'
        model_inputs = self.tokenizer(
            sentence,
            return_tensors=self.framework,
            truncation=truncation,
            padding=padding,
            return_special_tokens_mask=True,
            return_offsets_mapping=self.tokenizer.is_fast,
        )
        if offset_mapping:
            model_inputs[&quot;offset_mapping&quot;] = offset_mapping
    
        model_inputs[&quot;sentence&quot;] = sentence
        return model_inputs
    
ner_pipeline = MyTokenClassificationPipeline(model=model_folder, tokenizer=model_folder)
out = ner_pipeline(text, aggregation_strategy='simple')
</code></pre>
",2,5,3869,2022-09-16 13:32:47,https://stackoverflow.com/questions/73745607/how-to-pass-arguments-to-huggingface-tokenclassificationpipelines-tokenizer
Can&#39;t run the spacy spancat (spancategorizer) model?,"<p>I am trying to train the spancat model without luck.
I am getting:</p>
<p><code>ValueError: [E143] Labels for component 'spancat' not initialized. This can be fixed by calling add_label, or by providing a representative batch of examples to the component's 'initialize' method.</code></p>
<p>I did convert my NER ents to spans:</p>
<pre><code>def main(loc: Path, lang: str, span_key: str):
    &quot;&quot;&quot;
    Set the NER data into the doc.spans, under a given key.
    The SpanCategorizer component uses the doc.spans, so that it can work with
    overlapping or nested annotations, which can't be represented on the
    per-token level.
    &quot;&quot;&quot;
    nlp = spacy.blank(lang)
    docbin = DocBin().from_disk(loc)
    docs = list(docbin.get_docs(nlp.vocab))
    for doc in docs:
        doc.spans[span_key] = list(doc.ents)
    DocBin(docs=docs).to_disk(loc)
</code></pre>
<p>Here is my config file:</p>
<pre><code>[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = null
seed = 444

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;spancat&quot;]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}

[components]

[components.spancat]
factory = &quot;spancat&quot;
max_positive = null
scorer = {&quot;@scorers&quot;:&quot;spacy.spancat_scorer.v1&quot;}
spans_key = &quot;sc&quot;
threshold = 0.5

[components.spancat.model]
@architectures = &quot;spacy.SpanCategorizer.v1&quot;

[components.spancat.model.reducer]
@layers = &quot;spacy.mean_max_reducer.v1&quot;
hidden_size = 128

[components.spancat.model.scorer]
@layers = &quot;spacy.LinearLogistic.v1&quot;
nO = null
nI = null

[components.spancat.model.tok2vec]
@architectures = &quot;spacy.Tok2VecListener.v1&quot;
width = ${components.tok2vec.model.encode.width}
upstream = &quot;*&quot;

[components.spancat.suggester]
@misc = &quot;spacy.ngram_suggester.v1&quot;
sizes = [1,2,3]

[components.tok2vec]
factory = &quot;tok2vec&quot;

[components.tok2vec.model]
@architectures = &quot;spacy.Tok2Vec.v2&quot;

[components.tok2vec.model.embed]
@architectures = &quot;spacy.MultiHashEmbed.v2&quot;
width = ${components.tok2vec.model.encode.width}
attrs = [&quot;NORM&quot;,&quot;PREFIX&quot;,&quot;SUFFIX&quot;,&quot;SHAPE&quot;]
rows = [5000,1000,2500,2500]
include_static_vectors = true

[components.tok2vec.model.encode]
@architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot;
width = 256
depth = 8
window_size = 1
maxout_pieces = 3

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
max_epochs = 70
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = &quot;spacy.ConsoleLogger.v1&quot;
progress_bar = false

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
spans_sc_f = 1.0
spans_sc_p = 0.0
spans_sc_r = 0.0

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>I am using the &quot;sc&quot; key. Please advise how to solve it.</p>
","python-3.x, spacy, named-entity-recognition, spacy-3","<p>I have solved it using the following function, but one should address the spans Span(doc, start, end, label) according to the project/text for their task. It worked for me because all the text (a few words in my case) are labeled with a label and this is my need.</p>
<pre><code>def convert_to_docbin(input, output_path=&quot;./train.spacy&quot;, lang='en'):
    &quot;&quot;&quot; Convert a pair of text annotations into DocBin then save &quot;&quot;&quot;
    # Load a new spacy model:
    nlp = spacy.blank(lang)
    # Create a DocBin object:
    db = DocBin()
    for text, annotations in input: # Data in previous format
        doc = nlp(text)
        ents = []
        spans = []
        for start, end, label in annotations: # Add character indexes
            spans.append(Span(doc, 0, len(doc), label=label))
            span = doc.char_span(start, end, label=label)
            ents.append(span)
        doc.ents = ents # Label the text with the ents
        group = SpanGroup(doc, name=&quot;sc&quot;, spans=spans)
        doc.spans[&quot;sc&quot;] = group
        db.add(doc)
    db.to_disk(output_path)
</code></pre>
",1,1,655,2022-09-18 17:16:52,https://stackoverflow.com/questions/73764895/cant-run-the-spacy-spancat-spancategorizer-model
Why do I need to specify vectors (en_core_web_lg) in spacy config file when I run model training using blank model?,"<p>Here is the start of my config file:</p>
<pre><code># This is an auto-generated partial config. To use it with 'spacy train'
# you can run spacy init fill-config to auto-fill all default settings:
# python -m spacy init fill-config ./base_config.cfg ./config.cfg
[paths]
train = null
dev = null
vectors = &quot;en_core_web_lg&quot;
</code></pre>
<p>But I am training my model on my own labels and spans.
What are the <code>vectors = &quot;en_core_web_lg&quot;</code> used for?</p>
<p>After all I am using the following logic to train my model:</p>
<pre><code># Load a new spacy model:
nlp = spacy.blank(&quot;en&quot;)
# Create a DocBin object:
db = DocBin()
for text, annotations in input: # Data in previous format
    doc = nlp(text)
    ents = []
    spans = []
    for start, end, label in annotations: # Add character indexes
        spans.append(Span(doc, 0, len(doc), label=label))
        span = doc.char_span(start, end, label=label)
        ents.append(span)
        doc.ents = ents # Label the text with the ents
        group = SpanGroup(doc, name=&quot;sc&quot;, spans=spans)
        doc.spans[&quot;sc&quot;] = group
        db.add(doc)
db.to_disk(output_path)
</code></pre>
<p>Please explain where these vectors are used in such configuration?
Consider I have a list of annotated data in the format of <code>[(text_1, [(start_1, end_1, LABEL_1)]), (text_2, [(start_2, end_2, LABEL_1)]).....]</code></p>
","python-3.x, spacy, named-entity-recognition, spancat","<p>The static word vectors are included as a tok2vec feature if you have <code>include_static_vectors = true</code> in the <code>tok2vec</code> config.</p>
",2,1,588,2022-09-25 20:37:50,https://stackoverflow.com/questions/73847703/why-do-i-need-to-specify-vectors-en-core-web-lg-in-spacy-config-file-when-i-ru
Complex Regex not working in Spacy entity ruler,"<p>I'm trying to identify the entities by passing the Regular expression (Regex) to the Spacy model using Entity Ruler but, Spacy is unable to identify based on the below regex.</p>
<p>But, I tested the regex <a href=""https://regex101.com/r/A5iiWc/4"" rel=""nofollow noreferrer"">here</a> and it's working.</p>
<pre><code>import model_training
import spacy

nlp = spacy.load('en_core_web_trf')
nlp.add_pipe(&quot;spacytextblob&quot;)

nlp = model_training.train_model_with_regex(nlp)
</code></pre>
<p>model_training.py</p>
<pre><code>def train_model_with_regex(nlp):
ruler = nlp.add_pipe(&quot;entity_ruler&quot;, before=&quot;ner&quot;)
patterns = [
    {
        &quot;label&quot;: &quot;VOLUME&quot;,
        &quot;pattern&quot;: [{&quot;LOWER&quot;: {'REGEX': &quot;(?:\d+\s(?:million|hundred|thousand|billion)*\s*)+&quot;}}]
    }
]

ruler.add_patterns(patterns)
return nlp
</code></pre>
<p>I wanted to achieve this, for the below example</p>
<pre><code>text = &quot;I have spent 5 million to buy house and 70 thousand for the furniture&quot;
</code></pre>
<p>expected output:</p>
<pre><code>{'result': [
    {'label': 'VOLUME', 'text': '5 million'},
    {'label': 'VOLUME', 'text': '70 thousand'}
]}
</code></pre>
","python, regex, nlp, spacy, named-entity-recognition","<p>The problem is that your pattern is supposed to match at least two tokens, while the <code>REGEX</code> operator is applied to a single token.</p>
<p>A solution can look like</p>
<pre><code>&quot;pattern&quot;: [
    {&quot;TEXT&quot;: {&quot;REGEX&quot;: r&quot;^\d+(?:[,.]\d+)*$&quot;}},
    {&quot;TEXT&quot;: {&quot;REGEX&quot;: r&quot;^(?:million|hundred|thousand|billion)s?$&quot;}}
]
</code></pre>
<p>The <code>LIKE_NUM</code> entity is defined  in Spacy source code mostly as a string of digits with all dots and commas removed, so the <code>^\d+(?:[,.]\d+)*$</code> pattern looks good enough. It matches a token that starts with one or more digits and then contains zero or more occurrences of a comma or dot and then one or more digits till the end of the token.</p>
",3,2,378,2022-09-26 11:32:39,https://stackoverflow.com/questions/73853551/complex-regex-not-working-in-spacy-entity-ruler
How to predict entities for multiple sentences using spaCy?,"<p>I have trained an ner model using spaCy. I know how to use it to recognize the entities for a single sentence (doc object) and visualize the results:</p>
<pre><code>doc = disease_blank('Example sentence')    
spacy.displacy.render(doc, style=&quot;ent&quot;, jupyter=True)
</code></pre>
<p>or</p>
<pre><code>for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p>Now i want to predict the entities for multiple such sentences. My idea is to filter the sentences by their entities. At the moment i just found the following way to do it:</p>
<pre><code>sentences = ['sentence 1', 'sentence2', 'sentence3']
for element in sentences:
    doc = nlp(element)
    for ent in doc.ents:
        if ent.label_ == &quot;LOC&quot;:
        print(doc)
 # returns all sentences which have the entitie &quot;LOC&quot;
</code></pre>
<p>My question is if there is a better and more efficient way to do this?</p>
","model, spacy, named-entity-recognition","<p>You have 2 options, to speed up you current implementation:</p>
<ul>
<li>Use the hints provided by spaCy developers <a href=""https://github.com/explosion/spaCy/discussions/8402"" rel=""nofollow noreferrer"">here</a>. Without knowing which specific components your custom NER model pipeline has, the refactorization of your code would like:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import spacy
import multiprocessing

cpu_cores = multiprocessing.cpu_count()-2 if multiprocessing.cpu_count()-2 &gt; 1 else 1
nlp = spacy.load(&quot;./path/to/your/own/model&quot;)

sentences = ['sentence 1', 'sentence2', 'sentence3']
for doc in nlp.pipe(sentences, n_process=cpu_cores):  # disable=[&quot;tok2vec&quot;, &quot;tagger&quot;, &quot;parser&quot;, &quot;attribute_ruler&quot;, &quot;lemmatizer&quot;] ... if your model has them. Check with `nlp.pipe_names`
    # returns all sentences which have the entitie &quot;LOC&quot;
    print([(doc) for ent in doc.ents if ent.label_ == &quot;LOC&quot;])
</code></pre>
<ul>
<li>Combine the previous knowledge, with the use of spaCy custom components (as carefully explained <a href=""https://course.spacy.io/en/chapter3"" rel=""nofollow noreferrer"">here</a>). Using this option, your refactorized / improved code would look like:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import spacy
import multiprocessing
from spacy.language import Language

cpu_cores = multiprocessing.cpu_count()-2 if multiprocessing.cpu_count()-2 &gt; 1 else 1

@Language.component(&quot;loc_label_filter&quot;)
def custom_component_function(doc):
    old_ents = doc.ents
    new_ents = [item for item in old_ents if item.label_ == &quot;LOC&quot;]
    doc.ents = new_ents
    return doc


nlp = spacy.load(&quot;./path/to/your/own/model&quot;)
nlp.add_pipe(&quot;loc_label_filter&quot;, after=&quot;ner&quot;)

sentences = ['sentence 1', 'sentence2', 'sentence3']

for doc in nlp.pipe(sentences, n_process=cpu_cores):
    print([(doc) for ent in doc.ents])
</code></pre>
<p><strong>IMPORTANT:</strong></p>
<ol>
<li>Please notice these results will be noticeable if your <code>sentences</code> variable contains hundreds or thousands of samples; if sentences is <em>&quot;small&quot;</em> (i.e., it only contains a hundred or less sentences), you (and the time benchmarks) may not notice a big difference.</li>
<li>Please also notice that <code>batch_size</code> parameter in <a href=""https://spacy.io/api/language#pipe"" rel=""nofollow noreferrer""><code>nlp.pipe</code></a> can be also fine tuned, but in my own experience, you want to do that ONLY if with the previous hints, you still don't see a considerable difference.</li>
</ol>
",1,1,770,2022-09-30 11:33:27,https://stackoverflow.com/questions/73908081/how-to-predict-entities-for-multiple-sentences-using-spacy
How to define ner_model in spacy,"<pre class=""lang-py prettyprint-override""><code>def all_ents(v):
        return [(ent.text, ent.label_) for ent in ner_model(v).ents]

df1['Entities'] = df1['text'].apply(lambda v: all_ents(v))

df1.head()
</code></pre>
<p>when executing this shows ner_model not defined can I please know how to define ner model in spacy</p>
","python, nlp, spacy, named-entity-recognition","<p>Something tells me you are not loading your spaCy model properly. Not knowing how <code>df1</code> looks like, I decided to go with one of my own, as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
import pandas as pd


# Building my own `df1`, it should look similar to yours
texts = [
    &quot;Net income was $9.4 million compared to the prior year of $2.7 million.&quot;,
    &quot;Revenue exceeded twelve billion dollars, with a loss of $1b.&quot;,
    &quot;I don't have any entity in me&quot;
]
df1 = pd.DataFrame(texts, columns =['text'])

# Loading spaCy model
model_to_use = &quot;en_core_web_lg&quot;  # Or use the path to your own model
ner_model = spacy.load(model_to_use)

# Your code works now
def all_ents(v):
        return [(ent.text, ent.label_) for ent in ner_model(v).ents]

df1['Entities'] = df1['text'].apply(lambda v: all_ents(v))
</code></pre>
<p><strong>NOTE</strong>:</p>
<p>In my own experience, if <code>df1</code> is considerably large (i.e., it contains thousands of sentences), you may want to convert <code>df1[&quot;text&quot;]</code> into a list or a generator, and then apply <a href=""https://stackoverflow.com/a/73954966/16706763"">these hints</a>. If that's not your case or if your are not interested in an speed-optimal code, then do not pay attention to this note and go ahead with your current implementation.</p>
",0,0,189,2022-10-01 06:27:40,https://stackoverflow.com/questions/73916269/how-to-define-ner-model-in-spacy
sklearn_crfsuite.CRF UnicodeEncodeError,"<ul>
<li>python version: 3.6</li>
<li>os: windows</li>
</ul>
<p>I am trying to train a Chinese NER model with <code>sklearn_crfsuite.CRF</code> with <a href=""http://openkg.cn/dataset/yidu-s4k"" rel=""nofollow noreferrer"">ner_dataset</a>.
After I cleanup the dataset and fit the model, it shows the error message:</p>
<pre><code>60loading training data to CRFsuite:   0%|                                                                                                                                                                     | 0/700 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;main_script.py&quot;, line 22, in &lt;module&gt;
    crf_pipeline.model.fit(x_train, y_train, x_test, y_test)
  File &quot;C:\Users\weber\PycharmProjects\demo-insurance-backend\venv\lib\site-packages\sklearn_crfsuite\estimator.py&quot;, line 314, in fit
    trainer.append(xseq, yseq)
  File &quot;pycrfsuite\_pycrfsuite.pyx&quot;, line 312, in pycrfsuite._pycrfsuite.BaseTrainer.append
  File &quot;stringsource&quot;, line 48, in vector.from_py.__pyx_convert_vector_from_py_std_3a__3a_string
  File &quot;stringsource&quot;, line 15, in string.from_py.__pyx_convert_string_from_py_std__in_string
UnicodeEncodeError: 'ascii' codec can't encode characters in position 2-6: ordinal not in range(128)
</code></pre>
<p>The data format is in <code>.txt</code> seperated with <code>\n</code>, with <code>OriginalText</code> storing the text data and <code>entities</code> storing the entities information.<br />
below is the code I preprocess the dataset:</p>
<pre><code>import ast
from opencc import OpenCC
import sklearn_crfsuite

from sklearn.model_selection import train_test_split
from tqdm import tqdm

tag_dictionary = {
    '影像檢查': 'I-影像檢查',
    '手術': 'S-手術',
    '實驗室檢驗': 'E-實驗室檢驗',
    '解剖部位': 'B-解剖部位',
    '疾病和診斷': 'D-疾病和診斷'
}

def check_entity(entities):
    return [
        entity
        for entity in entities
        if entity['label_type'] in tag_dictionary
    ]

def build_tag_seq(text, entities):
    tag_list = ['O' for token in text]
    for entity in entities:
        if tag_dictionary is None:
            tag = entity['label_type']
        else:
            tag = tag_dictionary[entity['label_type']]
        tag_list[entity['start_pos']] = f'{tag}-B'
        for i in range(entity['start_pos']+1, entity['end_pos']):
            tag_list[i] = f'{tag}-I'
    return tag_list

def data_coverter(data):
    cc = OpenCC('s2t')  # 轉繁體
    data_dict = ast.literal_eval(cc.convert(data))  # txt轉dict
    return data_dict

def process_data(data):
    data_dict = data_coverter(data)
    text = data_dict['originalText']
    entities = data_dict['entities']
    entities = check_entity(entities)
    tag_seq = build_tag_seq(text, entities)
    return text, tag_seq

def load_txt_data(stop=-1):
    data_x = list()  # 內文(token序列)
    data_y = list()  # 每個token的對應tag序列
    for path in ['subtask1_training_part1.txt']:
        with open(path, 'r', encoding='utf-8') as f:
            for i, line in tqdm(enumerate(f.readlines())):
                text = line.strip()
                if len(text) &gt; 3:
                    temp_x, temp_y = process_data(text)

                    data_x.append(temp_x)
                    data_y.append(temp_y)
                    if i == stop:
                        break
    return data_x, data_y

x, y = load_txt_data()

model = sklearn_crfsuite.CRF(
    algorithm='l2sgd',
    c2=1.0,
    max_iterations=1000,
    all_possible_transitions=True,
    all_possible_states=True,
    verbose=True
)

model.fit(x, y)
</code></pre>
<p>Below is pkgs list I used:</p>
<pre><code>pip install opencc sklearn sklearn_crfsuite 
</code></pre>
<p>Does anyone get the similiar error message before and solved it? please, any help would be appreciated.</p>
","python, scikit-learn, named-entity-recognition","<p>I figure out that I cannot use the Chinese symbols in NER tag from <a href=""https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/32#issuecomment-459425196"" rel=""nofollow noreferrer"">reference</a>.<br />
After changing the <code>tag_dictionary</code> with <code>int</code> in the value, it works.</p>
",0,1,162,2022-10-03 06:26:29,https://stackoverflow.com/questions/73931787/sklearn-crfsuite-crf-unicodeencodeerror
Spacy NER: Extract all Persons before a specific word,"<p>I know I can use spacy entity named recognition to extract persons in a text. But I only want to extract the person or personS who are before the word &quot;asked&quot;.</p>
<p>Should I use Matcher together with NER? I am new to Spacy so apologies if the question is simple</p>
<p><strong>Desired Output:</strong><br />
Louis Ng</p>
<p><strong>Current Output:</strong><br />
Louis Ng<br />
Lam Pin Min</p>
<pre><code>
import spacy

nlp = spacy.load(&quot;en_core_web_trf&quot;)


doc = nlp (
    &quot;Mr Louis Ng asked what kind of additional support can we give to sectors and businesses where the human interaction cannot be mechanised. Mr Lam Pin Min replied that businesses could hire extra workers in such cases.&quot;
    )

for ent in doc.ents:
    # Print the entity text and label
    print(ent.text, ent.label_)

</code></pre>
","spacy, named-entity-recognition","<p>You can use a <code>Matcher</code> to find <code>PERSON</code> entities that precede a specific word:</p>
<pre class=""lang-py prettyprint-override""><code>pattern = [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;}, {&quot;ORTH&quot;: &quot;asked&quot;}]
</code></pre>
<p>Because each dict corresponds to a single token, this pattern would only match the last word of the entity (&quot;Ng&quot;). You could let the first dict match more than one token with <code>{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;+&quot;}</code>, but this runs the risk of matching two person entities in a row in an example like &quot;Before Ms X spoke to Ms Y Ms Z asked ...&quot;.</p>
<p>To be able to match a single entity more easily with a <code>Matcher</code>, you can add the component <code>merge_entities</code> to the end of your pipeline (<a href=""https://spacy.io/api/pipeline-functions#merge_entities"" rel=""nofollow noreferrer"">https://spacy.io/api/pipeline-functions#merge_entities</a>), which merges each entity into a single token. Then this pattern would match &quot;Louis Ng&quot; as one token.</p>
",2,1,543,2022-10-09 05:46:12,https://stackoverflow.com/questions/74002390/spacy-ner-extract-all-persons-before-a-specific-word
AttributeError: &#39;list&#39; object has no attribute &#39;ents&#39; in building NER using BERT,"<p>I'm trying to build a <code>NER</code> model using <code>Bert-base-NER</code> for a <code>tweets dataset</code> and ending up getting this error . Please help</p>
<p>This is what I have done</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)

nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)

# ---------

def all_ents(v):
        return [(ent.text, ent.label_) for ent in nlp(v).ents]

df1['Entities'] = df['text'].apply(lambda v: all_ents(v))

df1.head()
</code></pre>
<pre><code>AttributeError: 'list' object has no attribute 'ents'
</code></pre>
<p>Thank you for the help</p>
","python, pandas, bert-language-model, named-entity-recognition","<p>It seems you mix code from different modules.</p>
<p><code>.ents</code> exists in module <code>spacy</code> but not in <code>transformers</code></p>
<pre><code>#import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()

doc = nlp('Hello World of Python. Have a nice day')

print([(x.text, x.label_) for x in doc.ents])
</code></pre>
<p>In <code>transformers</code> you should use directly <code>nlp(v)</code> but it gives directory with <code>ent[&quot;entity&quot;], ent[&quot;score&quot;], ent[&quot;index&quot;], ent[&quot;word&quot;], ent[&quot;start&quot;], ent[&quot;end&quot;]</code></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)

nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)

# ---------

import pandas as pd

df = pd.DataFrame({
    'text': ['Hello World of Python. Have a nice day']
})

# ---------

def all_ents(v):
    #print(nlp(v))
    return [(ent['word'], ent['entity']) for ent in nlp(v)]

df['Entities'] = df['text'].apply(all_ents)

#df1['Entities'] = df['text'].apply(lambda v: [(ent['word'], ent['entity']) for ent in nlp(v)])

print(df['Entities'].head())
</code></pre>
",1,0,301,2022-10-25 10:53:06,https://stackoverflow.com/questions/74192948/attributeerror-list-object-has-no-attribute-ents-in-building-ner-using-bert
"Input 0 of layer &quot;lstm&quot; is incompatible with the layer: expected shape=(128, None, 256), found shape=(32, 187, 256) - character RNN model - tensorflow","<p>below is the error traceback when I run the predict() using my trained model</p>
<pre><code>
ValueError                                Traceback (most recent call last)
\&lt;ipython-input-51-5ae18e06838a\&gt; in \&lt;module\&gt;
7 for input_example_batch, target_example_batch in ds_series_batch_test:
8
\----\&gt; 9   pred=model.predict(input_example_batch)
10   pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()
11   y_true=target_example_batch.numpy().flatten()

1 frames
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in tf__predict_function(iterator)
13                 try:
14                     do_return = True
\---\&gt; 15                     retval\_ = ag_\_.converted_call(ag_\_.ld(step_function), (ag_\_.ld(self), ag_\_.ld(iterator)), None, fscope)
16                 except:
17                     do_return = False

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1845, in predict_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1834, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1823, in run_step  **
        outputs = model.predict_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1791, in predict_step
        return self(x, training=False)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py&quot;, line 264, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer &quot;{layer_name}&quot; is '
    
    ValueError: Exception encountered when calling layer &quot;sequential&quot; (type Sequential).
    
    Input 0 of layer &quot;lstm&quot; is incompatible with the layer: expected shape=(128, None, 256), found shape=(32, 187, 256)
    
    Call arguments received by layer &quot;sequential&quot; (type Sequential):
      • inputs=tf.Tensor(shape=(32, 187), dtype=int32)
      • training=False
      • mask=None
</code></pre>
<p>below is the code used for generating train and test data</p>
<pre><code>X_total, X_test, y_total, y_test = train_test_split(train, test,
    test_size=0.2, shuffle = True, random_state = 8)

X_train, X_val, y_train, y_val = train_test_split(X_total, y_total,
    test_size=0.25, shuffle = True, random_state = 8)  # 0.25 x 0.8 = 0.2

print(&quot;X_train shape: {}&quot;.format(X_train.shape))
print(&quot;X_test shape: {}&quot;.format(X_test.shape))
print(&quot;X_val shape: {}&quot;.format(X_val.shape))
print(&quot;y_train shape: {}&quot;.format(y_train.shape))
print(&quot;y_test shape: {}&quot;.format(y_test.shape))
print(&quot;y val shape: {}&quot;.format(y_val.shape))
</code></pre>
<pre><code>X_train shape: (173424,)
X_test shape: (57809,)
X_val shape: (57809,)
y_train shape: (173424,)
y_test shape: (57809,)
y val shape: (57809,)
</code></pre>
<pre><code># train_formatted
train_formatted = []
for eg1, eg2 in zip(X_train, y_train):
  train_formatted.append((eg1,eg2))

# test_formatted
test_formatted = []
for eg1, eg2 in zip(X_test, y_test):
  test_formatted.append((eg1,eg2))
# valid_formatted
valid_formatted = []
for eg1, eg2 in zip(X_val, y_val):
  valid_formatted.append((eg1,eg2))
# training generator
def gen_train_series():

    for eg in train_formatted:
      yield eg[0],eg[1]

# validation generator
def gen_valid_series():

    for eg in valid_formatted:
      yield eg[0],eg[1]

# test generator
def gen_test_series():

  for eg in test_formatted:
      yield eg[0],eg[1]
  
  
# create Dataset objects for train, test and validation sets  
series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))
series_valid = tf.data.Dataset.from_generator(gen_valid_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))
series_test = tf.data.Dataset.from_generator(gen_test_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))

BATCH_SIZE = 128
BUFFER_SIZE=1000

# create padded batch series objects for train, test and validation sets
ds_series_batch = series.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)
ds_series_batch_valid = series_valid.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)
ds_series_batch_test = series_test.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)

# print example batches
for input_example_batch, target_example_batch in ds_series_batch_valid.take(1):
  print(input_example_batch)
  print(target_example_batch)

# below are the shapes of my input and target batch tensor shape
</code></pre>
<pre><code>input_example_batch - shape passed to predict() -  as follows
tf.Tensor(
[[36 26 37 ...  0  0  0]
 [40 40 43 ...  0  0  0]
 [26 39 26 ...  0  0  0]
 ...
 [11  8 12 ...  0  0  0]
 [44 28 33 ...  0  0  0]
 [46  1 38 ...  0  0  0]], shape=(128, 160), dtype=int32)
tf.Tensor(
[[6 6 6 ... 0 0 0]
 [6 6 6 ... 0 0 0]
 [6 6 6 ... 0 0 0]
 ...
 [2 6 2 ... 0 0 0]
 [2 2 2 ... 0 0 0]
 [2 6 2 ... 0 0 0]], shape=(128, 160), dtype=int32)
</code></pre>
<p>I am training my model as follows -</p>
<pre><code>vocab_size = len(vocabulary) #53

# The embedding dimension
embedding_dim = 256

# Number of RNN units
rnn_units = 1024

label_size = len(labels)  # - 0 to 7

def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):
      model = tf.keras.Sequential([
          tf.keras.layers.Embedding(vocab_size, embedding_dim,
                            batch_input_shape=[batch_size, None],mask_zero=True),
          tf.keras.layers.LSTM(rnn_units,
                      return_sequences=True,
                      stateful=True,
                      recurrent_initializer='glorot_uniform'),
          tf.keras.layers.Dense(label_size)
          ])
      return model


#TODO - check why vocab_size+1,
# passing label_size - because it already includes - 'OTHER' label

model = build_model(vocab_size = vocab_size+1 ,label_size = label_size,embedding_dim=embedding_dim,
      rnn_units=rnn_units,
              batch_size=BATCH_SIZE)

# define loss function
def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])


checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True, save_freq = 'epoch')

# fitting the model as follows

# using just 1 epoch - as I am debugging

EPOCHS = 1
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
      filepath=checkpoint_prefix,
      save_weights_only=True, save_freq = 'epoch')
  
ic(&quot;Fitting the Model...&quot;)
history = model.fit(ds_series_batch, epochs=EPOCHS, validation_data=ds_series_batch_valid,callbacks=[checkpoint_callback])

</code></pre>
<p>below displayed is the model summary</p>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (128, None, 256)          13824     
                                                                 
 lstm (LSTM)                 (128, 1024)               5246976   
                                                                 
 dense (Dense)               (128, 7)                  7175      
                                                                 
=================================================================
Total params: 5,267,975
Trainable params: 5,267,975
Non-trainable params: 0
_______________________________
</code></pre>
<pre><code>from sklearn.metrics import classification_report, confusion_matrix

preds = np.array([])
y_trues= np.array([])

# iterate through test set, make predictions based on trained model
for input_example_batch, target_example_batch in ds_series_batch_test:

  pred=model.predict(input_example_batch)
  pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()
  y_true=target_example_batch.numpy().flatten()

</code></pre>
<p>the above code is what I use to call predict() - which results in the error mentioned in the title</p>
<p>below link has the code referred to - test out a character level RNN implementation
<a href=""http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html"" rel=""nofollow noreferrer"">http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html</a>
I pretty much did the same as the one in the collab notebook, but training using a different input data.</p>
<p>Many solutions suggested were not very specific to this problem, and one I tried - was setting return_sequences = True, which is what I already had while building the model. Nothing has worked so far. with above code, training works fine, but unable to run predict - due to mismatch of output shape. I am very new to tensorflow. So any kind of help in understanding the issue, and ways to resolve would be greatly appreciated. Thank you in advance.</p>
<pre><code>    encoded_input_features                              output_features
0   [11, 11, 1, 39, 40, 43, 45, 33, 1, 44, 45, 43,...   [2, 2, 6, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, ...
1   [29, 5, 10, 5, 44, 33, 34, 47, 26, 37, 34, 36,...   [6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, ...
2   [10, 14, 5, 41, 43, 26, 28, 33, 34, 1, 27, 46,...   [2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, ...
3   [48, 51, 1, 14, 16, 10, 1, 41, 26, 29, 26, 38,...   [6, 6, 6, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
4   [32, 31, 1, 9, 11, 5, 45, 15, 5, 32, 43, 26, 2...   [6, 6, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, ...
... ... ...
289041  [47, 30, 39, 36, 30, 45, 30, 44, 33, 1, 39, 26...   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
289042  [43, 26, 27, 34, 39, 29, 43, 26, 1, 36, 30, 39...   [6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, ...
289043  [41, 26, 37, 35, 26, 1, 31, 26, 37, 34, 50, 46...   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
289044  [17, 13, 16, 10, 9, 15, 1, 36, 26, 37, 34, 1, ...   [2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
289045  [36, 40, 39, 32, 46, 1, 38, 30, 44, 44, 5, 36,...   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
</code></pre>
<p>now after I generate the train, test, validation split - from the above
dataset - trying to convert the input features - into a list of numpy array</p>
<pre><code>import numpy as np
address_character_list = []

def convert_input_features(x):
  address_character_list.append(np.asarray(x, dtype=np.float32))

pd.DataFrame(y_train)['output_features'].swifter.apply(lambda x: convert_input_features(x))
y_train_list = np.array(address_character_list)

import numpy as np
address_character_list = []

def convert_input_features(x):
  address_character_list.append(np.asarray(x, dtype=np.float32))

pd.DataFrame(X_train)['encoded_input_features'].swifter.apply(lambda x: convert_input_features(x))
X_train_list = np.array(address_character_list)
</code></pre>
<p>this gives</p>
<pre><code>type(X_train) = numpy.ndarray
type(y_train) = numpy.ndarray
</code></pre>
<p>for eg. X_train looks like</p>
<pre><code>array([array([...],
             dtype=float32)                                                ,[array([...], dtype = float32),
      dtype=object)
</code></pre>
<p>similarly - I have y_train as well,
now if I need to reshape my train example using code below - how do I do that, cause from_tensor_slices() gives the error below</p>
<pre><code># train_examples = tf.data.Dataset.from_tensor_slices((X_train_list, y_train_list))
train_examples = tf.data.Dataset.from_tensor_slices((X_train_list, y_train_list))
# x_train = np.random.randint(0,10, size=(289042))
# y_train = np.random.randint(0,7, size=(289042))
# train_examples = tf.data.Dataset.from_tensor_slices((x_train, y_train))

def preprocess(ds):
    return (
        ds
        .cache()
        .shuffle(buffer_size=1000)
        .batch(128)
        .prefetch(buffer_size=tf.data.AUTOTUNE)
    )

train_examples = preprocess(train_examples)

</code></pre>
<p>error as follows</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).
</code></pre>
<p>and the solutions given for this error didn't work.</p>
","tensorflow, keras, recurrent-neural-network, named-entity-recognition","<p>Updating the answer... Now, let suppose I as you said I have</p>
<pre><code>x_train = np.random.randint(0,10, size=(289042))
y_train = np.random.randint(0,7, size=(289042))
</code></pre>
<p>You don't need a generator to yield the dataset, if you have a numpy array simply load it with <strong>tf.data.DataSet.from_tensor_slices()</strong>,</p>
<pre><code>train_examples = tf.data.Dataset.from_tensor_slices((x_train, y_train))

def preprocess(ds):
    return (
        ds
        .cache()
        .shuffle(buffer_size=1000)
        .batch(128)
        .prefetch(buffer_size=tf.data.AUTOTUNE)
    )

train_examples = preprocess(train_examples)
</code></pre>
<p>Now, something more if you want stateful=True in LSTM then your batch_size should be equal for all the samples, there may be a chance if your last sample has less than 128 batch size. so, then it will throw the error.</p>
<pre><code>model = tf.keras.Sequential([
          tf.keras.layers.Input(shape=(1)),
          tf.keras.layers.Embedding(1024, 512, mask_zero=True),
          tf.keras.layers.LSTM(200,
                      return_sequences=False,
                      recurrent_initializer='glorot_uniform'),
          tf.keras.layers.Dense(7)
          ])
</code></pre>
<pre><code>model(next(iter(train_examples.take(1)))[0]).shape
model.summary()
</code></pre>
<p>Furthermore, use tf.keras.losses.SparseCategorical_crossentropy(from_logits=True)</p>
<pre><code>model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
              ,metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
</code></pre>
<pre><code>history = model.fit(train_examples, epochs=1)
</code></pre>
<p>Output:</p>
<pre><code>[==============================] - 7s 30ms/step - loss: 1.9473 - sparse_categorical_accuracy: 0.1398
</code></pre>
",-1,0,286,2022-11-11 02:52:49,https://stackoverflow.com/questions/74397544/input-0-of-layer-lstm-is-incompatible-with-the-layer-expected-shape-128-non
How to extract the output froman NLP model to a dataframe?,"<p>I have trained an NLP Model (NER) and I have results in the below format:</p>
<pre><code>for text, _ in TEST_DATA:
    doc = nlp(text)
    print([(ent.text, ent.label_) for ent in doc.ents])

#Output
[('1131547', 'ID'), ('12/9/2019', 'Date'), ('USA', 'ShippingAddress')]
[('567456', 'ID'), ('Hills', 'ShippingAddress')]

#I need the output in the below format

ID       Date     ShippingAddress 
1131547 12/9/2019 USA     
567456    NA      Hills    
</code></pre>
<p>Thanks for your help in advance</p>
","python, dictionary, nlp, spacy, named-entity-recognition","<p>In order to import the data into a Pandas dataframe, you can use</p>
<pre class=""lang-py prettyprint-override""><code>data_array = []

for text, _ in TEST_DATA:
    doc = nlp(text)
    data_array.append({ent.label_:ent.text for ent in doc.ents})

import pandas as pd
df = pd.DataFrame.from_dict(data_array)
</code></pre>
<p>The test result:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; pd.DataFrame.from_dict(data_array)
        ID       Date ShippingAddress
0  1131547  12/9/2019             USA
1   567456        NaN           Hills
</code></pre>
",1,2,78,2022-11-30 14:33:17,https://stackoverflow.com/questions/74629474/how-to-extract-the-output-froman-nlp-model-to-a-dataframe
Converting a dataset to CoNLL format. Label remaining tokens with O,"<p>I have a manually annotated dataset that contains records in the following format:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;id&quot;: 1,
    &quot;text&quot;: &quot;At the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the Third Amendment and until December 30, 1996, the Company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0.&quot;,
    &quot;label&quot;: [
        [
            209,
            230,
            &quot;COV_3&quot;
        ],
        [
            379,
            390,
            &quot;VAL_3&quot;
        ]
    ],
}
</code></pre>
<p>In the example above, <code>&quot;label&quot;</code> represents the custom entities I have in my dataset. In the example shown above, the phrase <code>fixed charge coverage</code> is located at position <code>[309, 336]</code> and is given the label <code>COV_3</code>. Likewise, the phrase <code>1.25 to 1.0</code> is located at <code>[379, 390]</code> and is given the label <code>VAL_3</code>.</p>
<p>Now, I would like to fine-tune some transformer model like BERT on this dataset, however, I realised that the dataset must be in CoNLL format. Or at least, all the tokens of each datapoint must be labelled. Is there any way I can easily label the remaining tokens with label <code>&quot;O&quot;</code> or I can transform this dataset in the CoNLL format?</p>
","nlp, stanford-nlp, huggingface-transformers, named-entity-recognition","<p>You use spacy to tokenize and convert character offset annotation to IOB tags with built-in utility methods. Note that this will skip any spans that don't align to the token boundaries, so you may need to customize the tokenizer or provide the tokenization from another source when creating a <code>Doc</code>.</p>
<p>The character offsets in the question don't line up with the text and are modified below.</p>
<pre class=""lang-py prettyprint-override""><code># tested with spacy v3.4.3, should work with spacy v3.x
import spacy
from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags

data = {
    &quot;id&quot;: 1,
    &quot;text&quot;: &quot;At the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the Third Amendment and until December 30, 1996, the Company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0.&quot;,
    &quot;label&quot;: [[209, 230, &quot;COV_3&quot;], [254, 265, &quot;VAL_3&quot;]],
}

nlp = spacy.blank(&quot;en&quot;)

# tokenize the text to create a doc
doc = nlp(data[&quot;text&quot;])

# convert annotation to entity spans and add them to the doc
ents = []
for start, end, label in data[&quot;label&quot;]:
    span = doc.char_span(start, end, label=label)
    if span is not None:
        ents.append(span)
    else:
        print(
            &quot;Skipping span (does not align to tokens):&quot;,
            start,
            end,
            label,
            doc.text[start:end],
        )
doc.ents = ents

# convert doc annotation to IOB tags
for token, iob_tag in zip(doc, biluo_to_iob(doc_to_biluo_tags(doc))):
    print(token.text + &quot; &quot; + iob_tag)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>At O
the O
end O
of O
each O
fiscal O
quarter O
, O
for O
the O
four O
consecutive O
fiscal O
quarters O
ending O
as O
of O
such O
fiscal O
quarter O
end O
, O
from O
the O
date O
of O
the O
Third O
Amendment O
and O
until O
December O
30 O
, O
1996 O
, O
the O
Company O
shall O
maintain O
a O
fixed B-COV_3
charge I-COV_3
coverage I-COV_3
ratio O
of O
not O
less O
than O
1.25 B-VAL_3
to I-VAL_3
1.0 I-VAL_3
. O
</code></pre>
<p>These are the 1st and 4th columns from the 4-column CoNLL 2003 format. You may want to insert blank lines for sentence boundaries or add the special document boundary lines, and you may need some real or placeholder values for the 2nd/3rd tag and chunk columns for use with other tools.</p>
",3,4,1036,2022-12-03 05:41:44,https://stackoverflow.com/questions/74664286/converting-a-dataset-to-conll-format-label-remaining-tokens-with-o
How to add simple custom pytorch-crf layer on top of TokenClassification model using pytorch and Trainer,"<p>I followed this link, but its implemented in Keras.</p>
<p><a href=""https://stackoverflow.com/questions/67095045/cannot-add-crf-layer-on-top-of-bert-in-keras-for-ner"">Cannot add CRF layer on top of BERT in keras for NER</a></p>
<h3>Model description</h3>
<p>Is it possible to add simple custom <code>pytorch-crf</code> layer on top of <code>TokenClassification model</code>. It will make the model more robust.</p>
<pre><code>from torchcrf import CRF

model_checkpoint = &quot;dslim/bert-base-NER&quot;
tokenizer = BertTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)
config = BertConfig.from_pretrained(model_checkpoint, output_hidden_states=True)
bert_model = BertForTokenClassification.from_pretrained(model_checkpoint,id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True)


class BERT_CRF(nn.Module):
    
    def __init__(self, bert_model, num_labels):
        super(BERT_CRF, self).__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(0.25)
        
        self.classifier = nn.Linear(4*768, num_labels)

        self.crf = CRF(num_labels, batch_first = True)
    
    def forward(self, input_ids, attention_mask,  labels=None, token_type_ids=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        
        **sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)**
        sequence_output = self.dropout(sequence_output)
        
        emission = self.classifier(sequence_output) # [32,256,17]
        labels=labels.reshape(attention_mask.size()[0],attention_mask.size()[1])
        
        if labels is not None:    
            loss = -self.crf(log_soft(emission, 2), labels, mask=attention_mask.type(torch.uint8), reduction='mean')
            prediction = self.crf.decode(emission, mask=attention_mask.type(torch.uint8))
            return [loss, prediction]
                
        else:         
            prediction = self.crf.decode(emission, mask=attention_mask.type(torch.uint8))
            return prediction

</code></pre>
<pre><code>args = TrainingArguments(
    &quot;spanbert_crf_ner-pos2&quot;,
    # evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=1,
    weight_decay=0.01,
    per_device_train_batch_size=8,
    # per_device_eval_batch_size=32
    fp16=True
    # bf16=True #Ampere GPU
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_data,
    # eval_dataset=train_data,
    # data_collator=data_collator,
    # compute_metrics=compute_metrics,
    tokenizer=tokenizer)
</code></pre>
<p>I get error on line <code>   **sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)**</code></p>
<p>As <code>outputs = self.bert(input_ids, attention_mask=attention_mask)</code> gives the logits for tokenclassification<code>. How can we get hidden states so that I can concate last 4 hidden states. so that I can do</code>outputs[1][-1]`?</p>
<p>Or is their easier way to implement <code>BERT-CRF</code> model?</p>
","python-3.x, pytorch, bert-language-model, named-entity-recognition, crf","<p>i know it's 10 months later, but maybe it helps other guys</p>
<p>Here is what I used for Trainer and it works in hyperparameter_search too:</p>
<pre><code>class BERT_CRF_Config(PretrainedConfig):
    model_type = &quot;BERT_CRF&quot;

    def __init__(self, **kwarg):
        super().__init__(**kwarg)
        self.model_name = &quot;BERT_CRF&quot;
        self.use_last_n_hidden_states = 1
        self.dropout = 0.5

class BERT_CRF(PreTrainedModel):
    config_class = BERT_CRF_Config

    def __init__(self, config):
        super().__init__(config)

        bert_config = BertConfig.from_pretrained(config.bert_name)

        bert_config.output_attentions = True
        bert_config.output_hidden_states = True

        self.bert = AutoModel.from_pretrained(config.bert_name, config=bert_config)

        self.dropout = nn.Dropout(p=config.dropout)

        self.linear = nn.Linear(
            self.bert.config.hidden_size*config.use_last_n_hidden_states, config.num_labels)
        
        self.crf = CRF(config.num_labels, batch_first=True)

    def forward(self,  input_ids = None, attention_mask = None, labels = None,
                labels_mask=None,  token_type_ids=None, return_dict = None, **kwargs):

        if not torch.is_tensor(input_ids):
          input_ids = torch.tensor(input_ids).to(self.device)

        if not torch.is_tensor(token_type_ids):
          token_type_ids = torch.tensor(token_type_ids).to(self.device)

        if not torch.is_tensor(attention_mask):
          attention_mask = torch.tensor(attention_mask).to(self.device)

        if not torch.is_tensor(labels):
          labels = torch.tensor(labels).to(self.device)

        if not torch.is_tensor(labels_mask):
          labels_mask = torch.tensor(labels_mask).to(self.device)

        bert_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, 
                                attention_mask=attention_mask)
        # last_hidden_layer = bert_output['last_hidden_state']
        # logits = self.linear(last_hidden_layer)

        last_hidden_layers = torch.cat(bert_output['hidden_states'][-self.config.use_last_n_hidden_states:], dim=2)
        last_hidden_layers = self.dropout(last_hidden_layers)
        logits = self.linear(last_hidden_layers)

        def to_tensor(x):
          x = list(map(lambda y: torch.as_tensor(y), x))
          x = torch.nested.as_nested_tensor(x)
          x = torch.nested.to_padded_tensor(x,padding=0)

          x = torch.clamp(x, min=0)

          return x

        if labels is not None:
          log_likelihood, outputs = (
                                     self.crf(logits, labels, mask=labels_mask.bool()), 
                                     self.crf.decode(logits, mask=labels_mask.bool())
                                    )
          outputs = to_tensor(outputs)
          loss = -log_likelihood
          if not return_dict:
            return loss, outputs
          else:
            return TokenClassifierOutput(
                loss=loss,
                logits=outputs,
                hidden_states=bert_output.hidden_states,
                attentions=bert_output.attentions,
            )
        
        outputs = self.crf.decode(logits, batch_first=True)
        outputs = to_tensor(outputs)

        return outputs

    @property
    def device(self):
        return next(self.parameters()).device
</code></pre>
<p>and for your hyperparameter search you can use something like this:</p>
<pre><code>def optuna_hp_space(trial):
    return {
        &quot;learning_rate&quot;: trial.suggest_categorical(&quot;learning_rate&quot;, [1e-5, 2e-5, 2e-5, 4e-5, 5e-5, 6e-5]),
        &quot;warmup_ratio&quot;: trial.suggest_categorical(&quot;warmup_ratio&quot;, [0, 0.1, 0.2, 0.3]),
        &quot;weight_decay&quot;: trial.suggest_categorical(&quot;weight_decay&quot;, [1e-6, 1e-5, 1e-4]),
        &quot;max_grad_norm&quot;: trial.suggest_categorical(&quot;max_grad_norm&quot;, [8, 9,10,11]),
    }

def model_init_crf(trial):
    config = BERT_CRF_Config.from_pretrained(BERT_MODEL, num_labels=NR_LABELS, )
    config.bert_name = BERT_MODEL
    config.dropout = trial.suggest_categorical(&quot;dropout&quot;, [0, 0.10,  0.30,  0.50])
    config.use_last_n_hidden_states = trial.suggest_categorical(&quot;last_n_hidden_states&quot;,
                                                          range(1, config.num_hidden_layers+1))

    model = BERT_CRF(config).to('cuda')
    return model

best_trial = trainer.hyperparameter_search(
    direction=&quot;maximize&quot;,
    backend=&quot;optuna&quot;,
    hp_space=optuna_hp_space,
    n_trials=50,
    compute_objective=my_objective,
)
</code></pre>
",2,1,877,2022-12-06 06:21:53,https://stackoverflow.com/questions/74698116/how-to-add-simple-custom-pytorch-crf-layer-on-top-of-tokenclassification-model-u
spaCy model .from_disk doesn&#39;t load patterns,"<p>For a Named Entity Recognition task in Dutch with spaCy, I added entities using EntityRuler. When I add the ruler to the pipeline in my notebook:</p>
<pre><code>nlp = spacy.load(&quot;nl_core_news_md&quot;)
ruler = nlp.add_pipe(&quot;entity_ruler&quot;, before=&quot;ner&quot;)
patterns  = complete_dicts # This is a list of dictionaries, e.g. [{&quot;label&quot;: &quot;PERSON&quot;, &quot;pattern&quot;: &quot;Staf Aerts&quot;}, {&quot;label&quot;: &quot;PERSON&quot;, &quot;pattern&quot;: &quot;Meyrem Almaci&quot;}]
ruler.add_patterns(patterns)
</code></pre>
<p>the NER-pipeline works very well. However, when I save it to my disk and then load this model again using</p>
<pre><code>nlp.from_disk(&quot;path/to_model&quot;)
</code></pre>
<p>the model misses entities that are added through the EntityRuler.</p>
<p>I found nothing in the documentation why this would happen. I would be grateful for anyone who has an explanation for this! Thanks.</p>
","spacy, named-entity-recognition, spacy-3","<p>To load a saved model, use <code>spacy.load</code>:</p>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load(&quot;/path/to/model&quot;)
</code></pre>
<p>More details about how <code>spacy.load</code> works (including <code>nlp.from_disk</code>): <a href=""https://spacy.io/usage/processing-pipelines#pipelines"" rel=""nofollow noreferrer"">https://spacy.io/usage/processing-pipelines#pipelines</a></p>
",4,4,282,2022-12-13 13:46:12,https://stackoverflow.com/questions/74785846/spacy-model-from-disk-doesnt-load-patterns
Why is the spaCy Scorer returning None for the entity scores but the model is extracting entities?,"<p>I am really confused why the Scorer.score is returning ents_p, ents_r, and ents_f as None for the below example. I am seeing something every similar with my own custom model and want to understand why it is returning None?</p>
<p><strong>Example Scorer Code - Returning None for ents_p, ents_r, ents_f</strong></p>
<pre><code>import spacy
from spacy.scorer import Scorer
from spacy.tokens import Doc
from spacy.training.example import Example

examples = [
    ('Who is Talha Tayyab?',
     {(7, 19, 'PERSON')}),
    ('I like London and Berlin.',
     {(7, 13, 'LOC'), (18, 24, 'LOC')}),
     ('Agra is famous for Tajmahal, The CEO of Facebook will visit India shortly to meet Murari Mahaseth and to visit Tajmahal.',
     {(0, 4, 'LOC'), (40, 48, 'ORG'), (60, 65, 'GPE'), (82, 97, 'PERSON'), (111, 119, 'GPE')})
]

def my_evaluate(ner_model, examples):
    scorer = Scorer()
    example = []
    for input_, annotations in examples:
        pred = ner_model(input_)
        print(pred,annotations)
        temp = Example.from_dict(pred, dict.fromkeys(annotations))
        example.append(temp)
    scores = scorer.score(example)
    return scores

ner_model = spacy.load('en_core_web_sm') # for spaCy's pretrained use 'en_core_web_sm'
results = my_evaluate(ner_model, examples)
print(results)
</code></pre>
<p><strong>Scorer Results</strong></p>
<pre><code>{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'sents_p': None, 'sents_r': None, 'sents_f': None, 'tag_acc': None, 'pos_acc': None, 'morph_acc': None, 'morph_micro_p': None, 'morph_micro_r': None, 'morph_micro_f': None, 'morph_per_feat': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'ents_p': None, 'ents_r': None, 'ents_f': None, 'ents_per_type': None, 'cats_score': 0.0, 'cats_score_desc': 'macro F', 'cats_micro_p': 0.0, 'cats_micro_r': 0.0, 'cats_micro_f': 0.0, 'cats_macro_p': 0.0, 'cats_macro_r': 0.0, 'cats_macro_f': 0.0, 'cats_macro_auc': 0.0, 'cats_f_per_type': {}, 'cats_auc_per_type': {}}
</code></pre>
<p>It is clearly picking out entities from the text</p>
<pre><code>doc = ner_model('Agra is famous for Tajmahal, The CEO of Facebook will visit India shortly to meet Murari Mahaseth and to visit Tajmahal.')
for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>Agra PERSON
Tajmahal ORG
Facebook ORG
India GPE
Murari Mahaseth PERSON
Tajmahal ORG
</code></pre>
","python, spacy, named-entity-recognition, precision-recall","<p>This line is the issue, the annotations are not added to the reference docs because they're not in the right format:</p>
<pre class=""lang-py prettyprint-override""><code>Example.from_dict(pred, dict.fromkeys(annotations))
</code></pre>
<p>The expected format is:</p>
<pre class=""lang-py prettyprint-override""><code>Example.from_dict(pred, {&quot;entities&quot;: [(start, end, label), (start, end, label), ...]})
</code></pre>
<p>You can also use the built-in <code>Language.evaluate</code> if you create examples where <code>Example.predicted</code> is unannotated, which also creates the scorer based on your pipeline so you don't end up a lot of irrelevant <code>None</code> scores:</p>
<pre class=""lang-py prettyprint-override""><code>Example.from_dict(nlp.make_doc(text), {&quot;entities&quot;: [(start, end, label), (start, end, label), ...]})

Once you have these kinds of examples, run:

```python
scores = ner_model.evaluate(examples)
</code></pre>
",2,0,456,2022-12-29 16:37:40,https://stackoverflow.com/questions/74953747/why-is-the-spacy-scorer-returning-none-for-the-entity-scores-but-the-model-is-ex
Simple NER - IndexError: string index out of range error,"<p>Here is a simple example of Named Entity Recognition (NER) using the named entity recognition tool in the Natural Language Toolkit (nltk) library in Python:</p>
<p>import nltk</p>
<h1>Input text</h1>
<p>text = &quot;Barack Obama was born in Hawaii. He was the 44th President of the United States.&quot;</p>
<h1>Tokenize the text</h1>
<p>tokens = nltk.word_tokenize(text)</p>
<h1>Perform named entity recognition</h1>
<p>entities = nltk.ne_chunk(tokens)</p>
<h1>Print the named entities</h1>
<p>print(entities)</p>
<p>When I run this code in my Jupyter Notebook, I get this error.</p>
<p>&quot;IndexError: string index out of range&quot;</p>
<p>Am I missing any installation? Please advise.</p>
<p>Expected output:</p>
<p>(PERSON Barack/NNP Obama/NNP)
(GPE Hawaii/NNP)
(ORGANIZATION United/NNP States/NNPS)</p>
",named-entity-recognition,"<p><code>nltk.ne_chunk</code> expects its input to be tagged tokens rather than just plain tokens, so I would recommend adding a tagging step between the tokenization and ne chunking via <code>nltk.pos_tag</code>. ne chunking still would give you every token, chunked by entities if there are any detected. Since you want only the entities, you can check for if there is a tree in a particular chunk. Like the following:</p>
<pre><code>text = &quot;Barack Obama was born in Hawaii. He was the 44th President of the United States.&quot;
tokens = nltk.word_tokenize(text)
tagged_tokens = nltk.pos_tag(tokens)
entities = [chunk for chunk in nltk.ne_chunk(tagged_tokens) if isinstance(chunk, nltk.Tree)]

for entity in entities:
    print(entity)
</code></pre>
<p>Please note that this code doesn't give exactly the output you want. Instead it gives:</p>
<pre><code>(PERSON Barack/NNP)
(PERSON Obama/NNP)
(GPE Hawaii/NNP)
(GPE United/NNP States/NNPS)
</code></pre>
",0,0,110,2023-01-06 01:24:44,https://stackoverflow.com/questions/75026054/simple-ner-indexerror-string-index-out-of-range-error
Error while loading spacy model from the pickle file,"<p>I am getting the following error while loading spacy NER model from the pickle file.</p>
<pre><code>self.model = pickle.load(open(model_path, 'rb')) 
</code></pre>
<pre><code>Traceback (most recent call last):   
File &quot;C:\Program Files\JetBrains\PyCharm Community Edition 2022.2.3\plugins\python-ce\helpers\pydev\pydevd.py&quot;, line 1496, in _exec     
pydev_imports.execfile(file, globals, locals)  # execute the script   File &quot;C:\Program Files\JetBrains\PyCharm Community Edition 2022.2.3\plugins\python-ce\helpers\pydev\_pydev_imps\_pydev_execfile.py&quot;, line 18, in execfile     
exec(compile(contents+&quot;\n&quot;, file, 'exec'), glob, loc)   File 
&quot;C:\Projects\pythonworkspace\invoice_processing_prototype\invoice_data_extractor_notebook.py&quot;, line 101, in 
&lt;module&gt;     
extractor = InvoiceDataExtractor(model_dir_path, input_file_paths[0], config_path)   File 
&quot;C:\Projects\pythonworkspace\invoice_processing_prototype\invoicedataextractor.py&quot;, line 27, in 
__init__ self.spatial_extractor = SpatialExtractor(model_dir_path, config_path)   
File &quot;C:\Projects\pythonworkspace\invoice_processing_prototype\spatialextractor.py&quot;, line 54, in __init__     
self.inv_date = Model(f{self.model_dir_path}\\invoice_date_with_corrected_training_data_and_line_seperator_21_07_2022.pkl&quot;,   File 
&quot;C:\Projects\pythonworkspace\invoice_processing_prototype\spatialextractor.py&quot;, line 34, in __init__     

self.model = pickle.load(open(model_path, 'rb'))   
File &quot;stringsource&quot;, line 6, in spacy.pipeline.trainable_pipe.__pyx_unpickle_TrainablePipe _pickle.PickleError: 
Incompatible checksums (0x417ddeb vs (0x61fbab5, 0x27e6ee8, 0xbe56bc9) = (cfg, model, name, scorer, vocab))

</code></pre>
<p>I am getting this error while running the following line of code:</p>
<p><code>self.model = pickle.load(open(model_path, 'rb'))</code></p>
<p>I have trained the NER model using the spacy version 3.1.2 and I recently upgraded the spacy to the latest 3.4. The error might be because of some version incompatibilities. If that is the case can someone confirm if is it possible to load spacy NER model trained on spacy version '3.1.2' can be loaded on the upgraded spacy '3.4'</p>
<p><strong>Environment</strong></p>
<p>Operating System: Windows 10</p>
<p>Python Version Used: 3.10</p>
<p>spaCy Version Used while training: 3.1.2</p>
<p>spaCy Version Used while prediction: 3.4</p>
","python-3.x, nlp, spacy, named-entity-recognition","<p>I found one workaround to this error.
While on <code>spacy 3.1.2</code> I have loaded the pickle model and saved it using</p>
<pre><code>model = pickle.load(open(filepath, 'rb'))    
model.to_disc('my_model')
</code></pre>
<p>Then I have updated the spacy version to latest <code>spacy 3.4.4</code> and reloaded the 'my_model' this time.</p>
<pre><code>model = spacy.load(&quot;my_model&quot;)
pickle.dump(model, open(filepath, 'wb'))  
</code></pre>
<p>In that case it worked for me without the error.</p>
",0,2,561,2023-01-06 10:28:46,https://stackoverflow.com/questions/75029755/error-while-loading-spacy-model-from-the-pickle-file
LSTM named entity recognition model - shape are incompatible or logits/labels have different dimensions - Tensorflow 2.9,"<p>I am working on NLP LSTM named entity extraction model but running into different errors below are more details about error. I am running this code in jupiter notebook</p>
<p>Tensorflow version 2.9</p>
<p>Both input and output are of length 50</p>
<p><strong>input sentence</strong> : [123 88 170 221 132 52 105 32 211 91 126 211 24 221 134 154 221 162
215 80 144 101 61 136 68 133 40 200 133 40 218 131 139 199 124 74
184 92 213 185 221 221 221 221 221 221 221 221 221 221]</p>
<p><strong>output sentece label</strong>: [ 7 7 7 7 0 7 6 2 7 5 1 7 7 7 7 7 7 7 7 10 7 7 7 7
3 8 7 3 8 7 7 7 7 7 7 7 7 6 2 7 7 7 7 7 7 7 7 7
7 7]</p>
<p>Added upto 5 layers to train the model</p>
<p><strong>Here is the model:</strong></p>
<pre><code>model = tf.keras.Sequential([

tf.keras.layers.Embedding(num_words, 50, input_length=50),

tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),

tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),

tf.keras.layers.Dropout(0.5),

tf.keras.layers.Dense(64, activation=‘relu’),

tf.keras.layers.Dense(num_tags, activation=‘softmax’)
])
</code></pre>
<p><strong>If I use loss function as “categorical_crossentropy” , I get this error:</strong>
ValueError: Shapes (None, 50) and (None, 11) are incompatible</p>
<p><strong>If I use loss function as “sparse_categorical_crossentropy” , I get this error</strong>:
logits and labels must have the same first dimension, got logits shape [13,11] and labels shape [650]
[[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]</p>
<p><strong>I tried adding input shape as first layer but still no luck</strong>
tf.keras.layers.Input(shape=(max_len,))</p>
<p>Can anyone help , how to solve this. Tried  different approaches but no luck</p>
<p><strong>Here is model summary</strong></p>
<pre><code>Layer (type)                Output Shape              Param #   
=================================================================
 embedding_18 (Embedding)    (None, 50, 50)            11100     
                                                                 
 bidirectional_35 (Bidirecti  (None, 50, 128)          58880     
 onal)                                                           
                                                                 
 bidirectional_36 (Bidirecti  (None, 64)               41216     
 onal)                                                           
                                                                 
 dropout_17 (Dropout)        (None, 64)                0         
                                                                 
 dense_35 (Dense)            (None, 64)                4160      
                                                                 
 dense_36 (Dense)            (None, 11)                715       
                                                                 
=================================================================
Total params: 116,071
Trainable params: 116,071
Non-trainable params: 0
_________________________________________________________________
</code></pre>
","keras, nlp, lstm, tensorflow2.0, named-entity-recognition","<p>I think you have a problem in 2 last dense layers. When run on a sequence of 50 numbers, you will get 'num_tags' numbers as output (11).</p>
<p>But you want to get 'num_tags' outputs at each step of the sequence, not at the end. To achieve this, you can use TimeDistributed layer:</p>
<pre><code>tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation=‘relu’)),
tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_tags, activation=‘softmax’))
</code></pre>
<p>Then you can use “sparse_categorical_crossentropy” loss function since your labels are ints.</p>
<p>Please see as example:
<a href=""https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54"" rel=""nofollow noreferrer"">https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54</a></p>
",1,2,89,2023-01-14 17:25:25,https://stackoverflow.com/questions/75119911/lstm-named-entity-recognition-model-shape-are-incompatible-or-logits-labels-ha
OSError: [E053] Could not read meta.json from model-best.zip,"<p>I tried to load the trained spacy model but this error appear:</p>
<pre><code>OSError: [E053] Could not read meta.json from model-best.zip
</code></pre>
<p>this is my code:</p>
<pre><code>nlp_ner = spacy.load(&quot;model-best.zip&quot;)
</code></pre>
","machine-learning, model, spacy, named-entity-recognition","<p>For spaCy 3, you can load models from 3 sources:</p>
<ul>
<li><a href=""https://github.com/explosion/spacy-models/releases"" rel=""nofollow noreferrer"">Pretrained models</a> (downloaded via a command like <code>python -m spacy download YOUR_MODEL</code></li>
<li>Custom models you have trained via <a href=""https://spacy.io/api/cli#train"" rel=""nofollow noreferrer""><code>spacy train</code></a>.</li>
<li>Loaded models via <a href=""https://spacy.io/api/language#from_disk"" rel=""nofollow noreferrer""><code>nlp.from_disk</code></a></li>
</ul>
<p>Usually, <strong>any of these models is stored as a folder or directory</strong>, with an structure similar to this one (for a NER model, which it seems it is what you are attempting to load):</p>
<pre><code>/path/to/your/model/
├── model-best  &lt;== THIS DIRECTORY IS WHAT YOU MIGHT HAVE
│   ├── config.cfg
│   ├── meta.json
│   ├── ner
│   │   ├── cfg
│   │   ├── model
│   │   └── moves
│   ├── tok2vec
│   │   ├── cfg
│   │   └── model
│   ├── tokenizer
│   └── vocab
│       ├── key2row
│       ├── lookups.bin
│       ├── strings.json
│       ├── vectors
│       └── vectors.cfg
└── model-last
    ├── config.cfg
    ├── meta.json
    ├── ner
    │   ├── cfg
    │   ├── model
    │   └── moves
    ├── tok2vec
    │   ├── cfg
    │   └── model
    ├── tokenizer
    └── vocab
        ├── key2row
        ├── lookups.bin
        ├── strings.json
        ├── vectors
        └── vectors.cfg

8 directories, 26 files
</code></pre>
<p>This discards &quot;loading directly from a <code>.zip</code> file&quot; as a valid option.</p>
<p>I think you may want to try the following:</p>
<ol>
<li>Try to unzip <code>model-best.zip</code> and see if you find a similar directory structure than the one shown above. If you are in a Linux-based system, <a href=""https://linuxize.com/post/how-to-unzip-files-in-linux/"" rel=""nofollow noreferrer"">here</a> is how.</li>
<li>If the previous structure is confirmed, then proceed with step 3, otherwise your file may be corrupted, or not a spaCy model as such, and you won't be able to load the model.</li>
<li>Try <code>nlp_ner = spacy.load(&quot;/path/to/your/model-best&quot;)</code> (<code>nlp_ner = spacy.load(&quot;./model-best&quot;)</code> in your case) and see if it works.</li>
</ol>
<p>Hope it helps.</p>
",0,0,2160,2023-01-27 13:44:25,https://stackoverflow.com/questions/75258945/oserror-e053-could-not-read-meta-json-from-model-best-zip
Bert Tokenizer punctuation for named entity recognition task,"<p>I'm working on a named entity recognition task, where I need to identify person names, books etc.</p>
<p>I am using Huggingface Transformer package and BERT with PyTorch. Generally it works very good, however, my issue is that for some first names a dot &quot;.&quot; is a part of the first name and shouldn't be separate it from it. For example, for the person name &quot;Paul Adam&quot;, the first name in the training data is shortened to one letter combined with dot &quot;P. Adam&quot;. The tokenizer tokenize it as [&quot;P&quot;, &quot;.&quot;, &quot;Adam&quot;] which later negatively impact the ner trained model performance as &quot;P.&quot; is presented in the training data and not only &quot;P&quot;. The model is capable to recognize the full names but fails in the shortened one. I used Spacy tokenizer before and I didn't face this issue. Here more details:</p>
<pre><code>from transformers import BertTokenizer, BertConfig, AutoTokenizer, AutoConfig, BertModel
path_pretrained_model='/model/bert/'
tokenizer = BertTokenizer.from_pretrained(path_pretrained_model)

print(tokenizer.tokenize(&quot;P. Adam is a scientist.&quot;))

Output:
['p', '.', 'adam', 'is', 'a', 'scientist', '.']

The helpful output would be 
['p.', 'adam', 'is', 'a', 'scientist', '.']
</code></pre>
","huggingface-transformers, bert-language-model, named-entity-recognition, punctuation","<p>Not sure whether this might be a viable solution for you, but here's a possible hack.</p>
<pre><code>from transformers import BertTokenizer, BertConfig, AutoTokenizer, AutoConfig, BertModel
import string

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_basic_tokenization=True, never_split=[f&quot;{letter}.&quot; for letter in list(string.ascii_lowercase)])

print(tokenizer.tokenize(&quot;P. Adam is a scientist.&quot;))   
# ['p.', 'adam', 'is', 'a', 'scientist', '.']
</code></pre>
<p>Indeed, from the <a href=""https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"" rel=""nofollow noreferrer"">documentation</a></p>
<blockquote>
<p>never_split (Iterable, optional) — Collection of tokens which will never be split during tokenization. Only has an effect when do_basic_tokenize=True</p>
</blockquote>
",0,1,477,2023-02-14 09:03:10,https://stackoverflow.com/questions/75445494/bert-tokenizer-punctuation-for-named-entity-recognition-task
How to normalise keywords extracted with Named Entity Recognition,"<p>I'm trying to employ NER to extract keywords (tags) from job postings. This can be anything along with <code>React, AWS, Team Building, Marketing</code>.</p>
<p>After training a custom model in SpaCy I'm presented with a problem - extracted tags are not unified/normalized across all of the data.</p>
<p>For example, if job posting is about <code>frontend development</code>, NER can extract the keyword <code>frontend</code> in many ways (depending on job description), for example: <code>Frontend</code>, <code>Front End</code>, <code>Front-End</code>, <code>front-end</code> and so on.</p>
<p>Is there a reliable way to normalise/unify the extracted keywords? All the keywords go directly into the database and, with all the variants of each keyword, I would end up with too much noise.</p>
<p>One way to tackle the problem would be to create mappings such as:</p>
<pre><code>&quot;Frontend&quot;: [&quot;Front End&quot;, &quot;Front-End&quot;, &quot;front-end&quot;]
</code></pre>
<p>but that approach seems not too bright. Perhaps within SpaCy itself there's an option to normalise tags?</p>
","python, nlp, spacy, named-entity-recognition","<p>Certainly these simple rules can quickly help you to collapse similar <code>s</code> strings:</p>
<ul>
<li><code>s.lower()</code></li>
<li><code>s.replace(&quot;-&quot;, &quot; &quot;)</code></li>
<li><code>s.replace(&quot; &quot;, &quot;&quot;)</code></li>
</ul>
<p>There are several
<a href=""https://en.wikipedia.org/wiki/Phonetic_algorithm"" rel=""nofollow noreferrer"">phonetic algorithms</a>
such as
<a href=""https://en.wikipedia.org/wiki/Metaphone"" rel=""nofollow noreferrer"">Metaphone</a>,
that are good at collapsing &quot;sounds alike&quot; variants
into a single base entity.</p>
<p>A frequent bi-gram analysis may help you to identify
common two-word phrases that denote a single entity.</p>
<p>Spacy's <code>token.lemma_</code> and <code>token.text</code> can help with stemming.</p>
<p>Learning that e.g. &quot;React&quot; and &quot;Frontend&quot; are more or less synonyms
in this context would require a heavier weight approach, such as word2vec,
<a href=""https://wordnet.princeton.edu"" rel=""nofollow noreferrer"">WordNet</a>,
or a LLM like ChatGPT.</p>
",3,2,779,2023-03-06 18:32:14,https://stackoverflow.com/questions/75654562/how-to-normalise-keywords-extracted-with-named-entity-recognition
Creating HuggingFace Dataset to train an BIO tagger,"<p>I have a list of dictionaries:</p>
<pre><code>sentences = [ 
{'text': ['I live in Madrid'], 'labels':[O, O, O, B-LOC]},
{'text': ['Peter lives in Spain'], 'labels':[B-PER, O, O, B-LOC]},
{'text': ['He likes pasta'], 'labels':[O, O, B-FOOD]},
...
]
</code></pre>
<p>I want to create a HuggingFace dataset object from this data so that I can later preprocess it and feed to a transformer model much more easily, but so far I have not found a viable way to do this.</p>
","python, nlp, huggingface-transformers, named-entity-recognition, huggingface-datasets","<p>First you'll need some extra libraries to use the metrics and datasets features.</p>
<pre><code>pip install -U transformers datasets evaluate seqeval
</code></pre>
<h3>To convert list of dict to Dataset object</h3>
<pre><code>import pandas as pd
from datasets import Dataset

sentences = [ 
{'text': 'I live in Madrid', 'labels':['O', 'O', 'O', 'B-LOC']},
{'text': 'Peter lives in Spain', 'labels':['B-PER', 'O', 'O', 'B-LOC']},
{'text': 'He likes pasta', 'labels':['O', 'O', 'B-FOOD']},
]


ds = Dataset.from_pandas(pd.DataFrame(data=sentences))

</code></pre>
<h3>Convert the dataset into a &quot;Trainer-able&quot; Dataset object</h3>
<pre><code>from datasets import Dataset
from datasets import ClassLabel

# Define a Classlabel object to use to map string labels to integers.
classmap = ClassLabel(num_classes=4, names=['B-LOC', 'B-PER', 'B-FOOD', 'O'])


train_sentences = [ 
{'text': 'I live in Madrid', 'labels':['O', 'O', 'O', 'B-LOC']},
{'text': 'Peter lives in Spain', 'labels':['B-PER', 'O', 'O', 'B-LOC']},
{'text': 'He likes pasta', 'labels':['O', 'O', 'B-FOOD']},
]

# Map text to tokenizer ids.
ds = ds.map(lambda x: tokenizer(x[&quot;text&quot;], truncation=True))

# Map labels to label ids.
ds = ds.map(lambda y: {&quot;labels&quot;: classmap.str2int(y[&quot;labels&quot;])})

</code></pre>
<h3>To compute metrics with the labeled inputs that you have:</h3>
<pre><code>import evaluate

metric = evaluate.load(&quot;seqeval&quot;)


def compute_metrics(p):
    predictions, labels = p
    predictions = predictions.argmax(axis=2)
    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        &quot;precision&quot;: results[&quot;overall_precision&quot;],
        &quot;recall&quot;: results[&quot;overall_recall&quot;],
        &quot;f1&quot;: results[&quot;overall_f1&quot;],
        &quot;accuracy&quot;: results[&quot;overall_accuracy&quot;],
    }

</code></pre>
<h3>To use with the <code>Trainer</code> object</h3>
<pre><code>import pandas as pd
import evaluate

from datasets import Dataset
from datasets import ClassLabel

from transformers import AutoModelForTokenClassification, Trainer, AutoTokenizer, DataCollatorForTokenClassification

# Define a Classlabel object to use to map string labels to integers.
classmap = ClassLabel(num_classes=4, names=['B-LOC', 'B-PER', 'B-FOOD', 'O'])

train_sentences = [ 
{'text': 'I live in Madrid', 'labels':['O', 'O', 'O', 'B-LOC']},
{'text': 'Peter lives in Spain', 'labels':['B-PER', 'O', 'O', 'B-LOC']},
{'text': 'He likes pasta', 'labels':['O', 'O', 'B-FOOD']},
]

eval_sentences = [
    {&quot;text&quot;: &quot;I like pasta from Madrid , Spain&quot;, 'labels': ['O', 'O', 'B-FOOD', 'O', 'B-LOC', 'O', 'B-LOC']}
]

ds_train = Dataset.from_pandas(pd.DataFrame(data=train_sentences))
ds_eval = Dataset.from_pandas(pd.DataFrame(data=eval_sentences))

model = AutoModelForTokenClassification.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;,
                                                        id2label={i:classmap.int2str(i) for i in range(classmap.num_classes)},
                                                        label2id={c:classmap.str2int(c) for c in classmap.names},
                                                        finetuning_task=&quot;ner&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;)
data_collator = DataCollatorForTokenClassification(tokenizer)


ds_train = ds_train.map(lambda x: tokenizer(x[&quot;text&quot;], truncation=True))
ds_eval = ds_eval.map(lambda x: tokenizer(x[&quot;text&quot;], truncation=True))

ds_train = ds_train.map(lambda y: {&quot;labels&quot;: classmap.str2int(y[&quot;labels&quot;])})
ds_eval = ds_eval.map(lambda y: {&quot;labels&quot;: classmap.str2int(y[&quot;labels&quot;])})


metric = evaluate.load(&quot;seqeval&quot;)


def compute_metrics(p):
    predictions, labels = p
    predictions = predictions.argmax(axis=2)
    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        &quot;precision&quot;: results[&quot;overall_precision&quot;],
        &quot;recall&quot;: results[&quot;overall_recall&quot;],
        &quot;f1&quot;: results[&quot;overall_f1&quot;],
        &quot;accuracy&quot;: results[&quot;overall_accuracy&quot;],
    }

# Initialize our Trainer
trainer = Trainer(
    model=model,
    train_dataset=ds_train,
    eval_dataset=ds_eval,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)


trainer.train()
</code></pre>
",8,5,3303,2023-03-08 15:06:37,https://stackoverflow.com/questions/75674773/creating-huggingface-dataset-to-train-an-bio-tagger
Getting the input text from transformers pipeline,"<p>I am following the tutorial on <a href=""https://huggingface.co/docs/transformers/pipeline_tutorial"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pipeline_tutorial</a> to use transformers pipeline for inference. For example, the following code snippet works for getting the NER results from ner pipeline.</p>
<pre><code>    # KeyDataset is a util that will just output the item we're interested in.
    from transformers.pipelines.pt_utils import KeyDataset
    from datasets import load_dataset
    model = ...
    tokenizer = ...
    pipe = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
    dataset = load_dataset(&quot;my_ner_dataset&quot;, split=&quot;test&quot;)
    
    for extracted_entities in pipe(KeyDataset(dataset, &quot;text&quot;)):
        print(extracted_entities)
</code></pre>
<p>In NER, as well as many applications, we would like to also get the input so that I can store the result as (text, extracted_entities) pair for later processing. Basically I am looking for something like:</p>
<pre><code>    # KeyDataset is a util that will just output the item we're interested in.
    from transformers.pipelines.pt_utils import KeyDataset
    from datasets import load_dataset
    model = ...
    tokenizer = ...
    pipe = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
    dataset = load_dataset(&quot;my_ner_dataset&quot;, split=&quot;test&quot;)
    
    for text, extracted_entities in pipe(KeyDataset(dataset, &quot;text&quot;)):
        print(text, extracted_entities)
</code></pre>
<p>Where <code>text</code> is the raw input text (possibly batched) that get fed into the pipeline.</p>
<p>Is this doable ?</p>
","nlp, pipeline, huggingface-transformers, bert-language-model, named-entity-recognition","<h2>Solution</h2>
<pre class=""lang-py prettyprint-override""><code># Datasets 2.11.0
from datasets import load_dataset
# Transformers 4.27.4, Torch 2.0.0+cu118, 
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    pipeline
)
from transformers.pipelines.pt_utils import KeyDataset

model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)

pipe = pipeline(task=&quot;ner&quot;, model=model, tokenizer=tokenizer)
dataset = load_dataset(&quot;argilla/gutenberg_spacy-ner&quot;, split=&quot;train&quot;)
results = pipe(KeyDataset(dataset, &quot;text&quot;))

for idx, extracted_entities in enumerate(results):
    print(&quot;Original text:\n{}&quot;.format(dataset[idx][&quot;text&quot;]))
    print(&quot;Extracted entities:&quot;)
    for entity in extracted_entities:
        print(entity)
</code></pre>
<h3>Example output</h3>
<pre class=""lang-none prettyprint-override""><code>Original text:
Would I wish to send up my name now ? Again I declined , to the polite astonishment of the concierge , who evidently considered me a queer sort of a friend . He was called to his desk by a guest , who wished to ask questions , of course , and I waited where I was . At a quarter to eleven Herbert Bayliss emerged from the elevator . His appearance almost shocked me . Out late the night before ! He looked as if he had been out all night for many nights .
Extracted entities:
{'entity': 'B-PER', 'score': 0.9996532, 'index': 68, 'word': 'Herbert', 'start': 289, 'end': 296}
{'entity': 'I-PER', 'score': 0.9996567, 'index': 69, 'word': 'Bay', 'start': 297, 'end': 300}
{'entity': 'I-PER', 'score': 0.9991698, 'index': 70, 'word': '##lis', 'start': 300, 'end': 303}
{'entity': 'I-PER', 'score': 0.96547437, 'index': 71, 'word': '##s', 'start': 303, 'end': 304}

...

Original text:
And you think our run will be better than five hundred and eighty ? '' `` It should be , unless there is a remarkable change . This ship makes over six hundred , day after day , in good weather . She should do at least six hundred by to-morrow noon , unless there is a sudden change , as I said . '' `` But six hundred would be -- it would be the high field , by Jove ! '' `` Anything over five hundred and ninety-four would be that . The numbers are very low to-night .
Extracted entities:
{'entity': 'B-MISC', 'score': 0.40225995, 'index': 90, 'word': 'Jo', 'start': 363, 'end': 365}
</code></pre>
<h2>Brief Explanation</h2>
<p>Each sample in the dataset created by the <code>load_dataset</code> call can be accessed using an index and the associated dictionary key.</p>
<p>Calls to the <code>pipeline</code> object with a <code>KeyDataset</code> as input returns <code>PipelineIterator</code> object that is iterable. Hence, one can <code>enumerate</code> the PipelineIterator object to get both the result and the index for the particular result, and then use that index to retrieve the associated sample in the dataset.</p>
<h2>Detailed Explanation</h2>
<p>The Huggingface <a href=""https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline"" rel=""nofollow noreferrer"">pipeline</a> abstraction is a wrapper for all available pipelines. When one instantiates a <code>pipeline</code> object it will <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/__init__.py#L523"" rel=""nofollow noreferrer"">return the appropriate pipeline</a> based on the <code>task</code> argument:</p>
<pre class=""lang-py prettyprint-override""><code>pipe = pipeline(task=&quot;ner&quot;, model=model, tokenizer=tokenizer)
</code></pre>
<p>Given that the NER task is specified, a <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/__init__.py#L523"" rel=""nofollow noreferrer"">TokenClassificationPipeline</a> will be returned (side note: &quot;ner&quot; is an alias for &quot;token-classification&quot;). This pipeline (and all others) inherits the base class <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/__init__.py#L523"" rel=""nofollow noreferrer"">Pipeline</a>. The <code>Pipeline</code> base class defines the <code>__call__</code> function which the <code>TokenClassificationPipeline</code> class <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/token_classification.py#L214"" rel=""nofollow noreferrer"">relies on</a> whenever the instantiated <code>pipeline</code> is called.</p>
<p>Once a pipeline is instantiated (see above), it is called with data passed in as either a single string, a list, or when working with full datasets, a <a href=""https://pypi.org/project/datasets/"" rel=""nofollow noreferrer"">Huggingface dataset</a> via the transformers.pipelines.pt_utils <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/pt_utils.py#L296"" rel=""nofollow noreferrer"">KeyDataset</a> class.</p>
<pre class=""lang-py prettyprint-override""><code>dataset = load_dataset(&quot;argilla/gutenberg_spacy-ner&quot;, split=&quot;train&quot;)
results = pipe(KeyDataset(dataset, &quot;text&quot;))  # pipeline call
</code></pre>
<p>When the pipeline is called, it <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/base.py#L1080"" rel=""nofollow noreferrer"">checks</a> whether the data passed in is iterable, and then calls an appropriate function. For Huggingface <code>Dataset</code> objects, the <code>get_iterator</code> function <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/base.py#L1087"" rel=""nofollow noreferrer"">is called</a> which returns a <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/pt_utils.py#L23"" rel=""nofollow noreferrer"">PipelineIterator</a> object. Given the <a href=""https://docs.python.org/3/glossary.html#term-iterator"" rel=""nofollow noreferrer"">known behaviour of iterator objects</a>, one can <a href=""https://docs.python.org/3/library/functions.html#enumerate"" rel=""nofollow noreferrer"">enumerate</a> the object to return a tuple containing a count (from start which defaults to 0) and the values obtained from iterating over iterable. The values are the NER extractions for each sample in the dataset. Hence, the following produces the desired results:</p>
<pre class=""lang-py prettyprint-override""><code>for idx, extracted_entities in enumerate(results):
    print(&quot;Original text:\n{}&quot;.format(dataset[idx][&quot;text&quot;]))
    print(&quot;Extracted entities:&quot;)
    for entity in extracted_entities:
        print(entity)
</code></pre>
",3,2,5374,2023-04-04 18:18:04,https://stackoverflow.com/questions/75932605/getting-the-input-text-from-transformers-pipeline
Unable to call Language Studio Custom Named Entity Recognition Endpoint,"<p>I have trained and deployed a <code>Custom Named Entity Recognition</code> model in <code>Language Studio</code>. The model is successfully deployed and I can test it from Language Studio UI, I can see the detected entities. But when I try to access the endpoint from either Postman or Python I get the message <code>&lt;Response [202]&gt;</code>, below are the configuration I am using for accessing the endpoint from Python or Postman.</p>
<p><strong>Code</strong></p>
<pre><code>import json
import requests 

url = &quot;https://&lt;language_service&gt;.cognitiveservices.azure.com/language/analyze-text/jobs?api-version=2022-10-01-preview&quot;

payload = json.dumps({
  &quot;tasks&quot;: [
    {
      &quot;kind&quot;: &quot;CustomEntityRecognition&quot;,
      &quot;parameters&quot;: {
        &quot;projectName&quot;: &quot;&lt;project_name&gt;&quot;,
        &quot;deploymentName&quot;: &quot;&lt;deployment_name&gt;&quot;,
        &quot;stringIndexType&quot;: &quot;TextElement_v8&quot;
      }
    }
  ],
  &quot;displayName&quot;: &quot;CustomTextPortal_CustomEntityRecognition&quot;,
  &quot;analysisInput&quot;: {
    &quot;documents&quot;: [
      {
        &quot;id&quot;: &quot;1&quot;,
        &quot;text&quot;: &quot;&lt;text&gt;&quot;,
        &quot;language&quot;: &quot;en-us&quot;
      },
      {
        &quot;id&quot;: &quot;2&quot;,
        &quot;text&quot;: &quot;&lt;text&gt;&quot;,
        &quot;language&quot;: &quot;en-us&quot;
      }
    ]
  }
})
headers = {
  'Content-Type': 'application/json',
  'Ocp-Apim-Subscription-Key': '&lt;key&gt;'
}

response = requests.post(url, headers=headers, data=payload)
print(response)
</code></pre>
<p>Can anyone please tell me what am I missing?</p>
","python, azure, named-entity-recognition, azure-cognitive-services, language-studio","<p>Issue has been resolved, actually I was missing the second part of the code. When the above code runs then we can extract the job id from the response object, then we can make another GET call to the endpoint as follows to get the results.</p>
<pre><code>job_id = response.headers[&quot;Operation-Location&quot;].split(&quot;/&quot;)[-1]
response = requests.get(url+'/jobs/'+job_id, None, headers=header)
dict = json.loads(response.text)
if dict['status'] == 'succeeded':
    entities = dict['tasks']['items'][0]['results']['documents'][0]['entities']
    print(entities)
</code></pre>
",0,0,189,2023-04-09 13:04:27,https://stackoverflow.com/questions/75970673/unable-to-call-language-studio-custom-named-entity-recognition-endpoint
TokenClassificationChunkPipeline is throwing error: &#39;BatchEncoding&#39; object is not an iterator,"<p>Following this <a href=""https://medium.com/@luccailliau/text-anonymization-using-hugging-face-transformers-75b5d7392833"" rel=""nofollow noreferrer"">HuggingFace Anonymisation Tutorial</a>.
Using pytorch 2.0.0 and transformers-4.28.1
Running the code as it is, I get an error over the custom pipeline:</p>
<pre><code>def anonymize(text):
    ents = pipe(text) # this errors out
    ...
TypeError: 'BatchEncoding' object is not an iterator
</code></pre>
<p>I realise it's a tokenizer issue,</p>
<pre><code>class TokenClassificationChunkPipeline(TokenClassificationPipeline):
def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)

def preprocess(self, sentence, offset_mapping=None):
    model_inputs = self.tokenizer(
        sentence,
        return_tensors=&quot;pt&quot;,
        truncation=True,
        return_special_tokens_mask=True,
        return_offsets_mapping=True,
        return_overflowing_tokens=True,  # Return multiple chunks
        max_length=self.tokenizer.model_max_length,
        padding=True
    )
    if offset_mapping:
        model_inputs[&quot;offset_mapping&quot;] = offset_mapping

    model_inputs[&quot;sentence&quot;] = sentence

    return model_inputs
</code></pre>
<p>This model_inputs is a</p>
<blockquote>
<p>&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;</p>
</blockquote>
<p>How can I make an iterator BatchEncoding object?
Else, is there another way?
For full code, please visit the tutorial link above.</p>
","pytorch, nlp, huggingface-transformers, torch, named-entity-recognition","<p>Not sure why the pipeline was coded that way in the <a href=""https://medium.com/@luccailliau/text-anonymization-using-hugging-face-transformers-75b5d7392833"" rel=""nofollow noreferrer"">blogpost</a>, but here's a working version:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers.pipelines.token_classification import TokenClassificationPipeline

model_checkpoint = &quot;Davlan/bert-base-multilingual-cased-ner-hrl&quot;

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)


class TokenClassificationChunkPipeline(TokenClassificationPipeline):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def preprocess(self, sentence, offset_mapping=None, **preprocess_params):
        tokenizer_params = preprocess_params.pop(&quot;tokenizer_params&quot;, {})
        truncation = True if self.tokenizer.model_max_length and self.tokenizer.model_max_length &gt; 0 else False
        inputs = self.tokenizer(
            sentence,
            return_tensors=&quot;pt&quot;,
            truncation=True,
            return_special_tokens_mask=True,
            return_offsets_mapping=True,
            return_overflowing_tokens=True,  # Return multiple chunks
            max_length=self.tokenizer.model_max_length,
            padding=True
        )
        #inputs.pop(&quot;overflow_to_sample_mapping&quot;, None)
        num_chunks = len(inputs[&quot;input_ids&quot;])

        for i in range(num_chunks):
            if self.framework == &quot;tf&quot;:
                model_inputs = {k: tf.expand_dims(v[i], 0) for k, v in inputs.items()}
            else:
                model_inputs = {k: v[i].unsqueeze(0) for k, v in inputs.items()}
            if offset_mapping is not None:
                model_inputs[&quot;offset_mapping&quot;] = offset_mapping
            model_inputs[&quot;sentence&quot;] = sentence if i == 0 else None
            model_inputs[&quot;is_last&quot;] = i == num_chunks - 1
            yield model_inputs

    def _forward(self, model_inputs):
        # Forward
        special_tokens_mask = model_inputs.pop(&quot;special_tokens_mask&quot;)
        offset_mapping = model_inputs.pop(&quot;offset_mapping&quot;, None)
        sentence = model_inputs.pop(&quot;sentence&quot;)
        is_last = model_inputs.pop(&quot;is_last&quot;)

        overflow_to_sample_mapping = model_inputs.pop(&quot;overflow_to_sample_mapping&quot;)

        output = self.model(**model_inputs)
        logits = output[&quot;logits&quot;] if isinstance(output, dict) else output[0]


        model_outputs = {
            &quot;logits&quot;: logits,
            &quot;special_tokens_mask&quot;: special_tokens_mask,
            &quot;offset_mapping&quot;: offset_mapping,
            &quot;sentence&quot;: sentence,
            &quot;overflow_to_sample_mapping&quot;: overflow_to_sample_mapping,
            &quot;is_last&quot;: is_last,
            **model_inputs,
        }

        # We reshape outputs to fit with the postprocess inputs
        model_outputs[&quot;input_ids&quot;] = torch.reshape(model_outputs[&quot;input_ids&quot;], (1, -1))
        model_outputs[&quot;token_type_ids&quot;] = torch.reshape(model_outputs[&quot;token_type_ids&quot;], (1, -1))
        model_outputs[&quot;attention_mask&quot;] = torch.reshape(model_outputs[&quot;attention_mask&quot;], (1, -1))
        model_outputs[&quot;special_tokens_mask&quot;] = torch.reshape(model_outputs[&quot;special_tokens_mask&quot;], (1, -1))
        model_outputs[&quot;offset_mapping&quot;] = torch.reshape(model_outputs[&quot;offset_mapping&quot;], (1, -1, 2))

        return model_outputs


pipe = TokenClassificationChunkPipeline(model=model, tokenizer=tokenizer, aggregation_strategy=&quot;simple&quot;)

pipe(&quot;Bernard works at BNP Paribas in Paris.&quot;)

</code></pre>
<p>[out]:</p>
<pre><code>[{'entity_group': 'PER',
  'score': 0.9994497,
  'word': 'Bernard',
  'start': 0,
  'end': 7},
 {'entity_group': 'ORG',
  'score': 0.9997708,
  'word': 'BNP Paribas',
  'start': 17,
  'end': 28},
 {'entity_group': 'LOC',
  'score': 0.99906,
  'word': 'Paris',
  'start': 32,
  'end': 37}]
</code></pre>
<p>For reference, take a look at how the <code>preproces()</code> and the <code>_forward()</code> functions are coded in the <code>TokenClassificationPipeline</code> class, <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/token_classification.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/token_classification.py</a></p>
<p>The preprocess should return a generator, that's why the _forward is expecting a generator and complains <code>TypeError: 'BatchEncoding' object is not an iterator</code>.</p>
",2,1,1124,2023-04-19 15:17:27,https://stackoverflow.com/questions/76056193/tokenclassificationchunkpipeline-is-throwing-error-batchencoding-object-is-no
How to resolve Error in seqeval in NER bert finetuning?,"<p>I'm trying to finetune a NER model, (BERT/BioBERT) and after first epoch of training, in Evaluation part, I got the following error, Any idea what is wrong?</p>
<pre><code>ValueError: Predictions and/or references don't match the expected format.
Expected format: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 
'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')},
Input predictions: [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], ..., [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],
Input references: [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], ..., [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]
</code></pre>
<p>I am using very standard eval function and if I remove the evaluation from trainer, The model trains without any problem and the results are good, but I have little to no metrics.</p>
<pre class=""lang-py prettyprint-override""><code>def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        &quot;precision&quot;: results[&quot;overall_precision&quot;],
        &quot;recall&quot;: results[&quot;overall_recall&quot;],
        &quot;f1&quot;: results[&quot;overall_f1&quot;],
        &quot;accuracy&quot;: results[&quot;overall_accuracy&quot;],
    }
</code></pre>
<p>Thanks.</p>
","nlp, huggingface-transformers, named-entity-recognition, evaluation, huggingface-evaluate","<p>As indicated by the error message, the expected <code>predictions</code> &amp; <code>references</code> should be lists of strings not integers. For <code>seqeval</code>, this makes sense, since the <code>seqeval</code> metric is concerned with matching entity spans exactly (as indicated by the <code>B-</code> &amp; <code>I-</code> prefixes of the tags).</p>
<p>So your <code>label_list</code> should map label identifiers to label tags, such as <code>[&quot;O&quot;, &quot;B-PER&quot;, &quot;I-PER&quot;, &quot;B-ORG&quot;, &quot;I-ORG&quot;, &quot;B-LOC&quot;, &quot;I-LOC&quot;]</code>.</p>
",4,2,934,2023-05-05 20:44:15,https://stackoverflow.com/questions/76185813/how-to-resolve-error-in-seqeval-in-ner-bert-finetuning
ent.sent.text in spacy returns labels instead of the sentence for NER problem,"<p>I'm trying to solve a Name Entity Recognision(NER) Problem using SpaCy of the PDF files. I want to get the modal verbs(will, shall, should, must, etc..) from the pdf files.</p>
<p>I trained the data in spaCy. When predicting using the trained modal, the <code>ent.sent.text</code> attribute of the modal usualy returns the text or can say the sentence from which the label extracted. But in my case it returns the label itself not the sentence. Anyone help me please.</p>
<p>The codes are giving below:</p>
<h1>Code for data preparation</h1>
<pre><code>def load_training_data_from_csv(file_path):
    nlp = spacy.load('en_core_web_md')
    train_data = []
    with open(file_path, 'r', encoding='cp1252') as f:
        reader = csv.DictReader(f)
        for row in reader:
            sentence = row['text']
            start, end = int(row['start']), int(row['end'])
            label = row['label']
            train_data.append((sentence, {&quot;entities&quot;: [(start, end, label)]}))
            # Check the alignment
            from spacy.training import offsets_to_biluo_tags
            doc = nlp.make_doc(sentence)
            tags = offsets_to_biluo_tags(doc, [(start, end, label)])
            if '-' in tags:
                print(f&quot;Warning: Misaligned entities in '{sentence}' with entities {[(start, end, label)]}&quot;)
    return train_data
</code></pre>
<h1>Training the model</h1>
<pre><code>def train_spacy_ner(train_data, n_iter=10):
    # Load the existing model
    nlp = spacy.load('en_core_web_md')

    # Add the NER pipeline if it doesn't exist
    if &quot;ner&quot; not in nlp.pipe_names:
        ner = nlp.create_pipe(&quot;ner&quot;)
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe(&quot;ner&quot;)


    # Add the new label &quot;CURRENCY&quot; to the NER model
    ner.add_label(&quot;WILL&quot;)
    ner.add_label(&quot;SHALL&quot;)
    ner.add_label(&quot;MUST&quot;)


    # Train the NER model
    optimizer = nlp.begin_training()
    for i in range(n_iter):
        print(&quot;Epoch - &quot;, i) if i % 2 == 0 or i == n_iter else None
        random.shuffle(train_data)
        losses = {}
        for text, annotations in train_data:
            doc = nlp.make_doc(text)
            example = spacy.training.Example.from_dict(doc, annotations)
            nlp.update([example], sgd=optimizer, losses=losses)
        print(&quot;loss : &quot;, losses) if i % 2 == 0 or i == n_iter else None

    return nlp
</code></pre>
<h1>Calling the functions</h1>
<pre><code># nlp = spacy.load(&quot;en_core_web_md&quot;)
file_path = &quot;/content/trainData.csv&quot;
TRAIN_DATA = load_training_data_from_csv(file_path)

# Train the model
nlp = train_spacy_ner(TRAIN_DATA)
nlp.to_disk('custom_NER')
</code></pre>
<h1>Predicting using model (here is the problem starting)</h1>
<pre><code>import spacy

nlp = spacy.load('custom_NER')
text = &quot;The language will be in english&quot;

doc = nlp(text)
# print(doc.ents)
for ent in doc.ents:
  print(ent.sent.text, ent.start_char, ent.end_char, ent.label_)
</code></pre>
<p><code>ent.sent.text</code> should return the sentence used above. But here the label itself is returing.</p>
<h1>Output getting</h1>
<pre><code>will 13 17 WILL
</code></pre>
<h1>Expecting output</h1>
<pre><code>The language will be in english 13 17 WILL
</code></pre>
<h1>Train Data</h1>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Text</th>
<th>start</th>
<th>end</th>
<th>label</th>
</tr>
</thead>
<tbody>
<tr>
<td>I will do the procedures</td>
<td>2</td>
<td>6</td>
<td>will</td>
</tr>
<tr>
<td>You should send the letters</td>
<td>4</td>
<td>10</td>
<td>should</td>
</tr>
</tbody>
</table>
</div>","python, machine-learning, nlp, spacy, named-entity-recognition","<p>The reason is that calling the below code. So remove it from the train function</p>
<pre><code>#optimizer = nlp.begin_training()
</code></pre>
<p>which will also reinitialize all models. As a result, the parser (which performs the sentence splitting), will predict the sentence boundaries using a zeroed-out softmax layer and will start detecting a boundary after every token.</p>
<p>So, should remove the line that calls <code>begin_training</code>. Then later when you update the pipe, you can remove the <code>sgd</code> parameter and the pipe will create an optimizer internally:</p>
<pre><code>nlp.update([example], losses=losses)
</code></pre>
",0,0,259,2023-05-16 15:20:57,https://stackoverflow.com/questions/76264711/ent-sent-text-in-spacy-returns-labels-instead-of-the-sentence-for-ner-problem
Create an unknown label for spaCy when returning list of text and label,"<p>I'm trying to create a condition statement for a function that will return the text and label for a passed list.
Here's the code:</p>
<pre><code>def get_label(text: list):
    doc = nlp('. '.join(text) + '.')
    keywords = []
    for ent in doc.ents:
        keywords.append((ent.text, ent.label_))
    return keywords
</code></pre>
<p>The input is:</p>
<pre><code>['Kaggle', 'Google', 'San Francisco', 'this week', 'as early as tomorrow', 'Kag-ingle', 'about half a million', 'Ben Hamner', '2010', 'Earlier this month', 'YouTube', 'Google Cloud Platform', 'Crunchbase', '$12.5 to $13 million', 'Index Ventures', 'SV Angel', 'Hal Varian', 'Khosla Ventures', 'Yuri Milner']
</code></pre>
<p>The output is:</p>
<pre><code>[('Google', 'ORG'), ('San Francisco', 'GPE'), ('this week', 'DATE'), ('as early as tomorrow', 'DATE'), ('Kag-ingle', 'PERSON'), ('about half a million', 'CARDINAL'), ('Ben Hamner', 'PERSON'), ('2010', 'DATE'), ('Earlier this month', 'DATE'), ('Google Cloud Platform', 'ORG'), ('Crunchbase', 'ORG'), ('$12.5 to $13 million', 'MONEY'), ('Index Ventures', 'ORG'), ('Hal Varian', 'PERSON'), ('Khosla Ventures', 'ORG'), ('Yuri Milner', 'PERSON')]
</code></pre>
<p>However, the output should include the entities that were not labelled, assigning them the &quot;UNKNOWN&quot; label like this:</p>
<pre><code>[('Kaggle', 'UNKNOWN'), ('Google', 'ORG'), ('San Francisco', 'GPE'), ('this week', 'DATE'), ('as early as tomorrow', 'DATE'), ('Kag-ingle', 'PERSON'), ('about half a million', 'CARDINAL'), ('Ben Hamner', 'PERSON'), ('2010', 'DATE'), ('Earlier this month', 'DATE'), ('YouTube', 'UNKNOWN'), ('Google Cloud Platform', 'ORG'), ('Crunchbase', 'ORG'), ('$12.5 to $13 million', 'MONEY'), ('Index Ventures', 'ORG'), ('Hal Varian', 'PERSON'), ('Khosla Ventures', 'ORG'), ('Yuri Milner', 'PERSON')]
</code></pre>
<p>I've tried using:</p>
<pre><code>for token in doc.sents:
       keywords.append((token.text, token.label_))
</code></pre>
<p>Which returns:</p>
<pre><code>[('Kaggle.', ''), ('Google.', ''), ('San Francisco.', ''), ('this week.', ''), ('as early as tomorrow.', ''), ('Kag-ingle.', ''), ('about half a million.', ''), ('Ben Hamner. 2010.', ''), ('Earlier this month.', ''), ('YouTube.', ''), ('Google Cloud Platform.', ''), ('Crunchbase.', ''), ('$12.5 to $13 million.', ''), ('Index Ventures.', ''), ('SV Angel.', ''), ('Hal Varian.', ''), ('Khosla Ventures.', ''), ('Yuri Milner.', '')]
</code></pre>
<p>This is (assuming) because there is a period at the end of each token preventing any label from returning.</p>
<p>If anyone has an idea of how I can fix this, I'd really appreciate the help.</p>
","python, nlp, spacy, named-entity-recognition","<p>Iterate over the items passed in and check whether they match one of the returned entities after spaCy has performed the labelling (see solution below).</p>
<p>Notes:</p>
<ul>
<li>The output labels vary depending on the spaCy version and pipeline/pipeline version being used. I used spaCy 3.5.3 and the <code>en_core_web_trf==3.5.0</code> pipeline to produce the following results.</li>
<li>spaCy returned &quot;Bill Hamner&quot; as &quot;Bill Hamner.&quot; as the labelled entity, hence the extra condition in the <code>if</code> statement to check for these edge cases.</li>
</ul>
<h3>Solution</h3>
<pre class=""lang-py prettyprint-override""><code>import spacy

txt = ['Kaggle', 'Google', 'San Francisco', 'this week', 'as early as tomorrow', 'Kag-ingle', 'about half a million', 'Ben Hamner', '2010', 'Earlier this month', 'YouTube', 'Google Cloud Platform', 'Crunchbase', '$12.5 to $13 million', 'Index Ventures', 'SV Angel', 'Hal Varian', 'Khosla Ventures', 'Yuri Milner']

nlp = spacy.load(&quot;en_core_web_trf&quot;)


def get_label(text: list):
    doc = nlp(&quot;. &quot;.join(text) + &quot;.&quot;)
    keywords = []
    for item in text:
        found_label = False
        for ent in doc.ents:
            if item == ent.text or (ent.text[-1] == &quot;.&quot; and item == ent.text[:-1]):
                found_label = True
                keywords.append((item, ent.label_))
                break
        if not found_label:
            keywords.append((item, &quot;UNKNOWN&quot;))
    return keywords


for kw in get_label(txt):
    print(kw)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>('Kaggle', 'UNKNOWN')
('Google', 'ORG')
('San Francisco', 'GPE')
('this week', 'DATE')
('as early as tomorrow', 'DATE')
('Kag-ingle', 'UNKNOWN')
('about half a million', 'CARDINAL')
('Ben Hamner', 'PERSON')
('2010', 'DATE')
('Earlier this month', 'DATE')
('YouTube', 'ORG')
('Google Cloud Platform', 'UNKNOWN')
('Crunchbase', 'ORG')
('$12.5 to $13 million', 'MONEY')
('Index Ventures', 'ORG')
('SV Angel', 'UNKNOWN')
('Hal Varian', 'PERSON')
('Khosla Ventures', 'ORG')
('Yuri Milner', 'PERSON')
</code></pre>
<p>Some <a href=""https://en.wikipedia.org/wiki/Program_optimization#When_to_optimize"" rel=""nofollow noreferrer"">premature optimization</a> for the <code>get_label</code> function which may be faster if dealing with very large documents returned by the spaCy pipline (i.e. a very large tuple of labelled entities for <code>doc.ents</code>). I'll leave it up to you to time the difference to see if its worth using this variation in your end-application:</p>
<pre class=""lang-py prettyprint-override""><code>def get_label(text: list):
    doc = nlp(&quot;. &quot;.join(text) + &quot;.&quot;)
    ents = list(doc.ents)
    keywords = []
    for item in text:
        found_label = False
        for idx, ent in enumerate(ents):
            if item == ent.text or (ent.text[-1] == &quot;.&quot; and item == ent.text[:-1]):
                found_label = True
                keywords.append((item, ent.label_))
                ents.pop(idx)  # reduce size of list to make subsequent searches faster
                break
        if not found_label:
            keywords.append((item, &quot;UNKNOWN&quot;))
    return keywords
</code></pre>
",1,1,236,2023-05-22 17:23:38,https://stackoverflow.com/questions/76308600/create-an-unknown-label-for-spacy-when-returning-list-of-text-and-label
Add a new element surrounding a given word in the texts of a given element and its tail using lxml,"<p>So I have a relatively complex XML encoding where the text can contain an open number of elements. Let's take this simplified example:</p>
<pre><code>&lt;div&gt;
&lt;p&gt;-I like James &lt;stage&gt;&lt;hi&gt;he said to her &lt;/hi&gt;&lt;/stage&gt;, but I am not sure James understands &lt;hi&gt;Peter&lt;/hi&gt;'s problems.&lt;/p&gt;
&lt;/div&gt;
</code></pre>
<p>I want to enclose all named entities in the sentence (the two instances of James and Peter) with an <code>rs</code> element:</p>
<pre><code>&lt;div&gt;
&lt;p&gt;-I like &lt;rs&gt;James&lt;/rs&gt; &lt;stage&gt;&lt;hi&gt;he said to her &lt;/hi&gt;&lt;/stage&gt;, but I am not sure &lt;rs&gt;James&lt;/rs&gt; understands &lt;hi&gt;&lt;rs&gt;Peter&lt;/rs&gt;&lt;/hi&gt;'s problems.&lt;/p&gt;
&lt;/div&gt;
</code></pre>
<p>To simplify this, let's say I have a list of names I could find in the text, such as:</p>
<pre><code>names = [&quot;James&quot;, &quot;Peter&quot;, &quot;Mary&quot;]
</code></pre>
<p>I want to use lxml for this. I know I could use the <code>etree.SubElement()</code> and append a new element at the end of the <code>p</code> element, but I don't know how to deal with the tails and the other possible elements.</p>
<p>I understand that I need to handle the three references in my example differently.</p>
<ol>
<li>The first <code>James</code> is in the text of the <code>p</code> element. I could just do this:</li>
</ol>
<pre><code>p = etree.SubElement(div, &quot;p&quot;)
p.text = &quot;-I like &lt;rs&gt;James&lt;/rs&gt;&quot;
</code></pre>
<p>Right?</p>
<ol start=""2"">
<li>The second <code>James</code> is in the tail of the <code>p</code> element. I don't know how to deal with that.</li>
<li>The reference to <code>Peter</code> is in the text of <code>hi</code> element. I guess I have to iterate through all possible elements, look both at the text and at the tail of each element and look for the named entities of my list.</li>
</ol>
<pre><code>rs = etree.SubElement(hi, &quot;rs&quot;)
rs.text = &quot;&lt;rs&gt;Peter&lt;/rs&gt;&quot;
</code></pre>
<p>My guess is that there is a much better way to handle all of this. Any help? Thanks in advance!</p>
","xml, lxml, named-entity-recognition, tei","<p>It's a little convoluted, but can be done.</p>
<p>Let's say your XML looks like this:</p>
<pre><code>play = '''&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;root&gt;
   &lt;div&gt;
      &lt;p&gt;
         -I like James
         &lt;stage&gt;
            &lt;hi&gt;he said to her&lt;/hi&gt;
         &lt;/stage&gt;
         , but I am not sure James understands
         &lt;hi&gt;Peter&lt;/hi&gt;
         's problems.
      &lt;/p&gt;
   &lt;/div&gt;
   &lt;div&gt;
      &lt;p&gt;
         -I like Mary
         &lt;stage&gt;
            &lt;hi&gt;he said to her&lt;/hi&gt;
         &lt;/stage&gt;
         , but I am not sure Peter understands
         &lt;hi&gt;James&lt;/hi&gt;
         's problems.
      &lt;/p&gt;
   &lt;/div&gt;
&lt;/root&gt;
'''
</code></pre>
<p>I inserted another div, and added formatting for clarity. Note that this  assumes that each <code>&lt;div&gt;</code> contains only one <code>&lt;p&gt;</code>; if that's not the case, it will have to be refined more.</p>
<pre><code>doc = etree.XML(play.encode())
names = [&quot;James&quot;, &quot;Peter&quot;, &quot;Mary&quot;]

#find all the divs that need changing
destinations = doc.xpath('//div')

#extract the string representation of the current &lt;p&gt; (the &quot;target&quot;)
for destination in destinations:
    target = destination.xpath('./p')[0]
    target_str = etree.tostring(target).decode()

    #replace the names with the required tag:
    for name in names:
        if name in target_str:
            target_str = target_str.replace(name, f'&lt;rs&gt;{name}&lt;/rs&gt;')
    
    #remove the original &lt;p&gt; and replace it with the new one,
    #as an element formed from the new string 
    destination.remove(target)
    destination.insert(0,etree.fromstring(target_str))

print(etree.tostring(doc).decode())
</code></pre>
<p>In this case, the output should be:</p>
<pre><code>&lt;root&gt;
   &lt;div&gt;
      &lt;p&gt;
         -I like &lt;rs&gt;James&lt;/rs&gt;
         &lt;stage&gt;
            &lt;hi&gt;he said to her&lt;/hi&gt;
         &lt;/stage&gt;
         , but I am not sure &lt;rs&gt;James&lt;/rs&gt; understands
         &lt;hi&gt;&lt;rs&gt;Peter&lt;/rs&gt;&lt;/hi&gt;
         's problems.
      &lt;/p&gt;&lt;/div&gt;
   &lt;div&gt;
      &lt;p&gt;
         -I like &lt;rs&gt;Mary&lt;/rs&gt;
         &lt;stage&gt;
            &lt;hi&gt;he said to her&lt;/hi&gt;
         &lt;/stage&gt;
         , but I am not sure &lt;rs&gt;Peter&lt;/rs&gt; understands
         &lt;hi&gt;&lt;rs&gt;James&lt;/rs&gt;&lt;/hi&gt;
         's problems.
      &lt;/p&gt;&lt;/div&gt;
&lt;/root&gt;
</code></pre>
",1,1,38,2023-05-26 12:41:30,https://stackoverflow.com/questions/76340935/add-a-new-element-surrounding-a-given-word-in-the-texts-of-a-given-element-and-i
Bert NER model start and end position None after fine-tuning,"<p>I have fine-tuned a BERT NER model to my dataset. The base model that I am fine-tuning is “dslim/bert-base-NER”. I have been successfully able to train the model using the following script as refrence:
<a href=""https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax</a></p>
<p>The code which does the prediction:</p>
<pre><code>from transformers import pipeline, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('dslim/bert-base-NER', return_offsets_mapping=True, is_split_into_words=True)
model = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')

pipe = pipeline(task=&quot;ner&quot;, model=model.to(&quot;cpu&quot;), tokenizer=tokenizer, grouped_entities=True)
pipe(&quot;this is a Abc Corp. Ltd&quot;)
</code></pre>
<p>The prediction form the base model contained the start and end position of the word in the original text like:</p>
<pre><code>{‘entity_group’: ‘ORG’, ‘score’: 0.9992545247077942, ‘word’: ‘A’, ‘start’: 10, ‘end’: 11}
{‘entity_group’: ‘ORG’, ‘score’: 0.998507097363472, ‘word’: ‘##bc Corp Ltd’, ‘start’: 11, ‘end’: 22}
</code></pre>
<p>While the prediction from the re-trained model is:</p>
<pre><code>{‘entity_group’: ‘ORG’, ‘score’: 0.747031033039093, ‘word’: ‘##7’, ‘start’: None, ‘end’: None},
{‘entity_group’: ‘ORG’, ‘score’: 0.9055356582005819, ‘word’: ‘Games , Inc’, ‘start’: None, ‘end’: None}
</code></pre>
<p>I am passing the <em><strong>position ids</strong></em> to the model during the training process. I looked at the model training parameters but, could not find a way to pass <em><strong>start and end position</strong></em> of the words to model training process. I have the start and end position of the tokenized words.</p>
","nlp, huggingface-transformers, bert-language-model, named-entity-recognition","<p>The pipeline can not return positions when you pass a &quot;slow&quot;-tokenizer. Use a &quot;fast&quot;-tokenizer to get the positions as well:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, BertTokenizer, BertTokenizerFast, BertForTokenClassification

fast_t = BertTokenizerFast.from_pretrained('dslim/bert-base-NER')
slow_t = BertTokenizer.from_pretrained('dslim/bert-base-NER')
model = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')


text= &quot;this is a Abc Corp. Ltd&quot;

slow_p = pipeline(task=&quot;ner&quot;, model=model, tokenizer=slow_t, device=&quot;cpu&quot;, aggregation_strategy=&quot;simple&quot;)
print(slow_p(text))
fast_p = pipeline(task=&quot;ner&quot;, model=model, tokenizer=fast_t, device=&quot;cpu&quot;, aggregation_strategy=&quot;simple&quot;)
print(fast_p(text))
</code></pre>
<p>Output:</p>
<pre><code>[{'entity_group': 'ORG', 'score': 0.9992956, 'word': 'A', 'start': None, 'end': None}, {'entity_group': 'ORG', 'score': 0.97245616, 'word': '##bc Corp . Ltd', 'start': None, 'end': None}]
[{'entity_group': 'ORG', 'score': 0.9992956, 'word': 'A', 'start': 10, 'end': 11}, {'entity_group': 'ORG', 'score': 0.97245616, 'word': '##bc Corp. Ltd', 'start': 11, 'end': 23}]
</code></pre>
",2,1,551,2023-06-02 23:23:53,https://stackoverflow.com/questions/76393971/bert-ner-model-start-and-end-position-none-after-fine-tuning
How to create a Entity Ruler pattern that includes dot and hyphen?,"<p>I am trying to include brazilian CPF as entity on my NER app using spacy. The current code is the follow:</p>
<pre><code>import spacy
from spacy.pipeline import EntityRuler

nlp = spacy.load(&quot;pt_core_news_sm&quot;)

text = &quot;João mora na Bahia, 22/11/1985, seu cpf é 111.222.333-11&quot;
ruler = nlp.add_pipe(&quot;entity_ruler&quot;)
patterns = [
    {&quot;label&quot;: &quot;CPF&quot;, &quot;pattern&quot;: [{&quot;SHAPE&quot;: &quot;ddd.ddd.ddd-dd&quot;}]},
]

ruler.add_patterns(patterns)
doc = nlp(text)

#extract entities
for ent in doc.ents:
    print (ent.text, ent.label_)
</code></pre>
<p>The result was only:</p>
<pre><code>João PER
Bahia LOC
</code></pre>
<p>I tried using regex too:</p>
<pre><code>{&quot;label&quot;: &quot;CPF&quot;, &quot;pattern&quot;: [{&quot;TEXT&quot;: {&quot;REGEX&quot;: r&quot;^\d{3}\.\d{3}\.\d{3}\-\d{2}$&quot;}}]},
</code></pre>
<p>But not worked too</p>
<p>How can I fix that to retrieve CPF?</p>
","python, named-entity-recognition, spacy-3","<p>After looking for token spacings, the brazilian tokenizer split cpf in two parts:</p>
<pre><code>token_spacings = [token.text_with_ws for token in doc]
</code></pre>
<p>Result:</p>
<pre><code>['João ', 'mora ', 'na ', 'Bahia', ', ', '22/11/1985', ', ', 'seu ', 'cpf ', 'é ', '111.222.', '333-11']
</code></pre>
<p>So i think you may try this:</p>
<pre><code>import spacy
from spacy.pipeline import EntityRuler

nlp = spacy.load(&quot;pt_core_news_sm&quot;)

text = &quot;João mora na Bahia, 22/11/1985, seu cpf é 111.222.333-11&quot;
ruler = nlp.add_pipe(&quot;entity_ruler&quot;)
patterns = [
    {&quot;label&quot;: &quot;CPF&quot;, &quot;pattern&quot;: [
            {&quot;SHAPE&quot;: &quot;ddd.ddd.&quot;},
            {&quot;SHAPE&quot;: &quot;ddd-dd&quot;},
    ]},
]

ruler.add_patterns(patterns)
doc = nlp(text)

#extract entities
for ent in doc.ents:
    print (ent.text, ent.label_)
</code></pre>
",1,1,177,2023-06-08 01:41:53,https://stackoverflow.com/questions/76428082/how-to-create-a-entity-ruler-pattern-that-includes-dot-and-hyphen
What loss function does Space use for Named Entity Recognition (NER),"<p>I'm interested in understanding the specific loss function used by the Space library for training models in the context of Named Entity Recognition. Is there a standard loss function recommended by Space for NER tasks? Are there any alternative loss functions recommended for specific NER scenarios? I would also like to know if the loss function is customizable and how it is implemented within the Space library.
.
.
.
.
Thank you for providing such a detailed response. I really appreciate your help!</p>
","spacy, loss-function, named-entity-recognition","<p>The answer to this is more complicated than you might expect, because spaCy uses a transition-based NER model with an imitation learning objective. The best description of the algorithm is this video, especially the structured prediction part: <a href=""https://www.youtube.com/watch?v=sqDHBH9IjRU"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=sqDHBH9IjRU</a></p>
<p>The actual loss function used to decide between the different actions is also a bit tricky. The implementation is here: <a href=""https://github.com/explosion/spaCy/blob/0367f864fe90dfa1dcdd0bfaf8f06dbcd5e97e45/spacy/syntax/_parser_model.pyx#L153"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/0367f864fe90dfa1dcdd0bfaf8f06dbcd5e97e45/spacy/syntax/_parser_model.pyx#L153</a></p>
<p>I'm sure I've described this in other comments but I can't immediately find it. Basically there may be several equally good transitions, and we want the objective function to account for that. The equations for this are described in Section 4 here: <a href=""https://aclanthology.org/P05-1022.pdf"" rel=""nofollow noreferrer"">https://aclanthology.org/P05-1022.pdf</a></p>
",1,1,1169,2023-06-11 18:03:10,https://stackoverflow.com/questions/76451819/what-loss-function-does-space-use-for-named-entity-recognition-ner
AttributeError: &#39;str&#39; object has no attribute &#39;is_context_set&#39;,"<p>I am trying to implement flair to obtain ner tags for a piece of text (doing this in colab).  Encountered this error while trying <code>tagger.predict(text)</code>. What should I do to resolve this?</p>
<p>Here's my code:</p>
<pre><code>from flair.data import Sentence
from flair.models import SequenceTagger

text = &quot;Apple is headquartered in Cupertino, California.&quot;
tagger = SequenceTagger.load(&quot;flair/ner-english&quot;)

tagger.predict(text)
</code></pre>
<p>Here's what I got:</p>
<pre><code>2023-09-01 09:00:29,790 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-17-02be6f83cf90&gt; in &lt;cell line: 8&gt;()
      6 
      7 
----&gt; 8 tagger.predict(text)
      9 print(text)
     10 print('The following NER tags are found:')

1 frames
/usr/local/lib/python3.10/dist-packages/flair/models/sequence_tagger_model.py in predict(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode, force_token_predictions)
    454                 sentences = [sentences]
    455 
--&gt; 456             Sentence.set_context_for_sentences(cast(List[Sentence], sentences))
    457 
    458             # filter empty sentences

/usr/local/lib/python3.10/dist-packages/flair/data.py in set_context_for_sentences(cls, sentences)
   1087         previous_sentence = None
   1088         for sentence in sentences:
-&gt; 1089             if sentence.is_context_set():
   1090                 continue
   1091             sentence._previous_sentence = previous_sentence

AttributeError: 'str' object has no attribute 'is_context_set'
</code></pre>
","python, machine-learning, nlp, named-entity-recognition, flair","<p>Turn your text into a <code>Sentence</code> first so the underlying code has access to the necessary functions and modify the sentence object.</p>
<pre><code>from flair.data import Sentence
from flair.models import SequenceTagger

text = &quot;Apple is headquartered in Cupertino, California.&quot;
sentence = Sentence(text) # &lt;---
tagger = SequenceTagger.load(&quot;flair/ner-english&quot;)

tagger.predict(sentence ) # &lt;---
print(sentence)
...
</code></pre>
",0,0,290,2023-09-01 09:12:12,https://stackoverflow.com/questions/77021885/attributeerror-str-object-has-no-attribute-is-context-set
How to use multiple NER pipes with the same spaCy nlp object?,"<p>I trained a custom NER model for specific entity types (say, DRUGS) that are different from those that come out of the box in the standard spaCy models (ORG, PERSON, etc.). Is it possible to add the NER pipe from this custom model to another model that already contains the standard spaCy NER pipe? I tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

custom_nlp = spacy.load('my_trained_model/model-best/') #trained with the GPU option from the spaCy Quickstart page
doc = custom_nlp('Chantix is a drug')
print(doc.ents) # Prints Chantix as a DRUG, as expected

main_nlp = spacy.load('en_core_web_trf')
doc = custom_nlp('Chantix is a drug')
print(doc.ents) # Prints Chantix as a PRODUCT, as expected

main_nlp.add_pipe('ner', source=custom_nlp, name='custom_ner', before='ner')
print(main_nlp.pipe_names) # Both custom_ner and ner are there in the expected order
doc = main_nlp('Chantix is a drug')
print(doc.ents) # Here hell breaks loose, basically any token becomes an entity
</code></pre>
","spacy, named-entity-recognition","<p>The problem is that your added <code>custom_ner</code> is listening to the <code>transformer</code> component from <code>en_core_web_trf</code> rather than the one from the <code>custom_nlp</code> pipeline, so it's not getting the right input and is producing nonsense.</p>
<p>You need to &quot;replace the listeners&quot; before you add the component to <code>en_core_web_trf</code>:</p>
<pre class=""lang-py prettyprint-override""><code>custom_nlp.replace_listeners(&quot;transformer&quot;, &quot;ner&quot;, [&quot;model.tok2vec&quot;])
main_nlp.add_pipe('ner', source=custom_nlp, name='custom_ner', before='ner')
</code></pre>
<p>Docs: <a href=""https://spacy.io/api/language/#replace_listeners"" rel=""nofollow noreferrer"">https://spacy.io/api/language/#replace_listeners</a></p>
",1,0,235,2023-09-05 08:28:37,https://stackoverflow.com/questions/77042911/how-to-use-multiple-ner-pipes-with-the-same-spacy-nlp-object
"How can I allow certain entities (e.g., names, organizations) in Azures PII Entity Recognition method so that they are not recognized/masked?","<p>I am using <a href=""https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/quickstart?pivots=programming-language-python"" rel=""nofollow noreferrer"">Azures PII Entity Recognition</a> method in Python to recognize PII entities in a list of documents.</p>
<ol>
<li><p>I am wondering if there is a way to pass a list of entities to the method, which will then not be recognized as PII information. These will be, e.g., names/organizations which are not sensitive in the context.</p>
</li>
<li><p>I would like my PII entities to be replaced with the category, rather than masked with a masking character (e.g., &quot;Andrew&quot; will become &quot;&lt;PERSON&gt;&quot; rather than &quot;******&quot;). I have currently solved this problem by adding my own method and looping through the responses. I am wondering, however, if there is a better way.</p>
</li>
</ol>
<p>Here is an example:</p>
<pre><code># Function to replace detected PII entities with their respective categories
def replace_with_category(document, doc_result):
    &quot;&quot;&quot;Replace PII entities in the document with their categories.&quot;&quot;&quot;
    redacted_text = document
    for entity in sorted(doc_result.entities, key=lambda e: e.offset, reverse=True):
        redacted_text = redacted_text[:entity.offset] + f&quot;&lt;{entity.category.upper()}&gt;&quot; + redacted_text[entity.offset + entity.length:]
    return redacted_text
    
# Function to redact PII entities in a list of documents
def pii_redact_list(documents, language):
     &quot;&quot;&quot;This function takes the list of 5 documents replaces all the PII entities with their respective categories. The result is that rather than ****** the category e.g., &lt;ORGANISATION&gt; is listed in the string.
    The function first detects the language of all the documents. Next, it recognizes the PII entities. Finally it replaces the PII entities with their categories.&quot;&quot;&quot;
    responses = azure_text_analytics_client.recognize_pii_entities(documents, categories_filter=pii_categories, language=language)
    redacted_texts = []
    for idx in range(0, len(responses)):
        doc_text = documents[idx]
        doc_result = responses[idx]
        redacted_text = self.replace_with_category(doc_text, doc_result)
        redacted_texts.append(redacted_text)
    return redacted_texts
</code></pre>
","python, azure, azure-cognitive-services, named-entity-recognition, pii","<p><strong>Point 1:</strong>
You can simply check at the moment when you replace your items using the result:</p>
<pre><code>for entity in sorted(doc_result.entities, key=lambda e: e.offset, reverse=True):
        redacted_text = redacted_text[:entity.offset] + f&quot;&lt;{entity.category.upper()}&gt;&quot; + redacted_text[entity.offset + entity.length:]
</code></pre>
<p>you can check if your <code>entity.text</code> matches one of the values you would like to keep</p>
<p><strong>Point 2</strong>: this looks like a correct way</p>
",0,0,435,2023-09-08 10:42:25,https://stackoverflow.com/questions/77066202/how-can-i-allow-certain-entities-e-g-names-organizations-in-azures-pii-enti
"Python API usage for coreference, semantic graph and NERC","<h1>Intro</h1>
<p>Hi, I have been using freeling for a few months now to extract triplets. So far I have succeded in doing so by using the dependency tree and the full parse tree, but I am trying to add NERC.</p>
<h1>My work so far</h1>
<p>I checked the tutorial for python, but I couldn't find anything beyond depdency parsing. So I went through the class list (since the same classes should be available for python and c++) but it is not very clear how to retrieve the named entities and after checking the output of the analyzer sampler I have a few questions about the performance of the NER module.</p>
<h1>Problems</h1>
<p>So what I'm asking if anyone can help me with is the following:</p>
<ol>
<li>Doubt about entities: Using the example &quot;Sobre la mesa María ve y coge una manzana, un sombrero, una llave y dos paraguas rojo.&quot; I realized that working with capitalized words and lowercase produce different results, but by making it all lowercase the entity recognition stops recognizing &quot;maría&quot; as a person. Is there are workaround for this or am I going in the wrong direction? The main problem is that &quot;maría&quot; not recognized as a named entity (which i need it to be by the way) results in &quot;maría&quot; not being the subject of the sentence anymore. Im using:</li>
</ol>
<p>neclass = pyfreeling.ner(lpath + &quot;/nerc/ner/ner-ab-rich.dat&quot;)</p>
<ol start=""2"">
<li>How to retrieve named entities: Kind of a follow up of the previous question, how do I get the named entities? I couldn't find any code related to this and the semantic graph i obtain holds 0 entities.</li>
</ol>
<p>Any comments and suggestions are welcomed, thanks in advance.</p>
","python, nlp, named-entity-recognition, freeling","<p>Well aparently there are 3 NERC modules, one rule-based and two ML-based. All of them use capitalization as a feature, and since both models are trained on standard text, all NEs seen in training are capitalized. Therefore lowercase named entities are not likely to be recognized.</p>
<p>About the retrieval it seems that the get_label() from the nodes can provide this info if a word (or multiword) has a pos-tag starting with &quot;NP&quot;, then it means it was recognized by the NERC module.</p>
<p>This is based on freelings authors own explanation which you can find <a href=""https://nlp.lsi.upc.edu/freeling/node/727#comment-847"" rel=""nofollow noreferrer"">here</a></p>
",0,0,49,2023-09-21 08:42:53,https://stackoverflow.com/questions/77148560/python-api-usage-for-coreference-semantic-graph-and-nerc
How to convert Doccano exported JSONL format to spaCy format？,"<p>I want to use my own data set to train a named entity recognition model. The data set is exported by the annotation tool Doccano. The format is JSONL (not JSON), but spaCy does not support such a data format input model. How do I convert it? ?
Here's what my dataset looks like:</p>
<pre><code>{&quot;id&quot;:17,&quot;text&quot;:&quot;In this work, the effect of CFs with zeolite on the mechanical, tribological prop-erties, and structure of PTFE was investigated. \nThe developed materials with a CF content of 1–5 wt.% retained their deformation and strength properties at the level of the initial polymer. \nThe compressive stress of PCM increased by 7–53%, and the yield point by 30% relative to the initial polymer. \nIt was found that with an increase in the content of fillers, the degree of crystallinity increased, and the density decreased in comparison with unfilled PTFE. \nCombining fillers (CF\/Zt) into PTFE reduced the wear rate by 810 times relative to the initial polymer. Tribochemical reactions were shown by IR spectroscopy.\nSEM established the formation of secondary structures in the form of tribofilms on the friction surface, which, together with CFs, protect the surface layer of the material from destruction during friction. \nThe wear resistance of the composite material PTFE\/CF\/Zt was effectively improved, and the coefficient of friction was low compared to PTFE\/CF\/Kl and PTFE\/CF\/Vl.&quot;,&quot;entities&quot;:[{&quot;id&quot;:298,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:1049,&quot;end_offset&quot;:1059},{&quot;id&quot;:545,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:960,&quot;end_offset&quot;:971},{&quot;id&quot;:299,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:1064,&quot;end_offset&quot;:1074},{&quot;id&quot;:607,&quot;label&quot;:&quot;value&quot;,&quot;start_offset&quot;:176,&quot;end_offset&quot;:184}],&quot;relations&quot;:[],&quot;Comments&quot;:[]}
</code></pre>
<p>I have also tried many online methods, but none of them seem to work.</p>
","nlp, spacy, named-entity-recognition, doccano","<p>You can modify the script given at <a href=""https://github.com/explosion/projects/blob/v3/pipelines/ner_demo/scripts/convert.py"" rel=""nofollow noreferrer"">explosion/projects/pipelines/ner_demo/scripts
/convert.py</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import json
import warnings

import spacy
from spacy.tokens import DocBin

def read_jsonl(fpath):
    with open(fpath, &quot;r&quot;) as f:
        for line in f:
            yield json.loads(line)


nlp = spacy.blank(&quot;en&quot;)
doc_bin = DocBin()
docs = []
for data in read_jsonl(&quot;data.jsonl&quot;):
    doc = nlp.make_doc(data[&quot;text&quot;])
    ents = []
    for entity in data[&quot;entities&quot;]:
        start = entity[&quot;start_offset&quot;]
        end = entity[&quot;end_offset&quot;]
        label = entity[&quot;label&quot;]
        span = doc.char_span(
            start_idx=start,
            end_idx=end,
            label=label,
            alignment_mode=&quot;strict&quot;,
        )
        if span is None:
            msg = (
                f&quot;Skipping entity [{start}, {end}, {label}] in the &quot;
                &quot;following text because the character span &quot;
                &quot;'{doc.text[start:end]}' does not align with token &quot;
                &quot;boundaries:\n\n{repr(text)}\n&quot;
            )
            warnings.warn(msg)
        else:
            ents.append(span)
    doc.set_ents(entities=ents)
    doc_bin.add(doc)

doc_bin.to_disk(&quot;train.spacy&quot;)
</code></pre>
<p>The data format you have given looks a bit different from the Doccano format that I'm used to, but the above should work.</p>
",0,0,826,2023-10-07 02:55:05,https://stackoverflow.com/questions/77248199/how-to-convert-doccano-exported-jsonl-format-to-spacy-format
Can a Named Entity Recognition (NER) spaCy model or any code like an entity ruler around it catch my new further date patterns also as DATE entities?,"<h2>Anonymization of entities found by a NER model</h2>
<p>I try to anonymize files by means of a NER model for German text that sometimes may have a few English words. If I take spaCy NER models for German and English like de_core_news_sm and en_core_web_sm, they find town names or persons, and at least the English model finds &quot;Dezember 2022&quot;, but it does not find the full date like &quot;15. Dezember 2022&quot;.</p>
<h2>Changing the entity recognition</h2>
<p>I cannot change the matches of the model. I thought I could take an entity ruler to change the NER model, but the NER model seems to be fixed, and I do not know how my own entity ruler can outweigh the spaCy NER model, and also, how I can get any entity ruler to work at all, even if I disable the NER model. I shifted the entity ruler before the NER model in the spaCy pipeline, but I do not see any new replacements in the output.</p>
<p>Easy example, mainly from the main spaCy guide at <a href=""https://spacy.io/usage/rule-based-matching#entityruler-usage"" rel=""nofollow noreferrer"">Using the entity ruler</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.lang.de import German

nlp = German()
ruler = nlp.add_pipe(&quot;entity_ruler&quot;)

patterns = [
    {&quot;label&quot;: &quot;DATE&quot;, &quot;pattern&quot;: [               
        {&quot;lower&quot;: {&quot;regex&quot;: &quot;(?:0?[1-9]|[12][0-9]|3[01])[\.\s]{1,2}?(jan(?:uar)?|feb(?:ruar)?|mär(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)\.?\s?['`]?\d{0,4}&quot;}},
        {&quot;shape&quot;: {&quot;regex&quot;: &quot;(?:0?[1-9]|[12][0-9]|3[01])[\.\s]{1,2}?(0?[1-9]|1[0-2])\.?\s?['`]?\d{0,4}&quot;}},
        {&quot;lower&quot;: {&quot;regex&quot;: &quot;(?:jan(?:uar)?|feb(?:ruar)?|mär(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)\.?\s?['`]?\d{2,4}&quot;}},
        {&quot;lower&quot;: {&quot;regex&quot;: &quot;(?:januar|feb(?:ruar)?|mär(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?\.?)&quot;}},
        {&quot;shape&quot;: &quot;dd&quot;},
        {&quot;TEXT&quot;: {&quot;in&quot;: [&quot;15&quot;]}}
    ]},
    {&quot;label&quot;: &quot;ORG1&quot;, &quot;pattern&quot;: {&quot;LOWER&quot;: &quot;apple&quot;}},
    {&quot;label&quot;: &quot;GPE1&quot;, &quot;pattern&quot;: {&quot;LOWER&quot;: &quot;san&quot;}},
    {&quot;label&quot;: &quot;DATE1&quot;, &quot;pattern&quot;: {&quot;TEXT&quot;: [{&quot;regex&quot;: &quot;^(?:0?[1-9]|[12][0-9]|3[01])$&quot;}]}}
]
ruler.add_patterns(patterns)

# Taking the German Dezember here for the test of the German RegEx
doc = nlp(&quot;Apple eröffnet ein Büro in San Francisco am 15. Dezember 2022.&quot;)
</code></pre>
<p>Output:</p>
<pre class=""lang-bash prettyprint-override""><code>[]
</code></pre>
<h3>Question</h3>
<p>Can I code around a Named Entity Recognition (NER) spaCy model to catch further date patterns also as DATE entities so that this will outweigh the choice of the NER model?</p>
<p>The aim is that the full &quot;15. Dezember 2022&quot; is found as one DATE entity.</p>
<hr />
<h2>PS</h2>
<h3>Duplicate?</h3>
<p>I found <a href=""https://stackoverflow.com/q/67906945/11154841"">spacy how to add patterns to existing Entity ruler?</a> that tells me to retrain the custom entity ruler and do not add patterns since the questioner has trained the NER model:</p>
<blockquote>
<p>I have an existing trained custom NER model with NER and Entity Ruler
pipes. I want to update and retrain this existing pipeline.</p>
</blockquote>
<p>The question is &quot;how to add patterns to existing Entity ruler?&quot; asks more or less the same as I do here. But since the NER model is a custom one, the answers tell you to retrain the NER model with those patterns. That is why this question here is hopefully not a duplicate: I cannot retrain the NER model since it is a ready-made download from spaCy.</p>
<h3>Catastrophic forgetting?</h3>
<p>Mind that the answers there tell you not to ever add an entity ruler at all to the NER model if you can retrain your NER model since it may lead to &quot;catastrophic forgetting&quot; of the already trained NER model, read there for more. If that is right, I wonder what I am doing here at all since that would mean that I cannot merge the entity recognition that the spaCy NER model is trained on with another entity ruler. I highly doubt that this is true. Why should I not be able to check a text for some patterns of some entities and then run the spaCy NER model on top of that, and then let the first found entities outweigh the second? Why should that lead to catastrophic forgetting if we speak about two models? Catastrophic forgetting means that the NER model gets retrained on only the new text that I take for the entity ruler. My new input text would be just one sentence with a date. Then, it would be easy to find out whether catastrophic forgetting happens at all. I can just run the pipeline on a sentence with more entities other than dates and see what happens.
Yet, I guess that my thoughts here are wrong, so that we do not have two models, but instead one entity recognition model that is a merger of the entity ruler and the NER model. That is also how I understood the entity ruler in the first place. But even then, I can still test this on catastrophic forgetting easily: if the entity recognition gets much worse on a big file, then I know that there is catastrophic forgetting. If you ask me, this sounds too strange to be true. I doubt that the answers of the other question are right.</p>
","python, python-3.x, spacy, named-entity-recognition, spacy-3","<h1>Main things</h1>
<h2>Each match is one label</h2>
<p>You have to list label under label, you cannot just put all of the regex patterns into one label. See a good code that underlines this at <a href=""https://stackoverflow.com/a/57546292/11154841"">Add multiple EntityRuler with spaCy (ValueError: 'entity_ruler' already exists in pipeline)</a>.</p>
<h2>Pattern format</h2>
<p>You have to write <code>ORTH</code>, <code>TEXT</code> or <code>LOWER</code> (and not &quot;SHAPE&quot; as I tried it above) and then in a nested bracket <code>REGEX</code>. See full list at <a href=""https://spacy.io/api/matcher#patterns"" rel=""nofollow noreferrer"">spaCy - Matcher - Patterns</a>.</p>
<h2>No embedded spaces in RegEx</h2>
<p>And you cannot RegEx match the already tokenized data with words that have spaces - since the tokenizer has already split the data into tokens by means of these spaces. There aren't any spaces left in the tokenized data. The only way to match them is to break up any regex with embedded \s+ into separate match tokens, see no answer to this question at:</p>
<ul>
<li><p><a href=""https://stackoverflow.com/q/56711352/11154841"">How to use standard regex with SpaCy's Matcher or PhraseMatcher while allowing spaces inside the regex</a></p>
<p>which is linked with:</p>
</li>
<li><p><a href=""https://stackoverflow.com/q/56533422/11154841"">Adding REGEX entities to SpaCy's Matcher</a>.</p>
</li>
</ul>
<h2>Squared brackets</h2>
<p>In the <a href=""https://spacy.io/api/entityruler/#config"" rel=""nofollow noreferrer"">spaCy guide on the entity ruler</a>, the code example &quot;explosion/spaCy/master/spacy/pipeline/entityruler.py&quot; that puts two matches in a row instead of a RegEx with embedded spaces is not bad coding, but needed just like that:</p>
<p><code>{'label': 'GPE', 'pattern': [{'lower': 'san'}, {'lower': 'francisco'}]}</code></p>
<p>Astonishingly, you have to write such squared brackets not just for two or more tokens (which means that they are neighboured, in a row), but even one token needs these squared brackets if you add the &quot;LOWER&quot; attribute at the beginning!(!) You would think that the squared brackets are just a start of a list, which they are, but the list format seems to be needed also for just one match. I checked this with <code>{'label': 'GPE', 'pattern': [{'LOWER': 'apple'}]}</code>, which worked, while it did not work without the squared brackets, the code did not find the word &quot;Apple&quot; as an entity, only &quot;apple&quot;.</p>
<h1>Code example</h1>
<p>A good guide that wraps it up is at:</p>
<ul>
<li><a href=""https://python.plainenglish.io/a-basic-named-entity-recognition-ner-with-spacy-in-10-lines-of-code-in-python-c53316ce9c4c"" rel=""nofollow noreferrer"">A basic Named entity recognition (NER) with SpaCy in 10 lines of code in Python</a>, which is followed by:</li>
<li><a href=""https://python.plainenglish.io/a-closer-look-at-entityruler-in-spacy-rule-based-matching-44d01c43fb6"" rel=""nofollow noreferrer"">A Closer Look at EntityRuler in SpaCy Rule-based Matching -&gt; Try with EntityRuler</a>. It shows how rule-based Matching in SpaCy works, both for phrase matcher and token matcher.</li>
</ul>
<h1>Fixed code</h1>
<p>With these hints, I could find an answer to the question above.</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.lang.de import German

nlp = German()
ruler = nlp.add_pipe(&quot;entity_ruler&quot;)
patterns = [
    {&quot;label&quot;: &quot;ORG1&quot;, &quot;pattern&quot;: {&quot;LOWER&quot;: &quot;apple&quot;}},
    {&quot;label&quot;: &quot;ORG2&quot;, &quot;pattern&quot;: [{&quot;LOWER&quot;: &quot;apple&quot;}]},
    {&quot;label&quot;: &quot;GPE1&quot;, &quot;pattern&quot;: {&quot;LOWER&quot;: &quot;san&quot;}},
    {&quot;label&quot;: &quot;GPE2&quot;, &quot;pattern&quot;: [{&quot;LOWER&quot;: &quot;san&quot;}]},
    {&quot;label&quot;: &quot;GPE4&quot;, &quot;pattern&quot;: [{&quot;LOWER&quot;: &quot;san&quot;}, {&quot;LOWER&quot;: &quot;francisco&quot;}]},
    {&quot;label&quot;: &quot;DATE1&quot;, &quot;pattern&quot;: {&quot;TEXT&quot;: [{&quot;regex&quot;: &quot;^(?:0?[1-9]|[12][0-9]|3[01])$&quot;}]}},
    {&quot;label&quot;: &quot;DATE2&quot;, &quot;pattern&quot;: [{&quot;TEXT&quot;: {&quot;regex&quot;: &quot;^(?:0?[1-9]|[12][0-9]|3[01])$&quot;}}]},
    {&quot;label&quot;: &quot;DATE3&quot;, &quot;pattern&quot;: [{&quot;TEXT&quot;: {&quot;regex&quot;: &quot;(?:0?[1-9]|[12][0-9]|3[01])&quot;}}, {&quot;LOWER&quot;: {&quot;regex&quot;: &quot;(jan(?:uar)?|feb(?:ruar)?|mär(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)&quot;}}, {&quot;TEXT&quot;: {&quot;regex&quot;: &quot;['`]?\d{2,4}&quot;}}]},
    {&quot;label&quot;: &quot;DATE4&quot;, &quot;pattern&quot;: [{&quot;TEXT&quot;: {&quot;regex&quot;: &quot;(?:0?[1-9]|[12][0-9]|3[01])&quot;}}, {&quot;LOWER&quot;: {&quot;regex&quot;: &quot;(jan(?:uar)?|feb(?:ruar)?|mär(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)&quot;}}]},
    {&quot;label&quot;: &quot;DATE5&quot;, &quot;pattern&quot;: [{&quot;LOWER&quot;: {&quot;regex&quot;: &quot;(?:jan(?:uar)?|feb(?:ruar)?|mär(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)&quot;}}, {&quot;TEXT&quot;: {&quot;regex&quot;: &quot;['`]?\d{2,4}&quot;}}]},
    {&quot;label&quot;: &quot;DATE6&quot;, &quot;pattern&quot;: [{&quot;LOWER&quot;: {&quot;regex&quot;: &quot;^(?:januar|feb(?:ruar)?|mär(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)$&quot;}}]}
            ]
ruler.add_patterns(patterns)

# Taking the German Dezember here for the test of the German RegEx
doc = nlp(&quot;Apple is opening its first big office in San Francisco on 15. Dezember 2022.&quot;)
print([(ent.text, ent.label_) for ent in doc.ents])
</code></pre>
<p>Out:</p>
<pre class=""lang-py prettyprint-override""><code>[('Apple', 'ORG2'), ('San Francisco', 'GPE4'), ('15. Dezember 2022', 'DATE3')]
</code></pre>
<p>Mind that the same code, but with an English model will only find &quot;15 Dezember 2022&quot;:</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.lang.en import English
nlp = English()
</code></pre>
<p>Only if you run this on a German model, it will also find &quot;15. Dezember 2022&quot; with the dot. I guess that in English, this dot is read as a full stop of a sentence. Since the sentence tokenizer runs before the word tokenizer, the tokens &quot;15&quot;, &quot;Dezember&quot;, and &quot;2022&quot; cannot be found together as one match anymore.</p>
<p>The code above also proves that you do not need to sort the patterns by the number of tokens, like &quot;15. Dezember 2022&quot;, &quot;15. Dezember&quot;, &quot;Dezember 2022&quot;, and &quot;Dezember&quot;, since it chooses the match with the most tokens by default. Else it would catch the number of label &quot;DATE2&quot; at first, and then, the full date could not be found anymore.</p>
",0,1,882,2023-12-21 22:02:13,https://stackoverflow.com/questions/77700760/can-a-named-entity-recognition-ner-spacy-model-or-any-code-like-an-entity-rule
Extracting and Identifying locations with NLP + Spacy,"<p>My goal is to be able to recognize (aka identify) and identify (aka name, retrieve an ID) locations from text using NLP. I'm using Spacy specifically.</p>
<p>There are about 1,000 possible locations, but the difficulty is that they are unlikely to be written in a fully qualified way, not to mention spelling mistakes and aliases. For example, the Mission neighborhood in San Francisco written in a fully-qualified way might be <code>(1) Mission (2) City of San Francisco (3) San Francisco County (4) California (5) US</code>. (The numbers are just to illustrate the separate pieces.) However, many people might write it as <code>(1) Mission (2) City of San Francisco</code>, or <code>(1) Mission</code>, or <code>(1) Mission (2) City of San Francisco (4) California</code>. (Not to mention that #1 might be called &quot;Mission District&quot;, #2 might be called &quot;San Francisco&quot;, #4 might be &quot;CA&quot;, etc.)</p>
<p>So my goal is to be able have an ID for &quot;Mission&quot; and all other neighborhoods, and ID for California and some other states, etc. If the text is <em>like</em> <code>Mission, San Francisco, CA</code> then I get the Mission ID. If the text is <em>like</em> <code>San Francisco, CA</code> then I get the San Francisco ID.</p>
<p>It's also easy to create synthetic training data by creating aliases of the individual location pieces (e.g., (a) &quot;City of San Francisco&quot;, (b) &quot;San Francisco&quot;, (c) &quot;San Francisco City&quot;) and permutations of the &quot;name chain&quot; (e.g, 1 + 2 + 3 + 4 + 5, 1 + 2, 1 + 2 + 5, etc) for each alias. Rough estimate is about 50 combinations of alias and name chain per location, or about O(50,000) total values.</p>
<p>So, extraction seems to be a good job for NER. The surrounding text usually has a bit of context (e.g., &quot;Location: ....&quot; or &quot;Comes from ...&quot;.</p>
<p>However, I'm unsure about the ability to do identification. My understanding is that much of the NER identification (e.g., Spacy's <a href=""https://spacy.io/api/entitylinker"" rel=""nofollow noreferrer"">EntityLinker</a>, which I planned on using) relies on <a href=""https://github.com/explosion/projects/tree/master/nel-emerson/"" rel=""nofollow noreferrer"">surrounding context</a>. I expect that there will be very little surrounding context that would help disambiguate one of the O(1000) locations from others. I also understand that EntityLinker matches on the token is lookup and not statistical (in other words, the value is from disambiguating when you have multiple exact-string matches and not from disambiguating from multiple very-fuzzy matches).</p>
<p>The KnowledgeBase / LookupDB does have a <a href=""https://spacy.io/api/inmemorylookupkb#add_alias"" rel=""nofollow noreferrer"">mechanism for setting aliases</a>, so I could add each permutation as an alias. But at that point I feel like I'm not getting any value out of the EntityLinker's statistical models.</p>
<p>If I have to create a gazetteer for the identification aspect, then maybe it makes sense to put all my effort into the gazetteer and skip the NER?</p>
","nlp, spacy, named-entity-recognition","<p>Thanks to Vimal for the thoughts. As I suspected, and Vishal confirmed, I needed to extract the string first, and then process it with a separate, non-NLP algorithm.</p>
<p>I ended up solving this in two ways, and wanted to document my findings.</p>
<ol>
<li>With some testing I found that an LLM (GPT) was actually pretty effective at determining the &quot;administrative hierarchy&quot; given the extracted string. This prompt, for example:</li>
</ol>
<pre><code>Evaluate the following place identifier and determine the most likely place. List the administrative entities from the lowest-level to the highest-level. Explain your reasoning. &quot;&quot;&quot;'Castro, San Francisco, U.S.&quot;&quot;&quot; 
</code></pre>
<p>returns this (plus some additional explanation):</p>
<pre><code>
The place identifier &quot;Castro, San Francisco, U.S.&quot; likely refers to a specific location within the city of San Francisco in the United States. 
</code></pre>
<p>I can't find the final version of my prompt at the moment, but I was able to tweak it to get it to provide JSON with the administrative entities in order (national, first level, second level, etc), plus I asked for any &quot;geographical feature&quot; as a catch-all. (In some cases, I found that my extracted term was something like &quot;Bay Area, United States&quot;, and GPT was able to sort that out with the right prompting.) There was a little bit of hallucination in my testing, which worried me.</p>
<ol start=""2"">
<li>With all that being said, I ended up on a much lower-tech approach, along the lines of my original gazetteer idea. My original plan was to use the gazetteer idea and then sort out the remaining strings with GPT. I ended up matching like 98% using this approach, and the unmatched strings were pretty objectively wrong and not worth sending to GPT. (Like a city with an incorrect country.)</li>
</ol>
<p>To create the gazetteer:</p>
<p>This approach used a dictionary which I compiled from the wikidata API search API. I did a first-pass and sent all the strings through Wikidata search to get the top 10 matching entities for each search string. E.g.,</p>
<pre><code>https://www.wikidata.org/w/api.php?action=query&amp;list=search&amp;srsearch=castro%20san%20francisco%20california&amp;srwhat=text&amp;srlimit=10&amp;srprop=titlesnippet|categorysnippet&amp;srsort=incoming_links_desc
</code></pre>
<p>I did some pre-filtering to exclude any entity that wasn't an <code>instance of</code> or <code>subclass of</code> geographical features or administrative regions (using lists that I manually searched for and downloaded manually).</p>
<p>Then I ran all those entities through a  method that stored the data (including name, aliases, lat/lng, etc) and walked up through the &quot;administrative entity&quot; and &quot;country&quot; paths to collect the family tree.</p>
<p>To search for place names:</p>
<ol>
<li>Compiled a dictionary where the keys were entity names and aliases.</li>
<li>I took the list of places <code>(castro, san francisco, california)</code> and started with the lowest-level string (castro) and did a fuzzy search against the dictionary keys to look for a match. Anything that matched over, e.g., 85% was chosen as a candidate.</li>
<li>Then I created a list of all the candidate's parent (+grandparent/etc) names and aliases, and I looped through the next place names to try to match every place against a name in the parents list, and got those scores.</li>
<li>Added the scores up. Some other operations to divide by the number of places that I was looking at, bias toward places with fewer parents (so Castro Valley, California would score higher than Castro, San Francisco, California), etc.</li>
</ol>
<p>All in all, this was surprisingly effective.</p>
",0,0,1093,2024-02-06 22:34:12,https://stackoverflow.com/questions/77951208/extracting-and-identifying-locations-with-nlp-spacy
SpaCy: Regex pattern does not work in rule-based matcher,"<p>I am trying to define a regular expression to use as text pattern in the entity ruler component in my spaCy model.
The aim is to add tokens with &quot;COMP&quot; label whenever it finds words structured like this:</p>
<ul>
<li>XXX-Ynnn</li>
<li>XXX Ynnn
Where 'XXX' are trigrams from a list, 'Y' is a letter and 'nnn' a digit combination.</li>
</ul>
<p>To do so, I use the following method</p>
<pre><code>def add_component_patterns_re(input_references, model_ruler):
    ruler = model_ruler
    ref_patterns = []
    letters = ['V', 'B', 'F', 'K', 'S']

    print(&quot;Adding component patterns&quot;)
    for ref in input_references.iloc[:, 0]:
        # print(f&quot;Adding references for system: {ref}&quot;)
        for letter in letters:
            pattern_text = fr'{ref}(-| ){letter}[0-9]{{3}}'
            pattern = {&quot;TEXT&quot;: {&quot;REGEX&quot;: fr'{ref}(-| ){letter}[0-9]{{3}}'}}
            ref_patterns.append({&quot;label&quot;:&quot;COMP&quot;, &quot;pattern&quot;:pattern})
    ruler.add_patterns(ref_patterns)

    return ref_patterns
</code></pre>
<p>Printing out the added patterns, it seems to me that the output list is correct. So my guess is that I am doing something wrong when defining the pattern to add to the ruler.
For information, i've also tried to change the pattern variable as a list entry, like this:</p>
<p><code>            pattern = [{&quot;TEXT&quot;: {&quot;REGEX&quot;: fr'{ref}(-| ){letter}[0-9]{{3}}'}}]</code></p>
<p>But the result is the same, it can't seem to get any match.</p>
<p>Does someone have any suggestion? Thanks in advance!</p>
","python, nlp, spacy, named-entity-recognition","<p>In the end I got</p>
<pre><code>print(f&quot;Adding references for system: {ref}&quot;)
    for letter in letters:
        for nnn in range(1000):
            pattern = f&quot;{ref}-{letter}{nnn:03d}&quot;
            ref_patterns.append({&quot;label&quot;: &quot;COMP&quot;, &quot;pattern&quot;: pattern})
            pattern = f&quot;{ref} {letter}{nnn:03d}&quot;
            ref_patterns.append({&quot;label&quot;: &quot;COMP&quot;, &quot;pattern&quot;: pattern})
</code></pre>
<p>For each pattern. The code is lengthier and a tad slower but it does the job just fine!</p>
",0,1,96,2024-03-11 13:19:59,https://stackoverflow.com/questions/78140912/spacy-regex-pattern-does-not-work-in-rule-based-matcher
PubTator API doesn&#39;t return session number as expected,"<p>Currently trying to validate that PubTator's API for Named Entity Recognition (NER) works and returns expected output format. I downloaded the example files included in the sample Python code at <a href=""https://www.ncbi.nlm.nih.gov/research/pubtator/api.html"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/research/pubtator/api.html</a>. Using one of these example input files , I tried the following <code>curl</code> command as indicated in their documentation:</p>
<pre class=""lang-bash prettyprint-override""><code>curl -X POST --data-binary @input/ex2.PubTator https://www.ncbi.nlm.nih.gov/research/pubtator-api/annotations/annotate/submit/Gene
</code></pre>
<p>I just get html code for an error-500 webpage. input/ex.PubTator is simply a single abstract, and it doesn't contain any special characters that I can discern:</p>
<pre><code>20085714|t|Autosomal-dominant striatal degeneration is caused by a mutation in the phosphodiesterase 8B gene.
20085714|a|Autosomal-dominant striatal degeneration (ADSD) is an autosomal-dominant movement disorder affecting the striatal part of the basal ganglia. ADSD is characterized by bradykinesia, dysarthria, and muscle rigidity. These symptoms resemble idiopathic Parkinson disease, but tremor is not present. Using genetic linkage analysis, we have mapped the causative genetic defect to a 3.25 megabase candidate region on chromosome 5q13.3-q14.1. A maximum LOD score of 4.1 (Theta = 0) was obtained at marker D5S1962. Here we show that ADSD is caused by a complex frameshift mutation (c.94G&gt;C+c.95delT) in the phosphodiesterase 8B (PDE8B) gene, which results in a loss of enzymatic phosphodiesterase activity. We found that PDE8B is highly expressed in the brain, especially in the putamen, which is affected by ADSD. PDE8B degrades cyclic AMP, a second messenger implied in dopamine signaling. Dopamine is one of the main neurotransmitters involved in movement control and is deficient in Parkinson disease. We believe that the functional analysis of PDE8B will help to further elucidate the pathomechanism of ADSD as well as contribute to a better understanding of movement disorders.
</code></pre>
<p>What am I doing wrong?</p>
","named-entity-recognition, ncbi, pubmed-api","<p>The URL I'm using for my named entity recognition API request appears to be deprecated now that PubTator3 is in beta.</p>
<p>If submitting a short string of text, use ...</p>
<pre class=""lang-bash prettyprint-override""><code>curl -X POST https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/RESTful/request.cgi -H &quot;Content-Type: application/x-www-form-urlencoded&quot; -d &quot;text=Possible role of valvular serotonin 5-HT receptors in the cardiopathy associated with fenfluramine.&amp;bioconcept=Gene&quot;
</code></pre>
<p>If submitting entire contents of a plain text file, which is probably the more relevant use case, one needs to work around the <code>curl</code> character limit on arguments. Best I could manage was ...</p>
<pre class=""lang-bash prettyprint-override""><code>printf &quot;text=%s&amp;bioconcept=Gene&quot; &quot;$(cat ___data/plain_txts/yang2020.txt)&quot; | curl -X POST https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/RESTful/request.cgi -H &quot;Content-Type: application/x-www-form-urlencoded&quot; --data-binary @-
</code></pre>
<p>Regrettably, the output annotation can still be truncated if the input file has too much content.</p>
",1,0,137,2024-03-27 16:34:43,https://stackoverflow.com/questions/78233240/pubtator-api-doesnt-return-session-number-as-expected
Extracting dates from a sentence in spaCy,"<p>I have a string like so:</p>
<pre><code>&quot;The dates are from 30 June 2019 to 1 January 2022 inclusive&quot;
</code></pre>
<p>I want to extract the dates from this string using spaCy.</p>
<p>Here is my function so far:</p>
<pre><code>def extract_dates_with_year(text):
    doc = nlp(text)
    dates_with_year = []
    for ent in doc.ents:
        if ent.label_ == &quot;DATE&quot;:
            dates_with_year.append(ent.text)
    return dates_with_year
</code></pre>
<p>This returns the following output:</p>
<pre><code>['30 June 2019 to 1 January 2022']
</code></pre>
<p>However, I want output like:</p>
<pre><code>['30 June 2019', '1 January 2022']
</code></pre>
","python, regex, nlp, spacy, named-entity-recognition","<p>The issue is that <code>&quot;to&quot;</code> is considered part of the date. So when you do <code>for ent in doc.ents</code>, your loop only has one iteration, as <code>&quot;30 June 2019 to 1 January 2022&quot;</code> is considered one entity.</p>
<p>As you don't want this behaviour, you can amend your function to split on <code>&quot;to&quot;</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def extract_dates_with_year(text):
    doc = nlp(text)
    dates_with_year = []
    for ent in doc.ents:
        if ent.label_ == &quot;DATE&quot;:
            for ent_txt in ent.text.split(&quot;to&quot;):
                dates_with_year.append(ent_txt.strip())
    return dates_with_year
</code></pre>
<p>This will correctly handle dates like these, as well as single dates, and strings with multiple dates:</p>
<pre class=""lang-py prettyprint-override""><code>txt = &quot;&quot;&quot;
     The dates are from 30 June 2019 to 1 January 2022 inclusive.
     And oddly also 5 January 2024.
     And exclude 21 July 2019 until 23 July 2019.
&quot;&quot;&quot;

extract_dates_with_year(txt)

# Output:
[
 '30 June 2019',
 '1 January 2022',
 '5 January 2024',
 '21 July 2019',
 '23 July 2019'
]
</code></pre>
",-1,1,76,2024-04-06 17:12:39,https://stackoverflow.com/questions/78285241/extracting-dates-from-a-sentence-in-spacy
Create BIO format to a sentence from a json file - To train NER model,"<p>I have a JSON file that'll be used as data for a NER model.
It has a sentence and the relevant entities in that specific sentence.
I want to create a function that will generate a BIO-labeled string for each sentence according to the entities</p>
<p>for example the following object from the JSON file</p>
<pre class=""lang-js prettyprint-override""><code>{
      &quot;request&quot;: &quot;I want to fly to New York on the 13.3&quot;,
      &quot;entities&quot;: [
        {&quot;start&quot;: 16, &quot;end&quot;: 23, &quot;text&quot;: &quot;New York&quot;, &quot;category&quot;: &quot;DESTINATION&quot;},
        {&quot;start&quot;: 32, &quot;end&quot;: 35, &quot;text&quot;: &quot;13.3&quot;, &quot;category&quot;: &quot;DATE&quot;}
      ]
} 
</code></pre>
<p>&quot;I want to fly to New York on the 13.3&quot;
The corresponding BIO label will be
&quot;O O O O O B-DESTINATION I-DESTINATION O O B-DATE&quot;
where B-category is the beginning of that category
I-category stands for inside and O for outside.</p>
<p>I'm looking for a Python code to iterate on each object in the JSON file that will generate a BIO-label for it.</p>
<p>change the JSON format if necessary</p>
","python, machine-learning, named-entity-recognition","<p>This is just a quick implementation for the above task, and many optimizations are possible, which can be explored later, but at first glace here is the function:</p>
<pre><code>def BIO_converter(r, entities):
    to_replace = {} # needed to maintain all the NER to be replaced
    for i in entities:
        sub = r[i['start']+1:i['end']+2].split(' ') # 1 indexed values in entities
        if len(sub) &gt; 1:
            vals = [f&quot;B-{i['category']}&quot;] + ([f&quot;I-{i['category']}&quot;] * (len(sub)-1))
        else:
            vals = [f&quot;B-{i['category']}&quot;]

        to_replace = to_replace | dict(zip(sub,vals))

    r = r.split(' ')
    r = [to_replace[i] if i in to_replace else 'O' for i in r ]
    return ' '.join(r)

js = {
        &quot;request&quot;: &quot;I want to fly to New York on the 13.3&quot;,
        &quot;entities&quot;: [
          {&quot;start&quot;: 16, &quot;end&quot;: 23, &quot;text&quot;: &quot;New York&quot;, &quot;category&quot;: &quot;DESTINATION&quot;},
          {&quot;start&quot;: 32, &quot;end&quot;: 35, &quot;text&quot;: &quot;13.3&quot;, &quot;category&quot;: &quot;DATE&quot;}
        ]
      }
BIO_converter(js['request'], js['entities'])
</code></pre>
<p>Should output:</p>
<pre><code>O O O O O B-DESTINATION I-DESTINATION O O B-DATE
</code></pre>
",1,0,126,2024-05-02 16:56:56,https://stackoverflow.com/questions/78420651/create-bio-format-to-a-sentence-from-a-json-file-to-train-ner-model
SpaCy GPU memory utilization for NER training,"<p>My training code:</p>
<pre><code>spacy.require_gpu()
nlp = spacy.blank('en')

if 'ner' not in nlp.pipe_names:
    ner = nlp.add_pipe('ner')
else:
    ner = nlp.get_pipe('ner')

docs = load_data(ANNOTATED_DATA_FILENAME_BIN)
train_data, test_data = split_data(docs, DATA_SPLIT)

unique_labels = set(ent.label_ for doc in train_data for ent in doc.ents)
for label in unique_labels:
    ner.add_label(label)

optimizer = nlp.initialize()

for i in range(EPOCHS):
    print(f&quot;Starting Epoch {i+1}...&quot;)
    losses = {}
    batches = minibatch(train_data, size=compounding(4., 4096, 1.001))
    for batch in batches:
        for doc in batch:
            example = Example.from_dict(doc, {'entities': [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})
            nlp.update([example], drop=0.5, losses=losses, sgd=optimizer)
    print(f&quot;Losses at iteration {i}: {losses}&quot;)
</code></pre>
<p>This code almost completely does not utilize GPU memory. Utilization is about 11-13% during training, which is almost the same as idle.</p>
<p><a href=""https://i.sstatic.net/82YRwe4T.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82YRwe4T.jpg"" alt=""nvidia-smi"" /></a></p>
<p>I did allocation test with torch, and all 8Gigs are allocated, so server works fine.
The problem is with SpaCy or my code.</p>
<p>Could you please help?</p>
","python, gpu, spacy, named-entity-recognition, custom-training","<p>Quoting William James Mattingly, Ph.D:</p>
<blockquote>
<p>This may due to spaCy's change in training for 3.0. Training is done
for projects and via the command line. This is how we used to train
models for 2.0 and while it works, I believe there are certain issues
that arise. This may be one of those issues. The newer approach passes
an argument in the CLI when you train the model.
<a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">https://spacy.io/usage/training</a></p>
<p>In the docs you can specify in the config how to train and on which
device. Training spaCy's Statistical Models - spaCy spacy.io spaCy is
a free open-source library featuring state-of-the-art speed and
accuracy and a powerful Python API.</p>
<p><code>[system] gpu_allocator = &quot;pytorch&quot;</code></p>
<p>this is the important bit 👏 👍 😊</p>
<p>Then when you run train in the CLI, you'd do something like this:</p>
<p><code>python -m spacy train config.cfg --gpu-id 0</code></p>
</blockquote>
",0,2,85,2024-05-03 07:52:34,https://stackoverflow.com/questions/78423352/spacy-gpu-memory-utilization-for-ner-training
Do I need to use Named Entity Recognition (NER) in tokenization?,"<p>I am working on an NLP project for sentiment analysis. I am using SpaCy to tokenize sentences. As I was reading the <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">documentation</a>, I learned about NER. I've read that it can be used to extract entities from text for aiding a user's searching.</p>
<p>The thing I am trying to understand is how to embody it (<em>if I should</em>) in my tokenization process. I am giving an example.</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;Let's not forget that Apple Pay in 2014 required a brand new iPhone in order to use it.  A significant portion of Apple's user base wasn't able to use it even if they wanted to.  As each successive iPhone incorporated the technology and older iPhones were replaced the number of people who could use the technology increased.&quot;

sentence = sp(text) # sp = spacy.load('en_core_web_sm')

for word in sentence:
    print(word.text)

# Let
# 's
# not
# forget
# that
# Apple
# Pay
# in
# etc...

for word in sentence.ents:
  print(word.text + &quot; _ &quot; + word.label_ + &quot; _ &quot; + str(spacy.explain(word.label_)))

# Apple Pay _ ORG _ Companies, agencies, institutions, etc.
# 2014 _ DATE _ Absolute or relative dates or periods
# iPhone _ ORG _ Companies, agencies, institutions, etc.
# Apple _ ORG _ Companies, agencies, institutions, etc.
# iPhones _ ORG _ Companies, agencies, institutions, etc.
</code></pre>
<p>The first loops shows that 'Apple' and 'Pay' are different tokens. When printing the discovered entities in the second loop, it understands that 'Apply Pay' is an ORG. If yes, how could I achieve that (let's say) &quot;type&quot; of tokenization?</p>
<p>My thinking is, shouldn't 'Apple' and 'Pay' be tokenized as a single word together so that, when I create my classifier it will recognize it as an entity and not recognize a fruit ('Apple') and a verb ('Pay').</p>
","python, python-3.x, nlp, spacy, named-entity-recognition","<p>Tokenization typically is the splitting of a sentence into words or even subwords. I am not sure what you later plan to do with the data, but it is a convention in NLP to stick to either the document level, sentence level or word/token level. Having some mix of token and n-gram level (like <code>[&quot;Apple Pay&quot;, &quot;required&quot;, &quot;an&quot;, &quot;iPhone&quot;, &quot;to&quot;, &quot;use&quot;, &quot;it&quot;, &quot;.&quot;]</code> in my opinion will not help you in most later use cases.</p>
<p>If you later train a classifier (assuming you're talking about fine-tuning a transformer based language model on a token classification task) would then use something like the <a href=""https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"" rel=""nofollow noreferrer"">IOB format</a> to handle n-grams, e.g. like so:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Token</th>
<th>Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>Apple</td>
<td>B</td>
</tr>
<tr>
<td>Pay</td>
<td>I</td>
</tr>
<tr>
<td>required</td>
<td>O</td>
</tr>
<tr>
<td>an</td>
<td>O</td>
</tr>
<tr>
<td>iPhone</td>
<td>B</td>
</tr>
<tr>
<td>to</td>
<td>O</td>
</tr>
<tr>
<td>use</td>
<td>O</td>
</tr>
<tr>
<td>it</td>
<td>O</td>
</tr>
<tr>
<td>.</td>
<td>O</td>
</tr>
</tbody>
</table></div>
<p>Of course this depends on your application and directly merging to n-grams might work well for you. If you have some application where you are searching for frequent n-grams, you could use collocation metrics to extract those n-grams, e.g. using <a href=""https://www.nltk.org/howto/collocations.html"" rel=""nofollow noreferrer"">NLTK's CollocationFinder</a>.</p>
<p>Or as you mentioned use SpaCy either for <a href=""https://spacy.io/usage/linguistic-features#noun-chunks"" rel=""nofollow noreferrer"">noun chunk extraction</a> or <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">named entity recognition</a>. For the latter one, you could access the <a href=""https://stackoverflow.com/a/63285698/18189622"">token level ent_type_ and ent_iob_ attributes</a> to iterate over the tokens in the processed docs once and then merge these n-grams together based on their IOB-tags.</p>
",2,0,366,2024-07-22 13:28:14,https://stackoverflow.com/questions/78778988/do-i-need-to-use-named-entity-recognition-ner-in-tokenization
"NER versus LLM to extract name, gender, role and company from text","<p>I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.</p>
<p>I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.</p>
<p>Is there another, smaller LLM that might be good at this while using fewer processing resources?</p>
<p>Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)</p>
<p>Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?</p>
<p>Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.</p>
<p>Thanks in advance!</p>
","nlp, large-language-model, named-entity-recognition","<p>Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. <code>NER</code> is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use <code>NLTK</code> or <code>SpaCy</code> for this matter. My personal choice is <code>SpaCy</code>, however a <code>gender</code> as you defined is not a known named entity. you can see a list of named entities in <a href=""https://github.com/explosion/spaCy/discussions/9147"" rel=""nofollow noreferrer"">this doc</a>.</p>
<p>I guess what you mean by <code>gender</code> is the possible <code>gender</code> associated with the names of a <code>PERSON</code> mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use <a href=""https://pypi.org/project/gender-guesser/"" rel=""nofollow noreferrer""><code>gender-guesser</code> package</a>.</p>
<p>A possible solution would be like this:</p>
<pre><code>import spacy
import gender_guesser.detector as gender


nlp = spacy.load(&quot;en_core_web_sm&quot;)

def extract_info(text):
    doc = nlp(text)
    gender_detector = gender.Detector()

    for ent in doc.ents:
        if ent.label_ == &quot;PERSON&quot;:
            name = ent.text
            name_gender = gender_detector.get_gender(name)
    
    return doc.ents, name_gender
</code></pre>
<p>Note that <code>en_core_web_sm</code> is the small model available via spaCy, you can use the large model by specifying <code>en_core_web_lg</code>, just make sure that the model is downloaded before running your code. here's how you can download the model:</p>
<pre><code>python -m spacy download en_core_web_sm
</code></pre>
",1,1,1533,2024-08-21 07:39:13,https://stackoverflow.com/questions/78895710/ner-versus-llm-to-extract-name-gender-role-and-company-from-text
"How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)","<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>
<p>I have short product descriptions that I’d like to transform into structured attributes.</p>
<p>Example:</p>
<p>Input:</p>
<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”
</code></pre>
<p>Output:</p>
<pre><code>Year = 2017

Color = Red

Weight = 750

Weight Unit = ml
</code></pre>
<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>
<ol>
<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>
</li>
<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>
</li>
<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>
</li>
<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>
</li>
</ol>
<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>
<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>
<p>Appreciate any insight into approaches or suggested learning resources.</p>
<p></p>
<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>
","nlp, artificial-intelligence, large-language-model, named-entity-recognition","<p>LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.</p>
<p>For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.</p>
<pre><code>You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:

- **Year**: The vintage year of the wine.
- **Color**: The color of the wine (e.g., Red, White, Rosé).
- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).
- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).
- **Brand**: The brand or producer of the wine.
- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).

**Instructions:**

- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.
- Be cautious of potential ambiguities. For example:
  - A brand name may include words like &quot;Red&quot; or &quot;White&quot; (e.g., &quot;Red Head Vineyard&quot;) which should not be confused with the wine color.
  - Large numbers may represent weight (e.g., &quot;1500 ml&quot;) rather than a year.
- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.

**Output Format:**

Provide the extracted information in XML format, using the following structure:

&lt;Wine&gt;
&lt;Year&gt;{{Year}}&lt;/Year&gt;
&lt;Color&gt;{{Color}}&lt;/Color&gt;
&lt;Weight&gt;{{Weight}}&lt;/Weight&gt;
&lt;WeightUnit&gt;{{WeightUnit}}&lt;/WeightUnit&gt;
&lt;Brand&gt;{{Brand}}&lt;/Brand&gt;
&lt;GrapeVariety&gt;{{GrapeVariety}}&lt;/GrapeVariety&gt;
&lt;/Wine&gt;

**Examples:**

  1. **Input:**

 `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`

 **Output:**



```xml
   &lt;Wine&gt;
     &lt;Year&gt;2017&lt;/Year&gt;
     &lt;Color&gt;Red&lt;/Color&gt;
     &lt;Weight&gt;750&lt;/Weight&gt;
     &lt;WeightUnit&gt;ml&lt;/WeightUnit&gt;
     &lt;Brand&gt;La Lecciaia&lt;/Brand&gt;
     &lt;GrapeVariety&gt;Cabernet Sauvignon&lt;/GrapeVariety&gt;
   &lt;/Wine&gt;
   ```

   
   `Red Head Vineyard Chardonnay 2020 1.5L`

   **Output:**

   &lt;Wine&gt;
     &lt;Year&gt;2020&lt;/Year&gt;
     &lt;Color&gt;&lt;/Color&gt;
     &lt;Weight&gt;1.5&lt;/Weight&gt;
     &lt;WeightUnit&gt;L&lt;/WeightUnit&gt;
     &lt;Brand&gt;Red Head Vineyard&lt;/Brand&gt;
     &lt;GrapeVariety&gt;Chardonnay&lt;/GrapeVariety&gt;
   &lt;/Wine&gt;

 

    **Task:**
    
    Given the following wine description, extract the components and provide the output in XML format as specified.
    
    {win_description}
</code></pre>
<p>Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.</p>
<p>The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs</p>
",1,0,166,2024-10-21 20:54:56,https://stackoverflow.com/questions/79111733/how-to-derive-attributes-labels-from-short-plain-text-descriptions-ner-llm
Attaching custom KB to Spacy &quot;entity_linker&quot; pipe makes NER calls very poor,"<p>I want to run an entity linking job using a custom Knowledgebase alone, <strong>and not use the second step ML re-ranker that requires a training dataset / Spacy corpus</strong>. I want the NEL pipeline to only assign kb_ids based on the Knowledgebase-driven get_candidates() and using the prior_probabilities from my KB object. However, I'm noticing that as soon as I attached my custom KB, the <code>doc.ents</code> calls are super dumb, like it was lobotomized by the <code>entity_linker</code> pipe. Does the <code>entity_linker</code> pipe modify the spans that that ner pipe (which comes before it), calls?</p>
<p>Here are my doc.ents calls when I use the pretrained &quot;en_core_web_lg&quot; model:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_lg&quot;)
doc = nlp(&quot;The NY Times is my favorite newspaper.&quot;)
doc.ents
(The NY Times,)
</code></pre>
<p>When I run the same example through the nlp object with my custom KB attached to a downstream NEL pipe, I get this:</p>
<pre><code>entity_linker = nlp.add_pipe(&quot;entity_linker&quot;)
def create_kb(vocab):
    kb = DefaultKB(vocab=vocab, entity_vector_length=300)
    kb.from_disk(f&quot;assets/en/kb&quot;)
    return kb

entity_linker.set_kb(create_kb)
nlp.initialize()
doc = nlp(&quot;The NY Times is my favorite newspaper.&quot;)
print([(ent.text, ent.label_) for ent in doc.ents])  # Output of the NER
print([(ent.text, ent.kb_id_) for ent in doc.ents if ent.kb_id_])  # Output of the entity linker

[('The', 'ORG'), ('Times', 'ORG'), ('is', 'ORG'), ('my', 'ORG'), ('favorite', 'ORG'), ('newspaper', 'ORG'), ('.', 'ORG')]
[('The', 'Q3048768'), ('Times', 'Q11259'), ('is', 'NIL'), ('my', 'NIL'), ('favorite', 'NIL'), ('newspaper', 'NIL'), ('.', 'NIL')]
</code></pre>
<p>So the NER pipe is definitely calling different NER spans after I attach my custom Knowledgebase object. Please educate me on this pipe and whether or not I can attached my KB object to be intelligent straight away, without messing up the pretrained model's NER intelligence.</p>
","spacy, named-entity-recognition, entity-linking, knowledge-base-population","<p>What happens here is that this line</p>
<pre><code>nlp.initialize()
</code></pre>
<p>actually re-initializes <strong>all</strong> trained components in your pipeline: it sets all weights back to random initializations, effectively producing garbage as you saw from the NER results.</p>
<p>You do need to initialize the new <code>entity_linker</code> component however. You can do so by calling it on the component directly:</p>
<pre><code>entity_linker.initialize(...)
</code></pre>
<p>The latter will require an additional <code>get_examples</code> parameter (<a href=""https://spacy.io/api/entitylinker#initialize"" rel=""nofollow noreferrer"">https://spacy.io/api/entitylinker#initialize</a>) which might be a small hassle. Alternatively, you can disable the pipes that should be kept as is in a context manager when making the call on the <code>nlp</code> object:</p>
<pre><code>with nlp.select_pipes(disable=&quot;ner&quot;):
    nlp.initialize()
</code></pre>
<p>You can read more about this mechanism in the usage docs: <a href=""https://spacy.io/usage/training#initialization"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#initialization</a>. And in the API docs: <a href=""https://spacy.io/api/language#initialize"" rel=""nofollow noreferrer"">https://spacy.io/api/language#initialize</a></p>
<p>By the way - all of this should happen &quot;automagically&quot; when you use the config system in spaCy v3 and set the &quot;frozen&quot; components correctly, cf. <a href=""https://spacy.io/usage/training#config-custom"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#config-custom</a>.</p>
<p>You would have something like this:</p>
<pre><code>[nlp]
lang = &quot;en&quot;
pipeline = [&quot;ner&quot;,&quot;entity_linker&quot;]

[components.ner]
source = &quot;en_core_web_lg&quot;
component = &quot;ner&quot;

[components.entity_linker]
factory = &quot;entity_linker&quot;

[training]
frozen_components = [&quot;ner&quot;]
...
</code></pre>
<p>For the config route, you can take inspiration from this example project: <a href=""https://github.com/explosion/projects/tree/v3/benchmarks/nel"" rel=""nofollow noreferrer"">https://github.com/explosion/projects/tree/v3/benchmarks/nel</a></p>
<p>Hope that resolves things for you!</p>
",1,2,67,2024-12-18 19:29:03,https://stackoverflow.com/questions/79292283/attaching-custom-kb-to-spacy-entity-linker-pipe-makes-ner-calls-very-poor
